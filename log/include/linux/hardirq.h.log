commit 98a3bf195e1a14755da3d2b83e1dbb4a3158866d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:22 2020 +0200

    genirq: Provide __irq_enter/exit_raw()
    
    Like __irq_enter/exit() but without time accounting. To be used for "empty"
    system vectors like the scheduler IPI to avoid the overhead.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.671682341@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 3dc9102d16cf..03c9fece7d43 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -37,6 +37,17 @@ static __always_inline void rcu_irq_enter_check_tick(void)
 		lockdep_hardirq_enter();		\
 	} while (0)
 
+/*
+ * Like __irq_enter() without time accounting for fast
+ * interrupts, e.g. reschedule IPI where time accounting
+ * is more expensive than the actual interrupt.
+ */
+#define __irq_enter_raw()				\
+	do {						\
+		preempt_count_add(HARDIRQ_OFFSET);	\
+		lockdep_hardirq_enter();		\
+	} while (0)
+
 /*
  * Enter irq context (on NO_HZ, update jiffies):
  */
@@ -56,6 +67,15 @@ void irq_enter_rcu(void);
 		preempt_count_sub(HARDIRQ_OFFSET);	\
 	} while (0)
 
+/*
+ * Like __irq_exit() without time accounting
+ */
+#define __irq_exit_raw()				\
+	do {						\
+		lockdep_hardirq_exit();			\
+		preempt_count_sub(HARDIRQ_OFFSET);	\
+	} while (0)
+
 /*
  * Exit irq context and process softirqs if needed:
  */

commit 8a6bc4787f05d087fda8e11ead225c8830250703
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:21 2020 +0200

    genirq: Provide irq_enter/exit_rcu()
    
    irq_enter()/exit() currently include RCU handling. To properly separate the RCU
    handling code, provide variants which contain only the non-RCU related
    functionality.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.567023613@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 29b862aba740..3dc9102d16cf 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -40,7 +40,11 @@ static __always_inline void rcu_irq_enter_check_tick(void)
 /*
  * Enter irq context (on NO_HZ, update jiffies):
  */
-extern void irq_enter(void);
+void irq_enter(void);
+/*
+ * Like irq_enter(), but RCU is already watching.
+ */
+void irq_enter_rcu(void);
 
 /*
  * Exit irq context without processing softirqs:
@@ -55,7 +59,12 @@ extern void irq_enter(void);
 /*
  * Exit irq context and process softirqs if needed:
  */
-extern void irq_exit(void);
+void irq_exit(void);
+
+/*
+ * Like irq_exit(), but return with RCU watching.
+ */
+void irq_exit_rcu(void);
 
 #ifndef arch_nmi_enter
 #define arch_nmi_enter()	do { } while (0)

commit 2ab70319bc1f79228da4dce7b9d604740c9beeef
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:14 2020 +0200

    nmi, tracing: Make hardware latency tracing noinstr safe
    
    The hardware latency tracer calls into instrumentable functions. Move the
    calls into the RCU watching sections and annotate them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Link: https://lore.kernel.org/r/20200521202116.904176298@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index e07cf853aa16..29b862aba740 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -87,20 +87,24 @@ extern void rcu_nmi_exit(void);
 		arch_nmi_enter();				\
 		printk_nmi_enter();				\
 		lockdep_off();					\
-		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi() == NMI_MASK);			\
 		__preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		rcu_nmi_enter();				\
 		lockdep_hardirq_enter();			\
+		instrumentation_begin();			\
+		ftrace_nmi_enter();				\
+		instrumentation_end();				\
 	} while (0)
 
 #define nmi_exit()						\
 	do {							\
+		instrumentation_begin();			\
+		ftrace_nmi_exit();				\
+		instrumentation_end();				\
 		lockdep_hardirq_exit();				\
 		rcu_nmi_exit();					\
 		BUG_ON(!in_nmi());				\
 		__preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
-		ftrace_nmi_exit();				\
 		lockdep_on();					\
 		printk_nmi_exit();				\
 		arch_nmi_exit();				\

commit aaf2bc50df1f4bfc6857fc601fc7b21d5a18c6a1
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu May 21 22:05:15 2020 +0200

    rcu: Abstract out rcu_irq_enter_check_tick() from rcu_nmi_enter()
    
    There will likely be exception handlers that can sleep, which rules
    out the usual approach of invoking rcu_nmi_enter() on entry and also
    rcu_nmi_exit() on all exit paths.  However, the alternative approach of
    just not calling anything can prevent RCU from coaxing quiescent states
    from nohz_full CPUs that are looping in the kernel:  RCU must instead
    IPI them explicitly.  It would be better to enable the scheduler tick
    on such CPUs to interact with RCU in a lighter-weight manner, and this
    enabling is one of the things that rcu_nmi_enter() currently does.
    
    What is needed is something that helps RCU coax quiescent states while
    not preventing subsequent sleeps.  This commit therefore splits out the
    nohz_full scheduler-tick enabling from the rest of the rcu_nmi_enter()
    logic into a new function named rcu_irq_enter_check_tick().
    
    [ tglx: Renamed the function and made it a nop when context tracking is off ]
    [ mingo: Fixed a CONFIG_NO_HZ_FULL assumption, harmonized and fixed all the
             comment blocks and cleaned up rcu_nmi_enter()/exit() definitions. ]
    
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200521202116.996113173@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 621556efe45f..e07cf853aa16 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -2,31 +2,28 @@
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 
+#include <linux/context_tracking_state.h>
 #include <linux/preempt.h>
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>
 #include <asm/hardirq.h>
 
-
 extern void synchronize_irq(unsigned int irq);
 extern bool synchronize_hardirq(unsigned int irq);
 
-#if defined(CONFIG_TINY_RCU)
-
-static inline void rcu_nmi_enter(void)
-{
-}
+#ifdef CONFIG_NO_HZ_FULL
+void __rcu_irq_enter_check_tick(void);
+#else
+static inline void __rcu_irq_enter_check_tick(void) { }
+#endif
 
-static inline void rcu_nmi_exit(void)
+static __always_inline void rcu_irq_enter_check_tick(void)
 {
+	if (context_tracking_enabled())
+		__rcu_irq_enter_check_tick();
 }
 
-#else
-extern void rcu_nmi_enter(void);
-extern void rcu_nmi_exit(void);
-#endif
-
 /*
  * It is safe to do non-atomic ops on ->hardirq_context,
  * because NMI handlers may not preempt and the ops are
@@ -65,6 +62,14 @@ extern void irq_exit(void);
 #define arch_nmi_exit()		do { } while (0)
 #endif
 
+#ifdef CONFIG_TINY_RCU
+static inline void rcu_nmi_enter(void) { }
+static inline void rcu_nmi_exit(void) { }
+#else
+extern void rcu_nmi_enter(void);
+extern void rcu_nmi_exit(void);
+#endif
+
 /*
  * NMI vs Tracing
  * --------------

commit f93524eb9c54f49be150167918f6546b0a2e09b1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 12 21:01:16 2020 +0100

    sched,rcu,tracing: Avoid tracing before in_nmi() is correct
    
    If a tracer is invoked before in_nmi() becomes true, the tracer can no
    longer detect it is called from NMI context and behave correctly.
    
    Therefore change nmi_{enter,exit}() to use __preempt_count_{add,sub}()
    as the normal preempt_count_{add,sub}() have a (desired) function
    trace entry.
    
    This fixes a potential issue with the current code; when the function-tracer
    has stack-tracing enabled __trace_stack() will malfunction when it hits the
    preempt_count_add() function entry from NMI context.
    
    Suggested-by: Steven Rostedt (VMware) <rosted@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134101.434193525@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index a043ad826c67..621556efe45f 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -65,6 +65,15 @@ extern void irq_exit(void);
 #define arch_nmi_exit()		do { } while (0)
 #endif
 
+/*
+ * NMI vs Tracing
+ * --------------
+ *
+ * We must not land in a tracer until (or after) we've changed preempt_count
+ * such that in_nmi() becomes true. To that effect all NMI C entry points must
+ * be marked 'notrace' and call nmi_enter() as soon as possible.
+ */
+
 /*
  * nmi_enter() can nest up to 15 times; see NMI_BITS.
  */
@@ -75,7 +84,7 @@ extern void irq_exit(void);
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi() == NMI_MASK);			\
-		preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		__preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		rcu_nmi_enter();				\
 		lockdep_hardirq_enter();			\
 	} while (0)
@@ -85,7 +94,7 @@ extern void irq_exit(void);
 		lockdep_hardirq_exit();				\
 		rcu_nmi_exit();					\
 		BUG_ON(!in_nmi());				\
-		preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		__preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		ftrace_nmi_exit();				\
 		lockdep_on();					\
 		printk_nmi_exit();				\

commit 69ea03b56ed2c7189ccd0b5910ad39f3cad1df21
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 19 09:46:47 2020 +0100

    hardirq/nmi: Allow nested nmi_enter()
    
    Since there are already a number of sites (ARM64, PowerPC) that effectively
    nest nmi_enter(), make the primitive support this before adding even more.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lkml.kernel.org/r/20200505134100.864179229@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 7c8b82f69288..a043ad826c67 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -65,13 +65,16 @@ extern void irq_exit(void);
 #define arch_nmi_exit()		do { } while (0)
 #endif
 
+/*
+ * nmi_enter() can nest up to 15 times; see NMI_BITS.
+ */
 #define nmi_enter()						\
 	do {							\
 		arch_nmi_enter();				\
 		printk_nmi_enter();				\
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
-		BUG_ON(in_nmi());				\
+		BUG_ON(in_nmi() == NMI_MASK);			\
 		preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		rcu_nmi_enter();				\
 		lockdep_hardirq_enter();			\

commit 2502ec37a7b228b34c1e2e89480f98b92f53046a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 20 12:56:40 2020 +0100

    lockdep: Rename trace_hardirq_{enter,exit}()
    
    Continue what commit:
    
      d820ac4c2fa8 ("locking: rename trace_softirq_[enter|exit] => lockdep_softirq_[enter|exit]")
    
    started, rename these to avoid confusing them with tracepoints.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200320115859.060481361@infradead.org

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index da0af631ded5..7c8b82f69288 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -37,7 +37,7 @@ extern void rcu_nmi_exit(void);
 	do {						\
 		account_irq_enter_time(current);	\
 		preempt_count_add(HARDIRQ_OFFSET);	\
-		trace_hardirq_enter();			\
+		lockdep_hardirq_enter();		\
 	} while (0)
 
 /*
@@ -50,7 +50,7 @@ extern void irq_enter(void);
  */
 #define __irq_exit()					\
 	do {						\
-		trace_hardirq_exit();			\
+		lockdep_hardirq_exit();			\
 		account_irq_exit_time(current);		\
 		preempt_count_sub(HARDIRQ_OFFSET);	\
 	} while (0)
@@ -74,12 +74,12 @@ extern void irq_exit(void);
 		BUG_ON(in_nmi());				\
 		preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		rcu_nmi_enter();				\
-		trace_hardirq_enter();				\
+		lockdep_hardirq_enter();			\
 	} while (0)
 
 #define nmi_exit()						\
 	do {							\
-		trace_hardirq_exit();				\
+		lockdep_hardirq_exit();				\
 		rcu_nmi_exit();					\
 		BUG_ON(!in_nmi());				\
 		preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\

commit 5870970b9a828d8693aa6d15742573289d7dbcd0
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:39 2019 +0000

    arm64: Fix HCR.TGE status for NMI contexts
    
    When using VHE, the host needs to clear HCR_EL2.TGE bit in order
    to interact with guest TLBs, switching from EL2&0 translation regime
    to EL1&0.
    
    However, some non-maskable asynchronous event could happen while TGE is
    cleared like SDEI. Because of this address translation operations
    relying on EL2&0 translation regime could fail (tlb invalidation,
    userspace access, ...).
    
    Fix this by properly setting HCR_EL2.TGE when entering NMI context and
    clear it if necessary when returning to the interrupted context.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 0fbbcdf0c178..da0af631ded5 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -60,8 +60,14 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
+#ifndef arch_nmi_enter
+#define arch_nmi_enter()	do { } while (0)
+#define arch_nmi_exit()		do { } while (0)
+#endif
+
 #define nmi_enter()						\
 	do {							\
+		arch_nmi_enter();				\
 		printk_nmi_enter();				\
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
@@ -80,6 +86,7 @@ extern void irq_exit(void);
 		ftrace_nmi_exit();				\
 		lockdep_on();					\
 		printk_nmi_exit();				\
+		arch_nmi_exit();				\
 	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index c683996110b1..0fbbcdf0c178 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 

commit 42a0bb3f71383b457a7db362f1c69e7afb96732b
Author: Petr Mladek <pmladek@suse.com>
Date:   Fri May 20 17:00:33 2016 -0700

    printk/nmi: generic solution for safe printk in NMI
    
    printk() takes some locks and could not be used a safe way in NMI
    context.
    
    The chance of a deadlock is real especially when printing stacks from
    all CPUs.  This particular problem has been addressed on x86 by the
    commit a9edc8809328 ("x86/nmi: Perform a safe NMI stack trace on all
    CPUs").
    
    The patchset brings two big advantages.  First, it makes the NMI
    backtraces safe on all architectures for free.  Second, it makes all NMI
    messages almost safe on all architectures (the temporary buffer is
    limited.  We still should keep the number of messages in NMI context at
    minimum).
    
    Note that there already are several messages printed in NMI context:
    WARN_ON(in_nmi()), BUG_ON(in_nmi()), anything being printed out from MCE
    handlers.  These are not easy to avoid.
    
    This patch reuses most of the code and makes it generic.  It is useful
    for all messages and architectures that support NMI.
    
    The alternative printk_func is set when entering and is reseted when
    leaving NMI context.  It queues IRQ work to copy the messages into the
    main ring buffer in a safe context.
    
    __printk_nmi_flush() copies all available messages and reset the buffer.
    Then we could use a simple cmpxchg operations to get synchronized with
    writers.  There is also used a spinlock to get synchronized with other
    flushers.
    
    We do not longer use seq_buf because it depends on external lock.  It
    would be hard to make all supported operations safe for a lockless use.
    It would be confusing and error prone to make only some operations safe.
    
    The code is put into separate printk/nmi.c as suggested by Steven
    Rostedt.  It needs a per-CPU buffer and is compiled only on
    architectures that call nmi_enter().  This is achieved by the new
    HAVE_NMI Kconfig flag.
    
    The are MN10300 and Xtensa architectures.  We need to clean up NMI
    handling there first.  Let's do it separately.
    
    The patch is heavily based on the draft from Peter Zijlstra, see
    
      https://lkml.org/lkml/2015/6/10/327
    
    [arnd@arndb.de: printk-nmi: use %zu format string for size_t]
    [akpm@linux-foundation.org: min_t->min - all types are size_t here]
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jan Kara <jack@suse.cz>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>    [arm part]
    Cc: Daniel Thompson <daniel.thompson@linaro.org>
    Cc: Jiri Kosina <jkosina@suse.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Daniel Thompson <daniel.thompson@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index dfd59d6bc6f0..c683996110b1 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -61,6 +61,7 @@ extern void irq_exit(void);
 
 #define nmi_enter()						\
 	do {							\
+		printk_nmi_enter();				\
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi());				\
@@ -77,6 +78,7 @@ extern void irq_exit(void);
 		preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		ftrace_nmi_exit();				\
 		lockdep_on();					\
+		printk_nmi_exit();				\
 	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit 92cf211874e954027b8e91cc9a15485a50b58d6b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue May 12 16:41:46 2015 +0200

    sched/preempt: Merge preempt_mask.h into preempt.h
    
    preempt_mask.h defines all the preempt_count semantics and related
    symbols: preempt, softirq, hardirq, nmi, preempt active, need resched,
    etc...
    
    preempt.h defines the accessors and mutators of preempt_count.
    
    But there is a messy dependency game around those two header files:
    
            * preempt_mask.h includes preempt.h in order to access preempt_count()
    
            * preempt_mask.h defines all preempt_count semantic and symbols
              except PREEMPT_NEED_RESCHED that is needed by asm/preempt.h
              Thus we need to define it from preempt.h, right before including
              asm/preempt.h, instead of defining it to preempt_mask.h with the
              other preempt_count symbols. Therefore the preempt_count semantics
              happen to be spread out.
    
            * We plan to introduce preempt_active_[enter,exit]() to consolidate
              preempt_schedule*() code. But we'll need to access both preempt_count
              mutators (preempt_count_add()) and preempt_count symbols
              (PREEMPT_ACTIVE, PREEMPT_OFFSET). The usual place to define preempt
              operations is in preempt.h but then we'll need symbols in
              preempt_mask.h which already includes preempt.h. So we end up with
              a ressource circle dependency.
    
    Lets merge preempt_mask.h into preempt.h to solve these dependency issues.
    This way we gather semantic symbols and operation definition of
    preempt_count in a single file.
    
    This is a dumb copy-paste merge. Further merge re-arrangments are
    performed in a subsequent patch to ease review.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1431441711-29753-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index f4af03404b97..dfd59d6bc6f0 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -1,7 +1,7 @@
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 
-#include <linux/preempt_mask.h>
+#include <linux/preempt.h>
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>

commit 02cea3958664723a5d2236f0f0058de97c7e4693
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 5 14:06:23 2015 +0100

    genirq: Provide disable_hardirq()
    
    For things like netpoll there is a need to disable an interrupt from
    atomic context. Currently netpoll uses disable_irq() which will
    sleep-wait on threaded handlers and thus forced_irqthreads breaks
    things.
    
    Provide disable_hardirq(), which uses synchronize_hardirq() to only wait
    for active hardirq handlers; also change synchronize_hardirq() to
    return the status of threaded handlers.
    
    This will allow one to try-disable an interrupt from atomic context, or
    in case of request_threaded_irq() to only wait for the hardirq part.
    
    Suggested-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eyal Perry <eyalpe@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Quentin Lambert <lambert.quentin@gmail.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Link: http://lkml.kernel.org/r/20150205130623.GH5029@twins.programming.kicks-ass.net
    [ Fixed typos and such. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index cba442ec3c66..f4af03404b97 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -9,7 +9,7 @@
 
 
 extern void synchronize_irq(unsigned int irq);
-extern void synchronize_hardirq(unsigned int irq);
+extern bool synchronize_hardirq(unsigned int irq);
 
 #if defined(CONFIG_TINY_RCU)
 

commit 18258f7239a61d8929b8e0c7b6d46c446459074c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Feb 15 00:55:18 2014 +0000

    genirq: Provide synchronize_hardirq()
    
    synchronize_irq() waits for hard irq and threaded handlers to complete
    before returning. For some special cases we only need to make sure
    that the hard interrupt part of the irq line is not in progress when
    we disabled the - possibly shared - interrupt at the device level.
    
    A proper use case for this was provided by Russell. The sdhci driver
    requires some irq triggered functions to be run in thread context. The
    current implementation of the thread context is a sdio private kthread
    construct, which has quite some shortcomings. These can be avoided
    when the thread is directly associated to the device interrupt via the
    generic threaded irq infrastructure.
    
    Though there is a corner case related to run time power management
    where one side disables the device interrupts at the device level and
    needs to make sure, that an already running hard interrupt handler has
    completed before proceeding further. Though that hard interrupt
    handler might wake the associated thread, which in turn can request
    the runtime PM to reenable the device. Using synchronize_irq() leads
    to an immediate deadlock of the irq thread waiting for the PM lock and
    the synchronize_irq() waiting for the irq thread to complete.
    
    Due to the fact that it is sufficient for this case to ensure that no
    hard irq handler is executing a new function which avoids the check
    for the thread is required.
    
    Add a function, which just monitors the hard irq parts and ignores the
    threaded handlers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Russell King <linux@arm.linux.org.uk>
    Cc: Chris Ball <chris@printf.net>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140215003823.653236081@linutronix.de

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 12d5f972f23f..cba442ec3c66 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -9,6 +9,7 @@
 
 
 extern void synchronize_irq(unsigned int irq);
+extern void synchronize_hardirq(unsigned int irq);
 
 #if defined(CONFIG_TINY_RCU)
 

commit 0bd3a173d711857fc9f583eb5825386cc08f3948
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Nov 19 16:13:38 2013 +0100

    sched/preempt, locking: Rework local_bh_{dis,en}able()
    
    Currently local_bh_disable() is out-of-line for no apparent reason.
    So inline it to save a few cycles on call/return nonsense, the
    function body is a single add on x86 (a few loads and store extra on
    load/store archs).
    
    Also expose two new local_bh functions:
    
      __local_bh_{dis,en}able_ip(unsigned long ip, unsigned int cnt);
    
    Which implement the actual local_bh_{dis,en}able() behaviour.
    
    The next patch uses the exposed @cnt argument to optimize bh lock
    functions.
    
    With build fixes from Jacob Pan.
    
    Cc: rjw@rjwysocki.net
    Cc: rui.zhang@intel.com
    Cc: jacob.jun.pan@linux.intel.com
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: hpa@zytor.com
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: lenb@kernel.org
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131119151338.GF3694@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index d9cf963ac832..12d5f972f23f 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -5,6 +5,7 @@
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>
+#include <asm/hardirq.h>
 
 
 extern void synchronize_irq(unsigned int irq);

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 1e041063b226..d9cf963ac832 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -33,7 +33,7 @@ extern void rcu_nmi_exit(void);
 #define __irq_enter()					\
 	do {						\
 		account_irq_enter_time(current);	\
-		add_preempt_count(HARDIRQ_OFFSET);	\
+		preempt_count_add(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
 	} while (0)
 
@@ -49,7 +49,7 @@ extern void irq_enter(void);
 	do {						\
 		trace_hardirq_exit();			\
 		account_irq_exit_time(current);		\
-		sub_preempt_count(HARDIRQ_OFFSET);	\
+		preempt_count_sub(HARDIRQ_OFFSET);	\
 	} while (0)
 
 /*
@@ -62,7 +62,7 @@ extern void irq_exit(void);
 		lockdep_off();					\
 		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi());				\
-		add_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		preempt_count_add(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		rcu_nmi_enter();				\
 		trace_hardirq_enter();				\
 	} while (0)
@@ -72,7 +72,7 @@ extern void irq_exit(void);
 		trace_hardirq_exit();				\
 		rcu_nmi_exit();					\
 		BUG_ON(!in_nmi());				\
-		sub_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		preempt_count_sub(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		ftrace_nmi_exit();				\
 		lockdep_on();					\
 	} while (0)

commit 0244ad004a54e39308d495fee0a2e637f8b5c317
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 30 09:39:53 2013 +0200

    Remove GENERIC_HARDIRQ config option
    
    After the last architecture switched to generic hard irqs the config
    options HAVE_GENERIC_HARDIRQS & GENERIC_HARDIRQS and the related code
    for !CONFIG_GENERIC_HARDIRQS can be removed.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index ccfe17c5c8da..1e041063b226 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -7,11 +7,7 @@
 #include <linux/vtime.h>
 
 
-#if defined(CONFIG_SMP) || defined(CONFIG_GENERIC_HARDIRQS)
 extern void synchronize_irq(unsigned int irq);
-#else
-# define synchronize_irq(irq)	barrier()
-#endif
 
 #if defined(CONFIG_TINY_RCU)
 

commit 2d4b84739f0acf61f37267b4e80c5bd85fb90a7e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jul 29 20:29:43 2013 +0200

    hardirq: Split preempt count mask definitions
    
    In order to use static keys with vtime APIs, we'll need to
    add static keys headers to vtime.h
    
    hardirq.h then becomes a problem because it needs vtime.h
    for irqtime accounting in irq_enter/irq_exit, but it's
    often included just to get the irq mask definitions in the
    task preempt_count field and the APIs that come along:
    in_interrupt(), in_hardirq(), etc...
    
    Some very low level arch headers sometimes need these masks
    and APIs such as arch/m68k/include/asm/irqflags.h for example.
    But they don't want to include hardirq.h if vtime.h, jump_label.h
    and even workqueue.h come along. Including such bloated high
    level header from arch headers can quickly result in circular
    headers dependency that crash the build.
    
    So let's split hardirq.h in two parts:
    
    * preempt_mask.h that gathers all the preempt_count definitions
    and the APIs associated. This one is considered low level and can
    be safely included anywhere.
    
    * hardirq.h that includes the previous one. It defines the irq
    entry/exit APIs.
    
    To avoid future circular headers dependencies, the preempt_mask.h
    inclusion can replace hardirq.h on files that don't implement irq
    low level handlers but just need the atomic/context check APIs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 05bcc0903766..ccfe17c5c8da 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -1,126 +1,11 @@
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 
-#include <linux/preempt.h>
+#include <linux/preempt_mask.h>
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>
-#include <asm/hardirq.h>
 
-/*
- * We put the hardirq and softirq counter into the preemption
- * counter. The bitmask has the following meaning:
- *
- * - bits 0-7 are the preemption count (max preemption depth: 256)
- * - bits 8-15 are the softirq count (max # of softirqs: 256)
- *
- * The hardirq count can in theory reach the same as NR_IRQS.
- * In reality, the number of nested IRQS is limited to the stack
- * size as well. For archs with over 1000 IRQS it is not practical
- * to expect that they will all nest. We give a max of 10 bits for
- * hardirq nesting. An arch may choose to give less than 10 bits.
- * m68k expects it to be 8.
- *
- * - bits 16-25 are the hardirq count (max # of nested hardirqs: 1024)
- * - bit 26 is the NMI_MASK
- * - bit 27 is the PREEMPT_ACTIVE flag
- *
- * PREEMPT_MASK: 0x000000ff
- * SOFTIRQ_MASK: 0x0000ff00
- * HARDIRQ_MASK: 0x03ff0000
- *     NMI_MASK: 0x04000000
- */
-#define PREEMPT_BITS	8
-#define SOFTIRQ_BITS	8
-#define NMI_BITS	1
-
-#define MAX_HARDIRQ_BITS 10
-
-#ifndef HARDIRQ_BITS
-# define HARDIRQ_BITS	MAX_HARDIRQ_BITS
-#endif
-
-#if HARDIRQ_BITS > MAX_HARDIRQ_BITS
-#error HARDIRQ_BITS too high!
-#endif
-
-#define PREEMPT_SHIFT	0
-#define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
-#define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
-#define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)
-
-#define __IRQ_MASK(x)	((1UL << (x))-1)
-
-#define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
-#define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
-#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
-#define NMI_MASK	(__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)
-
-#define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
-#define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
-#define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
-#define NMI_OFFSET	(1UL << NMI_SHIFT)
-
-#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
-
-#ifndef PREEMPT_ACTIVE
-#define PREEMPT_ACTIVE_BITS	1
-#define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)
-#define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)
-#endif
-
-#if PREEMPT_ACTIVE < (1 << (NMI_SHIFT + NMI_BITS))
-#error PREEMPT_ACTIVE is too low!
-#endif
-
-#define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
-				 | NMI_MASK))
-
-/*
- * Are we doing bottom half or hardware interrupt processing?
- * Are we in a softirq context? Interrupt context?
- * in_softirq - Are we currently processing softirq or have bh disabled?
- * in_serving_softirq - Are we currently processing softirq?
- */
-#define in_irq()		(hardirq_count())
-#define in_softirq()		(softirq_count())
-#define in_interrupt()		(irq_count())
-#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
-
-/*
- * Are we in NMI context?
- */
-#define in_nmi()	(preempt_count() & NMI_MASK)
-
-#if defined(CONFIG_PREEMPT_COUNT)
-# define PREEMPT_CHECK_OFFSET 1
-#else
-# define PREEMPT_CHECK_OFFSET 0
-#endif
-
-/*
- * Are we running in atomic context?  WARNING: this macro cannot
- * always detect atomic context; in particular, it cannot know about
- * held spinlocks in non-preemptible kernels.  Thus it should not be
- * used in the general case to determine whether sleeping is possible.
- * Do not use in_atomic() in driver code.
- */
-#define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != 0)
-
-/*
- * Check whether we were atomic before we did preempt_disable():
- * (used by the scheduler, *after* releasing the kernel lock)
- */
-#define in_atomic_preempt_off() \
-		((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_CHECK_OFFSET)
-
-#ifdef CONFIG_PREEMPT_COUNT
-# define preemptible()	(preempt_count() == 0 && !irqs_disabled())
-#else
-# define preemptible()	0
-#endif
 
 #if defined(CONFIG_SMP) || defined(CONFIG_GENERIC_HARDIRQS)
 extern void synchronize_irq(unsigned int irq);

commit 127781d1ba1ee5bbe1780afa35dd0e71583b143d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 27 08:44:00 2013 -0700

    rcu: Remove TINY_PREEMPT_RCU
    
    TINY_PREEMPT_RCU adds significant code and complexity, but does not
    offer commensurate benefits.  People currently using TINY_PREEMPT_RCU
    can get much better memory footprint with TINY_RCU, or, if they really
    need preemptible RCU, they can use TREE_PREEMPT_RCU with a relatively
    minor degradation in memory footprint.  Please note that this move
    has been widely publicized on LKML (https://lkml.org/lkml/2012/11/12/545)
    and on LWN (http://lwn.net/Articles/541037/).
    
    This commit therefore removes TINY_PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Updated to eliminate #else in rcutiny.h as suggested by Josh ]
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index c1d6555d2567..05bcc0903766 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -128,7 +128,7 @@ extern void synchronize_irq(unsigned int irq);
 # define synchronize_irq(irq)	barrier()
 #endif
 
-#if defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)
+#if defined(CONFIG_TINY_RCU)
 
 static inline void rcu_nmi_enter(void)
 {

commit 4d4c4e24cf48400a24d33feffc7cca4f4e8cabe1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Feb 22 00:05:07 2013 +0100

    irq: Remove IRQ_EXIT_OFFSET workaround
    
    The IRQ_EXIT_OFFSET trick was used to make sure the irq
    doesn't get preempted after we substract the HARDIRQ_OFFSET
    until we are entirely done with any code in irq_exit().
    
    This workaround was necessary because some archs may call
    irq_exit() with irqs enabled and there is still some code
    in the end of this function that is not covered by the
    HARDIRQ_OFFSET but want to stay non-preemptible.
    
    Now that irq are always disabled in irq_exit(), the whole code
    is guaranteed not to be preempted. We can thus remove this hack.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 29eb805ea4a6..c1d6555d2567 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -118,10 +118,8 @@
 
 #ifdef CONFIG_PREEMPT_COUNT
 # define preemptible()	(preempt_count() == 0 && !irqs_disabled())
-# define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)
 #else
 # define preemptible()	0
-# define IRQ_EXIT_OFFSET HARDIRQ_OFFSET
 #endif
 
 #if defined(CONFIG_SMP) || defined(CONFIG_GENERIC_HARDIRQS)

commit d652e1eb8e7b739fccbfb503a3da3e9f640fbf3d
Merge: 8f55cea410db 77852fea6e24
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 18:19:48 2013 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Main changes:
    
       - scheduler side full-dynticks (user-space execution is undisturbed
         and receives no timer IRQs) preparation changes that convert the
         cputime accounting code to be full-dynticks ready, from Frederic
         Weisbecker.
    
       - Initial sched.h split-up changes, by Clark Williams
    
       - select_idle_sibling() performance improvement by Mike Galbraith:
    
            " 1 tbench pair (worst case) in a 10 core + SMT package:
    
              pre   15.22 MB/sec 1 procs
              post 252.01 MB/sec 1 procs "
    
      - sched_rr_get_interval() ABI fix/change.  We think this detail is not
        used by apps (so it's not an ABI in practice), but lets keep it
        under observation.
    
      - misc RT scheduling cleanups, optimizations"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      sched/rt: Add <linux/sched/rt.h> header to <linux/init_task.h>
      cputime: Remove irqsave from seqlock readers
      sched, powerpc: Fix sched.h split-up build failure
      cputime: Restore CPU_ACCOUNTING config defaults for PPC64
      sched/rt: Move rt specific bits into new header file
      sched/rt: Add a tuning knob to allow changing SCHED_RR timeslice
      sched: Move sched.h sysctl bits into separate header
      sched: Fix signedness bug in yield_to()
      sched: Fix select_idle_sibling() bouncing cow syndrome
      sched/rt: Further simplify pick_rt_task()
      sched/rt: Do not account zero delta_exec in update_curr_rt()
      cputime: Safely read cputime of full dynticks CPUs
      kvm: Prepare to add generic guest entry/exit callbacks
      cputime: Use accessors to read task cputime stats
      cputime: Allow dynamic switch between tick/virtual based cputime accounting
      cputime: Generic on-demand virtual cputime accounting
      cputime: Move default nsecs_to_cputime() to jiffies based cputime file
      cputime: Librarize per nsecs resolution cputime definitions
      cputime: Avoid multiplication overflow on utime scaling
      context_tracking: Export context state for generic vtime
      ...
    
    Fix up conflict in kernel/context_tracking.c due to comment additions.

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 624ef3f45c8e..7105d5cbb762 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -153,7 +153,7 @@ extern void rcu_nmi_exit(void);
  */
 #define __irq_enter()					\
 	do {						\
-		vtime_account_irq_enter(current);	\
+		account_irq_enter_time(current);	\
 		add_preempt_count(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
 	} while (0)
@@ -169,7 +169,7 @@ extern void irq_enter(void);
 #define __irq_exit()					\
 	do {						\
 		trace_hardirq_exit();			\
-		vtime_account_irq_exit(current);	\
+		account_irq_exit_time(current);		\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
 	} while (0)
 

commit 0f1ac8fd254b6c3e77950a1c4ee67be5dc88f7e0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jan 15 22:11:19 2013 -0500

    tracing/lockdep: Disable lockdep first in entering NMI
    
    When function tracing with either debug locks enabled or tracing
    preempt disabled, the add_preempt_count() is traced. This is an
    issue with lockdep and function tracing. As function tracing
    can disable interrupts, and lockdep records that change,
    lockdep may not be able to handle this recursion if it happens from
    an NMI context.
    
    The first thing that an NMI does is:
    
     #define nmi_enter()                                    \
            do {                                                    \
                    ftrace_nmi_enter();                             \
                    BUG_ON(in_nmi());                               \
                    add_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET); \
                    lockdep_off();                                  \
                    rcu_nmi_enter();                                \
                    trace_hardirq_enter();                          \
            } while (0)
    
    When the add_preempt_count() is traced, and the tracing callback
    disables interrupts, it will jump into the lockdep code. There's
    some places in lockdep that can't handle this re-entrance, and
    causes lockdep to fail.
    
    As the lockdep_off() (and lockdep_on) is a simple:
    
    void lockdep_off(void)
    {
            current->lockdep_recursion++;
    }
    
    and is never traced, it can be called first in nmi_enter()
    and lockdep_on() last in nmi_exit().
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 624ef3f45c8e..57bfdce8fb90 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -180,10 +180,10 @@ extern void irq_exit(void);
 
 #define nmi_enter()						\
 	do {							\
+		lockdep_off();					\
 		ftrace_nmi_enter();				\
 		BUG_ON(in_nmi());				\
 		add_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
-		lockdep_off();					\
 		rcu_nmi_enter();				\
 		trace_hardirq_enter();				\
 	} while (0)
@@ -192,10 +192,10 @@ extern void irq_exit(void);
 	do {							\
 		trace_hardirq_exit();				\
 		rcu_nmi_exit();					\
-		lockdep_on();					\
 		BUG_ON(!in_nmi());				\
 		sub_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
 		ftrace_nmi_exit();				\
+		lockdep_on();					\
 	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit fa5058f3b63153e0147ef65bcdb3a4ee63581346
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 6 04:07:19 2012 +0200

    cputime: Specialize irq vtime hooks
    
    With CONFIG_VIRT_CPU_ACCOUNTING, when vtime_account()
    is called in irq entry/exit, we perform a check on the
    context: if we are interrupting the idle task we
    account the pending cputime to idle, otherwise account
    to system time or its sub-areas: tsk->stime, hardirq time,
    softirq time, ...
    
    However this check for idle only concerns the hardirq entry
    and softirq entry:
    
    * Hardirq may directly interrupt the idle task, in which case
    we need to flush the pending CPU time to idle.
    
    * The idle task may be directly interrupted by a softirq if
    it calls local_bh_enable(). There is probably no such call
    in any idle task but we need to cover every case. Ksoftirqd
    is not concerned because the idle time is flushed on context
    switch and softirq in the end of hardirq have the idle time
    already flushed from the hardirq entry.
    
    In the other cases we always account to system/irq time:
    
    * On hardirq exit we account the time to hardirq time.
    * On softirq exit we account the time to softirq time.
    
    To optimize this and avoid the indirect call to vtime_account()
    and the checks it performs, specialize the vtime irq APIs and
    only perform the check on irq entry. Irq exit can directly call
    vtime_account_system().
    
    CONFIG_IRQ_TIME_ACCOUNTING behaviour doesn't change and directly
    maps to its own vtime_account() implementation. One may want
    to take benefits from the new APIs to optimize irq time accounting
    as well in the future.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index b083a475423d..624ef3f45c8e 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -153,7 +153,7 @@ extern void rcu_nmi_exit(void);
  */
 #define __irq_enter()					\
 	do {						\
-		vtime_account(current);		\
+		vtime_account_irq_enter(current);	\
 		add_preempt_count(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
 	} while (0)
@@ -169,7 +169,7 @@ extern void irq_enter(void);
 #define __irq_exit()					\
 	do {						\
 		trace_hardirq_exit();			\
-		vtime_account(current);		\
+		vtime_account_irq_exit(current);	\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
 	} while (0)
 

commit dcbf832e5823156e8f155359b47bd108cac8ad68
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 5 23:07:19 2012 +0200

    vtime: Gather vtime declarations to their own header file
    
    These APIs are scattered around and are going to expand a bit.
    Let's create a dedicated header file for sanity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index cab3da3d0949..b083a475423d 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -4,6 +4,7 @@
 #include <linux/preempt.h>
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
+#include <linux/vtime.h>
 #include <asm/hardirq.h>
 
 /*
@@ -129,16 +130,6 @@ extern void synchronize_irq(unsigned int irq);
 # define synchronize_irq(irq)	barrier()
 #endif
 
-struct task_struct;
-
-#if !defined(CONFIG_VIRT_CPU_ACCOUNTING) && !defined(CONFIG_IRQ_TIME_ACCOUNTING)
-static inline void vtime_account(struct task_struct *tsk)
-{
-}
-#else
-extern void vtime_account(struct task_struct *tsk);
-#endif
-
 #if defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)
 
 static inline void rcu_nmi_enter(void)

commit bf9fae9f5e4ca8dce4708812f9ad6281e61df109
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 15:23:11 2012 +0200

    cputime: Use a proper subsystem naming for vtime related APIs
    
    Use a naming based on vtime as a prefix for virtual based
    cputime accounting APIs:
    
    - account_system_vtime() -> vtime_account()
    - account_switch_vtime() -> vtime_task_switch()
    
    It makes it easier to allow for further declension such
    as vtime_account_system(), vtime_account_idle(), ... if we
    want to find out the context we account to from generic code.
    
    This also make it better to know on which subsystem these APIs
    refer to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 305f23cd7cff..cab3da3d0949 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -132,11 +132,11 @@ extern void synchronize_irq(unsigned int irq);
 struct task_struct;
 
 #if !defined(CONFIG_VIRT_CPU_ACCOUNTING) && !defined(CONFIG_IRQ_TIME_ACCOUNTING)
-static inline void account_system_vtime(struct task_struct *tsk)
+static inline void vtime_account(struct task_struct *tsk)
 {
 }
 #else
-extern void account_system_vtime(struct task_struct *tsk);
+extern void vtime_account(struct task_struct *tsk);
 #endif
 
 #if defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)
@@ -162,7 +162,7 @@ extern void rcu_nmi_exit(void);
  */
 #define __irq_enter()					\
 	do {						\
-		account_system_vtime(current);		\
+		vtime_account(current);		\
 		add_preempt_count(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
 	} while (0)
@@ -178,7 +178,7 @@ extern void irq_enter(void);
 #define __irq_exit()					\
 	do {						\
 		trace_hardirq_exit();			\
-		account_system_vtime(current);		\
+		vtime_account(current);		\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
 	} while (0)
 

commit a7e4786b937a3ae918a7520cfdba557a80915fa7
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Jul 21 00:54:59 2012 +0530

    sched: Fix comment about PREEMPT_ACTIVE bit location
    
    PREEMPT_ACTIVE flag is bit 27, not 28. Fix the comment.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: paulmck@linux.vnet.ibm.com
    Cc: josh@joshtriplett.org
    Cc: rostedt@goodmis.org
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120720192459.6149.14821.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index bb7f30971858..305f23cd7cff 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -22,7 +22,7 @@
  *
  * - bits 16-25 are the hardirq count (max # of nested hardirqs: 1024)
  * - bit 26 is the NMI_MASK
- * - bit 28 is the PREEMPT_ACTIVE flag
+ * - bit 27 is the PREEMPT_ACTIVE flag
  *
  * PREEMPT_MASK: 0x000000ff
  * SOFTIRQ_MASK: 0x0000ff00

commit 9b2e4f1880b789be1f24f9684f7a54b90310b5c0
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Sep 30 12:10:22 2011 -0700

    rcu: Track idleness independent of idle tasks
    
    Earlier versions of RCU used the scheduling-clock tick to detect idleness
    by checking for the idle task, but handled idleness differently for
    CONFIG_NO_HZ=y.  But there are now a number of uses of RCU read-side
    critical sections in the idle task, for example, for tracing.  A more
    fine-grained detection of idleness is therefore required.
    
    This commit presses the old dyntick-idle code into full-time service,
    so that rcu_idle_enter(), previously known as rcu_enter_nohz(), is
    always invoked at the beginning of an idle loop iteration.  Similarly,
    rcu_idle_exit(), previously known as rcu_exit_nohz(), is always invoked
    at the end of an idle-loop iteration.  This allows the idle task to
    use RCU everywhere except between consecutive rcu_idle_enter() and
    rcu_idle_exit() calls, in turn allowing architecture maintainers to
    specify exactly where in the idle loop that RCU may be used.
    
    Because some of the userspace upcall uses can result in what looks
    to RCU like half of an interrupt, it is not possible to expect that
    the irq_enter() and irq_exit() hooks will give exact counts.  This
    patch therefore expands the ->dynticks_nesting counter to 64 bits
    and uses two separate bitfields to count process/idle transitions
    and interrupt entry/exit transitions.  It is presumed that userspace
    upcalls do not happen in the idle loop or from usermode execution
    (though usermode might do a system call that results in an upcall).
    The counter is hard-reset on each process/idle transition, which
    avoids the interrupt entry/exit error from accumulating.  Overflow
    is avoided by the 64-bitness of the ->dyntick_nesting counter.
    
    This commit also adds warnings if a non-idle task asks RCU to enter
    idle state (and these checks will need some adjustment before applying
    Frederic's OS-jitter patches (http://lkml.org/lkml/2011/10/7/246).
    In addition, validation of ->dynticks and ->dynticks_nesting is added.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index f743883f769e..bb7f30971858 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -139,20 +139,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
 extern void account_system_vtime(struct task_struct *tsk);
 #endif
 
-#if defined(CONFIG_NO_HZ)
 #if defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)
-extern void rcu_enter_nohz(void);
-extern void rcu_exit_nohz(void);
-
-static inline void rcu_irq_enter(void)
-{
-	rcu_exit_nohz();
-}
-
-static inline void rcu_irq_exit(void)
-{
-	rcu_enter_nohz();
-}
 
 static inline void rcu_nmi_enter(void)
 {
@@ -163,17 +150,9 @@ static inline void rcu_nmi_exit(void)
 }
 
 #else
-extern void rcu_irq_enter(void);
-extern void rcu_irq_exit(void);
 extern void rcu_nmi_enter(void);
 extern void rcu_nmi_exit(void);
 #endif
-#else
-# define rcu_irq_enter() do { } while (0)
-# define rcu_irq_exit() do { } while (0)
-# define rcu_nmi_enter() do { } while (0)
-# define rcu_nmi_exit() do { } while (0)
-#endif /* #if defined(CONFIG_NO_HZ) */
 
 /*
  * It is safe to do non-atomic ops on ->hardirq_context,

commit bdd4e85dc36cdbcfc1608a5b2a17c80a9db8986a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 8 01:13:27 2011 +0200

    sched: Isolate preempt counting in its own config option
    
    Create a new CONFIG_PREEMPT_COUNT that handles the inc/dec
    of preempt count offset independently. So that the offset
    can be updated by preempt_disable() and preempt_enable()
    even without the need for CONFIG_PREEMPT beeing set.
    
    This prepares to make CONFIG_DEBUG_SPINLOCK_SLEEP working
    with !CONFIG_PREEMPT where it currently doesn't detect
    code that sleeps inside explicit preemption disabled
    sections.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index ba362171e8ae..f743883f769e 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -93,7 +93,7 @@
  */
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
-#if defined(CONFIG_PREEMPT)
+#if defined(CONFIG_PREEMPT_COUNT)
 # define PREEMPT_CHECK_OFFSET 1
 #else
 # define PREEMPT_CHECK_OFFSET 0
@@ -115,7 +115,7 @@
 #define in_atomic_preempt_off() \
 		((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_CHECK_OFFSET)
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPT_COUNT
 # define preemptible()	(preempt_count() == 0 && !irqs_disabled())
 # define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)
 #else

commit 4ba8216cd90560bc402f52076f64d8546e8aefcb
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Jan 25 22:52:22 2011 +0100

    BKL: That's all, folks
    
    This removes the implementation of the big kernel lock,
    at last. A lot of people have worked on this in the
    past, I so the credit for this patch should be with
    everyone who participated in the hunt.
    
    The names on the Cc list are the people that were the
    most active in this, according to the recorded git
    history, in alphabetical order.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Alan Cox <alan@linux.intel.com>
    Cc: Alessio Igor Bogani <abogani@texware.it>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Hans Verkuil <hverkuil@xs4all.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Blunck <jblunck@infradead.org>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Oliver Neukum <oliver@neukum.org>
    Cc: Paul Menage <menage@google.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 32f9fd6619b4..ba362171e8ae 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -93,13 +93,6 @@
  */
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_BKL)
-# include <linux/sched.h>
-# define PREEMPT_INATOMIC_BASE (current->lock_depth >= 0)
-#else
-# define PREEMPT_INATOMIC_BASE 0
-#endif
-
 #if defined(CONFIG_PREEMPT)
 # define PREEMPT_CHECK_OFFSET 1
 #else
@@ -113,7 +106,7 @@
  * used in the general case to determine whether sleeping is possible.
  * Do not use in_atomic() in driver code.
  */
-#define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_INATOMIC_BASE)
+#define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != 0)
 
 /*
  * Check whether we were atomic before we did preempt_disable():

commit ed1d77b18c9f4ff06d5b42c65041aa55a1447053
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 18 10:56:29 2010 -0800

    hardirq.h: needs sched.h if using BKL
    
    This really isn't the right thing to do, and strictly speaking we should
    have the BKL depth count in the thread info right next to the preempt
    count.  The two really do go together.
    
    However, since that would involve a patch to all architectures, and the
    BKL is finally going away, it's simply not worth the effort to do the
    RightThing(tm).  Just re-instate the <linux/sched.h> include that we
    used to get accidentally from the smp_lock.h one.
    
    This is all fallout from the same old "BKL: remove extraneous #include
    <smp_lock.h>" commit.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Tested-by: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 714da7e5d10c..32f9fd6619b4 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -94,6 +94,7 @@
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
 #if defined(CONFIG_PREEMPT) && defined(CONFIG_BKL)
+# include <linux/sched.h>
 # define PREEMPT_INATOMIC_BASE (current->lock_depth >= 0)
 #else
 # define PREEMPT_INATOMIC_BASE 0

commit 0a5b871ea4c6bfb2723ac2ffc7ef5c32452abb89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 17 18:36:25 2010 -0800

    hardirq.h: remove now-empty #ifdef/#endif pair
    
    Commit 451a3c24b013 ("BKL: remove extraneous #include <smp_lock.h>")
    removed the #include line that was the only thing that was surrounded by
    the #ifdef/#endif.
    
    So now that #ifdef is guarding nothing at all. Just remove it.
    
    Reported-by: Byeong-ryeol Kim <brofkims@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index bea1612d8f5c..714da7e5d10c 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -2,8 +2,6 @@
 #define LINUX_HARDIRQ_H
 
 #include <linux/preempt.h>
-#ifdef CONFIG_PREEMPT
-#endif
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <asm/hardirq.h>

commit 7957f0a857754c555e07f58a3fb83ac29501478c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 17 14:58:36 2010 -0800

    Fix build failure due to hwirq.h needing smp_lock.h
    
    Arnd Bergmann did an automated scripting run to find left-over instances
    of <linux/smp_lock.h>, and had made it trigger it on the normal BKL use
    of lock_kernel and unlock_lernel (and apparently release_kernel_lock and
    reacquire_kernel_lock too, used by the scheduler).
    
    That resulted in commit 451a3c24b013 ("BKL: remove extraneous #include
    <smp_lock.h>").
    
    However, hardirq.h was the only remaining user of the old
    'kernel_locked()' interface, and Arnd's script hadn't checked for that.
    So depending on your configuration and what header files had been
    included, you would get errors like "implicit declaration of function
    'kernel_locked'" during the build.
    
    The right fix is not to just re-instate the smp_lock.h include - it is
    to just remove 'kernel_locked()' entirely, since the only use was this
    one special low-level detail.  Just make hardirq.h do it directly.
    
    In fact this simplifies and clarifies the code, because some trivial
    analysis makes it clear that hardirq.h only ever used _one_ of the two
    definitions of kernel_locked(), so we can remove the other one entirely.
    
    Reported-by: Zimny Lech <napohybelskurwysynom2010@gmail.com>
    Reported-and-acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8f3f467c57c6..bea1612d8f5c 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -96,7 +96,7 @@
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
 #if defined(CONFIG_PREEMPT) && defined(CONFIG_BKL)
-# define PREEMPT_INATOMIC_BASE kernel_locked()
+# define PREEMPT_INATOMIC_BASE (current->lock_depth >= 0)
 #else
 # define PREEMPT_INATOMIC_BASE 0
 #endif

commit 451a3c24b0135bce54542009b5fde43846c7cf67
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Nov 17 16:26:55 2010 +0100

    BKL: remove extraneous #include <smp_lock.h>
    
    The big kernel lock has been removed from all these files at some point,
    leaving only the #include.
    
    Remove this too as a cleanup.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 41cb31f14ee3..8f3f467c57c6 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -3,7 +3,6 @@
 
 #include <linux/preempt.h>
 #ifdef CONFIG_PREEMPT
-#include <linux/smp_lock.h>
 #endif
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>

commit 7fe19da4ca38fc20cdbc7020fcf2eca8fc756410
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Oct 28 16:12:33 2010 +0200

    preempt: fix kernel build with !CONFIG_BKL
    
    The preempt count logic tries to take the BKL into account, which breaks
    when CONFIG_BKL is not set.
    
    Use the same preempt_count offset that we use without CONFIG_PREEMPT
    when CONFIG_BKL is disabled.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reported-and-tested-by: Kirill A. Shutemov <kirill@shutemov.name>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8a389b608ce3..41cb31f14ee3 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -96,11 +96,15 @@
  */
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
-#if defined(CONFIG_PREEMPT)
+#if defined(CONFIG_PREEMPT) && defined(CONFIG_BKL)
 # define PREEMPT_INATOMIC_BASE kernel_locked()
-# define PREEMPT_CHECK_OFFSET 1
 #else
 # define PREEMPT_INATOMIC_BASE 0
+#endif
+
+#if defined(CONFIG_PREEMPT)
+# define PREEMPT_CHECK_OFFSET 1
+#else
 # define PREEMPT_CHECK_OFFSET 0
 #endif
 

commit e36f561a2c88394ef2708f1ab300fe8a79e9f651
Merge: 70ada7792072 df9ee29270c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 14:37:27 2010 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-2.6-irqflags
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-2.6-irqflags:
      Fix IRQ flag handling naming
      MIPS: Add missing #inclusions of <linux/irq.h>
      smc91x: Add missing #inclusion of <linux/irq.h>
      Drop a couple of unnecessary asm/system.h inclusions
      SH: Add missing consts to sys_execve() declaration
      Blackfin: Rename IRQ flags handling functions
      Blackfin: Add missing dep to asm/irqflags.h
      Blackfin: Rename DES PC2() symbol to avoid collision
      Blackfin: Split the BF532 BFIN_*_FIO_FLAG() functions to their own header
      Blackfin: Split PLL code from mach-specific cdef headers

commit bc4016f48161454a9a8e5eb209b0693c6cde9f62
Merge: 5d70f79b5ef6 b7dadc387975
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 12:55:43 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (29 commits)
      sched: Export account_system_vtime()
      sched: Call tick_check_idle before __irq_enter
      sched: Remove irq time from available CPU power
      sched: Do not account irq time to current task
      x86: Add IRQ_TIME_ACCOUNTING
      sched: Add IRQ_TIME_ACCOUNTING, finer accounting of irq time
      sched: Add a PF flag for ksoftirqd identification
      sched: Consolidate account_system_vtime extern declaration
      sched: Fix softirq time accounting
      sched: Drop group_capacity to 1 only if local group has extra capacity
      sched: Force balancing on newidle balance if local group has capacity
      sched: Set group_imb only a task can be pulled from the busiest cpu
      sched: Do not consider SCHED_IDLE tasks to be cache hot
      sched: Drop all load weight manipulation for RT tasks
      sched: Create special class for stop/migrate work
      sched: Unindent labels
      sched: Comment updates: fix default latency and granularity numbers
      tracing/sched: Add sched_pi_setprio tracepoint
      sched: Give CPU bound RT tasks preference
      sched: Try not to migrate higher priority RT tasks
      ...

commit b52bfee445d315549d41eacf2fa7c156e7d153d5
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:19 2010 -0700

    sched: Add IRQ_TIME_ACCOUNTING, finer accounting of irq time
    
    s390/powerpc/ia64 have support for CONFIG_VIRT_CPU_ACCOUNTING which does
    the fine granularity accounting of user, system, hardirq, softirq times.
    Adding that option on archs like x86 will be challenging however, given the
    state of TSC reliability on various platforms and also the overhead it will
    add in syscall entry exit.
    
    Instead, add a lighter variant that only does finer accounting of
    hardirq and softirq times, providing precise irq times (instead of timer tick
    based samples). This accounting is added with a new config option
    CONFIG_IRQ_TIME_ACCOUNTING so that there won't be any overhead for users not
    interested in paying the perf penalty.
    
    This accounting is based on sched_clock, with the code being generic.
    So, other archs may find it useful as well.
    
    This patch just adds the core logic and does not enable this logic yet.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-5-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 41367c5c3c68..ff43e9268449 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -137,7 +137,7 @@ extern void synchronize_irq(unsigned int irq);
 
 struct task_struct;
 
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+#if !defined(CONFIG_VIRT_CPU_ACCOUNTING) && !defined(CONFIG_IRQ_TIME_ACCOUNTING)
 static inline void account_system_vtime(struct task_struct *tsk)
 {
 }

commit e1e10a265d28273ab8c70be19d43dcbdeead6c5a
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:17 2010 -0700

    sched: Consolidate account_system_vtime extern declaration
    
    Just a minor cleanup patch that makes things easier to the following patches.
    No functionality change in this patch.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-3-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index e37a77cbd588..41367c5c3c68 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -141,6 +141,8 @@ struct task_struct;
 static inline void account_system_vtime(struct task_struct *tsk)
 {
 }
+#else
+extern void account_system_vtime(struct task_struct *tsk);
 #endif
 
 #if defined(CONFIG_NO_HZ)

commit 75e1056f5c57050415b64cb761a3acc35d91f013
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:16 2010 -0700

    sched: Fix softirq time accounting
    
    Peter Zijlstra found a bug in the way softirq time is accounted in
    VIRT_CPU_ACCOUNTING on this thread:
    
       http://lkml.indiana.edu/hypermail//linux/kernel/1009.2/01366.html
    
    The problem is, softirq processing uses local_bh_disable internally. There
    is no way, later in the flow, to differentiate between whether softirq is
    being processed or is it just that bh has been disabled. So, a hardirq when bh
    is disabled results in time being wrongly accounted as softirq.
    
    Looking at the code a bit more, the problem exists in !VIRT_CPU_ACCOUNTING
    as well. As account_system_time() in normal tick based accouting also uses
    softirq_count, which will be set even when not in softirq with bh disabled.
    
    Peter also suggested solution of using 2*SOFTIRQ_OFFSET as irq count
    for local_bh_{disable,enable} and using just SOFTIRQ_OFFSET while softirq
    processing. The patch below does that and adds API in_serving_softirq() which
    returns whether we are currently processing softirq or not.
    
    Also changes one of the usages of softirq_count in net/sched/cls_cgroup.c
    to in_serving_softirq.
    
    Looks like many usages of in_softirq really want in_serving_softirq. Those
    changes can be made individually on a case by case basis.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-2-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index d5b387669dab..e37a77cbd588 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -64,6 +64,8 @@
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
 #define NMI_OFFSET	(1UL << NMI_SHIFT)
 
+#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
+
 #ifndef PREEMPT_ACTIVE
 #define PREEMPT_ACTIVE_BITS	1
 #define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)
@@ -82,10 +84,13 @@
 /*
  * Are we doing bottom half or hardware interrupt processing?
  * Are we in a softirq context? Interrupt context?
+ * in_softirq - Are we currently processing softirq or have bh disabled?
+ * in_serving_softirq - Are we currently processing softirq?
  */
 #define in_irq()		(hardirq_count())
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
+#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
 
 /*
  * Are we in NMI context?

commit bcdb714c8856c76383ca455294f0074168705eab
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 7 14:08:53 2010 +0100

    Drop a couple of unnecessary asm/system.h inclusions
    
    Drop inclusions of asm/system.h from linux/hardirq.h and linux/list.h as
    they're no longer required and prevent the M68K arch's IRQ flag handling macros
    from being made into inlined functions due to circular dependencies.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Greg Ungerer <gerg@uclinux.org>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index d5b387669dab..7dfdc06c7e18 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -8,7 +8,6 @@
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <asm/hardirq.h>
-#include <asm/system.h>
 
 /*
  * We put the hardirq and softirq counter into the preemption

commit a57eb940d130477a799dfb24a570ee04979c0f7f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 29 16:49:16 2010 -0700

    rcu: Add a TINY_PREEMPT_RCU
    
    Implement a small-memory-footprint uniprocessor-only implementation of
    preemptible RCU.  This implementation uses but a single blocked-tasks
    list rather than the combinatorial number used per leaf rcu_node by
    TREE_PREEMPT_RCU, which reduces memory consumption and greatly simplifies
    processing.  This version also takes advantage of uniprocessor execution
    to accelerate grace periods in the case where there are no readers.
    
    The general design is otherwise broadly similar to that of TREE_PREEMPT_RCU.
    
    This implementation is a step towards having RCU implementation driven
    off of the SMP and PREEMPT kernel configuration variables, which can
    happen once this implementation has accumulated sufficient experience.
    
    Removed ACCESS_ONCE() from __rcu_read_unlock() and added barrier() as
    suggested by Steve Rostedt in order to avoid the compiler-reordering
    issue noted by Mathieu Desnoyers (http://lkml.org/lkml/2010/8/16/183).
    
    As can be seen below, CONFIG_TINY_PREEMPT_RCU represents almost 5Kbyte
    savings compared to CONFIG_TREE_PREEMPT_RCU.  Of course, for non-real-time
    workloads, CONFIG_TINY_RCU is even better.
    
            CONFIG_TREE_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               6170     825      28    7023    kernel/rcutree.o
                                       ----
                                       7026    Total
    
            CONFIG_TINY_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               2081      81       8    2170    kernel/rcutiny.o
                                       ----
                                       2183    Total
    
            CONFIG_TINY_RCU (non-preemptible)
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
                719      25       0     744    kernel/rcutiny.o
                                        ---
                                        757    Total
    
    Requested-by: Loc Minier <loic.minier@canonical.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index d5b387669dab..1f4517d55b19 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -139,7 +139,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
 #endif
 
 #if defined(CONFIG_NO_HZ)
-#if defined(CONFIG_TINY_RCU)
+#if defined(CONFIG_TINY_RCU) || defined(CONFIG_TINY_PREEMPT_RCU)
 extern void rcu_enter_nohz(void);
 extern void rcu_exit_nohz(void);
 

commit 9b1d82fa1611706fa7ee1505f290160a18caf95d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Oct 25 19:03:50 2009 -0700

    rcu: "Tiny RCU", The Bloatwatch Edition
    
    This patch is a version of RCU designed for !SMP provided for a
    small-footprint RCU implementation.  In particular, the
    implementation of synchronize_rcu() is extremely lightweight and
    high performance. It passes rcutorture testing in each of the
    four relevant configurations (combinations of NO_HZ and PREEMPT)
    on x86.  This saves about 1K bytes compared to old Classic RCU
    (which is no longer in mainline), and more than three kilobytes
    compared to Hierarchical RCU (updated to 2.6.30):
    
            CONFIG_TREE_RCU:
    
               text    data     bss     dec     filename
                183       4       0     187     kernel/rcupdate.o
               2783     520      36    3339     kernel/rcutree.o
                                       3526 Total (vs 4565 for v7)
    
            CONFIG_TREE_PREEMPT_RCU:
    
               text    data     bss     dec     filename
                263       4       0     267     kernel/rcupdate.o
               4594     776      52    5422     kernel/rcutree.o
                                       5689 Total (6155 for v7)
    
            CONFIG_TINY_RCU:
    
               text    data     bss     dec     filename
                 96       4       0     100     kernel/rcupdate.o
                734      24       0     758     kernel/rcutiny.o
                                        858 Total (vs 848 for v7)
    
    The above is for x86.  Your mileage may vary on other platforms.
    Further compression is possible, but is being procrastinated.
    
    Changes from v7 (http://lkml.org/lkml/2009/10/9/388)
    
    o       Apply Lai Jiangshan's review comments (aside from
    might_sleep()   in synchronize_sched(), which is covered by SMP builds).
    
    o       Fix up expedited primitives.
    
    Changes from v6 (http://lkml.org/lkml/2009/9/23/293).
    
    o       Forward ported to put it into the 2.6.33 stream.
    
    o       Added lockdep support.
    
    o       Make lightweight rcu_barrier.
    
    Changes from v5 (http://lkml.org/lkml/2009/6/23/12).
    
    o       Ported to latest pre-2.6.32 merge window kernel.
    
            - Renamed rcu_qsctr_inc() to rcu_sched_qs().
            - Renamed rcu_bh_qsctr_inc() to rcu_bh_qs().
            - Provided trivial rcu_cpu_notify().
            - Provided trivial exit_rcu().
            - Provided trivial rcu_needs_cpu().
            - Fixed up the rcu_*_enter/exit() functions in linux/hardirq.h.
    
    o       Removed the dependence on EMBEDDED, with a view to making
            TINY_RCU default for !SMP at some time in the future.
    
    o       Added (trivial) support for expedited grace periods.
    
    Changes from v4 (http://lkml.org/lkml/2009/5/2/91) include:
    
    o       Squeeze the size down a bit further by removing the
            ->completed field from struct rcu_ctrlblk.
    
    o       This permits synchronize_rcu() to become the empty function.
            Previous concerns about rcutorture were unfounded, as
            rcutorture correctly handles a constant value from
            rcu_batches_completed() and rcu_batches_completed_bh().
    
    Changes from v3 (http://lkml.org/lkml/2009/3/29/221) include:
    
    o       Changed rcu_batches_completed(), rcu_batches_completed_bh()
            rcu_enter_nohz(), rcu_exit_nohz(), rcu_nmi_enter(), and
            rcu_nmi_exit(), to be static inlines, as suggested by David
            Howells.  Doing this saves about 100 bytes from rcutiny.o.
            (The numbers between v3 and this v4 of the patch are not directly
            comparable, since they are against different versions of Linux.)
    
    Changes from v2 (http://lkml.org/lkml/2009/2/3/333) include:
    
    o       Fix whitespace issues.
    
    o       Change short-circuit "||" operator to instead be "+" in order
    to      fix performance bug noted by "kraai" on LWN.
    
                    (http://lwn.net/Articles/324348/)
    
    Changes from v1 (http://lkml.org/lkml/2009/1/13/440) include:
    
    o       This version depends on EMBEDDED as well as !SMP, as suggested
            by Ingo.
    
    o       Updated rcu_needs_cpu() to unconditionally return zero,
            permitting the CPU to enter dynticks-idle mode at any time.
            This works because callbacks can be invoked upon entry to
            dynticks-idle mode.
    
    o       Paul is now OK with this being included, based on a poll at
    the     Kernel Miniconf at linux.conf.au, where about ten people said
            that they cared about saving 900 bytes on single-CPU systems.
    
    o       Applies to both mainline and tip/core/rcu.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Acked-by: Josh Triplett <josh@joshtriplett.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: avi@redhat.com
    Cc: mtosatti@redhat.com
    LKML-Reference: <12565226351355-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 6d527ee82b2b..d5b387669dab 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -139,10 +139,34 @@ static inline void account_system_vtime(struct task_struct *tsk)
 #endif
 
 #if defined(CONFIG_NO_HZ)
+#if defined(CONFIG_TINY_RCU)
+extern void rcu_enter_nohz(void);
+extern void rcu_exit_nohz(void);
+
+static inline void rcu_irq_enter(void)
+{
+	rcu_exit_nohz();
+}
+
+static inline void rcu_irq_exit(void)
+{
+	rcu_enter_nohz();
+}
+
+static inline void rcu_nmi_enter(void)
+{
+}
+
+static inline void rcu_nmi_exit(void)
+{
+}
+
+#else
 extern void rcu_irq_enter(void);
 extern void rcu_irq_exit(void);
 extern void rcu_nmi_enter(void);
 extern void rcu_nmi_exit(void);
+#endif
 #else
 # define rcu_irq_enter() do { } while (0)
 # define rcu_irq_exit() do { } while (0)

commit 774a694f8cd08115d130a290d73c6d8563f26b1b
Merge: 4f0ac8541678 e1f8450854d6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 11 13:23:18 2009 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (64 commits)
      sched: Fix sched::sched_stat_wait tracepoint field
      sched: Disable NEW_FAIR_SLEEPERS for now
      sched: Keep kthreads at default priority
      sched: Re-tune the scheduler latency defaults to decrease worst-case latencies
      sched: Turn off child_runs_first
      sched: Ensure that a child can't gain time over it's parent after fork()
      sched: enable SD_WAKE_IDLE
      sched: Deal with low-load in wake_affine()
      sched: Remove short cut from select_task_rq_fair()
      sched: Turn on SD_BALANCE_NEWIDLE
      sched: Clean up topology.h
      sched: Fix dynamic power-balancing crash
      sched: Remove reciprocal for cpu_power
      sched: Try to deal with low capacity, fix update_sd_power_savings_stats()
      sched: Try to deal with low capacity
      sched: Scale down cpu_power due to RT tasks
      sched: Implement dynamic cpu_power
      sched: Add smt_gain
      sched: Update the cpu_power sum during load-balance
      sched: Add SD_PREFER_SIBLING
      ...

commit b560d8ad8583803978aaaeba50ef29dc8e97a610
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 21 22:08:51 2009 -0700

    rcu: Expunge lingering references to CONFIG_CLASSIC_RCU, optimize on !SMP
    
    A couple of references to CONFIG_CLASSIC_RCU have survived.
    Although these are harmless, it is past time for them to go.
    The one in hardirq.h is strictly a readability problem.
    
    The two in pagemap.h appear to disable a !SMP performance
    optimization (which this patch re-enables).
    
    This does raise the issue as to whether pagemap.h should really
    be referring to the CPU implementation.  Long term, I intend to
    make the RCU implementation driven by CONFIG_PREEMPT, at which
    point these should change from defined(CONFIG_TREE_RCU) to
    !defined(CONFIG_PREEMPT). In the meantime, is there something
    else that could be done in pagemap.h?
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <20090822050851.GA8414@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8246c697863d..330cb31bb496 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -132,7 +132,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
 }
 #endif
 
-#if defined(CONFIG_NO_HZ) && !defined(CONFIG_CLASSIC_RCU)
+#if defined(CONFIG_NO_HZ)
 extern void rcu_irq_enter(void);
 extern void rcu_irq_exit(void);
 extern void rcu_nmi_enter(void);
@@ -142,7 +142,7 @@ extern void rcu_nmi_exit(void);
 # define rcu_irq_exit() do { } while (0)
 # define rcu_nmi_enter() do { } while (0)
 # define rcu_nmi_exit() do { } while (0)
-#endif /* #if defined(CONFIG_NO_HZ) && !defined(CONFIG_CLASSIC_RCU) */
+#endif /* #if defined(CONFIG_NO_HZ) */
 
 /*
  * It is safe to do non-atomic ops on ->hardirq_context,

commit 8e5b59a2d728e6963b35dba8bb36e0b70267462e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Aug 6 16:02:50 2009 -0700

    sched: Add default defines for PREEMPT_ACTIVE
    
    The PREEMPT_ACTIVE setting doesn't actually need to be
    arch-specific, so set up a sane default for all arches to
    (hopefully) migrate to.
    
    > if we look at linux/hardirq.h, it makes this claim:
    >  * - bit 28 is the PREEMPT_ACTIVE flag
    > if that's true, then why are we letting any arch set this define ?  a
    > quick survey shows that half the arches (11) are using 0x10000000 (bit
    > 28) while the other half (10) are using 0x4000000 (bit 26).  and then
    > there is the ia64 oddity which uses bit 30.  the exact value here
    > shouldnt really matter across arches though should it ?
    
    actually alpha, arm and avr32 also use bit 30 (0x40000000),
    there are only five (or eight, depending on how you count)
    architectures (blackfin, h8300, m68k, s390 and sparc) using bit
    26.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8246c697863d..0d885fd75111 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -64,6 +64,12 @@
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
 #define NMI_OFFSET	(1UL << NMI_SHIFT)
 
+#ifndef PREEMPT_ACTIVE
+#define PREEMPT_ACTIVE_BITS	1
+#define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)
+#define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)
+#endif
+
 #if PREEMPT_ACTIVE < (1 << (NMI_SHIFT + NMI_BITS))
 #error PREEMPT_ACTIVE is too low!
 #endif

commit 405f55712dfe464b3240d7816cc4fe4174831be2
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Jul 11 22:08:37 2009 +0400

    headers: smp_lock.h redux
    
    * Remove smp_lock.h from files which don't need it (including some headers!)
    * Add smp_lock.h to files which do need it
    * Make smp_lock.h include conditional in hardirq.h
      It's needed only for one kernel_locked() usage which is under CONFIG_PREEMPT
    
      This will make hardirq.h inclusion cheaper for every PREEMPT=n config
      (which includes allmodconfig/allyesconfig, BTW)
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 45257475623c..8246c697863d 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -2,7 +2,9 @@
 #define LINUX_HARDIRQ_H
 
 #include <linux/preempt.h>
+#ifdef CONFIG_PREEMPT
 #include <linux/smp_lock.h>
+#endif
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <asm/hardirq.h>

commit 9efe21cb82b5dbe3b0b2ae4de4eccc64ecb94e95
Merge: de18836e447c 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 01:41:22 2009 +0200

    Merge branch 'linus' into irq/threaded
    
    Conflicts:
            include/linux/irq.h
            kernel/irq/handle.c

commit 3aa551c9b4c40018f0e261a178e3d25478dc04a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 23 18:28:15 2009 +0100

    genirq: add threaded interrupt handler support
    
    Add support for threaded interrupt handlers:
    
    A device driver can request that its main interrupt handler runs in a
    thread. To achive this the device driver requests the interrupt with
    request_threaded_irq() and provides additionally to the handler a
    thread function. The handler function is called in hard interrupt
    context and needs to check whether the interrupt originated from the
    device. If the interrupt originated from the device then the handler
    can either return IRQ_HANDLED or IRQ_WAKE_THREAD. IRQ_HANDLED is
    returned when no further action is required. IRQ_WAKE_THREAD causes
    the genirq code to invoke the threaded (main) handler. When
    IRQ_WAKE_THREAD is returned handler must have disabled the interrupt
    on the device level. This is mandatory for shared interrupt handlers,
    but we need to do it as well for obscure x86 hardware where disabling
    an interrupt on the IO_APIC level redirects the interrupt to the
    legacy PIC interrupt lines.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index f83288347dda..2dfaadbdb2ac 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -105,7 +105,7 @@
 # define IRQ_EXIT_OFFSET HARDIRQ_OFFSET
 #endif
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_GENERIC_HARDIRQS)
 extern void synchronize_irq(unsigned int irq);
 #else
 # define synchronize_irq(irq)	barrier()

commit 2a7b8df04c11a70105c1abe67d006455d3bdc944
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 12 14:16:46 2009 -0500

    sched: do not account for NMIs
    
    Impact: avoid corruption in system time accounting
    
    Martin Schwidefsky told me that there was an issue with NMIs and
    system accounting. The problem is that the accounting code is
    not reentrant, and if an NMI goes off after an interrupt it can
    corrupt the accounting.
    
    For now, the best we can do is to treat NMIs like SMIs and they
    are not accounted for.
    
    This patch changes nmi_enter to not call __irq_enter and to do
    the preempt-count and tracing calls directly.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 9841221f53f2..faa1cf848bcd 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -175,24 +175,24 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
-#define nmi_enter()				\
-	do {					\
-		ftrace_nmi_enter();		\
-		BUG_ON(in_nmi());		\
-		add_preempt_count(NMI_OFFSET);	\
-		lockdep_off();			\
-		rcu_nmi_enter();		\
-		__irq_enter();			\
+#define nmi_enter()						\
+	do {							\
+		ftrace_nmi_enter();				\
+		BUG_ON(in_nmi());				\
+		add_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		lockdep_off();					\
+		rcu_nmi_enter();				\
+		trace_hardirq_enter();				\
 	} while (0)
 
-#define nmi_exit()				\
-	do {					\
-		__irq_exit();			\
-		rcu_nmi_exit();			\
-		lockdep_on();			\
-		BUG_ON(!in_nmi());		\
-		sub_preempt_count(NMI_OFFSET);	\
-		ftrace_nmi_exit();		\
+#define nmi_exit()						\
+	do {							\
+		trace_hardirq_exit();				\
+		rcu_nmi_exit();					\
+		lockdep_on();					\
+		BUG_ON(!in_nmi());				\
+		sub_preempt_count(NMI_OFFSET + HARDIRQ_OFFSET);	\
+		ftrace_nmi_exit();				\
 	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit 5a5fb7dbe88dd57dc2bef0f3be9da991e789612d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 12 10:53:37 2009 -0500

    preempt-count: force hardirq-count to max of 10
    
    To add a bit in the preempt_count to be set when in NMI context, we
    found that some archs did not have enough bits to spare. This is
    due to the hardirq_count being a mask that can hold NR_IRQS.
    
    Some archs allow for over 16000 IRQs, and that would require a mask
    of 14 bits. The sofitrq mask is 8 bits and the preempt disable mask
    is also 8 bits.  The PREEMP_ACTIVE bit is bit 30, and bit 31 would
    make the preempt_count (which is type int) a negative number.
    A negative preempt_count is a sign of failure.
    
    Add them up 14+8+8+1+1 you get 32 bits. No room for the NMI bit.
    
    But the hardirq_count is to track the number of nested IRQs, not
    the number of total IRQs.  This originally took the paranoid approach
    of setting the max nesting to NR_IRQS. But when we have archs with
    over 1000 IRQs, it is not practical to think they will ever all
    nest on a single CPU. Not to mention that this would most definitely
    cause a stack overflow.
    
    This patch sets a max of 10 bits to be used for IRQ nesting.
    I did a 'git grep HARDIRQ' to examine all users of HARDIRQ_BITS and
    HARDIRQ_MASK, and found that making it a max of 10 would not hurt
    anyone. I did find that the m68k expected it to be 8 bits, so
    I allow for the archs to set the number to be less than 10.
    
    I removed the setting of HARDIRQ_BITS from the archs that set it
    to more than 10. This includes ALPHA, ia64 and avr32.
    
    This will always allow room for the NMI bit, and if we need to allow
    for NMI nesting, we have 4 bits to play with.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index f3cf86e1465b..9841221f53f2 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -15,61 +15,61 @@
  * - bits 0-7 are the preemption count (max preemption depth: 256)
  * - bits 8-15 are the softirq count (max # of softirqs: 256)
  *
- * The hardirq count can be overridden per architecture, the default is:
+ * The hardirq count can in theory reach the same as NR_IRQS.
+ * In reality, the number of nested IRQS is limited to the stack
+ * size as well. For archs with over 1000 IRQS it is not practical
+ * to expect that they will all nest. We give a max of 10 bits for
+ * hardirq nesting. An arch may choose to give less than 10 bits.
+ * m68k expects it to be 8.
  *
- * - bits 16-27 are the hardirq count (max # of hardirqs: 4096)
- * - ( bit 28 is the PREEMPT_ACTIVE flag. )
+ * - bits 16-25 are the hardirq count (max # of nested hardirqs: 1024)
+ * - bit 26 is the NMI_MASK
+ * - bit 28 is the PREEMPT_ACTIVE flag
  *
  * PREEMPT_MASK: 0x000000ff
  * SOFTIRQ_MASK: 0x0000ff00
- * HARDIRQ_MASK: 0x0fff0000
+ * HARDIRQ_MASK: 0x03ff0000
+ *     NMI_MASK: 0x04000000
  */
 #define PREEMPT_BITS	8
 #define SOFTIRQ_BITS	8
+#define NMI_BITS	1
 
-#ifndef HARDIRQ_BITS
-#define HARDIRQ_BITS	12
+#define MAX_HARDIRQ_BITS 10
 
-#ifndef MAX_HARDIRQS_PER_CPU
-#define MAX_HARDIRQS_PER_CPU NR_IRQS
+#ifndef HARDIRQ_BITS
+# define HARDIRQ_BITS	MAX_HARDIRQ_BITS
 #endif
 
-/*
- * The hardirq mask has to be large enough to have space for potentially
- * all IRQ sources in the system nesting on a single CPU.
- */
-#if (1 << HARDIRQ_BITS) < MAX_HARDIRQS_PER_CPU
-# error HARDIRQ_BITS is too low!
-#endif
+#if HARDIRQ_BITS > MAX_HARDIRQ_BITS
+#error HARDIRQ_BITS too high!
 #endif
 
 #define PREEMPT_SHIFT	0
 #define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
 #define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
+#define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)
 
 #define __IRQ_MASK(x)	((1UL << (x))-1)
 
 #define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
 #define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
 #define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
+#define NMI_MASK	(__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)
 
 #define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
 #define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
+#define NMI_OFFSET	(1UL << NMI_SHIFT)
 
-#if PREEMPT_ACTIVE < (1 << (HARDIRQ_SHIFT + HARDIRQ_BITS))
+#if PREEMPT_ACTIVE < (1 << (NMI_SHIFT + NMI_BITS))
 #error PREEMPT_ACTIVE is too low!
 #endif
 
-#define NMI_OFFSET	(PREEMPT_ACTIVE << 1)
-
-#if NMI_OFFSET >= 0x80000000
-#error PREEMPT_ACTIVE too high!
-#endif
-
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))
+#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
+				 | NMI_MASK))
 
 /*
  * Are we doing bottom half or hardware interrupt processing?
@@ -82,7 +82,7 @@
 /*
  * Are we in NMI context?
  */
-#define in_nmi()	(preempt_count() & NMI_OFFSET)
+#define in_nmi()	(preempt_count() & NMI_MASK)
 
 #if defined(CONFIG_PREEMPT)
 # define PREEMPT_INATOMIC_BASE kernel_locked()

commit 375b38b4214f29109a393ab762d468054bf52354
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 6 00:51:37 2009 -0500

    nmi: add generic nmi tracking state
    
    This code adds an in_nmi() macro that uses the current tasks preempt count
    to track when it is in NMI context. Other parts of the kernel can
    use this to determine if the context is in NMI context or not.
    
    This code was inspired by the -rt patch in_nmi version that was
    written by Peter Zijlstra, who borrowed that code from
    Mathieu Desnoyers.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index f83288347dda..f3cf86e1465b 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -61,6 +61,12 @@
 #error PREEMPT_ACTIVE is too low!
 #endif
 
+#define NMI_OFFSET	(PREEMPT_ACTIVE << 1)
+
+#if NMI_OFFSET >= 0x80000000
+#error PREEMPT_ACTIVE too high!
+#endif
+
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))
@@ -73,6 +79,11 @@
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
 
+/*
+ * Are we in NMI context?
+ */
+#define in_nmi()	(preempt_count() & NMI_OFFSET)
+
 #if defined(CONFIG_PREEMPT)
 # define PREEMPT_INATOMIC_BASE kernel_locked()
 # define PREEMPT_CHECK_OFFSET 1
@@ -167,6 +178,8 @@ extern void irq_exit(void);
 #define nmi_enter()				\
 	do {					\
 		ftrace_nmi_enter();		\
+		BUG_ON(in_nmi());		\
+		add_preempt_count(NMI_OFFSET);	\
 		lockdep_off();			\
 		rcu_nmi_enter();		\
 		__irq_enter();			\
@@ -177,6 +190,8 @@ extern void irq_exit(void);
 		__irq_exit();			\
 		rcu_nmi_exit();			\
 		lockdep_on();			\
+		BUG_ON(!in_nmi());		\
+		sub_preempt_count(NMI_OFFSET);	\
 		ftrace_nmi_exit();		\
 	} while (0)
 

commit 5f34fe1cfc1bdd8b4711bbe37421fba4ed0d1ed4
Merge: eca1bf5b4fab 6638101c1124
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 30 16:10:19 2008 -0800

    Merge branch 'core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (63 commits)
      stacktrace: provide save_stack_trace_tsk() weak alias
      rcu: provide RCU options on non-preempt architectures too
      printk: fix discarding message when recursion_bug
      futex: clean up futex_(un)lock_pi fault handling
      "Tree RCU": scalable classic RCU implementation
      futex: rename field in futex_q to clarify single waiter semantics
      x86/swiotlb: add default swiotlb_arch_range_needs_mapping
      x86/swiotlb: add default phys<->bus conversion
      x86: unify pci iommu setup and allow swiotlb to compile for 32 bit
      x86: add swiotlb allocation functions
      swiotlb: consolidate swiotlb info message printing
      swiotlb: support bouncing of HighMem pages
      swiotlb: factor out copy to/from device
      swiotlb: add arch hook to force mapping
      swiotlb: allow architectures to override phys<->bus<->phys conversions
      swiotlb: add comment where we handle the overflow of a dma mask on 32 bit
      rcu: fix rcutorture behavior during reboot
      resources: skip sanity check of busy resources
      swiotlb: move some definitions to header
      swiotlb: allow architectures to override swiotlb pool allocation
      ...
    
    Fix up trivial conflicts in
      arch/x86/kernel/Makefile
      arch/x86/mm/init_32.c
      include/linux/hardirq.h
    as per Ingo's suggestions.

commit 64db4cfff99c04cd5f550357edcc8780f96b54a2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Dec 18 21:55:32 2008 +0100

    "Tree RCU": scalable classic RCU implementation
    
    This patch fixes a long-standing performance bug in classic RCU that
    results in massive internal-to-RCU lock contention on systems with
    more than a few hundred CPUs.  Although this patch creates a separate
    flavor of RCU for ease of review and patch maintenance, it is intended
    to replace classic RCU.
    
    This patch still handles stress better than does mainline, so I am still
    calling it ready for inclusion.  This patch is against the -tip tree.
    Nevertheless, experience on an actual 1000+ CPU machine would still be
    most welcome.
    
    Most of the changes noted below were found while creating an rcutiny
    (which should permit ejecting the current rcuclassic) and while doing
    detailed line-by-line documentation.
    
    Updates from v9 (http://lkml.org/lkml/2008/12/2/334):
    
    o       Fixes from remainder of line-by-line code walkthrough,
            including comment spelling, initialization, undesirable
            narrowing due to type conversion, removing redundant memory
            barriers, removing redundant local-variable initialization,
            and removing redundant local variables.
    
            I do not believe that any of these fixes address the CPU-hotplug
            issues that Andi Kleen was seeing, but please do give it a whirl
            in case the machine is smarter than I am.
    
            A writeup from the walkthrough may be found at the following
            URL, in case you are suffering from terminal insomnia or
            masochism:
    
            http://www.kernel.org/pub/linux/kernel/people/paulmck/tmp/rcutree-walkthrough.2008.12.16a.pdf
    
    o       Made rcutree tracing use seq_file, as suggested some time
            ago by Lai Jiangshan.
    
    o       Added a .csv variant of the rcudata debugfs trace file, to allow
            people having thousands of CPUs to drop the data into
            a spreadsheet.  Tested with oocalc and gnumeric.  Updated
            documentation to suit.
    
    Updates from v8 (http://lkml.org/lkml/2008/11/15/139):
    
    o       Fix a theoretical race between grace-period initialization and
            force_quiescent_state() that could occur if more than three
            jiffies were required to carry out the grace-period
            initialization.  Which it might, if you had enough CPUs.
    
    o       Apply Ingo's printk-standardization patch.
    
    o       Substitute local variables for repeated accesses to global
            variables.
    
    o       Fix comment misspellings and redundant (but harmless) increments
            of ->n_rcu_pending (this latter after having explicitly added it).
    
    o       Apply checkpatch fixes.
    
    Updates from v7 (http://lkml.org/lkml/2008/10/10/291):
    
    o       Fixed a number of problems noted by Gautham Shenoy, including
            the cpu-stall-detection bug that he was having difficulty
            convincing me was real.  ;-)
    
    o       Changed cpu-stall detection to wait for ten seconds rather than
            three in order to reduce false positive, as suggested by Ingo
            Molnar.
    
    o       Produced a design document (http://lwn.net/Articles/305782/).
            The act of writing this document uncovered a number of both
            theoretical and "here and now" bugs as noted below.
    
    o       Fix dynticks_nesting accounting confusion, simplify WARN_ON()
            condition, fix kerneldoc comments, and add memory barriers
            in dynticks interface functions.
    
    o       Add more data to tracing.
    
    o       Remove unused "rcu_barrier" field from rcu_data structure.
    
    o       Count calls to rcu_pending() from scheduling-clock interrupt
            to use as a surrogate timebase should jiffies stop counting.
    
    o       Fix a theoretical race between force_quiescent_state() and
            grace-period initialization.  Yes, initialization does have to
            go on for some jiffies for this race to occur, but given enough
            CPUs...
    
    Updates from v6 (http://lkml.org/lkml/2008/9/23/448):
    
    o       Fix a number of checkpatch.pl complaints.
    
    o       Apply review comments from Ingo Molnar and Lai Jiangshan
            on the stall-detection code.
    
    o       Fix several bugs in !CONFIG_SMP builds.
    
    o       Fix a misspelled config-parameter name so that RCU now announces
            at boot time if stall detection is configured.
    
    o       Run tests on numerous combinations of configurations parameters,
            which after the fixes above, now build and run correctly.
    
    Updates from v5 (http://lkml.org/lkml/2008/9/15/92, bad subject line):
    
    o       Fix a compiler error in the !CONFIG_FANOUT_EXACT case (blew a
            changeset some time ago, and finally got around to retesting
            this option).
    
    o       Fix some tracing bugs in rcupreempt that caused incorrect
            totals to be printed.
    
    o       I now test with a more brutal random-selection online/offline
            script (attached).  Probably more brutal than it needs to be
            on the people reading it as well, but so it goes.
    
    o       A number of optimizations and usability improvements:
    
            o       Make rcu_pending() ignore the grace-period timeout when
                    there is no grace period in progress.
    
            o       Make force_quiescent_state() avoid going for a global
                    lock in the case where there is no grace period in
                    progress.
    
            o       Rearrange struct fields to improve struct layout.
    
            o       Make call_rcu() initiate a grace period if RCU was
                    idle, rather than waiting for the next scheduling
                    clock interrupt.
    
            o       Invoke rcu_irq_enter() and rcu_irq_exit() only when
                    idle, as suggested by Andi Kleen.  I still don't
                    completely trust this change, and might back it out.
    
            o       Make CONFIG_RCU_TRACE be the single config variable
                    manipulated for all forms of RCU, instead of the prior
                    confusion.
    
            o       Document tracing files and formats for both rcupreempt
                    and rcutree.
    
    Updates from v4 for those missing v5 given its bad subject line:
    
    o       Separated dynticks interface so that NMIs and irqs call separate
            functions, greatly simplifying it.  In particular, this code
            no longer requires a proof of correctness.  ;-)
    
    o       Separated dynticks state out into its own per-CPU structure,
            avoiding the duplicated accounting.
    
    o       The case where a dynticks-idle CPU runs an irq handler that
            invokes call_rcu() is now correctly handled, forcing that CPU
            out of dynticks-idle mode.
    
    o       Review comments have been applied (thank you all!!!).
            For but one example, fixed the dynticks-ordering issue that
            Manfred pointed out, saving me much debugging.  ;-)
    
    o       Adjusted rcuclassic and rcupreempt to handle dynticks changes.
    
    Attached is an updated patch to Classic RCU that applies a hierarchy,
    greatly reducing the contention on the top-level lock for large machines.
    This passes 10-hour concurrent rcutorture and online-offline testing on
    128-CPU ppc64 without dynticks enabled, and exposes some timekeeping
    bugs in presence of dynticks (exciting working on a system where
    "sleep 1" hangs until interrupted...), which were fixed in the
    2.6.27 kernel.  It is getting more reliable than mainline by some
    measures, so the next version will be against -tip for inclusion.
    See also Manfred Spraul's recent patches (or his earlier work from
    2004 at http://marc.info/?l=linux-kernel&m=108546384711797&w=2).
    We will converge onto a common patch in the fullness of time, but are
    currently exploring different regions of the design space.  That said,
    I have already gratefully stolen quite a few of Manfred's ideas.
    
    This patch provides CONFIG_RCU_FANOUT, which controls the bushiness
    of the RCU hierarchy.  Defaults to 32 on 32-bit machines and 64 on
    64-bit machines.  If CONFIG_NR_CPUS is less than CONFIG_RCU_FANOUT,
    there is no hierarchy.  By default, the RCU initialization code will
    adjust CONFIG_RCU_FANOUT to balance the hierarchy, so strongly NUMA
    architectures may choose to set CONFIG_RCU_FANOUT_EXACT to disable
    this balancing, allowing the hierarchy to be exactly aligned to the
    underlying hardware.  Up to two levels of hierarchy are permitted
    (in addition to the root node), allowing up to 16,384 CPUs on 32-bit
    systems and up to 262,144 CPUs on 64-bit systems.  I just know that I
    am going to regret saying this, but this seems more than sufficient
    for the foreseeable future.  (Some architectures might wish to set
    CONFIG_RCU_FANOUT=4, which would limit such architectures to 64 CPUs.
    If this becomes a real problem, additional levels can be added, but I
    doubt that it will make a significant difference on real hardware.)
    
    In the common case, a given CPU will manipulate its private rcu_data
    structure and the rcu_node structure that it shares with its immediate
    neighbors.  This can reduce both lock and memory contention by multiple
    orders of magnitude, which should eliminate the need for the strange
    manipulations that are reported to be required when running Linux on
    very large systems.
    
    Some shortcomings:
    
    o       More bugs will probably surface as a result of an ongoing
            line-by-line code inspection.
    
            Patches will be provided as required.
    
    o       There are probably hangs, rcutorture failures, &c.  Seems
            quite stable on a 128-CPU machine, but that is kind of small
            compared to 4096 CPUs.  However, seems to do better than
            mainline.
    
            Patches will be provided as required.
    
    o       The memory footprint of this version is several KB larger
            than rcuclassic.
    
            A separate UP-only rcutiny patch will be provided, which will
            reduce the memory footprint significantly, even compared
            to the old rcuclassic.  One such patch passes light testing,
            and has a memory footprint smaller even than rcuclassic.
            Initial reaction from various embedded guys was "it is not
            worth it", so am putting it aside.
    
    Credits:
    
    o       Manfred Spraul for ideas, review comments, and bugs spotted,
            as well as some good friendly competition.  ;-)
    
    o       Josh Triplett, Ingo Molnar, Peter Zijlstra, Mathieu Desnoyers,
            Lai Jiangshan, Andi Kleen, Andy Whitcroft, and Andrew Morton
            for reviews and comments.
    
    o       Thomas Gleixner for much-needed help with some timer issues
            (see patches below).
    
    o       Jon M. Tollefson, Tim Pepper, Andrew Theurer, Jose R. Santos,
            Andy Whitcroft, Darrick Wong, Nishanth Aravamudan, Anton
            Blanchard, Dave Kleikamp, and Nathan Lynch for keeping machines
            alive despite my heavy abuse^Wtesting.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 181006cc94a0..9b70b9231693 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -118,13 +118,17 @@ static inline void account_system_vtime(struct task_struct *tsk)
 }
 #endif
 
-#if defined(CONFIG_PREEMPT_RCU) && defined(CONFIG_NO_HZ)
+#if defined(CONFIG_NO_HZ) && !defined(CONFIG_CLASSIC_RCU)
 extern void rcu_irq_enter(void);
 extern void rcu_irq_exit(void);
+extern void rcu_nmi_enter(void);
+extern void rcu_nmi_exit(void);
 #else
 # define rcu_irq_enter() do { } while (0)
 # define rcu_irq_exit() do { } while (0)
-#endif /* CONFIG_PREEMPT_RCU */
+# define rcu_nmi_enter() do { } while (0)
+# define rcu_nmi_exit() do { } while (0)
+#endif /* #if defined(CONFIG_NO_HZ) && !defined(CONFIG_CLASSIC_RCU) */
 
 /*
  * It is safe to do non-atomic ops on ->hardirq_context,
@@ -134,7 +138,6 @@ extern void rcu_irq_exit(void);
  */
 #define __irq_enter()					\
 	do {						\
-		rcu_irq_enter();			\
 		account_system_vtime(current);		\
 		add_preempt_count(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
@@ -153,7 +156,6 @@ extern void irq_enter(void);
 		trace_hardirq_exit();			\
 		account_system_vtime(current);		\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
-		rcu_irq_exit();				\
 	} while (0)
 
 /*
@@ -161,7 +163,7 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
-#define nmi_enter()		do { lockdep_off(); __irq_enter(); } while (0)
-#define nmi_exit()		do { __irq_exit(); lockdep_on(); } while (0)
+#define nmi_enter()		do { lockdep_off(); rcu_nmi_enter(); __irq_enter(); } while (0)
+#define nmi_exit()		do { __irq_exit(); rcu_nmi_exit(); lockdep_on(); } while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit 6a60dd121c5b6c2d827e99b38c1326f2600c3891
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 6 15:55:21 2008 -0500

    ftrace: split out hardirq ftrace code into own header
    
    Impact: moving of function prototypes into own header file
    
    ftrace.h is too big of a file for hardirq.h, and some archs will fail
    to build because of the include dependencies not being met.
    
    This patch pulls out the required prototypes for hardirq.h into a smaller
    and safer ftrace_irq.h file.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index ffc16ab5a878..89a56d79e4c6 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -4,7 +4,7 @@
 #include <linux/preempt.h>
 #include <linux/smp_lock.h>
 #include <linux/lockdep.h>
-#include <linux/ftrace.h>
+#include <linux/ftrace_irq.h>
 #include <asm/hardirq.h>
 #include <asm/system.h>
 

commit 7e5e26a3d8ac4bcadb380073dc9604c07a9a6198
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 31 09:36:38 2008 -0400

    ftrace: fix hardirq header for non ftrace archs
    
    Impact: build fix for non-ftrace architectures
    
    Not all archs implement ftrace, and therefore do not have an asm/ftrace.h.
    This patch corrects the problem.
    
    The ftrace_nmi_enter/exit now must be defined for all archs that implement
    dynamic ftrace. Currently, only x86 does.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 0087cb43becf..ffc16ab5a878 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -4,8 +4,8 @@
 #include <linux/preempt.h>
 #include <linux/smp_lock.h>
 #include <linux/lockdep.h>
+#include <linux/ftrace.h>
 #include <asm/hardirq.h>
-#include <asm/ftrace.h>
 #include <asm/system.h>
 
 /*

commit 17666f02b118099028522dfc3df00a235700e216
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 30 16:08:32 2008 -0400

    ftrace: nmi safe code modification
    
    Impact: fix crashes that can occur in NMI handlers, if their code is modified
    
    Modifying code is something that needs special care. On SMP boxes,
    if code that is being modified is also being executed on another CPU,
    that CPU will have undefined results.
    
    The dynamic ftrace uses kstop_machine to make the system act like a
    uniprocessor system. But this does not address NMIs, that can still
    run on other CPUs.
    
    One approach to handle this is to make all code that are used by NMIs
    not be traced. But NMIs can call notifiers that spread throughout the
    kernel and this will be very hard to maintain, and the chance of missing
    a function is very high.
    
    The approach that this patch takes is to have the NMIs modify the code
    if the modification is taking place. The way this works is that just
    writing to code executing on another CPU is not harmful if what is
    written is the same as what exists.
    
    Two buffers are used: an IP buffer and a "code" buffer.
    
    The steps that the patcher takes are:
    
     1) Put in the instruction pointer into the IP buffer
        and the new code into the "code" buffer.
     2) Set a flag that says we are modifying code
     3) Wait for any running NMIs to finish.
     4) Write the code
     5) clear the flag.
     6) Wait for any running NMIs to finish.
    
    If an NMI is executed, it will also write the pending code.
    Multiple writes are OK, because what is being written is the same.
    Then the patcher must wait for all running NMIs to finish before
    going to the next line that must be patched.
    
    This is basically the RCU approach to code modification.
    
    Thanks to Ingo Molnar for suggesting the idea, and to Arjan van de Ven
    for his guidence on what is safe and what is not.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 181006cc94a0..0087cb43becf 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -5,6 +5,7 @@
 #include <linux/smp_lock.h>
 #include <linux/lockdep.h>
 #include <asm/hardirq.h>
+#include <asm/ftrace.h>
 #include <asm/system.h>
 
 /*
@@ -161,7 +162,17 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
-#define nmi_enter()		do { lockdep_off(); __irq_enter(); } while (0)
-#define nmi_exit()		do { __irq_exit(); lockdep_on(); } while (0)
+#define nmi_enter()				\
+	do {					\
+		ftrace_nmi_enter();		\
+		lockdep_off();			\
+		__irq_enter();			\
+	} while (0)
+#define nmi_exit()				\
+	do {					\
+		__irq_exit();			\
+		lockdep_on();			\
+		ftrace_nmi_exit();		\
+	} while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit 8e3e076c5a78519a9f64cd384e8f18bc21882ce0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 10 20:58:02 2008 -0700

    BKL: revert back to the old spinlock implementation
    
    The generic semaphore rewrite had a huge performance regression on AIM7
    (and potentially other BKL-heavy benchmarks) because the generic
    semaphores had been rewritten to be simple to understand and fair.  The
    latter, in particular, turns a semaphore-based BKL implementation into a
    mess of scheduling.
    
    The attempt to fix the performance regression failed miserably (see the
    previous commit 00b41ec2611dc98f87f30753ee00a53db648d662 'Revert
    "semaphore: fix"'), and so for now the simple and sane approach is to
    instead just go back to the old spinlock-based BKL implementation that
    never had any issues like this.
    
    This patch also has the advantage of being reported to fix the
    regression completely according to Yanmin Zhang, unlike the semaphore
    hack which still left a couple percentage point regression.
    
    As a spinlock, the BKL obviously has the potential to be a latency
    issue, but it's not really any different from any other spinlock in that
    respect.  We do want to get rid of the BKL asap, but that has been the
    plan for several years.
    
    These days, the biggest users are in the tty layer (open/release in
    particular) and Alan holds out some hope:
    
      "tty release is probably a few months away from getting cured - I'm
       afraid it will almost certainly be the very last user of the BKL in
       tty to get fixed as it depends on everything else being sanely locked."
    
    so while we're not there yet, we do have a plan of action.
    
    Tested-by: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Alexander Viro <viro@ftp.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 897f723bd222..181006cc94a0 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -72,6 +72,14 @@
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
 
+#if defined(CONFIG_PREEMPT)
+# define PREEMPT_INATOMIC_BASE kernel_locked()
+# define PREEMPT_CHECK_OFFSET 1
+#else
+# define PREEMPT_INATOMIC_BASE 0
+# define PREEMPT_CHECK_OFFSET 0
+#endif
+
 /*
  * Are we running in atomic context?  WARNING: this macro cannot
  * always detect atomic context; in particular, it cannot know about
@@ -79,17 +87,11 @@
  * used in the general case to determine whether sleeping is possible.
  * Do not use in_atomic() in driver code.
  */
-#define in_atomic()		((preempt_count() & ~PREEMPT_ACTIVE) != 0)
-
-#ifdef CONFIG_PREEMPT
-# define PREEMPT_CHECK_OFFSET 1
-#else
-# define PREEMPT_CHECK_OFFSET 0
-#endif
+#define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_INATOMIC_BASE)
 
 /*
  * Check whether we were atomic before we did preempt_disable():
- * (used by the scheduler)
+ * (used by the scheduler, *after* releasing the kernel lock)
  */
 #define in_atomic_preempt_off() \
 		((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_CHECK_OFFSET)

commit 8c703d35fa91911dd92a18c31a718853f483ad80
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Fri Mar 28 14:15:49 2008 -0700

    in_atomic(): document why it is unsuitable for general use
    
    Discourage people from inappropriately using in_atomic()
    
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 49829988bfa0..897f723bd222 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -72,6 +72,13 @@
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
 
+/*
+ * Are we running in atomic context?  WARNING: this macro cannot
+ * always detect atomic context; in particular, it cannot know about
+ * held spinlocks in non-preemptible kernels.  Thus it should not be
+ * used in the general case to determine whether sleeping is possible.
+ * Do not use in_atomic() in driver code.
+ */
 #define in_atomic()		((preempt_count() & ~PREEMPT_ACTIVE) != 0)
 
 #ifdef CONFIG_PREEMPT

commit 2232c2d8e0a6a31061dec311f3d1cf7624bc14f1
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Feb 29 18:46:50 2008 +0100

    rcu: add support for dynamic ticks and preempt rcu
    
    The PREEMPT-RCU can get stuck if a CPU goes idle and NO_HZ is set. The
    idle CPU will not progress the RCU through its grace period and a
    synchronize_rcu my get stuck. Without this patch I have a box that will
    not boot when PREEMPT_RCU and NO_HZ are set. That same box boots fine
    with this patch.
    
    This patch comes from the -rt kernel where it has been tested for
    several months.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 2961ec788046..49829988bfa0 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -109,6 +109,14 @@ static inline void account_system_vtime(struct task_struct *tsk)
 }
 #endif
 
+#if defined(CONFIG_PREEMPT_RCU) && defined(CONFIG_NO_HZ)
+extern void rcu_irq_enter(void);
+extern void rcu_irq_exit(void);
+#else
+# define rcu_irq_enter() do { } while (0)
+# define rcu_irq_exit() do { } while (0)
+#endif /* CONFIG_PREEMPT_RCU */
+
 /*
  * It is safe to do non-atomic ops on ->hardirq_context,
  * because NMI handlers may not preempt and the ops are
@@ -117,6 +125,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
  */
 #define __irq_enter()					\
 	do {						\
+		rcu_irq_enter();			\
 		account_system_vtime(current);		\
 		add_preempt_count(HARDIRQ_OFFSET);	\
 		trace_hardirq_enter();			\
@@ -135,6 +144,7 @@ extern void irq_enter(void);
 		trace_hardirq_exit();			\
 		account_system_vtime(current);		\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
+		rcu_irq_exit();				\
 	} while (0)
 
 /*

commit 6478d8800b75253b2a934ddcb734e13ade023ad0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:33 2008 +0100

    sched: remove the !PREEMPT_BKL code
    
    remove the !PREEMPT_BKL code.
    
    this removes 160 lines of legacy code.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8d302298a161..2961ec788046 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -72,11 +72,7 @@
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
 
-#if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
-# define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != kernel_locked())
-#else
-# define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != 0)
-#endif
+#define in_atomic()		((preempt_count() & ~PREEMPT_ACTIVE) != 0)
 
 #ifdef CONFIG_PREEMPT
 # define PREEMPT_CHECK_OFFSET 1

commit 4da1ce6d9c7e2a6d9236bf4dcfd33cf506082794
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: add in_atomic_preempt_off()
    
    add in_atomic_preempt_off() - debugging helper that will
    simplify schedule().
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 7803014f3a11..8d302298a161 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -78,6 +78,19 @@
 # define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != 0)
 #endif
 
+#ifdef CONFIG_PREEMPT
+# define PREEMPT_CHECK_OFFSET 1
+#else
+# define PREEMPT_CHECK_OFFSET 0
+#endif
+
+/*
+ * Check whether we were atomic before we did preempt_disable():
+ * (used by the scheduler)
+ */
+#define in_atomic_preempt_off() \
+		((preempt_count() & ~PREEMPT_ACTIVE) != PREEMPT_CHECK_OFFSET)
+
 #ifdef CONFIG_PREEMPT
 # define preemptible()	(preempt_count() == 0 && !irqs_disabled())
 # define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)

commit 79bf2bb335b85db25d27421c798595a2fa2a0e82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:03 2007 -0800

    [PATCH] tick-management: dyntick / highres functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add functions to provide dynamic ticks and high resolution timers.  The code
    which keeps track of jiffies and handles the long idle periods is shared
    between tick based and high resolution timer based dynticks.  The dyntick
    functionality can be disabled on the kernel commandline.  Provide also the
    infrastructure to support high resolution timers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 6f657d7f2d04..7803014f3a11 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -106,6 +106,16 @@ static inline void account_system_vtime(struct task_struct *tsk)
  * always balanced, so the interrupted value of ->hardirq_context
  * will always be restored.
  */
+#define __irq_enter()					\
+	do {						\
+		account_system_vtime(current);		\
+		add_preempt_count(HARDIRQ_OFFSET);	\
+		trace_hardirq_enter();			\
+	} while (0)
+
+/*
+ * Enter irq context (on NO_HZ, update jiffies):
+ */
 extern void irq_enter(void);
 
 /*
@@ -123,7 +133,7 @@ extern void irq_enter(void);
  */
 extern void irq_exit(void);
 
-#define nmi_enter()		do { lockdep_off(); irq_enter(); } while (0)
+#define nmi_enter()		do { lockdep_off(); __irq_enter(); } while (0)
 #define nmi_exit()		do { __irq_exit(); lockdep_on(); } while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit dde4b2b5f4ed275250488dabdaf282d9c6e7e2b8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 16 01:27:45 2007 -0800

    [PATCH] uninline irq_enter()
    
    Uninline irq_enter().  [dynticks adds more stuff to it]
    
    No functional changes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 612472aaa79c..6f657d7f2d04 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -106,12 +106,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
  * always balanced, so the interrupted value of ->hardirq_context
  * will always be restored.
  */
-#define irq_enter()					\
-	do {						\
-		account_system_vtime(current);		\
-		add_preempt_count(HARDIRQ_OFFSET);	\
-		trace_hardirq_enter();			\
-	} while (0)
+extern void irq_enter(void);
 
 /*
  * Exit irq context without processing softirqs:

commit 23d0b8b053391afe15c9667d80de77ca88e18b8b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Oct 4 02:16:49 2006 -0700

    [PATCH] genirq: irq: generalize the check for HARDIRQ_BITS
    
    This patch adds support for systems that cannot receive every interrupt on a
    single cpu simultaneously, in the check to see if we have enough HARDIRQ_BITS.
    
    MAX_HARDIRQS_PER_CPU becomes the count of the maximum number of hardare
    generated interrupts per cpu.
    
    On architectures that support per cpu interrupt delivery this can be a
    significant space savings and scalability bonus.
    
    This patch adds support for systems that cannot receive every interrupt on
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rajesh Shah <rajesh.shah@intel.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: "Protasevich, Natalie" <Natalie.Protasevich@UNISYS.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 50d8b5744cf6..612472aaa79c 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -28,11 +28,16 @@
 
 #ifndef HARDIRQ_BITS
 #define HARDIRQ_BITS	12
+
+#ifndef MAX_HARDIRQS_PER_CPU
+#define MAX_HARDIRQS_PER_CPU NR_IRQS
+#endif
+
 /*
  * The hardirq mask has to be large enough to have space for potentially
  * all IRQ sources in the system nesting on a single CPU.
  */
-#if (1 << HARDIRQ_BITS) < NR_IRQS
+#if (1 << HARDIRQ_BITS) < MAX_HARDIRQS_PER_CPU
 # error HARDIRQ_BITS is too low!
 #endif
 #endif

commit fbb9ce9530fd9b66096d5187fa6a115d16d9746c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:50 2006 -0700

    [PATCH] lockdep: core
    
    Do 'make oldconfig' and accept all the defaults for new config options -
    reboot into the kernel and if everything goes well it should boot up fine and
    you should have /proc/lockdep and /proc/lockdep_stats files.
    
    Typically if the lock validator finds some problem it will print out
    voluminous debug output that begins with "BUG: ..." and which syslog output
    can be used by kernel developers to figure out the precise locking scenario.
    
    What does the lock validator do?  It "observes" and maps all locking rules as
    they occur dynamically (as triggered by the kernel's natural use of spinlocks,
    rwlocks, mutexes and rwsems).  Whenever the lock validator subsystem detects a
    new locking scenario, it validates this new rule against the existing set of
    rules.  If this new rule is consistent with the existing set of rules then the
    new rule is added transparently and the kernel continues as normal.  If the
    new rule could create a deadlock scenario then this condition is printed out.
    
    When determining validity of locking, all possible "deadlock scenarios" are
    considered: assuming arbitrary number of CPUs, arbitrary irq context and task
    context constellations, running arbitrary combinations of all the existing
    locking scenarios.  In a typical system this means millions of separate
    scenarios.  This is why we call it a "locking correctness" validator - for all
    rules that are observed the lock validator proves it with mathematical
    certainty that a deadlock could not occur (assuming that the lock validator
    implementation itself is correct and its internal data structures are not
    corrupted by some other kernel subsystem).  [see more details and conditionals
    of this statement in include/linux/lockdep.h and
    Documentation/lockdep-design.txt]
    
    Furthermore, this "all possible scenarios" property of the validator also
    enables the finding of complex, highly unlikely multi-CPU multi-context races
    via single single-context rules, increasing the likelyhood of finding bugs
    drastically.  In practical terms: the lock validator already found a bug in
    the upstream kernel that could only occur on systems with 3 or more CPUs, and
    which needed 3 very unlikely code sequences to occur at once on the 3 CPUs.
    That bug was found and reported on a single-CPU system (!).  So in essence a
    race will be found "piecemail-wise", triggering all the necessary components
    for the race, without having to reproduce the race scenario itself!  In its
    short existence the lock validator found and reported many bugs before they
    actually caused a real deadlock.
    
    To further increase the efficiency of the validator, the mapping is not per
    "lock instance", but per "lock-class".  For example, all struct inode objects
    in the kernel have inode->inotify_mutex.  If there are 10,000 inodes cached,
    then there are 10,000 lock objects.  But ->inotify_mutex is a single "lock
    type", and all locking activities that occur against ->inotify_mutex are
    "unified" into this single lock-class.  The advantage of the lock-class
    approach is that all historical ->inotify_mutex uses are mapped into a single
    (and as narrow as possible) set of locking rules - regardless of how many
    different tasks or inode structures it took to build this set of rules.  The
    set of rules persist during the lifetime of the kernel.
    
    To see the rough magnitude of checking that the lock validator does, here's a
    portion of /proc/lockdep_stats, fresh after bootup:
    
     lock-classes:                            694 [max: 2048]
     direct dependencies:                  1598 [max: 8192]
     indirect dependencies:               17896
     all direct dependencies:             16206
     dependency chains:                    1910 [max: 8192]
     in-hardirq chains:                      17
     in-softirq chains:                     105
     in-process chains:                    1065
     stack-trace entries:                 38761 [max: 131072]
     combined max dependencies:         2033928
     hardirq-safe locks:                     24
     hardirq-unsafe locks:                  176
     softirq-safe locks:                     53
     softirq-unsafe locks:                  137
     irq-safe locks:                         59
     irq-unsafe locks:                      176
    
    The lock validator has observed 1598 actual single-thread locking patterns,
    and has validated all possible 2033928 distinct locking scenarios.
    
    More details about the design of the lock validator can be found in
    Documentation/lockdep-design.txt, which can also found at:
    
       http://redhat.com/~mingo/lockdep-patches/lockdep-design.txt
    
    [bunk@stusta.de: cleanups]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index b1d4332b5cf0..50d8b5744cf6 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -3,6 +3,7 @@
 
 #include <linux/preempt.h>
 #include <linux/smp_lock.h>
+#include <linux/lockdep.h>
 #include <asm/hardirq.h>
 #include <asm/system.h>
 
@@ -122,7 +123,7 @@ static inline void account_system_vtime(struct task_struct *tsk)
  */
 extern void irq_exit(void);
 
-#define nmi_enter()		irq_enter()
-#define nmi_exit()		__irq_exit()
+#define nmi_enter()		do { lockdep_off(); irq_enter(); } while (0)
+#define nmi_exit()		do { __irq_exit(); lockdep_on(); } while (0)
 
 #endif /* LINUX_HARDIRQ_H */

commit de30a2b355ea85350ca2f58f3b9bf4e5bc007986
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:42 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, core
    
    Accurate hard-IRQ-flags and softirq-flags state tracing.
    
    This allows us to attach extra functionality to IRQ flags on/off
    events (such as trace-on/off).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 114ae583cca9..b1d4332b5cf0 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -86,9 +86,6 @@ extern void synchronize_irq(unsigned int irq);
 # define synchronize_irq(irq)	barrier()
 #endif
 
-#define nmi_enter()		irq_enter()
-#define nmi_exit()		sub_preempt_count(HARDIRQ_OFFSET)
-
 struct task_struct;
 
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
@@ -97,12 +94,35 @@ static inline void account_system_vtime(struct task_struct *tsk)
 }
 #endif
 
+/*
+ * It is safe to do non-atomic ops on ->hardirq_context,
+ * because NMI handlers may not preempt and the ops are
+ * always balanced, so the interrupted value of ->hardirq_context
+ * will always be restored.
+ */
 #define irq_enter()					\
 	do {						\
 		account_system_vtime(current);		\
 		add_preempt_count(HARDIRQ_OFFSET);	\
+		trace_hardirq_enter();			\
+	} while (0)
+
+/*
+ * Exit irq context without processing softirqs:
+ */
+#define __irq_exit()					\
+	do {						\
+		trace_hardirq_exit();			\
+		account_system_vtime(current);		\
+		sub_preempt_count(HARDIRQ_OFFSET);	\
 	} while (0)
 
+/*
+ * Exit irq context and process softirqs if needed:
+ */
 extern void irq_exit(void);
 
+#define nmi_enter()		irq_enter()
+#define nmi_exit()		__irq_exit()
+
 #endif /* LINUX_HARDIRQ_H */

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index eab537091f2a..114ae583cca9 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -1,7 +1,6 @@
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 
-#include <linux/config.h>
 #include <linux/preempt.h>
 #include <linux/smp_lock.h>
 #include <asm/hardirq.h>

commit 1f1c12afe5c3e0ef901eec12dee09df4947462ee
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Sat Jan 14 13:21:03 2006 -0800

    [PATCH] s390: cputime misaccounting
    
    finish_arch_switch needs to update the user cpu time as well, not just the
    system cpu time.  Otherwise the partial user cpu time of a process that is
    stored in the lowcore will be (mis-)accounted to the next process.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 71d2b8a723b9..eab537091f2a 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -93,10 +93,6 @@ extern void synchronize_irq(unsigned int irq);
 struct task_struct;
 
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
-static inline void account_user_vtime(struct task_struct *tsk)
-{
-}
-
 static inline void account_system_vtime(struct task_struct *tsk)
 {
 }

commit f037360f2ed111fe89a8f5cb6ba351f4e9934e53
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:57 2005 -0800

    [PATCH] m68k: thread_info header cleanup
    
    a) in smp_lock.h #include of sched.h and spinlock.h moved under #ifdef
       CONFIG_LOCK_KERNEL.
    
    b) interrupt.h now explicitly pulls sched.h (not via smp_lock.h from
       hardirq.h as it used to)
    
    c) in three more places we need changes to compensate for (a) - one place
       in arch/sparc needs string.h now, hardirq.h needs forward declaration of
       task_struct and preempt.h needs direct include of thread_info.h.
    
    d) thread_info-related helpers in sched.h and thread_info.h put under
       ifndef __HAVE_THREAD_FUNCTIONS.  Obviously safe.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 5912874ca83c..71d2b8a723b9 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -90,6 +90,8 @@ extern void synchronize_irq(unsigned int irq);
 #define nmi_enter()		irq_enter()
 #define nmi_exit()		sub_preempt_count(HARDIRQ_OFFSET)
 
+struct task_struct;
+
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
 static inline void account_user_vtime(struct task_struct *tsk)
 {

commit 67bc4eb0b1140a4bf364f2dcca152be659ed9057
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Tue Jul 12 13:58:36 2005 -0700

    [PATCH] hardirq uses preempt
    
    hardirq.h uses preempt_count() from preempt.h
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 8336dba18971..5912874ca83c 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -2,6 +2,7 @@
 #define LINUX_HARDIRQ_H
 
 #include <linux/config.h>
+#include <linux/preempt.h>
 #include <linux/smp_lock.h>
 #include <asm/hardirq.h>
 #include <asm/system.h>

commit 8f28e8fa46625310102aea06fac61ba04c8b5b88
Author: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
Date:   Sat May 28 15:52:02 2005 -0700

    [PATCH] irq code: Add coherence test for PREEMPT_ACTIVE
    
    After porting this fixlet to UML:
    
    http://linux.bkbits.net:8080/linux-2.5/cset@41791ab52lfMuF2i3V-eTIGRBbDYKQ
    
    , I've also added a warning which should refuse compilation with insane values
    for PREEMPT_ACTIVE...  maybe we should simply move PREEMPT_ACTIVE out of
    architectures using GENERIC_IRQS.
    
    Signed-off-by: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index ebc712e91066..8336dba18971 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -43,13 +43,17 @@
 #define __IRQ_MASK(x)	((1UL << (x))-1)
 
 #define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
-#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
 #define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
+#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
 
 #define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
 #define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
 
+#if PREEMPT_ACTIVE < (1 << (HARDIRQ_SHIFT + HARDIRQ_BITS))
+#error PREEMPT_ACTIVE is too low!
+#endif
+
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
new file mode 100644
index 000000000000..ebc712e91066
--- /dev/null
+++ b/include/linux/hardirq.h
@@ -0,0 +1,106 @@
+#ifndef LINUX_HARDIRQ_H
+#define LINUX_HARDIRQ_H
+
+#include <linux/config.h>
+#include <linux/smp_lock.h>
+#include <asm/hardirq.h>
+#include <asm/system.h>
+
+/*
+ * We put the hardirq and softirq counter into the preemption
+ * counter. The bitmask has the following meaning:
+ *
+ * - bits 0-7 are the preemption count (max preemption depth: 256)
+ * - bits 8-15 are the softirq count (max # of softirqs: 256)
+ *
+ * The hardirq count can be overridden per architecture, the default is:
+ *
+ * - bits 16-27 are the hardirq count (max # of hardirqs: 4096)
+ * - ( bit 28 is the PREEMPT_ACTIVE flag. )
+ *
+ * PREEMPT_MASK: 0x000000ff
+ * SOFTIRQ_MASK: 0x0000ff00
+ * HARDIRQ_MASK: 0x0fff0000
+ */
+#define PREEMPT_BITS	8
+#define SOFTIRQ_BITS	8
+
+#ifndef HARDIRQ_BITS
+#define HARDIRQ_BITS	12
+/*
+ * The hardirq mask has to be large enough to have space for potentially
+ * all IRQ sources in the system nesting on a single CPU.
+ */
+#if (1 << HARDIRQ_BITS) < NR_IRQS
+# error HARDIRQ_BITS is too low!
+#endif
+#endif
+
+#define PREEMPT_SHIFT	0
+#define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
+#define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
+
+#define __IRQ_MASK(x)	((1UL << (x))-1)
+
+#define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
+#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
+#define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
+
+#define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
+#define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
+#define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
+
+#define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
+#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
+#define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))
+
+/*
+ * Are we doing bottom half or hardware interrupt processing?
+ * Are we in a softirq context? Interrupt context?
+ */
+#define in_irq()		(hardirq_count())
+#define in_softirq()		(softirq_count())
+#define in_interrupt()		(irq_count())
+
+#if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
+# define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != kernel_locked())
+#else
+# define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != 0)
+#endif
+
+#ifdef CONFIG_PREEMPT
+# define preemptible()	(preempt_count() == 0 && !irqs_disabled())
+# define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)
+#else
+# define preemptible()	0
+# define IRQ_EXIT_OFFSET HARDIRQ_OFFSET
+#endif
+
+#ifdef CONFIG_SMP
+extern void synchronize_irq(unsigned int irq);
+#else
+# define synchronize_irq(irq)	barrier()
+#endif
+
+#define nmi_enter()		irq_enter()
+#define nmi_exit()		sub_preempt_count(HARDIRQ_OFFSET)
+
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+static inline void account_user_vtime(struct task_struct *tsk)
+{
+}
+
+static inline void account_system_vtime(struct task_struct *tsk)
+{
+}
+#endif
+
+#define irq_enter()					\
+	do {						\
+		account_system_vtime(current);		\
+		add_preempt_count(HARDIRQ_OFFSET);	\
+	} while (0)
+
+extern void irq_exit(void);
+
+#endif /* LINUX_HARDIRQ_H */
