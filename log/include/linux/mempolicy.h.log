commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 8165278c348a..ea9c15b60a96 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -31,7 +31,7 @@ struct mm_struct;
  * Locking policy for interlave:
  * In process context there is no locking because only the process accesses
  * its own state. All vma manipulation is somewhat protected by a down_read on
- * mmap_sem.
+ * mmap_lock.
  *
  * Freeing policy:
  * Mempolicy objects are reference counted.  A mempolicy will be freed when

commit 20ca87f22b82d5fbf1938e1668ac4f55d749fb8f
Author: Li Xinhai <lixinhai.lxh@gmail.com>
Date:   Wed Apr 1 21:10:52 2020 -0700

    mm/mempolicy: check hugepage migration is supported by arch in vma_migratable()
    
    vma_migratable() is called to check if pages in vma can be migrated before
    go ahead to further actions.  Currently it is used in below code path:
    
    - task_numa_work
    - mbind
    - move_pages
    
    For hugetlb mapping, whether vma is migratable or not is determined by:
    - CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
    - arch_hugetlb_migration_supported
    
    Issue: current code only checks for CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
    alone, and no code should use it directly.  (note that current code in
    vma_migratable don't cause failure or bug because
    unmap_and_move_huge_page() will catch unsupported hugepage and handle it
    properly)
    
    This patch checks the two factors by hugepage_migration_supported for
    impoving code logic and robustness.  It will enable early bail out of
    hugepage migration procedure, but because currently all architecture
    supporting hugepage migration is able to support all page size, we would
    not see performance gain with this patch applied.
    
    vma_migratable() is moved to mm/mempolicy.c, because of the circular
    reference of mempolicy.h and hugetlb.h cause defining it as inline not
    feasible.
    
    Signed-off-by: Li Xinhai <lixinhai.lxh@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Link: http://lkml.kernel.org/r/1579786179-30633-1-git-send-email-lixinhai.lxh@gmail.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5228c62af416..8165278c348a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -173,34 +173,7 @@ extern int mpol_parse_str(char *str, struct mempolicy **mpol);
 extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 
 /* Check if a vma is migratable */
-static inline bool vma_migratable(struct vm_area_struct *vma)
-{
-	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
-		return false;
-
-	/*
-	 * DAX device mappings require predictable access latency, so avoid
-	 * incurring periodic faults.
-	 */
-	if (vma_is_dax(vma))
-		return false;
-
-#ifndef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
-	if (vma->vm_flags & VM_HUGETLB)
-		return false;
-#endif
-
-	/*
-	 * Migration allocates pages in the highest zone. If we cannot
-	 * do so then migration (at least from node to node) is not
-	 * possible.
-	 */
-	if (vma->vm_file &&
-		gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
-								< policy_zone)
-			return false;
-	return true;
-}
+extern bool vma_migratable(struct vm_area_struct *vma);
 
 extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
 extern void mpol_put_task_policy(struct task_struct *);

commit ac79f78dab892fcdc11fda8af5cc5e80d09dca8a
Author: David Rientjes <rientjes@google.com>
Date:   Wed Sep 4 12:54:18 2019 -0700

    Revert "Revert "mm, thp: restore node-local hugepage allocations""
    
    This reverts commit a8282608c88e08b1782141026eab61204c1e533f.
    
    The commit references the original intended semantic for MADV_HUGEPAGE
    which has subsequently taken on three unique purposes:
    
     - enables or disables thp for a range of memory depending on the system's
       config (is thp "enabled" set to "always" or "madvise"),
    
     - determines the synchronous compaction behavior for thp allocations at
       fault (is thp "defrag" set to "always", "defer+madvise", or "madvise"),
       and
    
     - reverts a previous MADV_NOHUGEPAGE (there is no madvise mode to only
       clear previous hugepage advice).
    
    These are the three purposes that currently exist in 5.2 and over the
    past several years that userspace has been written around.  Adding a
    NUMA locality preference adds a fourth dimension to an already conflated
    advice mode.
    
    Based on the semantic that MADV_HUGEPAGE has provided over the past
    several years, there exist workloads that use the tunable based on these
    principles: specifically that the allocation should attempt to
    defragment a local node before falling back.  It is agreed that remote
    hugepages typically (but not always) have a better access latency than
    remote native pages, although on Naples this is at parity for
    intersocket.
    
    The revert commit that this patch reverts allows hugepage allocation to
    immediately allocate remotely when local memory is fragmented.  This is
    contrary to the semantic of MADV_HUGEPAGE over the past several years:
    that is, memory compaction should be attempted locally before falling
    back.
    
    The performance degradation of remote hugepages over local hugepages on
    Rome, for example, is 53.5% increased access latency.  For this reason,
    the goal is to revert back to the 5.2 and previous behavior that would
    attempt local defragmentation before falling back.  With the patch that
    is reverted by this patch, we see performance degradations at the tail
    because the allocator happily allocates the remote hugepage rather than
    even attempting to make a local hugepage available.
    
    zone_reclaim_mode is not a solution to this problem since it does not
    only impact hugepage allocations but rather changes the memory
    allocation strategy for *all* page allocations.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Stefan Priebe - Profihost AG <s.priebe@profihost.ag>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index bac395f1d00a..5228c62af416 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -139,8 +139,6 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 struct mempolicy *get_task_policy(struct task_struct *p);
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 		unsigned long addr);
-struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
-						unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);

commit a8282608c88e08b1782141026eab61204c1e533f
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Tue Aug 13 15:37:53 2019 -0700

    Revert "mm, thp: restore node-local hugepage allocations"
    
    This reverts commit 2f0799a0ffc033b ("mm, thp: restore node-local
    hugepage allocations").
    
    commit 2f0799a0ffc033b was rightfully applied to avoid the risk of a
    severe regression that was reported by the kernel test robot at the end
    of the merge window.  Now we understood the regression was a false
    positive and was caused by a significant increase in fairness during a
    swap trashing benchmark.  So it's safe to re-apply the fix and continue
    improving the code from there.  The benchmark that reported the
    regression is very useful, but it provides a meaningful result only when
    there is no significant alteration in fairness during the workload.  The
    removal of __GFP_THISNODE increased fairness.
    
    __GFP_THISNODE cannot be used in the generic page faults path for new
    memory allocations under the MPOL_DEFAULT mempolicy, or the allocation
    behavior significantly deviates from what the MPOL_DEFAULT semantics are
    supposed to be for THP and 4k allocations alike.
    
    Setting THP defrag to "always" or using MADV_HUGEPAGE (with THP defrag
    set to "madvise") has never meant to provide an implicit MPOL_BIND on
    the "current" node the task is running on, causing swap storms and
    providing a much more aggressive behavior than even zone_reclaim_node =
    3.
    
    Any workload who could have benefited from __GFP_THISNODE has now to
    enable zone_reclaim_mode=1||2||3.  __GFP_THISNODE implicitly provided
    the zone_reclaim_mode behavior, but it only did so if THP was enabled:
    if THP was disabled, there would have been no chance to get any 4k page
    from the current node if the current node was full of pagecache, which
    further shows how this __GFP_THISNODE was misplaced in MADV_HUGEPAGE.
    MADV_HUGEPAGE has never been intended to provide any zone_reclaim_mode
    semantics, in fact the two are orthogonal, zone_reclaim_mode = 1|2|3
    must work exactly the same with MADV_HUGEPAGE set or not.
    
    The performance characteristic of memory depends on the hardware
    details.  The numbers below are obtained on Naples/EPYC architecture and
    the N/A projection extends them to show what we should aim for in the
    future as a good THP NUMA locality default.  The benchmark used
    exercises random memory seeks (note: the cost of the page faults is not
    part of the measurement).
    
      D0 THP | D0 4k | D1 THP | D1 4k | D2 THP | D2 4k | D3 THP | D3 4k | ...
      0%     | +43%  | +45%   | +106% | +131%  | +224% | N/A    | N/A
    
    D0 means distance zero (i.e.  local memory), D1 means distance one (i.e.
    intra socket memory), D2 means distance two (i.e.  inter socket memory),
    etc...
    
    For the guest physical memory allocated by qemu and for guest mode
    kernel the performance characteristic of RAM is more complex and an
    ideal default could be:
    
      D0 THP | D1 THP | D0 4k | D2 THP | D1 4k | D3 THP | D2 4k | D3 4k | ...
      0%     | +58%   | +101% | N/A    | +222% | N/A    | N/A   | N/A
    
    NOTE: the N/A are projections and haven't been measured yet, the
    measurement in this case is done on a 1950x with only two NUMA nodes.
    The THP case here means THP was used both in the host and in the guest.
    
    After applying this commit the THP NUMA locality order that we'll get
    out of MADV_HUGEPAGE is this:
    
      D0 THP | D1 THP | D2 THP | D3 THP | ... | D0 4k | D1 4k | D2 4k | D3 4k | ...
    
    Before this commit it was:
    
      D0 THP | D0 4k | D1 4k | D2 4k | D3 4k | ...
    
    Even if we ignore the breakage of large workloads that can't fit in a
    single node that the __GFP_THISNODE implicit "current node" mbind
    caused, the THP NUMA locality order provided by __GFP_THISNODE was still
    not the one we shall aim for in the long term (i.e.  the first one at
    the top).
    
    After this commit is applied, we can introduce a new allocator multi
    order API and to replace those two alloc_pages_vmas calls in the page
    fault path, with a single multi order call:
    
            unsigned int order = (1 << HPAGE_PMD_ORDER) | (1 << 0);
            page = alloc_pages_multi_order(..., &order);
            if (!page)
                    goto out;
            if (!(order & (1 << 0))) {
                    VM_WARN_ON(order != 1 << HPAGE_PMD_ORDER);
                    /* THP fault */
            } else {
                    VM_WARN_ON(order != 1 << 0);
                    /* 4k fallback */
            }
    
    The page allocator logic has to be altered so that when it fails on any
    zone with order 9, it has to try again with a order 0 before falling
    back to the next zone in the zonelist.
    
    After that we need to do more measurements and evaluate if adding an
    opt-in feature for guest mode is worth it, to swap "DN 4k | DN+1 THP"
    with "DN+1 THP | DN 4k" at every NUMA distance crossing.
    
    Link: http://lkml.kernel.org/r/20190503223146.2312-3-aarcange@redhat.com
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Cc: Stefan Priebe - Profihost AG <s.priebe@profihost.ag>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5228c62af416..bac395f1d00a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -139,6 +139,8 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 struct mempolicy *get_task_policy(struct task_struct *p);
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 		unsigned long addr);
+struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
+						unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);

commit 2f0799a0ffc033bf3cc82d5032acc3ec633464c2
Author: David Rientjes <rientjes@google.com>
Date:   Wed Dec 5 15:45:54 2018 -0800

    mm, thp: restore node-local hugepage allocations
    
    This is a full revert of ac5b2c18911f ("mm: thp: relax __GFP_THISNODE for
    MADV_HUGEPAGE mappings") and a partial revert of 89c83fb539f9 ("mm, thp:
    consolidate THP gfp handling into alloc_hugepage_direct_gfpmask").
    
    By not setting __GFP_THISNODE, applications can allocate remote hugepages
    when the local node is fragmented or low on memory when either the thp
    defrag setting is "always" or the vma has been madvised with
    MADV_HUGEPAGE.
    
    Remote access to hugepages often has much higher latency than local pages
    of the native page size.  On Haswell, ac5b2c18911f was shown to have a
    13.9% access regression after this commit for binaries that remap their
    text segment to be backed by transparent hugepages.
    
    The intent of ac5b2c18911f is to address an issue where a local node is
    low on memory or fragmented such that a hugepage cannot be allocated.  In
    every scenario where this was described as a fix, there is abundant and
    unfragmented remote memory available to allocate from, even with a greater
    access latency.
    
    If remote memory is also low or fragmented, not setting __GFP_THISNODE was
    also measured on Haswell to have a 40% regression in allocation latency.
    
    Restore __GFP_THISNODE for thp allocations.
    
    Fixes: ac5b2c18911f ("mm: thp: relax __GFP_THISNODE for MADV_HUGEPAGE mappings")
    Fixes: 89c83fb539f9 ("mm, thp: consolidate THP gfp handling into alloc_hugepage_direct_gfpmask")
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index bac395f1d00a..5228c62af416 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -139,8 +139,6 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 struct mempolicy *get_task_policy(struct task_struct *p);
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 		unsigned long addr);
-struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
-						unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);

commit 89c83fb539f95491be80cdd5158e6f0ce329e317
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Nov 2 15:48:31 2018 -0700

    mm, thp: consolidate THP gfp handling into alloc_hugepage_direct_gfpmask
    
    THP allocation mode is quite complex and it depends on the defrag mode.
    This complexity is hidden in alloc_hugepage_direct_gfpmask from a large
    part currently. The NUMA special casing (namely __GFP_THISNODE) is
    however independent and placed in alloc_pages_vma currently. This both
    adds an unnecessary branch to all vma based page allocation requests and
    it makes the code more complex unnecessarily as well. Not to mention
    that e.g. shmem THP used to do the node reclaiming unconditionally
    regardless of the defrag mode until recently. This was not only
    unexpected behavior but it was also hardly a good default behavior and I
    strongly suspect it was just a side effect of the code sharing more than
    a deliberate decision which suggests that such a layering is wrong.
    
    Get rid of the thp special casing from alloc_pages_vma and move the
    logic to alloc_hugepage_direct_gfpmask. __GFP_THISNODE is applied to the
    resulting gfp mask only when the direct reclaim is not requested and
    when there is no explicit numa binding to preserve the current logic.
    
    Please note that there's also a slight difference wrt MPOL_BIND now. The
    previous code would avoid using __GFP_THISNODE if the local node was
    outside of policy_nodemask(). After this patch __GFP_THISNODE is avoided
    for all MPOL_BIND policies. So there's a difference that if local node
    is actually allowed by the bind policy's nodemask, previously
    __GFP_THISNODE would be added, but now it won't be. From the behavior
    POV this is still correct because the policy nodemask is used.
    
    Link: http://lkml.kernel.org/r/20180925120326.24392-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Alex Williamson <alex.williamson@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Stefan Priebe - Profihost AG <s.priebe@profihost.ag>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5228c62af416..bac395f1d00a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -139,6 +139,8 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 struct mempolicy *get_task_policy(struct task_struct *p);
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 		unsigned long addr);
+struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
+						unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 3a58b4be1b0c..5228c62af416 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * NUMA memory policies for Linux.
  * Copyright 2003,2004 Andi Kleen SuSE Labs

commit 213980c0f23b6c4932fd5516da7e8443b2a615ea
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Jul 6 15:40:06 2017 -0700

    mm, mempolicy: simplify rebinding mempolicies when updating cpusets
    
    Commit c0ff7453bb5c ("cpuset,mm: fix no node to alloc memory when
    changing cpuset's mems") has introduced a two-step protocol when
    rebinding task's mempolicy due to cpuset update, in order to avoid a
    parallel allocation seeing an empty effective nodemask and failing.
    
    Later, commit cc9a6c877661 ("cpuset: mm: reduce large amounts of memory
    barrier related damage v3") introduced a seqlock protection and removed
    the synchronization point between the two update steps.  At that point
    (or perhaps later), the two-step rebinding became unnecessary.
    
    Currently it only makes sure that the update first adds new nodes in
    step 1 and then removes nodes in step 2.  Without memory barriers the
    effects are questionable, and even then this cannot prevent a parallel
    zonelist iteration checking the nodemask at each step to observe all
    nodes as unusable for allocation.  We now fully rely on the seqlock to
    prevent premature OOMs and allocation failures.
    
    We can thus remove the two-step update parts and simplify the code.
    
    Link: http://lkml.kernel.org/r/20170517081140.30654-5-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index ecb6cbeede5a..3a58b4be1b0c 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -142,8 +142,7 @@ bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
-extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,
-				enum mpol_rebind_step step);
+extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 
 extern int huge_node(struct vm_area_struct *vma,
@@ -260,8 +259,7 @@ static inline void numa_default_policy(void)
 }
 
 static inline void mpol_rebind_task(struct task_struct *tsk,
-				const nodemask_t *new,
-				enum mpol_rebind_step step)
+				const nodemask_t *new)
 {
 }
 

commit 04ec6264f28793e56114d0a367bb4d3af667ab6a
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Jul 6 15:40:03 2017 -0700

    mm, page_alloc: pass preferred nid instead of zonelist to allocator
    
    The main allocator function __alloc_pages_nodemask() takes a zonelist
    pointer as one of its parameters.  All of its callers directly or
    indirectly obtain the zonelist via node_zonelist() using a preferred
    node id and gfp_mask.  We can make the code a bit simpler by doing the
    zonelist lookup in __alloc_pages_nodemask(), passing it a preferred node
    id instead (gfp_mask is already another parameter).
    
    There are some code size benefits thanks to removal of inlined
    node_zonelist():
    
      bloat-o-meter add/remove: 2/2 grow/shrink: 4/36 up/down: 399/-1351 (-952)
    
    This will also make things simpler if we proceed with converting cpusets
    to zonelists.
    
    Link: http://lkml.kernel.org/r/20170517081140.30654-4-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5f4d8281832b..ecb6cbeede5a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -146,7 +146,7 @@ extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,
 				enum mpol_rebind_step step);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 
-extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
+extern int huge_node(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask);
 extern bool init_nodemask_of_mempolicy(nodemask_t *mask);
@@ -269,13 +269,13 @@ static inline void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 }
 
-static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
+static inline int huge_node(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask)
 {
 	*mpol = NULL;
 	*nodemask = NULL;
-	return node_zonelist(0, gfp_flags);
+	return 0;
 }
 
 static inline bool init_nodemask_of_mempolicy(nodemask_t *m)

commit c1ef8e2c0235bffe4b0505c3325bb8a6af954021
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 12 16:43:12 2016 -0800

    mm: disable numa migration faults for dax vmas
    
    Mark dax vmas as not migratable to exclude them from task_numa_work().
    This is especially relevant for device-dax which wants to ensure
    predictable access latency and not incur periodic faults.
    
    [akpm@linux-foundation.org: add comment]
    Link: http://lkml.kernel.org/r/147892450132.22062.16875659431109209179.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5e5b2969d931..5f4d8281832b 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -7,6 +7,7 @@
 
 
 #include <linux/mmzone.h>
+#include <linux/dax.h>
 #include <linux/slab.h>
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
@@ -177,6 +178,13 @@ static inline bool vma_migratable(struct vm_area_struct *vma)
 	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 		return false;
 
+	/*
+	 * DAX device mappings require predictable access latency, so avoid
+	 * incurring periodic faults.
+	 */
+	if (vma_is_dax(vma))
+		return false;
+
 #ifndef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
 	if (vma->vm_flags & VM_HUGETLB)
 		return false;

commit c11600e4fed67ae4cd6a8096936afd445410e8ed
Author: David Rientjes <rientjes@google.com>
Date:   Thu Sep 1 16:15:07 2016 -0700

    mm, mempolicy: task->mempolicy must be NULL before dropping final reference
    
    KASAN allocates memory from the page allocator as part of
    kmem_cache_free(), and that can reference current->mempolicy through any
    number of allocation functions.  It needs to be NULL'd out before the
    final reference is dropped to prevent a use-after-free bug:
    
            BUG: KASAN: use-after-free in alloc_pages_current+0x363/0x370 at addr ffff88010b48102c
            CPU: 0 PID: 15425 Comm: trinity-c2 Not tainted 4.8.0-rc2+ #140
            ...
            Call Trace:
                    dump_stack
                    kasan_object_err
                    kasan_report_error
                    __asan_report_load2_noabort
                    alloc_pages_current     <-- use after free
                    depot_save_stack
                    save_stack
                    kasan_slab_free
                    kmem_cache_free
                    __mpol_put              <-- free
                    do_exit
    
    This patch sets current->mempolicy to NULL before dropping the final
    reference.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1608301442180.63329@chino.kir.corp.google.com
    Fixes: cd11016e5f52 ("mm, kasan: stackdepot implementation. Enable stackdepot for SLAB")
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: <stable@vger.kernel.org>    [4.6+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 4429d255c8ab..5e5b2969d931 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -195,6 +195,7 @@ static inline bool vma_migratable(struct vm_area_struct *vma)
 }
 
 extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
+extern void mpol_put_task_policy(struct task_struct *);
 
 #else
 
@@ -297,5 +298,8 @@ static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
 	return -1; /* no node preference */
 }
 
+static inline void mpol_put_task_policy(struct task_struct *task)
+{
+}
 #endif /* CONFIG_NUMA */
 #endif

commit 75edd345e8ede51bc8f00672feff5d622f2b3af6
Author: Hugh Dickins <hughd@google.com>
Date:   Thu May 19 17:12:44 2016 -0700

    tmpfs: preliminary minor tidyups
    
    Make a few cleanups in mm/shmem.c, before going on to complicate it.
    
    shmem_alloc_page() will become more complicated: we can't afford to to
    have that complication duplicated between a CONFIG_NUMA version and a
    !CONFIG_NUMA version, so rearrange the #ifdef'ery there to yield a
    single shmem_swapin() and a single shmem_alloc_page().
    
    Yes, it's a shame to inflict the horrid pseudo-vma on non-NUMA
    configurations, but eliminating it is a larger cleanup: I have an
    alloc_pages_mpol() patchset not yet ready - mpol handling is subtle and
    bug-prone, and changed yet again since my last version.
    
    Move __SetPageLocked, __SetPageSwapBacked from shmem_getpage_gfp() to
    shmem_alloc_page(): that SwapBacked flag will be useful in future, to
    help to distinguish different cases appropriately.
    
    And the SGP_DIRTY variant of SGP_CACHE is hard to understand and of
    little use (IIRC it dates back to when shmem_getpage() returned the page
    unlocked): kill it and do the necessary in shmem_file_read_iter().
    
    But an arm64 build then complained that info may be uninitialized (where
    shmem_getpage_gfp() deletes a freshly alloced page beyond eof), and
    advancing to an "sgp <= SGP_CACHE" test jogged it back to reality.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Yang Shi <yang.shi@linaro.org>
    Cc: Ning Qu <quning@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 6978a99e571f..4429d255c8ab 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -228,6 +228,12 @@ static inline void mpol_free_shared_policy(struct shared_policy *p)
 {
 }
 
+static inline struct mempolicy *
+mpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)
+{
+	return NULL;
+}
+
 #define vma_policy(vma) NULL
 
 static inline int

commit 4ee815be1d34a6f254b3d09bdebcb27f294f2bd3
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu May 19 17:11:32 2016 -0700

    mm/mempolicy.c: vma_migratable() can return bool
    
    Make vma_migratable() return bool due to this particular function only
    using either one or zero as its return value.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 2696c1f05ed1..6978a99e571f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -172,14 +172,14 @@ extern int mpol_parse_str(char *str, struct mempolicy **mpol);
 extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 
 /* Check if a vma is migratable */
-static inline int vma_migratable(struct vm_area_struct *vma)
+static inline bool vma_migratable(struct vm_area_struct *vma)
 {
 	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
-		return 0;
+		return false;
 
 #ifndef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
 	if (vma->vm_flags & VM_HUGETLB)
-		return 0;
+		return false;
 #endif
 
 	/*
@@ -190,8 +190,8 @@ static inline int vma_migratable(struct vm_area_struct *vma)
 	if (vma->vm_file &&
 		gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
 								< policy_zone)
-			return 0;
-	return 1;
+			return false;
+	return true;
 }
 
 extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);

commit 4a8c7bb59ac85b038c29adf6d32ff56e11fbb267
Author: Nathan Zimmer <nzimmer@sgi.com>
Date:   Thu Jan 14 15:18:36 2016 -0800

    mm/mempolicy.c: convert the shared_policy lock to a rwlock
    
    When running the SPECint_rate gcc on some very large boxes it was
    noticed that the system was spending lots of time in
    mpol_shared_policy_lookup().  The gamess benchmark can also show it and
    is what I mostly used to chase down the issue since the setup for that I
    found to be easier.
    
    To be clear the binaries were on tmpfs because of disk I/O requirements.
    We then used text replication to avoid icache misses and having all the
    copies banging on the memory where the instruction code resides.  This
    results in us hitting a bottleneck in mpol_shared_policy_lookup() since
    lookup is serialised by the shared_policy lock.
    
    I have only reproduced this on very large (3k+ cores) boxes.  The
    problem starts showing up at just a few hundred ranks getting worse
    until it threatens to livelock once it gets large enough.  For example
    on the gamess benchmark at 128 ranks this area consumes only ~1% of
    time, at 512 ranks it consumes nearly 13%, and at 2k ranks it is over
    90%.
    
    To alleviate the contention in this area I converted the spinlock to an
    rwlock.  This allows a large number of lookups to happen simultaneously.
    The results were quite good reducing this consumtion at max ranks to
    around 2%.
    
    [akpm@linux-foundation.org: tidy up code comments]
    Signed-off-by: Nathan Zimmer <nzimmer@sgi.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Nadia Yvette Chambers <nyc@holomorphy.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 3d385c81c153..2696c1f05ed1 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -122,7 +122,7 @@ struct sp_node {
 
 struct shared_policy {
 	struct rb_root root;
-	spinlock_t lock;
+	rwlock_t lock;
 };
 
 int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst);

commit dd6eecb917938c1b7e505a83df307b3476e7c8bd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Oct 9 15:27:57 2014 -0700

    mempolicy: unexport get_vma_policy() and remove its "task" arg
    
    - get_vma_policy(task) is not safe if task != current, remove this
      argument.
    
    - get_vma_policy() no longer has callers outside of mempolicy.c,
      make it static.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index e1abe249892a..3d385c81c153 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -137,8 +137,6 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 struct mempolicy *get_task_policy(struct task_struct *p);
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 		unsigned long addr);
-struct mempolicy *get_vma_policy(struct task_struct *tsk,
-		struct vm_area_struct *vma, unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);

commit 74d2c3a05cc6c1eef2d7236a9919036ed85ddaaf
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Oct 9 15:27:50 2014 -0700

    mempolicy: introduce __get_vma_policy(), export get_task_policy()
    
    Extract the code which looks for vma's policy from get_vma_policy()
    into the new helper, __get_vma_policy(). Export get_task_policy().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5e4bfcedd2ce..e1abe249892a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -134,6 +134,9 @@ void mpol_free_shared_policy(struct shared_policy *p);
 struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 					    unsigned long idx);
 
+struct mempolicy *get_task_policy(struct task_struct *p);
+struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
+		unsigned long addr);
 struct mempolicy *get_vma_policy(struct task_struct *tsk,
 		struct vm_area_struct *vma, unsigned long addr);
 bool vma_policy_mof(struct vm_area_struct *vma);

commit 6b6482bbf64ef6f6dbc8b52f7a7cf88a0498bd51
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Oct 9 15:27:48 2014 -0700

    mempolicy: remove the "task" arg of vma_policy_mof() and simplify it
    
    1. vma_policy_mof(task) is simply not safe unless task == current,
       it can race with do_exit()->mpol_put(). Remove this arg and update
       its single caller.
    
    2. vma can not be NULL, remove this check and simplify the code.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index f230a978e6ba..5e4bfcedd2ce 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -136,7 +136,7 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 
 struct mempolicy *get_vma_policy(struct task_struct *tsk,
 		struct vm_area_struct *vma, unsigned long addr);
-bool vma_policy_mof(struct task_struct *task, struct vm_area_struct *vma);
+bool vma_policy_mof(struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);

commit c177c81e09e517bbf75b67762cdab1b83aba6976
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Jun 4 16:05:35 2014 -0700

    hugetlb: restrict hugepage_migration_support() to x86_64
    
    Currently hugepage migration is available for all archs which support
    pmd-level hugepage, but testing is done only for x86_64 and there're
    bugs for other archs.  So to avoid breaking such archs, this patch
    limits the availability strictly to x86_64 until developers of other
    archs get interested in enabling this feature.
    
    Simply disabling hugepage migration on non-x86_64 archs is not enough to
    fix the reported problem where sys_move_pages() hits the BUG_ON() in
    follow_page(FOLL_GET), so let's fix this by checking if hugepage
    migration is supported in vma_migratable().
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: <stable@vger.kernel.org>    [3.12+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 3c1b968da0ca..f230a978e6ba 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -175,6 +175,12 @@ static inline int vma_migratable(struct vm_area_struct *vma)
 {
 	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 		return 0;
+
+#ifndef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
+	if (vma->vm_flags & VM_HUGETLB)
+		return 0;
+#endif
+
 	/*
 	 * Migration allocates pages in the highest zone. If we cannot
 	 * do so then migration (at least from node to node) is not

commit f0432d159601f96839f514f286eaa5b75c4112dc
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 7 15:37:30 2014 -0700

    mm, mempolicy: remove per-process flag
    
    PF_MEMPOLICY is an unnecessary optimization for CONFIG_SLAB users.
    There's no significant performance degradation to checking
    current->mempolicy rather than current->flags & PF_MEMPOLICY in the
    allocation path, especially since this is considered unlikely().
    
    Running TCP_RR with netperf-2.4.5 through localhost on 16 cpu machine with
    64GB of memory and without a mempolicy:
    
            threads         before          after
            16              1249409         1244487
            32              1281786         1246783
            48              1239175         1239138
            64              1244642         1241841
            80              1244346         1248918
            96              1266436         1254316
            112             1307398         1312135
            128             1327607         1326502
    
    Per-process flags are a scarce resource so we should free them up whenever
    possible and make them available.  We'll be using it shortly for memcg oom
    reserves.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Tim Hockin <thockin@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index cfe55dfca015..3c1b968da0ca 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -143,7 +143,6 @@ extern void numa_policy_init(void);
 extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,
 				enum mpol_rebind_step step);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
-extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,

commit 2a389610a7331d22344698f23ef2e8c55b2cde7b
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 7 15:37:29 2014 -0700

    mm, mempolicy: rename slab_node for clarity
    
    slab_node() is actually a mempolicy function, so rename it to
    mempolicy_slab_node() to make it clearer that it used for processes with
    mempolicies.
    
    At the same time, cleanup its code by saving numa_mem_id() in a local
    variable (since we require a node with memory, not just any node) and
    remove an obsolete comment that assumes the mempolicy is actually passed
    into the function.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Tim Hockin <thockin@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5f1ea756aace..cfe55dfca015 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -151,7 +151,7 @@ extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 extern bool init_nodemask_of_mempolicy(nodemask_t *mask);
 extern bool mempolicy_nodemask_intersects(struct task_struct *tsk,
 				const nodemask_t *mask);
-extern unsigned slab_node(void);
+extern unsigned int mempolicy_slab_node(void);
 
 extern enum zone_type policy_zone;
 

commit d80be7c75136fb58ed7264ac8a49dd917ace77a1
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jan 21 15:49:10 2014 -0800

    mm, mempolicy: remove unneeded functions for UMA configs
    
    Mempolicies only exist for CONFIG_NUMA configurations.  Therefore, a
    certain class of functions are unneeded in configurations where
    CONFIG_NUMA is disabled such as functions that duplicate existing
    mempolicies, lookup existing policies, set certain mempolicy traits, or
    test mempolicies for certain attributes.
    
    Remove the unneeded functions so that any future callers get a compile-
    time error and protect their code with CONFIG_NUMA as required.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 9fe426b30a41..5f1ea756aace 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -211,20 +211,8 @@ static inline void mpol_get(struct mempolicy *pol)
 {
 }
 
-static inline struct mempolicy *mpol_dup(struct mempolicy *old)
-{
-	return NULL;
-}
-
 struct shared_policy {};
 
-static inline int mpol_set_shared_policy(struct shared_policy *info,
-					struct vm_area_struct *vma,
-					struct mempolicy *new)
-{
-	return -EINVAL;
-}
-
 static inline void mpol_shared_policy_init(struct shared_policy *sp,
 						struct mempolicy *mpol)
 {
@@ -234,12 +222,6 @@ static inline void mpol_free_shared_policy(struct shared_policy *p)
 {
 }
 
-static inline struct mempolicy *
-mpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)
-{
-	return NULL;
-}
-
 #define vma_policy(vma) NULL
 
 static inline int
@@ -266,10 +248,6 @@ static inline void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 }
 
-static inline void mpol_fix_fork_child_flag(struct task_struct *p)
-{
-}
-
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask)
@@ -284,12 +262,6 @@ static inline bool init_nodemask_of_mempolicy(nodemask_t *m)
 	return false;
 }
 
-static inline bool mempolicy_nodemask_intersects(struct task_struct *tsk,
-			const nodemask_t *mask)
-{
-	return false;
-}
-
 static inline int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
 				   const nodemask_t *to, int flags)
 {
@@ -307,10 +279,6 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 }
 #endif
 
-static inline void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
-{
-}
-
 static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
 				 unsigned long address)
 {

commit 948927ee9e4f35f287e61a79c9f0e85ca2202c7d
Author: David Rientjes <rientjes@google.com>
Date:   Tue Nov 12 15:07:28 2013 -0800

    mm, mempolicy: make mpol_to_str robust and always succeed
    
    mpol_to_str() should not fail.  Currently, it either fails because the
    string buffer is too small or because a string hasn't been defined for a
    mempolicy mode.
    
    If a new mempolicy mode is introduced and no string is defined for it,
    just warn and return "unknown".
    
    If the buffer is too small, just truncate the string and return, the
    same behavior as snprintf().
    
    This also fixes a bug where there was no NULL-byte termination when doing
    *p++ = '=' and *p++ ':' and maxlen has been reached.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Chen Gang <gang.chen@asianux.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index ea4d2495c646..9fe426b30a41 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -169,7 +169,7 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
 extern int mpol_parse_str(char *str, struct mempolicy **mpol);
 #endif
 
-extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
+extern void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
@@ -307,9 +307,8 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 }
 #endif
 
-static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
+static inline void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
 {
-	return 0;
 }
 
 static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,

commit fc3147245d193bd0f57307859c698fa28a20b0fe
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:09 2013 +0100

    mm: numa: Limit NUMA scanning to migrate-on-fault VMAs
    
    There is a 90% regression observed with a large Oracle performance test
    on a 4 node system. Profiles indicated that the overhead was due to
    contention on sp_lock when looking up shared memory policies. These
    policies do not have the appropriate flags to allow them to be
    automatically balanced so trapping faults on them is pointless. This
    patch skips VMAs that do not have MPOL_F_MOF set.
    
    [riel@redhat.com: Initial patch]
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-and-tested-by: Joe Mario <jmario@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-32-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index da6716b9e3fe..ea4d2495c646 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -136,6 +136,7 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 
 struct mempolicy *get_vma_policy(struct task_struct *tsk,
 		struct vm_area_struct *vma, unsigned long addr);
+bool vma_policy_mof(struct task_struct *task, struct vm_area_struct *vma);
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);

commit 71ea2efb1e936a127690a0a540b3a6162f95e48a
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:08 2013 -0700

    mm: migrate: remove VM_HUGETLB from vma flag check in vma_migratable()
    
    Enable hugepage migration from migrate_pages(2), move_pages(2), and
    mbind(2).
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index b2f897789838..da6716b9e3fe 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -173,7 +173,7 @@ extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
 {
-	if (vma->vm_flags & (VM_IO | VM_HUGETLB | VM_PFNMAP))
+	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 		return 0;
 	/*
 	 * Migration allocates pages in the highest zone. If we cannot

commit ef0855d334e1e4af7c3e0c42146a8479ea14a5ab
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:20:14 2013 -0700

    mm: mempolicy: turn vma_set_policy() into vma_dup_policy()
    
    Simple cleanup.  Every user of vma_set_policy() does the same work, this
    looks a bit annoying imho.  And the new trivial helper which does
    mpol_dup() + vma_set_policy() to simplify the callers.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 0d7df39a5885..b2f897789838 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -91,7 +91,6 @@ static inline struct mempolicy *mpol_dup(struct mempolicy *pol)
 }
 
 #define vma_policy(vma) ((vma)->vm_policy)
-#define vma_set_policy(vma, pol) ((vma)->vm_policy = (pol))
 
 static inline void mpol_get(struct mempolicy *pol)
 {
@@ -126,6 +125,7 @@ struct shared_policy {
 	spinlock_t lock;
 };
 
+int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst);
 void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
@@ -240,7 +240,12 @@ mpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)
 }
 
 #define vma_policy(vma) NULL
-#define vma_set_policy(vma, pol) do {} while(0)
+
+static inline int
+vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)
+{
+	return 0;
+}
 
 static inline void numa_policy_init(void)
 {

commit 42288fe366c4f1ce7522bc9f27d0bc2a81c55264
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Dec 21 23:10:25 2012 +0000

    mm: mempolicy: Convert shared_policy mutex to spinlock
    
    Sasha was fuzzing with trinity and reported the following problem:
    
      BUG: sleeping function called from invalid context at kernel/mutex.c:269
      in_atomic(): 1, irqs_disabled(): 0, pid: 6361, name: trinity-main
      2 locks held by trinity-main/6361:
       #0:  (&mm->mmap_sem){++++++}, at: [<ffffffff810aa314>] __do_page_fault+0x1e4/0x4f0
       #1:  (&(&mm->page_table_lock)->rlock){+.+...}, at: [<ffffffff8122f017>] handle_pte_fault+0x3f7/0x6a0
      Pid: 6361, comm: trinity-main Tainted: G        W
      3.7.0-rc2-next-20121024-sasha-00001-gd95ef01-dirty #74
      Call Trace:
        __might_sleep+0x1c3/0x1e0
        mutex_lock_nested+0x29/0x50
        mpol_shared_policy_lookup+0x2e/0x90
        shmem_get_policy+0x2e/0x30
        get_vma_policy+0x5a/0xa0
        mpol_misplaced+0x41/0x1d0
        handle_pte_fault+0x465/0x6a0
    
    This was triggered by a different version of automatic NUMA balancing
    but in theory the current version is vunerable to the same problem.
    
    do_numa_page
      -> numa_migrate_prep
        -> mpol_misplaced
          -> get_vma_policy
            -> shmem_get_policy
    
    It's very unlikely this will happen as shared pages are not marked
    pte_numa -- see the page_mapcount() check in change_pte_range() -- but
    it is possible.
    
    To address this, this patch restores sp->lock as originally implemented
    by Kosaki Motohiro.  In the path where get_vma_policy() is called, it
    should not be calling sp_alloc() so it is not necessary to treat the PTL
    specially.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Tested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 92bc9988a180..0d7df39a5885 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -123,7 +123,7 @@ struct sp_node {
 
 struct shared_policy {
 	struct rb_root root;
-	struct mutex mutex;
+	spinlock_t lock;
 };
 
 void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol);

commit a7a88b23737095e6c18a20c5d4eef9e25ec5b829
Author: Hugh Dickins <hughd@google.com>
Date:   Wed Jan 2 02:04:23 2013 -0800

    mempolicy: remove arg from mpol_parse_str, mpol_to_str
    
    Remove the unused argument (formerly no_context) from mpol_parse_str()
    and from mpol_to_str().
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 9adc270de7ef..92bc9988a180 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -165,11 +165,10 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
 
 
 #ifdef CONFIG_TMPFS
-extern int mpol_parse_str(char *str, struct mempolicy **mpol, int no_context);
+extern int mpol_parse_str(char *str, struct mempolicy **mpol);
 #endif
 
-extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
-			int no_context);
+extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
 
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
@@ -296,15 +295,13 @@ static inline void check_highest_zone(int k)
 }
 
 #ifdef CONFIG_TMPFS
-static inline int mpol_parse_str(char *str, struct mempolicy **mpol,
-				int no_context)
+static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 {
 	return 1;	/* error */
 }
 #endif
 
-static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
-				int no_context)
+static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
 {
 	return 0;
 }

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 771fb4d806a92bf6c988fcfbd286ae40a9374332
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Thu Oct 25 14:16:30 2012 +0200

    mm: mempolicy: Check for misplaced page
    
    This patch provides a new function to test whether a page resides
    on a node that is appropriate for the mempolicy for the vma and
    address where the page is supposed to be mapped.  This involves
    looking up the node where the page belongs.  So, the function
    returns that node so that it may be used to allocated the page
    without consulting the policy again.
    
    A subsequent patch will call this function from the fault path.
    Because of this, I don't want to go ahead and allocate the page, e.g.,
    via alloc_page_vma() only to have to free it if it has the correct
    policy.  So, I just mimic the alloc_page_vma() node computation
    logic--sort of.
    
    Note:  we could use this function to implement a MPOL_MF_STRICT
    behavior when migrating pages to match mbind() mempolicy--e.g.,
    to ensure that pages in an interleaved range are reinterleaved
    rather than left where they are when they reside on any page in
    the interleave nodemask.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    [ Added MPOL_F_LAZY to trigger migrate-on-fault;
      simplified code now that we don't have to bother
      with special crap for interleaved ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index e5ccb9ddd90e..c511e2523560 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -198,6 +198,8 @@ static inline int vma_migratable(struct vm_area_struct *vma)
 	return 1;
 }
 
+extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
+
 #else
 
 struct mempolicy {};
@@ -323,5 +325,11 @@ static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 	return 0;
 }
 
+static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
+				 unsigned long address)
+{
+	return -1; /* no node preference */
+}
+
 #endif /* CONFIG_NUMA */
 #endif

commit 18a2f371f5edf41810f6469cb9be39931ef9deb9
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Dec 5 14:01:41 2012 -0800

    tmpfs: fix shared mempolicy leak
    
    This fixes a regression in 3.7-rc, which has since gone into stable.
    
    Commit 00442ad04a5e ("mempolicy: fix a memory corruption by refcount
    imbalance in alloc_pages_vma()") changed get_vma_policy() to raise the
    refcount on a shmem shared mempolicy; whereas shmem_alloc_page() went
    on expecting alloc_page_vma() to drop the refcount it had acquired.
    This deserves a rework: but for now fix the leak in shmem_alloc_page().
    
    Hugh: shmem_swapin() did not need a fix, but surely it's clearer to use
    the same refcounting there as in shmem_alloc_page(), delete its onstack
    mempolicy, and the strange mpol_cond_copy() and __mpol_cond_copy() -
    those were invented to let swapin_readahead() make an unknown number of
    calls to alloc_pages_vma() with one mempolicy; but since 00442ad04a5e,
    alloc_pages_vma() has kept refcount in balance, so now no problem.
    
    Reported-and-tested-by: Tommi Rantala <tt.rantala@gmail.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index e5ccb9ddd90e..dbd212723b74 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -82,16 +82,6 @@ static inline void mpol_cond_put(struct mempolicy *pol)
 		__mpol_put(pol);
 }
 
-extern struct mempolicy *__mpol_cond_copy(struct mempolicy *tompol,
-					  struct mempolicy *frompol);
-static inline struct mempolicy *mpol_cond_copy(struct mempolicy *tompol,
-						struct mempolicy *frompol)
-{
-	if (!frompol)
-		return frompol;
-	return __mpol_cond_copy(tompol, frompol);
-}
-
 extern struct mempolicy *__mpol_dup(struct mempolicy *pol);
 static inline struct mempolicy *mpol_dup(struct mempolicy *pol)
 {
@@ -215,12 +205,6 @@ static inline void mpol_cond_put(struct mempolicy *pol)
 {
 }
 
-static inline struct mempolicy *mpol_cond_copy(struct mempolicy *to,
-						struct mempolicy *from)
-{
-	return from;
-}
-
 static inline void mpol_get(struct mempolicy *pol)
 {
 }

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index cec569325608..e5ccb9ddd90e 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -1,66 +1,10 @@
-#ifndef _LINUX_MEMPOLICY_H
-#define _LINUX_MEMPOLICY_H 1
-
-#include <linux/errno.h>
-
 /*
  * NUMA memory policies for Linux.
  * Copyright 2003,2004 Andi Kleen SuSE Labs
  */
+#ifndef _LINUX_MEMPOLICY_H
+#define _LINUX_MEMPOLICY_H 1
 
-/*
- * Both the MPOL_* mempolicy mode and the MPOL_F_* optional mode flags are
- * passed by the user to either set_mempolicy() or mbind() in an 'int' actual.
- * The MPOL_MODE_FLAGS macro determines the legal set of optional mode flags.
- */
-
-/* Policies */
-enum {
-	MPOL_DEFAULT,
-	MPOL_PREFERRED,
-	MPOL_BIND,
-	MPOL_INTERLEAVE,
-	MPOL_MAX,	/* always last member of enum */
-};
-
-enum mpol_rebind_step {
-	MPOL_REBIND_ONCE,	/* do rebind work at once(not by two step) */
-	MPOL_REBIND_STEP1,	/* first step(set all the newly nodes) */
-	MPOL_REBIND_STEP2,	/* second step(clean all the disallowed nodes)*/
-	MPOL_REBIND_NSTEP,
-};
-
-/* Flags for set_mempolicy */
-#define MPOL_F_STATIC_NODES	(1 << 15)
-#define MPOL_F_RELATIVE_NODES	(1 << 14)
-
-/*
- * MPOL_MODE_FLAGS is the union of all possible optional mode flags passed to
- * either set_mempolicy() or mbind().
- */
-#define MPOL_MODE_FLAGS	(MPOL_F_STATIC_NODES | MPOL_F_RELATIVE_NODES)
-
-/* Flags for get_mempolicy */
-#define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
-#define MPOL_F_ADDR	(1<<1)	/* look up vma using address */
-#define MPOL_F_MEMS_ALLOWED (1<<2) /* return allowed memories */
-
-/* Flags for mbind */
-#define MPOL_MF_STRICT	(1<<0)	/* Verify existing pages in the mapping */
-#define MPOL_MF_MOVE	(1<<1)	/* Move pages owned by this process to conform to mapping */
-#define MPOL_MF_MOVE_ALL (1<<2)	/* Move every page to conform to mapping */
-#define MPOL_MF_INTERNAL (1<<3)	/* Internal flags start here */
-
-/*
- * Internal flags that share the struct mempolicy flags word with
- * "mode flags".  These flags are allocated from bit 0 up, as they
- * are never OR'ed into the mode in mempolicy API arguments.
- */
-#define MPOL_F_SHARED  (1 << 0)	/* identify shared policies */
-#define MPOL_F_LOCAL   (1 << 1)	/* preferred local allocation */
-#define MPOL_F_REBINDING (1 << 2)	/* identify policies in rebinding */
-
-#ifdef __KERNEL__
 
 #include <linux/mmzone.h>
 #include <linux/slab.h>
@@ -68,6 +12,7 @@ enum mpol_rebind_step {
 #include <linux/spinlock.h>
 #include <linux/nodemask.h>
 #include <linux/pagemap.h>
+#include <uapi/linux/mempolicy.h>
 
 struct mm_struct;
 
@@ -379,6 +324,4 @@ static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 }
 
 #endif /* CONFIG_NUMA */
-#endif /* __KERNEL__ */
-
 #endif

commit b22d127a39ddd10d93deee3d96e643657ad53a49
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:29:17 2012 -0700

    mempolicy: fix a race in shared_policy_replace()
    
    shared_policy_replace() use of sp_alloc() is unsafe.  1) sp_node cannot
    be dereferenced if sp->lock is not held and 2) another thread can modify
    sp_node between spin_unlock for allocating a new sp node and next
    spin_lock.  The bug was introduced before 2.6.12-rc2.
    
    Kosaki's original patch for this problem was to allocate an sp node and
    policy within shared_policy_replace and initialise it when the lock is
    reacquired.  I was not keen on this approach because it partially
    duplicates sp_alloc().  As the paths were sp->lock is taken are not that
    performance critical this patch converts sp->lock to sp->mutex so it can
    sleep when calling sp_alloc().
    
    [kosaki.motohiro@jp.fujitsu.com: Original patch]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Cc: Josh Boyer <jwboyer@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index ba7a0ff19d39..cec569325608 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -188,7 +188,7 @@ struct sp_node {
 
 struct shared_policy {
 	struct rb_root root;
-	spinlock_t lock;
+	struct mutex mutex;
 };
 
 void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol);

commit 314e51b9851b4f4e8ab302243ff5a6fc6147f379
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Mon Oct 8 16:29:02 2012 -0700

    mm: kill vma flag VM_RESERVED and mm->reserved_vm counter
    
    A long time ago, in v2.4, VM_RESERVED kept swapout process off VMA,
    currently it lost original meaning but still has some effects:
    
     | effect                 | alternative flags
    -+------------------------+---------------------------------------------
    1| account as reserved_vm | VM_IO
    2| skip in core dump      | VM_IO, VM_DONTDUMP
    3| do not merge or expand | VM_IO, VM_DONTEXPAND, VM_HUGETLB, VM_PFNMAP
    4| do not mlock           | VM_IO, VM_DONTEXPAND, VM_HUGETLB, VM_PFNMAP
    
    This patch removes reserved_vm counter from mm_struct.  Seems like nobody
    cares about it, it does not exported into userspace directly, it only
    reduces total_vm showed in proc.
    
    Thus VM_RESERVED can be replaced with VM_IO or pair VM_DONTEXPAND | VM_DONTDUMP.
    
    remap_pfn_range() and io_remap_pfn_range() set VM_IO|VM_DONTEXPAND|VM_DONTDUMP.
    remap_vmalloc_range() set VM_DONTEXPAND | VM_DONTDUMP.
    
    [akpm@linux-foundation.org: drivers/vfio/pci/vfio_pci.c fixup]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Carsten Otte <cotte@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Kentaro Takeda <takedakn@nttdata.co.jp>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 95b738c7abff..ba7a0ff19d39 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -239,7 +239,7 @@ extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
 {
-	if (vma->vm_flags & (VM_IO|VM_HUGETLB|VM_PFNMAP|VM_RESERVED))
+	if (vma->vm_flags & (VM_IO | VM_HUGETLB | VM_PFNMAP))
 		return 0;
 	/*
 	 * Migration allocates pages in the highest zone. If we cannot

commit e7b691b085fda913830e5280ae6f724b2a63c824
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sat Jun 9 02:40:03 2012 -0700

    slab/mempolicy: always use local policy from interrupt context
    
    slab_node() could access current->mempolicy from interrupt context.
    However there's a race condition during exit where the mempolicy
    is first freed and then the pointer zeroed.
    
    Using this from interrupts seems bogus anyways. The interrupt
    will interrupt a random process and therefore get a random
    mempolicy. Many times, this will be idle's, which noone can change.
    
    Just disable this here and always use local for slab
    from interrupts. I also cleaned up the callers of slab_node a bit
    which always passed the same argument.
    
    I believe the original mempolicy code did that in fact,
    so it's likely a regression.
    
    v2: send version with correct logic
    v3: simplify. fix typo.
    Reported-by: Arun Sharma <asharma@fb.com>
    Cc: penberg@kernel.org
    Cc: cl@linux.com
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    [tdmackey@twitter.com: Rework control flow based on feedback from
    cl@linux.com, fix logic, and cleanup current task_struct reference]
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: David Mackey <tdmackey@twitter.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 4aa42732e47f..95b738c7abff 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -215,7 +215,7 @@ extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 extern bool init_nodemask_of_mempolicy(nodemask_t *mask);
 extern bool mempolicy_nodemask_intersects(struct task_struct *tsk,
 				const nodemask_t *mask);
-extern unsigned slab_node(struct mempolicy *policy);
+extern unsigned slab_node(void);
 
 extern enum zone_type policy_zone;
 

commit 0ce72d4f7333248efbef1f3309770c7edb1b2625
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue May 29 15:06:24 2012 -0700

    mm: do_migrate_pages(): rename arguments
    
    s/from_nodes/from and s/to_nodes/to/.  The "_nodes" is redundant - it
    duplicates the argument's type.
    
    Done in a fit of irritation over 80-col issues :(
    
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <mkosaki@redhat.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 7c727a90d70d..4aa42732e47f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -225,8 +225,8 @@ static inline void check_highest_zone(enum zone_type k)
 		policy_zone = k;
 }
 
-int do_migrate_pages(struct mm_struct *mm,
-	const nodemask_t *from_nodes, const nodemask_t *to_nodes, int flags);
+int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
+		     const nodemask_t *to, int flags);
 
 
 #ifdef CONFIG_TMPFS
@@ -354,9 +354,8 @@ static inline bool mempolicy_nodemask_intersects(struct task_struct *tsk,
 	return false;
 }
 
-static inline int do_migrate_pages(struct mm_struct *mm,
-			const nodemask_t *from_nodes,
-			const nodemask_t *to_nodes, int flags)
+static inline int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
+				   const nodemask_t *to, int flags)
 {
 	return 0;
 }

commit fcfb4dcc9698f932836aa63ba0d82e7dbd300fb3
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jan 10 15:08:21 2012 -0800

    mm/mempolicy.c: mpol_equal(): use bool
    
    mpol_equal() logically returns a boolean.  Use a bool type to slightly
    improve readability.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Stephen Wilson <wilsons@start.ca>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 7978eec1b7d9..7c727a90d70d 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -164,11 +164,11 @@ static inline void mpol_get(struct mempolicy *pol)
 		atomic_inc(&pol->refcnt);
 }
 
-extern int __mpol_equal(struct mempolicy *a, struct mempolicy *b);
-static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
+extern bool __mpol_equal(struct mempolicy *a, struct mempolicy *b);
+static inline bool mpol_equal(struct mempolicy *a, struct mempolicy *b)
 {
 	if (a == b)
-		return 1;
+		return true;
 	return __mpol_equal(a, b);
 }
 
@@ -257,9 +257,9 @@ static inline int vma_migratable(struct vm_area_struct *vma)
 
 struct mempolicy {};
 
-static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
+static inline bool mpol_equal(struct mempolicy *a, struct mempolicy *b)
 {
-	return 1;
+	return true;
 }
 
 static inline void mpol_put(struct mempolicy *p)

commit 13057efb0a0063eb8042d99093ec88a52c4f1593
Author: Stephen Wilson <wilsons@start.ca>
Date:   Tue May 24 17:12:46 2011 -0700

    mm: declare mpol_to_str() when CONFIG_TMPFS=n
    
    When CONFIG_TMPFS=n mpol_to_str() is not declared in mempolicy.h.
    However, in the NUMA case, the definition is always compiled.
    
    Since it is not strictly true that tmpfs is the only client, and since the
    symbol was always lurking around anyways, export mpol_to_str()
    unconditionally.  Furthermore, this will allow us to move show_numa_map()
    out of mempolicy.c and into the procfs subsystem.
    
    Signed-off-by: Stephen Wilson <wilsons@start.ca>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index c2f603218fbc..7978eec1b7d9 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -231,10 +231,10 @@ int do_migrate_pages(struct mm_struct *mm,
 
 #ifdef CONFIG_TMPFS
 extern int mpol_parse_str(char *str, struct mempolicy **mpol, int no_context);
+#endif
 
 extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 			int no_context);
-#endif
 
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
@@ -371,13 +371,13 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol,
 {
 	return 1;	/* error */
 }
+#endif
 
 static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 				int no_context)
 {
 	return 0;
 }
-#endif
 
 #endif /* CONFIG_NUMA */
 #endif /* __KERNEL__ */

commit d98f6cb67fb5b9376d4957d7ba9f32eac35c2e08
Author: Stephen Wilson <wilsons@start.ca>
Date:   Tue May 24 17:12:41 2011 -0700

    mm: export get_vma_policy()
    
    In commit 48fce3429d ("mempolicies: unexport get_vma_policy()")
    get_vma_policy() was marked static as all clients were local to
    mempolicy.c.
    
    However, the decision to generate /proc/pid/numa_maps in the numa memory
    policy code and outside the procfs subsystem introduces an artificial
    interdependency between the two systems.  Exporting get_vma_policy() once
    again is the first step to clean up this interdependency.
    
    Signed-off-by: Stephen Wilson <wilsons@start.ca>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 31ac26ca4acf..c2f603218fbc 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -199,6 +199,9 @@ void mpol_free_shared_policy(struct shared_policy *p);
 struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 					    unsigned long idx);
 
+struct mempolicy *get_vma_policy(struct task_struct *tsk,
+		struct vm_area_struct *vma, unsigned long addr);
+
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
 extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,

commit 6f48d0ebd907ae419387f27b602ee98870cfa7bb
Author: David Rientjes <rientjes@google.com>
Date:   Mon Aug 9 17:18:52 2010 -0700

    oom: select task from tasklist for mempolicy ooms
    
    The oom killer presently kills current whenever there is no more memory
    free or reclaimable on its mempolicy's nodes.  There is no guarantee that
    current is a memory-hogging task or that killing it will free any
    substantial amount of memory, however.
    
    In such situations, it is better to scan the tasklist for nodes that are
    allowed to allocate on current's set of nodes and kill the task with the
    highest badness() score.  This ensures that the most memory-hogging task,
    or the one configured by the user with /proc/pid/oom_adj, is always
    selected in such scenarios.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 7b9ef6bf45aa..31ac26ca4acf 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -210,6 +210,8 @@ extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask);
 extern bool init_nodemask_of_mempolicy(nodemask_t *mask);
+extern bool mempolicy_nodemask_intersects(struct task_struct *tsk,
+				const nodemask_t *mask);
 extern unsigned slab_node(struct mempolicy *policy);
 
 extern enum zone_type policy_zone;
@@ -338,7 +340,16 @@ static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 	return node_zonelist(0, gfp_flags);
 }
 
-static inline bool init_nodemask_of_mempolicy(nodemask_t *m) { return false; }
+static inline bool init_nodemask_of_mempolicy(nodemask_t *m)
+{
+	return false;
+}
+
+static inline bool mempolicy_nodemask_intersects(struct task_struct *tsk,
+			const nodemask_t *mask)
+{
+	return false;
+}
 
 static inline int do_migrate_pages(struct mm_struct *mm,
 			const nodemask_t *from_nodes,

commit 708c1bbc9d0c3e57f40501794d9b0eed29d10fce
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon May 24 14:32:07 2010 -0700

    mempolicy: restructure rebinding-mempolicy functions
    
    Nick Piggin reported that the allocator may see an empty nodemask when
    changing cpuset's mems[1].  It happens only on the kernel that do not do
    atomic nodemask_t stores.  (MAX_NUMNODES > BITS_PER_LONG)
    
    But I found that there is also a problem on the kernel that can do atomic
    nodemask_t stores.  The problem is that the allocator can't find a node to
    alloc page when changing cpuset's mems though there is a lot of free
    memory.  The reason is like this:
    
    (mpol: mempolicy)
            task1                   task1's mpol    task2
            alloc page              1
              alloc on node0? NO    1
                                    1               change mems from 1 to 0
                                    1               rebind task1's mpol
                                    0-1               set new bits
                                    0                 clear disallowed bits
              alloc on node1? NO    0
              ...
            can't alloc page
              goto oom
    
    I can use the attached program reproduce it by the following step:
    
    # mkdir /dev/cpuset
    # mount -t cpuset cpuset /dev/cpuset
    # mkdir /dev/cpuset/1
    # echo `cat /dev/cpuset/cpus` > /dev/cpuset/1/cpus
    # echo `cat /dev/cpuset/mems` > /dev/cpuset/1/mems
    # echo $$ > /dev/cpuset/1/tasks
    # numactl --membind=`cat /dev/cpuset/mems` ./cpuset_mem_hog <nr_tasks> &
       <nr_tasks> = max(nr_cpus - 1, 1)
    # killall -s SIGUSR1 cpuset_mem_hog
    # ./change_mems.sh
    
    several hours later, oom will happen though there is a lot of free memory.
    
    This patchset fixes this problem by expanding the nodes range first(set
    newly allowed bits) and shrink it lazily(clear newly disallowed bits).  So
    we use a variable to tell the write-side task that read-side task is
    reading nodemask, and the write-side task clears newly disallowed nodes
    after read-side task ends the current memory allocation.
    
    This patch:
    
    In order to fix no node to alloc memory, when we want to update mempolicy
    and mems_allowed, we expand the set of nodes first (set all the newly
    nodes) and shrink the set of nodes lazily(clean disallowed nodes), But the
    mempolicy's rebind functions may breaks the expanding.
    
    So we restructure the mempolicy's rebind functions and split the rebind
    work to two steps, just like the update of cpuset's mems: The 1st step:
    expand the set of the mempolicy's nodes.  The 2nd step: shrink the set of
    the mempolicy's nodes.  It is used when there is no real lock to protect
    the mempolicy in the read-side.  Otherwise we can do rebind work at once.
    
    In order to implement it, we define
    
            enum mpol_rebind_step {
                    MPOL_REBIND_ONCE,
                    MPOL_REBIND_STEP1,
                    MPOL_REBIND_STEP2,
                    MPOL_REBIND_NSTEP,
            };
    
    If the mempolicy needn't be updated by two steps, we can pass
    MPOL_REBIND_ONCE to the rebind functions.  Or we can pass
    MPOL_REBIND_STEP1 to do the first step of the rebind work and pass
    MPOL_REBIND_STEP2 to do the second step work.
    
    Besides that, it maybe long time between these two step and we have to
    release the lock that protects mempolicy and mems_allowed.  If we hold the
    lock once again, we must check whether the current mempolicy is under the
    rebinding (the first step has been done) or not, because the task may
    alloc a new mempolicy when we don't hold the lock.  So we defined the
    following flag to identify it:
    
    #define MPOL_F_REBINDING (1 << 2)
    
    The new functions will be used in the next patch.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Paul Menage <menage@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 1cc966cd3e5f..7b9ef6bf45aa 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -23,6 +23,13 @@ enum {
 	MPOL_MAX,	/* always last member of enum */
 };
 
+enum mpol_rebind_step {
+	MPOL_REBIND_ONCE,	/* do rebind work at once(not by two step) */
+	MPOL_REBIND_STEP1,	/* first step(set all the newly nodes) */
+	MPOL_REBIND_STEP2,	/* second step(clean all the disallowed nodes)*/
+	MPOL_REBIND_NSTEP,
+};
+
 /* Flags for set_mempolicy */
 #define MPOL_F_STATIC_NODES	(1 << 15)
 #define MPOL_F_RELATIVE_NODES	(1 << 14)
@@ -51,6 +58,7 @@ enum {
  */
 #define MPOL_F_SHARED  (1 << 0)	/* identify shared policies */
 #define MPOL_F_LOCAL   (1 << 1)	/* preferred local allocation */
+#define MPOL_F_REBINDING (1 << 2)	/* identify policies in rebinding */
 
 #ifdef __KERNEL__
 
@@ -193,8 +201,8 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
-extern void mpol_rebind_task(struct task_struct *tsk,
-					const nodemask_t *new);
+extern void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,
+				enum mpol_rebind_step step);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
@@ -308,7 +316,8 @@ static inline void numa_default_policy(void)
 }
 
 static inline void mpol_rebind_task(struct task_struct *tsk,
-					const nodemask_t *new)
+				const nodemask_t *new,
+				enum mpol_rebind_step step)
 {
 }
 

commit 06808b0827e1cd14eedc96bac2655d5b37ac246c
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Dec 14 17:58:21 2009 -0800

    hugetlb: derive huge pages nodes allowed from task mempolicy
    
    This patch derives a "nodes_allowed" node mask from the numa mempolicy of
    the task modifying the number of persistent huge pages to control the
    allocation, freeing and adjusting of surplus huge pages when the pool page
    count is modified via the new sysctl or sysfs attribute
    "nr_hugepages_mempolicy".  The nodes_allowed mask is derived as follows:
    
    * For "default" [NULL] task mempolicy, a NULL nodemask_t pointer
      is produced.  This will cause the hugetlb subsystem to use
      node_online_map as the "nodes_allowed".  This preserves the
      behavior before this patch.
    * For "preferred" mempolicy, including explicit local allocation,
      a nodemask with the single preferred node will be produced.
      "local" policy will NOT track any internode migrations of the
      task adjusting nr_hugepages.
    * For "bind" and "interleave" policy, the mempolicy's nodemask
      will be used.
    * Other than to inform the construction of the nodes_allowed node
      mask, the actual mempolicy mode is ignored.  That is, all modes
      behave like interleave over the resulting nodes_allowed mask
      with no "fallback".
    
    See the updated documentation [next patch] for more information
    about the implications of this patch.
    
    Examples:
    
    Starting with:
    
            Node 0 HugePages_Total:     0
            Node 1 HugePages_Total:     0
            Node 2 HugePages_Total:     0
            Node 3 HugePages_Total:     0
    
    Default behavior [with or without this patch] balances persistent
    hugepage allocation across nodes [with sufficient contiguous memory]:
    
            sysctl vm.nr_hugepages[_mempolicy]=32
    
    yields:
    
            Node 0 HugePages_Total:     8
            Node 1 HugePages_Total:     8
            Node 2 HugePages_Total:     8
            Node 3 HugePages_Total:     8
    
    Of course, we only have nr_hugepages_mempolicy with the patch,
    but with default mempolicy, nr_hugepages_mempolicy behaves the
    same as nr_hugepages.
    
    Applying mempolicy--e.g., with numactl [using '-m' a.k.a.
    '--membind' because it allows multiple nodes to be specified
    and it's easy to type]--we can allocate huge pages on
    individual nodes or sets of nodes.  So, starting from the
    condition above, with 8 huge pages per node, add 8 more to
    node 2 using:
    
            numactl -m 2 sysctl vm.nr_hugepages_mempolicy=40
    
    This yields:
    
            Node 0 HugePages_Total:     8
            Node 1 HugePages_Total:     8
            Node 2 HugePages_Total:    16
            Node 3 HugePages_Total:     8
    
    The incremental 8 huge pages were restricted to node 2 by the
    specified mempolicy.
    
    Similarly, we can use mempolicy to free persistent huge pages
    from specified nodes:
    
            numactl -m 0,1 sysctl vm.nr_hugepages_mempolicy=32
    
    yields:
    
            Node 0 HugePages_Total:     4
            Node 1 HugePages_Total:     4
            Node 2 HugePages_Total:    16
            Node 3 HugePages_Total:     8
    
    The 8 huge pages freed were balanced over nodes 0 and 1.
    
    [rientjes@google.com: accomodate reworked NODEMASK_ALLOC]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Andi Kleen <andi@firstfloor.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@canonical.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 085c903fe0f1..1cc966cd3e5f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -201,6 +201,7 @@ extern void mpol_fix_fork_child_flag(struct task_struct *p);
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask);
+extern bool init_nodemask_of_mempolicy(nodemask_t *mask);
 extern unsigned slab_node(struct mempolicy *policy);
 
 extern enum zone_type policy_zone;
@@ -328,6 +329,8 @@ static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 	return node_zonelist(0, gfp_flags);
 }
 
+static inline bool init_nodemask_of_mempolicy(nodemask_t *m) { return false; }
+
 static inline int do_migrate_pages(struct mm_struct *mm,
 			const nodemask_t *from_nodes,
 			const nodemask_t *to_nodes, int flags)

commit 83d1674a946141c3c59d430e96c224f7937e6158
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jul 23 21:28:22 2008 -0700

    mm: make CONFIG_MIGRATION available w/o CONFIG_NUMA
    
    We'd like to support CONFIG_MEMORY_HOTREMOVE on s390, which depends on
    CONFIG_MIGRATION.  So far, CONFIG_MIGRATION is only available with NUMA
    support.
    
    This patch makes CONFIG_MIGRATION selectable for architectures that define
    ARCH_ENABLE_MEMORY_HOTREMOVE.  When MIGRATION is enabled w/o NUMA, the
    kernel won't compile because migrate_vmas() does not know about
    vm_ops->migrate() and vma_migratable() does not know about policy_zone.
    To fix this, those two functions can be restricted to '#ifdef CONFIG_NUMA'
    because they are not being used w/o NUMA.  vma_migratable() is moved over
    from migrate.h to mempolicy.h.
    
    [kosaki.motohiro@jp.fujitsu.com: build fix]
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: KOSAKI Motorhiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 3a39570b81b8..085c903fe0f1 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -59,6 +59,7 @@ enum {
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
 #include <linux/nodemask.h>
+#include <linux/pagemap.h>
 
 struct mm_struct;
 
@@ -220,6 +221,24 @@ extern int mpol_parse_str(char *str, struct mempolicy **mpol, int no_context);
 extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
 			int no_context);
 #endif
+
+/* Check if a vma is migratable */
+static inline int vma_migratable(struct vm_area_struct *vma)
+{
+	if (vma->vm_flags & (VM_IO|VM_HUGETLB|VM_PFNMAP|VM_RESERVED))
+		return 0;
+	/*
+	 * Migration allocates pages in the highest zone. If we cannot
+	 * do so then migration (at least from node to node) is not
+	 * possible.
+	 */
+	if (vma->vm_file &&
+		gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
+								< policy_zone)
+			return 0;
+	return 1;
+}
+
 #else
 
 struct mempolicy {};

commit 71fe804b6d56d6a7aed680e096901434cef6a2c3
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:26 2008 -0700

    mempolicy: use struct mempolicy pointer in shmem_sb_info
    
    This patch replaces the mempolicy mode, mode_flags, and nodemask in the
    shmem_sb_info struct with a struct mempolicy pointer, initialized to NULL.
    This removes dependency on the details of mempolicy from shmem.c and hugetlbfs
    inode.c and simplifies the interfaces.
    
    mpol_parse_str() in mempolicy.c is changed to return, via a pointer to a
    pointer arg, a struct mempolicy pointer on success.  For MPOL_DEFAULT, the
    returned pointer is NULL.  Further, mpol_parse_str() now takes a 'no_context'
    argument that causes the input nodemask to be stored in the w.user_nodemask of
    the created mempolicy for use when the mempolicy is installed in a tmpfs inode
    shared policy tree.  At that time, any cpuset contextualization is applied to
    the original input nodemask.  This preserves the previous behavior where the
    input nodemask was stored in the superblock.  We can think of the returned
    mempolicy as "context free".
    
    Because mpol_parse_str() is now calling mpol_new(), we can remove from
    mpol_to_str() the semantic checks that mpol_new() already performs.
    
    Add 'no_context' parameter to mpol_to_str() to specify that it should format
    the nodemask in w.user_nodemask for 'bind' and 'interleave' policies.
    
    Change mpol_shared_policy_init() to take a pointer to a "context free" struct
    mempolicy and to create a new, "contextualized" mempolicy using the mode,
    mode_flags and user_nodemask from the input mempolicy.
    
      Note: we know that the mempolicy passed to mpol_to_str() or
      mpol_shared_policy_init() from a tmpfs superblock is "context free".  This
      is currently the only instance thereof.  However, if we found more uses for
      this concept, and introduced any ambiguity as to whether a mempolicy was
      context free or not, we could add another internal mode flag to identify
      context free mempolicies.  Then, we could remove the 'no_context' argument
      from mpol_to_str().
    
    Added shmem_get_sbmpol() to return a reference counted superblock mempolicy,
    if one exists, to pass to mpol_shared_policy_init().  We must add the
    reference under the sb stat_lock to prevent races with replacement of the mpol
    by remount.  This reference is removed in mpol_shared_policy_init().
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: another build fix]
    [akpm@linux-foundation.org: yet another build fix]
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index dcc17378c952..3a39570b81b8 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -182,8 +182,7 @@ struct shared_policy {
 	spinlock_t lock;
 };
 
-void mpol_shared_policy_init(struct shared_policy *info, unsigned short mode,
-				unsigned short flags, nodemask_t *nodes);
+void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
 				struct mempolicy *new);
@@ -216,10 +215,10 @@ int do_migrate_pages(struct mm_struct *mm,
 
 
 #ifdef CONFIG_TMPFS
-extern int mpol_parse_str(char *str, unsigned short *mode,
-			unsigned short *mode_flags, nodemask_t *policy_nodes);
+extern int mpol_parse_str(char *str, struct mempolicy **mpol, int no_context);
 
-extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
+extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
+			int no_context);
 #endif
 #else
 
@@ -262,8 +261,8 @@ static inline int mpol_set_shared_policy(struct shared_policy *info,
 	return -EINVAL;
 }
 
-static inline void mpol_shared_policy_init(struct shared_policy *info,
-		unsigned short mode, unsigned short flags, nodemask_t *nodes)
+static inline void mpol_shared_policy_init(struct shared_policy *sp,
+						struct mempolicy *mpol)
 {
 }
 
@@ -322,13 +321,14 @@ static inline void check_highest_zone(int k)
 }
 
 #ifdef CONFIG_TMPFS
-static inline int mpol_parse_str(char *value, unsigned short *policy,
-				unsigned short flags, nodemask_t *policy_nodes)
+static inline int mpol_parse_str(char *str, struct mempolicy **mpol,
+				int no_context)
 {
-	return 1;
+	return 1;	/* error */
 }
 
-static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
+static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol,
+				int no_context)
 {
 	return 0;
 }

commit 095f1fc4ebf36c64fddf9b6db29b1ab5517378e6
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:23 2008 -0700

    mempolicy: rework shmem mpol parsing and display
    
    mm/shmem.c currently contains functions to parse and display memory policy
    strings for the tmpfs 'mpol' mount option.  Move this to mm/mempolicy.c with
    the rest of the mempolicy support.  With subsequent patches, we'll be able to
    remove knowledge of the details [mode, flags, policy, ...] completely from
    shmem.c
    
    1) replace shmem_parse_mpol() in mm/shmem.c with mpol_parse_str() in
       mm/mempolicy.c.  Rework to use the policy_types[] array [used by
       mpol_to_str()] to look up mode by name.
    
    2) use mpol_to_str() to format policy for shmem_show_mpol().  mpol_to_str()
       expects a pointer to a struct mempolicy, so temporarily construct one.
       This will be replaced with a reference to a struct mempolicy in the tmpfs
       superblock in a subsequent patch.
    
       NOTE 1: I changed mpol_to_str() to use a colon ':' rather than an equal
       sign '=' as the nodemask delimiter to match mpol_parse_str() and the
       tmpfs/shmem mpol mount option formatting that now uses mpol_to_str().  This
       is a user visible change to numa_maps, but then the addition of the mode
       flags already changed the display.  It makes sense to me to have the mounts
       and numa_maps display the policy in the same format.  However, if anyone
       objects strongly, I can pass the desired nodemask delimeter as an arg to
       mpol_to_str().
    
       Note 2: Like show_numa_map(), I don't check the return code from
       mpol_to_str().  I do use a longer buffer than the one provided by
       show_numa_map(), which seems to have sufficed so far.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index b0fab9e80655..dcc17378c952 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -214,6 +214,13 @@ static inline void check_highest_zone(enum zone_type k)
 int do_migrate_pages(struct mm_struct *mm,
 	const nodemask_t *from_nodes, const nodemask_t *to_nodes, int flags);
 
+
+#ifdef CONFIG_TMPFS
+extern int mpol_parse_str(char *str, unsigned short *mode,
+			unsigned short *mode_flags, nodemask_t *policy_nodes);
+
+extern int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol);
+#endif
 #else
 
 struct mempolicy {};
@@ -313,6 +320,20 @@ static inline int do_migrate_pages(struct mm_struct *mm,
 static inline void check_highest_zone(int k)
 {
 }
+
+#ifdef CONFIG_TMPFS
+static inline int mpol_parse_str(char *value, unsigned short *policy,
+				unsigned short flags, nodemask_t *policy_nodes)
+{
+	return 1;
+}
+
+static inline int mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
+{
+	return 0;
+}
+#endif
+
 #endif /* CONFIG_NUMA */
 #endif /* __KERNEL__ */
 

commit fc36b8d3d819047eb4d23ca079fb4d3af20ff076
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:21 2008 -0700

    mempolicy: use MPOL_F_LOCAL to Indicate Preferred Local Policy
    
    Now that we're using "preferred local" policy for system default, we need to
    make this as fast as possible.  Because of the variable size of the mempolicy
    structure [based on size of nodemasks], the preferred_node may be in a
    different cacheline from the mode.  This can result in accessing an extra
    cacheline in the normal case of system default policy.  Suspect this is the
    cause of an observed 2-3% slowdown in page fault testing relative to kernel
    without this patch series.
    
    To alleviate this, use an internal mode flag, MPOL_F_LOCAL in the mempolicy
    flags member which is guaranteed [?] to be in the same cacheline as the mode
    itself.
    
    Verified that reworked mempolicy now performs slightly better on 25-rc8-mm1
    for both anon and shmem segments with system default and vma [preferred local]
    policy.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 172b9c6acb91..b0fab9e80655 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -50,6 +50,7 @@ enum {
  * are never OR'ed into the mode in mempolicy API arguments.
  */
 #define MPOL_F_SHARED  (1 << 0)	/* identify shared policies */
+#define MPOL_F_LOCAL   (1 << 1)	/* preferred local allocation */
 
 #ifdef __KERNEL__
 

commit 52cd3b074050dd664380b5e8cfc85d4a6ed8ad48
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:16 2008 -0700

    mempolicy: rework mempolicy Reference Counting [yet again]
    
    After further discussion with Christoph Lameter, it has become clear that my
    earlier attempts to clean up the mempolicy reference counting were a bit of
    overkill in some areas, resulting in superflous ref/unref in what are usually
    fast paths.  In other areas, further inspection reveals that I botched the
    unref for interleave policies.
    
    A separate patch, suitable for upstream/stable trees, fixes up the known
    errors in the previous attempt to fix reference counting.
    
    This patch reworks the memory policy referencing counting and, one hopes,
    simplifies the code.  Maybe I'll get it right this time.
    
    See the update to the numa_memory_policy.txt document for a discussion of
    memory policy reference counting that motivates this patch.
    
    Summary:
    
    Lookup of mempolicy, based on (vma, address) need only add a reference for
    shared policy, and we need only unref the policy when finished for shared
    policies.  So, this patch backs out all of the unneeded extra reference
    counting added by my previous attempt.  It then unrefs only shared policies
    when we're finished with them, using the mpol_cond_put() [conditional put]
    helper function introduced by this patch.
    
    Note that shmem_swapin() calls read_swap_cache_async() with a dummy vma
    containing just the policy.  read_swap_cache_async() can call alloc_page_vma()
    multiple times, so we can't let alloc_page_vma() unref the shared policy in
    this case.  To avoid this, we make a copy of any non-null shared policy and
    remove the MPOL_F_SHARED flag from the copy.  This copy occurs before reading
    a page [or multiple pages] from swap, so the overhead should not be an issue
    here.
    
    I introduced a new static inline function "mpol_cond_copy()" to copy the
    shared policy to an on-stack policy and remove the flags that would require a
    conditional free.  The current implementation of mpol_cond_copy() assumes that
    the struct mempolicy contains no pointers to dynamically allocated structures
    that must be duplicated or reference counted during copy.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 017def89e568..172b9c6acb91 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -112,6 +112,31 @@ static inline void mpol_put(struct mempolicy *pol)
 		__mpol_put(pol);
 }
 
+/*
+ * Does mempolicy pol need explicit unref after use?
+ * Currently only needed for shared policies.
+ */
+static inline int mpol_needs_cond_ref(struct mempolicy *pol)
+{
+	return (pol && (pol->flags & MPOL_F_SHARED));
+}
+
+static inline void mpol_cond_put(struct mempolicy *pol)
+{
+	if (mpol_needs_cond_ref(pol))
+		__mpol_put(pol);
+}
+
+extern struct mempolicy *__mpol_cond_copy(struct mempolicy *tompol,
+					  struct mempolicy *frompol);
+static inline struct mempolicy *mpol_cond_copy(struct mempolicy *tompol,
+						struct mempolicy *frompol)
+{
+	if (!frompol)
+		return frompol;
+	return __mpol_cond_copy(tompol, frompol);
+}
+
 extern struct mempolicy *__mpol_dup(struct mempolicy *pol);
 static inline struct mempolicy *mpol_dup(struct mempolicy *pol)
 {
@@ -201,6 +226,16 @@ static inline void mpol_put(struct mempolicy *p)
 {
 }
 
+static inline void mpol_cond_put(struct mempolicy *pol)
+{
+}
+
+static inline struct mempolicy *mpol_cond_copy(struct mempolicy *to,
+						struct mempolicy *from)
+{
+	return from;
+}
+
 static inline void mpol_get(struct mempolicy *pol)
 {
 }

commit aab0b1029f0843756b68e0ed3ca983685bf43ed6
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:13 2008 -0700

    mempolicy: mark shared policies for unref
    
    As part of yet another rework of mempolicy reference counting, we want to be
    able to identify shared policies efficiently, because they have an extra ref
    taken on lookup that needs to be removed when we're finished using the policy.
    
      Note:  the extra ref is required because the policies are
      shared between tasks/processes and can be changed/freed
      by one task while another task is using them--e.g., for
      page allocation.
    
    Building on David Rientjes mempolicy "mode flags" enhancement, this patch
    indicates a "shared" policy by setting a new MPOL_F_SHARED flag in the flags
    member of the struct mempolicy added by David.  MPOL_F_SHARED, and any future
    "internal mode flags" are reserved from bit zero up, as they will never be
    passed in the upper bits of the mode argument of a mempolicy API.
    
    I set the MPOL_F_SHARED flag when the policy is installed in the shared policy
    rb-tree.  Don't need/want to clear the flag when removing from the tree as the
    mempolicy is freed [unref'd] internally to the sp_delete() function.  However,
    a task could hold another reference on this mempolicy from a prior lookup.  We
    need the MPOL_F_SHARED flag to stay put so that any tasks holding a ref will
    unref, eventually freeing, the mempolicy.
    
    A later patch in this series will introduce a function to conditionally unref
    [mpol_free] a policy.  The MPOL_F_SHARED flag is one reason [currently the
    only reason] to unref/free a policy via the conditional free.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 9080fab1426d..017def89e568 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -44,6 +44,13 @@ enum {
 #define MPOL_MF_MOVE_ALL (1<<2)	/* Move every page to conform to mapping */
 #define MPOL_MF_INTERNAL (1<<3)	/* Internal flags start here */
 
+/*
+ * Internal flags that share the struct mempolicy flags word with
+ * "mode flags".  These flags are allocated from bit 0 up, as they
+ * are never OR'ed into the mode in mempolicy API arguments.
+ */
+#define MPOL_F_SHARED  (1 << 0)	/* identify shared policies */
+
 #ifdef __KERNEL__
 
 #include <linux/mmzone.h>

commit 45c4745af381851b0406d8e4db99e62e265691c2
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:12 2008 -0700

    mempolicy: rename struct mempolicy 'policy' member to 'mode'
    
    The terms 'policy' and 'mode' are both used in various places to describe the
    semantics of the value stored in the 'policy' member of struct mempolicy.
    Furthermore, the term 'policy' is used to refer to that member, to the entire
    struct mempolicy and to the more abstract concept of the tuple consisting of a
    "mode" and an optional node or set of nodes.  Recently, we have added "mode
    flags" that are passed in the upper bits of the 'mode' [or sometimes,
    'policy'] member of the numa APIs.
    
    I'd like to resolve this confusion, which perhaps only exists in my mind, by
    renaming the 'policy' member to 'mode' throughout, and fixing up the
    Documentation.  Man pages will be updated separately.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5e19c2275a6f..9080fab1426d 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -80,7 +80,7 @@ struct mm_struct;
  */
 struct mempolicy {
 	atomic_t refcnt;
-	unsigned short policy; 	/* See MPOL_* above */
+	unsigned short mode; 	/* See MPOL_* above */
 	unsigned short flags;	/* See set_mempolicy() MPOL_F_* above */
 	union {
 		short 		 preferred_node; /* preferred */
@@ -149,7 +149,7 @@ struct shared_policy {
 	spinlock_t lock;
 };
 
-void mpol_shared_policy_init(struct shared_policy *info, unsigned short policy,
+void mpol_shared_policy_init(struct shared_policy *info, unsigned short mode,
 				unsigned short flags, nodemask_t *nodes);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
@@ -213,7 +213,7 @@ static inline int mpol_set_shared_policy(struct shared_policy *info,
 }
 
 static inline void mpol_shared_policy_init(struct shared_policy *info,
-		unsigned short policy, unsigned short flags, nodemask_t *nodes)
+		unsigned short mode, unsigned short flags, nodemask_t *nodes)
 {
 }
 

commit 846a16bf0fc80dc95a414ffce465e3cbf9680247
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:09 2008 -0700

    mempolicy: rename mpol_copy to mpol_dup
    
    This patch renames mpol_copy() to mpol_dup() because, well, that's what it
    does.  Like, e.g., strdup() for strings, mpol_dup() takes a pointer to an
    existing mempolicy, allocates a new one and copies the contents.
    
    In a later patch, I want to use the name mpol_copy() to copy the contents from
    one mempolicy to another like, e.g., strcpy() does for strings.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 507bf5e29f24..5e19c2275a6f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -73,10 +73,10 @@ struct mm_struct;
  * Mempolicy objects are reference counted.  A mempolicy will be freed when
  * mpol_put() decrements the reference count to zero.
  *
- * Copying policy objects:
- * mpol_copy() allocates a new mempolicy and copies the specified mempolicy
+ * Duplicating policy objects:
+ * mpol_dup() allocates a new mempolicy and copies the specified mempolicy
  * to the new storage.  The reference count of the new object is initialized
- * to 1, representing the caller of mpol_copy().
+ * to 1, representing the caller of mpol_dup().
  */
 struct mempolicy {
 	atomic_t refcnt;
@@ -105,11 +105,11 @@ static inline void mpol_put(struct mempolicy *pol)
 		__mpol_put(pol);
 }
 
-extern struct mempolicy *__mpol_copy(struct mempolicy *pol);
-static inline struct mempolicy *mpol_copy(struct mempolicy *pol)
+extern struct mempolicy *__mpol_dup(struct mempolicy *pol);
+static inline struct mempolicy *mpol_dup(struct mempolicy *pol)
 {
 	if (pol)
-		pol = __mpol_copy(pol);
+		pol = __mpol_dup(pol);
 	return pol;
 }
 
@@ -198,7 +198,7 @@ static inline void mpol_get(struct mempolicy *pol)
 {
 }
 
-static inline struct mempolicy *mpol_copy(struct mempolicy *old)
+static inline struct mempolicy *mpol_dup(struct mempolicy *old)
 {
 	return NULL;
 }

commit f0be3d32b05d3fea2fcdbbb81a39dac2a7163169
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:08 2008 -0700

    mempolicy: rename mpol_free to mpol_put
    
    This is a change that was requested some time ago by Mel Gorman.  Makes sense
    to me, so here it is.
    
    Note: I retain the name "mpol_free_shared_policy()" because it actually does
    free the shared_policy, which is NOT a reference counted object.  However, ...
    
    The mempolicy object[s] referenced by the shared_policy are reference counted,
    so mpol_put() is used to release the reference held by the shared_policy.  The
    mempolicy might not be freed at this time, because some task attached to the
    shared object associated with the shared policy may be in the process of
    allocating a page based on the mempolicy.  In that case, the task performing
    the allocation will hold a reference on the mempolicy, obtained via
    mpol_shared_policy_lookup().  The mempolicy will be freed when all tasks
    holding such a reference have called mpol_put() for the mempolicy.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 319fd342b1b7..507bf5e29f24 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -71,7 +71,7 @@ struct mm_struct;
  *
  * Freeing policy:
  * Mempolicy objects are reference counted.  A mempolicy will be freed when
- * mpol_free() decrements the reference count to zero.
+ * mpol_put() decrements the reference count to zero.
  *
  * Copying policy objects:
  * mpol_copy() allocates a new mempolicy and copies the specified mempolicy
@@ -98,11 +98,11 @@ struct mempolicy {
  * The default fast path of a NULL MPOL_DEFAULT policy is always inlined.
  */
 
-extern void __mpol_free(struct mempolicy *pol);
-static inline void mpol_free(struct mempolicy *pol)
+extern void __mpol_put(struct mempolicy *pol);
+static inline void mpol_put(struct mempolicy *pol)
 {
 	if (pol)
-		__mpol_free(pol);
+		__mpol_put(pol);
 }
 
 extern struct mempolicy *__mpol_copy(struct mempolicy *pol);
@@ -190,7 +190,7 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 	return 1;
 }
 
-static inline void mpol_free(struct mempolicy *p)
+static inline void mpol_put(struct mempolicy *p)
 {
 }
 

commit 3842b46de626d1a3c44ad280d67ab0a4dc047d13
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 28 02:12:34 2008 -0700

    mempolicy: small header file cleanup
    
    Removes forward definition of vm_area_struct in linux/mempolicy.h.  We already
    get it from the linux/slab.h -> linux/gfp.h include.
    
    Removes the unused mpol_set_vma_default() macro from linux/mempolicy.h.
    
    Removes the extern definition of default_policy since it is only referenced,
    as it should be, in mm/mempolicy.c.
    
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 02b11efd7066..319fd342b1b7 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -52,7 +52,6 @@ enum {
 #include <linux/spinlock.h>
 #include <linux/nodemask.h>
 
-struct vm_area_struct;
 struct mm_struct;
 
 #ifdef CONFIG_NUMA
@@ -131,10 +130,6 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 	return __mpol_equal(a, b);
 }
 
-/* Could later add inheritance of the process policy here. */
-
-#define mpol_set_vma_default(vma) ((vma)->vm_policy = NULL)
-
 /*
  * Tree of shared policies for a shared memory region.
  * Maintain the policies in a pseudo mm that contains vmas. The vmas
@@ -170,7 +165,6 @@ extern void mpol_rebind_task(struct task_struct *tsk,
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
-extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 				unsigned long addr, gfp_t gfp_flags,
 				struct mempolicy **mpol, nodemask_t **nodemask);
@@ -196,8 +190,6 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 	return 1;
 }
 
-#define mpol_set_vma_default(vma) do {} while(0)
-
 static inline void mpol_free(struct mempolicy *p)
 {
 }

commit 4c50bc0116cf3cc35e7152d6a8424b4db65f52d6
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 28 02:12:30 2008 -0700

    mempolicy: add MPOL_F_RELATIVE_NODES flag
    
    Adds another optional mode flag, MPOL_F_RELATIVE_NODES, that specifies
    nodemasks passed via set_mempolicy() or mbind() should be considered relative
    to the current task's mems_allowed.
    
    When the mempolicy is created, the passed nodemask is folded and mapped onto
    the current task's mems_allowed.  For example, consider a task using
    set_mempolicy() to pass MPOL_INTERLEAVE | MPOL_F_RELATIVE_NODES with a
    nodemask of 1-3.  If current's mems_allowed is 4-7, the effected nodemask is
    5-7 (the second, third, and fourth node of mems_allowed).
    
    If the same task is attached to a cpuset, the mempolicy nodemask is rebound
    each time the mems are changed.  Some possible rebinds and results are:
    
            mems                    result
            1-3                     1-3
            1-7                     2-4
            1,5-6                   1,5-6
            1,5-7                   5-7
    
    Likewise, the zonelist built for MPOL_BIND acts on the set of zones assigned
    to the resultant nodemask from the relative remap.
    
    In the MPOL_PREFERRED case, the preferred node is remapped from the currently
    effected nodemask to the relative nodemask.
    
    This mempolicy mode flag was conceived of by Paul Jackson <pj@sgi.com>.
    
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 07350d7b8d96..02b11efd7066 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -25,12 +25,13 @@ enum {
 
 /* Flags for set_mempolicy */
 #define MPOL_F_STATIC_NODES	(1 << 15)
+#define MPOL_F_RELATIVE_NODES	(1 << 14)
 
 /*
  * MPOL_MODE_FLAGS is the union of all possible optional mode flags passed to
  * either set_mempolicy() or mbind().
  */
-#define MPOL_MODE_FLAGS	(MPOL_F_STATIC_NODES)
+#define MPOL_MODE_FLAGS	(MPOL_F_STATIC_NODES | MPOL_F_RELATIVE_NODES)
 
 /* Flags for get_mempolicy */
 #define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */

commit f5b087b52f1710eb0bf15a2d2b030c51a6a1ca9e
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 28 02:12:27 2008 -0700

    mempolicy: add MPOL_F_STATIC_NODES flag
    
    Add an optional mempolicy mode flag, MPOL_F_STATIC_NODES, that suppresses the
    node remap when the policy is rebound.
    
    Adds another member to struct mempolicy, nodemask_t user_nodemask, as part of
    a union with cpuset_mems_allowed:
    
            struct mempolicy {
                    ...
                    union {
                            nodemask_t cpuset_mems_allowed;
                            nodemask_t user_nodemask;
                    } w;
            }
    
    that stores the the nodemask that the user passed when he or she created the
    mempolicy via set_mempolicy() or mbind().  When using MPOL_F_STATIC_NODES,
    which is passed with any mempolicy mode, the user's passed nodemask
    intersected with the VMA or task's allowed nodes is always used when
    determining the preferred node, setting the MPOL_BIND zonelist, or creating
    the interleave nodemask.  This happens whenever the policy is rebound,
    including when a task's cpuset assignment changes or the cpuset's mems are
    changed.
    
    This creates an interesting side-effect in that it allows the mempolicy
    "intent" to lie dormant and uneffected until it has access to the node(s) that
    it desires.  For example, if you currently ask for an interleaved policy over
    a set of nodes that you do not have access to, the mempolicy is not created
    and the task continues to use the previous policy.  With this change, however,
    it is possible to create the same mempolicy; it is only effected when access
    to nodes in the nodemask is acquired.
    
    It is also possible to mount tmpfs with the static nodemask behavior when
    specifying a node or nodemask.  To do this, simply add "=static" immediately
    following the mempolicy mode at mount time:
    
            mount -o remount mpol=interleave=static:1-3
    
    Also removes mpol_check_policy() and folds its logic into mpol_new() since it
    is now obsoleted.  The unused vma_mpol_equal() is also removed.
    
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index f2bab4d2fc40..07350d7b8d96 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -24,11 +24,13 @@ enum {
 };
 
 /* Flags for set_mempolicy */
+#define MPOL_F_STATIC_NODES	(1 << 15)
+
 /*
  * MPOL_MODE_FLAGS is the union of all possible optional mode flags passed to
  * either set_mempolicy() or mbind().
  */
-#define MPOL_MODE_FLAGS	(0)
+#define MPOL_MODE_FLAGS	(MPOL_F_STATIC_NODES)
 
 /* Flags for get_mempolicy */
 #define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
@@ -85,7 +87,10 @@ struct mempolicy {
 		nodemask_t	 nodes;		/* interleave/bind */
 		/* undefined for default */
 	} v;
-	nodemask_t cpuset_mems_allowed;	/* mempolicy relative to these nodes */
+	union {
+		nodemask_t cpuset_mems_allowed;	/* relative to these nodes */
+		nodemask_t user_nodemask;	/* nodemask passed by user */
+	} w;
 };
 
 /*
@@ -124,7 +129,6 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 		return 1;
 	return __mpol_equal(a, b);
 }
-#define vma_mpol_equal(a,b) mpol_equal(vma_policy(a), vma_policy(b))
 
 /* Could later add inheritance of the process policy here. */
 
@@ -190,7 +194,6 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 {
 	return 1;
 }
-#define vma_mpol_equal(a,b) 1
 
 #define mpol_set_vma_default(vma) do {} while(0)
 

commit 028fec414d803117eb4b2ed12acb4dd5da65b32d
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 28 02:12:25 2008 -0700

    mempolicy: support optional mode flags
    
    With the evolution of mempolicies, it is necessary to support mempolicy mode
    flags that specify how the policy shall behave in certain circumstances.  The
    most immediate need for mode flag support is to suppress remapping the
    nodemask of a policy at the time of rebind.
    
    Both the mempolicy mode and flags are passed by the user in the 'int policy'
    formal of either the set_mempolicy() or mbind() syscall.  A new constant,
    MPOL_MODE_FLAGS, represents the union of legal optional flags that may be
    passed as part of this int.  Mempolicies that include illegal flags as part of
    their policy are rejected as invalid.
    
    An additional member to struct mempolicy is added to support the mode flags:
    
            struct mempolicy {
                    ...
                    unsigned short policy;
                    unsigned short flags;
            }
    
    The splitting of the 'int' actual passed by the user is done in
    sys_set_mempolicy() and sys_mbind() for their respective syscalls.  This is
    done by intersecting the actual with MPOL_MODE_FLAGS, rejecting the syscall of
    there are additional flags, and storing it in the new 'flags' member of struct
    mempolicy.  The intersection of the actual with ~MPOL_MODE_FLAGS is stored in
    the 'policy' member of the struct and all current users of pol->policy remain
    unchanged.
    
    The union of the policy mode and optional mode flags is passed back to the
    user in get_mempolicy().
    
    This combination of mode and flags within the same actual does not break
    userspace code that relies on get_mempolicy(&policy, ...) and either
    
            switch (policy) {
            case MPOL_BIND:
                    ...
            case MPOL_INTERLEAVE:
                    ...
            };
    
    statements or
    
            if (policy == MPOL_INTERLEAVE) {
                    ...
            }
    
    statements.  Such applications would need to use optional mode flags when
    calling set_mempolicy() or mbind() for these previously implemented statements
    to stop working.  If an application does start using optional mode flags, it
    will need to mask the optional flags off the policy in switch and conditional
    statements that only test mode.
    
    An additional member is also added to struct shmem_sb_info to store the
    optional mode flags.
    
    [hugh@veritas.com: shmem mpol: fix build warning]
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 389a06e8ee21..f2bab4d2fc40 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -8,6 +8,12 @@
  * Copyright 2003,2004 Andi Kleen SuSE Labs
  */
 
+/*
+ * Both the MPOL_* mempolicy mode and the MPOL_F_* optional mode flags are
+ * passed by the user to either set_mempolicy() or mbind() in an 'int' actual.
+ * The MPOL_MODE_FLAGS macro determines the legal set of optional mode flags.
+ */
+
 /* Policies */
 enum {
 	MPOL_DEFAULT,
@@ -17,7 +23,14 @@ enum {
 	MPOL_MAX,	/* always last member of enum */
 };
 
-/* Flags for get_mem_policy */
+/* Flags for set_mempolicy */
+/*
+ * MPOL_MODE_FLAGS is the union of all possible optional mode flags passed to
+ * either set_mempolicy() or mbind().
+ */
+#define MPOL_MODE_FLAGS	(0)
+
+/* Flags for get_mempolicy */
 #define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
 #define MPOL_F_ADDR	(1<<1)	/* look up vma using address */
 #define MPOL_F_MEMS_ALLOWED (1<<2) /* return allowed memories */
@@ -66,6 +79,7 @@ struct mm_struct;
 struct mempolicy {
 	atomic_t refcnt;
 	unsigned short policy; 	/* See MPOL_* above */
+	unsigned short flags;	/* See set_mempolicy() MPOL_F_* above */
 	union {
 		short 		 preferred_node; /* preferred */
 		nodemask_t	 nodes;		/* interleave/bind */
@@ -136,7 +150,7 @@ struct shared_policy {
 };
 
 void mpol_shared_policy_init(struct shared_policy *info, unsigned short policy,
-				nodemask_t *nodes);
+				unsigned short flags, nodemask_t *nodes);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
 				struct mempolicy *new);
@@ -203,7 +217,7 @@ static inline int mpol_set_shared_policy(struct shared_policy *info,
 }
 
 static inline void mpol_shared_policy_init(struct shared_policy *info,
-				unsigned short policy, nodemask_t *nodes)
+		unsigned short policy, unsigned short flags, nodemask_t *nodes)
 {
 }
 

commit a3b51e0142d1be156ac697eaadadd6cfbb7ba32b
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 28 02:12:23 2008 -0700

    mempolicy: convert MPOL constants to enum
    
    The mempolicy mode constants, MPOL_DEFAULT, MPOL_PREFERRED, MPOL_BIND, and
    MPOL_INTERLEAVE, are better declared as part of an enum since they are
    sequentially numbered and cannot be combined.
    
    The policy member of struct mempolicy is also converted from type short to
    type unsigned short.  A negative policy does not have any legitimate meaning,
    so it is possible to change its type in preparation for adding optional mode
    flags later.
    
    The equivalent member of struct shmem_sb_info is also changed from int to
    unsigned short.
    
    For compatibility, the policy formal to get_mempolicy() remains as a pointer
    to an int:
    
            int get_mempolicy(int *policy, unsigned long *nmask,
                              unsigned long maxnode, unsigned long addr,
                              unsigned long flags);
    
    although the only possible values is the range of type unsigned short.
    
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index b8b3da7a3315..389a06e8ee21 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -9,12 +9,13 @@
  */
 
 /* Policies */
-#define MPOL_DEFAULT	0
-#define MPOL_PREFERRED	1
-#define MPOL_BIND	2
-#define MPOL_INTERLEAVE	3
-
-#define MPOL_MAX MPOL_INTERLEAVE
+enum {
+	MPOL_DEFAULT,
+	MPOL_PREFERRED,
+	MPOL_BIND,
+	MPOL_INTERLEAVE,
+	MPOL_MAX,	/* always last member of enum */
+};
 
 /* Flags for get_mem_policy */
 #define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
@@ -64,7 +65,7 @@ struct mm_struct;
  */
 struct mempolicy {
 	atomic_t refcnt;
-	short policy; 	/* See MPOL_* above */
+	unsigned short policy; 	/* See MPOL_* above */
 	union {
 		short 		 preferred_node; /* preferred */
 		nodemask_t	 nodes;		/* interleave/bind */
@@ -134,7 +135,7 @@ struct shared_policy {
 	spinlock_t lock;
 };
 
-void mpol_shared_policy_init(struct shared_policy *info, int policy,
+void mpol_shared_policy_init(struct shared_policy *info, unsigned short policy,
 				nodemask_t *nodes);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
@@ -202,7 +203,7 @@ static inline int mpol_set_shared_policy(struct shared_policy *info,
 }
 
 static inline void mpol_shared_policy_init(struct shared_policy *info,
-					int policy, nodemask_t *nodes)
+				unsigned short policy, nodemask_t *nodes)
 {
 }
 

commit 19770b32609b6bf97a3dece2529089494cbfc549
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:18 2008 -0700

    mm: filter based on a nodemask as well as a gfp_mask
    
    The MPOL_BIND policy creates a zonelist that is used for allocations
    controlled by that mempolicy.  As the per-node zonelist is already being
    filtered based on a zone id, this patch adds a version of __alloc_pages() that
    takes a nodemask for further filtering.  This eliminates the need for
    MPOL_BIND to create a custom zonelist.
    
    A positive benefit of this is that allocations using MPOL_BIND now use the
    local node's distance-ordered zonelist instead of a custom node-id-ordered
    zonelist.  I.e., pages will be allocated from the closest allowed node with
    available memory.
    
    [Lee.Schermerhorn@hp.com: Mempolicy: update stale documentation and comments]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask rework]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 69160dc32d48..b8b3da7a3315 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -54,19 +54,20 @@ struct mm_struct;
  * mmap_sem.
  *
  * Freeing policy:
- * When policy is MPOL_BIND v.zonelist is kmalloc'ed and must be kfree'd.
- * All other policies don't have any external state. mpol_free() handles this.
+ * Mempolicy objects are reference counted.  A mempolicy will be freed when
+ * mpol_free() decrements the reference count to zero.
  *
  * Copying policy objects:
- * For MPOL_BIND the zonelist must be always duplicated. mpol_clone() does this.
+ * mpol_copy() allocates a new mempolicy and copies the specified mempolicy
+ * to the new storage.  The reference count of the new object is initialized
+ * to 1, representing the caller of mpol_copy().
  */
 struct mempolicy {
 	atomic_t refcnt;
 	short policy; 	/* See MPOL_* above */
 	union {
-		struct zonelist  *zonelist;	/* bind */
 		short 		 preferred_node; /* preferred */
-		nodemask_t	 nodes;		/* interleave */
+		nodemask_t	 nodes;		/* interleave/bind */
 		/* undefined for default */
 	} v;
 	nodemask_t cpuset_mems_allowed;	/* mempolicy relative to these nodes */
@@ -151,7 +152,8 @@ extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
-		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol);
+				unsigned long addr, gfp_t gfp_flags,
+				struct mempolicy **mpol, nodemask_t **nodemask);
 extern unsigned slab_node(struct mempolicy *policy);
 
 extern enum zone_type policy_zone;
@@ -239,8 +241,11 @@ static inline void mpol_fix_fork_child_flag(struct task_struct *p)
 }
 
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
- 		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol)
+				unsigned long addr, gfp_t gfp_flags,
+				struct mempolicy **mpol, nodemask_t **nodemask)
 {
+	*mpol = NULL;
+	*nodemask = NULL;
 	return node_zonelist(0, gfp_flags);
 }
 

commit 0e88460da6ab7bb6a7ef83675412ed5b6315d741
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:14 2008 -0700

    mm: introduce node_zonelist() for accessing the zonelist for a GFP mask
    
    Introduce a node_zonelist() helper function.  It is used to lookup the
    appropriate zonelist given a node and a GFP mask.  The patch on its own is a
    cleanup but it helps clarify parts of the two-zonelist-per-node patchset.  If
    necessary, it can be merged with the next patch in this set without problems.
    
    Reviewed-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 59c4865bc85f..69160dc32d48 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -241,7 +241,7 @@ static inline void mpol_fix_fork_child_flag(struct task_struct *p)
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
  		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol)
 {
-	return NODE_DATA(0)->node_zonelists + gfp_zone(gfp_flags);
+	return node_zonelist(0, gfp_flags);
 }
 
 static inline int do_migrate_pages(struct mm_struct *mm,

commit 8793d854edbc2774943a4b0de3304dc73991159a
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:39 2007 -0700

    Task Control Groups: make cpusets a client of cgroups
    
    Remove the filesystem support logic from the cpusets system and makes cpusets
    a cgroup subsystem
    
    The "cpuset" filesystem becomes a dummy filesystem; attempts to mount it get
    passed through to the cgroup filesystem with the appropriate options to
    emulate the old cpuset filesystem behaviour.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 38c04d61ee06..59c4865bc85f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -148,14 +148,6 @@ extern void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 extern void mpol_fix_fork_child_flag(struct task_struct *p);
-#define set_cpuset_being_rebound(x) (cpuset_being_rebound = (x))
-
-#ifdef CONFIG_CPUSETS
-#define current_cpuset_is_being_rebound() \
-				(cpuset_being_rebound == current->cpuset)
-#else
-#define current_cpuset_is_being_rebound() 0
-#endif
 
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
@@ -173,8 +165,6 @@ static inline void check_highest_zone(enum zone_type k)
 int do_migrate_pages(struct mm_struct *mm,
 	const nodemask_t *from_nodes, const nodemask_t *to_nodes, int flags);
 
-extern void *cpuset_being_rebound;	/* Trigger mpol_copy vma rebind */
-
 #else
 
 struct mempolicy {};
@@ -248,8 +238,6 @@ static inline void mpol_fix_fork_child_flag(struct task_struct *p)
 {
 }
 
-#define set_cpuset_being_rebound(x) do {} while (0)
-
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
  		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol)
 {

commit dbcb0f19c877df9026b8c1227758d38bd561e9c4
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Oct 16 01:26:26 2007 -0700

    mm/mempolicy.c: cleanups
    
    This patch contains the following cleanups:
    - every file should include the headers containing the prototypes for
      its global functions
    - make the follosing needlessly global functions static:
      - migrate_to_node()
      - do_mbind()
      - sp_alloc()
      - mpol_rebind_policy()
    
    [akpm@linux-foundation.org: fix uninitialised var warning]
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 7e9698ec839b..38c04d61ee06 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -144,7 +144,6 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
-extern void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *new);
 extern void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
@@ -236,11 +235,6 @@ static inline void numa_default_policy(void)
 {
 }
 
-static inline void mpol_rebind_policy(struct mempolicy *pol,
-					const nodemask_t *new)
-{
-}
-
 static inline void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new)
 {

commit 754af6f5a85fcd1ecb456851d20c65e4c6ce10ab
Author: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Date:   Tue Oct 16 01:24:51 2007 -0700

    Mem Policy: add MPOL_F_MEMS_ALLOWED get_mempolicy() flag
    
    Allow an application to query the memories allowed by its context.
    
    Updated numa_memory_policy.txt to mention that applications can use this to
    obtain allowed memories for constructing valid policies.
    
    TODO:  update out-of-tree libnuma wrapper[s], or maybe add a new
    wrapper--e.g.,  numa_get_mems_allowed() ?
    
    Also, update numa syscall man pages.
    
    Tested with memtoy V>=0.13.
    
    Signed-off-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index a020eb2d4e2a..7e9698ec839b 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -19,6 +19,7 @@
 /* Flags for get_mem_policy */
 #define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
 #define MPOL_F_ADDR	(1<<1)	/* look up vma using address */
+#define MPOL_F_MEMS_ALLOWED (1<<2) /* return allowed memories */
 
 /* Flags for mbind */
 #define MPOL_MF_STRICT	(1<<0)	/* Verify existing pages in the mapping */

commit 480eccf9ae1073b87bb4fe118971fbf134a5bc61
Author: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Date:   Tue Sep 18 22:46:47 2007 -0700

    Fix NUMA Memory Policy Reference Counting
    
    This patch proposes fixes to the reference counting of memory policy in the
    page allocation paths and in show_numa_map().  Extracted from my "Memory
    Policy Cleanups and Enhancements" series as stand-alone.
    
    Shared policy lookup [shmem] has always added a reference to the policy,
    but this was never unrefed after page allocation or after formatting the
    numa map data.
    
    Default system policy should not require additional ref counting, nor
    should the current task's task policy.  However, show_numa_map() calls
    get_vma_policy() to examine what may be [likely is] another task's policy.
    The latter case needs protection against freeing of the policy.
    
    This patch adds a reference count to a mempolicy returned by
    get_vma_policy() when the policy is a vma policy or another task's
    mempolicy.  Again, shared policy is already reference counted on lookup.  A
    matching "unref" [__mpol_free()] is performed in alloc_page_vma() for
    shared and vma policies, and in show_numa_map() for shared and another
    task's mempolicy.  We can call __mpol_free() directly, saving an admittedly
    inexpensive inline NULL test, because we know we have a non-NULL policy.
    
    Handling policy ref counts for hugepages is a bit trickier.
    huge_zonelist() returns a zone list that might come from a shared or vma
    'BIND policy.  In this case, we should hold the reference until after the
    huge page allocation in dequeue_hugepage().  The patch modifies
    huge_zonelist() to return a pointer to the mempolicy if it needs to be
    unref'd after allocation.
    
    Kernel Build [16cpu, 32GB, ia64] - average of 10 runs:
    
                    w/o patch       w/ refcount patch
                Avg   Std Devn         Avg    Std Devn
    Real:    100.59     0.38         100.63     0.43
    User:   1209.60     0.37        1209.91     0.31
    System:   81.52     0.42          81.64     0.34
    
    Signed-off-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Andi Kleen <ak@suse.de>
    Cc: Christoph Lameter <clameter@sgi.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5bdd656e88cf..a020eb2d4e2a 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -159,7 +159,7 @@ extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
-		unsigned long addr, gfp_t gfp_flags);
+		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol);
 extern unsigned slab_node(struct mempolicy *policy);
 
 extern enum zone_type policy_zone;
@@ -256,7 +256,7 @@ static inline void mpol_fix_fork_child_flag(struct task_struct *p)
 #define set_cpuset_being_rebound(x) do {} while (0)
 
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
-		unsigned long addr, gfp_t gfp_flags)
+ 		unsigned long addr, gfp_t gfp_flags, struct mempolicy **mpol)
 {
 	return NODE_DATA(0)->node_zonelists + gfp_zone(gfp_flags);
 }

commit b377fd3982ad957c796758a90e2988401a884241
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Aug 22 14:02:05 2007 -0700

    Apply memory policies to top two highest zones when highest zone is ZONE_MOVABLE
    
    The NUMA layer only supports NUMA policies for the highest zone.  When
    ZONE_MOVABLE is configured with kernelcore=, the the highest zone becomes
    ZONE_MOVABLE.  The result is that policies are only applied to allocations
    like anonymous pages and page cache allocated from ZONE_MOVABLE when the
    zone is used.
    
    This patch applies policies to the two highest zones when the highest zone
    is ZONE_MOVABLE.  As ZONE_MOVABLE consists of pages from the highest "real"
    zone, it's always functionally equivalent.
    
    The patch has been tested on a variety of machines both NUMA and non-NUMA
    covering x86, x86_64 and ppc64.  No abnormal results were seen in
    kernbench, tbench, dbench or hackbench.  It passes regression tests from
    the numactl package with and without kernelcore= once numactl tests are
    patched to wait for vmstat counters to update.
    
    akpm: this is the nasty hack to fix NUMA mempolicies in the presence of
    ZONE_MOVABLE and kernelcore= in 2.6.23.  Christoph says "For .24 either merge
    the mobility or get the other solution that Mel is working on.  That solution
    would only use a single zonelist per node and filter on the fly.  That may
    help performance and also help to make memory policies work better."
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Tested-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index e147cf50529f..5bdd656e88cf 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -166,7 +166,7 @@ extern enum zone_type policy_zone;
 
 static inline void check_highest_zone(enum zone_type k)
 {
-	if (k > policy_zone)
+	if (k > policy_zone && k != ZONE_MOVABLE)
 		policy_zone = k;
 }
 

commit 396faf0303d273219db5d7eb4a2879ad977ed185
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jul 17 04:03:13 2007 -0700

    Allow huge page allocations to use GFP_HIGH_MOVABLE
    
    Huge pages are not movable so are not allocated from ZONE_MOVABLE.  However,
    as ZONE_MOVABLE will always have pages that can be migrated or reclaimed, it
    can be used to satisfy hugepage allocations even when the system has been
    running a long time.  This allows an administrator to resize the hugepage pool
    at runtime depending on the size of ZONE_MOVABLE.
    
    This patch adds a new sysctl called hugepages_treat_as_movable.  When a
    non-zero value is written to it, future allocations for the huge page pool
    will use ZONE_MOVABLE.  Despite huge pages being non-movable, we do not
    introduce additional external fragmentation of note as huge pages are always
    the largest contiguous block we care about.
    
    [akpm@linux-foundation.org: various fixes]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index daabb3aa1ec6..e147cf50529f 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -159,7 +159,7 @@ extern void mpol_fix_fork_child_flag(struct task_struct *p);
 
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
-		unsigned long addr);
+		unsigned long addr, gfp_t gfp_flags);
 extern unsigned slab_node(struct mempolicy *policy);
 
 extern enum zone_type policy_zone;
@@ -256,9 +256,9 @@ static inline void mpol_fix_fork_child_flag(struct task_struct *p)
 #define set_cpuset_being_rebound(x) do {} while (0)
 
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
-		unsigned long addr)
+		unsigned long addr, gfp_t gfp_flags)
 {
-	return NODE_DATA(0)->node_zonelists + gfp_zone(GFP_HIGHUSER);
+	return NODE_DATA(0)->node_zonelists + gfp_zone(gfp_flags);
 }
 
 static inline int do_migrate_pages(struct mm_struct *mm,

commit faf6bbcf94caee10ba34adb86db4ecca96bfd3bf
Author: Paul Jackson <pj@sgi.com>
Date:   Sat Oct 21 10:24:17 2006 -0700

    [PATCH] cpuset: mempolicy migration typo fix
    
    Mistyped an ifdef CONFIG_CPUSETS - fixed.
    
    I doubt that anyone ever noticed.  The impact of this typo was
    that if someone:
     1) was using MPOL_BIND to force off node allocations
     2) while using cpusets to constrain memory placement
     3) when that cpuset was migrating that jobs memory
     4) while the tasks in that job were actively forking
    then there was a rare chance that future allocations using
    that MPOL_BIND policy would be node local, not off node.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 09f0f575ddff..daabb3aa1ec6 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -150,7 +150,7 @@ extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 extern void mpol_fix_fork_child_flag(struct task_struct *p);
 #define set_cpuset_being_rebound(x) (cpuset_being_rebound = (x))
 
-#ifdef CONFIG_CPUSET
+#ifdef CONFIG_CPUSETS
 #define current_cpuset_is_being_rebound() \
 				(cpuset_being_rebound == current->cpuset)
 #else

commit 2f6726e54a9410e2e4cee864947c05e954051916
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:18 2006 -0700

    [PATCH] Apply type enum zone_type
    
    After we have done this we can now do some typing cleanup.
    
    The memory policy layer keeps a policy_zone that specifies
    the zone that gets memory policies applied. This variable
    can now be of type enum zone_type.
    
    The check_highest_zone function and the build_zonelists funnctionm must
    then also take a enum zone_type parameter.
    
    Plus there are a number of loops over zones that also should use
    zone_type.
    
    We run into some troubles at some points with functions that need a
    zone_type variable to become -1. Fix that up.
    
    [pj@sgi.com: fix set_mempolicy() crash]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 72440f0a443d..09f0f575ddff 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -162,9 +162,9 @@ extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr);
 extern unsigned slab_node(struct mempolicy *policy);
 
-extern int policy_zone;
+extern enum zone_type policy_zone;
 
-static inline void check_highest_zone(int k)
+static inline void check_highest_zone(enum zone_type k)
 {
 	if (k > policy_zone)
 		policy_zone = k;

commit cee4cca740d209bcb4b9857baa2253d5ba4e3fbe
Merge: 2edc322d420a 9348f0de2d2b
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:10:08 2006 -0700

    Merge git://git.infradead.org/hdrcleanup-2.6
    
    * git://git.infradead.org/hdrcleanup-2.6: (63 commits)
      [S390] __FD_foo definitions.
      Switch to __s32 types in joystick.h instead of C99 types for consistency.
      Add <sys/types.h> to headers included for userspace in <linux/input.h>
      Move inclusion of <linux/compat.h> out of user scope in asm-x86_64/mtrr.h
      Remove struct fddi_statistics from user view in <linux/if_fddi.h>
      Move user-visible parts of drivers/s390/crypto/z90crypt.h to include/asm-s390
      Revert include/media changes: Mauro says those ioctls are only used in-kernel(!)
      Include <linux/types.h> and use __uXX types in <linux/cramfs_fs.h>
      Use __uXX types in <linux/i2o_dev.h>, include <linux/ioctl.h> too
      Remove private struct dx_hash_info from public view in <linux/ext3_fs.h>
      Include <linux/types.h> and use __uXX types in <linux/affs_hardblocks.h>
      Use __uXX types in <linux/divert.h> for struct divert_blk et al.
      Use __u32 for elf_addr_t in <asm-powerpc/elf.h>, not u32. It's user-visible.
      Remove PPP_FCS from user view in <linux/ppp_defs.h>, remove __P mess entirely
      Use __uXX types in user-visible structures in <linux/nbd.h>
      Don't use 'u32' in user-visible struct ip_conntrack_old_tuple.
      Use __uXX types for S390 DASD volume label definitions which are user-visible
      S390 BIODASDREADCMB ioctl should use __u64 not u64 type.
      Remove unneeded inclusion of <linux/time.h> from <linux/ufs_fs.h>
      Fix private integer types used in V4L2 ioctls.
      ...
    
    Manually resolve conflict in include/linux/mtd/physmap.h

commit 45b35a5ced474b9fbbbfcfd5cf346c432d28d9fd
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Jun 8 00:43:41 2006 -0700

    [PATCH] Fix mempolicy.h build error
    
    From: Ralf Baechle <ralf@linux-mips.org>
    
    <linux/mempolicy.h> uses struct mm_struct and relies on a definition or
    declaration somehow magically being dragged in which may result in a
    build:
    
    [...]
      CC      mm/mempolicy.o
    In file included from mm/mempolicy.c:69:
    include/linux/mempolicy.h:150: warning: âstruct mm_structâ declared inside parameter list
    include/linux/mempolicy.h:150: warning: its scope is only this definition or declaration, which is probably not what you want
    include/linux/mempolicy.h:175: warning: âstruct mm_structâ declared inside parameter list
    mm/mempolicy.c:622: error: conflicting types for âdo_migrate_pagesâ
    include/linux/mempolicy.h:175: error: previous declaration of âdo_migrate_pagesâ was here
    mm/mempolicy.c:1661: error: conflicting types for âmpol_rebind_mmâ
    include/linux/mempolicy.h:150: error: previous declaration of âmpol_rebind_mmâ was here
    make[1]: *** [mm/mempolicy.o] Error 1
    make: *** [mm] Error 2
    [ralf@denk linux-ip35]$
    
    Including <linux/sched.h> is a step into direction of include hell so
    fixed by adding a forward declaration of struct mm_struct instead.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 6a7621b2b12b..f5fdca1d67e6 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -36,6 +36,7 @@
 #include <linux/nodemask.h>
 
 struct vm_area_struct;
+struct mm_struct;
 
 #ifdef CONFIG_NUMA
 

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 6a7621b2b12b..8dfdd352bccd 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -28,7 +28,6 @@
 
 #ifdef __KERNEL__
 
-#include <linux/config.h>
 #include <linux/mmzone.h>
 #include <linux/slab.h>
 #include <linux/rbtree.h>

commit c61afb181c649754ea221f104e268cbacfc993e3
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:08 2006 -0800

    [PATCH] cpuset memory spread slab cache optimizations
    
    The hooks in the slab cache allocator code path for support of NUMA
    mempolicies and cpuset memory spreading are in an important code path.  Many
    systems will use neither feature.
    
    This patch optimizes those hooks down to a single check of some bits in the
    current tasks task_struct flags.  For non NUMA systems, this hook and related
    code is already ifdef'd out.
    
    The optimization is done by using another task flag, set if the task is using
    a non-default NUMA mempolicy.  Taking this flag bit along with the
    PF_SPREAD_PAGE and PF_SPREAD_SLAB flag bits added earlier in this 'cpuset
    memory spreading' patch set, one can check for the combination of any of these
    special case memory placement mechanisms with a single test of the current
    tasks task_struct flags.
    
    This patch also tightens up the code, to save a few bytes of kernel text
    space, and moves some of it out of line.  Due to the nested inlines called
    from multiple places, we were ending up with three copies of this code, which
    once we get off the main code path (for local node allocation) seems a bit
    wasteful of instruction memory.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index bbd2221923c3..6a7621b2b12b 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -147,6 +147,7 @@ extern void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *new);
 extern void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new);
 extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
+extern void mpol_fix_fork_child_flag(struct task_struct *p);
 #define set_cpuset_being_rebound(x) (cpuset_being_rebound = (x))
 
 #ifdef CONFIG_CPUSET
@@ -248,6 +249,10 @@ static inline void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 }
 
+static inline void mpol_fix_fork_child_flag(struct task_struct *p)
+{
+}
+
 #define set_cpuset_being_rebound(x) do {} while (0)
 
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,

commit dc85da15d42b0efc792b0f5eab774dc5dbc1ceec
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Wed Jan 18 17:42:36 2006 -0800

    [PATCH] NUMA policies in the slab allocator V2
    
    This patch fixes a regression in 2.6.14 against 2.6.13 that causes an
    imbalance in memory allocation during bootup.
    
    The slab allocator in 2.6.13 is not numa aware and simply calls
    alloc_pages().  This means that memory policies may control the behavior of
    alloc_pages().  During bootup the memory policy is set to MPOL_INTERLEAVE
    resulting in the spreading out of allocations during bootup over all
    available nodes.  The slab allocator in 2.6.13 has only a single list of
    slab pages.  As a result the per cpu slab cache and the spinlock controlled
    page lists may contain slab entries from off node memory.  The slab
    allocator in 2.6.13 makes no effort to discern the locality of an entry on
    its lists.
    
    The NUMA aware slab allocator in 2.6.14 controls locality of the slab pages
    explicitly by calling alloc_pages_node().  The NUMA slab allocator manages
    slab entries by having lists of available slab pages for each node.  The
    per cpu slab cache can only contain slab entries associated with the node
    local to the processor.  This guarantees that the default allocation mode
    of the slab allocator always assigns local memory if available.
    
    Setting MPOL_INTERLEAVE as a default policy during bootup has no effect
    anymore.  In 2.6.14 all node unspecific slab allocations are performed on
    the boot processor.  This means that most of key data structures are
    allocated on one node.  Most processors will have to refer to these
    structures making the boot node a potential bottleneck.  This may reduce
    performance and cause unnecessary memory pressure on the boot node.
    
    This patch implements NUMA policies in the slab layer.  There is the need
    of explicit application of NUMA memory policies by the slab allcator itself
    since the NUMA slab allocator does no longer let the page_allocator control
    locality.
    
    The check for policies is made directly at the beginning of __cache_alloc
    using current->mempolicy.  The memory policy is already frequently checked
    by the page allocator (alloc_page_vma() and alloc_page_current()).  So it
    is highly likely that the cacheline is present.  For MPOL_INTERLEAVE
    kmalloc() will spread out each request to one node after another so that an
    equal distribution of allocations can be obtained during bootup.
    
    It is not possible to push the policy check to lower layers of the NUMA
    slab allocator since the per cpu caches are now only containing slab
    entries from the current node.  If the policy says that the local node is
    not to be preferred or forbidden then there is no point in checking the
    slab cache or local list of slab pages.  The allocation better be directed
    immediately to the lists containing slab entries for the allowed set of
    nodes.
    
    This way of applying policy also fixes another strange behavior in 2.6.13.
    alloc_pages() is controlled by the memory allocation policy of the current
    process.  It could therefore be that one process is running with
    MPOL_INTERLEAVE and would f.e.  obtain a new page following that policy
    since no slab entries are in the lists anymore.  A page can typically be
    used for multiple slab entries but lets say that the current process is
    only using one.  The other entries are then added to the slab lists.  These
    are now non local entries in the slab lists despite of the possible
    availability of local pages that would provide faster access and increase
    the performance of the application.
    
    Another process without MPOL_INTERLEAVE may now run and expect a local slab
    entry from kmalloc().  However, there are still these free slab entries
    from the off node page obtained from the other process via MPOL_INTERLEAVE
    in the cache.  The process will then get an off node slab entry although
    other slab entries may be available that are local to that process.  This
    means that the policy if one process may contaminate the locality of the
    slab caches for other processes.
    
    This patch in effect insures that a per process policy is followed for the
    allocation of slab entries and that there cannot be a memory policy
    influence from one process to another.  A process with default policy will
    always get a local slab entry if one is available.  And the process using
    memory policies will get its memory arranged as requested.  Off-node slab
    allocation will require the use of spinlocks and will make the use of per
    cpu caches not possible.  A process using memory policies to redirect
    allocations offnode will have to cope with additional lock overhead in
    addition to the latency added by the need to access a remote slab entry.
    
    Changes V1->V2
    - Remove #ifdef CONFIG_NUMA by moving forward declaration into
      prior #ifdef CONFIG_NUMA section.
    
    - Give the function determining the node number to use a saner
      name.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index d6a53ed6ab6c..bbd2221923c3 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -159,6 +159,7 @@ extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr);
+extern unsigned slab_node(struct mempolicy *policy);
 
 extern int policy_zone;
 

commit 7339ff8302fd70aabf5f1ae26e0c4905fa74a495
Author: Robin Holt <holt@sgi.com>
Date:   Sat Jan 14 13:20:48 2006 -0800

    [PATCH] Add tmpfs options for memory placement policies
    
    Anything that writes into a tmpfs filesystem is liable to disproportionately
    decrease the available memory on a particular node.  Since there's no telling
    what sort of application (e.g.  dd/cp/cat) might be dropping large files
    there, this lets the admin choose the appropriate default behavior for their
    site's situation.
    
    Introduce a tmpfs mount option which allows specifying a memory policy and
    a second option to specify the nodelist for that policy.  With the default
    policy, tmpfs will behave as it does today.  This patch adds support for
    preferred, bind, and interleave policies.
    
    The default policy will cause pages to be added to tmpfs files on the node
    which is doing the writing.  Some jobs expect a single process to create
    and manage the tmpfs files.  This results in a node which has a
    significantly reduced number of free pages.
    
    With this patch, the administrator can specify the policy and nodes for
    that policy where they would prefer allocations.
    
    This patch was originally written by Brent Casavant and Hugh Dickins.  I
    added support for the bind and preferred policies and the mpol_nodelist
    mount option.
    
    Signed-off-by: Brent Casavant <bcasavan@sgi.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index c7ac77e873b3..d6a53ed6ab6c 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -132,12 +132,8 @@ struct shared_policy {
 	spinlock_t lock;
 };
 
-static inline void mpol_shared_policy_init(struct shared_policy *info)
-{
-	info->root = RB_ROOT;
-	spin_lock_init(&info->lock);
-}
-
+void mpol_shared_policy_init(struct shared_policy *info, int policy,
+				nodemask_t *nodes);
 int mpol_set_shared_policy(struct shared_policy *info,
 				struct vm_area_struct *vma,
 				struct mempolicy *new);
@@ -211,7 +207,8 @@ static inline int mpol_set_shared_policy(struct shared_policy *info,
 	return -EINVAL;
 }
 
-static inline void mpol_shared_policy_init(struct shared_policy *info)
+static inline void mpol_shared_policy_init(struct shared_policy *info,
+					int policy, nodemask_t *nodes)
 {
 }
 

commit 4225399a66b315d4d1fb1cb61b75dda201c832e3
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:59 2006 -0800

    [PATCH] cpuset: rebind vma mempolicies fix
    
    Fix more of longstanding bug in cpuset/mempolicy interaction.
    
    NUMA mempolicies (mm/mempolicy.c) are constrained by the current tasks cpuset
    to just the Memory Nodes allowed by that cpuset.  The kernel maintains
    internal state for each mempolicy, tracking what nodes are used for the
    MPOL_INTERLEAVE, MPOL_BIND or MPOL_PREFERRED policies.
    
    When a tasks cpuset memory placement changes, whether because the cpuset
    changed, or because the task was attached to a different cpuset, then the
    tasks mempolicies have to be rebound to the new cpuset placement, so as to
    preserve the cpuset-relative numbering of the nodes in that policy.
    
    An earlier fix handled such mempolicy rebinding for mempolicies attached to a
    task.
    
    This fix rebinds mempolicies attached to vma's (address ranges in a tasks
    address space.) Due to the need to hold the task->mm->mmap_sem semaphore while
    updating vma's, the rebinding of vma mempolicies has to be done when the
    cpuset memory placement is changed, at which time mmap_sem can be safely
    acquired.  The tasks mempolicy is rebound later, when the task next attempts
    to allocate memory and notices that its task->cpuset_mems_generation is
    out-of-date with its cpusets mems_generation.
    
    Because walking the tasklist to find all tasks attached to a changing cpuset
    requires holding tasklist_lock, a spinlock, one cannot update the vma's of the
    affected tasks while doing the tasklist scan.  In general, one cannot acquire
    a semaphore (which can sleep) while already holding a spinlock (such as
    tasklist_lock).  So a list of mm references has to be built up during the
    tasklist scan, then the tasklist lock dropped, then for each mm, its mmap_sem
    acquired, and the vma's in that mm rebound.
    
    Once the tasklist lock is dropped, affected tasks may fork new tasks, before
    their mm's are rebound.  A kernel global 'cpuset_being_rebound' is set to
    point to the cpuset being rebound (there can only be one; cpuset modifications
    are done under a global 'manage_sem' semaphore), and the mpol_copy code that
    is used to copy a tasks mempolicies during fork catches such forking tasks,
    and ensures their children are also rebound.
    
    When a task is moved to a different cpuset, it is easier, as there is only one
    task involved.  It's mm->vma's are scanned, using the same
    mpol_rebind_policy() as used above.
    
    It may happen that both the mpol_copy hook and the update done via the
    tasklist scan update the same mm twice.  This is ok, as the mempolicies of
    each vma in an mm keep track of what mems_allowed they are relative to, and
    safely no-op a second request to rebind to the same nodes.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 74357cb9bc7c..c7ac77e873b3 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -150,6 +150,16 @@ extern void numa_policy_init(void);
 extern void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *new);
 extern void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new);
+extern void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new);
+#define set_cpuset_being_rebound(x) (cpuset_being_rebound = (x))
+
+#ifdef CONFIG_CPUSET
+#define current_cpuset_is_being_rebound() \
+				(cpuset_being_rebound == current->cpuset)
+#else
+#define current_cpuset_is_being_rebound() 0
+#endif
+
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr);
@@ -165,6 +175,8 @@ static inline void check_highest_zone(int k)
 int do_migrate_pages(struct mm_struct *mm,
 	const nodemask_t *from_nodes, const nodemask_t *to_nodes, int flags);
 
+extern void *cpuset_being_rebound;	/* Trigger mpol_copy vma rebind */
+
 #else
 
 struct mempolicy {};
@@ -234,6 +246,12 @@ static inline void mpol_rebind_task(struct task_struct *tsk,
 {
 }
 
+static inline void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
+{
+}
+
+#define set_cpuset_being_rebound(x) do {} while (0)
+
 static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr)
 {

commit 74cb21553f4bf244185b9bec4c26e4e3169ad55e
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:56 2006 -0800

    [PATCH] cpuset: numa_policy_rebind cleanup
    
    Cleanup, reorganize and make more robust the mempolicy.c code to rebind
    mempolicies relative to the containing cpuset after a tasks memory placement
    changes.
    
    The real motivator for this cleanup patch is to lay more groundwork for the
    upcoming patch to correctly rebind NUMA mempolicies that are attached to vma's
    after the containing cpuset memory placement changes.
    
    NUMA mempolicies are constrained by the cpuset their task is a member of.
    When either (1) a task is moved to a different cpuset, or (2) the 'mems'
    mems_allowed of a cpuset is changed, then the NUMA mempolicies have embedded
    node numbers (for MPOL_BIND, MPOL_INTERLEAVE and MPOL_PREFERRED) that need to
    be recalculated, relative to their new cpuset placement.
    
    The old code used an unreliable method of determining what was the old
    mems_allowed constraining the mempolicy.  It just looked at the tasks
    mems_allowed value.  This sort of worked with the present code, that just
    rebinds the -task- mempolicy, and leaves any -vma- mempolicies broken,
    referring to the old nodes.  But in an upcoming patch, the vma mempolicies
    will be rebound as well.  Then the order in which the various task and vma
    mempolicies are updated will no longer be deterministic, and one can no longer
    count on the task->mems_allowed holding the old value for as long as needed.
    It's not even clear if the current code was guaranteed to work reliably for
    task mempolicies.
    
    So I added a mems_allowed field to each mempolicy, stating exactly what
    mems_allowed the policy is relative to, and updated synchronously and reliably
    anytime that the mempolicy is rebound.
    
    Also removed a useless wrapper routine, numa_policy_rebind(), and had its
    caller, cpuset_update_task_memory_state(), call directly to the rewritten
    policy_rebind() routine, and made that rebind routine extern instead of
    static, and added a "mpol_" prefix to its name, making it
    mpol_rebind_policy().
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 05fddd5bee5d..74357cb9bc7c 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -68,6 +68,7 @@ struct mempolicy {
 		nodemask_t	 nodes;		/* interleave */
 		/* undefined for default */
 	} v;
+	nodemask_t cpuset_mems_allowed;	/* mempolicy relative to these nodes */
 };
 
 /*
@@ -146,7 +147,9 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
-extern void numa_policy_rebind(const nodemask_t *old, const nodemask_t *new);
+extern void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *new);
+extern void mpol_rebind_task(struct task_struct *tsk,
+					const nodemask_t *new);
 extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr);
@@ -221,7 +224,12 @@ static inline void numa_default_policy(void)
 {
 }
 
-static inline void numa_policy_rebind(const nodemask_t *old,
+static inline void mpol_rebind_policy(struct mempolicy *pol,
+					const nodemask_t *new)
+{
+}
+
+static inline void mpol_rebind_task(struct task_struct *tsk,
 					const nodemask_t *new)
 {
 }

commit 48fce3429df84a94766fbbc845fa8450d0715b48
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Jan 8 01:01:03 2006 -0800

    [PATCH] mempolicies: unexport get_vma_policy()
    
    Since the numa_maps functionality is now in mempolicy.c we no longer need to
    export get_vma_policy().
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 66247eff24a0..05fddd5bee5d 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -144,9 +144,6 @@ void mpol_free_shared_policy(struct shared_policy *p);
 struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 					    unsigned long idx);
 
-struct mempolicy *get_vma_policy(struct task_struct *task,
-			struct vm_area_struct *vma, unsigned long addr);
-
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
 extern void numa_policy_rebind(const nodemask_t *old, const nodemask_t *new);

commit 45b07ef31d1182d2cfde7711327e3afb268bb1ac
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:00:56 2006 -0800

    [PATCH] cpusets: swap migration interface
    
    Add a boolean "memory_migrate" to each cpuset, represented by a file
    containing "0" or "1" in each directory below /dev/cpuset.
    
    It defaults to false (file contains "0").  It can be set true by writing
    "1" to the file.
    
    If true, then anytime that a task is attached to the cpuset so marked, the
    pages of that task will be moved to that cpuset, preserving, to the extent
    practical, the cpuset-relative placement of the pages.
    
    Also anytime that a cpuset so marked has its memory placement changed (by
    writing to its "mems" file), the tasks in that cpuset will have their pages
    moved to the cpusets new nodes, preserving, to the extent practical, the
    cpuset-relative placement of the moved pages.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 3e61e829681d..66247eff24a0 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -235,6 +235,13 @@ static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 	return NODE_DATA(0)->node_zonelists + gfp_zone(GFP_HIGHUSER);
 }
 
+static inline int do_migrate_pages(struct mm_struct *mm,
+			const nodemask_t *from_nodes,
+			const nodemask_t *to_nodes, int flags)
+{
+	return 0;
+}
+
 static inline void check_highest_zone(int k)
 {
 }

commit 39743889aaf76725152f16aa90ca3c45f6d52da3
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Jan 8 01:00:51 2006 -0800

    [PATCH] Swap Migration V5: sys_migrate_pages interface
    
    sys_migrate_pages implementation using swap based page migration
    
    This is the original API proposed by Ray Bryant in his posts during the first
    half of 2005 on linux-mm@kvack.org and linux-kernel@vger.kernel.org.
    
    The intent of sys_migrate is to migrate memory of a process.  A process may
    have migrated to another node.  Memory was allocated optimally for the prior
    context.  sys_migrate_pages allows to shift the memory to the new node.
    
    sys_migrate_pages is also useful if the processes available memory nodes have
    changed through cpuset operations to manually move the processes memory.  Paul
    Jackson is working on an automated mechanism that will allow an automatic
    migration if the cpuset of a process is changed.  However, a user may decide
    to manually control the migration.
    
    This implementation is put into the policy layer since it uses concepts and
    functions that are also needed for mbind and friends.  The patch also provides
    a do_migrate_pages function that may be useful for cpusets to automatically
    move memory.  sys_migrate_pages does not modify policies in contrast to Ray's
    implementation.
    
    The current code here is based on the swap based page migration capability and
    thus is not able to preserve the physical layout relative to it containing
    nodeset (which may be a cpuset).  When direct page migration becomes available
    then the implementation needs to be changed to do a isomorphic move of pages
    between different nodesets.  The current implementation simply evicts all
    pages in source nodeset that are not in the target nodeset.
    
    Patch supports ia64, i386 and x86_64.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 05443a766cb8..3e61e829681d 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -162,6 +162,9 @@ static inline void check_highest_zone(int k)
 		policy_zone = k;
 }
 
+int do_migrate_pages(struct mm_struct *mm,
+	const nodemask_t *from_nodes, const nodemask_t *to_nodes, int flags);
+
 #else
 
 struct mempolicy {};

commit dc9aa5b9d65fd11b1f5246b46ec610ee8b83c6dd
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Jan 8 01:00:50 2006 -0800

    [PATCH] Swap Migration V5: MPOL_MF_MOVE interface
    
    Add page migration support via swap to the NUMA policy layer
    
    This patch adds page migration support to the NUMA policy layer.  An
    additional flag MPOL_MF_MOVE is introduced for mbind.  If MPOL_MF_MOVE is
    specified then pages that do not conform to the memory policy will be evicted
    from memory.  When they get pages back in new pages will be allocated
    following the numa policy.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index ed00b278cb93..05443a766cb8 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -22,6 +22,9 @@
 
 /* Flags for mbind */
 #define MPOL_MF_STRICT	(1<<0)	/* Verify existing pages in the mapping */
+#define MPOL_MF_MOVE	(1<<1)	/* Move pages owned by this process to conform to mapping */
+#define MPOL_MF_MOVE_ALL (1<<2)	/* Move every page to conform to mapping */
+#define MPOL_MF_INTERNAL (1<<3)	/* Internal flags start here */
 
 #ifdef __KERNEL__
 

commit 4be38e351c5f455f6f490f5aff29053e33ab4f99
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Fri Jan 6 00:11:17 2006 -0800

    [PATCH] mm: move determination of policy_zone into page allocator
    
    Currently the function to build a zonelist for a BIND policy has the side
    effect to set the policy_zone.  This seems to be a bit strange.  policy
    zone seems to not be initialized elsewhere and therefore 0.  Do we police
    ZONE_DMA if no bind policy has been used yet?
    
    This patch moves the determination of the zone to apply policies to into
    the page allocator.  We determine the zone while building the zonelist for
    nodes.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index b972f985a3c5..ed00b278cb93 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -151,6 +151,14 @@ extern struct mempolicy default_policy;
 extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 		unsigned long addr);
 
+extern int policy_zone;
+
+static inline void check_highest_zone(int k)
+{
+	if (k > policy_zone)
+		policy_zone = k;
+}
+
 #else
 
 struct mempolicy {};
@@ -221,6 +229,9 @@ static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
 	return NODE_DATA(0)->node_zonelists + gfp_zone(GFP_HIGHUSER);
 }
 
+static inline void check_highest_zone(int k)
+{
+}
 #endif /* CONFIG_NUMA */
 #endif /* __KERNEL__ */
 

commit 21abb1478a87e26f5fa71dbcb7cf4264272c2248
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Fri Jan 6 00:10:47 2006 -0800

    [PATCH] Remove old node based policy interface from mempolicy.c
    
    mempolicy.c contains provisional interface for huge page allocation based on
    node numbers.  This is in use in SLES9 but was never used (AFAIK) in upstream
    versions of Linux.
    
    Huge page allocations now use zonelists to figure out where to allocate pages.
     The use of zonelists allows us to find the closest hugepage which was the
    consideration of the NUMA distance for huge page allocations.
    
    Remove the obsolete functions.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@muc.de>
    Acked-by: William Lee Irwin III <wli@holomorphy.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Acked-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 817db6427113..b972f985a3c5 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -109,14 +109,6 @@ static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
 
 #define mpol_set_vma_default(vma) ((vma)->vm_policy = NULL)
 
-/*
- * Hugetlb policy. i386 hugetlb so far works with node numbers
- * instead of zone lists, so give it special interfaces for now.
- */
-extern int mpol_first_node(struct vm_area_struct *vma, unsigned long addr);
-extern int mpol_node_valid(int nid, struct vm_area_struct *vma,
-			unsigned long addr);
-
 /*
  * Tree of shared policies for a shared memory region.
  * Maintain the policies in a pseudo mm that contains vmas. The vmas
@@ -184,17 +176,6 @@ static inline struct mempolicy *mpol_copy(struct mempolicy *old)
 	return NULL;
 }
 
-static inline int mpol_first_node(struct vm_area_struct *vma, unsigned long a)
-{
-	return numa_node_id();
-}
-
-static inline int
-mpol_node_valid(int nid, struct vm_area_struct *vma, unsigned long a)
-{
-	return 1;
-}
-
 struct shared_policy {};
 
 static inline int mpol_set_shared_policy(struct shared_policy *info,

commit 5da7ca86078964cbfe6c83efc1205904587706fe
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Fri Jan 6 00:10:46 2006 -0800

    [PATCH] Add NUMA policy support for huge pages.
    
    The huge_zonelist() function in the memory policy layer provides an list of
    zones ordered by NUMA distance.  The hugetlb layer will walk that list looking
    for a zone that has available huge pages but is also in the nodeset of the
    current cpuset.
    
    This patch does not contain the folding of find_or_alloc_huge_page() that was
    controversial in the earlier discussion.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@muc.de>
    Acked-by: William Lee Irwin III <wli@holomorphy.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 8b67cf837ca9..817db6427113 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -156,6 +156,8 @@ extern void numa_default_policy(void);
 extern void numa_policy_init(void);
 extern void numa_policy_rebind(const nodemask_t *old, const nodemask_t *new);
 extern struct mempolicy default_policy;
+extern struct zonelist *huge_zonelist(struct vm_area_struct *vma,
+		unsigned long addr);
 
 #else
 
@@ -232,6 +234,12 @@ static inline void numa_policy_rebind(const nodemask_t *old,
 {
 }
 
+static inline struct zonelist *huge_zonelist(struct vm_area_struct *vma,
+		unsigned long addr)
+{
+	return NODE_DATA(0)->node_zonelists + gfp_zone(GFP_HIGHUSER);
+}
+
 #endif /* CONFIG_NUMA */
 #endif /* __KERNEL__ */
 

commit 68860ec10bcc07ab4f89f9d940e3b77ae5ca13b3
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Oct 30 15:02:36 2005 -0800

    [PATCH] cpusets: automatic numa mempolicy rebinding
    
    This patch automatically updates a tasks NUMA mempolicy when its cpuset
    memory placement changes.  It does so within the context of the task,
    without any need to support low level external mempolicy manipulation.
    
    If a system is not using cpusets, or if running on a system with just the
    root (all-encompassing) cpuset, then this remap is a no-op.  Only when a
    task is moved between cpusets, or a cpusets memory placement is changed
    does the following apply.  Otherwise, the main routine below,
    rebind_policy() is not even called.
    
    When mixing cpusets, scheduler affinity, and NUMA mempolicies, the
    essential role of cpusets is to place jobs (several related tasks) on a set
    of CPUs and Memory Nodes, the essential role of sched_setaffinity is to
    manage a jobs processor placement within its allowed cpuset, and the
    essential role of NUMA mempolicy (mbind, set_mempolicy) is to manage a jobs
    memory placement within its allowed cpuset.
    
    However, CPU affinity and NUMA memory placement are managed within the
    kernel using absolute system wide numbering, not cpuset relative numbering.
    
    This is ok until a job is migrated to a different cpuset, or what's the
    same, a jobs cpuset is moved to different CPUs and Memory Nodes.
    
    Then the CPU affinity and NUMA memory placement of the tasks in the job
    need to be updated, to preserve their cpuset-relative position.  This can
    be done for CPU affinity using sched_setaffinity() from user code, as one
    task can modify anothers CPU affinity.  This cannot be done from an
    external task for NUMA memory placement, as that can only be modified in
    the context of the task using it.
    
    However, it easy enough to remap a tasks NUMA mempolicy automatically when
    a task is migrated, using the existing cpuset mechanism to trigger a
    refresh of a tasks memory placement after its cpuset has changed.  All that
    is needed is the old and new nodemask, and notice to the task that it needs
    to rebind its mempolicy.  The tasks mems_allowed has the old mask, the
    tasks cpuset has the new mask, and the existing
    cpuset_update_current_mems_allowed() mechanism provides the notice.  The
    bitmap/cpumask/nodemask remap operators provide the cpuset relative
    calculations.
    
    This patch leaves open a couple of issues:
    
     1) Updating vma and shmfs/tmpfs/hugetlbfs memory policies:
    
        These mempolicies may reference nodes outside of those allowed to
        the current task by its cpuset.  Tasks are migrated as part of jobs,
        which reside on what might be several cpusets in a subtree.  When such
        a job is migrated, all NUMA memory policy references to nodes within
        that cpuset subtree should be translated, and references to any nodes
        outside that subtree should be left untouched.  A future patch will
        provide the cpuset mechanism needed to mark such subtrees.  With that
        patch, we will be able to correctly migrate these other memory policies
        across a job migration.
    
     2) Updating cpuset, affinity and memory policies in user space:
    
        This is harder.  Any placement state stored in user space using
        system-wide numbering will be invalidated across a migration.  More
        work will be required to provide user code with a migration-safe means
        to manage its cpuset relative placement, while preserving the current
        API's that pass system wide numbers, not cpuset relative numbers across
        the kernel-user boundary.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 7af8cb836e78..8b67cf837ca9 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -154,6 +154,7 @@ struct mempolicy *get_vma_policy(struct task_struct *task,
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
+extern void numa_policy_rebind(const nodemask_t *old, const nodemask_t *new);
 extern struct mempolicy default_policy;
 
 #else
@@ -226,6 +227,11 @@ static inline void numa_default_policy(void)
 {
 }
 
+static inline void numa_policy_rebind(const nodemask_t *old,
+					const nodemask_t *new)
+{
+}
+
 #endif /* CONFIG_NUMA */
 #endif /* __KERNEL__ */
 

commit b8072f099b7829a6ff3eba618e1d079a81f753f8
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:41 2005 -0700

    [PATCH] mm: update comments to pte lock
    
    Updated several references to page_table_lock in common code comments.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 38e60a099399..7af8cb836e78 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -47,8 +47,7 @@ struct vm_area_struct;
  * Locking policy for interlave:
  * In process context there is no locking because only the process accesses
  * its own state. All vma manipulation is somewhat protected by a down_read on
- * mmap_sem. For allocating in the interleave policy the page_table_lock
- * must be also aquired to protect il_next.
+ * mmap_sem.
  *
  * Freeing policy:
  * When policy is MPOL_BIND v.zonelist is kmalloc'ed and must be kfree'd.

commit dfcd3c0dc426bb75770c34b40e14f2da8845ea62
Author: Andi Kleen <ak@suse.de>
Date:   Sat Oct 29 18:15:48 2005 -0700

    [PATCH] Convert mempolicies to nodemask_t
    
    The NUMA policy code predated nodemask_t so it used open coded bitmaps.
    Convert everything to nodemask_t.  Big patch, but shouldn't have any actual
    behaviour changes (except I removed one unnecessary check against
    node_online_map and one unnecessary BUG_ON)
    
    Signed-off-by: "Andi Kleen" <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 58385ee1c0ac..38e60a099399 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -27,10 +27,10 @@
 
 #include <linux/config.h>
 #include <linux/mmzone.h>
-#include <linux/bitmap.h>
 #include <linux/slab.h>
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
+#include <linux/nodemask.h>
 
 struct vm_area_struct;
 
@@ -63,7 +63,7 @@ struct mempolicy {
 	union {
 		struct zonelist  *zonelist;	/* bind */
 		short 		 preferred_node; /* preferred */
-		DECLARE_BITMAP(nodes, MAX_NUMNODES); /* interleave */
+		nodemask_t	 nodes;		/* interleave */
 		/* undefined for default */
 	} v;
 };

commit d42c69972b853fd33a26c8c7405624be41a22136
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jul 6 19:56:03 2005 +0200

    [PATCH] PCI: Run PCI driver initialization on local node
    
    Run PCI driver initialization on local node
    
    Instead of adding messy kmalloc_node()s everywhere run the
    PCI driver probe on the node local to the device.
    
    This would not have helped for IDE, but should for
    other more clean drivers that do more initialization in probe().
    It won't help for drivers that do most of the work
    on first open (like many network drivers)
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 94a46f38c532..58385ee1c0ac 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -155,6 +155,7 @@ struct mempolicy *get_vma_policy(struct task_struct *task,
 
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
+extern struct mempolicy default_policy;
 
 #else
 

commit 6e21c8f145f5052c1c2fb4a4b41bee01c848159b
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Sat Sep 3 15:54:45 2005 -0700

    [PATCH] /proc/<pid>/numa_maps to show on which nodes pages reside
    
    This patch was recently discussed on linux-mm:
    http://marc.theaimsgroup.com/?t=112085728500002&r=1&w=2
    
    I inherited a large code base from Ray for page migration.  There was a
    small patch in there that I find to be very useful since it allows the
    display of the locality of the pages in use by a process.  I reworked that
    patch and came up with a /proc/<pid>/numa_maps that gives more information
    about the vma's of a process.  numa_maps is indexes by the start address
    found in /proc/<pid>/maps.  F.e.  with this patch you can see the page use
    of the "getty" process:
    
    margin:/proc/12008 # cat maps
    00000000-00004000 r--p 00000000 00:00 0
    2000000000000000-200000000002c000 r-xp 00000000 08:04 516                /lib/ld-2.3.3.so
    2000000000038000-2000000000040000 rw-p 00028000 08:04 516                /lib/ld-2.3.3.so
    2000000000040000-2000000000044000 rw-p 2000000000040000 00:00 0
    2000000000058000-2000000000260000 r-xp 00000000 08:04 54707842           /lib/tls/libc.so.6.1
    2000000000260000-2000000000268000 ---p 00208000 08:04 54707842           /lib/tls/libc.so.6.1
    2000000000268000-2000000000274000 rw-p 00200000 08:04 54707842           /lib/tls/libc.so.6.1
    2000000000274000-2000000000280000 rw-p 2000000000274000 00:00 0
    2000000000280000-20000000002b4000 r--p 00000000 08:04 9126923            /usr/lib/locale/en_US.utf8/LC_CTYPE
    2000000000300000-2000000000308000 r--s 00000000 08:04 60071467           /usr/lib/gconv/gconv-modules.cache
    2000000000318000-2000000000328000 rw-p 2000000000318000 00:00 0
    4000000000000000-4000000000008000 r-xp 00000000 08:04 29576399           /sbin/mingetty
    6000000000004000-6000000000008000 rw-p 00004000 08:04 29576399           /sbin/mingetty
    6000000000008000-600000000002c000 rw-p 6000000000008000 00:00 0          [heap]
    60000fff7fffc000-60000fff80000000 rw-p 60000fff7fffc000 00:00 0
    60000ffffff44000-60000ffffff98000 rw-p 60000ffffff44000 00:00 0          [stack]
    a000000000000000-a000000000020000 ---p 00000000 00:00 0                  [vdso]
    
    cat numa_maps
    2000000000000000 default MaxRef=43 Pages=11 Mapped=11 N0=4 N1=3 N2=2 N3=2
    2000000000038000 default MaxRef=1 Pages=2 Mapped=2 Anon=2 N0=2
    2000000000040000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N0=1
    2000000000058000 default MaxRef=43 Pages=61 Mapped=61 N0=14 N1=15 N2=16 N3=16
    2000000000268000 default MaxRef=1 Pages=2 Mapped=2 Anon=2 N0=2
    2000000000274000 default MaxRef=1 Pages=3 Mapped=3 Anon=3 N0=3
    2000000000280000 default MaxRef=8 Pages=3 Mapped=3 N0=3
    2000000000300000 default MaxRef=8 Pages=2 Mapped=2 N0=2
    2000000000318000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N2=1
    4000000000000000 default MaxRef=6 Pages=2 Mapped=2 N1=2
    6000000000004000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N0=1
    6000000000008000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N0=1
    60000fff7fffc000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N0=1
    60000ffffff44000 default MaxRef=1 Pages=1 Mapped=1 Anon=1 N0=1
    
    getty uses ld.so.  The first vma is the code segment which is used by 43
    other processes and the pages are evenly distributed over the 4 nodes.
    
    The second vma is the process specific data portion for ld.so.  This is
    only one page.
    
    The display format is:
    
    <startaddress>   Links to information in /proc/<pid>/map
    <memory policy>  This can be "default" "interleave={}", "prefer=<node>" or "bind={<zones>}"
    MaxRef=         <maximum reference to a page in this vma>
    Pages=          <Nr of pages in use>
    Mapped=         <Nr of pages with mapcount >
    Anon=           <nr of anonymous pages>
    Nx=             <Nr of pages on Node x>
    
    The content of the proc-file is self-evident.  If this would be tied into
    the sparsemem system then the contents of this file would not be too
    useful.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 8480aef10e62..94a46f38c532 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -150,6 +150,9 @@ void mpol_free_shared_policy(struct shared_policy *p);
 struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
 					    unsigned long idx);
 
+struct mempolicy *get_vma_policy(struct task_struct *task,
+			struct vm_area_struct *vma, unsigned long addr);
+
 extern void numa_default_policy(void);
 extern void numa_policy_init(void);
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
new file mode 100644
index 000000000000..8480aef10e62
--- /dev/null
+++ b/include/linux/mempolicy.h
@@ -0,0 +1,229 @@
+#ifndef _LINUX_MEMPOLICY_H
+#define _LINUX_MEMPOLICY_H 1
+
+#include <linux/errno.h>
+
+/*
+ * NUMA memory policies for Linux.
+ * Copyright 2003,2004 Andi Kleen SuSE Labs
+ */
+
+/* Policies */
+#define MPOL_DEFAULT	0
+#define MPOL_PREFERRED	1
+#define MPOL_BIND	2
+#define MPOL_INTERLEAVE	3
+
+#define MPOL_MAX MPOL_INTERLEAVE
+
+/* Flags for get_mem_policy */
+#define MPOL_F_NODE	(1<<0)	/* return next IL mode instead of node mask */
+#define MPOL_F_ADDR	(1<<1)	/* look up vma using address */
+
+/* Flags for mbind */
+#define MPOL_MF_STRICT	(1<<0)	/* Verify existing pages in the mapping */
+
+#ifdef __KERNEL__
+
+#include <linux/config.h>
+#include <linux/mmzone.h>
+#include <linux/bitmap.h>
+#include <linux/slab.h>
+#include <linux/rbtree.h>
+#include <linux/spinlock.h>
+
+struct vm_area_struct;
+
+#ifdef CONFIG_NUMA
+
+/*
+ * Describe a memory policy.
+ *
+ * A mempolicy can be either associated with a process or with a VMA.
+ * For VMA related allocations the VMA policy is preferred, otherwise
+ * the process policy is used. Interrupts ignore the memory policy
+ * of the current process.
+ *
+ * Locking policy for interlave:
+ * In process context there is no locking because only the process accesses
+ * its own state. All vma manipulation is somewhat protected by a down_read on
+ * mmap_sem. For allocating in the interleave policy the page_table_lock
+ * must be also aquired to protect il_next.
+ *
+ * Freeing policy:
+ * When policy is MPOL_BIND v.zonelist is kmalloc'ed and must be kfree'd.
+ * All other policies don't have any external state. mpol_free() handles this.
+ *
+ * Copying policy objects:
+ * For MPOL_BIND the zonelist must be always duplicated. mpol_clone() does this.
+ */
+struct mempolicy {
+	atomic_t refcnt;
+	short policy; 	/* See MPOL_* above */
+	union {
+		struct zonelist  *zonelist;	/* bind */
+		short 		 preferred_node; /* preferred */
+		DECLARE_BITMAP(nodes, MAX_NUMNODES); /* interleave */
+		/* undefined for default */
+	} v;
+};
+
+/*
+ * Support for managing mempolicy data objects (clone, copy, destroy)
+ * The default fast path of a NULL MPOL_DEFAULT policy is always inlined.
+ */
+
+extern void __mpol_free(struct mempolicy *pol);
+static inline void mpol_free(struct mempolicy *pol)
+{
+	if (pol)
+		__mpol_free(pol);
+}
+
+extern struct mempolicy *__mpol_copy(struct mempolicy *pol);
+static inline struct mempolicy *mpol_copy(struct mempolicy *pol)
+{
+	if (pol)
+		pol = __mpol_copy(pol);
+	return pol;
+}
+
+#define vma_policy(vma) ((vma)->vm_policy)
+#define vma_set_policy(vma, pol) ((vma)->vm_policy = (pol))
+
+static inline void mpol_get(struct mempolicy *pol)
+{
+	if (pol)
+		atomic_inc(&pol->refcnt);
+}
+
+extern int __mpol_equal(struct mempolicy *a, struct mempolicy *b);
+static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
+{
+	if (a == b)
+		return 1;
+	return __mpol_equal(a, b);
+}
+#define vma_mpol_equal(a,b) mpol_equal(vma_policy(a), vma_policy(b))
+
+/* Could later add inheritance of the process policy here. */
+
+#define mpol_set_vma_default(vma) ((vma)->vm_policy = NULL)
+
+/*
+ * Hugetlb policy. i386 hugetlb so far works with node numbers
+ * instead of zone lists, so give it special interfaces for now.
+ */
+extern int mpol_first_node(struct vm_area_struct *vma, unsigned long addr);
+extern int mpol_node_valid(int nid, struct vm_area_struct *vma,
+			unsigned long addr);
+
+/*
+ * Tree of shared policies for a shared memory region.
+ * Maintain the policies in a pseudo mm that contains vmas. The vmas
+ * carry the policy. As a special twist the pseudo mm is indexed in pages, not
+ * bytes, so that we can work with shared memory segments bigger than
+ * unsigned long.
+ */
+
+struct sp_node {
+	struct rb_node nd;
+	unsigned long start, end;
+	struct mempolicy *policy;
+};
+
+struct shared_policy {
+	struct rb_root root;
+	spinlock_t lock;
+};
+
+static inline void mpol_shared_policy_init(struct shared_policy *info)
+{
+	info->root = RB_ROOT;
+	spin_lock_init(&info->lock);
+}
+
+int mpol_set_shared_policy(struct shared_policy *info,
+				struct vm_area_struct *vma,
+				struct mempolicy *new);
+void mpol_free_shared_policy(struct shared_policy *p);
+struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
+					    unsigned long idx);
+
+extern void numa_default_policy(void);
+extern void numa_policy_init(void);
+
+#else
+
+struct mempolicy {};
+
+static inline int mpol_equal(struct mempolicy *a, struct mempolicy *b)
+{
+	return 1;
+}
+#define vma_mpol_equal(a,b) 1
+
+#define mpol_set_vma_default(vma) do {} while(0)
+
+static inline void mpol_free(struct mempolicy *p)
+{
+}
+
+static inline void mpol_get(struct mempolicy *pol)
+{
+}
+
+static inline struct mempolicy *mpol_copy(struct mempolicy *old)
+{
+	return NULL;
+}
+
+static inline int mpol_first_node(struct vm_area_struct *vma, unsigned long a)
+{
+	return numa_node_id();
+}
+
+static inline int
+mpol_node_valid(int nid, struct vm_area_struct *vma, unsigned long a)
+{
+	return 1;
+}
+
+struct shared_policy {};
+
+static inline int mpol_set_shared_policy(struct shared_policy *info,
+					struct vm_area_struct *vma,
+					struct mempolicy *new)
+{
+	return -EINVAL;
+}
+
+static inline void mpol_shared_policy_init(struct shared_policy *info)
+{
+}
+
+static inline void mpol_free_shared_policy(struct shared_policy *p)
+{
+}
+
+static inline struct mempolicy *
+mpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)
+{
+	return NULL;
+}
+
+#define vma_policy(vma) NULL
+#define vma_set_policy(vma, pol) do {} while(0)
+
+static inline void numa_policy_init(void)
+{
+}
+
+static inline void numa_default_policy(void)
+{
+}
+
+#endif /* CONFIG_NUMA */
+#endif /* __KERNEL__ */
+
+#endif
