commit 7878c231dae05bae9dcf2ad4d309f02e51625033
Author: Qian Cai <cai@lca.pw>
Date:   Thu May 16 15:57:41 2019 -0400

    slab: remove /proc/slab_allocators
    
    It turned out that DEBUG_SLAB_LEAK is still broken even after recent
    recue efforts that when there is a large number of objects like
    kmemleak_object which is normal on a debug kernel,
    
      # grep kmemleak /proc/slabinfo
      kmemleak_object   2243606 3436210 ...
    
    reading /proc/slab_allocators could easily loop forever while processing
    the kmemleak_object cache and any additional freeing or allocating
    objects will trigger a reprocessing. To make a situation worse,
    soft-lockups could easily happen in this sitatuion which will call
    printk() to allocate more kmemleak objects to guarantee an infinite
    loop.
    
    Also, since it seems no one had noticed when it was totally broken
    more than 2-year ago - see the commit fcf88917dd43 ("slab: fix a crash
    by reading /proc/slab_allocators"), probably nobody cares about it
    anymore due to the decline of the SLAB. Just remove it entirely.
    
    Suggested-by: Vlastimil Babka <vbabka@suse.cz>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 9a5eafb7145b..abc7de77b988 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -61,9 +61,6 @@ struct kmem_cache {
 	atomic_t allocmiss;
 	atomic_t freehit;
 	atomic_t freemiss;
-#ifdef CONFIG_DEBUG_SLAB_LEAK
-	atomic_t store_user_clean;
-#endif
 
 	/*
 	 * If debugging is enabled, then the allocator can add additional

commit 5b7c4148222d7acaa1612e5eec84fc66c88d54f3
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:46 2018 -0800

    mm: move obj_to_index to include/linux/slab_def.h
    
    While with SLUB we can actually preassign tags for caches with contructors
    and store them in pointers in the freelist, SLAB doesn't allow that since
    the freelist is stored as an array of indexes, so there are no pointers to
    store the tags.
    
    Instead we compute the tag twice, once when a slab is created before
    calling the constructor and then again each time when an object is
    allocated with kmalloc.  Tag is computed simply by taking the lowest byte
    of the index that corresponds to the object.  However in kasan_kmalloc we
    only have access to the objects pointer, so we need a way to find out
    which index this object corresponds to.
    
    This patch moves obj_to_index from slab.c to include/linux/slab_def.h to
    be reused by KASAN.
    
    Link: http://lkml.kernel.org/r/c02cd9e574cfd93858e43ac94b05e38f891fef64.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 3485c58cfd1c..9a5eafb7145b 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -104,4 +104,17 @@ static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
 		return object;
 }
 
+/*
+ * We want to avoid an expensive divide : (offset / cache->size)
+ *   Using the fact that size is a constant for a particular cache,
+ *   we can replace (offset / cache->size) by
+ *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
+ */
+static inline unsigned int obj_to_index(const struct kmem_cache *cache,
+					const struct page *page, void *obj)
+{
+	u32 offset = (obj - page->s_mem);
+	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
+}
+
 #endif	/* _LINUX_SLAB_DEF_H */

commit 05fec35ebb0e0e1f4603cc241fdf65fd3abb3092
Author: Baoquan He <bhe@redhat.com>
Date:   Thu Jun 7 17:05:24 2018 -0700

    slab: clean up the code comment in slab kmem_cache struct
    
    In commit 3b0efdfa1e7 ("mm, sl[aou]b: Extract common fields from struct
    kmem_cache") the variable 'obj_size' was moved above, however the
    related code comment is not updated accordingly.  Do it here.
    
    Link: http://lkml.kernel.org/r/20180603032402.27526-1-bhe@redhat.com
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index d9228e4d0320..3485c58cfd1c 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -67,9 +67,10 @@ struct kmem_cache {
 
 	/*
 	 * If debugging is enabled, then the allocator can add additional
-	 * fields and/or padding to every object. size contains the total
-	 * object size including these internal fields, the following two
-	 * variables contain the offset to the user object and its size.
+	 * fields and/or padding to every object. 'size' contains the total
+	 * object size including these internal fields, while 'obj_offset'
+	 * and 'object_size' contain the offset to the user object and its
+	 * size.
 	 */
 	int obj_offset;
 #endif /* CONFIG_DEBUG_SLAB */

commit 7bbdb81ee3de73f2381ceec1bbee831f4c913b5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:31 2018 -0700

    slab: make usercopy region 32-bit
    
    If kmem case sizes are 32-bit, then usecopy region should be too.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-21-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 7385547c04b1..d9228e4d0320 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -85,8 +85,8 @@ struct kmem_cache {
 	unsigned int *random_seq;
 #endif
 
-	size_t useroffset;		/* Usercopy region offset */
-	size_t usersize;		/* Usercopy region size */
+	unsigned int useroffset;	/* Usercopy region offset */
+	unsigned int usersize;		/* Usercopy region size */
 
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };

commit 8eb8284b412906181357c2b0110d879d5af95e52
Author: David Windsor <dave@nullcore.net>
Date:   Sat Jun 10 22:50:28 2017 -0400

    usercopy: Prepare for usercopy whitelisting
    
    This patch prepares the slab allocator to handle caches having annotations
    (useroffset and usersize) defining usercopy regions.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on
    my understanding of the code. Changes or omissions from the original
    code are mine and don't reflect the original grsecurity/PaX code.
    
    Currently, hardened usercopy performs dynamic bounds checking on slab
    cache objects. This is good, but still leaves a lot of kernel memory
    available to be copied to/from userspace in the face of bugs. To further
    restrict what memory is available for copying, this creates a way to
    whitelist specific areas of a given slab cache object for copying to/from
    userspace, allowing much finer granularity of access control. Slab caches
    that are never exposed to userspace can declare no whitelist for their
    objects, thereby keeping them unavailable to userspace via dynamic copy
    operations. (Note, an implicit form of whitelisting is the use of constant
    sizes in usercopy operations and get_user()/put_user(); these bypass
    hardened usercopy checks since these sizes cannot change at runtime.)
    
    To support this whitelist annotation, usercopy region offset and size
    members are added to struct kmem_cache. The slab allocator receives a
    new function, kmem_cache_create_usercopy(), that creates a new cache
    with a usercopy region defined, suitable for declaring spans of fields
    within the objects that get copied to/from userspace.
    
    In this patch, the default kmem_cache_create() marks the entire allocation
    as whitelisted, leaving it semantically unchanged. Once all fine-grained
    whitelists have been added (in subsequent patches), this will be changed
    to a usersize of 0, making caches created with kmem_cache_create() not
    copyable to/from userspace.
    
    After the entire usercopy whitelist series is applied, less than 15%
    of the slab cache memory remains exposed to potential usercopy bugs
    after a fresh boot:
    
    Total Slab Memory:           48074720
    Usercopyable Memory:          6367532  13.2%
             task_struct                    0.2%         4480/1630720
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%       269760/8740224
             dentry                        11.1%       585984/5273856
             mm_struct                     29.1%         54912/188448
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          81920/81920
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        167936/167936
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        455616/455616
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        812032/812032
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1310720/1310720
    
    After some kernel build workloads, the percentage (mainly driven by
    dentry and inode caches expanding) drops under 10%:
    
    Total Slab Memory:           95516184
    Usercopyable Memory:          8497452   8.8%
             task_struct                    0.2%         4000/1456000
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%     1217280/39439872
             dentry                        11.1%     1623200/14608800
             mm_struct                     29.1%         73216/251264
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          94208/94208
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        245760/245760
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        563520/563520
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        794624/794624
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1257472/1257472
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split out a few extra kmalloc hunks]
    [kees: add field names to function declarations]
    [kees: convert BUGs to WARNs and fail closed]
    [kees: add attack surface reduction analysis to commit log]
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: linux-xfs@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Christoph Lameter <cl@linux.com>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 072e46e9e1d5..7385547c04b1 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -85,6 +85,9 @@ struct kmem_cache {
 	unsigned int *random_seq;
 #endif
 
+	size_t useroffset;		/* Usercopy region offset */
+	size_t usersize;		/* Usercopy region size */
+
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 

commit d50112edde1d0c621520e53747044009f11c656b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:18 2017 -0800

    slab, slub, slob: add slab_flags_t
    
    Add sparse-checked slab_flags_t for struct kmem_cache::flags (SLAB_POISON,
    etc).
    
    SLAB is bloated temporarily by switching to "unsigned long", but only
    temporarily.
    
    Link: http://lkml.kernel.org/r/20171021100225.GA22428@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8f7d2b1656d2..072e46e9e1d5 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -20,7 +20,7 @@ struct kmem_cache {
 	struct reciprocal_value reciprocal_buffer_size;
 /* 2) touched by every alloc & free from the backend */
 
-	unsigned int flags;		/* constant flags */
+	slab_flags_t flags;		/* constant flags */
 	unsigned int num;		/* # of objs per slab */
 
 /* 3) cache_grow/shrink */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 4ad2c5a26399..8f7d2b1656d2 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_SLAB_DEF_H
 #define	_LINUX_SLAB_DEF_H
 

commit 80a9201a5965f4715d5c09790862e0df84ce0614
Author: Alexander Potapenko <glider@google.com>
Date:   Thu Jul 28 15:49:07 2016 -0700

    mm, kasan: switch SLUB to stackdepot, enable memory quarantine for SLUB
    
    For KASAN builds:
     - switch SLUB allocator to using stackdepot instead of storing the
       allocation/deallocation stacks in the objects;
     - change the freelist hook so that parts of the freelist can be put
       into the quarantine.
    
    [aryabinin@virtuozzo.com: fixes]
      Link: http://lkml.kernel.org/r/1468601423-28676-1-git-send-email-aryabinin@virtuozzo.com
    Link: http://lkml.kernel.org/r/1468347165-41906-3-git-send-email-glider@google.com
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Kuthonuzo Luruo <kuthonuzo.luruo@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 339ba027ade9..4ad2c5a26399 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -88,7 +88,8 @@ struct kmem_cache {
 };
 
 static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
-				void *x) {
+				void *x)
+{
 	void *object = x - (x - page->s_mem) % cache->size;
 	void *last_object = page->s_mem + (cache->num - 1) * cache->size;
 

commit 7c00fce98c3e15334a603925b41aa49f76e83227
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Jul 26 15:21:56 2016 -0700

    mm: reorganize SLAB freelist randomization
    
    The kernel heap allocators are using a sequential freelist making their
    allocation predictable.  This predictability makes kernel heap overflow
    easier to exploit.  An attacker can careful prepare the kernel heap to
    control the following chunk overflowed.
    
    For example these attacks exploit the predictability of the heap:
     - Linux Kernel CAN SLUB overflow (https://goo.gl/oMNWkU)
     - Exploiting Linux Kernel Heap corruptions (http://goo.gl/EXLn95)
    
    ***Problems that needed solving:
     - Randomize the Freelist (singled linked) used in the SLUB allocator.
     - Ensure good performance to encourage usage.
     - Get best entropy in early boot stage.
    
    ***Parts:
     - 01/02 Reorganize the SLAB Freelist randomization to share elements
       with the SLUB implementation.
     - 02/02 The SLUB Freelist randomization implementation. Similar approach
       than the SLAB but tailored to the singled freelist used in SLUB.
    
    ***Performance data:
    
    slab_test impact is between 3% to 4% on average for 100000 attempts
    without smp.  It is a very focused testing, kernbench show the overall
    impact on the system is way lower.
    
    Before:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      100000 times kmalloc(8) -> 49 cycles kfree -> 77 cycles
      100000 times kmalloc(16) -> 51 cycles kfree -> 79 cycles
      100000 times kmalloc(32) -> 53 cycles kfree -> 83 cycles
      100000 times kmalloc(64) -> 62 cycles kfree -> 90 cycles
      100000 times kmalloc(128) -> 81 cycles kfree -> 97 cycles
      100000 times kmalloc(256) -> 98 cycles kfree -> 121 cycles
      100000 times kmalloc(512) -> 95 cycles kfree -> 122 cycles
      100000 times kmalloc(1024) -> 96 cycles kfree -> 126 cycles
      100000 times kmalloc(2048) -> 115 cycles kfree -> 140 cycles
      100000 times kmalloc(4096) -> 149 cycles kfree -> 171 cycles
      2. Kmalloc: alloc/free test
      100000 times kmalloc(8)/kfree -> 70 cycles
      100000 times kmalloc(16)/kfree -> 70 cycles
      100000 times kmalloc(32)/kfree -> 70 cycles
      100000 times kmalloc(64)/kfree -> 70 cycles
      100000 times kmalloc(128)/kfree -> 70 cycles
      100000 times kmalloc(256)/kfree -> 69 cycles
      100000 times kmalloc(512)/kfree -> 70 cycles
      100000 times kmalloc(1024)/kfree -> 73 cycles
      100000 times kmalloc(2048)/kfree -> 72 cycles
      100000 times kmalloc(4096)/kfree -> 71 cycles
    
    After:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      100000 times kmalloc(8) -> 57 cycles kfree -> 78 cycles
      100000 times kmalloc(16) -> 61 cycles kfree -> 81 cycles
      100000 times kmalloc(32) -> 76 cycles kfree -> 93 cycles
      100000 times kmalloc(64) -> 83 cycles kfree -> 94 cycles
      100000 times kmalloc(128) -> 106 cycles kfree -> 107 cycles
      100000 times kmalloc(256) -> 118 cycles kfree -> 117 cycles
      100000 times kmalloc(512) -> 114 cycles kfree -> 116 cycles
      100000 times kmalloc(1024) -> 115 cycles kfree -> 118 cycles
      100000 times kmalloc(2048) -> 147 cycles kfree -> 131 cycles
      100000 times kmalloc(4096) -> 214 cycles kfree -> 161 cycles
      2. Kmalloc: alloc/free test
      100000 times kmalloc(8)/kfree -> 66 cycles
      100000 times kmalloc(16)/kfree -> 66 cycles
      100000 times kmalloc(32)/kfree -> 66 cycles
      100000 times kmalloc(64)/kfree -> 66 cycles
      100000 times kmalloc(128)/kfree -> 65 cycles
      100000 times kmalloc(256)/kfree -> 67 cycles
      100000 times kmalloc(512)/kfree -> 67 cycles
      100000 times kmalloc(1024)/kfree -> 64 cycles
      100000 times kmalloc(2048)/kfree -> 67 cycles
      100000 times kmalloc(4096)/kfree -> 67 cycles
    
    Kernbench, before:
    
      Average Optimal load -j 12 Run (std deviation):
      Elapsed Time 101.873 (1.16069)
      User Time 1045.22 (1.60447)
      System Time 88.969 (0.559195)
      Percent CPU 1112.9 (13.8279)
      Context Switches 189140 (2282.15)
      Sleeps 99008.6 (768.091)
    
    After:
    
      Average Optimal load -j 12 Run (std deviation):
      Elapsed Time 102.47 (0.562732)
      User Time 1045.3 (1.34263)
      System Time 88.311 (0.342554)
      Percent CPU 1105.8 (6.49444)
      Context Switches 189081 (2355.78)
      Sleeps 99231.5 (800.358)
    
    This patch (of 2):
    
    This commit reorganizes the previous SLAB freelist randomization to
    prepare for the SLUB implementation.  It moves functions that will be
    shared to slab_common.
    
    The entropy functions are changed to align with the SLUB implementation,
    now using get_random_(int|long) functions.  These functions were chosen
    because they provide a bit more entropy early on boot and better
    performance when specific arch instructions are not available.
    
    [akpm@linux-foundation.org: fix build]
    Link: http://lkml.kernel.org/r/1464295031-26375-2-git-send-email-thgarnie@google.com
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8694f7a5d92b..339ba027ade9 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -81,7 +81,7 @@ struct kmem_cache {
 #endif
 
 #ifdef CONFIG_SLAB_FREELIST_RANDOM
-	void *random_seq;
+	unsigned int *random_seq;
 #endif
 
 	struct kmem_cache_node *node[MAX_NUMNODES];

commit c7ce4f60ac199fb3521c5fcd64da21cee801ec2b
Author: Thomas Garnier <thgarnie@google.com>
Date:   Thu May 19 17:10:37 2016 -0700

    mm: SLAB freelist randomization
    
    Provides an optional config (CONFIG_SLAB_FREELIST_RANDOM) to randomize
    the SLAB freelist.  The list is randomized during initialization of a
    new set of pages.  The order on different freelist sizes is pre-computed
    at boot for performance.  Each kmem_cache has its own randomized
    freelist.  Before pre-computed lists are available freelists are
    generated dynamically.  This security feature reduces the predictability
    of the kernel SLAB allocator against heap overflows rendering attacks
    much less stable.
    
    For example this attack against SLUB (also applicable against SLAB)
    would be affected:
    
      https://jon.oberheide.org/blog/2010/09/10/linux-kernel-can-slub-overflow/
    
    Also, since v4.6 the freelist was moved at the end of the SLAB.  It
    means a controllable heap is opened to new attacks not yet publicly
    discussed.  A kernel heap overflow can be transformed to multiple
    use-after-free.  This feature makes this type of attack harder too.
    
    To generate entropy, we use get_random_bytes_arch because 0 bits of
    entropy is available in the boot stage.  In the worse case this function
    will fallback to the get_random_bytes sub API.  We also generate a shift
    random number to shift pre-computed freelist for each new set of pages.
    
    The config option name is not specific to the SLAB as this approach will
    be extended to other allocators like SLUB.
    
    Performance results highlighted no major changes:
    
    Hackbench (running 90 10 times):
    
      Before average: 0.0698
      After average: 0.0663 (-5.01%)
    
    slab_test 1 run on boot.  Difference only seen on the 2048 size test
    being the worse case scenario covered by freelist randomization.  New
    slab pages are constantly being created on the 10000 allocations.
    Variance should be mainly due to getting new pages every few
    allocations.
    
    Before:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      10000 times kmalloc(8) -> 99 cycles kfree -> 112 cycles
      10000 times kmalloc(16) -> 109 cycles kfree -> 140 cycles
      10000 times kmalloc(32) -> 129 cycles kfree -> 137 cycles
      10000 times kmalloc(64) -> 141 cycles kfree -> 141 cycles
      10000 times kmalloc(128) -> 152 cycles kfree -> 148 cycles
      10000 times kmalloc(256) -> 195 cycles kfree -> 167 cycles
      10000 times kmalloc(512) -> 257 cycles kfree -> 199 cycles
      10000 times kmalloc(1024) -> 393 cycles kfree -> 251 cycles
      10000 times kmalloc(2048) -> 649 cycles kfree -> 228 cycles
      10000 times kmalloc(4096) -> 806 cycles kfree -> 370 cycles
      10000 times kmalloc(8192) -> 814 cycles kfree -> 411 cycles
      10000 times kmalloc(16384) -> 892 cycles kfree -> 455 cycles
      2. Kmalloc: alloc/free test
      10000 times kmalloc(8)/kfree -> 121 cycles
      10000 times kmalloc(16)/kfree -> 121 cycles
      10000 times kmalloc(32)/kfree -> 121 cycles
      10000 times kmalloc(64)/kfree -> 121 cycles
      10000 times kmalloc(128)/kfree -> 121 cycles
      10000 times kmalloc(256)/kfree -> 119 cycles
      10000 times kmalloc(512)/kfree -> 119 cycles
      10000 times kmalloc(1024)/kfree -> 119 cycles
      10000 times kmalloc(2048)/kfree -> 119 cycles
      10000 times kmalloc(4096)/kfree -> 121 cycles
      10000 times kmalloc(8192)/kfree -> 119 cycles
      10000 times kmalloc(16384)/kfree -> 119 cycles
    
    After:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      10000 times kmalloc(8) -> 130 cycles kfree -> 86 cycles
      10000 times kmalloc(16) -> 118 cycles kfree -> 86 cycles
      10000 times kmalloc(32) -> 121 cycles kfree -> 85 cycles
      10000 times kmalloc(64) -> 176 cycles kfree -> 102 cycles
      10000 times kmalloc(128) -> 178 cycles kfree -> 100 cycles
      10000 times kmalloc(256) -> 205 cycles kfree -> 109 cycles
      10000 times kmalloc(512) -> 262 cycles kfree -> 136 cycles
      10000 times kmalloc(1024) -> 342 cycles kfree -> 157 cycles
      10000 times kmalloc(2048) -> 701 cycles kfree -> 238 cycles
      10000 times kmalloc(4096) -> 803 cycles kfree -> 364 cycles
      10000 times kmalloc(8192) -> 835 cycles kfree -> 404 cycles
      10000 times kmalloc(16384) -> 896 cycles kfree -> 441 cycles
      2. Kmalloc: alloc/free test
      10000 times kmalloc(8)/kfree -> 121 cycles
      10000 times kmalloc(16)/kfree -> 121 cycles
      10000 times kmalloc(32)/kfree -> 123 cycles
      10000 times kmalloc(64)/kfree -> 142 cycles
      10000 times kmalloc(128)/kfree -> 121 cycles
      10000 times kmalloc(256)/kfree -> 119 cycles
      10000 times kmalloc(512)/kfree -> 119 cycles
      10000 times kmalloc(1024)/kfree -> 119 cycles
      10000 times kmalloc(2048)/kfree -> 119 cycles
      10000 times kmalloc(4096)/kfree -> 119 cycles
      10000 times kmalloc(8192)/kfree -> 119 cycles
      10000 times kmalloc(16384)/kfree -> 119 cycles
    
    [akpm@linux-foundation.org: propagate gfp_t into cache_random_seq_create()]
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 9edbbf352340..8694f7a5d92b 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -80,6 +80,10 @@ struct kmem_cache {
 	struct kasan_cache kasan_info;
 #endif
 
+#ifdef CONFIG_SLAB_FREELIST_RANDOM
+	void *random_seq;
+#endif
+
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 

commit 7ed2f9e663854db313f177a511145630e398b402
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:21:59 2016 -0700

    mm, kasan: SLAB support
    
    Add KASAN hooks to SLAB allocator.
    
    This patch is based on the "mm: kasan: unified support for SLUB and SLAB
    allocators" patch originally prepared by Dmitry Chernenkov.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index e878ba35ae91..9edbbf352340 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -76,8 +76,22 @@ struct kmem_cache {
 #ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;
 #endif
+#ifdef CONFIG_KASAN
+	struct kasan_cache kasan_info;
+#endif
 
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
+static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
+				void *x) {
+	void *object = x - (x - page->s_mem) % cache->size;
+	void *last_object = page->s_mem + (cache->num - 1) * cache->size;
+
+	if (unlikely(object > last_object))
+		return last_object;
+	else
+		return object;
+}
+
 #endif	/* _LINUX_SLAB_DEF_H */

commit d31676dfde257cb2b3e52d4e657d8ad2251e4d49
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Mar 15 14:54:24 2016 -0700

    mm/slab: alternative implementation for DEBUG_SLAB_LEAK
    
    DEBUG_SLAB_LEAK is a debug option.  It's current implementation requires
    status buffer so we need more memory to use it.  And, it cause
    kmem_cache initialization step more complex.
    
    To remove this extra memory usage and to simplify initialization step,
    this patch implement this feature with another way.
    
    When user requests to get slab object owner information, it marks that
    getting information is started.  And then, all free objects in caches
    are flushed to corresponding slab page.  Now, we can distinguish all
    freed object so we can know all allocated objects, too.  After
    collecting slab object owner information on allocated objects, mark is
    checked that there is no free during the processing.  If true, we can be
    sure that our information is correct so information is returned to user.
    
    Although this way is rather complex, it has two important benefits
    mentioned above.  So, I think it is worth changing.
    
    There is one drawback that it takes more time to get slab object owner
    information but it is just a debug option so it doesn't matter at all.
    
    To help review, this patch implements new way only.  Following patch
    will remove useless code.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index cf139d3fa513..e878ba35ae91 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -60,6 +60,9 @@ struct kmem_cache {
 	atomic_t allocmiss;
 	atomic_t freehit;
 	atomic_t freemiss;
+#ifdef CONFIG_DEBUG_SLAB_LEAK
+	atomic_t store_user_clean;
+#endif
 
 	/*
 	 * If debugging is enabled, then the allocator can add additional

commit 127424c86bb6cb87f0b563d9fdcfbbaf3c86ecec
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 20 15:02:32 2016 -0800

    mm: memcontrol: move kmem accounting code to CONFIG_MEMCG
    
    The cgroup2 memory controller will account important in-kernel memory
    consumers per default.  Move all necessary components to CONFIG_MEMCG.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 33d049066c3d..cf139d3fa513 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -69,7 +69,8 @@ struct kmem_cache {
 	 */
 	int obj_offset;
 #endif /* CONFIG_DEBUG_SLAB */
-#ifdef CONFIG_MEMCG_KMEM
+
+#ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;
 #endif
 

commit f7ce3190c4a35bf887adb7a1aa1ba899b679872d
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:20 2015 -0800

    slab: embed memcg_cache_params to kmem_cache
    
    Currently, kmem_cache stores a pointer to struct memcg_cache_params
    instead of embedding it.  The rationale is to save memory when kmem
    accounting is disabled.  However, the memcg_cache_params has shrivelled
    drastically since it was first introduced:
    
    * Initially:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct kmem_cache *memcg_caches[0];
                    struct {
                            struct mem_cgroup *memcg;
                            struct list_head list;
                            struct kmem_cache *root_cache;
                            bool dead;
                            atomic_t nr_pages;
                            struct work_struct destroy;
                    };
            };
    };
    
    * Now:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct {
                            struct rcu_head rcu_head;
                            struct kmem_cache *memcg_caches[0];
                    };
                    struct {
                            struct mem_cgroup *memcg;
                            struct kmem_cache *root_cache;
                    };
            };
    };
    
    So the memory saving does not seem to be a clear win anymore.
    
    OTOH, keeping a pointer to memcg_cache_params struct instead of embedding
    it results in touching one more cache line on kmem alloc/free hot paths.
    Besides, it makes linking kmem caches in a list chained by a field of
    struct memcg_cache_params really painful due to a level of indirection,
    while I want to make them linked in the following patch.  That said, let
    us embed it.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index b869d1662ba3..33d049066c3d 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -70,7 +70,7 @@ struct kmem_cache {
 	int obj_offset;
 #endif /* CONFIG_DEBUG_SLAB */
 #ifdef CONFIG_MEMCG_KMEM
-	struct memcg_cache_params *memcg_params;
+	struct memcg_cache_params memcg_params;
 #endif
 
 	struct kmem_cache_node *node[MAX_NUMNODES];

commit bf0dea23a9c094ae869a88bb694fbe966671bf6d
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 9 15:26:27 2014 -0700

    mm/slab: use percpu allocator for cpu cache
    
    Because of chicken and egg problem, initialization of SLAB is really
    complicated.  We need to allocate cpu cache through SLAB to make the
    kmem_cache work, but before initialization of kmem_cache, allocation
    through SLAB is impossible.
    
    On the other hand, SLUB does initialization in a more simple way.  It uses
    percpu allocator to allocate cpu cache so there is no chicken and egg
    problem.
    
    So, this patch try to use percpu allocator in SLAB.  This simplifies the
    initialization step in SLAB so that we could maintain SLAB code more
    easily.
    
    In my testing there is no performance difference.
    
    This implementation relies on percpu allocator.  Because percpu allocator
    uses vmalloc address space, vmalloc address space could be exhausted by
    this change on many cpu system with *32 bit* kernel.  This implementation
    can cover 1024 cpus in worst case by following calculation.
    
    Worst: 1024 cpus * 4 bytes for pointer * 300 kmem_caches *
            120 objects per cpu_cache = 140 MB
    Normal: 1024 cpus * 4 bytes for pointer * 150 kmem_caches(slab merge) *
            80 objects per cpu_cache = 46 MB
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jeremiah Mahler <jmmahler@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8235dfbb3b05..b869d1662ba3 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -8,6 +8,8 @@
  */
 
 struct kmem_cache {
+	struct array_cache __percpu *cpu_cache;
+
 /* 1) Cache tunables. Protected by slab_mutex */
 	unsigned int batchcount;
 	unsigned int limit;
@@ -71,23 +73,7 @@ struct kmem_cache {
 	struct memcg_cache_params *memcg_params;
 #endif
 
-/* 6) per-cpu/per-node data, touched during every alloc/free */
-	/*
-	 * We put array[] at the end of kmem_cache, because we want to size
-	 * this array to nr_cpu_ids slots instead of NR_CPUS
-	 * (see kmem_cache_init())
-	 * We still use [NR_CPUS] and not [1] or [0] because cache_cache
-	 * is statically defined, so we reserve the max number of cpus.
-	 *
-	 * We also need to guarantee that the list is able to accomodate a
-	 * pointer for each node since "nodelists" uses the remainder of
-	 * available pointers.
-	 */
-	struct kmem_cache_node **node;
-	struct array_cache *array[NR_CPUS + MAX_NUMNODES];
-	/*
-	 * Do not add fields after array[]
-	 */
+	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
 #endif	/* _LINUX_SLAB_DEF_H */

commit 4ba9920e5e9c0e16b5ed24292d45322907bb9035
Merge: 82c477669a46 8b662fe70c68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 25 11:17:34 2014 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) BPF debugger and asm tool by Daniel Borkmann.
    
     2) Speed up create/bind in AF_PACKET, also from Daniel Borkmann.
    
     3) Correct reciprocal_divide and update users, from Hannes Frederic
        Sowa and Daniel Borkmann.
    
     4) Currently we only have a "set" operation for the hw timestamp socket
        ioctl, add a "get" operation to match.  From Ben Hutchings.
    
     5) Add better trace events for debugging driver datapath problems, also
        from Ben Hutchings.
    
     6) Implement auto corking in TCP, from Eric Dumazet.  Basically, if we
        have a small send and a previous packet is already in the qdisc or
        device queue, defer until TX completion or we get more data.
    
     7) Allow userspace to manage ipv6 temporary addresses, from Jiri Pirko.
    
     8) Add a qdisc bypass option for AF_PACKET sockets, from Daniel
        Borkmann.
    
     9) Share IP header compression code between Bluetooth and IEEE802154
        layers, from Jukka Rissanen.
    
    10) Fix ipv6 router reachability probing, from Jiri Benc.
    
    11) Allow packets to be captured on macvtap devices, from Vlad Yasevich.
    
    12) Support tunneling in GRO layer, from Jerry Chu.
    
    13) Allow bonding to be configured fully using netlink, from Scott
        Feldman.
    
    14) Allow AF_PACKET users to obtain the VLAN TPID, just like they can
        already get the TCI.  From Atzm Watanabe.
    
    15) New "Heavy Hitter" qdisc, from Terry Lam.
    
    16) Significantly improve the IPSEC support in pktgen, from Fan Du.
    
    17) Allow ipv4 tunnels to cache routes, just like sockets.  From Tom
        Herbert.
    
    18) Add Proportional Integral Enhanced packet scheduler, from Vijay
        Subramanian.
    
    19) Allow openvswitch to mmap'd netlink, from Thomas Graf.
    
    20) Key TCP metrics blobs also by source address, not just destination
        address.  From Christoph Paasch.
    
    21) Support 10G in generic phylib.  From Andy Fleming.
    
    22) Try to short-circuit GRO flow compares using device provided RX
        hash, if provided.  From Tom Herbert.
    
    The wireless and netfilter folks have been busy little bees too.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (2064 commits)
      net/cxgb4: Fix referencing freed adapter
      ipv6: reallocate addrconf router for ipv6 address when lo device up
      fib_frontend: fix possible NULL pointer dereference
      rtnetlink: remove IFLA_BOND_SLAVE definition
      rtnetlink: remove check for fill_slave_info in rtnl_have_link_slave_info
      qlcnic: update version to 5.3.55
      qlcnic: Enhance logic to calculate msix vectors.
      qlcnic: Refactor interrupt coalescing code for all adapters.
      qlcnic: Update poll controller code path
      qlcnic: Interrupt code cleanup
      qlcnic: Enhance Tx timeout debugging.
      qlcnic: Use bool for rx_mac_learn.
      bonding: fix u64 division
      rtnetlink: add missing IFLA_BOND_AD_INFO_UNSPEC
      sfc: Use the correct maximum TX DMA ring size for SFC9100
      Add Shradha Shah as the sfc driver maintainer.
      net/vxlan: Share RX skb de-marking and checksum checks with ovs
      tulip: cleanup by using ARRAY_SIZE()
      ip_tunnel: clear IPCB in ip_tunnel_xmit() in case dst_link_failure() is called
      net/cxgb4: Don't retrieve stats during recovery
      ...

commit 809fa972fd90ff27225294b17a027e908b2d7b7a
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Wed Jan 22 02:29:41 2014 +0100

    reciprocal_divide: update/correction of the algorithm
    
    Jakub Zawadzki noticed that some divisions by reciprocal_divide()
    were not correct [1][2], which he could also show with BPF code
    after divisions are transformed into reciprocal_value() for runtime
    invariance which can be passed to reciprocal_divide() later on;
    reverse in BPF dump ended up with a different, off-by-one K in
    some situations.
    
    This has been fixed by Eric Dumazet in commit aee636c4809fa5
    ("bpf: do not use reciprocal divide"). This follow-up patch
    improves reciprocal_value() and reciprocal_divide() to work in
    all cases by using Granlund and Montgomery method, so that also
    future use is safe and without any non-obvious side-effects.
    Known problems with the old implementation were that division by 1
    always returned 0 and some off-by-ones when the dividend and divisor
    where very large. This seemed to not be problematic with its
    current users, as far as we can tell. Eric Dumazet checked for
    the slab usage, we cannot surely say so in the case of flex_array.
    Still, in order to fix that, we propose an extension from the
    original implementation from commit 6a2d7a955d8d resp. [3][4],
    by using the algorithm proposed in "Division by Invariant Integers
    Using Multiplication" [5], Torbjrn Granlund and Peter L.
    Montgomery, that is, pseudocode for q = n/d where q, n, d is in
    u32 universe:
    
    1) Initialization:
    
      int l = ceil(log_2 d)
      uword m' = floor((1<<32)*((1<<l)-d)/d)+1
      int sh_1 = min(l,1)
      int sh_2 = max(l-1,0)
    
    2) For q = n/d, all uword:
    
      uword t = (n*m')>>32
      q = (t+((n-t)>>sh_1))>>sh_2
    
    The assembler implementation from Agner Fog [6] also helped a lot
    while implementing. We have tested the implementation on x86_64,
    ppc64, i686, s390x; on x86_64/haswell we're still half the latency
    compared to normal divide.
    
    Joint work with Daniel Borkmann.
    
      [1] http://www.wireshark.org/~darkjames/reciprocal-buggy.c
      [2] http://www.wireshark.org/~darkjames/set-and-dump-filter-k-bug.c
      [3] https://gmplib.org/~tege/division-paper.pdf
      [4] http://homepage.cs.uiowa.edu/~jones/bcd/divide.html
      [5] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.2556
      [6] http://www.agner.org/optimize/asmlib.zip
    
    Reported-by: Jakub Zawadzki <darkjames-ws@darkjames.pl>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Austin S Hemmelgarn <ahferroin7@gmail.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Jesse Gross <jesse@nicira.com>
    Cc: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Stephen Hemminger <stephen@networkplumber.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andy Gospodarek <andy@greyhouse.net>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Jay Vosburgh <fubar@us.ibm.com>
    Cc: Jakub Zawadzki <darkjames-ws@darkjames.pl>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 09bfffb08a56..96e8abae19a9 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -1,6 +1,8 @@
 #ifndef _LINUX_SLAB_DEF_H
 #define	_LINUX_SLAB_DEF_H
 
+#include <linux/reciprocal_div.h>
+
 /*
  * Definitions unique to the original Linux SLAB allocator.
  */
@@ -12,7 +14,7 @@ struct kmem_cache {
 	unsigned int shared;
 
 	unsigned int size;
-	u32 reciprocal_buffer_size;
+	struct reciprocal_value reciprocal_buffer_size;
 /* 2) touched by every alloc & free from the backend */
 
 	unsigned int flags;		/* constant flags */

commit 24755e2e3a21ba4e7d0911b26e265f231b2cea28
Author: Fam Zheng <famz@redhat.com>
Date:   Tue Jan 21 09:12:42 2014 +0800

    slab: struct kmem_cache is protected by slab_mutex
    
    cache_chain_mutex has been replaced by slab_mutex. Fix this remaining
    outdated comment.
    
    Signed-off-by: Fam Zheng <famz@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 09bfffb08a56..40fc39d22d53 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -6,7 +6,7 @@
  */
 
 struct kmem_cache {
-/* 1) Cache tunables. Protected by cache_chain_mutex */
+/* 1) Cache tunables. Protected by slab_mutex */
 	unsigned int batchcount;
 	unsigned int limit;
 	unsigned int shared;

commit 24f971abbda045c24d5d6f2438a7785567d2fde9
Merge: 3bab0bf045e1 721ae22ae1a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 22 08:10:34 2013 -0800

    Merge branch 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull SLAB changes from Pekka Enberg:
     "The patches from Joonsoo Kim switch mm/slab.c to use 'struct page' for
      slab internals similar to mm/slub.c.  This reduces memory usage and
      improves performance:
    
        https://lkml.org/lkml/2013/10/16/155
    
      Rest of the changes are bug fixes from various people"
    
    * 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux: (21 commits)
      mm, slub: fix the typo in mm/slub.c
      mm, slub: fix the typo in include/linux/slub_def.h
      slub: Handle NULL parameter in kmem_cache_flags
      slab: replace non-existing 'struct freelist *' with 'void *'
      slab: fix to calm down kmemleak warning
      slub: proper kmemleak tracking if CONFIG_SLUB_DEBUG disabled
      slab: rename slab_bufctl to slab_freelist
      slab: remove useless statement for checking pfmemalloc
      slab: use struct page for slab management
      slab: replace free and inuse in struct slab with newly introduced active
      slab: remove SLAB_LIMIT
      slab: remove kmem_bufctl_t
      slab: change the management method of free objects of the slab
      slab: use __GFP_COMP flag for allocating slab pages
      slab: use well-defined macro, virt_to_slab()
      slab: overloading the RCU head over the LRU for RCU free
      slab: remove cachep in struct slab_rcu
      slab: remove nodeid in struct slab
      slab: remove colouroff in struct slab
      slab: change return type of kmem_getpages() to struct page
      ...

commit 8456a648cf44f14365f1f44de90a3da2526a4776
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 24 10:07:49 2013 +0900

    slab: use struct page for slab management
    
    Now, there are a few field in struct slab, so we can overload these
    over struct page. This will save some memory and reduce cache footprint.
    
    After this change, slabp_cache and slab_size no longer related to
    a struct slab, so rename them as freelist_cache and freelist_size.
    
    These changes are just mechanical ones and there is no functional change.
    
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Pekka Enberg <penberg@iki.fi>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index cd401580bdd3..ca82e8ff89fa 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -41,8 +41,8 @@ struct kmem_cache {
 
 	size_t colour;			/* cache colouring range */
 	unsigned int colour_off;	/* colour offset */
-	struct kmem_cache *slabp_cache;
-	unsigned int slab_size;
+	struct kmem_cache *freelist_cache;
+	unsigned int freelist_size;
 
 	/* constructor func */
 	void (*ctor)(void *obj);

commit f1b6eb6e6be149b40ebb013f5bfe2ac86b6f1c1b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 4 16:35:34 2013 +0000

    mm/sl[aou]b: Move kmallocXXX functions to common code
    
    The kmalloc* functions of all slab allcoators are similar now so
    lets move them into slab.h. This requires some function naming changes
    in slob.
    
    As a results of this patch there is a common set of functions for
    all allocators. Also means that kmalloc_large() is now available
    in general to perform large order allocations that go directly
    via the page allocator. kmalloc_large() can be substituted if
    kmalloc() throws warnings because of too large allocations.
    
    kmalloc_large() has exactly the same semantics as kmalloc but
    can only used for allocations > PAGE_SIZE.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index cd401580bdd3..e9346b4f1ef4 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -3,20 +3,6 @@
 
 /*
  * Definitions unique to the original Linux SLAB allocator.
- *
- * What we provide here is a way to optimize the frequent kmalloc
- * calls in the kernel by selecting the appropriate general cache
- * if kmalloc was called with a size that can be established at
- * compile time.
- */
-
-#include <linux/init.h>
-#include <linux/compiler.h>
-
-/*
- * struct kmem_cache
- *
- * manages a cache.
  */
 
 struct kmem_cache {
@@ -102,96 +88,4 @@ struct kmem_cache {
 	 */
 };
 
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
-void *__kmalloc(size_t size, gfp_t flags);
-
-#ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t);
-#else
-static __always_inline void *
-kmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)
-{
-	return kmem_cache_alloc(cachep, flags);
-}
-#endif
-
-static __always_inline void *kmalloc(size_t size, gfp_t flags)
-{
-	struct kmem_cache *cachep;
-	void *ret;
-
-	if (__builtin_constant_p(size)) {
-		int i;
-
-		if (!size)
-			return ZERO_SIZE_PTR;
-
-		if (WARN_ON_ONCE(size > KMALLOC_MAX_SIZE))
-			return NULL;
-
-		i = kmalloc_index(size);
-
-#ifdef CONFIG_ZONE_DMA
-		if (flags & GFP_DMA)
-			cachep = kmalloc_dma_caches[i];
-		else
-#endif
-			cachep = kmalloc_caches[i];
-
-		ret = kmem_cache_alloc_trace(cachep, flags, size);
-
-		return ret;
-	}
-	return __kmalloc(size, flags);
-}
-
-#ifdef CONFIG_NUMA
-extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
-extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
-
-#ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
-					 gfp_t flags,
-					 int nodeid,
-					 size_t size);
-#else
-static __always_inline void *
-kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
-			    gfp_t flags,
-			    int nodeid,
-			    size_t size)
-{
-	return kmem_cache_alloc_node(cachep, flags, nodeid);
-}
-#endif
-
-static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	struct kmem_cache *cachep;
-
-	if (__builtin_constant_p(size)) {
-		int i;
-
-		if (!size)
-			return ZERO_SIZE_PTR;
-
-		if (WARN_ON_ONCE(size > KMALLOC_MAX_SIZE))
-			return NULL;
-
-		i = kmalloc_index(size);
-
-#ifdef CONFIG_ZONE_DMA
-		if (flags & GFP_DMA)
-			cachep = kmalloc_dma_caches[i];
-		else
-#endif
-			cachep = kmalloc_caches[i];
-
-		return kmem_cache_alloc_node_trace(cachep, flags, node, size);
-	}
-	return __kmalloc_node(size, flags, node);
-}
-
-#endif	/* CONFIG_NUMA */
-
 #endif	/* _LINUX_SLAB_DEF_H */

commit 6286ae97d10ea2b5cd90532163797ab217bfdbdf
Author: Christoph Lameter <cl@linux.com>
Date:   Fri May 3 15:43:18 2013 +0000

    slab: Return NULL for oversized allocations
    
    The inline path seems to have changed the SLAB behavior for very large
    kmalloc allocations with  commit e3366016 ("slab: Use common
    kmalloc_index/kmalloc_size functions"). This patch restores the old
    behavior but also adds diagnostics so that we can figure where in the
    code these large allocations occur.
    
    Reported-and-tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Link: http://lkml.kernel.org/r/201305040348.CIF81716.OStQOHFJMFLOVF@I-love.SAKURA.ne.jp
    [ penberg@kernel.org: use WARN_ON_ONCE ]
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 113ec080313f..cd401580bdd3 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -126,6 +126,9 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 		if (!size)
 			return ZERO_SIZE_PTR;
 
+		if (WARN_ON_ONCE(size > KMALLOC_MAX_SIZE))
+			return NULL;
+
 		i = kmalloc_index(size);
 
 #ifdef CONFIG_ZONE_DMA
@@ -172,6 +175,9 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 		if (!size)
 			return ZERO_SIZE_PTR;
 
+		if (WARN_ON_ONCE(size > KMALLOC_MAX_SIZE))
+			return NULL;
+
 		i = kmalloc_index(size);
 
 #ifdef CONFIG_ZONE_DMA

commit 9425c58e5445277699ff3c2a87bac1cfebc1b48d
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:12:17 2013 +0000

    slab: Common definition for the array of kmalloc caches
    
    Have a common definition fo the kmalloc cache arrays in
    SLAB and SLUB
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 4ff50e8d1a2c..113ec080313f 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -102,9 +102,6 @@ struct kmem_cache {
 	 */
 };
 
-extern struct kmem_cache *kmalloc_caches[PAGE_SHIFT + MAX_ORDER];
-extern struct kmem_cache *kmalloc_dma_caches[PAGE_SHIFT + MAX_ORDER];
-
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 

commit 6a67368c36e2c0c2578ba62f6264ab739af08cce
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Rename nodelists to node
    
    Have a common naming between both slab caches for future changes.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8b5b2f6b36d3..4ff50e8d1a2c 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -95,7 +95,7 @@ struct kmem_cache {
 	 * pointer for each node since "nodelists" uses the remainder of
 	 * available pointers.
 	 */
-	struct kmem_cache_node **nodelists;
+	struct kmem_cache_node **node;
 	struct array_cache *array[NR_CPUS + MAX_NUMNODES];
 	/*
 	 * Do not add fields after array[]

commit 6744f087ba2a49f6d6935d9daa0b20a0f03567b5
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:12:17 2013 +0000

    slab: Common name for the per node structures
    
    Rename the structure used for the per node structures in slab
    to have a name that expresses that fact.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index e0f30ef9525d..8b5b2f6b36d3 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -95,7 +95,7 @@ struct kmem_cache {
 	 * pointer for each node since "nodelists" uses the remainder of
 	 * available pointers.
 	 */
-	struct kmem_list3 **nodelists;
+	struct kmem_cache_node **nodelists;
 	struct array_cache *array[NR_CPUS + MAX_NUMNODES];
 	/*
 	 * Do not add fields after array[]

commit e33660165c901d18e7d3df2290db070d3e4b46df
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:18 2013 +0000

    slab: Use common kmalloc_index/kmalloc_size functions
    
    Make slab use the common functions. We can get rid of a lot
    of old ugly stuff as a results. Among them the sizes
    array and the weird include/linux/kmalloc_sizes file and
    some pretty bad #include statements in slab_def.h.
    
    The one thing that is different in slab is that the 32 byte
    cache will also be created for arches that have page sizes
    larger than 4K. There are numerous smaller allocations that
    SLOB and SLUB can handle better because of their support for
    smaller allocation sizes so lets keep the 32 byte slab also
    for arches with > 4K pages.
    
    Reviewed-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8bb6e0eaf3c6..e0f30ef9525d 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -11,8 +11,6 @@
  */
 
 #include <linux/init.h>
-#include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
-#include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
 
 /*
@@ -104,15 +102,8 @@ struct kmem_cache {
 	 */
 };
 
-/* Size description struct for general caches. */
-struct cache_sizes {
-	size_t		 	cs_size;
-	struct kmem_cache	*cs_cachep;
-#ifdef CONFIG_ZONE_DMA
-	struct kmem_cache	*cs_dmacachep;
-#endif
-};
-extern struct cache_sizes malloc_sizes[];
+extern struct kmem_cache *kmalloc_caches[PAGE_SHIFT + MAX_ORDER];
+extern struct kmem_cache *kmalloc_dma_caches[PAGE_SHIFT + MAX_ORDER];
 
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
@@ -133,26 +124,19 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 	void *ret;
 
 	if (__builtin_constant_p(size)) {
-		int i = 0;
+		int i;
 
 		if (!size)
 			return ZERO_SIZE_PTR;
 
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include <linux/kmalloc_sizes.h>
-#undef CACHE
-		return NULL;
-found:
+		i = kmalloc_index(size);
+
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)
-			cachep = malloc_sizes[i].cs_dmacachep;
+			cachep = kmalloc_dma_caches[i];
 		else
 #endif
-			cachep = malloc_sizes[i].cs_cachep;
+			cachep = kmalloc_caches[i];
 
 		ret = kmem_cache_alloc_trace(cachep, flags, size);
 
@@ -186,26 +170,19 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 	struct kmem_cache *cachep;
 
 	if (__builtin_constant_p(size)) {
-		int i = 0;
+		int i;
 
 		if (!size)
 			return ZERO_SIZE_PTR;
 
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include <linux/kmalloc_sizes.h>
-#undef CACHE
-		return NULL;
-found:
+		i = kmalloc_index(size);
+
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)
-			cachep = malloc_sizes[i].cs_dmacachep;
+			cachep = kmalloc_dma_caches[i];
 		else
 #endif
-			cachep = malloc_sizes[i].cs_cachep;
+			cachep = kmalloc_caches[i];
 
 		return kmem_cache_alloc_node_trace(cachep, flags, node, size);
 	}

commit ba6c496ed834a37a26fc6fc87fc9aecb0fa0014d
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:27 2012 -0800

    slab/slub: struct memcg_params
    
    For the kmem slab controller, we need to record some extra information in
    the kmem_cache structure.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Suleiman Souhlal <suleiman@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 45c0356fdc8c..8bb6e0eaf3c6 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -81,6 +81,9 @@ struct kmem_cache {
 	 */
 	int obj_offset;
 #endif /* CONFIG_DEBUG_SLAB */
+#ifdef CONFIG_MEMCG_KMEM
+	struct memcg_cache_params *memcg_params;
+#endif
 
 /* 6) per-cpu/per-node data, touched during every alloc/free */
 	/*

commit 3c58346525d82625e68e24f071804c2dc057b6f4
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Nov 28 16:23:01 2012 +0000

    slab: Simplify bootstrap
    
    The nodelists field in kmem_cache is pointing to the first unused
    object in the array field when bootstrap is complete.
    
    A problem with the current approach is that the statically sized
    kmem_cache structure use on boot can only contain NR_CPUS entries.
    If the number of nodes plus the number of cpus is greater then we
    would overwrite memory following the kmem_cache_boot definition.
    
    Increase the size of the array field to ensure that also the node
    pointers fit into the array field.
    
    Once we do that we no longer need the kmem_cache_nodelists
    array and we can then also use that structure elsewhere.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index cc290f0bdb34..45c0356fdc8c 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -89,9 +89,13 @@ struct kmem_cache {
 	 * (see kmem_cache_init())
 	 * We still use [NR_CPUS] and not [1] or [0] because cache_cache
 	 * is statically defined, so we reserve the max number of cpus.
+	 *
+	 * We also need to guarantee that the list is able to accomodate a
+	 * pointer for each node since "nodelists" uses the remainder of
+	 * available pointers.
 	 */
 	struct kmem_list3 **nodelists;
-	struct array_cache *array[NR_CPUS];
+	struct array_cache *array[NR_CPUS + MAX_NUMNODES];
 	/*
 	 * Do not add fields after array[]
 	 */

commit dffa3f985047979ace185e253da89cabda3f7b31
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Wed Sep 26 09:21:33 2012 -0300

    mm/slab: Fix kmem_cache_alloc_node_trace() declaration
    
    This bug was introduced in commit 4052147 ("mm, slab: Match SLAB and
    SLUB kmem_cache_alloc_xxx_trace() prototype").
    
    Cc: David Rientjes <rientjes@google.com>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index e98caebdd0bc..cc290f0bdb34 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -159,16 +159,16 @@ extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_node_trace(size_t size,
-					 struct kmem_cache *cachep,
+extern void *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
 					 gfp_t flags,
-					 int nodeid);
+					 int nodeid,
+					 size_t size);
 #else
 static __always_inline void *
-kmem_cache_alloc_node_trace(size_t size,
-			    struct kmem_cache *cachep,
+kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
 			    gfp_t flags,
-			    int nodeid)
+			    int nodeid,
+			    size_t size)
 {
 	return kmem_cache_alloc_node(cachep, flags, nodeid);
 }
@@ -200,7 +200,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #endif
 			cachep = malloc_sizes[i].cs_cachep;
 
-		return kmem_cache_alloc_node_trace(size, cachep, flags, node);
+		return kmem_cache_alloc_node_trace(cachep, flags, node, size);
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit 4052147c0afa1cf05780ed846f37e87cdde9f628
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:56 2012 -0300

    mm, slab: Match SLAB and SLUB kmem_cache_alloc_xxx_trace() prototype
    
    This long (seemingly unnecessary) patch does not fix anything and
    its only goal is to produce common code between SLAB and SLUB.
    
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 604ebc832a9a..e98caebdd0bc 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -111,11 +111,10 @@ void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_trace(size_t size,
-				    struct kmem_cache *cachep, gfp_t flags);
+extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t);
 #else
 static __always_inline void *
-kmem_cache_alloc_trace(size_t size, struct kmem_cache *cachep, gfp_t flags)
+kmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)
 {
 	return kmem_cache_alloc(cachep, flags);
 }
@@ -148,7 +147,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 #endif
 			cachep = malloc_sizes[i].cs_cachep;
 
-		ret = kmem_cache_alloc_trace(size, cachep, flags);
+		ret = kmem_cache_alloc_trace(cachep, flags, size);
 
 		return ret;
 	}

commit ff4fcd01ec86d98d15d2fd96f22f19bb1d341b88
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Sat Sep 8 17:47:52 2012 -0300

    mm, slab: Remove silly function slab_buffer_size()
    
    This function is seldom used, and can be simply replaced with cachep->size.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 36d7031a1a53..604ebc832a9a 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -113,17 +113,12 @@ void *__kmalloc(size_t size, gfp_t flags);
 #ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_trace(size_t size,
 				    struct kmem_cache *cachep, gfp_t flags);
-extern size_t slab_buffer_size(struct kmem_cache *cachep);
 #else
 static __always_inline void *
 kmem_cache_alloc_trace(size_t size, struct kmem_cache *cachep, gfp_t flags)
 {
 	return kmem_cache_alloc(cachep, flags);
 }
-static inline size_t slab_buffer_size(struct kmem_cache *cachep)
-{
-	return 0;
-}
 #endif
 
 static __always_inline void *kmalloc(size_t size, gfp_t flags)

commit c0931ef1f3e7305e55de1591602e5f25e4c11d1d
Author: David Rientjes <rientjes@google.com>
Date:   Thu Aug 16 12:26:20 2012 -0700

    mm, slab: remove dflags
    
    cachep->dflags is never referenced, so remove it.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 0c634fa376c9..36d7031a1a53 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -45,7 +45,6 @@ struct kmem_cache {
 	unsigned int colour_off;	/* colour offset */
 	struct kmem_cache *slabp_cache;
 	unsigned int slab_size;
-	unsigned int dflags;		/* dynamic flags */
 
 	/* constructor func */
 	void (*ctor)(void *obj);

commit a618e89f1e6fb3cdfc8ef0ad54a0d57830bf8881
Author: Glauber Costa <glommer@parallels.com>
Date:   Thu Jun 14 16:17:21 2012 +0400

    slab: rename gfpflags to allocflags
    
    A consistent name with slub saves us an acessor function.
    In both caches, this field represents the same thing. We would
    like to use it from the mem_cgroup code.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    CC: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 1d93f27d81de..0c634fa376c9 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -39,7 +39,7 @@ struct kmem_cache {
 	unsigned int gfporder;
 
 	/* force GFP flags, e.g. GFP_DMA */
-	gfp_t gfpflags;
+	gfp_t allocflags;
 
 	size_t colour;			/* cache colouring range */
 	unsigned int colour_off;	/* colour offset */

commit 3b0efdfa1e719303536c04d9abca43abeb40f80a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:57 2012 -0500

    mm, sl[aou]b: Extract common fields from struct kmem_cache
    
    Define a struct that describes common fields used in all slab allocators.
    A slab allocator either uses the common definition (like SLOB) or is
    required to provide members of kmem_cache with the definition given.
    
    After that it will be possible to share code that
    only operates on those fields of kmem_cache.
    
    The patch basically takes the slob definition of kmem cache and
    uses the field namees for the other allocators.
    
    It also standardizes the names used for basic object lengths in
    allocators:
    
    object_size     Struct size specified at kmem_cache_create. Basically
                    the payload expected to be used by the subsystem.
    
    size            The size of memory allocator for each object. This size
                    is larger than object_size and includes padding, alignment
                    and extra metadata for each object (f.e. for debugging
                    and rcu).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index fbd1117fdfde..1d93f27d81de 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -27,7 +27,7 @@ struct kmem_cache {
 	unsigned int limit;
 	unsigned int shared;
 
-	unsigned int buffer_size;
+	unsigned int size;
 	u32 reciprocal_buffer_size;
 /* 2) touched by every alloc & free from the backend */
 
@@ -52,7 +52,10 @@ struct kmem_cache {
 
 /* 4) cache creation/removal */
 	const char *name;
-	struct list_head next;
+	struct list_head list;
+	int refcount;
+	int object_size;
+	int align;
 
 /* 5) statistics */
 #ifdef CONFIG_DEBUG_SLAB
@@ -73,12 +76,11 @@ struct kmem_cache {
 
 	/*
 	 * If debugging is enabled, then the allocator can add additional
-	 * fields and/or padding to every object. buffer_size contains the total
+	 * fields and/or padding to every object. size contains the total
 	 * object size including these internal fields, the following two
 	 * variables contain the offset to the user object and its size.
 	 */
 	int obj_offset;
-	int obj_size;
 #endif /* CONFIG_DEBUG_SLAB */
 
 /* 6) per-cpu/per-node data, touched during every alloc/free */

commit 4dee6b64ee7cfef94b47733c6d9fef07f8051c7c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Jan 9 17:15:42 2012 -0500

    tracing/mm: Move include of trace/events/kmem.h out of header into slab.c
    
    Including trace/events/*.h TRACE_EVENT() macro headers in other headers
    can cause strange side effects if another trace/event/*.h header
    includes that header.  Having trace/events/kmem.h inside slab_def.h
    caused a compile error in sparc64 when changes were done to some header
    files.  Moving the kmem.h trace header out of slab.h and into slab.c
    fixes the problem.
    
    Note, both slub.c and slob.c already include the trace/events/kmem.h
    file. Only slab.c had it missing.
    
    Link: http://lkml.kernel.org/r/20120105190405.1e3191fb5a43b2a0f1655e1f@canb.auug.org.au
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index d00e0bacda93..fbd1117fdfde 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -15,8 +15,6 @@
 #include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
 
-#include <trace/events/kmem.h>
-
 /*
  * struct kmem_cache
  *

commit b56efcf0a45aa7fc32de90d5f9838541082fbc19
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Jul 20 19:04:23 2011 +0200

    slab: shrink sizeof(struct kmem_cache)
    
    Reduce high order allocations for some setups.
    (NR_CPUS=4096 -> we need 64KB per kmem_cache struct)
    
    We now allocate exact needed size (using nr_cpu_ids and nr_node_ids)
    
    This also makes code a bit smaller on x86_64, since some field offsets
    are less than the 127 limit :
    
    Before patch :
    # size mm/slab.o
       text    data     bss     dec     hex filename
      22605  361665      32  384302   5dd2e mm/slab.o
    
    After patch :
    # size mm/slab.o
       text    data     bss     dec     hex filename
      22349  353473    8224  384046   5dc2e mm/slab.o
    
    CC: Andrew Morton <akpm@linux-foundation.org>
    Reported-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index d7f63112f63c..d00e0bacda93 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -24,21 +24,19 @@
  */
 
 struct kmem_cache {
-/* 1) per-cpu data, touched during every alloc/free */
-	struct array_cache *array[NR_CPUS];
-/* 2) Cache tunables. Protected by cache_chain_mutex */
+/* 1) Cache tunables. Protected by cache_chain_mutex */
 	unsigned int batchcount;
 	unsigned int limit;
 	unsigned int shared;
 
 	unsigned int buffer_size;
 	u32 reciprocal_buffer_size;
-/* 3) touched by every alloc & free from the backend */
+/* 2) touched by every alloc & free from the backend */
 
 	unsigned int flags;		/* constant flags */
 	unsigned int num;		/* # of objs per slab */
 
-/* 4) cache_grow/shrink */
+/* 3) cache_grow/shrink */
 	/* order of pgs per slab (2^n) */
 	unsigned int gfporder;
 
@@ -54,11 +52,11 @@ struct kmem_cache {
 	/* constructor func */
 	void (*ctor)(void *obj);
 
-/* 5) cache creation/removal */
+/* 4) cache creation/removal */
 	const char *name;
 	struct list_head next;
 
-/* 6) statistics */
+/* 5) statistics */
 #ifdef CONFIG_DEBUG_SLAB
 	unsigned long num_active;
 	unsigned long num_allocations;
@@ -85,16 +83,18 @@ struct kmem_cache {
 	int obj_size;
 #endif /* CONFIG_DEBUG_SLAB */
 
+/* 6) per-cpu/per-node data, touched during every alloc/free */
 	/*
-	 * We put nodelists[] at the end of kmem_cache, because we want to size
-	 * this array to nr_node_ids slots instead of MAX_NUMNODES
+	 * We put array[] at the end of kmem_cache, because we want to size
+	 * this array to nr_cpu_ids slots instead of NR_CPUS
 	 * (see kmem_cache_init())
-	 * We still use [MAX_NUMNODES] and not [1] or [0] because cache_cache
-	 * is statically defined, so we reserve the max number of nodes.
+	 * We still use [NR_CPUS] and not [1] or [0] because cache_cache
+	 * is statically defined, so we reserve the max number of cpus.
 	 */
-	struct kmem_list3 *nodelists[MAX_NUMNODES];
+	struct kmem_list3 **nodelists;
+	struct array_cache *array[NR_CPUS];
 	/*
-	 * Do not add fields after nodelists[]
+	 * Do not add fields after array[]
 	 */
 };
 

commit 3192b920bf7d0c528ab54e7d3689f44055316a37
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jun 14 16:16:36 2011 -0500

    slab, slub, slob: Unify alignment definition
    
    Every slab has its on alignment definition in include/linux/sl?b_def.h. Extract those
    and define a common set in include/linux/slab.h.
    
    SLOB: As notes sometimes we need double word alignment on 32 bit. This gives all
    structures allocated by SLOB a unsigned long long alignment like the others do.
    
    SLAB: If ARCH_SLAB_MINALIGN is not set SLAB would set ARCH_SLAB_MINALIGN to
    zero meaning no alignment at all. Give it the default unsigned long long alignment.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 83203ae9390b..d7f63112f63c 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -17,32 +17,6 @@
 
 #include <trace/events/kmem.h>
 
-/*
- * Enforce a minimum alignment for the kmalloc caches.
- * Usually, the kmalloc caches are cache_line_size() aligned, except when
- * DEBUG and FORCED_DEBUG are enabled, then they are BYTES_PER_WORD aligned.
- * Some archs want to perform DMA into kmalloc caches and need a guaranteed
- * alignment larger than the alignment of a 64-bit integer.
- * ARCH_KMALLOC_MINALIGN allows that.
- * Note that increasing this value may disable some debug features.
- */
-#ifdef ARCH_DMA_MINALIGN
-#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
-#else
-#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
-#endif
-
-#ifndef ARCH_SLAB_MINALIGN
-/*
- * Enforce a minimum alignment for all caches.
- * Intended for archs that get misalignment faults even for BYTES_PER_WORD
- * aligned buffers. Includes ARCH_KMALLOC_MINALIGN.
- * If possible: Do not enable this flag for CONFIG_DEBUG_SLAB, it disables
- * some debug features.
- */
-#define ARCH_SLAB_MINALIGN 0
-#endif
-
 /*
  * struct kmem_cache
  *

commit 85beb5869a4f6abb52a7cf8e01de6fa57e9ee47d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 24 16:23:34 2010 -0500

    tracing/slab: Move kmalloc tracepoint out of inline code
    
    The tracepoint for kmalloc is in the slab inlined code which causes
    every instance of kmalloc to have the tracepoint.
    
    This patch moves the tracepoint out of the inline code to the
    slab C file, which removes a large number of inlined trace
    points.
    
      objdump -dr vmlinux.slab| grep 'jmpq.*<trace_kmalloc' |wc -l
    213
      objdump -dr vmlinux.slab.patched| grep 'jmpq.*<trace_kmalloc' |wc -l
    1
    
    This also has a nice impact on size.
    
       text    data     bss     dec     hex filename
    7023060 2121564 2482432 11627056         b16a30 vmlinux.slab
    6970579 2109772 2482432 11562783         b06f1f vmlinux.slab.patched
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 791a502f6906..83203ae9390b 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -138,11 +138,12 @@ void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_notrace(struct kmem_cache *cachep, gfp_t flags);
+extern void *kmem_cache_alloc_trace(size_t size,
+				    struct kmem_cache *cachep, gfp_t flags);
 extern size_t slab_buffer_size(struct kmem_cache *cachep);
 #else
 static __always_inline void *
-kmem_cache_alloc_notrace(struct kmem_cache *cachep, gfp_t flags)
+kmem_cache_alloc_trace(size_t size, struct kmem_cache *cachep, gfp_t flags)
 {
 	return kmem_cache_alloc(cachep, flags);
 }
@@ -179,10 +180,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 #endif
 			cachep = malloc_sizes[i].cs_cachep;
 
-		ret = kmem_cache_alloc_notrace(cachep, flags);
-
-		trace_kmalloc(_THIS_IP_, ret,
-			      size, slab_buffer_size(cachep), flags);
+		ret = kmem_cache_alloc_trace(size, cachep, flags);
 
 		return ret;
 	}
@@ -194,14 +192,16 @@ extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
-					   gfp_t flags,
-					   int nodeid);
+extern void *kmem_cache_alloc_node_trace(size_t size,
+					 struct kmem_cache *cachep,
+					 gfp_t flags,
+					 int nodeid);
 #else
 static __always_inline void *
-kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
-			      gfp_t flags,
-			      int nodeid)
+kmem_cache_alloc_node_trace(size_t size,
+			    struct kmem_cache *cachep,
+			    gfp_t flags,
+			    int nodeid)
 {
 	return kmem_cache_alloc_node(cachep, flags, nodeid);
 }
@@ -210,7 +210,6 @@ kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	struct kmem_cache *cachep;
-	void *ret;
 
 	if (__builtin_constant_p(size)) {
 		int i = 0;
@@ -234,13 +233,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #endif
 			cachep = malloc_sizes[i].cs_cachep;
 
-		ret = kmem_cache_alloc_node_notrace(cachep, flags, node);
-
-		trace_kmalloc_node(_THIS_IP_, ret,
-				   size, slab_buffer_size(cachep),
-				   flags, node);
-
-		return ret;
+		return kmem_cache_alloc_node_trace(size, cachep, flags, node);
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit a6eb9fe105d5de0053b261148cee56c94b4720ca
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:22 2010 -0700

    dma-mapping: rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN
    
    Now each architecture has the own dma_get_cache_alignment implementation.
    
    dma_get_cache_alignment returns the minimum DMA alignment.  Architectures
    define it as ARCH_KMALLOC_MINALIGN (it's used to make sure that malloc'ed
    buffer is DMA-safe; the buffer doesn't share a cache with the others).  So
    we can unify dma_get_cache_alignment implementations.
    
    This patch:
    
    dma_get_cache_alignment() needs to know if an architecture defines
    ARCH_KMALLOC_MINALIGN or not (needs to know if architecture has DMA
    alignment restriction).  However, slab.h define ARCH_KMALLOC_MINALIGN if
    architectures doesn't define it.
    
    Let's rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN.
    ARCH_KMALLOC_MINALIGN is used only in the internals of slab/slob/slub
    (except for crypto).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 1acfa73ce2ac..791a502f6906 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -17,7 +17,6 @@
 
 #include <trace/events/kmem.h>
 
-#ifndef ARCH_KMALLOC_MINALIGN
 /*
  * Enforce a minimum alignment for the kmalloc caches.
  * Usually, the kmalloc caches are cache_line_size() aligned, except when
@@ -27,6 +26,9 @@
  * ARCH_KMALLOC_MINALIGN allows that.
  * Note that increasing this value may disable some debug features.
  */
+#ifdef ARCH_DMA_MINALIGN
+#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
+#else
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
 #endif
 

commit 039ca4e74a1cf60bd7487324a564ecf5c981f254
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 26 17:22:17 2010 +0800

    tracing: Remove kmemtrace ftrace plugin
    
    We have been resisting new ftrace plugins and removing existing
    ones, and kmemtrace has been superseded by kmem trace events
    and perf-kmem, so we remove it.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    [ remove kmemtrace from the makefile, handle slob too ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 1812dac8c496..1acfa73ce2ac 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -14,7 +14,8 @@
 #include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
 #include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
-#include <linux/kmemtrace.h>
+
+#include <trace/events/kmem.h>
 
 #ifndef ARCH_KMALLOC_MINALIGN
 /*

commit 1f0ce8b3dd667dca720a47869f8110c298f0e5b8
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed May 19 12:01:42 2010 +0100

    mm: Move ARCH_SLAB_MINALIGN and ARCH_KMALLOC_MINALIGN to <linux/slab_def.h>
    
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index ca6b2b317991..1812dac8c496 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -16,6 +16,30 @@
 #include <linux/compiler.h>
 #include <linux/kmemtrace.h>
 
+#ifndef ARCH_KMALLOC_MINALIGN
+/*
+ * Enforce a minimum alignment for the kmalloc caches.
+ * Usually, the kmalloc caches are cache_line_size() aligned, except when
+ * DEBUG and FORCED_DEBUG are enabled, then they are BYTES_PER_WORD aligned.
+ * Some archs want to perform DMA into kmalloc caches and need a guaranteed
+ * alignment larger than the alignment of a 64-bit integer.
+ * ARCH_KMALLOC_MINALIGN allows that.
+ * Note that increasing this value may disable some debug features.
+ */
+#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+#endif
+
+#ifndef ARCH_SLAB_MINALIGN
+/*
+ * Enforce a minimum alignment for all caches.
+ * Intended for archs that get misalignment faults even for BYTES_PER_WORD
+ * aligned buffers. Includes ARCH_KMALLOC_MINALIGN.
+ * If possible: Do not enable this flag for CONFIG_DEBUG_SLAB, it disables
+ * some debug features.
+ */
+#define ARCH_SLAB_MINALIGN 0
+#endif
+
 /*
  * struct kmem_cache
  *

commit 0f24f1287a86b198c1e4bd4ce45e8565e40ff804
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Dec 11 15:45:30 2009 +0800

    tracing, slab: Define kmem_cache_alloc_notrace ifdef CONFIG_TRACING
    
    Define kmem_trace_alloc_{,node}_notrace() if CONFIG_TRACING is
    enabled, otherwise perf-kmem will show wrong stats ifndef
    CONFIG_KMEM_TRACE, because a kmalloc() memory allocation may
    be traced by both trace_kmalloc() and trace_kmem_cache_alloc().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: linux-mm@kvack.org <linux-mm@kvack.org>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    LKML-Reference: <4B21F89A.7000801@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 850d057500de..ca6b2b317991 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -110,7 +110,7 @@ extern struct cache_sizes malloc_sizes[];
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
-#ifdef CONFIG_KMEMTRACE
+#ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_notrace(struct kmem_cache *cachep, gfp_t flags);
 extern size_t slab_buffer_size(struct kmem_cache *cachep);
 #else
@@ -166,7 +166,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
-#ifdef CONFIG_KMEMTRACE
+#ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
 					   gfp_t flags,
 					   int nodeid);

commit 8eae985f08138758e06503588f5f1196269bc415
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Fri May 9 20:32:44 2008 +0200

    slab: move struct kmem_cache to headers
    
    Move the SLAB struct kmem_cache definition to <linux/slab_def.h> like
    with SLUB so kmemcheck can access ->ctor and ->flags.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 713f841ecaa9..850d057500de 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -16,6 +16,87 @@
 #include <linux/compiler.h>
 #include <linux/kmemtrace.h>
 
+/*
+ * struct kmem_cache
+ *
+ * manages a cache.
+ */
+
+struct kmem_cache {
+/* 1) per-cpu data, touched during every alloc/free */
+	struct array_cache *array[NR_CPUS];
+/* 2) Cache tunables. Protected by cache_chain_mutex */
+	unsigned int batchcount;
+	unsigned int limit;
+	unsigned int shared;
+
+	unsigned int buffer_size;
+	u32 reciprocal_buffer_size;
+/* 3) touched by every alloc & free from the backend */
+
+	unsigned int flags;		/* constant flags */
+	unsigned int num;		/* # of objs per slab */
+
+/* 4) cache_grow/shrink */
+	/* order of pgs per slab (2^n) */
+	unsigned int gfporder;
+
+	/* force GFP flags, e.g. GFP_DMA */
+	gfp_t gfpflags;
+
+	size_t colour;			/* cache colouring range */
+	unsigned int colour_off;	/* colour offset */
+	struct kmem_cache *slabp_cache;
+	unsigned int slab_size;
+	unsigned int dflags;		/* dynamic flags */
+
+	/* constructor func */
+	void (*ctor)(void *obj);
+
+/* 5) cache creation/removal */
+	const char *name;
+	struct list_head next;
+
+/* 6) statistics */
+#ifdef CONFIG_DEBUG_SLAB
+	unsigned long num_active;
+	unsigned long num_allocations;
+	unsigned long high_mark;
+	unsigned long grown;
+	unsigned long reaped;
+	unsigned long errors;
+	unsigned long max_freeable;
+	unsigned long node_allocs;
+	unsigned long node_frees;
+	unsigned long node_overflow;
+	atomic_t allochit;
+	atomic_t allocmiss;
+	atomic_t freehit;
+	atomic_t freemiss;
+
+	/*
+	 * If debugging is enabled, then the allocator can add additional
+	 * fields and/or padding to every object. buffer_size contains the total
+	 * object size including these internal fields, the following two
+	 * variables contain the offset to the user object and its size.
+	 */
+	int obj_offset;
+	int obj_size;
+#endif /* CONFIG_DEBUG_SLAB */
+
+	/*
+	 * We put nodelists[] at the end of kmem_cache, because we want to size
+	 * this array to nr_node_ids slots instead of MAX_NUMNODES
+	 * (see kmem_cache_init())
+	 * We still use [MAX_NUMNODES] and not [1] or [0] because cache_cache
+	 * is statically defined, so we reserve the max number of nodes.
+	 */
+	struct kmem_list3 *nodelists[MAX_NUMNODES];
+	/*
+	 * Do not add fields after nodelists[]
+	 */
+};
+
 /* Size description struct for general caches. */
 struct cache_sizes {
 	size_t		 	cs_size;

commit 02af61bb50f5d5f0322dbe5ab2a0d75808d25c7b
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Fri Apr 10 14:26:18 2009 +0800

    tracing, kmemtrace: Separate include/trace/kmemtrace.h to kmemtrace part and tracepoint part
    
    Impact: refactor code for future changes
    
    Current kmemtrace.h is used both as header file of kmemtrace and kmem's
    tracepoints definition.
    
    Tracepoints' definition file may be used by other code, and should only have
    definition of tracepoint.
    
    We can separate include/trace/kmemtrace.h into 2 files:
    
      include/linux/kmemtrace.h: header file for kmemtrace
      include/trace/kmem.h:      definition of kmem tracepoints
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <49DEE68A.5040902@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 5ac9b0bcaf9a..713f841ecaa9 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -14,7 +14,7 @@
 #include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
 #include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
-#include <trace/kmemtrace.h>
+#include <linux/kmemtrace.h>
 
 /* Size description struct for general caches. */
 struct cache_sizes {

commit ca2b84cb3c4a0d4d2143b46ec072cdff5d1b3b87
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Mon Mar 23 15:12:24 2009 +0200

    kmemtrace: use tracepoints
    
    kmemtrace now uses tracepoints instead of markers. We no longer need to
    use format specifiers to pass arguments.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    [ folded: Use the new TP_PROTO and TP_ARGS to fix the build.     ]
    [ folded: fix build when CONFIG_KMEMTRACE is disabled.           ]
    [ folded: define tracepoints when CONFIG_TRACEPOINTS is enabled. ]
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    LKML-Reference: <ae61c0f37156db8ec8dc0d5778018edde60a92e3.1237813499.git.eduard.munteanu@linux360.ro>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index f4523651fa42..5ac9b0bcaf9a 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -73,8 +73,8 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 
 		ret = kmem_cache_alloc_notrace(cachep, flags);
 
-		kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_, ret,
-				     size, slab_buffer_size(cachep), flags);
+		trace_kmalloc(_THIS_IP_, ret,
+			      size, slab_buffer_size(cachep), flags);
 
 		return ret;
 	}
@@ -128,9 +128,9 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 
 		ret = kmem_cache_alloc_node_notrace(cachep, flags, node);
 
-		kmemtrace_mark_alloc_node(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_,
-					  ret, size, slab_buffer_size(cachep),
-					  flags, node);
+		trace_kmalloc_node(_THIS_IP_, ret,
+				   size, slab_buffer_size(cachep),
+				   flags, node);
 
 		return ret;
 	}

commit dc573f9b20c8710105ac35c08ed0fe1da5160ecd
Merge: b3a8c34886d0 ecf441b593ac b1792e367053
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 3 06:25:38 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/kmemtrace' and 'linus' into tracing/core

commit 1cf3eb2ff6b0844c678f2f48d0053b9d12b7da67
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Jan 27 23:48:59 2009 +0200

    kmalloc: return NULL instead of link failure
    
    The SLAB kmalloc with a constant value isn't consistent with the other
    implementations because it bails out with __you_cannot_kmalloc_that_much
    rather than returning NULL and properly allowing the caller to fall back
    to vmalloc or take other action.  This doesn't happen with a non-constant
    value or with SLOB or SLUB.
    
    Starting with 2.6.28, I've been seeing build failures on s390x.  This is
    due to init_section_page_cgroup trying to allocate 2.5MB when the max size
    for a kmalloc on s390x is 2MB.
    
    It's failing because the value is constant.  The workarounds at the call
    size are ugly and the caller shouldn't have to change behavior depending
    on what the backend of the API is.
    
    So, this patch eliminates the link failure and returns NULL like the other
    implementations.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: <stable@kernel.org>         [2.6.28.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 39c3a5eb8ebe..6ca6a7b66d75 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -43,10 +43,7 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 			i++;
 #include <linux/kmalloc_sizes.h>
 #undef CACHE
-		{
-			extern void __you_cannot_kmalloc_that_much(void);
-			__you_cannot_kmalloc_that_much();
-		}
+		return NULL;
 found:
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)
@@ -77,10 +74,7 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 			i++;
 #include <linux/kmalloc_sizes.h>
 #undef CACHE
-		{
-			extern void __you_cannot_kmalloc_that_much(void);
-			__you_cannot_kmalloc_that_much();
-		}
+		return NULL;
 found:
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)

commit 36994e58a48fb8f9651c7dc845a6de298aba5bfc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Dec 29 13:42:23 2008 -0800

    tracing/kmemtrace: normalize the raw tracer event to the unified tracing API
    
    Impact: new tracer plugin
    
    This patch adapts kmemtrace raw events tracing to the unified tracing API.
    
    To enable and use this tracer, just do the following:
    
     echo kmemtrace > /debugfs/tracing/current_tracer
     cat /debugfs/tracing/trace
    
    You will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
    type_id 1 call_site 18446744071565527833 ptr 18446612134395152256
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164672 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164912 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345165152 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 0 call_site 18446744071566144042 ptr 18446612134346191680 bytes_req 1304 bytes_alloc 1312 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    
    That was to stay backward compatible with the format output produced in
    inux/tracepoint.h.
    
    This is the default ouput, but note that I tried something else.
    
    If you change an option:
    
    echo kmem_minimalistic > /debugfs/trace_options
    
    and then cat /debugfs/trace, you will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
       -      C                            0xffff88007c088780          file_free_rcu
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc780     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc870     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc960     -1   d_alloc
       +      K   1304   1312   000000d0   0xffff8800791d7340     -1   reiserfs_alloc_inode
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K    992   1000   000000d0   0xffff880079045b58     -1   alloc_inode
       +      K    768   1024   000080d0   0xffff88007c096400     -1   alloc_pipe_info
       +      K    240    240   000000d0   0xffff8800790dca50     -1   d_alloc
       +      K    272    320   000080d0   0xffff88007c088780     -1   get_empty_filp
       +      K    272    320   000080d0   0xffff88007c088000     -1   get_empty_filp
    
    Yeah I shall confess kmem_minimalistic should be: kmem_alternative.
    
    Whatever, I find it more readable but this a personal opinion of course.
    We can drop it if you want.
    
    On the ALLOC/FREE column, + means an allocation and - a free.
    
    On the type column, you have K = kmalloc, C = cache, P = page
    
    I would like the flags to be GFP_* strings but that would not be easy to not
    break the column with strings....
    
    About the node...it seems to always be -1. I don't know why but that shouldn't
    be difficult to find.
    
    I moved linux/tracepoint.h to trace/tracepoint.h as well. I think that would
    be more easy to find the tracer headers if they are all in their common
    directory.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 7555ce99f6d2..455f9affea9a 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -14,7 +14,7 @@
 #include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
 #include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
-#include <linux/kmemtrace.h>
+#include <trace/kmemtrace.h>
 
 /* Size description struct for general caches. */
 struct cache_sizes {

commit 36555751c6751a5bdfd6d7bdf0648343bb1ef0de
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Sun Aug 10 20:14:05 2008 +0300

    kmemtrace: SLAB hooks.
    
    This adds hooks for the SLAB allocator, to allow tracing with kmemtrace.
    
    We also convert some inline functions to __always_inline to make sure
    _RET_IP_, which expands to __builtin_return_address(0), always works
    as expected.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 39c3a5eb8ebe..7555ce99f6d2 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -14,6 +14,7 @@
 #include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
 #include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
 #include <linux/compiler.h>
+#include <linux/kmemtrace.h>
 
 /* Size description struct for general caches. */
 struct cache_sizes {
@@ -28,8 +29,26 @@ extern struct cache_sizes malloc_sizes[];
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
-static inline void *kmalloc(size_t size, gfp_t flags)
+#ifdef CONFIG_KMEMTRACE
+extern void *kmem_cache_alloc_notrace(struct kmem_cache *cachep, gfp_t flags);
+extern size_t slab_buffer_size(struct kmem_cache *cachep);
+#else
+static __always_inline void *
+kmem_cache_alloc_notrace(struct kmem_cache *cachep, gfp_t flags)
 {
+	return kmem_cache_alloc(cachep, flags);
+}
+static inline size_t slab_buffer_size(struct kmem_cache *cachep)
+{
+	return 0;
+}
+#endif
+
+static __always_inline void *kmalloc(size_t size, gfp_t flags)
+{
+	struct kmem_cache *cachep;
+	void *ret;
+
 	if (__builtin_constant_p(size)) {
 		int i = 0;
 
@@ -50,10 +69,17 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 found:
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)
-			return kmem_cache_alloc(malloc_sizes[i].cs_dmacachep,
-						flags);
+			cachep = malloc_sizes[i].cs_dmacachep;
+		else
 #endif
-		return kmem_cache_alloc(malloc_sizes[i].cs_cachep, flags);
+			cachep = malloc_sizes[i].cs_cachep;
+
+		ret = kmem_cache_alloc_notrace(cachep, flags);
+
+		kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_, ret,
+				     size, slab_buffer_size(cachep), flags);
+
+		return ret;
 	}
 	return __kmalloc(size, flags);
 }
@@ -62,8 +88,25 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+#ifdef CONFIG_KMEMTRACE
+extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
+					   gfp_t flags,
+					   int nodeid);
+#else
+static __always_inline void *
+kmem_cache_alloc_node_notrace(struct kmem_cache *cachep,
+			      gfp_t flags,
+			      int nodeid)
+{
+	return kmem_cache_alloc_node(cachep, flags, nodeid);
+}
+#endif
+
+static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
+	struct kmem_cache *cachep;
+	void *ret;
+
 	if (__builtin_constant_p(size)) {
 		int i = 0;
 
@@ -84,11 +127,18 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 found:
 #ifdef CONFIG_ZONE_DMA
 		if (flags & GFP_DMA)
-			return kmem_cache_alloc_node(malloc_sizes[i].cs_dmacachep,
-						flags, node);
+			cachep = malloc_sizes[i].cs_dmacachep;
+		else
 #endif
-		return kmem_cache_alloc_node(malloc_sizes[i].cs_cachep,
-						flags, node);
+			cachep = malloc_sizes[i].cs_cachep;
+
+		ret = kmem_cache_alloc_node_notrace(cachep, flags, node);
+
+		kmemtrace_mark_alloc_node(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_,
+					  ret, size, slab_buffer_size(cachep),
+					  flags, node);
+
+		return ret;
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit 1c61fc40fc264059ff41a614ed2d899127288281
Author: Joe Perches <joe@perches.com>
Date:   Wed Mar 5 13:58:17 2008 -0800

    slab - use angle brackets for include of kmalloc_sizes.h
    
    Make them all use angle brackets and the directory name.
    
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index fcc48096ee64..39c3a5eb8ebe 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -41,7 +41,7 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 			goto found; \
 		else \
 			i++;
-#include "kmalloc_sizes.h"
+#include <linux/kmalloc_sizes.h>
 #undef CACHE
 		{
 			extern void __you_cannot_kmalloc_that_much(void);
@@ -75,7 +75,7 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 			goto found; \
 		else \
 			i++;
-#include "kmalloc_sizes.h"
+#include <linux/kmalloc_sizes.h>
 #undef CACHE
 		{
 			extern void __you_cannot_kmalloc_that_much(void);

commit 158a962422e4a54dc256b6a9b9562f3d30d34d9c
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Jan 2 13:04:48 2008 -0800

    Unify /proc/slabinfo configuration
    
    Both SLUB and SLAB really did almost exactly the same thing for
    /proc/slabinfo setup, using duplicate code and per-allocator #ifdef's.
    
    This just creates a common CONFIG_SLABINFO that is enabled by both SLUB
    and SLAB, and shares all the setup code.  Maybe SLOB will want this some
    day too.
    
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 32bdc2ffd715..fcc48096ee64 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -95,7 +95,4 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 
 #endif	/* CONFIG_NUMA */
 
-extern const struct seq_operations slabinfo_op;
-ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
-
 #endif	/* _LINUX_SLAB_DEF_H */

commit 81cda6626178cd55297831296ba8ecedbfd8b52d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:29 2007 -0700

    Slab allocators: Cleanup zeroing allocations
    
    It becomes now easy to support the zeroing allocs with generic inline
    functions in slab.h.  Provide inline definitions to allow the continued use of
    kzalloc, kmem_cache_zalloc etc but remove other definitions of zeroing
    functions from the slab allocators and util.c.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 16e814ffab8d..32bdc2ffd715 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -58,36 +58,6 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 	return __kmalloc(size, flags);
 }
 
-static inline void *kzalloc(size_t size, gfp_t flags)
-{
-	if (__builtin_constant_p(size)) {
-		int i = 0;
-
-		if (!size)
-			return ZERO_SIZE_PTR;
-
-#define CACHE(x) \
-		if (size <= x) \
-			goto found; \
-		else \
-			i++;
-#include "kmalloc_sizes.h"
-#undef CACHE
-		{
-			extern void __you_cannot_kzalloc_that_much(void);
-			__you_cannot_kzalloc_that_much();
-		}
-found:
-#ifdef CONFIG_ZONE_DMA
-		if (flags & GFP_DMA)
-			return kmem_cache_zalloc(malloc_sizes[i].cs_dmacachep,
-						flags);
-#endif
-		return kmem_cache_zalloc(malloc_sizes[i].cs_cachep, flags);
-	}
-	return __kzalloc(size, flags);
-}
-
 #ifdef CONFIG_NUMA
 extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
 extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);

commit 6cb8f91320d3e720351c21741da795fed580b21b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:22 2007 -0700

    Slab allocators: consistent ZERO_SIZE_PTR support and NULL result semantics
    
    Define ZERO_OR_NULL_PTR macro to be able to remove the checks from the
    allocators.  Move ZERO_SIZE_PTR related stuff into slab.h.
    
    Make ZERO_SIZE_PTR work for all slab allocators and get rid of the
    WARN_ON_ONCE(size == 0) that is still remaining in SLAB.
    
    Make slub return NULL like the other allocators if a too large memory segment
    is requested via __kmalloc.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 365d036c454a..16e814ffab8d 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -32,6 +32,10 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
 		int i = 0;
+
+		if (!size)
+			return ZERO_SIZE_PTR;
+
 #define CACHE(x) \
 		if (size <= x) \
 			goto found; \
@@ -58,6 +62,10 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
 		int i = 0;
+
+		if (!size)
+			return ZERO_SIZE_PTR;
+
 #define CACHE(x) \
 		if (size <= x) \
 			goto found; \
@@ -88,6 +96,10 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size)) {
 		int i = 0;
+
+		if (!size)
+			return ZERO_SIZE_PTR;
+
 #define CACHE(x) \
 		if (size <= x) \
 			goto found; \

commit 6193a2ff180920f84ee06977165ebf32431fc2d2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Jul 15 23:38:22 2007 -0700

    slob: initial NUMA support
    
    This adds preliminary NUMA support to SLOB, primarily aimed at systems with
    small nodes (tested all the way down to a 128kB SRAM block), whether
    asymmetric or otherwise.
    
    We follow the same conventions as SLAB/SLUB, preferring current node
    placement for new pages, or with explicit placement, if a node has been
    specified.  Presently on UP NUMA this has the side-effect of preferring
    node#0 allocations (since numa_node_id() == 0, though this could be
    reworked if we could hand off a pfn to determine node placement), so
    single-CPU NUMA systems will want to place smaller nodes further out in
    terms of node id.  Once a page has been bound to a node (via explicit node
    id typing), we only do block allocations from partial free pages that have
    a matching node id in the page flags.
    
    The current implementation does have some scalability problems, in that all
    partial free pages are tracked in the global freelist (with contention due
    to the single spinlock).  However, these are things that are being reworked
    for SMP scalability first, while things like per-node freelists can easily
    be built on top of this sort of functionality once it's been added.
    
    More background can be found in:
    
            http://marc.info/?l=linux-mm&m=118117916022379&w=2
            http://marc.info/?l=linux-mm&m=118170446306199&w=2
            http://marc.info/?l=linux-mm&m=118187859420048&w=2
    
    and subsequent threads.
    
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 8d81a60518e4..365d036c454a 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -25,6 +25,9 @@ struct cache_sizes {
 };
 extern struct cache_sizes malloc_sizes[];
 
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
+void *__kmalloc(size_t size, gfp_t flags);
+
 static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
@@ -79,6 +82,7 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 
 #ifdef CONFIG_NUMA
 extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
+extern void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {

commit 3ca12ee549f7837b8a685dddc9515f9fc28434ee
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:10:52 2007 -0700

    SLAB: Move two remaining SLAB specific definitions to slab_def.h
    
    Two definitions remained in slab.h that are particular to the SLAB allocator.
    Move to slab_def.h
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 5e4364644ed1..8d81a60518e4 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -109,4 +109,7 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 
 #endif	/* CONFIG_NUMA */
 
+extern const struct seq_operations slabinfo_op;
+ssize_t slabinfo_write(struct file *, const char __user *, size_t, loff_t *);
+
 #endif	/* _LINUX_SLAB_DEF_H */

commit 4b51d66989218aad731a721b5b28c79bf5388c09
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:10 2007 -0800

    [PATCH] optional ZONE_DMA: optional ZONE_DMA in the VM
    
    Make ZONE_DMA optional in core code.
    
    - ifdef all code for ZONE_DMA and related definitions following the example
      for ZONE_DMA32 and ZONE_HIGHMEM.
    
    - Without ZONE_DMA, ZONE_HIGHMEM and ZONE_DMA32 we get to a ZONES_SHIFT of
      0.
    
    - Modify the VM statistics to work correctly without a DMA zone.
    
    - Modify slab to not create DMA slabs if there is no ZONE_DMA.
    
    [akpm@osdl.org: cleanup]
    [jdike@addtoit.com: build fix]
    [apw@shadowen.org: Simplify calculation of the number of bits we need for ZONES_SHIFT]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Matthew Wilcox <willy@debian.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 4b463e66ddea..5e4364644ed1 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -19,7 +19,9 @@
 struct cache_sizes {
 	size_t		 	cs_size;
 	struct kmem_cache	*cs_cachep;
+#ifdef CONFIG_ZONE_DMA
 	struct kmem_cache	*cs_dmacachep;
+#endif
 };
 extern struct cache_sizes malloc_sizes[];
 
@@ -39,9 +41,12 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 			__you_cannot_kmalloc_that_much();
 		}
 found:
-		return kmem_cache_alloc((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags);
+#ifdef CONFIG_ZONE_DMA
+		if (flags & GFP_DMA)
+			return kmem_cache_alloc(malloc_sizes[i].cs_dmacachep,
+						flags);
+#endif
+		return kmem_cache_alloc(malloc_sizes[i].cs_cachep, flags);
 	}
 	return __kmalloc(size, flags);
 }
@@ -62,9 +67,12 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 			__you_cannot_kzalloc_that_much();
 		}
 found:
-		return kmem_cache_zalloc((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags);
+#ifdef CONFIG_ZONE_DMA
+		if (flags & GFP_DMA)
+			return kmem_cache_zalloc(malloc_sizes[i].cs_dmacachep,
+						flags);
+#endif
+		return kmem_cache_zalloc(malloc_sizes[i].cs_cachep, flags);
 	}
 	return __kzalloc(size, flags);
 }
@@ -88,9 +96,13 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 			__you_cannot_kmalloc_that_much();
 		}
 found:
-		return kmem_cache_alloc_node((flags & GFP_DMA) ?
-			malloc_sizes[i].cs_dmacachep :
-			malloc_sizes[i].cs_cachep, flags, node);
+#ifdef CONFIG_ZONE_DMA
+		if (flags & GFP_DMA)
+			return kmem_cache_alloc_node(malloc_sizes[i].cs_dmacachep,
+						flags, node);
+#endif
+		return kmem_cache_alloc_node(malloc_sizes[i].cs_cachep,
+						flags, node);
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit 2e892f43ccb602e8ffad73396a1000f2040c9e0b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 13 00:34:23 2006 -0800

    [PATCH] Cleanup slab headers / API to allow easy addition of new slab allocators
    
    This is a response to an earlier discussion on linux-mm about splitting
    slab.h components per allocator.  Patch is against 2.6.19-git11.  See
    http://marc.theaimsgroup.com/?l=linux-mm&m=116469577431008&w=2
    
    This patch cleans up the slab header definitions.  We define the common
    functions of slob and slab in slab.h and put the extra definitions needed
    for slab's kmalloc implementations in <linux/slab_def.h>.  In order to get
    a greater set of common functions we add several empty functions to slob.c
    and also rename slob's kmalloc to __kmalloc.
    
    Slob does not need any special definitions since we introduce a fallback
    case.  If there is no need for a slab implementation to provide its own
    kmalloc mess^H^H^Hacros then we simply fall back to __kmalloc functions.
    That is sufficient for SLOB.
    
    Sort the function in slab.h according to their functionality.  First the
    functions operating on struct kmem_cache * then the kmalloc related
    functions followed by special debug and fallback definitions.
    
    Also redo a lot of comments.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>?
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
new file mode 100644
index 000000000000..4b463e66ddea
--- /dev/null
+++ b/include/linux/slab_def.h
@@ -0,0 +1,100 @@
+#ifndef _LINUX_SLAB_DEF_H
+#define	_LINUX_SLAB_DEF_H
+
+/*
+ * Definitions unique to the original Linux SLAB allocator.
+ *
+ * What we provide here is a way to optimize the frequent kmalloc
+ * calls in the kernel by selecting the appropriate general cache
+ * if kmalloc was called with a size that can be established at
+ * compile time.
+ */
+
+#include <linux/init.h>
+#include <asm/page.h>		/* kmalloc_sizes.h needs PAGE_SIZE */
+#include <asm/cache.h>		/* kmalloc_sizes.h needs L1_CACHE_BYTES */
+#include <linux/compiler.h>
+
+/* Size description struct for general caches. */
+struct cache_sizes {
+	size_t		 	cs_size;
+	struct kmem_cache	*cs_cachep;
+	struct kmem_cache	*cs_dmacachep;
+};
+extern struct cache_sizes malloc_sizes[];
+
+static inline void *kmalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kmalloc_that_much(void);
+			__you_cannot_kmalloc_that_much();
+		}
+found:
+		return kmem_cache_alloc((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags);
+	}
+	return __kmalloc(size, flags);
+}
+
+static inline void *kzalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kzalloc_that_much(void);
+			__you_cannot_kzalloc_that_much();
+		}
+found:
+		return kmem_cache_zalloc((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags);
+	}
+	return __kzalloc(size, flags);
+}
+
+#ifdef CONFIG_NUMA
+extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
+
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	if (__builtin_constant_p(size)) {
+		int i = 0;
+#define CACHE(x) \
+		if (size <= x) \
+			goto found; \
+		else \
+			i++;
+#include "kmalloc_sizes.h"
+#undef CACHE
+		{
+			extern void __you_cannot_kmalloc_that_much(void);
+			__you_cannot_kmalloc_that_much();
+		}
+found:
+		return kmem_cache_alloc_node((flags & GFP_DMA) ?
+			malloc_sizes[i].cs_dmacachep :
+			malloc_sizes[i].cs_cachep, flags, node);
+	}
+	return __kmalloc_node(size, flags, node);
+}
+
+#endif	/* CONFIG_NUMA */
+
+#endif	/* _LINUX_SLAB_DEF_H */
