commit 2b69942f9021bf75bd1b001f53bd2578361fadf3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:04 2019 +0200

    posix-cpu-timers: Create a container struct
    
    Per task/process data of posix CPU timers is all over the place which
    makes the code hard to follow and requires ifdeffery.
    
    Create a container to hold all this information in one place, so data is
    consolidated and the ifdeffery can be confined to the posix timer header
    file and removed from places like fork.
    
    As a first step, move the cpu_timers list head array into the new struct
    and clean up the initializers and simplify fork. The remaining #ifdef in
    fork will be removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20190821192920.819418976@linutronix.de

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6049baa5b8bc..2c620d7ac432 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -36,17 +36,6 @@ extern struct cred init_cred;
 #define INIT_PREV_CPUTIME(x)
 #endif
 
-#ifdef CONFIG_POSIX_TIMERS
-#define INIT_CPU_TIMERS(s)						\
-	.cpu_timers = {							\
-		LIST_HEAD_INIT(s.cpu_timers[0]),			\
-		LIST_HEAD_INIT(s.cpu_timers[1]),			\
-		LIST_HEAD_INIT(s.cpu_timers[2]),			\
-	},
-#else
-#define INIT_CPU_TIMERS(s)
-#endif
-
 #define INIT_TASK_COMM "swapper"
 
 /* Attach to the init_task data structure for proper alignment */

commit f0b89d3958d73cd0785ec381f0ddf8efb6f183d8
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:30 2019 +0200

    sched/core: Convert task_struct.stack_refcount to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.stack_refcount is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.stack_refcount it might make a difference
    in following places:
    
     - try_get_task_stack(): increment in refcount_inc_not_zero() only
       guarantees control dependency on success vs. fully ordered
       atomic counterpart
     - put_task_stack(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-6-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a7083a45a26c..6049baa5b8bc 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -13,6 +13,7 @@
 #include <linux/securebits.h>
 #include <linux/seqlock.h>
 #include <linux/rbtree.h>
+#include <linux/refcount.h>
 #include <linux/sched/autogroup.h>
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>

commit 2c4704756cab7cfa031ada4dab361562f0e357c0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 26 13:06:43 2017 -0500

    pids: Move the pgrp and session pid pointers from task_struct to signal_struct
    
    To access these fields the code always has to go to group leader so
    going to signal struct is no loss and is actually a fundamental simplification.
    
    This saves a little bit of memory by only allocating the pid pointer array
    once instead of once for every thread, and even better this removes a
    few potential races caused by the fact that group_leader can be changed
    by de_thread, while signal_struct can not.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a454b8aeb938..a7083a45a26c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -46,15 +46,6 @@ extern struct cred init_cred;
 #define INIT_CPU_TIMERS(s)
 #endif
 
-#define INIT_PID_LINK(type) 					\
-{								\
-	.node = {						\
-		.next = NULL,					\
-		.pprev = NULL,					\
-	},							\
-	.pid = &init_struct_pid,				\
-}
-
 #define INIT_TASK_COMM "swapper"
 
 /* Attach to the init_task data structure for proper alignment */

commit e1e871aff3ded26348c631b1370e257d401cd22d
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand INIT_STRUCT_PID and remove
    
    Expand INIT_STRUCT_PID in the single place that uses it and then remove it.
    There doesn't seem any point in the macro.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5b5f41328115..a454b8aeb938 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -46,21 +46,6 @@ extern struct cred init_cred;
 #define INIT_CPU_TIMERS(s)
 #endif
 
-
-#define INIT_STRUCT_PID {						\
-	.count 		= ATOMIC_INIT(1),				\
-	.tasks		= {						\
-		{ .first = NULL },					\
-		{ .first = NULL },					\
-		{ .first = NULL },					\
-	},								\
-	.level		= 0,						\
-	.numbers	= { {						\
-		.nr		= 0,					\
-		.ns		= &init_pid_ns,				\
-	}, }								\
-}
-
 #define INIT_PID_LINK(type) 					\
 {								\
 	.node = {						\

commit 3678e2fcc293cf2ff1fe961838734a70c185de8a
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand the INIT_SIGNALS and INIT_SIGHAND macros and remove
    
    There doesn't seem to be any need to have the INIT_SIGNALS and INIT_SIGHAND
    macros, so expand them in their single places of use and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b1385e1dca63..5b5f41328115 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -23,6 +23,9 @@
 
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
+extern struct nsproxy init_nsproxy;
+extern struct group_info init_groups;
+extern struct cred init_cred;
 
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 #define INIT_PREV_CPUTIME(x)	.prev_cputime = {			\
@@ -33,52 +36,16 @@ extern struct fs_struct init_fs;
 #endif
 
 #ifdef CONFIG_POSIX_TIMERS
-#define INIT_POSIX_TIMERS(s)						\
-	.posix_timers = LIST_HEAD_INIT(s.posix_timers),
 #define INIT_CPU_TIMERS(s)						\
 	.cpu_timers = {							\
 		LIST_HEAD_INIT(s.cpu_timers[0]),			\
 		LIST_HEAD_INIT(s.cpu_timers[1]),			\
-		LIST_HEAD_INIT(s.cpu_timers[2]),								\
-	},
-#define INIT_CPUTIMER(s)						\
-	.cputimer	= { 						\
-		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
-		.running	= false,				\
-		.checking_timer = false,				\
+		LIST_HEAD_INIT(s.cpu_timers[2]),			\
 	},
 #else
-#define INIT_POSIX_TIMERS(s)
 #define INIT_CPU_TIMERS(s)
-#define INIT_CPUTIMER(s)
 #endif
 
-#define INIT_SIGNALS(sig) {						\
-	.nr_threads	= 1,						\
-	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),	\
-	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
-	.shared_pending	= { 						\
-		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\
-		.signal =  {{0}}},					\
-	INIT_POSIX_TIMERS(sig)						\
-	INIT_CPU_TIMERS(sig)						\
-	.rlim		= INIT_RLIMITS,					\
-	INIT_CPUTIMER(sig)						\
-	INIT_PREV_CPUTIME(sig)						\
-	.cred_guard_mutex =						\
-		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
-}
-
-extern struct nsproxy init_nsproxy;
-
-#define INIT_SIGHAND(sighand) {						\
-	.count		= ATOMIC_INIT(1), 				\
-	.action		= { { { .sa_handler = SIG_DFL, } }, },		\
-	.siglock	= __SPIN_LOCK_UNLOCKED(sighand.siglock),	\
-	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(sighand.signalfd_wqh),	\
-}
-
-extern struct group_info init_groups;
 
 #define INIT_STRUCT_PID {						\
 	.count 		= ATOMIC_INIT(1),				\
@@ -103,8 +70,6 @@ extern struct group_info init_groups;
 	.pid = &init_struct_pid,				\
 }
 
-extern struct cred init_cred;
-
 #define INIT_TASK_COMM "swapper"
 
 /* Attach to the init_task data structure for proper alignment */

commit 4e7e3adbba5224604b34b0d42003ff6dbdc8ddd9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand various INIT_* macros and remove
    
    Expand various INIT_* macros into the single places they're used in
    init/init_task.c and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9711611b831d..b1385e1dca63 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -21,23 +21,9 @@
 
 #include <asm/thread_info.h>
 
-#ifdef CONFIG_SMP
-# define INIT_PUSHABLE_TASKS(tsk)					\
-	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO),
-#else
-# define INIT_PUSHABLE_TASKS(tsk)
-#endif
-
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
-#ifdef CONFIG_CPUSETS
-#define INIT_CPUSET_SEQ(tsk)							\
-	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
-#else
-#define INIT_CPUSET_SEQ(tsk)
-#endif
-
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 #define INIT_PREV_CPUTIME(x)	.prev_cputime = {			\
 	.lock = __RAW_SPIN_LOCK_UNLOCKED(x.prev_cputime.lock),		\
@@ -117,107 +103,10 @@ extern struct group_info init_groups;
 	.pid = &init_struct_pid,				\
 }
 
-#ifdef CONFIG_AUDITSYSCALL
-#define INIT_IDS \
-	.loginuid = INVALID_UID, \
-	.sessionid = (unsigned int)-1,
-#else
-#define INIT_IDS
-#endif
-
-#ifdef CONFIG_PREEMPT_RCU
-#define INIT_TASK_RCU_PREEMPT(tsk)					\
-	.rcu_read_lock_nesting = 0,					\
-	.rcu_read_unlock_special.s = 0,					\
-	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
-	.rcu_blocked_node = NULL,
-#else
-#define INIT_TASK_RCU_PREEMPT(tsk)
-#endif
-#ifdef CONFIG_TASKS_RCU
-#define INIT_TASK_RCU_TASKS(tsk)					\
-	.rcu_tasks_holdout = false,					\
-	.rcu_tasks_holdout_list =					\
-		LIST_HEAD_INIT(tsk.rcu_tasks_holdout_list),		\
-	.rcu_tasks_idle_cpu = -1,
-#else
-#define INIT_TASK_RCU_TASKS(tsk)
-#endif
-
 extern struct cred init_cred;
 
-#ifdef CONFIG_CGROUP_SCHED
-# define INIT_CGROUP_SCHED(tsk)						\
-	.sched_task_group = &root_task_group,
-#else
-# define INIT_CGROUP_SCHED(tsk)
-#endif
-
-#ifdef CONFIG_PERF_EVENTS
-# define INIT_PERF_EVENTS(tsk)						\
-	.perf_event_mutex = 						\
-		 __MUTEX_INITIALIZER(tsk.perf_event_mutex),		\
-	.perf_event_list = LIST_HEAD_INIT(tsk.perf_event_list),
-#else
-# define INIT_PERF_EVENTS(tsk)
-#endif
-
-#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-# define INIT_VTIME(tsk)						\
-	.vtime.seqcount = SEQCNT_ZERO(tsk.vtime.seqcount),		\
-	.vtime.starttime = 0,						\
-	.vtime.state = VTIME_SYS,
-#else
-# define INIT_VTIME(tsk)
-#endif
-
 #define INIT_TASK_COMM "swapper"
 
-#ifdef CONFIG_RT_MUTEXES
-# define INIT_RT_MUTEXES(tsk)						\
-	.pi_waiters = RB_ROOT_CACHED,					\
-	.pi_top_task = NULL,
-#else
-# define INIT_RT_MUTEXES(tsk)
-#endif
-
-#ifdef CONFIG_NUMA_BALANCING
-# define INIT_NUMA_BALANCING(tsk)					\
-	.numa_preferred_nid = -1,					\
-	.numa_group = NULL,						\
-	.numa_faults = NULL,
-#else
-# define INIT_NUMA_BALANCING(tsk)
-#endif
-
-#ifdef CONFIG_KASAN
-# define INIT_KASAN(tsk)						\
-	.kasan_depth = 1,
-#else
-# define INIT_KASAN(tsk)
-#endif
-
-#ifdef CONFIG_LIVEPATCH
-# define INIT_LIVEPATCH(tsk)						\
-	.patch_state = KLP_UNDEFINED,
-#else
-# define INIT_LIVEPATCH(tsk)
-#endif
-
-#ifdef CONFIG_THREAD_INFO_IN_TASK
-# define INIT_TASK_TI(tsk)			\
-	.thread_info = INIT_THREAD_INFO(tsk),	\
-	.stack_refcount = ATOMIC_INIT(1),
-#else
-# define INIT_TASK_TI(tsk)
-#endif
-
-#ifdef CONFIG_SECURITY
-#define INIT_TASK_SECURITY .security = NULL,
-#else
-#define INIT_TASK_SECURITY
-#endif
-
 /* Attach to the init_task data structure for proper alignment */
 #ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 #define __init_task_data __attribute__((__section__(".data..init_task")))
@@ -228,5 +117,4 @@ extern struct cred init_cred;
 /* Attach to the thread_info data structure for proper alignment */
 #define __init_thread_info __attribute__((__section__(".data..init_thread_info")))
 
-
 #endif

commit d11ed3ab3166a2bfad60681aebf3e13e1c3408a9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand INIT_TASK() in init/init_task.c and remove
    
    It's no longer necessary to have an INIT_TASK() macro, and this can be
    expanded into the one place it is now used and removed.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 30a89b99a5af..9711611b831d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -218,91 +218,12 @@ extern struct cred init_cred;
 #define INIT_TASK_SECURITY
 #endif
 
-/*
- *  INIT_TASK is used to set up the first task table, touch at
- * your own risk!. Base=0, limit=0x1fffff (=2MB)
- */
-#define INIT_TASK(tsk)	\
-{									\
-	INIT_TASK_TI(tsk)						\
-	.state		= 0,						\
-	.stack		= init_stack,					\
-	.usage		= ATOMIC_INIT(2),				\
-	.flags		= PF_KTHREAD,					\
-	.prio		= MAX_PRIO-20,					\
-	.static_prio	= MAX_PRIO-20,					\
-	.normal_prio	= MAX_PRIO-20,					\
-	.policy		= SCHED_NORMAL,					\
-	.cpus_allowed	= CPU_MASK_ALL,					\
-	.nr_cpus_allowed= NR_CPUS,					\
-	.mm		= NULL,						\
-	.active_mm	= &init_mm,					\
-	.restart_block = {						\
-		.fn = do_no_restart_syscall,				\
-	},								\
-	.se		= {						\
-		.group_node 	= LIST_HEAD_INIT(tsk.se.group_node),	\
-	},								\
-	.rt		= {						\
-		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
-		.time_slice	= RR_TIMESLICE,				\
-	},								\
-	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
-	INIT_PUSHABLE_TASKS(tsk)					\
-	INIT_CGROUP_SCHED(tsk)						\
-	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
-	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
-	.real_parent	= &tsk,						\
-	.parent		= &tsk,						\
-	.children	= LIST_HEAD_INIT(tsk.children),			\
-	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
-	.group_leader	= &tsk,						\
-	RCU_POINTER_INITIALIZER(real_cred, &init_cred),			\
-	RCU_POINTER_INITIALIZER(cred, &init_cred),			\
-	.comm		= INIT_TASK_COMM,				\
-	.thread		= INIT_THREAD,					\
-	.fs		= &init_fs,					\
-	.files		= &init_files,					\
-	.signal		= &init_signals,				\
-	.sighand	= &init_sighand,				\
-	.nsproxy	= &init_nsproxy,				\
-	.pending	= {						\
-		.list = LIST_HEAD_INIT(tsk.pending.list),		\
-		.signal = {{0}}},					\
-	.blocked	= {{0}},					\
-	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
-	.journal_info	= NULL,						\
-	INIT_CPU_TIMERS(tsk)						\
-	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
-	.timer_slack_ns = 50000, /* 50 usec default slack */		\
-	.pids = {							\
-		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
-		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
-		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
-	},								\
-	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
-	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),	\
-	INIT_IDS							\
-	INIT_PERF_EVENTS(tsk)						\
-	INIT_TRACE_IRQFLAGS						\
-	INIT_LOCKDEP							\
-	INIT_FTRACE_GRAPH						\
-	INIT_TRACE_RECURSION						\
-	INIT_TASK_RCU_PREEMPT(tsk)					\
-	INIT_TASK_RCU_TASKS(tsk)					\
-	INIT_CPUSET_SEQ(tsk)						\
-	INIT_RT_MUTEXES(tsk)						\
-	INIT_PREV_CPUTIME(tsk)						\
-	INIT_VTIME(tsk)							\
-	INIT_NUMA_BALANCING(tsk)					\
-	INIT_KASAN(tsk)							\
-	INIT_LIVEPATCH(tsk)						\
-	INIT_TASK_SECURITY						\
-}
-
-
 /* Attach to the init_task data structure for proper alignment */
+#ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 #define __init_task_data __attribute__((__section__(".data..init_task")))
+#else
+#define __init_task_data /**/
+#endif
 
 /* Attach to the thread_info data structure for proper alignment */
 #define __init_thread_info __attribute__((__section__(".data..init_thread_info")))

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6a532629c983..30a89b99a5af 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -304,5 +304,8 @@ extern struct cred init_cred;
 /* Attach to the init_task data structure for proper alignment */
 #define __init_task_data __attribute__((__section__(".data..init_task")))
 
+/* Attach to the thread_info data structure for proper alignment */
+#define __init_thread_info __attribute__((__section__(".data..init_thread_info")))
+
 
 #endif

commit e8cfbc245e24887e3c30235f71e9e9405e0cfc39
Author: Gargi Sharma <gs051095@gmail.com>
Date:   Fri Nov 17 15:30:34 2017 -0800

    pid: remove pidhash
    
    pidhash is no longer required as all the information can be looked up
    from idr tree.  nr_hashed represented the number of pids that had been
    hashed.  Since, nr_hashed and PIDNS_HASH_ADDING are no longer relevant,
    it has been renamed to pid_allocated and PIDNS_ADDING respectively.
    
    [gs051095@gmail.com: v6]
      Link: http://lkml.kernel.org/r/1507760379-21662-3-git-send-email-gs051095@gmail.com
    Link: http://lkml.kernel.org/r/1507583624-22146-3-git-send-email-gs051095@gmail.com
    Signed-off-by: Gargi Sharma <gs051095@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>      [ia64]
    Cc: Julia Lawall <julia.lawall@lip6.fr>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 8062e6cc607c..6a532629c983 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -105,7 +105,6 @@ extern struct group_info init_groups;
 	.numbers	= { {						\
 		.nr		= 0,					\
 		.ns		= &init_pid_ns,				\
-		.pid_chain	= { .next = NULL, .pprev = NULL },	\
 	}, }								\
 }
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3c07ace5b431..8062e6cc607c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX__INIT_TASK_H
 #define _LINUX__INIT_TASK_H
 

commit a23ba907d5e65d6aeea3e59c82fda9cd206a7aad
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:15:01 2017 -0700

    locking/rtmutex: replace top-waiter and pi_waiters leftmost caching
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-10-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 0e849715e5be..3c07ace5b431 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -175,9 +175,8 @@ extern struct cred init_cred;
 
 #ifdef CONFIG_RT_MUTEXES
 # define INIT_RT_MUTEXES(tsk)						\
-	.pi_waiters = RB_ROOT,						\
-	.pi_top_task = NULL,						\
-	.pi_waiters_leftmost = NULL,
+	.pi_waiters = RB_ROOT_CACHED,					\
+	.pi_top_task = NULL,
 #else
 # define INIT_RT_MUTEXES(tsk)
 #endif

commit 4e3274705324c695f56b3e1c194cd99b76eeea0f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 7 16:02:24 2017 -0700

    init_task: Remove redundant INIT_TASK_RCU_TREE_PREEMPT() macro
    
    Back in the dim distant past, the task_struct structure's RCU-related
    fields optionally included those needed for CONFIG_RCU_BOOST, even in
    CONFIG_PREEMPT_RCU builds.  The INIT_TASK_RCU_TREE_PREEMPT() macro was
    used to provide initializers for those optional CONFIG_RCU_BOOST fields.
    However, the CONFIG_RCU_BOOST fields are now included unconditionally
    in CONFIG_PREEMPT_RCU builds, so there is no longer any need fro the
    INIT_TASK_RCU_TREE_PREEMPT() macro.  This commit therefore removes it
    in favor of initializing the ->rcu_blocked_node field directly in the
    INIT_TASK_RCU_PREEMPT() macro.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a2f6707e9fc0..0e849715e5be 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -125,18 +125,12 @@ extern struct group_info init_groups;
 #define INIT_IDS
 #endif
 
-#ifdef CONFIG_PREEMPT_RCU
-#define INIT_TASK_RCU_TREE_PREEMPT()					\
-	.rcu_blocked_node = NULL,
-#else
-#define INIT_TASK_RCU_TREE_PREEMPT(tsk)
-#endif
 #ifdef CONFIG_PREEMPT_RCU
 #define INIT_TASK_RCU_PREEMPT(tsk)					\
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special.s = 0,					\
 	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
-	INIT_TASK_RCU_TREE_PREEMPT()
+	.rcu_blocked_node = NULL,
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)
 #endif

commit bac5b6b6b11560f323e71d0ebac4061cfe5f56c0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:10 2017 +0200

    sched/cputime: Move the vtime task fields to their own struct
    
    We are about to add vtime accumulation fields to the task struct. Let's
    avoid more bloatification and gather vtime information to their own
    struct.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-5-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3d537331cd4e..a2f6707e9fc0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -170,9 +170,9 @@ extern struct cred init_cred;
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 # define INIT_VTIME(tsk)						\
-	.vtime_seqcount = SEQCNT_ZERO(tsk.vtime_seqcount),	\
-	.vtime_starttime = 0,				\
-	.vtime_state = VTIME_SYS,
+	.vtime.seqcount = SEQCNT_ZERO(tsk.vtime.seqcount),		\
+	.vtime.starttime = 0,						\
+	.vtime.state = VTIME_SYS,
 #else
 # define INIT_VTIME(tsk)
 #endif

commit 60a9ce57e7c5ac1df3a39fb941022bbfa40c0862
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:09 2017 +0200

    sched/cputime: Rename vtime fields
    
    The current "snapshot" based naming on vtime fields suggests we record
    some past event but that's a low level picture of their actual purpose
    which comes out blurry. The real point of these fields is to run a basic
    state machine that tracks down cputime entry while switching between
    contexts.
    
    So lets reflect that with more meaningful names.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e049526bc188..3d537331cd4e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -171,8 +171,8 @@ extern struct cred init_cred;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 # define INIT_VTIME(tsk)						\
 	.vtime_seqcount = SEQCNT_ZERO(tsk.vtime_seqcount),	\
-	.vtime_snap = 0,				\
-	.vtime_snap_whence = VTIME_SYS,
+	.vtime_starttime = 0,				\
+	.vtime_state = VTIME_SYS,
 #else
 # define INIT_VTIME(tsk)
 #endif

commit 0302e28dee643932ee7b3c112ebccdbb9f8ec32c
Merge: 89c9fea3c803 8979b02aaf1d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 08:50:52 2017 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "Highlights:
    
      IMA:
       - provide ">" and "<" operators for fowner/uid/euid rules
    
      KEYS:
       - add a system blacklist keyring
    
       - add KEYCTL_RESTRICT_KEYRING, exposes keyring link restriction
         functionality to userland via keyctl()
    
      LSM:
       - harden LSM API with __ro_after_init
    
       - add prlmit security hook, implement for SELinux
    
       - revive security_task_alloc hook
    
      TPM:
       - implement contextual TPM command 'spaces'"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (98 commits)
      tpm: Fix reference count to main device
      tpm_tis: convert to using locality callbacks
      tpm: fix handling of the TPM 2.0 event logs
      tpm_crb: remove a cruft constant
      keys: select CONFIG_CRYPTO when selecting DH / KDF
      apparmor: Make path_max parameter readonly
      apparmor: fix parameters so that the permission test is bypassed at boot
      apparmor: fix invalid reference to index variable of iterator line 836
      apparmor: use SHASH_DESC_ON_STACK
      security/apparmor/lsm.c: set debug messages
      apparmor: fix boolreturn.cocci warnings
      Smack: Use GFP_KERNEL for smk_netlbl_mls().
      smack: fix double free in smack_parse_opts_str()
      KEYS: add SP800-56A KDF support for DH
      KEYS: Keyring asymmetric key restrict method with chaining
      KEYS: Restrict asymmetric key linkage using a specific keychain
      KEYS: Add a lookup_restriction function for the asymmetric key type
      KEYS: Add KEYCTL_RESTRICT_KEYRING
      KEYS: Consistent ordering for __key_link_begin and restrict check
      KEYS: Add an optional lookup_restriction hook to key_type
      ...

commit 76f1948a79b26d5f57a5ee9941876b745c6baaea
Merge: 7af4c727c7b6 a0841609f658
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 18:24:16 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatch updates from Jiri Kosina:
    
     - a per-task consistency model is being added for architectures that
       support reliable stack dumping (extending this, currently rather
       trivial set, is currently in the works).
    
       This extends the nature of the types of patches that can be applied
       by live patching infrastructure. The code stems from the design
       proposal made [1] back in November 2014. It's a hybrid of SUSE's
       kGraft and RH's kpatch, combining advantages of both: it uses
       kGraft's per-task consistency and syscall barrier switching combined
       with kpatch's stack trace switching. There are also a number of
       fallback options which make it quite flexible.
    
       Most of the heavy lifting done by Josh Poimboeuf with help from
       Miroslav Benes and Petr Mladek
    
       [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
     - module load time patch optimization from Zhou Chengming
    
     - a few assorted small fixes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: add missing printk newlines
      livepatch: Cancel transition a safe way for immediate patches
      livepatch: Reduce the time of finding module symbols
      livepatch: make klp_mutex proper part of API
      livepatch: allow removal of a disabled patch
      livepatch: add /proc/<pid>/patch_state
      livepatch: change to a per-task consistency model
      livepatch: store function sizes
      livepatch: use kstrtobool() in enabled_store()
      livepatch: move patching functions into patch.c
      livepatch: remove unnecessary object loaded check
      livepatch: separate enabled and patched states
      livepatch/s390: add TIF_PATCH_PENDING thread flag
      livepatch/s390: reorganize TIF thread flag bits
      livepatch/powerpc: add TIF_PATCH_PENDING thread flag
      livepatch/x86: add TIF_PATCH_PENDING thread flag
      livepatch: create temporary klp_update_patch_state() stub
      x86/entry: define _TIF_ALLWORK_MASK flags explicitly
      stacktrace/x86: add function for detecting reliable stack traces

commit e96a7705e7d3fef96aec9b590c63b2f6f7d2ba22
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Thu Mar 23 15:56:08 2017 +0100

    sched/rtmutex/deadline: Fix a PI crash for deadline tasks
    
    A crash happened while I was playing with deadline PI rtmutex.
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
        IP: [<ffffffff810eeb8f>] rt_mutex_get_top_task+0x1f/0x30
        PGD 232a75067 PUD 230947067 PMD 0
        Oops: 0000 [#1] SMP
        CPU: 1 PID: 10994 Comm: a.out Not tainted
    
        Call Trace:
        [<ffffffff810b658c>] enqueue_task+0x2c/0x80
        [<ffffffff810ba763>] activate_task+0x23/0x30
        [<ffffffff810d0ab5>] pull_dl_task+0x1d5/0x260
        [<ffffffff810d0be6>] pre_schedule_dl+0x16/0x20
        [<ffffffff8164e783>] __schedule+0xd3/0x900
        [<ffffffff8164efd9>] schedule+0x29/0x70
        [<ffffffff8165035b>] __rt_mutex_slowlock+0x4b/0xc0
        [<ffffffff81650501>] rt_mutex_slowlock+0xd1/0x190
        [<ffffffff810eeb33>] rt_mutex_timed_lock+0x53/0x60
        [<ffffffff810ecbfc>] futex_lock_pi.isra.18+0x28c/0x390
        [<ffffffff810ed8b0>] do_futex+0x190/0x5b0
        [<ffffffff810edd50>] SyS_futex+0x80/0x180
    
    This is because rt_mutex_enqueue_pi() and rt_mutex_dequeue_pi()
    are only protected by pi_lock when operating pi waiters, while
    rt_mutex_get_top_task(), will access them with rq lock held but
    not holding pi_lock.
    
    In order to tackle it, we introduce new "pi_top_task" pointer
    cached in task_struct, and add new rt_mutex_update_top_task()
    to update its value, it can be called by rt_mutex_setprio()
    which held both owner's pi_lock and rq lock. Thus "pi_top_task"
    can be safely accessed by enqueue_task_dl() under rq lock.
    
    Originally-From: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: juri.lelli@arm.com
    Cc: bigeasy@linutronix.de
    Cc: mathieu.desnoyers@efficios.com
    Cc: jdesfossez@efficios.com
    Cc: bristot@redhat.com
    Link: http://lkml.kernel.org/r/20170323150216.157682758@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 91d9049f0039..2c487e0879d5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -181,6 +181,7 @@ extern struct cred init_cred;
 #ifdef CONFIG_RT_MUTEXES
 # define INIT_RT_MUTEXES(tsk)						\
 	.pi_waiters = RB_ROOT,						\
+	.pi_top_task = NULL,						\
 	.pi_waiters_leftmost = NULL,
 #else
 # define INIT_RT_MUTEXES(tsk)

commit e4e55b47ed9ae2c05ff062601ff6dacbe9dc4775
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Mar 24 20:46:33 2017 +0900

    LSM: Revive security_task_alloc() hook and per "struct task_struct" security blob.
    
    We switched from "struct task_struct"->security to "struct cred"->security
    in Linux 2.6.29. But not all LSM modules were happy with that change.
    TOMOYO LSM module is an example which want to use per "struct task_struct"
    security blob, for TOMOYO's security context is defined based on "struct
    task_struct" rather than "struct cred". AppArmor LSM module is another
    example which want to use it, for AppArmor is currently abusing the cred
    a little bit to store the change_hat and setexeccon info. Although
    security_task_free() hook was revived in Linux 3.4 because Yama LSM module
    wanted to release per "struct task_struct" security blob,
    security_task_alloc() hook and "struct task_struct"->security field were
    not revived. Nowadays, we are getting proposals of lightweight LSM modules
    which want to use per "struct task_struct" security blob.
    
    We are already allowing multiple concurrent LSM modules (up to one fully
    armored module which uses "struct cred"->security field or exclusive hooks
    like security_xfrm_state_pol_flow_match(), plus unlimited number of
    lightweight modules which do not use "struct cred"->security nor exclusive
    hooks) as long as they are built into the kernel. But this patch does not
    implement variable length "struct task_struct"->security field which will
    become needed when multiple LSM modules want to use "struct task_struct"->
    security field. Although it won't be difficult to implement variable length
    "struct task_struct"->security field, let's think about it after we merged
    this patch.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: John Johansen <john.johansen@canonical.com>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Acked-by: Casey Schaufler <casey@schaufler-ca.com>
    Tested-by: Djalal Harouni <tixxdz@gmail.com>
    Acked-by: José Bollo <jobol@nonadev.net>
    Cc: Paul Moore <paul@paul-moore.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Eric Paris <eparis@parisplace.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: José Bollo <jobol@nonadev.net>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 91d9049f0039..926f2f553cc5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -210,6 +210,12 @@ extern struct cred init_cred;
 # define INIT_TASK_TI(tsk)
 #endif
 
+#ifdef CONFIG_SECURITY
+#define INIT_TASK_SECURITY .security = NULL,
+#else
+#define INIT_TASK_SECURITY
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -288,6 +294,7 @@ extern struct cred init_cred;
 	INIT_VTIME(tsk)							\
 	INIT_NUMA_BALANCING(tsk)					\
 	INIT_KASAN(tsk)							\
+	INIT_TASK_SECURITY						\
 }
 
 

commit d83a7cb375eec21f04c83542395d08b2f6641da2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:40 2017 -0600

    livepatch: change to a per-task consistency model
    
    Change livepatch to use a basic per-task consistency model.  This is the
    foundation which will eventually enable us to patch those ~10% of
    security patches which change function or data semantics.  This is the
    biggest remaining piece needed to make livepatch more generally useful.
    
    This code stems from the design proposal made by Vojtech [1] in November
    2014.  It's a hybrid of kGraft and kpatch: it uses kGraft's per-task
    consistency and syscall barrier switching combined with kpatch's stack
    trace switching.  There are also a number of fallback options which make
    it quite flexible.
    
    Patches are applied on a per-task basis, when the task is deemed safe to
    switch over.  When a patch is enabled, livepatch enters into a
    transition state where tasks are converging to the patched state.
    Usually this transition state can complete in a few seconds.  The same
    sequence occurs when a patch is disabled, except the tasks converge from
    the patched state to the unpatched state.
    
    An interrupt handler inherits the patched state of the task it
    interrupts.  The same is true for forked tasks: the child inherits the
    patched state of the parent.
    
    Livepatch uses several complementary approaches to determine when it's
    safe to patch tasks:
    
    1. The first and most effective approach is stack checking of sleeping
       tasks.  If no affected functions are on the stack of a given task,
       the task is patched.  In most cases this will patch most or all of
       the tasks on the first try.  Otherwise it'll keep trying
       periodically.  This option is only available if the architecture has
       reliable stacks (HAVE_RELIABLE_STACKTRACE).
    
    2. The second approach, if needed, is kernel exit switching.  A
       task is switched when it returns to user space from a system call, a
       user space IRQ, or a signal.  It's useful in the following cases:
    
       a) Patching I/O-bound user tasks which are sleeping on an affected
          function.  In this case you have to send SIGSTOP and SIGCONT to
          force it to exit the kernel and be patched.
       b) Patching CPU-bound user tasks.  If the task is highly CPU-bound
          then it will get patched the next time it gets interrupted by an
          IRQ.
       c) In the future it could be useful for applying patches for
          architectures which don't yet have HAVE_RELIABLE_STACKTRACE.  In
          this case you would have to signal most of the tasks on the
          system.  However this isn't supported yet because there's
          currently no way to patch kthreads without
          HAVE_RELIABLE_STACKTRACE.
    
    3. For idle "swapper" tasks, since they don't ever exit the kernel, they
       instead have a klp_update_patch_state() call in the idle loop which
       allows them to be patched before the CPU enters the idle state.
    
       (Note there's not yet such an approach for kthreads.)
    
    All the above approaches may be skipped by setting the 'immediate' flag
    in the 'klp_patch' struct, which will disable per-task consistency and
    patch all tasks immediately.  This can be useful if the patch doesn't
    change any function or data semantics.  Note that, even with this flag
    set, it's possible that some tasks may still be running with an old
    version of the function, until that function returns.
    
    There's also an 'immediate' flag in the 'klp_func' struct which allows
    you to specify that certain functions in the patch can be applied
    without per-task consistency.  This might be useful if you want to patch
    a common function like schedule(), and the function change doesn't need
    consistency but the rest of the patch does.
    
    For architectures which don't have HAVE_RELIABLE_STACKTRACE, the user
    must set patch->immediate which causes all tasks to be patched
    immediately.  This option should be used with care, only when the patch
    doesn't change any function or data semantics.
    
    In the future, architectures which don't have HAVE_RELIABLE_STACKTRACE
    may be allowed to use per-task consistency if we can come up with
    another way to patch kthreads.
    
    The /sys/kernel/livepatch/<patch>/transition file shows whether a patch
    is in transition.  Only a single patch (the topmost patch on the stack)
    can be in transition at a given time.  A patch can remain in transition
    indefinitely, if any of the tasks are stuck in the initial patch state.
    
    A transition can be reversed and effectively canceled by writing the
    opposite value to the /sys/kernel/livepatch/<patch>/enabled file while
    the transition is in progress.  Then all the tasks will attempt to
    converge back to the original patch state.
    
    [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the scheduler changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 91d9049f0039..5a791055b176 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -15,6 +15,7 @@
 #include <linux/sched/autogroup.h>
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>
+#include <linux/livepatch.h>
 #include <linux/mm_types.h>
 
 #include <asm/thread_info.h>
@@ -202,6 +203,13 @@ extern struct cred init_cred;
 # define INIT_KASAN(tsk)
 #endif
 
+#ifdef CONFIG_LIVEPATCH
+# define INIT_LIVEPATCH(tsk)						\
+	.patch_state = KLP_UNDEFINED,
+#else
+# define INIT_LIVEPATCH(tsk)
+#endif
+
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 # define INIT_TASK_TI(tsk)			\
 	.thread_info = INIT_THREAD_INFO(tsk),	\
@@ -288,6 +296,7 @@ extern struct cred init_cred;
 	INIT_VTIME(tsk)							\
 	INIT_NUMA_BALANCING(tsk)					\
 	INIT_KASAN(tsk)							\
+	INIT_LIVEPATCH(tsk)						\
 }
 
 

commit 70806b21e4d64f01193a2b64a053b75ff53a2b10
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:15:21 2017 +0100

    sched/headers: Move the 'root_task_group' declaration to <linux/sched/autogroup.h>
    
    Also remove the duplicate declaration from <linux/init_task.h>.
    
    ( That declaration was originally duplicated for dependency hell reasons,
      but there's no problem including the much smaller <linux/sched/autogroup.h>
      header now, to pick up the right prototype. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f6841c19c913..91d9049f0039 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -151,8 +151,6 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
-extern struct task_group root_task_group;
-
 #ifdef CONFIG_CGROUP_SCHED
 # define INIT_CGROUP_SCHED(tsk)						\
 	.sched_task_group = &root_task_group,

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 452c9799318c..f6841c19c913 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -15,6 +15,7 @@
 #include <linux/sched/autogroup.h>
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>
+#include <linux/mm_types.h>
 
 #include <asm/thread_info.h>
 

commit dfc3401a33086a3fd465468e171ea0e82430569b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:15:21 2017 +0100

    sched/headers: Prepare to move the 'root_task_group' declaration to <linux/sched/autogroup.h>
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3a85d61f7614..452c9799318c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -12,6 +12,7 @@
 #include <linux/securebits.h>
 #include <linux/seqlock.h>
 #include <linux/rbtree.h>
+#include <linux/sched/autogroup.h>
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>
 

commit b18b6a9cef7f30e9a8b7738d5fc8d568cf660855
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sat Jan 21 00:09:08 2017 -0500

    timers: Omit POSIX timer stuff from task_struct when disabled
    
    When CONFIG_POSIX_TIMERS is disabled, it is preferable to remove related
    structures from struct task_struct and struct signal_struct as they
    won't contain anything useful and shouldn't be relied upon by mistake.
    Code still referencing those structures is also disabled here.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 325f649d77ff..3a85d61f7614 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -42,6 +42,27 @@ extern struct fs_struct init_fs;
 #define INIT_PREV_CPUTIME(x)
 #endif
 
+#ifdef CONFIG_POSIX_TIMERS
+#define INIT_POSIX_TIMERS(s)						\
+	.posix_timers = LIST_HEAD_INIT(s.posix_timers),
+#define INIT_CPU_TIMERS(s)						\
+	.cpu_timers = {							\
+		LIST_HEAD_INIT(s.cpu_timers[0]),			\
+		LIST_HEAD_INIT(s.cpu_timers[1]),			\
+		LIST_HEAD_INIT(s.cpu_timers[2]),								\
+	},
+#define INIT_CPUTIMER(s)						\
+	.cputimer	= { 						\
+		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
+		.running	= false,				\
+		.checking_timer = false,				\
+	},
+#else
+#define INIT_POSIX_TIMERS(s)
+#define INIT_CPU_TIMERS(s)
+#define INIT_CPUTIMER(s)
+#endif
+
 #define INIT_SIGNALS(sig) {						\
 	.nr_threads	= 1,						\
 	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),	\
@@ -49,14 +70,10 @@ extern struct fs_struct init_fs;
 	.shared_pending	= { 						\
 		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\
 		.signal =  {{0}}},					\
-	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
-	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
+	INIT_POSIX_TIMERS(sig)						\
+	INIT_CPU_TIMERS(sig)						\
 	.rlim		= INIT_RLIMITS,					\
-	.cputimer	= { 						\
-		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
-		.running	= false,				\
-		.checking_timer = false,				\
-	},								\
+	INIT_CPUTIMER(sig)						\
 	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
@@ -247,7 +264,7 @@ extern struct task_group root_task_group;
 	.blocked	= {{0}},					\
 	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
 	.journal_info	= NULL,						\
-	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
+	INIT_CPU_TIMERS(tsk)						\
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
 	.timer_slack_ns = 50000, /* 50 usec default slack */		\
 	.pids = {							\
@@ -274,13 +291,6 @@ extern struct task_group root_task_group;
 }
 
 
-#define INIT_CPU_TIMERS(cpu_timers)					\
-{									\
-	LIST_HEAD_INIT(cpu_timers[0]),					\
-	LIST_HEAD_INIT(cpu_timers[1]),					\
-	LIST_HEAD_INIT(cpu_timers[2]),					\
-}
-
 /* Attach to the init_task data structure for proper alignment */
 #define __init_task_data __attribute__((__section__(".data..init_task")))
 

commit 68f24b08ee892d47bdef925d676e1ae1ccc316f8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:48 2016 -0700

    sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
    
    We currently keep every task's stack around until the task_struct
    itself is freed.  This means that we keep the stack allocation alive
    for longer than necessary and that, under load, we free stacks in
    big batches whenever RCU drops the last task reference.  Neither of
    these is good for reuse of cache-hot memory, and freeing in batches
    prevents us from usefully caching small numbers of vmalloced stacks.
    
    On architectures that have thread_info on the stack, we can't easily
    change this, but on architectures that set THREAD_INFO_IN_TASK, we
    can free it as soon as the task is dead.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/08ca06cde00ebed0046c5d26cbbf3fbb7ef5b812.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9c04d44eeb3c..325f649d77ff 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -186,7 +186,9 @@ extern struct task_group root_task_group;
 #endif
 
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-# define INIT_TASK_TI(tsk) .thread_info = INIT_THREAD_INFO(tsk),
+# define INIT_TASK_TI(tsk)			\
+	.thread_info = INIT_THREAD_INFO(tsk),	\
+	.stack_refcount = ATOMIC_INIT(1),
 #else
 # define INIT_TASK_TI(tsk)
 #endif

commit c65eacbe290b8141554c71b2c94489e73ade8c8d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:24 2016 -0700

    sched/core: Allow putting thread_info into task_struct
    
    If an arch opts in by setting CONFIG_THREAD_INFO_IN_TASK_STRUCT,
    then thread_info is defined as a single 'u32 flags' and is the first
    entry of task_struct.  thread_info::task is removed (it serves no
    purpose if thread_info is embedded in task_struct), and
    thread_info::cpu gets its own slot in task_struct.
    
    This is heavily based on a patch written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a0898196f0476195ca02713691a5037a14f2aac5.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f8834f820ec2..9c04d44eeb3c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -15,6 +15,8 @@
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>
 
+#include <asm/thread_info.h>
+
 #ifdef CONFIG_SMP
 # define INIT_PUSHABLE_TASKS(tsk)					\
 	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO),
@@ -183,12 +185,19 @@ extern struct task_group root_task_group;
 # define INIT_KASAN(tsk)
 #endif
 
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+# define INIT_TASK_TI(tsk) .thread_info = INIT_THREAD_INFO(tsk),
+#else
+# define INIT_TASK_TI(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
 #define INIT_TASK(tsk)	\
 {									\
+	INIT_TASK_TI(tsk)						\
 	.state		= 0,						\
 	.stack		= init_stack,					\
 	.usage		= ATOMIC_INIT(2),				\

commit 7f1a00b6fcd0e3c19beba2e92d157dc0c2cf3494
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 24 17:07:33 2016 -0700

    fix up initial thread stack pointer vs thread_info confusion
    
    The INIT_TASK() initializer was similarly confused about the stack vs
    thread_info allocation that the allocators had, and that were fixed in
    commit b235beea9e99 ("Clarify naming of thread info/stack allocators").
    
    The task ->stack pointer only incidentally ends up having the same value
    as the thread_info, and in fact that will change.
    
    So fix the initial task struct initializer to point to 'init_stack'
    instead of 'init_thread_info', and make sure the ia64 definition for
    that exists.
    
    This actually makes the ia64 tsk->stack pointer be sensible for the
    initial task, but not for any other task.  As mentioned in commit
    b235beea9e99, that whole pointer isn't actually used on ia64, since
    task_stack_page() there just points to the (single) allocation.
    
    All the other architectures seem to have copied the 'init_stack'
    definition, even if it tended to be generally unusued.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f2cb8d45513d..f8834f820ec2 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -190,7 +190,7 @@ extern struct task_group root_task_group;
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
-	.stack		= &init_thread_info,				\
+	.stack		= init_stack,					\
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= PF_KTHREAD,					\
 	.prio		= MAX_PRIO-20,					\

commit b7ce2277f087fd052e7e1bbf432f7fecbee82bb6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 19 16:47:34 2015 +0100

    sched/cputime: Convert vtime_seqlock to seqcount
    
    The cputime can only be updated by the current task itself, even in
    vtime case. So we can safely use seqcount instead of seqlock as there
    is no writer concurrency involved.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447948054-28668-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1c1ff7e4faa4..f2cb8d45513d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -150,7 +150,7 @@ extern struct task_group root_task_group;
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 # define INIT_VTIME(tsk)						\
-	.vtime_seqlock = __SEQLOCK_UNLOCKED(tsk.vtime_seqlock),	\
+	.vtime_seqcount = SEQCNT_ZERO(tsk.vtime_seqcount),	\
 	.vtime_snap = 0,				\
 	.vtime_snap_whence = VTIME_SYS,
 #else

commit 69234acee54407962a20bedf90ef9c96326994b5
Merge: 11eaaadb3ea3 d57456753787
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 14:51:32 2015 -0800

    Merge branch 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "The cgroup core saw several significant updates this cycle:
    
       - percpu_rwsem for threadgroup locking is reinstated.  This was
         temporarily dropped due to down_write latency issues.  Oleg's
         rework of percpu_rwsem which is scheduled to be merged in this
         merge window resolves the issue.
    
       - On the v2 hierarchy, when controllers are enabled and disabled, all
         operations are atomic and can fail and revert cleanly.  This allows
         ->can_attach() failure which is necessary for cpu RT slices.
    
       - Tasks now stay associated with the original cgroups after exit
         until released.  This allows tracking resources held by zombies
         (e.g.  pids) and makes it easy to find out where zombies came from
         on the v2 hierarchy.  The pids controller was broken before these
         changes as zombies escaped the limits; unfortunately, updating this
         behavior required too many invasive changes and I don't think it's
         a good idea to backport them, so the pids controller on 4.3, the
         first version which included the pids controller, will stay broken
         at least until I'm sure about the cgroup core changes.
    
       - Optimization of a couple common tests using static_key"
    
    * 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (38 commits)
      cgroup: fix race condition around termination check in css_task_iter_next()
      blkcg: don't create "io.stat" on the root cgroup
      cgroup: drop cgroup__DEVEL__legacy_files_on_dfl
      cgroup: replace error handling in cgroup_init() with WARN_ON()s
      cgroup: add cgroup_subsys->free() method and use it to fix pids controller
      cgroup: keep zombies associated with their original cgroups
      cgroup: make css_set_rwsem a spinlock and rename it to css_set_lock
      cgroup: don't hold css_set_rwsem across css task iteration
      cgroup: reorganize css_task_iter functions
      cgroup: factor out css_set_move_task()
      cgroup: keep css_set and task lists in chronological order
      cgroup: make cgroup_destroy_locked() test cgroup_is_populated()
      cgroup: make css_sets pin the associated cgroups
      cgroup: relocate cgroup_[try]get/put()
      cgroup: move check_for_release() invocation
      cgroup: replace cgroup_has_tasks() with cgroup_is_populated()
      cgroup: make cgroup->nr_populated count the number of populated css_sets
      cgroup: remove an unused parameter from cgroup_task_migrate()
      cgroup: fix too early usage of static_branch_disable()
      cgroup: make cgroup_update_dfl_csses() migrate all target processes atomically
      ...

commit c8d75aa47dd585c9538a8205e9bb9847e12cfb84
Author: Jason Low <jason.low2@hp.com>
Date:   Wed Oct 14 12:07:56 2015 -0700

    posix_cpu_timer: Reduce unnecessary sighand lock contention
    
    It was found while running a database workload on large systems that
    significant time was spent trying to acquire the sighand lock.
    
    The issue was that whenever an itimer expired, many threads ended up
    simultaneously trying to send the signal. Most of the time, nothing
    happened after acquiring the sighand lock because another thread
    had just already sent the signal and updated the "next expire" time.
    The fastpath_timer_check() didn't help much since the "next expire"
    time was updated after the threads exit fastpath_timer_check().
    
    This patch addresses this by having the thread_group_cputimer structure
    maintain a boolean to signify when a thread in the group is already
    checking for process wide timers, and adds extra logic in the fastpath
    to check the boolean.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: George Spelvin <linux@horizon.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: hideaki.kimura@hpe.com
    Cc: terry.rudd@hpe.com
    Cc: scott.norton@hpe.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1444849677-29330-5-git-send-email-jason.low2@hp.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index c43b80f3f875..810a34f60424 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -60,6 +60,7 @@ extern struct fs_struct init_fs;
 	.cputimer	= { 						\
 		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
 		.running	= false,				\
+		.checking_timer = false,				\
 	},								\
 	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\

commit d5c373eb5610686162ff50429f63f4c00c554799
Author: Jason Low <jason.low2@hp.com>
Date:   Wed Oct 14 12:07:55 2015 -0700

    posix_cpu_timer: Convert cputimer->running to bool
    
    In the next patch in this series, a new field 'checking_timer' will
    be added to 'struct thread_group_cputimer'. Both this and the
    existing 'running' integer field are just used as boolean values. To
    save space in the structure, we can make both of these fields booleans.
    
    This is a preparatory patch to convert the existing running integer
    field to a boolean.
    
    Suggested-by: George Spelvin <linux@horizon.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed: George Spelvin <linux@horizon.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: hideaki.kimura@hpe.com
    Cc: terry.rudd@hpe.com
    Cc: scott.norton@hpe.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1444849677-29330-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e38681f4912d..c43b80f3f875 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -59,7 +59,7 @@ extern struct fs_struct init_fs;
 	.rlim		= INIT_RLIMITS,					\
 	.cputimer	= { 						\
 		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
-		.running	= 0,					\
+		.running	= false,				\
 	},								\
 	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\

commit 1ed1328792ff46e4bb86a3d7f7be2971f4549f6c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 12:53:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    Note: This commit was originally committed as d59cfc09c32a but got
          reverted by 0c986253b939 due to the performance regression from
          the percpu_rwsem write down/up operations added to cgroup task
          migration path.  percpu_rwsem changes which alleviate the
          performance issue are pending for v4.4-rc1 merge window.
          Re-apply.
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e38681f4912d..d0b380ee7d67 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -25,13 +25,6 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
-#ifdef CONFIG_CGROUPS
-#define INIT_GROUP_RWSEM(sig)						\
-	.group_rwsem = __RWSEM_INITIALIZER(sig.group_rwsem),
-#else
-#define INIT_GROUP_RWSEM(sig)
-#endif
-
 #ifdef CONFIG_CPUSETS
 #define INIT_CPUSET_SEQ(tsk)							\
 	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
@@ -64,7 +57,6 @@ extern struct fs_struct init_fs;
 	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
-	INIT_GROUP_RWSEM(sig)						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 0c986253b939cc14c69d4adbe2b4121bdf4aa220
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 11:51:12 2015 -0400

    Revert "sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem"
    
    This reverts commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14.
    
    d59cfc09c32a ("sched, cgroup: replace signal_struct->group_rwsem with
    a global percpu_rwsem") and b5ba75b5fc0e ("cgroup: simplify
    threadgroup locking") changed how cgroup synchronizes against task
    fork and exits so that it uses global percpu_rwsem instead of
    per-process rwsem; unfortunately, the write [un]lock paths of
    percpu_rwsem always involve synchronize_rcu_expedited() which turned
    out to be too expensive.
    
    Improvements for percpu_rwsem are scheduled to be merged in the coming
    v4.4-rc1 merge window which alleviates this issue.  For now, revert
    the two commits to restore per-process rwsem.  They will be re-applied
    for the v4.4-rc1 merge window.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: stable@vger.kernel.org # v4.2+

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d0b380ee7d67..e38681f4912d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -25,6 +25,13 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
+#ifdef CONFIG_CGROUPS
+#define INIT_GROUP_RWSEM(sig)						\
+	.group_rwsem = __RWSEM_INITIALIZER(sig.group_rwsem),
+#else
+#define INIT_GROUP_RWSEM(sig)
+#endif
+
 #ifdef CONFIG_CPUSETS
 #define INIT_CPUSET_SEQ(tsk)							\
 	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
@@ -57,6 +64,7 @@ extern struct fs_struct init_fs;
 	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
+	INIT_GROUP_RWSEM(sig)						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 9d7fb04276481c59610983362d8e023d262b58ca
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 30 11:30:54 2015 +0200

    sched/cputime: Guarantee stime + utime == rtime
    
    While the current code guarantees monotonicity for stime and utime
    independently of one another, it does not guarantee that the sum of
    both is equal to the total time we started out with.
    
    This confuses things (and peoples) who look at this sum, like top, and
    will report >100% usage followed by a matching period of 0%.
    
    Rework the code to provide both individual monotonicity and a coherent
    sum.
    
    Suggested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Reported-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Tested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jason.low2@hp.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e8493fee8160..d0b380ee7d67 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -32,6 +32,14 @@ extern struct fs_struct init_fs;
 #define INIT_CPUSET_SEQ(tsk)
 #endif
 
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
+#define INIT_PREV_CPUTIME(x)	.prev_cputime = {			\
+	.lock = __RAW_SPIN_LOCK_UNLOCKED(x.prev_cputime.lock),		\
+},
+#else
+#define INIT_PREV_CPUTIME(x)
+#endif
+
 #define INIT_SIGNALS(sig) {						\
 	.nr_threads	= 1,						\
 	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),	\
@@ -46,6 +54,7 @@ extern struct fs_struct init_fs;
 		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
 		.running	= 0,					\
 	},								\
+	INIT_PREV_CPUTIME(sig)						\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
 }
@@ -246,6 +255,7 @@ extern struct task_group root_task_group;
 	INIT_TASK_RCU_TASKS(tsk)					\
 	INIT_CPUSET_SEQ(tsk)						\
 	INIT_RT_MUTEXES(tsk)						\
+	INIT_PREV_CPUTIME(tsk)						\
 	INIT_VTIME(tsk)							\
 	INIT_NUMA_BALANCING(tsk)					\
 	INIT_KASAN(tsk)							\

commit bbe179f88d39274630823a0dc07d2714fd19a103
Merge: 4b703b1d4c46 8a0792ef8e01
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 19:50:04 2015 -0700

    Merge branch 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - threadgroup_lock got reorganized so that its users can pick the
       actual locking mechanism to use.  Its only user - cgroups - is
       updated to use a percpu_rwsem instead of per-process rwsem.
    
       This makes things a bit lighter on hot paths and allows cgroups to
       perform and fail multi-task (a process) migrations atomically.
       Multi-task migrations are used in several places including the
       unified hierarchy.
    
     - Delegation rule and documentation added to unified hierarchy.  This
       will likely be the last interface update from the cgroup core side
       for unified hierarchy before lifting the devel mask.
    
     - Some groundwork for the pids controller which is scheduled to be
       merged in the coming devel cycle.
    
    * 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: add delegation section to unified hierarchy documentation
      cgroup: require write perm on common ancestor when moving processes on the default hierarchy
      cgroup: separate out cgroup_procs_write_permission() from __cgroup_procs_write()
      kernfs: make kernfs_get_inode() public
      MAINTAINERS: add a cgroup core co-maintainer
      cgroup: fix uninitialised iterator in for_each_subsys_which
      cgroup: replace explicit ss_mask checking with for_each_subsys_which
      cgroup: use bitmask to filter for_each_subsys
      cgroup: add seq_file forward declaration for struct cftype
      cgroup: simplify threadgroup locking
      sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
      sched, cgroup: reorganize threadgroup locking
      cgroup: switch to unsigned long for bitmasks
      cgroup: reorganize include/linux/cgroup.h
      cgroup: separate out include/linux/cgroup-defs.h
      cgroup: fix some comment typos

commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 696d22312b31..0cc0bbf20022 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -25,13 +25,6 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
-#ifdef CONFIG_CGROUPS
-#define INIT_GROUP_RWSEM(sig)						\
-	.group_rwsem = __RWSEM_INITIALIZER(sig.group_rwsem),
-#else
-#define INIT_GROUP_RWSEM(sig)
-#endif
-
 #ifdef CONFIG_CPUSETS
 #define INIT_CPUSET_SEQ(tsk)							\
 	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
@@ -56,7 +49,6 @@ extern struct fs_struct init_fs;
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
-	INIT_GROUP_RWSEM(sig)						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 7110744516276e906f9197e2857d026eb2343393
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:24 2015 -0700

    sched, timer: Use the atomic task_cputime in thread_group_cputimer
    
    Recent optimizations were made to thread_group_cputimer to improve its
    scalability by keeping track of cputime stats without a lock. However,
    the values were open coded to the structure, causing them to be at
    a different abstraction level from the regular task_cputime structure.
    Furthermore, any subsequent similar optimizations would not be able to
    share the new code, since they are specific to thread_group_cputimer.
    
    This patch adds the new task_cputime_atomic data structure (introduced in
    the previous patch in the series) to thread_group_cputimer for keeping
    track of the cputime atomically, which also helps generalize the code.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-6-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 7b9d8b59e7bf..bb9b075f0eb0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -50,10 +50,8 @@ extern struct fs_struct init_fs;
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
 	.cputimer	= { 						\
-		.utime 		  = ATOMIC64_INIT(0),			\
-		.stime		  = ATOMIC64_INIT(0),			\
-		.sum_exec_runtime = ATOMIC64_INIT(0),			\
-		.running 	  = 0					\
+		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\
+		.running	= 0,					\
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\

commit 1018016c706f7ff9f56fde3a649789c47085a293
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:22 2015 -0700

    sched, timer: Replace spinlocks with atomics in thread_group_cputimer(), to improve scalability
    
    While running a database workload, we found a scalability issue with itimers.
    
    Much of the problem was caused by the thread_group_cputimer spinlock.
    Each time we account for group system/user time, we need to obtain a
    thread_group_cputimer's spinlock to update the timers. On larger systems
    (such as a 16 socket machine), this caused more than 30% of total time
    spent trying to obtain this kernel lock to update these group timer stats.
    
    This patch converts the timers to 64-bit atomic variables and use
    atomic add to update them without a lock. With this patch, the percent
    of total time spent updating thread group cputimer timers was reduced
    from 30% down to less than 1%.
    
    Note: On 32-bit systems using the generic 64-bit atomics, this causes
    sample_group_cputimer() to take locks 3 times instead of just 1 time.
    However, we tested this patch on a 32-bit system ARM system using the
    generic atomics and did not find the overhead to be much of an issue.
    An explanation for why this isn't an issue is that 32-bit systems usually
    have small numbers of CPUs, and cacheline contention from extra spinlocks
    called periodically is not really apparent on smaller systems.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 696d22312b31..7b9d8b59e7bf 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -50,9 +50,10 @@ extern struct fs_struct init_fs;
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
 	.cputimer	= { 						\
-		.cputime = INIT_CPUTIME,				\
-		.running = 0,						\
-		.lock = __RAW_SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
+		.utime 		  = ATOMIC64_INIT(0),			\
+		.stime		  = ATOMIC64_INIT(0),			\
+		.sum_exec_runtime = ATOMIC64_INIT(0),			\
+		.running 	  = 0					\
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\

commit c420f167db8c799d69fe43a801c58a7f02e9d57c
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:39:59 2015 -0800

    kasan: enable stack instrumentation
    
    Stack instrumentation allows to detect out of bounds memory accesses for
    variables allocated on stack.  Compiler adds redzones around every
    variable on stack and poisons redzones in function's prologue.
    
    Such approach significantly increases stack usage, so all in-kernel stacks
    size were doubled.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d3d43ecf148c..696d22312b31 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -175,6 +175,13 @@ extern struct task_group root_task_group;
 # define INIT_NUMA_BALANCING(tsk)
 #endif
 
+#ifdef CONFIG_KASAN
+# define INIT_KASAN(tsk)						\
+	.kasan_depth = 1,
+#else
+# define INIT_KASAN(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -250,6 +257,7 @@ extern struct task_group root_task_group;
 	INIT_RT_MUTEXES(tsk)						\
 	INIT_VTIME(tsk)							\
 	INIT_NUMA_BALANCING(tsk)					\
+	INIT_KASAN(tsk)							\
 }
 
 

commit f56141e3e2d9aabf7e6b89680ab572c2cdbb2a24
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Feb 12 15:01:14 2015 -0800

    all arches, signal: move restart_block to struct task_struct
    
    If an attacker can cause a controlled kernel stack overflow, overwriting
    the restart block is a very juicy exploit target.  This is because the
    restart_block is held in the same memory allocation as the kernel stack.
    
    Moving the restart block to struct task_struct prevents this exploit by
    making the restart_block harder to locate.
    
    Note that there are other fields in thread_info that are also easy
    targets, at least on some architectures.
    
    It's also a decent simplification, since the restart code is more or less
    identical on all architectures.
    
    [james.hogan@imgtec.com: metag: align thread_info::supervisor_stack]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Richard Weinberger <richard@nod.at>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3037fc085e8e..d3d43ecf148c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -193,6 +193,9 @@ extern struct task_group root_task_group;
 	.nr_cpus_allowed= NR_CPUS,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
+	.restart_block = {						\
+		.fn = do_no_restart_syscall,				\
+	},								\
 	.se		= {						\
 		.group_node 	= LIST_HEAD_INIT(tsk.se.group_node),	\
 	},								\

commit 86c6a2fddf0b89b494c7616f2c06cf915c4bff01
Merge: bee2782f30f6 fd7de1e8d5b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 9 21:21:34 2014 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - 'Nested Sleep Debugging', activated when CONFIG_DEBUG_ATOMIC_SLEEP=y.
    
         This instruments might_sleep() checks to catch places that nest
         blocking primitives - such as mutex usage in a wait loop.  Such
         bugs can result in hard to debug races/hangs.
    
         Another category of invalid nesting that this facility will detect
         is the calling of blocking functions from within schedule() ->
         sched_submit_work() -> blk_schedule_flush_plug().
    
         There's some potential for false positives (if secondary blocking
         primitives themselves are not ready yet for this facility), but the
         kernel will warn once about such bugs per bootup, so the warning
         isn't much of a nuisance.
    
         This feature comes with a number of fixes, for problems uncovered
         with it, so no messages are expected normally.
    
       - Another round of sched/numa optimizations and refinements, for
         CONFIG_NUMA_BALANCING=y.
    
       - Another round of sched/dl fixes and refinements.
    
      Plus various smaller fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      sched: Add missing rcu protection to wake_up_all_idle_cpus
      sched/deadline: Introduce start_hrtick_dl() for !CONFIG_SCHED_HRTICK
      sched/numa: Init numa balancing fields of init_task
      sched/deadline: Remove unnecessary definitions in cpudeadline.h
      sched/cpupri: Remove unnecessary definitions in cpupri.h
      sched/deadline: Fix rq->dl.pushable_tasks bug in push_dl_task()
      sched/fair: Fix stale overloaded status in the busiest group finding logic
      sched: Move p->nr_cpus_allowed check to select_task_rq()
      sched/completion: Document when to use wait_for_completion_io_*()
      sched: Update comments about CLONE_NEWUTS and CLONE_NEWIPC
      sched/fair: Kill task_struct::numa_entry and numa_group::task_list
      sched: Refactor task_struct to use numa_faults instead of numa_* pointers
      sched/deadline: Don't check CONFIG_SMP in switched_from_dl()
      sched/deadline: Reschedule from switched_from_dl() after a successful pull
      sched/deadline: Push task away if the deadline is equal to curr during wakeup
      sched/deadline: Add deadline rq status print
      sched/deadline: Fix artificial overrun introduced by yield_task_dl()
      sched/rt: Clean up check_preempt_equal_prio()
      sched/core: Use dl_bw_of() under rcu_read_lock_sched()
      sched: Check if we got a shallowest_idle_cpu before searching for least_loaded_cpu
      ...

commit d8b163c4c657478ef33c082cff78d03a4ca07bb2
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Tue Nov 11 12:46:29 2014 +0300

    sched/numa: Init numa balancing fields of init_task
    
    We do not initialize init_task.numa_preferred_nid,
    but this value is inherited by userspace "init"
    process:
    
    rest_init()->kernel_thread(kernel_init)->do_fork(CLONE_VM);
    
    __sched_fork()
    {
            if (clone_flags & CLONE_VM)
                    p->numa_preferred_nid = current->numa_preferred_nid;
            else
                    p->numa_preferred_nid = -1;
    }
    
    kernel_init() becomes userspace "init" process.
    
    So, we propagate garbage nid to userspace, and it may be used
    during numa balancing.
    
    Currently, we do not have reports about this brings a problem,
    but it seem we should set it for sure.
    
    Even if init_task.numa_preferred_nid is zero, we may meet a weird
    configuration without nid#0. On sparc64, where processors are
    numbered physically, I saw a machine without cpu#1, while cpu#2
    existed. Possible, something similar may be with numa nodes.
    So, let's initialize it and be sure we're safe.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sergey Dyasly <dserrg@gmail.com>
    Link: http://lkml.kernel.org/r/1415699189.15631.6.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 77fc43f8fb72..5f30ac8c82bc 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -166,6 +166,15 @@ extern struct task_group root_task_group;
 # define INIT_RT_MUTEXES(tsk)
 #endif
 
+#ifdef CONFIG_NUMA_BALANCING
+# define INIT_NUMA_BALANCING(tsk)					\
+	.numa_preferred_nid = -1,					\
+	.numa_group = NULL,						\
+	.numa_faults = NULL,
+#else
+# define INIT_NUMA_BALANCING(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -237,6 +246,7 @@ extern struct task_group root_task_group;
 	INIT_CPUSET_SEQ(tsk)						\
 	INIT_RT_MUTEXES(tsk)						\
 	INIT_VTIME(tsk)							\
+	INIT_NUMA_BALANCING(tsk)					\
 }
 
 

commit 28f6569ab7d036cd4ee94c26bb76dc1b3f3fc056
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Mon Sep 22 14:00:48 2014 -0400

    rcu: Remove redundant TREE_PREEMPT_RCU config option
    
    PREEMPT_RCU and TREE_PREEMPT_RCU serve the same function after
    TINY_PREEMPT_RCU has been removed. This patch removes TREE_PREEMPT_RCU
    and uses PREEMPT_RCU config option in its place.
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 77fc43f8fb72..d996aef8044f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -102,7 +102,7 @@ extern struct group_info init_groups;
 #define INIT_IDS
 #endif
 
-#ifdef CONFIG_TREE_PREEMPT_RCU
+#ifdef CONFIG_PREEMPT_RCU
 #define INIT_TASK_RCU_TREE_PREEMPT()					\
 	.rcu_blocked_node = NULL,
 #else

commit 1d082fd061884a587c490c4fc8a2056ce1e47624
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 14 16:01:53 2014 -0700

    rcu: Remove local_irq_disable() in rcu_preempt_note_context_switch()
    
    The rcu_preempt_note_context_switch() function is on a scheduling fast
    path, so it would be good to avoid disabling irqs.  The reason that irqs
    are disabled is to synchronize process-level and irq-handler access to
    the task_struct ->rcu_read_unlock_special bitmask.  This commit therefore
    makes ->rcu_read_unlock_special instead be a union of bools with a short
    allowing single-access checks in RCU's __rcu_read_unlock().  This results
    in the process-level and irq-handler accesses being simple loads and
    stores, so that irqs need no longer be disabled.  This commit therefore
    removes the irq disabling from rcu_preempt_note_context_switch().
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 03b274873b06..77fc43f8fb72 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -111,7 +111,7 @@ extern struct group_info init_groups;
 #ifdef CONFIG_PREEMPT_RCU
 #define INIT_TASK_RCU_PREEMPT(tsk)					\
 	.rcu_read_lock_nesting = 0,					\
-	.rcu_read_unlock_special = 0,					\
+	.rcu_read_unlock_special.s = 0,					\
 	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
 	INIT_TASK_RCU_TREE_PREEMPT()
 #else

commit 176f8f7a52cc6d09d686f0d900abda6942a52fbb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 4 17:43:50 2014 -0700

    rcu: Make TASKS_RCU handle nohz_full= CPUs
    
    Currently TASKS_RCU would ignore a CPU running a task in nohz_full=
    usermode execution.  There would be neither a context switch nor a
    scheduling-clock interrupt to tell TASKS_RCU that the task in question
    had passed through a quiescent state.  The grace period would therefore
    extend indefinitely.  This commit therefore makes RCU's dyntick-idle
    subsystem record the task_struct structure of the task that is running
    in dyntick-idle mode on each CPU.  The TASKS_RCU grace period can
    then access this information and record a quiescent state on
    behalf of any CPU running in dyntick-idle usermode.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index dffd9258ee60..03b274873b06 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -121,7 +121,8 @@ extern struct group_info init_groups;
 #define INIT_TASK_RCU_TASKS(tsk)					\
 	.rcu_tasks_holdout = false,					\
 	.rcu_tasks_holdout_list =					\
-		LIST_HEAD_INIT(tsk.rcu_tasks_holdout_list),
+		LIST_HEAD_INIT(tsk.rcu_tasks_holdout_list),		\
+	.rcu_tasks_idle_cpu = -1,
 #else
 #define INIT_TASK_RCU_TASKS(tsk)
 #endif

commit 8315f42295d2667a7f942f154b73a86fd7cb2227
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 27 13:42:20 2014 -0700

    rcu: Add call_rcu_tasks()
    
    This commit adds a new RCU-tasks flavor of RCU, which provides
    call_rcu_tasks().  This RCU flavor's quiescent states are voluntary
    context switch (not preemption!) and userspace execution (not the idle
    loop -- use some sort of schedule_on_each_cpu() if you need to handle the
    idle tasks.  Note that unlike other RCU flavors, these quiescent states
    occur in tasks, not necessarily CPUs.  Includes fixes from Steven Rostedt.
    
    This RCU flavor is assumed to have very infrequent latency-tolerant
    updaters.  This assumption permits significant simplifications, including
    a single global callback list protected by a single global lock, along
    with a single task-private linked list containing all tasks that have not
    yet passed through a quiescent state.  If experience shows this assumption
    to be incorrect, the required additional complexity will be added.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 2bb4c4f3531a..dffd9258ee60 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -117,6 +117,14 @@ extern struct group_info init_groups;
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)
 #endif
+#ifdef CONFIG_TASKS_RCU
+#define INIT_TASK_RCU_TASKS(tsk)					\
+	.rcu_tasks_holdout = false,					\
+	.rcu_tasks_holdout_list =					\
+		LIST_HEAD_INIT(tsk.rcu_tasks_holdout_list),
+#else
+#define INIT_TASK_RCU_TASKS(tsk)
+#endif
 
 extern struct cred init_cred;
 
@@ -224,6 +232,7 @@ extern struct task_group root_task_group;
 	INIT_FTRACE_GRAPH						\
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
+	INIT_TASK_RCU_TASKS(tsk)					\
 	INIT_CPUSET_SEQ(tsk)						\
 	INIT_RT_MUTEXES(tsk)						\
 	INIT_VTIME(tsk)							\

commit abaa93d9e1de2c29297e69ddba8ddd38f15064cf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 12 13:30:25 2014 -0700

    rcu: Simplify priority boosting by putting rt_mutex in rcu_node
    
    RCU priority boosting currently checks for boosting via a pointer in
    task_struct.  However, this is not needed: As Oleg noted, if the
    rt_mutex is placed in the rcu_node instead of on the booster's stack,
    the boostee can simply check it see if it owns the lock.  This commit
    makes this change, shrinking task_struct by one pointer and the kernel
    by thirteen lines.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6df7f9fe0d01..2bb4c4f3531a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -102,12 +102,6 @@ extern struct group_info init_groups;
 #define INIT_IDS
 #endif
 
-#ifdef CONFIG_RCU_BOOST
-#define INIT_TASK_RCU_BOOST()						\
-	.rcu_boost_mutex = NULL,
-#else
-#define INIT_TASK_RCU_BOOST()
-#endif
 #ifdef CONFIG_TREE_PREEMPT_RCU
 #define INIT_TASK_RCU_TREE_PREEMPT()					\
 	.rcu_blocked_node = NULL,
@@ -119,8 +113,7 @@ extern struct group_info init_groups;
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special = 0,					\
 	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
-	INIT_TASK_RCU_TREE_PREEMPT()					\
-	INIT_TASK_RCU_BOOST()
+	INIT_TASK_RCU_TREE_PREEMPT()
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)
 #endif

commit 6dd9158ae8577372aa433e6b0eae3c3d4caa5439
Merge: 90804ed61f24 f3411cb2b2e3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 23 18:08:10 2014 -0800

    Merge git://git.infradead.org/users/eparis/audit
    
    Pull audit update from Eric Paris:
     "Again we stayed pretty well contained inside the audit system.
      Venturing out was fixing a couple of function prototypes which were
      inconsistent (didn't hurt anything, but we used the same value as an
      int, uint, u32, and I think even a long in a couple of places).
    
      We also made a couple of minor changes to when a couple of LSMs called
      the audit system.  We hoped to add aarch64 audit support this go
      round, but it wasn't ready.
    
      I'm disappearing on vacation on Thursday.  I should have internet
      access, but it'll be spotty.  If anything goes wrong please be sure to
      cc rgb@redhat.com.  He'll make fixing things his top priority"
    
    * git://git.infradead.org/users/eparis/audit: (50 commits)
      audit: whitespace fix in kernel-parameters.txt
      audit: fix location of __net_initdata for audit_net_ops
      audit: remove pr_info for every network namespace
      audit: Modify a set of system calls in audit class definitions
      audit: Convert int limit uses to u32
      audit: Use more current logging style
      audit: Use hex_byte_pack_upper
      audit: correct a type mismatch in audit_syscall_exit()
      audit: reorder AUDIT_TTY_SET arguments
      audit: rework AUDIT_TTY_SET to only grab spin_lock once
      audit: remove needless switch in AUDIT_SET
      audit: use define's for audit version
      audit: documentation of audit= kernel parameter
      audit: wait_for_auditd rework for readability
      audit: update MAINTAINERS
      audit: log task info on feature change
      audit: fix incorrect set of audit_sock
      audit: print error message when fail to create audit socket
      audit: fix dangling keywords in audit_log_set_loginuid() output
      audit: log on errors from filter user rules
      ...

commit 0c740d0afc3bff0a097ad03a1c8df92757516f5c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 21 15:49:56 2014 -0800

    introduce for_each_thread() to replace the buggy while_each_thread()
    
    while_each_thread() and next_thread() should die, almost every lockless
    usage is wrong.
    
    1. Unless g == current, the lockless while_each_thread() is not safe.
    
       while_each_thread(g, t) can loop forever if g exits, next_thread()
       can't reach the unhashed thread in this case. Note that this can
       happen even if g is the group leader, it can exec.
    
    2. Even if while_each_thread() itself was correct, people often use
       it wrongly.
    
       It was never safe to just take rcu_read_lock() and loop unless
       you verify that pid_alive(g) == T, even the first next_thread()
       can point to the already freed/reused memory.
    
    This patch adds signal_struct->thread_head and task->thread_node to
    create the normal rcu-safe list with the stable head.  The new
    for_each_thread(g, t) helper is always safe under rcu_read_lock() as
    long as this task_struct can't go away.
    
    Note: of course it is ugly to have both task_struct->thread_node and the
    old task_struct->thread_group, we will kill it later, after we change
    the users of while_each_thread() to use for_each_thread().
    
    Perhaps we can kill it even before we convert all users, we can
    reimplement next_thread(t) using the new thread_head/thread_node.  But
    we can't do this right now because this will lead to subtle behavioural
    changes.  For example, do/while_each_thread() always sees at least one
    task, while for_each_thread() can do nothing if the whole thread group
    has died.  Or thread_group_empty(), currently its semantics is not clear
    unless thread_group_leader(p) and we need to audit the callers before we
    can change it.
    
    So this patch adds the new interface which has to coexist with the old
    one for some time, hopefully the next changes will be more or less
    straightforward and the old one will go away soon.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Sergey Dyasly <dserrg@gmail.com>
    Tested-by: Sergey Dyasly <dserrg@gmail.com>
    Reviewed-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: "Ma, Xindong" <xindong.ma@intel.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: "Tu, Xiaobing" <xiaobing.tu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f0e52383a001..1516a8ff8f92 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -41,6 +41,7 @@ extern struct fs_struct init_fs;
 
 #define INIT_SIGNALS(sig) {						\
 	.nr_threads	= 1,						\
+	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),	\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
 	.shared_pending	= { 						\
 		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\
@@ -222,6 +223,7 @@ extern struct task_group root_task_group;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
+	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),	\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\

commit 4440e8548153e9e6d56db9abe6f3bc0e5b9eb74f
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Nov 27 17:35:17 2013 -0500

    audit: convert all sessionid declaration to unsigned int
    
    Right now the sessionid value in the kernel is a combination of u32,
    int, and unsigned int.  Just use unsigned int throughout.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5cd0f0949927..a143df5ee548 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -95,7 +95,7 @@ extern struct group_info init_groups;
 #ifdef CONFIG_AUDITSYSCALL
 #define INIT_IDS \
 	.loginuid = INVALID_UID, \
-	.sessionid = -1,
+	.sessionid = (unsigned int)-1,
 #else
 #define INIT_IDS
 #endif

commit fb00aca474405f4fa8a8519c3179fed722eabd83
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 7 14:43:43 2013 +0100

    rtmutex: Turn the plist into an rb-tree
    
    Turn the pi-chains from plist to rb-tree, in the rt_mutex code,
    and provide a proper comparison function for -deadline and
    -priority tasks.
    
    This is done mainly because:
     - classical prio field of the plist is just an int, which might
       not be enough for representing a deadline;
     - manipulating such a list would become O(nr_deadline_tasks),
       which might be to much, as the number of -deadline task increases.
    
    Therefore, an rb-tree is used, and tasks are queued in it according
    to the following logic:
     - among two -priority (i.e., SCHED_BATCH/OTHER/RR/FIFO) tasks, the
       one with the higher (lower, actually!) prio wins;
     - among a -priority and a -deadline task, the latter always wins;
     - among two -deadline tasks, the one with the earliest deadline
       wins.
    
    Queueing and dequeueing functions are changed accordingly, for both
    the list of a task's pi-waiters and the list of tasks blocked on
    a pi-lock.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-again-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-10-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b0ed422e4e4a..f0e52383a001 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -11,6 +11,7 @@
 #include <linux/user_namespace.h>
 #include <linux/securebits.h>
 #include <linux/seqlock.h>
+#include <linux/rbtree.h>
 #include <net/net_namespace.h>
 #include <linux/sched/rt.h>
 
@@ -154,6 +155,14 @@ extern struct task_group root_task_group;
 
 #define INIT_TASK_COMM "swapper"
 
+#ifdef CONFIG_RT_MUTEXES
+# define INIT_RT_MUTEXES(tsk)						\
+	.pi_waiters = RB_ROOT,						\
+	.pi_waiters_leftmost = NULL,
+#else
+# define INIT_RT_MUTEXES(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -221,6 +230,7 @@ extern struct task_group root_task_group;
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
 	INIT_CPUSET_SEQ(tsk)						\
+	INIT_RT_MUTEXES(tsk)						\
 	INIT_VTIME(tsk)							\
 }
 

commit 1ca7d67cf5d5a2aef26a8d9afd789006fa098347
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Oct 7 15:51:59 2013 -0700

    seqcount: Add lockdep functionality to seqcount/seqlock structures
    
    Currently seqlocks and seqcounts don't support lockdep.
    
    After running across a seqcount related deadlock in the timekeeping
    code, I used a less-refined and more focused variant of this patch
    to narrow down the cause of the issue.
    
    This is a first-pass attempt to properly enable lockdep functionality
    on seqlocks and seqcounts.
    
    Since seqcounts are used in the vdso gettimeofday code, I've provided
    non-lockdep accessors for those needs.
    
    I've also handled one case where there were nested seqlock writers
    and there may be more edge cases.
    
    Comments and feedback would be appreciated!
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/1381186321-4906-3-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5cd0f0949927..b0ed422e4e4a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -32,10 +32,10 @@ extern struct fs_struct init_fs;
 #endif
 
 #ifdef CONFIG_CPUSETS
-#define INIT_CPUSET_SEQ							\
-	.mems_allowed_seq = SEQCNT_ZERO,
+#define INIT_CPUSET_SEQ(tsk)							\
+	.mems_allowed_seq = SEQCNT_ZERO(tsk.mems_allowed_seq),
 #else
-#define INIT_CPUSET_SEQ
+#define INIT_CPUSET_SEQ(tsk)
 #endif
 
 #define INIT_SIGNALS(sig) {						\
@@ -220,7 +220,7 @@ extern struct task_group root_task_group;
 	INIT_FTRACE_GRAPH						\
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
-	INIT_CPUSET_SEQ							\
+	INIT_CPUSET_SEQ(tsk)						\
 	INIT_VTIME(tsk)							\
 }
 

commit 77852fea6e2442a0e654a9292060489895de18c7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 16 09:46:48 2013 +0100

    sched/rt: Add <linux/sched/rt.h> header to <linux/init_task.h>
    
    IA64 relied on it through sched.h inclusion:
    
      arch/ia64/kernel/init_task.c:38:11: error: 'MAX_PRIO' undeclared here (not in a function)
      arch/ia64/kernel/init_task.c:38:11: error: 'RR_TIMESLICE' undeclared here (not in a function)
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/n/tip-xaan1twswggedMR0airtpjui@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index cc898b871cef..5cd0f0949927 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -12,6 +12,7 @@
 #include <linux/securebits.h>
 #include <linux/seqlock.h>
 #include <net/net_namespace.h>
+#include <linux/sched/rt.h>
 
 #ifdef CONFIG_SMP
 # define INIT_PUSHABLE_TASKS(tsk)					\

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6d087c5f57f7..cc898b871cef 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -10,6 +10,7 @@
 #include <linux/pid_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/securebits.h>
+#include <linux/seqlock.h>
 #include <net/net_namespace.h>
 
 #ifdef CONFIG_SMP
@@ -141,6 +142,15 @@ extern struct task_group root_task_group;
 # define INIT_PERF_EVENTS(tsk)
 #endif
 
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+# define INIT_VTIME(tsk)						\
+	.vtime_seqlock = __SEQLOCK_UNLOCKED(tsk.vtime_seqlock),	\
+	.vtime_snap = 0,				\
+	.vtime_snap_whence = VTIME_SYS,
+#else
+# define INIT_VTIME(tsk)
+#endif
+
 #define INIT_TASK_COMM "swapper"
 
 /*
@@ -210,6 +220,7 @@ extern struct task_group root_task_group;
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
 	INIT_CPUSET_SEQ							\
+	INIT_VTIME(tsk)							\
 }
 
 

commit e1760bd5ffae8cb98cffb030ee8e631eba28f3d8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Sep 10 22:39:43 2012 -0700

    userns: Convert the audit loginuid  to be a kuid
    
    Always store audit loginuids in type kuid_t.
    
    Print loginuids by converting them into uids in the appropriate user
    namespace, and then printing the resulting uid.
    
    Modify audit_get_loginuid to return a kuid_t.
    
    Modify audit_set_loginuid to take a kuid_t.
    
    Modify /proc/<pid>/loginuid on read to convert the loginuid into the
    user namespace of the opener of the file.
    
    Modify /proc/<pid>/loginud on write to convert the loginuid
    rom the user namespace of the opener of the file.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Paul Moore <paul@paul-moore.com> ?
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 89f1cb1056f0..6d087c5f57f7 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -92,7 +92,7 @@ extern struct group_info init_groups;
 
 #ifdef CONFIG_AUDITSYSCALL
 #define INIT_IDS \
-	.loginuid = -1, \
+	.loginuid = INVALID_UID, \
 	.sessionid = -1,
 #else
 #define INIT_IDS

commit 79071638ce655c1f78a50d05c7dae0ad04a3e92a
Merge: 44a6b8442190 8323f26ce342
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 26 13:08:01 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is a performance improvement on SMP systems:
    
      | 4 socket 40 core + SMT Westmere box, single 30 sec tbench
      | runs, higher is better:
      |
      | clients     1       2       4        8       16       32       64      128
      |..........................................................................
      | pre        30      41     118      645     3769     6214    12233    14312
      | post      299     603    1211     2418     4697     6847    11606    14557
      |
      | A nice increase in performance.
    
      which speedup is particularly noticeable on heavily interacting
      few-tasks workloads, so the changes should help desktop-style Xorg
      workloads and interactivity as well, on multi-core CPUs.
    
      There are also cpuset suspend behavior fixes/restructuring and various
      smaller tweaks."
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix race in task_group()
      sched: Improve balance_cpu() to consider other cpus in its group as target of (pinned) task
      sched: Reset loop counters if all tasks are pinned and we need to redo load balance
      sched: Reorder 'struct lb_env' members to reduce its size
      sched: Improve scalability via 'CPU buddies', which withstand random perturbations
      cpusets: Remove/update outdated comments
      cpusets, hotplug: Restructure functions that are invoked during hotplug
      cpusets, hotplug: Implement cpuset tree traversal in a helper function
      CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
      sched/x86: Remove broken power estimation

commit 8323f26ce3425460769605a6aece7a174edaa7d1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jun 22 13:36:05 2012 +0200

    sched: Fix race in task_group()
    
    Stefan reported a crash on a kernel before a3e5d1091c1 ("sched:
    Don't call task_group() too many times in set_task_rq()"), he
    found the reason to be that the multiple task_group()
    invocations in set_task_rq() returned different values.
    
    Looking at all that I found a lack of serialization and plain
    wrong comments.
    
    The below tries to fix it using an extra pointer which is
    updated under the appropriate scheduler locks. Its not pretty,
    but I can't really see another way given how all the cgroup
    stuff works.
    
    Reported-and-tested-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340364965.18025.71.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9e65eff6af3b..b806b821e735 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -123,8 +123,17 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
+extern struct task_group root_task_group;
+
+#ifdef CONFIG_CGROUP_SCHED
+# define INIT_CGROUP_SCHED(tsk)						\
+	.sched_task_group = &root_task_group,
+#else
+# define INIT_CGROUP_SCHED(tsk)
+#endif
+
 #ifdef CONFIG_PERF_EVENTS
-# define INIT_PERF_EVENTS(tsk)					\
+# define INIT_PERF_EVENTS(tsk)						\
 	.perf_event_mutex = 						\
 		 __MUTEX_INITIALIZER(tsk.perf_event_mutex),		\
 	.perf_event_list = LIST_HEAD_INIT(tsk.perf_event_list),
@@ -161,6 +170,7 @@ extern struct cred init_cred;
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	INIT_PUSHABLE_TASKS(tsk)					\
+	INIT_CGROUP_SCHED(tsk)						\
 	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
 	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
 	.real_parent	= &tsk,						\

commit d36cc701b28983ea2c8d80afe68aae5452aea3bf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 16 15:33:15 2012 -0700

    rcu: Use new RCU_POINTER_INITIALIZER for gcc-style initializations
    
    This commit applies the INIT_RCU_POINTER() macro to all uses of
    RCU_POINTER_INITIALIZER() that were all too cleverly creating gcc-style
    initializations.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9e65eff6af3b..8a7476186990 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -168,8 +168,8 @@ extern struct cred init_cred;
 	.children	= LIST_HEAD_INIT(tsk.children),			\
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
-	RCU_INIT_POINTER(.real_cred, &init_cred),			\
-	RCU_INIT_POINTER(.cred, &init_cred),				\
+	RCU_POINTER_INITIALIZER(real_cred, &init_cred),			\
+	RCU_POINTER_INITIALIZER(cred, &init_cred),			\
 	.comm		= INIT_TASK_COMM,				\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit 29baa7478ba47d746e3625c91d3b2afbf46b4312
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Apr 23 12:11:21 2012 +0200

    sched: Move nr_cpus_allowed out of 'struct sched_rt_entity'
    
    Since nr_cpus_allowed is used outside of sched/rt.c and wants to be
    used outside of there more, move it to a more natural site.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-kr61f02y9brwzkh6x53pdptm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e4baff5f7ff4..9e65eff6af3b 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -149,6 +149,7 @@ extern struct cred init_cred;
 	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
+	.nr_cpus_allowed= NR_CPUS,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.se		= {						\
@@ -157,7 +158,6 @@ extern struct cred init_cred;
 	.rt		= {						\
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
 		.time_slice	= RR_TIMESLICE,				\
-		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	INIT_PUSHABLE_TASKS(tsk)					\

commit cc9a6c8776615f9c194ccf0b63a0aa5628235545
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Mar 21 16:34:11 2012 -0700

    cpuset: mm: reduce large amounts of memory barrier related damage v3
    
    Commit c0ff7453bb5c ("cpuset,mm: fix no node to alloc memory when
    changing cpuset's mems") wins a super prize for the largest number of
    memory barriers entered into fast paths for one commit.
    
    [get|put]_mems_allowed is incredibly heavy with pairs of full memory
    barriers inserted into a number of hot paths.  This was detected while
    investigating at large page allocator slowdown introduced some time
    after 2.6.32.  The largest portion of this overhead was shown by
    oprofile to be at an mfence introduced by this commit into the page
    allocator hot path.
    
    For extra style points, the commit introduced the use of yield() in an
    implementation of what looks like a spinning mutex.
    
    This patch replaces the full memory barriers on both read and write
    sides with a sequence counter with just read barriers on the fast path
    side.  This is much cheaper on some architectures, including x86.  The
    main bulk of the patch is the retry logic if the nodemask changes in a
    manner that can cause a false failure.
    
    While updating the nodemask, a check is made to see if a false failure
    is a risk.  If it is, the sequence number gets bumped and parallel
    allocators will briefly stall while the nodemask update takes place.
    
    In a page fault test microbenchmark, oprofile samples from
    __alloc_pages_nodemask went from 4.53% of all samples to 1.15%.  The
    actual results were
    
                                 3.3.0-rc3          3.3.0-rc3
                                 rc3-vanilla        nobarrier-v2r1
        Clients   1 UserTime       0.07 (  0.00%)   0.08 (-14.19%)
        Clients   2 UserTime       0.07 (  0.00%)   0.07 (  2.72%)
        Clients   4 UserTime       0.08 (  0.00%)   0.07 (  3.29%)
        Clients   1 SysTime        0.70 (  0.00%)   0.65 (  6.65%)
        Clients   2 SysTime        0.85 (  0.00%)   0.82 (  3.65%)
        Clients   4 SysTime        1.41 (  0.00%)   1.41 (  0.32%)
        Clients   1 WallTime       0.77 (  0.00%)   0.74 (  4.19%)
        Clients   2 WallTime       0.47 (  0.00%)   0.45 (  3.73%)
        Clients   4 WallTime       0.38 (  0.00%)   0.37 (  1.58%)
        Clients   1 Flt/sec/cpu  497620.28 (  0.00%) 520294.53 (  4.56%)
        Clients   2 Flt/sec/cpu  414639.05 (  0.00%) 429882.01 (  3.68%)
        Clients   4 Flt/sec/cpu  257959.16 (  0.00%) 258761.48 (  0.31%)
        Clients   1 Flt/sec      495161.39 (  0.00%) 517292.87 (  4.47%)
        Clients   2 Flt/sec      820325.95 (  0.00%) 850289.77 (  3.65%)
        Clients   4 Flt/sec      1020068.93 (  0.00%) 1022674.06 (  0.26%)
        MMTests Statistics: duration
        Sys Time Running Test (seconds)             135.68    132.17
        User+Sys Time Running Test (seconds)         164.2    160.13
        Total Elapsed Time (seconds)                123.46    120.87
    
    The overall improvement is small but the System CPU time is much
    improved and roughly in correlation to what oprofile reported (these
    performance figures are without profiling so skew is expected).  The
    actual number of page faults is noticeably improved.
    
    For benchmarks like kernel builds, the overall benefit is marginal but
    the system CPU time is slightly reduced.
    
    To test the actual bug the commit fixed I opened two terminals.  The
    first ran within a cpuset and continually ran a small program that
    faulted 100M of anonymous data.  In a second window, the nodemask of the
    cpuset was continually randomised in a loop.
    
    Without the commit, the program would fail every so often (usually
    within 10 seconds) and obviously with the commit everything worked fine.
    With this patch applied, it also worked fine so the fix should be
    functionally equivalent.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f994d51f70f2..e4baff5f7ff4 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -29,6 +29,13 @@ extern struct fs_struct init_fs;
 #define INIT_GROUP_RWSEM(sig)
 #endif
 
+#ifdef CONFIG_CPUSETS
+#define INIT_CPUSET_SEQ							\
+	.mems_allowed_seq = SEQCNT_ZERO,
+#else
+#define INIT_CPUSET_SEQ
+#endif
+
 #define INIT_SIGNALS(sig) {						\
 	.nr_threads	= 1,						\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
@@ -192,6 +199,7 @@ extern struct cred init_cred;
 	INIT_FTRACE_GRAPH						\
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
+	INIT_CPUSET_SEQ							\
 }
 
 

commit de5bdff7a72acc281219be2b8edeeca1fd81c542
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Thu Feb 16 14:52:21 2012 +0900

    sched: Make initial SCHED_RR timeslace DEF_TIMESLICE
    
    Current the initial SCHED_RR timeslice of init_task is HZ, which means
    1s, and is not same as the default SCHED_RR timeslice DEF_TIMESLICE.
    
    Change that initial timeslice to the DEF_TIMESLICE.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    [ s/DEF_TIMESLICE/RR_TIMESLICE/g ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F3C9995.3010800@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9c66b1ada9d7..f994d51f70f2 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -149,7 +149,7 @@ extern struct cred init_cred;
 	},								\
 	.rt		= {						\
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
-		.time_slice	= HZ, 					\
+		.time_slice	= RR_TIMESLICE,				\
 		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\

commit db0c2bf69aa095d4a6de7b1145f29fe9a7c0f6a3
Merge: ac69e0928054 0d19ea866562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 9 12:59:24 2012 -0800

    Merge branch 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    * 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (21 commits)
      cgroup: fix to allow mounting a hierarchy by name
      cgroup: move assignement out of condition in cgroup_attach_proc()
      cgroup: Remove task_lock() from cgroup_post_fork()
      cgroup: add sparse annotation to cgroup_iter_start() and cgroup_iter_end()
      cgroup: mark cgroup_rmdir_waitq and cgroup_attach_proc() as static
      cgroup: only need to check oldcgrp==newgrp once
      cgroup: remove redundant get/put of task struct
      cgroup: remove redundant get/put of old css_set from migrate
      cgroup: Remove unnecessary task_lock before fetching css_set on migration
      cgroup: Drop task_lock(parent) on cgroup_fork()
      cgroups: remove redundant get/put of css_set from css_set_check_fetched()
      resource cgroups: remove bogus cast
      cgroup: kill subsys->can_attach_task(), pre_attach() and attach_task()
      cgroup, cpuset: don't use ss->pre_attach()
      cgroup: don't use subsys->can_attach_task() or ->attach_task()
      cgroup: introduce cgroup_taskset and use it in subsys->can_attach(), cancel_attach() and attach()
      cgroup: improve old cgroup handling in cgroup_attach_proc()
      cgroup: always lock threadgroup during migration
      threadgroup: extend threadgroup_lock() to cover exit and exec
      threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
      ...
    
    Fix up conflict in kernel/cgroup.c due to commit e0197aae59e5: "cgroups:
    fix a css_set not found bug in cgroup_attach_proc" that already
    mentioned that the bug is fixed (differently) in Tejun's cgroup
    patchset. This one, in other words.

commit 257058ae2b971646b96ab3a15605ac69186e562a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 12 18:12:21 2011 -0800

    threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
    
    Make the following renames to prepare for extension of threadgroup
    locking.
    
    * s/signal->threadgroup_fork_lock/signal->group_rwsem/
    * s/threadgroup_fork_read_lock()/threadgroup_change_begin()/
    * s/threadgroup_fork_read_unlock()/threadgroup_change_end()/
    * s/threadgroup_fork_write_lock()/threadgroup_lock()/
    * s/threadgroup_fork_write_unlock()/threadgroup_unlock()/
    
    This patch doesn't cause any behavior change.
    
    -v2: Rename threadgroup_change_done() to threadgroup_change_end() per
         KAMEZAWA's suggestion.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Menage <paul@paulmenage.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 94b1e356c02a..f4544b99efe4 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -23,11 +23,10 @@ extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
 #ifdef CONFIG_CGROUPS
-#define INIT_THREADGROUP_FORK_LOCK(sig)					\
-	.threadgroup_fork_lock =					\
-		__RWSEM_INITIALIZER(sig.threadgroup_fork_lock),
+#define INIT_GROUP_RWSEM(sig)						\
+	.group_rwsem = __RWSEM_INITIALIZER(sig.group_rwsem),
 #else
-#define INIT_THREADGROUP_FORK_LOCK(sig)
+#define INIT_GROUP_RWSEM(sig)
 #endif
 
 #define INIT_SIGNALS(sig) {						\
@@ -46,7 +45,7 @@ extern struct fs_struct init_fs;
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
-	INIT_THREADGROUP_FORK_LOCK(sig)					\
+	INIT_GROUP_RWSEM(sig)						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 7125faceabe43067293d0c9e2ef7154ecea51721
Merge: 35337c834124 4cecf6d401a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 5 16:50:24 2011 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched, x86: Avoid unnecessary overflow in sched_clock
      sched: Fix buglet in return_cfs_rq_runtime()
      sched: Avoid SMT siblings in select_idle_sibling() if possible
      sched: Set the command name of the idle tasks in SMP kernels
      sched, rt: Provide means of disabling cross-cpu bandwidth sharing
      sched: Document wait_for_completion_*() return values
      sched_fair: Fix a typo in the comment describing update_sd_lb_stats
      sched: Add a comment to effective_load() since it's a pain

commit 468e6a20afaccb67e2a7d7f60d301f90e1c6f301
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Sep 7 10:41:32 2011 -0600

    writeback: remove vm_dirties and task->dirties
    
    They are not used any more.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 08ffab01e76c..94b1e356c02a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -184,7 +184,6 @@ extern struct cred init_cred;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
-	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\

commit f1c6f1a7eed963ed233ba4c8b6fa8addb86c6ddc
Author: Carsten Emde <C.Emde@osadl.org>
Date:   Wed Oct 26 23:14:16 2011 +0200

    sched: Set the command name of the idle tasks in SMP kernels
    
    In UP systems, the idle task is initialized using the init_task
    structure from which the command name is taken (currently "swapper").
    
    In SMP systems, one idle task per CPU is forked by the worker thread
    from which the task structure is copied. The command name is, therefore,
    "kworker/0:0" or "kworker/0:1", if not updated. Since such update was
    lacking, all idle tasks in SMP systems were incorrectly named. This
    longtime bug was not discovered immediately, because there is no /proc/0
    entry - the bug only becomes apparent when tracing is enabled.
    
    This patch sets the command name of the idle tasks in SMP systems to the
    name that is used in the INIT_TASK structure suffixed by a slash and the
    number of the CPU.
    
    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20111026211708.768925506@osadl.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 08ffab01e76c..b6e5b8b000e0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -126,6 +126,8 @@ extern struct cred init_cred;
 # define INIT_PERF_EVENTS(tsk)
 #endif
 
+#define INIT_TASK_COMM "swapper"
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -162,7 +164,7 @@ extern struct cred init_cred;
 	.group_leader	= &tsk,						\
 	RCU_INIT_POINTER(.real_cred, &init_cred),			\
 	RCU_INIT_POINTER(.cred, &init_cred),				\
-	.comm		= "swapper",					\
+	.comm		= INIT_TASK_COMM,				\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\
 	.files		= &init_files,					\

commit ee30a7b2fc072f139dac44826860d2c1f422137c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 18:56:56 2009 +0200

    locking, sched: Annotate thread_group_cputimer as raw
    
    The thread_group_cputimer lock can be taken in atomic context and therefore
    cannot be preempted on -rt - annotate it.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d14e058aaeed..08ffab01e76c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -42,7 +42,7 @@ extern struct fs_struct init_fs;
 	.cputimer	= { 						\
 		.cputime = INIT_CPUTIME,				\
 		.running = 0,						\
-		.lock = __SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
+		.lock = __RAW_SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\

commit 4aede84b33d6beb401136a3deca0651ae07c5e99
Author: Justin TerAvest <teravest@google.com>
Date:   Tue Jul 12 08:31:45 2011 +0200

    fixlet: Remove fs_excl from struct task.
    
    fs_excl is a poor man's priority inheritance for filesystems to hint to
    the block layer that an operation is important. It was never clearly
    specified, not widely adopted, and will not prevent starvation in many
    cases (like across cgroups).
    
    fs_excl was introduced with the time sliced CFQ IO scheduler, to
    indicate when a process held FS exclusive resources and thus needed
    a boost.
    
    It doesn't cover all file systems, and it was never fully complete.
    Lets kill it.
    
    Signed-off-by: Justin TerAvest <teravest@google.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 580f70c02391..d14e058aaeed 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -176,7 +176,6 @@ extern struct cred init_cred;
 	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
-	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
 	.timer_slack_ns = 50000, /* 50 usec default slack */		\
 	.pids = {							\

commit 4714d1d32d97239fb5ae3e10521d3f133a899b66
Author: Ben Blum <bblum@andrew.cmu.edu>
Date:   Thu May 26 16:25:18 2011 -0700

    cgroups: read-write lock CLONE_THREAD forking per threadgroup
    
    Adds functionality to read/write lock CLONE_THREAD fork()ing per-threadgroup
    
    Add an rwsem that lives in a threadgroup's signal_struct that's taken for
    reading in the fork path, under CONFIG_CGROUPS.  If another part of the
    kernel later wants to use such a locking mechanism, the CONFIG_CGROUPS
    ifdefs should be changed to a higher-up flag that CGROUPS and the other
    system would both depend on.
    
    This is a pre-patch for cgroup-procs-write.patch.
    
    Signed-off-by: Ben Blum <bblum@andrew.cmu.edu>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index bafc58c00fc3..580f70c02391 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -22,6 +22,14 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
+#ifdef CONFIG_CGROUPS
+#define INIT_THREADGROUP_FORK_LOCK(sig)					\
+	.threadgroup_fork_lock =					\
+		__RWSEM_INITIALIZER(sig.threadgroup_fork_lock),
+#else
+#define INIT_THREADGROUP_FORK_LOCK(sig)
+#endif
+
 #define INIT_SIGNALS(sig) {						\
 	.nr_threads	= 1,						\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
@@ -38,6 +46,7 @@ extern struct fs_struct init_fs;
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
+	INIT_THREADGROUP_FORK_LOCK(sig)					\
 }
 
 extern struct nsproxy init_nsproxy;

commit 434d42cfd05a7cc452457a81d2029540cba12150
Merge: d762f4383100 12a5a2621b1e
Author: James Morris <jmorris@namei.org>
Date:   Tue May 24 22:55:24 2011 +1000

    Merge branch 'next' into for-linus

commit 625f2a378e5a10f45fdc37932fc9f8a21676de9e
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Fri Apr 22 11:19:10 2011 -0600

    sched: Get rid of lock_depth
    
    Neil Brown pointed out that lock_depth somehow escaped the BKL
    removal work.  Let's get rid of it now.
    
    Note that the perf scripting utilities still have a bunch of
    code for dealing with common_lock_depth in tracepoints; I have
    left that in place in case anybody wants to use that code with
    older kernels.
    
    Suggested-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110422111910.456c0e84@bike.lwn.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index caa151fbebb7..689496bb6654 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -134,7 +134,6 @@ extern struct cred init_cred;
 	.stack		= &init_thread_info,				\
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= PF_KTHREAD,					\
-	.lock_depth	= -1,						\
 	.prio		= MAX_PRIO-20,					\
 	.static_prio	= MAX_PRIO-20,					\
 	.normal_prio	= MAX_PRIO-20,					\

commit a3232d2fa2e3cbab3e76d91cdae5890fee8a4034
Author: Eric Paris <eparis@redhat.com>
Date:   Fri Apr 1 17:08:45 2011 -0400

    capabilities: delete all CAP_INIT macros
    
    The CAP_INIT macros of INH, BSET, and EFF made sense at one point in time,
    but now days they aren't helping.  Just open code the logic in the
    init_cred.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index caa151fbebb7..1f277204de34 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -83,13 +83,6 @@ extern struct group_info init_groups;
 #define INIT_IDS
 #endif
 
-/*
- * Because of the reduced scope of CAP_SETPCAP when filesystem
- * capabilities are in effect, it is safe to allow CAP_SETPCAP to
- * be available in the default configuration.
- */
-# define CAP_INIT_BSET  CAP_FULL_SET
-
 #ifdef CONFIG_RCU_BOOST
 #define INIT_TASK_RCU_BOOST()						\
 	.rcu_boost_mutex = NULL,

commit 65b2074f84be2287e020839e93b4cdaaf60eb37c
Merge: 28d9bfc37c86 6bf4123760a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 6 10:23:33 2011 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (30 commits)
      sched: Change wait_for_completion_*_timeout() to return a signed long
      sched, autogroup: Fix reference leak
      sched, autogroup: Fix potential access to freed memory
      sched: Remove redundant CONFIG_CGROUP_SCHED ifdef
      sched: Fix interactivity bug by charging unaccounted run-time on entity re-weight
      sched: Move periodic share updates to entity_tick()
      printk: Use this_cpu_{read|write} api on printk_pending
      sched: Make pushable_tasks CONFIG_SMP dependant
      sched: Add 'autogroup' scheduling feature: automated per session task groups
      sched: Fix unregister_fair_sched_group()
      sched: Remove unused argument dest_cpu to migrate_task()
      mutexes, sched: Introduce arch_mutex_cpu_relax()
      sched: Add some clock info to sched_debug
      cpu: Remove incorrect BUG_ON
      cpu: Remove unused variable
      sched: Fix UP build breakage
      sched: Make task dump print all 15 chars of proc comm
      sched: Update tg->shares after cpu.shares write
      sched: Allow update_cfs_load() to update global load
      sched: Implement demand based update_cfs_load()
      ...

commit 394f4528c523d88daabd50f883a8d6b164075555
Merge: 90a8a73c06cc 3c2dcf2aed5e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 23 12:57:04 2010 +0100

    Merge branch 'rcu/next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-2.6-rcu into core/rcu

commit 806c09a7db457be3758e14b1f152761135d89af5
Author: Dario Faggioli <raistlin@linux.it>
Date:   Tue Nov 30 19:51:33 2010 +0100

    sched: Make pushable_tasks CONFIG_SMP dependant
    
    As noted by Peter Zijlstra at https://lkml.org/lkml/2010/11/10/391
    (while reviewing other stuff, though), tracking pushable tasks
    only makes sense on SMP systems.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1291143093.2697.298.camel@Palantir>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1f8c06ce0fa6..6ed8812bfe2d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -12,6 +12,13 @@
 #include <linux/securebits.h>
 #include <net/net_namespace.h>
 
+#ifdef CONFIG_SMP
+# define INIT_PUSHABLE_TASKS(tsk)					\
+	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO),
+#else
+# define INIT_PUSHABLE_TASKS(tsk)
+#endif
+
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
@@ -137,7 +144,7 @@ extern struct cred init_cred;
 		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
-	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO), \
+	INIT_PUSHABLE_TASKS(tsk)					\
 	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
 	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
 	.real_parent	= &tsk,						\

commit 24278d148316d2180be6df40e06db013d8b232b8
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Sep 27 17:25:23 2010 -0700

    rcu: priority boosting for TINY_PREEMPT_RCU
    
    Add priority boosting, but only for TINY_PREEMPT_RCU.  This is enabled
    by the default-off RCU_BOOST kernel parameter.  The priority to which to
    boost preempted RCU readers is controlled by the RCU_BOOST_PRIO kernel
    parameter (defaulting to real-time priority 1) and the time to wait
    before boosting the readers blocking a given grace period is controlled
    by the RCU_BOOST_DELAY kernel parameter (defaulting to 500 milliseconds).
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 2fea6c8ef6ba..69f91aacdeee 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -81,6 +81,12 @@ extern struct group_info init_groups;
  */
 # define CAP_INIT_BSET  CAP_FULL_SET
 
+#ifdef CONFIG_RCU_BOOST
+#define INIT_TASK_RCU_BOOST()						\
+	.rcu_boost_mutex = NULL,
+#else
+#define INIT_TASK_RCU_BOOST()
+#endif
 #ifdef CONFIG_TREE_PREEMPT_RCU
 #define INIT_TASK_RCU_TREE_PREEMPT()					\
 	.rcu_blocked_node = NULL,
@@ -92,7 +98,8 @@ extern struct group_info init_groups;
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special = 0,					\
 	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
-	INIT_TASK_RCU_TREE_PREEMPT()
+	INIT_TASK_RCU_TREE_PREEMPT()					\
+	INIT_TASK_RCU_BOOST()
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)
 #endif

commit 9b1bf12d5d51bca178dea21b04a0805e29d60cf1
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 27 15:34:08 2010 -0700

    signals: move cred_guard_mutex from task_struct to signal_struct
    
    Oleg Nesterov pointed out we have to prevent multiple-threads-inside-exec
    itself and we can reuse ->cred_guard_mutex for it.  Yes, concurrent
    execve() has no worth.
    
    Let's move ->cred_guard_mutex from task_struct to signal_struct.  It
    naturally prevent multiple-threads-inside-exec.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 2fea6c8ef6ba..1f8c06ce0fa6 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -29,6 +29,8 @@ extern struct fs_struct init_fs;
 		.running = 0,						\
 		.lock = __SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
 	},								\
+	.cred_guard_mutex =						\
+		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
 }
 
 extern struct nsproxy init_nsproxy;
@@ -145,8 +147,6 @@ extern struct cred init_cred;
 	.group_leader	= &tsk,						\
 	RCU_INIT_POINTER(.real_cred, &init_cred),			\
 	RCU_INIT_POINTER(.cred, &init_cred),				\
-	.cred_guard_mutex =						\
-		 __MUTEX_INITIALIZER(tsk.cred_guard_mutex),		\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit a57eb940d130477a799dfb24a570ee04979c0f7f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 29 16:49:16 2010 -0700

    rcu: Add a TINY_PREEMPT_RCU
    
    Implement a small-memory-footprint uniprocessor-only implementation of
    preemptible RCU.  This implementation uses but a single blocked-tasks
    list rather than the combinatorial number used per leaf rcu_node by
    TREE_PREEMPT_RCU, which reduces memory consumption and greatly simplifies
    processing.  This version also takes advantage of uniprocessor execution
    to accelerate grace periods in the case where there are no readers.
    
    The general design is otherwise broadly similar to that of TREE_PREEMPT_RCU.
    
    This implementation is a step towards having RCU implementation driven
    off of the SMP and PREEMPT kernel configuration variables, which can
    happen once this implementation has accumulated sufficient experience.
    
    Removed ACCESS_ONCE() from __rcu_read_unlock() and added barrier() as
    suggested by Steve Rostedt in order to avoid the compiler-reordering
    issue noted by Mathieu Desnoyers (http://lkml.org/lkml/2010/8/16/183).
    
    As can be seen below, CONFIG_TINY_PREEMPT_RCU represents almost 5Kbyte
    savings compared to CONFIG_TREE_PREEMPT_RCU.  Of course, for non-real-time
    workloads, CONFIG_TINY_RCU is even better.
    
            CONFIG_TREE_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               6170     825      28    7023    kernel/rcutree.o
                                       ----
                                       7026    Total
    
            CONFIG_TINY_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               2081      81       8    2170    kernel/rcutiny.o
                                       ----
                                       2183    Total
    
            CONFIG_TINY_RCU (non-preemptible)
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
                719      25       0     744    kernel/rcutiny.o
                                        ---
                                        757    Total
    
    Requested-by: Loïc Minier <loic.minier@canonical.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6460fc65ed6b..2fea6c8ef6ba 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -82,11 +82,17 @@ extern struct group_info init_groups;
 # define CAP_INIT_BSET  CAP_FULL_SET
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
+#define INIT_TASK_RCU_TREE_PREEMPT()					\
+	.rcu_blocked_node = NULL,
+#else
+#define INIT_TASK_RCU_TREE_PREEMPT(tsk)
+#endif
+#ifdef CONFIG_PREEMPT_RCU
 #define INIT_TASK_RCU_PREEMPT(tsk)					\
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special = 0,					\
-	.rcu_blocked_node = NULL,					\
-	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),
+	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),		\
+	INIT_TASK_RCU_TREE_PREEMPT()
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)
 #endif

commit 4d2deb40b20c2608486598364e63e37b09a9ac2f
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 24 20:01:56 2010 +0100

    kernel: __rcu annotations
    
    This adds annotations for RCU operations in core kernel components
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1f43fa56f600..6460fc65ed6b 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -137,8 +137,8 @@ extern struct cred init_cred;
 	.children	= LIST_HEAD_INIT(tsk.children),			\
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
-	.real_cred	= &init_cred,					\
-	.cred		= &init_cred,					\
+	RCU_INIT_POINTER(.real_cred, &init_cred),			\
+	RCU_INIT_POINTER(.cred, &init_cred),				\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(tsk.cred_guard_mutex),		\
 	.comm		= "swapper",					\

commit 1f73897861b8ef0be64ff4b801f8d6f830f683b5
Merge: b904d7131d11 64ffc9ff424c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 1 08:55:52 2010 -0700

    Merge branch 'for-35' of git://repo.or.cz/linux-kbuild
    
    * 'for-35' of git://repo.or.cz/linux-kbuild: (81 commits)
      kbuild: Revert part of e8d400a to resolve a conflict
      kbuild: Fix checking of scm-identifier variable
      gconfig: add support to show hidden options that have prompts
      menuconfig: add support to show hidden options which have prompts
      gconfig: remove show_debug option
      gconfig: remove dbg_print_ptype() and dbg_print_stype()
      kconfig: fix zconfdump()
      kconfig: some small fixes
      add random binaries to .gitignore
      kbuild: Include gen_initramfs_list.sh and the file list in the .d file
      kconfig: recalc symbol value before showing search results
      .gitignore: ignore *.lzo files
      headerdep: perlcritic warning
      scripts/Makefile.lib: Align the output of LZO
      kbuild: Generate modules.builtin in make modules_install
      Revert "kbuild: specify absolute paths for cscope"
      kbuild: Do not unnecessarily regenerate modules.builtin
      headers_install: use local file handles
      headers_check: fix perl warnings
      export_report: fix perl warnings
      ...

commit 0a14a130cac9c6826bc81a089b12ab5cbb2b97fc
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:44:12 2010 -0700

    INIT_SIGHAND: use SIG_DFL instead of NULL
    
    Cosmetic, no changes in the compiled code. Just s/NULL/SIG_DFL/ to make
    it more readable and grep-friendly.
    
    Note: probably SIG_IGN makes more sense, we could kill ignore_signals().
    But then kernel_init() should do flush_signal_handlers() before exec().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Mathias Krause <Mathias.Krause@secunet.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 94fecb748a28..2beaa13492be 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -35,7 +35,7 @@ extern struct nsproxy init_nsproxy;
 
 #define INIT_SIGHAND(sighand) {						\
 	.count		= ATOMIC_INIT(1), 				\
-	.action		= { { { .sa_handler = NULL, } }, },		\
+	.action		= { { { .sa_handler = SIG_DFL, } }, },		\
 	.siglock	= __SPIN_LOCK_UNLOCKED(sighand.siglock),	\
 	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(sighand.signalfd_wqh),	\
 }

commit f20011457f41c11edb5ea5038ad0c8ea9f392023
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:44:10 2010 -0700

    pids: init_struct_pid.tasks should never see the swapper process
    
    "statically initialize struct pid for swapper" commit 820e45db says:
    
            Statically initialize a struct pid for the swapper process (pid_t == 0)
            and attach it to init_task.  This is needed so task_pid(), task_pgrp()
            and task_session() interfaces work on the swapper process also.
    
    OK, but:
    
            - it doesn't make sense to add init_task.pids[].node into
              init_struct_pid.tasks[], and in fact this just wrong.
    
              idle threads are special, they shouldn't be visible on any
              global list. In particular do_each_pid_task(init_struct_pid)
              shouldn't see swapper.
    
              This is the actual reason why kill(0, SIGKILL) from /sbin/init
              (which starts with 0,0 special pids) crashes the kernel. The
              signal sent to pgid/sid == 0 must never see idle threads, even
              if the previous patch fixed the crash itself.
    
            - we have other idle threads running on the non-boot CPUs, see
              the next patch.
    
    Change INIT_STRUCT_PID/INIT_PID_LINK to create the empty/unhashed
    hlist_head/hlist_node. Like any other idle thread swapper can never exit,
    so detach_pid()->__hlist_del() is not possible, but we could change
    INIT_PID_LINK() to set pprev = &next if needed.
    
    All we need is the valid swapper->pids[].pid == &init_struct_pid.
    
    Reported-by: Mathias Krause <mathias.krause@secunet.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Mathias Krause <Mathias.Krause@secunet.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6deb1ba52fdb..94fecb748a28 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -45,9 +45,9 @@ extern struct group_info init_groups;
 #define INIT_STRUCT_PID {						\
 	.count 		= ATOMIC_INIT(1),				\
 	.tasks		= {						\
-		{ .first = &init_task.pids[PIDTYPE_PID].node },		\
-		{ .first = &init_task.pids[PIDTYPE_PGID].node },	\
-		{ .first = &init_task.pids[PIDTYPE_SID].node },		\
+		{ .first = NULL },					\
+		{ .first = NULL },					\
+		{ .first = NULL },					\
 	},								\
 	.level		= 0,						\
 	.numbers	= { {						\
@@ -61,7 +61,7 @@ extern struct group_info init_groups;
 {								\
 	.node = {						\
 		.next = NULL,					\
-		.pprev = &init_struct_pid.tasks[type].first,	\
+		.pprev = NULL,					\
 	},							\
 	.pid = &init_struct_pid,				\
 }

commit fa2755e20ab0c7215d99c2dc7c262e98a09b01df
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:44:08 2010 -0700

    INIT_TASK() should initialize ->thread_group list
    
    The trivial /sbin/init doing
    
            int main(void)
            {
                    kill(0, SIGKILL)
            }
    
    crashes the kernel.
    
    This happens because __kill_pgrp_info(init_struct_pid) also sends SIGKILL
    to the swapper process which runs with the uninitialized ->thread_group.
    
    Change INIT_TASK() to initialize ->thread_group properly.
    
    Note: the real problem is that the swapper process must not be visible to
    signals, see the next patch. But this change is right anyway and fixes
    the crash.
    
    Reported-and-tested-by: Mathias Krause <mathias.krause@secunet.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Mathias Krause <Mathias.Krause@secunet.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Acked-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 0551e0dcb71b..6deb1ba52fdb 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -163,6 +163,7 @@ extern struct cred init_cred;
 		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
+	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\

commit b3ac022cb9dc5883505a88b159d1b240ad1ef405
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:24 2010 -0700

    proc: turn signal_struct->count into "int nr_threads"
    
    No functional changes, just s/atomic_t count/int nr_threads/.
    
    With the recent changes this counter has a single user, get_nr_threads()
    And, none of its callers need the really accurate number of threads, not
    to mention each caller obviously races with fork/exit.  It is only used to
    report this value to the user-space, except first_tid() uses it to avoid
    the unnecessary while_each_thread() loop in the unlikely case.
    
    It is a bit sad we need a word in struct signal_struct for this, perhaps
    we can change get_nr_threads() to approximate the number of threads using
    signal->live and kill ->nr_threads later.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 7996fc2c9ba9..0551e0dcb71b 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -16,7 +16,7 @@ extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
 #define INIT_SIGNALS(sig) {						\
-	.count		= ATOMIC_INIT(1), 				\
+	.nr_threads	= 1,						\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
 	.shared_pending	= { 						\
 		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\

commit 72d5a9f7a9542f88397558c65bcfc3b115a65e34
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 10 17:12:17 2010 -0700

    rcu: remove all rcu head initializations, except on_stack initializations
    
    Remove all rcu head inits. We don't care about the RCU head state before passing
    it to call_rcu() anyway. Only leave the "on_stack" variants so debugobjects can
    keep track of objects on stack.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b1ed1cd8e2a8..7996fc2c9ba9 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -49,7 +49,6 @@ extern struct group_info init_groups;
 		{ .first = &init_task.pids[PIDTYPE_PGID].node },	\
 		{ .first = &init_task.pids[PIDTYPE_SID].node },		\
 	},								\
-	.rcu		= RCU_HEAD_INIT,				\
 	.level		= 0,						\
 	.numbers	= { {						\
 		.nr		= 0,					\

commit 8467005da3ef6104b89a4cc5e9c9d9445b75565f
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Mar 10 15:23:10 2010 -0800

    nsproxy: remove INIT_NSPROXY()
    
    Remove INIT_NSPROXY(), use C99 initializer.
    Remove INIT_IPC_NS(), INIT_NET_NS() while I'm at it.
    
    Note: headers trim will be done later, now it's quite pointless because
    results will be invalidated by merge window.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index abec69b63d7e..b1ed1cd8e2a8 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -32,14 +32,6 @@ extern struct fs_struct init_fs;
 }
 
 extern struct nsproxy init_nsproxy;
-#define INIT_NSPROXY(nsproxy) {						\
-	.pid_ns		= &init_pid_ns,					\
-	.count		= ATOMIC_INIT(1),				\
-	.uts_ns		= &init_uts_ns,					\
-	.mnt_ns		= NULL,						\
-	INIT_NET_NS(net_ns)                                             \
-	INIT_IPC_NS(ipc_ns)						\
-}
 
 #define INIT_SIGHAND(sighand) {						\
 	.count		= ATOMIC_INIT(1), 				\

commit 2af7687f1ad2c4571b9835f9bb2e3db9a738d258
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sat Feb 20 01:03:35 2010 +0100

    Rename .data.init_task to .data..init_task.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index abec69b63d7e..f00253b3fc47 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -191,7 +191,7 @@ extern struct cred init_cred;
 }
 
 /* Attach to the init_task data structure for proper alignment */
-#define __init_task_data __attribute__((__section__(".data.init_task")))
+#define __init_task_data __attribute__((__section__(".data..init_task")))
 
 
 #endif

commit b6e3224fb20954f155e41ec5709b2ab70b50ae2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 17 13:23:24 2009 -0800

    Revert "task_struct: make journal_info conditional"
    
    This reverts commit e4c570c4cb7a95dbfafa3d016d2739bf3fdfe319, as
    requested by Alexey:
    
     "I think I gave a good enough arguments to not merge it.
      To iterate:
       * patch makes impossible to start using ext3 on EXT3_FS=n kernels
         without reboot.
       * this is done only for one pointer on task_struct"
    
      None of config options which define task_struct are tristate directly
      or effectively."
    
    Requested-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5ed8b9c50355..abec69b63d7e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -111,12 +111,6 @@ extern struct cred init_cred;
 # define INIT_PERF_EVENTS(tsk)
 #endif
 
-#ifdef CONFIG_FS_JOURNAL_INFO
-#define INIT_JOURNAL_INFO	.journal_info = NULL,
-#else
-#define INIT_JOURNAL_INFO
-#endif
-
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -168,6 +162,7 @@ extern struct cred init_cred;
 		.signal = {{0}}},					\
 	.blocked	= {{0}},					\
 	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
+	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
@@ -178,7 +173,6 @@ extern struct cred init_cred;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
-	INIT_JOURNAL_INFO						\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\

commit 8f0ddf91f2aeb09602373e400cf8b403e9017210
Merge: 050cbb09dac0 b5f91da0a697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 15 09:02:01 2009 -0800

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (26 commits)
      clockevents: Convert to raw_spinlock
      clockevents: Make tick_device_lock static
      debugobjects: Convert to raw_spinlocks
      perf_event: Convert to raw_spinlock
      hrtimers: Convert to raw_spinlocks
      genirq: Convert irq_desc.lock to raw_spinlock
      smp: Convert smplocks to raw_spinlocks
      rtmutes: Convert rtmutex.lock to raw_spinlock
      sched: Convert pi_lock to raw_spinlock
      sched: Convert cpupri lock to raw_spinlock
      sched: Convert rt_runtime_lock to raw_spinlock
      sched: Convert rq->lock to raw_spinlock
      plist: Make plist debugging raw_spinlock aware
      bkl: Fixup core_lock fallout
      locking: Cleanup the name space completely
      locking: Further name space cleanups
      alpha: Fix fallout from locking changes
      locking: Implement new raw_spinlock
      locking: Convert raw_rwlock functions to arch_rwlock
      locking: Convert raw_rwlock to arch_rwlock
      ...

commit e4c570c4cb7a95dbfafa3d016d2739bf3fdfe319
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Dec 14 18:00:26 2009 -0800

    task_struct: make journal_info conditional
    
    journal_info in task_struct is used in journaling file system only.  So
    introduce CONFIG_FS_JOURNAL_INFO and make it conditional.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: KONISHI Ryusuke <konishi.ryusuke@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 8d10aa7fd4c9..8ed0abf06f89 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -111,6 +111,12 @@ extern struct cred init_cred;
 # define INIT_PERF_EVENTS(tsk)
 #endif
 
+#ifdef CONFIG_FS_JOURNAL_INFO
+#define INIT_JOURNAL_INFO	.journal_info = NULL,
+#else
+#define INIT_JOURNAL_INFO
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -162,7 +168,6 @@ extern struct cred init_cred;
 		.signal = {{0}}},					\
 	.blocked	= {{0}},					\
 	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
-	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= __SPIN_LOCK_UNLOCKED(tsk.pi_lock),		\
@@ -173,6 +178,7 @@ extern struct cred init_cred;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
+	INIT_JOURNAL_INFO						\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\

commit 1d615482547584b9a8bb6316a58fed6ce90dd9ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 14:54:03 2009 +0100

    sched: Convert pi_lock to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 8d10aa7fd4c9..abec69b63d7e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -165,7 +165,7 @@ extern struct cred init_cred;
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
-	.pi_lock	= __SPIN_LOCK_UNLOCKED(tsk.pi_lock),		\
+	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
 	.timer_slack_ns = 50000, /* 50 usec default slack */		\
 	.pids = {							\
 		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\

commit b3a222e52e4d4be77cc4520a57af1a4a0d8222d1
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Nov 23 16:21:30 2009 -0600

    remove CONFIG_SECURITY_FILE_CAPABILITIES compile option
    
    As far as I know, all distros currently ship kernels with default
    CONFIG_SECURITY_FILE_CAPABILITIES=y.  Since having the option on
    leaves a 'no_file_caps' option to boot without file capabilities,
    the main reason to keep the option is that turning it off saves
    you (on my s390x partition) 5k.  In particular, vmlinux sizes
    came to:
    
    without patch fscaps=n:                 53598392
    without patch fscaps=y:                 53603406
    with this patch applied:                53603342
    
    with the security-next tree.
    
    Against this we must weigh the fact that there is no simple way for
    userspace to figure out whether file capabilities are supported,
    while things like per-process securebits, capability bounding
    sets, and adding bits to pI if CAP_SETPCAP is in pE are not supported
    with SECURITY_FILE_CAPABILITIES=n, leaving a bit of a problem for
    applications wanting to know whether they can use them and/or why
    something failed.
    
    It also adds another subtly different set of semantics which we must
    maintain at the risk of severe security regressions.
    
    So this patch removes the SECURITY_FILE_CAPABILITIES compile
    option.  It drops the kernel size by about 50k over the stock
    SECURITY_FILE_CAPABILITIES=y kernel, by removing the
    cap_limit_ptraced_target() function.
    
    Changelog:
            Nov 20: remove cap_limit_ptraced_target() as it's logic
                    was ifndef'ed.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Andrew G. Morgan" <morgan@kernel.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 21a6f5d9af22..8d10aa7fd4c9 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -83,16 +83,12 @@ extern struct group_info init_groups;
 #define INIT_IDS
 #endif
 
-#ifdef CONFIG_SECURITY_FILE_CAPABILITIES
 /*
  * Because of the reduced scope of CAP_SETPCAP when filesystem
  * capabilities are in effect, it is safe to allow CAP_SETPCAP to
  * be available in the default configuration.
  */
 # define CAP_INIT_BSET  CAP_FULL_SET
-#else
-# define CAP_INIT_BSET  CAP_INIT_EFF_SET
-#endif
 
 #ifdef CONFIG_TREE_PREEMPT_RCU
 #define INIT_TASK_RCU_PREEMPT(tsk)					\

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9e7f2e8fc66e..21a6f5d9af22 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -106,13 +106,13 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
-#ifdef CONFIG_PERF_COUNTERS
-# define INIT_PERF_COUNTERS(tsk)					\
-	.perf_counter_mutex = 						\
-		 __MUTEX_INITIALIZER(tsk.perf_counter_mutex),		\
-	.perf_counter_list = LIST_HEAD_INIT(tsk.perf_counter_list),
+#ifdef CONFIG_PERF_EVENTS
+# define INIT_PERF_EVENTS(tsk)					\
+	.perf_event_mutex = 						\
+		 __MUTEX_INITIALIZER(tsk.perf_event_mutex),		\
+	.perf_event_list = LIST_HEAD_INIT(tsk.perf_event_list),
 #else
-# define INIT_PERF_COUNTERS(tsk)
+# define INIT_PERF_EVENTS(tsk)
 #endif
 
 /*
@@ -178,7 +178,7 @@ extern struct cred init_cred;
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_IDS							\
-	INIT_PERF_COUNTERS(tsk)						\
+	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 	INIT_FTRACE_GRAPH						\

commit dd5d19bafd90d33043a4a14b2e2d98612caa293c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 27 14:58:16 2009 -0700

    rcu: Create rcutree plugins to handle hotplug CPU for multi-level trees
    
    When offlining CPUs from a multi-level tree, there is the
    possibility of offlining the last CPU from a given node when
    there are preempted RCU read-side critical sections that
    started life on one of the CPUs on that node.
    
    In this case, the corresponding tasks will be enqueued via the
    task_struct's rcu_node_entry list_head onto one of the
    rcu_node's blocked_tasks[] lists.  These tasks need to be moved
    somewhere else so that they will prevent the current grace
    period from ending. That somewhere is the root rcu_node.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <20090827215816.GA30472@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 79d4baee31b6..9e7f2e8fc66e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -98,7 +98,7 @@ extern struct group_info init_groups;
 #define INIT_TASK_RCU_PREEMPT(tsk)					\
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special = 0,					\
-	.rcu_blocked_cpu = -1,						\
+	.rcu_blocked_node = NULL,					\
 	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),
 #else
 #define INIT_TASK_RCU_PREEMPT(tsk)

commit 6b3ef48adf847f7adf11c870e3ffacac150f1564
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:53 2009 -0700

    rcu: Remove CONFIG_PREEMPT_RCU
    
    Now that CONFIG_TREE_PREEMPT_RCU is in place, there is no
    further need for CONFIG_PREEMPT_RCU.  Remove it, along with
    whatever subtle bugs it may (or may not) contain.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <125097461396-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 971a968831bf..79d4baee31b6 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -94,11 +94,7 @@ extern struct group_info init_groups;
 # define CAP_INIT_BSET  CAP_INIT_EFF_SET
 #endif
 
-#ifdef CONFIG_PREEMPT_RCU
-#define INIT_TASK_RCU_PREEMPT(tsk)					\
-	.rcu_read_lock_nesting = 0,					\
-	.rcu_flipctr_idx = 0,
-#elif defined(CONFIG_TREE_PREEMPT_RCU)
+#ifdef CONFIG_TREE_PREEMPT_RCU
 #define INIT_TASK_RCU_PREEMPT(tsk)					\
 	.rcu_read_lock_nesting = 0,					\
 	.rcu_read_unlock_special = 0,					\

commit f41d911f8c49a5d65c86504c19e8204bb605c4fd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:52 2009 -0700

    rcu: Merge preemptable-RCU functionality into hierarchical RCU
    
    Create a kernel/rcutree_plugin.h file that contains definitions
    for preemptable RCU (or, under the #else branch of the #ifdef,
    empty definitions for the classic non-preemptable semantics).
    These definitions fit into plugins defined in kernel/rcutree.c
    for this purpose.
    
    This variant of preemptable RCU uses a new algorithm whose
    read-side expense is roughly that of classic hierarchical RCU
    under CONFIG_PREEMPT. This new algorithm's update-side expense
    is similar to that of classic hierarchical RCU, and, in absence
    of read-side preemption or blocking, is exactly that of classic
    hierarchical RCU.  Perhaps more important, this new algorithm
    has a much simpler implementation, saving well over 1,000 lines
    of code compared to mainline's implementation of preemptable
    RCU, which will hopefully be retired in favor of this new
    algorithm.
    
    The simplifications are obtained by maintaining per-task
    nesting state for running tasks, and using a simple
    lock-protected algorithm to handle accounting when tasks block
    within RCU read-side critical sections, making use of lessons
    learned while creating numerous user-level RCU implementations
    over the past 18 months.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746134003-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 7fc01b13be43..971a968831bf 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -94,6 +94,20 @@ extern struct group_info init_groups;
 # define CAP_INIT_BSET  CAP_INIT_EFF_SET
 #endif
 
+#ifdef CONFIG_PREEMPT_RCU
+#define INIT_TASK_RCU_PREEMPT(tsk)					\
+	.rcu_read_lock_nesting = 0,					\
+	.rcu_flipctr_idx = 0,
+#elif defined(CONFIG_TREE_PREEMPT_RCU)
+#define INIT_TASK_RCU_PREEMPT(tsk)					\
+	.rcu_read_lock_nesting = 0,					\
+	.rcu_read_unlock_special = 0,					\
+	.rcu_blocked_cpu = -1,						\
+	.rcu_node_entry = LIST_HEAD_INIT(tsk.rcu_node_entry),
+#else
+#define INIT_TASK_RCU_PREEMPT(tsk)
+#endif
+
 extern struct cred init_cred;
 
 #ifdef CONFIG_PERF_COUNTERS
@@ -173,6 +187,7 @@ extern struct cred init_cred;
 	INIT_LOCKDEP							\
 	INIT_FTRACE_GRAPH						\
 	INIT_TRACE_RECURSION						\
+	INIT_TASK_RCU_PREEMPT(tsk)					\
 }
 
 

commit 857eceebd2803c9a3459f784acf45e5266921e4d
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Tue Jun 23 19:59:36 2009 -0400

    Add new __init_task_data macro to be used in arch init_task.c files.
    
    This patch is preparation for replacing most ".data.init_task" in the
    kernel with macros, so that the section name can later be changed
    without having to touch a lot of the kernel.
    
    The long-term goal here is to be able to change the kernel's magic
    section names to those that are compatible with -ffunction-sections
    -fdata-sections.  This requires renaming all magic sections with names
    of the form ".data.foo".
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5368fbdc7801..7fc01b13be43 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -183,5 +183,8 @@ extern struct cred init_cred;
 	LIST_HEAD_INIT(cpu_timers[2]),					\
 }
 
+/* Attach to the init_task data structure for proper alignment */
+#define __init_task_data __attribute__((__section__(".data.init_task")))
+
 
 #endif

commit bb1f17b0372de93758653ca3454bc0df18dc2e5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Jun 16 15:31:18 2009 -0700

    mm: consolidate init_mm definition
    
    * create mm/init-mm.c, move init_mm there
    * remove INIT_MM, initialize init_mm with C99 initializer
    * unexport init_mm on all arches:
    
      init_mm is already unexported on x86.
    
      One strange place is some OMAP driver (drivers/video/omap/) which
      won't build modular, but it's already wants get_vm_area() export.
      Somebody should look there.
    
    [akpm@linux-foundation.org: add missing #includes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Mike Frysinger <vapier.adi@gmail.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 28b1f30601b5..5368fbdc7801 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -15,18 +15,6 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
-#define INIT_MM(name) \
-{			 					\
-	.mm_rb		= RB_ROOT,				\
-	.pgd		= swapper_pg_dir, 			\
-	.mm_users	= ATOMIC_INIT(2), 			\
-	.mm_count	= ATOMIC_INIT(1), 			\
-	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
-	.page_table_lock =  __SPIN_LOCK_UNLOCKED(name.page_table_lock),	\
-	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
-	.cpu_vm_mask	= CPU_MASK_ALL,				\
-}
-
 #define INIT_SIGNALS(sig) {						\
 	.count		= ATOMIC_INIT(1), 				\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\

commit 8a1ca8cedd108c8e76a6ab34079d0bbb4f244799
Merge: b640f042faa2 940010c5a314
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 14:01:07 2009 -0700

    Merge branch 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (574 commits)
      perf_counter: Turn off by default
      perf_counter: Add counter->id to the throttle event
      perf_counter: Better align code
      perf_counter: Rename L2 to LL cache
      perf_counter: Standardize event names
      perf_counter: Rename enums
      perf_counter tools: Clean up u64 usage
      perf_counter: Rename perf_counter_limit sysctl
      perf_counter: More paranoia settings
      perf_counter: powerpc: Implement generalized cache events for POWER processors
      perf_counters: powerpc: Add support for POWER7 processors
      perf_counter: Accurate period data
      perf_counter: Introduce struct for sample data
      perf_counter tools: Normalize data using per sample period data
      perf_counter: Annotate exit ctx recursion
      perf_counter tools: Propagate signals properly
      perf_counter tools: Small frequency related fixes
      perf_counter: More aggressive frequency adjustment
      perf_counter/x86: Fix the model number of Intel Core2 processors
      perf_counter, x86: Correct some event and umask values for Intel processors
      ...

commit 3296ca27f50ecbd71db1d808c7a72d311027f919
Merge: e893123c7378 73fbad283cfb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 10:01:41 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6: (44 commits)
      nommu: Provide mmap_min_addr definition.
      TOMOYO: Add description of lists and structures.
      TOMOYO: Remove unused field.
      integrity: ima audit dentry_open failure
      TOMOYO: Remove unused parameter.
      security: use mmap_min_addr indepedently of security models
      TOMOYO: Simplify policy reader.
      TOMOYO: Remove redundant markers.
      SELinux: define audit permissions for audit tree netlink messages
      TOMOYO: Remove unused mutex.
      tomoyo: avoid get+put of task_struct
      smack: Remove redundant initialization.
      integrity: nfsd imbalance bug fix
      rootplug: Remove redundant initialization.
      smack: do not beyond ARRAY_SIZE of data
      integrity: move ima_counts_get
      integrity: path_check update
      IMA: Add __init notation to ima functions
      IMA: Minimal IMA policy and boot param for TCB IMA policy
      selinux: remove obsolete read buffer limit from sel_read_bool
      ...

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit 082ff5a2767a0679ee543f14883adbafb631ffbe
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat May 23 18:29:00 2009 +0200

    perf_counter: Change pctrl() behaviour
    
    Instead of en/dis-abling all counters acting on a particular
    task, en/dis- able all counters we created.
    
    [ v2: fix crash on first counter enable ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    LKML-Reference: <20090523163012.916937244@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d87247d2641f..353c0ac7723a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -108,6 +108,15 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
+#ifdef CONFIG_PERF_COUNTERS
+# define INIT_PERF_COUNTERS(tsk)					\
+	.perf_counter_mutex = 						\
+		 __MUTEX_INITIALIZER(tsk.perf_counter_mutex),		\
+	.perf_counter_list = LIST_HEAD_INIT(tsk.perf_counter_list),
+#else
+# define INIT_PERF_COUNTERS(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -171,6 +180,7 @@ extern struct cred init_cred;
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_IDS							\
+	INIT_PERF_COUNTERS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 	INIT_FTRACE_GRAPH						\

commit a63eaf34ae60bdb067a354cc8def2e8f4a01f5f4
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 22 14:17:31 2009 +1000

    perf_counter: Dynamically allocate tasks' perf_counter_context struct
    
    This replaces the struct perf_counter_context in the task_struct with
    a pointer to a dynamically allocated perf_counter_context struct.  The
    main reason for doing is this is to allow us to transfer a
    perf_counter_context from one task to another when we do lazy PMU
    switching in a later patch.
    
    This has a few side-benefits: the task_struct becomes a little smaller,
    we save some memory because only tasks that have perf_counters attached
    get a perf_counter_context allocated for them, and we can remove the
    inclusion of <linux/perf_counter.h> in sched.h, meaning that we don't
    end up recompiling nearly everything whenever perf_counter.h changes.
    
    The perf_counter_context structures are reference-counted and freed
    when the last reference is dropped.  A context can have references
    from its task and the counters on its task.  Counters can outlive the
    task so it is possible that a context will be freed well after its
    task has exited.
    
    Contexts are allocated on fork if the parent had a context, or
    otherwise the first time that a per-task counter is created on a task.
    In the latter case, we set the context pointer in the task struct
    locklessly using an atomic compare-and-exchange operation in case we
    raced with some other task in creating a context for the subject task.
    
    This also removes the task pointer from the perf_counter struct.  The
    task pointer was not used anywhere and would make it harder to move a
    context from one task to another.  Anything that needed to know which
    task a counter was attached to was already using counter->ctx->task.
    
    The __perf_counter_init_context function moves up in perf_counter.c
    so that it can be called from find_get_context, and now initializes
    the refcount, but is otherwise unchanged.
    
    We were potentially calling list_del_counter twice: once from
    __perf_counter_exit_task when the task exits and once from
    __perf_counter_remove_from_context when the counter's fd gets closed.
    This adds a check in list_del_counter so it doesn't do anything if
    the counter has already been removed from the lists.
    
    Since perf_counter_task_sched_in doesn't do anything if the task doesn't
    have a context, and leaves cpuctx->task_ctx = NULL, this adds code to
    __perf_install_in_context to set cpuctx->task_ctx if necessary, i.e. in
    the case where the current task adds the first counter to itself and
    thus creates a context for itself.
    
    This also adds similar code to __perf_counter_enable to handle a
    similar situation which can arise when the counters have been disabled
    using prctl; that also leaves cpuctx->task_ctx = NULL.
    
    [ Impact: refactor counter context management to prepare for new feature ]
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <18966.10075.781053.231153@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 503afaa0afa7..d87247d2641f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -108,18 +108,6 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
-#ifdef CONFIG_PERF_COUNTERS
-# define INIT_PERF_COUNTERS(tsk)					\
-	.perf_counter_ctx.counter_list =				\
-		LIST_HEAD_INIT(tsk.perf_counter_ctx.counter_list),	\
-	.perf_counter_ctx.event_list =					\
-		LIST_HEAD_INIT(tsk.perf_counter_ctx.event_list),	\
-	.perf_counter_ctx.lock =					\
-		__SPIN_LOCK_UNLOCKED(tsk.perf_counter_ctx.lock),
-#else
-# define INIT_PERF_COUNTERS(tsk)
-#endif
-
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -183,7 +171,6 @@ extern struct cred init_cred;
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_IDS							\
-	INIT_PERF_COUNTERS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 	INIT_FTRACE_GRAPH						\

commit 5e751e992f3fb08ba35e1ca8095ec8fbf9eda523
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 8 13:55:22 2009 +0100

    CRED: Rename cred_exec_mutex to reflect that it's a guard against ptrace
    
    Rename cred_exec_mutex to reflect that it's a guard against foreign
    intervention on a process's credential state, such as is made by ptrace().  The
    attachment of a debugger to a process affects execve()'s calculation of the new
    credential state - _and_ also setprocattr()'s calculation of that state.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d87247d2641f..7f54ba942429 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -145,8 +145,8 @@ extern struct cred init_cred;
 	.group_leader	= &tsk,						\
 	.real_cred	= &init_cred,					\
 	.cred		= &init_cred,					\
-	.cred_exec_mutex =						\
-		 __MUTEX_INITIALIZER(tsk.cred_exec_mutex),		\
+	.cred_guard_mutex =						\
+		 __MUTEX_INITIALIZER(tsk.cred_guard_mutex),		\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit 44347d947f628060b92449702071bfe1d31dfb75
Merge: d94fc523f3c3 413f81eba35d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:17:13 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: tracing/core was on a .30-rc1 base and was missing out on
                  on a handful of tracing fixes present in .30-rc5-almost.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e7fd5d4b3d240f42c30a9e3d20a4689c4d3a795a
Merge: 1130b0296184 56a50adda49b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 29 14:46:59 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Merge reason: This brach was on -rc1, refresh it to almost-rc4 to pick up
                  the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 261842b7c9099f56de2eb969c8ad65402d68e00e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 16 21:41:52 2009 -0400

    tracing: add same level recursion detection
    
    The tracing infrastructure allows for recursion. That is, an interrupt
    may interrupt the act of tracing an event, and that interrupt may very well
    perform its own trace. This is a recursive trace, and is fine to do.
    
    The problem arises when there is a bug, and the utility doing the trace
    calls something that recurses back into the tracer. This recursion is not
    caused by an external event like an interrupt, but by code that is not
    expected to recurse. The result could be a lockup.
    
    This patch adds a bitmask to the task structure that keeps track
    of the trace recursion. To find the interrupt depth, the following
    algorithm is used:
    
      level = hardirq_count() + softirq_count() + in_nmi;
    
    Here, level will be the depth of interrutps and softirqs, and even handles
    the nmi. Then the corresponding bit is set in the recursion bitmask.
    If the bit was already set, we know we had a recursion at the same level
    and we warn about it and fail the writing to the buffer.
    
    After the data has been committed to the buffer, we clear the bit.
    No atomics are needed. The only races are with interrupts and they reset
    the bitmask before returning anywy.
    
    [ Impact: detect same irq level trace recursion ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index dcfb93337e9a..6fc218529863 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -187,6 +187,7 @@ extern struct cred init_cred;
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 	INIT_FTRACE_GRAPH						\
+	INIT_TRACE_RECURSION						\
 }
 
 

commit 17a5138d204014b00cb9c1d6e8ff311993041b5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Apr 13 14:39:47 2009 -0700

    aio: remove INIT_KIOCTX
    
    Unused after 20dcae32439384b6863c626bb3b2a09bed65b33e aka
    "[PATCH] aio: remove kioctx from mm_struct".
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index dcfb93337e9a..d87247d2641f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -15,19 +15,6 @@
 extern struct files_struct init_files;
 extern struct fs_struct init_fs;
 
-#define INIT_KIOCTX(name, which_mm) \
-{							\
-	.users		= ATOMIC_INIT(1),		\
-	.dead		= 0,				\
-	.mm		= &which_mm,			\
-	.user_id	= 0,				\
-	.next		= NULL,				\
-	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER(name.wait), \
-	.ctx_lock	= __SPIN_LOCK_UNLOCKED(name.ctx_lock), \
-	.reqs_active	= 0U,				\
-	.max_reqs	= ~0U,				\
-}
-
 #define INIT_MM(name) \
 {			 					\
 	.mm_rb		= RB_ROOT,				\

commit 5ea472a77f8e4811ceee3f44a9deda6ad6e8b789
Merge: 6c009ecef8cc 577c9c456f0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 8 10:35:30 2009 +0200

    Merge commit 'v2.6.30-rc1' into perfcounters/core
    
    Conflicts:
            arch/powerpc/include/asm/systbl.h
            arch/powerpc/include/asm/unistd.h
            include/linux/init_task.h
    
    Merge reason: the conflicts are non-trivial: PowerPC placement
                  of sys_perf_counter_open has to be mixed with the
                  new preadv/pwrite syscalls.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c93f216b5b985a12a18323e5ca2eb01db3d2f000
Merge: c61b79b6ef26 ab3c9c686e22
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 14:10:10 2009 -0700

    Merge branch 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      branch tracer, intel-iommu: fix build with CONFIG_BRANCH_TRACER=y
      branch tracer: Fix for enabling branch profiling makes sparse unusable
      ftrace: Correct a text align for event format output
      Update /debug/tracing/README
      tracing/ftrace: alloc the started cpumask for the trace file
      tracing, x86: remove duplicated #include
      ftrace: Add check of sched_stopped for probe_sched_wakeup
      function-graph: add proper initialization for init task
      tracing/ftrace: fix missing include string.h
      tracing: fix incorrect return type of ns2usecs()
      tracing: remove CALLER_ADDR2 from wakeup tracer
      blktrace: fix pdu_len when tracing packet command requests
      blktrace: small cleanup in blk_msg_write()
      blktrace: NUL-terminate user space messages
      tracing: move scripts/trace/power.pl to scripts/tracing/power.pl

commit 86665c75da41889f92b774f31ea5a9a436f392a8
Merge: 93776a8ec746 1bbe2a83ab68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 14:41:14 2009 +0200

    Merge branch 'tracing/urgent' into tracing/ftrace

commit 5ac9f62267dc92c7735c642a5942d9e6c1190308
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Mar 25 20:55:00 2009 -0400

    function-graph: add proper initialization for init task
    
    Impact: fix to crash going to kexec
    
    The init task did not properly initialize the function graph pointers.
    Altough these pointers are NULL, they can not be assumed to be NULL
    for the init task, and must still be properly initialize.
    
    This usually is not an issue since a problem only arises when a task
    exits, and the init tasks do not usually exit. But when doing tests
    with kexec, the init tasks do exit, and the bug appears.
    
    This patch properly initializes the init tasks function graph data
    structures.
    
    Reported-and-Tested-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <alpine.DEB.2.00.0903252053080.5675@gandalf.stny.rr.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e752d973fa21..cada05447bc8 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -5,6 +5,7 @@
 #include <linux/irqflags.h>
 #include <linux/utsname.h>
 #include <linux/lockdep.h>
+#include <linux/ftrace.h>
 #include <linux/ipc.h>
 #include <linux/pid_namespace.h>
 #include <linux/user_namespace.h>
@@ -184,6 +185,7 @@ extern struct cred init_cred;
 	INIT_IDS							\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
+	INIT_FTRACE_GRAPH						\
 }
 
 

commit 01ef09d9ffb5ce9f8d62d1e5206da3d5ca612acc
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 19 20:26:11 2009 +0100

    perf_counter: fix uninitialized usage of event_list
    
    Impact: fix boot crash
    
    When doing the generic context switch event I ran into some early
    boot hangs, which were caused by inf func recursion (event, fault,
    event, fault).
    
    I eventually tracked it down to event_list not being initialized
    at the time of the first event. Fix this.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Orig-LKML-Reference: <20090319194233.195392657@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 219748d00262..ca226a91abee 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -124,6 +124,8 @@ extern struct cred init_cred;
 # define INIT_PERF_COUNTERS(tsk)					\
 	.perf_counter_ctx.counter_list =				\
 		LIST_HEAD_INIT(tsk.perf_counter_ctx.counter_list),	\
+	.perf_counter_ctx.event_list =					\
+		LIST_HEAD_INIT(tsk.perf_counter_ctx.event_list),	\
 	.perf_counter_ctx.lock =					\
 		__SPIN_LOCK_UNLOCKED(tsk.perf_counter_ctx.lock),
 #else

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 5274f8354d6a1ed9d6688e6a89b705b94aa1b6e9
Merge: ad0b0fd554df a0490fa35dc0 d2f8d7ee1a9b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 15 21:15:16 2009 +0100

    Merge branch 'sched/urgent'; commit 'v2.6.29-rc5' into sched/core

commit e9c4ffb11f0b19005b5b9dc8481687a3637e5887
Merge: 4bcf349a0f90 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:34:07 2009 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit 95fd4845ed0ffcab305b4f30ce1c12dc34f1b56c
Merge: d278c4843562 8e4921515c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 11 09:22:04 2009 +0100

    Merge commit 'v2.6.29-rc4' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c
            arch/x86/mm/fault.c
            drivers/acpi/processor_idle.c
            kernel/irq/handle.c

commit 140573d33b703194b7e1893711e78b7f546cca7c
Merge: 34cb61359b50 ceacc2c1c85a 483b4ee60edb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 8 20:12:46 2009 +0100

    Merge branches 'sched/rt' and 'sched/urgent' into sched/core

commit 4cd4c1b40d40447fb5e7ba80746c6d7ba91d7a53
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Feb 5 12:24:16 2009 +0100

    timers: split process wide cpu clocks/timers
    
    Change the process wide cpu timers/clocks so that we:
    
     1) don't mess up the kernel with too many threads,
     2) don't have a per-cpu allocation for each process,
     3) have no impact when not used.
    
    In order to accomplish this we're going to split it into two parts:
    
     - clocks; which can take all the time they want since they run
               from user context -- ie. sys_clock_gettime(CLOCK_PROCESS_CPUTIME_ID)
    
     - timers; which need constant time sampling but since they're
               explicity used, the user can pay the overhead.
    
    The clock readout will go back to a full sum of the thread group, while the
    timers will run of a global 'clock' that only runs when needed, so only
    programs that make use of the facility pay the price.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index ea0ea1a4c36f..e752d973fa21 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -48,12 +48,11 @@ extern struct fs_struct init_fs;
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
-	.cputime	= { .totals = {					\
-		.utime = cputime_zero,					\
-		.stime = cputime_zero,					\
-		.sum_exec_runtime = 0,					\
-		.lock = __SPIN_LOCK_UNLOCKED(sig.cputime.totals.lock),	\
-	}, },								\
+	.cputimer	= { 						\
+		.cputime = INIT_CPUTIME,				\
+		.running = 0,						\
+		.lock = __SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\
+	},								\
 }
 
 extern struct nsproxy init_nsproxy;

commit 0a6d4e1dc9154c4376358663d74060d1e33d203e
Merge: c59765042f53 1563513d34ed
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 04:58:49 2009 +0100

    Merge branch 'sched/latest' of git://git.kernel.org/pub/scm/linux/kernel/git/ghaskins/linux-2.6-hacks into sched/rt

commit 506c10f26c481b7f8ef27c1c79290f68989b2e9e
Merge: e1df957670ae c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:42:53 2009 +0100

    Merge commit 'v2.6.29-rc1' into perfcounters/core
    
    Conflicts:
            include/linux/kernel_stat.h

commit 490dea45d00f01847ebebd007685d564aaf2cd98
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 24 17:06:57 2008 +0100

    itimers: remove the per-cpu-ish-ness
    
    Either we bounce once cacheline per cpu per tick, yielding n^2 bounces
    or we just bounce a single..
    
    Also, using per-cpu allocations for the thread-groups complicates the
    per-cpu allocator in that its currently aimed to be a fixed sized
    allocator and the only possible extention to that would be vmap based,
    which is seriously constrained on 32 bit archs.
    
    So making the per-cpu memory requirement depend on the number of
    processes is an issue.
    
    Lastly, it didn't deal with cpu-hotplug, although admittedly that might
    be fixable.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 2f3c2d4ef73b..ea0ea1a4c36f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -48,6 +48,12 @@ extern struct fs_struct init_fs;
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
+	.cputime	= { .totals = {					\
+		.utime = cputime_zero,					\
+		.stime = cputime_zero,					\
+		.sum_exec_runtime = 0,					\
+		.lock = __SPIN_LOCK_UNLOCKED(sig.cputime.totals.lock),	\
+	}, },								\
 }
 
 extern struct nsproxy init_nsproxy;

commit 18d8fda7c3c9439be04d7ea2e82da2513b121acb
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Dec 26 00:35:37 2008 -0500

    take init_fs to saner place
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 959f5522d10a..2f3c2d4ef73b 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -12,6 +12,7 @@
 #include <net/net_namespace.h>
 
 extern struct files_struct init_files;
+extern struct fs_struct init_fs;
 
 #define INIT_KIOCTX(name, which_mm) \
 {							\

commit 917b627d4d981dc614519d7b34ea31a976b14e12
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Mon Dec 29 09:39:53 2008 -0500

    sched: create "pushable_tasks" list to limit pushing to one attempt
    
    The RT scheduler employs a "push/pull" design to actively balance tasks
    within the system (on a per disjoint cpuset basis).  When a task is
    awoken, it is immediately determined if there are any lower priority
    cpus which should be preempted.  This is opposed to the way normal
    SCHED_OTHER tasks behave, which will wait for a periodic rebalancing
    operation to occur before spreading out load.
    
    When a particular RQ has more than 1 active RT task, it is said to
    be in an "overloaded" state.  Once this occurs, the system enters
    the active balancing mode, where it will try to push the task away,
    or persuade a different cpu to pull it over.  The system will stay
    in this state until the system falls back below the <= 1 queued RT
    task per RQ.
    
    However, the current implementation suffers from a limitation in the
    push logic.  Once overloaded, all tasks (other than current) on the
    RQ are analyzed on every push operation, even if it was previously
    unpushable (due to affinity, etc).  Whats more, the operation stops
    at the first task that is unpushable and will not look at items
    lower in the queue.  This causes two problems:
    
    1) We can have the same tasks analyzed over and over again during each
       push, which extends out the fast path in the scheduler for no
       gain.  Consider a RQ that has dozens of tasks that are bound to a
       core.  Each one of those tasks will be encountered and skipped
       for each push operation while they are queued.
    
    2) There may be lower-priority tasks under the unpushable task that
       could have been successfully pushed, but will never be considered
       until either the unpushable task is cleared, or a pull operation
       succeeds.  The net result is a potential latency source for mid
       priority tasks.
    
    This patch aims to rectify these two conditions by introducing a new
    priority sorted list: "pushable_tasks".  A task is added to the list
    each time a task is activated or preempted.  It is removed from the
    list any time it is deactivated, made current, or fails to push.
    
    This works because a task only needs to be attempted to push once.
    After an initial failure to push, the other cpus will eventually try to
    pull the task when the conditions are proper.  This also solves the
    problem that we don't completely analyze all tasks due to encountering
    an unpushable tasks.  Now every task will have a push attempted (when
    appropriate).
    
    This reduces latency both by shorting the critical section of the
    rq->lock for certain workloads, and by making sure the algorithm
    considers all eligible tasks in the system.
    
    [ rostedt: added a couple more BUG_ONs ]
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Acked-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 23fd8909b9e5..6851225f44a7 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -140,6 +140,7 @@ extern struct group_info init_groups;
 		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
+	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO), \
 	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
 	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
 	.real_parent	= &tsk,						\

commit e1df957670aef74ffd9a4ad93e6d2c90bf6b4845
Merge: 2b583d8bc8d7 3c92ec8ae91e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 09:45:15 2008 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            fs/exec.c
            include/linux/init_task.h
    
    Simple context conflicts.

commit 78b6084c907cea15bb40a564b974e072f5163781
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Dec 21 15:07:49 2008 +0100

    perfcounters: fix init context lock
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 54fa2fa2c8e4..467cff545c30 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -116,7 +116,9 @@ extern struct group_info init_groups;
 #ifdef CONFIG_PERF_COUNTERS
 # define INIT_PERF_COUNTERS(tsk)					\
 	.perf_counter_ctx.counter_list =				\
-		LIST_HEAD_INIT(tsk.perf_counter_ctx.counter_list),
+		LIST_HEAD_INIT(tsk.perf_counter_ctx.counter_list),	\
+	.perf_counter_ctx.lock =					\
+		__SPIN_LOCK_UNLOCKED(tsk.perf_counter_ctx.lock),
 #else
 # define INIT_PERF_COUNTERS(tsk)
 #endif

commit eef6cbf5844c620d9db9be99e4908cdf92492fb9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 19 10:20:42 2008 +0100

    perfcounters: pull inherited counters
    
    Change counter inheritance from a 'push' to a 'pull' model: instead of
    child tasks pushing their final counts to the parent, reuse the wait4
    infrastructure to pull counters as child tasks are exit-processed,
    much like how cutime/cstime is collected.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 23fd8909b9e5..54fa2fa2c8e4 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -113,6 +113,14 @@ extern struct group_info init_groups;
 # define CAP_INIT_BSET  CAP_INIT_EFF_SET
 #endif
 
+#ifdef CONFIG_PERF_COUNTERS
+# define INIT_PERF_COUNTERS(tsk)					\
+	.perf_counter_ctx.counter_list =				\
+		LIST_HEAD_INIT(tsk.perf_counter_ctx.counter_list),
+#else
+# define INIT_PERF_COUNTERS(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -180,6 +188,7 @@ extern struct group_info init_groups;
 	INIT_IDS							\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
+	INIT_PERF_COUNTERS(tsk)						\
 }
 
 

commit 18b6e0414e42d95183f07d8177e3ff0241abd825
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Wed Oct 15 16:38:45 2008 -0500

    User namespaces: set of cleanups (v2)
    
    The user_ns is moved from nsproxy to user_struct, so that a struct
    cred by itself is sufficient to determine access (which it otherwise
    would not be).  Corresponding ecryptfs fixes (by David Howells) are
    here as well.
    
    Fix refcounting.  The following rules now apply:
            1. The task pins the user struct.
            2. The user struct pins its user namespace.
            3. The user namespace pins the struct user which created it.
    
    User namespaces are cloned during copy_creds().  Unsharing a new user_ns
    is no longer possible.  (We could re-add that, but it'll cause code
    duplication and doesn't seem useful if PAM doesn't need to clone user
    namespaces).
    
    When a user namespace is created, its first user (uid 0) gets empty
    keyrings and a clean group_info.
    
    This incorporates a previous patch by David Howells.  Here
    is his original patch description:
    
    >I suggest adding the attached incremental patch.  It makes the following
    >changes:
    >
    > (1) Provides a current_user_ns() macro to wrap accesses to current's user
    >     namespace.
    >
    > (2) Fixes eCryptFS.
    >
    > (3) Renames create_new_userns() to create_user_ns() to be more consistent
    >     with the other associated functions and because the 'new' in the name is
    >     superfluous.
    >
    > (4) Moves the argument and permission checks made for CLONE_NEWUSER to the
    >     beginning of do_fork() so that they're done prior to making any attempts
    >     at allocation.
    >
    > (5) Calls create_user_ns() after prepare_creds(), and gives it the new creds
    >     to fill in rather than have it return the new root user.  I don't imagine
    >     the new root user being used for anything other than filling in a cred
    >     struct.
    >
    >     This also permits me to get rid of a get_uid() and a free_uid(), as the
    >     reference the creds were holding on the old user_struct can just be
    >     transferred to the new namespace's creator pointer.
    >
    > (6) Makes create_user_ns() reset the UIDs and GIDs of the creds under
    >     preparation rather than doing it in copy_creds().
    >
    >David
    
    >Signed-off-by: David Howells <dhowells@redhat.com>
    
    Changelog:
            Oct 20: integrate dhowells comments
                    1. leave thread_keyring alone
                    2. use current_user_ns() in set_user()
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 2597858035cd..959f5522d10a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -57,7 +57,6 @@ extern struct nsproxy init_nsproxy;
 	.mnt_ns		= NULL,						\
 	INIT_NET_NS(net_ns)                                             \
 	INIT_IPC_NS(ipc_ns)						\
-	.user_ns	= &init_user_ns,				\
 }
 
 #define INIT_SIGHAND(sighand) {						\

commit 3b11a1decef07c19443d24ae926982bc8ec9f4c0
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:26 2008 +1100

    CRED: Differentiate objective and effective subjective credentials on a task
    
    Differentiate the objective and real subjective credentials from the effective
    subjective credentials on a task by introducing a second credentials pointer
    into the task_struct.
    
    task_struct::real_cred then refers to the objective and apparent real
    subjective credentials of a task, as perceived by the other tasks in the
    system.
    
    task_struct::cred then refers to the effective subjective credentials of a
    task, as used by that task when it's actually running.  These are not visible
    to the other tasks in the system.
    
    __task_cred(task) then refers to the objective/real credentials of the task in
    question.
    
    current_cred() refers to the effective subjective credentials of the current
    task.
    
    prepare_creds() uses the objective creds as a base and commit_creds() changes
    both pointers in the task_struct (indeed commit_creds() requires them to be the
    same).
    
    override_creds() and revert_creds() change the subjective creds pointer only,
    and the former returns the old subjective creds.  These are used by NFSD,
    faccessat() and do_coredump(), and will by used by CacheFiles.
    
    In SELinux, current_has_perm() is provided as an alternative to
    task_has_perm().  This uses the effective subjective context of current,
    whereas task_has_perm() uses the objective/real context of the subject.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 08c3b24ad9a8..2597858035cd 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -149,6 +149,7 @@ extern struct cred init_cred;
 	.children	= LIST_HEAD_INIT(tsk.children),			\
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
+	.real_cred	= &init_cred,					\
 	.cred		= &init_cred,					\
 	.cred_exec_mutex =						\
 		 __MUTEX_INITIALIZER(tsk.cred_exec_mutex),		\

commit d84f4f992cbd76e8f39c488cf0c5d123843923b1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:23 2008 +1100

    CRED: Inaugurate COW credentials
    
    Inaugurate copy-on-write credentials management.  This uses RCU to manage the
    credentials pointer in the task_struct with respect to accesses by other tasks.
    A process may only modify its own credentials, and so does not need locking to
    access or modify its own credentials.
    
    A mutex (cred_replace_mutex) is added to the task_struct to control the effect
    of PTRACE_ATTACHED on credential calculations, particularly with respect to
    execve().
    
    With this patch, the contents of an active credentials struct may not be
    changed directly; rather a new set of credentials must be prepared, modified
    and committed using something like the following sequence of events:
    
            struct cred *new = prepare_creds();
            int ret = blah(new);
            if (ret < 0) {
                    abort_creds(new);
                    return ret;
            }
            return commit_creds(new);
    
    There are some exceptions to this rule: the keyrings pointed to by the active
    credentials may be instantiated - keyrings violate the COW rule as managing
    COW keyrings is tricky, given that it is possible for a task to directly alter
    the keys in a keyring in use by another task.
    
    To help enforce this, various pointers to sets of credentials, such as those in
    the task_struct, are declared const.  The purpose of this is compile-time
    discouragement of altering credentials through those pointers.  Once a set of
    credentials has been made public through one of these pointers, it may not be
    modified, except under special circumstances:
    
      (1) Its reference count may incremented and decremented.
    
      (2) The keyrings to which it points may be modified, but not replaced.
    
    The only safe way to modify anything else is to create a replacement and commit
    using the functions described in Documentation/credentials.txt (which will be
    added by a later patch).
    
    This patch and the preceding patches have been tested with the LTP SELinux
    testsuite.
    
    This patch makes several logical sets of alteration:
    
     (1) execve().
    
         This now prepares and commits credentials in various places in the
         security code rather than altering the current creds directly.
    
     (2) Temporary credential overrides.
    
         do_coredump() and sys_faccessat() now prepare their own credentials and
         temporarily override the ones currently on the acting thread, whilst
         preventing interference from other threads by holding cred_replace_mutex
         on the thread being dumped.
    
         This will be replaced in a future patch by something that hands down the
         credentials directly to the functions being called, rather than altering
         the task's objective credentials.
    
     (3) LSM interface.
    
         A number of functions have been changed, added or removed:
    
         (*) security_capset_check(), ->capset_check()
         (*) security_capset_set(), ->capset_set()
    
             Removed in favour of security_capset().
    
         (*) security_capset(), ->capset()
    
             New.  This is passed a pointer to the new creds, a pointer to the old
             creds and the proposed capability sets.  It should fill in the new
             creds or return an error.  All pointers, barring the pointer to the
             new creds, are now const.
    
         (*) security_bprm_apply_creds(), ->bprm_apply_creds()
    
             Changed; now returns a value, which will cause the process to be
             killed if it's an error.
    
         (*) security_task_alloc(), ->task_alloc_security()
    
             Removed in favour of security_prepare_creds().
    
         (*) security_cred_free(), ->cred_free()
    
             New.  Free security data attached to cred->security.
    
         (*) security_prepare_creds(), ->cred_prepare()
    
             New. Duplicate any security data attached to cred->security.
    
         (*) security_commit_creds(), ->cred_commit()
    
             New. Apply any security effects for the upcoming installation of new
             security by commit_creds().
    
         (*) security_task_post_setuid(), ->task_post_setuid()
    
             Removed in favour of security_task_fix_setuid().
    
         (*) security_task_fix_setuid(), ->task_fix_setuid()
    
             Fix up the proposed new credentials for setuid().  This is used by
             cap_set_fix_setuid() to implicitly adjust capabilities in line with
             setuid() changes.  Changes are made to the new credentials, rather
             than the task itself as in security_task_post_setuid().
    
         (*) security_task_reparent_to_init(), ->task_reparent_to_init()
    
             Removed.  Instead the task being reparented to init is referred
             directly to init's credentials.
    
             NOTE!  This results in the loss of some state: SELinux's osid no
             longer records the sid of the thread that forked it.
    
         (*) security_key_alloc(), ->key_alloc()
         (*) security_key_permission(), ->key_permission()
    
             Changed.  These now take cred pointers rather than task pointers to
             refer to the security context.
    
     (4) sys_capset().
    
         This has been simplified and uses less locking.  The LSM functions it
         calls have been merged.
    
     (5) reparent_to_kthreadd().
    
         This gives the current thread the same credentials as init by simply using
         commit_thread() to point that way.
    
     (6) __sigqueue_alloc() and switch_uid()
    
         __sigqueue_alloc() can't stop the target task from changing its creds
         beneath it, so this function gets a reference to the currently applicable
         user_struct which it then passes into the sigqueue struct it returns if
         successful.
    
         switch_uid() is now called from commit_creds(), and possibly should be
         folded into that.  commit_creds() should take care of protecting
         __sigqueue_alloc().
    
     (7) [sg]et[ug]id() and co and [sg]et_current_groups.
    
         The set functions now all use prepare_creds(), commit_creds() and
         abort_creds() to build and check a new set of credentials before applying
         it.
    
         security_task_set[ug]id() is called inside the prepared section.  This
         guarantees that nothing else will affect the creds until we've finished.
    
         The calling of set_dumpable() has been moved into commit_creds().
    
         Much of the functionality of set_user() has been moved into
         commit_creds().
    
         The get functions all simply access the data directly.
    
     (8) security_task_prctl() and cap_task_prctl().
    
         security_task_prctl() has been modified to return -ENOSYS if it doesn't
         want to handle a function, or otherwise return the return value directly
         rather than through an argument.
    
         Additionally, cap_task_prctl() now prepares a new set of credentials, even
         if it doesn't end up using it.
    
     (9) Keyrings.
    
         A number of changes have been made to the keyrings code:
    
         (a) switch_uid_keyring(), copy_keys(), exit_keys() and suid_keys() have
             all been dropped and built in to the credentials functions directly.
             They may want separating out again later.
    
         (b) key_alloc() and search_process_keyrings() now take a cred pointer
             rather than a task pointer to specify the security context.
    
         (c) copy_creds() gives a new thread within the same thread group a new
             thread keyring if its parent had one, otherwise it discards the thread
             keyring.
    
         (d) The authorisation key now points directly to the credentials to extend
             the search into rather pointing to the task that carries them.
    
         (e) Installing thread, process or session keyrings causes a new set of
             credentials to be created, even though it's not strictly necessary for
             process or session keyrings (they're shared).
    
    (10) Usermode helper.
    
         The usermode helper code now carries a cred struct pointer in its
         subprocess_info struct instead of a new session keyring pointer.  This set
         of credentials is derived from init_cred and installed on the new process
         after it has been cloned.
    
         call_usermodehelper_setup() allocates the new credentials and
         call_usermodehelper_freeinfo() discards them if they haven't been used.  A
         special cred function (prepare_usermodeinfo_creds()) is provided
         specifically for call_usermodehelper_setup() to call.
    
         call_usermodehelper_setkeys() adjusts the credentials to sport the
         supplied keyring as the new session keyring.
    
    (11) SELinux.
    
         SELinux has a number of changes, in addition to those to support the LSM
         interface changes mentioned above:
    
         (a) selinux_setprocattr() no longer does its check for whether the
             current ptracer can access processes with the new SID inside the lock
             that covers getting the ptracer's SID.  Whilst this lock ensures that
             the check is done with the ptracer pinned, the result is only valid
             until the lock is released, so there's no point doing it inside the
             lock.
    
    (12) is_single_threaded().
    
         This function has been extracted from selinux_setprocattr() and put into
         a file of its own in the lib/ directory as join_session_keyring() now
         wants to use it too.
    
         The code in SELinux just checked to see whether a task shared mm_structs
         with other tasks (CLONE_VM), but that isn't good enough.  We really want
         to know if they're part of the same thread group (CLONE_THREAD).
    
    (13) nfsd.
    
         The NFS server daemon now has to use the COW credentials to set the
         credentials it is going to use.  It really needs to pass the credentials
         down to the functions it calls, but it can't do that until other patches
         in this series have been applied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5e24c54b6dfd..08c3b24ad9a8 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -150,6 +150,8 @@ extern struct cred init_cred;
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
 	.cred		= &init_cred,					\
+	.cred_exec_mutex =						\
+		 __MUTEX_INITIALIZER(tsk.cred_exec_mutex),		\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit f1752eec6145c97163dbce62d17cf5d928e28a27
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:17 2008 +1100

    CRED: Detach the credentials from task_struct
    
    Detach the credentials from task_struct, duplicating them in copy_process()
    and releasing them in __put_task_struct().
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9de41ccd67b5..5e24c54b6dfd 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -115,19 +115,6 @@ extern struct group_info init_groups;
 
 extern struct cred init_cred;
 
-#define INIT_CRED(p)						\
-{								\
-	.usage			= ATOMIC_INIT(3),		\
-	.securebits		= SECUREBITS_DEFAULT,		\
-	.cap_inheritable	= CAP_INIT_INH_SET,		\
-	.cap_permitted		= CAP_FULL_SET,			\
-	.cap_effective		= CAP_INIT_EFF_SET,		\
-	.cap_bset		= CAP_INIT_BSET,		\
-	.user			= INIT_USER,			\
-	.group_info		= &init_groups,			\
-	.lock			= __SPIN_LOCK_UNLOCKED(p.lock),	\
-}
-
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -162,8 +149,7 @@ extern struct cred init_cred;
 	.children	= LIST_HEAD_INIT(tsk.children),			\
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
-	.__temp_cred	= INIT_CRED(tsk.__temp_cred),			\
-	.cred		= &tsk.__temp_cred,				\
+	.cred		= &init_cred,					\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit b6dff3ec5e116e3af6f537d4caedcad6b9e5082a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:16 2008 +1100

    CRED: Separate task security context from task_struct
    
    Separate the task security context from task_struct.  At this point, the
    security data is temporarily embedded in the task_struct with two pointers
    pointing to it.
    
    Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
    entry.S via asm-offsets.
    
    With comment fixes Signed-off-by: Marc Dionne <marc.c.dionne@gmail.com>
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 23fd8909b9e5..9de41ccd67b5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -113,6 +113,21 @@ extern struct group_info init_groups;
 # define CAP_INIT_BSET  CAP_INIT_EFF_SET
 #endif
 
+extern struct cred init_cred;
+
+#define INIT_CRED(p)						\
+{								\
+	.usage			= ATOMIC_INIT(3),		\
+	.securebits		= SECUREBITS_DEFAULT,		\
+	.cap_inheritable	= CAP_INIT_INH_SET,		\
+	.cap_permitted		= CAP_FULL_SET,			\
+	.cap_effective		= CAP_INIT_EFF_SET,		\
+	.cap_bset		= CAP_INIT_BSET,		\
+	.user			= INIT_USER,			\
+	.group_info		= &init_groups,			\
+	.lock			= __SPIN_LOCK_UNLOCKED(p.lock),	\
+}
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -147,13 +162,8 @@ extern struct group_info init_groups;
 	.children	= LIST_HEAD_INIT(tsk.children),			\
 	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
 	.group_leader	= &tsk,						\
-	.group_info	= &init_groups,					\
-	.cap_effective	= CAP_INIT_EFF_SET,				\
-	.cap_inheritable = CAP_INIT_INH_SET,				\
-	.cap_permitted	= CAP_FULL_SET,					\
-	.cap_bset 	= CAP_INIT_BSET,				\
-	.securebits     = SECUREBITS_DEFAULT,				\
-	.user		= INIT_USER,					\
+	.__temp_cred	= INIT_CRED(tsk.__temp_cred),			\
+	.cred		= &tsk.__temp_cred,				\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\
 	.fs		= &init_fs,					\

commit 6976675d94042fbd446231d1bd8b7de71a980ada
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Sep 1 15:52:40 2008 -0700

    hrtimer: create a "timer_slack" field in the task struct
    
    We want to be able to control the default "rounding" that is used by
    select() and poll() and friends. This is a per process property
    (so that we can have a "nice" like program to start certain programs with
    a looser or stricter rounding) that can be set/get via a prctl().
    
    For this purpose, a field called "timer_slack_ns" is added to the task
    struct. In addition, a field called "default_timer_slack"ns" is added
    so that tasks easily can temporarily to a more/less accurate slack and then
    back to the default.
    
    The default value of the slack is set to 50 usec; this is significantly less
    than 2.6.27's average select() and poll() timing error but still allows
    the kernel to group timers somewhat to preserve power behavior. Applications
    and admins can override this via the prctl()
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 021d8e720c79..23fd8909b9e5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -170,6 +170,7 @@ extern struct group_info init_groups;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= __SPIN_LOCK_UNLOCKED(tsk.pi_lock),		\
+	.timer_slack_ns = 50000, /* 50 usec default slack */		\
 	.pids = {							\
 		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
 		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\

commit 7b34e4283c685f5cc6ba6d30e939906eee0d4bcf
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:37 2008 -0700

    introduce PF_KTHREAD flag
    
    Introduce the new PF_KTHREAD flag to mark the kernel threads.  It is set
    by INIT_TASK() and copied to the forked childs (we could set it in
    kthreadd() along with PF_NOFREEZE instead).
    
    daemonize() was changed as well.  In that case testing of PF_KTHREAD is
    racy, but daemonize() is hopeless anyway.
    
    This flag is cleared in do_execve(), before search_binary_handler().
    Probably not the best place, we can do this in exec_mmap() or in
    start_thread(), or clear it along with PF_FORKNOEXEC.  But I think this
    doesn't matter in practice, and if do_execve() fails kthread should die
    soon.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 93c45acf249a..021d8e720c79 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -122,7 +122,7 @@ extern struct group_info init_groups;
 	.state		= 0,						\
 	.stack		= &init_thread_info,				\
 	.usage		= ATOMIC_INIT(2),				\
-	.flags		= 0,						\
+	.flags		= PF_KTHREAD,					\
 	.lock_depth	= -1,						\
 	.prio		= MAX_PRIO-20,					\
 	.static_prio	= MAX_PRIO-20,					\

commit f470021adb9190819c03d6d8c5c860a17480aa6d
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Mar 24 18:36:23 2008 -0700

    ptrace children revamp
    
    ptrace no longer fiddles with the children/sibling links, and the
    old ptrace_children list is gone.  Now ptrace, whether of one's own
    children or another's via PTRACE_ATTACH, just uses the new ptraced
    list instead.
    
    There should be no user-visible difference that matters.  The only
    change is the order in which do_wait() sees multiple stopped
    children and stopped ptrace attachees.  Since wait_task_stopped()
    was changed earlier so it no longer reorders the children list, we
    already know this won't cause any new problems.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 9927a88674a3..93c45acf249a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -140,8 +140,8 @@ extern struct group_info init_groups;
 		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
-	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
-	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\
+	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
+	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
 	.real_parent	= &tsk,						\
 	.parent		= &tsk,						\
 	.children	= LIST_HEAD_INIT(tsk.children),			\

commit f52111b1546943545e67573c4dde1c7613ca33d3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu May 8 18:19:16 2008 -0400

    [PATCH] take init_files to fs/file.c
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b24c2875aa05..9927a88674a3 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -1,7 +1,6 @@
 #ifndef _LINUX__INIT_TASK_H
 #define _LINUX__INIT_TASK_H
 
-#include <linux/fdtable.h>
 #include <linux/rcupdate.h>
 #include <linux/irqflags.h>
 #include <linux/utsname.h>
@@ -12,27 +11,7 @@
 #include <linux/securebits.h>
 #include <net/net_namespace.h>
 
-#define INIT_FDTABLE \
-{							\
-	.max_fds	= NR_OPEN_DEFAULT, 		\
-	.fd		= &init_files.fd_array[0], 	\
-	.close_on_exec	= (fd_set *)&init_files.close_on_exec_init, \
-	.open_fds	= (fd_set *)&init_files.open_fds_init, 	\
-	.rcu		= RCU_HEAD_INIT, 		\
-	.next		= NULL,		 		\
-}
-
-#define INIT_FILES \
-{ 							\
-	.count		= ATOMIC_INIT(1), 		\
-	.fdt		= &init_files.fdtab, 		\
-	.fdtab		= INIT_FDTABLE,			\
-	.file_lock	= __SPIN_LOCK_UNLOCKED(init_task.file_lock), \
-	.next_fd	= 0, 				\
-	.close_on_exec_init = { { 0, } }, 		\
-	.open_fds_init	= { { 0, } }, 			\
-	.fd_array	= { NULL, } 			\
-}
+extern struct files_struct init_files;
 
 #define INIT_KIOCTX(name, which_mm) \
 {							\

commit 9f3acc3140444a900ab280de942291959f0f615d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 24 07:44:08 2008 -0400

    [PATCH] split linux/file.h
    
    Initial splitoff of the low-level stuff; taken to fdtable.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index bf6b8a61f8db..b24c2875aa05 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -1,7 +1,7 @@
 #ifndef _LINUX__INIT_TASK_H
 #define _LINUX__INIT_TASK_H
 
-#include <linux/file.h>
+#include <linux/fdtable.h>
 #include <linux/rcupdate.h>
 #include <linux/irqflags.h>
 #include <linux/utsname.h>

commit 3898b1b4ebff8dcfbcf1807e0661585e06c9a91c
Author: Andrew G. Morgan <morgan@kernel.org>
Date:   Mon Apr 28 02:13:40 2008 -0700

    capabilities: implement per-process securebits
    
    Filesystem capability support makes it possible to do away with (set)uid-0
    based privilege and use capabilities instead.  That is, with filesystem
    support for capabilities but without this present patch, it is (conceptually)
    possible to manage a system with capabilities alone and never need to obtain
    privilege via (set)uid-0.
    
    Of course, conceptually isn't quite the same as currently possible since few
    user applications, certainly not enough to run a viable system, are currently
    prepared to leverage capabilities to exercise privilege.  Further, many
    applications exist that may never get upgraded in this way, and the kernel
    will continue to want to support their setuid-0 base privilege needs.
    
    Where pure-capability applications evolve and replace setuid-0 binaries, it is
    desirable that there be a mechanisms by which they can contain their
    privilege.  In addition to leveraging the per-process bounding and inheritable
    sets, this should include suppressing the privilege of the uid-0 superuser
    from the process' tree of children.
    
    The feature added by this patch can be leveraged to suppress the privilege
    associated with (set)uid-0.  This suppression requires CAP_SETPCAP to
    initiate, and only immediately affects the 'current' process (it is inherited
    through fork()/exec()).  This reimplementation differs significantly from the
    historical support for securebits which was system-wide, unwieldy and which
    has ultimately withered to a dead relic in the source of the modern kernel.
    
    With this patch applied a process, that is capable(CAP_SETPCAP), can now drop
    all legacy privilege (through uid=0) for itself and all subsequently
    fork()'d/exec()'d children with:
    
      prctl(PR_SET_SECUREBITS, 0x2f);
    
    This patch represents a no-op unless CONFIG_SECURITY_FILE_CAPABILITIES is
    enabled at configure time.
    
    [akpm@linux-foundation.org: fix uninitialised var warning]
    [serue@us.ibm.com: capabilities: use cap_task_prctl when !CONFIG_SECURITY]
    Signed-off-by: Andrew G. Morgan <morgan@kernel.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Reviewed-by: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Paul Moore <paul.moore@hp.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 37a6f5bc4a92..bf6b8a61f8db 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -9,6 +9,7 @@
 #include <linux/ipc.h>
 #include <linux/pid_namespace.h>
 #include <linux/user_namespace.h>
+#include <linux/securebits.h>
 #include <net/net_namespace.h>
 
 #define INIT_FDTABLE \
@@ -172,7 +173,7 @@ extern struct group_info init_groups;
 	.cap_inheritable = CAP_INIT_INH_SET,				\
 	.cap_permitted	= CAP_FULL_SET,					\
 	.cap_bset 	= CAP_INIT_BSET,				\
-	.keep_capabilities = 0,						\
+	.securebits     = SECUREBITS_DEFAULT,				\
 	.user		= INIT_USER,					\
 	.comm		= "swapper",					\
 	.thread		= INIT_THREAD,					\

commit 4a55bd5e97b1775913f88f11108a4f144f590e89
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: fair-group: de-couple load-balancing from the rb-trees
    
    De-couple load-balancing from the rb-trees, so that I can change their
    organization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1f74e1d7415f..37a6f5bc4a92 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -151,6 +151,9 @@ extern struct group_info init_groups;
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
+	.se		= {						\
+		.group_node 	= LIST_HEAD_INIT(tsk.se.group_node),	\
+	},								\
 	.rt		= {						\
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
 		.time_slice	= HZ, 					\

commit 3b7391de67da515c91f48aa371de77cb6cc5c07e
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Feb 4 22:29:45 2008 -0800

    capabilities: introduce per-process capability bounding set
    
    The capability bounding set is a set beyond which capabilities cannot grow.
     Currently cap_bset is per-system.  It can be manipulated through sysctl,
    but only init can add capabilities.  Root can remove capabilities.  By
    default it includes all caps except CAP_SETPCAP.
    
    This patch makes the bounding set per-process when file capabilities are
    enabled.  It is inherited at fork from parent.  Noone can add elements,
    CAP_SETPCAP is required to remove them.
    
    One example use of this is to start a safer container.  For instance, until
    device namespaces or per-container device whitelists are introduced, it is
    best to take CAP_MKNOD away from a container.
    
    The bounding set will not affect pP and pE immediately.  It will only
    affect pP' and pE' after subsequent exec()s.  It also does not affect pI,
    and exec() does not constrain pI'.  So to really start a shell with no way
    of regain CAP_MKNOD, you would do
    
            prctl(PR_CAPBSET_DROP, CAP_MKNOD);
            cap_t cap = cap_get_proc();
            cap_value_t caparray[1];
            caparray[0] = CAP_MKNOD;
            cap_set_flag(cap, CAP_INHERITABLE, 1, caparray, CAP_DROP);
            cap_set_proc(cap);
            cap_free(cap);
    
    The following test program will get and set the bounding
    set (but not pI).  For instance
    
            ./bset get
                    (lists capabilities in bset)
            ./bset drop cap_net_raw
                    (starts shell with new bset)
                    (use capset, setuid binary, or binary with
                    file capabilities to try to increase caps)
    
    ************************************************************
    cap_bound.c
    ************************************************************
     #include <sys/prctl.h>
     #include <linux/capability.h>
     #include <sys/types.h>
     #include <unistd.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
    
     #ifndef PR_CAPBSET_READ
     #define PR_CAPBSET_READ 23
     #endif
    
     #ifndef PR_CAPBSET_DROP
     #define PR_CAPBSET_DROP 24
     #endif
    
    int usage(char *me)
    {
            printf("Usage: %s get\n", me);
            printf("       %s drop <capability>\n", me);
            return 1;
    }
    
     #define numcaps 32
    char *captable[numcaps] = {
            "cap_chown",
            "cap_dac_override",
            "cap_dac_read_search",
            "cap_fowner",
            "cap_fsetid",
            "cap_kill",
            "cap_setgid",
            "cap_setuid",
            "cap_setpcap",
            "cap_linux_immutable",
            "cap_net_bind_service",
            "cap_net_broadcast",
            "cap_net_admin",
            "cap_net_raw",
            "cap_ipc_lock",
            "cap_ipc_owner",
            "cap_sys_module",
            "cap_sys_rawio",
            "cap_sys_chroot",
            "cap_sys_ptrace",
            "cap_sys_pacct",
            "cap_sys_admin",
            "cap_sys_boot",
            "cap_sys_nice",
            "cap_sys_resource",
            "cap_sys_time",
            "cap_sys_tty_config",
            "cap_mknod",
            "cap_lease",
            "cap_audit_write",
            "cap_audit_control",
            "cap_setfcap"
    };
    
    int getbcap(void)
    {
            int comma=0;
            unsigned long i;
            int ret;
    
            printf("i know of %d capabilities\n", numcaps);
            printf("capability bounding set:");
            for (i=0; i<numcaps; i++) {
                    ret = prctl(PR_CAPBSET_READ, i);
                    if (ret < 0)
                            perror("prctl");
                    else if (ret==1)
                            printf("%s%s", (comma++) ? ", " : " ", captable[i]);
            }
            printf("\n");
            return 0;
    }
    
    int capdrop(char *str)
    {
            unsigned long i;
    
            int found=0;
            for (i=0; i<numcaps; i++) {
                    if (strcmp(captable[i], str) == 0) {
                            found=1;
                            break;
                    }
            }
            if (!found)
                    return 1;
            if (prctl(PR_CAPBSET_DROP, i)) {
                    perror("prctl");
                    return 1;
            }
            return 0;
    }
    
    int main(int argc, char *argv[])
    {
            if (argc<2)
                    return usage(argv[0]);
            if (strcmp(argv[1], "get")==0)
                    return getbcap();
            if (strcmp(argv[1], "drop")!=0 || argc<3)
                    return usage(argv[0]);
            if (capdrop(argv[2])) {
                    printf("unknown capability\n");
                    return 1;
            }
            return execl("/bin/bash", "/bin/bash", NULL);
    }
    ************************************************************
    
    [serue@us.ibm.com: fix typo]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew G. Morgan <morgan@kernel.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Casey Schaufler <casey@schaufler-ca.com>a
    Signed-off-by: "Serge E. Hallyn" <serue@us.ibm.com>
    Tested-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f42663eaf655..1f74e1d7415f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -121,6 +121,18 @@ extern struct group_info init_groups;
 #else
 #define INIT_IDS
 #endif
+
+#ifdef CONFIG_SECURITY_FILE_CAPABILITIES
+/*
+ * Because of the reduced scope of CAP_SETPCAP when filesystem
+ * capabilities are in effect, it is safe to allow CAP_SETPCAP to
+ * be available in the default configuration.
+ */
+# define CAP_INIT_BSET  CAP_FULL_SET
+#else
+# define CAP_INIT_BSET  CAP_INIT_EFF_SET
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -156,6 +168,7 @@ extern struct group_info init_groups;
 	.cap_effective	= CAP_INIT_EFF_SET,				\
 	.cap_inheritable = CAP_INIT_INH_SET,				\
 	.cap_permitted	= CAP_FULL_SET,					\
+	.cap_bset 	= CAP_INIT_BSET,				\
 	.keep_capabilities = 0,						\
 	.user		= INIT_USER,					\
 	.comm		= "swapper",					\

commit 4746ec5b01ed07205a91e4f7ed9de9d70f371407
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Jan 8 10:06:53 2008 -0500

    [AUDIT] add session id to audit messages
    
    In order to correlate audit records to an individual login add a session
    id.  This is incremented every time a user logs in and is included in
    almost all messages which currently output the auid.  The field is
    labeled ses=  or oses=
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index ea3e9efd7396..f42663eaf655 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -116,7 +116,8 @@ extern struct group_info init_groups;
 
 #ifdef CONFIG_AUDITSYSCALL
 #define INIT_IDS \
-	.loginuid = -1,
+	.loginuid = -1, \
+	.sessionid = -1,
 #else
 #define INIT_IDS
 #endif

commit bfef93a5d1fb5654fe2025276c55e202d10b5255
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Jan 10 04:53:18 2008 -0500

    [PATCH] get rid of loginuid races
    
    Keeping loginuid in audit_context is racy and results in messier
    code.  Taken to task_struct, out of the way of ->audit_context
    changes.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e6b3f7080679..ea3e9efd7396 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -114,6 +114,12 @@ extern struct group_info init_groups;
 	.pid = &init_struct_pid,				\
 }
 
+#ifdef CONFIG_AUDITSYSCALL
+#define INIT_IDS \
+	.loginuid = -1,
+#else
+#define INIT_IDS
+#endif
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -173,6 +179,7 @@ extern struct group_info init_groups;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
+	INIT_IDS							\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 }

commit fd0928df98b9578be8a786ac0cb78a47a5e17a20
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:52:45 2008 +0100

    ioprio: move io priority from task_struct to io_context
    
    This is where it belongs and then it doesn't take up space for a
    process that doesn't do IO.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 796019b22b6f..e6b3f7080679 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -137,7 +137,6 @@ extern struct group_info init_groups;
 		.time_slice	= HZ, 					\
 		.nr_cpus_allowed = NR_CPUS,				\
 	},								\
-	.ioprio		= 0,						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
 	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\

commit 6f505b16425a51270058e4a93441fe64de3dd435
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:30 2008 +0100

    sched: rt group scheduling
    
    Extend group scheduling to also cover the realtime classes. It uses the time
    limiting introduced by the previous patch to allow multiple realtime groups.
    
    The hard time limit is required to keep behaviour deterministic.
    
    The algorithms used make the realtime scheduler O(tg), linear scaling wrt the
    number of task groups. This is the worst case behaviour I can't seem to get out
    of, the avg. case of the algorithms can be improved, I focused on correctness
    and worst case.
    
    [ akpm@linux-foundation.org: move side-effects out of BUG_ON(). ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index ee65d87bedb7..796019b22b6f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -130,12 +130,13 @@ extern struct group_info init_groups;
 	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
-	.nr_cpus_allowed = NR_CPUS,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.rt		= {						\
 		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
-		.time_slice	= HZ, },				\
+		.time_slice	= HZ, 					\
+		.nr_cpus_allowed = NR_CPUS,				\
+	},								\
 	.ioprio		= 0,						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\

commit fa717060f1ab7eb6570f2fb49136f838fc9195a9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:27 2008 +0100

    sched: sched_rt_entity
    
    Move the task_struct members specific to rt scheduling together.
    A future optimization could be to put sched_entity and sched_rt_entity
    into a union.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 572c65bcc80f..ee65d87bedb7 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -133,9 +133,10 @@ extern struct group_info init_groups;
 	.nr_cpus_allowed = NR_CPUS,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
-	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
+	.rt		= {						\
+		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
+		.time_slice	= HZ, },				\
 	.ioprio		= 0,						\
-	.time_slice	= HZ,						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
 	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\

commit 73fe6aae84400e2b475e2a1dc4e8592cd3ed6e69
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Fri Jan 25 21:08:07 2008 +0100

    sched: add RT-balance cpu-weight
    
    Some RT tasks (particularly kthreads) are bound to one specific CPU.
    It is fairly common for two or more bound tasks to get queued up at the
    same time.  Consider, for instance, softirq_timer and softirq_sched.  A
    timer goes off in an ISR which schedules softirq_thread to run at RT50.
    Then the timer handler determines that it's time to smp-rebalance the
    system so it schedules softirq_sched to run.  So we are in a situation
    where we have two RT50 tasks queued, and the system will go into
    rt-overload condition to request other CPUs for help.
    
    This causes two problems in the current code:
    
    1) If a high-priority bound task and a low-priority unbounded task queue
       up behind the running task, we will fail to ever relocate the unbounded
       task because we terminate the search on the first unmovable task.
    
    2) We spend precious futile cycles in the fast-path trying to pull
       overloaded tasks over.  It is therefore optimial to strive to avoid the
       overhead all together if we can cheaply detect the condition before
       overload even occurs.
    
    This patch tries to achieve this optimization by utilizing the hamming
    weight of the task->cpus_allowed mask.  A weight of 1 indicates that
    the task cannot be migrated.  We will then utilize this information to
    skip non-migratable tasks and to eliminate uncessary rebalance attempts.
    
    We introduce a per-rq variable to count the number of migratable tasks
    that are currently running.  We only go into overload if we have more
    than one rt task, AND at least one of them is migratable.
    
    In addition, we introduce a per-task variable to cache the cpus_allowed
    weight, since the hamming calculation is probably relatively expensive.
    We only update the cached value when the mask is updated which should be
    relatively infrequent, especially compared to scheduling frequency
    in the fast path.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index cae35b6b9aec..572c65bcc80f 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -130,6 +130,7 @@ extern struct group_info init_groups;
 	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
+	.nr_cpus_allowed = NR_CPUS,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\

commit 9a2e70572e94e21e7ec4186702d045415422bda0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:39 2007 -0700

    Isolate the explicit usage of signal->pgrp
    
    The pgrp field is not used widely around the kernel so it is now marked as
    deprecated with appropriate comment.
    
    The initialization of INIT_SIGNALS is trimmed because
    a) they are set to 0 automatically;
    b) gcc cannot properly initialize two anonymous (the second one
       is the one with the session) unions. In this particular case
       to make it compile we'd have to add some field initialized
       right before the .pgrp.
    
    This is the same patch as the 1ec320afdc9552c92191d5f89fcd1ebe588334ca one
    (from Cedric), but for the pgrp field.
    
    Some progress report:
    
    We have to deprecate the pid, tgid, session and pgrp fields on struct
    task_struct and struct signal_struct.  The session and pgrp are already
    deprecated.  The tgid value is close to being such - the worst known usage
    in in fs/locks.c and audit code.  The pid field deprecation is mainly
    blocked by numerous printk-s around the kernel that print the tsk->pid to
    log.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5ac5945696a0..cae35b6b9aec 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -67,9 +67,6 @@
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
-	.pgrp		= 0,						\
-	.tty_old_pgrp   = NULL,						\
-	{ .__session      = 0},						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 19b9b9b54e5f115907efd56be2c3799775a46561
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:17 2007 -0700

    pid namespaces: remove the struct pid unneeded fields
    
    Since we've switched from using pid->nr to pid->upids->nr some
    fields on struct pid are no longer needed
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 243e287304d0..5ac5945696a0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -94,9 +94,6 @@ extern struct group_info init_groups;
 
 #define INIT_STRUCT_PID {						\
 	.count 		= ATOMIC_INIT(1),				\
-	.nr		= 0, 						\
-	/* Don't put this struct pid in pid_hash */			\
-	.pid_chain	= { .next = NULL, .pprev = NULL },		\
 	.tasks		= {						\
 		{ .first = &init_task.pids[PIDTYPE_PID].node },		\
 		{ .first = &init_task.pids[PIDTYPE_PGID].node },	\

commit 4c3f2ead5a3dff9069a45560ba4d007c8ae2e2ee
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu Oct 18 23:40:03 2007 -0700

    pid namespaces: introduce struct upid
    
    Since task will be visible from different pid namespaces each of them have to
    be addressed by multiple pids.  struct upid is to store the information about
    which id refers to which namespace.
    
    The constuciton looks like this.  Each struct pid carried the reference
    counter and the list of tasks attached to this pid.  At its end it has a
    variable length array of struct upid-s.  Each struct upid has a numerical id
    (pid itself), pointer to the namespace, this ID is valid in and is hashed into
    a pid_hash for searching the pids.
    
    The nr and pid_chain fields are kept in struct pid for a while to make kernel
    still work (no patch initialize the upids yet), but it will be removed at the
    end of this series when we switch to upids completely.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index d4b2f1c76e12..243e287304d0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -103,6 +103,12 @@ extern struct group_info init_groups;
 		{ .first = &init_task.pids[PIDTYPE_SID].node },		\
 	},								\
 	.rcu		= RCU_HEAD_INIT,				\
+	.level		= 0,						\
+	.numbers	= { {						\
+		.nr		= 0,					\
+		.ns		= &init_pid_ns,				\
+		.pid_chain	= { .next = NULL, .pprev = NULL },	\
+	}, }								\
 }
 
 #define INIT_PID_LINK(type) 					\

commit 1efd24fa05976ea20582c18dd4b80d7311b9b94a
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Tue Oct 16 23:30:10 2007 -0700

    Remove unused member from nsproxy
    
    The nslock spinlock is not used in the kernel at all.  Remove it.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3a619f57a2b2..d4b2f1c76e12 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -76,7 +76,6 @@ extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
 	.pid_ns		= &init_pid_ns,					\
 	.count		= ATOMIC_INIT(1),				\
-	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
 	INIT_NET_NS(net_ns)                                             \

commit 3e26c149c358529b1605f8959341d34bc4b880a3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:50 2007 -0700

    mm: dirty balancing for tasks
    
    Based on ideas of Andrew:
      http://marc.info/?l=linux-kernel&m=102912915020543&w=2
    
    Scale the bdi dirty limit inversly with the tasks dirty rate.
    This makes heavy writers have a lower dirty limit than the occasional writer.
    
    Andrea proposed something similar:
      http://lwn.net/Articles/152277/
    
    The main disadvantage to his patch is that he uses an unrelated quantity to
    measure time, which leaves him with a workload dependant tunable. Other than
    that the two approaches appear quite similar.
    
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 513bc3e489f0..3a619f57a2b2 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -171,6 +171,7 @@ extern struct group_info init_groups;
 		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
+	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 }

commit 4fabcd7118162e36eea5c53e8895ecc13762bef3
Author: Daniel Lezcano <dlezcano@fr.ibm.com>
Date:   Thu Sep 13 09:16:29 2007 +0200

    [NETNS]: Fix allnoconfig compilation error.
    
    When CONFIG_NET=no, init_net is unresolved because net_namespace.c
    is not compiled and the include pull init_net definition.
    
    This problem was very similar with the ipc namespace where the kernel
    can be compiled with SYSV ipc out.
    
    This patch fix that defining a macro which simply remove init_net
    initialization from nsproxy namespace aggregator.
    
    Compiled and booted on qemu-i386 with CONFIG_NET=no and CONFIG_NET=yes.
    
    Signed-off-by: Daniel Lezcano <dlezcano@fr.ibm.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e2c1ffcff62c..513bc3e489f0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -79,7 +79,7 @@ extern struct nsproxy init_nsproxy;
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
-	.net_ns		= &init_net,					\
+	INIT_NET_NS(net_ns)                                             \
 	INIT_IPC_NS(ipc_ns)						\
 	.user_ns	= &init_user_ns,				\
 }

commit 772698f6362680b65211f7efc68121f1e4c28aa5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Sep 12 11:55:17 2007 +0200

    [NET]: Add a network namespace parameter to tasks
    
    This is the network namespace from which all which all sockets
    and anything else under user control ultimately get their network
    namespace parameters.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index f8abfa349ef9..e2c1ffcff62c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -9,6 +9,7 @@
 #include <linux/ipc.h>
 #include <linux/pid_namespace.h>
 #include <linux/user_namespace.h>
+#include <net/net_namespace.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -78,6 +79,7 @@ extern struct nsproxy init_nsproxy;
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
+	.net_ns		= &init_net,					\
 	INIT_IPC_NS(ipc_ns)						\
 	.user_ns	= &init_user_ns,				\
 }

commit b8fceee17a310f189188599a8fa5e9beaff57eb0
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu Sep 20 12:40:16 2007 -0700

    signalfd simplification
    
    This simplifies signalfd code, by avoiding it to remain attached to the
    sighand during its lifetime.
    
    In this way, the signalfd remain attached to the sighand only during
    poll(2) (and select and epoll) and read(2).  This also allows to remove
    all the custom "tsk == current" checks in kernel/signal.c, since
    dequeue_signal() will only be called by "current".
    
    I think this is also what Ben was suggesting time ago.
    
    The external effect of this, is that a thread can extract only its own
    private signals and the group ones.  I think this is an acceptable
    behaviour, in that those are the signals the thread would be able to
    fetch w/out signalfd.
    
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index cab741c2d603..f8abfa349ef9 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -86,7 +86,7 @@ extern struct nsproxy init_nsproxy;
 	.count		= ATOMIC_INIT(1), 				\
 	.action		= { { { .sa_handler = NULL, } }, },		\
 	.siglock	= __SPIN_LOCK_UNLOCKED(sighand.siglock),	\
-	.signalfd_list	= LIST_HEAD_INIT(sighand.signalfd_list),	\
+	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(sighand.signalfd_wqh),	\
 }
 
 extern struct group_info init_groups;

commit acce292c82d4d82d35553b928df2b0597c3a9c78
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Sun Jul 15 23:40:59 2007 -0700

    user namespace: add the framework
    
    Basically, it will allow a process to unshare its user_struct table,
    resetting at the same time its own user_struct and all the associated
    accounting.
    
    A new root user (uid == 0) is added to the user namespace upon creation.
    Such root users have full privileges and it seems that theses privileges
    should be controlled through some means (process capabilities ?)
    
    The unshare is not included in this patch.
    
    Changes since [try #4]:
            - Updated get_user_ns and put_user_ns to accept NULL, and
              get_user_ns to return the namespace.
    
    Changes since [try #3]:
            - moved struct user_namespace to files user_namespace.{c,h}
    
    Changes since [try #2]:
            - removed struct user_namespace* argument from find_user()
    
    Changes since [try #1]:
            - removed struct user_namespace* argument from find_user()
            - added a root_user per user namespace
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 276ccaa2670c..cab741c2d603 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -8,6 +8,7 @@
 #include <linux/lockdep.h>
 #include <linux/ipc.h>
 #include <linux/pid_namespace.h>
+#include <linux/user_namespace.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -78,6 +79,7 @@ extern struct nsproxy init_nsproxy;
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
 	INIT_IPC_NS(ipc_ns)						\
+	.user_ns	= &init_user_ns,				\
 }
 
 #define INIT_SIGHAND(sighand) {						\

commit fba2afaaec790dc5ab4ae8827972f342211bbb86
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu May 10 22:23:13 2007 -0700

    signal/timer/event: signalfd core
    
    This patch series implements the new signalfd() system call.
    
    I took part of the original Linus code (and you know how badly it can be
    broken :), and I added even more breakage ;) Signals are fetched from the same
    signal queue used by the process, so signalfd will compete with standard
    kernel delivery in dequeue_signal().  If you want to reliably fetch signals on
    the signalfd file, you need to block them with sigprocmask(SIG_BLOCK).  This
    seems to be working fine on my Dual Opteron machine.  I made a quick test
    program for it:
    
    http://www.xmailserver.org/signafd-test.c
    
    The signalfd() system call implements signal delivery into a file descriptor
    receiver.  The signalfd file descriptor if created with the following API:
    
    int signalfd(int ufd, const sigset_t *mask, size_t masksize);
    
    The "ufd" parameter allows to change an existing signalfd sigmask, w/out going
    to close/create cycle (Linus idea).  Use "ufd" == -1 if you want a brand new
    signalfd file.
    
    The "mask" allows to specify the signal mask of signals that we are interested
    in.  The "masksize" parameter is the size of "mask".
    
    The signalfd fd supports the poll(2) and read(2) system calls.  The poll(2)
    will return POLLIN when signals are available to be dequeued.  As a direct
    consequence of supporting the Linux poll subsystem, the signalfd fd can use
    used together with epoll(2) too.
    
    The read(2) system call will return a "struct signalfd_siginfo" structure in
    the userspace supplied buffer.  The return value is the number of bytes copied
    in the supplied buffer, or -1 in case of error.  The read(2) call can also
    return 0, in case the sighand structure to which the signalfd was attached,
    has been orphaned.  The O_NONBLOCK flag is also supported, and read(2) will
    return -EAGAIN in case no signal is available.
    
    If the size of the buffer passed to read(2) is lower than sizeof(struct
    signalfd_siginfo), -EINVAL is returned.  A read from the signalfd can also
    return -ERESTARTSYS in case a signal hits the process.  The format of the
    struct signalfd_siginfo is, and the valid fields depends of the (->code &
    __SI_MASK) value, in the same way a struct siginfo would:
    
    struct signalfd_siginfo {
            __u32 signo;    /* si_signo */
            __s32 err;      /* si_errno */
            __s32 code;     /* si_code */
            __u32 pid;      /* si_pid */
            __u32 uid;      /* si_uid */
            __s32 fd;       /* si_fd */
            __u32 tid;      /* si_fd */
            __u32 band;     /* si_band */
            __u32 overrun;  /* si_overrun */
            __u32 trapno;   /* si_trapno */
            __s32 status;   /* si_status */
            __s32 svint;    /* si_int */
            __u64 svptr;    /* si_ptr */
            __u64 utime;    /* si_utime */
            __u64 stime;    /* si_stime */
            __u64 addr;     /* si_addr */
    };
    
    [akpm@linux-foundation.org: fix signalfd_copyinfo() on i386]
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 66e2f0a70814..276ccaa2670c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -84,6 +84,7 @@ extern struct nsproxy init_nsproxy;
 	.count		= ATOMIC_INIT(1), 				\
 	.action		= { { { .sa_handler = NULL, } }, },		\
 	.siglock	= __SPIN_LOCK_UNLOCKED(sighand.siglock),	\
+	.signalfd_list	= LIST_HEAD_INIT(sighand.signalfd_list),	\
 }
 
 extern struct group_info init_groups;

commit 325aa33da784e5997c756a151bc46e9cc9b79db2
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:23:10 2007 -0700

    Don't init pgrp and __session in INIT_SIGNALS
    
    Remove initialization of pgrp and __session in INIT_SIGNALS, as these are
    later set by the call to __set_special_pids() in init/main.c by the patch:
    
            explicitly-set-pgid-and-sid-of-init-process.patch
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a68835fc8c04..66e2f0a70814 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -65,9 +65,9 @@
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
-	.pgrp		= 1,						\
+	.pgrp		= 0,						\
 	.tty_old_pgrp   = NULL,						\
-	{ .__session      = 1},						\
+	{ .__session      = 0},						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 820e45db2380eb1545fa2bc5d34b8b2f2933faeb
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:23:00 2007 -0700

    statically initialize struct pid for swapper
    
    Statically initialize a struct pid for the swapper process (pid_t == 0) and
    attach it to init_task.  This is needed so task_pid(), task_pgrp() and
    task_session() interfaces work on the swapper process also.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: <containers@lists.osdl.org>
    Acked-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 45170b2fa253..a68835fc8c04 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -88,6 +88,28 @@ extern struct nsproxy init_nsproxy;
 
 extern struct group_info init_groups;
 
+#define INIT_STRUCT_PID {						\
+	.count 		= ATOMIC_INIT(1),				\
+	.nr		= 0, 						\
+	/* Don't put this struct pid in pid_hash */			\
+	.pid_chain	= { .next = NULL, .pprev = NULL },		\
+	.tasks		= {						\
+		{ .first = &init_task.pids[PIDTYPE_PID].node },		\
+		{ .first = &init_task.pids[PIDTYPE_PGID].node },	\
+		{ .first = &init_task.pids[PIDTYPE_SID].node },		\
+	},								\
+	.rcu		= RCU_HEAD_INIT,				\
+}
+
+#define INIT_PID_LINK(type) 					\
+{								\
+	.node = {						\
+		.next = NULL,					\
+		.pprev = &init_struct_pid.tasks[type].first,	\
+	},							\
+	.pid = &init_struct_pid,				\
+}
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -139,6 +161,11 @@ extern struct group_info init_groups;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= __SPIN_LOCK_UNLOCKED(tsk.pi_lock),		\
+	.pids = {							\
+		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
+		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
+		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
+	},								\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 }

commit f7e4217b007d1f73e7e3cf10ba4fea4a608c603f
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Wed May 9 02:35:17 2007 -0700

    rename thread_info to stack
    
    This finally renames the thread_info field in task structure to stack, so that
    the assumptions about this field are gone and archs have more freedom about
    placing the thread_info structure.
    
    Nonbroken archs which have a proper thread pointer can do the access to both
    current thread and task structure via a single pointer.
    
    It'll allow for a few more cleanups of the fork code, from which e.g.  ia64
    could benefit.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    [akpm@linux-foundation.org: build fix]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: Greg Ungerer <gerg@uclinux.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Chris Zankel <chris@zankel.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 795102309bf1..45170b2fa253 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -95,7 +95,7 @@ extern struct group_info init_groups;
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
-	.thread_info	= &init_thread_info,				\
+	.stack		= &init_thread_info,				\
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= 0,						\
 	.lock_depth	= -1,						\

commit b32e41bb97971161ad34ea69364c4f9ec3909151
Author: Milind Arun Choudhary <milindchoudhary@gmail.com>
Date:   Tue May 8 00:30:08 2007 -0700

    SPIN_LOCK_UNLOCKED cleanup in init_task.h
    
    SPIN_LOCK_UNLOCKED cleanup,use __SPIN_LOCK_UNLOCKED instead
    
    Signed-off-by: Milind Arun Choudhary <milindchoudhary@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a2d95ff50e9b..795102309bf1 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -138,7 +138,7 @@ extern struct group_info init_groups;
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
-	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
+	.pi_lock	= __SPIN_LOCK_UNLOCKED(tsk.pi_lock),		\
 	INIT_TRACE_IRQFLAGS						\
 	INIT_LOCKDEP							\
 }

commit ab521dc0f8e117fd808d3e425216864d60390500
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:53:00 2007 -0800

    [PATCH] tty: update the tty layer to work with struct pid
    
    Of kernel subsystems that work with pids the tty layer is probably the largest
    consumer.  But it has the nice virtue that the assiation with a session only
    lasts until the session leader exits.  Which means that no reference counting
    is required.  So using struct pid winds up being a simple optimization to
    avoid hash table lookups.
    
    In the long term the use of pid_nr also ensures that when we have multiple pid
    spaces mixed everything will work correctly.
    
    Signed-off-by: Eric W. Biederman <eric@maxwell.lnxi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 6383d2d83bb0..a2d95ff50e9b 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -66,7 +66,7 @@
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
 	.pgrp		= 1,						\
-	.tty_old_pgrp   = 0,						\
+	.tty_old_pgrp   = NULL,						\
 	{ .__session      = 1},						\
 }
 

commit 5f8442edfb214908e9c6ca1142bf882c9bc364e5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Dec 13 00:34:04 2006 -0800

    [PATCH] Revert "[PATCH] identifier to nsproxy"
    
    This reverts commit 373beb35cd6b625e0ba4ad98baace12310a26aa8.
    
    No one is using this identifier yet.  The purpose of this identifier is to
    export nsproxy to user space which is wrong.  nsproxy is an internal
    implementation optimization, which should keep our fork times from getting
    slower as we increase the number of global namespaces you don't have to
    share.
    
    Adding a global identifier like this is inappropriate because it makes
    namespaces inherently non-recursive, greatly limiting what we can do with
    them in the future.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b5315150199e..6383d2d83bb0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -75,7 +75,6 @@ extern struct nsproxy init_nsproxy;
 	.pid_ns		= &init_pid_ns,					\
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
-	.id		= 0,						\
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
 	INIT_IPC_NS(ipc_ns)						\

commit 4fd45812cbe875a620c86a096a5d46c742694b7e
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:17 2006 -0800

    [PATCH] fdtable: Remove the free_files field
    
    An fdtable can either be embedded inside a files_struct or standalone (after
    being expanded).  When an fdtable is being discarded after all RCU references
    to it have expired, we must either free it directly, in the standalone case,
    or free the files_struct it is contained within, in the embedded case.
    
    Currently the free_files field controls this behavior, but we can get rid of
    it entirely, as all the necessary information is already recorded.  We can
    distinguish embedded and standalone fdtables using max_fds, and if it is
    embedded we can divine the relevant files_struct using container_of().
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 58c18daab65d..b5315150199e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -16,7 +16,6 @@
 	.close_on_exec	= (fd_set *)&init_files.close_on_exec_init, \
 	.open_fds	= (fd_set *)&init_files.open_fds_init, 	\
 	.rcu		= RCU_HEAD_INIT, 		\
-	.free_files	= NULL,		 		\
 	.next		= NULL,		 		\
 }
 

commit bbea9f69668a3d0cf9feba15a724cd02896f8675
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:12 2006 -0800

    [PATCH] fdtable: Make fdarray and fdsets equal in size
    
    Currently, each fdtable supports three dynamically-sized arrays of data: the
    fdarray and two fdsets.  The code allows the number of fds supported by the
    fdarray (fdtable->max_fds) to differ from the number of fds supported by each
    of the fdsets (fdtable->max_fdset).
    
    In practice, it is wasteful for these two sizes to differ: whenever we hit a
    limit on the smaller-capacity structure, we will reallocate the entire fdtable
    and all the dynamic arrays within it, so any delta in the memory used by the
    larger-capacity structure will never be touched at all.
    
    Rather than hogging this excess, we shouldn't even allocate it in the first
    place, and keep the capacities of the fdarray and the fdsets equal.  This
    patch removes fdtable->max_fdset.  As an added bonus, most of the supporting
    code becomes simpler.
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 7272ff9ee77c..58c18daab65d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -12,7 +12,6 @@
 #define INIT_FDTABLE \
 {							\
 	.max_fds	= NR_OPEN_DEFAULT, 		\
-	.max_fdset	= EMBEDDED_FD_SET_SIZE,		\
 	.fd		= &init_files.fd_array[0], 	\
 	.close_on_exec	= (fd_set *)&init_files.close_on_exec_init, \
 	.open_fds	= (fd_set *)&init_files.open_fds_init, 	\

commit 9a575a92db3312a40cdf0b0406d88de88ad9741e
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:59 2006 -0800

    [PATCH] to nsproxy
    
    Add the pid namespace framework to the nsproxy object.  The copy of the pid
    namespace only increases the refcount on the global pid namespace,
    init_pid_ns, and unshare is not implemented.
    
    There is no configuration option to activate or deactivate this feature
    because this not relevant for the moment.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 90c5f9a07730..7272ff9ee77c 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -7,6 +7,7 @@
 #include <linux/utsname.h>
 #include <linux/lockdep.h>
 #include <linux/ipc.h>
+#include <linux/pid_namespace.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -73,6 +74,7 @@
 
 extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
+	.pid_ns		= &init_pid_ns,					\
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.id		= 0,						\

commit 373beb35cd6b625e0ba4ad98baace12310a26aa8
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:57 2006 -0800

    [PATCH] identifier to nsproxy
    
    Add an identifier to nsproxy.  The default init_ns_proxy has identifier 0 and
    allocated nsproxies are given -1.
    
    This identifier will be used by a new syscall sys_bind_ns.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 5c4989172f7e..90c5f9a07730 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -75,6 +75,7 @@ extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
+	.id		= 0,						\
 	.uts_ns		= &init_uts_ns,					\
 	.mnt_ns		= NULL,						\
 	INIT_IPC_NS(ipc_ns)						\

commit 6b3286ed1169d74fea401367d6d4d6c6ec758a81
Author: Kirill Korotaev <dev@sw.ru>
Date:   Fri Dec 8 02:37:56 2006 -0800

    [PATCH] rename struct namespace to struct mnt_namespace
    
    Rename 'struct namespace' to 'struct mnt_namespace' to avoid confusion with
    other namespaces being developped for the containers : pid, uts, ipc, etc.
    'namespace' variables and attributes are also renamed to 'mnt_ns'
    
    Signed-off-by: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 848a68af3d42..5c4989172f7e 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -76,7 +76,7 @@ extern struct nsproxy init_nsproxy;
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.uts_ns		= &init_uts_ns,					\
-	.namespace	= NULL,						\
+	.mnt_ns		= NULL,						\
 	INIT_IPC_NS(ipc_ns)						\
 }
 

commit 1ec320afdc9552c92191d5f89fcd1ebe588334ca
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:55 2006 -0800

    [PATCH] add process_session() helper routine: deprecate old field
    
    Add an anonymous union and ((deprecated)) to catch direct usage of the
    session field.
    
    [akpm@osdl.org: fix various missed conversions]
    [jdike@addtoit.com: fix UML bug]
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 733790d4f7db..848a68af3d42 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -57,17 +57,18 @@
 	.cpu_vm_mask	= CPU_MASK_ALL,				\
 }
 
-#define INIT_SIGNALS(sig) {	\
-	.count		= ATOMIC_INIT(1), 		\
+#define INIT_SIGNALS(sig) {						\
+	.count		= ATOMIC_INIT(1), 				\
 	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
-	.shared_pending	= { 				\
+	.shared_pending	= { 						\
 		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\
-		.signal =  {{0}}}, \
+		.signal =  {{0}}},					\
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
 	.pgrp		= 1,						\
-	.session	= 1,						\
+	.tty_old_pgrp   = 0,						\
+	{ .__session      = 1},						\
 }
 
 extern struct nsproxy init_nsproxy;

commit 6cfd76a26d9fe2ba54b9d496a48c1d9285e5c5ed
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 6 20:37:22 2006 -0800

    [PATCH] lockdep: name some old style locks
    
    Name some of the remaning 'old_style_spin_init' locks
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 33c5daacc743..733790d4f7db 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -73,7 +73,7 @@
 extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
 	.count		= ATOMIC_INIT(1),				\
-	.nslock		= SPIN_LOCK_UNLOCKED,				\
+	.nslock		= __SPIN_LOCK_UNLOCKED(nsproxy.nslock),		\
 	.uts_ns		= &init_uts_ns,					\
 	.namespace	= NULL,						\
 	INIT_IPC_NS(ipc_ns)						\

commit 73ea41302bab5e02c9e86ab15c509494a550f1db
Author: Kirill Korotaev <dev@openvz.org>
Date:   Mon Oct 2 02:18:20 2006 -0700

    [PATCH] IPC namespace - utils
    
    This patch adds basic IPC namespace functionality to
    IPC utils:
    - init_ipc_ns
    - copy/clone/unshare/free IPC ns
    - /proc preparations
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index ceecf69dfa39..33c5daacc743 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -6,6 +6,7 @@
 #include <linux/irqflags.h>
 #include <linux/utsname.h>
 #include <linux/lockdep.h>
+#include <linux/ipc.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -74,8 +75,8 @@ extern struct nsproxy init_nsproxy;
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= SPIN_LOCK_UNLOCKED,				\
 	.uts_ns		= &init_uts_ns,					\
-	.ipc_ns		= &init_ipc_ns,					\
 	.namespace	= NULL,						\
+	INIT_IPC_NS(ipc_ns)						\
 }
 
 #define INIT_SIGHAND(sighand) {						\

commit 25b21cb2f6d69b0475b134e0a3e8e269137270fa
Author: Kirill Korotaev <dev@openvz.org>
Date:   Mon Oct 2 02:18:19 2006 -0700

    [PATCH] IPC namespace core
    
    This patch set allows to unshare IPCs and have a private set of IPC objects
    (sem, shm, msg) inside namespace.  Basically, it is another building block of
    containers functionality.
    
    This patch implements core IPC namespace changes:
    - ipc_namespace structure
    - new config option CONFIG_IPC_NS
    - adds CLONE_NEWIPC flag
    - unshare support
    
    [clg@fr.ibm.com: small fix for unshare of ipc namespace]
    [akpm@osdl.org: build fix]
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e08531ec32f0..ceecf69dfa39 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -74,6 +74,7 @@ extern struct nsproxy init_nsproxy;
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= SPIN_LOCK_UNLOCKED,				\
 	.uts_ns		= &init_uts_ns,					\
+	.ipc_ns		= &init_ipc_ns,					\
 	.namespace	= NULL,						\
 }
 

commit 4865ecf1315b450ab3317a745a6678c04d311e40
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:14 2006 -0700

    [PATCH] namespaces: utsname: implement utsname namespaces
    
    This patch defines the uts namespace and some manipulators.
    Adds the uts namespace to task_struct, and initializes a
    system-wide init namespace.
    
    It leaves a #define for system_utsname so sysctl will compile.
    This define will be removed in a separate patch.
    
    [akpm@osdl.org: build fix, cleanup]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 4865348ca8bd..e08531ec32f0 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -4,6 +4,7 @@
 #include <linux/file.h>
 #include <linux/rcupdate.h>
 #include <linux/irqflags.h>
+#include <linux/utsname.h>
 #include <linux/lockdep.h>
 
 #define INIT_FDTABLE \
@@ -72,6 +73,7 @@ extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= SPIN_LOCK_UNLOCKED,				\
+	.uts_ns		= &init_uts_ns,					\
 	.namespace	= NULL,						\
 }
 

commit 1651e14e28a2d9f446018ef522882e0709a2ce4f
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:08 2006 -0700

    [PATCH] namespaces: incorporate fs namespace into nsproxy
    
    This moves the mount namespace into the nsproxy.  The mount namespace count
    now refers to the number of nsproxies point to it, rather than the number of
    tasks.  As a result, the unshare_namespace() function in kernel/fork.c no
    longer checks whether it is being shared.
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 8f8bb422a5c7..4865348ca8bd 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -72,6 +72,7 @@ extern struct nsproxy init_nsproxy;
 #define INIT_NSPROXY(nsproxy) {						\
 	.count		= ATOMIC_INIT(1),				\
 	.nslock		= SPIN_LOCK_UNLOCKED,				\
+	.namespace	= NULL,						\
 }
 
 #define INIT_SIGHAND(sighand) {						\

commit ab516013ad9ca47f1d3a936fa81303bfbf734d52
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:06 2006 -0700

    [PATCH] namespaces: add nsproxy
    
    This patch adds a nsproxy structure to the task struct.  Later patches will
    move the fs namespace pointer into this structure, and introduce a new utsname
    namespace into the nsproxy.
    
    The vserver and openvz functionality, then, would be implemented in large part
    by virtualizing/isolating more and more resources into namespaces, each
    contained in the nsproxy.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 60aac2cea0cf..8f8bb422a5c7 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -68,6 +68,12 @@
 	.session	= 1,						\
 }
 
+extern struct nsproxy init_nsproxy;
+#define INIT_NSPROXY(nsproxy) {						\
+	.count		= ATOMIC_INIT(1),				\
+	.nslock		= SPIN_LOCK_UNLOCKED,				\
+}
+
 #define INIT_SIGHAND(sighand) {						\
 	.count		= ATOMIC_INIT(1), 				\
 	.action		= { { { .sa_handler = NULL, } }, },		\
@@ -117,6 +123,7 @@ extern struct group_info init_groups;
 	.files		= &init_files,					\
 	.signal		= &init_signals,				\
 	.sighand	= &init_sighand,				\
+	.nsproxy	= &init_nsproxy,				\
 	.pending	= {						\
 		.list = LIST_HEAD_INIT(tsk.pending.list),		\
 		.signal = {{0}}},					\

commit fbb9ce9530fd9b66096d5187fa6a115d16d9746c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:50 2006 -0700

    [PATCH] lockdep: core
    
    Do 'make oldconfig' and accept all the defaults for new config options -
    reboot into the kernel and if everything goes well it should boot up fine and
    you should have /proc/lockdep and /proc/lockdep_stats files.
    
    Typically if the lock validator finds some problem it will print out
    voluminous debug output that begins with "BUG: ..." and which syslog output
    can be used by kernel developers to figure out the precise locking scenario.
    
    What does the lock validator do?  It "observes" and maps all locking rules as
    they occur dynamically (as triggered by the kernel's natural use of spinlocks,
    rwlocks, mutexes and rwsems).  Whenever the lock validator subsystem detects a
    new locking scenario, it validates this new rule against the existing set of
    rules.  If this new rule is consistent with the existing set of rules then the
    new rule is added transparently and the kernel continues as normal.  If the
    new rule could create a deadlock scenario then this condition is printed out.
    
    When determining validity of locking, all possible "deadlock scenarios" are
    considered: assuming arbitrary number of CPUs, arbitrary irq context and task
    context constellations, running arbitrary combinations of all the existing
    locking scenarios.  In a typical system this means millions of separate
    scenarios.  This is why we call it a "locking correctness" validator - for all
    rules that are observed the lock validator proves it with mathematical
    certainty that a deadlock could not occur (assuming that the lock validator
    implementation itself is correct and its internal data structures are not
    corrupted by some other kernel subsystem).  [see more details and conditionals
    of this statement in include/linux/lockdep.h and
    Documentation/lockdep-design.txt]
    
    Furthermore, this "all possible scenarios" property of the validator also
    enables the finding of complex, highly unlikely multi-CPU multi-context races
    via single single-context rules, increasing the likelyhood of finding bugs
    drastically.  In practical terms: the lock validator already found a bug in
    the upstream kernel that could only occur on systems with 3 or more CPUs, and
    which needed 3 very unlikely code sequences to occur at once on the 3 CPUs.
    That bug was found and reported on a single-CPU system (!).  So in essence a
    race will be found "piecemail-wise", triggering all the necessary components
    for the race, without having to reproduce the race scenario itself!  In its
    short existence the lock validator found and reported many bugs before they
    actually caused a real deadlock.
    
    To further increase the efficiency of the validator, the mapping is not per
    "lock instance", but per "lock-class".  For example, all struct inode objects
    in the kernel have inode->inotify_mutex.  If there are 10,000 inodes cached,
    then there are 10,000 lock objects.  But ->inotify_mutex is a single "lock
    type", and all locking activities that occur against ->inotify_mutex are
    "unified" into this single lock-class.  The advantage of the lock-class
    approach is that all historical ->inotify_mutex uses are mapped into a single
    (and as narrow as possible) set of locking rules - regardless of how many
    different tasks or inode structures it took to build this set of rules.  The
    set of rules persist during the lifetime of the kernel.
    
    To see the rough magnitude of checking that the lock validator does, here's a
    portion of /proc/lockdep_stats, fresh after bootup:
    
     lock-classes:                            694 [max: 2048]
     direct dependencies:                  1598 [max: 8192]
     indirect dependencies:               17896
     all direct dependencies:             16206
     dependency chains:                    1910 [max: 8192]
     in-hardirq chains:                      17
     in-softirq chains:                     105
     in-process chains:                    1065
     stack-trace entries:                 38761 [max: 131072]
     combined max dependencies:         2033928
     hardirq-safe locks:                     24
     hardirq-unsafe locks:                  176
     softirq-safe locks:                     53
     softirq-unsafe locks:                  137
     irq-safe locks:                         59
     irq-unsafe locks:                      176
    
    The lock validator has observed 1598 actual single-thread locking patterns,
    and has validated all possible 2033928 distinct locking scenarios.
    
    More details about the design of the lock validator can be found in
    Documentation/lockdep-design.txt, which can also found at:
    
       http://redhat.com/~mingo/lockdep-patches/lockdep-design.txt
    
    [bunk@stusta.de: cleanups]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 444a3ae0de2a..60aac2cea0cf 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -4,6 +4,7 @@
 #include <linux/file.h>
 #include <linux/rcupdate.h>
 #include <linux/irqflags.h>
+#include <linux/lockdep.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -126,6 +127,7 @@ extern struct group_info init_groups;
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
 	INIT_TRACE_IRQFLAGS						\
+	INIT_LOCKDEP							\
 }
 
 

commit de30a2b355ea85350ca2f58f3b9bf4e5bc007986
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:42 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, core
    
    Accurate hard-IRQ-flags and softirq-flags state tracing.
    
    This allows us to attach extra functionality to IRQ flags on/off
    events (such as trace-on/off).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1b7bb37624bb..444a3ae0de2a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -3,6 +3,7 @@
 
 #include <linux/file.h>
 #include <linux/rcupdate.h>
+#include <linux/irqflags.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -124,6 +125,7 @@ extern struct group_info init_groups;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
+	INIT_TRACE_IRQFLAGS						\
 }
 
 

commit e4d919188554a77c798a267e098059bc9aa39726
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:34 2006 -0700

    [PATCH] lockdep: locking init debugging improvement
    
    Locking init improvement:
    
     - introduce and use __SPIN_LOCK_UNLOCKED for array initializations,
       to pass in the name string of locks, used by debugging
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 678c1a90380d..1b7bb37624bb 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -21,7 +21,7 @@
 	.count		= ATOMIC_INIT(1), 		\
 	.fdt		= &init_files.fdtab, 		\
 	.fdtab		= INIT_FDTABLE,			\
-	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+	.file_lock	= __SPIN_LOCK_UNLOCKED(init_task.file_lock), \
 	.next_fd	= 0, 				\
 	.close_on_exec_init = { { 0, } }, 		\
 	.open_fds_init	= { { 0, } }, 			\
@@ -36,7 +36,7 @@
 	.user_id	= 0,				\
 	.next		= NULL,				\
 	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER(name.wait), \
-	.ctx_lock	= SPIN_LOCK_UNLOCKED,		\
+	.ctx_lock	= __SPIN_LOCK_UNLOCKED(name.ctx_lock), \
 	.reqs_active	= 0U,				\
 	.max_reqs	= ~0U,				\
 }
@@ -48,7 +48,7 @@
 	.mm_users	= ATOMIC_INIT(2), 			\
 	.mm_count	= ATOMIC_INIT(1), 			\
 	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
-	.page_table_lock =  SPIN_LOCK_UNLOCKED, 		\
+	.page_table_lock =  __SPIN_LOCK_UNLOCKED(name.page_table_lock),	\
 	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
 	.cpu_vm_mask	= CPU_MASK_ALL,				\
 }
@@ -69,7 +69,7 @@
 #define INIT_SIGHAND(sighand) {						\
 	.count		= ATOMIC_INIT(1), 				\
 	.action		= { { { .sa_handler = NULL, } }, },		\
-	.siglock	= SPIN_LOCK_UNLOCKED, 				\
+	.siglock	= __SPIN_LOCK_UNLOCKED(sighand.siglock),	\
 }
 
 extern struct group_info init_groups;
@@ -119,7 +119,7 @@ extern struct group_info init_groups;
 		.list = LIST_HEAD_INIT(tsk.pending.list),		\
 		.signal = {{0}}},					\
 	.blocked	= {{0}},					\
-	.alloc_lock	= SPIN_LOCK_UNLOCKED,				\
+	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\

commit 9a11b49a805665e13a56aa067afaf81d43ec1514
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:33 2006 -0700

    [PATCH] lockdep: better lock debugging
    
    Generic lock debugging:
    
     - generalized lock debugging framework. For example, a bug in one lock
       subsystem turns off debugging in all lock subsystems.
    
     - got rid of the caller address passing (__IP__/__IP_DECL__/etc.) from
       the mutex/rtmutex debugging code: it caused way too much prototype
       hackery, and lockdep will give the same information anyway.
    
     - ability to do silent tests
    
     - check lock freeing in vfree too.
    
     - more finegrained debugging options, to allow distributions to
       turn off more expensive debugging features.
    
    There's no separate 'held mutexes' list anymore - but there's a 'held locks'
    stack within lockdep, which unifies deadlock detection across all lock
    classes.  (this is independent of the lockdep validation stuff - lockdep first
    checks whether we are holding a lock already)
    
    Here are the current debugging options:
    
    CONFIG_DEBUG_MUTEXES=y
    CONFIG_DEBUG_LOCK_ALLOC=y
    
    which do:
    
     config DEBUG_MUTEXES
              bool "Mutex debugging, basic checks"
    
     config DEBUG_LOCK_ALLOC
             bool "Detect incorrect freeing of live mutexes"
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 3a256957fb56..678c1a90380d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -124,7 +124,6 @@ extern struct group_info init_groups;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
-	INIT_RT_MUTEXES(tsk)						\
 }
 
 

commit 23f78d4a03c53cbd75d87a795378ea540aa08c86
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:53 2006 -0700

    [PATCH] pi-futex: rt mutex core
    
    Core functions for the rt-mutex subsystem.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 678c1a90380d..3a256957fb56 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -124,6 +124,7 @@ extern struct group_info init_groups;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
 	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
+	INIT_RT_MUTEXES(tsk)						\
 }
 
 

commit b29739f902ee76a05493fb7d2303490fc75364f4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:51 2006 -0700

    [PATCH] pi-futex: scheduler support for pi
    
    Add framework to boost/unboost the priority of RT tasks.
    
    This consists of:
    
     - caching the 'normal' priority in ->normal_prio
     - providing a functions to set/get the priority of the task
     - make sched_setscheduler() aware of boosting
    
    The effective_prio() cleanups also fix a priority-calculation bug pointed out
    by Andrey Gelman, in set_user_nice().
    
    has_rt_policy() fix: Peter Williams <pwil3058@bigpond.net.au>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Andrey Gelman <agelman@012.net.il>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index e127ef7e8da8..678c1a90380d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -87,6 +87,7 @@ extern struct group_info init_groups;
 	.lock_depth	= -1,						\
 	.prio		= MAX_PRIO-20,					\
 	.static_prio	= MAX_PRIO-20,					\
+	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
@@ -122,6 +123,7 @@ extern struct group_info init_groups;
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\
+	.pi_lock	= SPIN_LOCK_UNLOCKED,				\
 }
 
 

commit 48e6484d49020dba3578ad117b461e8a391e8f0f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jun 26 00:25:48 2006 -0700

    [PATCH] proc: Rewrite the proc dentry flush on exit optimization
    
    To keep the dcache from filling up with dead /proc entries we flush them on
    process exit.  However over the years that code has gotten hairy with a
    dentry_pointer and a lock in task_struct and misdocumented as a correctness
    feature.
    
    I have rewritten this code to look and see if we have a corresponding entry in
    the dcache and if so flush it on process exit.  This removes the extra fields
    in the task_struct and allows me to trivially handle the case of a
    /proc/<tgid>/task/<pid> entry as well as the current /proc/<pid> entries.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 41ecbb847f32..e127ef7e8da8 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -119,7 +119,6 @@ extern struct group_info init_groups;
 		.signal = {{0}}},					\
 	.blocked	= {{0}},					\
 	.alloc_lock	= SPIN_LOCK_UNLOCKED,				\
-	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.fs_excl	= ATOMIC_INIT(0),				\

commit c7c6464117a02b0d54feb4ebeca4db70fa493678
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:09 2006 -0800

    [PATCH] pidhash: don't use zero pids
    
    daemonize() calls set_special_pids(1,1), while init and kernel threads spawned
    from init/main.c:init() run with 0,0 special pids.  This patch changes
    INIT_SIGNALS() so that that they run with ->pgrp == ->session == 1 also.  This
    patch relies on fact that swapper's pid == 1.
    
    Now we have no hashed zero pids in pid_hash[].
    
    User-space visibible change is that now /sbin/init runs with (1,1) special
    pids and becomes a session leader.
    
    Quoting Eric W. Biederman:
    >
    > daemonize consuming pids (1,1) then consumes pgrp 1.  So that when
    > /sbin/init calls setsid() it thinks /sbin/init is a process group
    > leader and setsid() fails.  So /sbin/init wants pgrp 1 session 1
    > but doesn't get it.  I am pretty certain daemonize did not exist so
    > /sbin/init got pgrp 1 session 1 in 2.4.
    >
    > That is the bug that is being fixed.
    >
    > This patch takes things one step farther and essentially calls
    > setsid() for pid == 1 before init is execed.  That is new behavior
    > but it cleans up the kernel as we now do not need to support the
    > case of a process without a process group or a session.
    >
    > The only process that could have possibly cared was /sbin/init
    > and it already calls setsid() because it doesn't want that.
    >
    > If this was going to break anything noticeable the change in behavior
    > from 2.4 to 2.6 would have already done that.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 92146f3b7423..41ecbb847f32 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -62,6 +62,8 @@
 	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
+	.pgrp		= 1,						\
+	.session	= 1,						\
 }
 
 #define INIT_SIGHAND(sighand) {						\

commit 0c9e63fd38a2fb2181668a0cdd622a3c23cfd567
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Mar 23 03:00:12 2006 -0800

    [PATCH] Shrinks sizeof(files_struct) and better layout
    
    1) Reduce the size of (struct fdtable) to exactly 64 bytes on 32bits
       platforms, lowering kmalloc() allocated space by 50%.
    
    2) Reduce the size of (files_struct), using a special 32 bits (or
       64bits) embedded_fd_set, instead of a 1024 bits fd_set for the
       close_on_exec_init and open_fds_init fields.  This save some ram (248
       bytes per task) as most tasks dont open more than 32 files.  D-Cache
       footprint for such tasks is also reduced to the minimum.
    
    3) Reduce size of allocated fdset.  Currently two full pages are
       allocated, that is 32768 bits on x86 for example, and way too much.  The
       minimum is now L1_CACHE_BYTES.
    
    UP and SMP should benefit from this patch, because most tasks will touch
    only one cache line when open()/close() stdin/stdout/stderr (0/1/2),
    (next_fd, close_on_exec_init, open_fds_init, fd_array[0 ..  2] being in the
    same cache line)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index dcfd2ecccb5d..92146f3b7423 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -7,11 +7,10 @@
 #define INIT_FDTABLE \
 {							\
 	.max_fds	= NR_OPEN_DEFAULT, 		\
-	.max_fdset	= __FD_SETSIZE, 		\
-	.next_fd	= 0, 				\
+	.max_fdset	= EMBEDDED_FD_SET_SIZE,		\
 	.fd		= &init_files.fd_array[0], 	\
-	.close_on_exec	= &init_files.close_on_exec_init, \
-	.open_fds	= &init_files.open_fds_init, 	\
+	.close_on_exec	= (fd_set *)&init_files.close_on_exec_init, \
+	.open_fds	= (fd_set *)&init_files.open_fds_init, 	\
 	.rcu		= RCU_HEAD_INIT, 		\
 	.free_files	= NULL,		 		\
 	.next		= NULL,		 		\
@@ -20,9 +19,10 @@
 #define INIT_FILES \
 { 							\
 	.count		= ATOMIC_INIT(1), 		\
-	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
 	.fdt		= &init_files.fdtab, 		\
 	.fdtab		= INIT_FDTABLE,			\
+	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+	.next_fd	= 0, 				\
 	.close_on_exec_init = { { 0, } }, 		\
 	.open_fds_init	= { { 0, } }, 			\
 	.fd_array	= { NULL, } 			\

commit 20dcae32439384b6863c626bb3b2a09bed65b33e
Author: Zach Brown <zach.brown@oracle.com>
Date:   Sun Nov 13 16:07:33 2005 -0800

    [PATCH] aio: remove kioctx from mm_struct
    
    Sync iocbs have a life cycle that don't need a kioctx.  Their retrying, if
    any, is done in the context of their owner who has allocated them on the
    stack.
    
    The sole user of a sync iocb's ctx reference was aio_complete() checking for
    an elevated iocb ref count that could never happen.  No path which grabs an
    iocb ref has access to sync iocbs.
    
    If we were to implement sync iocb cancelation it would be done by the owner of
    the iocb using its on-stack reference.
    
    Removing this chunk from aio_complete allows us to remove the entire kioctx
    instance from mm_struct, reducing its size by a third.  On a i386 testing box
    the slab size went from 768 to 504 bytes and from 5 to 8 per page.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>
    Acked-by: Benjamin LaHaise <bcrl@kvack.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 68ab5f2ab9cd..dcfd2ecccb5d 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -51,7 +51,6 @@
 	.page_table_lock =  SPIN_LOCK_UNLOCKED, 		\
 	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
 	.cpu_vm_mask	= CPU_MASK_ALL,				\
-	.default_kioctx = INIT_KIOCTX(name.default_kioctx, name),	\
 }
 
 #define INIT_SIGNALS(sig) {	\

commit ab2af1f5005069321c5d130f09cce577b03f43ef
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:13 2005 -0700

    [PATCH] files: files struct with RCU
    
    Patch to eliminate struct files_struct.file_lock spinlock on the reader side
    and use rcu refcounting rcuref_xxx api for the f_count refcounter.  The
    updates to the fdtable are done by allocating a new fdtable structure and
    setting files->fdt to point to the new structure.  The fdtable structure is
    protected by RCU thereby allowing lock-free lookup.  For fd arrays/sets that
    are vmalloced, we use keventd to free them since RCU callbacks can't sleep.  A
    global list of fdtable to be freed is not scalable, so we use a per-cpu list.
    If keventd is already handling the current cpu's work, we use a timer to defer
    queueing of that work.
    
    Since the last publication, this patch has been re-written to avoid using
    explicit memory barriers and use rcu_assign_pointer(), rcu_dereference()
    premitives instead.  This required that the fd information is kept in a
    separate structure (fdtable) and updated atomically.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 94aefa54a1b5..68ab5f2ab9cd 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -2,6 +2,7 @@
 #define _LINUX__INIT_TASK_H
 
 #include <linux/file.h>
+#include <linux/rcupdate.h>
 
 #define INIT_FDTABLE \
 {							\
@@ -11,12 +12,16 @@
 	.fd		= &init_files.fd_array[0], 	\
 	.close_on_exec	= &init_files.close_on_exec_init, \
 	.open_fds	= &init_files.open_fds_init, 	\
+	.rcu		= RCU_HEAD_INIT, 		\
+	.free_files	= NULL,		 		\
+	.next		= NULL,		 		\
 }
 
 #define INIT_FILES \
 { 							\
 	.count		= ATOMIC_INIT(1), 		\
 	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+	.fdt		= &init_files.fdtab, 		\
 	.fdtab		= INIT_FDTABLE,			\
 	.close_on_exec_init = { { 0, } }, 		\
 	.open_fds_init	= { { 0, } }, 			\

commit badf16621c1f9d1ac753be056fce11b43d6e0be5
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:10 2005 -0700

    [PATCH] files: break up files struct
    
    In order for the RCU to work, the file table array, sets and their sizes must
    be updated atomically.  Instead of ensuring this through too many memory
    barriers, we put the arrays and their sizes in a separate structure.  This
    patch takes the first step of putting the file table elements in a separate
    structure fdtable that is embedded withing files_struct.  It also changes all
    the users to refer to the file table using files_fdtable() macro.  Subsequent
    applciation of RCU becomes easier after this.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index c727c195a91a..94aefa54a1b5 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -3,16 +3,21 @@
 
 #include <linux/file.h>
 
-#define INIT_FILES \
-{ 							\
-	.count		= ATOMIC_INIT(1), 		\
-	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+#define INIT_FDTABLE \
+{							\
 	.max_fds	= NR_OPEN_DEFAULT, 		\
 	.max_fdset	= __FD_SETSIZE, 		\
 	.next_fd	= 0, 				\
 	.fd		= &init_files.fd_array[0], 	\
 	.close_on_exec	= &init_files.close_on_exec_init, \
 	.open_fds	= &init_files.open_fds_init, 	\
+}
+
+#define INIT_FILES \
+{ 							\
+	.count		= ATOMIC_INIT(1), 		\
+	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+	.fdtab		= INIT_FDTABLE,			\
 	.close_on_exec_init = { { 0, } }, 		\
 	.open_fds_init	= { { 0, } }, 			\
 	.fd_array	= { NULL, } 			\

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 03206a425d7a..c727c195a91a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -81,6 +81,7 @@ extern struct group_info init_groups;
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
+	.ioprio		= 0,						\
 	.time_slice	= HZ,						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
@@ -110,6 +111,7 @@ extern struct group_info init_groups;
 	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
+	.fs_excl	= ATOMIC_INIT(0),				\
 }
 
 

commit 4866cde064afbb6c2a488c265e696879de616daa
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:23 2005 -0700

    [PATCH] sched: cleanup context switch locking
    
    Instead of requiring architecture code to interact with the scheduler's
    locking implementation, provide a couple of defines that can be used by the
    architecture to request runqueue unlocked context switches, and ask for
    interrupts to be enabled over the context switch.
    
    Also replaces the "switch_lock" used by these architectures with an oncpu
    flag (note, not a potentially slow bitflag).  This eliminates one bus
    locked memory operation when context switching, and simplifies the
    task_running function.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index a6a8c1a38d5e..03206a425d7a 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -108,7 +108,6 @@ extern struct group_info init_groups;
 	.blocked	= {{0}},					\
 	.alloc_lock	= SPIN_LOCK_UNLOCKED,				\
 	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
-	.switch_lock	= SPIN_LOCK_UNLOCKED,				\
 	.journal_info	= NULL,						\
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 }

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
new file mode 100644
index 000000000000..a6a8c1a38d5e
--- /dev/null
+++ b/include/linux/init_task.h
@@ -0,0 +1,125 @@
+#ifndef _LINUX__INIT_TASK_H
+#define _LINUX__INIT_TASK_H
+
+#include <linux/file.h>
+
+#define INIT_FILES \
+{ 							\
+	.count		= ATOMIC_INIT(1), 		\
+	.file_lock	= SPIN_LOCK_UNLOCKED, 		\
+	.max_fds	= NR_OPEN_DEFAULT, 		\
+	.max_fdset	= __FD_SETSIZE, 		\
+	.next_fd	= 0, 				\
+	.fd		= &init_files.fd_array[0], 	\
+	.close_on_exec	= &init_files.close_on_exec_init, \
+	.open_fds	= &init_files.open_fds_init, 	\
+	.close_on_exec_init = { { 0, } }, 		\
+	.open_fds_init	= { { 0, } }, 			\
+	.fd_array	= { NULL, } 			\
+}
+
+#define INIT_KIOCTX(name, which_mm) \
+{							\
+	.users		= ATOMIC_INIT(1),		\
+	.dead		= 0,				\
+	.mm		= &which_mm,			\
+	.user_id	= 0,				\
+	.next		= NULL,				\
+	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER(name.wait), \
+	.ctx_lock	= SPIN_LOCK_UNLOCKED,		\
+	.reqs_active	= 0U,				\
+	.max_reqs	= ~0U,				\
+}
+
+#define INIT_MM(name) \
+{			 					\
+	.mm_rb		= RB_ROOT,				\
+	.pgd		= swapper_pg_dir, 			\
+	.mm_users	= ATOMIC_INIT(2), 			\
+	.mm_count	= ATOMIC_INIT(1), 			\
+	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
+	.page_table_lock =  SPIN_LOCK_UNLOCKED, 		\
+	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
+	.cpu_vm_mask	= CPU_MASK_ALL,				\
+	.default_kioctx = INIT_KIOCTX(name.default_kioctx, name),	\
+}
+
+#define INIT_SIGNALS(sig) {	\
+	.count		= ATOMIC_INIT(1), 		\
+	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
+	.shared_pending	= { 				\
+		.list = LIST_HEAD_INIT(sig.shared_pending.list),	\
+		.signal =  {{0}}}, \
+	.posix_timers	 = LIST_HEAD_INIT(sig.posix_timers),		\
+	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
+	.rlim		= INIT_RLIMITS,					\
+}
+
+#define INIT_SIGHAND(sighand) {						\
+	.count		= ATOMIC_INIT(1), 				\
+	.action		= { { { .sa_handler = NULL, } }, },		\
+	.siglock	= SPIN_LOCK_UNLOCKED, 				\
+}
+
+extern struct group_info init_groups;
+
+/*
+ *  INIT_TASK is used to set up the first task table, touch at
+ * your own risk!. Base=0, limit=0x1fffff (=2MB)
+ */
+#define INIT_TASK(tsk)	\
+{									\
+	.state		= 0,						\
+	.thread_info	= &init_thread_info,				\
+	.usage		= ATOMIC_INIT(2),				\
+	.flags		= 0,						\
+	.lock_depth	= -1,						\
+	.prio		= MAX_PRIO-20,					\
+	.static_prio	= MAX_PRIO-20,					\
+	.policy		= SCHED_NORMAL,					\
+	.cpus_allowed	= CPU_MASK_ALL,					\
+	.mm		= NULL,						\
+	.active_mm	= &init_mm,					\
+	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
+	.time_slice	= HZ,						\
+	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
+	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
+	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\
+	.real_parent	= &tsk,						\
+	.parent		= &tsk,						\
+	.children	= LIST_HEAD_INIT(tsk.children),			\
+	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
+	.group_leader	= &tsk,						\
+	.group_info	= &init_groups,					\
+	.cap_effective	= CAP_INIT_EFF_SET,				\
+	.cap_inheritable = CAP_INIT_INH_SET,				\
+	.cap_permitted	= CAP_FULL_SET,					\
+	.keep_capabilities = 0,						\
+	.user		= INIT_USER,					\
+	.comm		= "swapper",					\
+	.thread		= INIT_THREAD,					\
+	.fs		= &init_fs,					\
+	.files		= &init_files,					\
+	.signal		= &init_signals,				\
+	.sighand	= &init_sighand,				\
+	.pending	= {						\
+		.list = LIST_HEAD_INIT(tsk.pending.list),		\
+		.signal = {{0}}},					\
+	.blocked	= {{0}},					\
+	.alloc_lock	= SPIN_LOCK_UNLOCKED,				\
+	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
+	.switch_lock	= SPIN_LOCK_UNLOCKED,				\
+	.journal_info	= NULL,						\
+	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
+}
+
+
+#define INIT_CPU_TIMERS(cpu_timers)					\
+{									\
+	LIST_HEAD_INIT(cpu_timers[0]),					\
+	LIST_HEAD_INIT(cpu_timers[1]),					\
+	LIST_HEAD_INIT(cpu_timers[2]),					\
+}
+
+
+#endif
