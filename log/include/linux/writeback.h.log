commit 0b166a57e6222666292a481b742af92b50c3ba50
Merge: b25c6644bfd3 6b8ed62008a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 16:19:28 2020 -0700

    Merge tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4
    
    Pull ext4 updates from Ted Ts'o:
     "A lot of bug fixes and cleanups for ext4, including:
    
       - Fix performance problems found in dioread_nolock now that it is the
         default, caused by transaction leaks.
    
       - Clean up fiemap handling in ext4
    
       - Clean up and refactor multiple block allocator (mballoc) code
    
       - Fix a problem with mballoc with a smaller file systems running out
         of blocks because they couldn't properly use blocks that had been
         reserved by inode preallocation.
    
       - Fixed a race in ext4_sync_parent() versus rename()
    
       - Simplify the error handling in the extent manipulation code
    
       - Make sure all metadata I/O errors are felected to
         ext4_ext_dirty()'s and ext4_make_inode_dirty()'s callers.
    
       - Avoid passing an error pointer to brelse in ext4_xattr_set()
    
       - Fix race which could result to freeing an inode on the dirty last
         in data=journal mode.
    
       - Fix refcount handling if ext4_iget() fails
    
       - Fix a crash in generic/019 caused by a corrupted extent node"
    
    * tag 'ext4_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4: (58 commits)
      ext4: avoid unnecessary transaction starts during writeback
      ext4: don't block for O_DIRECT if IOCB_NOWAIT is set
      ext4: remove the access_ok() check in ext4_ioctl_get_es_cache
      fs: remove the access_ok() check in ioctl_fiemap
      fs: handle FIEMAP_FLAG_SYNC in fiemap_prep
      fs: move fiemap range validation into the file systems instances
      iomap: fix the iomap_fiemap prototype
      fs: move the fiemap definitions out of fs.h
      fs: mark __generic_block_fiemap static
      ext4: remove the call to fiemap_check_flags in ext4_fiemap
      ext4: split _ext4_fiemap
      ext4: fix fiemap size checks for bitmap files
      ext4: fix EXT4_MAX_LOGICAL_BLOCK macro
      add comment for ext4_dir_entry_2 file_type member
      jbd2: avoid leaking transaction credits when unreserving handle
      ext4: drop ext4_journal_free_reserved()
      ext4: mballoc: use lock for checking free blocks while retrying
      ext4: mballoc: refactor ext4_mb_good_group()
      ext4: mballoc: introduce pcpu seqcnt for freeing PA to improve ENOSPC handling
      ext4: mballoc: refactor ext4_mb_discard_preallocations()
      ...

commit 4301efa4c7cca11556dd89899eee066d49b47bf7
Author: Jan Kara <jack@suse.cz>
Date:   Tue Apr 21 10:54:44 2020 +0200

    writeback: Export inode_io_list_del()
    
    Ext4 needs to remove inode from writeback lists after it is out of
    visibility of its journalling machinery (which can still dirty the
    inode). Export inode_io_list_del() for it.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Link: https://lore.kernel.org/r/20200421085445.5731-3-jack@suse.cz
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a19d845dd7eb..902aa317621b 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -197,6 +197,7 @@ void wakeup_flusher_threads(enum wb_reason reason);
 void wakeup_flusher_threads_bdi(struct backing_dev_info *bdi,
 				enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
+void inode_io_list_del(struct inode *inode);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a19d845dd7eb..f8a7e1a850fb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -362,24 +362,18 @@ extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;
 
-extern int dirty_background_ratio_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-extern int dirty_background_bytes_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-extern int dirty_ratio_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-extern int dirty_bytes_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
+int dirty_background_ratio_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
+int dirty_background_bytes_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
+int dirty_ratio_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
+int dirty_bytes_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
 int dirtytime_interval_handler(struct ctl_table *table, int write,
-			       void __user *buffer, size_t *lenp, loff_t *ppos);
-
-struct ctl_table;
-int dirty_writeback_centisecs_handler(struct ctl_table *, int,
-				      void __user *, size_t *, loff_t *);
+		void *buffer, size_t *lenp, loff_t *ppos);
+int dirty_writeback_centisecs_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
 
 void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
 unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh);

commit d62241c7a406f0680d702bd974f6f17e28ab8e5d
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 26 09:06:55 2019 -0700

    writeback, memcg: Implement cgroup_writeback_by_id()
    
    Implement cgroup_writeback_by_id() which initiates cgroup writeback
    from bdi and memcg IDs.  This will be used by memcg foreign inode
    flushing.
    
    v2: Use wb_get_lookup() instead of wb_get_create() to avoid creating
        spurious wbs.
    
    v3: Interpret 0 @nr as 1.25 * nr_dirty to implement best-effort
        flushing while avoding possible livelocks.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 8945aac31392..a19d845dd7eb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -217,6 +217,8 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 void wbc_detach_inode(struct writeback_control *wbc);
 void wbc_account_cgroup_owner(struct writeback_control *wbc, struct page *page,
 			      size_t bytes);
+int cgroup_writeback_by_id(u64 bdi_id, int memcg_id, unsigned long nr_pages,
+			   enum wb_reason reason, struct wb_completion *done);
 void cgroup_writeback_umount(void);
 
 /**

commit d3f77dfdc71835f8db71ca57d272b1fbec9dfc18
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 27 13:39:52 2019 -0700

    blkcg: implement REQ_CGROUP_PUNT
    
    When a shared kthread needs to issue a bio for a cgroup, doing so
    synchronously can lead to priority inversions as the kthread can be
    trapped waiting for that cgroup.  This patch implements
    REQ_CGROUP_PUNT flag which makes submit_bio() punt the actual issuing
    to a dedicated per-blkcg work item to avoid such priority inversions.
    
    This will be used to fix priority inversions in btrfs compression and
    should be generally useful as we grow filesystem support for
    comprehensive IO control.
    
    Cc: Chris Mason <clm@fb.com>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e056a22075cf..8945aac31392 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -78,6 +78,8 @@ struct writeback_control {
 	 */
 	unsigned no_cgroup_owner:1;
 
+	unsigned punt_to_cgroup:1;	/* cgrp punting, see __REQ_CGROUP_PUNT */
+
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct bdi_writeback *wb;	/* wb this writeback is issued under */
 	struct inode *inode;		/* inode being written out */
@@ -94,12 +96,17 @@ struct writeback_control {
 
 static inline int wbc_to_write_flags(struct writeback_control *wbc)
 {
+	int flags = 0;
+
+	if (wbc->punt_to_cgroup)
+		flags = REQ_CGROUP_PUNT;
+
 	if (wbc->sync_mode == WB_SYNC_ALL)
-		return REQ_SYNC;
+		flags |= REQ_SYNC;
 	else if (wbc->for_kupdate || wbc->for_background)
-		return REQ_BACKGROUND;
+		flags |= REQ_BACKGROUND;
 
-	return 0;
+	return flags;
 }
 
 static inline struct cgroup_subsys_state *

commit 653c45c6b90c9659facbef10546d1f3a8e37d0cf
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 27 13:39:51 2019 -0700

    blkcg, writeback: Implement wbc_blkcg_css()
    
    Add a helper to determine the target blkcg from wbc.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 33a50fa09fac..e056a22075cf 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -11,6 +11,7 @@
 #include <linux/flex_proportions.h>
 #include <linux/backing-dev-defs.h>
 #include <linux/blk_types.h>
+#include <linux/blk-cgroup.h>
 
 struct bio;
 
@@ -101,6 +102,16 @@ static inline int wbc_to_write_flags(struct writeback_control *wbc)
 	return 0;
 }
 
+static inline struct cgroup_subsys_state *
+wbc_blkcg_css(struct writeback_control *wbc)
+{
+#ifdef CONFIG_CGROUP_WRITEBACK
+	if (wbc->wb)
+		return wbc->wb->blkcg_css;
+#endif
+	return blkcg_root_css;
+}
+
 /*
  * A wb_domain represents a domain that wb's (bdi_writeback's) belong to
  * and are measured against each other in.  There always is one global

commit 27b36d8fa81fa8274fb72f4eb1484026f6b6daa8
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 27 13:39:50 2019 -0700

    blkcg, writeback: Add wbc->no_cgroup_owner
    
    When writeback IOs are bounced through async layers, the IOs should
    only be accounted against the wbc from the original bdi writeback to
    avoid confusing cgroup inode ownership arbitration.  Add
    wbc->no_cgroup_owner to allow disabling wbc cgroup owner accounting.
    This will be used make btrfs compression work well with cgroup IO
    control.
    
    v2: Renamed from no_wbc_acct to no_cgroup_owner and added comment as
        per Jan.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index dda5cf228172..33a50fa09fac 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -68,6 +68,15 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
+
+	/*
+	 * When writeback IOs are bounced through async layers, only the
+	 * initial synchronous phase should be accounted towards inode
+	 * cgroup ownership arbitration to avoid confusion.  Later stages
+	 * can set the following flag to disable the accounting.
+	 */
+	unsigned no_cgroup_owner:1;
+
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct bdi_writeback *wb;	/* wb this writeback is issued under */
 	struct inode *inode;		/* inode being written out */

commit 34e51a5e1a6e939ed7d99c38173821ab86d577f4
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 27 13:39:49 2019 -0700

    blkcg, writeback: Rename wbc_account_io() to wbc_account_cgroup_owner()
    
    wbc_account_io() does a very specific job - try to see which cgroup is
    actually dirtying an inode and transfer its ownership to the majority
    dirtier if needed.  The name is too generic and confusing.  Let's
    rename it to something more specific.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 738a0c24874f..dda5cf228172 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -188,8 +188,8 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 				 struct inode *inode)
 	__releases(&inode->i_lock);
 void wbc_detach_inode(struct writeback_control *wbc);
-void wbc_account_io(struct writeback_control *wbc, struct page *page,
-		    size_t bytes);
+void wbc_account_cgroup_owner(struct writeback_control *wbc, struct page *page,
+			      size_t bytes);
 void cgroup_writeback_umount(void);
 
 /**
@@ -291,8 +291,8 @@ static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
 }
 
-static inline void wbc_account_io(struct writeback_control *wbc,
-				  struct page *page, size_t bytes)
+static inline void wbc_account_cgroup_owner(struct writeback_control *wbc,
+					    struct page *page, size_t bytes)
 {
 }
 

commit fd42df305f804ddc0d5ac028e944784283b2f92d
Author: Dennis Zhou <dennis@kernel.org>
Date:   Wed Dec 5 12:10:34 2018 -0500

    blkcg: associate writeback bios with a blkg
    
    One of the goals of this series is to remove a separate reference to
    the css of the bio. This can and should be accessed via bio_blkcg(). In
    this patch, wbc_init_bio() now requires a bio to have a device
    associated with it.
    
    Signed-off-by: Dennis Zhou <dennis@kernel.org>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e348f6..738a0c24874f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -246,7 +246,8 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  *
  * @bio is a part of the writeback in progress controlled by @wbc.  Perform
  * writeback specific initialization.  This is used to apply the cgroup
- * writeback context.
+ * writeback context.  Must be called after the bio has been associated with
+ * a device.
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
@@ -257,7 +258,7 @@ static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 	 * regular writeback instead of writing things out itself.
 	 */
 	if (wbc->wb)
-		bio_associate_blkcg(bio, wbc->wb->blkcg_css);
+		bio_associate_blkg_from_css(bio, wbc->wb->blkcg_css);
 }
 
 #else	/* CONFIG_CGROUP_WRITEBACK */

commit b5f2954d30c77649bce9c27e7a0a94299d9cfdf8
Author: Dennis Zhou <dennis@kernel.org>
Date:   Thu Nov 1 17:24:10 2018 -0400

    blkcg: revert blkcg cleanups series
    
    This reverts a series committed earlier due to null pointer exception
    bug report in [1]. It seems there are edge case interactions that I did
    not consider and will need some time to understand what causes the
    adverse interactions.
    
    The original series can be found in [2] with a follow up series in [3].
    
    [1] https://www.spinics.net/lists/cgroups/msg20719.html
    [2] https://lore.kernel.org/lkml/20180911184137.35897-1-dennisszhou@gmail.com/
    [3] https://lore.kernel.org/lkml/20181020185612.51587-1-dennis@kernel.org/
    
    This reverts the following commits:
    d459d853c2ed, b2c3fa546705, 101246ec02b5, b3b9f24f5fcc, e2b0989954ae,
    f0fcb3ec89f3, c839e7a03f92, bdc2491708c4, 74b7c02a9bc1, 5bf9a1f3b4ef,
    a7b39b4e961c, 07b05bcc3213, 49f4c2dc2b50, 27e6fa996c53
    
    Signed-off-by: Dennis Zhou <dennis@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 738a0c24874f..fdfd04e348f6 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -246,8 +246,7 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  *
  * @bio is a part of the writeback in progress controlled by @wbc.  Perform
  * writeback specific initialization.  This is used to apply the cgroup
- * writeback context.  Must be called after the bio has been associated with
- * a device.
+ * writeback context.
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
@@ -258,7 +257,7 @@ static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 	 * regular writeback instead of writing things out itself.
 	 */
 	if (wbc->wb)
-		bio_associate_blkg_from_css(bio, wbc->wb->blkcg_css);
+		bio_associate_blkcg(bio, wbc->wb->blkcg_css);
 }
 
 #else	/* CONFIG_CGROUP_WRITEBACK */

commit bdc2491708c47601603918a9a20acddef6e1d814
Author: Dennis Zhou (Facebook) <dennisszhou@gmail.com>
Date:   Tue Sep 11 14:41:32 2018 -0400

    blkcg: associate writeback bios with a blkg
    
    One of the goals of this series is to remove a separate reference to
    the css of the bio. This can and should be accessed via bio_blkcg. In
    this patch, the wbc_init_bio call is changed such that it must be called
    after a queue has been associated with the bio.
    
    Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e348f6..738a0c24874f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -246,7 +246,8 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  *
  * @bio is a part of the writeback in progress controlled by @wbc.  Perform
  * writeback specific initialization.  This is used to apply the cgroup
- * writeback context.
+ * writeback context.  Must be called after the bio has been associated with
+ * a device.
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
@@ -257,7 +258,7 @@ static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 	 * regular writeback instead of writing things out itself.
 	 */
 	if (wbc->wb)
-		bio_associate_blkcg(bio, wbc->wb->blkcg_css);
+		bio_associate_blkg_from_css(bio, wbc->wb->blkcg_css);
 }
 
 #else	/* CONFIG_CGROUP_WRITEBACK */

commit bca237a52ca0035b0a0380003283d8bf590188d5
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Aug 28 15:03:41 2017 -0700

    block/laptop_mode: Convert timers to use timer_setup()
    
    In preparation for unconditionally passing the struct timer_list pointer to
    all timer callbacks, switch to using the new timer_setup() and from_timer()
    to pass the timer pointer explicitly.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Cc: Jeff Layton <jlayton@redhat.com>
    Cc: linux-block@vger.kernel.org
    Cc: linux-mm@kvack.org
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f42d85631d17..fdfd04e348f6 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -308,7 +308,7 @@ static inline void cgroup_writeback_umount(void)
 void laptop_io_completion(struct backing_dev_info *info);
 void laptop_sync_completion(void);
 void laptop_mode_sync(struct work_struct *work);
-void laptop_mode_timer_fn(unsigned long data);
+void laptop_mode_timer_fn(struct timer_list *t);
 #else
 static inline void laptop_sync_completion(void) { }
 #endif

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d5815794416c..e12d92808e98 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * include/linux/writeback.h
  */

commit 8264c3214f28b52b399d9e03bfa7feec275a0d71
Author: Rakesh Pandit <rakesh@tuxera.com>
Date:   Mon Oct 9 13:34:41 2017 +0300

    writeback: merge try_to_writeback_inodes_sb_nr() into caller
    
    Since commit 925a6efb8ff0c ("Btrfs: stop using
    try_to_writeback_inodes_sb_nr to flush delalloc") this function hasn't
    been used outside so stop exporting it.
    
    In addition we merge it into try_to_writeback_inodes_sb() which is the
    only caller.  Also change return type of try_to_writeback_inodes_sb to
    void as the only user ext4 doesn't care.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Rakesh Pandit <rakesh@tuxera.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index dd1d2c23f743..e15ec14085ad 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -163,9 +163,7 @@ struct bdi_writeback;
 void writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 							enum wb_reason reason);
-bool try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
-bool try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
-				   enum wb_reason reason);
+void try_to_writeback_inodes_sb(struct super_block *sb, enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
 void wakeup_flusher_threads(enum wb_reason reason);
 void wakeup_flusher_threads_bdi(struct backing_dev_info *bdi,

commit 85009b4f5f0399669a44f07cb9a5622c0e71d419
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Sep 30 02:09:06 2017 -0600

    writeback: eliminate work item allocation in bd_start_writeback()
    
    Handle start-all writeback like we do periodic or kupdate
    style writeback - by marking the bdi_writeback as needing a full
    flush, and simply waking the thread. This eliminates the need to
    allocate and queue a specific work item just for this purpose.
    
    After this change, we truly only ever have one of them running at
    any point in time. We mark the need to start all flushes, and the
    writeback thread will clear it once it has processed the request.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 9c0091678af4..dd1d2c23f743 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -41,28 +41,6 @@ enum writeback_sync_modes {
 	WB_SYNC_ALL,	/* Wait on every mapping */
 };
 
-/*
- * why some writeback work was initiated
- */
-enum wb_reason {
-	WB_REASON_BACKGROUND,
-	WB_REASON_VMSCAN,
-	WB_REASON_SYNC,
-	WB_REASON_PERIODIC,
-	WB_REASON_LAPTOP_TIMER,
-	WB_REASON_FREE_MORE_MEM,
-	WB_REASON_FS_FREE_SPACE,
-	/*
-	 * There is no bdi forker thread any more and works are done
-	 * by emergency worker, however, this is TPs userland visible
-	 * and we'll be exposing exactly the same information,
-	 * so it has a mismatch name.
-	 */
-	WB_REASON_FORKER_THREAD,
-
-	WB_REASON_MAX,
-};
-
 /*
  * A control structure which tells the writeback code what to do.  These are
  * always on the stack, and hence need no locking.  They are always initialised

commit 595043e5f9ef1d8263bd9fda215cade489227491
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Sep 28 11:26:59 2017 -0600

    writeback: provide a wakeup_flusher_threads_bdi()
    
    Similar to wakeup_flusher_threads(), except that we only wake
    up the flusher threads on the specified backing device.
    
    No functional changes in this patch.
    
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Tested-by: Chris Mason <clm@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 1f9c6db5e29a..9c0091678af4 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -190,6 +190,8 @@ bool try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 				   enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
 void wakeup_flusher_threads(enum wb_reason reason);
+void wakeup_flusher_threads_bdi(struct backing_dev_info *bdi,
+				enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */

commit 9ba4b2dfafaa711b41cc2102b0e9a529f3981218
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Sep 20 08:58:25 2017 -0600

    fs: kill 'nr_pages' argument from wakeup_flusher_threads()
    
    Everybody is passing in 0 now, let's get rid of the argument.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d5815794416c..1f9c6db5e29a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -189,7 +189,7 @@ bool try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 bool try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 				   enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
-void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
+void wakeup_flusher_threads(enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */

commit f759741d9d913eb57784a94b9bca78b376fc26a9
Author: Jan Kara <jack@suse.cz>
Date:   Thu Mar 23 01:37:00 2017 +0100

    block: Fix oops in locked_inode_to_wb_and_lock_list()
    
    When block device is closed, we call inode_detach_wb() in __blkdev_put()
    which sets inode->i_wb to NULL. That is contrary to expectations that
    inode->i_wb stays valid once set during the whole inode's lifetime and
    leads to oops in wb_get() in locked_inode_to_wb_and_lock_list() because
    inode_to_wb() returned NULL.
    
    The reason why we called inode_detach_wb() is not valid anymore though.
    BDI is guaranteed to stay along until we call bdi_put() from
    bdev_evict_inode() so we can postpone calling inode_detach_wb() to that
    moment.
    
    Also add a warning to catch if someone uses inode_detach_wb() in a
    dangerous way.
    
    Reported-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a3c0cbd7c888..d5815794416c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -237,6 +237,7 @@ static inline void inode_attach_wb(struct inode *inode, struct page *page)
 static inline void inode_detach_wb(struct inode *inode)
 {
 	if (inode->i_wb) {
+		WARN_ON_ONCE(!(inode->i_state & I_CLEAR));
 		wb_put(inode->i_wb);
 		inode->i_wb = NULL;
 	}

commit 726d061fbd3658e4bfeffa1b8e82da97de2ca4dd
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Feb 24 14:56:14 2017 -0800

    mm: vmscan: kick flushers when we encounter dirty pages on the LRU
    
    Memory pressure can put dirty pages at the end of the LRU without
    anybody running into dirty limits.  Don't start writing individual pages
    from kswapd while the flushers might be asleep.
    
    Unlike the old direct reclaim flusher wakeup (removed in the next patch)
    that flushes the number of pages just scanned, this patch wakes the
    flushers for all outstanding dirty pages.  That seemed to perform better
    in a synthetic test that pushes dirty pages to the end of the LRU and
    into reclaim, because we know LRU aging outstrips writeback already, and
    this way we give younger dirty pages a headstart rather than wait until
    reclaim runs into them as well.  It also means less plugging and risk of
    exhausting the struct request pool from reclaim.
    
    There is a concern that this will cause temporary files that used to get
    dirtied and truncated before writeback to now get written to disk under
    memory pressure.  If this turns out to be a real problem, we'll have to
    revisit this and tame the reclaim flusher wakeups.
    
    [hannes@cmpxchg.org: mention dirty expiration as a condition]
      Link: http://lkml.kernel.org/r/20170126174739.GA30636@cmpxchg.org
    Link: http://lkml.kernel.org/r/20170123181641.23938-3-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 5527d910ba3d..a3c0cbd7c888 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -46,7 +46,7 @@ enum writeback_sync_modes {
  */
 enum wb_reason {
 	WB_REASON_BACKGROUND,
-	WB_REASON_TRY_TO_FREE_PAGES,
+	WB_REASON_VMSCAN,
 	WB_REASON_SYNC,
 	WB_REASON_PERIODIC,
 	WB_REASON_LAPTOP_TIMER,

commit 62906027091f1d02de44041524f0769f60bb9cf3
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Dec 25 13:00:30 2016 +1000

    mm: add PageWaiters indicating tasks are waiting for a page bit
    
    Add a new page flag, PageWaiters, to indicate the page waitqueue has
    tasks waiting. This can be tested rather than testing waitqueue_active
    which requires another cacheline load.
    
    This bit is always set when the page has tasks on page_waitqueue(page),
    and is set and cleared under the waitqueue lock. It may be set when
    there are no tasks on the waitqueue, which will cause a harmless extra
    wakeup check that will clears the bit.
    
    The generic bit-waitqueue infrastructure is no longer used for pages.
    Instead, waitqueues are used directly with a custom key type. The
    generic code was not flexible enough to have PageWaiters manipulation
    under the waitqueue lock (which simplifies concurrency).
    
    This improves the performance of page lock intensive microbenchmarks by
    2-3%.
    
    Putting two bits in the same word opens the opportunity to remove the
    memory barrier between clearing the lock bit and testing the waiters
    bit, after some work on the arch primitives (e.g., ensuring memory
    operand widths match and cover both bits).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Bob Peterson <rpeterso@redhat.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Andrew Lutomirski <luto@kernel.org>
    Cc: Andreas Gruenbacher <agruenba@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c78f9f0920b5..5527d910ba3d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -375,7 +375,6 @@ void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
 unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh);
 
 void wb_update_bandwidth(struct bdi_writeback *wb, unsigned long start_time);
-void page_writeback_init(void);
 void balance_dirty_pages_ratelimited(struct address_space *mapping);
 bool wb_over_bg_thresh(struct bdi_writeback *wb);
 

commit 13edd5e7315a26b448c5f7f33fc7721b1e0c17ef
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Nov 1 10:01:35 2016 -0600

    writeback: mark background writeback as such
    
    If we're doing background type writes, then use the appropriate
    background write flags for that.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 50c96ee8108f..c78f9f0920b5 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -107,6 +107,8 @@ static inline int wbc_to_write_flags(struct writeback_control *wbc)
 {
 	if (wbc->sync_mode == WB_SYNC_ALL)
 		return REQ_SYNC;
+	else if (wbc->for_kupdate || wbc->for_background)
+		return REQ_BACKGROUND;
 
 	return 0;
 }

commit 7637241e651ec36e409412869f986dd5f097735f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Nov 1 10:00:38 2016 -0600

    writeback: add wbc_to_write_flags()
    
    Add wbc_to_write_flags(), which returns the write modifier flags to use,
    based on a struct writeback_control. No functional changes in this
    patch, but it prepares us for factoring other wbc fields for write type.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e4c38703bf4e..50c96ee8108f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -9,6 +9,7 @@
 #include <linux/fs.h>
 #include <linux/flex_proportions.h>
 #include <linux/backing-dev-defs.h>
+#include <linux/blk_types.h>
 
 struct bio;
 
@@ -102,6 +103,14 @@ struct writeback_control {
 #endif
 };
 
+static inline int wbc_to_write_flags(struct writeback_control *wbc)
+{
+	if (wbc->sync_mode == WB_SYNC_ALL)
+		return REQ_SYNC;
+
+	return 0;
+}
+
 /*
  * A wb_domain represents a domain that wb's (bdi_writeback's) belong to
  * and are measured against each other in.  There always is one global

commit 2f8b544477e627a42e66902e948d87f86554aeca
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 07:40:13 2016 -0600

    block,fs: untangle fs.h and blk_types.h
    
    Nothing in fs.h should require blk_types.h to be included.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 797100e10010..e4c38703bf4e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -10,6 +10,8 @@
 #include <linux/flex_proportions.h>
 #include <linux/backing-dev-defs.h>
 
+struct bio;
+
 DECLARE_PER_CPU(int, dirty_throttle_leaks);
 
 /*

commit bf48438354a79df50fadd2e1c0b81baa2619a8b6
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:12 2016 -0700

    mm, vmscan: get rid of throttle_vm_writeout
    
    throttle_vm_writeout() was introduced back in 2005 to fix OOMs caused by
    excessive pageout activity during the reclaim.  Too many pages could be
    put under writeback therefore LRUs would be full of unreclaimable pages
    until the IO completes and in turn the OOM killer could be invoked.
    
    There have been some important changes introduced since then in the
    reclaim path though.  Writers are throttled by balance_dirty_pages when
    initiating the buffered IO and later during the memory pressure, the
    direct reclaim is throttled by wait_iff_congested if the node is
    considered congested by dirty pages on LRUs and the underlying bdi is
    congested by the queued IO.  The kswapd is throttled as well if it
    encounters pages marked for immediate reclaim or under writeback which
    signals that that there are too many pages under writeback already.
    Finally should_reclaim_retry does congestion_wait if the reclaim cannot
    make any progress and there are too many dirty/writeback pages.
    
    Another important aspect is that we do not issue any IO from the direct
    reclaim context anymore.  In a heavy parallel load this could queue a
    lot of IO which would be very scattered and thus unefficient which would
    just make the problem worse.
    
    This three mechanisms should throttle and keep the amount of IO in a
    steady state even under heavy IO and memory pressure so yet another
    throttling point doesn't really seem helpful.  Quite contrary, Mikulas
    Patocka has reported that swap backed by dm-crypt doesn't work properly
    because the swapout IO cannot make sufficient progress as the writeout
    path depends on dm_crypt worker which has to allocate memory to perform
    the encryption.  In order to guarantee a forward progress it relies on
    the mempool allocator.  mempool_alloc(), however, prefers to use the
    underlying (usually page) allocator before it grabs objects from the
    pool.  Such an allocation can dive into the memory reclaim and
    consequently to throttle_vm_writeout.  If there are too many dirty or
    pages under writeback it will get throttled even though it is in fact a
    flusher to clear pending pages.
    
      kworker/u4:0    D ffff88003df7f438 10488     6      2 0x00000000
      Workqueue: kcryptd kcryptd_crypt [dm_crypt]
      Call Trace:
        schedule+0x3c/0x90
        schedule_timeout+0x1d8/0x360
        io_schedule_timeout+0xa4/0x110
        congestion_wait+0x86/0x1f0
        throttle_vm_writeout+0x44/0xd0
        shrink_zone_memcg+0x613/0x720
        shrink_zone+0xe0/0x300
        do_try_to_free_pages+0x1ad/0x450
        try_to_free_pages+0xef/0x300
        __alloc_pages_nodemask+0x879/0x1210
        alloc_pages_current+0xa1/0x1f0
        new_slab+0x2d7/0x6a0
        ___slab_alloc+0x3fb/0x5c0
        __slab_alloc+0x51/0x90
        kmem_cache_alloc+0x27b/0x310
        mempool_alloc_slab+0x1d/0x30
        mempool_alloc+0x91/0x230
        bio_alloc_bioset+0xbd/0x260
        kcryptd_crypt+0x114/0x3b0 [dm_crypt]
    
    Let's just drop throttle_vm_writeout altogether.  It is not very much
    helpful anymore.
    
    I have tried to test a potential writeback IO runaway similar to the one
    described in the original patch which has introduced that [1].  Small
    virtual machine (512MB RAM, 4 CPUs, 2G of swap space and disk image on a
    rather slow NFS in a sync mode on the host) with 8 parallel writers each
    writing 1G worth of data.  As soon as the pagecache fills up and the
    direct reclaim hits then I start anon memory consumer in a loop
    (allocating 300M and exiting after populating it) in the background to
    make the memory pressure even stronger as well as to disrupt the steady
    state for the IO.  The direct reclaim is throttled because of the
    congestion as well as kswapd hitting congestion_wait due to nr_immediate
    but throttle_vm_writeout doesn't ever trigger the sleep throughout the
    test.  Dirty+writeback are close to nr_dirty_threshold with some
    fluctuations caused by the anon consumer.
    
    [1] https://www2.kernel.org/pub/linux/kernel/people/akpm/patches/2.6/2.6.9-rc1/2.6.9-rc1-mm3/broken-out/vm-pageout-throttling.patch
    Link: http://lkml.kernel.org/r/1471171473-21418-1-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: NeilBrown <neilb@suse.com>
    Cc: Ondrej Kozina <okozina@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fc1e16c25a29..797100e10010 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -319,7 +319,6 @@ void laptop_mode_timer_fn(unsigned long data);
 #else
 static inline void laptop_sync_completion(void) { }
 #endif
-void throttle_vm_writeout(gfp_t gfp_mask);
 bool node_dirty_ok(struct pglist_data *pgdat);
 int wb_domain_init(struct wb_domain *dom, gfp_t gfp);
 #ifdef CONFIG_CGROUP_WRITEBACK

commit 281e37265f2826ed401d84d6790226448ef3f0e8
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:11 2016 -0700

    mm, page_alloc: consider dirtyable memory in terms of nodes
    
    Historically dirty pages were spread among zones but now that LRUs are
    per-node it is more appropriate to consider dirty pages in a node.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-17-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 717e6149e753..fc1e16c25a29 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -320,7 +320,7 @@ void laptop_mode_timer_fn(unsigned long data);
 static inline void laptop_sync_completion(void) { }
 #endif
 void throttle_vm_writeout(gfp_t gfp_mask);
-bool zone_dirty_ok(struct zone *zone);
+bool node_dirty_ok(struct pglist_data *pgdat);
 int wb_domain_init(struct wb_domain *dom, gfp_t gfp);
 #ifdef CONFIG_CGROUP_WRITEBACK
 void wb_domain_exit(struct wb_domain *dom);

commit 6c60d2b5746cf23025ffe71bd7ff9075048fc90c
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jul 26 15:21:50 2016 -0700

    fs/fs-writeback.c: add a new writeback list for sync
    
    wait_sb_inodes() currently does a walk of all inodes in the filesystem
    to find dirty one to wait on during sync.  This is highly inefficient
    and wastes a lot of CPU when there are lots of clean cached inodes that
    we don't need to wait on.
    
    To avoid this "all inode" walk, we need to track inodes that are
    currently under writeback that we need to wait for.  We do this by
    adding inodes to a writeback list on the sb when the mapping is first
    tagged as having pages under writeback.  wait_sb_inodes() can then walk
    this list of "inodes under IO" and wait specifically just for the inodes
    that the current sync(2) needs to wait for.
    
    Define a couple helpers to add/remove an inode from the writeback list
    and call them when the overall mapping is tagged for or cleared from
    writeback.  Update wait_sb_inodes() to walk only the inodes under
    writeback due to the sync.
    
    With this change, filesystem sync times are significantly reduced for
    fs' with largely populated inode caches and otherwise no other work to
    do.  For example, on a 16xcpu 2GHz x86-64 server, 10TB XFS filesystem
    with a ~10m entry inode cache, sync times are reduced from ~7.3s to less
    than 0.1s when the filesystem is fully clean.
    
    Link: http://lkml.kernel.org/r/1466594593-6757-2-git-send-email-bfoster@redhat.com
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Brian Foster <bfoster@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Tested-by: Holger Hoffsttte <holger.hoffstaette@applied-asynchrony.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d0b5ca5d4e08..717e6149e753 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -384,4 +384,7 @@ void tag_pages_for_writeback(struct address_space *mapping,
 
 void account_page_redirty(struct page *page);
 
+void sb_mark_inode_writeback(struct inode *inode);
+void sb_clear_inode_writeback(struct inode *inode);
+
 #endif		/* WRITEBACK_H */

commit a1a0e23e49037c23ea84bc8cc146a03584d13577
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 29 18:28:53 2016 -0500

    writeback: flush inode cgroup wb switches instead of pinning super_block
    
    If cgroup writeback is in use, inodes can be scheduled for
    asynchronous wb switching.  Before 5ff8eaac1636 ("writeback: keep
    superblock pinned during cgroup writeback association switches"), this
    could race with umount leading to super_block being destroyed while
    inodes are pinned for wb switching.  5ff8eaac1636 fixed it by bumping
    s_active while wb switches are in flight; however, this allowed
    in-flight wb switches to make umounts asynchronous when the userland
    expected synchronosity - e.g. fsck immediately following umount may
    fail because the device is still busy.
    
    This patch removes the problematic super_block pinning and instead
    makes generic_shutdown_super() flush in-flight wb switches.  wb
    switches are now executed on a dedicated isw_wq so that they can be
    flushed and isw_nr_in_flight keeps track of the number of in-flight wb
    switches so that flushing can be avoided in most cases.
    
    v2: Move cgroup_writeback_umount() further below and add MS_ACTIVE
        check in inode_switch_wbs() as Jan an Al suggested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Tahsin Erdogan <tahsin@google.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Link: http://lkml.kernel.org/g/CAAeU0aNCq7LGODvVGRU-oU_o-6enii5ey0p1c26D1ZzYwkDc5A@mail.gmail.com
    Fixes: 5ff8eaac1636 ("writeback: keep superblock pinned during cgroup writeback association switches")
    Cc: stable@vger.kernel.org #v4.5
    Reviewed-by: Jan Kara <jack@suse.cz>
    Tested-by: Tahsin Erdogan <tahsin@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b333c945e571..d0b5ca5d4e08 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -198,6 +198,7 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 void wbc_detach_inode(struct writeback_control *wbc);
 void wbc_account_io(struct writeback_control *wbc, struct page *page,
 		    size_t bytes);
+void cgroup_writeback_umount(void);
 
 /**
  * inode_attach_wb - associate an inode with its wb
@@ -301,6 +302,10 @@ static inline void wbc_account_io(struct writeback_control *wbc,
 {
 }
 
+static inline void cgroup_writeback_umount(void)
+{
+}
+
 #endif	/* CONFIG_CGROUP_WRITEBACK */
 
 /*

commit 2a81490811d0296d390c571bb64eaa93e5ed7def
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 28 14:50:51 2015 -0400

    writeback: implement foreign cgroup inode detection
    
    As concurrent write sharing of an inode is expected to be very rare
    and memcg only tracks page ownership on first-use basis severely
    confining the usefulness of such sharing, cgroup writeback tracks
    ownership per-inode.  While the support for concurrent write sharing
    of an inode is deemed unnecessary, an inode being written to by
    different cgroups at different points in time is a lot more common,
    and, more importantly, charging only by first-use can too readily lead
    to grossly incorrect behaviors (single foreign page can lead to
    gigabytes of writeback to be incorrectly attributed).
    
    To resolve this issue, cgroup writeback detects the majority dirtier
    of an inode and will transfer the ownership to it.  To avoid
    unnnecessary oscillation, the detection mechanism keeps track of
    history and gives out the switch verdict only if the foreign usage
    pattern is stable over a certain amount of time and/or writeback
    attempts.
    
    The detection mechanism has fairly low space and computation overhead.
    It adds 8 bytes to struct inode (one int and two u16's) and minimal
    amount of calculation per IO.  The detection mechanism converges to
    the correct answer usually in several seconds of IO time when there's
    a clear majority dirtier.  Even when there isn't, it can reach an
    acceptable answer fairly quickly under most circumstances.
    
    Please see wb_detach_inode() for more details.
    
    This patch only implements detection.  Following patches will
    implement actual switching.
    
    v2: wbc_account_io() now checks whether the wbc is associated with a
        wb before dereferencing it.  This can happen when pageout() is
        writing pages directly without going through the usual writeback
        path.  As pageout() path is single-threaded, we don't want it to
        be blocked behind a slow cgroup and ultimately want it to delegate
        actual writing to the usual writeback path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 8f964e558af5..b333c945e571 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -88,6 +88,15 @@ struct writeback_control {
 	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct bdi_writeback *wb;	/* wb this writeback is issued under */
+	struct inode *inode;		/* inode being written out */
+
+	/* foreign inode detection, see wbc_detach_inode() */
+	int wb_id;			/* current wb id */
+	int wb_lcand_id;		/* last foreign candidate wb id */
+	int wb_tcand_id;		/* this foreign candidate wb id */
+	size_t wb_bytes;		/* bytes written by current wb */
+	size_t wb_lcand_bytes;		/* bytes written by last candidate */
+	size_t wb_tcand_bytes;		/* bytes written by this candidate */
 #endif
 };
 
@@ -187,6 +196,8 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 				 struct inode *inode)
 	__releases(&inode->i_lock);
 void wbc_detach_inode(struct writeback_control *wbc);
+void wbc_account_io(struct writeback_control *wbc, struct page *page,
+		    size_t bytes);
 
 /**
  * inode_attach_wb - associate an inode with its wb
@@ -285,6 +296,11 @@ static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
 }
 
+static inline void wbc_account_io(struct writeback_control *wbc,
+				  struct page *page, size_t bytes)
+{
+}
+
 #endif	/* CONFIG_CGROUP_WRITEBACK */
 
 /*

commit b16b1deb553adcd7b3b7ce3e6d6fd1b923f314da
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 2 08:39:48 2015 -0600

    writeback: make writeback_control track the inode being written back
    
    Currently, for cgroup writeback, the IO submission paths directly
    associate the bio's with the blkcg from inode_to_wb_blkcg_css();
    however, it'd be necessary to keep more writeback context to implement
    foreign inode writeback detection.  wbc (writeback_control) is the
    natural fit for the extra context - it persists throughout the
    writeback of each inode and is passed all the way down to IO
    submission paths.
    
    This patch adds wbc_attach_and_unlock_inode(), wbc_detach_inode(), and
    wbc_attach_fdatawrite_inode() which are used to associate wbc with the
    inode being written back.  IO submission paths now use wbc_init_bio()
    instead of directly associating bio's with blkcg themselves.  This
    leaves inode_to_wb_blkcg_css() w/o any user.  The function is removed.
    
    wbc currently only tracks the associated wb (bdi_writeback).  Future
    patches will add more for foreign inode detection.  The association is
    established under i_lock which will be depended upon when migrating
    foreign inodes to other wb's.
    
    As currently, once established, inode to wb association never changes,
    going through wbc when initializing bio's doesn't cause any behavior
    changes.
    
    v2: submit_blk_blkcg() now checks whether the wbc is associated with a
        wb before dereferencing it.  This can happen when pageout() is
        writing pages directly without going through the usual writeback
        path.  As pageout() path is single-threaded, we don't want it to
        be blocked behind a slow cgroup and ultimately want it to delegate
        actual writing to the usual writeback path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 6726b7e56beb..8f964e558af5 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -86,6 +86,9 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
+#ifdef CONFIG_CGROUP_WRITEBACK
+	struct bdi_writeback *wb;	/* wb this writeback is issued under */
+#endif
 };
 
 /*
@@ -176,7 +179,14 @@ static inline void wait_on_inode(struct inode *inode)
 
 #ifdef CONFIG_CGROUP_WRITEBACK
 
+#include <linux/cgroup.h>
+#include <linux/bio.h>
+
 void __inode_attach_wb(struct inode *inode, struct page *page);
+void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
+				 struct inode *inode)
+	__releases(&inode->i_lock);
+void wbc_detach_inode(struct writeback_control *wbc);
 
 /**
  * inode_attach_wb - associate an inode with its wb
@@ -207,6 +217,44 @@ static inline void inode_detach_wb(struct inode *inode)
 	}
 }
 
+/**
+ * wbc_attach_fdatawrite_inode - associate wbc and inode for fdatawrite
+ * @wbc: writeback_control of interest
+ * @inode: target inode
+ *
+ * This function is to be used by __filemap_fdatawrite_range(), which is an
+ * alternative entry point into writeback code, and first ensures @inode is
+ * associated with a bdi_writeback and attaches it to @wbc.
+ */
+static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
+					       struct inode *inode)
+{
+	spin_lock(&inode->i_lock);
+	inode_attach_wb(inode, NULL);
+	wbc_attach_and_unlock_inode(wbc, inode);
+}
+
+/**
+ * wbc_init_bio - writeback specific initializtion of bio
+ * @wbc: writeback_control for the writeback in progress
+ * @bio: bio to be initialized
+ *
+ * @bio is a part of the writeback in progress controlled by @wbc.  Perform
+ * writeback specific initialization.  This is used to apply the cgroup
+ * writeback context.
+ */
+static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
+{
+	/*
+	 * pageout() path doesn't attach @wbc to the inode being written
+	 * out.  This is intentional as we don't want the function to block
+	 * behind a slow cgroup.  Ultimately, we want pageout() to kick off
+	 * regular writeback instead of writing things out itself.
+	 */
+	if (wbc->wb)
+		bio_associate_blkcg(bio, wbc->wb->blkcg_css);
+}
+
 #else	/* CONFIG_CGROUP_WRITEBACK */
 
 static inline void inode_attach_wb(struct inode *inode, struct page *page)
@@ -217,6 +265,26 @@ static inline void inode_detach_wb(struct inode *inode)
 {
 }
 
+static inline void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
+					       struct inode *inode)
+	__releases(&inode->i_lock)
+{
+	spin_unlock(&inode->i_lock);
+}
+
+static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
+					       struct inode *inode)
+{
+}
+
+static inline void wbc_detach_inode(struct writeback_control *wbc)
+{
+}
+
+static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
+{
+}
+
 #endif	/* CONFIG_CGROUP_WRITEBACK */
 
 /*

commit 21c6321fbb3a3787af07f1bc031d713a707fb69c
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 28 14:50:49 2015 -0400

    writeback: relocate wb[_try]_get(), wb_put(), inode_{attach|detach}_wb()
    
    Currently, majority of cgroup writeback support including all the
    above functions are implemented in include/linux/backing-dev.h and
    mm/backing-dev.c; however, the portion closely related to writeback
    logic implemented in include/linux/writeback.h and mm/page-writeback.c
    will expand to support foreign writeback detection and correction.
    
    This patch moves wb[_try]_get() and wb_put() to
    include/linux/backing-dev-defs.h so that they can be used from
    writeback.h and inode_{attach|detach}_wb() to writeback.h and
    page-writeback.c.
    
    This is pure reorganization and doesn't introduce any functional
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3b73e97ecfc7..6726b7e56beb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -8,6 +8,7 @@
 #include <linux/workqueue.h>
 #include <linux/fs.h>
 #include <linux/flex_proportions.h>
+#include <linux/backing-dev-defs.h>
 
 DECLARE_PER_CPU(int, dirty_throttle_leaks);
 
@@ -173,6 +174,51 @@ static inline void wait_on_inode(struct inode *inode)
 	wait_on_bit(&inode->i_state, __I_NEW, TASK_UNINTERRUPTIBLE);
 }
 
+#ifdef CONFIG_CGROUP_WRITEBACK
+
+void __inode_attach_wb(struct inode *inode, struct page *page);
+
+/**
+ * inode_attach_wb - associate an inode with its wb
+ * @inode: inode of interest
+ * @page: page being dirtied (may be NULL)
+ *
+ * If @inode doesn't have its wb, associate it with the wb matching the
+ * memcg of @page or, if @page is NULL, %current.  May be called w/ or w/o
+ * @inode->i_lock.
+ */
+static inline void inode_attach_wb(struct inode *inode, struct page *page)
+{
+	if (!inode->i_wb)
+		__inode_attach_wb(inode, page);
+}
+
+/**
+ * inode_detach_wb - disassociate an inode from its wb
+ * @inode: inode of interest
+ *
+ * @inode is being freed.  Detach from its wb.
+ */
+static inline void inode_detach_wb(struct inode *inode)
+{
+	if (inode->i_wb) {
+		wb_put(inode->i_wb);
+		inode->i_wb = NULL;
+	}
+}
+
+#else	/* CONFIG_CGROUP_WRITEBACK */
+
+static inline void inode_attach_wb(struct inode *inode, struct page *page)
+{
+}
+
+static inline void inode_detach_wb(struct inode *inode)
+{
+}
+
+#endif	/* CONFIG_CGROUP_WRITEBACK */
+
 /*
  * mm/page-writeback.c
  */

commit 2529bb3aadc40a93e642f5f3650f63379a964467
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:34 2015 -0400

    writeback: reset wb_domain->dirty_limit[_tstmp] when memcg domain size changes
    
    The amount of available memory to a memcg wb_domain can change as
    memcg configuration changes.  A domain's ->dirty_limit exists to
    smooth out sudden drops in dirty threshold; however, when a domain's
    size actually drops significantly, it hinders the dirty throttling
    from adjusting to the new configuration leading to unexpected
    behaviors including unnecessary OOM kills.
    
    This patch resolves the issue by adding wb_domain_size_changed() which
    resets ->dirty_limit[_tstmp] and making memcg call it on configuration
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 04a3786c456f..3b73e97ecfc7 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -132,6 +132,26 @@ struct wb_domain {
 	unsigned long dirty_limit;
 };
 
+/**
+ * wb_domain_size_changed - memory available to a wb_domain has changed
+ * @dom: wb_domain of interest
+ *
+ * This function should be called when the amount of memory available to
+ * @dom has changed.  It resets @dom's dirty limit parameters to prevent
+ * the past values which don't match the current configuration from skewing
+ * dirty throttling.  Without this, when memory size of a wb_domain is
+ * greatly reduced, the dirty throttling logic may allow too many pages to
+ * be dirtied leading to consecutive unnecessary OOMs and may get stuck in
+ * that situation.
+ */
+static inline void wb_domain_size_changed(struct wb_domain *dom)
+{
+	spin_lock(&dom->lock);
+	dom->dirty_limit_tstamp = jiffies;
+	dom->dirty_limit = 0;
+	spin_unlock(&dom->lock);
+}
+
 /*
  * fs/fs-writeback.c
  */	

commit 841710aa6e4acd066ab9fe8c8cb6f4e4e6709d83
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:33 2015 -0400

    writeback: implement memcg wb_domain
    
    Dirtyable memory is distributed to a wb (bdi_writeback) according to
    the relative bandwidth the wb is writing out in the whole system.
    This distribution is global - each wb is measured against all other
    wb's and gets the proportinately sized portion of the memory in the
    whole system.
    
    For cgroup writeback, the amount of dirtyable memory is scoped by
    memcg and thus each wb would need to be measured and controlled in its
    memcg.  IOW, a wb will belong to two writeback domains - the global
    and memcg domains.
    
    The previous patches laid the groundwork to support the two wb_domains
    and this patch implements memcg wb_domain.  memcg->cgwb_domain is
    initialized on css online and destroyed on css release,
    wb->memcg_completions is added, and __wb_writeout_inc() is updated to
    increment completions against both global and memcg wb_domains.
    
    The following patches will update balance_dirty_pages() and its
    subroutines to actually consider memcg wb_domain for throttling.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b57c2786b5aa..04a3786c456f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -167,6 +167,9 @@ static inline void laptop_sync_completion(void) { }
 void throttle_vm_writeout(gfp_t gfp_mask);
 bool zone_dirty_ok(struct zone *zone);
 int wb_domain_init(struct wb_domain *dom, gfp_t gfp);
+#ifdef CONFIG_CGROUP_WRITEBACK
+void wb_domain_exit(struct wb_domain *dom);
+#endif
 
 extern struct wb_domain global_wb_domain;
 

commit aa661bbe1e61ce80ca4ae98804f673ede94b0827
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:31 2015 -0400

    writeback: move over_bground_thresh() to mm/page-writeback.c
    
    and rename it to wb_over_bg_thresh().  The function is closely tied to
    the dirty throttling mechanism implemented in page-writeback.c.  This
    relocation will allow future updates necessary for cgroup writeback
    support.
    
    While at it, add function comment.
    
    This is pure reorganization and doesn't introduce any behavioral
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 5fdd4e1805e6..b57c2786b5aa 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -207,6 +207,7 @@ unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh);
 void wb_update_bandwidth(struct bdi_writeback *wb, unsigned long start_time);
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited(struct address_space *mapping);
+bool wb_over_bg_thresh(struct bdi_writeback *wb);
 
 typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
 				void *data);

commit dcc25ae76eb7b8ff883eaaab57e30e8f2f085be3
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:22 2015 -0400

    writeback: move global_dirty_limit into wb_domain
    
    This patch is a part of the series to define wb_domain which
    represents a domain that wb's (bdi_writeback's) belong to and are
    measured against each other in.  This will enable IO backpressure
    propagation for cgroup writeback.
    
    global_dirty_limit exists to regulate the global dirty threshold which
    is a property of the wb_domain.  This patch moves hard_dirty_limit,
    dirty_lock, and update_time into wb_domain.
    
    This is pure reorganization and doesn't introduce any behavioral
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3148db1296a2..5fdd4e1805e6 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -95,6 +95,8 @@ struct writeback_control {
  * dirtyable memory accordingly.
  */
 struct wb_domain {
+	spinlock_t lock;
+
 	/*
 	 * Scale the writeback cache size proportional to the relative
 	 * writeout speed.
@@ -115,6 +117,19 @@ struct wb_domain {
 	struct fprop_global completions;
 	struct timer_list period_timer;	/* timer for aging of completions */
 	unsigned long period_time;
+
+	/*
+	 * The dirtyable memory and dirty threshold could be suddenly
+	 * knocked down by a large amount (eg. on the startup of KVM in a
+	 * swapless system). This may throw the system into deep dirty
+	 * exceeded state and throttle heavy/light dirtiers alike. To
+	 * retain good responsiveness, maintain global_dirty_limit for
+	 * tracking slowly down to the knocked down dirty threshold.
+	 *
+	 * Both fields are protected by ->lock.
+	 */
+	unsigned long dirty_limit_tstamp;
+	unsigned long dirty_limit;
 };
 
 /*
@@ -153,7 +168,7 @@ void throttle_vm_writeout(gfp_t gfp_mask);
 bool zone_dirty_ok(struct zone *zone);
 int wb_domain_init(struct wb_domain *dom, gfp_t gfp);
 
-extern unsigned long global_dirty_limit;
+extern struct wb_domain global_wb_domain;
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;

commit 380c27ca33ebecc9da35aa90c8b3a9154f90aac2
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:21 2015 -0400

    writeback: implement wb_domain
    
    Dirtyable memory is distributed to a wb (bdi_writeback) according to
    the relative bandwidth the wb is writing out in the whole system.
    This distribution is global - each wb is measured against all other
    wb's and gets the proportinately sized portion of the memory in the
    whole system.
    
    For cgroup writeback, the amount of dirtyable memory is scoped by
    memcg and thus each wb would need to be measured and controlled in its
    memcg.  IOW, a wb will belong to two writeback domains - the global
    and memcg domains.
    
    Currently, what constitutes the global writeback domain are scattered
    across a number of global states.  This patch starts collecting them
    into struct wb_domain.
    
    * fprop_global which serves as the basis for proportional bandwidth
      measurement and its period timer are moved into struct wb_domain.
    
    * global_wb_domain hosts the states for the global domain.
    
    * While at it, flatten wb_writeout_fraction() into its callers.  This
      thin wrapper doesn't provide any actual benefits while getting in
      the way.
    
    This is pure reorganization and doesn't introduce any behavioral
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 80adf3d88d9d..3148db1296a2 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -7,6 +7,7 @@
 #include <linux/sched.h>
 #include <linux/workqueue.h>
 #include <linux/fs.h>
+#include <linux/flex_proportions.h>
 
 DECLARE_PER_CPU(int, dirty_throttle_leaks);
 
@@ -86,6 +87,36 @@ struct writeback_control {
 	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
 };
 
+/*
+ * A wb_domain represents a domain that wb's (bdi_writeback's) belong to
+ * and are measured against each other in.  There always is one global
+ * domain, global_wb_domain, that every wb in the system is a member of.
+ * This allows measuring the relative bandwidth of each wb to distribute
+ * dirtyable memory accordingly.
+ */
+struct wb_domain {
+	/*
+	 * Scale the writeback cache size proportional to the relative
+	 * writeout speed.
+	 *
+	 * We do this by keeping a floating proportion between BDIs, based
+	 * on page writeback completions [end_page_writeback()]. Those
+	 * devices that write out pages fastest will get the larger share,
+	 * while the slower will get a smaller share.
+	 *
+	 * We use page writeout completions because we are interested in
+	 * getting rid of dirty pages. Having them written out is the
+	 * primary goal.
+	 *
+	 * We introduce a concept of time, a period over which we measure
+	 * these events, because demand can/will vary over time. The length
+	 * of this period itself is measured in page writeback completions.
+	 */
+	struct fprop_global completions;
+	struct timer_list period_timer;	/* timer for aging of completions */
+	unsigned long period_time;
+};
+
 /*
  * fs/fs-writeback.c
  */	
@@ -120,6 +151,7 @@ static inline void laptop_sync_completion(void) { }
 #endif
 void throttle_vm_writeout(gfp_t gfp_mask);
 bool zone_dirty_ok(struct zone *zone);
+int wb_domain_init(struct wb_domain *dom, gfp_t gfp);
 
 extern unsigned long global_dirty_limit;
 

commit 8a73179956e649df0d4b3250db17734f272d8266
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:20 2015 -0400

    writeback: reorganize [__]wb_update_bandwidth()
    
    __wb_update_bandwidth() is called from two places -
    fs/fs-writeback.c::balance_dirty_pages() and
    mm/page-writeback.c::wb_writeback().  The latter updates only the
    write bandwidth while the former also deals with the dirty ratelimit.
    The two callsites are distinguished by whether @thresh parameter is
    zero or not, which is cryptic.  In addition, the two files define
    their own different versions of wb_update_bandwidth() on top of
    __wb_update_bandwidth(), which is confusing to say the least.  This
    patch cleans up [__]wb_update_bandwidth() in the following ways.
    
    * __wb_update_bandwidth() now takes explicit @update_ratelimit
      parameter to gate dirty ratelimit handling.
    
    * mm/page-writeback.c::wb_update_bandwidth() is flattened into its
      caller - balance_dirty_pages().
    
    * fs/fs-writeback.c::wb_update_bandwidth() is moved to
      mm/page-writeback.c and __wb_update_bandwidth() is made static.
    
    * While at it, add a lockdep assertion to __wb_update_bandwidth().
    
    Except for the lockdep addition, this is pure reorganization and
    doesn't introduce any behavioral changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 0435c85d4cfa..80adf3d88d9d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -157,14 +157,7 @@ int dirty_writeback_centisecs_handler(struct ctl_table *, int,
 void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
 unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh);
 
-void __wb_update_bandwidth(struct bdi_writeback *wb,
-			   unsigned long thresh,
-			   unsigned long bg_thresh,
-			   unsigned long dirty,
-			   unsigned long bdi_thresh,
-			   unsigned long bdi_dirty,
-			   unsigned long start_time);
-
+void wb_update_bandwidth(struct bdi_writeback *wb, unsigned long start_time);
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited(struct address_space *mapping);
 

commit 0d960a383ae7aa791b2833e122ba7519d264cf92
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:19 2015 -0400

    writeback: clean up wb_dirty_limit()
    
    The function name wb_dirty_limit(), its argument @dirty and the local
    variable @wb_dirty are mortally confusing given that the function
    calculates per-wb threshold value not dirty pages, especially given
    that @dirty and @wb_dirty are used elsewhere for dirty pages.
    
    Let's rename the function to wb_calc_thresh() and wb_dirty to
    wb_thresh.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 23af355d5471..0435c85d4cfa 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -155,7 +155,7 @@ int dirty_writeback_centisecs_handler(struct ctl_table *, int,
 				      void __user *, size_t *, loff_t *);
 
 void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
-unsigned long wb_dirty_limit(struct bdi_writeback *wb, unsigned long dirty);
+unsigned long wb_calc_thresh(struct bdi_writeback *wb, unsigned long thresh);
 
 void __wb_update_bandwidth(struct bdi_writeback *wb,
 			   unsigned long thresh,

commit f30a7d0cc8d9096d6728fadd0ab024e648010ec0
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:14:00 2015 -0400

    writeback: restructure try_writeback_inodes_sb[_nr]()
    
    try_writeback_inodes_sb_nr() wraps writeback_inodes_sb_nr() so that it
    handles s_umount locking and skips if writeback is already in
    progress.  The in progress test is performed on the root wb
    (bdi_writeback) which isn't sufficient for cgroup writeback support.
    The test must be done per-wb.
    
    To prepare for the change, this patch factors out
    __writeback_inodes_sb_nr() from writeback_inodes_sb_nr() and adds
    @skip_if_busy and moves the in progress test right before queueing the
    wb_writeback_work.  try_writeback_inodes_sb_nr() now just grabs
    s_umount and invokes __writeback_inodes_sb_nr() with asserted
    @skip_if_busy.  This way, later addition of multiple wb handling can
    skip only the wb's which already have writeback in progress.
    
    This swaps the order between in progress test and s_umount test which
    can flip the return value when writeback is in progress and s_umount
    is being held by someone else but this shouldn't cause any meaningful
    difference.  It's a fringe condition and the return value is an
    unsynchronized hint anyway.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a6b9db7fcee8..23af355d5471 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -93,9 +93,9 @@ struct bdi_writeback;
 void writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 							enum wb_reason reason);
-int try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
-int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
-				  enum wb_reason reason);
+bool try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
+bool try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
+				   enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);

commit a88a341a73be4ef035ca26170c849f002797da27
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:28 2015 -0400

    writeback: move bandwidth related fields from backing_dev_info into bdi_writeback
    
    Currently, a bdi (backing_dev_info) embeds single wb (bdi_writeback)
    and the role of the separation is unclear.  For cgroup support for
    writeback IOs, a bdi will be updated to host multiple wb's where each
    wb serves writeback IOs of a different cgroup on the bdi.  To achieve
    that, a wb should carry all states necessary for servicing writeback
    IOs for a cgroup independently.
    
    This patch moves bandwidth related fields from backing_dev_info into
    bdi_writeback.
    
    * The moved fields are: bw_time_stamp, dirtied_stamp, written_stamp,
      write_bandwidth, avg_write_bandwidth, dirty_ratelimit,
      balanced_dirty_ratelimit, completions and dirty_exceeded.
    
    * writeback_chunk_size() and over_bground_thresh() now take @wb
      instead of @bdi.
    
    * bdi_writeout_fraction(bdi, ...)       -> wb_writeout_fraction(wb, ...)
      bdi_dirty_limit(bdi, ...)             -> wb_dirty_limit(wb, ...)
      bdi_position_ration(bdi, ...)         -> wb_position_ratio(wb, ...)
      bdi_update_writebandwidth(bdi, ...)   -> wb_update_write_bandwidth(wb, ...)
      [__]bdi_update_bandwidth(bdi, ...)    -> [__]wb_update_bandwidth(wb, ...)
      bdi_{max|min}_pause(bdi, ...)         -> wb_{max|min}_pause(wb, ...)
      bdi_dirty_limits(bdi, ...)            -> wb_dirty_limits(wb, ...)
    
    * Init/exits of the relocated fields are moved to bdi_wb_init/exit()
      respectively.  Note that explicit zeroing is dropped in the process
      as wb's are cleared in entirety anyway.
    
    * As there's still only one bdi_writeback per backing_dev_info, all
      uses of bdi->stat[] are mechanically replaced with bdi->wb.stat[]
      introducing no behavior changes.
    
    v2: Typo in description fixed as suggested by Jan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b2dd371ec0ca..a6b9db7fcee8 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -155,16 +155,15 @@ int dirty_writeback_centisecs_handler(struct ctl_table *, int,
 				      void __user *, size_t *, loff_t *);
 
 void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
-unsigned long bdi_dirty_limit(struct backing_dev_info *bdi,
-			       unsigned long dirty);
-
-void __bdi_update_bandwidth(struct backing_dev_info *bdi,
-			    unsigned long thresh,
-			    unsigned long bg_thresh,
-			    unsigned long dirty,
-			    unsigned long bdi_thresh,
-			    unsigned long bdi_dirty,
-			    unsigned long start_time);
+unsigned long wb_dirty_limit(struct bdi_writeback *wb, unsigned long dirty);
+
+void __wb_update_bandwidth(struct bdi_writeback *wb,
+			   unsigned long thresh,
+			   unsigned long bg_thresh,
+			   unsigned long dirty,
+			   unsigned long bdi_thresh,
+			   unsigned long bdi_dirty,
+			   unsigned long start_time);
 
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited(struct address_space *mapping);

commit 1efff914afac8a965ad63817ecf8861a927c2ace
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Tue Mar 17 12:23:32 2015 -0400

    fs: add dirtytime_expire_seconds sysctl
    
    Add a tuning knob so we can adjust the dirtytime expiration timeout,
    which is very useful for testing lazytime.
    
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 00048339c23e..b2dd371ec0ca 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -130,6 +130,7 @@ extern int vm_dirty_ratio;
 extern unsigned long vm_dirty_bytes;
 extern unsigned int dirty_writeback_interval;
 extern unsigned int dirty_expire_interval;
+extern unsigned int dirtytime_expire_interval;
 extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;
@@ -146,6 +147,8 @@ extern int dirty_ratio_handler(struct ctl_table *table, int write,
 extern int dirty_bytes_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
+int dirtytime_interval_handler(struct ctl_table *table, int write,
+			       void __user *buffer, size_t *lenp, loff_t *ppos);
 
 struct ctl_table;
 int dirty_writeback_centisecs_handler(struct ctl_table *, int,

commit 2d6d7f98284648c5ed113fe22a132148950b140f
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 8 14:32:18 2015 -0800

    mm: protect set_page_dirty() from ongoing truncation
    
    Tejun, while reviewing the code, spotted the following race condition
    between the dirtying and truncation of a page:
    
    __set_page_dirty_nobuffers()       __delete_from_page_cache()
      if (TestSetPageDirty(page))
                                         page->mapping = NULL
                                         if (PageDirty())
                                           dec_zone_page_state(page, NR_FILE_DIRTY);
                                           dec_bdi_stat(mapping->backing_dev_info, BDI_RECLAIMABLE);
        if (page->mapping)
          account_page_dirtied(page)
            __inc_zone_page_state(page, NR_FILE_DIRTY);
            __inc_bdi_stat(mapping->backing_dev_info, BDI_RECLAIMABLE);
    
    which results in an imbalance of NR_FILE_DIRTY and BDI_RECLAIMABLE.
    
    Dirtiers usually lock out truncation, either by holding the page lock
    directly, or in case of zap_pte_range(), by pinning the mapcount with
    the page table lock held.  The notable exception to this rule, though,
    is do_wp_page(), for which this race exists.  However, do_wp_page()
    already waits for a locked page to unlock before setting the dirty bit,
    in order to prevent a race where clear_page_dirty() misses the page bit
    in the presence of dirty ptes.  Upgrade that wait to a fully locked
    set_page_dirty() to also cover the situation explained above.
    
    Afterwards, the code in set_page_dirty() dealing with a truncation race
    is no longer needed.  Remove it.
    
    Reported-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a219be961c0a..00048339c23e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -177,7 +177,6 @@ int write_cache_pages(struct address_space *mapping,
 		      struct writeback_control *wbc, writepage_t writepage,
 		      void *data);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
-void set_page_dirty_balance(struct page *page);
 void writeback_set_ratelimit(void);
 void tag_pages_for_writeback(struct address_space *mapping,
 			     pgoff_t start, pgoff_t end);

commit 743162013d40ca612b4cb53d3a200dff2d9ab26e
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jul 7 15:16:04 2014 +1000

    sched: Remove proliferation of wait_on_bit() action functions
    
    The current "wait_on_bit" interface requires an 'action'
    function to be provided which does the actual waiting.
    There are over 20 such functions, many of them identical.
    Most cases can be satisfied by one of just two functions, one
    which uses io_schedule() and one which just uses schedule().
    
    So:
     Rename wait_on_bit and        wait_on_bit_lock to
            wait_on_bit_action and wait_on_bit_lock_action
     to make it explicit that they need an action function.
    
     Introduce new wait_on_bit{,_lock} and wait_on_bit{,_lock}_io
     which are *not* given an action function but implicitly use
     a standard one.
     The decision to error-out if a signal is pending is now made
     based on the 'mode' argument rather than being encoded in the action
     function.
    
     All instances of the old wait_on_bit and wait_on_bit_lock which
     can use the new version have been changed accordingly and their
     action functions have been discarded.
     wait_on_bit{_lock} does not return any specific error code in the
     event of a signal so the caller must check for non-zero and
     interpolate their own error code as appropriate.
    
    The wait_on_bit() call in __fscache_wait_on_invalidate() was
    ambiguous as it specified TASK_UNINTERRUPTIBLE but used
    fscache_wait_bit_interruptible as an action function.
    David Howells confirms this should be uniformly
    "uninterruptible"
    
    The main remaining user of wait_on_bit{,_lock}_action is NFS
    which needs to use a freezer-aware schedule() call.
    
    A comment in fs/gfs2/glock.c notes that having multiple 'action'
    functions is useful as they display differently in the 'wchan'
    field of 'ps'. (and /proc/$PID/wchan).
    As the new bit_wait{,_io} functions are tagged "__sched", they
    will not show up at all, but something higher in the stack.  So
    the distinction will still be visible, only with different
    function names (gds2_glock_wait versus gfs2_glock_dq_wait in the
    gfs2/glock.c case).
    
    Since first version of this patch (against 3.15) two new action
    functions appeared, on in NFS and one in CIFS.  CIFS also now
    uses an action function that makes the same freezer aware
    schedule call as NFS.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Acked-by: David Howells <dhowells@redhat.com> (fscache, keys)
    Acked-by: Steven Whitehouse <swhiteho@redhat.com> (gfs2)
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steve French <sfrench@samba.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140707051603.28027.72349.stgit@notabene.brown
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 5777c13849ba..a219be961c0a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -90,7 +90,6 @@ struct writeback_control {
  * fs/fs-writeback.c
  */	
 struct bdi_writeback;
-int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 							enum wb_reason reason);
@@ -105,7 +104,7 @@ void inode_wait_for_writeback(struct inode *inode);
 static inline void wait_on_inode(struct inode *inode)
 {
 	might_sleep();
-	wait_on_bit(&inode->i_state, __I_NEW, inode_wait, TASK_UNINTERRUPTIBLE);
+	wait_on_bit(&inode->i_state, __I_NEW, TASK_UNINTERRUPTIBLE);
 }
 
 /*

commit ed6d7c8e578331cad594ee70d60e2e146b5dce7b
Author: Miklos Szeredi <mszeredi@suse.cz>
Date:   Mon Apr 7 15:37:51 2014 -0700

    mm: remove unused arg of set_page_dirty_balance()
    
    There's only one caller of set_page_dirty_balance() and that will call it
    with page_mkwrite == 0.
    
    The page_mkwrite argument was unused since commit b827e496c893 "mm: close
    page_mkwrite races".
    
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 021b8a319b9e..5777c13849ba 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -178,7 +178,7 @@ int write_cache_pages(struct address_space *mapping,
 		      struct writeback_control *wbc, writepage_t writepage,
 		      void *data);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
-void set_page_dirty_balance(struct page *page, int page_mkwrite);
+void set_page_dirty_balance(struct page *page);
 void writeback_set_ratelimit(void);
 void tag_pages_for_writeback(struct address_space *mapping,
 			     pgoff_t start, pgoff_t end);

commit 0dc83bd30b0bf5410c0933cfbbf8853248eff0a9
Author: Jan Kara <jack@suse.cz>
Date:   Fri Feb 21 11:19:04 2014 +0100

    Revert "writeback: do not sync data dirtied after sync start"
    
    This reverts commit c4a391b53a72d2df4ee97f96f78c1d5971b47489. Dave
    Chinner <david@fromorbit.com> has reported the commit may cause some
    inodes to be left out from sync(2). This is because we can call
    redirty_tail() for some inode (which sets i_dirtied_when to current time)
    after sync(2) has started or similarly requeue_inode() can set
    i_dirtied_when to current time if writeback had to skip some pages. The
    real problem is in the functions clobbering i_dirtied_when but fixing
    that isn't trivial so revert is a safer choice for now.
    
    CC: stable@vger.kernel.org # >= 3.13
    Signed-off-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fc0e4320aa6d..021b8a319b9e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -97,7 +97,7 @@ void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 int try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 				  enum wb_reason reason);
-void sync_inodes_sb(struct super_block *sb, unsigned long older_than_this);
+void sync_inodes_sb(struct super_block *);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 

commit c4a391b53a72d2df4ee97f96f78c1d5971b47489
Author: Jan Kara <jack@suse.cz>
Date:   Tue Nov 12 15:07:51 2013 -0800

    writeback: do not sync data dirtied after sync start
    
    When there are processes heavily creating small files while sync(2) is
    running, it can easily happen that quite some new files are created
    between WB_SYNC_NONE and WB_SYNC_ALL pass of sync(2).  That can happen
    especially if there are several busy filesystems (remember that sync
    traverses filesystems sequentially and waits in WB_SYNC_ALL phase on one
    fs before starting it on another fs).  Because WB_SYNC_ALL pass is slow
    (e.g.  causes a transaction commit and cache flush for each inode in
    ext3), resulting sync(2) times are rather large.
    
    The following script reproduces the problem:
    
      function run_writers
      {
        for (( i = 0; i < 10; i++ )); do
          mkdir $1/dir$i
          for (( j = 0; j < 40000; j++ )); do
            dd if=/dev/zero of=$1/dir$i/$j bs=4k count=4 &>/dev/null
          done &
        done
      }
    
      for dir in "$@"; do
        run_writers $dir
      done
    
      sleep 40
      time sync
    
    Fix the problem by disregarding inodes dirtied after sync(2) was called
    in the WB_SYNC_ALL pass.  To allow for this, sync_inodes_sb() now takes
    a time stamp when sync has started which is used for setting up work for
    flusher threads.
    
    To give some numbers, when above script is run on two ext4 filesystems
    on simple SATA drive, the average sync time from 10 runs is 267.549
    seconds with standard deviation 104.799426.  With the patched kernel,
    the average sync time from 10 runs is 2.995 seconds with standard
    deviation 0.096.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Fengguang Wu <fengguang.wu@intel.com>
    Reviewed-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 021b8a319b9e..fc0e4320aa6d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -97,7 +97,7 @@ void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 int try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 				  enum wb_reason reason);
-void sync_inodes_sb(struct super_block *);
+void sync_inodes_sb(struct super_block *sb, unsigned long older_than_this);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 

commit 7d9f073b8da45a894bb7148433bd84d21eed6757
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Wed Sep 11 14:22:40 2013 -0700

    mm/writeback: make writeback_inodes_wb static
    
    It's not used globally and could be static.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 4e198ca1f685..021b8a319b9e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -98,8 +98,6 @@ int try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 				  enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
-long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,
-				enum wb_reason reason);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 

commit fc6df808aaf00eed564e2e7fc0f246691363cd12
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Mon Jul 8 16:00:15 2013 -0700

    mm/writeback: commit reason of WB_REASON_FORKER_THREAD mismatch name
    
    After commit 839a8e8660b6 ("writeback: replace custom worker pool
    implementation with unbound workqueue"), there is no bdi forker thread
    any more.  However, WB_REASON_FORKER_THREAD is still used due to it is
    TPs userland visible and we won't be exposing exactly the same
    information with just a different name.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e1703ded543a..4e198ca1f685 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -47,6 +47,12 @@ enum wb_reason {
 	WB_REASON_LAPTOP_TIMER,
 	WB_REASON_FREE_MORE_MEM,
 	WB_REASON_FS_FREE_SPACE,
+	/*
+	 * There is no bdi forker thread any more and works are done
+	 * by emergency worker, however, this is TPs userland visible
+	 * and we'll be exposing exactly the same information,
+	 * so it has a mismatch name.
+	 */
 	WB_REASON_FORKER_THREAD,
 
 	WB_REASON_MAX,

commit 6ce1bc86ae8b8f74095f2694732ccbab2f3849e5
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Mon Jul 8 16:00:12 2013 -0700

    mm/writeback: remove wb_reason_name
    
    wb_reason_name is not used any more - remove it.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e0efffafcd31..e1703ded543a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -51,7 +51,6 @@ enum wb_reason {
 
 	WB_REASON_MAX,
 };
-extern const char *wb_reason_name[];
 
 /*
  * A control structure which tells the writeback code what to do.  These are

commit 12057841008534236e52df3d3e63e089f27c5406
Author: Haicheng Li <haicheng.li@linux.intel.com>
Date:   Mon Jul 8 16:00:11 2013 -0700

    fs/fs-writeback.c: : make wb_do_writeback() as static
    
    It's not used globally and could be static.
    
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index abfe11787af3..e0efffafcd31 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -95,7 +95,6 @@ int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 void sync_inodes_sb(struct super_block *);
 long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,
 				enum wb_reason reason);
-long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 void inode_wait_for_writeback(struct inode *inode);
 

commit 7747bd4bceb3079572695d3942294a6c7b265557
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jul 2 22:38:35 2013 +1000

    sync: don't block the flusher thread waiting on IO
    
    When sync does it's WB_SYNC_ALL writeback, it issues data Io and
    then immediately waits for IO completion. This is done in the
    context of the flusher thread, and hence completely ties up the
    flusher thread for the backing device until all the dirty inodes
    have been synced. On filesystems that are dirtying inodes constantly
    and quickly, this means the flusher thread can be tied up for
    minutes per sync call and hence badly affect system level write IO
    performance as the page cache cannot be cleaned quickly.
    
    We already have a wait loop for IO completion for sync(2), so cut
    this out of the flusher thread and delegate it to wait_sb_inodes().
    Hence we can do rapid IO submission, and then wait for it all to
    complete.
    
    Effect of sync on fsmark before the patch:
    
    FSUse%        Count         Size    Files/sec     App Overhead
    .....
         0       640000         4096      35154.6          1026984
         0       720000         4096      36740.3          1023844
         0       800000         4096      36184.6           916599
         0       880000         4096       1282.7          1054367
         0       960000         4096       3951.3           918773
         0      1040000         4096      40646.2           996448
         0      1120000         4096      43610.1           895647
         0      1200000         4096      40333.1           921048
    
    And a single sync pass took:
    
      real    0m52.407s
      user    0m0.000s
      sys     0m0.090s
    
    After the patch, there is no impact on fsmark results, and each
    individual sync(2) operation run concurrently with the same fsmark
    workload takes roughly 7s:
    
      real    0m6.930s
      user    0m0.000s
      sys     0m0.039s
    
    IOWs, sync is 7-8x faster on a busy filesystem and does not have an
    adverse impact on ongoing async data write operations.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 579a5007c696..abfe11787af3 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -78,6 +78,7 @@ struct writeback_control {
 	unsigned tagged_writepages:1;	/* tag-and-write to avoid livelock */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
+	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
 };
 
 /*

commit a27bb332c04cec8c4afd7912df0dc7890db27560
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 7 16:19:08 2013 -0700

    aio: don't include aio.h in sched.h
    
    Faster kernel compiles by way of fewer unnecessary includes.
    
    [akpm@linux-foundation.org: fix fallout]
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Zach Brown <zab@redhat.com>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Reviewed-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 9a9367c0c076..579a5007c696 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -5,6 +5,7 @@
 #define WRITEBACK_H
 
 #include <linux/sched.h>
+#include <linux/workqueue.h>
 #include <linux/fs.h>
 
 DECLARE_PER_CPU(int, dirty_throttle_leaks);

commit 10ee27a06cc8eb57f83342a8eabcb75deb872d52
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Jan 10 13:47:57 2013 +0800

    vfs: re-implement writeback_inodes_sb(_nr)_if_idle() and rename them
    
    writeback_inodes_sb(_nr)_if_idle() is re-implemented by replacing down_read()
    with down_read_trylock() because
    
    - If ->s_umount is write locked, then the sb is not idle. That is
      writeback_inodes_sb(_nr)_if_idle() needn't wait for the lock.
    
    - writeback_inodes_sb(_nr)_if_idle() grabs s_umount lock when it want to start
      writeback, it may bring us deadlock problem when doing umount. In order to
      fix the problem, ext4 and btrfs implemented their own writeback functions
      instead of writeback_inodes_sb(_nr)_if_idle(), but it introduced the redundant
      code, it is better to implement a new writeback_inodes_sb(_nr)_if_idle().
    
    The name of these two functions is cumbersome, so rename them to
    try_to_writeback_inodes_sb(_nr).
    
    This idea came from Christoph Hellwig.
    Some code is from the patch of Kamal Mostafa.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b82a83aba311..9a9367c0c076 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -87,9 +87,9 @@ int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *, enum wb_reason reason);
 void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
 							enum wb_reason reason);
-int writeback_inodes_sb_if_idle(struct super_block *, enum wb_reason reason);
-int writeback_inodes_sb_nr_if_idle(struct super_block *, unsigned long nr,
-							enum wb_reason reason);
+int try_to_writeback_inodes_sb(struct super_block *, enum wb_reason reason);
+int try_to_writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
+				  enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
 long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,
 				enum wb_reason reason);

commit d0e1d66b5aa1ec9f556f951aa9a114cc192cd01c
Author: Namjae Jeon <linkinjeon@gmail.com>
Date:   Tue Dec 11 16:00:21 2012 -0800

    writeback: remove nr_pages_dirtied arg from balance_dirty_pages_ratelimited_nr()
    
    There is no reason to pass the nr_pages_dirtied argument, because
    nr_pages_dirtied value from the caller is unused in
    balance_dirty_pages_ratelimited_nr().
    
    Signed-off-by: Namjae Jeon <linkinjeon@gmail.com>
    Signed-off-by: Vivek Trivedi <vtrivedi018@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 50c3e8fa06a8..b82a83aba311 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -161,14 +161,7 @@ void __bdi_update_bandwidth(struct backing_dev_info *bdi,
 			    unsigned long start_time);
 
 void page_writeback_init(void);
-void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,
-					unsigned long nr_pages_dirtied);
-
-static inline void
-balance_dirty_pages_ratelimited(struct address_space *mapping)
-{
-	balance_dirty_pages_ratelimited_nr(mapping, 1);
-}
+void balance_dirty_pages_ratelimited(struct address_space *mapping);
 
 typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
 				void *data);

commit 0d5c3eba2e1e5aa74e097f49bc90b58f607e101c
Author: Artem Bityutskiy <artem.bityutskiy@linux.intel.com>
Date:   Wed Jul 25 18:12:08 2012 +0300

    vfs: nuke pdflush from comments
    
    The pdflush thread is long gone, so this patch removes references to pdflush
    from vfs comments.
    
    Signed-off-by: Artem Bityutskiy <artem.bityutskiy@linux.intel.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c66fe3332d83..50c3e8fa06a8 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -104,7 +104,6 @@ static inline void wait_on_inode(struct inode *inode)
 	wait_on_bit(&inode->i_state, __I_NEW, inode_wait, TASK_UNINTERRUPTIBLE);
 }
 
-
 /*
  * mm/page-writeback.c
  */

commit 3965c9ae47d64aadf6f13b6fcd37767b83c0689a
Author: Wanpeng Li <liwp@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:41:52 2012 -0700

    mm: prepare for removal of obsolete /proc/sys/vm/nr_pdflush_threads
    
    Since per-BDI flusher threads were introduced in 2.6, the pdflush
    mechanism is not used any more.  But the old interface exported through
    /proc/sys/vm/nr_pdflush_threads still exists and is obviously useless.
    
    For back-compatibility, printk warning information and return 2 to notify
    the users that the interface is removed.
    
    Signed-off-by: Wanpeng Li <liwp@linux.vnet.ibm.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 6d0a0fcd80e7..c66fe3332d83 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -189,9 +189,4 @@ void tag_pages_for_writeback(struct address_space *mapping,
 
 void account_page_redirty(struct page *page);
 
-/* pdflush.c */
-extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl
-				   read-only. */
-
-
 #endif		/* WRITEBACK_H */

commit 169ebd90131b2ffca74bb2dbe7eeacd39fb83714
Author: Jan Kara <jack@suse.cz>
Date:   Thu May 3 14:48:03 2012 +0200

    writeback: Avoid iput() from flusher thread
    
    Doing iput() from flusher thread (writeback_sb_inodes()) can create problems
    because iput() can do a lot of work - for example truncate the inode if it's
    the last iput on unlinked file. Some filesystems depend on flusher thread
    progressing (e.g. because they need to flush delay allocated blocks to reduce
    allocation uncertainty) and so flusher thread doing truncate creates
    interesting dependencies and possibilities for deadlocks.
    
    We get rid of iput() in flusher thread by using the fact that I_SYNC inode
    flag effectively pins the inode in memory. So if we take care to either hold
    i_lock or have I_SYNC set, we can get away without taking inode reference
    in writeback_sb_inodes().
    
    As a side effect of these changes, we also fix possible use-after-free in
    wb_writeback() because inode_wait_for_writeback() call could try to reacquire
    i_lock on the inode that was already free.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3309736ff059..6d0a0fcd80e7 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -95,6 +95,7 @@ long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,
 				enum wb_reason reason);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
 void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
+void inode_wait_for_writeback(struct inode *inode);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)
@@ -102,12 +103,6 @@ static inline void wait_on_inode(struct inode *inode)
 	might_sleep();
 	wait_on_bit(&inode->i_state, __I_NEW, inode_wait, TASK_UNINTERRUPTIBLE);
 }
-static inline void inode_sync_wait(struct inode *inode)
-{
-	might_sleep();
-	wait_on_bit(&inode->i_state, __I_SYNC, inode_wait,
-							TASK_UNINTERRUPTIBLE);
-}
 
 
 /*

commit 4cd9069a0a0e5fb8b007425c937642682ac96c76
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Wed Apr 25 14:53:05 2012 +0100

    fs: remove 8 bytes of padding from struct writeback_control on 64 bit builds
    
    Reorder structure writeback_control to remove 8 bytes of padding on 64
    bit builds, this shrinks its size from 48 to 40 bytes.
    
    This structure is always on the stack and uses C99 named initialisation,
    so should be safe and have a small impact on stack usage.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a2b84f598e2b..3309736ff059 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -58,7 +58,6 @@ extern const char *wb_reason_name[];
  * in a manner such that unspecified fields are set to zero.
  */
 struct writeback_control {
-	enum writeback_sync_modes sync_mode;
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
 	long pages_skipped;		/* Pages which were not written */
@@ -71,6 +70,8 @@ struct writeback_control {
 	loff_t range_start;
 	loff_t range_end;
 
+	enum writeback_sync_modes sync_mode;
+
 	unsigned for_kupdate:1;		/* A kupdate writeback */
 	unsigned for_background:1;	/* A background writeback */
 	unsigned tagged_writepages:1;	/* tag-and-write to avoid livelock */

commit 95468fd41a8930743180ea3560f4c601d4174275
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Mar 5 15:06:02 2012 -0800

    writeback: fix typo in the writeback_control comment
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 995b8bf630ac..a2b84f598e2b 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -64,7 +64,7 @@ struct writeback_control {
 	long pages_skipped;		/* Pages which were not written */
 
 	/*
-	 * For a_ops->writepages(): is start or end are non-zero then this is
+	 * For a_ops->writepages(): if start or end are non-zero then this is
 	 * a hint that the filesystem need only write out the pages inside that
 	 * byterange.  The byte at `end' is included in the writeout request.
 	 */

commit 001a541ea9163ace5e8243ee0e907ad80a4c0ec2
Merge: 40ba587923ae bc31b86a5923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 10 16:59:59 2012 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
      writeback: balanced_rate cannot exceed write bandwidth
      writeback: do strict bdi dirty_exceeded
      writeback: avoid tiny dirty poll intervals
      writeback: max, min and target dirty pause time
      writeback: dirty ratelimit - think time compensation
      btrfs: fix dirtied pages accounting on sub-page writes
      writeback: fix dirtied pages accounting on redirty
      writeback: fix dirtied pages accounting on sub-page writes
      writeback: charge leaked page dirties to active tasks
      writeback: Include all dirty inodes in background writeback

commit a756cf5908530e8b40bdf569eb48b40139e8d7fd
Author: Johannes Weiner <jweiner@redhat.com>
Date:   Tue Jan 10 15:07:49 2012 -0800

    mm: try to distribute dirty pages fairly across zones
    
    The maximum number of dirty pages that exist in the system at any time is
    determined by a number of pages considered dirtyable and a user-configured
    percentage of those, or an absolute number in bytes.
    
    This number of dirtyable pages is the sum of memory provided by all the
    zones in the system minus their lowmem reserves and high watermarks, so
    that the system can retain a healthy number of free pages without having
    to reclaim dirty pages.
    
    But there is a flaw in that we have a zoned page allocator which does not
    care about the global state but rather the state of individual memory
    zones.  And right now there is nothing that prevents one zone from filling
    up with dirty pages while other zones are spared, which frequently leads
    to situations where kswapd, in order to restore the watermark of free
    pages, does indeed have to write pages from that zone's LRU list.  This
    can interfere so badly with IO from the flusher threads that major
    filesystems (btrfs, xfs, ext4) mostly ignore write requests from reclaim
    already, taking away the VM's only possibility to keep such a zone
    balanced, aside from hoping the flushers will soon clean pages from that
    zone.
    
    Enter per-zone dirty limits.  They are to a zone's dirtyable memory what
    the global limit is to the global amount of dirtyable memory, and try to
    make sure that no single zone receives more than its fair share of the
    globally allowed dirty pages in the first place.  As the number of pages
    considered dirtyable excludes the zones' lowmem reserves and high
    watermarks, the maximum number of dirty pages in a zone is such that the
    zone can always be balanced without requiring page cleaning.
    
    As this is a placement decision in the page allocator and pages are
    dirtied only after the allocation, this patch allows allocators to pass
    __GFP_WRITE when they know in advance that the page will be written to and
    become dirty soon.  The page allocator will then attempt to allocate from
    the first zone of the zonelist - which on NUMA is determined by the task's
    NUMA memory policy - that has not exceeded its dirty limit.
    
    At first glance, it would appear that the diversion to lower zones can
    increase pressure on them, but this is not the case.  With a full high
    zone, allocations will be diverted to lower zones eventually, so it is
    more of a shift in timing of the lower zone allocations.  Workloads that
    previously could fit their dirty pages completely in the higher zone may
    be forced to allocate from lower zones, but the amount of pages that
    "spill over" are limited themselves by the lower zones' dirty constraints,
    and thus unlikely to become a problem.
    
    For now, the problem of unfair dirty page distribution remains for NUMA
    configurations where the zones allowed for allocation are in sum not big
    enough to trigger the global dirty limits, wake up the flusher threads and
    remedy the situation.  Because of this, an allocation that could not
    succeed on any of the considered zones is allowed to ignore the dirty
    limits before going into direct reclaim or even failing the allocation,
    until a future patch changes the global dirty throttling and flusher
    thread activation so that they take individual zone states into account.
    
                            Test results
    
    15M DMA + 3246M DMA32 + 504 Normal = 3765M memory
    40% dirty ratio
    16G USB thumb drive
    10 runs of dd if=/dev/zero of=disk/zeroes bs=32k count=$((10 << 15))
    
                    seconds                 nr_vmscan_write
                            (stddev)               min|     median|        max
    xfs
    vanilla:         549.747( 3.492)             0.000|      0.000|      0.000
    patched:         550.996( 3.802)             0.000|      0.000|      0.000
    
    fuse-ntfs
    vanilla:        1183.094(53.178)         54349.000|  59341.000|  65163.000
    patched:         558.049(17.914)             0.000|      0.000|     43.000
    
    btrfs
    vanilla:         573.679(14.015)        156657.000| 460178.000| 606926.000
    patched:         563.365(11.368)             0.000|      0.000|   1362.000
    
    ext4
    vanilla:         561.197(15.782)             0.000|2725438.000|4143837.000
    patched:         568.806(17.496)             0.000|      0.000|      0.000
    
    Signed-off-by: Johannes Weiner <jweiner@redhat.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Tested-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 34a005515fef..6dff47304971 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -124,6 +124,7 @@ void laptop_mode_timer_fn(unsigned long data);
 static inline void laptop_sync_completion(void) { }
 #endif
 void throttle_vm_writeout(gfp_t gfp_mask);
+bool zone_dirty_ok(struct zone *zone);
 
 extern unsigned long global_dirty_limit;
 

commit 1edf223485c42c99655dcd001db1e46ad5e5d2d7
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Jan 10 15:06:57 2012 -0800

    mm/page-writeback.c: make determine_dirtyable_memory static again
    
    The tracing ring-buffer used this function briefly, but not anymore.
    Make it local to the writeback code again.
    
    Also, move the function so that no forward declaration needs to be
    reintroduced.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a378c295851f..34a005515fef 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -138,8 +138,6 @@ extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;
 
-extern unsigned long determine_dirtyable_memory(void);
-
 extern int dirty_background_ratio_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);

commit bc31b86a5923fad5f3fbb6192f767f410241ba27
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Jan 7 20:41:55 2012 -0600

    writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
    
    Fix compile error
    
     fs/fs-writeback.c:515:33: error: PAGE_CACHE_SHIFT undeclared (first use in this function)
    
    Reported-by: Randy Dunlap <rdunlap@xenotime.net>
    Acked-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b30419cd425e..4e0a55493023 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -25,11 +25,6 @@ DECLARE_PER_CPU(int, dirty_throttle_leaks);
 #define DIRTY_SCOPE		8
 #define DIRTY_FULL_SCOPE	(DIRTY_SCOPE / 2)
 
-/*
- * 4MB minimal write chunk size
- */
-#define MIN_WRITEBACK_PAGES	(4096UL >> (PAGE_CACHE_SHIFT - 10))
-
 struct backing_dev_info;
 
 /*

commit 2f800fbd777b792de54187088df19a7df0251254
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Mon Aug 8 15:22:00 2011 -0600

    writeback: fix dirtied pages accounting on redirty
    
    De-account the accumulative dirty counters on page redirty.
    
    Page redirties (very common in ext4) will introduce mismatch between
    counters (a) and (b)
    
    a) NR_DIRTIED, BDI_DIRTIED, tsk->nr_dirtied
    b) NR_WRITTEN, BDI_WRITTEN
    
    This will introduce systematic errors in balanced_rate and result in
    dirty page position errors (ie. the dirty pages are no longer balanced
    around the global/bdi setpoints).
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 05eaf5e3aad7..b30419cd425e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -197,6 +197,8 @@ void writeback_set_ratelimit(void);
 void tag_pages_for_writeback(struct address_space *mapping,
 			     pgoff_t start, pgoff_t end);
 
+void account_page_redirty(struct page *page);
+
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl
 				   read-only. */

commit 54848d73f9f254631303d6eab9b976855988b266
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Apr 5 13:21:19 2011 -0600

    writeback: charge leaked page dirties to active tasks
    
    It's a years long problem that a large number of short-lived dirtiers
    (eg. gcc instances in a fast kernel build) may starve long-run dirtiers
    (eg. dd) as well as pushing the dirty pages to the global hard limit.
    
    The solution is to charge the pages dirtied by the exited gcc to the
    other random dirtying tasks. It sounds not perfect, however should
    behave good enough in practice, seeing as that throttled tasks aren't
    actually running so those that are running are more likely to pick it up
    and get throttled, therefore promoting an equal spread.
    
    Randy: fix compile error: 'dirty_throttle_leaks' undeclared in exit.c
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a378c295851f..05eaf5e3aad7 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -7,6 +7,8 @@
 #include <linux/sched.h>
 #include <linux/fs.h>
 
+DECLARE_PER_CPU(int, dirty_throttle_leaks);
+
 /*
  * The 1/4 region under the global dirty thresh is for smooth dirty throttling:
  *

commit 0e175a1835ffc979e55787774e58ec79e41957d7
Author: Curt Wohlgemuth <curtw@google.com>
Date:   Fri Oct 7 21:54:10 2011 -0600

    writeback: Add a 'reason' to wb_writeback_work
    
    This creates a new 'reason' field in a wb_writeback_work
    structure, which unambiguously identifies who initiates
    writeback activity.  A 'wb_reason' enumeration has been
    added to writeback.h, to enumerate the possible reasons.
    
    The 'writeback_work_class' and tracepoint event class and
    'writeback_queue_io' tracepoints are updated to include the
    symbolic 'reason' in all trace events.
    
    And the 'writeback_inodes_sbXXX' family of routines has had
    a wb_stats parameter added to them, so callers can specify
    why writeback is being started.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Curt Wohlgemuth <curtw@google.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index ddb4652cb337..a378c295851f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -38,6 +38,23 @@ enum writeback_sync_modes {
 	WB_SYNC_ALL,	/* Wait on every mapping */
 };
 
+/*
+ * why some writeback work was initiated
+ */
+enum wb_reason {
+	WB_REASON_BACKGROUND,
+	WB_REASON_TRY_TO_FREE_PAGES,
+	WB_REASON_SYNC,
+	WB_REASON_PERIODIC,
+	WB_REASON_LAPTOP_TIMER,
+	WB_REASON_FREE_MORE_MEM,
+	WB_REASON_FS_FREE_SPACE,
+	WB_REASON_FORKER_THREAD,
+
+	WB_REASON_MAX,
+};
+extern const char *wb_reason_name[];
+
 /*
  * A control structure which tells the writeback code what to do.  These are
  * always on the stack, and hence need no locking.  They are always initialised
@@ -69,14 +86,17 @@ struct writeback_control {
  */	
 struct bdi_writeback;
 int inode_wait(void *);
-void writeback_inodes_sb(struct super_block *);
-void writeback_inodes_sb_nr(struct super_block *, unsigned long nr);
-int writeback_inodes_sb_if_idle(struct super_block *);
-int writeback_inodes_sb_nr_if_idle(struct super_block *, unsigned long nr);
+void writeback_inodes_sb(struct super_block *, enum wb_reason reason);
+void writeback_inodes_sb_nr(struct super_block *, unsigned long nr,
+							enum wb_reason reason);
+int writeback_inodes_sb_if_idle(struct super_block *, enum wb_reason reason);
+int writeback_inodes_sb_nr_if_idle(struct super_block *, unsigned long nr,
+							enum wb_reason reason);
 void sync_inodes_sb(struct super_block *);
-long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages);
+long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,
+				enum wb_reason reason);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
-void wakeup_flusher_threads(long nr_pages);
+void wakeup_flusher_threads(long nr_pages, enum wb_reason reason);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)

commit af6a311384bce6c88e15c80ab22ab051a918b4eb
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Mon Oct 3 20:46:17 2011 -0600

    writeback: add bg_threshold parameter to __bdi_update_bandwidth()
    
    No behavior change.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 2b8963ff0f35..ddb4652cb337 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -143,6 +143,7 @@ unsigned long bdi_dirty_limit(struct backing_dev_info *bdi,
 
 void __bdi_update_bandwidth(struct backing_dev_info *bdi,
 			    unsigned long thresh,
+			    unsigned long bg_thresh,
 			    unsigned long dirty,
 			    unsigned long bdi_thresh,
 			    unsigned long bdi_dirty,

commit bb0822954aab7d23a3f902c2a103ee0242f6046e
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Aug 16 13:37:14 2011 -0600

    squeeze max-pause area and drop pass-good area
    
    Revert the pass-good area introduced in ffd1f609ab10 ("writeback:
    introduce max-pause and pass-good dirty limits") and make the max-pause
    area smaller and safe.
    
    This fixes ~30% performance regression in the ext3 data=writeback
    fio_mmap_randwrite_64k/fio_mmap_randrw_64k test cases, where there are
    12 JBOD disks, on each disk runs 8 concurrent tasks doing reads+writes.
    
    Using deadline scheduler also has a regression, but not that big as CFQ,
    so this suggests we have some write starvation.
    
    The test logs show that
    
    - the disks are sometimes under utilized
    
    - global dirty pages sometimes rush high to the pass-good area for
      several hundred seconds, while in the mean time some bdi dirty pages
      drop to very low value (bdi_dirty << bdi_thresh).  Then suddenly the
      global dirty pages dropped under global dirty threshold and bdi_dirty
      rush very high (for example, 2 times higher than bdi_thresh). During
      which time balance_dirty_pages() is not called at all.
    
    So the problems are
    
    1) The random writes progress so slow that they break the assumption of
       the max-pause logic that "8 pages per 200ms is typically more than
       enough to curb heavy dirtiers".
    
    2) The max-pause logic ignored task_bdi_thresh and thus opens the possibility
       for some bdi's to over dirty pages, leading to (bdi_dirty >> bdi_thresh)
       and then (bdi_thresh >> bdi_dirty) for others.
    
    3) The higher max-pause/pass-good thresholds somehow leads to the bad
       swing of dirty pages.
    
    The fix is to allow the task to slightly dirty over task_bdi_thresh, but
    no way to exceed bdi_dirty and/or global dirty_thresh.
    
    Tests show that it fixed the JBOD regression completely (both behavior
    and performance), while still being able to cut down large pause times
    in balance_dirty_pages() for single-disk cases.
    
    Reported-by: Li Shaohua <shaohua.li@intel.com>
    Tested-by: Li Shaohua <shaohua.li@intel.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f1bfa12ea246..2b8963ff0f35 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -12,15 +12,6 @@
  *
  *	(thresh - thresh/DIRTY_FULL_SCOPE, thresh)
  *
- * The 1/16 region above the global dirty limit will be put to maximum pauses:
- *
- *	(limit, limit + limit/DIRTY_MAXPAUSE_AREA)
- *
- * The 1/16 region above the max-pause region, dirty exceeded bdi's will be put
- * to loops:
- *
- *	(limit + limit/DIRTY_MAXPAUSE_AREA, limit + limit/DIRTY_PASSGOOD_AREA)
- *
  * Further beyond, all dirtier tasks will enter a loop waiting (possibly long
  * time) for the dirty pages to drop, unless written enough pages.
  *
@@ -31,8 +22,6 @@
  */
 #define DIRTY_SCOPE		8
 #define DIRTY_FULL_SCOPE	(DIRTY_SCOPE / 2)
-#define DIRTY_MAXPAUSE_AREA		16
-#define DIRTY_PASSGOOD_AREA		8
 
 /*
  * 4MB minimal write chunk size

commit 1a12d8bd7b2998be01ee55edb64e7473728abb9c
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sun Aug 29 13:28:09 2010 -0600

    writeback: scale IO chunk size up to half device bandwidth
    
    Originally, MAX_WRITEBACK_PAGES was hard-coded to 1024 because of a
    concern of not holding I_SYNC for too long.  (At least, that was the
    comment previously.)  This doesn't make sense now because the only
    time we wait for I_SYNC is if we are calling sync or fsync, and in
    that case we need to write out all of the data anyway.  Previously
    there may have been other code paths that waited on I_SYNC, but not
    any more.                                           -- Theodore Ts'o
    
    So remove the MAX_WRITEBACK_PAGES constraint. The writeback pages
    will adapt to as large as the storage device can write within 500ms.
    
    XFS is observed to do IO completions in a batch, and the batch size is
    equal to the write chunk size. To avoid dirty pages to suddenly drop
    out of balance_dirty_pages()'s dirty control scope and create large
    fluctuations, the chunk size is also limited to half the control scope.
    
    The balance_dirty_pages() control scrope is
    
            [(background_thresh + dirty_thresh) / 2, dirty_thresh]
    
    which is by default [15%, 20%] of global dirty pages, whose range size
    is dirty_thresh / DIRTY_FULL_SCOPE.
    
    The adpative write chunk size will be rounded to the nearest 4MB
    boundary.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=13930
    
    CC: Theodore Ts'o <tytso@mit.edu>
    CC: Dave Chinner <david@fromorbit.com>
    CC: Chris Mason <chris.mason@oracle.com>
    CC: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b625073b80c8..f1bfa12ea246 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -8,6 +8,10 @@
 #include <linux/fs.h>
 
 /*
+ * The 1/4 region under the global dirty thresh is for smooth dirty throttling:
+ *
+ *	(thresh - thresh/DIRTY_FULL_SCOPE, thresh)
+ *
  * The 1/16 region above the global dirty limit will be put to maximum pauses:
  *
  *	(limit, limit + limit/DIRTY_MAXPAUSE_AREA)
@@ -25,9 +29,16 @@
  * knocks down the global dirty threshold quickly, in which case the global
  * dirty limit will follow down slowly to prevent livelocking all dirtier tasks.
  */
+#define DIRTY_SCOPE		8
+#define DIRTY_FULL_SCOPE	(DIRTY_SCOPE / 2)
 #define DIRTY_MAXPAUSE_AREA		16
 #define DIRTY_PASSGOOD_AREA		8
 
+/*
+ * 4MB minimal write chunk size
+ */
+#define MIN_WRITEBACK_PAGES	(4096UL >> (PAGE_CACHE_SHIFT - 10))
+
 struct backing_dev_info;
 
 /*

commit ffd1f609ab10532e8137b4b981fdf903ef4d0b32
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sun Jun 19 22:18:42 2011 -0600

    writeback: introduce max-pause and pass-good dirty limits
    
    The max-pause limit helps to keep the sleep time inside
    balance_dirty_pages() within MAX_PAUSE=200ms. The 200ms max sleep means
    per task rate limit of 8pages/200ms=160KB/s when dirty exceeded, which
    normally is enough to stop dirtiers from continue pushing the dirty
    pages high, unless there are a sufficient large number of slow dirtiers
    (eg. 500 tasks doing 160KB/s will still sum up to 80MB/s, exceeding the
    write bandwidth of a slow disk and hence accumulating more and more dirty
    pages).
    
    The pass-good limit helps to let go of the good bdi's in the presence of
    a blocked bdi (ie. NFS server not responding) or slow USB disk which for
    some reason build up a large number of initial dirty pages that refuse
    to go away anytime soon.
    
    For example, given two bdi's A and B and the initial state
    
            bdi_thresh_A = dirty_thresh / 2
            bdi_thresh_B = dirty_thresh / 2
            bdi_dirty_A  = dirty_thresh / 2
            bdi_dirty_B  = dirty_thresh / 2
    
    Then A get blocked, after a dozen seconds
    
            bdi_thresh_A = 0
            bdi_thresh_B = dirty_thresh
            bdi_dirty_A  = dirty_thresh / 2
            bdi_dirty_B  = dirty_thresh / 2
    
    The (bdi_dirty_B < bdi_thresh_B) test is now useless and the dirty pages
    will be effectively throttled by condition (nr_dirty < dirty_thresh).
    This has two problems:
    (1) we lose the protections for light dirtiers
    (2) balance_dirty_pages() effectively becomes IO-less because the
        (bdi_nr_reclaimable > bdi_thresh) test won't be true. This is good
        for IO, but balance_dirty_pages() loses an important way to break
        out of the loop which leads to more spread out throttle delays.
    
    DIRTY_PASSGOOD_AREA can eliminate the above issues. The only problem is,
    DIRTY_PASSGOOD_AREA needs to be defined as 2 to fully cover the above
    example while this patch uses the more conservative value 8 so as not to
    surprise people with too many dirty pages than expected.
    
    The max-pause limit won't noticeably impact the speed dirty pages are
    knocked down when there is a sudden drop of global/bdi dirty thresholds.
    Because the heavy dirties will be throttled below 160KB/s which is slow
    enough. It does help to avoid long dirty throttle delays and especially
    will make light dirtiers more responsive.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e9d371b6053b..b625073b80c8 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -7,6 +7,27 @@
 #include <linux/sched.h>
 #include <linux/fs.h>
 
+/*
+ * The 1/16 region above the global dirty limit will be put to maximum pauses:
+ *
+ *	(limit, limit + limit/DIRTY_MAXPAUSE_AREA)
+ *
+ * The 1/16 region above the max-pause region, dirty exceeded bdi's will be put
+ * to loops:
+ *
+ *	(limit + limit/DIRTY_MAXPAUSE_AREA, limit + limit/DIRTY_PASSGOOD_AREA)
+ *
+ * Further beyond, all dirtier tasks will enter a loop waiting (possibly long
+ * time) for the dirty pages to drop, unless written enough pages.
+ *
+ * The global dirty threshold is normally equal to the global dirty limit,
+ * except when the system suddenly allocates a lot of anonymous memory and
+ * knocks down the global dirty threshold quickly, in which case the global
+ * dirty limit will follow down slowly to prevent livelocking all dirtier tasks.
+ */
+#define DIRTY_MAXPAUSE_AREA		16
+#define DIRTY_PASSGOOD_AREA		8
+
 struct backing_dev_info;
 
 /*

commit c42843f2f0bbc9d716a32caf667d18fc2bf3bc4c
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Mar 2 15:54:09 2011 -0600

    writeback: introduce smoothed global dirty limit
    
    The start of a heavy weight application (ie. KVM) may instantly knock
    down determine_dirtyable_memory() if the swap is not enabled or full.
    global_dirty_limits() and bdi_dirty_limit() will in turn get global/bdi
    dirty thresholds that are _much_ lower than the global/bdi dirty pages.
    
    balance_dirty_pages() will then heavily throttle all dirtiers including
    the light ones, until the dirty pages drop below the new dirty thresholds.
    During this _deep_ dirty-exceeded state, the system may appear rather
    unresponsive to the users.
    
    About "deep" dirty-exceeded: task_dirty_limit() assigns 1/8 lower dirty
    threshold to heavy dirtiers than light ones, and the dirty pages will
    be throttled around the heavy dirtiers' dirty threshold and reasonably
    below the light dirtiers' dirty threshold. In this state, only the heavy
    dirtiers will be throttled and the dirty pages are carefully controlled
    to not exceed the light dirtiers' dirty threshold. However if the
    threshold itself suddenly drops below the number of dirty pages, the
    light dirtiers will get heavily throttled.
    
    So introduce global_dirty_limit for tracking the global dirty threshold
    with policies
    
    - follow downwards slowly
    - follow up in one shot
    
    global_dirty_limit can effectively mask out the impact of sudden drop of
    dirtyable memory. It will be used in the next patch for two new type of
    dirty limits. Note that the new dirty limits are not going to avoid
    throttling the light dirtiers, but could limit their sleep time to 200ms.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 66862f2d90c8..e9d371b6053b 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -84,6 +84,8 @@ static inline void laptop_sync_completion(void) { }
 #endif
 void throttle_vm_writeout(gfp_t gfp_mask);
 
+extern unsigned long global_dirty_limit;
+
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;
 extern unsigned long dirty_background_bytes;
@@ -119,6 +121,10 @@ unsigned long bdi_dirty_limit(struct backing_dev_info *bdi,
 			       unsigned long dirty);
 
 void __bdi_update_bandwidth(struct backing_dev_info *bdi,
+			    unsigned long thresh,
+			    unsigned long dirty,
+			    unsigned long bdi_thresh,
+			    unsigned long bdi_dirty,
 			    unsigned long start_time);
 
 void page_writeback_init(void);

commit e98be2d599207c6b31e9bb340d52a231b2f3662d
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sun Aug 29 11:22:30 2010 -0600

    writeback: bdi write bandwidth estimation
    
    The estimation value will start from 100MB/s and adapt to the real
    bandwidth in seconds.
    
    It tries to update the bandwidth only when disk is fully utilized.
    Any inactive period of more than one second will be skipped.
    
    The estimated bandwidth will be reflecting how fast the device can
    writeout when _fully utilized_, and won't drop to 0 when it goes idle.
    The value will remain constant at disk idle time. At busy write time, if
    not considering fluctuations, it will also remain high unless be knocked
    down by possible concurrent reads that compete for the disk time and
    bandwidth with async writes.
    
    The estimation is not done purely in the flusher because there is no
    guarantee for write_cache_pages() to return timely to update bandwidth.
    
    The bdi->avg_write_bandwidth smoothing is very effective for filtering
    out sudden spikes, however may be a little biased in long term.
    
    The overheads are low because the bdi bandwidth update only occurs at
    200ms intervals.
    
    The 200ms update interval is suitable, because it's not possible to get
    the real bandwidth for the instance at all, due to large fluctuations.
    
    The NFS commits can be as large as seconds worth of data. One XFS
    completion may be as large as half second worth of data if we are going
    to increase the write chunk to half second worth of data. In ext4,
    fluctuations with time period of around 5 seconds is observed. And there
    is another pattern of irregular periods of up to 20 seconds on SSD tests.
    
    That's why we are not only doing the estimation at 200ms intervals, but
    also averaging them over a period of 3 seconds and then go further to do
    another level of smoothing in avg_write_bandwidth.
    
    CC: Li Shaohua <shaohua.li@intel.com>
    CC: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index df1b7f18f100..66862f2d90c8 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -118,6 +118,9 @@ void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
 unsigned long bdi_dirty_limit(struct backing_dev_info *bdi,
 			       unsigned long dirty);
 
+void __bdi_update_bandwidth(struct backing_dev_info *bdi,
+			    unsigned long start_time);
+
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,
 					unsigned long nr_pages_dirtied);

commit d46db3d58233be4be980eb1e42eebe7808bcabab
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed May 4 19:54:37 2011 -0600

    writeback: make writeback_control.nr_to_write straight
    
    Pass struct wb_writeback_work all the way down to writeback_sb_inodes(),
    and initialize the struct writeback_control there.
    
    struct writeback_control is basically designed to control writeback of a
    single file, but we keep abuse it for writing multiple files in
    writeback_sb_inodes() and its callers.
    
    It immediately clean things up, e.g. suddenly wbc.nr_to_write vs
    work->nr_pages starts to make sense, and instead of saving and restoring
    pages_skipped in writeback_sb_inodes it can always start with a clean
    zero value.
    
    It also makes a neat IO pattern change: large dirty files are now
    written in the full 4MB writeback chunk size, rather than whatever
    remained quota in wbc->nr_to_write.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Proposed-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 2f1b512bd6e0..df1b7f18f100 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -24,12 +24,9 @@ enum writeback_sync_modes {
  */
 struct writeback_control {
 	enum writeback_sync_modes sync_mode;
-	unsigned long *older_than_this;	/* If !NULL, only write back inodes
-					   older than this */
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
 	long pages_skipped;		/* Pages which were not written */
-	long inodes_written;		/* # of inodes written (at least) */
 
 	/*
 	 * For a_ops->writepages(): is start or end are non-zero then this is
@@ -56,8 +53,7 @@ void writeback_inodes_sb_nr(struct super_block *, unsigned long nr);
 int writeback_inodes_sb_if_idle(struct super_block *);
 int writeback_inodes_sb_nr_if_idle(struct super_block *, unsigned long nr);
 void sync_inodes_sb(struct super_block *);
-void writeback_inodes_wb(struct bdi_writeback *wb,
-		struct writeback_control *wbc);
+long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
 void wakeup_flusher_threads(long nr_pages);
 

commit 846d5a091b0506b75489577cde27f39b37a192a4
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Thu May 5 21:10:38 2011 -0600

    writeback: remove .nonblocking and .encountered_congestion
    
    Remove two unused struct writeback_control fields:
    
            .encountered_congestion (completely unused)
            .nonblocking            (never set, checked/showed in XFS,NFS/btrfs)
    
    The .for_background check in nfs_write_inode() is also removed btw,
    as .for_background implies WB_SYNC_NONE.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Proposed-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 8797b20dd22b..2f1b512bd6e0 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -39,8 +39,6 @@ struct writeback_control {
 	loff_t range_start;
 	loff_t range_end;
 
-	unsigned nonblocking:1;		/* Don't get stuck on request queues */
-	unsigned encountered_congestion:1; /* An output: a queue is full */
 	unsigned for_kupdate:1;		/* A kupdate writeback */
 	unsigned for_background:1;	/* A background writeback */
 	unsigned tagged_writepages:1;	/* tag-and-write to avoid livelock */

commit b7a2441f9966fe3e1be960a876ab52e6029ea005
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Jul 21 22:19:51 2010 -0600

    writeback: remove writeback_control.more_io
    
    When wbc.more_io was first introduced, it indicates whether there are
    at least one superblock whose s_more_io contains more IO work. Now with
    the per-bdi writeback, it can be replaced with a simple b_more_io test.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d8e96a480850..8797b20dd22b 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -46,7 +46,6 @@ struct writeback_control {
 	unsigned tagged_writepages:1;	/* tag-and-write to avoid livelock */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
-	unsigned more_io:1;		/* more io to be dispatched */
 };
 
 /*

commit e185dda89d69cde142b48059413a03561f41f78a
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Apr 23 11:26:07 2011 -0600

    writeback: avoid extra sync work at enqueue time
    
    This removes writeback_control.wb_start and does more straightforward
    sync livelock prevention by setting .older_than_this to prevent extra
    inodes from being enqueued in the first place.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c2d957fb38d3..d8e96a480850 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -26,9 +26,6 @@ struct writeback_control {
 	enum writeback_sync_modes sync_mode;
 	unsigned long *older_than_this;	/* If !NULL, only write back inodes
 					   older than this */
-	unsigned long wb_start;         /* Time writeback_inodes_wb was
-					   called. This is needed to avoid
-					   extra jobs and livelock */
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
 	long pages_skipped;		/* Pages which were not written */

commit f758eeabeb96f878c860e8f110f94ec8820822a9
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Apr 21 18:19:44 2011 -0600

    writeback: split inode_wb_list_lock into bdi_writeback.list_lock
    
    Split the global inode_wb_list_lock into a per-bdi_writeback list_lock,
    as it's currently the most contended lock in the system for metadata
    heavy workloads.  It won't help for single-filesystem workloads for
    which we'll need the I/O-less balance_dirty_pages, but at least we
    can dedicate a cpu to spinning on each bdi now for larger systems.
    
    Based on earlier patches from Nick Piggin and Dave Chinner.
    
    It reduces lock contentions to 1/4 in this test case:
    10 HDD JBOD, 100 dd on each disk, XFS, 6GB ram
    
    lock_stat version 0.3
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                  class name    con-bounces    contentions   waittime-min   waittime-max waittime-total    acq-bounces   acquisitions   holdtime-min   holdtime-max holdtime-total
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    vanilla 2.6.39-rc3:
                          inode_wb_list_lock:         42590          44433           0.12         147.74      144127.35         252274         886792           0.08         121.34      917211.23
                          ------------------
                          inode_wb_list_lock              2          [<ffffffff81165da5>] bdev_inode_switch_bdi+0x29/0x85
                          inode_wb_list_lock             34          [<ffffffff8115bd0b>] inode_wb_list_del+0x22/0x49
                          inode_wb_list_lock          12893          [<ffffffff8115bb53>] __mark_inode_dirty+0x170/0x1d0
                          inode_wb_list_lock          10702          [<ffffffff8115afef>] writeback_single_inode+0x16d/0x20a
                          ------------------
                          inode_wb_list_lock              2          [<ffffffff81165da5>] bdev_inode_switch_bdi+0x29/0x85
                          inode_wb_list_lock             19          [<ffffffff8115bd0b>] inode_wb_list_del+0x22/0x49
                          inode_wb_list_lock           5550          [<ffffffff8115bb53>] __mark_inode_dirty+0x170/0x1d0
                          inode_wb_list_lock           8511          [<ffffffff8115b4ad>] writeback_sb_inodes+0x10f/0x157
    
    2.6.39-rc3 + patch:
                    &(&wb->list_lock)->rlock:         11383          11657           0.14         151.69       40429.51          90825         527918           0.11         145.90      556843.37
                    ------------------------
                    &(&wb->list_lock)->rlock             10          [<ffffffff8115b189>] inode_wb_list_del+0x5f/0x86
                    &(&wb->list_lock)->rlock           1493          [<ffffffff8115b1ed>] writeback_inodes_wb+0x3d/0x150
                    &(&wb->list_lock)->rlock           3652          [<ffffffff8115a8e9>] writeback_sb_inodes+0x123/0x16f
                    &(&wb->list_lock)->rlock           1412          [<ffffffff8115a38e>] writeback_single_inode+0x17f/0x223
                    ------------------------
                    &(&wb->list_lock)->rlock              3          [<ffffffff8110b5af>] bdi_lock_two+0x46/0x4b
                    &(&wb->list_lock)->rlock              6          [<ffffffff8115b189>] inode_wb_list_del+0x5f/0x86
                    &(&wb->list_lock)->rlock           2061          [<ffffffff8115af97>] __mark_inode_dirty+0x173/0x1cf
                    &(&wb->list_lock)->rlock           2629          [<ffffffff8115a8e9>] writeback_sb_inodes+0x123/0x16f
    
    hughd@google.com: fix recursive lock when bdi_lock_two() is called with new the same as old
    akpm@linux-foundation.org: cleanup bdev_inode_switch_bdi() comment
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 7df9026f7129..c2d957fb38d3 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -9,8 +9,6 @@
 
 struct backing_dev_info;
 
-extern spinlock_t inode_wb_list_lock;
-
 /*
  * fs/fs-writeback.c
  */

commit cb9bd1159c5fe8995e151fa7df10fa19f8c119cc
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Jul 21 22:50:57 2010 -0600

    writeback: introduce writeback_control.inodes_written
    
    The flusher works on dirty inodes in batches, and may quit prematurely
    if the batch of inodes happen to be metadata-only dirtied: in this case
    wbc->nr_to_write won't be decreased at all, which stands for "no pages
    written" but also mis-interpreted as "no progress".
    
    So introduce writeback_control.inodes_written to count the inodes get
    cleaned from VFS POV.  A non-zero value means there are some progress on
    writeback, in which case more writeback can be tried.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3f6542ca6198..7df9026f7129 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -34,6 +34,7 @@ struct writeback_control {
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
 	long pages_skipped;		/* Pages which were not written */
+	long inodes_written;		/* # of inodes written (at least) */
 
 	/*
 	 * For a_ops->writepages(): is start or end are non-zero then this is

commit 6e6938b6d3130305a5960c86b1a9b21e58cf6144
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sun Jun 6 10:38:15 2010 -0600

    writeback: introduce .tagged_writepages for the WB_SYNC_NONE sync stage
    
    sync(2) is performed in two stages: the WB_SYNC_NONE sync and the
    WB_SYNC_ALL sync. Identify the first stage with .tagged_writepages and
    do livelock prevention for it, too.
    
    Jan's commit f446daaea9 ("mm: implement writeback livelock avoidance
    using page tagging") is a partial fix in that it only fixed the
    WB_SYNC_ALL phase livelock.
    
    Although ext4 is tested to no longer livelock with commit f446daaea9,
    it may due to some "redirty_tail() after pages_skipped" effect which
    is by no means a guarantee for _all_ the file systems.
    
    Note that writeback_inodes_sb() is called by not only sync(), they are
    treated the same because the other callers also need livelock prevention.
    
    Impact:  It changes the order in which pages/inodes are synced to disk.
    Now in the WB_SYNC_NONE stage, it won't proceed to write the next inode
    until finished with the current inode.
    
    Acked-by: Jan Kara <jack@suse.cz>
    CC: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 17e7ccc322a5..3f6542ca6198 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -47,6 +47,7 @@ struct writeback_control {
 	unsigned encountered_congestion:1; /* An output: a queue is full */
 	unsigned for_kupdate:1;		/* A kupdate writeback */
 	unsigned for_background:1;	/* A background writeback */
+	unsigned tagged_writepages:1;	/* tag-and-write to avoid livelock */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */

commit 67a23c494621ff1d5431c3bc320947865b224625
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Mar 22 22:23:42 2011 +1100

    fs: rename inode_lock to inode_hash_lock
    
    All that remains of the inode_lock is protecting the inode hash list
    manipulation and traversals. Rename the inode_lock to
    inode_hash_lock to reflect it's actual function.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3f5fee718329..17e7ccc322a5 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -9,7 +9,6 @@
 
 struct backing_dev_info;
 
-extern spinlock_t inode_lock;
 extern spinlock_t inode_wb_list_lock;
 
 /*

commit a66979abad090b2765a6c6790c9fdeab996833f2
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Mar 22 22:23:41 2011 +1100

    fs: move i_wb_list out from under inode_lock
    
    Protect the inode writeback list with a new global lock
    inode_wb_list_lock and use it to protect the list manipulations and
    traversals. This lock replaces the inode_lock as the inodes on the
    list can be validity checked while holding the inode->i_lock and
    hence the inode_lock is no longer needed to protect the list.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 0ead399e08b5..3f5fee718329 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -10,6 +10,7 @@
 struct backing_dev_info;
 
 extern spinlock_t inode_lock;
+extern spinlock_t inode_wb_list_lock;
 
 /*
  * fs/fs-writeback.c

commit 925d169f5b86fe57e2f5264ea574cce9a89b719d
Merge: cdf01dd5443d 6418c96107a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 30 09:05:48 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mason/btrfs-unstable: (39 commits)
      Btrfs: deal with errors from updating the tree log
      Btrfs: allow subvol deletion by unprivileged user with -o user_subvol_rm_allowed
      Btrfs: make SNAP_DESTROY async
      Btrfs: add SNAP_CREATE_ASYNC ioctl
      Btrfs: add START_SYNC, WAIT_SYNC ioctls
      Btrfs: async transaction commit
      Btrfs: fix deadlock in btrfs_commit_transaction
      Btrfs: fix lockdep warning on clone ioctl
      Btrfs: fix clone ioctl where range is adjacent to extent
      Btrfs: fix delalloc checks in clone ioctl
      Btrfs: drop unused variable in block_alloc_rsv
      Btrfs: cleanup warnings from gcc 4.6 (nonbugs)
      Btrfs: Fix variables set but not read (bugs found by gcc 4.6)
      Btrfs: Use ERR_CAST helpers
      Btrfs: use memdup_user helpers
      Btrfs: fix raid code for removing missing drives
      Btrfs: Switch the extent buffer rbtree into a radix tree
      Btrfs: restructure try_release_extent_buffer()
      Btrfs: use the flusher threads for delalloc throttling
      Btrfs: tune the chunk allocation to 5% of the FS as metadata
      ...
    
    Fix up trivial conflicts in fs/btrfs/super.c and fs/fs-writeback.c, and
    remove use of INIT_RCU_HEAD in fs/btrfs/extent_io.c (that init macro was
    useless and removed in commit 5e8067adfdba: "rcu head remove init")

commit 3259f8bed2f0f57c2fdcdac1b510c3fa319ef97e
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Oct 29 11:16:17 2010 -0400

    Add new functions for triggering inode writeback
    
    When btrfs is running low on metadata space, it needs to force delayed
    allocation pages to disk.  It currently does this with a suboptimal walk
    of a private list of inodes with delayed allocation, and it would be
    much better if we used the generic flusher threads.
    
    writeback_inodes_sb_if_idle would be ideal, but it waits for the flusher
    thread to start IO on all the dirty pages in the FS before it returns.
    This adds variants of writeback_inodes_sb* that allow the caller to
    control how many pages get sent down.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 72a5d647a5f2..a4cf84511e79 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -60,7 +60,9 @@ struct writeback_control {
 struct bdi_writeback;
 int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *);
+void writeback_inodes_sb_nr(struct super_block *, unsigned long nr);
 int writeback_inodes_sb_if_idle(struct super_block *);
+int writeback_inodes_sb_nr_if_idle(struct super_block *, unsigned long nr);
 void sync_inodes_sb(struct super_block *);
 void writeback_inodes_wb(struct bdi_writeback *wb,
 		struct writeback_control *wbc);

commit a107e5a3a473a2ea62bd5af24e11b84adf1486ff
Merge: e3e1288e86a0 a269029d0e21
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Wed Oct 27 23:44:47 2010 -0400

    Merge branch 'next' into upstream-merge
    
    Conflicts:
            fs/ext4/inode.c
            fs/ext4/mballoc.c
            include/trace/events/ext4.h

commit 5b41d92437f1ae19b3f3ffa3b16589fd5df50ac0
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Wed Oct 27 21:30:13 2010 -0400

    ext4: implement writeback livelock avoidance using page tagging
    
    This is analogous to Jan Kara's commit,
    f446daaea9d4a420d16c606f755f3689dcb2d0ce
    mm: implement writeback livelock avoidance using page tagging
    
    but since we forked write_cache_pages, we need to reimplement
    it there (and in ext4_da_writepages, since range_cyclic handling
    was moved to there)
    
    If you start a large buffered IO to a file, and then set
    fsync after it, you'll find that fsync does not complete
    until the other IO stops.
    
    If you continue re-dirtying the file (say, putting dd
    with conv=notrunc in a loop), when fsync finally completes
    (after all IO is done), it reports via tracing that
    it has written many more pages than the file contains;
    in other words it has synced and re-synced pages in
    the file multiple times.
    
    This then leads to problems with our writeback_index
    update, since it advances it by pages written, and
    essentially sets writeback_index off the end of the
    file...
    
    With the following patch, we only sync as much as was
    dirty at the time of the sync.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 72a5d647a5f2..3d132bfb4f3d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -143,6 +143,8 @@ typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
 
 int generic_writepages(struct address_space *mapping,
 		       struct writeback_control *wbc);
+void tag_pages_for_writeback(struct address_space *mapping,
+			     pgoff_t start, pgoff_t end);
 int write_cache_pages(struct address_space *mapping,
 		      struct writeback_control *wbc, writepage_t writepage,
 		      void *data);

commit 426e1f5cec4821945642230218876b0e89aafab1
Merge: 9e5fca251f44 63997e98a3be
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 26 17:58:44 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6: (52 commits)
      split invalidate_inodes()
      fs: skip I_FREEING inodes in writeback_sb_inodes
      fs: fold invalidate_list into invalidate_inodes
      fs: do not drop inode_lock in dispose_list
      fs: inode split IO and LRU lists
      fs: switch bdev inode bdi's correctly
      fs: fix buffer invalidation in invalidate_list
      fsnotify: use dget_parent
      smbfs: use dget_parent
      exportfs: use dget_parent
      fs: use RCU read side protection in d_validate
      fs: clean up dentry lru modification
      fs: split __shrink_dcache_sb
      fs: improve DCACHE_REFERENCED usage
      fs: use percpu counter for nr_dentry and nr_dentry_unused
      fs: simplify __d_free
      fs: take dcache_lock inside __d_path
      fs: do not assign default i_ino in new_inode
      fs: introduce a per-cpu last_ino allocator
      new helper: ihold()
      ...

commit 92c09c041f15fc88b35f8628e07639f52e1fbb38
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Tue Oct 26 14:22:03 2010 -0700

    mm: declare some external symbols
    
    Declare 'bdi_pending_list' and 'tag_pages_for_writeback()' to remove
    following sparse warnings:
    
     mm/backing-dev.c:46:1: warning: symbol 'bdi_pending_list' was not declared. Should it be static?
     mm/page-writeback.c:825:6: warning: symbol 'tag_pages_for_writeback' was not declared. Should it be static?
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 72a5d647a5f2..c7299d2ace6b 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -149,6 +149,8 @@ int write_cache_pages(struct address_space *mapping,
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 void set_page_dirty_balance(struct page *page, int page_mkwrite);
 void writeback_set_ratelimit(void);
+void tag_pages_for_writeback(struct address_space *mapping,
+			     pgoff_t start, pgoff_t end);
 
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl

commit 9e38d86ff2d8a8db99570e982230861046df32b5
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 23 06:55:17 2010 -0400

    fs: Implement lazy LRU updates for inodes
    
    Convert the inode LRU to use lazy updates to reduce lock and
    cacheline traffic.  We avoid moving inodes around in the LRU list
    during iget/iput operations so these frequent operations don't need
    to access the LRUs. Instead, we defer the refcount checks to
    reclaim-time and use a per-inode state flag, I_REFERENCED, to tell
    reclaim that iget has touched the inode in the past. This means that
    only reclaim should be touching the LRU with any frequency, hence
    significantly reducing lock acquisitions and the amount contention
    on LRU updates.
    
    This also removes the inode_in_use list, which means we now only
    have one list for tracking the inode LRU status. This makes it much
    simpler to split out the LRU list operations under it's own lock.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 72a5d647a5f2..242b6f812ba6 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -10,8 +10,6 @@
 struct backing_dev_info;
 
 extern spinlock_t inode_lock;
-extern struct list_head inode_in_use;
-extern struct list_head inode_unused;
 
 /*
  * fs/fs-writeback.c

commit 16c4042f08919f447d6b2a55679546c9b97c7264
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Aug 11 14:17:39 2010 -0700

    writeback: avoid unnecessary calculation of bdi dirty thresholds
    
    Split get_dirty_limits() into global_dirty_limits()+bdi_dirty_limit(), so
    that the latter can be avoided when under global dirty background
    threshold (which is the normal state for most systems).
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c24eca71e80c..72a5d647a5f2 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -124,8 +124,9 @@ struct ctl_table;
 int dirty_writeback_centisecs_handler(struct ctl_table *, int,
 				      void __user *, size_t *, loff_t *);
 
-void get_dirty_limits(unsigned long *pbackground, unsigned long *pdirty,
-		      unsigned long *pbdi_dirty, struct backing_dev_info *bdi);
+void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty);
+unsigned long bdi_dirty_limit(struct backing_dev_info *bdi,
+			       unsigned long dirty);
 
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,

commit edadfb10ba35da7253541e4155aa92eff758ebe6
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 10 12:07:54 2010 +0200

    writeback: split writeback_inodes_wb
    
    The case where we have a superblock doesn't require a loop here as we scan
    over all inodes in writeback_sb_inodes. Split it out into a separate helper
    to make the code simpler.  This also allows to get rid of the sb member in
    struct writeback_control, which was rather out of place there.
    
    Also update the comments in writeback_sb_inodes that explain the handling
    of inodes from wrong superblocks.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f6756f6a610c..c24eca71e80c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -27,8 +27,6 @@ enum writeback_sync_modes {
  * in a manner such that unspecified fields are set to zero.
  */
 struct writeback_control {
-	struct super_block *sb;		/* if !NULL, only write inodes from
-					   this super_block */
 	enum writeback_sync_modes sync_mode;
 	unsigned long *older_than_this;	/* If !NULL, only write back inodes
 					   older than this */

commit 9c3a8ee8a1d72c5c0d7fbdf426d80e270ddfa54c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 10 12:07:27 2010 +0200

    writeback: remove writeback_inodes_wbc
    
    This was just an odd wrapper around writeback_inodes_wb.  Removing this
    also allows to get rid of the bdi member of struct writeback_control
    which was rather out of place there.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d63ef8f9609f..f6756f6a610c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -27,8 +27,6 @@ enum writeback_sync_modes {
  * in a manner such that unspecified fields are set to zero.
  */
 struct writeback_control {
-	struct backing_dev_info *bdi;	/* If !NULL, only write back this
-					   queue */
 	struct super_block *sb;		/* if !NULL, only write inodes from
 					   this super_block */
 	enum writeback_sync_modes sync_mode;
@@ -66,7 +64,8 @@ int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *);
 int writeback_inodes_sb_if_idle(struct super_block *);
 void sync_inodes_sb(struct super_block *);
-void writeback_inodes_wbc(struct writeback_control *wbc);
+void writeback_inodes_wb(struct bdi_writeback *wb,
+		struct writeback_control *wbc);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
 void wakeup_flusher_threads(long nr_pages);
 

commit 0b5649278e39a068aaf91399941bab1b4a4a3cc2
Author: Dave Chinner <dchinner@redhat.com>
Date:   Wed Jun 9 10:37:18 2010 +1000

    writeback: pay attention to wbc->nr_to_write in write_cache_pages
    
    If a filesystem writes more than one page in ->writepage, write_cache_pages
    fails to notice this and continues to attempt writeback when wbc->nr_to_write
    has gone negative - this trace was captured from XFS:
    
        wbc_writeback_start: towrt=1024
        wbc_writepage: towrt=1024
        wbc_writepage: towrt=0
        wbc_writepage: towrt=-1
        wbc_writepage: towrt=-5
        wbc_writepage: towrt=-21
        wbc_writepage: towrt=-85
    
    This has adverse effects on filesystem writeback behaviour. write_cache_pages()
    needs to terminate after a certain number of pages are written, not after a
    certain number of calls to ->writepage are made.  This is a regression
    introduced by 17bc6c30cf6bfffd816bdc53682dd46fc34a2cf4 ("vfs: Add
    no_nrwrite_index_update writeback control flag"), but cannot be reverted
    directly due to subsequent bug fixes that have gone in on top of it.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f64134653a8c..d63ef8f9609f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -56,15 +56,6 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */
-	/*
-	 * write_cache_pages() won't update wbc->nr_to_write and
-	 * mapping->writeback_index if no_nrwrite_index_update
-	 * is set.  write_cache_pages() may write more than we
-	 * requested and we want to make sure nr_to_write and
-	 * writeback_index are updated in a consistent manner
-	 * so we use a single control to update them
-	 */
-	unsigned no_nrwrite_index_update:1;
 };
 
 /*

commit 0e3c9a2284f5417f196e327c254d0b84c9ee8929
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Jun 1 11:08:43 2010 +0200

    Revert "writeback: fix WB_SYNC_NONE writeback from umount"
    
    This reverts commit e913fc825dc685a444cb4c1d0f9d32f372f59861.
    
    We are investigating a hang associated with the WB_SYNC_NONE changes,
    so revert them for now.
    
    Conflicts:
    
            fs/fs-writeback.c
            mm/page-writeback.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index cc97d6caf2b3..f64134653a8c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -65,15 +65,6 @@ struct writeback_control {
 	 * so we use a single control to update them
 	 */
 	unsigned no_nrwrite_index_update:1;
-
-	/*
-	 * For WB_SYNC_ALL, the sb must always be pinned. For WB_SYNC_NONE,
-	 * the writeback code will pin the sb for the caller. However,
-	 * for eg umount, the caller does WB_SYNC_NONE but already has
-	 * the sb pinned. If the below is set, caller already has the
-	 * sb pinned.
-	 */
-	unsigned sb_pinned:1;
 };
 
 /*
@@ -82,7 +73,6 @@ struct writeback_control {
 struct bdi_writeback;
 int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *);
-void writeback_inodes_sb_locked(struct super_block *);
 int writeback_inodes_sb_if_idle(struct super_block *);
 void sync_inodes_sb(struct super_block *);
 void writeback_inodes_wbc(struct writeback_control *wbc);

commit c2c4986eddaa7dc3d036cb2bfa5c8c5f1f2492a0
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu May 20 09:18:47 2010 +0200

    writeback: fix problem with !CONFIG_BLOCK compilation
    
    When CONFIG_BLOCK isn't enabled:
    
    mm/page-writeback.c: In function 'laptop_mode_timer_fn':
    mm/page-writeback.c:708: error: dereferencing pointer to incomplete type
    mm/page-writeback.c:709: error: dereferencing pointer to incomplete type
    
    Fix this by essentially eliminating the laptop sync handlers when
    CONFIG_BLOCK isn't set, as most are only used from the block layer code.
    The exception is laptop_sync_completion() which is used from sys_sync(),
    make that an empty declaration in that case.
    
    Reported-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 47e1c686cb02..cc97d6caf2b3 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -106,10 +106,14 @@ static inline void inode_sync_wait(struct inode *inode)
 /*
  * mm/page-writeback.c
  */
+#ifdef CONFIG_BLOCK
 void laptop_io_completion(struct backing_dev_info *info);
 void laptop_sync_completion(void);
 void laptop_mode_sync(struct work_struct *work);
 void laptop_mode_timer_fn(unsigned long data);
+#else
+static inline void laptop_sync_completion(void) { }
+#endif
 void throttle_vm_writeout(gfp_t gfp_mask);
 
 /* These are exported to sysctl. */

commit e913fc825dc685a444cb4c1d0f9d32f372f59861
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon May 17 12:55:07 2010 +0200

    writeback: fix WB_SYNC_NONE writeback from umount
    
    When umount calls sync_filesystem(), we first do a WB_SYNC_NONE
    writeback to kick off writeback of pending dirty inodes, then follow
    that up with a WB_SYNC_ALL to wait for it. Since umount already holds
    the sb s_umount mutex, WB_SYNC_NONE ends up doing nothing and all
    writeback happens as WB_SYNC_ALL. This can greatly slow down umount,
    since WB_SYNC_ALL writeback is a data integrity operation and thus
    a bigger hammer than simple WB_SYNC_NONE. For barrier aware file systems
    it's a lot slower.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index eb38a2c645f6..47e1c686cb02 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -65,6 +65,15 @@ struct writeback_control {
 	 * so we use a single control to update them
 	 */
 	unsigned no_nrwrite_index_update:1;
+
+	/*
+	 * For WB_SYNC_ALL, the sb must always be pinned. For WB_SYNC_NONE,
+	 * the writeback code will pin the sb for the caller. However,
+	 * for eg umount, the caller does WB_SYNC_NONE but already has
+	 * the sb pinned. If the below is set, caller already has the
+	 * sb pinned.
+	 */
+	unsigned sb_pinned:1;
 };
 
 /*
@@ -73,6 +82,7 @@ struct writeback_control {
 struct bdi_writeback;
 int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *);
+void writeback_inodes_sb_locked(struct super_block *);
 int writeback_inodes_sb_if_idle(struct super_block *);
 void sync_inodes_sb(struct super_block *);
 void writeback_inodes_wbc(struct writeback_control *wbc);

commit 31373d09da5b7fe21fe6f781e92bd534a3495f00
Author: Matthew Garrett <mjg@redhat.com>
Date:   Tue Apr 6 14:25:14 2010 +0200

    laptop-mode: Make flushes per-device
    
    One of the features of laptop-mode is that it forces a writeout of dirty
    pages if something else triggers a physical read or write from a device.
    The current implementation flushes pages on all devices, rather than only
    the one that triggered the flush. This patch alters the behaviour so that
    only the recently accessed block device is flushed, preventing other
    disks being spun up for no terribly good reason.
    
    Signed-off-by: Matthew Garrett <mjg@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 36520ded3e06..eb38a2c645f6 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -96,8 +96,10 @@ static inline void inode_sync_wait(struct inode *inode)
 /*
  * mm/page-writeback.c
  */
-void laptop_io_completion(void);
+void laptop_io_completion(struct backing_dev_info *info);
 void laptop_sync_completion(void);
+void laptop_mode_sync(struct work_struct *work);
+void laptop_mode_timer_fn(unsigned long data);
 void throttle_vm_writeout(gfp_t gfp_mask);
 
 /* These are exported to sysctl. */

commit f11c9c5c259cb2c3d698548dc3936f773ab1f5b9
Author: Edward Shishkin <edward.shishkin@gmail.com>
Date:   Thu Mar 11 14:09:47 2010 -0800

    vfs: improve writeback_inodes_wb()
    
    Do not pin/unpin superblock for every inode in writeback_inodes_wb(), pin
    it for the whole group of inodes which belong to the same superblock and
    call writeback_sb_inodes() handler for them.
    
    Signed-off-by: Edward Shishkin <edward.shishkin@gmail.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 76e8903cd204..36520ded3e06 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -34,6 +34,9 @@ struct writeback_control {
 	enum writeback_sync_modes sync_mode;
 	unsigned long *older_than_this;	/* If !NULL, only write back inodes
 					   older than this */
+	unsigned long wb_start;         /* Time writeback_inodes_wb was
+					   called. This is needed to avoid
+					   extra jobs and livelock */
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
 	long pages_skipped;		/* Pages which were not written */

commit 17bd55d037a02b04d9119511cfd1a4b985d20f63
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Wed Dec 23 07:57:07 2009 -0500

    fs-writeback: Add helper function to start writeback if idle
    
    ext4, at least, would like to start pushing on writeback if it starts
    to get close to ENOSPC when reserving worst-case blocks for delalloc
    writes.  Writing out delalloc data will convert those worst-case
    predictions into usually smaller actual usage, freeing up space
    before we hit ENOSPC based on this speculation.
    
    Thanks to Jens for the suggestion for the helper function,
    & the naming help.
    
    I've made the helper return status on whether writeback was
    started even though I don't plan to use it in the ext4 patch;
    it seems like it would be potentially useful to test this
    in some cases.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Acked-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c18c008f4bbf..76e8903cd204 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -70,6 +70,7 @@ struct writeback_control {
 struct bdi_writeback;
 int inode_wait(void *);
 void writeback_inodes_sb(struct super_block *);
+int writeback_inodes_sb_if_idle(struct super_block *);
 void sync_inodes_sb(struct super_block *);
 void writeback_inodes_wbc(struct writeback_control *wbc);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);

commit eaff8079d4f1016a12e34ab323737314f24127dd
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 17 14:25:01 2009 +0100

    kill I_LOCK
    
    After I_SYNC was split from I_LOCK the leftover is always used together with
    I_NEW and thus superflous.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 705f01fe413a..c18c008f4bbf 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -79,8 +79,7 @@ void wakeup_flusher_threads(long nr_pages);
 static inline void wait_on_inode(struct inode *inode)
 {
 	might_sleep();
-	wait_on_bit(&inode->i_state, __I_LOCK, inode_wait,
-							TASK_UNINTERRUPTIBLE);
+	wait_on_bit(&inode->i_state, __I_NEW, inode_wait, TASK_UNINTERRUPTIBLE);
 }
 static inline void inode_sync_wait(struct inode *inode)
 {

commit b17621fed6aa039387e35f9b4d34d98f213e5673
Author: Wu Fengguang <fengguang.wu@gmail.com>
Date:   Thu Dec 3 13:54:25 2009 +0100

    writeback: introduce wbc.for_background
    
    It will lower the flush priority for NFS, and maybe more in future.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 66ebddcff664..705f01fe413a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -49,6 +49,7 @@ struct writeback_control {
 	unsigned nonblocking:1;		/* Don't get stuck on request queues */
 	unsigned encountered_congestion:1; /* An output: a queue is full */
 	unsigned for_kupdate:1;		/* A kupdate writeback */
+	unsigned for_background:1;	/* A background writeback */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 75cf58666ff9..66ebddcff664 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -110,21 +110,20 @@ extern int laptop_mode;
 extern unsigned long determine_dirtyable_memory(void);
 
 extern int dirty_background_ratio_handler(struct ctl_table *table, int write,
-		struct file *filp, void __user *buffer, size_t *lenp,
+		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 extern int dirty_background_bytes_handler(struct ctl_table *table, int write,
-		struct file *filp, void __user *buffer, size_t *lenp,
+		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 extern int dirty_ratio_handler(struct ctl_table *table, int write,
-		struct file *filp, void __user *buffer, size_t *lenp,
+		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 extern int dirty_bytes_handler(struct ctl_table *table, int write,
-		struct file *filp, void __user *buffer, size_t *lenp,
+		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
 struct ctl_table;
-struct file;
-int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,
+int dirty_writeback_centisecs_handler(struct ctl_table *, int,
 				      void __user *, size_t *, loff_t *);
 
 void get_dirty_limits(unsigned long *pbackground, unsigned long *pdirty,

commit b6e51316daede0633e9274e1e30391cfa4747877
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Sep 16 15:13:54 2009 +0200

    writeback: separate starting of sync vs opportunistic writeback
    
    bdi_start_writeback() is currently split into two paths, one for
    WB_SYNC_NONE and one for WB_SYNC_ALL. Add bdi_sync_writeback()
    for WB_SYNC_ALL writeback and let bdi_start_writeback() handle
    only WB_SYNC_NONE.
    
    Push down the writeback_control allocation and only accept the
    parameters that make sense for each function. This cleans up
    the API considerably.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 48a054e2b716..75cf58666ff9 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -68,8 +68,8 @@ struct writeback_control {
  */	
 struct bdi_writeback;
 int inode_wait(void *);
-long writeback_inodes_sb(struct super_block *);
-long sync_inodes_sb(struct super_block *);
+void writeback_inodes_sb(struct super_block *);
+void sync_inodes_sb(struct super_block *);
 void writeback_inodes_wbc(struct writeback_control *wbc);
 long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
 void wakeup_flusher_threads(long nr_pages);

commit 1fe06ad89255c211fe100d7f690d10b161398df8
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Sep 15 15:10:20 2009 +0200

    writeback: get rid of wbc->for_writepages
    
    It's only set, it's never checked. Kill it.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d347632f1861..48a054e2b716 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -50,7 +50,6 @@ struct writeback_control {
 	unsigned encountered_congestion:1; /* An output: a queue is full */
 	unsigned for_kupdate:1;		/* A kupdate writeback */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
-	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */
 	/*

commit 18f2ee705d98034b0f229a3202d827468d4bffd9
Author: Jan Kara <jack@suse.cz>
Date:   Tue Aug 18 18:43:15 2009 +0200

    vfs: Remove generic_osync_inode() and sync_page_range{_nolock}()
    
    Remove these three functions since nobody uses them anymore.
    
    Signed-off-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 78b1e4684cc9..d347632f1861 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -150,10 +150,6 @@ int write_cache_pages(struct address_space *mapping,
 		      struct writeback_control *wbc, writepage_t writepage,
 		      void *data);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
-int sync_page_range(struct inode *inode, struct address_space *mapping,
-			loff_t pos, loff_t count);
-int sync_page_range_nolock(struct inode *inode, struct address_space *mapping,
-			   loff_t pos, loff_t count);
 void set_page_dirty_balance(struct page *page, int page_mkwrite);
 void writeback_set_ratelimit(void);
 

commit d0bceac747b547c0b4769b91fec7d3c15600153f
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon May 18 08:20:32 2009 +0200

    writeback: get rid of pdflush completely
    
    It is now unused, so kill it off.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index cef75527a14c..78b1e4684cc9 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -13,17 +13,6 @@ extern spinlock_t inode_lock;
 extern struct list_head inode_in_use;
 extern struct list_head inode_unused;
 
-/*
- * Yes, writeback.h requires sched.h
- * No, sched.h is not included from here.
- */
-static inline int task_is_pdflush(struct task_struct *task)
-{
-	return task->flags & PF_FLUSHER;
-}
-
-#define current_is_pdflush()	task_is_pdflush(current)
-
 /*
  * fs/fs-writeback.c
  */
@@ -155,7 +144,6 @@ balance_dirty_pages_ratelimited(struct address_space *mapping)
 typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
 				void *data);
 
-int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
 int generic_writepages(struct address_space *mapping,
 		       struct writeback_control *wbc);
 int write_cache_pages(struct address_space *mapping,

commit 03ba3782e8dcc5b0e1efe440d33084f066e38cae
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Sep 9 09:08:54 2009 +0200

    writeback: switch to per-bdi threads for flushing data
    
    This gets rid of pdflush for bdi writeout and kupdated style cleaning.
    pdflush writeout suffers from lack of locality and also requires more
    threads to handle the same workload, since it has to work in a
    non-blocking fashion against each queue. This also introduces lumpy
    behaviour and potential request starvation, since pdflush can be starved
    for queue access if others are accessing it. A sample ffsb workload that
    does random writes to files is about 8% faster here on a simple SATA drive
    during the benchmark phase. File layout also seems a LOT more smooth in
    vmstat:
    
     r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
     0  1      0 608848   2652 375372    0    0     0 71024  604    24  1 10 48 42
     0  1      0 549644   2712 433736    0    0     0 60692  505    27  1  8 48 44
     1  0      0 476928   2784 505192    0    0     4 29540  553    24  0  9 53 37
     0  1      0 457972   2808 524008    0    0     0 54876  331    16  0  4 38 58
     0  1      0 366128   2928 614284    0    0     4 92168  710    58  0 13 53 34
     0  1      0 295092   3000 684140    0    0     0 62924  572    23  0  9 53 37
     0  1      0 236592   3064 741704    0    0     4 58256  523    17  0  8 48 44
     0  1      0 165608   3132 811464    0    0     0 57460  560    21  0  8 54 38
     0  1      0 102952   3200 873164    0    0     4 74748  540    29  1 10 48 41
     0  1      0  48604   3252 926472    0    0     0 53248  469    29  0  7 47 45
    
    where vanilla tends to fluctuate a lot in the creation phase:
    
     r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
     1  1      0 678716   5792 303380    0    0     0 74064  565    50  1 11 52 36
     1  0      0 662488   5864 319396    0    0     4   352  302   329  0  2 47 51
     0  1      0 599312   5924 381468    0    0     0 78164  516    55  0  9 51 40
     0  1      0 519952   6008 459516    0    0     4 78156  622    56  1 11 52 37
     1  1      0 436640   6092 541632    0    0     0 82244  622    54  0 11 48 41
     0  1      0 436640   6092 541660    0    0     0     8  152    39  0  0 51 49
     0  1      0 332224   6200 644252    0    0     4 102800  728    46  1 13 49 36
     1  0      0 274492   6260 701056    0    0     4 12328  459    49  0  7 50 43
     0  1      0 211220   6324 763356    0    0     0 106940  515    37  1 10 51 39
     1  0      0 160412   6376 813468    0    0     0  8224  415    43  0  6 49 45
     1  1      0  85980   6452 886556    0    0     4 113516  575    39  1 11 54 34
     0  2      0  85968   6452 886620    0    0     0  1640  158   211  0  0 46 54
    
    A 10 disk test with btrfs performs 26% faster with per-bdi flushing. A
    SSD based writeback test on XFS performs over 20% better as well, with
    the throughput being very stable around 1GB/sec, where pdflush only
    manages 750MB/sec and fluctuates wildly while doing so. Random buffered
    writes to many files behave a lot better as well, as does random mmap'ed
    writes.
    
    A separate thread is added to sync the super blocks. In the long term,
    adding sync_supers_bdi() functionality could get rid of this thread again.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 07039299603d..cef75527a14c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -40,6 +40,8 @@ enum writeback_sync_modes {
 struct writeback_control {
 	struct backing_dev_info *bdi;	/* If !NULL, only write back this
 					   queue */
+	struct super_block *sb;		/* if !NULL, only write inodes from
+					   this super_block */
 	enum writeback_sync_modes sync_mode;
 	unsigned long *older_than_this;	/* If !NULL, only write back inodes
 					   older than this */
@@ -76,10 +78,13 @@ struct writeback_control {
 /*
  * fs/fs-writeback.c
  */	
-void writeback_inodes(struct writeback_control *wbc);
+struct bdi_writeback;
 int inode_wait(void *);
 long writeback_inodes_sb(struct super_block *);
 long sync_inodes_sb(struct super_block *);
+void writeback_inodes_wbc(struct writeback_control *wbc);
+long wb_do_writeback(struct bdi_writeback *wb, int force_wait);
+void wakeup_flusher_threads(long nr_pages);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)
@@ -99,7 +104,6 @@ static inline void inode_sync_wait(struct inode *inode)
 /*
  * mm/page-writeback.c
  */
-int wakeup_pdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
 void throttle_vm_writeout(gfp_t gfp_mask);

commit d8a8559cd7a9ccac98d5f6f13297a2ff68a43627
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Sep 2 12:34:32 2009 +0200

    writeback: get rid of generic_sync_sb_inodes() export
    
    This adds two new exported functions:
    
    - writeback_inodes_sb(), which only attempts to writeback dirty inodes on
      this super_block, for WB_SYNC_NONE writeout.
    - sync_inodes_sb(), which writes out all dirty inodes on this super_block
      and also waits for the IO to complete.
    
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 3224820c8514..07039299603d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -78,7 +78,8 @@ struct writeback_control {
  */	
 void writeback_inodes(struct writeback_control *wbc);
 int inode_wait(void *);
-void sync_inodes_sb(struct super_block *, int wait);
+long writeback_inodes_sb(struct super_block *);
+long sync_inodes_sb(struct super_block *);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)

commit 5cee5815d1564bbbd505fea86f4550f1efdb5cd0
Author: Jan Kara <jack@suse.cz>
Date:   Mon Apr 27 16:43:51 2009 +0200

    vfs: Make sys_sync() use fsync_super() (version 4)
    
    It is unnecessarily fragile to have two places (fsync_super() and do_sync())
    doing data integrity sync of the filesystem. Alter __fsync_super() to
    accommodate needs of both callers and use it. So after this patch
    __fsync_super() is the only place where we gather all the calls needed to
    properly send all data on a filesystem to disk.
    
    Nice bonus is that we get a complete livelock avoidance and write_supers()
    is now only used for periodic writeback of superblocks.
    
    sync_blockdevs() introduced a couple of patches ago is gone now.
    
    [build fixes folded]
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 93445477f86a..3224820c8514 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -79,7 +79,6 @@ struct writeback_control {
 void writeback_inodes(struct writeback_control *wbc);
 int inode_wait(void *);
 void sync_inodes_sb(struct super_block *, int wait);
-void sync_inodes(int wait);
 
 /* writeback.h requires fs.h; it, too, is not included from here. */
 static inline void wait_on_inode(struct inode *inode)

commit cd17cbfda004fe5f406c01b318c6378d9895896f
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 15 11:32:24 2009 +0200

    Revert "mm: add /proc controls for pdflush threads"
    
    This reverts commit fafd688e4c0c34da0f3de909881117d374e4c7af.
    
    Work is progressing to switch away from pdflush as the process backing
    for flushing out dirty data. So it seems pointless to add more knobs
    to control pdflush threads. The original author of the patch did not
    have any specific use cases for adding the knobs, so we can easily
    revert this before 2.6.30 to avoid having to maintain this API
    forever.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 9c1ed1fb6ddb..93445477f86a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -168,8 +168,6 @@ void writeback_set_ratelimit(void);
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl
 				   read-only. */
-extern int nr_pdflush_threads_max; /* Global so it can be exported to sysctl */
-extern int nr_pdflush_threads_min; /* Global so it can be exported to sysctl */
 
 
 #endif		/* WRITEBACK_H */

commit fafd688e4c0c34da0f3de909881117d374e4c7af
Author: Peter W Morreale <pmorreale@novell.com>
Date:   Mon Apr 6 19:00:29 2009 -0700

    mm: add /proc controls for pdflush threads
    
    Add /proc entries to give the admin the ability to control the minimum and
    maximum number of pdflush threads.  This allows finer control of pdflush
    on both large and small machines.
    
    The rationale is simply one size does not fit all.  Admins on large and/or
    small systems may want to tune the min/max pdflush thread count to best
    suit their needs.  Right now the min/max is hardcoded to 2/8.  While
    probably a fair estimate for smaller machines, large machines with large
    numbers of CPUs and large numbers of filesystems/block devices may benefit
    from larger numbers of threads working on different block devices.
    
    Even if the background flushing algorithm is radically changed, it is
    still likely that multiple threads will be involved and admins would still
    desire finer control on the min/max other than to have to recompile the
    kernel.
    
    The patch adds '/proc/sys/vm/nr_pdflush_threads_min' and
    '/proc/sys/vm/nr_pdflush_threads_max' with r/w permissions.
    
    The minimum value for nr_pdflush_threads_min is 1 and the maximum value is
    the current value of nr_pdflush_threads_max.  This minimum is required
    since additional thread creation is performed in a pdflush thread itself.
    
    The minimum value for nr_pdflush_threads_max is the current value of
    nr_pdflush_threads_min and the maximum value can be 1000.
    
    Documentation/sysctl/vm.txt is also updated.
    
    [akpm@linux-foundation.org: fix comment, fix whitespace, use __read_mostly]
    Signed-off-by: Peter W Morreale <pmorreale@novell.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 93445477f86a..9c1ed1fb6ddb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -168,6 +168,8 @@ void writeback_set_ratelimit(void);
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl
 				   read-only. */
+extern int nr_pdflush_threads_max; /* Global so it can be exported to sysctl */
+extern int nr_pdflush_threads_min; /* Global so it can be exported to sysctl */
 
 
 #endif		/* WRITEBACK_H */

commit 704503d836042d4a4c7685b7036e7de0418fbc0f
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Mar 31 15:23:18 2009 -0700

    mm: fix proc_dointvec_userhz_jiffies "breakage"
    
    Addresses http://bugzilla.kernel.org/show_bug.cgi?id=9838
    
    On i386, HZ=1000, jiffies_to_clock_t() converts time in a somewhat strange
    way from the user's point of view:
    
            # echo 500 >/proc/sys/vm/dirty_writeback_centisecs
            # cat /proc/sys/vm/dirty_writeback_centisecs
            499
    
    So, we have 5000 jiffies converted to only 499 clock ticks and reported
    back.
    
    TICK_NSEC = 999848
    ACTHZ = 256039
    
    Keeping in-kernel variable in units passed from userspace will fix issue
    of course, but this probably won't be right for every sysctl.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 7300ecdc480c..93445477f86a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -109,8 +109,8 @@ extern int dirty_background_ratio;
 extern unsigned long dirty_background_bytes;
 extern int vm_dirty_ratio;
 extern unsigned long vm_dirty_bytes;
-extern int dirty_writeback_interval;
-extern int dirty_expire_interval;
+extern unsigned int dirty_writeback_interval;
+extern unsigned int dirty_expire_interval;
 extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;

commit 4f5a99d64c17470a784a6c68064207d82e3e74a5
Author: Nick Piggin <npiggin@suse.de>
Date:   Tue Jan 6 14:40:25 2009 -0800

    fs: remove WB_SYNC_HOLD
    
    Remove WB_SYNC_HOLD.  The primary motiviation is the design of my
    anti-starvation code for fsync.  It requires taking an inode lock over the
    sync operation, so we could run into lock ordering problems with multiple
    inodes.  It is possible to take a single global lock to solve the ordering
    problem, but then that would prevent a future nice implementation of "sync
    multiple inodes" based on lock order via inode address.
    
    Seems like a backward step to remove this, but actually it is busted
    anyway: we can't use the inode lists for data integrity wait: an inode can
    be taken off the dirty lists but still be under writeback.  In order to
    satisfy data integrity semantics, we should wait for it to finish
    writeback, but if we only search the dirty lists, we'll miss it.
    
    It would be possible to have a "writeback" list, for sys_sync, I suppose.
    But why complicate things by prematurely optimise?  For unmounting, we
    could avoid the "livelock avoidance" code, which would be easier, but
    again premature IMO.
    
    Fixing the existing data integrity problem will come next.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index bb28c975c1d7..7300ecdc480c 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -30,7 +30,6 @@ static inline int task_is_pdflush(struct task_struct *task)
 enum writeback_sync_modes {
 	WB_SYNC_NONE,	/* Don't wait on anything */
 	WB_SYNC_ALL,	/* Wait on every mapping */
-	WB_SYNC_HOLD,	/* Hold the inode on sb_dirty for sys_sync() */
 };
 
 /*

commit 2da02997e08d3efe8174c7a47696e6f7cbe69ba9
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jan 6 14:39:31 2009 -0800

    mm: add dirty_background_bytes and dirty_bytes sysctls
    
    This change introduces two new sysctls to /proc/sys/vm:
    dirty_background_bytes and dirty_bytes.
    
    dirty_background_bytes is the counterpart to dirty_background_ratio and
    dirty_bytes is the counterpart to dirty_ratio.
    
    With growing memory capacities of individual machines, it's no longer
    sufficient to specify dirty thresholds as a percentage of the amount of
    dirtyable memory over the entire system.
    
    dirty_background_bytes and dirty_bytes specify quantities of memory, in
    bytes, that represent the dirty limits for the entire system.  If either
    of these values is set, its value represents the amount of dirty memory
    that is needed to commence either background or direct writeback.
    
    When a `bytes' or `ratio' file is written, its counterpart becomes a
    function of the written value.  For example, if dirty_bytes is written to
    be 8096, 8K of memory is required to commence direct writeback.
    dirty_ratio is then functionally equivalent to 8K / the amount of
    dirtyable memory:
    
            dirtyable_memory = free pages + mapped pages + file cache
    
            dirty_background_bytes = dirty_background_ratio * dirtyable_memory
                    -or-
            dirty_background_ratio = dirty_background_bytes / dirtyable_memory
    
                    AND
    
            dirty_bytes = dirty_ratio * dirtyable_memory
                    -or-
            dirty_ratio = dirty_bytes / dirtyable_memory
    
    Only one of dirty_background_bytes and dirty_background_ratio may be
    specified at a time, and only one of dirty_bytes and dirty_ratio may be
    specified.  When one sysctl is written, the other appears as 0 when read.
    
    The `bytes' files operate on a page size granularity since dirty limits
    are compared with ZVC values, which are in page units.
    
    Prior to this change, the minimum dirty_ratio was 5 as implemented by
    get_dirty_limits() although /proc/sys/vm/dirty_ratio would show any user
    written value between 0 and 100.  This restriction is maintained, but
    dirty_bytes has a lower limit of only one page.
    
    Also prior to this change, the dirty_background_ratio could not equal or
    exceed dirty_ratio.  This restriction is maintained in addition to
    restricting dirty_background_bytes.  If either background threshold equals
    or exceeds that of the dirty threshold, it is implicitly set to half the
    dirty threshold.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 259e9ea58cab..bb28c975c1d7 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -107,7 +107,9 @@ void throttle_vm_writeout(gfp_t gfp_mask);
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;
+extern unsigned long dirty_background_bytes;
 extern int vm_dirty_ratio;
+extern unsigned long vm_dirty_bytes;
 extern int dirty_writeback_interval;
 extern int dirty_expire_interval;
 extern int vm_highmem_is_dirtyable;
@@ -116,9 +118,18 @@ extern int laptop_mode;
 
 extern unsigned long determine_dirtyable_memory(void);
 
+extern int dirty_background_ratio_handler(struct ctl_table *table, int write,
+		struct file *filp, void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+extern int dirty_background_bytes_handler(struct ctl_table *table, int write,
+		struct file *filp, void __user *buffer, size_t *lenp,
+		loff_t *ppos);
 extern int dirty_ratio_handler(struct ctl_table *table, int write,
 		struct file *filp, void __user *buffer, size_t *lenp,
 		loff_t *ppos);
+extern int dirty_bytes_handler(struct ctl_table *table, int write,
+		struct file *filp, void __user *buffer, size_t *lenp,
+		loff_t *ppos);
 
 struct ctl_table;
 struct file;

commit 364aeb2849789b51bf4b9af2ddd02fee7285c54e
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jan 6 14:39:29 2009 -0800

    mm: change dirty limit type specifiers to unsigned long
    
    The background dirty and dirty limits are better defined with type
    specifiers of unsigned long since negative writeback thresholds are not
    possible.
    
    These values, as returned by get_dirty_limits(), are normally compared
    with ZVC values to determine whether writeback shall commence or be
    throttled.  Such page counts cannot be negative, so declaring the page
    limits as signed is unnecessary.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index e585657e9831..259e9ea58cab 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -125,8 +125,8 @@ struct file;
 int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,
 				      void __user *, size_t *, loff_t *);
 
-void get_dirty_limits(long *pbackground, long *pdirty, long *pbdi_dirty,
-		 struct backing_dev_info *bdi);
+void get_dirty_limits(unsigned long *pbackground, unsigned long *pdirty,
+		      unsigned long *pbdi_dirty, struct backing_dev_info *bdi);
 
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,

commit 17bc6c30cf6bfffd816bdc53682dd46fc34a2cf4
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Oct 16 10:09:17 2008 -0400

    vfs: Add no_nrwrite_index_update writeback control flag
    
    If no_nrwrite_index_update is set we don't update nr_to_write and
    address space writeback_index in write_cache_pages.  This change
    enables a file system to skip these updates in write_cache_pages and do
    them in the writepages() callback.  This patch will be followed by an
    ext4 patch that make use of these new flags.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    CC: linux-fsdevel@vger.kernel.org

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index bd91987c065f..e585657e9831 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -63,6 +63,15 @@ struct writeback_control {
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */
+	/*
+	 * write_cache_pages() won't update wbc->nr_to_write and
+	 * mapping->writeback_index if no_nrwrite_index_update
+	 * is set.  write_cache_pages() may write more than we
+	 * requested and we want to make sure nr_to_write and
+	 * writeback_index are updated in a consistent manner
+	 * so we use a single control to update them
+	 */
+	unsigned no_nrwrite_index_update:1;
 };
 
 /*

commit 74baaaaec8b4f22e1ae279f5ecca4ff705b28912
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Oct 14 09:21:02 2008 -0400

    vfs: Remove the range_cont writeback mode.
    
    Ext4 was the only user of range_cont writeback mode and ext4 switched
    to a different method. So remove the range_cont mode which is not used
    in the kernel.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    CC: linux-fsdevel@vger.kernel.org

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 12b15c561a1f..bd91987c065f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -63,7 +63,6 @@ struct writeback_control {
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */
-	unsigned range_cont:1;
 };
 
 /*

commit 8d2567a620ae8c24968a2bdc1c906c724fac1f6a
Merge: bcf559e385ba 49f1487b2e41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 15 08:36:38 2008 -0700

    Merge branch 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4
    
    * 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4: (61 commits)
      ext4: Documention update for new ordered mode and delayed allocation
      ext4: do not set extents feature from the kernel
      ext4: Don't allow nonextenst mount option for large filesystem
      ext4: Enable delalloc by default.
      ext4: delayed allocation i_blocks fix for stat
      ext4: fix delalloc i_disksize early update issue
      ext4: Handle page without buffers in ext4_*_writepage()
      ext4: Add ordered mode support for delalloc
      ext4: Invert lock ordering of page_lock and transaction start in delalloc
      mm: Add range_cont mode for writeback
      ext4: delayed allocation ENOSPC handling
      percpu_counter: new function percpu_counter_sum_and_set
      ext4: Add delayed allocation support in data=writeback mode
      vfs: add hooks for ext4's delayed allocation support
      jbd2: Remove data=ordered mode support using jbd buffer heads
      ext4: Use new framework for data=ordered mode in JBD2
      jbd2: Implement data=ordered mode handling via inodes
      vfs: export filemap_fdatawrite_range()
      ext4: Fix lock inversion in ext4_ext_truncate()
      ext4: Invert the locking order of page_lock and transaction start
      ...

commit 06d6cf6959d22037fcec598f4f954db5db3d7356
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Jul 11 19:27:31 2008 -0400

    mm: Add range_cont mode for writeback
    
    Filesystems like ext4 needs to start a new transaction in
    the writepages for block allocation. This happens with delayed
    allocation and there is limit to how many credits we can request
    from the journal layer. So we call write_cache_pages multiple
    times with wbc->nr_to_write set to the maximum possible value
    limitted by the max journal credits available.
    
    Add a new mode to writeback that enables us to handle this
    behaviour. In the new mode we update the wbc->range_start
    to point to the new offset to be written. Next call to
    call to write_cache_pages will start writeout from specified
    range_start offset. In the new mode we also limit writing
    to the specified wbc->range_end.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Mingming Cao <cmm@us.ibm.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f462439cc288..0d8573e6b9ec 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -63,6 +63,7 @@ struct writeback_control {
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned more_io:1;		/* more io to be dispatched */
+	unsigned range_cont:1;
 };
 
 /*

commit 3eefae994d9224fb7771a3ddb683868363c23510
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:04 2008 +0200

    ftrace: limit trace entries
    
    Currently there is no protection from the root user to use up all of
    memory for trace buffers. If the root user allocates too many entries,
    the OOM killer might start kill off all tasks.
    
    This patch adds an algorith to check the following condition:
    
     pages_requested > (freeable_memory + current_trace_buffer_pages) / 4
    
    If the above is met then the allocation fails. The above prevents more
    than 1/4th of freeable memory from being used by trace buffers.
    
    To determine the freeable_memory, I made determine_dirtyable_memory in
    mm/page-writeback.c global.
    
    Special thanks goes to Peter Zijlstra for suggesting the above calculation.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index f462439cc288..bd91987c065f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -105,6 +105,8 @@ extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;
 
+extern unsigned long determine_dirtyable_memory(void);
+
 extern int dirty_ratio_handler(struct ctl_table *table, int write,
 		struct file *filp, void __user *buffer, size_t *lenp,
 		loff_t *ppos);

commit cf0ca9fe5dd9e3693d935757a7b2fc50fc576554
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Apr 30 00:54:32 2008 -0700

    mm: bdi: export BDI attributes in sysfs
    
    Provide a place in sysfs (/sys/class/bdi) for the backing_dev_info object.
    This allows us to see and set the various BDI specific variables.
    
    In particular this properly exposes the read-ahead window for all relevant
    users and /sys/block/<block>/queue/read_ahead_kb should be deprecated.
    
    With patient help from Kay Sievers and Greg KH
    
    [mszeredi@suse.cz]
    
     - split off NFS and FUSE changes into separate patches
     - document new sysfs attributes under Documentation/ABI
     - do bdi_class_init as a core_initcall, otherwise the "default" BDI
       won't be initialized
     - remove bdi_init_fmt macro, it's not used very much
    
    [akpm@linux-foundation.org: fix ia64 warning]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Acked-by: Greg KH <greg@kroah.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b7b3362f7717..f462439cc288 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -114,6 +114,9 @@ struct file;
 int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,
 				      void __user *, size_t *, loff_t *);
 
+void get_dirty_limits(long *pbackground, long *pdirty, long *pbdi_dirty,
+		 struct backing_dev_info *bdi);
+
 void page_writeback_init(void);
 void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,
 					unsigned long nr_pages_dirtied);

commit 8bc3be2751b4f74ab90a446da1912fd8204d53f7
Author: Fengguang Wu <wfg@mail.ustc.edu.cn>
Date:   Mon Feb 4 22:29:36 2008 -0800

    writeback: speed up writeback of big dirty files
    
    After making dirty a 100M file, the normal behavior is to start the
    writeback for all data after 30s delays.  But sometimes the following
    happens instead:
    
            - after 30s:    ~4M
            - after 5s:     ~4M
            - after 5s:     all remaining 92M
    
    Some analyze shows that the internal io dispatch queues goes like this:
    
                    s_io            s_more_io
                    -------------------------
            1)      100M,1K         0
            2)      1K              96M
            3)      0               96M
    1) initial state with a 100M file and a 1K file
    
    2) 4M written, nr_to_write <= 0, so write more
    
    3) 1K written, nr_to_write > 0, no more writes(BUG)
    
    nr_to_write > 0 in (3) fools the upper layer to think that data have all
    been written out.  The big dirty file is actually still sitting in
    s_more_io.  We cannot simply splice s_more_io back to s_io as soon as s_io
    becomes empty, and let the loop in generic_sync_sb_inodes() continue: this
    may starve newly expired inodes in s_dirty.  It is also not an option to
    draw inodes from both s_more_io and s_dirty, an let the loop go on: this
    might lead to live locks, and might also starve other superblocks in sync
    time(well kupdate may still starve some superblocks, that's another bug).
    
    We have to return when a full scan of s_io completes.  So nr_to_write > 0
    does not necessarily mean that "all data are written".  This patch
    introduces a flag writeback_control.more_io to indicate that more io should
    be done.  With it the big dirty file no longer has to wait for the next
    kupdate invokation 5s later.
    
    In sync_sb_inodes() we only set more_io on super_blocks we actually
    visited.  This avoids the interaction between two pdflush deamons.
    
    Also in __sync_single_inode() we don't blindly keep requeuing the io if the
    filesystem cannot progress.  Failing to do so may lead to 100% iowait.
    
    Tested-by: Mike Snitzer <snitzer@gmail.com>
    Signed-off-by: Fengguang Wu <wfg@mail.ustc.edu.cn>
    Cc: Michael Rubin <mrubin@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b2cd826a8c90..b7b3362f7717 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -62,6 +62,7 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
+	unsigned more_io:1;		/* more io to be dispatched */
 };
 
 /*

commit 195cf453d2c3d789cbe80e3735755f860c2fb222
Author: Bron Gondwana <brong@fastmail.fm>
Date:   Mon Feb 4 22:29:20 2008 -0800

    mm/page-writeback: highmem_is_dirtyable option
    
    Add vm.highmem_is_dirtyable toggle
    
    A 32 bit machine with HIGHMEM64 enabled running DCC has an MMAPed file of
    approximately 2Gb size which contains a hash format that is written
    randomly by the dbclean process.  On 2.6.16 this process took a few
    minutes.  With lowmem only accounting of dirty ratios, this takes about 12
    hours of 100% disk IO, all random writes.
    
    Include a toggle in /proc/sys/vm/highmem_is_dirtyable which can be set to 1 to
    add the highmem back to the total available memory count.
    
    [akpm@linux-foundation.org: Fix the CONFIG_DETECT_SOFTLOCKUP=y build]
    Signed-off-by: Bron Gondwana <brong@fastmail.fm>
    Cc: Ethan Solomita <solo@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: WU Fengguang <wfg@mail.ustc.edu.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index c6148bbf1250..b2cd826a8c90 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -100,6 +100,7 @@ extern int dirty_background_ratio;
 extern int vm_dirty_ratio;
 extern int dirty_writeback_interval;
 extern int dirty_expire_interval;
+extern int vm_highmem_is_dirtyable;
 extern int block_dump;
 extern int laptop_mode;
 

commit c23f72cae9523d29ff94eec8f30ccbdaf234b20e
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Mon Jan 14 21:21:29 2008 -0800

    Revert "writeback: introduce writeback_control.more_io to indicate more io"
    
    This reverts commit 2e6883bdf49abd0e7f0d9b6297fc3be7ebb2250b, as
    requested by Fengguang Wu.  It's not quite fully baked yet, and while
    there are patches around to fix the problems it caused, they should get
    more testing.  Says Fengguang: "I'll resend them both for -mm later on,
    in a more complete patchset".
    
    See
    
            http://bugzilla.kernel.org/show_bug.cgi?id=9738
    
    for some of this discussion.
    
    Requested-by: Fengguang Wu <wfg@mail.ustc.edu.cn>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index bef7d66601cb..c6148bbf1250 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -62,7 +62,6 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
-	unsigned more_io:1;		/* more io to be dispatched */
 };
 
 /*

commit 1c0eeaf5698597146ed9b873e2f9e0961edcf0f9
Author: Joern Engel <joern@wohnheim.fh-wedel.de>
Date:   Tue Oct 16 23:30:44 2007 -0700

    introduce I_SYNC
    
    I_LOCK was used for several unrelated purposes, which caused deadlock
    situations in certain filesystems as a side effect.  One of the purposes
    now uses the new I_SYNC bit.
    
    Also document the various bits and change their order from historical to
    logical.
    
    [bunk@stusta.de: make fs/inode.c:wake_up_inode() static]
    Signed-off-by: Joern Engel <joern@wohnheim.fh-wedel.de>
    Cc: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Cc: David Chinner <dgc@sgi.com>
    Cc: Anton Altaparmakov <aia21@cam.ac.uk>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 1200868a5dee..bef7d66601cb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -69,7 +69,6 @@ struct writeback_control {
  * fs/fs-writeback.c
  */	
 void writeback_inodes(struct writeback_control *wbc);
-void wake_up_inode(struct inode *inode);
 int inode_wait(void *);
 void sync_inodes_sb(struct super_block *, int wait);
 void sync_inodes(int wait);
@@ -81,6 +80,13 @@ static inline void wait_on_inode(struct inode *inode)
 	wait_on_bit(&inode->i_state, __I_LOCK, inode_wait,
 							TASK_UNINTERRUPTIBLE);
 }
+static inline void inode_sync_wait(struct inode *inode)
+{
+	might_sleep();
+	wait_on_bit(&inode->i_state, __I_SYNC, inode_wait,
+							TASK_UNINTERRUPTIBLE);
+}
+
 
 /*
  * mm/page-writeback.c

commit 2e6883bdf49abd0e7f0d9b6297fc3be7ebb2250b
Author: Fengguang Wu <wfg@mail.ustc.edu.cn>
Date:   Tue Oct 16 23:30:43 2007 -0700

    writeback: introduce writeback_control.more_io to indicate more io
    
    After making dirty a 100M file, the normal behavior is to start the writeback
    for all data after 30s delays.  But sometimes the following happens instead:
    
            - after 30s:    ~4M
            - after 5s:     ~4M
            - after 5s:     all remaining 92M
    
    Some analyze shows that the internal io dispatch queues goes like this:
    
                    s_io            s_more_io
                    -------------------------
            1)      100M,1K         0
            2)      1K              96M
            3)      0               96M
    
    1) initial state with a 100M file and a 1K file
    2) 4M written, nr_to_write <= 0, so write more
    3) 1K written, nr_to_write > 0, no more writes(BUG)
    
    nr_to_write > 0 in (3) fools the upper layer to think that data have all been
    written out.  The big dirty file is actually still sitting in s_more_io.  We
    cannot simply splice s_more_io back to s_io as soon as s_io becomes empty, and
    let the loop in generic_sync_sb_inodes() continue: this may starve newly
    expired inodes in s_dirty.  It is also not an option to draw inodes from both
    s_more_io and s_dirty, an let the loop go on: this might lead to live locks,
    and might also starve other superblocks in sync time(well kupdate may still
    starve some superblocks, that's another bug).
    
    We have to return when a full scan of s_io completes.  So nr_to_write > 0 does
    not necessarily mean that "all data are written".  This patch introduces a
    flag writeback_control.more_io to indicate this situation.  With it the big
    dirty file no longer has to wait for the next kupdate invocation 5s later.
    
    Cc: David Chinner <dgc@sgi.com>
    Cc: Ken Chen <kenchen@google.com>
    Signed-off-by: Fengguang Wu <wfg@mail.ustc.edu.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 52be879793ed..1200868a5dee 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -62,6 +62,7 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
+	unsigned more_io:1;		/* more io to be dispatched */
 };
 
 /*

commit 04fbfdc14e5f48463820d6b9807daa5e9c92c51f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:50 2007 -0700

    mm: per device dirty threshold
    
    Scale writeback cache per backing device, proportional to its writeout speed.
    
    By decoupling the BDI dirty thresholds a number of problems we currently have
    will go away, namely:
    
     - mutual interference starvation (for any number of BDIs);
     - deadlocks with stacked BDIs (loop, FUSE and local NFS mounts).
    
    It might be that all dirty pages are for a single BDI while other BDIs are
    idling. By giving each BDI a 'fair' share of the dirty limit, each one can have
    dirty pages outstanding and make progress.
    
    A global threshold also creates a deadlock for stacked BDIs; when A writes to
    B, and A generates enough dirty pages to get throttled, B will never start
    writeback until the dirty pages go away. Again, by giving each BDI its own
    'independent' dirty limit, this problem is avoided.
    
    So the problem is to determine how to distribute the total dirty limit across
    the BDIs fairly and efficiently. A DBI that has a large dirty limit but does
    not have any dirty pages outstanding is a waste.
    
    What is done is to keep a floating proportion between the DBIs based on
    writeback completions. This way faster/more active devices get a larger share
    than slower/idle devices.
    
    [akpm@linux-foundation.org: fix warnings]
    [hugh@veritas.com: Fix occasional hang when a task couldn't get out of balance_dirty_pages]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d1321a81c9c4..52be879793ed 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -97,6 +97,10 @@ extern int dirty_expire_interval;
 extern int block_dump;
 extern int laptop_mode;
 
+extern int dirty_ratio_handler(struct ctl_table *table, int write,
+		struct file *filp, void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 struct ctl_table;
 struct file;
 int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,

commit f4921aff5b174349bc36551f142a5dbac782ea3f
Merge: 419217cb1d02 05c88babab95
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Mon Oct 15 10:46:05 2007 -0700

    Merge git://git.linux-nfs.org/pub/linux/nfs-2.6
    
    * git://git.linux-nfs.org/pub/linux/nfs-2.6: (131 commits)
      NFSv4: Fix a typo in nfs_inode_reclaim_delegation
      NFS: Add a boot parameter to disable 64 bit inode numbers
      NFS: nfs_refresh_inode should clear cache_validity flags on success
      NFS: Fix a connectathon regression in NFSv3 and NFSv4
      NFS: Use nfs_refresh_inode() in ops that aren't expected to change the inode
      SUNRPC: Don't call xprt_release in call refresh
      SUNRPC: Don't call xprt_release() if call_allocate fails
      SUNRPC: Fix buggy UDP transmission
      [23/37] Clean up duplicate includes in
      [2.6 patch] net/sunrpc/rpcb_clnt.c: make struct rpcb_program static
      SUNRPC: Use correct type in buffer length calculations
      SUNRPC: Fix default hostname created in rpc_create()
      nfs: add server port to rpc_pipe info file
      NFS: Get rid of some obsolete macros
      NFS: Simplify filehandle revalidation
      NFS: Ensure that nfs_link() returns a hashed dentry
      NFS: Be strict about dentry revalidation when doing exclusive create
      NFS: Don't zap the readdir caches upon error
      NFS: Remove the redundant nfs_reval_fsid()
      NFSv3: Always use directory post-op attributes in nfs3_proc_lookup
      ...
    
    Fix up trivial conflict due to sock_owned_by_user() cleanup manually in
    net/sunrpc/xprtsock.c

commit f5ff8422bbdd59f8c1f699df248e1b7a11073027
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Sep 21 09:19:54 2007 +0200

    Fix warnings with !CONFIG_BLOCK
    
    Hide everything in blkdev.h with CONFIG_BLOCK isn't set, and fixup
    the (few) files that fail to build because they were relying on blkdev.h
    pulling in extra includes for them.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b4af6bcb7b7a..c7c3337c3a88 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -5,6 +5,7 @@
 #define WRITEBACK_H
 
 #include <linux/sched.h>
+#include <linux/fs.h>
 
 struct backing_dev_info;
 

commit 90e9a3f9b0a14198a8ae5a0a5c13ad30f0b8b40d
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Sun Jul 22 19:27:46 2007 -0400

    VFS: Remove writeback_control->fs_private
    
    The only user of this field was NFS.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b4af6bcb7b7a..a111d3de2a57 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -61,8 +61,6 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
-
-	void *fs_private;		/* For use by ->writepages() */
 };
 
 /*

commit a200ee182a016752464a12cb2e8762e48254bb09
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 8 18:54:37 2007 +0200

    mm: set_page_dirty_balance() vs ->page_mkwrite()
    
    All the current page_mkwrite() implementations also set the page dirty. Which
    results in the set_page_dirty_balance() call to _not_ call balance, because the
    page is already found dirty.
    
    This allows us to dirty a _lot_ of pages without ever hitting
    balance_dirty_pages().  Not good (tm).
    
    Force a balance call if ->page_mkwrite() was successful.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 4ef4d22e5e43..b4af6bcb7b7a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -127,7 +127,7 @@ int sync_page_range(struct inode *inode, struct address_space *mapping,
 			loff_t pos, loff_t count);
 int sync_page_range_nolock(struct inode *inode, struct address_space *mapping,
 			   loff_t pos, loff_t count);
-void set_page_dirty_balance(struct page *page);
+void set_page_dirty_balance(struct page *page, int page_mkwrite);
 void writeback_set_ratelimit(void);
 
 /* pdflush.c */

commit e8edc6e03a5c8562dc70a6d969f732bdb355a7e7
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 21 01:22:52 2007 +0400

    Detach sched.h from mm.h
    
    First thing mm.h does is including sched.h solely for can_do_mlock() inline
    function which has "current" dereference inside. By dealing with can_do_mlock()
    mm.h can be detached from sched.h which is good. See below, why.
    
    This patch
    a) removes unconditional inclusion of sched.h from mm.h
    b) makes can_do_mlock() normal function in mm/mlock.c
    c) exports can_do_mlock() to not break compilation
    d) adds sched.h inclusions back to files that were getting it indirectly.
    e) adds less bloated headers to some files (asm/signal.h, jiffies.h) that were
       getting them indirectly
    
    Net result is:
    a) mm.h users would get less code to open, read, preprocess, parse, ... if
       they don't need sched.h
    b) sched.h stops being dependency for significant number of files:
       on x86_64 allmodconfig touching sched.h results in recompile of 4083 files,
       after patch it's only 3744 (-8.3%).
    
    Cross-compile tested on
    
            all arm defconfigs, all mips defconfigs, all powerpc defconfigs,
            alpha alpha-up
            arm
            i386 i386-up i386-defconfig i386-allnoconfig
            ia64 ia64-up
            m68k
            mips
            parisc parisc-up
            powerpc powerpc-up
            s390 s390-up
            sparc sparc-up
            sparc64 sparc64-up
            um-x86_64
            x86_64 x86_64-up x86_64-defconfig x86_64-allnoconfig
    
    as well as my two usual configs.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 050915b59576..4ef4d22e5e43 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -4,6 +4,8 @@
 #ifndef WRITEBACK_H
 #define WRITEBACK_H
 
+#include <linux/sched.h>
+
 struct backing_dev_info;
 
 extern spinlock_t inode_lock;

commit 0ea971801625184a91a6d80ea85e53875caa0bf5
Author: Miklos Szeredi <mszeredi@suse.cz>
Date:   Thu May 10 22:22:51 2007 -0700

    consolidate generic_writepages and mpage_writepages
    
    Clean up massive code duplication between mpage_writepages() and
    generic_writepages().
    
    The new generic function, write_cache_pages() takes a function pointer
    argument, which will be called for each page to be written.
    
    Maybe cifs_writepages() too can use this infrastructure, but I'm not
    touching that with a ten-foot pole.
    
    The upcoming page writeback support in fuse will also want this.
    
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
    Acked-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index daa6c125f66e..050915b59576 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -111,9 +111,15 @@ balance_dirty_pages_ratelimited(struct address_space *mapping)
 	balance_dirty_pages_ratelimited_nr(mapping, 1);
 }
 
+typedef int (*writepage_t)(struct page *page, struct writeback_control *wbc,
+				void *data);
+
 int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
-extern int generic_writepages(struct address_space *mapping,
-			      struct writeback_control *wbc);
+int generic_writepages(struct address_space *mapping,
+		       struct writeback_control *wbc);
+int write_cache_pages(struct address_space *mapping,
+		      struct writeback_control *wbc, writepage_t writepage,
+		      void *data);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 int sync_page_range(struct inode *inode, struct address_space *mapping,
 			loff_t pos, loff_t count);

commit c63c7b051395368573779c8309aa5c990dcf2f96
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Mon Apr 2 19:29:52 2007 -0400

    NFS: Fix a race when doing NFS write coalescing
    
    Currently we do write coalescing in a very inefficient manner: one pass in
    generic_writepages() in order to lock the pages for writing, then one pass
    in nfs_flush_mapping() and/or nfs_sync_mapping_wait() in order to gather
    the locked pages for coalescing into RPC requests of size "wsize".
    
    In fact, it turns out there is actually a deadlock possible here since we
    only start I/O on the second pass. If the user signals the process while
    we're in nfs_sync_mapping_wait(), for instance, then we may exit before
    starting I/O on all the requests that have been queued up.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 0c78f7f4a976..daa6c125f66e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -59,6 +59,8 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
+
+	void *fs_private;		/* For use by ->writepages() */
 };
 
 /*

commit 232ea4d69d81169453344b7d05203425c88d973b
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Feb 28 20:13:21 2007 -0800

    [PATCH] throttle_vm_writeout(): don't loop on GFP_NOFS and GFP_NOIO allocations
    
    throttle_vm_writeout() is designed to wait for the dirty levels to subside.
    But if the caller holds IO or FS locks, we might be holding up that writeout.
    
    So change it to take a single nap to give other devices a chance to clean some
    memory, then return.
    
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Pete Zaitcev <zaitcev@redhat.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fc35e6bdfb93..0c78f7f4a976 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -84,7 +84,7 @@ static inline void wait_on_inode(struct inode *inode)
 int wakeup_pdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
-void throttle_vm_writeout(void);
+void throttle_vm_writeout(gfp_t gfp_mask);
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;

commit 3fcfab16c5b86eaa3db3a9a31adba550c5b67141
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Oct 19 23:28:16 2006 -0700

    [PATCH] separate bdi congestion functions from queue congestion functions
    
    Separate out the concept of "queue congestion" from "backing-dev congestion".
    Congestion is a backing-dev concept, not a queue concept.
    
    The blk_* congestion functions are retained, as wrappers around the core
    backing-dev congestion functions.
    
    This proper layering is needed so that NFS can cleanly use the congestion
    functions, and so that CONFIG_BLOCK=n actually links.
    
    Cc: "Thomas Maier" <balagi@justmail.de>
    Cc: "Jens Axboe" <jens.axboe@oracle.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Peter Osterlund <petero2@telia.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index a341c8032866..fc35e6bdfb93 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -85,7 +85,6 @@ int wakeup_pdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
 void throttle_vm_writeout(void);
-void writeback_congestion_end(void);
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;

commit f30c2269544bffc7bf1b0d7c0abe5be1be83b8cb
Author: Uwe Zeisberger <Uwe_Zeisberger@digi.com>
Date:   Tue Oct 3 23:01:26 2006 +0200

    fix file specification in comments
    
    Many files include the filename at the beginning, serveral used a wrong one.
    
    Signed-off-by: Uwe Zeisberger <Uwe_Zeisberger@digi.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 4f4d98addb44..a341c8032866 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -1,5 +1,5 @@
 /*
- * include/linux/writeback.h.
+ * include/linux/writeback.h
  */
 #ifndef WRITEBACK_H
 #define WRITEBACK_H

commit 811d736f9e8013966e1a5a930c0db09508bdbb15
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 29 19:06:09 2006 +0100

    [PATCH] BLOCK: Dissociate generic_writepages() from mpage stuff [try #6]
    
    Dissociate the generic_writepages() function from the mpage stuff, moving its
    declaration to linux/mm.h and actually emitting a full implementation into
    mm/page-writeback.c.
    
    The implementation is a partial duplicate of mpage_writepages() with all BIO
    references removed.
    
    It is used by NFS to do writeback.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 9d4074ecd0cd..4f4d98addb44 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -111,6 +111,8 @@ balance_dirty_pages_ratelimited(struct address_space *mapping)
 }
 
 int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
+extern int generic_writepages(struct address_space *mapping,
+			      struct writeback_control *wbc);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 int sync_page_range(struct inode *inode, struct address_space *mapping,
 			loff_t pos, loff_t count);

commit 2d1d43f6a43b703587e759145f69467e7c6553a7
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Fri Sep 29 02:01:25 2006 -0700

    [PATCH] call mm/page-writeback.c:set_ratelimit() when new pages are hot-added
    
    ratelimit_pages in page-writeback.c is recalculated (in set_ratelimit())
    every time a CPU is hot-added/removed.  But this value is not recalculated
    when new pages are hot-added.
    
    This patch fixes that problem by calling set_ratelimit() when new pages
    are hot-added.
    
    [akpm@osdl.org: cleanups]
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 56a23a0e7f2e..9d4074ecd0cd 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -117,6 +117,7 @@ int sync_page_range(struct inode *inode, struct address_space *mapping,
 int sync_page_range_nolock(struct inode *inode, struct address_space *mapping,
 			   loff_t pos, loff_t count);
 void set_page_dirty_balance(struct page *page);
+void writeback_set_ratelimit(void);
 
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl

commit edc79b2a46ed854595e40edcf3f8b37f9f14aa3f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 25 23:30:58 2006 -0700

    [PATCH] mm: balance dirty pages
    
    Now that we can detect writers of shared mappings, throttle them.  Avoids OOM
    by surprise.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 0422036af4eb..56a23a0e7f2e 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -116,6 +116,7 @@ int sync_page_range(struct inode *inode, struct address_space *mapping,
 			loff_t pos, loff_t count);
 int sync_page_range_nolock(struct inode *inode, struct address_space *mapping,
 			   loff_t pos, loff_t count);
+void set_page_dirty_balance(struct page *page);
 
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl

commit 275a082fe9308e710324e26ccb5363c53d8fd45f
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Aug 22 20:06:24 2006 -0400

    Add a real API for dealing with blk_congestion_wait()
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 9e38b566d0e7..0422036af4eb 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -85,6 +85,7 @@ int wakeup_pdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
 void throttle_vm_writeout(void);
+void writeback_congestion_end(void);
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;

commit 111ebb6e6f7bd7de6d722c5848e95621f43700d9
Author: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
Date:   Fri Jun 23 02:03:26 2006 -0700

    [PATCH] writeback: fix range handling
    
    When a writeback_control's `start' and `end' fields are used to
    indicate a one-byte-range starting at file offset zero, the required
    values of .start=0,.end=0 mean that the ->writepages() implementation
    has no way of telling that it is being asked to perform a range
    request.  Because we're currently overloading (start == 0 && end == 0)
    to mean "this is not a write-a-range request".
    
    To make all this sane, the patch changes range of writeback_control.
    
    So caller does: If it is calling ->writepages() to write pages, it
    sets range (range_start/end or range_cyclic) always.
    
    And if range_cyclic is true, ->writepages() thinks the range is
    cyclic, otherwise it just uses range_start and range_end.
    
    This patch does,
    
        - Add LLONG_MAX, LLONG_MIN, ULLONG_MAX to include/linux/kernel.h
          -1 is usually ok for range_end (type is long long). But, if someone did,
    
                    range_end += val;               range_end is "val - 1"
                    u64val = range_end >> bits;     u64val is "~(0ULL)"
    
          or something, they are wrong. So, this adds LLONG_MAX to avoid nasty
          things, and uses LLONG_MAX for range_end.
    
        - All callers of ->writepages() sets range_start/end or range_cyclic.
    
        - Fix updates of ->writeback_index. It seems already bit strange.
          If it starts at 0 and ended by check of nr_to_write, this last
          index may reduce chance to scan end of file.  So, this updates
          ->writeback_index only if range_cyclic is true or whole-file is
          scanned.
    
    Signed-off-by: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Nathan Scott <nathans@sgi.com>
    Cc: Anton Altaparmakov <aia21@cantab.net>
    Cc: Steven French <sfrench@us.ibm.com>
    Cc: "Vladimir V. Saveliev" <vs@namesys.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 56f92fcbe94a..9e38b566d0e7 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -50,14 +50,15 @@ struct writeback_control {
 	 * a hint that the filesystem need only write out the pages inside that
 	 * byterange.  The byte at `end' is included in the writeout request.
 	 */
-	loff_t start;
-	loff_t end;
+	loff_t range_start;
+	loff_t range_end;
 
 	unsigned nonblocking:1;		/* Don't get stuck on request queues */
 	unsigned encountered_congestion:1; /* An output: a queue is full */
 	unsigned for_kupdate:1;		/* A kupdate writeback */
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned for_writepages:1;	/* This is a writepages() call */
+	unsigned range_cyclic:1;	/* range_start is cyclic */
 };
 
 /*

commit fa5a734e406b53761fcc5ee22366006f71112c2d
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Mar 24 03:18:10 2006 -0800

    [PATCH] balance_dirty_pages_ratelimited: take nr_pages arg
    
    Modify balance_dirty_pages_ratelimited() so that it can take a
    number-of-pages-which-I-just-dirtied argument.  For msync().
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 609565961494..56f92fcbe94a 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -99,7 +99,15 @@ int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,
 				      void __user *, size_t *, loff_t *);
 
 void page_writeback_init(void);
-void balance_dirty_pages_ratelimited(struct address_space *mapping);
+void balance_dirty_pages_ratelimited_nr(struct address_space *mapping,
+					unsigned long nr_pages_dirtied);
+
+static inline void
+balance_dirty_pages_ratelimited(struct address_space *mapping)
+{
+	balance_dirty_pages_ratelimited_nr(mapping, 1);
+}
+
 int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 int sync_page_range(struct inode *inode, struct address_space *mapping,

commit f6ef943813ac3085ece7252ea101d663581219f6
Author: Bart Samwel <bart@samwel.tk>
Date:   Fri Mar 24 03:15:48 2006 -0800

    [PATCH] Represent dirty_*_centisecs as jiffies internally
    
    Make that the internal values for:
    
    /proc/sys/vm/dirty_writeback_centisecs
    /proc/sys/vm/dirty_expire_centisecs
    
    are stored as jiffies instead of centiseconds.  Let the sysctl interface do
    the conversions with full precision using clock_t_to_jiffies, instead of
    doing overflow-sensitive on-the-fly conversions every time the values are
    used.
    
    Cons: apparent precision loss if HZ is not a multiple of 100, because of
    conversion back and forth.  This is a common problem for all sysctl values
    that use proc_dointvec_userhz_jiffies.  (There is only one other in-tree
    use, in net/core/neighbour.c.)
    
    Signed-off-by: Bart Samwel <bart@samwel.tk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index beaef5c7a0ea..609565961494 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -88,8 +88,8 @@ void throttle_vm_writeout(void);
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;
 extern int vm_dirty_ratio;
-extern int dirty_writeback_centisecs;
-extern int dirty_expire_centisecs;
+extern int dirty_writeback_interval;
+extern int dirty_expire_interval;
 extern int block_dump;
 extern int laptop_mode;
 

commit 268fc16e343b4f8e249468747db2e658da46a814
Author: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
Date:   Sun Jan 8 01:02:12 2006 -0800

    [PATCH] export/change sync_page_range/_nolock()
    
    This exports/changes the sync_page_range/_nolock().  The fatfs needs
    sync_page_range/_nolock() for expanding truncate, and changes "size_t count"
    to "loff_t count".
    
    Signed-off-by: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index b096159086e8..beaef5c7a0ea 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -103,7 +103,9 @@ void balance_dirty_pages_ratelimited(struct address_space *mapping);
 int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 int sync_page_range(struct inode *inode, struct address_space *mapping,
-			loff_t pos, size_t count);
+			loff_t pos, loff_t count);
+int sync_page_range_nolock(struct inode *inode, struct address_space *mapping,
+			   loff_t pos, loff_t count);
 
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl

commit 22905f775dd6a8b73be99826dcad07ceec00244b
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Nov 16 15:07:01 2005 -0800

    identify multipage ->writepages() calls
    
     NFS needs to be able to distinguish between single-page ->writepage() calls and
     multipage ->writepages() calls.
    
     For the single-page writepage calls NFS can kick off the I/O within the
     context of ->writepage().
    
     For multipage ->writepages calls, nfs_writepage() will leave the I/O pending
     and nfs_writepages() will kick off the I/O when it all has been queued up
     within NFS.
    
     Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
     Signed-off-by: Andrew Morton <akpm@osdl.org>
     Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 64a36ba43b2f..b096159086e8 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -53,10 +53,11 @@ struct writeback_control {
 	loff_t start;
 	loff_t end;
 
-	unsigned nonblocking:1;			/* Don't get stuck on request queues */
-	unsigned encountered_congestion:1;	/* An output: a queue is full */
-	unsigned for_kupdate:1;			/* A kupdate writeback */
-	unsigned for_reclaim:1;			/* Invoked from the page allocator */
+	unsigned nonblocking:1;		/* Don't get stuck on request queues */
+	unsigned encountered_congestion:1; /* An output: a queue is full */
+	unsigned for_kupdate:1;		/* A kupdate writeback */
+	unsigned for_reclaim:1;		/* Invoked from the page allocator */
+	unsigned for_writepages:1;	/* This is a writepages() call */
 };
 
 /*

commit 994fc28c7b1e697ac56befe4aecabf23f0689f46
Author: Zach Brown <zach.brown@oracle.com>
Date:   Thu Dec 15 14:28:17 2005 -0800

    [PATCH] add AOP_TRUNCATED_PAGE, prepend AOP_ to WRITEPAGE_ACTIVATE
    
    readpage(), prepare_write(), and commit_write() callers are updated to
    understand the special return code AOP_TRUNCATED_PAGE in the style of
    writepage() and WRITEPAGE_ACTIVATE.  AOP_TRUNCATED_PAGE tells the caller that
    the callee has unlocked the page and that the operation should be tried again
    with a new page.  OCFS2 uses this to detect and work around a lock inversion in
    its aop methods.  There should be no change in behaviour for methods that don't
    return AOP_TRUNCATED_PAGE.
    
    WRITEPAGE_ACTIVATE is also prepended with AOP_ for consistency and they are
    made enums so that kerneldoc can be used to document their semantics.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 343d883d69c5..64a36ba43b2f 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -59,12 +59,6 @@ struct writeback_control {
 	unsigned for_reclaim:1;			/* Invoked from the page allocator */
 };
 
-/*
- * ->writepage() return values (make these much larger than a pagesize, in
- * case some fs is returning number-of-bytes-written from writepage)
- */
-#define WRITEPAGE_ACTIVATE	0x80000	/* IO was not started: activate page */
-
 /*
  * fs/fs-writeback.c
  */	

commit 5ce7852cdf07ab903fb1c72d0915ac492c6e07c7
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sat Sep 10 00:26:28 2005 -0700

    [PATCH] mm/filemap.c: make two functions static
    
    With Nick Piggin <npiggin@suse.de>
    
    Give some things static scope.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 542dbaee6512..343d883d69c5 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -109,8 +109,6 @@ int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
 int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
 int sync_page_range(struct inode *inode, struct address_space *mapping,
 			loff_t pos, size_t count);
-int sync_page_range_nolock(struct inode *inode, struct address_space
-		*mapping, loff_t pos, size_t count);
 
 /* pdflush.c */
 extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl

commit 687a21cee17000177b1935896b9b475acf136678
Author: Pekka J Enberg <penberg@cs.Helsinki.FI>
Date:   Tue Jun 28 20:44:55 2005 -0700

    [PATCH] rename wakeup_bdflush to wakeup_pdflush
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index d5c3fe1bf33d..542dbaee6512 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -85,7 +85,7 @@ static inline void wait_on_inode(struct inode *inode)
 /*
  * mm/page-writeback.c
  */
-int wakeup_bdflush(long nr_pages);
+int wakeup_pdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
 void throttle_vm_writeout(void);

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index 1262cb43c3ab..d5c3fe1bf33d 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -14,11 +14,13 @@ extern struct list_head inode_unused;
  * Yes, writeback.h requires sched.h
  * No, sched.h is not included from here.
  */
-static inline int current_is_pdflush(void)
+static inline int task_is_pdflush(struct task_struct *task)
 {
-	return current->flags & PF_FLUSHER;
+	return task->flags & PF_FLUSHER;
 }
 
+#define current_is_pdflush()	task_is_pdflush(current)
+
 /*
  * fs/fs-writeback.c
  */

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/writeback.h b/include/linux/writeback.h
new file mode 100644
index 000000000000..1262cb43c3ab
--- /dev/null
+++ b/include/linux/writeback.h
@@ -0,0 +1,118 @@
+/*
+ * include/linux/writeback.h.
+ */
+#ifndef WRITEBACK_H
+#define WRITEBACK_H
+
+struct backing_dev_info;
+
+extern spinlock_t inode_lock;
+extern struct list_head inode_in_use;
+extern struct list_head inode_unused;
+
+/*
+ * Yes, writeback.h requires sched.h
+ * No, sched.h is not included from here.
+ */
+static inline int current_is_pdflush(void)
+{
+	return current->flags & PF_FLUSHER;
+}
+
+/*
+ * fs/fs-writeback.c
+ */
+enum writeback_sync_modes {
+	WB_SYNC_NONE,	/* Don't wait on anything */
+	WB_SYNC_ALL,	/* Wait on every mapping */
+	WB_SYNC_HOLD,	/* Hold the inode on sb_dirty for sys_sync() */
+};
+
+/*
+ * A control structure which tells the writeback code what to do.  These are
+ * always on the stack, and hence need no locking.  They are always initialised
+ * in a manner such that unspecified fields are set to zero.
+ */
+struct writeback_control {
+	struct backing_dev_info *bdi;	/* If !NULL, only write back this
+					   queue */
+	enum writeback_sync_modes sync_mode;
+	unsigned long *older_than_this;	/* If !NULL, only write back inodes
+					   older than this */
+	long nr_to_write;		/* Write this many pages, and decrement
+					   this for each page written */
+	long pages_skipped;		/* Pages which were not written */
+
+	/*
+	 * For a_ops->writepages(): is start or end are non-zero then this is
+	 * a hint that the filesystem need only write out the pages inside that
+	 * byterange.  The byte at `end' is included in the writeout request.
+	 */
+	loff_t start;
+	loff_t end;
+
+	unsigned nonblocking:1;			/* Don't get stuck on request queues */
+	unsigned encountered_congestion:1;	/* An output: a queue is full */
+	unsigned for_kupdate:1;			/* A kupdate writeback */
+	unsigned for_reclaim:1;			/* Invoked from the page allocator */
+};
+
+/*
+ * ->writepage() return values (make these much larger than a pagesize, in
+ * case some fs is returning number-of-bytes-written from writepage)
+ */
+#define WRITEPAGE_ACTIVATE	0x80000	/* IO was not started: activate page */
+
+/*
+ * fs/fs-writeback.c
+ */	
+void writeback_inodes(struct writeback_control *wbc);
+void wake_up_inode(struct inode *inode);
+int inode_wait(void *);
+void sync_inodes_sb(struct super_block *, int wait);
+void sync_inodes(int wait);
+
+/* writeback.h requires fs.h; it, too, is not included from here. */
+static inline void wait_on_inode(struct inode *inode)
+{
+	might_sleep();
+	wait_on_bit(&inode->i_state, __I_LOCK, inode_wait,
+							TASK_UNINTERRUPTIBLE);
+}
+
+/*
+ * mm/page-writeback.c
+ */
+int wakeup_bdflush(long nr_pages);
+void laptop_io_completion(void);
+void laptop_sync_completion(void);
+void throttle_vm_writeout(void);
+
+/* These are exported to sysctl. */
+extern int dirty_background_ratio;
+extern int vm_dirty_ratio;
+extern int dirty_writeback_centisecs;
+extern int dirty_expire_centisecs;
+extern int block_dump;
+extern int laptop_mode;
+
+struct ctl_table;
+struct file;
+int dirty_writeback_centisecs_handler(struct ctl_table *, int, struct file *,
+				      void __user *, size_t *, loff_t *);
+
+void page_writeback_init(void);
+void balance_dirty_pages_ratelimited(struct address_space *mapping);
+int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0);
+int do_writepages(struct address_space *mapping, struct writeback_control *wbc);
+int sync_page_range(struct inode *inode, struct address_space *mapping,
+			loff_t pos, size_t count);
+int sync_page_range_nolock(struct inode *inode, struct address_space
+		*mapping, loff_t pos, size_t count);
+
+/* pdflush.c */
+extern int nr_pdflush_threads;	/* Global so it can be exported to sysctl
+				   read-only. */
+
+
+#endif		/* WRITEBACK_H */
