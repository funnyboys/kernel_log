commit 497a6c1b09902b22ceccc0f25ba4dd623e1ddb7d
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jun 3 16:02:34 2020 -0700

    mm: keep separate anon and file statistics on page reclaim activity
    
    Having statistics on pages scanned and pages reclaimed for both anon and
    file pages makes it easier to evaluate changes to LRU balancing.
    
    While at it, clean up the stat-keeping mess for isolation, putback,
    reclaim stats etc.  a bit: first the physical LRU operation (isolation and
    putback), followed by vmstats, reclaim_stats, and then vm events.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/20200520232525.798933-3-hannes@cmpxchg.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index ffef0f279747..24fc7c3ae7d6 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -35,6 +35,10 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGSCAN_KSWAPD,
 		PGSCAN_DIRECT,
 		PGSCAN_DIRECT_THROTTLE,
+		PGSCAN_ANON,
+		PGSCAN_FILE,
+		PGSTEAL_ANON,
+		PGSTEAL_FILE,
 #ifdef CONFIG_NUMA
 		PGSCAN_ZONE_RECLAIM_FAILED,
 #endif

commit 85b9f46e8ea451633ccd60a7d8cacbfff9f34047
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 6 20:04:28 2020 -0700

    mm, thp: track fallbacks due to failed memcg charges separately
    
    The thp_fault_fallback and thp_file_fallback vmstats are incremented if
    either the hugepage allocation fails through the page allocator or the
    hugepage charge fails through mem cgroup.
    
    This patch leaves this field untouched but adds two new fields,
    thp_{fault,file}_fallback_charge, which is incremented only when the mem
    cgroup charge fails.
    
    This distinguishes between attempted hugepage allocations that fail due to
    fragmentation (or low memory conditions) and those that fail due to mem
    cgroup limits.  That can be used to determine the impact of fragmentation
    on the system by excluding faults that failed due to memcg usage.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Yang Shi <yang.shi@linux.alibaba.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Jeremy Cline <jcline@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.21.2003061422070.7412@chino.kir.corp.google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 41a4c7568748..ffef0f279747 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -73,10 +73,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 		THP_FAULT_ALLOC,
 		THP_FAULT_FALLBACK,
+		THP_FAULT_FALLBACK_CHARGE,
 		THP_COLLAPSE_ALLOC,
 		THP_COLLAPSE_ALLOC_FAILED,
 		THP_FILE_ALLOC,
 		THP_FILE_FALLBACK,
+		THP_FILE_FALLBACK_CHARGE,
 		THP_FILE_MAPPED,
 		THP_SPLIT_PAGE,
 		THP_SPLIT_PAGE_FAILED,
@@ -117,6 +119,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #ifndef CONFIG_TRANSPARENT_HUGEPAGE
 #define THP_FILE_ALLOC ({ BUILD_BUG(); 0; })
 #define THP_FILE_FALLBACK ({ BUILD_BUG(); 0; })
+#define THP_FILE_FALLBACK_CHARGE ({ BUILD_BUG(); 0; })
 #define THP_FILE_MAPPED ({ BUILD_BUG(); 0; })
 #endif
 

commit dcdf11ee144133328664d90836e712d840d047d9
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 6 20:04:25 2020 -0700

    mm, shmem: add vmstat for hugepage fallback
    
    The existing thp_fault_fallback indicates when thp attempts to allocate a
    hugepage but fails, or if the hugepage cannot be charged to the mem cgroup
    hierarchy.
    
    Extend this to shmem as well.  Adds a new thp_file_fallback to complement
    thp_file_alloc that gets incremented when a hugepage is attempted to be
    allocated but fails, or if it cannot be charged to the mem cgroup
    hierarchy.
    
    Additionally, remove the check for CONFIG_TRANSPARENT_HUGE_PAGECACHE from
    shmem_alloc_hugepage() since it is only called with this configuration
    option.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Yang Shi <yang.shi@linux.alibaba.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Jeremy Cline <jcline@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.21.2003061421240.7412@chino.kir.corp.google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 47a3441cf4c4..41a4c7568748 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -76,6 +76,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_COLLAPSE_ALLOC,
 		THP_COLLAPSE_ALLOC_FAILED,
 		THP_FILE_ALLOC,
+		THP_FILE_FALLBACK,
 		THP_FILE_MAPPED,
 		THP_SPLIT_PAGE,
 		THP_SPLIT_PAGE_FAILED,
@@ -115,6 +116,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 
 #ifndef CONFIG_TRANSPARENT_HUGEPAGE
 #define THP_FILE_ALLOC ({ BUILD_BUG(); 0; })
+#define THP_FILE_FALLBACK ({ BUILD_BUG(); 0; })
 #define THP_FILE_MAPPED ({ BUILD_BUG(); 0; })
 #endif
 

commit 7a9cdebdcc17e426fb5287e4a82db1dfe86339b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 12 23:57:48 2018 -1000

    mm: get rid of vmacache_flush_all() entirely
    
    Jann Horn points out that the vmacache_flush_all() function is not only
    potentially expensive, it's buggy too.  It also happens to be entirely
    unnecessary, because the sequence number overflow case can be avoided by
    simply making the sequence number be 64-bit.  That doesn't even grow the
    data structures in question, because the other adjacent fields are
    already 64-bit.
    
    So simplify the whole thing by just making the sequence number overflow
    case go away entirely, which gets rid of all the complications and makes
    the code faster too.  Win-win.
    
    [ Oleg Nesterov points out that the VMACACHE_FULL_FLUSHES statistics
      also just goes away entirely with this ]
    
    Reported-by: Jann Horn <jannh@google.com>
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 5c7f010676a7..47a3441cf4c4 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -105,7 +105,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #ifdef CONFIG_DEBUG_VM_VMACACHE
 		VMACACHE_FIND_CALLS,
 		VMACACHE_FIND_HITS,
-		VMACACHE_FULL_FLUSHES,
 #endif
 #ifdef CONFIG_SWAP
 		SWAP_RA,

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index d77bc35278b0..5c7f010676a7 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef VM_EVENT_ITEM_H_INCLUDED
 #define VM_EVENT_ITEM_H_INCLUDED
 

commit cbc65df240c104bf540af1ad58595bf1eaa5ee10
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Sep 6 16:24:29 2017 -0700

    mm, swap: add swap readahead hit statistics
    
    Patch series "mm, swap: VMA based swap readahead", v4.
    
    The swap readahead is an important mechanism to reduce the swap in
    latency.  Although pure sequential memory access pattern isn't very
    popular for anonymous memory, the space locality is still considered
    valid.
    
    In the original swap readahead implementation, the consecutive blocks in
    swap device are readahead based on the global space locality estimation.
    But the consecutive blocks in swap device just reflect the order of page
    reclaiming, don't necessarily reflect the access pattern in virtual
    memory space.  And the different tasks in the system may have different
    access patterns, which makes the global space locality estimation
    incorrect.
    
    In this patchset, when page fault occurs, the virtual pages near the
    fault address will be readahead instead of the swap slots near the fault
    swap slot in swap device.  This avoid to readahead the unrelated swap
    slots.  At the same time, the swap readahead is changed to work on
    per-VMA from globally.  So that the different access patterns of the
    different VMAs could be distinguished, and the different readahead
    policy could be applied accordingly.  The original core readahead
    detection and scaling algorithm is reused, because it is an effect
    algorithm to detect the space locality.
    
    In addition to the swap readahead changes, some new sysfs interface is
    added to show the efficiency of the readahead algorithm and some other
    swap statistics.
    
    This new implementation will incur more small random read, on SSD, the
    improved correctness of estimation and readahead target should beat the
    potential increased overhead, this is also illustrated in the test
    results below.  But on HDD, the overhead may beat the benefit, so the
    original implementation will be used by default.
    
    The test and result is as follow,
    
    Common test condition
    =====================
    
    Test Machine: Xeon E5 v3 (2 sockets, 72 threads, 32G RAM)
    Swap device: NVMe disk
    
    Micro-benchmark with combined access pattern
    ============================================
    
    vm-scalability, sequential swap test case, 4 processes to eat 50G
    virtual memory space, repeat the sequential memory writing until 300
    seconds.  The first round writing will trigger swap out, the following
    rounds will trigger sequential swap in and out.
    
    At the same time, run vm-scalability random swap test case in
    background, 8 processes to eat 30G virtual memory space, repeat the
    random memory write until 300 seconds.  This will trigger random swap-in
    in the background.
    
    This is a combined workload with sequential and random memory accessing
    at the same time.  The result (for sequential workload) is as follow,
    
                            Base            Optimized
                            ----            ---------
    throughput              345413 KB/s     414029 KB/s (+19.9%)
    latency.average         97.14 us        61.06 us (-37.1%)
    latency.50th            2 us            1 us
    latency.60th            2 us            1 us
    latency.70th            98 us           2 us
    latency.80th            160 us          2 us
    latency.90th            260 us          217 us
    latency.95th            346 us          369 us
    latency.99th            1.34 ms         1.09 ms
    ra_hit%                 52.69%          99.98%
    
    The original swap readahead algorithm is confused by the background
    random access workload, so readahead hit rate is lower.  The VMA-base
    readahead algorithm works much better.
    
    Linpack
    =======
    
    The test memory size is bigger than RAM to trigger swapping.
    
                            Base            Optimized
                            ----            ---------
    elapsed_time            393.49 s        329.88 s (-16.2%)
    ra_hit%                 86.21%          98.82%
    
    The score of base and optimized kernel hasn't visible changes.  But the
    elapsed time reduced and readahead hit rate improved, so the optimized
    kernel runs better for startup and tear down stages.  And the absolute
    value of readahead hit rate is high, shows that the space locality is
    still valid in some practical workloads.
    
    This patch (of 5):
    
    The statistics for total readahead pages and total readahead hits are
    recorded and exported via the following sysfs interface.
    
    /sys/kernel/mm/swap/ra_hits
    /sys/kernel/mm/swap/ra_total
    
    With them, the efficiency of the swap readahead could be measured, so
    that the swap readahead algorithm and parameters could be tuned
    accordingly.
    
    [akpm@linux-foundation.org: don't display swap stats if CONFIG_SWAP=n]
    Link: http://lkml.kernel.org/r/20170807054038.1843-2-ying.huang@intel.com
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index e02820fc2861..d77bc35278b0 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -105,6 +105,10 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		VMACACHE_FIND_CALLS,
 		VMACACHE_FIND_HITS,
 		VMACACHE_FULL_FLUSHES,
+#endif
+#ifdef CONFIG_SWAP
+		SWAP_RA,
+		SWAP_RA_HIT,
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit fe490cc0fe9e6ee48cc48bb5dc463bc5f0f1428f
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Sep 6 16:22:52 2017 -0700

    mm, THP, swap: add THP swapping out fallback counting
    
    When swapping out THP (Transparent Huge Page), instead of swapping out
    the THP as a whole, sometimes we have to fallback to split the THP into
    normal pages before swapping, because no free swap clusters are
    available, or cgroup limit is exceeded, etc.  To count the number of the
    fallback, a new VM event THP_SWPOUT_FALLBACK is added, and counted when
    we fallback to split the THP.
    
    Link: http://lkml.kernel.org/r/20170724051840.2309-13-ying.huang@intel.com
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ross Zwisler <ross.zwisler@intel.com> [for brd.c, zram_drv.c, pmem.c]
    Cc: Vishal L Verma <vishal.l.verma@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index c75024e80eed..e02820fc2861 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -86,6 +86,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 		THP_SWPOUT,
+		THP_SWPOUT_FALLBACK,
 #endif
 #ifdef CONFIG_MEMORY_BALLOON
 		BALLOON_INFLATE,

commit 225311a46411c37e20e73d99f4382f141e12f6f9
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Sep 6 16:22:30 2017 -0700

    mm: test code to write THP to swap device as a whole
    
    To support delay splitting THP (Transparent Huge Page) after swapped
    out, we need to enhance swap writing code to support to write a THP as a
    whole.  This will improve swap write IO performance.
    
    As Ming Lei <ming.lei@redhat.com> pointed out, this should be based on
    multipage bvec support, which hasn't been merged yet.  So this patch is
    only for testing the functionality of the other patches in the series.
    And will be reimplemented after multipage bvec support is merged.
    
    Link: http://lkml.kernel.org/r/20170724051840.2309-7-ying.huang@intel.com
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ross Zwisler <ross.zwisler@intel.com> [for brd.c, zram_drv.c, pmem.c]
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Vishal L Verma <vishal.l.verma@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 37e8d31a4632..c75024e80eed 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -85,6 +85,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #endif
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
+		THP_SWPOUT,
 #endif
 #ifdef CONFIG_MEMORY_BALLOON
 		BALLOON_INFLATE,

commit 8e675f7af50747e1e9e96538e8706767e4f80e2c
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Thu Jul 6 15:40:28 2017 -0700

    mm/oom_kill: count global and memory cgroup oom kills
    
    Show count of oom killer invocations in /proc/vmstat and count of
    processes killed in memory cgroup in knob "memory.events" (in
    memory.oom_control for v1 cgroup).
    
    Also describe difference between "oom" and "oom_kill" in memory cgroup
    documentation.  Currently oom in memory cgroup kills tasks iff shortage
    has happened inside page fault.
    
    These counters helps in monitoring oom kills - for now the only way is
    grepping for magic words in kernel log.
    
    [akpm@linux-foundation.org: fix for mem_cgroup_count_vm_event() rename]
    [akpm@linux-foundation.org: fix comment, per Konstantin]
    Link: http://lkml.kernel.org/r/149570810989.203600.9492483715840752937.stgit@buzz
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Roman Guschin <guroan@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index be3ab2d13adf..37e8d31a4632 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -41,6 +41,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		PAGEOUTRUN, PGROTATED,
 		DROP_PAGECACHE, DROP_SLAB,
+		OOM_KILL,
 #ifdef CONFIG_NUMA_BALANCING
 		NUMA_PTE_UPDATES,
 		NUMA_HUGE_PTE_UPDATES,

commit 5dd0b16cdaff9b94da06074d5888b03235c0bf17
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Jun 5 07:40:25 2017 -0700

    mm/vmstat: Make NR_TLB_REMOTE_FLUSH_RECEIVED available even on UP
    
    This fixes CONFIG_SMP=n, CONFIG_DEBUG_TLBFLUSH=y without introducing
    further #ifdef soup.  Caught by a Kbuild bot randconfig build.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: ce4a4e565f52 ("x86/mm: Remove the UP asm/tlbflush.h code, always use the (formerly) SMP code")
    Link: http://lkml.kernel.org/r/76da9a3cc4415996f2ad2c905b93414add322021.1496673616.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index d84ae90ccd5c..be3ab2d13adf 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -93,10 +93,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #endif
 #endif
 #ifdef CONFIG_DEBUG_TLBFLUSH
-#ifdef CONFIG_SMP
 		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
 		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
-#endif /* CONFIG_SMP */
 		NR_TLB_LOCAL_FLUSH_ALL,
 		NR_TLB_LOCAL_FLUSH_ONE,
 #endif /* CONFIG_DEBUG_TLBFLUSH */

commit f7ad2a6cb9f7c4040004bedee84a70a9b985583e
Author: Shaohua Li <shli@fb.com>
Date:   Wed May 3 14:52:29 2017 -0700

    mm: move MADV_FREE pages into LRU_INACTIVE_FILE list
    
    madv()'s MADV_FREE indicate pages are 'lazyfree'.  They are still
    anonymous pages, but they can be freed without pageout.  To distinguish
    these from normal anonymous pages, we clear their SwapBacked flag.
    
    MADV_FREE pages could be freed without pageout, so they pretty much like
    used once file pages.  For such pages, we'd like to reclaim them once
    there is memory pressure.  Also it might be unfair reclaiming MADV_FREE
    pages always before used once file pages and we definitively want to
    reclaim the pages before other anonymous and file pages.
    
    To speed up MADV_FREE pages reclaim, we put the pages into
    LRU_INACTIVE_FILE list.  The rationale is LRU_INACTIVE_FILE list is tiny
    nowadays and should be full of used once file pages.  Reclaiming
    MADV_FREE pages will not have much interfere of anonymous and active
    file pages.  And the inactive file pages and MADV_FREE pages will be
    reclaimed according to their age, so we don't reclaim too many MADV_FREE
    pages too.  Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also
    means we can reclaim the pages without swap support.  This idea is
    suggested by Johannes.
    
    This patch doesn't move MADV_FREE pages to LRU_INACTIVE_FILE list yet to
    avoid bisect failure, next patch will do it.
    
    The patch is based on Minchan's original patch.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/2f87063c1e9354677b7618c647abde77b07561e5.1487965799.git.shli@fb.com
    Signed-off-by: Shaohua Li <shli@fb.com>
    Suggested-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index a80b7b59cf33..d84ae90ccd5c 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -25,7 +25,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
 		FOR_ALL_ZONES(ALLOCSTALL),
 		FOR_ALL_ZONES(PGSCAN_SKIP),
-		PGFREE, PGACTIVATE, PGDEACTIVATE,
+		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,
 		PGFAULT, PGMAJFAULT,
 		PGLAZYFREED,
 		PGREFILL,

commit ce9311cf95ad8baf044a014738d76973d93b739a
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Thu Mar 9 16:17:00 2017 -0800

    mm/vmstats: add thp_split_pud event for clarity
    
    We added support for PUD-sized transparent hugepages, however we count
    the event "thp split pud" into thp_split_pmd event.
    
    To separate the event count of thp split pud from pmd, add a new event
    named thp_split_pud.
    
    Link: http://lkml.kernel.org/r/1488282380-5076-1-git-send-email-xieyisheng1@huawei.com
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Ebru Akagunduz <ebru.akagunduz@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hanjun Guo <guohanjun@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 6aa1b6cb5828..a80b7b59cf33 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -79,6 +79,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_SPLIT_PAGE_FAILED,
 		THP_DEFERRED_SPLIT_PAGE,
 		THP_SPLIT_PMD,
+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+		THP_SPLIT_PUD,
+#endif
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif

commit 7f354a548d1cb6bb01b6ee74aee9264aa152f1ec
Author: David Rientjes <rientjes@google.com>
Date:   Wed Feb 22 15:44:50 2017 -0800

    mm, compaction: add vmstats for kcompactd work
    
    A "compact_daemon_wake" vmstat exists that represents the number of
    times kcompactd has woken up.  This doesn't represent how much work it
    actually did, though.
    
    It's useful to understand how much compaction work is being done by
    kcompactd versus other methods such as direct compaction and explicitly
    triggered per-node (or system) compaction.
    
    This adds two new vmstats: "compact_daemon_migrate_scanned" and
    "compact_daemon_free_scanned" to represent the number of pages kcompactd
    has scanned as part of its migration scanner and freeing scanner,
    respectively.
    
    These values are still accounted for in the general
    "compact_migrate_scanned" and "compact_free_scanned" for compatibility.
    
    It could be argued that explicitly triggered compaction could also be
    tracked separately, and that could be added if others find it useful.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1612071749390.69852@chino.kir.corp.google.com
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 4d6ec58a8d45..6aa1b6cb5828 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -56,6 +56,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		COMPACTISOLATED,
 		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
 		KCOMPACTD_WAKE,
+		KCOMPACTD_MIGRATE_SCANNED, KCOMPACTD_FREE_SCANNED,
 #endif
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,

commit 7cc30fcfd2a894589d832a192cac3dc5cd302bb8
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:59 2016 -0700

    mm: vmstat: account per-zone stalls and pages skipped during reclaim
    
    The vmstat allocstall was fairly useful in the general sense but
    node-based LRUs change that.  It's important to know if a stall was for
    an address-limited allocation request as this will require skipping
    pages from other zones.  This patch adds pgstall_* counters to replace
    allocstall.  The sum of the counters will equal the old allocstall so it
    can be trivially recalculated.  A high number of address-limited
    allocation requests may result in a lot of useless LRU scanning for
    suitable pages.
    
    As address-limited allocations require pages to be skipped, it's
    important to know how much useless LRU scanning took place so this patch
    adds pgskip* counters.  This yields the following model
    
    1. The number of address-space limited stalls can be accounted for (pgstall)
    2. The amount of useless work required to reclaim the data is accounted (pgskip)
    3. The total number of scans is available from pgscan_kswapd and pgscan_direct
       so from that the ratio of useful to useless scans can be calculated.
    
    [mgorman@techsingularity.net: s/pgstall/allocstall/]
      Link: http://lkml.kernel.org/r/1468404004-5085-3-git-send-email-mgorman@techsingularity.netLink: http://lkml.kernel.org/r/1467970510-21195-33-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 1798ff542517..4d6ec58a8d45 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -23,6 +23,8 @@
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
+		FOR_ALL_ZONES(ALLOCSTALL),
+		FOR_ALL_ZONES(PGSCAN_SKIP),
 		PGFREE, PGACTIVATE, PGDEACTIVATE,
 		PGFAULT, PGMAJFAULT,
 		PGLAZYFREED,
@@ -37,7 +39,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #endif
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
-		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+		PAGEOUTRUN, PGROTATED,
 		DROP_PAGECACHE, DROP_SLAB,
 #ifdef CONFIG_NUMA_BALANCING
 		NUMA_PTE_UPDATES,

commit 599d0c954f91d0689c9bb421b5bc04ea02437a41
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:31 2016 -0700

    mm, vmscan: move LRU lists to node
    
    This moves the LRU lists from the zone to the node and related data such
    as counters, tracing, congestion tracking and writeback tracking.
    
    Unfortunately, due to reclaim and compaction retry logic, it is
    necessary to account for the number of LRU pages on both zone and node
    logic.  Most reclaim logic is based on the node counters but the retry
    logic uses the zone counters which do not distinguish inactive and
    active sizes.  It would be possible to leave the LRU counters on a
    per-zone basis but it's a heavier calculation across multiple cache
    lines that is much more frequent than the retry checks.
    
    Other than the LRU counters, this is mostly a mechanical patch but note
    that it introduces a number of anomalies.  For example, the scans are
    per-zone but using per-node counters.  We also mark a node as congested
    when a zone is congested.  This causes weird problems that are fixed
    later but is easier to review.
    
    In the event that there is excessive overhead on 32-bit systems due to
    the nodes being on LRU then there are two potential solutions
    
    1. Long-term isolation of highmem pages when reclaim is lowmem
    
       When pages are skipped, they are immediately added back onto the LRU
       list. If lowmem reclaim persisted for long periods of time, the same
       highmem pages get continually scanned. The idea would be that lowmem
       keeps those pages on a separate list until a reclaim for highmem pages
       arrives that splices the highmem pages back onto the LRU. It potentially
       could be implemented similar to the UNEVICTABLE list.
    
       That would reduce the skip rate with the potential corner case is that
       highmem pages have to be scanned and reclaimed to free lowmem slab pages.
    
    2. Linear scan lowmem pages if the initial LRU shrink fails
    
       This will break LRU ordering but may be preferable and faster during
       memory pressure than skipping LRU pages.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-4-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 42604173f122..1798ff542517 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -26,11 +26,11 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGFREE, PGACTIVATE, PGDEACTIVATE,
 		PGFAULT, PGMAJFAULT,
 		PGLAZYFREED,
-		FOR_ALL_ZONES(PGREFILL),
-		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
-		FOR_ALL_ZONES(PGSTEAL_DIRECT),
-		FOR_ALL_ZONES(PGSCAN_KSWAPD),
-		FOR_ALL_ZONES(PGSCAN_DIRECT),
+		PGREFILL,
+		PGSTEAL_KSWAPD,
+		PGSTEAL_DIRECT,
+		PGSCAN_KSWAPD,
+		PGSCAN_DIRECT,
 		PGSCAN_DIRECT_THROTTLE,
 #ifdef CONFIG_NUMA
 		PGSCAN_ZONE_RECLAIM_FAILED,

commit 95ecedcd6abbb05d8177331e2fa697888dcd634b
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Jul 26 15:25:31 2016 -0700

    thp, vmstats: add counters for huge file pages
    
    THP_FILE_ALLOC: how many times huge page was allocated and put page
    cache.
    
    THP_FILE_MAPPED: how many times file huge page was mapped.
    
    Link: http://lkml.kernel.org/r/1466021202-61880-13-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index ec084321fe09..42604173f122 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -70,6 +70,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_FAULT_FALLBACK,
 		THP_COLLAPSE_ALLOC,
 		THP_COLLAPSE_ALLOC_FAILED,
+		THP_FILE_ALLOC,
+		THP_FILE_MAPPED,
 		THP_SPLIT_PAGE,
 		THP_SPLIT_PAGE_FAILED,
 		THP_DEFERRED_SPLIT_PAGE,
@@ -100,4 +102,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
+#ifndef CONFIG_TRANSPARENT_HUGEPAGE
+#define THP_FILE_ALLOC ({ BUILD_BUG(); 0; })
+#define THP_FILE_MAPPED ({ BUILD_BUG(); 0; })
+#endif
+
 #endif		/* VM_EVENT_ITEM_H_INCLUDED */

commit f9719a03de51e13526d614e79d002f838770b2d6
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 17 14:18:45 2016 -0700

    thp, vmstats: count deferred split events
    
    Count how many times we put a THP in split queue.  Currently, it happens
    on partial unmap of a THP.
    
    Rapidly growing value can indicate that an application behaves
    unfriendly wrt THP: often fault in huge page and then unmap part of it.
    This leads to unnecessary memory fragmentation and the application may
    require tuning.
    
    The event also can help with debugging kernel [mis-]behaviour.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 58ecc056ee45..ec084321fe09 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -72,6 +72,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_COLLAPSE_ALLOC_FAILED,
 		THP_SPLIT_PAGE,
 		THP_SPLIT_PAGE_FAILED,
+		THP_DEFERRED_SPLIT_PAGE,
 		THP_SPLIT_PMD,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,

commit 698b1b30642f1ff0ea10ef1de9745ab633031377
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Mar 17 14:18:08 2016 -0700

    mm, compaction: introduce kcompactd
    
    Memory compaction can be currently performed in several contexts:
    
     - kswapd balancing a zone after a high-order allocation failure
     - direct compaction to satisfy a high-order allocation, including THP
       page fault attemps
     - khugepaged trying to collapse a hugepage
     - manually from /proc
    
    The purpose of compaction is two-fold.  The obvious purpose is to
    satisfy a (pending or future) high-order allocation, and is easy to
    evaluate.  The other purpose is to keep overal memory fragmentation low
    and help the anti-fragmentation mechanism.  The success wrt the latter
    purpose is more
    
    The current situation wrt the purposes has a few drawbacks:
    
     - compaction is invoked only when a high-order page or hugepage is not
       available (or manually).  This might be too late for the purposes of
       keeping memory fragmentation low.
     - direct compaction increases latency of allocations.  Again, it would
       be better if compaction was performed asynchronously to keep
       fragmentation low, before the allocation itself comes.
     - (a special case of the previous) the cost of compaction during THP
       page faults can easily offset the benefits of THP.
     - kswapd compaction appears to be complex, fragile and not working in
       some scenarios.  It could also end up compacting for a high-order
       allocation request when it should be reclaiming memory for a later
       order-0 request.
    
    To improve the situation, we should be able to benefit from an
    equivalent of kswapd, but for compaction - i.e. a background thread
    which responds to fragmentation and the need for high-order allocations
    (including hugepages) somewhat proactively.
    
    One possibility is to extend the responsibilities of kswapd, which could
    however complicate its design too much.  It should be better to let
    kswapd handle reclaim, as order-0 allocations are often more critical
    than high-order ones.
    
    Another possibility is to extend khugepaged, but this kthread is a
    single instance and tied to THP configs.
    
    This patch goes with the option of a new set of per-node kthreads called
    kcompactd, and lays the foundations, without introducing any new
    tunables.  The lifecycle mimics kswapd kthreads, including the memory
    hotplug hooks.
    
    For compaction, kcompactd uses the standard compaction_suitable() and
    ompact_finished() criteria and the deferred compaction functionality.
    Unlike direct compaction, it uses only sync compaction, as there's no
    allocation latency to minimize.
    
    This patch doesn't yet add a call to wakeup_kcompactd.  The kswapd
    compact/reclaim loop for high-order pages will be replaced by waking up
    kcompactd in the next patch with the description of what's wrong with
    the old approach.
    
    Waking up of the kcompactd threads is also tied to kswapd activity and
    follows these rules:
     - we don't want to affect any fastpaths, so wake up kcompactd only from
       the slowpath, as it's done for kswapd
     - if kswapd is doing reclaim, it's more important than compaction, so
       don't invoke kcompactd until kswapd goes to sleep
     - the target order used for kswapd is passed to kcompactd
    
    Future possible future uses for kcompactd include the ability to wake up
    kcompactd on demand in special situations, such as when hugepages are
    not available (currently not done due to __GFP_NO_KSWAPD) or when a
    fragmentation event (i.e.  __rmqueue_fallback()) occurs.  It's also
    possible to perform periodic compaction with kcompactd.
    
    [arnd@arndb.de: fix build errors with kcompactd]
    [paul.gortmaker@windriver.com: don't use modular references for non modular code]
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 67c1dbd19c6d..58ecc056ee45 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -53,6 +53,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
 		COMPACTISOLATED,
 		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
+		KCOMPACTD_WAKE,
 #endif
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,

commit 854e9ed09dedf0c19ac8640e91bcc74bc3f9e5c9
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Jan 15 16:54:53 2016 -0800

    mm: support madvise(MADV_FREE)
    
    Linux doesn't have an ability to free pages lazy while other OS already
    have been supported that named by madvise(MADV_FREE).
    
    The gain is clear that kernel can discard freed pages rather than
    swapping out or OOM if memory pressure happens.
    
    Without memory pressure, freed pages would be reused by userspace
    without another additional overhead(ex, page fault + allocation +
    zeroing).
    
    Jason Evans said:
    
    : Facebook has been using MAP_UNINITIALIZED
    : (https://lkml.org/lkml/2012/1/18/308) in some of its applications for
    : several years, but there are operational costs to maintaining this
    : out-of-tree in our kernel and in jemalloc, and we are anxious to retire it
    : in favor of MADV_FREE.  When we first enabled MAP_UNINITIALIZED it
    : increased throughput for much of our workload by ~5%, and although the
    : benefit has decreased using newer hardware and kernels, there is still
    : enough benefit that we cannot reasonably retire it without a replacement.
    :
    : Aside from Facebook operations, there are numerous broadly used
    : applications that would benefit from MADV_FREE.  The ones that immediately
    : come to mind are redis, varnish, and MariaDB.  I don't have much insight
    : into Android internals and development process, but I would hope to see
    : MADV_FREE support eventually end up there as well to benefit applications
    : linked with the integrated jemalloc.
    :
    : jemalloc will use MADV_FREE once it becomes available in the Linux kernel.
    : In fact, jemalloc already uses MADV_FREE or equivalent everywhere it's
    : available: *BSD, OS X, Windows, and Solaris -- every platform except Linux
    : (and AIX, but I'm not sure it even compiles on AIX).  The lack of
    : MADV_FREE on Linux forced me down a long series of increasingly
    : sophisticated heuristics for madvise() volume reduction, and even so this
    : remains a common performance issue for people using jemalloc on Linux.
    : Please integrate MADV_FREE; many people will benefit substantially.
    
    How it works:
    
    When madvise syscall is called, VM clears dirty bit of ptes of the
    range.  If memory pressure happens, VM checks dirty bit of page table
    and if it found still "clean", it means it's a "lazyfree pages" so VM
    could discard the page instead of swapping out.  Once there was store
    operation for the page before VM peek a page to reclaim, dirty bit is
    set so VM can swap out the page instead of discarding.
    
    One thing we should notice is that basically, MADV_FREE relies on dirty
    bit in page table entry to decide whether VM allows to discard the page
    or not.  IOW, if page table entry includes marked dirty bit, VM
    shouldn't discard the page.
    
    However, as a example, if swap-in by read fault happens, page table
    entry doesn't have dirty bit so MADV_FREE could discard the page
    wrongly.
    
    For avoiding the problem, MADV_FREE did more checks with PageDirty and
    PageSwapCache.  It worked out because swapped-in page lives on swap
    cache and since it is evicted from the swap cache, the page has PG_dirty
    flag.  So both page flags check effectively prevent wrong discarding by
    MADV_FREE.
    
    However, a problem in above logic is that swapped-in page has PG_dirty
    still after they are removed from swap cache so VM cannot consider the
    page as freeable any more even if madvise_free is called in future.
    
    Look at below example for detail.
    
        ptr = malloc();
        memset(ptr);
        ..
        ..
        .. heavy memory pressure so all of pages are swapped out
        ..
        ..
        var = *ptr; -> a page swapped-in and could be removed from
                       swapcache. Then, page table doesn't mark
                       dirty bit and page descriptor includes PG_dirty
        ..
        ..
        madvise_free(ptr); -> It doesn't clear PG_dirty of the page.
        ..
        ..
        ..
        .. heavy memory pressure again.
        .. In this time, VM cannot discard the page because the page
        .. has *PG_dirty*
    
    To solve the problem, this patch clears PG_dirty if only the page is
    owned exclusively by current process when madvise is called because
    PG_dirty represents ptes's dirtiness in several processes so we could
    clear it only if we own it exclusively.
    
    Firstly, heavy users would be general allocators(ex, jemalloc, tcmalloc
    and hope glibc supports it) and jemalloc/tcmalloc already have supported
    the feature for other OS(ex, FreeBSD)
    
      barrios@blaptop:~/benchmark/ebizzy$ lscpu
      Architecture:          x86_64
      CPU op-mode(s):        32-bit, 64-bit
      Byte Order:            Little Endian
      CPU(s):                12
      On-line CPU(s) list:   0-11
      Thread(s) per core:    1
      Core(s) per socket:    1
      Socket(s):             12
      NUMA node(s):          1
      Vendor ID:             GenuineIntel
      CPU family:            6
      Model:                 2
      Stepping:              3
      CPU MHz:               3200.185
      BogoMIPS:              6400.53
      Virtualization:        VT-x
      Hypervisor vendor:     KVM
      Virtualization type:   full
      L1d cache:             32K
      L1i cache:             32K
      L2 cache:              4096K
      NUMA node0 CPU(s):     0-11
      ebizzy benchmark(./ebizzy -S 10 -n 512)
    
      Higher avg is better.
    
       vanilla-jemalloc             MADV_free-jemalloc
    
      1 thread
      records: 10                   records: 10
      avg:   2961.90                avg:  12069.70
      std:     71.96(2.43%)         std:    186.68(1.55%)
      max:   3070.00                max:  12385.00
      min:   2796.00                min:  11746.00
    
      2 thread
      records: 10                   records: 10
      avg:   5020.00                avg:  17827.00
      std:    264.87(5.28%)         std:    358.52(2.01%)
      max:   5244.00                max:  18760.00
      min:   4251.00                min:  17382.00
    
      4 thread
      records: 10                   records: 10
      avg:   8988.80                avg:  27930.80
      std:   1175.33(13.08%)        std:   3317.33(11.88%)
      max:   9508.00                max:  30879.00
      min:   5477.00                min:  21024.00
    
      8 thread
      records: 10                   records: 10
      avg:  13036.50                avg:  33739.40
      std:    170.67(1.31%)         std:   5146.22(15.25%)
      max:  13371.00                max:  40572.00
      min:  12785.00                min:  24088.00
    
      16 thread
      records: 10                   records: 10
      avg:  11092.40                avg:  31424.20
      std:    710.60(6.41%)         std:   3763.89(11.98%)
      max:  12446.00                max:  36635.00
      min:   9949.00                min:  25669.00
    
      32 thread
      records: 10                   records: 10
      avg:  11067.00                avg:  34495.80
      std:    971.06(8.77%)         std:   2721.36(7.89%)
      max:  12010.00                max:  38598.00
      min:   9002.00                min:  30636.00
    
    In summary, MADV_FREE is about much faster than MADV_DONTNEED.
    
    This patch (of 12):
    
    Add core MADV_FREE implementation.
    
    [akpm@linux-foundation.org: small cleanups]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Mika Penttil <mika.penttila@nextfour.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jason Evans <je@fb.com>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: <yalin.wang2010@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: "Shaohua Li" <shli@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Gang <gang.chen.5i5j@gmail.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index e1f8c993e73b..67c1dbd19c6d 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -25,6 +25,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
 		PGFREE, PGACTIVATE, PGDEACTIVATE,
 		PGFAULT, PGMAJFAULT,
+		PGLAZYFREED,
 		FOR_ALL_ZONES(PGREFILL),
 		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
 		FOR_ALL_ZONES(PGSTEAL_DIRECT),

commit 122afea9626ab3f717b250a8dd3d5ebf57cdb56c
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:52:46 2016 -0800

    mm, vmstats: new THP splitting event
    
    The patch replaces THP_SPLIT with tree events: THP_SPLIT_PAGE,
    THP_SPLIT_PAGE_FAILED and THP_SPLIT_PMD.  It reflects the fact that we
    are going to be able split PMD without the compound page and that
    split_huge_page() can fail.
    
    Signed-off-by: Kirill A.  Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index e623d392db0c..e1f8c993e73b 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -68,7 +68,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_FAULT_FALLBACK,
 		THP_COLLAPSE_ALLOC,
 		THP_COLLAPSE_ALLOC_FAILED,
-		THP_SPLIT,
+		THP_SPLIT_PAGE,
+		THP_SPLIT_PAGE_FAILED,
+		THP_SPLIT_PMD,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif

commit f7ae3a95eab4e6766768cef664d2d429f53bd5f4
Author: yalin wang <yalin.wang2010@gmail.com>
Date:   Thu Nov 5 18:48:50 2015 -0800

    include/linux/vm_event_item.h: change HIGHMEM_ZONE macro definition
    
    Change HIGHMEM_ZONE to be the same as the DMA_ZONE macro.
    
    Signed-off-by: yalin wang <yalin.wang2010@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 9246d32dc973..e623d392db0c 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -14,12 +14,12 @@
 #endif
 
 #ifdef CONFIG_HIGHMEM
-#define HIGHMEM_ZONE(xx) , xx##_HIGH
+#define HIGHMEM_ZONE(xx) xx##_HIGH,
 #else
 #define HIGHMEM_ZONE(xx)
 #endif
 
-#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
+#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL, HIGHMEM_ZONE(xx) xx##_MOVABLE
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),

commit f5f302e21257ebb0c074bbafc37606c26d28cc3d
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Dec 12 16:56:10 2014 -0800

    mm,vmacache: count number of system-wide flushes
    
    These flushes deal with sequence number overflows, such as for long lived
    threads.  These are rare, but interesting from a debugging PoV.  As such,
    display the number of flushes when vmacache debugging is enabled.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 730334cdf037..9246d32dc973 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -90,6 +90,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #ifdef CONFIG_DEBUG_VM_VMACACHE
 		VMACACHE_FIND_CALLS,
 		VMACACHE_FIND_HITS,
+		VMACACHE_FULL_FLUSHES,
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit 09316c09dde33aae14f34489d9e3d243ec0d5938
Author: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
Date:   Thu Oct 9 15:29:32 2014 -0700

    mm/balloon_compaction: add vmstat counters and kpageflags bit
    
    Always mark pages with PageBalloon even if balloon compaction is disabled
    and expose this mark in /proc/kpageflags as KPF_BALLOON.
    
    Also this patch adds three counters into /proc/vmstat: "balloon_inflate",
    "balloon_deflate" and "balloon_migrate".  They accumulate balloon
    activity.  Current size of balloon is (balloon_inflate - balloon_deflate)
    pages.
    
    All generic balloon code now gathered under option CONFIG_MEMORY_BALLOON.
    It should be selected by ballooning driver which wants use this feature.
    Currently virtio-balloon is the only user.
    
    Signed-off-by: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index ced92345c963..730334cdf037 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -72,6 +72,13 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif
+#ifdef CONFIG_MEMORY_BALLOON
+		BALLOON_INFLATE,
+		BALLOON_DEFLATE,
+#ifdef CONFIG_BALLOON_COMPACTION
+		BALLOON_MIGRATE,
+#endif
+#endif
 #ifdef CONFIG_DEBUG_TLBFLUSH
 #ifdef CONFIG_SMP
 		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */

commit 4f115147ff802267d0aa41e361c5aa5bd933d896
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Wed Jun 4 16:06:46 2014 -0700

    mm,vmacache: add debug data
    
    Introduce a CONFIG_DEBUG_VM_VMACACHE option to enable counting the cache
    hit rate -- exported in /proc/vmstat.
    
    Any updates to the caching scheme needs this kind of data, thus it can
    save some work re-implementing the counting all the time.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 486c3972c0be..ced92345c963 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -80,6 +80,10 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_TLB_LOCAL_FLUSH_ALL,
 		NR_TLB_LOCAL_FLUSH_ONE,
 #endif /* CONFIG_DEBUG_TLBFLUSH */
+#ifdef CONFIG_DEBUG_VM_VMACACHE
+		VMACACHE_FIND_CALLS,
+		VMACACHE_FIND_HITS,
+#endif
 		NR_VM_EVENT_ITEMS
 };
 

commit 5509a5d27b971a90b940e148ca9ca53312e4fa7a
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Thu Apr 3 14:48:19 2014 -0700

    drop_caches: add some documentation and info message
    
    There is plenty of anecdotal evidence and a load of blog posts
    suggesting that using "drop_caches" periodically keeps your system
    running in "tip top shape".  Perhaps adding some kernel documentation
    will increase the amount of accurate data on its use.
    
    If we are not shrinking caches effectively, then we have real bugs.
    Using drop_caches will simply mask the bugs and make them harder to
    find, but certainly does not fix them, nor is it an appropriate
    "workaround" to limit the size of the caches.  On the contrary, there
    have been bug reports on issues that turned out to be misguided use of
    cache dropping.
    
    Dropping caches is a very drastic and disruptive operation that is good
    for debugging and running tests, but if it creates bug reports from
    production use, kernel developers should be aware of its use.
    
    Add a bit more documentation about it, a syslog message to track down
    abusers, and vmstat drop counters to help analyze problem reports.
    
    [akpm@linux-foundation.org: checkpatch fixes]
    [hannes@cmpxchg.org: add runtime suppression control]
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 3a712e2e7d76..486c3972c0be 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -37,6 +37,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+		DROP_PAGECACHE, DROP_SLAB,
 #ifdef CONFIG_NUMA_BALANCING
 		NUMA_PTE_UPDATES,
 		NUMA_HUGE_PTE_UPDATES,

commit ec65993443736a5091b68e80ff1734548944a4b8
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 14:33:16 2014 -0800

    mm, x86: Account for TLB flushes only when debugging
    
    Bisection between 3.11 and 3.12 fingered commit 9824cf97 ("mm:
    vmstats: tlb flush counters") to cause overhead problems.
    
    The counters are undeniably useful but how often do we really
    need to debug TLB flush related issues?  It does not justify
    taking the penalty everywhere so make it a debugging option.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-XzxjntugxuwpxXhcrxqqh53b@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index c557c6d096de..3a712e2e7d76 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -71,12 +71,14 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif
+#ifdef CONFIG_DEBUG_TLBFLUSH
 #ifdef CONFIG_SMP
 		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
 		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
-#endif
+#endif /* CONFIG_SMP */
 		NR_TLB_LOCAL_FLUSH_ALL,
 		NR_TLB_LOCAL_FLUSH_ONE,
+#endif /* CONFIG_DEBUG_TLBFLUSH */
 		NR_VM_EVENT_ITEMS
 };
 

commit 72403b4a0fbdf433c1fe0127e49864658f6f6468
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Nov 12 15:08:32 2013 -0800

    mm: numa: return the number of base pages altered by protection changes
    
    Commit 0255d4918480 ("mm: Account for a THP NUMA hinting update as one
    PTE update") was added to account for the number of PTE updates when
    marking pages prot_numa.  task_numa_work was using the old return value
    to track how much address space had been updated.  Altering the return
    value causes the scanner to do more work than it is configured or
    documented to in a single unit of work.
    
    This patch reverts that commit and accounts for the number of THP
    updates separately in vmstat.  It is up to the administrator to
    interpret the pair of values correctly.  This is a straight-forward
    operation and likely to only be of interest when actively debugging NUMA
    balancing problems.
    
    The impact of this patch is that the NUMA PTE scanner will scan slower
    when THP is enabled and workloads may converge slower as a result.  On
    the flip size system CPU usage should be lower than recent tests
    reported.  This is an illustrative example of a short single JVM specjbb
    test
    
    specjbb
                           3.12.0                3.12.0
                          vanilla      acctupdates
    TPut 1      26143.00 (  0.00%)     25747.00 ( -1.51%)
    TPut 7     185257.00 (  0.00%)    183202.00 ( -1.11%)
    TPut 13    329760.00 (  0.00%)    346577.00 (  5.10%)
    TPut 19    442502.00 (  0.00%)    460146.00 (  3.99%)
    TPut 25    540634.00 (  0.00%)    549053.00 (  1.56%)
    TPut 31    512098.00 (  0.00%)    519611.00 (  1.47%)
    TPut 37    461276.00 (  0.00%)    474973.00 (  2.97%)
    TPut 43    403089.00 (  0.00%)    414172.00 (  2.75%)
    
                  3.12.0      3.12.0
                 vanillaacctupdates
    User         5169.64     5184.14
    System        100.45       80.02
    Elapsed       252.75      251.85
    
    Performance is similar but note the reduction in system CPU time.  While
    this showed a performance gain, it will not be universal but at least
    it'll be behaving as documented.  The vmstats are obviously different but
    here is an obvious interpretation of them from mmtests.
    
                                    3.12.0      3.12.0
                                   vanillaacctupdates
    NUMA page range updates        1408326    11043064
    NUMA huge PMD updates                0       21040
    NUMA PTE updates               1408326      291624
    
    "NUMA page range updates" == nr_pte_updates and is the value returned to
    the NUMA pte scanner.  NUMA huge PMD updates were the number of THP
    updates which in combination can be used to calculate how many ptes were
    updated from userspace.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-by: Alex Thorlton <athorlton@sgi.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 1855f0a22add..c557c6d096de 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -39,6 +39,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_NUMA_BALANCING
 		NUMA_PTE_UPDATES,
+		NUMA_HUGE_PTE_UPDATES,
 		NUMA_HINT_FAULTS,
 		NUMA_HINT_FAULTS_LOCAL,
 		NUMA_PAGE_MIGRATE,

commit 6df46865ff8715932e7d42e52cac17e8461758cb
Author: Dave Hansen <dave@sr71.net>
Date:   Wed Sep 11 14:20:24 2013 -0700

    mm: vmstats: track TLB flush stats on UP too
    
    The previous patch doing vmstats for TLB flushes ("mm: vmstats: tlb flush
    counters") effectively missed UP since arch/x86/mm/tlb.c is only compiled
    for SMP.
    
    UP systems do not do remote TLB flushes, so compile those counters out on
    UP.
    
    arch/x86/kernel/cpu/mtrr/generic.c calls __flush_tlb() directly.  This is
    probably an optimization since both the mtrr code and __flush_tlb() write
    cr4.  It would probably be safe to make that a flush_tlb_all() (and then
    get these statistics), but the mtrr code is ancient and I'm hesitant to
    touch it other than to just stick in the counters.
    
    [akpm@linux-foundation.org: tweak comments]
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index dc2cdf07ac14..1855f0a22add 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -70,11 +70,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif
+#ifdef CONFIG_SMP
 		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
 		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
+#endif
 		NR_TLB_LOCAL_FLUSH_ALL,
 		NR_TLB_LOCAL_FLUSH_ONE,
-		NR_TLB_LOCAL_FLUSH_ONE_KERNEL,
 		NR_VM_EVENT_ITEMS
 };
 

commit 9824cf9753ecbe8f5b47aa9b2f218207defea211
Author: Dave Hansen <dave@sr71.net>
Date:   Wed Sep 11 14:20:23 2013 -0700

    mm: vmstats: tlb flush counters
    
    I was investigating some TLB flush scaling issues and realized that we do
    not have any good methods for figuring out how many TLB flushes we are
    doing.
    
    It would be nice to be able to do these in generic code, but the
    arch-independent calls don't explicitly specify whether we actually need
    to do remote flushes or not.  In the end, we really need to know if we
    actually _did_ global vs.  local invalidations, so that leaves us with few
    options other than to muck with the counters from arch-specific code.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index bd6cf61142be..dc2cdf07ac14 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -70,6 +70,11 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_ZERO_PAGE_ALLOC,
 		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif
+		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
+		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
+		NR_TLB_LOCAL_FLUSH_ALL,
+		NR_TLB_LOCAL_FLUSH_ONE,
+		NR_TLB_LOCAL_FLUSH_ONE_KERNEL,
 		NR_VM_EVENT_ITEMS
 };
 

commit 258401a60c4df39332f30ef57afbc6dbf29a7e84
Author: Zlatko Calusic <zlatko.calusic@iskon.hr>
Date:   Fri Feb 22 16:34:06 2013 -0800

    mm: don't wait on congested zones in balance_pgdat()
    
    From: Zlatko Calusic <zlatko.calusic@iskon.hr>
    
    Commit 92df3a723f84 ("mm: vmscan: throttle reclaim if encountering too
    many dirty pages under writeback") introduced waiting on congested zones
    based on a sane algorithm in shrink_inactive_list().
    
    What this means is that there's no more need for throttling and
    additional heuristics in balance_pgdat().  So, let's remove it and tidy
    up the code.
    
    Signed-off-by: Zlatko Calusic <zlatko.calusic@iskon.hr>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index fce0a2799d43..bd6cf61142be 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -36,7 +36,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #endif
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
-		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_NUMA_BALANCING
 		NUMA_PTE_UPDATES,

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit d8a8e1f0da3d29d7268b3300c96a059d63901b76
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Dec 12 13:51:09 2012 -0800

    thp, vmstat: implement HZP_ALLOC and HZP_ALLOC_FAILED events
    
    hzp_alloc is incremented every time a huge zero page is successfully
            allocated. It includes allocations which where dropped due
            race with other allocation. Note, it doesn't count every map
            of the huge zero page, only its allocation.
    
    hzp_alloc_failed is incremented if kernel fails to allocate huge zero
            page and falls back to using small pages.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 3d3114594370..fe786f07d2bd 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -58,6 +58,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		THP_COLLAPSE_ALLOC,
 		THP_COLLAPSE_ALLOC_FAILED,
 		THP_SPLIT,
+		THP_ZERO_PAGE_ALLOC,
+		THP_ZERO_PAGE_ALLOC_FAILED,
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit 03c5a6e16322c997bf8f264851bfa3f532ad515f
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Nov 2 14:52:48 2012 +0000

    mm: numa: Add pte updates, hinting and migration stats
    
    It is tricky to quantify the basic cost of automatic NUMA placement in a
    meaningful manner. This patch adds some vmstats that can be used as part
    of a basic costing model.
    
    u    = basic unit = sizeof(void *)
    Ca   = cost of struct page access = sizeof(struct page) / u
    Cpte = Cost PTE access = Ca
    Cupdate = Cost PTE update = (2 * Cpte) + (2 * Wlock)
            where Cpte is incurred twice for a read and a write and Wlock
            is a constant representing the cost of taking or releasing a
            lock
    Cnumahint = Cost of a minor page fault = some high constant e.g. 1000
    Cpagerw = Cost to read or write a full page = Ca + PAGE_SIZE/u
    Ci = Cost of page isolation = Ca + Wi
            where Wi is a constant that should reflect the approximate cost
            of the locking operation
    Cpagecopy = Cpagerw + (Cpagerw * Wnuma) + Ci + (Ci * Wnuma)
            where Wnuma is the approximate NUMA factor. 1 is local. 1.2
            would imply that remote accesses are 20% more expensive
    
    Balancing cost = Cpte * numa_pte_updates +
                    Cnumahint * numa_hint_faults +
                    Ci * numa_pages_migrated +
                    Cpagecopy * numa_pages_migrated
    
    Note that numa_pages_migrated is used as a measure of how many pages
    were isolated even though it would miss pages that failed to migrate. A
    vmstat counter could have been added for it but the isolation cost is
    pretty marginal in comparison to the overall cost so it seemed overkill.
    
    The ideal way to measure automatic placement benefit would be to count
    the number of remote accesses versus local accesses and do something like
    
            benefit = (remote_accesses_before - remove_access_after) * Wnuma
    
    but the information is not readily available. As a workload converges, the
    expection would be that the number of remote numa hints would reduce to 0.
    
            convergence = numa_hint_faults_local / numa_hint_faults
                    where this is measured for the last N number of
                    numa hints recorded. When the workload is fully
                    converged the value is 1.
    
    This can measure if the placement policy is converging and how fast it is
    doing it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index a1f750b8e72a..55600049e794 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -38,6 +38,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+#ifdef CONFIG_NUMA_BALANCING
+		NUMA_PTE_UPDATES,
+		NUMA_HINT_FAULTS,
+		NUMA_HINT_FAULTS_LOCAL,
+		NUMA_PAGE_MIGRATE,
+#endif
 #ifdef CONFIG_MIGRATION
 		PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
 #endif

commit 397487db696cae0b026a474a5cd66f4e372995e6
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 12:00:10 2012 +0100

    mm: compaction: Add scanned and isolated counters for compaction
    
    Compaction already has tracepoints to count scanned and isolated pages
    but it requires that ftrace be enabled and if that information has to be
    written to disk then it can be disruptive. This patch adds vmstat counters
    for compaction called compact_migrate_scanned, compact_free_scanned and
    compact_isolated.
    
    With these counters, it is possible to define a basic cost model for
    compaction. This approximates of how much work compaction is doing and can
    be compared that with an oprofile showing TLB misses and see if the cost of
    compaction is being offset by THP for example. Minimally a compaction patch
    can be evaluated in terms of whether it increases or decreases cost. The
    basic cost model looks like this
    
    Fundamental unit u:     a word  sizeof(void *)
    
    Ca  = cost of struct page access = sizeof(struct page) / u
    
    Cmc = Cost migrate page copy = (Ca + PAGE_SIZE/u) * 2
    Cmf = Cost migrate failure   = Ca * 2
    Ci  = Cost page isolation    = (Ca + Wi)
            where Wi is a constant that should reflect the approximate
            cost of the locking operation.
    
    Csm = Cost migrate scanning = Ca
    Csf = Cost free    scanning = Ca
    
    Overall cost =  (Csm * compact_migrate_scanned) +
                    (Csf * compact_free_scanned)    +
                    (Ci  * compact_isolated)        +
                    (Cmc * pgmigrate_success)       +
                    (Cmf * pgmigrate_failed)
    
    Where the values are read from /proc/vmstat.
    
    This is very basic and ignores certain costs such as the allocation cost
    to do a migrate page copy but any improvement to the model would still
    use the same vmstat counters.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 8aa7cb94b5fb..a1f750b8e72a 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -42,6 +42,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
 #endif
 #ifdef CONFIG_COMPACTION
+		COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
+		COMPACTISOLATED,
 		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
 #endif
 #ifdef CONFIG_HUGETLB_PAGE

commit 5647bc293ab15f66a7b1cda850c5e9d162a6c7c2
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 10:46:20 2012 +0100

    mm: compaction: Move migration fail/success stats to migrate.c
    
    The compact_pages_moved and compact_pagemigrate_failed events are
    convenient for determining if compaction is active and to what
    degree migration is succeeding but it's at the wrong level. Other
    users of migration may also want to know if migration is working
    properly and this will be particularly true for any automated
    NUMA migration. This patch moves the counters down to migration
    with the new events called pgmigrate_success and pgmigrate_fail.
    The compact_blocks_moved counter is removed because while it was
    useful for debugging initially, it's worthless now as no meaningful
    conclusions can be drawn from its value.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 3d3114594370..8aa7cb94b5fb 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -38,8 +38,10 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+#ifdef CONFIG_MIGRATION
+		PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
+#endif
 #ifdef CONFIG_COMPACTION
-		COMPACTBLOCKS, COMPACTPAGES, COMPACTPAGEFAILED,
 		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
 #endif
 #ifdef CONFIG_HUGETLB_PAGE

commit 8befedfe678ff84d5bc356be4f3cb1fd84959d02
Author: Hugh Dickins <hughd@google.com>
Date:   Mon Oct 8 16:33:42 2012 -0700

    mm: remove unevictable_pgs_mlockfreed
    
    Simply remove UNEVICTABLE_MLOCKFREED and unevictable_pgs_mlockfreed line
    from /proc/vmstat: Johannes and Mel point out that it was very unlikely to
    have been used by any tool, and of course we can restore it easily enough
    if that turns out to be wrong.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Ying Han <yinghan@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index ede4ddd46695..3d3114594370 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -52,7 +52,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGMUNLOCKED,
 		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
 		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
-		UNEVICTABLE_MLOCKFREED,	/* no longer useful: always zero */
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 		THP_FAULT_ALLOC,
 		THP_FAULT_FALLBACK,

commit a0c5e813f087dffc0d9b173d2e7d3328b1482fd5
Author: Hugh Dickins <hughd@google.com>
Date:   Mon Oct 8 16:33:21 2012 -0700

    mm: remove free_page_mlock
    
    We should not be seeing non-0 unevictable_pgs_mlockfreed any longer.  So
    remove free_page_mlock() from the page freeing paths: __PG_MLOCKED is
    already in PAGE_FLAGS_CHECK_AT_FREE, so free_pages_check() will now be
    checking it, reporting "BUG: Bad page state" if it's ever found set.
    Comment UNEVICTABLE_MLOCKFREED and unevictable_pgs_mlockfreed always 0.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Ying Han <yinghan@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 57f7b1091511..ede4ddd46695 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -52,7 +52,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGMUNLOCKED,
 		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
 		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
-		UNEVICTABLE_MLOCKFREED,
+		UNEVICTABLE_MLOCKFREED,	/* no longer useful: always zero */
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 		THP_FAULT_ALLOC,
 		THP_FAULT_FALLBACK,

commit 68243e76ee343d63c6cf76978588a885951e2818
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:39 2012 -0700

    mm: account for the number of times direct reclaimers get throttled
    
    Under significant pressure when writing back to network-backed storage,
    direct reclaimers may get throttled.  This is expected to be a short-lived
    event and the processes get woken up again but processes do get stalled.
    This patch counts how many times such stalling occurs.  It's up to the
    administrator whether to reduce these stalls by increasing
    min_free_kbytes.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 06f8e3858251..57f7b1091511 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -30,6 +30,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGSTEAL_DIRECT),
 		FOR_ALL_ZONES(PGSCAN_KSWAPD),
 		FOR_ALL_ZONES(PGSCAN_DIRECT),
+		PGSCAN_DIRECT_THROTTLE,
 #ifdef CONFIG_NUMA
 		PGSCAN_ZONE_RECLAIM_FAILED,
 #endif

commit 904249aa68010c8e223263c922fcbb840a3f42e4
Author: Ying Han <yinghan@google.com>
Date:   Wed Apr 25 16:01:48 2012 -0700

    mm: fix up the vmscan stat in vmstat
    
    The "pgsteal" stat is confusing because it counts both direct reclaim as
    well as background reclaim.  However, we have "kswapd_steal" which also
    counts background reclaim value.
    
    This patch fixes it and also makes it match the existng "pgscan_" stats.
    
    Test:
    pgsteal_kswapd_dma32 447623
    pgsteal_kswapd_normal 42272677
    pgsteal_kswapd_movable 0
    pgsteal_direct_dma32 2801
    pgsteal_direct_normal 44353270
    pgsteal_direct_movable 0
    
    Signed-off-by: Ying Han <yinghan@google.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index 03b90cdc1921..06f8e3858251 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -26,13 +26,14 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGFREE, PGACTIVATE, PGDEACTIVATE,
 		PGFAULT, PGMAJFAULT,
 		FOR_ALL_ZONES(PGREFILL),
-		FOR_ALL_ZONES(PGSTEAL),
+		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
+		FOR_ALL_ZONES(PGSTEAL_DIRECT),
 		FOR_ALL_ZONES(PGSCAN_KSWAPD),
 		FOR_ALL_ZONES(PGSCAN_DIRECT),
 #ifdef CONFIG_NUMA
 		PGSCAN_ZONE_RECLAIM_FAILED,
 #endif
-		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
+		PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,

commit f042e707ee671e4beb5389abeb9a1819a2cf5532
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu May 26 16:25:24 2011 -0700

    mm: move enum vm_event_item into a standalone header file
    
    enums are problematic because they cannot be forward-declared:
    
      akpm2:/home/akpm> cat t.c
    
      enum foo;
    
      static inline void bar(enum foo f)
      {
      }
      akpm2:/home/akpm> gcc -c t.c
      t.c:4: error: parameter 1 ('f') has incomplete type
    
    So move the enum's definition into a standalone header file which can be used
    wherever its definition is needed.
    
    Cc: Ying Han <yinghan@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
new file mode 100644
index 000000000000..03b90cdc1921
--- /dev/null
+++ b/include/linux/vm_event_item.h
@@ -0,0 +1,64 @@
+#ifndef VM_EVENT_ITEM_H_INCLUDED
+#define VM_EVENT_ITEM_H_INCLUDED
+
+#ifdef CONFIG_ZONE_DMA
+#define DMA_ZONE(xx) xx##_DMA,
+#else
+#define DMA_ZONE(xx)
+#endif
+
+#ifdef CONFIG_ZONE_DMA32
+#define DMA32_ZONE(xx) xx##_DMA32,
+#else
+#define DMA32_ZONE(xx)
+#endif
+
+#ifdef CONFIG_HIGHMEM
+#define HIGHMEM_ZONE(xx) , xx##_HIGH
+#else
+#define HIGHMEM_ZONE(xx)
+#endif
+
+#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
+
+enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
+		FOR_ALL_ZONES(PGALLOC),
+		PGFREE, PGACTIVATE, PGDEACTIVATE,
+		PGFAULT, PGMAJFAULT,
+		FOR_ALL_ZONES(PGREFILL),
+		FOR_ALL_ZONES(PGSTEAL),
+		FOR_ALL_ZONES(PGSCAN_KSWAPD),
+		FOR_ALL_ZONES(PGSCAN_DIRECT),
+#ifdef CONFIG_NUMA
+		PGSCAN_ZONE_RECLAIM_FAILED,
+#endif
+		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
+		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
+		KSWAPD_SKIP_CONGESTION_WAIT,
+		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+#ifdef CONFIG_COMPACTION
+		COMPACTBLOCKS, COMPACTPAGES, COMPACTPAGEFAILED,
+		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
+#endif
+#ifdef CONFIG_HUGETLB_PAGE
+		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
+#endif
+		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
+		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
+		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
+		UNEVICTABLE_PGMLOCKED,
+		UNEVICTABLE_PGMUNLOCKED,
+		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
+		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
+		UNEVICTABLE_MLOCKFREED,
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+		THP_FAULT_ALLOC,
+		THP_FAULT_FALLBACK,
+		THP_COLLAPSE_ALLOC,
+		THP_COLLAPSE_ALLOC_FAILED,
+		THP_SPLIT,
+#endif
+		NR_VM_EVENT_ITEMS
+};
+
+#endif		/* VM_EVENT_ITEM_H_INCLUDED */
