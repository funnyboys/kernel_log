commit 1cd925d583857ee3ead6cfbf1e4b1cd067d28591
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 4 14:48:01 2020 +0200

    bdi: remove the name field in struct backing_dev_info
    
    The name is only printed for a not registered bdi in writeback.  Use the
    device name there as is more useful anyway for the unlike case that the
    warning triggers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 7367150f962a..90a7e844a098 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -193,8 +193,6 @@ struct backing_dev_info {
 	congested_fn *congested_fn; /* Function pointer if device is md/dm */
 	void *congested_data;	/* Pointer to aux data for congested func */
 
-	const char *name;
-
 	struct kref refcnt;	/* Reference counter for the structure */
 	unsigned int capabilities; /* Device capabilities */
 	unsigned int min_ratio;

commit 6bd87eec23cbc9ed222bed0f5b5b02bf300e9a8d
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 4 14:47:56 2020 +0200

    bdi: add a ->dev_name field to struct backing_dev_info
    
    Cache a copy of the name for the life time of the backing_dev_info
    structure so that we can reference it even after unregistering.
    
    Fixes: 68f23b89067f ("memcg: fix a crash in wb_workfn when a device disappears")
    Reported-by: Yufen Yu <yuyufen@huawei.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index ee577a83cfe6..7367150f962a 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -219,6 +219,7 @@ struct backing_dev_info {
 	wait_queue_head_t wb_waitq;
 
 	struct device *dev;
+	char dev_name[64];
 	struct device *owner;
 
 	struct timer_list laptop_mode_wb_timer;

commit c4b4c2a78a9fc0c532c58504e8cb5441224ff1d9
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Apr 13 13:12:10 2020 +0800

    buffer: remove useless comment and WB_REASON_FREE_MORE_MEM, reason.
    
    free_more_memory func has been completely removed in commit bc48f001de12
    ("buffer: eliminate the need to call free_more_memory() in __getblk_slow()")
    
    So comment and `WB_REASON_FREE_MORE_MEM` reason about free_more_memory
    are no longer needed.
    
    Fixes: bc48f001de12 ("buffer: eliminate the need to call free_more_memory() in __getblk_slow()")
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 4fc87dee005a..ee577a83cfe6 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -54,7 +54,6 @@ enum wb_reason {
 	WB_REASON_SYNC,
 	WB_REASON_PERIODIC,
 	WB_REASON_LAPTOP_TIMER,
-	WB_REASON_FREE_MORE_MEM,
 	WB_REASON_FS_FREE_SPACE,
 	/*
 	 * There is no bdi forker thread any more and works are done

commit 97b27821b4854ca744946dae32a3f2fd55bcd5bc
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 26 09:06:56 2019 -0700

    writeback, memcg: Implement foreign dirty flushing
    
    There's an inherent mismatch between memcg and writeback.  The former
    trackes ownership per-page while the latter per-inode.  This was a
    deliberate design decision because honoring per-page ownership in the
    writeback path is complicated, may lead to higher CPU and IO overheads
    and deemed unnecessary given that write-sharing an inode across
    different cgroups isn't a common use-case.
    
    Combined with inode majority-writer ownership switching, this works
    well enough in most cases but there are some pathological cases.  For
    example, let's say there are two cgroups A and B which keep writing to
    different but confined parts of the same inode.  B owns the inode and
    A's memory is limited far below B's.  A's dirty ratio can rise enough
    to trigger balance_dirty_pages() sleeps but B's can be low enough to
    avoid triggering background writeback.  A will be slowed down without
    a way to make writeback of the dirty pages happen.
    
    This patch implements foreign dirty recording and foreign mechanism so
    that when a memcg encounters a condition as above it can trigger
    flushes on bdi_writebacks which can clean its pages.  Please see the
    comment on top of mem_cgroup_track_foreign_dirty_slowpath() for
    details.
    
    A reproducer follows.
    
    write-range.c::
    
      #include <stdio.h>
      #include <stdlib.h>
      #include <unistd.h>
      #include <fcntl.h>
      #include <sys/types.h>
    
      static const char *usage = "write-range FILE START SIZE\n";
    
      int main(int argc, char **argv)
      {
              int fd;
              unsigned long start, size, end, pos;
              char *endp;
              char buf[4096];
    
              if (argc < 4) {
                      fprintf(stderr, usage);
                      return 1;
              }
    
              fd = open(argv[1], O_WRONLY);
              if (fd < 0) {
                      perror("open");
                      return 1;
              }
    
              start = strtoul(argv[2], &endp, 0);
              if (*endp != '\0') {
                      fprintf(stderr, usage);
                      return 1;
              }
    
              size = strtoul(argv[3], &endp, 0);
              if (*endp != '\0') {
                      fprintf(stderr, usage);
                      return 1;
              }
    
              end = start + size;
    
              while (1) {
                      for (pos = start; pos < end; ) {
                              long bread, bwritten = 0;
    
                              if (lseek(fd, pos, SEEK_SET) < 0) {
                                      perror("lseek");
                                      return 1;
                              }
    
                              bread = read(0, buf, sizeof(buf) < end - pos ?
                                                   sizeof(buf) : end - pos);
                              if (bread < 0) {
                                      perror("read");
                                      return 1;
                              }
                              if (bread == 0)
                                      return 0;
    
                              while (bwritten < bread) {
                                      long this;
    
                                      this = write(fd, buf + bwritten,
                                                   bread - bwritten);
                                      if (this < 0) {
                                              perror("write");
                                              return 1;
                                      }
    
                                      bwritten += this;
                                      pos += bwritten;
                              }
                      }
              }
      }
    
    repro.sh::
    
      #!/bin/bash
    
      set -e
      set -x
    
      sysctl -w vm.dirty_expire_centisecs=300000
      sysctl -w vm.dirty_writeback_centisecs=300000
      sysctl -w vm.dirtytime_expire_seconds=300000
      echo 3 > /proc/sys/vm/drop_caches
    
      TEST=/sys/fs/cgroup/test
      A=$TEST/A
      B=$TEST/B
    
      mkdir -p $A $B
      echo "+memory +io" > $TEST/cgroup.subtree_control
      echo $((1<<30)) > $A/memory.high
      echo $((32<<30)) > $B/memory.high
    
      rm -f testfile
      touch testfile
      fallocate -l 4G testfile
    
      echo "Starting B"
    
      (echo $BASHPID > $B/cgroup.procs
       pv -q --rate-limit 70M < /dev/urandom | ./write-range testfile $((2<<30)) $((2<<30))) &
    
      echo "Waiting 10s to ensure B claims the testfile inode"
      sleep 5
      sync
      sleep 5
      sync
      echo "Starting A"
    
      (echo $BASHPID > $A/cgroup.procs
       pv < /dev/urandom | ./write-range testfile 0 $((2<<30)))
    
    v2: Added comments explaining why the specific intervals are being used.
    
    v3: Use 0 @nr when calling cgroup_writeback_by_id() to use best-effort
        flushing while avoding possible livelocks.
    
    v4: Use get_jiffies_64() and time_before/after64() instead of raw
        jiffies_64 and arthimetic comparisons as suggested by Jan.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 1075f2552cfc..4fc87dee005a 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -63,6 +63,7 @@ enum wb_reason {
 	 * so it has a mismatch name.
 	 */
 	WB_REASON_FORKER_THREAD,
+	WB_REASON_FOREIGN_FLUSH,
 
 	WB_REASON_MAX,
 };

commit 34f8fe501f0624de115d087680c84000b5d9abc9
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 26 09:06:53 2019 -0700

    bdi: Add bdi->id
    
    There currently is no way to universally identify and lookup a bdi
    without holding a reference and pointer to it.  This patch adds an
    non-recycling bdi->id and implements bdi_get_by_id() which looks up
    bdis by their ids.  This will be used by memcg foreign inode flushing.
    
    I left bdi_list alone for simplicity and because while rb_tree does
    support rcu assignment it doesn't seem to guarantee lossless walk when
    walk is racing aginst tree rebalance operations.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 8fb740178d5d..1075f2552cfc 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -185,6 +185,8 @@ struct bdi_writeback {
 };
 
 struct backing_dev_info {
+	u64 id;
+	struct rb_node rb_node; /* keyed by ->id */
 	struct list_head bdi_list;
 	unsigned long ra_pages;	/* max readahead in PAGE_SIZE units */
 	unsigned long io_pages;	/* max allowed IO size */

commit 5b9cce4c7eb0696558dfd4946074ae1fb9d8f05d
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 26 09:06:52 2019 -0700

    writeback: Generalize and expose wb_completion
    
    wb_completion is used to track writeback completions.  We want to use
    it from memcg side for foreign inode flushes.  This patch updates it
    to remember the target waitq instead of assuming bdi->wb_waitq and
    expose it outside of fs-writeback.c.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 6a1a8a314d85..8fb740178d5d 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -67,6 +67,26 @@ enum wb_reason {
 	WB_REASON_MAX,
 };
 
+struct wb_completion {
+	atomic_t		cnt;
+	wait_queue_head_t	*waitq;
+};
+
+#define __WB_COMPLETION_INIT(_waitq)	\
+	(struct wb_completion){ .cnt = ATOMIC_INIT(1), .waitq = (_waitq) }
+
+/*
+ * If one wants to wait for one or more wb_writeback_works, each work's
+ * ->done should be set to a wb_completion defined using the following
+ * macro.  Once all work items are issued with wb_queue_work(), the caller
+ * can wait for the completion of all using wb_wait_for_completion().  Work
+ * items which are waited upon aren't freed automatically on completion.
+ */
+#define WB_COMPLETION_INIT(bdi)		__WB_COMPLETION_INIT(&(bdi)->wb_waitq)
+
+#define DEFINE_WB_COMPLETION(cmpl, bdi)	\
+	struct wb_completion cmpl = WB_COMPLETION_INIT(bdi)
+
 /*
  * For cgroup writeback, multiple wb's may map to the same blkcg.  Those
  * wb's can operate mostly independently but should share the congested

commit 2d146b924ec3c0873f06308d149684dc1105d9a3
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Tue Jan 22 16:21:07 2019 +0100

    backing-dev: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    And as the return value does not matter at all, no need to save the
    dentry in struct backing_dev_info, so delete it.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anders Roxell <anders.roxell@linaro.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: linux-mm@kvack.org
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 07e02d6df5ad..6a1a8a314d85 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -203,7 +203,6 @@ struct backing_dev_info {
 
 #ifdef CONFIG_DEBUG_FS
 	struct dentry *debug_dir;
-	struct dentry *debug_stats;
 #endif
 };
 

commit 7fc5854f8c6efae9e7624970ab49a1eac2faefb1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 12 08:38:30 2017 -0800

    writeback: synchronize sync(2) against cgroup writeback membership switches
    
    sync_inodes_sb() can race against cgwb (cgroup writeback) membership
    switches and fail to writeback some inodes.  For example, if an inode
    switches to another wb while sync_inodes_sb() is in progress, the new
    wb might not be visible to bdi_split_work_to_wbs() at all or the inode
    might jump from a wb which hasn't issued writebacks yet to one which
    already has.
    
    This patch adds backing_dev_info->wb_switch_rwsem to synchronize cgwb
    switch path against sync_inodes_sb() so that sync_inodes_sb() is
    guaranteed to see all the target wbs and inodes can't jump wbs to
    escape syncing.
    
    v2: Fixed misplaced rwsem init.  Spotted by Jiufei.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Jiufei Xue <xuejiufei@gmail.com>
    Link: http://lkml.kernel.org/r/dc694ae2-f07f-61e1-7097-7c8411cee12d@gmail.com
    Acked-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index c31157135598..07e02d6df5ad 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -190,6 +190,7 @@ struct backing_dev_info {
 	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
 	struct rb_root cgwb_congested_tree; /* their congested states */
 	struct mutex cgwb_release_mutex;  /* protect shutdown of wb structs */
+	struct rw_semaphore wb_switch_rwsem; /* no cgwb switch while syncing */
 #else
 	struct bdi_writeback_congested *wb_congested;
 #endif

commit 368686a95e55fd66b88542b5b23d802a4886b1aa
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Fri Dec 28 00:33:31 2018 -0800

    writeback: don't decrement wb->refcnt if !wb->bdi
    
    This happened while running in qemu-system-aarch64, the AMBA PL011 UART
    driver when enabling CONFIG_DEBUG_TEST_DRIVER_REMOVE.
    arch_initcall(pl011_init) came before subsys_initcall(default_bdi_init),
    devtmpfs' handle_remove() crashes because the reference count is a NULL
    pointer only because wb->bdi hasn't been initialized yet.
    
    Rework so that wb_put have an extra check if wb->bdi before decrement
    wb->refcnt and also add a WARN_ON_ONCE to get a warning if it happens
    again in other drivers.
    
    Link: http://lkml.kernel.org/r/20181030113545.30999-2-anders.roxell@linaro.org
    Fixes: 52ebea749aae ("writeback: make backing_dev_info host cgroup-specific bdi_writebacks")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Co-developed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 9a6bc0951cfa..c31157135598 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -258,6 +258,14 @@ static inline void wb_get(struct bdi_writeback *wb)
  */
 static inline void wb_put(struct bdi_writeback *wb)
 {
+	if (WARN_ON_ONCE(!wb->bdi)) {
+		/*
+		 * A driver bug might cause a file to be removed before bdi was
+		 * initialized.
+		 */
+		return;
+	}
+
 	if (wb != &wb->bdi->wb)
 		percpu_ref_put(&wb->refcnt);
 }

commit e58dd0de5eadf145895b13451a1fef8ef03946eb
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 21 21:55:31 2018 -0700

    bdi: use refcount_t for reference counting instead atomic_t
    
    refcount_t type and corresponding API should be used instead of atomic_t
    when the variable is used as a reference counter.  This permits avoiding
    accidental refcounter overflows that might lead to use-after-free
    situations.
    
    Link: http://lkml.kernel.org/r/20180703200141.28415-4-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 24251762c20c..9a6bc0951cfa 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -12,6 +12,7 @@
 #include <linux/timer.h>
 #include <linux/workqueue.h>
 #include <linux/kref.h>
+#include <linux/refcount.h>
 
 struct page;
 struct device;
@@ -75,7 +76,7 @@ enum wb_reason {
  */
 struct bdi_writeback_congested {
 	unsigned long state;		/* WB_[a]sync_congested flags */
-	atomic_t refcnt;		/* nr of attached wb's and blkg */
+	refcount_t refcnt;		/* nr of attached wb's and blkg */
 
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct backing_dev_info *__bdi;	/* the associated bdi, set to NULL

commit 3ee7e8697d5860b173132606d80a9cd35e7113ee
Author: Jan Kara <jack@suse.cz>
Date:   Mon Jun 18 15:46:58 2018 +0200

    bdi: Fix another oops in wb_workfn()
    
    syzbot is reporting NULL pointer dereference at wb_workfn() [1] due to
    wb->bdi->dev being NULL. And Dmitry confirmed that wb->state was
    WB_shutting_down after wb->bdi->dev became NULL. This indicates that
    unregister_bdi() failed to call wb_shutdown() on one of wb objects.
    
    The problem is in cgwb_bdi_unregister() which does cgwb_kill() and thus
    drops bdi's reference to wb structures before going through the list of
    wbs again and calling wb_shutdown() on each of them. This way the loop
    iterating through all wbs can easily miss a wb if that wb has already
    passed through cgwb_remove_from_bdi_list() called from wb_shutdown()
    from cgwb_release_workfn() and as a result fully shutdown bdi although
    wb_workfn() for this wb structure is still running. In fact there are
    also other ways cgwb_bdi_unregister() can race with
    cgwb_release_workfn() leading e.g. to use-after-free issues:
    
    CPU1                            CPU2
                                    cgwb_bdi_unregister()
                                      cgwb_kill(*slot);
    
    cgwb_release()
      queue_work(cgwb_release_wq, &wb->release_work);
    cgwb_release_workfn()
                                      wb = list_first_entry(&bdi->wb_list, ...)
                                      spin_unlock_irq(&cgwb_lock);
      wb_shutdown(wb);
      ...
      kfree_rcu(wb, rcu);
                                      wb_shutdown(wb); -> oops use-after-free
    
    We solve these issues by synchronizing writeback structure shutdown from
    cgwb_bdi_unregister() with cgwb_release_workfn() using a new mutex. That
    way we also no longer need synchronization using WB_shutting_down as the
    mutex provides it for CONFIG_CGROUP_WRITEBACK case and without
    CONFIG_CGROUP_WRITEBACK wb_shutdown() can be called only once from
    bdi_unregister().
    
    Reported-by: syzbot <syzbot+4a7438e774b21ddd8eca@syzkaller.appspotmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 0bd432a4d7bd..24251762c20c 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -22,7 +22,6 @@ struct dentry;
  */
 enum wb_state {
 	WB_registered,		/* bdi_register() was done */
-	WB_shutting_down,	/* wb_shutdown() in progress */
 	WB_writeback_running,	/* Writeback is in progress */
 	WB_has_dirty_io,	/* Dirty inodes on ->b_{dirty|io|more_io} */
 	WB_start_all,		/* nr_pages == 0 (all) work pending */
@@ -189,6 +188,7 @@ struct backing_dev_info {
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
 	struct rb_root cgwb_congested_tree; /* their congested states */
+	struct mutex cgwb_release_mutex;  /* protect shutdown of wb structs */
 #else
 	struct bdi_writeback_congested *wb_congested;
 #endif

commit 2e898e4c0a3897ccd434adac5abb8330194f527b
Author: Greg Thelen <gthelen@google.com>
Date:   Fri Apr 20 14:55:42 2018 -0700

    writeback: safer lock nesting
    
    lock_page_memcg()/unlock_page_memcg() use spin_lock_irqsave/restore() if
    the page's memcg is undergoing move accounting, which occurs when a
    process leaves its memcg for a new one that has
    memory.move_charge_at_immigrate set.
    
    unlocked_inode_to_wb_begin,end() use spin_lock_irq/spin_unlock_irq() if
    the given inode is switching writeback domains.  Switches occur when
    enough writes are issued from a new domain.
    
    This existing pattern is thus suspicious:
        lock_page_memcg(page);
        unlocked_inode_to_wb_begin(inode, &locked);
        ...
        unlocked_inode_to_wb_end(inode, locked);
        unlock_page_memcg(page);
    
    If both inode switch and process memcg migration are both in-flight then
    unlocked_inode_to_wb_end() will unconditionally enable interrupts while
    still holding the lock_page_memcg() irq spinlock.  This suggests the
    possibility of deadlock if an interrupt occurs before unlock_page_memcg().
    
        truncate
        __cancel_dirty_page
        lock_page_memcg
        unlocked_inode_to_wb_begin
        unlocked_inode_to_wb_end
        <interrupts mistakenly enabled>
                                        <interrupt>
                                        end_page_writeback
                                        test_clear_page_writeback
                                        lock_page_memcg
                                        <deadlock>
        unlock_page_memcg
    
    Due to configuration limitations this deadlock is not currently possible
    because we don't mix cgroup writeback (a cgroupv2 feature) and
    memory.move_charge_at_immigrate (a cgroupv1 feature).
    
    If the kernel is hacked to always claim inode switching and memcg
    moving_account, then this script triggers lockup in less than a minute:
    
      cd /mnt/cgroup/memory
      mkdir a b
      echo 1 > a/memory.move_charge_at_immigrate
      echo 1 > b/memory.move_charge_at_immigrate
      (
        echo $BASHPID > a/cgroup.procs
        while true; do
          dd if=/dev/zero of=/mnt/big bs=1M count=256
        done
      ) &
      while true; do
        sync
      done &
      sleep 1h &
      SLEEP=$!
      while true; do
        echo $SLEEP > a/cgroup.procs
        echo $SLEEP > b/cgroup.procs
      done
    
    The deadlock does not seem possible, so it's debatable if there's any
    reason to modify the kernel.  I suggest we should to prevent future
    surprises.  And Wang Long said "this deadlock occurs three times in our
    environment", so there's more reason to apply this, even to stable.
    Stable 4.4 has minor conflicts applying this patch.  For a clean 4.4 patch
    see "[PATCH for-4.4] writeback: safer lock nesting"
    https://lkml.org/lkml/2018/4/11/146
    
    Wang Long said "this deadlock occurs three times in our environment"
    
    [gthelen@google.com: v4]
      Link: http://lkml.kernel.org/r/20180411084653.254724-1-gthelen@google.com
    [akpm@linux-foundation.org: comment tweaks, struct initialization simplification]
    Change-Id: Ibb773e8045852978f6207074491d262f1b3fb613
    Link: http://lkml.kernel.org/r/20180410005908.167976-1-gthelen@google.com
    Fixes: 682aa8e1a6a1 ("writeback: implement unlocked_inode_to_wb transaction and use it for stat updates")
    Signed-off-by: Greg Thelen <gthelen@google.com>
    Reported-by: Wang Long <wanglong19@meituan.com>
    Acked-by: Wang Long <wanglong19@meituan.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: <stable@vger.kernel.org>    [v4.2+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index bfe86b54f6c1..0bd432a4d7bd 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -223,6 +223,11 @@ static inline void set_bdi_congested(struct backing_dev_info *bdi, int sync)
 	set_wb_congested(bdi->wb.congested, sync);
 }
 
+struct wb_lock_cookie {
+	bool locked;
+	unsigned long flags;
+};
+
 #ifdef CONFIG_CGROUP_WRITEBACK
 
 /**

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 866c433e7d32..fff4cfa0c21d 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __LINUX_BACKING_DEV_DEFS_H
 #define __LINUX_BACKING_DEV_DEFS_H
 

commit 85009b4f5f0399669a44f07cb9a5622c0e71d419
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Sep 30 02:09:06 2017 -0600

    writeback: eliminate work item allocation in bd_start_writeback()
    
    Handle start-all writeback like we do periodic or kupdate
    style writeback - by marking the bdi_writeback as needing a full
    flush, and simply waking the thread. This eliminates the need to
    allocate and queue a specific work item just for this purpose.
    
    After this change, we truly only ever have one of them running at
    any point in time. We mark the need to start all flushes, and the
    writeback thread will clear it once it has processed the request.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 420de5c7c7f9..b7c7be6f5986 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -44,6 +44,28 @@ enum wb_stat_item {
 
 #define WB_STAT_BATCH (8*(1+ilog2(nr_cpu_ids)))
 
+/*
+ * why some writeback work was initiated
+ */
+enum wb_reason {
+	WB_REASON_BACKGROUND,
+	WB_REASON_VMSCAN,
+	WB_REASON_SYNC,
+	WB_REASON_PERIODIC,
+	WB_REASON_LAPTOP_TIMER,
+	WB_REASON_FREE_MORE_MEM,
+	WB_REASON_FS_FREE_SPACE,
+	/*
+	 * There is no bdi forker thread any more and works are done
+	 * by emergency worker, however, this is TPs userland visible
+	 * and we'll be exposing exactly the same information,
+	 * so it has a mismatch name.
+	 */
+	WB_REASON_FORKER_THREAD,
+
+	WB_REASON_MAX,
+};
+
 /*
  * For cgroup writeback, multiple wb's may map to the same blkcg.  Those
  * wb's can operate mostly independently but should share the congested
@@ -116,6 +138,7 @@ struct bdi_writeback {
 
 	struct fprop_local_percpu completions;
 	int dirty_exceeded;
+	enum wb_reason start_all_reason;
 
 	spinlock_t work_lock;		/* protects work_list & dwork scheduling */
 	struct list_head work_list;

commit aac8d41cd438f25bf3110fc6b98f1d16d7dbc169
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Sep 28 11:31:55 2017 -0600

    writeback: only allow one inflight and pending full flush
    
    When someone calls wakeup_flusher_threads() or
    wakeup_flusher_threads_bdi(), they schedule writeback of all dirty
    pages in the system (or on that bdi). If we are tight on memory, we
    can get tons of these queued from kswapd/vmscan. This causes (at
    least) two problems:
    
    1) We consume a ton of memory just allocating writeback work items.
       We've seen as much as 600 million of these writeback work items
       pending. That's a lot of memory to pointlessly hold hostage,
       while the box is under memory pressure.
    
    2) We spend so much time processing these work items, that we
       introduce a softlockup in writeback processing. This is because
       each of the writeback work items don't end up doing any work (it's
       hard when you have millions of identical ones coming in to the
       flush machinery), so we just sit in a tight loop pulling work
       items and deleting/freeing them.
    
    Fix this by adding a 'start_all' bit to the writeback structure, and
    set that when someone attempts to flush all dirty pages. The bit is
    cleared when we start writeback on that work item. If the bit is
    already set when we attempt to queue !nr_pages writeback, then we
    simply ignore it.
    
    This provides us one full flush in flight, with one pending as well,
    and makes for more efficient handling of this type of writeback.
    
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Tested-by: Chris Mason <clm@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 866c433e7d32..420de5c7c7f9 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -24,6 +24,7 @@ enum wb_state {
 	WB_shutting_down,	/* wb_shutdown() in progress */
 	WB_writeback_running,	/* Writeback is in progress */
 	WB_has_dirty_io,	/* Dirty inodes on ->b_{dirty|io|more_io} */
+	WB_start_all,		/* nr_pages == 0 (all) work pending */
 };
 
 enum wb_congested_state {

commit fca39346a55bb7196888ffc77d9e3557340d1d0b
Author: Jan Kara <jack@suse.cz>
Date:   Wed Apr 12 12:24:28 2017 +0200

    fs: Provide infrastructure for dynamic BDIs in filesystems
    
    Provide helper functions for setting up dynamically allocated
    backing_dev_info structures for filesystems and cleaning them up on
    superblock destruction.
    
    CC: linux-mtd@lists.infradead.org
    CC: linux-nfs@vger.kernel.org
    CC: Petr Vandrovec <petr@vandrovec.name>
    CC: linux-nilfs@vger.kernel.org
    CC: cluster-devel@redhat.com
    CC: osd-dev@open-osd.org
    CC: codalist@coda.cs.cmu.edu
    CC: linux-afs@lists.infradead.org
    CC: ecryptfs@vger.kernel.org
    CC: linux-cifs@vger.kernel.org
    CC: ceph-devel@vger.kernel.org
    CC: linux-btrfs@vger.kernel.org
    CC: v9fs-developer@lists.sourceforge.net
    CC: lustre-devel@lists.lustre.org
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index e66d4722db8e..866c433e7d32 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -146,7 +146,7 @@ struct backing_dev_info {
 	congested_fn *congested_fn; /* Function pointer if device is md/dm */
 	void *congested_data;	/* Pointer to aux data for congested func */
 
-	char *name;
+	const char *name;
 
 	struct kref refcnt;	/* Reference counter for the structure */
 	unsigned int capabilities; /* Device capabilities */

commit 4514451e79ae5baabb85d22ba3523602e59d5218
Author: Jan Kara <jack@suse.cz>
Date:   Thu Mar 23 01:36:58 2017 +0100

    bdi: Do not wait for cgwbs release in bdi_unregister()
    
    Currently we wait for all cgwbs to get released in cgwb_bdi_destroy()
    (called from bdi_unregister()). That is however unnecessary now when
    cgwb->bdi is a proper refcounted reference (thus bdi cannot get
    released before all cgwbs are released) and when cgwb_bdi_destroy()
    shuts down writeback directly.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 8af720f22a2d..e66d4722db8e 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -164,7 +164,6 @@ struct backing_dev_info {
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
 	struct rb_root cgwb_congested_tree; /* their congested states */
-	atomic_t usage_cnt; /* counts both cgwbs and cgwb_contested's */
 #else
 	struct bdi_writeback_congested *wb_congested;
 #endif

commit 5318ce7d46866e1dbc20ab9349b93753edba0b3e
Author: Jan Kara <jack@suse.cz>
Date:   Thu Mar 23 01:36:57 2017 +0100

    bdi: Shutdown writeback on all cgwbs in cgwb_bdi_destroy()
    
    Currently we waited for all cgwbs to get freed in cgwb_bdi_destroy()
    which also means that writeback has been shutdown on them. Since this
    wait is going away, directly shutdown writeback on cgwbs from
    cgwb_bdi_destroy() to avoid live writeback structures after
    bdi_unregister() has finished. To make that safe with concurrent
    shutdown from cgwb_release_workfn(), we also have to make sure
    wb_shutdown() returns only after the bdi_writeback structure is really
    shutdown.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 8fb3dcdebc80..8af720f22a2d 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -21,6 +21,7 @@ struct dentry;
  */
 enum wb_state {
 	WB_registered,		/* bdi_register() was done */
+	WB_shutting_down,	/* wb_shutdown() in progress */
 	WB_writeback_running,	/* Writeback is in progress */
 	WB_has_dirty_io,	/* Dirty inodes on ->b_{dirty|io|more_io} */
 };

commit b7d680d7bf584bce6023343304b819009a7c3336
Author: Jan Kara <jack@suse.cz>
Date:   Thu Mar 23 01:36:54 2017 +0100

    bdi: Mark congested->bdi as internal
    
    congested->bdi pointer is used only to be able to remove congested
    structure from bdi->cgwb_congested_tree on structure release. Moreover
    the pointer can become NULL when we unregister the bdi. Rename the field
    to __bdi and add a comment to make it more explicit this is internal
    stuff of memcg writeback code and people should not use the field as
    such use will be likely race prone.
    
    We do not bother with converting congested->bdi to a proper refcounted
    reference. It will be slightly ugly to special-case bdi->wb.congested to
    avoid effectively a cyclic reference of bdi to itself and the reference
    gets cleared from bdi_unregister() making it impossible to reference
    a freed bdi.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index ad955817916d..8fb3dcdebc80 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -54,7 +54,9 @@ struct bdi_writeback_congested {
 	atomic_t refcnt;		/* nr of attached wb's and blkg */
 
 #ifdef CONFIG_CGROUP_WRITEBACK
-	struct backing_dev_info *bdi;	/* the associated bdi */
+	struct backing_dev_info *__bdi;	/* the associated bdi, set to NULL
+					 * on bdi unregistration. For memcg-wb
+					 * internal use only! */
 	int blkcg_id;			/* ID of the associated blkcg */
 	struct rb_node rb_node;		/* on bdi->cgwb_congestion_tree */
 #endif

commit d03f6cdc1fc422accb734c7c07a661a0018d8631
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 2 15:56:51 2017 +0100

    block: Dynamically allocate and refcount backing_dev_info
    
    Instead of storing backing_dev_info inside struct request_queue,
    allocate it dynamically, reference count it, and free it when the last
    reference is dropped. Currently only request_queue holds the reference
    but in the following patch we add other users referencing
    backing_dev_info.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index e850e76acaaf..ad955817916d 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -10,6 +10,7 @@
 #include <linux/flex_proportions.h>
 #include <linux/timer.h>
 #include <linux/workqueue.h>
+#include <linux/kref.h>
 
 struct page;
 struct device;
@@ -144,6 +145,7 @@ struct backing_dev_info {
 
 	char *name;
 
+	struct kref refcnt;	/* Reference counter for the structure */
 	unsigned int capabilities; /* Device capabilities */
 	unsigned int min_ratio;
 	unsigned int max_ratio, max_prop_frac;

commit 36869cb93d36269f34800b3384ba7991060a69cf
Merge: 9439b3710df6 7cd54aa84389
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 10:19:16 2016 -0800

    Merge branch 'for-4.10/block' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
     "This is the main block pull request this series. Contrary to previous
      release, I've kept the core and driver changes in the same branch. We
      always ended up having dependencies between the two for obvious
      reasons, so makes more sense to keep them together. That said, I'll
      probably try and keep more topical branches going forward, especially
      for cycles that end up being as busy as this one.
    
      The major parts of this pull request is:
    
       - Improved support for O_DIRECT on block devices, with a small
         private implementation instead of using the pig that is
         fs/direct-io.c. From Christoph.
    
       - Request completion tracking in a scalable fashion. This is utilized
         by two components in this pull, the new hybrid polling and the
         writeback queue throttling code.
    
       - Improved support for polling with O_DIRECT, adding a hybrid mode
         that combines pure polling with an initial sleep. From me.
    
       - Support for automatic throttling of writeback queues on the block
         side. This uses feedback from the device completion latencies to
         scale the queue on the block side up or down. From me.
    
       - Support from SMR drives in the block layer and for SD. From Hannes
         and Shaun.
    
       - Multi-connection support for nbd. From Josef.
    
       - Cleanup of request and bio flags, so we have a clear split between
         which are bio (or rq) private, and which ones are shared. From
         Christoph.
    
       - A set of patches from Bart, that improve how we handle queue
         stopping and starting in blk-mq.
    
       - Support for WRITE_ZEROES from Chaitanya.
    
       - Lightnvm updates from Javier/Matias.
    
       - Supoort for FC for the nvme-over-fabrics code. From James Smart.
    
       - A bunch of fixes from a whole slew of people, too many to name
         here"
    
    * 'for-4.10/block' of git://git.kernel.dk/linux-block: (182 commits)
      blk-stat: fix a few cases of missing batch flushing
      blk-flush: run the queue when inserting blk-mq flush
      elevator: make the rqhash helpers exported
      blk-mq: abstract out blk_mq_dispatch_rq_list() helper
      blk-mq: add blk_mq_start_stopped_hw_queue()
      block: improve handling of the magic discard payload
      blk-wbt: don't throttle discard or write zeroes
      nbd: use dev_err_ratelimited in io path
      nbd: reset the setup task for NBD_CLEAR_SOCK
      nvme-fabrics: Add FC LLDD loopback driver to test FC-NVME
      nvme-fabrics: Add target support for FC transport
      nvme-fabrics: Add host support for FC transport
      nvme-fabrics: Add FC transport LLDD api definitions
      nvme-fabrics: Add FC transport FC-NVME definitions
      nvme-fabrics: Add FC transport error codes to nvme.h
      Add type 0x28 NVME type code to scsi fc headers
      nvme-fabrics: patch target code in prep for FC transport support
      nvme-fabrics: set sqe.command_id in core not transports
      parser: add u64 number parser
      nvme-rdma: align to generic ib_event logging helper
      ...

commit 8db378a570330fa0aaa9d75299fe264e4a5b6348
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Dec 12 16:43:29 2016 -0800

    include/linux/backing-dev-defs.h: shrink struct backing_dev_info
    
    Move the 4-byte `capabilities' field next to other 4-byte things.
    Shrinks sizeof(backing_dev_info) by 8 bytes on x86_64.
    
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index b8144b2d59ce..0b5b1af35e5e 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -137,12 +137,12 @@ struct backing_dev_info {
 	struct list_head bdi_list;
 	unsigned long ra_pages;	/* max readahead in PAGE_SIZE units */
 	unsigned long io_pages;	/* max allowed IO size */
-	unsigned int capabilities; /* Device capabilities */
 	congested_fn *congested_fn; /* Function pointer if device is md/dm */
 	void *congested_data;	/* Pointer to aux data for congested func */
 
 	char *name;
 
+	unsigned int capabilities; /* Device capabilities */
 	unsigned int min_ratio;
 	unsigned int max_ratio, max_prop_frac;
 

commit 9491ae4aade6814afcfa67f4eb3e3342c2b39750
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Dec 12 16:43:26 2016 -0800

    mm: don't cap request size based on read-ahead setting
    
    We ran into a funky issue, where someone doing 256K buffered reads saw
    128K requests at the device level.  Turns out it is read-ahead capping
    the request size, since we use 128K as the default setting.  This
    doesn't make a lot of sense - if someone is issuing 256K reads, they
    should see 256K reads, regardless of the read-ahead setting, if the
    underlying device can support a 256K read in a single command.
    
    This patch introduces a bdi hint, io_pages.  This is the soft max IO
    size for the lower level, I've hooked it up to the bdev settings here.
    Read-ahead is modified to issue the maximum of the user request size,
    and the read-ahead max size, but capped to the max request size on the
    device side.  The latter is done to avoid reading ahead too much, if the
    application asks for a huge read.  With this patch, the kernel behaves
    like the application expects.
    
    Link: http://lkml.kernel.org/r/1479498073-8657-1-git-send-email-axboe@fb.com
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index c357f27d5483..b8144b2d59ce 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -136,6 +136,7 @@ struct bdi_writeback {
 struct backing_dev_info {
 	struct list_head bdi_list;
 	unsigned long ra_pages;	/* max readahead in PAGE_SIZE units */
+	unsigned long io_pages;	/* max allowed IO size */
 	unsigned int capabilities; /* Device capabilities */
 	congested_fn *congested_fn; /* Function pointer if device is md/dm */
 	void *congested_data;	/* Pointer to aux data for congested func */

commit b57d74aff9ab92fbfb7c197c384d1adfa2827b2e
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Sep 1 10:20:33 2016 -0600

    writeback: track if we're sleeping on progress in balance_dirty_pages()
    
    Note in the bdi_writeback structure whenever a task ends up sleeping
    waiting for progress. We can use that information in the lower layers
    to increase the priority of writes.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index c357f27d5483..dc5f76d7f648 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -116,6 +116,8 @@ struct bdi_writeback {
 	struct list_head work_list;
 	struct delayed_work dwork;	/* work item used for writeback */
 
+	unsigned long dirty_sleep;	/* last wait */
+
 	struct list_head bdi_node;	/* anchored at bdi->wb_list */
 
 #ifdef CONFIG_CGROUP_WRITEBACK

commit df08c32ce3be5be138c1dbfcba203314a3a7cd6f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Jul 31 11:15:13 2016 -0700

    block: fix bdi vs gendisk lifetime mismatch
    
    The name for a bdi of a gendisk is derived from the gendisk's devt.
    However, since the gendisk is destroyed before the bdi it leaves a
    window where a new gendisk could dynamically reuse the same devt while a
    bdi with the same name is still live.  Arrange for the bdi to hold a
    reference against its "owner" disk device while it is registered.
    Otherwise we can hit sysfs duplicate name collisions like the following:
    
     WARNING: CPU: 10 PID: 2078 at fs/sysfs/dir.c:31 sysfs_warn_dup+0x64/0x80
     sysfs: cannot create duplicate filename '/devices/virtual/bdi/259:1'
    
     Hardware name: HP ProLiant DL580 Gen8, BIOS P79 05/06/2015
      0000000000000286 0000000002c04ad5 ffff88006f24f970 ffffffff8134caec
      ffff88006f24f9c0 0000000000000000 ffff88006f24f9b0 ffffffff8108c351
      0000001f0000000c ffff88105d236000 ffff88105d1031e0 ffff8800357427f8
     Call Trace:
      [<ffffffff8134caec>] dump_stack+0x63/0x87
      [<ffffffff8108c351>] __warn+0xd1/0xf0
      [<ffffffff8108c3cf>] warn_slowpath_fmt+0x5f/0x80
      [<ffffffff812a0d34>] sysfs_warn_dup+0x64/0x80
      [<ffffffff812a0e1e>] sysfs_create_dir_ns+0x7e/0x90
      [<ffffffff8134faaa>] kobject_add_internal+0xaa/0x320
      [<ffffffff81358d4e>] ? vsnprintf+0x34e/0x4d0
      [<ffffffff8134ff55>] kobject_add+0x75/0xd0
      [<ffffffff816e66b2>] ? mutex_lock+0x12/0x2f
      [<ffffffff8148b0a5>] device_add+0x125/0x610
      [<ffffffff8148b788>] device_create_groups_vargs+0xd8/0x100
      [<ffffffff8148b7cc>] device_create_vargs+0x1c/0x20
      [<ffffffff811b775c>] bdi_register+0x8c/0x180
      [<ffffffff811b7877>] bdi_register_dev+0x27/0x30
      [<ffffffff813317f5>] add_disk+0x175/0x4a0
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Yi Zhang <yizhan@redhat.com>
    Tested-by: Yi Zhang <yizhan@redhat.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    
    Fixed up missing 0 return in bdi_register_owner().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 3f103076d0bf..c357f27d5483 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -163,6 +163,7 @@ struct backing_dev_info {
 	wait_queue_head_t wb_waitq;
 
 	struct device *dev;
+	struct device *owner;
 
 	struct timer_list laptop_mode_wb_timer;
 

commit ea1754a084760e68886f5b725c8eaada9cc57155
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:48 2016 +0300

    mm, fs: remove remaining PAGE_CACHE_* and page_cache_{get,release} usage
    
    Mostly direct substitution with occasional adjustment or removing
    outdated comments.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 1b4d69f68c33..3f103076d0bf 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -135,7 +135,7 @@ struct bdi_writeback {
 
 struct backing_dev_info {
 	struct list_head bdi_list;
-	unsigned long ra_pages;	/* max readahead in PAGE_CACHE_SIZE units */
+	unsigned long ra_pages;	/* max readahead in PAGE_SIZE units */
 	unsigned int capabilities; /* Device capabilities */
 	congested_fn *congested_fn; /* Function pointer if device is md/dm */
 	void *congested_data;	/* Pointer to aux data for congested func */

commit b817525a4a80c04e4ca44192d97a1ffa9f2be572
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Oct 2 14:47:05 2015 -0400

    writeback: bdi_writeback iteration must not skip dying ones
    
    bdi_for_each_wb() is used in several places to wake up or issue
    writeback work items to all wb's (bdi_writeback's) on a given bdi.
    The iteration is performed by walking bdi->cgwb_tree; however, the
    tree only indexes wb's which are currently active.
    
    For example, when a memcg gets associated with a different blkcg, the
    old wb is removed from the tree so that the new one can be indexed.
    The old wb starts dying from then on but will linger till all its
    inodes are drained.  As these dying wb's may still host dirty inodes,
    writeback operations which affect all wb's must include them.
    bdi_for_each_wb() skipping dying wb's led to sync(2) missing and
    failing to sync the inodes belonging to those wb's.
    
    This patch adds a RCU protected @bdi->wb_list which lists all wb's
    beloinging to that bdi.  wb's are added on creation and removed on
    release rather than on the start of destruction.  bdi_for_each_wb()
    usages are replaced with list_for_each[_continue]_rcu() iterations
    over @bdi->wb_list and bdi_for_each_wb() and its helpers are removed.
    
    v2: Updated as per Jan.  last_wb ref leak in bdi_split_work_to_wbs()
        fixed and unnecessary list head severing in cgwb_bdi_destroy()
        removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Artem Bityutskiy <dedekind1@gmail.com>
    Fixes: ebe41ab0c79d ("writeback: implement bdi_for_each_wb()")
    Link: http://lkml.kernel.org/g/1443012552.19983.209.camel@gmail.com
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index a23209b43842..1b4d69f68c33 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -116,6 +116,8 @@ struct bdi_writeback {
 	struct list_head work_list;
 	struct delayed_work dwork;	/* work item used for writeback */
 
+	struct list_head bdi_node;	/* anchored at bdi->wb_list */
+
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct percpu_ref refcnt;	/* used only for !root wb's */
 	struct fprop_local_percpu memcg_completions;
@@ -150,6 +152,7 @@ struct backing_dev_info {
 	atomic_long_t tot_write_bandwidth;
 
 	struct bdi_writeback wb;  /* the root writeback info for this bdi */
+	struct list_head wb_list; /* list of all wbs */
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
 	struct rb_root cgwb_congested_tree; /* their congested states */

commit a13f35e8714009145e32ebe2bf25b84e1376e314
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 2 08:44:34 2015 -0600

    writeback: don't embed root bdi_writeback_congested in bdi_writeback
    
    52ebea749aae ("writeback: make backing_dev_info host cgroup-specific
    bdi_writebacks") made bdi (backing_dev_info) host per-cgroup wb's
    (bdi_writeback's).  As the congested state needs to be per-wb and
    referenced from blkcg side and multiple wbs, the patch made all
    non-root cong's (bdi_writeback_congested's) reference counted and
    indexed on bdi.
    
    When a bdi is destroyed, cgwb_bdi_destroy() tries to drain all
    non-root cong's; however, this can hang indefinitely because wb's can
    also be referenced from blkcg_gq's which are destroyed after bdi
    destruction is complete.
    
    To fix the bug, bdi destruction will be updated to not wait for cong's
    to drain, which naturally means that cong's may outlive the associated
    bdi.  This is fine for non-root cong's but is problematic for the root
    cong's which are embedded in their bdi's as they may end up getting
    dereferenced after the containing bdi's are freed.
    
    This patch makes root cong's behave the same as non-root cong's.  They
    are no longer embedded in their bdi's but allocated separately during
    bdi initialization, indexed and reference counted the same way.
    
    * As cong handling is the same for all wb's, wb->congested
      initialization is moved into wb_init().
    
    * When !CONFIG_CGROUP_WRITEBACK, there was no indexing or refcnting.
      bdi->wb_congested is now a pointer pointing to the root cong
      allocated during bdi init and minimal refcnting operations are
      implemented.
    
    * The above makes root wb init paths diverge depending on
      CONFIG_CGROUP_WRITEBACK.  root wb init is moved to cgwb_bdi_init().
    
    This patch in itself shouldn't cause any consequential behavior
    differences but prepares for the actual fix.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Jon Christopherson <jon@jons.org>
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=100681
    Tested-by: Jon Christopherson <jon@jons.org>
    
    Added <linux/slab.h> include to backing-dev.h for kfree() definition.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index a48d90e3bcbb..a23209b43842 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -50,10 +50,10 @@ enum wb_stat_item {
  */
 struct bdi_writeback_congested {
 	unsigned long state;		/* WB_[a]sync_congested flags */
+	atomic_t refcnt;		/* nr of attached wb's and blkg */
 
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct backing_dev_info *bdi;	/* the associated bdi */
-	atomic_t refcnt;		/* nr of attached wb's and blkg */
 	int blkcg_id;			/* ID of the associated blkcg */
 	struct rb_node rb_node;		/* on bdi->cgwb_congestion_tree */
 #endif
@@ -150,11 +150,12 @@ struct backing_dev_info {
 	atomic_long_t tot_write_bandwidth;
 
 	struct bdi_writeback wb;  /* the root writeback info for this bdi */
-	struct bdi_writeback_congested wb_congested; /* its congested state */
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
 	struct rb_root cgwb_congested_tree; /* their congested states */
 	atomic_t usage_cnt; /* counts both cgwbs and cgwb_contested's */
+#else
+	struct bdi_writeback_congested *wb_congested;
 #endif
 	wait_queue_head_t wb_waitq;
 

commit e8a7abf5a5bd302a1e06a3c21a629eaa4cba57d6
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 28 14:50:57 2015 -0400

    writeback: disassociate inodes from dying bdi_writebacks
    
    For the purpose of foreign inode detection, wb's (bdi_writeback's) are
    identified by the associated memcg ID.  As we create a separate wb for
    each memcg, this is enough to identify the active wb's; however, when
    blkcg is enabled or disabled higher up in the hierarchy, the mapping
    between memcg and blkcg changes which in turn creates a new wb to
    service the new mapping.  The old wb is unlinked from index and
    released after all references are drained.  The foreign inode
    detection logic can't detect this condition because both the old and
    new wb's point to the same memcg and thus never decides to move inodes
    attached to the old wb to the new one.
    
    This patch adds logic to initiate switching immediately in
    wbc_attach_and_unlock_inode() if the associated wb is dying.  We can
    make the usual foreign detection logic to distinguish the different
    wb's mapped to the memcg but the dying wb is never gonna be in active
    service again and there's no point in tracking the usage history and
    reaching the switch verdict after enough data points are collected.
    It's already known that the wb has to be switched.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index e047b496a0b9..a48d90e3bcbb 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -219,6 +219,17 @@ static inline void wb_put(struct bdi_writeback *wb)
 		percpu_ref_put(&wb->refcnt);
 }
 
+/**
+ * wb_dying - is a wb dying?
+ * @wb: bdi_writeback of interest
+ *
+ * Returns whether @wb is unlinked and being drained.
+ */
+static inline bool wb_dying(struct bdi_writeback *wb)
+{
+	return percpu_ref_is_dying(&wb->refcnt);
+}
+
 #else	/* CONFIG_CGROUP_WRITEBACK */
 
 static inline bool wb_tryget(struct bdi_writeback *wb)
@@ -234,6 +245,11 @@ static inline void wb_put(struct bdi_writeback *wb)
 {
 }
 
+static inline bool wb_dying(struct bdi_writeback *wb)
+{
+	return false;
+}
+
 #endif	/* CONFIG_CGROUP_WRITEBACK */
 
 #endif	/* __LINUX_BACKING_DEV_DEFS_H */

commit 21c6321fbb3a3787af07f1bc031d713a707fb69c
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 28 14:50:49 2015 -0400

    writeback: relocate wb[_try]_get(), wb_put(), inode_{attach|detach}_wb()
    
    Currently, majority of cgroup writeback support including all the
    above functions are implemented in include/linux/backing-dev.h and
    mm/backing-dev.c; however, the portion closely related to writeback
    logic implemented in include/linux/writeback.h and mm/page-writeback.c
    will expand to support foreign writeback detection and correction.
    
    This patch moves wb[_try]_get() and wb_put() to
    include/linux/backing-dev-defs.h so that they can be used from
    writeback.h and inode_{attach|detach}_wb() to writeback.h and
    page-writeback.c.
    
    This is pure reorganization and doesn't introduce any functional
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 8d470b73824f..e047b496a0b9 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -186,4 +186,54 @@ static inline void set_bdi_congested(struct backing_dev_info *bdi, int sync)
 	set_wb_congested(bdi->wb.congested, sync);
 }
 
+#ifdef CONFIG_CGROUP_WRITEBACK
+
+/**
+ * wb_tryget - try to increment a wb's refcount
+ * @wb: bdi_writeback to get
+ */
+static inline bool wb_tryget(struct bdi_writeback *wb)
+{
+	if (wb != &wb->bdi->wb)
+		return percpu_ref_tryget(&wb->refcnt);
+	return true;
+}
+
+/**
+ * wb_get - increment a wb's refcount
+ * @wb: bdi_writeback to get
+ */
+static inline void wb_get(struct bdi_writeback *wb)
+{
+	if (wb != &wb->bdi->wb)
+		percpu_ref_get(&wb->refcnt);
+}
+
+/**
+ * wb_put - decrement a wb's refcount
+ * @wb: bdi_writeback to put
+ */
+static inline void wb_put(struct bdi_writeback *wb)
+{
+	if (wb != &wb->bdi->wb)
+		percpu_ref_put(&wb->refcnt);
+}
+
+#else	/* CONFIG_CGROUP_WRITEBACK */
+
+static inline bool wb_tryget(struct bdi_writeback *wb)
+{
+	return true;
+}
+
+static inline void wb_get(struct bdi_writeback *wb)
+{
+}
+
+static inline void wb_put(struct bdi_writeback *wb)
+{
+}
+
+#endif	/* CONFIG_CGROUP_WRITEBACK */
+
 #endif	/* __LINUX_BACKING_DEV_DEFS_H */

commit 841710aa6e4acd066ab9fe8c8cb6f4e4e6709d83
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:33 2015 -0400

    writeback: implement memcg wb_domain
    
    Dirtyable memory is distributed to a wb (bdi_writeback) according to
    the relative bandwidth the wb is writing out in the whole system.
    This distribution is global - each wb is measured against all other
    wb's and gets the proportinately sized portion of the memory in the
    whole system.
    
    For cgroup writeback, the amount of dirtyable memory is scoped by
    memcg and thus each wb would need to be measured and controlled in its
    memcg.  IOW, a wb will belong to two writeback domains - the global
    and memcg domains.
    
    The previous patches laid the groundwork to support the two wb_domains
    and this patch implements memcg wb_domain.  memcg->cgwb_domain is
    initialized on css online and destroyed on css release,
    wb->memcg_completions is added, and __wb_writeout_inc() is updated to
    increment completions against both global and memcg wb_domains.
    
    The following patches will update balance_dirty_pages() and its
    subroutines to actually consider memcg wb_domain for throttling.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 97a92fa0cdb5..8d470b73824f 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -118,6 +118,7 @@ struct bdi_writeback {
 
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct percpu_ref refcnt;	/* used only for !root wb's */
+	struct fprop_local_percpu memcg_completions;
 	struct cgroup_subsys_state *memcg_css; /* the associated memcg */
 	struct cgroup_subsys_state *blkcg_css; /* and blkcg */
 	struct list_head memcg_node;	/* anchored at memcg->cgwb_list */

commit cc395d7f1f7b9c740ab6d367ef1f6eb248595dff
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:58 2015 -0400

    writeback: implement bdi_wait_for_completion()
    
    If the completion of a wb_writeback_work can be waited upon by setting
    its ->done to a struct completion and waiting on it; however, for
    cgroup writeback support, it's necessary to issue multiple work items
    to multiple bdi_writebacks and wait for the completion of all.
    
    This patch implements wb_completion which can wait for multiple work
    items and replaces the struct completion with it.  It can be defined
    using DEFINE_WB_COMPLETION_ONSTACK(), used for multiple work items and
    waited for by wb_wait_for_completion().
    
    Nobody currently issues multiple work items and this patch doesn't
    introduce any behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 8c857d723023..97a92fa0cdb5 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -155,6 +155,8 @@ struct backing_dev_info {
 	struct rb_root cgwb_congested_tree; /* their congested states */
 	atomic_t usage_cnt; /* counts both cgwbs and cgwb_contested's */
 #endif
+	wait_queue_head_t wb_waitq;
+
 	struct device *dev;
 
 	struct timer_list laptop_mode_wb_timer;

commit 95a46c65e3c09edb9f17dabf2dc16670cd328739
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:47 2015 -0400

    writeback: make bdi_has_dirty_io() take multiple bdi_writeback's into account
    
    bdi_has_dirty_io() used to only reflect whether the root wb
    (bdi_writeback) has dirty inodes.  For cgroup writeback support, it
    needs to take all active wb's into account.  If any wb on the bdi has
    dirty inodes, bdi_has_dirty_io() should return true.
    
    To achieve that, as inode_wb_list_{move|del}_locked() now keep track
    of the dirty state transition of each wb, the number of dirty wbs can
    be counted in the bdi; however, bdi is already aggregating
    wb->avg_write_bandwidth which can easily be guaranteed to be > 0 when
    there are any dirty inodes by ensuring wb->avg_write_bandwidth can't
    dip below 1.  bdi_has_dirty_io() can simply test whether
    bdi->tot_write_bandwidth is zero or not.
    
    While this bumps the value of wb->avg_write_bandwidth to one when it
    used to be zero, this shouldn't cause any meaningful behavior
    difference.
    
    bdi_has_dirty_io() is made an inline function which tests whether
    ->tot_write_bandwidth is non-zero.  Also, WARN_ON_ONCE()'s on its
    value are added to inode_wb_list_{move|del}_locked().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index d631a61f4023..8c857d723023 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -98,7 +98,7 @@ struct bdi_writeback {
 	unsigned long dirtied_stamp;
 	unsigned long written_stamp;	/* pages written at bw_time_stamp */
 	unsigned long write_bandwidth;	/* the estimated write bandwidth */
-	unsigned long avg_write_bandwidth; /* further smoothed write bw */
+	unsigned long avg_write_bandwidth; /* further smoothed write bw, > 0 */
 
 	/*
 	 * The base dirty throttle rate, re-calculated on every 200ms.
@@ -142,7 +142,11 @@ struct backing_dev_info {
 	unsigned int min_ratio;
 	unsigned int max_ratio, max_prop_frac;
 
-	atomic_long_t tot_write_bandwidth; /* sum of active avg_write_bw */
+	/*
+	 * Sum of avg_write_bw of wbs with dirty inodes.  > 0 if there are
+	 * any dirty wbs, which is depended upon by bdi_has_dirty().
+	 */
+	atomic_long_t tot_write_bandwidth;
 
 	struct bdi_writeback wb;  /* the root writeback info for this bdi */
 	struct bdi_writeback_congested wb_congested; /* its congested state */

commit 766a9d6e60578f1ef6de71f89f022084f8bffc82
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:46 2015 -0400

    writeback: implement backing_dev_info->tot_write_bandwidth
    
    cgroup writeback support needs to keep track of the sum of
    avg_write_bandwidth of all wb's (bdi_writeback's) with dirty inodes to
    distribute write workload.  This patch adds bdi->tot_write_bandwidth
    and updates inode_wb_list_move_locked(), inode_wb_list_del_locked()
    and wb_update_write_bandwidth() to adjust it as wb's gain and lose
    dirty inodes and its avg_write_bandwidth gets updated.
    
    As the update events are not synchronized with each other,
    bdi->tot_write_bandwidth is an atomic_long_t.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 7a94b7850b7c..d631a61f4023 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -142,6 +142,8 @@ struct backing_dev_info {
 	unsigned int min_ratio;
 	unsigned int max_ratio, max_prop_frac;
 
+	atomic_long_t tot_write_bandwidth; /* sum of active avg_write_bw */
+
 	struct bdi_writeback wb;  /* the root writeback info for this bdi */
 	struct bdi_writeback_congested wb_congested; /* its congested state */
 #ifdef CONFIG_CGROUP_WRITEBACK

commit d6c10f1fc8626dc55946f4768ae322b4c57b07dd
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:45 2015 -0400

    writeback: implement WB_has_dirty_io wb_state flag
    
    Currently, wb_has_dirty_io() determines whether a wb (bdi_writeback)
    has any dirty inode by testing all three IO lists on each invocation
    without actively keeping track.  For cgroup writeback support, a
    single bdi will host multiple wb's each of which will host dirty
    inodes separately and we'll need to make bdi_has_dirty_io(), which
    currently only represents the root wb, aggregate has_dirty_io from all
    member wb's, which requires tracking transitions in has_dirty_io state
    on each wb.
    
    This patch introduces inode_wb_list_{move|del}_locked() to consolidate
    IO list operations leaving queue_io() the only other function which
    directly manipulates IO lists (via move_expired_inodes()).  All three
    functions are updated to call wb_io_lists_[de]populated() which keep
    track of whether the wb has dirty inodes or not and record it using
    the new WB_has_dirty_io flag.  inode_wb_list_moved_locked()'s return
    value indicates whether the wb had no dirty inodes before.
    
    mark_inode_dirty() is restructured so that the return value of
    inode_wb_list_move_locked() can be used for deciding whether to wake
    up the wb.
    
    While at it, change {bdi|wb}_has_dirty_io()'s return values to bool.
    These functions were returning 0 and 1 before.  Also, add a comment
    explaining the synchronization of wb_state flags.
    
    v2: Updated to accommodate b_dirty_time.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index eb386766b5f3..7a94b7850b7c 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -21,6 +21,7 @@ struct dentry;
 enum wb_state {
 	WB_registered,		/* bdi_register() was done */
 	WB_writeback_running,	/* Writeback is in progress */
+	WB_has_dirty_io,	/* Dirty inodes on ->b_{dirty|io|more_io} */
 };
 
 enum wb_congested_state {

commit ec8a6f2643923ee5b74d24fa8d134240379f436b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:41 2015 -0400

    writeback: make congestion functions per bdi_writeback
    
    Currently, all congestion functions take bdi (backing_dev_info) and
    always operate on the root wb (bdi->wb) and the congestion state from
    the block layer is propagated only for the root blkcg.  This patch
    introduces {set|clear}_wb_congested() and wb_congested() which take a
    bdi_writeback_congested and bdi_writeback respectively.  The bdi
    counteparts are now wrappers invoking the wb based functions on
    @bdi->wb.
    
    While converting clear_bdi_congested() to clear_wb_congested(), the
    local variable declaration order between @wqh and @bit is swapped for
    cosmetic reason.
    
    This patch just adds the new wb based functions.  The following
    patches will apply them.
    
    v2: Updated for bdi_writeback_congested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index a1e9c407a59a..eb386766b5f3 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -163,7 +163,17 @@ enum {
 	BLK_RW_SYNC	= 1,
 };
 
-void clear_bdi_congested(struct backing_dev_info *bdi, int sync);
-void set_bdi_congested(struct backing_dev_info *bdi, int sync);
+void clear_wb_congested(struct bdi_writeback_congested *congested, int sync);
+void set_wb_congested(struct bdi_writeback_congested *congested, int sync);
+
+static inline void clear_bdi_congested(struct backing_dev_info *bdi, int sync)
+{
+	clear_wb_congested(bdi->wb.congested, sync);
+}
+
+static inline void set_bdi_congested(struct backing_dev_info *bdi, int sync)
+{
+	set_wb_congested(bdi->wb.congested, sync);
+}
 
 #endif	/* __LINUX_BACKING_DEV_DEFS_H */

commit 52ebea749aaed195245701a8f90a23d672c7a933
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:37 2015 -0400

    writeback: make backing_dev_info host cgroup-specific bdi_writebacks
    
    For the planned cgroup writeback support, on each bdi
    (backing_dev_info), each memcg will be served by a separate wb
    (bdi_writeback).  This patch updates bdi so that a bdi can host
    multiple wbs (bdi_writebacks).
    
    On the default hierarchy, blkcg implicitly enables memcg.  This allows
    using memcg's page ownership for attributing writeback IOs, and every
    memcg - blkcg combination can be served by its own wb by assigning a
    dedicated wb to each memcg.  This means that there may be multiple
    wb's of a bdi mapped to the same blkcg.  As congested state is per
    blkcg - bdi combination, those wb's should share the same congested
    state.  This is achieved by tracking congested state via
    bdi_writeback_congested structs which are keyed by blkcg.
    
    bdi->wb remains unchanged and will keep serving the root cgroup.
    cgwb's (cgroup wb's) for non-root cgroups are created on-demand or
    looked up while dirtying an inode according to the memcg of the page
    being dirtied or current task.  Each cgwb is indexed on bdi->cgwb_tree
    by its memcg id.  Once an inode is associated with its wb, it can be
    retrieved using inode_to_wb().
    
    Currently, none of the filesystems has FS_CGROUP_WRITEBACK and all
    pages will keep being associated with bdi->wb.
    
    v3: inode_attach_wb() in account_page_dirtied() moved inside
        mapping_cap_account_dirty() block where it's known to be !NULL.
        Also, an unnecessary NULL check before kfree() removed.  Both
        detected by the kbuild bot.
    
    v2: Updated so that wb association is per inode and wb is per memcg
        rather than blkcg.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: kbuild test robot <fengguang.wu@intel.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 9e9eafa5f5aa..a1e9c407a59a 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -2,8 +2,11 @@
 #define __LINUX_BACKING_DEV_DEFS_H
 
 #include <linux/list.h>
+#include <linux/radix-tree.h>
+#include <linux/rbtree.h>
 #include <linux/spinlock.h>
 #include <linux/percpu_counter.h>
+#include <linux/percpu-refcount.h>
 #include <linux/flex_proportions.h>
 #include <linux/timer.h>
 #include <linux/workqueue.h>
@@ -37,10 +40,43 @@ enum wb_stat_item {
 
 #define WB_STAT_BATCH (8*(1+ilog2(nr_cpu_ids)))
 
+/*
+ * For cgroup writeback, multiple wb's may map to the same blkcg.  Those
+ * wb's can operate mostly independently but should share the congested
+ * state.  To facilitate such sharing, the congested state is tracked using
+ * the following struct which is created on demand, indexed by blkcg ID on
+ * its bdi, and refcounted.
+ */
 struct bdi_writeback_congested {
 	unsigned long state;		/* WB_[a]sync_congested flags */
+
+#ifdef CONFIG_CGROUP_WRITEBACK
+	struct backing_dev_info *bdi;	/* the associated bdi */
+	atomic_t refcnt;		/* nr of attached wb's and blkg */
+	int blkcg_id;			/* ID of the associated blkcg */
+	struct rb_node rb_node;		/* on bdi->cgwb_congestion_tree */
+#endif
 };
 
+/*
+ * Each wb (bdi_writeback) can perform writeback operations, is measured
+ * and throttled, independently.  Without cgroup writeback, each bdi
+ * (bdi_writeback) is served by its embedded bdi->wb.
+ *
+ * On the default hierarchy, blkcg implicitly enables memcg.  This allows
+ * using memcg's page ownership for attributing writeback IOs, and every
+ * memcg - blkcg combination can be served by its own wb by assigning a
+ * dedicated wb to each memcg, which enables isolation across different
+ * cgroups and propagation of IO back pressure down from the IO layer upto
+ * the tasks which are generating the dirty pages to be written back.
+ *
+ * A cgroup wb is indexed on its bdi by the ID of the associated memcg,
+ * refcounted with the number of inodes attached to it, and pins the memcg
+ * and the corresponding blkcg.  As the corresponding blkcg for a memcg may
+ * change as blkcg is disabled and enabled higher up in the hierarchy, a wb
+ * is tested for blkcg after lookup and removed from index on mismatch so
+ * that a new wb for the combination can be created.
+ */
 struct bdi_writeback {
 	struct backing_dev_info *bdi;	/* our parent bdi */
 
@@ -78,6 +114,19 @@ struct bdi_writeback {
 	spinlock_t work_lock;		/* protects work_list & dwork scheduling */
 	struct list_head work_list;
 	struct delayed_work dwork;	/* work item used for writeback */
+
+#ifdef CONFIG_CGROUP_WRITEBACK
+	struct percpu_ref refcnt;	/* used only for !root wb's */
+	struct cgroup_subsys_state *memcg_css; /* the associated memcg */
+	struct cgroup_subsys_state *blkcg_css; /* and blkcg */
+	struct list_head memcg_node;	/* anchored at memcg->cgwb_list */
+	struct list_head blkcg_node;	/* anchored at blkcg->cgwb_list */
+
+	union {
+		struct work_struct release_work;
+		struct rcu_head rcu;
+	};
+#endif
 };
 
 struct backing_dev_info {
@@ -92,9 +141,13 @@ struct backing_dev_info {
 	unsigned int min_ratio;
 	unsigned int max_ratio, max_prop_frac;
 
-	struct bdi_writeback wb;  /* default writeback info for this bdi */
-	struct bdi_writeback_congested wb_congested;
-
+	struct bdi_writeback wb;  /* the root writeback info for this bdi */
+	struct bdi_writeback_congested wb_congested; /* its congested state */
+#ifdef CONFIG_CGROUP_WRITEBACK
+	struct radix_tree_root cgwb_tree; /* radix tree of active cgroup wbs */
+	struct rb_root cgwb_congested_tree; /* their congested states */
+	atomic_t usage_cnt; /* counts both cgwbs and cgwb_contested's */
+#endif
 	struct device *dev;
 
 	struct timer_list laptop_mode_wb_timer;

commit 4aa9c692e052cf6db99db62a8fe0543e5c455da7
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:35 2015 -0400

    bdi: separate out congested state into a separate struct
    
    Currently, a wb's (bdi_writeback) congestion state is carried in its
    ->state field; however, cgroup writeback support will require multiple
    wb's sharing the same congestion state.  This patch separates out
    congestion state into its own struct - struct bdi_writeback_congested.
    A new field wb field, wb_congested, points to its associated congested
    struct.  The default wb, bdi->wb, always points to bdi->wb_congested.
    
    While this patch adds a layer of indirection, it doesn't introduce any
    behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index aa18c4bd43c1..9e9eafa5f5aa 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -16,12 +16,15 @@ struct dentry;
  * Bits in bdi_writeback.state
  */
 enum wb_state {
-	WB_async_congested,	/* The async (write) queue is getting full */
-	WB_sync_congested,	/* The sync queue is getting full */
 	WB_registered,		/* bdi_register() was done */
 	WB_writeback_running,	/* Writeback is in progress */
 };
 
+enum wb_congested_state {
+	WB_async_congested,	/* The async (write) queue is getting full */
+	WB_sync_congested,	/* The sync queue is getting full */
+};
+
 typedef int (congested_fn)(void *, int);
 
 enum wb_stat_item {
@@ -34,6 +37,10 @@ enum wb_stat_item {
 
 #define WB_STAT_BATCH (8*(1+ilog2(nr_cpu_ids)))
 
+struct bdi_writeback_congested {
+	unsigned long state;		/* WB_[a]sync_congested flags */
+};
+
 struct bdi_writeback {
 	struct backing_dev_info *bdi;	/* our parent bdi */
 
@@ -48,6 +55,8 @@ struct bdi_writeback {
 
 	struct percpu_counter stat[NR_WB_STAT_ITEMS];
 
+	struct bdi_writeback_congested *congested;
+
 	unsigned long bw_time_stamp;	/* last time write bw is updated */
 	unsigned long dirtied_stamp;
 	unsigned long written_stamp;	/* pages written at bw_time_stamp */
@@ -84,6 +93,7 @@ struct backing_dev_info {
 	unsigned int max_ratio, max_prop_frac;
 
 	struct bdi_writeback wb;  /* default writeback info for this bdi */
+	struct bdi_writeback_congested wb_congested;
 
 	struct device *dev;
 

commit 66114cad64bf76a155fec1f0fff0de771cf909d5
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:32 2015 -0400

    writeback: separate out include/linux/backing-dev-defs.h
    
    With the planned cgroup writeback support, backing-dev related
    declarations will be more widely used across block and cgroup;
    unfortunately, including backing-dev.h from include/linux/blkdev.h
    makes cyclic include dependency quite likely.
    
    This patch separates out backing-dev-defs.h which only has the
    essential definitions and updates blkdev.h to include it.  c files
    which need access to more backing-dev details now include
    backing-dev.h directly.  This takes backing-dev.h off the common
    include dependency chain making it a lot easier to use it across block
    and cgroup.
    
    v2: fs/fat build failure fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
new file mode 100644
index 000000000000..aa18c4bd43c1
--- /dev/null
+++ b/include/linux/backing-dev-defs.h
@@ -0,0 +1,106 @@
+#ifndef __LINUX_BACKING_DEV_DEFS_H
+#define __LINUX_BACKING_DEV_DEFS_H
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/percpu_counter.h>
+#include <linux/flex_proportions.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+
+struct page;
+struct device;
+struct dentry;
+
+/*
+ * Bits in bdi_writeback.state
+ */
+enum wb_state {
+	WB_async_congested,	/* The async (write) queue is getting full */
+	WB_sync_congested,	/* The sync queue is getting full */
+	WB_registered,		/* bdi_register() was done */
+	WB_writeback_running,	/* Writeback is in progress */
+};
+
+typedef int (congested_fn)(void *, int);
+
+enum wb_stat_item {
+	WB_RECLAIMABLE,
+	WB_WRITEBACK,
+	WB_DIRTIED,
+	WB_WRITTEN,
+	NR_WB_STAT_ITEMS
+};
+
+#define WB_STAT_BATCH (8*(1+ilog2(nr_cpu_ids)))
+
+struct bdi_writeback {
+	struct backing_dev_info *bdi;	/* our parent bdi */
+
+	unsigned long state;		/* Always use atomic bitops on this */
+	unsigned long last_old_flush;	/* last old data flush */
+
+	struct list_head b_dirty;	/* dirty inodes */
+	struct list_head b_io;		/* parked for writeback */
+	struct list_head b_more_io;	/* parked for more writeback */
+	struct list_head b_dirty_time;	/* time stamps are dirty */
+	spinlock_t list_lock;		/* protects the b_* lists */
+
+	struct percpu_counter stat[NR_WB_STAT_ITEMS];
+
+	unsigned long bw_time_stamp;	/* last time write bw is updated */
+	unsigned long dirtied_stamp;
+	unsigned long written_stamp;	/* pages written at bw_time_stamp */
+	unsigned long write_bandwidth;	/* the estimated write bandwidth */
+	unsigned long avg_write_bandwidth; /* further smoothed write bw */
+
+	/*
+	 * The base dirty throttle rate, re-calculated on every 200ms.
+	 * All the bdi tasks' dirty rate will be curbed under it.
+	 * @dirty_ratelimit tracks the estimated @balanced_dirty_ratelimit
+	 * in small steps and is much more smooth/stable than the latter.
+	 */
+	unsigned long dirty_ratelimit;
+	unsigned long balanced_dirty_ratelimit;
+
+	struct fprop_local_percpu completions;
+	int dirty_exceeded;
+
+	spinlock_t work_lock;		/* protects work_list & dwork scheduling */
+	struct list_head work_list;
+	struct delayed_work dwork;	/* work item used for writeback */
+};
+
+struct backing_dev_info {
+	struct list_head bdi_list;
+	unsigned long ra_pages;	/* max readahead in PAGE_CACHE_SIZE units */
+	unsigned int capabilities; /* Device capabilities */
+	congested_fn *congested_fn; /* Function pointer if device is md/dm */
+	void *congested_data;	/* Pointer to aux data for congested func */
+
+	char *name;
+
+	unsigned int min_ratio;
+	unsigned int max_ratio, max_prop_frac;
+
+	struct bdi_writeback wb;  /* default writeback info for this bdi */
+
+	struct device *dev;
+
+	struct timer_list laptop_mode_wb_timer;
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *debug_dir;
+	struct dentry *debug_stats;
+#endif
+};
+
+enum {
+	BLK_RW_ASYNC	= 0,
+	BLK_RW_SYNC	= 1,
+};
+
+void clear_bdi_congested(struct backing_dev_info *bdi, int sync);
+void set_bdi_congested(struct backing_dev_info *bdi, int sync);
+
+#endif	/* __LINUX_BACKING_DEV_DEFS_H */
