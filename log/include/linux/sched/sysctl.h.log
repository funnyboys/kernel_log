commit 0ec9dc9bcba0a62b0844e54c1caf6b8b0bf6b5b4
Author: Guilherme G. Piccoli <gpiccoli@canonical.com>
Date:   Sun Jun 7 21:40:45 2020 -0700

    kernel/hung_task.c: introduce sysctl to print all traces when a hung task is detected
    
    Commit 401c636a0eeb ("kernel/hung_task.c: show all hung tasks before
    panic") introduced a change in that we started to show all CPUs
    backtraces when a hung task is detected _and_ the sysctl/kernel
    parameter "hung_task_panic" is set.  The idea is good, because usually
    when observing deadlocks (that may lead to hung tasks), the culprit is
    another task holding a lock and not necessarily the task detected as
    hung.
    
    The problem with this approach is that dumping backtraces is a slightly
    expensive task, specially printing that on console (and specially in
    many CPU machines, as servers commonly found nowadays).  So, users that
    plan to collect a kdump to investigate the hung tasks and narrow down
    the deadlock definitely don't need the CPUs backtrace on dmesg/console,
    which will delay the panic and pollute the log (crash tool would easily
    grab all CPUs traces with 'bt -a' command).
    
    Also, there's the reciprocal scenario: some users may be interested in
    seeing the CPUs backtraces but not have the system panic when a hung
    task is detected.  The current approach hence is almost as embedding a
    policy in the kernel, by forcing the CPUs backtraces' dump (only) on
    hung_task_panic.
    
    This patch decouples the panic event on hung task from the CPUs
    backtraces dump, by creating (and documenting) a new sysctl called
    "hung_task_all_cpu_backtrace", analog to the approach taken on soft/hard
    lockups, that have both a panic and an "all_cpu_backtrace" sysctl to
    allow individual control.  The new mechanism for dumping the CPUs
    backtraces on hung task detection respects "hung_task_warnings" by not
    dumping the traces in case there's no warnings left.
    
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Link: http://lkml.kernel.org/r/20200327223646.20779-1-gpiccoli@canonical.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 7b4d3a49b6c5..660ac49f2b53 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -7,6 +7,13 @@
 struct ctl_table;
 
 #ifdef CONFIG_DETECT_HUNG_TASK
+
+#ifdef CONFIG_SMP
+extern unsigned int sysctl_hung_task_all_cpu_backtrace;
+#else
+#define sysctl_hung_task_all_cpu_backtrace 0
+#endif /* CONFIG_SMP */
+
 extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_timeout_secs;

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index d4f6215ee03f..7b4d3a49b6c5 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -12,9 +12,8 @@ extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_timeout_secs;
 extern unsigned long sysctl_hung_task_check_interval_secs;
 extern int sysctl_hung_task_warnings;
-extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
-					 void __user *buffer,
-					 size_t *lenp, loff_t *ppos);
+int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
 #else
 /* Avoid need for ifdefs elsewhere in the code */
 enum { sysctl_hung_task_timeout_secs = 0 };
@@ -43,8 +42,7 @@ extern __read_mostly unsigned int sysctl_sched_migration_cost;
 extern __read_mostly unsigned int sysctl_sched_nr_migrate;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *length,
-		loff_t *ppos);
+		void *buffer, size_t *length, loff_t *ppos);
 #endif
 
 /*
@@ -72,33 +70,21 @@ extern unsigned int sysctl_sched_autogroup_enabled;
 extern int sysctl_sched_rr_timeslice;
 extern int sched_rr_timeslice;
 
-extern int sched_rr_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-
-extern int sched_rt_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-
-#ifdef CONFIG_UCLAMP_TASK
-extern int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
-				       void __user *buffer, size_t *lenp,
-				       loff_t *ppos);
-#endif
-
-extern int sysctl_numa_balancing(struct ctl_table *table, int write,
-				 void __user *buffer, size_t *lenp,
-				 loff_t *ppos);
-
-extern int sysctl_schedstats(struct ctl_table *table, int write,
-				 void __user *buffer, size_t *lenp,
-				 loff_t *ppos);
+int sched_rr_handler(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
+int sched_rt_handler(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
+int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
+int sysctl_numa_balancing(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
+int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
+		size_t *lenp, loff_t *ppos);
 
 #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
 extern unsigned int sysctl_sched_energy_aware;
-extern int sched_energy_aware_handler(struct ctl_table *table, int write,
-				 void __user *buffer, size_t *lenp,
-				 loff_t *ppos);
+int sched_energy_aware_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
 #endif
 
 #endif /* _LINUX_SCHED_SYSCTL_H */

commit e8f14172c6b11e9a86c65532497087f8eb0f91b1
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:05 2019 +0100

    sched/uclamp: Add system default clamps
    
    Tasks without a user-defined clamp value are considered not clamped
    and by default their utilization can have any value in the
    [0..SCHED_CAPACITY_SCALE] range.
    
    Tasks with a user-defined clamp value are allowed to request any value
    in that range, and the required clamp is unconditionally enforced.
    However, a "System Management Software" could be interested in limiting
    the range of clamp values allowed for all tasks.
    
    Add a privileged interface to define a system default configuration via:
    
      /proc/sys/kernel/sched_uclamp_util_{min,max}
    
    which works as an unconditional clamp range restriction for all tasks.
    
    With the default configuration, the full SCHED_CAPACITY_SCALE range of
    values is allowed for each clamp index. Otherwise, the task-specific
    clamp is capped by the corresponding system default value.
    
    Do that by tracking, for each task, the "effective" clamp value and
    bucket the task has been refcounted in at enqueue time. This
    allows to lazy aggregate "requested" and "system default" values at
    enqueue time and simplifies refcounting updates at dequeue time.
    
    The cached bucket ids are used to avoid (relatively) more expensive
    integer divisions every time a task is enqueued.
    
    An active flag is used to report when the "effective" value is valid and
    thus the task is actually refcounted in the corresponding rq's bucket.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-5-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 99ce6d728df7..d4f6215ee03f 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -56,6 +56,11 @@ int sched_proc_update_handler(struct ctl_table *table, int write,
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
+#ifdef CONFIG_UCLAMP_TASK
+extern unsigned int sysctl_sched_uclamp_util_min;
+extern unsigned int sysctl_sched_uclamp_util_max;
+#endif
+
 #ifdef CONFIG_CFS_BANDWIDTH
 extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 #endif
@@ -75,6 +80,12 @@ extern int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+#ifdef CONFIG_UCLAMP_TASK
+extern int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
+				       void __user *buffer, size_t *lenp,
+				       loff_t *ppos);
+#endif
+
 extern int sysctl_numa_balancing(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp,
 				 loff_t *ppos);

commit 8d5d0cfb63cbcb4005e19a332b31d687b1d01e58
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:23 2018 +0000

    sched/topology: Introduce a sysctl for Energy Aware Scheduling
    
    In its current state, Energy Aware Scheduling (EAS) starts automatically
    on asymmetric platforms having an Energy Model (EM). However, there are
    users who want to have an EM (for thermal management for example), but
    don't want EAS with it.
    
    In order to let users disable EAS explicitly, introduce a new sysctl
    called 'sched_energy_aware'. It is enabled by default so that EAS can
    start automatically on platforms where it makes sense. Flipping it to 0
    rebuilds the scheduling domains and disables EAS.
    
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-11-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index a9c32daeb9d8..99ce6d728df7 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -83,4 +83,11 @@ extern int sysctl_schedstats(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp,
 				 loff_t *ppos);
 
+#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+extern unsigned int sysctl_sched_energy_aware;
+extern int sched_energy_aware_handler(struct ctl_table *table, int write,
+				 void __user *buffer, size_t *lenp,
+				 loff_t *ppos);
+#endif
+
 #endif /* _LINUX_SCHED_SYSCTL_H */

commit a2e514453861dd39b53b7a50b6771bd3f9852078
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Aug 21 21:55:52 2018 -0700

    kernel/hung_task.c: allow to set checking interval separately from timeout
    
    Currently task hung checking interval is equal to timeout, as the result
    hung is detected anywhere between timeout and 2*timeout.  This is fine for
    most interactive environments, but this hurts automated testing setups
    (syzbot).  In an automated setup we need to strictly order CPU lockup <
    RCU stall < workqueue lockup < task hung < silent loss, so that RCU stall
    is not detected as task hung and task hung is not detected as silent
    machine loss.  The large variance in task hung detection timeout requires
    setting silent machine loss timeout to a very large value (e.g.  if task
    hung is 3 mins, then silent loss need to be set to ~7 mins).  The
    additional 3 minutes significantly reduce testing efficiency because
    usually we crash kernel within a minute, and this can add hours to bug
    localization process as it needs to do dozens of tests.
    
    Allow setting checking interval separately from timeout.  This allows to
    set timeout to, say, 3 minutes, but checking interval to 10 secs.
    
    The interval is controlled via a new hung_task_check_interval_secs sysctl,
    similar to the existing hung_task_timeout_secs sysctl.  The default value
    of 0 results in the current behavior: checking interval is equal to
    timeout.
    
    [akpm@linux-foundation.org: update hung_task_timeout_max's comment]
    Link: http://lkml.kernel.org/r/20180611111004.203513-1-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 913488d828cb..a9c32daeb9d8 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -10,6 +10,7 @@ struct ctl_table;
 extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_timeout_secs;
+extern unsigned long sysctl_hung_task_check_interval_secs;
 extern int sysctl_hung_task_warnings;
 extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 					 void __user *buffer,

commit 5fd778915ad29184a5ff8eb82d1118f6916b79e4
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:14 2018 +0200

    sched/sysctl: Remove unused sched_time_avg_ms sysctl
    
    /proc/sys/kernel/sched_time_avg_ms entry is not used anywhere,
    remove it.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-12-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 1c1a1512ec55..913488d828cb 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -40,7 +40,6 @@ extern unsigned int sysctl_numa_balancing_scan_size;
 #ifdef CONFIG_SCHED_DEBUG
 extern __read_mostly unsigned int sysctl_sched_migration_cost;
 extern __read_mostly unsigned int sysctl_sched_nr_migrate;
-extern __read_mostly unsigned int sysctl_sched_time_avg;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,

commit 8a103df440afea30c91ebd42e61dc644e647f4bd
Merge: a9903f04e0a4 fbc3edf7d773
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 8 10:17:15 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 0f5ecd4d298e..d6a18a3839cc 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_SCHED_SYSCTL_H
 #define _LINUX_SCHED_SYSCTL_H
 

commit a9903f04e0a4ea522d959c2f287cdf0ab029e324
Author: Matthias Kaehlcke <mka@chromium.org>
Date:   Mon Oct 30 11:08:16 2017 -0700

    sched/sysctl: Fix attributes of some extern declarations
    
    The definition of sysctl_sched_migration_cost, sysctl_sched_nr_migrate
    and sysctl_sched_time_avg includes the attribute const_debug. This
    attribute is not part of the extern declaration of these variables in
    include/linux/sched/sysctl.h, while it is in kernel/sched/sched.h,
    and as a result Clang generates warnings like this:
    
      kernel/sched/sched.h:1618:33: warning: section attribute is specified on redeclared variable [-Wsection]
      extern const_debug unsigned int sysctl_sched_time_avg;
                                    ^
      ./include/linux/sched/sysctl.h:42:21: note: previous declaration is here
      extern unsigned int sysctl_sched_time_avg;
    
    The header only declares the variables when CONFIG_SCHED_DEBUG is defined,
    therefore it is not necessary to duplicate the definition of const_debug.
    Instead we can use the attribute __read_mostly, which is the expansion of
    const_debug when CONFIG_SCHED_DEBUG=y is set.
    
    Signed-off-by: Matthias Kaehlcke <mka@chromium.org>
    Reviewed-by: Nick Desaulniers <nick.desaulniers@gmail.com>
    Cc: Douglas Anderson <dianders@chromium.org>
    Cc: Guenter Roeck <groeck@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Shile Zhang <shile.zhang@nokia.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171030180816.170850-1-mka@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 0f5ecd4d298e..d34c823f3d36 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -37,9 +37,9 @@ extern unsigned int sysctl_numa_balancing_scan_period_max;
 extern unsigned int sysctl_numa_balancing_scan_size;
 
 #ifdef CONFIG_SCHED_DEBUG
-extern unsigned int sysctl_sched_migration_cost;
-extern unsigned int sysctl_sched_nr_migrate;
-extern unsigned int sysctl_sched_time_avg;
+extern __read_mostly unsigned int sysctl_sched_migration_cost;
+extern __read_mostly unsigned int sysctl_sched_nr_migrate;
+extern __read_mostly unsigned int sysctl_sched_time_avg;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,

commit 9ccd27cc2e8bfbec502d7c64a353dc4489536b81
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 21:11:09 2017 +0100

    sched/headers: Make all include/linux/sched/*.h headers build standalone
    
    Make each header self-sufficient, so that it can be built successfully
    both in an allnoconfig and allyesconfig kernel.
    
    Also standardize the naming of their header guards.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 49308e142aae..0f5ecd4d298e 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -1,5 +1,9 @@
-#ifndef _SCHED_SYSCTL_H
-#define _SCHED_SYSCTL_H
+#ifndef _LINUX_SCHED_SYSCTL_H
+#define _LINUX_SCHED_SYSCTL_H
+
+#include <linux/types.h>
+
+struct ctl_table;
 
 #ifdef CONFIG_DETECT_HUNG_TASK
 extern int	     sysctl_hung_task_check_count;
@@ -78,4 +82,4 @@ extern int sysctl_schedstats(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp,
 				 loff_t *ppos);
 
-#endif /* _SCHED_SYSCTL_H */
+#endif /* _LINUX_SCHED_SYSCTL_H */

commit 975e155ed8732cb81f55c021c441ae662dd040b5
Author: Shile Zhang <shile.zhang@nokia.com>
Date:   Sat Jan 28 22:00:49 2017 +0800

    sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
    
    We added the 'sched_rr_timeslice_ms' SCHED_RR tuning knob in this commit:
    
      ce0dbbbb30ae ("sched/rt: Add a tuning knob to allow changing SCHED_RR timeslice")
    
    ... which name suggests to users that it's in milliseconds, while in reality
    it's being set in milliseconds but the result is shown in jiffies.
    
    This is obviously confusing when HZ is not 1000, it makes it appear like the
    value set failed, such as HZ=100:
    
      root# echo 100 > /proc/sys/kernel/sched_rr_timeslice_ms
      root# cat /proc/sys/kernel/sched_rr_timeslice_ms
      10
    
    Fix this to be milliseconds all around.
    
    Signed-off-by: Shile Zhang <shile.zhang@nokia.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1485612049-20923-1-git-send-email-shile.zhang@nokia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 441145351301..49308e142aae 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -59,6 +59,7 @@ extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 extern unsigned int sysctl_sched_autogroup_enabled;
 #endif
 
+extern int sysctl_sched_rr_timeslice;
 extern int sched_rr_timeslice;
 
 extern int sched_rr_handler(struct ctl_table *table, int write,

commit 3c3fcb45d524feb5d14a14f332e3eec7f2aff8f3
Author: Matt Fleming <matt@codeblueprint.co.uk>
Date:   Wed Oct 19 15:10:59 2016 +0100

    sched/fair: Kill the unused 'sched_shares_window_ns' tunable
    
    The last user of this tunable was removed in 2012 in commit:
    
      82958366cfea ("sched: Replace update_shares weight distribution with per-entity computation")
    
    Delete it since its very existence confuses people.
    
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161019141059.26408-1-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 22db1e63707e..441145351301 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -36,7 +36,6 @@ extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;
-extern unsigned int sysctl_sched_shares_window;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,

commit ea606cf5d8df370e7932460dfd960b21f20e7c6d
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu Mar 17 14:18:48 2016 -0700

    mm: move max_map_count bits into mm.h
    
    max_map_count sysctl unrelated to scheduler. Move its bits from
    include/linux/sched/sysctl.h to include/linux/mm.h.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 4f080ab4f2cd..22db1e63707e 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -14,27 +14,6 @@ extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 enum { sysctl_hung_task_timeout_secs = 0 };
 #endif
 
-/*
- * Default maximum number of active map areas, this limits the number of vmas
- * per mm struct. Users can overwrite this number by sysctl but there is a
- * problem.
- *
- * When a program's coredump is generated as ELF format, a section is created
- * per a vma. In ELF, the number of sections is represented in unsigned short.
- * This means the number of sections should be smaller than 65535 at coredump.
- * Because the kernel adds some informative sections to a image of program at
- * generating coredump, we need some margin. The number of extra sections is
- * 1-3 now and depends on arch. We use "5" as safe margin, here.
- *
- * ELF extended numbering allows more than 65535 sections, so 16-bit bound is
- * not a hard limit any more. Although some userspace tools can be surprised by
- * that.
- */
-#define MAPCOUNT_ELF_CORE_MARGIN	(5)
-#define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
-
-extern int sysctl_max_map_count;
-
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;

commit cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Feb 5 09:08:36 2016 +0000

    sched/debug: Make schedstats a runtime tunable that is disabled by default
    
    schedstats is very useful during debugging and performance tuning but it
    incurs overhead to calculate the stats. As such, even though it can be
    disabled at build time, it is often enabled as the information is useful.
    
    This patch adds a kernel command-line and sysctl tunable to enable or
    disable schedstats on demand (when it's built in). It is disabled
    by default as someone who knows they need it can also learn to enable
    it when necessary.
    
    The benefits are dependent on how scheduler-intensive the workload is.
    If it is then the patch reduces the number of cycles spent calculating
    the stats with a small benefit from reducing the cache footprint of the
    scheduler.
    
    These measurements were taken from a 48-core 2-socket
    machine with Xeon(R) E5-2670 v3 cpus although they were also tested on a
    single socket machine 8-core machine with Intel i7-3770 processors.
    
    netperf-tcp
                               4.5.0-rc1             4.5.0-rc1
                                 vanilla          nostats-v3r1
    Hmean    64         560.45 (  0.00%)      575.98 (  2.77%)
    Hmean    128        766.66 (  0.00%)      795.79 (  3.80%)
    Hmean    256        950.51 (  0.00%)      981.50 (  3.26%)
    Hmean    1024      1433.25 (  0.00%)     1466.51 (  2.32%)
    Hmean    2048      2810.54 (  0.00%)     2879.75 (  2.46%)
    Hmean    3312      4618.18 (  0.00%)     4682.09 (  1.38%)
    Hmean    4096      5306.42 (  0.00%)     5346.39 (  0.75%)
    Hmean    8192     10581.44 (  0.00%)    10698.15 (  1.10%)
    Hmean    16384    18857.70 (  0.00%)    18937.61 (  0.42%)
    
    Small gains here, UDP_STREAM showed nothing intresting and neither did
    the TCP_RR tests. The gains on the 8-core machine were very similar.
    
    tbench4
                                     4.5.0-rc1             4.5.0-rc1
                                       vanilla          nostats-v3r1
    Hmean    mb/sec-1         500.85 (  0.00%)      522.43 (  4.31%)
    Hmean    mb/sec-2         984.66 (  0.00%)     1018.19 (  3.41%)
    Hmean    mb/sec-4        1827.91 (  0.00%)     1847.78 (  1.09%)
    Hmean    mb/sec-8        3561.36 (  0.00%)     3611.28 (  1.40%)
    Hmean    mb/sec-16       5824.52 (  0.00%)     5929.03 (  1.79%)
    Hmean    mb/sec-32      10943.10 (  0.00%)    10802.83 ( -1.28%)
    Hmean    mb/sec-64      15950.81 (  0.00%)    16211.31 (  1.63%)
    Hmean    mb/sec-128     15302.17 (  0.00%)    15445.11 (  0.93%)
    Hmean    mb/sec-256     14866.18 (  0.00%)    15088.73 (  1.50%)
    Hmean    mb/sec-512     15223.31 (  0.00%)    15373.69 (  0.99%)
    Hmean    mb/sec-1024    14574.25 (  0.00%)    14598.02 (  0.16%)
    Hmean    mb/sec-2048    13569.02 (  0.00%)    13733.86 (  1.21%)
    Hmean    mb/sec-3072    12865.98 (  0.00%)    13209.23 (  2.67%)
    
    Small gains of 2-4% at low thread counts and otherwise flat.  The
    gains on the 8-core machine were slightly different
    
    tbench4 on 8-core i7-3770 single socket machine
    Hmean    mb/sec-1        442.59 (  0.00%)      448.73 (  1.39%)
    Hmean    mb/sec-2        796.68 (  0.00%)      794.39 ( -0.29%)
    Hmean    mb/sec-4       1322.52 (  0.00%)     1343.66 (  1.60%)
    Hmean    mb/sec-8       2611.65 (  0.00%)     2694.86 (  3.19%)
    Hmean    mb/sec-16      2537.07 (  0.00%)     2609.34 (  2.85%)
    Hmean    mb/sec-32      2506.02 (  0.00%)     2578.18 (  2.88%)
    Hmean    mb/sec-64      2511.06 (  0.00%)     2569.16 (  2.31%)
    Hmean    mb/sec-128     2313.38 (  0.00%)     2395.50 (  3.55%)
    Hmean    mb/sec-256     2110.04 (  0.00%)     2177.45 (  3.19%)
    Hmean    mb/sec-512     2072.51 (  0.00%)     2053.97 ( -0.89%)
    
    In constract, this shows a relatively steady 2-3% gain at higher thread
    counts. Due to the nature of the patch and the type of workload, it's
    not a surprise that the result will depend on the CPU used.
    
    hackbench-pipes
                             4.5.0-rc1             4.5.0-rc1
                               vanilla          nostats-v3r1
    Amean    1        0.0637 (  0.00%)      0.0660 ( -3.59%)
    Amean    4        0.1229 (  0.00%)      0.1181 (  3.84%)
    Amean    7        0.1921 (  0.00%)      0.1911 (  0.52%)
    Amean    12       0.3117 (  0.00%)      0.2923 (  6.23%)
    Amean    21       0.4050 (  0.00%)      0.3899 (  3.74%)
    Amean    30       0.4586 (  0.00%)      0.4433 (  3.33%)
    Amean    48       0.5910 (  0.00%)      0.5694 (  3.65%)
    Amean    79       0.8663 (  0.00%)      0.8626 (  0.43%)
    Amean    110      1.1543 (  0.00%)      1.1517 (  0.22%)
    Amean    141      1.4457 (  0.00%)      1.4290 (  1.16%)
    Amean    172      1.7090 (  0.00%)      1.6924 (  0.97%)
    Amean    192      1.9126 (  0.00%)      1.9089 (  0.19%)
    
    Some small gains and losses and while the variance data is not included,
    it's close to the noise. The UMA machine did not show anything particularly
    different
    
    pipetest
                                 4.5.0-rc1             4.5.0-rc1
                                   vanilla          nostats-v2r2
    Min         Time        4.13 (  0.00%)        3.99 (  3.39%)
    1st-qrtle   Time        4.38 (  0.00%)        4.27 (  2.51%)
    2nd-qrtle   Time        4.46 (  0.00%)        4.39 (  1.57%)
    3rd-qrtle   Time        4.56 (  0.00%)        4.51 (  1.10%)
    Max-90%     Time        4.67 (  0.00%)        4.60 (  1.50%)
    Max-93%     Time        4.71 (  0.00%)        4.65 (  1.27%)
    Max-95%     Time        4.74 (  0.00%)        4.71 (  0.63%)
    Max-99%     Time        4.88 (  0.00%)        4.79 (  1.84%)
    Max         Time        4.93 (  0.00%)        4.83 (  2.03%)
    Mean        Time        4.48 (  0.00%)        4.39 (  1.91%)
    Best99%Mean Time        4.47 (  0.00%)        4.39 (  1.91%)
    Best95%Mean Time        4.46 (  0.00%)        4.38 (  1.93%)
    Best90%Mean Time        4.45 (  0.00%)        4.36 (  1.98%)
    Best50%Mean Time        4.36 (  0.00%)        4.25 (  2.49%)
    Best10%Mean Time        4.23 (  0.00%)        4.10 (  3.13%)
    Best5%Mean  Time        4.19 (  0.00%)        4.06 (  3.20%)
    Best1%Mean  Time        4.13 (  0.00%)        4.00 (  3.39%)
    
    Small improvement and similar gains were seen on the UMA machine.
    
    The gain is small but it stands to reason that doing less work in the
    scheduler is a good thing. The downside is that the lack of schedstats and
    tracepoints may be surprising to experts doing performance analysis until
    they find the existence of the schedstats= parameter or schedstats sysctl.
    It will be automatically activated for latencytop and sleep profiling to
    alleviate the problem. For tracepoints, there is a simple warning as it's
    not safe to activate schedstats in the context when it's known the tracepoint
    may be wanted but is unavailable.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454663316-22048-1-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index c9e4731cf10b..4f080ab4f2cd 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -95,4 +95,8 @@ extern int sysctl_numa_balancing(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp,
 				 loff_t *ppos);
 
+extern int sysctl_schedstats(struct ctl_table *table, int write,
+				 void __user *buffer, size_t *lenp,
+				 loff_t *ppos);
+
 #endif /* _SCHED_SYSCTL_H */

commit bc7a34b8b9ebfb0f4b8a35a72a0b134fd6c5ef50
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 26 22:50:33 2015 +0000

    timer: Reduce timer migration overhead if disabled
    
    Eric reported that the timer_migration sysctl is not really nice
    performance wise as it needs to check at every timer insertion whether
    the feature is enabled or not. Further the check does not live in the
    timer code, so we have an extra function call which checks an extra
    cache line to figure out that it is disabled.
    
    We can do better and store that information in the per cpu (hr)timer
    bases. I pondered to use a static key, but that's a nightmare to
    update from the nohz code and the timer base cache line is hot anyway
    when we select a timer base.
    
    The old logic enabled the timer migration unconditionally if
    CONFIG_NO_HZ was set even if nohz was disabled on the kernel command
    line.
    
    With this modification, we start off with migration disabled. The user
    visible sysctl is still set to enabled. If the kernel switches to NOHZ
    migration is enabled, if the user did not disable it via the sysctl
    prior to the switch. If nohz=off is on the kernel command line,
    migration stays disabled no matter what.
    
    Before:
      47.76%  hog       [.] main
      14.84%  [kernel]  [k] _raw_spin_lock_irqsave
       9.55%  [kernel]  [k] _raw_spin_unlock_irqrestore
       6.71%  [kernel]  [k] mod_timer
       6.24%  [kernel]  [k] lock_timer_base.isra.38
       3.76%  [kernel]  [k] detach_if_pending
       3.71%  [kernel]  [k] del_timer
       2.50%  [kernel]  [k] internal_add_timer
       1.51%  [kernel]  [k] get_nohz_timer_target
       1.28%  [kernel]  [k] __internal_add_timer
       0.78%  [kernel]  [k] timerfn
       0.48%  [kernel]  [k] wake_up_nohz_cpu
    
    After:
      48.10%  hog       [.] main
      15.25%  [kernel]  [k] _raw_spin_lock_irqsave
       9.76%  [kernel]  [k] _raw_spin_unlock_irqrestore
       6.50%  [kernel]  [k] mod_timer
       6.44%  [kernel]  [k] lock_timer_base.isra.38
       3.87%  [kernel]  [k] detach_if_pending
       3.80%  [kernel]  [k] del_timer
       2.67%  [kernel]  [k] internal_add_timer
       1.33%  [kernel]  [k] __internal_add_timer
       0.73%  [kernel]  [k] timerfn
       0.54%  [kernel]  [k] wake_up_nohz_cpu
    
    
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Joonwoo Park <joonwoop@codeaurora.org>
    Cc: Wenbo Wang <wenbo.wang@memblaze.com>
    Link: http://lkml.kernel.org/r/20150526224512.127050787@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 596a0e007c62..c9e4731cf10b 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -57,24 +57,12 @@ extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;
-extern unsigned int sysctl_timer_migration;
 extern unsigned int sysctl_sched_shares_window;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,
 		loff_t *ppos);
 #endif
-#ifdef CONFIG_SCHED_DEBUG
-static inline unsigned int get_sysctl_timer_migration(void)
-{
-	return sysctl_timer_migration;
-}
-#else
-static inline unsigned int get_sysctl_timer_migration(void)
-{
-	return 1;
-}
-#endif
 
 /*
  *  control realtime throttling:

commit 3fb1c8dcfcda2f5bfb7d79d8b08bf2f04b1eed8f
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Jun 4 16:08:20 2014 -0700

    mm: update comment for DEFAULT_MAX_MAP_COUNT
    
    With ELF extended numbering 16-bit bound is not hard limit any more.
    
    [akpm@linux-foundation.org: fix typo]
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 8045a554cafb..596a0e007c62 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -25,6 +25,10 @@ enum { sysctl_hung_task_timeout_secs = 0 };
  * Because the kernel adds some informative sections to a image of program at
  * generating coredump, we need some margin. The number of extra sections is
  * 1-3 now and depends on arch. We use "5" as safe margin, here.
+ *
+ * ELF extended numbering allows more than 65535 sections, so 16-bit bound is
+ * not a hard limit any more. Although some userspace tools can be surprised by
+ * that.
  */
 #define MAPCOUNT_ELF_CORE_MARGIN	(5)
 #define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)

commit ab5318788c6725b6d5c95aff28e63af4c35a0e2c
Merge: 14164b46fc99 270750dbc18a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 31 08:59:46 2014 -0800

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core debug changes from Ingo Molnar:
     "This contains mostly kernel debugging related updates:
    
       - make hung_task detection more configurable to distros
       - add final bits for x86 UV NMI debugging, with related KGDB changes
       - update the mailing-list of MAINTAINERS entries I'm involved with"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      hung_task: Display every hung task warning
      sysctl: Add neg_one as a standard constraint
      x86/uv/nmi, kgdb/kdb: Fix UV NMI handler when KDB not configured
      x86/uv/nmi: Fix Sparse warnings
      kgdb/kdb: Fix no KDB config problem
      MAINTAINERS: Restore "L: linux-kernel@vger.kernel.org" entries

commit 270750dbc18a71b23d660df110e433ff9616a2d4
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Mon Jan 20 17:34:13 2014 +0000

    hung_task: Display every hung task warning
    
    When khungtaskd detects hung tasks, it prints out
    backtraces from a number of those tasks.
    
    Limiting the number of backtraces being printed
    out can result in the user not seeing the information
    necessary to debug the issue. The hung_task_warnings
    sysctl controls this feature.
    
    This patch makes it possible for hung_task_warnings
    to accept a special value to print an unlimited
    number of backtraces when khungtaskd detects hung
    tasks.
    
    The special value is -1. To use this value it is
    necessary to change types from ulong to int.
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: oleg@redhat.com
    Link: http://lkml.kernel.org/r/1390239253-24030-3-git-send-email-atomlin@redhat.com
    [ Build warning fix. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 31e0193cb0c5..3a93f842306a 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -5,7 +5,7 @@
 extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_timeout_secs;
-extern unsigned long sysctl_hung_task_warnings;
+extern int sysctl_hung_task_warnings;
 extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 					 void __user *buffer,
 					 size_t *lenp, loff_t *ppos);

commit 54a43d54988a3731d644fdeb7a1d6f46b4ac64c7
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Jan 23 15:53:13 2014 -0800

    numa: add a sysctl for numa_balancing
    
    Add a working sysctl to enable/disable automatic numa memory balancing
    at runtime.
    
    This allows us to track down performance problems with this feature and
    is generally a good idea.
    
    This was possible earlier through debugfs, but only with special
    debugging options set.  Also fix the boot message.
    
    [akpm@linux-foundation.org: s/sched_numa_balancing/sysctl_numa_balancing/]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 31e0193cb0c5..b13cf430764f 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -99,4 +99,8 @@ extern int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+extern int sysctl_numa_balancing(struct ctl_table *table, int write,
+				 void __user *buffer, size_t *lenp,
+				 loff_t *ppos);
+
 #endif /* _SCHED_SYSCTL_H */

commit 1724813d9f2c7ff702b46d3e4a4f6d9b10a8f8c2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 17 12:44:49 2013 +0100

    sched/deadline: Remove the sysctl_sched_dl knobs
    
    Remove the deadline specific sysctls for now. The problem with them is
    that the interaction with the exisiting rt knobs is nearly impossible
    to get right.
    
    The current (as per before this patch) situation is that the rt and dl
    bandwidth is completely separate and we enforce rt+dl < 100%. This is
    undesirable because this means that the rt default of 95% leaves us
    hardly any room, even though dl tasks are saver than rt tasks.
    
    Another proposed solution was (a discarted patch) to have the dl
    bandwidth be a fraction of the rt bandwidth. This is highly
    confusing imo.
    
    Furthermore neither proposal is consistent with the situation we
    actually want; which is rt tasks ran from a dl server. In which case
    the rt bandwidth is a direct subset of dl.
    
    So whichever way we go, the introduction of dl controls at this point
    is painful. Therefore remove them and instead share the rt budget.
    
    This means that for now the rt knobs are used for dl admission control
    and the dl runtime is accounted against the rt runtime. I realise that
    this isn't entirely desirable either; but whatever we do we appear to
    need to change the interface later, so better have a small interface
    for now.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-zpyqbqds1r0vyxtxza1e7rdc@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 8070a83dbedc..31e0193cb0c5 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -81,15 +81,6 @@ static inline unsigned int get_sysctl_timer_migration(void)
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
-/*
- *  control SCHED_DEADLINE reservations:
- *
- *  /proc/sys/kernel/sched_dl_period_us
- *  /proc/sys/kernel/sched_dl_runtime_us
- */
-extern unsigned int sysctl_sched_dl_period;
-extern int sysctl_sched_dl_runtime;
-
 #ifdef CONFIG_CFS_BANDWIDTH
 extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 #endif
@@ -108,8 +99,4 @@ extern int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
-int sched_dl_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-
 #endif /* _SCHED_SYSCTL_H */

commit 332ac17ef5bfcff4766dfdfd3b4cdf10b8f8f155
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:45 2013 +0100

    sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks
    
    In order of deadline scheduling to be effective and useful, it is
    important that some method of having the allocation of the available
    CPU bandwidth to tasks and task groups under control.
    This is usually called "admission control" and if it is not performed
    at all, no guarantee can be given on the actual scheduling of the
    -deadline tasks.
    
    Since when RT-throttling has been introduced each task group have a
    bandwidth associated to itself, calculated as a certain amount of
    runtime over a period. Moreover, to make it possible to manipulate
    such bandwidth, readable/writable controls have been added to both
    procfs (for system wide settings) and cgroupfs (for per-group
    settings).
    
    Therefore, the same interface is being used for controlling the
    bandwidth distrubution to -deadline tasks and task groups, i.e.,
    new controls but with similar names, equivalent meaning and with
    the same usage paradigm are added.
    
    However, more discussion is needed in order to figure out how
    we want to manage SCHED_DEADLINE bandwidth at the task group level.
    Therefore, this patch adds a less sophisticated, but actually
    very sensible, mechanism to ensure that a certain utilization
    cap is not overcome per each root_domain (the single rq for !SMP
    configurations).
    
    Another main difference between deadline bandwidth management and
    RT-throttling is that -deadline tasks have bandwidth on their own
    (while -rt ones doesn't!), and thus we don't need an higher level
    throttling mechanism to enforce the desired bandwidth.
    
    This patch, therefore:
    
     - adds system wide deadline bandwidth management by means of:
        * /proc/sys/kernel/sched_dl_runtime_us,
        * /proc/sys/kernel/sched_dl_period_us,
       that determine (i.e., runtime / period) the total bandwidth
       available on each CPU of each root_domain for -deadline tasks;
    
     - couples the RT and deadline bandwidth management, i.e., enforces
       that the sum of how much bandwidth is being devoted to -rt
       -deadline tasks to stay below 100%.
    
    This means that, for a root_domain comprising M CPUs, -deadline tasks
    can be created until the sum of their bandwidths stay below:
    
        M * (sched_dl_runtime_us / sched_dl_period_us)
    
    It is also possible to disable this bandwidth management logic, and
    be thus free of oversubscribing the system up to any arbitrary level.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-12-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 31e0193cb0c5..8070a83dbedc 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -81,6 +81,15 @@ static inline unsigned int get_sysctl_timer_migration(void)
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
+/*
+ *  control SCHED_DEADLINE reservations:
+ *
+ *  /proc/sys/kernel/sched_dl_period_us
+ *  /proc/sys/kernel/sched_dl_runtime_us
+ */
+extern unsigned int sysctl_sched_dl_period;
+extern int sysctl_sched_dl_runtime;
+
 #ifdef CONFIG_CFS_BANDWIDTH
 extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 #endif
@@ -99,4 +108,8 @@ extern int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+int sched_dl_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 #endif /* _SCHED_SYSCTL_H */

commit 1bd53a7efdc988163ec4c25f656df38dbe500632
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Thu Dec 12 15:23:23 2013 +0800

    sched/numa: Drop sysctl_numa_balancing_settle_count sysctl
    
    commit 887c290e (sched/numa: Decide whether to favour task or group weights
    based on swap candidate relationships) drop the check against
    sysctl_numa_balancing_settle_count, this patch remove the sysctl.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Link: http://lkml.kernel.org/r/1386833006-6600-1-git-send-email-liwanp@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 41467f8ff8ec..31e0193cb0c5 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -48,7 +48,6 @@ extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
 extern unsigned int sysctl_numa_balancing_scan_size;
-extern unsigned int sysctl_numa_balancing_settle_count;
 
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_migration_cost;

commit c90423d1de12fbeaf0c898e1db0e962de347302b
Merge: ecf1f014325b b8a216269ec0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 6 07:50:37 2013 +0100

    Merge branch 'sched/core' into core/locking, to prepare the kernel/locking/ file move
    
    Conflicts:
            kernel/Makefile
    
    There are conflicts in kernel/Makefile due to file moving in the
    scheduler tree - resolve them.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 930aa174fcc8b0efaad102fd80f677b92f35eaa2
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:37 2013 +0100

    sched/numa: Remove the numa_balancing_scan_period_reset sysctl
    
    With scan rate adaptions based on whether the workload has properly
    converged or not there should be no need for the scan period reset
    hammer. Get rid of it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-60-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index bf8086b2506e..10d16c4fbe89 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -47,7 +47,6 @@ extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
-extern unsigned int sysctl_numa_balancing_scan_period_reset;
 extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_settle_count;
 

commit cd64647f043e3fd3569bcf068f47f030198ff93a
Author: Li Zefan <lizefan@huawei.com>
Date:   Mon Sep 23 16:43:58 2013 +0800

    hung_task: Change sysctl_hung_task_check_count to 'int'
    
    As 'sysctl_hung_task_check_count' is 'unsigned long' when this
    value is assigned to max_count in check_hung_uninterruptible_tasks(),
    it's truncated to 'int' type.
    
    This causes a minor artifact: if we write 2^32 to sysctl.hung_task_check_count,
    hung task detection will be effectively disabled.
    
    With this fix, it will still truncate the user input to 32 bits, but
    reading sysctl.hung_task_check_count reflects the actual truncated value.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/523FFF4E.9050401@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index bf8086b2506e..9552afa733d8 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -2,8 +2,8 @@
 #define _SCHED_SYSCTL_H
 
 #ifdef CONFIG_DETECT_HUNG_TASK
+extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
-extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;
 extern unsigned long sysctl_hung_task_warnings;
 extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,

commit bc681593b588786e6326b3e5f78ccc1683e2269c
Author: Clark Williams <clark.williams@gmail.com>
Date:   Fri Feb 22 09:20:11 2013 -0800

    sched: move RR_TIMESLICE from sysctl.h to rt.h
    
    Originally submitted by Clark Williams as part of a cleanup,
    but happens also to fix an ia64 build problem:
    
    arch/ia64/kernel/init_task.c:38: error: 'RR_TIMESLICE' undeclared here (not in a function)
    
    Signed-off-by: Clark Williams <clark.williams@gmail.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index d2bb0ae979d0..bf8086b2506e 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -91,12 +91,6 @@ extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 extern unsigned int sysctl_sched_autogroup_enabled;
 #endif
 
-/*
- * default timeslice is 100 msecs (used only for SCHED_RR tasks).
- * Timeslices get refilled after they expire.
- */
-#define RR_TIMESLICE		(100 * HZ / 1000)
-
 extern int sched_rr_timeslice;
 
 extern int sched_rr_handler(struct ctl_table *table, int write,

commit ce0dbbbb30aee6a835511d5be446462388ba9eee
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:47:04 2013 -0600

    sched/rt: Add a tuning knob to allow changing SCHED_RR timeslice
    
    Add a /proc/sys/kernel scheduler knob named
    sched_rr_timeslice_ms that allows global changing of the
    SCHED_RR timeslice value. User visable value is in milliseconds
    but is stored as jiffies.  Setting to 0 (zero) resets to the
    default (currently 100ms).
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094704.13751796@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index bac914e458ca..d2bb0ae979d0 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -73,6 +73,13 @@ static inline unsigned int get_sysctl_timer_migration(void)
 	return 1;
 }
 #endif
+
+/*
+ *  control realtime throttling:
+ *
+ *  /proc/sys/kernel/sched_rt_period_us
+ *  /proc/sys/kernel/sched_rt_runtime_us
+ */
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
@@ -90,7 +97,13 @@ extern unsigned int sysctl_sched_autogroup_enabled;
  */
 #define RR_TIMESLICE		(100 * HZ / 1000)
 
-int sched_rt_handler(struct ctl_table *table, int write,
+extern int sched_rr_timeslice;
+
+extern int sched_rr_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
+extern int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 

commit cf4aebc292fac7f34f8345664320e9d4a42ca76c
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:46:59 2013 -0600

    sched: Move sched.h sysctl bits into separate header
    
    Move the sysctl-related bits from include/linux/sched.h into
    a new file: include/linux/sched/sysctl.h. Then update source
    files requiring access to those bits by including the new
    header file.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094659.06dced96@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
new file mode 100644
index 000000000000..bac914e458ca
--- /dev/null
+++ b/include/linux/sched/sysctl.h
@@ -0,0 +1,97 @@
+#ifndef _SCHED_SYSCTL_H
+#define _SCHED_SYSCTL_H
+
+#ifdef CONFIG_DETECT_HUNG_TASK
+extern unsigned int  sysctl_hung_task_panic;
+extern unsigned long sysctl_hung_task_check_count;
+extern unsigned long sysctl_hung_task_timeout_secs;
+extern unsigned long sysctl_hung_task_warnings;
+extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
+					 void __user *buffer,
+					 size_t *lenp, loff_t *ppos);
+#else
+/* Avoid need for ifdefs elsewhere in the code */
+enum { sysctl_hung_task_timeout_secs = 0 };
+#endif
+
+/*
+ * Default maximum number of active map areas, this limits the number of vmas
+ * per mm struct. Users can overwrite this number by sysctl but there is a
+ * problem.
+ *
+ * When a program's coredump is generated as ELF format, a section is created
+ * per a vma. In ELF, the number of sections is represented in unsigned short.
+ * This means the number of sections should be smaller than 65535 at coredump.
+ * Because the kernel adds some informative sections to a image of program at
+ * generating coredump, we need some margin. The number of extra sections is
+ * 1-3 now and depends on arch. We use "5" as safe margin, here.
+ */
+#define MAPCOUNT_ELF_CORE_MARGIN	(5)
+#define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
+
+extern int sysctl_max_map_count;
+
+extern unsigned int sysctl_sched_latency;
+extern unsigned int sysctl_sched_min_granularity;
+extern unsigned int sysctl_sched_wakeup_granularity;
+extern unsigned int sysctl_sched_child_runs_first;
+
+enum sched_tunable_scaling {
+	SCHED_TUNABLESCALING_NONE,
+	SCHED_TUNABLESCALING_LOG,
+	SCHED_TUNABLESCALING_LINEAR,
+	SCHED_TUNABLESCALING_END,
+};
+extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
+
+extern unsigned int sysctl_numa_balancing_scan_delay;
+extern unsigned int sysctl_numa_balancing_scan_period_min;
+extern unsigned int sysctl_numa_balancing_scan_period_max;
+extern unsigned int sysctl_numa_balancing_scan_period_reset;
+extern unsigned int sysctl_numa_balancing_scan_size;
+extern unsigned int sysctl_numa_balancing_settle_count;
+
+#ifdef CONFIG_SCHED_DEBUG
+extern unsigned int sysctl_sched_migration_cost;
+extern unsigned int sysctl_sched_nr_migrate;
+extern unsigned int sysctl_sched_time_avg;
+extern unsigned int sysctl_timer_migration;
+extern unsigned int sysctl_sched_shares_window;
+
+int sched_proc_update_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *length,
+		loff_t *ppos);
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+static inline unsigned int get_sysctl_timer_migration(void)
+{
+	return sysctl_timer_migration;
+}
+#else
+static inline unsigned int get_sysctl_timer_migration(void)
+{
+	return 1;
+}
+#endif
+extern unsigned int sysctl_sched_rt_period;
+extern int sysctl_sched_rt_runtime;
+
+#ifdef CONFIG_CFS_BANDWIDTH
+extern unsigned int sysctl_sched_cfs_bandwidth_slice;
+#endif
+
+#ifdef CONFIG_SCHED_AUTOGROUP
+extern unsigned int sysctl_sched_autogroup_enabled;
+#endif
+
+/*
+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).
+ * Timeslices get refilled after they expire.
+ */
+#define RR_TIMESLICE		(100 * HZ / 1000)
+
+int sched_rt_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
+#endif /* _SCHED_SYSCTL_H */
