commit ff895103a84abc85a5f43ecabc7f67cf36e1348f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Mar 17 17:32:23 2020 -0400

    tracing: Save off entry when peeking at next entry
    
    In order to have the iterator read the buffer even when it's still updating,
    it requires that the ring buffer iterator saves each event in a separate
    location outside the ring buffer such that its use is immutable.
    
    There's one use case that saves off the event returned from the ring buffer
    interator and calls it again to look at the next event, before going back to
    use the first event. As the ring buffer iterator will only have a single
    copy, this use case will no longer be supported.
    
    Instead, have the one use case create its own buffer to store the first
    event when looking at the next event. This way, when looking at the first
    event again, it wont be corrupted by the second read.
    
    Link: http://lkml.kernel.org/r/20200317213415.722539921@goodmis.org
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 6c7a10a6d71e..5c6943354049 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -85,6 +85,8 @@ struct trace_iterator {
 	struct mutex		mutex;
 	struct ring_buffer_iter	**buffer_iter;
 	unsigned long		iter_flags;
+	void			*temp;	/* temp holder */
+	unsigned int		temp_size;
 
 	/* trace_seq for __print_flags() and __print_symbolic() etc. */
 	struct trace_seq	tmp_seq;

commit 61a75954034f951a77d58b1cfb9186c62e6abcf8
Merge: 0a679e13ea30 7276531d4036
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 11 16:39:18 2020 -0800

    Merge tag 'trace-v5.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing fixes from Steven Rostedt:
     "Various fixes:
    
       - Fix an uninitialized variable
    
       - Fix compile bug to bootconfig userspace tool (in tools directory)
    
       - Suppress some error messages of bootconfig userspace tool
    
       - Remove unneded CONFIG_LIBXBC from bootconfig
    
       - Allocate bootconfig xbc_nodes dynamically. To ease complaints about
         taking up static memory at boot up
    
       - Use of parse_args() to parse bootconfig instead of strstr() usage
         Prevents issues of double quotes containing the interested string
    
       - Fix missing ring_buffer_nest_end() on synthetic event error path
    
       - Return zero not -EINVAL on soft disabled synthetic event (soft
         disabling must be the same as hard disabling, which returns zero)
    
       - Consolidate synthetic event code (remove duplicate code)"
    
    * tag 'trace-v5.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Consolidate trace() functions
      tracing: Don't return -EINVAL when tracing soft disabled synth events
      tracing: Add missing nest end to synth_event_trace_start() error case
      tools/bootconfig: Suppress non-error messages
      bootconfig: Allocate xbc_nodes array dynamically
      bootconfig: Use parse_args() to find bootconfig and '--'
      tracing/kprobe: Fix uninitialized variable bug
      bootconfig: Remove unneeded CONFIG_LIBXBC
      tools/bootconfig: Fix wrong __VA_ARGS__ usage

commit 7276531d4036f5db2af15c8b6caa02e7741f5d80
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Mon Feb 10 17:06:50 2020 -0600

    tracing: Consolidate trace() functions
    
    Move the checking, buffer reserve and buffer commit code in
    synth_event_trace_start/end() into inline functions
    __synth_event_trace_start/end() so they can also be used by
    synth_event_trace() and synth_event_trace_array(), and then have all
    those functions use them.
    
    Also, change synth_event_trace_state.enabled to disabled so it only
    needs to be set if the event is disabled, which is not normally the
    case.
    
    Link: http://lkml.kernel.org/r/b1f3108d0f450e58192955a300e31d0405ab4149.1581374549.git.zanussi@kernel.org
    
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 67f528ecb9e5..21098298b49b 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -424,7 +424,7 @@ struct synth_event_trace_state {
 	struct synth_event *event;
 	unsigned int cur_field;
 	unsigned int n_u64;
-	bool enabled;
+	bool disabled;
 	bool add_next;
 	bool add_name;
 };

commit e310396bb8d7db977a0e10ef7b5040e98b89c34c
Merge: c1ef57a3a3f5 a00574036c26
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 6 07:12:11 2020 +0000

    Merge tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Added new "bootconfig".
    
       This looks for a file appended to initrd to add boot config options,
       and has been discussed thoroughly at Linux Plumbers.
    
       Very useful for adding kprobes at bootup.
    
       Only enabled if "bootconfig" is on the real kernel command line.
    
     - Created dynamic event creation.
    
       Merges common code between creating synthetic events and kprobe
       events.
    
     - Rename perf "ring_buffer" structure to "perf_buffer"
    
     - Rename ftrace "ring_buffer" structure to "trace_buffer"
    
       Had to rename existing "trace_buffer" to "array_buffer"
    
     - Allow trace_printk() to work withing (some) tracing code.
    
     - Sort of tracing configs to be a little better organized
    
     - Fixed bug where ftrace_graph hash was not being protected properly
    
     - Various other small fixes and clean ups
    
    * tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (88 commits)
      bootconfig: Show the number of nodes on boot message
      tools/bootconfig: Show the number of bootconfig nodes
      bootconfig: Add more parse error messages
      bootconfig: Use bootconfig instead of boot config
      ftrace: Protect ftrace_graph_hash with ftrace_sync
      ftrace: Add comment to why rcu_dereference_sched() is open coded
      tracing: Annotate ftrace_graph_notrace_hash pointer with __rcu
      tracing: Annotate ftrace_graph_hash pointer with __rcu
      bootconfig: Only load bootconfig if "bootconfig" is on the kernel cmdline
      tracing: Use seq_buf for building dynevent_cmd string
      tracing: Remove useless code in dynevent_arg_pair_add()
      tracing: Remove check_arg() callbacks from dynevent args
      tracing: Consolidate some synth_event_trace code
      tracing: Fix now invalid var_ref_vals assumption in trace action
      tracing: Change trace_boot to use synth_event interface
      tracing: Move tracing selftests to bottom of menu
      tracing: Move mmio tracer config up with the other tracers
      tracing: Move tracing test module configs together
      tracing: Move all function tracing configs together
      tracing: Documentation for in-kernel synthetic event API
      ...

commit 2b90927c77c973771cc658d639724d5b247a83eb
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Fri Jan 31 15:55:34 2020 -0600

    tracing: Use seq_buf for building dynevent_cmd string
    
    The dynevent_cmd commands that build up the command string don't need
    to do that themselves - there's a seq_buf facility that does pretty
    much the same thing those command are doing manually, so use it
    instead.
    
    Link: http://lkml.kernel.org/r/eb8a6e835c964d0ab8a38cbf5ffa60746b54a465.1580506712.git.zanussi@kernel.org
    
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 7c307a7c9c6a..67f528ecb9e5 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -367,10 +367,8 @@ struct dynevent_cmd;
 typedef int (*dynevent_create_fn_t)(struct dynevent_cmd *cmd);
 
 struct dynevent_cmd {
-	char			*buf;
+	struct seq_buf		seq;
 	const char		*event_name;
-	int			maxlen;
-	int			remaining;
 	unsigned int		n_fields;
 	enum dynevent_type	type;
 	dynevent_create_fn_t	run_command;

commit 2a588dd1d5d649a183a2ff6fa1b80e870cf821d8
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:29 2020 -0600

    tracing: Add kprobe event command generation functions
    
    Add functions used to generate kprobe event commands, built on top of
    the dynevent_cmd interface.
    
    kprobe_event_gen_cmd_start() is used to create a kprobe event command
    using a variable arg list, and kretprobe_event_gen_cmd_start() does
    the same for kretprobe event commands.  kprobe_event_add_fields() can
    be used to add single fields one by one or as a group.  Once all
    desired fields are added, kprobe_event_gen_cmd_end() or
    kretprobe_event_gen_cmd_end() respectively are used to actually
    execute the command and create the event.
    
    Link: http://lkml.kernel.org/r/95cc4696502bb6017f9126f306a45ad19b4cc14f.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index bf03d12efb28..7c307a7c9c6a 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -358,6 +358,7 @@ extern void trace_put_event_file(struct trace_event_file *file);
 
 enum dynevent_type {
 	DYNEVENT_TYPE_SYNTH = 1,
+	DYNEVENT_TYPE_KPROBE,
 	DYNEVENT_TYPE_NONE,
 };
 
@@ -442,6 +443,36 @@ extern int synth_event_add_val(const char *field_name, u64 val,
 			       struct synth_event_trace_state *trace_state);
 extern int synth_event_trace_end(struct synth_event_trace_state *trace_state);
 
+extern int kprobe_event_delete(const char *name);
+
+extern void kprobe_event_cmd_init(struct dynevent_cmd *cmd,
+				  char *buf, int maxlen);
+
+#define kprobe_event_gen_cmd_start(cmd, name, loc, ...)			\
+	__kprobe_event_gen_cmd_start(cmd, false, name, loc, ## __VA_ARGS__, NULL)
+
+#define kretprobe_event_gen_cmd_start(cmd, name, loc, ...)		\
+	__kprobe_event_gen_cmd_start(cmd, true, name, loc, ## __VA_ARGS__, NULL)
+
+extern int __kprobe_event_gen_cmd_start(struct dynevent_cmd *cmd,
+					bool kretprobe,
+					const char *name,
+					const char *loc, ...);
+
+#define kprobe_event_add_fields(cmd, ...)	\
+	__kprobe_event_add_fields(cmd, ## __VA_ARGS__, NULL)
+
+#define kprobe_event_add_field(cmd, field)	\
+	__kprobe_event_add_fields(cmd, field, NULL)
+
+extern int __kprobe_event_add_fields(struct dynevent_cmd *cmd, ...);
+
+#define kprobe_event_gen_cmd_end(cmd)		\
+	dynevent_create(cmd)
+
+#define kretprobe_event_gen_cmd_end(cmd)	\
+	dynevent_create(cmd)
+
 /*
  * Event file flags:
  *  ENABLED	  - The event is enabled

commit 8dcc53ad956d2caf4c5c2dda196e6801b71a3154
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:27 2020 -0600

    tracing: Add synth_event_trace() and related functions
    
    Add an exported function named synth_event_trace(), allowing modules
    or other kernel code to trace synthetic events.
    
    Also added are several functions that allow the same functionality to
    be broken out in a piecewise fashion, which are useful in situations
    where tracing an event from a full array of values would be
    cumbersome.  Those functions are synth_event_trace_start/end() and
    synth_event_add_(next)_val().
    
    Link: http://lkml.kernel.org/r/7a84de5f1854acf4144b57efe835ca645afa764f.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 07b83532a3c6..bf03d12efb28 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -416,6 +416,32 @@ extern int synth_event_add_fields(struct dynevent_cmd *cmd,
 #define synth_event_gen_cmd_end(cmd)	\
 	dynevent_create(cmd)
 
+struct synth_event;
+
+struct synth_event_trace_state {
+	struct trace_event_buffer fbuffer;
+	struct synth_trace_event *entry;
+	struct trace_buffer *buffer;
+	struct synth_event *event;
+	unsigned int cur_field;
+	unsigned int n_u64;
+	bool enabled;
+	bool add_next;
+	bool add_name;
+};
+
+extern int synth_event_trace(struct trace_event_file *file,
+			     unsigned int n_vals, ...);
+extern int synth_event_trace_array(struct trace_event_file *file, u64 *vals,
+				   unsigned int n_vals);
+extern int synth_event_trace_start(struct trace_event_file *file,
+				   struct synth_event_trace_state *trace_state);
+extern int synth_event_add_next_val(u64 val,
+				    struct synth_event_trace_state *trace_state);
+extern int synth_event_add_val(const char *field_name, u64 val,
+			       struct synth_event_trace_state *trace_state);
+extern int synth_event_trace_end(struct synth_event_trace_state *trace_state);
+
 /*
  * Event file flags:
  *  ENABLED	  - The event is enabled

commit 35ca5207c2d111abb9e072f028945d5c12b20836
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:25 2020 -0600

    tracing: Add synthetic event command generation functions
    
    Add functions used to generate synthetic event commands, built on top
    of the dynevent_cmd interface.
    
    synth_event_gen_cmd_start() is used to create a synthetic event
    command using a variable arg list and
    synth_event_gen_cmd_array_start() does the same thing but using an
    array of field descriptors.  synth_event_add_field(),
    synth_event_add_field_str() and synth_event_add_fields() can be used
    to add single fields one by one or as a group.  Once all desired
    fields are added, synth_event_gen_cmd_end() is used to actually
    execute the command and create the event.
    
    synth_event_create() does everything, including creating the event, in
    a single call.
    
    Link: http://lkml.kernel.org/r/38fef702fad5ef208009f459552f34a94befd860.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 651b03d5e272..07b83532a3c6 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -357,6 +357,7 @@ extern void trace_put_event_file(struct trace_event_file *file);
 #define MAX_DYNEVENT_CMD_LEN	(2048)
 
 enum dynevent_type {
+	DYNEVENT_TYPE_SYNTH = 1,
 	DYNEVENT_TYPE_NONE,
 };
 
@@ -379,6 +380,42 @@ extern int dynevent_create(struct dynevent_cmd *cmd);
 
 extern int synth_event_delete(const char *name);
 
+extern void synth_event_cmd_init(struct dynevent_cmd *cmd,
+				 char *buf, int maxlen);
+
+extern int __synth_event_gen_cmd_start(struct dynevent_cmd *cmd,
+				       const char *name,
+				       struct module *mod, ...);
+
+#define synth_event_gen_cmd_start(cmd, name, mod, ...)	\
+	__synth_event_gen_cmd_start(cmd, name, mod, ## __VA_ARGS__, NULL)
+
+struct synth_field_desc {
+	const char *type;
+	const char *name;
+};
+
+extern int synth_event_gen_cmd_array_start(struct dynevent_cmd *cmd,
+					   const char *name,
+					   struct module *mod,
+					   struct synth_field_desc *fields,
+					   unsigned int n_fields);
+extern int synth_event_create(const char *name,
+			      struct synth_field_desc *fields,
+			      unsigned int n_fields, struct module *mod);
+
+extern int synth_event_add_field(struct dynevent_cmd *cmd,
+				 const char *type,
+				 const char *name);
+extern int synth_event_add_field_str(struct dynevent_cmd *cmd,
+				     const char *type_name);
+extern int synth_event_add_fields(struct dynevent_cmd *cmd,
+				  struct synth_field_desc *fields,
+				  unsigned int n_fields);
+
+#define synth_event_gen_cmd_end(cmd)	\
+	dynevent_create(cmd)
+
 /*
  * Event file flags:
  *  ENABLED	  - The event is enabled

commit 86c5426baddae9ff192e3159b9c2e7c14e3964c6
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:24 2020 -0600

    tracing: Add dynamic event command creation interface
    
    Add an interface used to build up dynamic event creation commands,
    such as synthetic and kprobe events.  Interfaces specific to those
    particular types of events and others can be built on top of this
    interface.
    
    Command creation is started by first using the dynevent_cmd_init()
    function to initialize the dynevent_cmd object.  Following that, args
    are appended and optionally checked by the dynevent_arg_add() and
    dynevent_arg_pair_add() functions, which use objects representing
    arguments and pairs of arguments, initialized respectively by
    dynevent_arg_init() and dynevent_arg_pair_init().  Finally, once all
    args have been successfully added, the command is finalized and
    actually created using dynevent_create().
    
    The code here for actually printing into the dyn_event->cmd buffer
    using snprintf() etc was adapted from v4 of Masami's 'tracing/boot:
    Add synthetic event support' patch.
    
    Link: http://lkml.kernel.org/r/1f65fa44390b6f238f6036777c3784ced1dcc6a0.1580323897.git.zanussi@kernel.org
    
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 25fe743bcbaf..651b03d5e272 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -354,6 +354,29 @@ extern struct trace_event_file *trace_get_event_file(const char *instance,
 						     const char *event);
 extern void trace_put_event_file(struct trace_event_file *file);
 
+#define MAX_DYNEVENT_CMD_LEN	(2048)
+
+enum dynevent_type {
+	DYNEVENT_TYPE_NONE,
+};
+
+struct dynevent_cmd;
+
+typedef int (*dynevent_create_fn_t)(struct dynevent_cmd *cmd);
+
+struct dynevent_cmd {
+	char			*buf;
+	const char		*event_name;
+	int			maxlen;
+	int			remaining;
+	unsigned int		n_fields;
+	enum dynevent_type	type;
+	dynevent_create_fn_t	run_command;
+	void			*private_data;
+};
+
+extern int dynevent_create(struct dynevent_cmd *cmd);
+
 extern int synth_event_delete(const char *name);
 
 /*

commit f5f6b255a253e2c3132ca283e9090a6343bfb719
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:23 2020 -0600

    tracing: Add synth_event_delete()
    
    create_or_delete_synth_event() contains code to delete a synthetic
    event, which would be useful on its own - specifically, it would be
    useful to allow event-creating modules to call it separately.
    
    Separate out the delete code from that function and create an exported
    function named synth_event_delete().
    
    Link: http://lkml.kernel.org/r/050db3b06df7f0a4b8a2922da602d1d879c7c1c2.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 8d621a73c97e..25fe743bcbaf 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -354,6 +354,8 @@ extern struct trace_event_file *trace_get_event_file(const char *instance,
 						     const char *event);
 extern void trace_put_event_file(struct trace_event_file *file);
 
+extern int synth_event_delete(const char *name);
+
 /*
  * Event file flags:
  *  ENABLED	  - The event is enabled

commit e3e2a2cc9c96725457ad6f31712ea7681a55666e
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:22 2020 -0600

    tracing: Add trace_get/put_event_file()
    
    Add a function to get an event file and prevent it from going away on
    module or instance removal.
    
    trace_get_event_file() will find an event file in a given instance (if
    instance is NULL, it assumes the top trace array) and return it,
    pinning the instance's trace array as well as the event's module, if
    applicable, so they won't go away while in use.
    
    trace_put_event_file() does the matching release.
    
    Link: http://lkml.kernel.org/r/bb31ac4bdda168d5ed3c4b5f5a4c8f633e8d9118.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    [ Moved trace_array_put() to end of trace_put_event_file() ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 20948ee56f8c..8d621a73c97e 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -349,6 +349,11 @@ enum {
 	EVENT_FILE_FL_WAS_ENABLED_BIT,
 };
 
+extern struct trace_event_file *trace_get_event_file(const char *instance,
+						     const char *system,
+						     const char *event);
+extern void trace_put_event_file(struct trace_event_file *file);
+
 /*
  * Event file flags:
  *  ENABLED	  - The event is enabled

commit 8cfcf15503f607e9597de19afeaa621897ae397e
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sat Jan 11 01:05:31 2020 +0900

    tracing: kprobes: Output kprobe event to printk buffer
    
    Since kprobe-events use event_trigger_unlock_commit_regs() directly,
    that events doesn't show up in printk buffer if "tp_printk" is set.
    
    Use trace_event_buffer_commit() in kprobe events so that it can
    invoke output_printk() as same as other trace events.
    
    Link: http://lkml.kernel.org/r/157867233085.17873.5210928676787339604.stgit@devnote2
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    [ Adjusted data var declaration placement in __kretprobe_trace_func() ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 5f7b2b1fce24..20948ee56f8c 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -216,6 +216,7 @@ struct trace_event_buffer {
 	void				*entry;
 	unsigned long			flags;
 	int				pc;
+	struct pt_regs			*regs;
 };
 
 void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,

commit 13292494379f92f532de71b31a54018336adc589
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:58:57 2019 -0500

    tracing: Make struct ring_buffer less ambiguous
    
    As there's two struct ring_buffers in the kernel, it causes some confusion.
    The other one being the perf ring buffer. It was agreed upon that as neither
    of the ring buffers are generic enough to be used globally, they should be
    renamed as:
    
       perf's ring_buffer -> perf_buffer
       ftrace's ring_buffer -> trace_buffer
    
    This implements the changes to the ring buffer that ftrace uses.
    
    Link: https://lore.kernel.org/r/20191213140531.116b3200@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index f70e5bc7e8db..5f7b2b1fce24 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -153,7 +153,7 @@ void tracing_generic_entry_update(struct trace_entry *entry,
 struct trace_event_file;
 
 struct ring_buffer_event *
-trace_event_buffer_lock_reserve(struct ring_buffer **current_buffer,
+trace_event_buffer_lock_reserve(struct trace_buffer **current_buffer,
 				struct trace_event_file *trace_file,
 				int type, unsigned long len,
 				unsigned long flags, int pc);
@@ -210,7 +210,7 @@ extern int trace_event_reg(struct trace_event_call *event,
 			    enum trace_reg type, void *data);
 
 struct trace_event_buffer {
-	struct ring_buffer		*buffer;
+	struct trace_buffer		*buffer;
 	struct ring_buffer_event	*event;
 	struct trace_event_file		*trace_file;
 	void				*entry;

commit 1c5eb4481e0151d579f738175497f998840f7bbc
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 9 18:53:48 2020 -0500

    tracing: Rename trace_buffer to array_buffer
    
    As we are working to remove the generic "ring_buffer" name that is used by
    both tracing and perf, the ring_buffer name for tracing will be renamed to
    trace_buffer, and perf's ring buffer will be renamed to perf_buffer.
    
    As there already exists a trace_buffer that is used by the trace_arrays, it
    needs to be first renamed to array_buffer.
    
    Link: https://lore.kernel.org/r/20191213153553.GE20583@krava
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 4c6e15605766..f70e5bc7e8db 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -11,7 +11,7 @@
 #include <linux/tracepoint.h>
 
 struct trace_array;
-struct trace_buffer;
+struct array_buffer;
 struct tracer;
 struct dentry;
 struct bpf_prog;
@@ -79,7 +79,7 @@ struct trace_entry {
 struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
-	struct trace_buffer	*trace_buffer;
+	struct array_buffer	*array_buffer;
 	void			*private;
 	int			cpu_file;
 	struct mutex		mutex;

commit 2040cf9f59037aa8aec749363e69ead165b67b43
Merge: f66c0447cca1 e42617b825f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 10 10:11:00 2019 +0100

    Merge tag 'v5.5-rc1' into core/kprobes, to resolve conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 04ae87a52074e2d448fc66143f1bd2c7d694d2b9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 24 22:26:59 2019 +0200

    ftrace: Rework event_create_dir()
    
    Rework event_create_dir() to use an array of static data instead of
    function pointers where possible.
    
    The problem is that it would call the function pointer on module load
    before parse_args(), possibly even before jump_labels were initialized.
    Luckily the generated functions don't use jump_labels but it still seems
    fragile. It also gets in the way of changing when we make the module map
    executable.
    
    The generated function are basically calling trace_define_field() with a
    bunch of static arguments. So instead of a function, capture these
    arguments in a static array, avoiding the function call.
    
    Now there are a number of cases where the fields are dynamic (syscall
    arguments, kprobes and uprobes), in which case a static array does not
    work, for these we preserve the function call. Luckily all these cases
    are not related to modules and so we can retain the function call for
    them.
    
    Also fix up all broken tracepoint definitions that now generate a
    compile error.
    
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191111132458.342979914@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 30a8cdcfd4a4..a379255c14a9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -187,6 +187,22 @@ enum trace_reg {
 
 struct trace_event_call;
 
+#define TRACE_FUNCTION_TYPE ((const char *)~0UL)
+
+struct trace_event_fields {
+	const char *type;
+	union {
+		struct {
+			const char *name;
+			const int  size;
+			const int  align;
+			const int  is_signed;
+			const int  filter_type;
+		};
+		int (*define_fields)(struct trace_event_call *);
+	};
+};
+
 struct trace_event_class {
 	const char		*system;
 	void			*probe;
@@ -195,7 +211,7 @@ struct trace_event_class {
 #endif
 	int			(*reg)(struct trace_event_call *event,
 				       enum trace_reg type, void *data);
-	int			(*define_fields)(struct trace_event_call *);
+	struct trace_event_fields *fields_array;
 	struct list_head	*(*get_fields)(struct trace_event_call *);
 	struct list_head	fields;
 	int			(*raw_init)(struct trace_event_call *);

commit 28879787147358e8ffcae397f11748de3dd26577
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Nov 20 11:08:38 2019 -0800

    tracing: Adding new functions for kernel access to Ftrace instances
    
    Adding 2 new functions -
    1) struct trace_array *trace_array_get_by_name(const char *name);
    
    Return pointer to a trace array with given name. If it does not exist,
    create and return pointer to the new trace array.
    
    2) int trace_array_set_clr_event(struct trace_array *tr,
    const char *system ,const char *event, bool enable);
    
    Enable/Disable events to this trace array.
    
    Additionally,
    - To handle reference counters, export trace_array_put()
    - Due to introduction of the above 2 new functions, we no longer need to
      export - ftrace_set_clr_event & trace_array_create APIs.
    
    Link: http://lkml.kernel.org/r/1574276919-11119-2-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Reviewed-by: Aruna Ramakrishna <aruna.ramakrishna@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 60a41b7069dd..4c6e15605766 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -555,7 +555,8 @@ extern int trace_event_get_offsets(struct trace_event_call *call);
 
 int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set);
 int trace_set_clr_event(const char *system, const char *event, int set);
-
+int trace_array_set_clr_event(struct trace_array *tr, const char *system,
+		const char *event, bool enable);
 /*
  * The double __builtin_constant_p is because gcc will give us an error
  * if we try to allocate the static variable to fmt if it is not a

commit ef56e047b2bd4dabb801fd073dfcab5f40de5f78
Author: Piotr Maziarz <piotrx.maziarz@linux.intel.com>
Date:   Thu Nov 7 13:45:38 2019 +0100

    tracing: Use seq_buf_hex_dump() to dump buffers
    
    Without this, buffers can be printed with __print_array macro that has
    no formatting options and can be hard to read. The other way is to
    mimic formatting capability with multiple calls of trace event with one
    call per row which gives performance impact and different timestamp in
    each row.
    
    Link: http://lkml.kernel.org/r/1573130738-29390-2-git-send-email-piotrx.maziarz@linux.intel.com
    
    Signed-off-by: Piotr Maziarz <piotrx.maziarz@linux.intel.com>
    Signed-off-by: Cezary Rojewski <cezary.rojewski@intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 30a8cdcfd4a4..60a41b7069dd 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -45,6 +45,11 @@ const char *trace_print_array_seq(struct trace_seq *p,
 				   const void *buf, int count,
 				   size_t el_size);
 
+const char *
+trace_print_hex_dump_seq(struct trace_seq *p, const char *prefix_str,
+			 int prefix_type, int rowsize, int groupsize,
+			 const void *buf, size_t len, bool ascii);
+
 struct trace_iterator;
 struct trace_event;
 

commit 595a438c78dbdc43d6c9db4f437267f0bd1548bf
Author: Denis Efremov <efremov@linux.com>
Date:   Thu Jul 4 20:21:10 2019 +0300

    tracing: Make exported ftrace_set_clr_event non-static
    
    The function ftrace_set_clr_event is declared static and marked
    EXPORT_SYMBOL_GPL(), which is at best an odd combination. Because the
    function was decided to be a part of API, this commit removes the static
    attribute and adds the declaration to the header.
    
    Link: http://lkml.kernel.org/r/20190704172110.27041-1-efremov@linux.com
    
    Fixes: f45d1225adb04 ("tracing: Kernel access to Ftrace instances")
    Reviewed-by: Joe Jin <joe.jin@oracle.com>
    Signed-off-by: Denis Efremov <efremov@linux.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 5150436783e8..30a8cdcfd4a4 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -548,6 +548,7 @@ extern int trace_event_get_offsets(struct trace_event_call *call);
 
 #define is_signed_type(type)	(((type)(-1)) < (type)1)
 
+int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set);
 int trace_set_clr_event(const char *system, const char *event, int set);
 
 /*

commit 0aeb1def44169cbe7119f26cf10b974a2046142e
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Sat May 25 09:58:01 2019 -0700

    tracing: Make trace_get_fields() global
    
    trace_get_fields() is the only way to read tracepoint fields at
    run time, as their fields are defined at compile-time with macros.
    Make this function visible to all users and it will be used by
    trace event injection code to calculate the size of a tracepoint
    entry.
    Link: http://lkml.kernel.org/r/20190525165802.25944-4-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 5c6f2a6c8cd2..5150436783e8 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -318,6 +318,14 @@ trace_event_name(struct trace_event_call *call)
 		return call->name;
 }
 
+static inline struct list_head *
+trace_get_fields(struct trace_event_call *event_call)
+{
+	if (!event_call->class->get_fields)
+		return &event_call->class->fields;
+	return event_call->class->get_fields(event_call);
+}
+
 struct trace_array;
 struct trace_subsystem_dir;
 

commit 46710f3a34b592ac5c51a95f696b2d2a2a0d9419
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Sat May 25 09:57:59 2019 -0700

    tracing: Pass type into tracing_generic_entry_update()
    
    All callers of tracing_generic_entry_update() have to initialize
    entry->type, so let's just simply move it inside.
    Link: http://lkml.kernel.org/r/20190525165802.25944-2-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 8a62731673f7..5c6f2a6c8cd2 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -142,6 +142,7 @@ enum print_line_t {
 enum print_line_t trace_handle_return(struct trace_seq *s);
 
 void tracing_generic_entry_update(struct trace_entry *entry,
+				  unsigned short type,
 				  unsigned long flags,
 				  int pc);
 struct trace_event_file;

commit a38d1107f937ca95dcf820161ef44ea683d6a0b1
Author: Matt Mullins <mmullins@fb.com>
Date:   Wed Dec 12 16:42:37 2018 -0800

    bpf: support raw tracepoints in modules
    
    Distributions build drivers as modules, including network and filesystem
    drivers which export numerous tracepoints.  This enables
    bpf(BPF_RAW_TRACEPOINT_OPEN) to attach to those tracepoints.
    
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 4130a5497d40..8a62731673f7 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -471,7 +471,8 @@ void perf_event_detach_bpf_prog(struct perf_event *event);
 int perf_event_query_prog_array(struct perf_event *event, void __user *info);
 int bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
 int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
-struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name);
+struct bpf_raw_event_map *bpf_get_raw_tracepoint(const char *name);
+void bpf_put_raw_tracepoint(struct bpf_raw_event_map *btp);
 int bpf_get_perf_event_info(const struct perf_event *event, u32 *prog_id,
 			    u32 *fd_type, const char **buf,
 			    u64 *probe_offset, u64 *probe_addr);
@@ -502,10 +503,13 @@ static inline int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf
 {
 	return -EOPNOTSUPP;
 }
-static inline struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name)
+static inline struct bpf_raw_event_map *bpf_get_raw_tracepoint(const char *name)
 {
 	return NULL;
 }
+static inline void bpf_put_raw_tracepoint(struct bpf_raw_event_map *btp)
+{
+}
 static inline int bpf_get_perf_event_info(const struct perf_event *event,
 					  u32 *prog_id, u32 *fd_type,
 					  const char **buf, u64 *probe_offset,

commit a6ca88b241d5e929e6e60b12ad8cd288f0ffa256
Author: Song Liu <songliubraving@fb.com>
Date:   Mon Oct 1 22:36:36 2018 -0700

    trace_uprobe: support reference counter in fd-based uprobe
    
    This patch enables uprobes with reference counter in fd-based uprobe.
    Highest 32 bits of perf_event_attr.config is used to stored offset
    of the reference count (semaphore).
    
    Format information in /sys/bus/event_source/devices/uprobe/format/ is
    updated to reflect this new feature.
    
    Link: http://lkml.kernel.org/r/20181002053636.1896903-1-songliubraving@fb.com
    
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-and-tested-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 78a010e19ed4..4130a5497d40 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -575,7 +575,8 @@ extern int bpf_get_kprobe_info(const struct perf_event *event,
 			       bool perf_type_tracepoint);
 #endif
 #ifdef CONFIG_UPROBE_EVENTS
-extern int  perf_uprobe_init(struct perf_event *event, bool is_retprobe);
+extern int  perf_uprobe_init(struct perf_event *event,
+			     unsigned long ref_ctr_offset, bool is_retprobe);
 extern void perf_uprobe_destroy(struct perf_event *event);
 extern int bpf_get_uprobe_info(const struct perf_event *event,
 			       u32 *fd_type, const char **filename,

commit 1c8c5a9d38f607c0b6fd12c91cbe1a4418762a21
Merge: 285767604576 7170e6045a6a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 6 18:39:49 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add Maglev hashing scheduler to IPVS, from Inju Song.
    
     2) Lots of new TC subsystem tests from Roman Mashak.
    
     3) Add TCP zero copy receive and fix delayed acks and autotuning with
        SO_RCVLOWAT, from Eric Dumazet.
    
     4) Add XDP_REDIRECT support to mlx5 driver, from Jesper Dangaard
        Brouer.
    
     5) Add ttl inherit support to vxlan, from Hangbin Liu.
    
     6) Properly separate ipv6 routes into their logically independant
        components. fib6_info for the routing table, and fib6_nh for sets of
        nexthops, which thus can be shared. From David Ahern.
    
     7) Add bpf_xdp_adjust_tail helper, which can be used to generate ICMP
        messages from XDP programs. From Nikita V. Shirokov.
    
     8) Lots of long overdue cleanups to the r8169 driver, from Heiner
        Kallweit.
    
     9) Add BTF ("BPF Type Format"), from Martin KaFai Lau.
    
    10) Add traffic condition monitoring to iwlwifi, from Luca Coelho.
    
    11) Plumb extack down into fib_rules, from Roopa Prabhu.
    
    12) Add Flower classifier offload support to igb, from Vinicius Costa
        Gomes.
    
    13) Add UDP GSO support, from Willem de Bruijn.
    
    14) Add documentation for eBPF helpers, from Quentin Monnet.
    
    15) Add TLS tx offload to mlx5, from Ilya Lesokhin.
    
    16) Allow applications to be given the number of bytes available to read
        on a socket via a control message returned from recvmsg(), from
        Soheil Hassas Yeganeh.
    
    17) Add x86_32 eBPF JIT compiler, from Wang YanQing.
    
    18) Add AF_XDP sockets, with zerocopy support infrastructure as well.
        From Björn Töpel.
    
    19) Remove indirect load support from all of the BPF JITs and handle
        these operations in the verifier by translating them into native BPF
        instead. From Daniel Borkmann.
    
    20) Add GRO support to ipv6 gre tunnels, from Eran Ben Elisha.
    
    21) Allow XDP programs to do lookups in the main kernel routing tables
        for forwarding. From David Ahern.
    
    22) Allow drivers to store hardware state into an ELF section of kernel
        dump vmcore files, and use it in cxgb4. From Rahul Lakkireddy.
    
    23) Various RACK and loss detection improvements in TCP, from Yuchung
        Cheng.
    
    24) Add TCP SACK compression, from Eric Dumazet.
    
    25) Add User Mode Helper support and basic bpfilter infrastructure, from
        Alexei Starovoitov.
    
    26) Support ports and protocol values in RTM_GETROUTE, from Roopa
        Prabhu.
    
    27) Support bulking in ->ndo_xdp_xmit() API, from Jesper Dangaard
        Brouer.
    
    28) Add lots of forwarding selftests, from Petr Machata.
    
    29) Add generic network device failover driver, from Sridhar Samudrala.
    
    * ra.kernel.org:/pub/scm/linux/kernel/git/davem/net-next: (1959 commits)
      strparser: Add __strp_unpause and use it in ktls.
      rxrpc: Fix terminal retransmission connection ID to include the channel
      net: hns3: Optimize PF CMDQ interrupt switching process
      net: hns3: Fix for VF mailbox receiving unknown message
      net: hns3: Fix for VF mailbox cannot receiving PF response
      bnx2x: use the right constant
      Revert "net: sched: cls: Fix offloading when ingress dev is vxlan"
      net: dsa: b53: Fix for brcm tag issue in Cygnus SoC
      enic: fix UDP rss bits
      netdev-FAQ: clarify DaveM's position for stable backports
      rtnetlink: validate attributes in do_setlink()
      mlxsw: Add extack messages for port_{un, }split failures
      netdevsim: Add extack error message for devlink reload
      devlink: Add extack to reload and port_{un, }split operations
      net: metrics: add proper netlink validation
      ipmr: fix error path when ipmr_new_table fails
      ip6mr: only set ip6mr_table from setsockopt when ip6mr_new_table succeeds
      net: hns3: remove unused hclgevf_cfg_func_mta_filter
      netfilter: provide udp*_lib_lookup for nf_tproxy
      qed*: Utilize FW 8.37.2.0
      ...

commit c94e45bc38ae484ee989e6e3b2496a52776da9e4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 7 16:02:14 2018 -0400

    tracing: Do not reference event data in post call triggers
    
    Trace event triggers can be called before or after the event has been
    committed. If it has been called after the commit, there's a possibility
    that the event no longer exists. Currently, the two post callers is the
    trigger to disable tracing (traceoff) and the one that will record a stack
    dump (stacktrace). Neither of them reference the trace event entry record,
    as that would lead to a race condition that could pass in corrupted data.
    
    To prevent any other users of the post data triggers from using the trace
    event record, pass in NULL to the post call trigger functions for the event
    record, as they should never need to use them in the first place.
    
    This does not fix any bug, but prevents bugs from happening by new post call
    trigger users.
    
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 2bde3eff564c..d1c442d9bd85 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -435,8 +435,7 @@ event_triggers_call(struct trace_event_file *file, void *rec,
 		    struct ring_buffer_event *event);
 extern void
 event_triggers_post_call(struct trace_event_file *file,
-			 enum event_trigger_type tt,
-			 void *rec, struct ring_buffer_event *event);
+			 enum event_trigger_type tt);
 
 bool trace_event_ignore_this_pid(struct trace_event_file *trace_file);
 

commit 41bdc4b40ed6fb26c6acc655ed9a243a348709c9
Author: Yonghong Song <yhs@fb.com>
Date:   Thu May 24 11:21:09 2018 -0700

    bpf: introduce bpf subcommand BPF_TASK_FD_QUERY
    
    Currently, suppose a userspace application has loaded a bpf program
    and attached it to a tracepoint/kprobe/uprobe, and a bpf
    introspection tool, e.g., bpftool, wants to show which bpf program
    is attached to which tracepoint/kprobe/uprobe. Such attachment
    information will be really useful to understand the overall bpf
    deployment in the system.
    
    There is a name field (16 bytes) for each program, which could
    be used to encode the attachment point. There are some drawbacks
    for this approaches. First, bpftool user (e.g., an admin) may not
    really understand the association between the name and the
    attachment point. Second, if one program is attached to multiple
    places, encoding a proper name which can imply all these
    attachments becomes difficult.
    
    This patch introduces a new bpf subcommand BPF_TASK_FD_QUERY.
    Given a pid and fd, if the <pid, fd> is associated with a
    tracepoint/kprobe/uprobe perf event, BPF_TASK_FD_QUERY will return
       . prog_id
       . tracepoint name, or
       . k[ret]probe funcname + offset or kernel addr, or
       . u[ret]probe filename + offset
    to the userspace.
    The user can use "bpftool prog" to find more information about
    bpf program itself with prog_id.
    
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 2bde3eff564c..d34144a3c5b5 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -473,6 +473,9 @@ int perf_event_query_prog_array(struct perf_event *event, void __user *info);
 int bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
 int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
 struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name);
+int bpf_get_perf_event_info(const struct perf_event *event, u32 *prog_id,
+			    u32 *fd_type, const char **buf,
+			    u64 *probe_offset, u64 *probe_addr);
 #else
 static inline unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)
 {
@@ -504,6 +507,13 @@ static inline struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name
 {
 	return NULL;
 }
+static inline int bpf_get_perf_event_info(const struct perf_event *event,
+					  u32 *prog_id, u32 *fd_type,
+					  const char **buf, u64 *probe_offset,
+					  u64 *probe_addr)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 enum {
@@ -560,10 +570,17 @@ extern void perf_trace_del(struct perf_event *event, int flags);
 #ifdef CONFIG_KPROBE_EVENTS
 extern int  perf_kprobe_init(struct perf_event *event, bool is_retprobe);
 extern void perf_kprobe_destroy(struct perf_event *event);
+extern int bpf_get_kprobe_info(const struct perf_event *event,
+			       u32 *fd_type, const char **symbol,
+			       u64 *probe_offset, u64 *probe_addr,
+			       bool perf_type_tracepoint);
 #endif
 #ifdef CONFIG_UPROBE_EVENTS
 extern int  perf_uprobe_init(struct perf_event *event, bool is_retprobe);
 extern void perf_uprobe_destroy(struct perf_event *event);
+extern int bpf_get_uprobe_info(const struct perf_event *event,
+			       u32 *fd_type, const char **filename,
+			       u64 *probe_offset, bool perf_type_tracepoint);
 #endif
 extern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,
 				     char *filter_str);

commit 2a56bb596b2c1fb612f9988afda9655c8c872a6e
Merge: 9f3a0941fb5e b0dc52f15e7f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 10 11:27:30 2018 -0700

    Merge tag 'trace-v4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "New features:
    
       - Tom Zanussi's extended histogram work.
    
         This adds the synthetic events to have histograms from multiple
         event data Adds triggers "onmatch" and "onmax" to call the
         synthetic events Several updates to the histogram code from this
    
       - Allow way to nest ring buffer calls in the same context
    
       - Allow absolute time stamps in ring buffer
    
       - Rewrite of filter code parsing based on Al Viro's suggestions
    
       - Setting of trace_clock to global if TSC is unstable (on boot)
    
       - Better OOM handling when allocating large ring buffers
    
       - Added initcall tracepoints (consolidated initcall_debug code with
         them)
    
      And other various fixes and clean ups"
    
    * tag 'trace-v4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (68 commits)
      init: Have initcall_debug still work without CONFIG_TRACEPOINTS
      init, tracing: Have printk come through the trace events for initcall_debug
      init, tracing: instrument security and console initcall trace events
      init, tracing: Add initcall trace events
      tracing: Add rcu dereference annotation for test func that touches filter->prog
      tracing: Add rcu dereference annotation for filter->prog
      tracing: Fixup logic inversion on setting trace_global_clock defaults
      tracing: Hide global trace clock from lockdep
      ring-buffer: Add set/clear_current_oom_origin() during allocations
      ring-buffer: Check if memory is available before allocation
      lockdep: Add print_irqtrace_events() to __warn
      vsprintf: Do not preprocess non-dereferenced pointers for bprintf (%px and %pK)
      tracing: Uninitialized variable in create_tracing_map_fields()
      tracing: Make sure variable string fields are NULL-terminated
      tracing: Add action comparisons when testing matching hist triggers
      tracing: Don't add flag strings when displaying variable references
      tracing: Fix display of hist trigger expressions containing timestamps
      ftrace: Drop a VLA in module_exists()
      tracing: Mention trace_clock=global when warning about unstable clocks
      tracing: Default to using trace_global_clock if sched_clock is unstable
      ...

commit 5bb053bef82523a8fd78d650bca81c9f114fa276
Merge: bb2407a72197 159f02977b2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 3 14:04:18 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Support offloading wireless authentication to userspace via
        NL80211_CMD_EXTERNAL_AUTH, from Srinivas Dasari.
    
     2) A lot of work on network namespace setup/teardown from Kirill Tkhai.
        Setup and cleanup of namespaces now all run asynchronously and thus
        performance is significantly increased.
    
     3) Add rx/tx timestamping support to mv88e6xxx driver, from Brandon
        Streiff.
    
     4) Support zerocopy on RDS sockets, from Sowmini Varadhan.
    
     5) Use denser instruction encoding in x86 eBPF JIT, from Daniel
        Borkmann.
    
     6) Support hw offload of vlan filtering in mvpp2 dreiver, from Maxime
        Chevallier.
    
     7) Support grafting of child qdiscs in mlxsw driver, from Nogah
        Frankel.
    
     8) Add packet forwarding tests to selftests, from Ido Schimmel.
    
     9) Deal with sub-optimal GSO packets better in BBR congestion control,
        from Eric Dumazet.
    
    10) Support 5-tuple hashing in ipv6 multipath routing, from David Ahern.
    
    11) Add path MTU tests to selftests, from Stefano Brivio.
    
    12) Various bits of IPSEC offloading support for mlx5, from Aviad
        Yehezkel, Yossi Kuperman, and Saeed Mahameed.
    
    13) Support RSS spreading on ntuple filters in SFC driver, from Edward
        Cree.
    
    14) Lots of sockmap work from John Fastabend. Applications can use eBPF
        to filter sendmsg and sendpage operations.
    
    15) In-kernel receive TLS support, from Dave Watson.
    
    16) Add XDP support to ixgbevf, this is significant because it should
        allow optimized XDP usage in various cloud environments. From Tony
        Nguyen.
    
    17) Add new Intel E800 series "ice" ethernet driver, from Anirudh
        Venkataramanan et al.
    
    18) IP fragmentation match offload support in nfp driver, from Pieter
        Jansen van Vuuren.
    
    19) Support XDP redirect in i40e driver, from Björn Töpel.
    
    20) Add BPF_RAW_TRACEPOINT program type for accessing the arguments of
        tracepoints in their raw form, from Alexei Starovoitov.
    
    21) Lots of striding RQ improvements to mlx5 driver with many
        performance improvements, from Tariq Toukan.
    
    22) Use rhashtable for inet frag reassembly, from Eric Dumazet.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1678 commits)
      net: mvneta: improve suspend/resume
      net: mvneta: split rxq/txq init and txq deinit into SW and HW parts
      ipv6: frags: fix /proc/sys/net/ipv6/ip6frag_low_thresh
      net: bgmac: Fix endian access in bgmac_dma_tx_ring_free()
      net: bgmac: Correctly annotate register space
      route: check sysctl_fib_multipath_use_neigh earlier than hash
      fix typo in command value in drivers/net/phy/mdio-bitbang.
      sky2: Increase D3 delay to sky2 stops working after suspend
      net/mlx5e: Set EQE based as default TX interrupt moderation mode
      ibmvnic: Disable irqs before exiting reset from closed state
      net: sched: do not emit messages while holding spinlock
      vlan: also check phy_driver ts_info for vlan's real device
      Bluetooth: Mark expected switch fall-throughs
      Bluetooth: Set HCI_QUIRK_SIMULTANEOUS_DISCOVERY for BTUSB_QCA_ROME
      Bluetooth: btrsi: remove unused including <linux/version.h>
      Bluetooth: hci_bcm: Remove DMI quirk for the MINIX Z83-4
      sh_eth: kill useless check in __sh_eth_get_regs()
      sh_eth: add sh_eth_cpu_data::no_xdfar flag
      ipv6: factorize sk_wmem_alloc updates done by __ip6_append_data()
      ipv4: factorize sk_wmem_alloc updates done by __ip_append_data()
      ...

commit c4f6699dfcb8558d138fe838f741b2c10f416cf9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Mar 28 12:05:37 2018 -0700

    bpf: introduce BPF_RAW_TRACEPOINT
    
    Introduce BPF_PROG_TYPE_RAW_TRACEPOINT bpf program type to access
    kernel internal arguments of the tracepoints in their raw form.
    
    >From bpf program point of view the access to the arguments look like:
    struct bpf_raw_tracepoint_args {
           __u64 args[0];
    };
    
    int bpf_prog(struct bpf_raw_tracepoint_args *ctx)
    {
      // program can read args[N] where N depends on tracepoint
      // and statically verified at program load+attach time
    }
    
    kprobe+bpf infrastructure allows programs access function arguments.
    This feature allows programs access raw tracepoint arguments.
    
    Similar to proposed 'dynamic ftrace events' there are no abi guarantees
    to what the tracepoints arguments are and what their meaning is.
    The program needs to type cast args properly and use bpf_probe_read()
    helper to access struct fields when argument is a pointer.
    
    For every tracepoint __bpf_trace_##call function is prepared.
    In assembler it looks like:
    (gdb) disassemble __bpf_trace_xdp_exception
    Dump of assembler code for function __bpf_trace_xdp_exception:
       0xffffffff81132080 <+0>:     mov    %ecx,%ecx
       0xffffffff81132082 <+2>:     jmpq   0xffffffff811231f0 <bpf_trace_run3>
    
    where
    
    TRACE_EVENT(xdp_exception,
            TP_PROTO(const struct net_device *dev,
                     const struct bpf_prog *xdp, u32 act),
    
    The above assembler snippet is casting 32-bit 'act' field into 'u64'
    to pass into bpf_trace_run3(), while 'dev' and 'xdp' args are passed as-is.
    All of ~500 of __bpf_trace_*() functions are only 5-10 byte long
    and in total this approach adds 7k bytes to .text.
    
    This approach gives the lowest possible overhead
    while calling trace_xdp_exception() from kernel C code and
    transitioning into bpf land.
    Since tracepoint+bpf are used at speeds of 1M+ events per second
    this is valuable optimization.
    
    The new BPF_RAW_TRACEPOINT_OPEN sys_bpf command is introduced
    that returns anon_inode FD of 'bpf-raw-tracepoint' object.
    
    The user space looks like:
    // load bpf prog with BPF_PROG_TYPE_RAW_TRACEPOINT type
    prog_fd = bpf_prog_load(...);
    // receive anon_inode fd for given bpf_raw_tracepoint with prog attached
    raw_tp_fd = bpf_raw_tracepoint_open("xdp_exception", prog_fd);
    
    Ctrl-C of tracing daemon or cmdline tool that uses this feature
    will automatically detach bpf program, unload it and
    unregister tracepoint probe.
    
    On the kernel side the __bpf_raw_tp_map section of pointers to
    tracepoint definition and to __bpf_trace_*() probe function is used
    to find a tracepoint with "xdp_exception" name and
    corresponding __bpf_trace_xdp_exception() probe function
    which are passed to tracepoint_probe_register() to connect probe
    with tracepoint.
    
    Addition of bpf_raw_tracepoint doesn't interfere with ftrace and perf
    tracepoint mechanisms. perf_event_open() can be used in parallel
    on the same tracepoint.
    Multiple bpf_raw_tracepoint_open("xdp_exception", prog_fd) are permitted.
    Each with its own bpf program. The kernel will execute
    all tracepoint probes and all attached bpf programs.
    
    In the future bpf_raw_tracepoints can be extended with
    query/introspection logic.
    
    __bpf_raw_tp_map section logic was contributed by Steven Rostedt
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 8a1442c4e513..b0357cd198b0 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -468,6 +468,9 @@ unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx);
 int perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog);
 void perf_event_detach_bpf_prog(struct perf_event *event);
 int perf_event_query_prog_array(struct perf_event *event, void __user *info);
+int bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
+int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *prog);
+struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name);
 #else
 static inline unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)
 {
@@ -487,6 +490,18 @@ perf_event_query_prog_array(struct perf_event *event, void __user *info)
 {
 	return -EOPNOTSUPP;
 }
+static inline int bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *p)
+{
+	return -EOPNOTSUPP;
+}
+static inline int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *p)
+{
+	return -EOPNOTSUPP;
+}
+static inline struct bpf_raw_event_map *bpf_find_raw_tracepoint(const char *name)
+{
+	return NULL;
+}
 #endif
 
 enum {
@@ -546,6 +561,33 @@ extern void ftrace_profile_free_filter(struct perf_event *event);
 void perf_trace_buf_update(void *record, u16 type);
 void *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp);
 
+void bpf_trace_run1(struct bpf_prog *prog, u64 arg1);
+void bpf_trace_run2(struct bpf_prog *prog, u64 arg1, u64 arg2);
+void bpf_trace_run3(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3);
+void bpf_trace_run4(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4);
+void bpf_trace_run5(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4, u64 arg5);
+void bpf_trace_run6(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4, u64 arg5, u64 arg6);
+void bpf_trace_run7(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7);
+void bpf_trace_run8(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,
+		    u64 arg8);
+void bpf_trace_run9(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,
+		    u64 arg8, u64 arg9);
+void bpf_trace_run10(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,
+		     u64 arg8, u64 arg9, u64 arg10);
+void bpf_trace_run11(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,
+		     u64 arg8, u64 arg9, u64 arg10, u64 arg11);
+void bpf_trace_run12(struct bpf_prog *prog, u64 arg1, u64 arg2,
+		     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,
+		     u64 arg8, u64 arg9, u64 arg10, u64 arg11, u64 arg12);
 void perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,
 			       struct trace_event_call *call, u64 count,
 			       struct pt_regs *regs, struct hlist_head *head,

commit 1ac4f51c0eb518e04ff3455f0c7d17ad9187eb27
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:51:42 2018 -0600

    tracing: Give event triggers access to ring_buffer_event
    
    The ring_buffer event can provide a timestamp that may be useful to
    various triggers - pass it into the handlers for that purpose.
    
    Link: http://lkml.kernel.org/r/6de592683b59fa70ffa5d43d0109896623fc1367.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 8a1442c4e513..0cf48c61cc6d 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -430,11 +430,13 @@ enum event_trigger_type {
 
 extern int filter_match_preds(struct event_filter *filter, void *rec);
 
-extern enum event_trigger_type event_triggers_call(struct trace_event_file *file,
-						   void *rec);
-extern void event_triggers_post_call(struct trace_event_file *file,
-				     enum event_trigger_type tt,
-				     void *rec);
+extern enum event_trigger_type
+event_triggers_call(struct trace_event_file *file, void *rec,
+		    struct ring_buffer_event *event);
+extern void
+event_triggers_post_call(struct trace_event_file *file,
+			 enum event_trigger_type tt,
+			 void *rec, struct ring_buffer_event *event);
 
 bool trace_event_ignore_this_pid(struct trace_event_file *trace_file);
 
@@ -454,7 +456,7 @@ trace_trigger_soft_disabled(struct trace_event_file *file)
 
 	if (!(eflags & EVENT_FILE_FL_TRIGGER_COND)) {
 		if (eflags & EVENT_FILE_FL_TRIGGER_MODE)
-			event_triggers_call(file, NULL);
+			event_triggers_call(file, NULL, NULL);
 		if (eflags & EVENT_FILE_FL_SOFT_DISABLED)
 			return true;
 		if (eflags & EVENT_FILE_FL_PID_FILTER)

commit 7057bb975dab827997e0ca9dd92cafef0856b0cc
Merge: 33ea4b24277b 297f9233b53a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 17 11:39:28 2018 +0100

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 33ea4b24277b06dbc55d7f5772a46f029600255e
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 6 14:45:16 2017 -0800

    perf/core: Implement the 'perf_uprobe' PMU
    
    This patch adds perf_uprobe support with similar pattern as previous
    patch (for kprobe).
    
    Two functions, create_local_trace_uprobe() and
    destroy_local_trace_uprobe(), are created so a uprobe can be created
    and attached to the file descriptor created by perf_event_open().
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Cc: <daniel@iogearbox.net>
    Cc: <davem@davemloft.net>
    Cc: <kernel-team@fb.com>
    Cc: <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171206224518.3598254-7-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 21c5d43a21af..0d9d6cb454b1 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -537,6 +537,10 @@ extern void perf_trace_del(struct perf_event *event, int flags);
 extern int  perf_kprobe_init(struct perf_event *event, bool is_retprobe);
 extern void perf_kprobe_destroy(struct perf_event *event);
 #endif
+#ifdef CONFIG_UPROBE_EVENTS
+extern int  perf_uprobe_init(struct perf_event *event, bool is_retprobe);
+extern void perf_uprobe_destroy(struct perf_event *event);
+#endif
 extern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,
 				     char *filter_str);
 extern void ftrace_profile_free_filter(struct perf_event *event);

commit e12f03d7031a977356e3d7b75a68c2185ff8d155
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 6 14:45:15 2017 -0800

    perf/core: Implement the 'perf_kprobe' PMU
    
    A new PMU type, perf_kprobe is added. Based on attr from perf_event_open(),
    perf_kprobe creates a kprobe (or kretprobe) for the perf_event. This
    kprobe is private to this perf_event, and thus not added to global
    lists, and not available in tracefs.
    
    Two functions, create_local_trace_kprobe() and
    destroy_local_trace_kprobe()  are added to created and destroy these
    local trace_kprobe.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Cc: <daniel@iogearbox.net>
    Cc: <davem@davemloft.net>
    Cc: <kernel-team@fb.com>
    Cc: <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171206224518.3598254-6-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index af44e7c2d577..21c5d43a21af 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -533,6 +533,10 @@ extern int  perf_trace_init(struct perf_event *event);
 extern void perf_trace_destroy(struct perf_event *event);
 extern int  perf_trace_add(struct perf_event *event, int flags);
 extern void perf_trace_del(struct perf_event *event, int flags);
+#ifdef CONFIG_KPROBE_EVENTS
+extern int  perf_kprobe_init(struct perf_event *event, bool is_retprobe);
+extern void perf_kprobe_destroy(struct perf_event *event);
+#endif
 extern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,
 				     char *filter_str);
 extern void ftrace_profile_free_filter(struct perf_event *event);

commit f4e2298e63d24bb7f5cf0f56f72867973cb7e652
Author: Yonghong Song <yhs@fb.com>
Date:   Wed Dec 13 10:35:37 2017 -0800

    bpf/tracing: fix kernel/events/core.c compilation error
    
    Commit f371b304f12e ("bpf/tracing: allow user space to
    query prog array on the same tp") introduced a perf
    ioctl command to query prog array attached to the
    same perf tracepoint. The commit introduced a
    compilation error under certain config conditions, e.g.,
      (1). CONFIG_BPF_SYSCALL is not defined, or
      (2). CONFIG_TRACING is defined but neither CONFIG_UPROBE_EVENTS
           nor CONFIG_KPROBE_EVENTS is defined.
    
    Error message:
      kernel/events/core.o: In function `perf_ioctl':
      core.c:(.text+0x98c4): undefined reference to `bpf_event_query_prog_array'
    
    This patch fixed this error by guarding the real definition under
    CONFIG_BPF_EVENTS and provided static inline dummy function
    if CONFIG_BPF_EVENTS was not defined.
    It renamed the function from bpf_event_query_prog_array to
    perf_event_query_prog_array and moved the definition from linux/bpf.h
    to linux/trace_events.h so the definition is in proximity to
    other prog_array related functions.
    
    Fixes: f371b304f12e ("bpf/tracing: allow user space to query prog array on the same tp")
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 5fea451f6e28..8a1442c4e513 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -467,6 +467,7 @@ trace_trigger_soft_disabled(struct trace_event_file *file)
 unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx);
 int perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog);
 void perf_event_detach_bpf_prog(struct perf_event *event);
+int perf_event_query_prog_array(struct perf_event *event, void __user *info);
 #else
 static inline unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)
 {
@@ -481,6 +482,11 @@ perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog)
 
 static inline void perf_event_detach_bpf_prog(struct perf_event *event) { }
 
+static inline int
+perf_event_query_prog_array(struct perf_event *event, void __user *info)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 enum {

commit 9802d86585db91655c7d1929a4f6bbe0952ea88e
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Dec 11 11:36:48 2017 -0500

    bpf: add a bpf_override_function helper
    
    Error injection is sloppy and very ad-hoc.  BPF could fill this niche
    perfectly with it's kprobe functionality.  We could make sure errors are
    only triggered in specific call chains that we care about with very
    specific situations.  Accomplish this with the bpf_override_funciton
    helper.  This will modify the probe'd callers return value to the
    specified value and set the PC to an override function that simply
    returns, bypassing the originally probed function.  This gives us a nice
    clean way to implement systematic error injection for all of our code
    paths.
    
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index af44e7c2d577..5fea451f6e28 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -528,6 +528,7 @@ do {									\
 struct perf_event;
 
 DECLARE_PER_CPU(struct pt_regs, perf_trace_regs);
+DECLARE_PER_CPU(int, bpf_kprobe_override);
 
 extern int  perf_trace_init(struct perf_event *event);
 extern void perf_trace_destroy(struct perf_event *event);

commit 2dcd9c71c1ffa9a036e09047f60e08383bb0abb6
Merge: b1c2a344cc19 a96a5037ed0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 17 14:58:01 2017 -0800

    Merge tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from
    
     - allow module init functions to be traced
    
     - clean up some unused or not used by config events (saves space)
    
     - clean up of trace histogram code
    
     - add support for preempt and interrupt enabled/disable events
    
     - other various clean ups
    
    * tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (30 commits)
      tracing, thermal: Hide cpu cooling trace events when not in use
      tracing, thermal: Hide devfreq trace events when not in use
      ftrace: Kill FTRACE_OPS_FL_PER_CPU
      perf/ftrace: Small cleanup
      perf/ftrace: Fix function trace events
      perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
      tracing, dma-buf: Remove unused trace event dma_fence_annotate_wait_on
      tracing, memcg, vmscan: Hide trace events when not in use
      tracing/xen: Hide events that are not used when X86_PAE is not defined
      tracing: mark trace_test_buffer as __maybe_unused
      printk: Remove superfluous memory barriers from printk_safe
      ftrace: Clear hashes of stale ips of init memory
      tracing: Add support for preempt and irq enable/disable events
      tracing: Prepare to add preempt and irq trace events
      ftrace/kallsyms: Have /proc/kallsyms show saved mod init functions
      ftrace: Add freeing algorithm to free ftrace_mod_maps
      ftrace: Save module init functions kallsyms symbols for tracing
      ftrace: Allow module init functions to be traced
      ftrace: Add a ftrace_free_mem() function for modules to use
      tracing: Reimplement log2
      ...

commit f3edacbd697f94a743fff1a3d26910ab99948ba7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 11 18:24:55 2017 +0900

    bpf: Revert bpf_overrid_function() helper changes.
    
    NACK'd by x86 maintainer.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 17e5e820a84c..84014ecfa67f 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -523,7 +523,6 @@ do {									\
 struct perf_event;
 
 DECLARE_PER_CPU(struct pt_regs, perf_trace_regs);
-DECLARE_PER_CPU(int, bpf_kprobe_override);
 
 extern int  perf_trace_init(struct perf_event *event);
 extern void perf_trace_destroy(struct perf_event *event);

commit dd0bb688eaa241b5655d396d45366cba9225aed9
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Nov 7 15:28:42 2017 -0500

    bpf: add a bpf_override_function helper
    
    Error injection is sloppy and very ad-hoc.  BPF could fill this niche
    perfectly with it's kprobe functionality.  We could make sure errors are
    only triggered in specific call chains that we care about with very
    specific situations.  Accomplish this with the bpf_override_funciton
    helper.  This will modify the probe'd callers return value to the
    specified value and set the PC to an override function that simply
    returns, bypassing the originally probed function.  This gives us a nice
    clean way to implement systematic error injection for all of our code
    paths.
    
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 84014ecfa67f..17e5e820a84c 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -523,6 +523,7 @@ do {									\
 struct perf_event;
 
 DECLARE_PER_CPU(struct pt_regs, perf_trace_regs);
+DECLARE_PER_CPU(int, bpf_kprobe_override);
 
 extern int  perf_trace_init(struct perf_event *event);
 extern void perf_trace_destroy(struct perf_event *event);

commit 2a171788ba7bb61995e98e8163204fc7880f63b2
Merge: bf5345882bd1 d4c2e9fca5b7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 4 09:26:51 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Files removed in 'net-next' had their license header updated
    in 'net'.  We take the remove from 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 2e0f22298fe9..2bcb4dc6df1a 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 
 #ifndef _LINUX_TRACE_EVENT_H
 #define _LINUX_TRACE_EVENT_H

commit e87c6bc3852b981e71c757be20771546ce9f76f3
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Oct 23 23:53:08 2017 -0700

    bpf: permit multiple bpf attachments for a single perf event
    
    This patch enables multiple bpf attachments for a
    kprobe/uprobe/tracepoint single trace event.
    Each trace_event keeps a list of attached perf events.
    When an event happens, all attached bpf programs will
    be executed based on the order of attachment.
    
    A global bpf_event_mutex lock is introduced to protect
    prog_array attaching and detaching. An alternative will
    be introduce a mutex lock in every trace_event_call
    structure, but it takes a lot of extra memory.
    So a global bpf_event_mutex lock is a good compromise.
    
    The bpf prog detachment involves allocation of memory.
    If the allocation fails, a dummy do-nothing program
    will replace to-be-detached program in-place.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 2e0f22298fe9..fc6aeca945db 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -271,14 +271,37 @@ struct trace_event_call {
 #ifdef CONFIG_PERF_EVENTS
 	int				perf_refcount;
 	struct hlist_head __percpu	*perf_events;
-	struct bpf_prog			*prog;
-	struct perf_event		*bpf_prog_owner;
+	struct bpf_prog_array __rcu	*prog_array;
 
 	int	(*perf_perm)(struct trace_event_call *,
 			     struct perf_event *);
 #endif
 };
 
+#ifdef CONFIG_PERF_EVENTS
+static inline bool bpf_prog_array_valid(struct trace_event_call *call)
+{
+	/*
+	 * This inline function checks whether call->prog_array
+	 * is valid or not. The function is called in various places,
+	 * outside rcu_read_lock/unlock, as a heuristic to speed up execution.
+	 *
+	 * If this function returns true, and later call->prog_array
+	 * becomes false inside rcu_read_lock/unlock region,
+	 * we bail out then. If this function return false,
+	 * there is a risk that we might miss a few events if the checking
+	 * were delayed until inside rcu_read_lock/unlock region and
+	 * call->prog_array happened to become non-NULL then.
+	 *
+	 * Here, READ_ONCE() is used instead of rcu_access_pointer().
+	 * rcu_access_pointer() requires the actual definition of
+	 * "struct bpf_prog_array" while READ_ONCE() only needs
+	 * a declaration of the same type.
+	 */
+	return !!READ_ONCE(call->prog_array);
+}
+#endif
+
 static inline const char *
 trace_event_name(struct trace_event_call *call)
 {
@@ -435,12 +458,23 @@ trace_trigger_soft_disabled(struct trace_event_file *file)
 }
 
 #ifdef CONFIG_BPF_EVENTS
-unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx);
+unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx);
+int perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog);
+void perf_event_detach_bpf_prog(struct perf_event *event);
 #else
-static inline unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx)
+static inline unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)
 {
 	return 1;
 }
+
+static inline int
+perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void perf_event_detach_bpf_prog(struct perf_event *event) { }
+
 #endif
 
 enum {
@@ -511,6 +545,7 @@ perf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,
 {
 	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task, event);
 }
+
 #endif
 
 #endif /* _LINUX_TRACE_EVENT_H */

commit 466c81c45b650deca213fda3d0ec4761667379a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 10 17:15:47 2017 +0200

    perf/ftrace: Fix function trace events
    
    The function-trace <-> perf interface is a tad messed up. Where all
    the other trace <-> perf interfaces use a single trace hook
    registration and use per-cpu RCU based hlist to iterate the events,
    function-trace actually needs multiple hook registrations in order to
    minimize function entry patching when filters are present.
    
    The end result is that we iterate events both on the trace hook and on
    the hlist, which results in reporting events multiple times.
    
    Since function-trace cannot use the regular scheme, fix it the other
    way around, use singleton hlists.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index a6349b76fd39..ca4e67e466a7 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -173,6 +173,11 @@ enum trace_reg {
 	TRACE_REG_PERF_UNREGISTER,
 	TRACE_REG_PERF_OPEN,
 	TRACE_REG_PERF_CLOSE,
+	/*
+	 * These (ADD/DEL) use a 'boolean' return value, where 1 (true) means a
+	 * custom action was taken and the default action is not to be
+	 * performed.
+	 */
 	TRACE_REG_PERF_ADD,
 	TRACE_REG_PERF_DEL,
 #endif

commit 8fd0fbbe8888f295eb34172a7e47bf7d3a0a4687
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 11 09:45:29 2017 +0200

    perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    Revert commit:
    
      75e8387685f6 ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    The reason I instantly stumbled on that patch is that it only addresses the
    ftrace situation and doesn't mention the other _5_ places that use this
    interface. It doesn't explain why those don't have the problem and if not, why
    their solution doesn't work for ftrace.
    
    It doesn't, but this is just putting more duct tape on.
    
    Link: http://lkml.kernel.org/r/20171011080224.200565770@infradead.org
    
    Cc: Zhou Chengming <zhouchengming1@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 2e0f22298fe9..a6349b76fd39 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -507,9 +507,9 @@ void perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,
 static inline void
 perf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,
 		       u64 count, struct pt_regs *regs, void *head,
-		       struct task_struct *task, struct perf_event *event)
+		       struct task_struct *task)
 {
-	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task, event);
+	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task);
 }
 #endif
 

commit ec9dd352d591f0c90402ec67a317c1ed4fb2e638
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Sep 18 16:38:36 2017 -0700

    bpf: one perf event close won't free bpf program attached by another perf event
    
    This patch fixes a bug exhibited by the following scenario:
      1. fd1 = perf_event_open with attr.config = ID1
      2. attach bpf program prog1 to fd1
      3. fd2 = perf_event_open with attr.config = ID1
         <this will be successful>
      4. user program closes fd2 and prog1 is detached from the tracepoint.
      5. user program with fd1 does not work properly as tracepoint
         no output any more.
    
    The issue happens at step 4. Multiple perf_event_open can be called
    successfully, but only one bpf prog pointer in the tp_event. In the
    current logic, any fd release for the same tp_event will free
    the tp_event->prog.
    
    The fix is to free tp_event->prog only when the closing fd
    corresponds to the one which registered the program.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 7f11050746ae..2e0f22298fe9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -272,6 +272,7 @@ struct trace_event_call {
 	int				perf_refcount;
 	struct hlist_head __percpu	*perf_events;
 	struct bpf_prog			*prog;
+	struct perf_event		*bpf_prog_owner;
 
 	int	(*perf_perm)(struct trace_event_call *,
 			     struct perf_event *);

commit 42c8e86c9c3e15ac653c31d57b2f985579c2991d
Merge: 3aea311c1b40 170b3b1050e2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 8 15:08:14 2017 -0700

    Merge tag 'trace-v4.14' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Nothing new in development for this release. These are mostly fixes
      that were found during development of changes for the next merge
      window and fixes that were sent to me late in the last cycle"
    
    * tag 'trace-v4.14' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Apply trace_clock changes to instance max buffer
      tracing: Fix clear of RECORDED_TGID flag when disabling trace event
      tracing: Add barrier to trace_printk() buffer nesting modification
      ftrace: Fix memleak when unregistering dynamic ops when tracing disabled
      ftrace: Fix selftest goto location on error
      ftrace: Zero out ftrace hashes when a module is removed
      tracing: Only have rmmod clear buffers that its events were active in
      ftrace: Fix debug preempt config name in stack_tracer_{en,dis}able

commit 065e63f951432068ba89a844fcbff68ea16ee186
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 31 17:03:47 2017 -0400

    tracing: Only have rmmod clear buffers that its events were active in
    
    Currently, when a module event is enabled, when that module is removed, it
    clears all ring buffers. This is to prevent another module from being loaded
    and having one of its trace event IDs from reusing a trace event ID of the
    removed module. This could cause undesirable effects as the trace event of
    the new module would be using its own processing algorithms to process raw
    data of another event. To prevent this, when a module is loaded, if any of
    its events have been used (signified by the WAS_ENABLED event call flag,
    which is never cleared), all ring buffers are cleared, just in case any one
    of them contains event data of the removed event.
    
    The problem is, there's no reason to clear all ring buffers if only one (or
    less than all of them) uses one of the events. Instead, only clear the ring
    buffers that recorded the events of a module that is being removed.
    
    To do this, instead of keeping the WAS_ENABLED flag with the trace event
    call, move it to the per instance (per ring buffer) event file descriptor.
    The event file descriptor maps each event to a separate ring buffer
    instance. Then when the module is removed, only the ring buffers that
    activated one of the module's events get cleared. The rest are not touched.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 536c80ff7ad9..3702b9cb5dc8 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -217,7 +217,6 @@ enum {
 	TRACE_EVENT_FL_CAP_ANY_BIT,
 	TRACE_EVENT_FL_NO_SET_FILTER_BIT,
 	TRACE_EVENT_FL_IGNORE_ENABLE_BIT,
-	TRACE_EVENT_FL_WAS_ENABLED_BIT,
 	TRACE_EVENT_FL_TRACEPOINT_BIT,
 	TRACE_EVENT_FL_KPROBE_BIT,
 	TRACE_EVENT_FL_UPROBE_BIT,
@@ -229,9 +228,6 @@ enum {
  *  CAP_ANY	  - Any user can enable for perf
  *  NO_SET_FILTER - Set when filter has error and is to be ignored
  *  IGNORE_ENABLE - For trace internal events, do not enable with debugfs file
- *  WAS_ENABLED   - Set and stays set when an event was ever enabled
- *                    (used for module unloading, if a module event is enabled,
- *                     it is best to clear the buffers that used it).
  *  TRACEPOINT    - Event is a tracepoint
  *  KPROBE        - Event is a kprobe
  *  UPROBE        - Event is a uprobe
@@ -241,7 +237,6 @@ enum {
 	TRACE_EVENT_FL_CAP_ANY		= (1 << TRACE_EVENT_FL_CAP_ANY_BIT),
 	TRACE_EVENT_FL_NO_SET_FILTER	= (1 << TRACE_EVENT_FL_NO_SET_FILTER_BIT),
 	TRACE_EVENT_FL_IGNORE_ENABLE	= (1 << TRACE_EVENT_FL_IGNORE_ENABLE_BIT),
-	TRACE_EVENT_FL_WAS_ENABLED	= (1 << TRACE_EVENT_FL_WAS_ENABLED_BIT),
 	TRACE_EVENT_FL_TRACEPOINT	= (1 << TRACE_EVENT_FL_TRACEPOINT_BIT),
 	TRACE_EVENT_FL_KPROBE		= (1 << TRACE_EVENT_FL_KPROBE_BIT),
 	TRACE_EVENT_FL_UPROBE		= (1 << TRACE_EVENT_FL_UPROBE_BIT),
@@ -306,6 +301,7 @@ enum {
 	EVENT_FILE_FL_TRIGGER_MODE_BIT,
 	EVENT_FILE_FL_TRIGGER_COND_BIT,
 	EVENT_FILE_FL_PID_FILTER_BIT,
+	EVENT_FILE_FL_WAS_ENABLED_BIT,
 };
 
 /*
@@ -321,6 +317,7 @@ enum {
  *  TRIGGER_MODE  - When set, invoke the triggers associated with the event
  *  TRIGGER_COND  - When set, one or more triggers has an associated filter
  *  PID_FILTER    - When set, the event is filtered based on pid
+ *  WAS_ENABLED   - Set when enabled to know to clear trace on module removal
  */
 enum {
 	EVENT_FILE_FL_ENABLED		= (1 << EVENT_FILE_FL_ENABLED_BIT),
@@ -333,6 +330,7 @@ enum {
 	EVENT_FILE_FL_TRIGGER_MODE	= (1 << EVENT_FILE_FL_TRIGGER_MODE_BIT),
 	EVENT_FILE_FL_TRIGGER_COND	= (1 << EVENT_FILE_FL_TRIGGER_COND_BIT),
 	EVENT_FILE_FL_PID_FILTER	= (1 << EVENT_FILE_FL_PID_FILTER_BIT),
+	EVENT_FILE_FL_WAS_ENABLED	= (1 << EVENT_FILE_FL_WAS_ENABLED_BIT),
 };
 
 struct trace_event_file {

commit 75e8387685f6c65feb195a4556110b58f852b848
Author: Zhou Chengming <zhouchengming1@huawei.com>
Date:   Fri Aug 25 21:49:37 2017 +0800

    perf/ftrace: Fix double traces of perf on ftrace:function
    
    When running perf on the ftrace:function tracepoint, there is a bug
    which can be reproduced by:
    
      perf record -e ftrace:function -a sleep 20 &
      perf record -e ftrace:function ls
      perf script
    
                  ls 10304 [005]   171.853235: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853237: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853239: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853240: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853242: ftrace:function:
      __task_pid_nr_ns
                  ls 10304 [005]   171.853244: ftrace:function:
      __task_pid_nr_ns
    
    We can see that all the function traces are doubled.
    
    The problem is caused by the inconsistency of the register
    function perf_ftrace_event_register() with the probe function
    perf_ftrace_function_call(). The former registers one probe
    for every perf_event. And the latter handles all perf_events
    on the current cpu. So when two perf_events on the current cpu,
    the traces of them will be doubled.
    
    So this patch adds an extra parameter "event" for perf_tp_event,
    only send sample data to this event when it's not NULL.
    
    Signed-off-by: Zhou Chengming <zhouchengming1@huawei.com>
    Reviewed-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: alexander.shishkin@linux.intel.com
    Cc: huawei.libin@huawei.com
    Link: http://lkml.kernel.org/r/1503668977-12526-1-git-send-email-zhouchengming1@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 536c80ff7ad9..5012b524283d 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -508,9 +508,9 @@ void perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,
 static inline void
 perf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,
 		       u64 count, struct pt_regs *regs, void *head,
-		       struct task_struct *task)
+		       struct task_struct *task, struct perf_event *event)
 {
-	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task);
+	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task, event);
 }
 #endif
 

commit f86f418059b94aa01f9342611a272ca60c583e89
Author: Chunyan Zhang <zhang.chunyan@linaro.org>
Date:   Wed Jun 7 16:12:51 2017 +0800

    trace: fix the errors caused by incompatible type of RCU variables
    
    The variables which are processed by RCU functions should be annotated
    as RCU, otherwise sparse will report the errors like below:
    
    "error: incompatible types in comparison expression (different
    address spaces)"
    
    Link: http://lkml.kernel.org/r/1496823171-7758-1-git-send-email-zhang.chunyan@linaro.org
    
    Signed-off-by: Chunyan Zhang <zhang.chunyan@linaro.org>
    [ Updated to not be 100% 80 column strict ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index f73cedfa2e0b..536c80ff7ad9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -338,7 +338,7 @@ enum {
 struct trace_event_file {
 	struct list_head		list;
 	struct trace_event_call		*event_call;
-	struct event_filter		*filter;
+	struct event_filter __rcu	*filter;
 	struct dentry			*dir;
 	struct trace_array		*tr;
 	struct trace_subsystem_dir	*system;

commit d914ba37d7145acb9fd3bb23075c2d56e5a44eb6
Author: Joel Fernandes <joelaf@google.com>
Date:   Mon Jun 26 19:01:55 2017 -0700

    tracing: Add support for recording tgid of tasks
    
    Inorder to support recording of tgid, the following changes are made:
    
    * Introduce a new API (tracing_record_taskinfo) to additionally record the tgid
      along with the task's comm at the same time. This has has the benefit of not
      setting trace_cmdline_save before all the information for a task is saved.
    * Add a new API tracing_record_taskinfo_sched_switch to record task information
      for 2 tasks at a time (previous and next) and use it from sched_switch probe.
    * Preserve the old API (tracing_record_cmdline) and create it as a wrapper
      around the new one so that existing callers aren't affected.
    * Reuse the existing sched_switch and sched_wakeup probes to record tgid
      information and add a new option 'record-tgid' to enable recording of tgid
    
    When record-tgid option isn't enabled to being with, we take care to make sure
    that there's isn't memory or runtime overhead.
    
    Link: http://lkml.kernel.org/r/20170627020155.5139-1-joelaf@google.com
    
    Cc: kernel-team@android.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Tested-by: Michael Sartain <mikesart@gmail.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index a556805eff8a..f73cedfa2e0b 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -151,7 +151,15 @@ trace_event_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				int type, unsigned long len,
 				unsigned long flags, int pc);
 
-void tracing_record_cmdline(struct task_struct *tsk);
+#define TRACE_RECORD_CMDLINE	BIT(0)
+#define TRACE_RECORD_TGID	BIT(1)
+
+void tracing_record_taskinfo(struct task_struct *task, int flags);
+void tracing_record_taskinfo_sched_switch(struct task_struct *prev,
+					  struct task_struct *next, int flags);
+
+void tracing_record_cmdline(struct task_struct *task);
+void tracing_record_tgid(struct task_struct *task);
 
 int trace_output_call(struct trace_iterator *iter, char *name, char *fmt, ...);
 
@@ -290,6 +298,7 @@ struct trace_subsystem_dir;
 enum {
 	EVENT_FILE_FL_ENABLED_BIT,
 	EVENT_FILE_FL_RECORDED_CMD_BIT,
+	EVENT_FILE_FL_RECORDED_TGID_BIT,
 	EVENT_FILE_FL_FILTERED_BIT,
 	EVENT_FILE_FL_NO_SET_FILTER_BIT,
 	EVENT_FILE_FL_SOFT_MODE_BIT,
@@ -303,6 +312,7 @@ enum {
  * Event file flags:
  *  ENABLED	  - The event is enabled
  *  RECORDED_CMD  - The comms should be recorded at sched_switch
+ *  RECORDED_TGID - The tgids should be recorded at sched_switch
  *  FILTERED	  - The event has a filter attached
  *  NO_SET_FILTER - Set when filter has error and is to be ignored
  *  SOFT_MODE     - The event is enabled/disabled by SOFT_DISABLED
@@ -315,6 +325,7 @@ enum {
 enum {
 	EVENT_FILE_FL_ENABLED		= (1 << EVENT_FILE_FL_ENABLED_BIT),
 	EVENT_FILE_FL_RECORDED_CMD	= (1 << EVENT_FILE_FL_RECORDED_CMD_BIT),
+	EVENT_FILE_FL_RECORDED_TGID	= (1 << EVENT_FILE_FL_RECORDED_TGID_BIT),
 	EVENT_FILE_FL_FILTERED		= (1 << EVENT_FILE_FL_FILTERED_BIT),
 	EVENT_FILE_FL_NO_SET_FILTER	= (1 << EVENT_FILE_FL_NO_SET_FILTER_BIT),
 	EVENT_FILE_FL_SOFT_MODE		= (1 << EVENT_FILE_FL_SOFT_MODE_BIT),

commit af0009fc16a45d091f896794e97a6457f9a7eddf
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Mar 16 11:01:06 2017 -0400

    tracing: Move trace_handle_return() out of line
    
    Currently trace_handle_return() looks like this:
    
     static inline enum print_line_t trace_handle_return(struct trace_seq *s)
     {
            return trace_seq_has_overflowed(s) ?
                    TRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;
     }
    
    Where trace_seq_overflowed(s) is:
    
     static inline bool trace_seq_has_overflowed(struct trace_seq *s)
     {
            return s->full || seq_buf_has_overflowed(&s->seq);
     }
    
    And seq_buf_has_overflowed(&s->seq) is:
    
     static inline bool
     seq_buf_has_overflowed(struct seq_buf *s)
     {
            return s->len > s->size;
     }
    
    Making trace_handle_return() into:
    
     return (s->full || (s->seq->len > s->seq->size)) ?
               TRACE_TYPE_PARTIAL_LINE :
               TRACE_TYPE_HANDLED;
    
    One would think this is not an issue to keep as an inline. But because this
    is used in the TRACE_EVENT() macro, it is extended for every tracepoint in
    the system. Taking a look at a single tracepoint x86_irq_vector (was the
    first one I randomly chosen). As trace_handle_return is used in the
    TRACE_EVENT() macro of trace_raw_output_##call() we disassemble
    trace_raw_output_x86_irq_vector and do a diff:
    
    - is the original
    + is the out-of-line code
    
    I removed identical lines that were different just due to different
    addresses.
    
    --- /tmp/irq-vec-orig   2017-03-16 09:12:48.569384851 -0400
    +++ /tmp/irq-vec-ool    2017-03-16 09:13:39.378153385 -0400
    @@ -6,27 +6,23 @@
            53                      push   %rbx
            48 89 fb                mov    %rdi,%rbx
            4c 8b a7 c0 20 00 00    mov    0x20c0(%rdi),%r12
            e8 f7 72 13 00          callq  ffffffff81155c80 <trace_raw_output_prep>
            83 f8 01                cmp    $0x1,%eax
            74 05                   je     ffffffff8101e993 <trace_raw_output_x86_irq_vector+0x23>
            5b                      pop    %rbx
            41 5c                   pop    %r12
            5d                      pop    %rbp
            c3                      retq
            41 8b 54 24 08          mov    0x8(%r12),%edx
    -       48 8d bb 98 10 00 00    lea    0x1098(%rbx),%rdi
    +       48 81 c3 98 10 00 00    add    $0x1098,%rbx
    -       48 c7 c6 7b 8a a0 81    mov    $0xffffffff81a08a7b,%rsi
    +       48 c7 c6 ab 8a a0 81    mov    $0xffffffff81a08aab,%rsi
    -       e8 c5 85 13 00          callq  ffffffff81156f70 <trace_seq_printf>
    
     === here's the start of the main difference ===
    
    +       48 89 df                mov    %rbx,%rdi
    +       e8 62 7e 13 00          callq  ffffffff81156810 <trace_seq_printf>
    -       8b 93 b8 20 00 00       mov    0x20b8(%rbx),%edx
    -       31 c0                   xor    %eax,%eax
    -       85 d2                   test   %edx,%edx
    -       75 11                   jne    ffffffff8101e9c8 <trace_raw_output_x86_irq_vector+0x58>
    -       48 8b 83 a8 20 00 00    mov    0x20a8(%rbx),%rax
    -       48 39 83 a0 20 00 00    cmp    %rax,0x20a0(%rbx)
    -       0f 93 c0                setae  %al
    +       48 89 df                mov    %rbx,%rdi
    +       e8 4a c5 12 00          callq  ffffffff8114af00 <trace_handle_return>
            5b                      pop    %rbx
    -       0f b6 c0                movzbl %al,%eax
    
     === end ===
    
            41 5c                   pop    %r12
            5d                      pop    %rbp
            c3                      retq
    
    If you notice, the original has 22 bytes of text more than the out of line
    version. As this is for every TRACE_EVENT() defined in the system, this can
    become quite large.
    
       text    data     bss     dec     hex filename
    8690305 5450490 1298432 15439227         eb957b vmlinux-orig
    8681725 5450490 1298432 15430647         eb73f7 vmlinux-handle
    
    This change has a total of 8580 bytes in savings.
    
     $ objdump -dr /tmp/vmlinux-orig | grep '^[0-9a-f]* <trace_raw_output' | wc -l
    324
    
    That's 324 tracepoints. But this does not include modules (which contain
    many more tracepoints). For an allyesconfig build:
    
     $ objdump -dr vmlinux-allyes-orig | grep '^[0-9a-f]* <trace_raw_output' | wc -l
    1401
    
    That's 1401 tracepoints giving us:
    
       text    data     bss     dec     hex filename
    137920629       140221067       53264384        331406080       13c0db00 vmlinux-allyes-orig
    137827709       140221067       53264384        331313160       13bf7008 vmlinux-allyes-handle
    
    92920 bytes in savings!!!
    
    Link: http://lkml.kernel.org/r/20170315021431.13107-2-andi@firstfloor.org
    
    Reported-by: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 0af63c4381b9..a556805eff8a 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -138,16 +138,7 @@ enum print_line_t {
 	TRACE_TYPE_NO_CONSUME	= 3	/* Handled but ask to not consume */
 };
 
-/*
- * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq
- * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function
- * simplifies those functions and keeps them in sync.
- */
-static inline enum print_line_t trace_handle_return(struct trace_seq *s)
-{
-	return trace_seq_has_overflowed(s) ?
-		TRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;
-}
+enum print_line_t trace_handle_return(struct trace_seq *s);
 
 void tracing_generic_entry_update(struct trace_entry *entry,
 				  unsigned long flags,

commit d3213e8fd4b0f18dfd438268ff480406ba743abb
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Wed Feb 22 15:39:47 2017 -0800

    tracing: add __print_flags_u64()
    
    Patch series "DAX tracepoints, mm argument simplification", v4.
    
    This contains both my DAX tracepoint code and Dave Jiang's MM argument
    simplifications.  Dave's code was written with my tracepoint code as a
    baseline, so it seemed simplest to keep them together in a single series.
    
    This patch (of 7):
    
    Add __print_flags_u64() and the helper trace_print_flags_seq_u64() in the
    same spirit as __print_symbolic_u64() and trace_print_symbols_seq_u64().
    These functions allow us to print symbols associated with flags that are
    64 bits wide even on 32 bit machines.
    
    These will be used by the DAX code so that we can print the flags set in a
    pfn_t such as PFN_SG_CHAIN, PFN_SG_LAST, PFN_DEV and PFN_MAP.
    
    Without this new function I was getting errors like the following when
    compiling for i386:
    
      include/linux/pfn_t.h:13:22: warning: large integer implicitly truncated to unsigned type [-Woverflow]
       #define PFN_SG_CHAIN (1ULL << (BITS_PER_LONG_LONG - 1))
        ^
    
    Link: http://lkml.kernel.org/r/1484085142-2297-2-git-send-email-ross.zwisler@linux.intel.com
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 0f165507495c..0af63c4381b9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -23,6 +23,10 @@ const char *trace_print_symbols_seq(struct trace_seq *p, unsigned long val,
 				    const struct trace_print_flags *symbol_array);
 
 #if BITS_PER_LONG == 32
+const char *trace_print_flags_seq_u64(struct trace_seq *p, const char *delim,
+		      unsigned long long flags,
+		      const struct trace_print_flags_u64 *flag_array);
+
 const char *trace_print_symbols_seq_u64(struct trace_seq *p,
 					unsigned long long val,
 					const struct trace_print_flags_u64

commit 3898fac1f488c76e0eef5b5267b4ba8112a82ac4
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Feb 2 17:09:54 2017 +0100

    trace: rename trace_print_hex_seq arg and add kdoc
    
    Steven suggested to improve trace_print_hex_seq() a bit after commit
    2acae0d5b0f7 ("trace: add variant without spacing in trace_print_hex_seq")
    in two ways: i) by adding a kdoc comment for the helper function
    itself and ii) by renaming 'spacing' argument into 'concatenate'
    to better denote that we don't add spaces between each hex bytes.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index cfa475a0e9ca..0f165507495c 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -34,7 +34,7 @@ const char *trace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,
 
 const char *trace_print_hex_seq(struct trace_seq *p,
 				const unsigned char *buf, int len,
-				bool spacing);
+				bool concatenate);
 
 const char *trace_print_array_seq(struct trace_seq *p,
 				   const void *buf, int count,

commit 2acae0d5b0f73a8fb4b180bd13491feb96e55fc6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jan 25 02:28:16 2017 +0100

    trace: add variant without spacing in trace_print_hex_seq
    
    For upcoming tracepoint support for BPF, we want to dump the program's
    tag. Format should be similar to __print_hex(), but without spacing.
    Add a __print_hex_str() variant for exactly that purpose that reuses
    trace_print_hex_seq().
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index be007610ceb0..cfa475a0e9ca 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -33,7 +33,8 @@ const char *trace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,
 				    unsigned int bitmask_size);
 
 const char *trace_print_hex_seq(struct trace_seq *p,
-				const unsigned char *buf, int len);
+				const unsigned char *buf, int len,
+				bool spacing);
 
 const char *trace_print_array_seq(struct trace_seq *p,
 				   const void *buf, int count,

commit 2600a46ee0ed57c0e0a382c2a37ebac64d374d20
Merge: 03e1aa1cbb65 0fc1b09ff1ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 18 18:55:19 2016 -0700

    Merge tag 'trace-v4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This includes two new updates for the ftrace infrastructure.
    
       - With the changing of the code for filtering events by pid, from a
         list of pids to a bitmask, we can now easily implement following
         forks.  With a new tracing option "event-fork" which, when set,
         will have tasks with pids in set_event_pid, when they fork, to have
         their child pids added to set_event_pid and the child will be
         traced as well.
    
         Note, if "event-fork" is set and a task with its pid in
         set_event_pid exits, its pid will be removed from set_event_pid
    
       - The addition of Tom Zanussi's hist triggers.  This includes a very
         thorough documentatino on how to use the hist triggers with events.
         This introduces a quick and easy way to get histogram data from
         events and their fields.
    
      Some other cleanups and updates were added as well.  Like Masami
      Hiramatsu added test cases for the event trigger and hist triggers.
      Also I added a speed up of filtering by using a temp buffer when
      filters are set"
    
    * tag 'trace-v4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (45 commits)
      tracing: Use temp buffer when filtering events
      tracing: Remove TRACE_EVENT_FL_USE_CALL_FILTER logic
      tracing: Remove unused function trace_current_buffer_lock_reserve()
      tracing: Remove one use of trace_current_buffer_lock_reserve()
      tracing: Have trace_buffer_unlock_commit() call the _regs version with NULL
      tracing: Remove unused function trace_current_buffer_discard_commit()
      tracing: Move trace_buffer_unlock_commit{_regs}() to local header
      tracing: Fold filter_check_discard() into its only user
      tracing: Make filter_check_discard() local
      tracing: Move event_trigger_unlock_commit{_regs}() to local header
      tracing: Don't use the address of the buffer array name in copy_from_user
      tracing: Handle tracing_map_alloc_elts() error path correctly
      tracing: Add check for NULL event field when creating hist field
      tracing: checking for NULL instead of IS_ERR()
      tracing: Do not inherit event-fork option for instances
      tracing: Fix unsigned comparison to zero in hist trigger code
      kselftests/ftrace: Add a test for log2 modifier of hist trigger
      tracing: Add hist trigger 'log2' modifier
      kselftests/ftrace: Add hist trigger testcases
      kselftests/ftrace : Add event trigger testcases
      ...

commit dcb0b5575d24a32f51a3f1003312fb94ed4e214a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon May 2 21:30:04 2016 -0400

    tracing: Remove TRACE_EVENT_FL_USE_CALL_FILTER logic
    
    Nothing sets TRACE_EVENT_FL_USE_CALL_FILTER anymore. Remove it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 3111a1efdad6..ba6456302e34 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -214,7 +214,6 @@ enum {
 	TRACE_EVENT_FL_NO_SET_FILTER_BIT,
 	TRACE_EVENT_FL_IGNORE_ENABLE_BIT,
 	TRACE_EVENT_FL_WAS_ENABLED_BIT,
-	TRACE_EVENT_FL_USE_CALL_FILTER_BIT,
 	TRACE_EVENT_FL_TRACEPOINT_BIT,
 	TRACE_EVENT_FL_KPROBE_BIT,
 	TRACE_EVENT_FL_UPROBE_BIT,
@@ -229,7 +228,6 @@ enum {
  *  WAS_ENABLED   - Set and stays set when an event was ever enabled
  *                    (used for module unloading, if a module event is enabled,
  *                     it is best to clear the buffers that used it).
- *  USE_CALL_FILTER - For trace internal events, don't use file filter
  *  TRACEPOINT    - Event is a tracepoint
  *  KPROBE        - Event is a kprobe
  *  UPROBE        - Event is a uprobe
@@ -240,7 +238,6 @@ enum {
 	TRACE_EVENT_FL_NO_SET_FILTER	= (1 << TRACE_EVENT_FL_NO_SET_FILTER_BIT),
 	TRACE_EVENT_FL_IGNORE_ENABLE	= (1 << TRACE_EVENT_FL_IGNORE_ENABLE_BIT),
 	TRACE_EVENT_FL_WAS_ENABLED	= (1 << TRACE_EVENT_FL_WAS_ENABLED_BIT),
-	TRACE_EVENT_FL_USE_CALL_FILTER	= (1 << TRACE_EVENT_FL_USE_CALL_FILTER_BIT),
 	TRACE_EVENT_FL_TRACEPOINT	= (1 << TRACE_EVENT_FL_TRACEPOINT_BIT),
 	TRACE_EVENT_FL_KPROBE		= (1 << TRACE_EVENT_FL_KPROBE_BIT),
 	TRACE_EVENT_FL_UPROBE		= (1 << TRACE_EVENT_FL_UPROBE_BIT),

commit 904d1857ad09b43f514897dd42daffe200d1ca50
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 29 18:11:54 2016 -0400

    tracing: Remove unused function trace_current_buffer_lock_reserve()
    
    trace_current_buffer_lock_reserve() has no more users. Remove it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 356c39b3abbb..3111a1efdad6 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -154,10 +154,6 @@ trace_event_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				struct trace_event_file *trace_file,
 				int type, unsigned long len,
 				unsigned long flags, int pc);
-struct ring_buffer_event *
-trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
-				  int type, unsigned long len,
-				  unsigned long flags, int pc);
 
 void tracing_record_cmdline(struct task_struct *tsk);
 

commit a9fe48dcde88fd48e210e4280f19cb9300ec9112
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 29 16:12:39 2016 -0400

    tracing: Remove unused function trace_current_buffer_discard_commit()
    
    The function trace_current_buffer_discard_commit() has no callers, remove
    it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 48cc5e19c5f5..356c39b3abbb 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -158,8 +158,6 @@ struct ring_buffer_event *
 trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				  int type, unsigned long len,
 				  unsigned long flags, int pc);
-void trace_current_buffer_discard_commit(struct ring_buffer *buffer,
-					 struct ring_buffer_event *event);
 
 void tracing_record_cmdline(struct task_struct *tsk);
 

commit fa66ddb870ca022342fe6d1312ef76d2f7233a1d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Apr 28 12:04:13 2016 -0400

    tracing: Move trace_buffer_unlock_commit{_regs}() to local header
    
    The functions trace_buffer_unlock_commit() and the _regs() version are only
    used within the kernel/trace directory. Move them to the local header and
    remove the export as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index bb383af35cc7..48cc5e19c5f5 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -158,15 +158,6 @@ struct ring_buffer_event *
 trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				  int type, unsigned long len,
 				  unsigned long flags, int pc);
-void trace_buffer_unlock_commit(struct trace_array *tr,
-				struct ring_buffer *buffer,
-				struct ring_buffer_event *event,
-				unsigned long flags, int pc);
-void trace_buffer_unlock_commit_regs(struct trace_array *tr,
-				     struct ring_buffer *buffer,
-				     struct ring_buffer_event *event,
-				     unsigned long flags, int pc,
-				     struct pt_regs *regs);
 void trace_current_buffer_discard_commit(struct ring_buffer *buffer,
 					 struct ring_buffer_event *event);
 

commit 65da9a0a3bf2202c2432f42d41eb908f2fa30579
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 27 10:13:46 2016 -0400

    tracing: Make filter_check_discard() local
    
    Nothing outside of the tracing directory calls filter_check_discard() or
    check_filter_check_discard(). They should not be called by modules. Move
    their prototypes into the local tracing header and remove their
    EXPORT_SYMBOL() macros.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 70a181cb3585..bb383af35cc7 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -413,12 +413,6 @@ enum event_trigger_type {
 
 extern int filter_match_preds(struct event_filter *filter, void *rec);
 
-extern int filter_check_discard(struct trace_event_file *file, void *rec,
-				struct ring_buffer *buffer,
-				struct ring_buffer_event *event);
-extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
-				     struct ring_buffer *buffer,
-				     struct ring_buffer_event *event);
 extern enum event_trigger_type event_triggers_call(struct trace_event_file *file,
 						   void *rec);
 extern void event_triggers_post_call(struct trace_event_file *file,

commit dad56ee742a3abbb5d9e8108f8537d412bff3f57
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 26 21:22:22 2016 -0400

    tracing: Move event_trigger_unlock_commit{_regs}() to local header
    
    The functions event_trigger_unlock_commit() and
    event_trigger_unlock_commit_regs() are no longer used outside the tracing
    system. Move them out of the generic headers and into the local one.
    
    Along with __event_trigger_test_discard() that is only used by them.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 5f89a5b0c7e6..70a181cb3585 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -452,100 +452,6 @@ trace_trigger_soft_disabled(struct trace_event_file *file)
 	return false;
 }
 
-/*
- * Helper function for event_trigger_unlock_commit{_regs}().
- * If there are event triggers attached to this event that requires
- * filtering against its fields, then they wil be called as the
- * entry already holds the field information of the current event.
- *
- * It also checks if the event should be discarded or not.
- * It is to be discarded if the event is soft disabled and the
- * event was only recorded to process triggers, or if the event
- * filter is active and this event did not match the filters.
- *
- * Returns true if the event is discarded, false otherwise.
- */
-static inline bool
-__event_trigger_test_discard(struct trace_event_file *file,
-			     struct ring_buffer *buffer,
-			     struct ring_buffer_event *event,
-			     void *entry,
-			     enum event_trigger_type *tt)
-{
-	unsigned long eflags = file->flags;
-
-	if (eflags & EVENT_FILE_FL_TRIGGER_COND)
-		*tt = event_triggers_call(file, entry);
-
-	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags))
-		ring_buffer_discard_commit(buffer, event);
-	else if (!filter_check_discard(file, entry, buffer, event))
-		return false;
-
-	return true;
-}
-
-/**
- * event_trigger_unlock_commit - handle triggers and finish event commit
- * @file: The file pointer assoctiated to the event
- * @buffer: The ring buffer that the event is being written to
- * @event: The event meta data in the ring buffer
- * @entry: The event itself
- * @irq_flags: The state of the interrupts at the start of the event
- * @pc: The state of the preempt count at the start of the event.
- *
- * This is a helper function to handle triggers that require data
- * from the event itself. It also tests the event against filters and
- * if the event is soft disabled and should be discarded.
- */
-static inline void
-event_trigger_unlock_commit(struct trace_event_file *file,
-			    struct ring_buffer *buffer,
-			    struct ring_buffer_event *event,
-			    void *entry, unsigned long irq_flags, int pc)
-{
-	enum event_trigger_type tt = ETT_NONE;
-
-	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
-		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
-
-	if (tt)
-		event_triggers_post_call(file, tt, entry);
-}
-
-/**
- * event_trigger_unlock_commit_regs - handle triggers and finish event commit
- * @file: The file pointer assoctiated to the event
- * @buffer: The ring buffer that the event is being written to
- * @event: The event meta data in the ring buffer
- * @entry: The event itself
- * @irq_flags: The state of the interrupts at the start of the event
- * @pc: The state of the preempt count at the start of the event.
- *
- * This is a helper function to handle triggers that require data
- * from the event itself. It also tests the event against filters and
- * if the event is soft disabled and should be discarded.
- *
- * Same as event_trigger_unlock_commit() but calls
- * trace_buffer_unlock_commit_regs() instead of trace_buffer_unlock_commit().
- */
-static inline void
-event_trigger_unlock_commit_regs(struct trace_event_file *file,
-				 struct ring_buffer *buffer,
-				 struct ring_buffer_event *event,
-				 void *entry, unsigned long irq_flags, int pc,
-				 struct pt_regs *regs)
-{
-	enum event_trigger_type tt = ETT_NONE;
-
-	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
-		trace_buffer_unlock_commit_regs(file->tr, buffer, event,
-						irq_flags, pc, regs);
-
-	if (tt)
-		event_triggers_post_call(file, tt, entry);
-}
-
 #ifdef CONFIG_BPF_EVENTS
 unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx);
 #else

commit 85b67bcb7e4a23ced05e7020bf5843b9857f6881
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Apr 18 20:11:50 2016 -0700

    perf, bpf: minimize the size of perf_trace_() tracepoint handler
    
    move trace_call_bpf() into helper function to minimize the size
    of perf_trace_*() tracepoint handlers.
        text           data     bss     dec            hex  filename
    10541679        5526646 2945024 19013349        1221ee5 vmlinux_before
    10509422        5526646 2945024 18981092        121a0e4 vmlinux_after
    
    It may seem that perf_fetch_caller_regs() can also be moved,
    but that is incorrect, since ip/sp will be wrong.
    
    bpf+tracepoint performance is not affected, since
    perf_swevent_put_recursion_context() is now inlined.
    export_symbol_gpl can also be dropped.
    
    No measurable change in normal perf tracepoints.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index fe6441203b59..222f6aa0418f 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -609,6 +609,11 @@ extern void ftrace_profile_free_filter(struct perf_event *event);
 void perf_trace_buf_update(void *record, u16 type);
 void *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp);
 
+void perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,
+			       struct trace_event_call *call, u64 count,
+			       struct pt_regs *regs, struct hlist_head *head,
+			       struct task_struct *task);
+
 static inline void
 perf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,
 		       u64 count, struct pt_regs *regs, void *head,

commit d0bad49bb0a094a1beb06640785f95cb256b7272
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:55 2016 -0600

    tracing: Add enable_hist/disable_hist triggers
    
    Similar to enable_event/disable_event triggers, these triggers enable
    and disable the aggregation of events into maps rather than enabling
    and disabling their writing into the trace buffer.
    
    They can be used to automatically start and stop hist triggers based
    on a matching filter condition.
    
    If there's a paused hist trigger on system:event, the following would
    start it when the filter condition was hit:
    
      # echo enable_hist:system:event [ if filter] > event/trigger
    
    And the following would disable a running system:event hist trigger:
    
      # echo disable_hist:system:event [ if filter] > event/trigger
    
    See Documentation/trace/events.txt for real examples.
    
    Link: http://lkml.kernel.org/r/f812f086e52c8b7c8ad5443487375e03c96a601f.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 404603720650..5f89a5b0c7e6 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -408,6 +408,7 @@ enum event_trigger_type {
 	ETT_STACKTRACE		= (1 << 2),
 	ETT_EVENT_ENABLE	= (1 << 3),
 	ETT_EVENT_HIST		= (1 << 4),
+	ETT_HIST_ENABLE		= (1 << 5),
 };
 
 extern int filter_match_preds(struct event_filter *filter, void *rec);

commit 7ef224d1d0e3a1ade02d02c01ce1dcffb736d2c3
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:42 2016 -0600

    tracing: Add 'hist' event trigger command
    
    'hist' triggers allow users to continually aggregate trace events,
    which can then be viewed afterwards by simply reading a 'hist' file
    containing the aggregation in a human-readable format.
    
    The basic idea is very simple and boils down to a mechanism whereby
    trace events, rather than being exhaustively dumped in raw form and
    viewed directly, are automatically 'compressed' into meaningful tables
    completely defined by the user.
    
    This is done strictly via single-line command-line commands and
    without the aid of any kind of programming language or interpreter.
    
    A surprising number of typical use cases can be accomplished by users
    via this simple mechanism.  In fact, a large number of the tasks that
    users typically do using the more complicated script-based tracing
    tools, at least during the initial stages of an investigation, can be
    accomplished by simply specifying a set of keys and values to be used
    in the creation of a hash table.
    
    The Linux kernel trace event subsystem happens to provide an extensive
    list of keys and values ready-made for such a purpose in the form of
    the event format files associated with each trace event.  By simply
    consulting the format file for field names of interest and by plugging
    them into the hist trigger command, users can create an endless number
    of useful aggregations to help with investigating various properties
    of the system.  See Documentation/trace/events.txt for examples.
    
    hist triggers are implemented on top of the existing event trigger
    infrastructure, and as such are consistent with the existing triggers
    from a user's perspective as well.
    
    The basic syntax follows the existing trigger syntax.  Users start an
    aggregation by writing a 'hist' trigger to the event of interest's
    trigger file:
    
      # echo hist:keys=xxx [ if filter] > event/trigger
    
    Once a hist trigger has been set up, by default it continually
    aggregates every matching event into a hash table using the event key
    and a value field named 'hitcount'.
    
    To view the aggregation at any point in time, simply read the 'hist'
    file in the same directory as the 'trigger' file:
    
      # cat event/hist
    
    The detailed syntax provides additional options for user control, and
    is described exhaustively in Documentation/trace/events.txt and in the
    virtual tracing/README file in the tracing subsystem.
    
    Link: http://lkml.kernel.org/r/72d263b5e1853fe9c314953b65833c3aa75479f2.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 0810f81b6db2..404603720650 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -407,6 +407,7 @@ enum event_trigger_type {
 	ETT_SNAPSHOT		= (1 << 1),
 	ETT_STACKTRACE		= (1 << 2),
 	ETT_EVENT_ENABLE	= (1 << 3),
+	ETT_EVENT_HIST		= (1 << 4),
 };
 
 extern int filter_match_preds(struct event_filter *filter, void *rec);

commit 32bbe0078afe86a8bf4c67c6b3477781b15e94dc
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:28 2016 -0700

    bpf: sanitize bpf tracepoint access
    
    during bpf program loading remember the last byte of ctx access
    and at the time of attaching the program to tracepoint check that
    the program doesn't access bytes beyond defined in tracepoint fields
    
    This also disallows access to __dynamic_array fields, but can be
    relaxed in the future.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 56f795e6a093..fe6441203b59 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -569,6 +569,7 @@ extern int trace_define_field(struct trace_event_call *call, const char *type,
 			      int is_signed, int filter_type);
 extern int trace_add_event_call(struct trace_event_call *call);
 extern int trace_remove_event_call(struct trace_event_call *call);
+extern int trace_event_get_offsets(struct trace_event_call *call);
 
 #define is_signed_type(type)	(((type)(-1)) < (type)1)
 

commit 1e1dcd93b468901e114f279c94a0b356adc5e7cd
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:24 2016 -0700

    perf: split perf_trace_buf_prepare into alloc and update parts
    
    split allows to move expensive update of 'struct trace_entry' to later phase.
    Repurpose unused 1st argument of perf_tp_event() to indicate event type.
    
    While splitting use temp variable 'rctx' instead of '*rctx' to avoid
    unnecessary loads done by the compiler due to -fno-strict-aliasing
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 0810f81b6db2..56f795e6a093 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -605,15 +605,15 @@ extern void perf_trace_del(struct perf_event *event, int flags);
 extern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,
 				     char *filter_str);
 extern void ftrace_profile_free_filter(struct perf_event *event);
-extern void *perf_trace_buf_prepare(int size, unsigned short type,
-				    struct pt_regs **regs, int *rctxp);
+void perf_trace_buf_update(void *record, u16 type);
+void *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp);
 
 static inline void
-perf_trace_buf_submit(void *raw_data, int size, int rctx, u64 addr,
+perf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,
 		       u64 count, struct pt_regs *regs, void *head,
 		       struct task_struct *task)
 {
-	perf_tp_event(addr, count, raw_data, size, regs, head, rctx, task);
+	perf_tp_event(type, count, raw_data, size, regs, head, rctx, task);
 }
 #endif
 

commit e46b4e2b46e173889b19999b8bd033d5e8b3acf0
Merge: faea72dd0f15 7e6867bf831c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:52:25 2016 -0700

    Merge tag 'trace-v4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Nothing major this round.  Mostly small clean ups and fixes.
    
      Some visible changes:
    
       - A new flag was added to distinguish traces done in NMI context.
    
       - Preempt tracer now shows functions where preemption is disabled but
         interrupts are still enabled.
    
      Other notes:
    
       - Updates were done to function tracing to allow better performance
         with perf.
    
       - Infrastructure code has been added to allow for a new histogram
         feature for recording live trace event histograms that can be
         configured by simple user commands.  The feature itself was just
         finished, but needs a round in linux-next before being pulled.
    
         This only includes some infrastructure changes that will be needed"
    
    * tag 'trace-v4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (22 commits)
      tracing: Record and show NMI state
      tracing: Fix trace_printk() to print when not using bprintk()
      tracing: Remove redundant reset per-CPU buff in irqsoff tracer
      x86: ftrace: Fix the misleading comment for arch/x86/kernel/ftrace.c
      tracing: Fix crash from reading trace_pipe with sendfile
      tracing: Have preempt(irqs)off trace preempt disabled functions
      tracing: Fix return while holding a lock in register_tracer()
      ftrace: Use kasprintf() in ftrace_profile_tracefs()
      ftrace: Update dynamic ftrace calls only if necessary
      ftrace: Make ftrace_hash_rec_enable return update bool
      tracing: Fix typoes in code comment and printk in trace_nop.c
      tracing, writeback: Replace cgroup path to cgroup ino
      tracing: Use flags instead of bool in trigger structure
      tracing: Add an unreg_all() callback to trigger commands
      tracing: Add needs_rec flag to event triggers
      tracing: Add a per-event-trigger 'paused' field
      tracing: Add get_syscall_name()
      tracing: Add event record param to trigger_ops.func()
      tracing: Make event trigger functions available
      tracing: Make ftrace_event_field checking functions available
      ...

commit 20f6e03a40ba536dfc7c6f83dd894d994aeb39f3
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Mar 15 14:55:42 2016 -0700

    tracepoints: move trace_print_flags definitions to tracepoint-defs.h
    
    The following patch will need to declare array of struct
    trace_print_flags in a header.  To prevent this header from pulling in
    all of RCU through trace_events.h, move the struct
    trace_print_flags{_64} definitions to the new lightweight
    tracepoint-defs.h header.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 925730bc9fc1..705df7db4482 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -15,16 +15,6 @@ struct tracer;
 struct dentry;
 struct bpf_prog;
 
-struct trace_print_flags {
-	unsigned long		mask;
-	const char		*name;
-};
-
-struct trace_print_flags_u64 {
-	unsigned long long	mask;
-	const char		*name;
-};
-
 const char *trace_print_flags_seq(struct trace_seq *p, const char *delim,
 				  unsigned long flags,
 				  const struct trace_print_flags *flag_array);

commit c4a5923055c9e0c87dfc0387f7cda5ee2bbac3c1
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:45 2015 -0600

    tracing: Add event record param to trigger_ops.func()
    
    Some triggers may need access to the trace event, so pass it in.  Also
    fix up the existing trigger funcs and their callers.
    
    Link: http://lkml.kernel.org/r/543e31e9fc445ef61077421ab219033401c39846.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 925730bc9fc1..0d6930e941e8 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -430,7 +430,8 @@ extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 extern enum event_trigger_type event_triggers_call(struct trace_event_file *file,
 						   void *rec);
 extern void event_triggers_post_call(struct trace_event_file *file,
-				     enum event_trigger_type tt);
+				     enum event_trigger_type tt,
+				     void *rec);
 
 bool trace_event_ignore_this_pid(struct trace_event_file *trace_file);
 
@@ -517,7 +518,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
 		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
 
 	if (tt)
-		event_triggers_post_call(file, tt);
+		event_triggers_post_call(file, tt, entry);
 }
 
 /**
@@ -550,7 +551,7 @@ event_trigger_unlock_commit_regs(struct trace_event_file *file,
 						irq_flags, pc, regs);
 
 	if (tt)
-		event_triggers_post_call(file, tt);
+		event_triggers_post_call(file, tt, entry);
 }
 
 #ifdef CONFIG_BPF_EVENTS

commit e57cbaf0eb006eaa207395f3bfd7ce52c1b5539c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 3 17:18:20 2016 -0500

    tracing: Do not have 'comm' filter override event 'comm' field
    
    Commit 9f61668073a8d "tracing: Allow triggers to filter for CPU ids and
    process names" added a 'comm' filter that will filter events based on the
    current tasks struct 'comm'. But this now hides the ability to filter events
    that have a 'comm' field too. For example, sched_migrate_task trace event.
    That has a 'comm' field of the task to be migrated.
    
     echo 'comm == "bash"' > events/sched_migrate_task/filter
    
    will now filter all sched_migrate_task events for tasks named "bash" that
    migrates other tasks (in interrupt context), instead of seeing when "bash"
    itself gets migrated.
    
    This fix requires a couple of changes.
    
    1) Change the look up order for filter predicates to look at the events
       fields before looking at the generic filters.
    
    2) Instead of basing the filter function off of the "comm" name, have the
       generic "comm" filter have its own filter_type (FILTER_COMM). Test
       against the type instead of the name to assign the filter function.
    
    3) Add a new "COMM" filter that works just like "comm" but will filter based
       on the current task, even if the trace event contains a "comm" field.
    
    Do the same for "cpu" field, adding a FILTER_CPU and a filter "CPU".
    
    Cc: stable@vger.kernel.org # v4.3+
    Fixes: 9f61668073a8d "tracing: Allow triggers to filter for CPU ids and process names"
    Reported-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 429fdfc3baf5..925730bc9fc1 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -568,6 +568,8 @@ enum {
 	FILTER_DYN_STRING,
 	FILTER_PTR_STRING,
 	FILTER_TRACE_FN,
+	FILTER_COMM,
+	FILTER_CPU,
 };
 
 extern int trace_event_raw_init(struct trace_event_call *call);

commit 3fdaf80f4a836911c0eda1cee92f8aa625f90197
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 12:58:44 2015 -0400

    tracing: Implement event pid filtering
    
    Add the necessary hooks to use the pids loaded in set_event_pid to filter
    all the events enabled in the tracing instance that match the pids listed.
    
    Two probes are added to both sched_switch and sched_wakeup tracepoints to be
    called before other probes are called and after the other probes are called.
    The first is used to set the necessary flags to let the probes know to test
    if they should be traced or not.
    
    The sched_switch pre probe will set the "ignore_pid" flag if neither the
    previous or next task has a matching pid.
    
    The sched_switch probe will set the "ignore_pid" flag if the next task
    does not match the matching pid.
    
    The pre probe allows for probes tracing sched_switch to be traced if
    necessary.
    
    The sched_wakeup pre probe will set the "ignore_pid" flag if neither the
    current task nor the wakee task has a matching pid.
    
    The sched_wakeup post probe will set the "ignore_pid" flag if the current
    task does not have a matching pid.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index f85693bbcdc3..429fdfc3baf5 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -328,6 +328,7 @@ enum {
 	EVENT_FILE_FL_SOFT_DISABLED_BIT,
 	EVENT_FILE_FL_TRIGGER_MODE_BIT,
 	EVENT_FILE_FL_TRIGGER_COND_BIT,
+	EVENT_FILE_FL_PID_FILTER_BIT,
 };
 
 /*
@@ -341,6 +342,7 @@ enum {
  *                   tracepoint may be enabled)
  *  TRIGGER_MODE  - When set, invoke the triggers associated with the event
  *  TRIGGER_COND  - When set, one or more triggers has an associated filter
+ *  PID_FILTER    - When set, the event is filtered based on pid
  */
 enum {
 	EVENT_FILE_FL_ENABLED		= (1 << EVENT_FILE_FL_ENABLED_BIT),
@@ -351,6 +353,7 @@ enum {
 	EVENT_FILE_FL_SOFT_DISABLED	= (1 << EVENT_FILE_FL_SOFT_DISABLED_BIT),
 	EVENT_FILE_FL_TRIGGER_MODE	= (1 << EVENT_FILE_FL_TRIGGER_MODE_BIT),
 	EVENT_FILE_FL_TRIGGER_COND	= (1 << EVENT_FILE_FL_TRIGGER_COND_BIT),
+	EVENT_FILE_FL_PID_FILTER	= (1 << EVENT_FILE_FL_PID_FILTER_BIT),
 };
 
 struct trace_event_file {
@@ -429,6 +432,8 @@ extern enum event_trigger_type event_triggers_call(struct trace_event_file *file
 extern void event_triggers_post_call(struct trace_event_file *file,
 				     enum event_trigger_type tt);
 
+bool trace_event_ignore_this_pid(struct trace_event_file *trace_file);
+
 /**
  * trace_trigger_soft_disabled - do triggers and test if soft disabled
  * @file: The file pointer of the event to test
@@ -448,6 +453,8 @@ trace_trigger_soft_disabled(struct trace_event_file *file)
 			event_triggers_call(file, NULL);
 		if (eflags & EVENT_FILE_FL_SOFT_DISABLED)
 			return true;
+		if (eflags & EVENT_FILE_FL_PID_FILTER)
+			return trace_event_ignore_this_pid(file);
 	}
 	return false;
 }

commit b7f0c959edfb4448f94bd33c39fda08e10ce6ede
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 17:38:44 2015 -0400

    tracing: Pass trace_array into trace_buffer_unlock_commit()
    
    In preparation for having trace options be per instance, the trace_array
    needs to be passed to the trace_buffer_unlock_commit(). The
    trace_event_buffer_lock_reserve() already passes in the trace_event_file
    where the trace_array can be derived from.
    
    Also added a "__init" to the boot up test event plus function tracing
    function function_test_events_call().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 71c1191b9954..f85693bbcdc3 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -168,10 +168,12 @@ struct ring_buffer_event *
 trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				  int type, unsigned long len,
 				  unsigned long flags, int pc);
-void trace_buffer_unlock_commit(struct ring_buffer *buffer,
+void trace_buffer_unlock_commit(struct trace_array *tr,
+				struct ring_buffer *buffer,
 				struct ring_buffer_event *event,
 				unsigned long flags, int pc);
-void trace_buffer_unlock_commit_regs(struct ring_buffer *buffer,
+void trace_buffer_unlock_commit_regs(struct trace_array *tr,
+				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event,
 				     unsigned long flags, int pc,
 				     struct pt_regs *regs);
@@ -505,7 +507,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
 	enum event_trigger_type tt = ETT_NONE;
 
 	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
-		trace_buffer_unlock_commit(buffer, event, irq_flags, pc);
+		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
 
 	if (tt)
 		event_triggers_post_call(file, tt);
@@ -537,7 +539,7 @@ event_trigger_unlock_commit_regs(struct trace_event_file *file,
 	enum event_trigger_type tt = ETT_NONE;
 
 	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
-		trace_buffer_unlock_commit_regs(buffer, event,
+		trace_buffer_unlock_commit_regs(file->tr, buffer, event,
 						irq_flags, pc, regs);
 
 	if (tt)

commit 41907416bc24412f839559ea10a2b9e4eeb21aa7
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 15:06:00 2015 -0400

    tracing: Remove unused function trace_current_buffer_lock_reserve()
    
    trace_current_buffer_lock_reserve() is not used by anything. Might as well
    get rid of it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index ed27917cabc9..71c1191b9954 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -168,9 +168,6 @@ struct ring_buffer_event *
 trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
 				  int type, unsigned long len,
 				  unsigned long flags, int pc);
-void trace_current_buffer_unlock_commit(struct ring_buffer *buffer,
-					struct ring_buffer_event *event,
-					unsigned long flags, int pc);
 void trace_buffer_unlock_commit(struct ring_buffer *buffer,
 				struct ring_buffer_event *event,
 				unsigned long flags, int pc);

commit 04a22fae4cbc1f7d3f7471e9b36359f98bd3f043
Author: Wang Nan <wangnan0@huawei.com>
Date:   Wed Jul 1 02:13:50 2015 +0000

    tracing, perf: Implement BPF programs attached to uprobes
    
    By copying BPF related operation to uprobe processing path, this patch
    allow users attach BPF programs to uprobes like what they are already
    doing on kprobes.
    
    After this patch, users are allowed to use PERF_EVENT_IOC_SET_BPF on a
    uprobe perf event. Which make it possible to profile user space programs
    and kernel events together using BPF.
    
    Because of this patch, CONFIG_BPF_EVENTS should be selected by
    CONFIG_UPROBE_EVENT to ensure trace_call_bpf() is compiled even if
    KPROBE_EVENT is not set.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Kaixu Xia <xiakaixu@huawei.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: pi3orama@163.com
    Link: http://lkml.kernel.org/r/1435716878-189507-3-git-send-email-wangnan0@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 180dbf8720f9..ed27917cabc9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -243,6 +243,7 @@ enum {
 	TRACE_EVENT_FL_USE_CALL_FILTER_BIT,
 	TRACE_EVENT_FL_TRACEPOINT_BIT,
 	TRACE_EVENT_FL_KPROBE_BIT,
+	TRACE_EVENT_FL_UPROBE_BIT,
 };
 
 /*
@@ -257,6 +258,7 @@ enum {
  *  USE_CALL_FILTER - For trace internal events, don't use file filter
  *  TRACEPOINT    - Event is a tracepoint
  *  KPROBE        - Event is a kprobe
+ *  UPROBE        - Event is a uprobe
  */
 enum {
 	TRACE_EVENT_FL_FILTERED		= (1 << TRACE_EVENT_FL_FILTERED_BIT),
@@ -267,8 +269,11 @@ enum {
 	TRACE_EVENT_FL_USE_CALL_FILTER	= (1 << TRACE_EVENT_FL_USE_CALL_FILTER_BIT),
 	TRACE_EVENT_FL_TRACEPOINT	= (1 << TRACE_EVENT_FL_TRACEPOINT_BIT),
 	TRACE_EVENT_FL_KPROBE		= (1 << TRACE_EVENT_FL_KPROBE_BIT),
+	TRACE_EVENT_FL_UPROBE		= (1 << TRACE_EVENT_FL_UPROBE_BIT),
 };
 
+#define TRACE_EVENT_FL_UKPROBE (TRACE_EVENT_FL_KPROBE | TRACE_EVENT_FL_UPROBE)
+
 struct trace_event_call {
 	struct list_head	list;
 	struct trace_event_class *class;

commit 098d2164e3441c252eaa28906d45e16b7bf1bd2b
Author: Wang Nan <wangnan0@huawei.com>
Date:   Wed Jul 1 02:13:49 2015 +0000

    bpf: Use correct #ifdef controller for trace_call_bpf()
    
    Commit e1abf2cc8d5d80b41c4419368ec743ccadbb131e ("bpf: Fix the build on
    BPF_SYSCALL=y && !CONFIG_TRACING kernels, make it more configurable")
    updated the building condition of bpf_trace.o from CONFIG_BPF_SYSCALL
    to CONFIG_BPF_EVENTS, but the corresponding #ifdef controller in
    trace_events.h for trace_call_bpf() was not changed. Which, in theory,
    is incorrect.
    
    With current Kconfigs, we can create a .config with CONFIG_BPF_SYSCALL=y
    and CONFIG_BPF_EVENTS=n by unselecting CONFIG_KPROBE_EVENT and
    selecting CONFIG_BPF_SYSCALL. With these options, trace_call_bpf() will
    be defined as an extern function, but if anyone calls it a symbol missing
    error will be triggered since bpf_trace.o was not built.
    
    This patch changes the #ifdef controller for trace_call_bpf() from
    CONFIG_BPF_SYSCALL to CONFIG_BPF_EVENTS. I'll show its correctness:
    
    Before this patch:
    
       BPF_SYSCALL   BPF_EVENTS   trace_call_bpf   bpf_trace.o
       y             y           normal           compiled
       n             n           inline           not compiled
       y             n           normal           not compiled (incorrect)
       n             y          impossible (BPF_EVENTS depends on BPF_SYSCALL)
    
    After this patch:
    
       BPF_SYSCALL   BPF_EVENTS   trace_call_bpf   bpf_trace.o
       y             y           normal           compiled
       n             n           inline           not compiled
       y             n           inline           not compiled (fixed)
       n             y          impossible (BPF_EVENTS depends on BPF_SYSCALL)
    
    So this patch doesn't break anything. QED.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Kaixu Xia <xiakaixu@huawei.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: pi3orama@163.com
    Link: http://lkml.kernel.org/r/1435716878-189507-2-git-send-email-wangnan0@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 1063c850dbab..180dbf8720f9 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -542,7 +542,7 @@ event_trigger_unlock_commit_regs(struct trace_event_file *file,
 		event_triggers_post_call(file, tt);
 }
 
-#ifdef CONFIG_BPF_SYSCALL
+#ifdef CONFIG_BPF_EVENTS
 unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx);
 #else
 static inline unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx)

commit 09a5059aa1a2cbf8c8993e61b013cc83a0dd5833
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 15:21:25 2015 -0400

    tracing: Rename ftrace_trigger_soft_disabled() to trace_trigger_soft_disabled()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The ftrace_trigger_soft_disabled() tests if a
    trace_event is soft disabled (called but not traced), and returns true if
    it is. It has nothing to do with function tracing and should be renamed.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index a46c138b2eea..1063c850dbab 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -426,7 +426,7 @@ extern void event_triggers_post_call(struct trace_event_file *file,
 				     enum event_trigger_type tt);
 
 /**
- * ftrace_trigger_soft_disabled - do triggers and test if soft disabled
+ * trace_trigger_soft_disabled - do triggers and test if soft disabled
  * @file: The file pointer of the event to test
  *
  * If any triggers without filters are attached to this event, they
@@ -435,7 +435,7 @@ extern void event_triggers_post_call(struct trace_event_file *file,
  * otherwise false.
  */
 static inline bool
-ftrace_trigger_soft_disabled(struct trace_event_file *file)
+trace_trigger_soft_disabled(struct trace_event_file *file)
 {
 	unsigned long eflags = file->flags;
 

commit 5d6ad960a71f0b36d95d74ef93285733b9f62f59
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 15:12:33 2015 -0400

    tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The FTRACE_EVENT_FL_* flags are flags to
    do with the trace_event files in the tracefs directory. They are not related
    to function tracing. Rename them to a more descriptive name.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index d4ad58ec684a..a46c138b2eea 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -250,11 +250,11 @@ enum {
  *  FILTERED	  - The event has a filter attached
  *  CAP_ANY	  - Any user can enable for perf
  *  NO_SET_FILTER - Set when filter has error and is to be ignored
- *  IGNORE_ENABLE - For ftrace internal events, do not enable with debugfs file
+ *  IGNORE_ENABLE - For trace internal events, do not enable with debugfs file
  *  WAS_ENABLED   - Set and stays set when an event was ever enabled
  *                    (used for module unloading, if a module event is enabled,
  *                     it is best to clear the buffers that used it).
- *  USE_CALL_FILTER - For ftrace internal events, don't use file filter
+ *  USE_CALL_FILTER - For trace internal events, don't use file filter
  *  TRACEPOINT    - Event is a tracepoint
  *  KPROBE        - Event is a kprobe
  */
@@ -286,7 +286,7 @@ struct trace_event_call {
 	 *   bit 0:		filter_active
 	 *   bit 1:		allow trace by non root (cap any)
 	 *   bit 2:		failed to apply filter
-	 *   bit 3:		ftrace internal event (do not enable)
+	 *   bit 3:		trace internal event (do not enable)
 	 *   bit 4:		Event was enabled by module
 	 *   bit 5:		use call filter rather than file filter
 	 *   bit 6:		Event is a tracepoint
@@ -316,18 +316,18 @@ struct trace_array;
 struct trace_subsystem_dir;
 
 enum {
-	FTRACE_EVENT_FL_ENABLED_BIT,
-	FTRACE_EVENT_FL_RECORDED_CMD_BIT,
-	FTRACE_EVENT_FL_FILTERED_BIT,
-	FTRACE_EVENT_FL_NO_SET_FILTER_BIT,
-	FTRACE_EVENT_FL_SOFT_MODE_BIT,
-	FTRACE_EVENT_FL_SOFT_DISABLED_BIT,
-	FTRACE_EVENT_FL_TRIGGER_MODE_BIT,
-	FTRACE_EVENT_FL_TRIGGER_COND_BIT,
+	EVENT_FILE_FL_ENABLED_BIT,
+	EVENT_FILE_FL_RECORDED_CMD_BIT,
+	EVENT_FILE_FL_FILTERED_BIT,
+	EVENT_FILE_FL_NO_SET_FILTER_BIT,
+	EVENT_FILE_FL_SOFT_MODE_BIT,
+	EVENT_FILE_FL_SOFT_DISABLED_BIT,
+	EVENT_FILE_FL_TRIGGER_MODE_BIT,
+	EVENT_FILE_FL_TRIGGER_COND_BIT,
 };
 
 /*
- * Ftrace event file flags:
+ * Event file flags:
  *  ENABLED	  - The event is enabled
  *  RECORDED_CMD  - The comms should be recorded at sched_switch
  *  FILTERED	  - The event has a filter attached
@@ -339,14 +339,14 @@ enum {
  *  TRIGGER_COND  - When set, one or more triggers has an associated filter
  */
 enum {
-	FTRACE_EVENT_FL_ENABLED		= (1 << FTRACE_EVENT_FL_ENABLED_BIT),
-	FTRACE_EVENT_FL_RECORDED_CMD	= (1 << FTRACE_EVENT_FL_RECORDED_CMD_BIT),
-	FTRACE_EVENT_FL_FILTERED	= (1 << FTRACE_EVENT_FL_FILTERED_BIT),
-	FTRACE_EVENT_FL_NO_SET_FILTER	= (1 << FTRACE_EVENT_FL_NO_SET_FILTER_BIT),
-	FTRACE_EVENT_FL_SOFT_MODE	= (1 << FTRACE_EVENT_FL_SOFT_MODE_BIT),
-	FTRACE_EVENT_FL_SOFT_DISABLED	= (1 << FTRACE_EVENT_FL_SOFT_DISABLED_BIT),
-	FTRACE_EVENT_FL_TRIGGER_MODE	= (1 << FTRACE_EVENT_FL_TRIGGER_MODE_BIT),
-	FTRACE_EVENT_FL_TRIGGER_COND	= (1 << FTRACE_EVENT_FL_TRIGGER_COND_BIT),
+	EVENT_FILE_FL_ENABLED		= (1 << EVENT_FILE_FL_ENABLED_BIT),
+	EVENT_FILE_FL_RECORDED_CMD	= (1 << EVENT_FILE_FL_RECORDED_CMD_BIT),
+	EVENT_FILE_FL_FILTERED		= (1 << EVENT_FILE_FL_FILTERED_BIT),
+	EVENT_FILE_FL_NO_SET_FILTER	= (1 << EVENT_FILE_FL_NO_SET_FILTER_BIT),
+	EVENT_FILE_FL_SOFT_MODE		= (1 << EVENT_FILE_FL_SOFT_MODE_BIT),
+	EVENT_FILE_FL_SOFT_DISABLED	= (1 << EVENT_FILE_FL_SOFT_DISABLED_BIT),
+	EVENT_FILE_FL_TRIGGER_MODE	= (1 << EVENT_FILE_FL_TRIGGER_MODE_BIT),
+	EVENT_FILE_FL_TRIGGER_COND	= (1 << EVENT_FILE_FL_TRIGGER_COND_BIT),
 };
 
 struct trace_event_file {
@@ -439,10 +439,10 @@ ftrace_trigger_soft_disabled(struct trace_event_file *file)
 {
 	unsigned long eflags = file->flags;
 
-	if (!(eflags & FTRACE_EVENT_FL_TRIGGER_COND)) {
-		if (eflags & FTRACE_EVENT_FL_TRIGGER_MODE)
+	if (!(eflags & EVENT_FILE_FL_TRIGGER_COND)) {
+		if (eflags & EVENT_FILE_FL_TRIGGER_MODE)
 			event_triggers_call(file, NULL);
-		if (eflags & FTRACE_EVENT_FL_SOFT_DISABLED)
+		if (eflags & EVENT_FILE_FL_SOFT_DISABLED)
 			return true;
 	}
 	return false;
@@ -470,10 +470,10 @@ __event_trigger_test_discard(struct trace_event_file *file,
 {
 	unsigned long eflags = file->flags;
 
-	if (eflags & FTRACE_EVENT_FL_TRIGGER_COND)
+	if (eflags & EVENT_FILE_FL_TRIGGER_COND)
 		*tt = event_triggers_call(file, entry);
 
-	if (test_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags))
+	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags))
 		ring_buffer_discard_commit(buffer, event);
 	else if (!filter_check_discard(file, entry, buffer, event))
 		return false;

commit 7967b3e0c40ff72fb2cf44d3b50e2cb388ef6c67
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 14:59:40 2015 -0400

    tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_subsystem_dir holds
    the information about trace event subsystems. It should not be named
    ftrace, rename it to trace_subsystem_dir.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 15617798849c..d4ad58ec684a 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -313,7 +313,7 @@ trace_event_name(struct trace_event_call *call)
 }
 
 struct trace_array;
-struct ftrace_subsystem_dir;
+struct trace_subsystem_dir;
 
 enum {
 	FTRACE_EVENT_FL_ENABLED_BIT,
@@ -355,7 +355,7 @@ struct trace_event_file {
 	struct event_filter		*filter;
 	struct dentry			*dir;
 	struct trace_array		*tr;
-	struct ftrace_subsystem_dir	*system;
+	struct trace_subsystem_dir	*system;
 	struct list_head		triggers;
 
 	/*

commit 687fcc4aee4567df14e31e82d6993418b826f408
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 14:20:14 2015 -0400

    tracing: Rename ftrace_event_name() to trace_event_name()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. ftrace_event_name() returns the name of
    an event tracepoint, has nothing to do with function tracing. Rename it
    to trace_event_name().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 6f28464be418..15617798849c 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -304,7 +304,7 @@ struct trace_event_call {
 };
 
 static inline const char *
-ftrace_event_name(struct trace_event_call *call)
+trace_event_name(struct trace_event_call *call)
 {
 	if (call->flags & TRACE_EVENT_FL_TRACEPOINT)
 		return call->tp ? call->tp->name : NULL;

commit 609a74045238c303bbe9396775eacf5bac1f51cc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 13:44:36 2015 -0400

    tracing: Rename FTRACE_MAX_EVENT to TRACE_EVENT_TYPE_MAX
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. Rename the max trace_event type size to
    something more descriptive and appropriate.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 12ca46322a94..6f28464be418 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -68,7 +68,7 @@ struct trace_entry {
 	int			pid;
 };
 
-#define FTRACE_MAX_EVENT						\
+#define TRACE_EVENT_TYPE_MAX						\
 	((1 << (sizeof(((struct trace_entry *)0)->type) * 8)) - 1)
 
 /*

commit 892c505aac2bdded3c8ec2ec27abc6d74fd210f5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 14:18:11 2015 -0400

    tracing: Rename ftrace_output functions to trace_output
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The ftrace_output_*() and ftrace_raw_output_*()
    functions represent the trace_event code. Rename them to just trace_output
    or trace_raw_output.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index a1fa8ebaf684..12ca46322a94 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -52,8 +52,8 @@ const char *trace_print_array_seq(struct trace_seq *p,
 struct trace_iterator;
 struct trace_event;
 
-int ftrace_raw_output_prep(struct trace_iterator *iter,
-			   struct trace_event *event);
+int trace_raw_output_prep(struct trace_iterator *iter,
+			  struct trace_event *event);
 
 /*
  * The trace entry - the most basic unit of tracing. This is what
@@ -183,7 +183,7 @@ void trace_current_buffer_discard_commit(struct ring_buffer *buffer,
 
 void tracing_record_cmdline(struct task_struct *tsk);
 
-int ftrace_output_call(struct trace_iterator *iter, char *name, char *fmt, ...);
+int trace_output_call(struct trace_iterator *iter, char *name, char *fmt, ...);
 
 struct event_filter;
 

commit 3f795dcfc7364cd811c3f6f03d115fcefbbdc1ca
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 13:18:46 2015 -0400

    tracing: Rename ftrace_event_buffer to trace_event_buffer.
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The ftrace_event_buffer functions and data
    structures are for trace_events and not for function hooks. Rename them
    to trace_event_buffer*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index d10ab04a17b2..a1fa8ebaf684 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -219,7 +219,7 @@ struct trace_event_class {
 extern int trace_event_reg(struct trace_event_call *event,
 			    enum trace_reg type, void *data);
 
-struct ftrace_event_buffer {
+struct trace_event_buffer {
 	struct ring_buffer		*buffer;
 	struct ring_buffer_event	*event;
 	struct trace_event_file		*trace_file;
@@ -228,11 +228,11 @@ struct ftrace_event_buffer {
 	int				pc;
 };
 
-void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
+void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 				  struct trace_event_file *trace_file,
 				  unsigned long len);
 
-void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer);
+void trace_event_buffer_commit(struct trace_event_buffer *fbuffer);
 
 enum {
 	TRACE_EVENT_FL_FILTERED_BIT,

commit 2425bcb9240f8c97d793cb31c8e8d8d0a843fa29
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 11:45:27 2015 -0400

    tracing: Rename ftrace_event_{call,class} to trace_event_{call,class}
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structures ftrace_event_call and
    ftrace_event_class have nothing to do with the function hooks, and are
    really trace_event structures. Rename ftrace_event_* to trace_event_*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index ae19233c7dd8..d10ab04a17b2 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -200,23 +200,23 @@ enum trace_reg {
 #endif
 };
 
-struct ftrace_event_call;
+struct trace_event_call;
 
-struct ftrace_event_class {
+struct trace_event_class {
 	const char		*system;
 	void			*probe;
 #ifdef CONFIG_PERF_EVENTS
 	void			*perf_probe;
 #endif
-	int			(*reg)(struct ftrace_event_call *event,
+	int			(*reg)(struct trace_event_call *event,
 				       enum trace_reg type, void *data);
-	int			(*define_fields)(struct ftrace_event_call *);
-	struct list_head	*(*get_fields)(struct ftrace_event_call *);
+	int			(*define_fields)(struct trace_event_call *);
+	struct list_head	*(*get_fields)(struct trace_event_call *);
 	struct list_head	fields;
-	int			(*raw_init)(struct ftrace_event_call *);
+	int			(*raw_init)(struct trace_event_call *);
 };
 
-extern int trace_event_reg(struct ftrace_event_call *event,
+extern int trace_event_reg(struct trace_event_call *event,
 			    enum trace_reg type, void *data);
 
 struct ftrace_event_buffer {
@@ -269,9 +269,9 @@ enum {
 	TRACE_EVENT_FL_KPROBE		= (1 << TRACE_EVENT_FL_KPROBE_BIT),
 };
 
-struct ftrace_event_call {
+struct trace_event_call {
 	struct list_head	list;
-	struct ftrace_event_class *class;
+	struct trace_event_class *class;
 	union {
 		char			*name;
 		/* Set TRACE_EVENT_FL_TRACEPOINT flag when using "tp" */
@@ -298,13 +298,13 @@ struct ftrace_event_call {
 	struct hlist_head __percpu	*perf_events;
 	struct bpf_prog			*prog;
 
-	int	(*perf_perm)(struct ftrace_event_call *,
+	int	(*perf_perm)(struct trace_event_call *,
 			     struct perf_event *);
 #endif
 };
 
 static inline const char *
-ftrace_event_name(struct ftrace_event_call *call)
+ftrace_event_name(struct trace_event_call *call)
 {
 	if (call->flags & TRACE_EVENT_FL_TRACEPOINT)
 		return call->tp ? call->tp->name : NULL;
@@ -351,7 +351,7 @@ enum {
 
 struct trace_event_file {
 	struct list_head		list;
-	struct ftrace_event_call	*event_call;
+	struct trace_event_call		*event_call;
 	struct event_filter		*filter;
 	struct dentry			*dir;
 	struct trace_array		*tr;
@@ -388,7 +388,7 @@ struct trace_event_file {
 	early_initcall(trace_init_flags_##name);
 
 #define __TRACE_EVENT_PERF_PERM(name, expr...)				\
-	static int perf_perm_##name(struct ftrace_event_call *tp_event, \
+	static int perf_perm_##name(struct trace_event_call *tp_event, \
 				    struct perf_event *p_event)		\
 	{								\
 		return ({ expr; });					\
@@ -417,7 +417,7 @@ extern int filter_match_preds(struct event_filter *filter, void *rec);
 extern int filter_check_discard(struct trace_event_file *file, void *rec,
 				struct ring_buffer *buffer,
 				struct ring_buffer_event *event);
-extern int call_filter_check_discard(struct ftrace_event_call *call, void *rec,
+extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event);
 extern enum event_trigger_type event_triggers_call(struct trace_event_file *file,
@@ -559,12 +559,12 @@ enum {
 	FILTER_TRACE_FN,
 };
 
-extern int trace_event_raw_init(struct ftrace_event_call *call);
-extern int trace_define_field(struct ftrace_event_call *call, const char *type,
+extern int trace_event_raw_init(struct trace_event_call *call);
+extern int trace_define_field(struct trace_event_call *call, const char *type,
 			      const char *name, int offset, int size,
 			      int is_signed, int filter_type);
-extern int trace_add_event_call(struct ftrace_event_call *call);
-extern int trace_remove_event_call(struct ftrace_event_call *call);
+extern int trace_add_event_call(struct trace_event_call *call);
+extern int trace_remove_event_call(struct trace_event_call *call);
 
 #define is_signed_type(type)	(((type)(-1)) < (type)1)
 
@@ -613,4 +613,4 @@ perf_trace_buf_submit(void *raw_data, int size, int rctx, u64 addr,
 }
 #endif
 
-#endif /* _LINUX_FTRACE_EVENT_H */
+#endif /* _LINUX_TRACE_EVENT_H */

commit 7f1d2f8210195c8c309d424a77dbf06a6d2186f4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 10:09:53 2015 -0400

    tracing: Rename ftrace_event_file to trace_event_file
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_event_file is really
    about trace events and not "ftrace". Rename it to trace_event_file.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 99924c07a042..ae19233c7dd8 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -157,11 +157,11 @@ static inline enum print_line_t trace_handle_return(struct trace_seq *s)
 void tracing_generic_entry_update(struct trace_entry *entry,
 				  unsigned long flags,
 				  int pc);
-struct ftrace_event_file;
+struct trace_event_file;
 
 struct ring_buffer_event *
 trace_event_buffer_lock_reserve(struct ring_buffer **current_buffer,
-				struct ftrace_event_file *ftrace_file,
+				struct trace_event_file *trace_file,
 				int type, unsigned long len,
 				unsigned long flags, int pc);
 struct ring_buffer_event *
@@ -222,14 +222,14 @@ extern int trace_event_reg(struct ftrace_event_call *event,
 struct ftrace_event_buffer {
 	struct ring_buffer		*buffer;
 	struct ring_buffer_event	*event;
-	struct ftrace_event_file	*ftrace_file;
+	struct trace_event_file		*trace_file;
 	void				*entry;
 	unsigned long			flags;
 	int				pc;
 };
 
 void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
-				  struct ftrace_event_file *ftrace_file,
+				  struct trace_event_file *trace_file,
 				  unsigned long len);
 
 void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer);
@@ -349,7 +349,7 @@ enum {
 	FTRACE_EVENT_FL_TRIGGER_COND	= (1 << FTRACE_EVENT_FL_TRIGGER_COND_BIT),
 };
 
-struct ftrace_event_file {
+struct trace_event_file {
 	struct list_head		list;
 	struct ftrace_event_call	*event_call;
 	struct event_filter		*filter;
@@ -414,15 +414,15 @@ enum event_trigger_type {
 
 extern int filter_match_preds(struct event_filter *filter, void *rec);
 
-extern int filter_check_discard(struct ftrace_event_file *file, void *rec,
+extern int filter_check_discard(struct trace_event_file *file, void *rec,
 				struct ring_buffer *buffer,
 				struct ring_buffer_event *event);
 extern int call_filter_check_discard(struct ftrace_event_call *call, void *rec,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event);
-extern enum event_trigger_type event_triggers_call(struct ftrace_event_file *file,
+extern enum event_trigger_type event_triggers_call(struct trace_event_file *file,
 						   void *rec);
-extern void event_triggers_post_call(struct ftrace_event_file *file,
+extern void event_triggers_post_call(struct trace_event_file *file,
 				     enum event_trigger_type tt);
 
 /**
@@ -435,7 +435,7 @@ extern void event_triggers_post_call(struct ftrace_event_file *file,
  * otherwise false.
  */
 static inline bool
-ftrace_trigger_soft_disabled(struct ftrace_event_file *file)
+ftrace_trigger_soft_disabled(struct trace_event_file *file)
 {
 	unsigned long eflags = file->flags;
 
@@ -462,7 +462,7 @@ ftrace_trigger_soft_disabled(struct ftrace_event_file *file)
  * Returns true if the event is discarded, false otherwise.
  */
 static inline bool
-__event_trigger_test_discard(struct ftrace_event_file *file,
+__event_trigger_test_discard(struct trace_event_file *file,
 			     struct ring_buffer *buffer,
 			     struct ring_buffer_event *event,
 			     void *entry,
@@ -495,7 +495,7 @@ __event_trigger_test_discard(struct ftrace_event_file *file,
  * if the event is soft disabled and should be discarded.
  */
 static inline void
-event_trigger_unlock_commit(struct ftrace_event_file *file,
+event_trigger_unlock_commit(struct trace_event_file *file,
 			    struct ring_buffer *buffer,
 			    struct ring_buffer_event *event,
 			    void *entry, unsigned long irq_flags, int pc)
@@ -526,7 +526,7 @@ event_trigger_unlock_commit(struct ftrace_event_file *file,
  * trace_buffer_unlock_commit_regs() instead of trace_buffer_unlock_commit().
  */
 static inline void
-event_trigger_unlock_commit_regs(struct ftrace_event_file *file,
+event_trigger_unlock_commit_regs(struct trace_event_file *file,
 				 struct ring_buffer *buffer,
 				 struct ring_buffer_event *event,
 				 void *entry, unsigned long irq_flags, int pc,

commit 9023c930902fbbcf0cebf6110828700f792989a4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 09:39:12 2015 -0400

    tracing: Rename (un)register_ftrace_event() to (un)register_trace_event()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The functions (un)register_ftrace_event() is
    really about trace_events, and the name should be register_trace_event()
    instead.
    
    Also renamed ftrace_event_reg() to trace_event_reg() for the same reason.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index 29627cbafdea..99924c07a042 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -132,8 +132,8 @@ struct trace_event {
 	struct trace_event_functions	*funcs;
 };
 
-extern int register_ftrace_event(struct trace_event *event);
-extern int unregister_ftrace_event(struct trace_event *event);
+extern int register_trace_event(struct trace_event *event);
+extern int unregister_trace_event(struct trace_event *event);
 
 /* Return values for print_line callback */
 enum print_line_t {
@@ -216,7 +216,7 @@ struct ftrace_event_class {
 	int			(*raw_init)(struct ftrace_event_call *);
 };
 
-extern int ftrace_event_reg(struct ftrace_event_call *event,
+extern int trace_event_reg(struct ftrace_event_call *event,
 			    enum trace_reg type, void *data);
 
 struct ftrace_event_buffer {

commit 645df987f7c1740bb1ba783ab907001720a20cf7
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon May 4 18:12:44 2015 -0400

    tracing: Rename ftrace_print_*() functions ta trace_print_*()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The functions ftrace_print_*() are not part of
    the function infrastructure, and the names can be confusing. Rename them
    to be trace_print_*().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index f8465d65f3c7..29627cbafdea 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -1,6 +1,6 @@
 
-#ifndef _LINUX_FTRACE_EVENT_H
-#define _LINUX_FTRACE_EVENT_H
+#ifndef _LINUX_TRACE_EVENT_H
+#define _LINUX_TRACE_EVENT_H
 
 #include <linux/ring_buffer.h>
 #include <linux/trace_seq.h>
@@ -25,27 +25,27 @@ struct trace_print_flags_u64 {
 	const char		*name;
 };
 
-const char *ftrace_print_flags_seq(struct trace_seq *p, const char *delim,
-				   unsigned long flags,
-				   const struct trace_print_flags *flag_array);
+const char *trace_print_flags_seq(struct trace_seq *p, const char *delim,
+				  unsigned long flags,
+				  const struct trace_print_flags *flag_array);
 
-const char *ftrace_print_symbols_seq(struct trace_seq *p, unsigned long val,
-				     const struct trace_print_flags *symbol_array);
+const char *trace_print_symbols_seq(struct trace_seq *p, unsigned long val,
+				    const struct trace_print_flags *symbol_array);
 
 #if BITS_PER_LONG == 32
-const char *ftrace_print_symbols_seq_u64(struct trace_seq *p,
-					 unsigned long long val,
-					 const struct trace_print_flags_u64
+const char *trace_print_symbols_seq_u64(struct trace_seq *p,
+					unsigned long long val,
+					const struct trace_print_flags_u64
 								 *symbol_array);
 #endif
 
-const char *ftrace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,
-				     unsigned int bitmask_size);
+const char *trace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,
+				    unsigned int bitmask_size);
 
-const char *ftrace_print_hex_seq(struct trace_seq *p,
-				 const unsigned char *buf, int len);
+const char *trace_print_hex_seq(struct trace_seq *p,
+				const unsigned char *buf, int len);
 
-const char *ftrace_print_array_seq(struct trace_seq *p,
+const char *trace_print_array_seq(struct trace_seq *p,
 				   const void *buf, int count,
 				   size_t el_size);
 

commit af658dca221207174fc0a7bcdcd4cff7c589fdd8
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 29 14:36:05 2015 -0400

    tracing: Rename ftrace_event.h to trace_events.h
    
    The term "ftrace" is really the infrastructure of the function hooks,
    and not the trace events. Rename ftrace_event.h to trace_events.h to
    represent the trace_event infrastructure and decouple the term ftrace
    from it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
new file mode 100644
index 000000000000..f8465d65f3c7
--- /dev/null
+++ b/include/linux/trace_events.h
@@ -0,0 +1,616 @@
+
+#ifndef _LINUX_FTRACE_EVENT_H
+#define _LINUX_FTRACE_EVENT_H
+
+#include <linux/ring_buffer.h>
+#include <linux/trace_seq.h>
+#include <linux/percpu.h>
+#include <linux/hardirq.h>
+#include <linux/perf_event.h>
+#include <linux/tracepoint.h>
+
+struct trace_array;
+struct trace_buffer;
+struct tracer;
+struct dentry;
+struct bpf_prog;
+
+struct trace_print_flags {
+	unsigned long		mask;
+	const char		*name;
+};
+
+struct trace_print_flags_u64 {
+	unsigned long long	mask;
+	const char		*name;
+};
+
+const char *ftrace_print_flags_seq(struct trace_seq *p, const char *delim,
+				   unsigned long flags,
+				   const struct trace_print_flags *flag_array);
+
+const char *ftrace_print_symbols_seq(struct trace_seq *p, unsigned long val,
+				     const struct trace_print_flags *symbol_array);
+
+#if BITS_PER_LONG == 32
+const char *ftrace_print_symbols_seq_u64(struct trace_seq *p,
+					 unsigned long long val,
+					 const struct trace_print_flags_u64
+								 *symbol_array);
+#endif
+
+const char *ftrace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,
+				     unsigned int bitmask_size);
+
+const char *ftrace_print_hex_seq(struct trace_seq *p,
+				 const unsigned char *buf, int len);
+
+const char *ftrace_print_array_seq(struct trace_seq *p,
+				   const void *buf, int count,
+				   size_t el_size);
+
+struct trace_iterator;
+struct trace_event;
+
+int ftrace_raw_output_prep(struct trace_iterator *iter,
+			   struct trace_event *event);
+
+/*
+ * The trace entry - the most basic unit of tracing. This is what
+ * is printed in the end as a single line in the trace output, such as:
+ *
+ *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
+ */
+struct trace_entry {
+	unsigned short		type;
+	unsigned char		flags;
+	unsigned char		preempt_count;
+	int			pid;
+};
+
+#define FTRACE_MAX_EVENT						\
+	((1 << (sizeof(((struct trace_entry *)0)->type) * 8)) - 1)
+
+/*
+ * Trace iterator - used by printout routines who present trace
+ * results to users and which routines might sleep, etc:
+ */
+struct trace_iterator {
+	struct trace_array	*tr;
+	struct tracer		*trace;
+	struct trace_buffer	*trace_buffer;
+	void			*private;
+	int			cpu_file;
+	struct mutex		mutex;
+	struct ring_buffer_iter	**buffer_iter;
+	unsigned long		iter_flags;
+
+	/* trace_seq for __print_flags() and __print_symbolic() etc. */
+	struct trace_seq	tmp_seq;
+
+	cpumask_var_t		started;
+
+	/* it's true when current open file is snapshot */
+	bool			snapshot;
+
+	/* The below is zeroed out in pipe_read */
+	struct trace_seq	seq;
+	struct trace_entry	*ent;
+	unsigned long		lost_events;
+	int			leftover;
+	int			ent_size;
+	int			cpu;
+	u64			ts;
+
+	loff_t			pos;
+	long			idx;
+
+	/* All new field here will be zeroed out in pipe_read */
+};
+
+enum trace_iter_flags {
+	TRACE_FILE_LAT_FMT	= 1,
+	TRACE_FILE_ANNOTATE	= 2,
+	TRACE_FILE_TIME_IN_NS	= 4,
+};
+
+
+typedef enum print_line_t (*trace_print_func)(struct trace_iterator *iter,
+				      int flags, struct trace_event *event);
+
+struct trace_event_functions {
+	trace_print_func	trace;
+	trace_print_func	raw;
+	trace_print_func	hex;
+	trace_print_func	binary;
+};
+
+struct trace_event {
+	struct hlist_node		node;
+	struct list_head		list;
+	int				type;
+	struct trace_event_functions	*funcs;
+};
+
+extern int register_ftrace_event(struct trace_event *event);
+extern int unregister_ftrace_event(struct trace_event *event);
+
+/* Return values for print_line callback */
+enum print_line_t {
+	TRACE_TYPE_PARTIAL_LINE	= 0,	/* Retry after flushing the seq */
+	TRACE_TYPE_HANDLED	= 1,
+	TRACE_TYPE_UNHANDLED	= 2,	/* Relay to other output functions */
+	TRACE_TYPE_NO_CONSUME	= 3	/* Handled but ask to not consume */
+};
+
+/*
+ * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq
+ * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function
+ * simplifies those functions and keeps them in sync.
+ */
+static inline enum print_line_t trace_handle_return(struct trace_seq *s)
+{
+	return trace_seq_has_overflowed(s) ?
+		TRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;
+}
+
+void tracing_generic_entry_update(struct trace_entry *entry,
+				  unsigned long flags,
+				  int pc);
+struct ftrace_event_file;
+
+struct ring_buffer_event *
+trace_event_buffer_lock_reserve(struct ring_buffer **current_buffer,
+				struct ftrace_event_file *ftrace_file,
+				int type, unsigned long len,
+				unsigned long flags, int pc);
+struct ring_buffer_event *
+trace_current_buffer_lock_reserve(struct ring_buffer **current_buffer,
+				  int type, unsigned long len,
+				  unsigned long flags, int pc);
+void trace_current_buffer_unlock_commit(struct ring_buffer *buffer,
+					struct ring_buffer_event *event,
+					unsigned long flags, int pc);
+void trace_buffer_unlock_commit(struct ring_buffer *buffer,
+				struct ring_buffer_event *event,
+				unsigned long flags, int pc);
+void trace_buffer_unlock_commit_regs(struct ring_buffer *buffer,
+				     struct ring_buffer_event *event,
+				     unsigned long flags, int pc,
+				     struct pt_regs *regs);
+void trace_current_buffer_discard_commit(struct ring_buffer *buffer,
+					 struct ring_buffer_event *event);
+
+void tracing_record_cmdline(struct task_struct *tsk);
+
+int ftrace_output_call(struct trace_iterator *iter, char *name, char *fmt, ...);
+
+struct event_filter;
+
+enum trace_reg {
+	TRACE_REG_REGISTER,
+	TRACE_REG_UNREGISTER,
+#ifdef CONFIG_PERF_EVENTS
+	TRACE_REG_PERF_REGISTER,
+	TRACE_REG_PERF_UNREGISTER,
+	TRACE_REG_PERF_OPEN,
+	TRACE_REG_PERF_CLOSE,
+	TRACE_REG_PERF_ADD,
+	TRACE_REG_PERF_DEL,
+#endif
+};
+
+struct ftrace_event_call;
+
+struct ftrace_event_class {
+	const char		*system;
+	void			*probe;
+#ifdef CONFIG_PERF_EVENTS
+	void			*perf_probe;
+#endif
+	int			(*reg)(struct ftrace_event_call *event,
+				       enum trace_reg type, void *data);
+	int			(*define_fields)(struct ftrace_event_call *);
+	struct list_head	*(*get_fields)(struct ftrace_event_call *);
+	struct list_head	fields;
+	int			(*raw_init)(struct ftrace_event_call *);
+};
+
+extern int ftrace_event_reg(struct ftrace_event_call *event,
+			    enum trace_reg type, void *data);
+
+struct ftrace_event_buffer {
+	struct ring_buffer		*buffer;
+	struct ring_buffer_event	*event;
+	struct ftrace_event_file	*ftrace_file;
+	void				*entry;
+	unsigned long			flags;
+	int				pc;
+};
+
+void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
+				  struct ftrace_event_file *ftrace_file,
+				  unsigned long len);
+
+void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer);
+
+enum {
+	TRACE_EVENT_FL_FILTERED_BIT,
+	TRACE_EVENT_FL_CAP_ANY_BIT,
+	TRACE_EVENT_FL_NO_SET_FILTER_BIT,
+	TRACE_EVENT_FL_IGNORE_ENABLE_BIT,
+	TRACE_EVENT_FL_WAS_ENABLED_BIT,
+	TRACE_EVENT_FL_USE_CALL_FILTER_BIT,
+	TRACE_EVENT_FL_TRACEPOINT_BIT,
+	TRACE_EVENT_FL_KPROBE_BIT,
+};
+
+/*
+ * Event flags:
+ *  FILTERED	  - The event has a filter attached
+ *  CAP_ANY	  - Any user can enable for perf
+ *  NO_SET_FILTER - Set when filter has error and is to be ignored
+ *  IGNORE_ENABLE - For ftrace internal events, do not enable with debugfs file
+ *  WAS_ENABLED   - Set and stays set when an event was ever enabled
+ *                    (used for module unloading, if a module event is enabled,
+ *                     it is best to clear the buffers that used it).
+ *  USE_CALL_FILTER - For ftrace internal events, don't use file filter
+ *  TRACEPOINT    - Event is a tracepoint
+ *  KPROBE        - Event is a kprobe
+ */
+enum {
+	TRACE_EVENT_FL_FILTERED		= (1 << TRACE_EVENT_FL_FILTERED_BIT),
+	TRACE_EVENT_FL_CAP_ANY		= (1 << TRACE_EVENT_FL_CAP_ANY_BIT),
+	TRACE_EVENT_FL_NO_SET_FILTER	= (1 << TRACE_EVENT_FL_NO_SET_FILTER_BIT),
+	TRACE_EVENT_FL_IGNORE_ENABLE	= (1 << TRACE_EVENT_FL_IGNORE_ENABLE_BIT),
+	TRACE_EVENT_FL_WAS_ENABLED	= (1 << TRACE_EVENT_FL_WAS_ENABLED_BIT),
+	TRACE_EVENT_FL_USE_CALL_FILTER	= (1 << TRACE_EVENT_FL_USE_CALL_FILTER_BIT),
+	TRACE_EVENT_FL_TRACEPOINT	= (1 << TRACE_EVENT_FL_TRACEPOINT_BIT),
+	TRACE_EVENT_FL_KPROBE		= (1 << TRACE_EVENT_FL_KPROBE_BIT),
+};
+
+struct ftrace_event_call {
+	struct list_head	list;
+	struct ftrace_event_class *class;
+	union {
+		char			*name;
+		/* Set TRACE_EVENT_FL_TRACEPOINT flag when using "tp" */
+		struct tracepoint	*tp;
+	};
+	struct trace_event	event;
+	char			*print_fmt;
+	struct event_filter	*filter;
+	void			*mod;
+	void			*data;
+	/*
+	 *   bit 0:		filter_active
+	 *   bit 1:		allow trace by non root (cap any)
+	 *   bit 2:		failed to apply filter
+	 *   bit 3:		ftrace internal event (do not enable)
+	 *   bit 4:		Event was enabled by module
+	 *   bit 5:		use call filter rather than file filter
+	 *   bit 6:		Event is a tracepoint
+	 */
+	int			flags; /* static flags of different events */
+
+#ifdef CONFIG_PERF_EVENTS
+	int				perf_refcount;
+	struct hlist_head __percpu	*perf_events;
+	struct bpf_prog			*prog;
+
+	int	(*perf_perm)(struct ftrace_event_call *,
+			     struct perf_event *);
+#endif
+};
+
+static inline const char *
+ftrace_event_name(struct ftrace_event_call *call)
+{
+	if (call->flags & TRACE_EVENT_FL_TRACEPOINT)
+		return call->tp ? call->tp->name : NULL;
+	else
+		return call->name;
+}
+
+struct trace_array;
+struct ftrace_subsystem_dir;
+
+enum {
+	FTRACE_EVENT_FL_ENABLED_BIT,
+	FTRACE_EVENT_FL_RECORDED_CMD_BIT,
+	FTRACE_EVENT_FL_FILTERED_BIT,
+	FTRACE_EVENT_FL_NO_SET_FILTER_BIT,
+	FTRACE_EVENT_FL_SOFT_MODE_BIT,
+	FTRACE_EVENT_FL_SOFT_DISABLED_BIT,
+	FTRACE_EVENT_FL_TRIGGER_MODE_BIT,
+	FTRACE_EVENT_FL_TRIGGER_COND_BIT,
+};
+
+/*
+ * Ftrace event file flags:
+ *  ENABLED	  - The event is enabled
+ *  RECORDED_CMD  - The comms should be recorded at sched_switch
+ *  FILTERED	  - The event has a filter attached
+ *  NO_SET_FILTER - Set when filter has error and is to be ignored
+ *  SOFT_MODE     - The event is enabled/disabled by SOFT_DISABLED
+ *  SOFT_DISABLED - When set, do not trace the event (even though its
+ *                   tracepoint may be enabled)
+ *  TRIGGER_MODE  - When set, invoke the triggers associated with the event
+ *  TRIGGER_COND  - When set, one or more triggers has an associated filter
+ */
+enum {
+	FTRACE_EVENT_FL_ENABLED		= (1 << FTRACE_EVENT_FL_ENABLED_BIT),
+	FTRACE_EVENT_FL_RECORDED_CMD	= (1 << FTRACE_EVENT_FL_RECORDED_CMD_BIT),
+	FTRACE_EVENT_FL_FILTERED	= (1 << FTRACE_EVENT_FL_FILTERED_BIT),
+	FTRACE_EVENT_FL_NO_SET_FILTER	= (1 << FTRACE_EVENT_FL_NO_SET_FILTER_BIT),
+	FTRACE_EVENT_FL_SOFT_MODE	= (1 << FTRACE_EVENT_FL_SOFT_MODE_BIT),
+	FTRACE_EVENT_FL_SOFT_DISABLED	= (1 << FTRACE_EVENT_FL_SOFT_DISABLED_BIT),
+	FTRACE_EVENT_FL_TRIGGER_MODE	= (1 << FTRACE_EVENT_FL_TRIGGER_MODE_BIT),
+	FTRACE_EVENT_FL_TRIGGER_COND	= (1 << FTRACE_EVENT_FL_TRIGGER_COND_BIT),
+};
+
+struct ftrace_event_file {
+	struct list_head		list;
+	struct ftrace_event_call	*event_call;
+	struct event_filter		*filter;
+	struct dentry			*dir;
+	struct trace_array		*tr;
+	struct ftrace_subsystem_dir	*system;
+	struct list_head		triggers;
+
+	/*
+	 * 32 bit flags:
+	 *   bit 0:		enabled
+	 *   bit 1:		enabled cmd record
+	 *   bit 2:		enable/disable with the soft disable bit
+	 *   bit 3:		soft disabled
+	 *   bit 4:		trigger enabled
+	 *
+	 * Note: The bits must be set atomically to prevent races
+	 * from other writers. Reads of flags do not need to be in
+	 * sync as they occur in critical sections. But the way flags
+	 * is currently used, these changes do not affect the code
+	 * except that when a change is made, it may have a slight
+	 * delay in propagating the changes to other CPUs due to
+	 * caching and such. Which is mostly OK ;-)
+	 */
+	unsigned long		flags;
+	atomic_t		sm_ref;	/* soft-mode reference counter */
+	atomic_t		tm_ref;	/* trigger-mode reference counter */
+};
+
+#define __TRACE_EVENT_FLAGS(name, value)				\
+	static int __init trace_init_flags_##name(void)			\
+	{								\
+		event_##name.flags |= value;				\
+		return 0;						\
+	}								\
+	early_initcall(trace_init_flags_##name);
+
+#define __TRACE_EVENT_PERF_PERM(name, expr...)				\
+	static int perf_perm_##name(struct ftrace_event_call *tp_event, \
+				    struct perf_event *p_event)		\
+	{								\
+		return ({ expr; });					\
+	}								\
+	static int __init trace_init_perf_perm_##name(void)		\
+	{								\
+		event_##name.perf_perm = &perf_perm_##name;		\
+		return 0;						\
+	}								\
+	early_initcall(trace_init_perf_perm_##name);
+
+#define PERF_MAX_TRACE_SIZE	2048
+
+#define MAX_FILTER_STR_VAL	256	/* Should handle KSYM_SYMBOL_LEN */
+
+enum event_trigger_type {
+	ETT_NONE		= (0),
+	ETT_TRACE_ONOFF		= (1 << 0),
+	ETT_SNAPSHOT		= (1 << 1),
+	ETT_STACKTRACE		= (1 << 2),
+	ETT_EVENT_ENABLE	= (1 << 3),
+};
+
+extern int filter_match_preds(struct event_filter *filter, void *rec);
+
+extern int filter_check_discard(struct ftrace_event_file *file, void *rec,
+				struct ring_buffer *buffer,
+				struct ring_buffer_event *event);
+extern int call_filter_check_discard(struct ftrace_event_call *call, void *rec,
+				     struct ring_buffer *buffer,
+				     struct ring_buffer_event *event);
+extern enum event_trigger_type event_triggers_call(struct ftrace_event_file *file,
+						   void *rec);
+extern void event_triggers_post_call(struct ftrace_event_file *file,
+				     enum event_trigger_type tt);
+
+/**
+ * ftrace_trigger_soft_disabled - do triggers and test if soft disabled
+ * @file: The file pointer of the event to test
+ *
+ * If any triggers without filters are attached to this event, they
+ * will be called here. If the event is soft disabled and has no
+ * triggers that require testing the fields, it will return true,
+ * otherwise false.
+ */
+static inline bool
+ftrace_trigger_soft_disabled(struct ftrace_event_file *file)
+{
+	unsigned long eflags = file->flags;
+
+	if (!(eflags & FTRACE_EVENT_FL_TRIGGER_COND)) {
+		if (eflags & FTRACE_EVENT_FL_TRIGGER_MODE)
+			event_triggers_call(file, NULL);
+		if (eflags & FTRACE_EVENT_FL_SOFT_DISABLED)
+			return true;
+	}
+	return false;
+}
+
+/*
+ * Helper function for event_trigger_unlock_commit{_regs}().
+ * If there are event triggers attached to this event that requires
+ * filtering against its fields, then they wil be called as the
+ * entry already holds the field information of the current event.
+ *
+ * It also checks if the event should be discarded or not.
+ * It is to be discarded if the event is soft disabled and the
+ * event was only recorded to process triggers, or if the event
+ * filter is active and this event did not match the filters.
+ *
+ * Returns true if the event is discarded, false otherwise.
+ */
+static inline bool
+__event_trigger_test_discard(struct ftrace_event_file *file,
+			     struct ring_buffer *buffer,
+			     struct ring_buffer_event *event,
+			     void *entry,
+			     enum event_trigger_type *tt)
+{
+	unsigned long eflags = file->flags;
+
+	if (eflags & FTRACE_EVENT_FL_TRIGGER_COND)
+		*tt = event_triggers_call(file, entry);
+
+	if (test_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags))
+		ring_buffer_discard_commit(buffer, event);
+	else if (!filter_check_discard(file, entry, buffer, event))
+		return false;
+
+	return true;
+}
+
+/**
+ * event_trigger_unlock_commit - handle triggers and finish event commit
+ * @file: The file pointer assoctiated to the event
+ * @buffer: The ring buffer that the event is being written to
+ * @event: The event meta data in the ring buffer
+ * @entry: The event itself
+ * @irq_flags: The state of the interrupts at the start of the event
+ * @pc: The state of the preempt count at the start of the event.
+ *
+ * This is a helper function to handle triggers that require data
+ * from the event itself. It also tests the event against filters and
+ * if the event is soft disabled and should be discarded.
+ */
+static inline void
+event_trigger_unlock_commit(struct ftrace_event_file *file,
+			    struct ring_buffer *buffer,
+			    struct ring_buffer_event *event,
+			    void *entry, unsigned long irq_flags, int pc)
+{
+	enum event_trigger_type tt = ETT_NONE;
+
+	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
+		trace_buffer_unlock_commit(buffer, event, irq_flags, pc);
+
+	if (tt)
+		event_triggers_post_call(file, tt);
+}
+
+/**
+ * event_trigger_unlock_commit_regs - handle triggers and finish event commit
+ * @file: The file pointer assoctiated to the event
+ * @buffer: The ring buffer that the event is being written to
+ * @event: The event meta data in the ring buffer
+ * @entry: The event itself
+ * @irq_flags: The state of the interrupts at the start of the event
+ * @pc: The state of the preempt count at the start of the event.
+ *
+ * This is a helper function to handle triggers that require data
+ * from the event itself. It also tests the event against filters and
+ * if the event is soft disabled and should be discarded.
+ *
+ * Same as event_trigger_unlock_commit() but calls
+ * trace_buffer_unlock_commit_regs() instead of trace_buffer_unlock_commit().
+ */
+static inline void
+event_trigger_unlock_commit_regs(struct ftrace_event_file *file,
+				 struct ring_buffer *buffer,
+				 struct ring_buffer_event *event,
+				 void *entry, unsigned long irq_flags, int pc,
+				 struct pt_regs *regs)
+{
+	enum event_trigger_type tt = ETT_NONE;
+
+	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
+		trace_buffer_unlock_commit_regs(buffer, event,
+						irq_flags, pc, regs);
+
+	if (tt)
+		event_triggers_post_call(file, tt);
+}
+
+#ifdef CONFIG_BPF_SYSCALL
+unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx);
+#else
+static inline unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx)
+{
+	return 1;
+}
+#endif
+
+enum {
+	FILTER_OTHER = 0,
+	FILTER_STATIC_STRING,
+	FILTER_DYN_STRING,
+	FILTER_PTR_STRING,
+	FILTER_TRACE_FN,
+};
+
+extern int trace_event_raw_init(struct ftrace_event_call *call);
+extern int trace_define_field(struct ftrace_event_call *call, const char *type,
+			      const char *name, int offset, int size,
+			      int is_signed, int filter_type);
+extern int trace_add_event_call(struct ftrace_event_call *call);
+extern int trace_remove_event_call(struct ftrace_event_call *call);
+
+#define is_signed_type(type)	(((type)(-1)) < (type)1)
+
+int trace_set_clr_event(const char *system, const char *event, int set);
+
+/*
+ * The double __builtin_constant_p is because gcc will give us an error
+ * if we try to allocate the static variable to fmt if it is not a
+ * constant. Even with the outer if statement optimizing out.
+ */
+#define event_trace_printk(ip, fmt, args...)				\
+do {									\
+	__trace_printk_check_format(fmt, ##args);			\
+	tracing_record_cmdline(current);				\
+	if (__builtin_constant_p(fmt)) {				\
+		static const char *trace_printk_fmt			\
+		  __attribute__((section("__trace_printk_fmt"))) =	\
+			__builtin_constant_p(fmt) ? fmt : NULL;		\
+									\
+		__trace_bprintk(ip, trace_printk_fmt, ##args);		\
+	} else								\
+		__trace_printk(ip, fmt, ##args);			\
+} while (0)
+
+#ifdef CONFIG_PERF_EVENTS
+struct perf_event;
+
+DECLARE_PER_CPU(struct pt_regs, perf_trace_regs);
+
+extern int  perf_trace_init(struct perf_event *event);
+extern void perf_trace_destroy(struct perf_event *event);
+extern int  perf_trace_add(struct perf_event *event, int flags);
+extern void perf_trace_del(struct perf_event *event, int flags);
+extern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,
+				     char *filter_str);
+extern void ftrace_profile_free_filter(struct perf_event *event);
+extern void *perf_trace_buf_prepare(int size, unsigned short type,
+				    struct pt_regs **regs, int *rctxp);
+
+static inline void
+perf_trace_buf_submit(void *raw_data, int size, int rctx, u64 addr,
+		       u64 count, struct pt_regs *regs, void *head,
+		       struct task_struct *task)
+{
+	perf_tp_event(addr, count, raw_data, size, regs, head, rctx, task);
+}
+#endif
+
+#endif /* _LINUX_FTRACE_EVENT_H */
