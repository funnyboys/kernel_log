commit c2ba8a15f310d915f8748dd8324c91c82b12b5ff
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Wed Jun 12 11:57:30 2019 +0200

    jump_label: Batch updates if arch supports it
    
    If the architecture supports the batching of jump label updates, use it!
    
    An easy way to see the benefits of this patch is switching the
    schedstats on and off. For instance:
    
    -------------------------- %< ----------------------------
      #!/bin/sh
      while [ true ]; do
          sysctl -w kernel.sched_schedstats=1
          sleep 2
          sysctl -w kernel.sched_schedstats=0
          sleep 2
      done
    -------------------------- >% ----------------------------
    
    while watching the IPI count:
    
    -------------------------- %< ----------------------------
      # watch -n1 "cat /proc/interrupts | grep Function"
    -------------------------- >% ----------------------------
    
    With the current mode, it is possible to see +- 168 IPIs each 2 seconds,
    while with this patch the number of IPIs goes to 3 each 2 seconds.
    
    Regarding the performance impact of this patch set, I made two measurements:
    
        The time to update a key (the task that is causing the change)
        The time to run the int3 handler (the side effect on a thread that
                                          hits the code being changed)
    
    The schedstats static key was chosen as the key to being switched on and off.
    The reason being is that it is used in more than 56 places, in a hot path. The
    change in the schedstats static key will be done with the following command:
    
    while [ true ]; do
        sysctl -w kernel.sched_schedstats=1
        usleep 500000
        sysctl -w kernel.sched_schedstats=0
        usleep 500000
    done
    
    In this way, they key will be updated twice per second. To force the hit of the
    int3 handler, the system will also run a kernel compilation with two jobs per
    CPU. The test machine is a two nodes/24 CPUs box with an Intel Xeon processor
    @2.27GHz.
    
    Regarding the update part, on average, the regular kernel takes 57 ms to update
    the schedstats key, while the kernel with the batch updates takes just 1.4 ms
    on average. Although it seems to be too good to be true, it makes sense: the
    schedstats key is used in 56 places, so it was expected that it would take
    around 56 times to update the keys with the current implementation, as the
    IPIs are the most expensive part of the update.
    
    Regarding the int3 handler, the non-batch handler takes 45 ns on average, while
    the batch version takes around 180 ns. At first glance, it seems to be a high
    value. But it is not, considering that it is doing 56 updates, rather than one!
    It is taking four times more, only. This gain is possible because the patch
    uses a binary search in the vector: log2(56)=5.8. So, it was expected to have
    an overhead within four times.
    
    (voice of tv propaganda) But, that is not all! As the int3 handler keeps on for
    a shorter period (because the update part is on for a shorter time), the number
    of hits in the int3 handler decreased by 10%.
    
    The question then is: Is it worth paying the price of "135 ns" more in the int3
    handler?
    
    Considering that, in this test case, we are saving the handling of 53 IPIs,
    that takes more than these 135 ns, it seems to be a meager price to be paid.
    Moreover, the test case was forcing the hit of the int3, in practice, it
    does not take that often. While the IPI takes place on all CPUs, hitting
    the int3 handler or not!
    
    For instance, in an isolated CPU with a process running in user-space
    (nohz_full use-case), the chances of hitting the int3 handler is barely zero,
    while there is no way to avoid the IPIs. By bounding the IPIs, we are improving
    a lot this scenario.
    
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris von Recklinghausen <crecklin@redhat.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Wood <swood@redhat.com>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/acc891dbc2dbc9fd616dd680529a2337b1d1274c.1560325897.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 3e113a1fa0f1..3526c0aee954 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -215,6 +215,9 @@ extern void arch_jump_label_transform(struct jump_entry *entry,
 				      enum jump_label_type type);
 extern void arch_jump_label_transform_static(struct jump_entry *entry,
 					     enum jump_label_type type);
+extern bool arch_jump_label_transform_queue(struct jump_entry *entry,
+					    enum jump_label_type type);
+extern void arch_jump_label_transform_apply(void);
 extern int jump_label_text_reserved(void *start, void *end);
 extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);

commit e9666d10a5677a494260d60d1fa0b73cc7646eb3
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Dec 31 00:14:15 2018 +0900

    jump_label: move 'asm goto' support test to Kconfig
    
    Currently, CONFIG_JUMP_LABEL just means "I _want_ to use jump label".
    
    The jump label is controlled by HAVE_JUMP_LABEL, which is defined
    like this:
    
      #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
      # define HAVE_JUMP_LABEL
      #endif
    
    We can improve this by testing 'asm goto' support in Kconfig, then
    make JUMP_LABEL depend on CC_HAS_ASM_GOTO.
    
    Ugly #ifdef HAVE_JUMP_LABEL will go away, and CONFIG_JUMP_LABEL will
    match to the real kernel capability.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 5df6a621e464..3e113a1fa0f1 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -71,10 +71,6 @@
  * Additional babbling in: Documentation/static-keys.txt
  */
 
-#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
-# define HAVE_JUMP_LABEL
-#endif
-
 #ifndef __ASSEMBLY__
 
 #include <linux/types.h>
@@ -86,7 +82,7 @@ extern bool static_key_initialized;
 				    "%s(): static key '%pS' used before call to jump_label_init()", \
 				    __func__, (key))
 
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 
 struct static_key {
 	atomic_t enabled;
@@ -114,10 +110,10 @@ struct static_key {
 struct static_key {
 	atomic_t enabled;
 };
-#endif	/* HAVE_JUMP_LABEL */
+#endif	/* CONFIG_JUMP_LABEL */
 #endif /* __ASSEMBLY__ */
 
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 #include <asm/jump_label.h>
 
 #ifndef __ASSEMBLY__
@@ -192,7 +188,7 @@ enum jump_label_type {
 
 struct module;
 
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 
 #define JUMP_TYPE_FALSE		0UL
 #define JUMP_TYPE_TRUE		1UL
@@ -245,7 +241,7 @@ extern void static_key_disable_cpuslocked(struct static_key *key);
 	{ .enabled = { 0 },					\
 	  { .entries = (void *)JUMP_TYPE_FALSE } }
 
-#else  /* !HAVE_JUMP_LABEL */
+#else  /* !CONFIG_JUMP_LABEL */
 
 #include <linux/atomic.h>
 #include <linux/bug.h>
@@ -330,7 +326,7 @@ static inline void static_key_disable(struct static_key *key)
 #define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }
 #define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }
 
-#endif	/* HAVE_JUMP_LABEL */
+#endif	/* CONFIG_JUMP_LABEL */
 
 #define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
 #define jump_label_enabled static_key_enabled
@@ -394,7 +390,7 @@ extern bool ____wrong_branch_error(void);
 	static_key_count((struct static_key *)x) > 0;				\
 })
 
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 
 /*
  * Combine the right initial value (type) with the right branch order
@@ -476,12 +472,12 @@ extern bool ____wrong_branch_error(void);
 	unlikely(branch);							\
 })
 
-#else /* !HAVE_JUMP_LABEL */
+#else /* !CONFIG_JUMP_LABEL */
 
 #define static_branch_likely(x)		likely(static_key_enabled(&(x)->key))
 #define static_branch_unlikely(x)	unlikely(static_key_enabled(&(x)->key))
 
-#endif /* HAVE_JUMP_LABEL */
+#endif /* CONFIG_JUMP_LABEL */
 
 /*
  * Advanced usage; refcount, branch is enabled when: count != 0

commit 19483677684b6ca01606f58503cb79cdfbbc7c72
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:42 2018 -0700

    jump_label: Annotate entries that operate on __init code earlier
    
    Jump table entries are mostly read-only, with the exception of the
    init and module loader code that defuses entries that point into init
    code when the code being referred to is freed.
    
    For robustness, it would be better to move these entries into the
    ro_after_init section, but clearing the 'code' member of each jump
    table entry referring to init code at module load time races with the
    module_enable_ro() call that remaps the ro_after_init section read
    only, so we'd like to do it earlier.
    
    So given that whether such an entry refers to init code can be decided
    much earlier, we can pull this check forward. Since we may still need
    the code entry at this point, let's switch to setting a low bit in the
    'key' member just like we do to annotate the default state of a jump
    table entry.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Link: https://lkml.kernel.org/r/20180919065144.25010-8-ard.biesheuvel@linaro.org

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 2768a925bafa..5df6a621e464 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -141,7 +141,7 @@ static inline unsigned long jump_entry_target(const struct jump_entry *entry)
 
 static inline struct static_key *jump_entry_key(const struct jump_entry *entry)
 {
-	long offset = entry->key & ~1L;
+	long offset = entry->key & ~3L;
 
 	return (struct static_key *)((unsigned long)&entry->key + offset);
 }
@@ -160,7 +160,7 @@ static inline unsigned long jump_entry_target(const struct jump_entry *entry)
 
 static inline struct static_key *jump_entry_key(const struct jump_entry *entry)
 {
-	return (struct static_key *)((unsigned long)entry->key & ~1UL);
+	return (struct static_key *)((unsigned long)entry->key & ~3UL);
 }
 
 #endif
@@ -172,12 +172,12 @@ static inline bool jump_entry_is_branch(const struct jump_entry *entry)
 
 static inline bool jump_entry_is_init(const struct jump_entry *entry)
 {
-	return entry->code == 0;
+	return (unsigned long)entry->key & 2UL;
 }
 
 static inline void jump_entry_set_init(struct jump_entry *entry)
 {
-	entry->code = 0;
+	entry->key |= 2;
 }
 
 #endif
@@ -213,7 +213,6 @@ extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
 extern void jump_label_init(void);
-extern void jump_label_invalidate_initmem(void);
 extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
@@ -261,8 +260,6 @@ static __always_inline void jump_label_init(void)
 	static_key_initialized = true;
 }
 
-static inline void jump_label_invalidate_initmem(void) {}
-
 static __always_inline bool static_key_false(struct static_key *key)
 {
 	if (unlikely(static_key_count(key) > 0))

commit 50ff18ab497aa22f6a59444625df7508c8918237
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:37 2018 -0700

    jump_label: Implement generic support for relative references
    
    To reduce the size taken up by absolute references in jump label
    entries themselves and the associated relocation records in the
    .init segment, add support for emitting them as relative references
    instead.
    
    Note that this requires some extra care in the sorting routine, given
    that the offsets change when entries are moved around in the jump_entry
    table.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Link: https://lkml.kernel.org/r/20180919065144.25010-3-ard.biesheuvel@linaro.org

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 2eadff9b3b90..2768a925bafa 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -121,6 +121,32 @@ struct static_key {
 #include <asm/jump_label.h>
 
 #ifndef __ASSEMBLY__
+#ifdef CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE
+
+struct jump_entry {
+	s32 code;
+	s32 target;
+	long key;	// key may be far away from the core kernel under KASLR
+};
+
+static inline unsigned long jump_entry_code(const struct jump_entry *entry)
+{
+	return (unsigned long)&entry->code + entry->code;
+}
+
+static inline unsigned long jump_entry_target(const struct jump_entry *entry)
+{
+	return (unsigned long)&entry->target + entry->target;
+}
+
+static inline struct static_key *jump_entry_key(const struct jump_entry *entry)
+{
+	long offset = entry->key & ~1L;
+
+	return (struct static_key *)((unsigned long)&entry->key + offset);
+}
+
+#else
 
 static inline unsigned long jump_entry_code(const struct jump_entry *entry)
 {
@@ -137,6 +163,8 @@ static inline struct static_key *jump_entry_key(const struct jump_entry *entry)
 	return (struct static_key *)((unsigned long)entry->key & ~1UL);
 }
 
+#endif
+
 static inline bool jump_entry_is_branch(const struct jump_entry *entry)
 {
 	return (unsigned long)entry->key & 1UL;

commit 9ae033aca8d600e36034d4d0743aad624cec92ed
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:36 2018 -0700

    jump_label: Abstract jump_entry member accessors
    
    In preparation of allowing architectures to use relative references
    in jump_label entries [which can dramatically reduce the memory
    footprint], introduce abstractions for references to the 'code' and
    'key' members of struct jump_entry.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Link: https://lkml.kernel.org/r/20180919065144.25010-2-ard.biesheuvel@linaro.org

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 1a0b6f17a5d6..2eadff9b3b90 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -119,6 +119,40 @@ struct static_key {
 
 #ifdef HAVE_JUMP_LABEL
 #include <asm/jump_label.h>
+
+#ifndef __ASSEMBLY__
+
+static inline unsigned long jump_entry_code(const struct jump_entry *entry)
+{
+	return entry->code;
+}
+
+static inline unsigned long jump_entry_target(const struct jump_entry *entry)
+{
+	return entry->target;
+}
+
+static inline struct static_key *jump_entry_key(const struct jump_entry *entry)
+{
+	return (struct static_key *)((unsigned long)entry->key & ~1UL);
+}
+
+static inline bool jump_entry_is_branch(const struct jump_entry *entry)
+{
+	return (unsigned long)entry->key & 1UL;
+}
+
+static inline bool jump_entry_is_init(const struct jump_entry *entry)
+{
+	return entry->code == 0;
+}
+
+static inline void jump_entry_set_init(struct jump_entry *entry)
+{
+	entry->code = 0;
+}
+
+#endif
 #endif
 
 #ifndef __ASSEMBLY__

commit b5cb15d9372abc9adc4e844c0c1bf594ca6a7695
Author: Chris von Recklinghausen <crecklin@redhat.com>
Date:   Tue Jul 3 15:43:08 2018 -0400

    usercopy: Allow boot cmdline disabling of hardening
    
    Enabling HARDENED_USERCOPY may cause measurable regressions in networking
    performance: up to 8% under UDP flood.
    
    I ran a small packet UDP flood using pktgen vs. a host b2b connected. On
    the receiver side the UDP packets are processed by a simple user space
    process that just reads and drops them:
    
    https://github.com/netoptimizer/network-testing/blob/master/src/udp_sink.c
    
    Not very useful from a functional PoV, but it helps to pin-point
    bottlenecks in the networking stack.
    
    When running a kernel with CONFIG_HARDENED_USERCOPY=y, I see a 5-8%
    regression in the receive tput, compared to the same kernel without this
    option enabled.
    
    With CONFIG_HARDENED_USERCOPY=y, perf shows ~6% of CPU time spent
    cumulatively in __check_object_size (~4%) and __virt_addr_valid (~2%).
    
    The call-chain is:
    
    __GI___libc_recvfrom
    entry_SYSCALL_64_after_hwframe
    do_syscall_64
    __x64_sys_recvfrom
    __sys_recvfrom
    inet_recvmsg
    udp_recvmsg
    __check_object_size
    
    udp_recvmsg() actually calls copy_to_iter() (inlined) and the latters
    calls check_copy_size() (again, inlined).
    
    A generic distro may want to enable HARDENED_USERCOPY in their default
    kernel config, but at the same time, such distro may want to be able to
    avoid the performance penalties in with the default configuration and
    disable the stricter check on a per-boot basis.
    
    This change adds a boot parameter that conditionally disables
    HARDENED_USERCOPY via "hardened_usercopy=off".
    
    Signed-off-by: Chris von Recklinghausen <crecklin@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index b46b541c67c4..1a0b6f17a5d6 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -299,12 +299,18 @@ struct static_key_false {
 #define DEFINE_STATIC_KEY_TRUE(name)	\
 	struct static_key_true name = STATIC_KEY_TRUE_INIT
 
+#define DEFINE_STATIC_KEY_TRUE_RO(name)	\
+	struct static_key_true name __ro_after_init = STATIC_KEY_TRUE_INIT
+
 #define DECLARE_STATIC_KEY_TRUE(name)	\
 	extern struct static_key_true name
 
 #define DEFINE_STATIC_KEY_FALSE(name)	\
 	struct static_key_false name = STATIC_KEY_FALSE_INIT
 
+#define DEFINE_STATIC_KEY_FALSE_RO(name)	\
+	struct static_key_false name __ro_after_init = STATIC_KEY_FALSE_INIT
+
 #define DECLARE_STATIC_KEY_FALSE(name)	\
 	extern struct static_key_false name
 

commit 578ae447e7e5d78c90ac40a06406c1741f79ba96
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Mar 19 13:18:57 2018 -0500

    jump_label: Disable jump labels in __exit code
    
    With the following commit:
    
      333522447063 ("jump_label: Explicitly disable jump labels in __init code")
    
    ... we explicitly disabled jump labels in __init code, so they could be
    detected and not warned about in the following commit:
    
      dc1dd184c2f0 ("jump_label: Warn on failed jump_label patching attempt")
    
    In-kernel __exit code has the same issue.  It's never used, so it's
    freed along with the rest of initmem.  But jump label entries in __exit
    code aren't explicitly disabled, so we get the following warning when
    enabling pr_debug() in __exit code:
    
      can't patch jump_label at dmi_sysfs_exit+0x0/0x2d
      WARNING: CPU: 0 PID: 22572 at kernel/jump_label.c:376 __jump_label_update+0x9d/0xb0
    
    Fix the warning by disabling all jump labels in initmem (which includes
    both __init and __exit code).
    
    Reported-and-tested-by: Li Wang <liwang@redhat.com>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: dc1dd184c2f0 ("jump_label: Warn on failed jump_label patching attempt")
    Link: http://lkml.kernel.org/r/7121e6e595374f06616c505b6e690e275c0054d1.1521483452.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 2168cc6b8b30..b46b541c67c4 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -151,7 +151,7 @@ extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
 extern void jump_label_init(void);
-extern void jump_label_invalidate_init(void);
+extern void jump_label_invalidate_initmem(void);
 extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
@@ -199,7 +199,7 @@ static __always_inline void jump_label_init(void)
 	static_key_initialized = true;
 }
 
-static inline void jump_label_invalidate_init(void) {}
+static inline void jump_label_invalidate_initmem(void) {}
 
 static __always_inline bool static_key_false(struct static_key *key)
 {

commit 33352244706369ea6736781ae41fe41692eb69bb
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Tue Feb 20 11:37:51 2018 -0600

    jump_label: Explicitly disable jump labels in __init code
    
    After initmem has been freed, any jump labels in __init code are
    prevented from being written to by the kernel_text_address() check in
    __jump_label_update().  However, this check is quite broad.  If
    kernel_text_address() were to return false for any other reason, the
    jump label write would fail silently with no warning.
    
    For jump labels in module init code, entry->code is set to zero to
    indicate that the entry is disabled.  Do the same thing for core kernel
    init code.  This makes the behavior more consistent, and will also make
    it more straightforward to detect non-init jump label write failures in
    the next patch.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/c52825c73f3a174e8398b6898284ec20d4deb126.1519051220.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index b6a29c126cc4..2168cc6b8b30 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -151,6 +151,7 @@ extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
 extern void jump_label_init(void);
+extern void jump_label_invalidate_init(void);
 extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
@@ -198,6 +199,8 @@ static __always_inline void jump_label_init(void)
 	static_key_initialized = true;
 }
 
+static inline void jump_label_invalidate_init(void) {}
+
 static __always_inline bool static_key_false(struct static_key *key)
 {
 	if (unlikely(static_key_count(key) > 0))

commit 81dcf89f03204d7e0118018666cf362254e734b6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 18 14:38:11 2018 +0100

    jump_label: Add branch hints to static_branch_{un,}likely()
    
    For some reason these were missing, I've not observed this patch
    making a difference in the few code locations I checked, but this
    makes sense.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index e0340ca08d98..b6a29c126cc4 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -393,7 +393,7 @@ extern bool ____wrong_branch_error(void);
 		branch = !arch_static_branch_jump(&(x)->key, true);		\
 	else									\
 		branch = ____wrong_branch_error();				\
-	branch;									\
+	likely(branch);								\
 })
 
 #define static_branch_unlikely(x)						\
@@ -405,7 +405,7 @@ extern bool ____wrong_branch_error(void);
 		branch = arch_static_branch(&(x)->key, false);			\
 	else									\
 		branch = ____wrong_branch_error();				\
-	branch;									\
+	unlikely(branch);							\
 })
 
 #else /* !HAVE_JUMP_LABEL */

commit ce48c146495a1a50e48cdbfbfaba3e708be7c07c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 22 22:53:28 2018 +0100

    sched/core: Fix cpu.max vs. cpuhotplug deadlock
    
    Tejun reported the following cpu-hotplug lock (percpu-rwsem) read recursion:
    
      tg_set_cfs_bandwidth()
        get_online_cpus()
          cpus_read_lock()
    
        cfs_bandwidth_usage_inc()
          static_key_slow_inc()
            cpus_read_lock()
    
    Reported-by: Tejun Heo <tj@kernel.org>
    Tested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180122215328.GP3397@worktop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index c7b368c734af..e0340ca08d98 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -160,6 +160,8 @@ extern void arch_jump_label_transform_static(struct jump_entry *entry,
 extern int jump_label_text_reserved(void *start, void *end);
 extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
+extern void static_key_slow_inc_cpuslocked(struct static_key *key);
+extern void static_key_slow_dec_cpuslocked(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
 extern int static_key_count(struct static_key *key);
 extern void static_key_enable(struct static_key *key);
@@ -222,6 +224,9 @@ static inline void static_key_slow_dec(struct static_key *key)
 	atomic_dec(&key->enabled);
 }
 
+#define static_key_slow_inc_cpuslocked(key) static_key_slow_inc(key)
+#define static_key_slow_dec_cpuslocked(key) static_key_slow_dec(key)
+
 static inline int jump_label_text_reserved(void *start, void *end)
 {
 	return 0;
@@ -416,6 +421,8 @@ extern bool ____wrong_branch_error(void);
 
 #define static_branch_inc(x)		static_key_slow_inc(&(x)->key)
 #define static_branch_dec(x)		static_key_slow_dec(&(x)->key)
+#define static_branch_inc_cpuslocked(x)	static_key_slow_inc_cpuslocked(&(x)->key)
+#define static_branch_dec_cpuslocked(x)	static_key_slow_dec_cpuslocked(&(x)->key)
 
 /*
  * Normal usage; boolean enable/disable.

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index cd5861651b17..3b7675bcca64 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_JUMP_LABEL_H
 #define _LINUX_JUMP_LABEL_H
 

commit 5cdda5117e125e0dbb020425cc55a4c143c6febc
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Oct 18 17:24:28 2017 +0200

    locking/static_keys: Improve uninitialized key warning
    
    Right now it says:
    
      static_key_disable_cpuslocked used before call to jump_label_init
      ------------[ cut here ]------------
      WARNING: CPU: 0 PID: 0 at kernel/jump_label.c:161 static_key_disable_cpuslocked+0x68/0x70
      Modules linked in:
      CPU: 0 PID: 0 Comm: swapper Not tainted 4.14.0-rc5+ #1
      Hardware name: SGI.COM C2112-4GP3/X10DRT-P-Series, BIOS 2.0a 05/09/2016
      task: ffffffff81c0e480 task.stack: ffffffff81c00000
      RIP: 0010:static_key_disable_cpuslocked+0x68/0x70
      RSP: 0000:ffffffff81c03ef0 EFLAGS: 00010096 ORIG_RAX: 0000000000000000
      RAX: 0000000000000041 RBX: ffffffff81c32680 RCX: ffffffff81c5cbf8
      RDX: 0000000000000001 RSI: 0000000000000092 RDI: 0000000000000002
      RBP: ffff88807fffd240 R08: 726f666562206465 R09: 0000000000000136
      R10: 0000000000000000 R11: 696e695f6c656261 R12: ffffffff82158900
      R13: ffffffff8215f760 R14: 0000000000000001 R15: 0000000000000008
      FS:  0000000000000000(0000) GS:ffff883f7f400000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: ffff88807ffff000 CR3: 0000000001c09000 CR4: 00000000000606b0
      Call Trace:
       static_key_disable+0x16/0x20
       start_kernel+0x15a/0x45d
       ? load_ucode_intel_bsp+0x11/0x2d
       secondary_startup_64+0xa5/0xb0
      Code: 48 c7 c7 a0 15 cf 81 e9 47 53 4b 00 48 89 df e8 5f fc ff ff eb e8 48 c7 c6 \
            c0 97 83 81 48 c7 c7 d0 ff a2 81 31 c0 e8 c5 9d f5 ff <0f> ff eb a7 0f ff eb \
            b0 e8 eb a2 4b 00 53 48 89 fb e8 42 0e f0
    
    but it doesn't tell me which key it is. So dump the key's name too:
    
      static_key_disable_cpuslocked(): static key 'virt_spin_lock_key' used before call to jump_label_init()
    
    And that makes pinpointing which key is causing that a lot easier.
    
     include/linux/jump_label.h           |   14 +++++++-------
     include/linux/jump_label_ratelimit.h |    6 +++---
     kernel/jump_label.c                  |   14 +++++++-------
     3 files changed, 17 insertions(+), 17 deletions(-)
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171018152428.ffjgak4o25f7ept6@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index cd5861651b17..979a2f2d529b 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -81,9 +81,9 @@
 
 extern bool static_key_initialized;
 
-#define STATIC_KEY_CHECK_USE() WARN(!static_key_initialized,		      \
-				    "%s used before call to jump_label_init", \
-				    __func__)
+#define STATIC_KEY_CHECK_USE(key) WARN(!static_key_initialized,		      \
+				    "%s(): static key '%pS' used before call to jump_label_init()", \
+				    __func__, (key))
 
 #ifdef HAVE_JUMP_LABEL
 
@@ -211,13 +211,13 @@ static __always_inline bool static_key_true(struct static_key *key)
 
 static inline void static_key_slow_inc(struct static_key *key)
 {
-	STATIC_KEY_CHECK_USE();
+	STATIC_KEY_CHECK_USE(key);
 	atomic_inc(&key->enabled);
 }
 
 static inline void static_key_slow_dec(struct static_key *key)
 {
-	STATIC_KEY_CHECK_USE();
+	STATIC_KEY_CHECK_USE(key);
 	atomic_dec(&key->enabled);
 }
 
@@ -236,7 +236,7 @@ static inline int jump_label_apply_nops(struct module *mod)
 
 static inline void static_key_enable(struct static_key *key)
 {
-	STATIC_KEY_CHECK_USE();
+	STATIC_KEY_CHECK_USE(key);
 
 	if (atomic_read(&key->enabled) != 0) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);
@@ -247,7 +247,7 @@ static inline void static_key_enable(struct static_key *key)
 
 static inline void static_key_disable(struct static_key *key)
 {
-	STATIC_KEY_CHECK_USE();
+	STATIC_KEY_CHECK_USE(key);
 
 	if (atomic_read(&key->enabled) != 1) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);

commit 5a40527f8f0798553764fc8db4111d7d9c33ea51
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 1 09:02:56 2017 +0100

    jump_label: Provide hotplug context variants
    
    As using the normal static key API under the hotplug lock is
    pretty much impossible, let's provide a variant of some of them
    that require the hotplug lock to have already been taken.
    
    These function are only meant to be used in CPU hotplug callbacks.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Leo Yan <leo.yan@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20170801080257.5056-4-marc.zyngier@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 740a42ea7f7f..cd5861651b17 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -163,6 +163,8 @@ extern void jump_label_apply_nops(struct module *mod);
 extern int static_key_count(struct static_key *key);
 extern void static_key_enable(struct static_key *key);
 extern void static_key_disable(struct static_key *key);
+extern void static_key_enable_cpuslocked(struct static_key *key);
+extern void static_key_disable_cpuslocked(struct static_key *key);
 
 /*
  * We should be using ATOMIC_INIT() for initializing .enabled, but
@@ -254,6 +256,9 @@ static inline void static_key_disable(struct static_key *key)
 	atomic_set(&key->enabled, 0);
 }
 
+#define static_key_enable_cpuslocked(k)		static_key_enable((k))
+#define static_key_disable_cpuslocked(k)	static_key_disable((k))
+
 #define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }
 #define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }
 
@@ -415,8 +420,10 @@ extern bool ____wrong_branch_error(void);
  * Normal usage; boolean enable/disable.
  */
 
-#define static_branch_enable(x)		static_key_enable(&(x)->key)
-#define static_branch_disable(x)	static_key_disable(&(x)->key)
+#define static_branch_enable(x)			static_key_enable(&(x)->key)
+#define static_branch_disable(x)		static_key_disable(&(x)->key)
+#define static_branch_enable_cpuslocked(x)	static_key_enable_cpuslocked(&(x)->key)
+#define static_branch_disable_cpuslocked(x)	static_key_disable_cpuslocked(&(x)->key)
 
 #endif /* __ASSEMBLY__ */
 

commit 1dbb6704de91b169a58d0c8221624afd6a95cfc7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Aug 1 17:24:04 2017 +0200

    jump_label: Fix concurrent static_key_enable/disable()
    
    static_key_enable/disable are trying to cap the static key count to
    0/1.  However, their use of key->enabled is outside jump_label_lock
    so they do not really ensure that.
    
    Rewrite them to do a quick check for an already enabled (respectively,
    already disabled), and then recheck under the jump label lock.  Unlike
    static_key_slow_inc/dec, a failed check under the jump label lock does
    not modify key->enabled.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1501601046-35683-2-git-send-email-pbonzini@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 2afd74b9d844..740a42ea7f7f 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -234,22 +234,24 @@ static inline int jump_label_apply_nops(struct module *mod)
 
 static inline void static_key_enable(struct static_key *key)
 {
-	int count = static_key_count(key);
-
-	WARN_ON_ONCE(count < 0 || count > 1);
+	STATIC_KEY_CHECK_USE();
 
-	if (!count)
-		static_key_slow_inc(key);
+	if (atomic_read(&key->enabled) != 0) {
+		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);
+		return;
+	}
+	atomic_set(&key->enabled, 1);
 }
 
 static inline void static_key_disable(struct static_key *key)
 {
-	int count = static_key_count(key);
-
-	WARN_ON_ONCE(count < 0 || count > 1);
+	STATIC_KEY_CHECK_USE();
 
-	if (count)
-		static_key_slow_dec(key);
+	if (atomic_read(&key->enabled) != 1) {
+		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);
+		return;
+	}
+	atomic_set(&key->enabled, 0);
 }
 
 #define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }

commit f26db9649af36b8eb76850108113d4056f494537
Merge: c1ae3cfa0e89 65a50c656276
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 09:37:28 2017 -0800

    Merge tag 'trace-v4.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing fixes from Steven Rostedt:
     "There was some breakage with the changes for jump labels in the 4.11
      merge window:
    
       - powerpc broke as jump labels uses the two LSB bits as flags in
         initialization.
    
         A check was added to make sure that all jump label entries were 4
         bytes aligned, but powerpc didn't work that way for modules. Adding
         an alignment in the module linker script appeared to be the best
         solution.
    
       - Jump labels also added an anonymous union to access those LSB bits
         as a normal long. But because this structure had static
         initialization, it broke older compilers that could not statically
         initialize anonymous unions without brackets.
    
       - The command line parameter for setting function graph filter broke
         the "EMPTY_HASH" descriptor by modifying it instead of creating a
         new hash to hold the entries.
    
       - The command line parameter ftrace_graph_max_depth was added to
         allow its setting at boot time. It uses existing code and only the
         command line hook was added.
    
         This is not really a fix, but as it uses existing code without
         affecting anything else, I added it to this release. It was ready
         before the merge window closed, but I wanted to let it sit in
         linux-next for a couple of days first"
    
    * tag 'trace-v4.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ftrace/graph: Add ftrace_graph_max_depth kernel parameter
      tracing: Add #undef to fix compile error
      jump_label: Add comment about initialization order for anonymous unions
      jump_label: Fix anonymous union initialization
      module: set __jump_table alignment to 8
      ftrace/graph: Do not modify the EMPTY_HASH for the function_graph filter
      tracing: Fix code comment for ftrace_ops_get_func()

commit b17ef2ed624aa7c1f68ed11acd16ecbf80fe01d7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Mar 2 17:28:45 2017 -0500

    jump_label: Add comment about initialization order for anonymous unions
    
    Commit 3821fd35b58d ("jump_label: Reduce the size of struct static_key")
    broke old compilers that could not handle static initialization of anonymous
    unions. Boris fixed it with a patch that added brackets around the static
    initializer. But this creates a dependency between those initializers and
    the structure's order of its fields. Document this dependency in case new
    fields are added to struct static_key in the future.
    
    Noted-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Suggested-by: Chris Mason <clm@fb.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a7f90117cf7d..28e04a33535a 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -90,6 +90,13 @@ extern bool static_key_initialized;
 struct static_key {
 	atomic_t enabled;
 /*
+ * Note:
+ *   To make anonymous unions work with old compilers, the static
+ *   initialization of them requires brackets. This creates a dependency
+ *   on the order of the struct with the initializers. If any fields
+ *   are added, STATIC_KEY_INIT_TRUE and STATIC_KEY_INIT_FALSE may need
+ *   to be modified.
+ *
  * bit 0 => 1 if key is initially true
  *	    0 if initially false
  * bit 1 => 1 if points to struct static_key_mod

commit cd8d860dcce906cd477be7d0648ba6f56a52eaa6
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Tue Feb 28 11:32:22 2017 -0500

    jump_label: Fix anonymous union initialization
    
    Pre-4.6 gcc do not allow direct static initialization of members of
    anonymous structs/unions. After commit 3821fd35b58d ("jump_label:
    Reduce the size of struct static_key") STATIC_KEY_INIT_{TRUE|FALSE}
    definitions cannot be compiled with those older compilers.
    
    Placing initializers inside curved brackets works around this problem.
    
    Link: http://lkml.kernel.org/r/1488299542-30765-1-git-send-email-boris.ostrovsky@oracle.com
    
    Fixes: 3821fd35b58d ("jump_label: Reduce the size of struct static_key")
    Reviewed-by: Jason Baron <jbaron@akamai.com>
    Compiled-by: Chris Mason <clm@fb.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 680c98b2f41c..a7f90117cf7d 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -166,10 +166,10 @@ extern void static_key_disable(struct static_key *key);
  */
 #define STATIC_KEY_INIT_TRUE					\
 	{ .enabled = { 1 },					\
-	  .entries = (void *)JUMP_TYPE_TRUE }
+	  { .entries = (void *)JUMP_TYPE_TRUE } }
 #define STATIC_KEY_INIT_FALSE					\
 	{ .enabled = { 0 },					\
-	  .entries = (void *)JUMP_TYPE_FALSE }
+	  { .entries = (void *)JUMP_TYPE_FALSE } }
 
 #else  /* !HAVE_JUMP_LABEL */
 

commit 79b17ea740d9fab178d6a1aa15d848b5e6c01b82
Merge: e5d56efc97f8 67d04bb2bcbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 27 13:26:17 2017 -0800

    Merge tag 'trace-v4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This release has no new tracing features, just clean ups, minor fixes
      and small optimizations"
    
    * tag 'trace-v4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (25 commits)
      tracing: Remove outdated ring buffer comment
      tracing/probes: Fix a warning message to show correct maximum length
      tracing: Fix return value check in trace_benchmark_reg()
      tracing: Use modern function declaration
      jump_label: Reduce the size of struct static_key
      tracing/probe: Show subsystem name in messages
      tracing/hwlat: Update old comment about migration
      timers: Make flags output in the timer_start tracepoint useful
      tracing: Have traceprobe_probes_write() not access userspace unnecessarily
      tracing: Have COMM event filter key be treated as a string
      ftrace: Have set_graph_function handle multiple functions in one write
      ftrace: Do not hold references of ftrace_graph_{notrace_}hash out of graph_lock
      tracing: Reset parser->buffer to allow multiple "puts"
      ftrace: Have set_graph_functions handle write with RDWR
      ftrace: Reset fgd->hash in ftrace_graph_write()
      ftrace: Replace (void *)1 with a meaningful macro name FTRACE_GRAPH_EMPTY
      ftrace: Create a slight optimization on searching the ftrace_hash
      tracing: Add ftrace_hash_key() helper function
      ftrace: Convert graph filter to use hash tables
      ftrace: Expose ftrace_hash_empty and ftrace_lookup_ip
      ...

commit 3821fd35b58dba449bd894014fbf4e1c43c9e951
Author: Jason Baron <jbaron@akamai.com>
Date:   Fri Feb 3 15:42:24 2017 -0500

    jump_label: Reduce the size of struct static_key
    
    The static_key->next field goes mostly unused. The field is used for
    associating module uses with a static key. Most uses of struct static_key
    define a static key in the core kernel and make use of it entirely within
    the core kernel, or define the static key in a module and make use of it
    only from within that module. In fact, of the ~3,000 static keys defined,
    I found only about 5 or so that did not fit this pattern.
    
    Thus, we can remove the static_key->next field entirely and overload
    the static_key->entries field. That is, when all the static_key uses
    are contained within the same module, static_key->entries continues
    to point to those uses. However, if the static_key uses are not contained
    within the module where the static_key is defined, then we allocate a
    struct static_key_mod, store a pointer to the uses within that
    struct static_key_mod, and have the static key point at the static_key_mod.
    This does incur some extra memory usage when a static_key is used in a
    module that does not define it, but since there are only a handful of such
    cases there is a net savings.
    
    In order to identify if the static_key->entries pointer contains a
    struct static_key_mod or a struct jump_entry pointer, bit 1 of
    static_key->entries is set to 1 if it points to a struct static_key_mod and
    is 0 if it points to a struct jump_entry. We were already using bit 0 in a
    similar way to store the initial value of the static_key. This does mean
    that allocations of struct static_key_mod and that the struct jump_entry
    tables need to be at least 4-byte aligned in memory. As far as I can tell
    all arches meet this criteria.
    
    For my .config, the patch increased the text by 778 bytes, but reduced
    the data + bss size by 14912, for a net savings of 14,134 bytes.
    
       text    data     bss     dec     hex filename
    8092427 5016512  790528 13899467         d416cb vmlinux.pre
    8093205 5001600  790528 13885333         d3df95 vmlinux.post
    
    Link: http://lkml.kernel.org/r/1486154544-4321-1-git-send-email-jbaron@akamai.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Joe Perches <joe@perches.com>
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a0547c571800..680c98b2f41c 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -89,11 +89,17 @@ extern bool static_key_initialized;
 
 struct static_key {
 	atomic_t enabled;
-/* Set lsb bit to 1 if branch is default true, 0 ot */
-	struct jump_entry *entries;
-#ifdef CONFIG_MODULES
-	struct static_key_mod *next;
-#endif
+/*
+ * bit 0 => 1 if key is initially true
+ *	    0 if initially false
+ * bit 1 => 1 if points to struct static_key_mod
+ *	    0 if points to struct jump_entry
+ */
+	union {
+		unsigned long type;
+		struct jump_entry *entries;
+		struct static_key_mod *next;
+	};
 };
 
 #else
@@ -118,9 +124,10 @@ struct module;
 
 #ifdef HAVE_JUMP_LABEL
 
-#define JUMP_TYPE_FALSE	0UL
-#define JUMP_TYPE_TRUE	1UL
-#define JUMP_TYPE_MASK	1UL
+#define JUMP_TYPE_FALSE		0UL
+#define JUMP_TYPE_TRUE		1UL
+#define JUMP_TYPE_LINKED	2UL
+#define JUMP_TYPE_MASK		3UL
 
 static __always_inline bool static_key_false(struct static_key *key)
 {

commit 85b36c931ff328297572a3e6136fac573795ad79
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Wed Jan 18 09:38:04 2017 -0800

    jump_labels: Move header guard #endif down where it belongs
    
    The ending header guard is misplaced. This has no
    functional change, this is just an eye-sore.
    
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: bp@suse.de
    Cc: catalin.marinas@arm.com
    Cc: jbaron@akamai.com
    Cc: pbonzini@redhat.com
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/20170118173804.16281-1-mcgrof@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a0547c571800..b63d6b7b0db0 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -402,6 +402,6 @@ extern bool ____wrong_branch_error(void);
 #define static_branch_enable(x)		static_key_enable(&(x)->key)
 #define static_branch_disable(x)	static_key_disable(&(x)->key)
 
-#endif	/* _LINUX_JUMP_LABEL_H */
-
 #endif /* __ASSEMBLY__ */
+
+#endif	/* _LINUX_JUMP_LABEL_H */

commit e606d81d2d9596ab2b4fd0dc052eea0485b7e8c2
Merge: 12b7bcb43e6e b199ac6c4943
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 13:22:39 2016 -0700

    Merge branch 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS updates from Ingo Molnar:
     "The main changes were:
    
       - Lots of enhancements for AMD SMCA (Scalable MCA
         features/extensions) systems: extract, decode and print more
         hardware error information and add matching support on the
         injection/testing side as well. (Yazn Ghannam)
    
       - Various MCE handling improvements on modern Intel Xeons. (Tony
         Luck)
    
       - Plus misc fixes and enhancements"
    
    * 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      x86/RAS/mce_amd_inj: Remove debugfs dir recursively on exit
      x86/RAS/mce_amd_inj: Fix signed wrap around when decrementing index 'i'
      x86/RAS/mce_amd_inj: Fix some W= warnings
      x86/MCE/AMD, EDAC: Handle reserved bank 4 on Fam17h properly
      x86/mce/AMD: Extract the error address on SMCA systems
      x86/mce, EDAC/mce_amd: Print MCA_SYND and MCA_IPID during MCE on SMCA systems
      x86/mce/AMD: Save MCA_IPID in MCE struct on SMCA systems
      x86/mce/AMD: Ensure the deferred error interrupt is of type APIC on SMCA systems
      x86/mce/AMD: Update sysfs bank names for SMCA systems
      x86/mce/AMD, EDAC/mce_amd: Define and use tables for known SMCA IP types
      EDAC/mce_amd: Use SMCA prefix for error descriptions arrays
      EDAC/mce_amd: Add missing SMCA error descriptions
      x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks
      x86/RAS: Add syndrome support to mce_amd_inj
      EDAC/mce_amd: Print syndrome register value on SMCA systems
      x86/mce: Add support for new MCA_SYND register
      x86/mce/AMD: Use msr_ops.misc() in allocate_threshold_blocks()
      x86/mce: Drop X86_FEATURE_MCE_RECOVERY and the related model string test
      x86/mce: Improve memcpy_mcsafe()
      x86/mce: Add PCI quirks to identify Xeons with machine check recovery
      ...

commit ef0da55a84a345f323ceddda3b6c78b25de90435
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 5 18:25:47 2016 +0100

    jump_labels: Allow array initialisers
    
    The static key API is currently designed around single variable
    definitions. There are cases where an array of static keys is desirable,
    so extend the API to allow this rather than using the internal static
    key implementation directly.
    
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Suggested-by: Dave P Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 661af564fae8..a534c7f15a61 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -21,6 +21,8 @@
  *
  * DEFINE_STATIC_KEY_TRUE(key);
  * DEFINE_STATIC_KEY_FALSE(key);
+ * DEFINE_STATIC_KEY_ARRAY_TRUE(keys, count);
+ * DEFINE_STATIC_KEY_ARRAY_FALSE(keys, count);
  * static_branch_likely()
  * static_branch_unlikely()
  *
@@ -270,6 +272,16 @@ struct static_key_false {
 #define DEFINE_STATIC_KEY_FALSE(name)	\
 	struct static_key_false name = STATIC_KEY_FALSE_INIT
 
+#define DEFINE_STATIC_KEY_ARRAY_TRUE(name, count)		\
+	struct static_key_true name[count] = {			\
+		[0 ... (count) - 1] = STATIC_KEY_TRUE_INIT,	\
+	}
+
+#define DEFINE_STATIC_KEY_ARRAY_FALSE(name, count)		\
+	struct static_key_false name[count] = {			\
+		[0 ... (count) - 1] = STATIC_KEY_FALSE_INIT,	\
+	}
+
 extern bool ____wrong_branch_error(void);
 
 #define static_key_enabled(x)							\

commit b8fb03785d4de097507d0cf45873525e0ac4d2b2
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Sep 1 11:39:33 2016 -0700

    locking/static_keys: Provide DECLARE and well as DEFINE macros
    
    We will need to provide declarations of static keys in header
    files. Provide DECLARE_STATIC_KEY_{TRUE,FALSE} macros.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/816881cf85bd3cf13385d212882618f38a3b5d33.1472754711.git.tony.luck@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 661af564fae8..595fb46213fc 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -267,9 +267,15 @@ struct static_key_false {
 #define DEFINE_STATIC_KEY_TRUE(name)	\
 	struct static_key_true name = STATIC_KEY_TRUE_INIT
 
+#define DECLARE_STATIC_KEY_TRUE(name)	\
+	extern struct static_key_true name
+
 #define DEFINE_STATIC_KEY_FALSE(name)	\
 	struct static_key_false name = STATIC_KEY_FALSE_INIT
 
+#define DECLARE_STATIC_KEY_FALSE(name)	\
+	extern struct static_key_false name
+
 extern bool ____wrong_branch_error(void);
 
 #define static_key_enabled(x)							\

commit 1f69bf9c6137602cd028c96b4f8329121ec89231
Author: Jason Baron <jbaron@akamai.com>
Date:   Wed Aug 3 13:46:36 2016 -0700

    jump_label: remove bug.h, atomic.h dependencies for HAVE_JUMP_LABEL
    
    The current jump_label.h includes bug.h for things such as WARN_ON().
    This makes the header problematic for inclusion by kernel.h or any
    headers that kernel.h includes, since bug.h includes kernel.h (circular
    dependency).  The inclusion of atomic.h is similarly problematic.  Thus,
    this should make jump_label.h 'includable' from most places.
    
    Link: http://lkml.kernel.org/r/7060ce35ddd0d20b33bf170685e6b0fab816bdf2.1467837322.git.jbaron@akamai.com
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 68904469fba1..661af564fae8 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -76,7 +76,6 @@
 
 #include <linux/types.h>
 #include <linux/compiler.h>
-#include <linux/bug.h>
 
 extern bool static_key_initialized;
 
@@ -115,20 +114,8 @@ enum jump_label_type {
 
 struct module;
 
-#include <linux/atomic.h>
-
 #ifdef HAVE_JUMP_LABEL
 
-static inline int static_key_count(struct static_key *key)
-{
-	/*
-	 * -1 means the first static_key_slow_inc() is in progress.
-	 *  static_key_enabled() must return true, so return 1 here.
-	 */
-	int n = atomic_read(&key->enabled);
-	return n >= 0 ? n : 1;
-}
-
 #define JUMP_TYPE_FALSE	0UL
 #define JUMP_TYPE_TRUE	1UL
 #define JUMP_TYPE_MASK	1UL
@@ -157,16 +144,29 @@ extern int jump_label_text_reserved(void *start, void *end);
 extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
+extern int static_key_count(struct static_key *key);
+extern void static_key_enable(struct static_key *key);
+extern void static_key_disable(struct static_key *key);
 
+/*
+ * We should be using ATOMIC_INIT() for initializing .enabled, but
+ * the inclusion of atomic.h is problematic for inclusion of jump_label.h
+ * in 'low-level' headers. Thus, we are initializing .enabled with a
+ * raw value, but have added a BUILD_BUG_ON() to catch any issues in
+ * jump_label_init() see: kernel/jump_label.c.
+ */
 #define STATIC_KEY_INIT_TRUE					\
-	{ .enabled = ATOMIC_INIT(1),				\
+	{ .enabled = { 1 },					\
 	  .entries = (void *)JUMP_TYPE_TRUE }
 #define STATIC_KEY_INIT_FALSE					\
-	{ .enabled = ATOMIC_INIT(0),				\
+	{ .enabled = { 0 },					\
 	  .entries = (void *)JUMP_TYPE_FALSE }
 
 #else  /* !HAVE_JUMP_LABEL */
 
+#include <linux/atomic.h>
+#include <linux/bug.h>
+
 static inline int static_key_count(struct static_key *key)
 {
 	return atomic_read(&key->enabled);
@@ -216,14 +216,6 @@ static inline int jump_label_apply_nops(struct module *mod)
 	return 0;
 }
 
-#define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }
-#define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }
-
-#endif	/* HAVE_JUMP_LABEL */
-
-#define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
-#define jump_label_enabled static_key_enabled
-
 static inline void static_key_enable(struct static_key *key)
 {
 	int count = static_key_count(key);
@@ -244,6 +236,14 @@ static inline void static_key_disable(struct static_key *key)
 		static_key_slow_dec(key);
 }
 
+#define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }
+#define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }
+
+#endif	/* HAVE_JUMP_LABEL */
+
+#define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
+#define jump_label_enabled static_key_enabled
+
 /* -------------------------------------------------------------------------- */
 
 /*

commit 4c5ea0a9cd02d6aa8adc86e100b2a4cff8d614ff
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jun 21 18:52:17 2016 +0200

    locking/static_key: Fix concurrent static_key_slow_inc()
    
    The following scenario is possible:
    
        CPU 1                                   CPU 2
        static_key_slow_inc()
         atomic_inc_not_zero()
          -> key.enabled == 0, no increment
         jump_label_lock()
         atomic_inc_return()
          -> key.enabled == 1 now
                                                static_key_slow_inc()
                                                 atomic_inc_not_zero()
                                                  -> key.enabled == 1, inc to 2
                                                 return
                                                ** static key is wrong!
         jump_label_update()
         jump_label_unlock()
    
    Testing the static key at the point marked by (**) will follow the
    wrong path for jumps that have not been patched yet.  This can
    actually happen when creating many KVM virtual machines with userspace
    LAPIC emulation; just run several copies of the following program:
    
        #include <fcntl.h>
        #include <unistd.h>
        #include <sys/ioctl.h>
        #include <linux/kvm.h>
    
        int main(void)
        {
            for (;;) {
                int kvmfd = open("/dev/kvm", O_RDONLY);
                int vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);
                close(ioctl(vmfd, KVM_CREATE_VCPU, 1));
                close(vmfd);
                close(kvmfd);
            }
            return 0;
        }
    
    Every KVM_CREATE_VCPU ioctl will attempt a static_key_slow_inc() call.
    The static key's purpose is to skip NULL pointer checks and indeed one
    of the processes eventually dereferences NULL.
    
    As explained in the commit that introduced the bug:
    
      706249c222f6 ("locking/static_keys: Rework update logic")
    
    jump_label_update() needs key.enabled to be true.  The solution adopted
    here is to temporarily make key.enabled == -1, and use go down the
    slow path when key.enabled <= 0.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org> # v4.3+
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 706249c222f6 ("locking/static_keys: Rework update logic")
    Link: http://lkml.kernel.org/r/1466527937-69798-1-git-send-email-pbonzini@redhat.com
    [ Small stylistic edits to the changelog and the code. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 0536524bb9eb..68904469fba1 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -117,13 +117,18 @@ struct module;
 
 #include <linux/atomic.h>
 
+#ifdef HAVE_JUMP_LABEL
+
 static inline int static_key_count(struct static_key *key)
 {
-	return atomic_read(&key->enabled);
+	/*
+	 * -1 means the first static_key_slow_inc() is in progress.
+	 *  static_key_enabled() must return true, so return 1 here.
+	 */
+	int n = atomic_read(&key->enabled);
+	return n >= 0 ? n : 1;
 }
 
-#ifdef HAVE_JUMP_LABEL
-
 #define JUMP_TYPE_FALSE	0UL
 #define JUMP_TYPE_TRUE	1UL
 #define JUMP_TYPE_MASK	1UL
@@ -162,6 +167,11 @@ extern void jump_label_apply_nops(struct module *mod);
 
 #else  /* !HAVE_JUMP_LABEL */
 
+static inline int static_key_count(struct static_key *key)
+{
+	return atomic_read(&key->enabled);
+}
+
 static __always_inline void jump_label_init(void)
 {
 	static_key_initialized = true;

commit 90eec103b96e30401c0b846045bf8a1c7159b6da
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 16 11:08:45 2015 +0100

    treewide: Remove old email address
    
    There were still a number of references to my old Red Hat email
    address in the kernel source. Remove these while keeping the
    Red Hat copyright notices intact.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 8dde55974f18..0536524bb9eb 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -5,7 +5,7 @@
  * Jump label support
  *
  * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
- * Copyright (C) 2011-2012 Peter Zijlstra <pzijlstr@redhat.com>
+ * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
  *
  * DEPRECATED API:
  *

commit 69234acee54407962a20bedf90ef9c96326994b5
Merge: 11eaaadb3ea3 d57456753787
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 14:51:32 2015 -0800

    Merge branch 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "The cgroup core saw several significant updates this cycle:
    
       - percpu_rwsem for threadgroup locking is reinstated.  This was
         temporarily dropped due to down_write latency issues.  Oleg's
         rework of percpu_rwsem which is scheduled to be merged in this
         merge window resolves the issue.
    
       - On the v2 hierarchy, when controllers are enabled and disabled, all
         operations are atomic and can fail and revert cleanly.  This allows
         ->can_attach() failure which is necessary for cpu RT slices.
    
       - Tasks now stay associated with the original cgroups after exit
         until released.  This allows tracking resources held by zombies
         (e.g.  pids) and makes it easy to find out where zombies came from
         on the v2 hierarchy.  The pids controller was broken before these
         changes as zombies escaped the limits; unfortunately, updating this
         behavior required too many invasive changes and I don't think it's
         a good idea to backport them, so the pids controller on 4.3, the
         first version which included the pids controller, will stay broken
         at least until I'm sure about the cgroup core changes.
    
       - Optimization of a couple common tests using static_key"
    
    * 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (38 commits)
      cgroup: fix race condition around termination check in css_task_iter_next()
      blkcg: don't create "io.stat" on the root cgroup
      cgroup: drop cgroup__DEVEL__legacy_files_on_dfl
      cgroup: replace error handling in cgroup_init() with WARN_ON()s
      cgroup: add cgroup_subsys->free() method and use it to fix pids controller
      cgroup: keep zombies associated with their original cgroups
      cgroup: make css_set_rwsem a spinlock and rename it to css_set_lock
      cgroup: don't hold css_set_rwsem across css task iteration
      cgroup: reorganize css_task_iter functions
      cgroup: factor out css_set_move_task()
      cgroup: keep css_set and task lists in chronological order
      cgroup: make cgroup_destroy_locked() test cgroup_is_populated()
      cgroup: make css_sets pin the associated cgroups
      cgroup: relocate cgroup_[try]get/put()
      cgroup: move check_for_release() invocation
      cgroup: replace cgroup_has_tasks() with cgroup_is_populated()
      cgroup: make cgroup->nr_populated count the number of populated css_sets
      cgroup: remove an unused parameter from cgroup_task_migrate()
      cgroup: fix too early usage of static_branch_disable()
      cgroup: make cgroup_update_dfl_csses() migrate all target processes atomically
      ...

commit fa128fd735bd236b6b04d3fedfed7a784137c185
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 18 11:56:28 2015 -0400

    jump_label: make static_key_enabled() work on static_key_true/false types too
    
    static_key_enabled() can be used on struct static_key but not on its
    wrapper types static_key_true and static_key_false.  The function is
    useful for debugging and management of static keys.  Update it so that
    it can be used for the wrapper types too.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 7f653e8f6690..c9ca050de846 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -216,11 +216,6 @@ static inline int jump_label_apply_nops(struct module *mod)
 #define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
 #define jump_label_enabled static_key_enabled
 
-static inline bool static_key_enabled(struct static_key *key)
-{
-	return static_key_count(key) > 0;
-}
-
 static inline void static_key_enable(struct static_key *key)
 {
 	int count = static_key_count(key);
@@ -267,6 +262,17 @@ struct static_key_false {
 #define DEFINE_STATIC_KEY_FALSE(name)	\
 	struct static_key_false name = STATIC_KEY_FALSE_INIT
 
+extern bool ____wrong_branch_error(void);
+
+#define static_key_enabled(x)							\
+({										\
+	if (!__builtin_types_compatible_p(typeof(*x), struct static_key) &&	\
+	    !__builtin_types_compatible_p(typeof(*x), struct static_key_true) &&\
+	    !__builtin_types_compatible_p(typeof(*x), struct static_key_false))	\
+		____wrong_branch_error();					\
+	static_key_count((struct static_key *)x) > 0;				\
+})
+
 #ifdef HAVE_JUMP_LABEL
 
 /*
@@ -325,8 +331,6 @@ struct static_key_false {
  * See jump_label_type() / jump_label_init_type().
  */
 
-extern bool ____wrong_branch_error(void);
-
 #define static_branch_likely(x)							\
 ({										\
 	bool branch;								\

commit 1975dbc276c6ab62230cf4f9df5ddc9ff0e0e473
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Mon Sep 14 17:11:05 2015 -0600

    locking/static_keys: Fix up the static keys documentation
    
    Fix a few small mistakes in the static key documentation and
    delete an unneeded sentence.
    
    Suggested-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150914171105.511e1e21@lwn.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 0684bd3a48fc..f1094238ab2a 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -21,8 +21,8 @@
  *
  * DEFINE_STATIC_KEY_TRUE(key);
  * DEFINE_STATIC_KEY_FALSE(key);
- * static_key_likely()
- * static_key_unlikely()
+ * static_branch_likely()
+ * static_branch_unlikely()
  *
  * Jump labels provide an interface to generate dynamic branches using
  * self-modifying code. Assuming toolchain and architecture support, if we
@@ -45,12 +45,10 @@
  * statement, setting the key to true requires us to patch in a jump
  * to the out-of-line of true branch.
  *
- * In addtion to static_branch_{enable,disable}, we can also reference count
+ * In addition to static_branch_{enable,disable}, we can also reference count
  * the key or branch direction via static_branch_{inc,dec}. Thus,
  * static_branch_inc() can be thought of as a 'make more true' and
- * static_branch_dec() as a 'make more false'. The inc()/dec()
- * interface is meant to be used exclusively from the inc()/dec() for a given
- * key.
+ * static_branch_dec() as a 'make more false'.
  *
  * Since this relies on modifying code, the branch modifying functions
  * must be considered absolute slow paths (machine wide synchronization etc.).

commit edcd591c77a48da753456f92daf8bb50fe9bac93
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Mon Sep 7 13:18:03 2015 -0600

    locking/static_keys: Fix a silly typo
    
    Commit:
    
      412758cb2670 ("jump label, locking/static_keys: Update docs")
    
    introduced a typo that might as well get fixed.
    
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150907131803.54c027e1@lwn.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 7f653e8f6690..0684bd3a48fc 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -22,7 +22,7 @@
  * DEFINE_STATIC_KEY_TRUE(key);
  * DEFINE_STATIC_KEY_FALSE(key);
  * static_key_likely()
- * statick_key_unlikely()
+ * static_key_unlikely()
  *
  * Jump labels provide an interface to generate dynamic branches using
  * self-modifying code. Assuming toolchain and architecture support, if we

commit 412758cb26704e5087ca2976ec3b28fb2bdbfad4
Author: Jason Baron <jbaron@akamai.com>
Date:   Thu Jul 30 03:59:48 2015 +0000

    jump label, locking/static_keys: Update docs
    
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: bp@alien8.de
    Cc: davem@davemloft.net
    Cc: ddaney@caviumnetworks.com
    Cc: heiko.carstens@de.ibm.com
    Cc: linux-kernel@vger.kernel.org
    Cc: liuj97@gmail.com
    Cc: luto@amacapital.net
    Cc: michael@ellerman.id.au
    Cc: rabin@rab.in
    Cc: ralf@linux-mips.org
    Cc: rostedt@goodmis.org
    Cc: vbabka@suse.cz
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/6b50f2f6423a2244f37f4b1d2d6c211b9dcdf4f8.1438227999.git.jbaron@akamai.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index e337a1961933..7f653e8f6690 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -7,17 +7,52 @@
  * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
  * Copyright (C) 2011-2012 Peter Zijlstra <pzijlstr@redhat.com>
  *
+ * DEPRECATED API:
+ *
+ * The use of 'struct static_key' directly, is now DEPRECATED. In addition
+ * static_key_{true,false}() is also DEPRECATED. IE DO NOT use the following:
+ *
+ * struct static_key false = STATIC_KEY_INIT_FALSE;
+ * struct static_key true = STATIC_KEY_INIT_TRUE;
+ * static_key_true()
+ * static_key_false()
+ *
+ * The updated API replacements are:
+ *
+ * DEFINE_STATIC_KEY_TRUE(key);
+ * DEFINE_STATIC_KEY_FALSE(key);
+ * static_key_likely()
+ * statick_key_unlikely()
+ *
  * Jump labels provide an interface to generate dynamic branches using
- * self-modifying code. Assuming toolchain and architecture support, the result
- * of a "if (static_key_false(&key))" statement is an unconditional branch (which
- * defaults to false - and the true block is placed out of line).
+ * self-modifying code. Assuming toolchain and architecture support, if we
+ * define a "key" that is initially false via "DEFINE_STATIC_KEY_FALSE(key)",
+ * an "if (static_branch_unlikely(&key))" statement is an unconditional branch
+ * (which defaults to false - and the true block is placed out of line).
+ * Similarly, we can define an initially true key via
+ * "DEFINE_STATIC_KEY_TRUE(key)", and use it in the same
+ * "if (static_branch_unlikely(&key))", in which case we will generate an
+ * unconditional branch to the out-of-line true branch. Keys that are
+ * initially true or false can be using in both static_branch_unlikely()
+ * and static_branch_likely() statements.
  *
- * However at runtime we can change the branch target using
- * static_key_slow_{inc,dec}(). These function as a 'reference' count on the key
- * object, and for as long as there are references all branches referring to
- * that particular key will point to the (out of line) true block.
+ * At runtime we can change the branch target by setting the key
+ * to true via a call to static_branch_enable(), or false using
+ * static_branch_disable(). If the direction of the branch is switched by
+ * these calls then we run-time modify the branch target via a
+ * no-op -> jump or jump -> no-op conversion. For example, for an
+ * initially false key that is used in an "if (static_branch_unlikely(&key))"
+ * statement, setting the key to true requires us to patch in a jump
+ * to the out-of-line of true branch.
  *
- * Since this relies on modifying code, the static_key_slow_{inc,dec}() functions
+ * In addtion to static_branch_{enable,disable}, we can also reference count
+ * the key or branch direction via static_branch_{inc,dec}. Thus,
+ * static_branch_inc() can be thought of as a 'make more true' and
+ * static_branch_dec() as a 'make more false'. The inc()/dec()
+ * interface is meant to be used exclusively from the inc()/dec() for a given
+ * key.
+ *
+ * Since this relies on modifying code, the branch modifying functions
  * must be considered absolute slow paths (machine wide synchronization etc.).
  * OTOH, since the affected branches are unconditional, their runtime overhead
  * will be absolutely minimal, esp. in the default (off) case where the total
@@ -29,20 +64,10 @@
  * cause significant performance degradation. Struct static_key_deferred and
  * static_key_slow_dec_deferred() provide for this.
  *
- * Lacking toolchain and or architecture support, jump labels fall back to a simple
- * conditional branch.
- *
- * struct static_key my_key = STATIC_KEY_INIT_TRUE;
- *
- *   if (static_key_true(&my_key)) {
- *   }
- *
- * will result in the true case being in-line and starts the key with a single
- * reference. Mixing static_key_true() and static_key_false() on the same key is not
- * allowed.
+ * Lacking toolchain and or architecture support, static keys fall back to a
+ * simple conditional branch.
  *
- * Not initializing the key (static data is initialized to 0s anyway) is the
- * same as using STATIC_KEY_INIT_FALSE.
+ * Additional babbling in: Documentation/static-keys.txt
  */
 
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)

commit 11276d5306b8e5b438a36bbff855fe792d7eaa61
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 15:09:55 2015 +0200

    locking/static_keys: Add a new static_key interface
    
    There are various problems and short-comings with the current
    static_key interface:
    
     - static_key_{true,false}() read like a branch depending on the key
       value, instead of the actual likely/unlikely branch depending on
       init value.
    
     - static_key_{true,false}() are, as stated above, tied to the
       static_key init values STATIC_KEY_INIT_{TRUE,FALSE}.
    
     - we're limited to the 2 (out of 4) possible options that compile to
       a default NOP because that's what our arch_static_branch() assembly
       emits.
    
    So provide a new static_key interface:
    
      DEFINE_STATIC_KEY_TRUE(name);
      DEFINE_STATIC_KEY_FALSE(name);
    
    Which define a key of different types with an initial true/false
    value.
    
    Then allow:
    
       static_branch_likely()
       static_branch_unlikely()
    
    to take a key of either type and emit the right instruction for the
    case.
    
    This means adding a second arch_static_branch_jump() assembly helper
    which emits a JMP per default.
    
    In order to determine the right instruction for the right state,
    encode the branch type in the LSB of jump_entry::key.
    
    This is the final step in removing the naming confusion that has led to
    a stream of avoidable bugs such as:
    
      a833581e372a ("x86, perf: Fix static_key bug in load_mm_cr4()")
    
    ... but it also allows new static key combinations that will give us
    performance enhancements in the subsequent patches.
    
    Tested-by: Rabin Vincent <rabin@rab.in> # arm
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> # ppc
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 65f0ebac63cf..e337a1961933 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -107,12 +107,12 @@ static inline int static_key_count(struct static_key *key)
 
 static __always_inline bool static_key_false(struct static_key *key)
 {
-	return arch_static_branch(key);
+	return arch_static_branch(key, false);
 }
 
 static __always_inline bool static_key_true(struct static_key *key)
 {
-	return !static_key_false(key);
+	return !arch_static_branch(key, true);
 }
 
 extern struct jump_entry __start___jump_table[];
@@ -130,12 +130,12 @@ extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
 
-#define STATIC_KEY_INIT_TRUE ((struct static_key)		\
+#define STATIC_KEY_INIT_TRUE					\
 	{ .enabled = ATOMIC_INIT(1),				\
-	  .entries = (void *)JUMP_TYPE_TRUE })
-#define STATIC_KEY_INIT_FALSE ((struct static_key)		\
+	  .entries = (void *)JUMP_TYPE_TRUE }
+#define STATIC_KEY_INIT_FALSE					\
 	{ .enabled = ATOMIC_INIT(0),				\
-	  .entries = (void *)JUMP_TYPE_FALSE })
+	  .entries = (void *)JUMP_TYPE_FALSE }
 
 #else  /* !HAVE_JUMP_LABEL */
 
@@ -183,10 +183,8 @@ static inline int jump_label_apply_nops(struct module *mod)
 	return 0;
 }
 
-#define STATIC_KEY_INIT_TRUE ((struct static_key) \
-		{ .enabled = ATOMIC_INIT(1) })
-#define STATIC_KEY_INIT_FALSE ((struct static_key) \
-		{ .enabled = ATOMIC_INIT(0) })
+#define STATIC_KEY_INIT_TRUE	{ .enabled = ATOMIC_INIT(1) }
+#define STATIC_KEY_INIT_FALSE	{ .enabled = ATOMIC_INIT(0) }
 
 #endif	/* HAVE_JUMP_LABEL */
 
@@ -218,6 +216,137 @@ static inline void static_key_disable(struct static_key *key)
 		static_key_slow_dec(key);
 }
 
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Two type wrappers around static_key, such that we can use compile time
+ * type differentiation to emit the right code.
+ *
+ * All the below code is macros in order to play type games.
+ */
+
+struct static_key_true {
+	struct static_key key;
+};
+
+struct static_key_false {
+	struct static_key key;
+};
+
+#define STATIC_KEY_TRUE_INIT  (struct static_key_true) { .key = STATIC_KEY_INIT_TRUE,  }
+#define STATIC_KEY_FALSE_INIT (struct static_key_false){ .key = STATIC_KEY_INIT_FALSE, }
+
+#define DEFINE_STATIC_KEY_TRUE(name)	\
+	struct static_key_true name = STATIC_KEY_TRUE_INIT
+
+#define DEFINE_STATIC_KEY_FALSE(name)	\
+	struct static_key_false name = STATIC_KEY_FALSE_INIT
+
+#ifdef HAVE_JUMP_LABEL
+
+/*
+ * Combine the right initial value (type) with the right branch order
+ * to generate the desired result.
+ *
+ *
+ * type\branch|	likely (1)	      |	unlikely (0)
+ * -----------+-----------------------+------------------
+ *            |                       |
+ *  true (1)  |	   ...		      |	   ...
+ *            |    NOP		      |	   JMP L
+ *            |    <br-stmts>	      |	1: ...
+ *            |	L: ...		      |
+ *            |			      |
+ *            |			      |	L: <br-stmts>
+ *            |			      |	   jmp 1b
+ *            |                       |
+ * -----------+-----------------------+------------------
+ *            |                       |
+ *  false (0) |	   ...		      |	   ...
+ *            |    JMP L	      |	   NOP
+ *            |    <br-stmts>	      |	1: ...
+ *            |	L: ...		      |
+ *            |			      |
+ *            |			      |	L: <br-stmts>
+ *            |			      |	   jmp 1b
+ *            |                       |
+ * -----------+-----------------------+------------------
+ *
+ * The initial value is encoded in the LSB of static_key::entries,
+ * type: 0 = false, 1 = true.
+ *
+ * The branch type is encoded in the LSB of jump_entry::key,
+ * branch: 0 = unlikely, 1 = likely.
+ *
+ * This gives the following logic table:
+ *
+ *	enabled	type	branch	  instuction
+ * -----------------------------+-----------
+ *	0	0	0	| NOP
+ *	0	0	1	| JMP
+ *	0	1	0	| NOP
+ *	0	1	1	| JMP
+ *
+ *	1	0	0	| JMP
+ *	1	0	1	| NOP
+ *	1	1	0	| JMP
+ *	1	1	1	| NOP
+ *
+ * Which gives the following functions:
+ *
+ *   dynamic: instruction = enabled ^ branch
+ *   static:  instruction = type ^ branch
+ *
+ * See jump_label_type() / jump_label_init_type().
+ */
+
+extern bool ____wrong_branch_error(void);
+
+#define static_branch_likely(x)							\
+({										\
+	bool branch;								\
+	if (__builtin_types_compatible_p(typeof(*x), struct static_key_true))	\
+		branch = !arch_static_branch(&(x)->key, true);			\
+	else if (__builtin_types_compatible_p(typeof(*x), struct static_key_false)) \
+		branch = !arch_static_branch_jump(&(x)->key, true);		\
+	else									\
+		branch = ____wrong_branch_error();				\
+	branch;									\
+})
+
+#define static_branch_unlikely(x)						\
+({										\
+	bool branch;								\
+	if (__builtin_types_compatible_p(typeof(*x), struct static_key_true))	\
+		branch = arch_static_branch_jump(&(x)->key, false);		\
+	else if (__builtin_types_compatible_p(typeof(*x), struct static_key_false)) \
+		branch = arch_static_branch(&(x)->key, false);			\
+	else									\
+		branch = ____wrong_branch_error();				\
+	branch;									\
+})
+
+#else /* !HAVE_JUMP_LABEL */
+
+#define static_branch_likely(x)		likely(static_key_enabled(&(x)->key))
+#define static_branch_unlikely(x)	unlikely(static_key_enabled(&(x)->key))
+
+#endif /* HAVE_JUMP_LABEL */
+
+/*
+ * Advanced usage; refcount, branch is enabled when: count != 0
+ */
+
+#define static_branch_inc(x)		static_key_slow_inc(&(x)->key)
+#define static_branch_dec(x)		static_key_slow_dec(&(x)->key)
+
+/*
+ * Normal usage; boolean enable/disable.
+ */
+
+#define static_branch_enable(x)		static_key_enable(&(x)->key)
+#define static_branch_disable(x)	static_key_disable(&(x)->key)
+
 #endif	/* _LINUX_JUMP_LABEL_H */
 
 #endif /* __ASSEMBLY__ */

commit e33886b38cc82a9fc3b2d655dfc7f50467594138
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 15:03:40 2015 +0200

    locking/static_keys: Add static_key_{en,dis}able() helpers
    
    Add two helpers to make it easier to treat the refcount as boolean.
    
    Suggested-by: Jason Baron <jasonbaron0@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 0ddb208b8449..65f0ebac63cf 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -198,6 +198,26 @@ static inline bool static_key_enabled(struct static_key *key)
 	return static_key_count(key) > 0;
 }
 
+static inline void static_key_enable(struct static_key *key)
+{
+	int count = static_key_count(key);
+
+	WARN_ON_ONCE(count < 0 || count > 1);
+
+	if (!count)
+		static_key_slow_inc(key);
+}
+
+static inline void static_key_disable(struct static_key *key)
+{
+	int count = static_key_count(key);
+
+	WARN_ON_ONCE(count < 0 || count > 1);
+
+	if (count)
+		static_key_slow_dec(key);
+}
+
 #endif	/* _LINUX_JUMP_LABEL_H */
 
 #endif /* __ASSEMBLY__ */

commit a1efb01feca597b2abbc89873b40ef8ec6690168
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 14:55:40 2015 +0200

    jump_label, locking/static_keys: Rename JUMP_LABEL_TYPE_* and related helpers to the static_key* pattern
    
    Rename the JUMP_LABEL_TYPE_* macros to be JUMP_TYPE_* and move the
    inline helpers into kernel/jump_label.c, since that's the only place
    they're ever used.
    
    Also rename the helpers where it's all about static keys.
    
    This is the second step in removing the naming confusion that has led to
    a stream of avoidable bugs such as:
    
      a833581e372a ("x86, perf: Fix static_key bug in load_mm_cr4()")
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 6a8b4fe10ad8..0ddb208b8449 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -101,24 +101,9 @@ static inline int static_key_count(struct static_key *key)
 
 #ifdef HAVE_JUMP_LABEL
 
-#define JUMP_LABEL_TYPE_FALSE_BRANCH	0UL
-#define JUMP_LABEL_TYPE_TRUE_BRANCH	1UL
-#define JUMP_LABEL_TYPE_MASK		1UL
-
-static
-inline struct jump_entry *jump_label_get_entries(struct static_key *key)
-{
-	return (struct jump_entry *)((unsigned long)key->entries
-						& ~JUMP_LABEL_TYPE_MASK);
-}
-
-static inline bool jump_label_get_branch_default(struct static_key *key)
-{
-	if (((unsigned long)key->entries & JUMP_LABEL_TYPE_MASK) ==
-	    JUMP_LABEL_TYPE_TRUE_BRANCH)
-		return true;
-	return false;
-}
+#define JUMP_TYPE_FALSE	0UL
+#define JUMP_TYPE_TRUE	1UL
+#define JUMP_TYPE_MASK	1UL
 
 static __always_inline bool static_key_false(struct static_key *key)
 {
@@ -147,10 +132,10 @@ extern void jump_label_apply_nops(struct module *mod);
 
 #define STATIC_KEY_INIT_TRUE ((struct static_key)		\
 	{ .enabled = ATOMIC_INIT(1),				\
-	  .entries = (void *)JUMP_LABEL_TYPE_TRUE_BRANCH })
+	  .entries = (void *)JUMP_TYPE_TRUE })
 #define STATIC_KEY_INIT_FALSE ((struct static_key)		\
 	{ .enabled = ATOMIC_INIT(0),				\
-	  .entries = (void *)JUMP_LABEL_TYPE_FALSE_BRANCH })
+	  .entries = (void *)JUMP_TYPE_FALSE })
 
 #else  /* !HAVE_JUMP_LABEL */
 

commit 76b235c6bcb16062d663e2ee96db0b69f2e6bc14
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 14:45:44 2015 +0200

    jump_label: Rename JUMP_LABEL_{EN,DIS}ABLE to JUMP_LABEL_{JMP,NOP}
    
    Since we've already stepped away from ENABLE is a JMP and DISABLE is a
    NOP with the branch_default bits, and are going to make it even worse,
    rename it to make it all clearer.
    
    This way we don't mix multiple levels of logic attributes, but have a
    plain 'physical' name for what the current instruction patching status
    of a jump label is.
    
    This is a first step in removing the naming confusion that has led to
    a stream of avoidable bugs such as:
    
      a833581e372a ("x86, perf: Fix static_key bug in load_mm_cr4()")
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    [ Beefed up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index f4de473f226b..6a8b4fe10ad8 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -86,8 +86,8 @@ struct static_key {
 #ifndef __ASSEMBLY__
 
 enum jump_label_type {
-	JUMP_LABEL_DISABLE = 0,
-	JUMP_LABEL_ENABLE,
+	JUMP_LABEL_NOP = 0,
+	JUMP_LABEL_JMP,
 };
 
 struct module;

commit c0ccf6f99e3a43b87980c9df7da48427885206d0
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Apr 9 13:51:31 2015 +1000

    jump_label: Allow jump labels to be used in assembly
    
    To use jump labels in assembly we need the HAVE_JUMP_LABEL
    define, so we select a fallback version if the toolchain does
    not support them.
    
    Modify linux/jump_label.h so it can be included by assembly
    files. We also need to add -DCC_HAVE_ASM_GOTO to KBUILD_AFLAGS.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: catalin.marinas@arm.com
    Cc: davem@davemloft.net
    Cc: heiko.carstens@de.ibm.com
    Cc: jbaron@akamai.com
    Cc: linux@arm.linux.org.uk
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: liuj97@gmail.com
    Cc: mgorman@suse.de
    Cc: mmarek@suse.cz
    Cc: mpe@ellerman.id.au
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: rostedt@goodmis.org
    Cc: schwidefsky@de.ibm.com
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1428551492-21977-2-git-send-email-anton@samba.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 98f923b6a0ea..f4de473f226b 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -45,6 +45,12 @@
  * same as using STATIC_KEY_INIT_FALSE.
  */
 
+#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
+# define HAVE_JUMP_LABEL
+#endif
+
+#ifndef __ASSEMBLY__
+
 #include <linux/types.h>
 #include <linux/compiler.h>
 #include <linux/bug.h>
@@ -55,7 +61,7 @@ extern bool static_key_initialized;
 				    "%s used before call to jump_label_init", \
 				    __func__)
 
-#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
+#ifdef HAVE_JUMP_LABEL
 
 struct static_key {
 	atomic_t enabled;
@@ -66,13 +72,18 @@ struct static_key {
 #endif
 };
 
-# include <asm/jump_label.h>
-# define HAVE_JUMP_LABEL
 #else
 struct static_key {
 	atomic_t enabled;
 };
-#endif	/* CC_HAVE_ASM_GOTO && CONFIG_JUMP_LABEL */
+#endif	/* HAVE_JUMP_LABEL */
+#endif /* __ASSEMBLY__ */
+
+#ifdef HAVE_JUMP_LABEL
+#include <asm/jump_label.h>
+#endif
+
+#ifndef __ASSEMBLY__
 
 enum jump_label_type {
 	JUMP_LABEL_DISABLE = 0,
@@ -203,3 +214,5 @@ static inline bool static_key_enabled(struct static_key *key)
 }
 
 #endif	/* _LINUX_JUMP_LABEL_H */
+
+#endif /* __ASSEMBLY__ */

commit fd3cbdc0d1b5254a2e8793df58c409b469899a3f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Aug 10 08:53:39 2014 +0200

    jump_label: Fix small typos in the documentation
    
    Was reading through the documentation of this code and noticed
    a few typos, missing commas, etc.
    
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 784304b222b3..98f923b6a0ea 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -8,28 +8,28 @@
  * Copyright (C) 2011-2012 Peter Zijlstra <pzijlstr@redhat.com>
  *
  * Jump labels provide an interface to generate dynamic branches using
- * self-modifying code. Assuming toolchain and architecture support the result
- * of a "if (static_key_false(&key))" statement is a unconditional branch (which
+ * self-modifying code. Assuming toolchain and architecture support, the result
+ * of a "if (static_key_false(&key))" statement is an unconditional branch (which
  * defaults to false - and the true block is placed out of line).
  *
  * However at runtime we can change the branch target using
  * static_key_slow_{inc,dec}(). These function as a 'reference' count on the key
- * object and for as long as there are references all branches referring to
+ * object, and for as long as there are references all branches referring to
  * that particular key will point to the (out of line) true block.
  *
- * Since this relies on modifying code the static_key_slow_{inc,dec}() functions
+ * Since this relies on modifying code, the static_key_slow_{inc,dec}() functions
  * must be considered absolute slow paths (machine wide synchronization etc.).
- * OTOH, since the affected branches are unconditional their runtime overhead
+ * OTOH, since the affected branches are unconditional, their runtime overhead
  * will be absolutely minimal, esp. in the default (off) case where the total
  * effect is a single NOP of appropriate size. The on case will patch in a jump
  * to the out-of-line block.
  *
- * When the control is directly exposed to userspace it is prudent to delay the
+ * When the control is directly exposed to userspace, it is prudent to delay the
  * decrement to avoid high frequency code modifications which can (and do)
  * cause significant performance degradation. Struct static_key_deferred and
  * static_key_slow_dec_deferred() provide for this.
  *
- * Lacking toolchain and or architecture support, it falls back to a simple
+ * Lacking toolchain and or architecture support, jump labels fall back to a simple
  * conditional branch.
  *
  * struct static_key my_key = STATIC_KEY_INIT_TRUE;
@@ -43,8 +43,7 @@
  *
  * Not initializing the key (static data is initialized to 0s anyway) is the
  * same as using STATIC_KEY_INIT_FALSE.
- *
-*/
+ */
 
 #include <linux/types.h>
 #include <linux/compiler.h>

commit ea5e9539abf1258f23e725cb9cb25aa74efa29eb
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:07 2014 -0700

    include/linux/jump_label.h: expose the reference count
    
    This patch exposes the jump_label reference count in preparation for the
    next patch.  cpusets cares about both the jump_label being enabled and how
    many users of the cpusets there currently are.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 5c1dfb2a9e73..784304b222b3 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -69,6 +69,10 @@ struct static_key {
 
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
+#else
+struct static_key {
+	atomic_t enabled;
+};
 #endif	/* CC_HAVE_ASM_GOTO && CONFIG_JUMP_LABEL */
 
 enum jump_label_type {
@@ -79,6 +83,12 @@ enum jump_label_type {
 struct module;
 
 #include <linux/atomic.h>
+
+static inline int static_key_count(struct static_key *key)
+{
+	return atomic_read(&key->enabled);
+}
+
 #ifdef HAVE_JUMP_LABEL
 
 #define JUMP_LABEL_TYPE_FALSE_BRANCH	0UL
@@ -134,10 +144,6 @@ extern void jump_label_apply_nops(struct module *mod);
 
 #else  /* !HAVE_JUMP_LABEL */
 
-struct static_key {
-	atomic_t enabled;
-};
-
 static __always_inline void jump_label_init(void)
 {
 	static_key_initialized = true;
@@ -145,14 +151,14 @@ static __always_inline void jump_label_init(void)
 
 static __always_inline bool static_key_false(struct static_key *key)
 {
-	if (unlikely(atomic_read(&key->enabled) > 0))
+	if (unlikely(static_key_count(key) > 0))
 		return true;
 	return false;
 }
 
 static __always_inline bool static_key_true(struct static_key *key)
 {
-	if (likely(atomic_read(&key->enabled) > 0))
+	if (likely(static_key_count(key) > 0))
 		return true;
 	return false;
 }
@@ -194,7 +200,7 @@ static inline int jump_label_apply_nops(struct module *mod)
 
 static inline bool static_key_enabled(struct static_key *key)
 {
-	return (atomic_read(&key->enabled) > 0);
+	return static_key_count(key) > 0;
 }
 
 #endif	/* _LINUX_JUMP_LABEL_H */

commit f4be8433fc2f2361509e39f17b88c260da52a7cd
Author: Jiang Liu <liuj97@gmail.com>
Date:   Tue Jan 7 22:17:14 2014 +0800

    jump_label: use defined macros instead of hard-coding for better readability
    
    Use macro JUMP_LABEL_TRUE_BRANCH instead of hard-coding for better
    readability.
    
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 39999775b922..5c1dfb2a9e73 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -81,18 +81,21 @@ struct module;
 #include <linux/atomic.h>
 #ifdef HAVE_JUMP_LABEL
 
-#define JUMP_LABEL_TRUE_BRANCH 1UL
+#define JUMP_LABEL_TYPE_FALSE_BRANCH	0UL
+#define JUMP_LABEL_TYPE_TRUE_BRANCH	1UL
+#define JUMP_LABEL_TYPE_MASK		1UL
 
 static
 inline struct jump_entry *jump_label_get_entries(struct static_key *key)
 {
 	return (struct jump_entry *)((unsigned long)key->entries
-						& ~JUMP_LABEL_TRUE_BRANCH);
+						& ~JUMP_LABEL_TYPE_MASK);
 }
 
 static inline bool jump_label_get_branch_default(struct static_key *key)
 {
-	if ((unsigned long)key->entries & JUMP_LABEL_TRUE_BRANCH)
+	if (((unsigned long)key->entries & JUMP_LABEL_TYPE_MASK) ==
+	    JUMP_LABEL_TYPE_TRUE_BRANCH)
 		return true;
 	return false;
 }
@@ -122,10 +125,12 @@ extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
 
-#define STATIC_KEY_INIT_TRUE ((struct static_key) \
-	{ .enabled = ATOMIC_INIT(1), .entries = (void *)1 })
-#define STATIC_KEY_INIT_FALSE ((struct static_key) \
-	{ .enabled = ATOMIC_INIT(0), .entries = (void *)0 })
+#define STATIC_KEY_INIT_TRUE ((struct static_key)		\
+	{ .enabled = ATOMIC_INIT(1),				\
+	  .entries = (void *)JUMP_LABEL_TYPE_TRUE_BRANCH })
+#define STATIC_KEY_INIT_FALSE ((struct static_key)		\
+	{ .enabled = ATOMIC_INIT(0),				\
+	  .entries = (void *)JUMP_LABEL_TYPE_FALSE_BRANCH })
 
 #else  /* !HAVE_JUMP_LABEL */
 

commit 42a2d923cc349583ebf6fdd52a7d35e1c2f7e6bd
Merge: 5cbb3d216e20 75ecab1df14d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 13 17:40:34 2013 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) The addition of nftables.  No longer will we need protocol aware
        firewall filtering modules, it can all live in userspace.
    
        At the core of nftables is a, for lack of a better term, virtual
        machine that executes byte codes to inspect packet or metadata
        (arriving interface index, etc.) and make verdict decisions.
    
        Besides support for loading packet contents and comparing them, the
        interpreter supports lookups in various datastructures as
        fundamental operations.  For example sets are supports, and
        therefore one could create a set of whitelist IP address entries
        which have ACCEPT verdicts attached to them, and use the appropriate
        byte codes to do such lookups.
    
        Since the interpreted code is composed in userspace, userspace can
        do things like optimize things before giving it to the kernel.
    
        Another major improvement is the capability of atomically updating
        portions of the ruleset.  In the existing netfilter implementation,
        one has to update the entire rule set in order to make a change and
        this is very expensive.
    
        Userspace tools exist to create nftables rules using existing
        netfilter rule sets, but both kernel implementations will need to
        co-exist for quite some time as we transition from the old to the
        new stuff.
    
        Kudos to Patrick McHardy, Pablo Neira Ayuso, and others who have
        worked so hard on this.
    
     2) Daniel Borkmann and Hannes Frederic Sowa made several improvements
        to our pseudo-random number generator, mostly used for things like
        UDP port randomization and netfitler, amongst other things.
    
        In particular the taus88 generater is updated to taus113, and test
        cases are added.
    
     3) Support 64-bit rates in HTB and TBF schedulers, from Eric Dumazet
        and Yang Yingliang.
    
     4) Add support for new 577xx tigon3 chips to tg3 driver, from Nithin
        Sujir.
    
     5) Fix two fatal flaws in TCP dynamic right sizing, from Eric Dumazet,
        Neal Cardwell, and Yuchung Cheng.
    
     6) Allow IP_TOS and IP_TTL to be specified in sendmsg() ancillary
        control message data, much like other socket option attributes.
        From Francesco Fusco.
    
     7) Allow applications to specify a cap on the rate computed
        automatically by the kernel for pacing flows, via a new
        SO_MAX_PACING_RATE socket option.  From Eric Dumazet.
    
     8) Make the initial autotuned send buffer sizing in TCP more closely
        reflect actual needs, from Eric Dumazet.
    
     9) Currently early socket demux only happens for TCP sockets, but we
        can do it for connected UDP sockets too.  Implementation from Shawn
        Bohrer.
    
    10) Refactor inet socket demux with the goal of improving hash demux
        performance for listening sockets.  With the main goals being able
        to use RCU lookups on even request sockets, and eliminating the
        listening lock contention.  From Eric Dumazet.
    
    11) The bonding layer has many demuxes in it's fast path, and an RCU
        conversion was started back in 3.11, several changes here extend the
        RCU usage to even more locations.  From Ding Tianhong and Wang
        Yufen, based upon suggestions by Nikolay Aleksandrov and Veaceslav
        Falico.
    
    12) Allow stackability of segmentation offloads to, in particular, allow
        segmentation offloading over tunnels.  From Eric Dumazet.
    
    13) Significantly improve the handling of secret keys we input into the
        various hash functions in the inet hashtables, TCP fast open, as
        well as syncookies.  From Hannes Frederic Sowa.  The key fundamental
        operation is "net_get_random_once()" which uses static keys.
    
        Hannes even extended this to ipv4/ipv6 fragmentation handling and
        our generic flow dissector.
    
    14) The generic driver layer takes care now to set the driver data to
        NULL on device removal, so it's no longer necessary for drivers to
        explicitly set it to NULL any more.  Many drivers have been cleaned
        up in this way, from Jingoo Han.
    
    15) Add a BPF based packet scheduler classifier, from Daniel Borkmann.
    
    16) Improve CRC32 interfaces and generic SKB checksum iterators so that
        SCTP's checksumming can more cleanly be handled.  Also from Daniel
        Borkmann.
    
    17) Add a new PMTU discovery mode, IP_PMTUDISC_INTERFACE, which forces
        using the interface MTU value.  This helps avoid PMTU attacks,
        particularly on DNS servers.  From Hannes Frederic Sowa.
    
    18) Use generic XPS for transmit queue steering rather than internal
        (re-)implementation in virtio-net.  From Jason Wang.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1622 commits)
      random32: add test cases for taus113 implementation
      random32: upgrade taus88 generator to taus113 from errata paper
      random32: move rnd_state to linux/random.h
      random32: add prandom_reseed_late() and call when nonblocking pool becomes initialized
      random32: add periodic reseeding
      random32: fix off-by-one in seeding requirement
      PHY: Add RTL8201CP phy_driver to realtek
      xtsonic: add missing platform_set_drvdata() in xtsonic_probe()
      macmace: add missing platform_set_drvdata() in mace_probe()
      ethernet/arc/arc_emac: add missing platform_set_drvdata() in arc_emac_probe()
      ipv6: protect for_each_sk_fl_rcu in mem_check with rcu_read_lock_bh
      vlan: Implement vlan_dev_get_egress_qos_mask as an inline.
      ixgbe: add warning when max_vfs is out of range.
      igb: Update link modes display in ethtool
      netfilter: push reasm skb through instead of original frag skbs
      ip6_output: fragment outgoing reassembled skb properly
      MAINTAINERS: mv643xx_eth: take over maintainership from Lennart
      net_sched: tbf: support of 64bit rates
      ixgbe: deleting dfwd stations out of order can cause null ptr deref
      ixgbe: fix build err, num_rx_queues is only available with CONFIG_RPS
      ...

commit 261adc9a609dbfde815337889b9e2c8728959ab8
Author: Roel Kluin <roel.kluin@gmail.com>
Date:   Tue Nov 12 15:08:44 2013 -0800

    jump_label: unlikely(x) > 0
    
    if (unlikely(x) > 0) doesn't seem to help branch prediction
    
    Signed-off-by: Roel Kluin <roel.kluin@gmail.com>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: "H. Peter Anvin" <hpa@linux.intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a5079072da66..cf08540d6204 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -132,14 +132,14 @@ static __always_inline void jump_label_init(void)
 
 static __always_inline bool static_key_false(struct static_key *key)
 {
-	if (unlikely(atomic_read(&key->enabled)) > 0)
+	if (unlikely(atomic_read(&key->enabled) > 0))
 		return true;
 	return false;
 }
 
 static __always_inline bool static_key_true(struct static_key *key)
 {
-	if (likely(atomic_read(&key->enabled)) > 0)
+	if (likely(atomic_read(&key->enabled) > 0))
 		return true;
 	return false;
 }

commit c4b2c0c5f647aa1093e8f9097a30c17ce0f94d4d
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Sat Oct 19 21:48:53 2013 +0200

    static_key: WARN on usage before jump_label_init was called
    
    Usage of the static key primitives to toggle a branch must not be used
    before jump_label_init() is called from init/main.c. jump_label_init
    reorganizes and wires up the jump_entries so usage before that could
    have unforeseen consequences.
    
    Following primitives are now checked for correct use:
    * static_key_slow_inc
    * static_key_slow_dec
    * static_key_slow_dec_deferred
    * jump_label_rate_limit
    
    The x86 architecture already checks this by testing if the default_nop
    was already replaced with an optimal nop or with a branch instruction. It
    will panic then. Other architectures don't check for this.
    
    Because we need to relax this check for the x86 arch to allow code to
    transition from default_nop to the enabled state and other architectures
    did not check for this at all this patch introduces checking on the
    static_key primitives in a non-arch dependent manner.
    
    All checked functions are considered slow-path so the additional check
    does no harm to performance.
    
    The warnings are best observed with earlyprintk.
    
    Based on a patch from Andi Kleen.
    
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a5079072da66..e96be7245717 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -48,6 +48,13 @@
 
 #include <linux/types.h>
 #include <linux/compiler.h>
+#include <linux/bug.h>
+
+extern bool static_key_initialized;
+
+#define STATIC_KEY_CHECK_USE() WARN(!static_key_initialized,		      \
+				    "%s used before call to jump_label_init", \
+				    __func__)
 
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
 
@@ -128,6 +135,7 @@ struct static_key {
 
 static __always_inline void jump_label_init(void)
 {
+	static_key_initialized = true;
 }
 
 static __always_inline bool static_key_false(struct static_key *key)
@@ -146,11 +154,13 @@ static __always_inline bool static_key_true(struct static_key *key)
 
 static inline void static_key_slow_inc(struct static_key *key)
 {
+	STATIC_KEY_CHECK_USE();
 	atomic_inc(&key->enabled);
 }
 
 static inline void static_key_slow_dec(struct static_key *key)
 {
+	STATIC_KEY_CHECK_USE();
 	atomic_dec(&key->enabled);
 }
 

commit 851cf6e7d6366195d4ee033cdc7787df1a649a14
Author: Andrew Jones <drjones@redhat.com>
Date:   Fri Aug 9 19:51:57 2013 +0530

    jump_label: Split jumplabel ratelimit
    
    Commit b202952075f62603bea9bfb6ebc6b0420db11949 ("perf, core: Rate limit
    perf_sched_events jump_label patching") introduced rate limiting
    for jump label disabling. The changes were made in the jump label code
    in order to be more widely available and to keep things tidier. This is
    all fine, except now jump_label.h includes linux/workqueue.h, which
    makes it impossible to include jump_label.h from anything that
    workqueue.h needs. For example, it's now impossible to include
    jump_label.h from asm/spinlock.h, which is done in proposed
    pv-ticketlock patches. This patch splits out the rate limiting related
    changes from jump_label.h into a new file, jump_label_ratelimit.h, to
    resolve the issue.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Link: http://lkml.kernel.org/r/1376058122-8248-10-git-send-email-raghavendra.kt@linux.vnet.ibm.com
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 0976fc46d1e0..a5079072da66 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -48,7 +48,6 @@
 
 #include <linux/types.h>
 #include <linux/compiler.h>
-#include <linux/workqueue.h>
 
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
 
@@ -61,12 +60,6 @@ struct static_key {
 #endif
 };
 
-struct static_key_deferred {
-	struct static_key key;
-	unsigned long timeout;
-	struct delayed_work work;
-};
-
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
 #endif	/* CC_HAVE_ASM_GOTO && CONFIG_JUMP_LABEL */
@@ -78,6 +71,7 @@ enum jump_label_type {
 
 struct module;
 
+#include <linux/atomic.h>
 #ifdef HAVE_JUMP_LABEL
 
 #define JUMP_LABEL_TRUE_BRANCH 1UL
@@ -119,10 +113,7 @@ extern void arch_jump_label_transform_static(struct jump_entry *entry,
 extern int jump_label_text_reserved(void *start, void *end);
 extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
-extern void static_key_slow_dec_deferred(struct static_key_deferred *key);
 extern void jump_label_apply_nops(struct module *mod);
-extern void
-jump_label_rate_limit(struct static_key_deferred *key, unsigned long rl);
 
 #define STATIC_KEY_INIT_TRUE ((struct static_key) \
 	{ .enabled = ATOMIC_INIT(1), .entries = (void *)1 })
@@ -131,8 +122,6 @@ jump_label_rate_limit(struct static_key_deferred *key, unsigned long rl);
 
 #else  /* !HAVE_JUMP_LABEL */
 
-#include <linux/atomic.h>
-
 struct static_key {
 	atomic_t enabled;
 };
@@ -141,10 +130,6 @@ static __always_inline void jump_label_init(void)
 {
 }
 
-struct static_key_deferred {
-	struct static_key  key;
-};
-
 static __always_inline bool static_key_false(struct static_key *key)
 {
 	if (unlikely(atomic_read(&key->enabled)) > 0)
@@ -169,11 +154,6 @@ static inline void static_key_slow_dec(struct static_key *key)
 	atomic_dec(&key->enabled);
 }
 
-static inline void static_key_slow_dec_deferred(struct static_key_deferred *key)
-{
-	static_key_slow_dec(&key->key);
-}
-
 static inline int jump_label_text_reserved(void *start, void *end)
 {
 	return 0;
@@ -187,12 +167,6 @@ static inline int jump_label_apply_nops(struct module *mod)
 	return 0;
 }
 
-static inline void
-jump_label_rate_limit(struct static_key_deferred *key,
-		unsigned long rl)
-{
-}
-
 #define STATIC_KEY_INIT_TRUE ((struct static_key) \
 		{ .enabled = ATOMIC_INIT(1) })
 #define STATIC_KEY_INIT_FALSE ((struct static_key) \

commit 47fbc518a4b5c9a949f7cab8b14a00d3549bf138
Author: Jason Baron <jbaron@redhat.com>
Date:   Thu Jun 28 15:05:02 2012 -0400

    jump label: Remove static_branch()
    
    Remove the obsolete static_branch() interface, since the supported interface
    is now static_key_false()/true() - which is used by all in-tree code.
    
    See commit:
    
      c5905afb0e ("static keys: Introduce 'struct static_key', static_key_true()/false()
      and static_key_slow_[inc|dec]()").
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: rostedt@goodmis.org
    Cc: mathieu.desnoyers@efficios.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/199332c47eef8005d5a5bf1018a80d25929a5746.1340909155.git.jbaron@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index c513a40510f5..0976fc46d1e0 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -42,8 +42,7 @@
  * allowed.
  *
  * Not initializing the key (static data is initialized to 0s anyway) is the
- * same as using STATIC_KEY_INIT_FALSE and static_key_false() is
- * equivalent with static_branch().
+ * same as using STATIC_KEY_INIT_FALSE.
  *
 */
 
@@ -107,12 +106,6 @@ static __always_inline bool static_key_true(struct static_key *key)
 	return !static_key_false(key);
 }
 
-/* Deprecated. Please use 'static_key_false() instead. */
-static __always_inline bool static_branch(struct static_key *key)
-{
-	return arch_static_branch(key);
-}
-
 extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
@@ -166,14 +159,6 @@ static __always_inline bool static_key_true(struct static_key *key)
 	return false;
 }
 
-/* Deprecated. Please use 'static_key_false() instead. */
-static __always_inline bool static_branch(struct static_key *key)
-{
-	if (unlikely(atomic_read(&key->enabled)) > 0)
-		return true;
-	return false;
-}
-
 static inline void static_key_slow_inc(struct static_key *key)
 {
 	atomic_inc(&key->enabled);

commit 8eedce996556d7d06522cd3a0e6069141c8dffe0
Author: Jason Baron <jbaron@redhat.com>
Date:   Tue Feb 28 13:49:01 2012 -0500

    static keys: Inline the static_key_enabled() function
    
    In the jump label enabled case, calling static_key_enabled()
    results in a function call. The function returns the results of
    a compare, so it really doesn't need the overhead of a full
    function call. Let's make it 'static inline' for both the jump
    label enabled and disabled cases.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: a.p.zijlstra@chello.nl
    Cc: rostedt@goodmis.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/201202281849.q1SIn1p2023270@int-mx10.intmail.prod.int.phx2.redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 2172da2d9bb4..c513a40510f5 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -127,7 +127,6 @@ extern int jump_label_text_reserved(void *start, void *end);
 extern void static_key_slow_inc(struct static_key *key);
 extern void static_key_slow_dec(struct static_key *key);
 extern void static_key_slow_dec_deferred(struct static_key_deferred *key);
-extern bool static_key_enabled(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
 extern void
 jump_label_rate_limit(struct static_key_deferred *key, unsigned long rl);
@@ -198,11 +197,6 @@ static inline int jump_label_text_reserved(void *start, void *end)
 static inline void jump_label_lock(void) {}
 static inline void jump_label_unlock(void) {}
 
-static inline bool static_key_enabled(struct static_key *key)
-{
-	return (atomic_read(&key->enabled) > 0);
-}
-
 static inline int jump_label_apply_nops(struct module *mod)
 {
 	return 0;
@@ -224,4 +218,9 @@ jump_label_rate_limit(struct static_key_deferred *key,
 #define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
 #define jump_label_enabled static_key_enabled
 
+static inline bool static_key_enabled(struct static_key *key)
+{
+	return (atomic_read(&key->enabled) > 0);
+}
+
 #endif	/* _LINUX_JUMP_LABEL_H */

commit c5905afb0ee6550b42c49213da1c22d67316c194
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 24 08:31:31 2012 +0100

    static keys: Introduce 'struct static_key', static_key_true()/false() and static_key_slow_[inc|dec]()
    
    So here's a boot tested patch on top of Jason's series that does
    all the cleanups I talked about and turns jump labels into a
    more intuitive to use facility. It should also address the
    various misconceptions and confusions that surround jump labels.
    
    Typical usage scenarios:
    
            #include <linux/static_key.h>
    
            struct static_key key = STATIC_KEY_INIT_TRUE;
    
            if (static_key_false(&key))
                    do unlikely code
            else
                    do likely code
    
    Or:
    
            if (static_key_true(&key))
                    do likely code
            else
                    do unlikely code
    
    The static key is modified via:
    
            static_key_slow_inc(&key);
            ...
            static_key_slow_dec(&key);
    
    The 'slow' prefix makes it abundantly clear that this is an
    expensive operation.
    
    I've updated all in-kernel code to use this everywhere. Note
    that I (intentionally) have not pushed through the rename
    blindly through to the lowest levels: the actual jump-label
    patching arch facility should be named like that, so we want to
    decouple jump labels from the static-key facility a bit.
    
    On non-jump-label enabled architectures static keys default to
    likely()/unlikely() branches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: a.p.zijlstra@chello.nl
    Cc: mathieu.desnoyers@efficios.com
    Cc: davem@davemloft.net
    Cc: ddaney.cavm@gmail.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120222085809.GA26397@elte.hu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index f7c69580fea7..2172da2d9bb4 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -9,15 +9,15 @@
  *
  * Jump labels provide an interface to generate dynamic branches using
  * self-modifying code. Assuming toolchain and architecture support the result
- * of a "if (static_branch(&key))" statement is a unconditional branch (which
+ * of a "if (static_key_false(&key))" statement is a unconditional branch (which
  * defaults to false - and the true block is placed out of line).
  *
- * However at runtime we can change the 'static' branch target using
- * jump_label_{inc,dec}(). These function as a 'reference' count on the key
+ * However at runtime we can change the branch target using
+ * static_key_slow_{inc,dec}(). These function as a 'reference' count on the key
  * object and for as long as there are references all branches referring to
  * that particular key will point to the (out of line) true block.
  *
- * Since this relies on modifying code the jump_label_{inc,dec}() functions
+ * Since this relies on modifying code the static_key_slow_{inc,dec}() functions
  * must be considered absolute slow paths (machine wide synchronization etc.).
  * OTOH, since the affected branches are unconditional their runtime overhead
  * will be absolutely minimal, esp. in the default (off) case where the total
@@ -26,12 +26,26 @@
  *
  * When the control is directly exposed to userspace it is prudent to delay the
  * decrement to avoid high frequency code modifications which can (and do)
- * cause significant performance degradation. Struct jump_label_key_deferred and
- * jump_label_dec_deferred() provide for this.
+ * cause significant performance degradation. Struct static_key_deferred and
+ * static_key_slow_dec_deferred() provide for this.
  *
  * Lacking toolchain and or architecture support, it falls back to a simple
  * conditional branch.
- */
+ *
+ * struct static_key my_key = STATIC_KEY_INIT_TRUE;
+ *
+ *   if (static_key_true(&my_key)) {
+ *   }
+ *
+ * will result in the true case being in-line and starts the key with a single
+ * reference. Mixing static_key_true() and static_key_false() on the same key is not
+ * allowed.
+ *
+ * Not initializing the key (static data is initialized to 0s anyway) is the
+ * same as using STATIC_KEY_INIT_FALSE and static_key_false() is
+ * equivalent with static_branch().
+ *
+*/
 
 #include <linux/types.h>
 #include <linux/compiler.h>
@@ -39,16 +53,17 @@
 
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
 
-struct jump_label_key {
+struct static_key {
 	atomic_t enabled;
+/* Set lsb bit to 1 if branch is default true, 0 ot */
 	struct jump_entry *entries;
 #ifdef CONFIG_MODULES
-	struct jump_label_mod *next;
+	struct static_key_mod *next;
 #endif
 };
 
-struct jump_label_key_deferred {
-	struct jump_label_key key;
+struct static_key_deferred {
+	struct static_key key;
 	unsigned long timeout;
 	struct delayed_work work;
 };
@@ -66,13 +81,34 @@ struct module;
 
 #ifdef HAVE_JUMP_LABEL
 
-#ifdef CONFIG_MODULES
-#define JUMP_LABEL_INIT {ATOMIC_INIT(0), NULL, NULL}
-#else
-#define JUMP_LABEL_INIT {ATOMIC_INIT(0), NULL}
-#endif
+#define JUMP_LABEL_TRUE_BRANCH 1UL
+
+static
+inline struct jump_entry *jump_label_get_entries(struct static_key *key)
+{
+	return (struct jump_entry *)((unsigned long)key->entries
+						& ~JUMP_LABEL_TRUE_BRANCH);
+}
+
+static inline bool jump_label_get_branch_default(struct static_key *key)
+{
+	if ((unsigned long)key->entries & JUMP_LABEL_TRUE_BRANCH)
+		return true;
+	return false;
+}
+
+static __always_inline bool static_key_false(struct static_key *key)
+{
+	return arch_static_branch(key);
+}
 
-static __always_inline bool static_branch(struct jump_label_key *key)
+static __always_inline bool static_key_true(struct static_key *key)
+{
+	return !static_key_false(key);
+}
+
+/* Deprecated. Please use 'static_key_false() instead. */
+static __always_inline bool static_branch(struct static_key *key)
 {
 	return arch_static_branch(key);
 }
@@ -88,21 +124,24 @@ extern void arch_jump_label_transform(struct jump_entry *entry,
 extern void arch_jump_label_transform_static(struct jump_entry *entry,
 					     enum jump_label_type type);
 extern int jump_label_text_reserved(void *start, void *end);
-extern void jump_label_inc(struct jump_label_key *key);
-extern void jump_label_dec(struct jump_label_key *key);
-extern void jump_label_dec_deferred(struct jump_label_key_deferred *key);
-extern bool jump_label_enabled(struct jump_label_key *key);
+extern void static_key_slow_inc(struct static_key *key);
+extern void static_key_slow_dec(struct static_key *key);
+extern void static_key_slow_dec_deferred(struct static_key_deferred *key);
+extern bool static_key_enabled(struct static_key *key);
 extern void jump_label_apply_nops(struct module *mod);
-extern void jump_label_rate_limit(struct jump_label_key_deferred *key,
-		unsigned long rl);
+extern void
+jump_label_rate_limit(struct static_key_deferred *key, unsigned long rl);
+
+#define STATIC_KEY_INIT_TRUE ((struct static_key) \
+	{ .enabled = ATOMIC_INIT(1), .entries = (void *)1 })
+#define STATIC_KEY_INIT_FALSE ((struct static_key) \
+	{ .enabled = ATOMIC_INIT(0), .entries = (void *)0 })
 
 #else  /* !HAVE_JUMP_LABEL */
 
 #include <linux/atomic.h>
 
-#define JUMP_LABEL_INIT {ATOMIC_INIT(0)}
-
-struct jump_label_key {
+struct static_key {
 	atomic_t enabled;
 };
 
@@ -110,30 +149,45 @@ static __always_inline void jump_label_init(void)
 {
 }
 
-struct jump_label_key_deferred {
-	struct jump_label_key  key;
+struct static_key_deferred {
+	struct static_key  key;
 };
 
-static __always_inline bool static_branch(struct jump_label_key *key)
+static __always_inline bool static_key_false(struct static_key *key)
+{
+	if (unlikely(atomic_read(&key->enabled)) > 0)
+		return true;
+	return false;
+}
+
+static __always_inline bool static_key_true(struct static_key *key)
 {
-	if (unlikely(atomic_read(&key->enabled)))
+	if (likely(atomic_read(&key->enabled)) > 0)
 		return true;
 	return false;
 }
 
-static inline void jump_label_inc(struct jump_label_key *key)
+/* Deprecated. Please use 'static_key_false() instead. */
+static __always_inline bool static_branch(struct static_key *key)
+{
+	if (unlikely(atomic_read(&key->enabled)) > 0)
+		return true;
+	return false;
+}
+
+static inline void static_key_slow_inc(struct static_key *key)
 {
 	atomic_inc(&key->enabled);
 }
 
-static inline void jump_label_dec(struct jump_label_key *key)
+static inline void static_key_slow_dec(struct static_key *key)
 {
 	atomic_dec(&key->enabled);
 }
 
-static inline void jump_label_dec_deferred(struct jump_label_key_deferred *key)
+static inline void static_key_slow_dec_deferred(struct static_key_deferred *key)
 {
-	jump_label_dec(&key->key);
+	static_key_slow_dec(&key->key);
 }
 
 static inline int jump_label_text_reserved(void *start, void *end)
@@ -144,9 +198,9 @@ static inline int jump_label_text_reserved(void *start, void *end)
 static inline void jump_label_lock(void) {}
 static inline void jump_label_unlock(void) {}
 
-static inline bool jump_label_enabled(struct jump_label_key *key)
+static inline bool static_key_enabled(struct static_key *key)
 {
-	return !!atomic_read(&key->enabled);
+	return (atomic_read(&key->enabled) > 0);
 }
 
 static inline int jump_label_apply_nops(struct module *mod)
@@ -154,13 +208,20 @@ static inline int jump_label_apply_nops(struct module *mod)
 	return 0;
 }
 
-static inline void jump_label_rate_limit(struct jump_label_key_deferred *key,
+static inline void
+jump_label_rate_limit(struct static_key_deferred *key,
 		unsigned long rl)
 {
 }
+
+#define STATIC_KEY_INIT_TRUE ((struct static_key) \
+		{ .enabled = ATOMIC_INIT(1) })
+#define STATIC_KEY_INIT_FALSE ((struct static_key) \
+		{ .enabled = ATOMIC_INIT(0) })
+
 #endif	/* HAVE_JUMP_LABEL */
 
-#define jump_label_key_enabled	((struct jump_label_key){ .enabled = ATOMIC_INIT(1), })
-#define jump_label_key_disabled	((struct jump_label_key){ .enabled = ATOMIC_INIT(0), })
+#define STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
+#define jump_label_enabled static_key_enabled
 
 #endif	/* _LINUX_JUMP_LABEL_H */

commit efb3040d481a1594592b1defb4526c406c7a4751
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 26 13:32:15 2012 +0100

    jump_label: Add some documentation
    
    akpm figured we could do with a blub explaining what static_branch()
    is and why it lives...
    
    Grumpily-requested-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-h02wu6kabpoojxf03wke704k@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 5ce8b140428f..f7c69580fea7 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -1,6 +1,38 @@
 #ifndef _LINUX_JUMP_LABEL_H
 #define _LINUX_JUMP_LABEL_H
 
+/*
+ * Jump label support
+ *
+ * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
+ * Copyright (C) 2011-2012 Peter Zijlstra <pzijlstr@redhat.com>
+ *
+ * Jump labels provide an interface to generate dynamic branches using
+ * self-modifying code. Assuming toolchain and architecture support the result
+ * of a "if (static_branch(&key))" statement is a unconditional branch (which
+ * defaults to false - and the true block is placed out of line).
+ *
+ * However at runtime we can change the 'static' branch target using
+ * jump_label_{inc,dec}(). These function as a 'reference' count on the key
+ * object and for as long as there are references all branches referring to
+ * that particular key will point to the (out of line) true block.
+ *
+ * Since this relies on modifying code the jump_label_{inc,dec}() functions
+ * must be considered absolute slow paths (machine wide synchronization etc.).
+ * OTOH, since the affected branches are unconditional their runtime overhead
+ * will be absolutely minimal, esp. in the default (off) case where the total
+ * effect is a single NOP of appropriate size. The on case will patch in a jump
+ * to the out-of-line block.
+ *
+ * When the control is directly exposed to userspace it is prudent to delay the
+ * decrement to avoid high frequency code modifications which can (and do)
+ * cause significant performance degradation. Struct jump_label_key_deferred and
+ * jump_label_dec_deferred() provide for this.
+ *
+ * Lacking toolchain and or architecture support, it falls back to a simple
+ * conditional branch.
+ */
+
 #include <linux/types.h>
 #include <linux/compiler.h>
 #include <linux/workqueue.h>

commit ac99b862fb98a36929831791da31714f709c2aa8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jul 6 14:20:14 2011 +0200

    jump_label: Provide jump_label_key initializers
    
    Provide two initializers for jump_label_key that initialize it enabled
    or disabled. Also modify all jump_label code to allow for jump_labels to be
    initialized enabled.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jason Baron <jbaron@redhat.com>
    Link: http://lkml.kernel.org/n/tip-p40e3yj21b68y03z1yv825e7@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index a1e7f909c801..5ce8b140428f 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -128,4 +128,7 @@ static inline void jump_label_rate_limit(struct jump_label_key_deferred *key,
 }
 #endif	/* HAVE_JUMP_LABEL */
 
+#define jump_label_key_enabled	((struct jump_label_key){ .enabled = ATOMIC_INIT(1), })
+#define jump_label_key_disabled	((struct jump_label_key){ .enabled = ATOMIC_INIT(0), })
+
 #endif	/* _LINUX_JUMP_LABEL_H */

commit b202952075f62603bea9bfb6ebc6b0420db11949
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Nov 27 17:59:09 2011 +0200

    perf, core: Rate limit perf_sched_events jump_label patching
    
    jump_lable patching is very expensive operation that involves pausing all
    cpus. The patching of perf_sched_events jump_label is easily controllable
    from userspace by unprivileged user.
    
    When te user runs a loop like this:
    
      "while true; do perf stat -e cycles true; done"
    
    ... the performance of my test application that just increments a counter
    for one second drops by 4%.
    
    This is on a 16 cpu box with my test application using only one of
    them. An impact on a real server doing real work will be worse.
    
    Performance of KVM PMU drops nearly 50% due to jump_lable for "perf
    record" since KVM PMU implementation creates and destroys perf event
    frequently.
    
    This patch introduces a way to rate limit jump_label patching and uses
    it to fix the above problem.
    
    I believe that as jump_label use will spread the problem will become more
    common and thus solving it in a generic code is appropriate. Also fixing
    it in the perf code would result in moving jump_label accounting logic to
    perf code with all the ifdefs in case of JUMP_LABEL=n kernel. With this
    patch all details are nicely hidden inside jump_label code.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20111127155909.GO2557@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 388b0d425b50..a1e7f909c801 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -3,6 +3,7 @@
 
 #include <linux/types.h>
 #include <linux/compiler.h>
+#include <linux/workqueue.h>
 
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
 
@@ -14,6 +15,12 @@ struct jump_label_key {
 #endif
 };
 
+struct jump_label_key_deferred {
+	struct jump_label_key key;
+	unsigned long timeout;
+	struct delayed_work work;
+};
+
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
 #endif	/* CC_HAVE_ASM_GOTO && CONFIG_JUMP_LABEL */
@@ -51,8 +58,11 @@ extern void arch_jump_label_transform_static(struct jump_entry *entry,
 extern int jump_label_text_reserved(void *start, void *end);
 extern void jump_label_inc(struct jump_label_key *key);
 extern void jump_label_dec(struct jump_label_key *key);
+extern void jump_label_dec_deferred(struct jump_label_key_deferred *key);
 extern bool jump_label_enabled(struct jump_label_key *key);
 extern void jump_label_apply_nops(struct module *mod);
+extern void jump_label_rate_limit(struct jump_label_key_deferred *key,
+		unsigned long rl);
 
 #else  /* !HAVE_JUMP_LABEL */
 
@@ -68,6 +78,10 @@ static __always_inline void jump_label_init(void)
 {
 }
 
+struct jump_label_key_deferred {
+	struct jump_label_key  key;
+};
+
 static __always_inline bool static_branch(struct jump_label_key *key)
 {
 	if (unlikely(atomic_read(&key->enabled)))
@@ -85,6 +99,11 @@ static inline void jump_label_dec(struct jump_label_key *key)
 	atomic_dec(&key->enabled);
 }
 
+static inline void jump_label_dec_deferred(struct jump_label_key_deferred *key)
+{
+	jump_label_dec(&key->key);
+}
+
 static inline int jump_label_text_reserved(void *start, void *end)
 {
 	return 0;
@@ -102,6 +121,11 @@ static inline int jump_label_apply_nops(struct module *mod)
 {
 	return 0;
 }
+
+static inline void jump_label_rate_limit(struct jump_label_key_deferred *key,
+		unsigned long rl)
+{
+}
 #endif	/* HAVE_JUMP_LABEL */
 
 #endif	/* _LINUX_JUMP_LABEL_H */

commit 97ce2c88f9ad42e3c60a9beb9fca87abf3639faa
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Oct 12 16:17:54 2011 -0700

    jump-label: initialize jump-label subsystem much earlier
    
    Initialize jump_labels much, much earlier, so they're available for use
    during system setup.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 56594e45b011..388b0d425b50 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -16,7 +16,7 @@ struct jump_label_key {
 
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
-#endif
+#endif	/* CC_HAVE_ASM_GOTO && CONFIG_JUMP_LABEL */
 
 enum jump_label_type {
 	JUMP_LABEL_DISABLE = 0,
@@ -41,6 +41,7 @@ static __always_inline bool static_branch(struct jump_label_key *key)
 extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
+extern void jump_label_init(void);
 extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
@@ -53,7 +54,7 @@ extern void jump_label_dec(struct jump_label_key *key);
 extern bool jump_label_enabled(struct jump_label_key *key);
 extern void jump_label_apply_nops(struct module *mod);
 
-#else
+#else  /* !HAVE_JUMP_LABEL */
 
 #include <linux/atomic.h>
 
@@ -63,6 +64,10 @@ struct jump_label_key {
 	atomic_t enabled;
 };
 
+static __always_inline void jump_label_init(void)
+{
+}
+
 static __always_inline bool static_branch(struct jump_label_key *key)
 {
 	if (unlikely(atomic_read(&key->enabled)))
@@ -97,7 +102,6 @@ static inline int jump_label_apply_nops(struct module *mod)
 {
 	return 0;
 }
+#endif	/* HAVE_JUMP_LABEL */
 
-#endif
-
-#endif
+#endif	/* _LINUX_JUMP_LABEL_H */

commit 20284aa77c0f6227da4783a920b72dc61d4bcc09
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Mon Oct 3 11:01:46 2011 -0700

    jump_label: add arch_jump_label_transform_static() to optimise non-live code updates
    
    When updating a newly loaded module, the code is definitely not yet
    executing on any processor, so it can be updated with no need for any
    heavyweight synchronization.
    
    This patch adds arch_jump_label_static() which is implemented as
    arch_jump_label_transform() by default, but architectures can override
    it if it avoids, say, a call to stop_machine().
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 12e804ea32ab..56594e45b011 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -45,6 +45,8 @@ extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
 				      enum jump_label_type type);
+extern void arch_jump_label_transform_static(struct jump_entry *entry,
+					     enum jump_label_type type);
 extern int jump_label_text_reserved(void *start, void *end);
 extern void jump_label_inc(struct jump_label_key *key);
 extern void jump_label_dec(struct jump_label_key *key);

commit 37348804e0289087d21ae8bff4c0732030a3c6ac
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Sep 29 11:10:05 2011 -0700

    jump_label: if a key has already been initialized, don't nop it out
    
    If a key has been enabled before jump_label_init() is called, don't
    nop it out.
    
    This removes arch_jump_label_text_poke_early() (which can only nop
    out a site) and uses arch_jump_label_transform() instead.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 1213e9d63f79..12e804ea32ab 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -44,8 +44,7 @@ extern struct jump_entry __stop___jump_table[];
 extern void jump_label_lock(void);
 extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
-				 enum jump_label_type type);
-extern void arch_jump_label_text_poke_early(jump_label_t addr);
+				      enum jump_label_type type);
 extern int jump_label_text_reserved(void *start, void *end);
 extern void jump_label_inc(struct jump_label_key *key);
 extern void jump_label_dec(struct jump_label_key *key);

commit d5d9a3b12adf339b851c8e595d2ca71c3d211363
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Sep 28 17:45:15 2011 -0700

    jump_label: use proper atomic_t initializer
    
    ATOMIC_INIT() is the proper thing to use.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 66f23dc5e76a..1213e9d63f79 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -28,9 +28,9 @@ struct module;
 #ifdef HAVE_JUMP_LABEL
 
 #ifdef CONFIG_MODULES
-#define JUMP_LABEL_INIT {{ 0 }, NULL, NULL}
+#define JUMP_LABEL_INIT {ATOMIC_INIT(0), NULL, NULL}
 #else
-#define JUMP_LABEL_INIT {{ 0 }, NULL}
+#define JUMP_LABEL_INIT {ATOMIC_INIT(0), NULL}
 #endif
 
 static __always_inline bool static_branch(struct jump_label_key *key)

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 83e745f3ead7..66f23dc5e76a 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -54,7 +54,7 @@ extern void jump_label_apply_nops(struct module *mod);
 
 #else
 
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 
 #define JUMP_LABEL_INIT {ATOMIC_INIT(0)}
 

commit d430d3d7e646eb1eac2bb4aa244a644312e67c76
Author: Jason Baron <jbaron@redhat.com>
Date:   Wed Mar 16 17:29:47 2011 -0400

    jump label: Introduce static_branch() interface
    
    Introduce:
    
    static __always_inline bool static_branch(struct jump_label_key *key);
    
    instead of the old JUMP_LABEL(key, label) macro.
    
    In this way, jump labels become really easy to use:
    
    Define:
    
            struct jump_label_key jump_key;
    
    Can be used as:
    
            if (static_branch(&jump_key))
                    do unlikely code
    
    enable/disale via:
    
            jump_label_inc(&jump_key);
            jump_label_dec(&jump_key);
    
    that's it!
    
    For the jump labels disabled case, the static_branch() becomes an
    atomic_read(), and jump_label_inc()/dec() are simply atomic_inc(),
    atomic_dec() operations. We show testing results for this change below.
    
    Thanks to H. Peter Anvin for suggesting the 'static_branch()' construct.
    
    Since we now require a 'struct jump_label_key *key', we can store a pointer into
    the jump table addresses. In this way, we can enable/disable jump labels, in
    basically constant time. This change allows us to completely remove the previous
    hashtable scheme. Thanks to Peter Zijlstra for this re-write.
    
    Testing:
    
    I ran a series of 'tbench 20' runs 5 times (with reboots) for 3
    configurations, where tracepoints were disabled.
    
    jump label configured in
    avg: 815.6
    
    jump label *not* configured in (using atomic reads)
    avg: 800.1
    
    jump label *not* configured in (regular reads)
    avg: 803.4
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110316212947.GA8792@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Suggested-by: H. Peter Anvin <hpa@linux.intel.com>
    Tested-by: David Daney <ddaney@caviumnetworks.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 7880f18e4b86..83e745f3ead7 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -1,20 +1,43 @@
 #ifndef _LINUX_JUMP_LABEL_H
 #define _LINUX_JUMP_LABEL_H
 
+#include <linux/types.h>
+#include <linux/compiler.h>
+
 #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
+
+struct jump_label_key {
+	atomic_t enabled;
+	struct jump_entry *entries;
+#ifdef CONFIG_MODULES
+	struct jump_label_mod *next;
+#endif
+};
+
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
 #endif
 
 enum jump_label_type {
+	JUMP_LABEL_DISABLE = 0,
 	JUMP_LABEL_ENABLE,
-	JUMP_LABEL_DISABLE
 };
 
 struct module;
 
 #ifdef HAVE_JUMP_LABEL
 
+#ifdef CONFIG_MODULES
+#define JUMP_LABEL_INIT {{ 0 }, NULL, NULL}
+#else
+#define JUMP_LABEL_INIT {{ 0 }, NULL}
+#endif
+
+static __always_inline bool static_branch(struct jump_label_key *key)
+{
+	return arch_static_branch(key);
+}
+
 extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
@@ -23,37 +46,37 @@ extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
 				 enum jump_label_type type);
 extern void arch_jump_label_text_poke_early(jump_label_t addr);
-extern void jump_label_update(unsigned long key, enum jump_label_type type);
-extern void jump_label_apply_nops(struct module *mod);
 extern int jump_label_text_reserved(void *start, void *end);
+extern void jump_label_inc(struct jump_label_key *key);
+extern void jump_label_dec(struct jump_label_key *key);
+extern bool jump_label_enabled(struct jump_label_key *key);
+extern void jump_label_apply_nops(struct module *mod);
 
-#define jump_label_enable(key) \
-	jump_label_update((unsigned long)key, JUMP_LABEL_ENABLE);
+#else
 
-#define jump_label_disable(key) \
-	jump_label_update((unsigned long)key, JUMP_LABEL_DISABLE);
+#include <asm/atomic.h>
 
-#else
+#define JUMP_LABEL_INIT {ATOMIC_INIT(0)}
 
-#define JUMP_LABEL(key, label)			\
-do {						\
-	if (unlikely(*key))			\
-		goto label;			\
-} while (0)
+struct jump_label_key {
+	atomic_t enabled;
+};
 
-#define jump_label_enable(cond_var)	\
-do {					\
-       *(cond_var) = 1;			\
-} while (0)
+static __always_inline bool static_branch(struct jump_label_key *key)
+{
+	if (unlikely(atomic_read(&key->enabled)))
+		return true;
+	return false;
+}
 
-#define jump_label_disable(cond_var)	\
-do {					\
-       *(cond_var) = 0;			\
-} while (0)
+static inline void jump_label_inc(struct jump_label_key *key)
+{
+	atomic_inc(&key->enabled);
+}
 
-static inline int jump_label_apply_nops(struct module *mod)
+static inline void jump_label_dec(struct jump_label_key *key)
 {
-	return 0;
+	atomic_dec(&key->enabled);
 }
 
 static inline int jump_label_text_reserved(void *start, void *end)
@@ -64,16 +87,16 @@ static inline int jump_label_text_reserved(void *start, void *end)
 static inline void jump_label_lock(void) {}
 static inline void jump_label_unlock(void) {}
 
-#endif
+static inline bool jump_label_enabled(struct jump_label_key *key)
+{
+	return !!atomic_read(&key->enabled);
+}
 
-#define COND_STMT(key, stmt)					\
-do {								\
-	__label__ jl_enabled;					\
-	JUMP_LABEL(key, jl_enabled);				\
-	if (0) {						\
-jl_enabled:							\
-		stmt;						\
-	}							\
-} while (0)
+static inline int jump_label_apply_nops(struct module *mod)
+{
+	return 0;
+}
+
+#endif
 
 #endif

commit 45f81b1c96d9793e47ce925d257ea693ce0b193e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Oct 29 12:33:43 2010 -0400

    jump label: Add work around to i386 gcc asm goto bug
    
    On i386 (not x86_64) early implementations of gcc would have a bug
    with asm goto causing it to produce code like the following:
    
    (This was noticed by Peter Zijlstra)
    
       56 pushl 0
       67 nopl         jmp 0x6f
          popl
          jmp 0x8c
    
       6f              mov
                       test
                       je 0x8c
    
       8c mov
          call *(%esp)
    
    The jump added in the asm goto skipped over the popl that matched
    the pushl 0, which lead up to a quick crash of the system when
    the jump was enabled. The nopl is defined in the asm goto () statement
    and when tracepoints are enabled, the nop changes to a jump to the label
    that was specified by the asm goto. asm goto is suppose to tell gcc that
    the code in the asm might jump to an external label. Here gcc obviously
    fails to make that work.
    
    The bug report for gcc is here:
    
      http://gcc.gnu.org/bugzilla/show_bug.cgi?id=46226
    
    The bug only appears on x86 when not compiled with
    -maccumulate-outgoing-args. This option is always set on x86_64 and it
    is also the work around for a function graph tracer i386 bug.
    (See commit: 746357d6a526d6da9d89a2ec645b28406e959c2e)
    This explains why the bug only showed up on i386 when function graph
    tracer was not enabled.
    
    This patch now adds a CONFIG_JUMP_LABEL option that is default
    off instead of using jump labels by default. When jump labels are
    enabled, the -maccumulate-outgoing-args will be used (causing a
    slightly larger kernel image on i386). This option will exist
    until we have a way to detect if the gcc compiler in use is safe
    to use on all configurations without the work around.
    
    Note, there exists such a test, but for now we will keep the enabling
    of jump label as a manual option.
    
    Archs that know the compiler is safe with asm goto, may choose to
    select JUMP_LABEL and enable it by default.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Cause-discovered-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: David Daney <ddaney@caviumnetworks.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Richard Henderson <rth@redhat.com>
    LKML-Reference: <1288028746.3673.11.camel@laptop>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 1947a1212678..7880f18e4b86 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -1,7 +1,7 @@
 #ifndef _LINUX_JUMP_LABEL_H
 #define _LINUX_JUMP_LABEL_H
 
-#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_HAVE_ARCH_JUMP_LABEL)
+#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
 # include <asm/jump_label.h>
 # define HAVE_JUMP_LABEL
 #endif

commit 91bad2f8d3057482b9afb599f14421b007136960
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Oct 1 17:23:48 2010 -0400

    jump label: Fix deadlock b/w jump_label_mutex vs. text_mutex
    
    register_kprobe() downs the 'text_mutex' and then calls
    jump_label_text_reserved(), which downs the 'jump_label_mutex'.
    However, the jump label code takes those mutexes in the reverse
    order.
    
    Fix by requiring the caller of jump_label_text_reserved() to do
    the jump label locking via the newly added: jump_label_lock(),
    jump_label_unlock(). Currently, kprobes is the only user
    of jump_label_text_reserved().
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <759032c48d5e30c27f0bba003d09bffa8e9f28bb.1285965957.git.jbaron@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index b67cb180e6e9..1947a1212678 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -18,6 +18,8 @@ struct module;
 extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
 
+extern void jump_label_lock(void);
+extern void jump_label_unlock(void);
 extern void arch_jump_label_transform(struct jump_entry *entry,
 				 enum jump_label_type type);
 extern void arch_jump_label_text_poke_early(jump_label_t addr);
@@ -59,6 +61,9 @@ static inline int jump_label_text_reserved(void *start, void *end)
 	return 0;
 }
 
+static inline void jump_label_lock(void) {}
+static inline void jump_label_unlock(void) {}
+
 #endif
 
 #define COND_STMT(key, stmt)					\

commit ebf31f502492527e2b6b5e5cf85a4ebc7fc8a52e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Oct 17 12:15:00 2010 +0200

    jump_label: Add COND_STMT(), reducer wrappery
    
    The use of the JUMP_LABEL() construct ends up creating endless silly
    wrappers, create a higher level construct to reduce this clutter.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 81be4962b7a1..b67cb180e6e9 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -61,4 +61,14 @@ static inline int jump_label_text_reserved(void *start, void *end)
 
 #endif
 
+#define COND_STMT(key, stmt)					\
+do {								\
+	__label__ jl_enabled;					\
+	JUMP_LABEL(key, jl_enabled);				\
+	if (0) {						\
+jl_enabled:							\
+		stmt;						\
+	}							\
+} while (0)
+
 #endif

commit 3b6e901f839f42afb40f614418df82c08b01320a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 21:10:38 2010 +0200

    jump_label: Use more consistent naming
    
    Now that there's still only a few users around, rename things to make
    them more consistent.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101014203625.448565169@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index b72cd9f92c2e..81be4962b7a1 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -25,10 +25,10 @@ extern void jump_label_update(unsigned long key, enum jump_label_type type);
 extern void jump_label_apply_nops(struct module *mod);
 extern int jump_label_text_reserved(void *start, void *end);
 
-#define enable_jump_label(key) \
+#define jump_label_enable(key) \
 	jump_label_update((unsigned long)key, JUMP_LABEL_ENABLE);
 
-#define disable_jump_label(key) \
+#define jump_label_disable(key) \
 	jump_label_update((unsigned long)key, JUMP_LABEL_DISABLE);
 
 #else
@@ -39,12 +39,12 @@ do {						\
 		goto label;			\
 } while (0)
 
-#define enable_jump_label(cond_var)	\
+#define jump_label_enable(cond_var)	\
 do {					\
        *(cond_var) = 1;			\
 } while (0)
 
-#define disable_jump_label(cond_var)	\
+#define jump_label_disable(cond_var)	\
 do {					\
        *(cond_var) = 0;			\
 } while (0)

commit 4c3ef6d79328c0e23ade60cbfc8d496123a6855c
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Sep 17 11:09:08 2010 -0400

    jump label: Add jump_label_text_reserved() to reserve jump points
    
    Add a jump_label_text_reserved(void *start, void *end), so that other
    pieces of code that want to modify kernel text, can first verify that
    jump label has not reserved the instruction.
    
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <06236663a3a7b1c1f13576bb9eccb6d9c17b7bfe.1284733808.git.jbaron@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index de58656d28e0..b72cd9f92c2e 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -20,9 +20,10 @@ extern struct jump_entry __stop___jump_table[];
 
 extern void arch_jump_label_transform(struct jump_entry *entry,
 				 enum jump_label_type type);
+extern void arch_jump_label_text_poke_early(jump_label_t addr);
 extern void jump_label_update(unsigned long key, enum jump_label_type type);
 extern void jump_label_apply_nops(struct module *mod);
-extern void arch_jump_label_text_poke_early(jump_label_t addr);
+extern int jump_label_text_reserved(void *start, void *end);
 
 #define enable_jump_label(key) \
 	jump_label_update((unsigned long)key, JUMP_LABEL_ENABLE);
@@ -53,6 +54,11 @@ static inline int jump_label_apply_nops(struct module *mod)
 	return 0;
 }
 
+static inline int jump_label_text_reserved(void *start, void *end)
+{
+	return 0;
+}
+
 #endif
 
 #endif

commit bf5438fca2950b03c21ad868090cc1a8fcd49536
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Sep 17 11:09:00 2010 -0400

    jump label: Base patch for jump label
    
    base patch to implement 'jump labeling'. Based on a new 'asm goto' inline
    assembly gcc mechanism, we can now branch to labels from an 'asm goto'
    statment. This allows us to create a 'no-op' fastpath, which can subsequently
    be patched with a jump to the slowpath code. This is useful for code which
    might be rarely used, but which we'd like to be able to call, if needed.
    Tracepoints are the current usecase that these are being implemented for.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <ee8b3595967989fdaf84e698dc7447d315ce972a.1284733808.git.jbaron@redhat.com>
    
    [ cleaned up some formating ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
new file mode 100644
index 000000000000..de58656d28e0
--- /dev/null
+++ b/include/linux/jump_label.h
@@ -0,0 +1,58 @@
+#ifndef _LINUX_JUMP_LABEL_H
+#define _LINUX_JUMP_LABEL_H
+
+#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_HAVE_ARCH_JUMP_LABEL)
+# include <asm/jump_label.h>
+# define HAVE_JUMP_LABEL
+#endif
+
+enum jump_label_type {
+	JUMP_LABEL_ENABLE,
+	JUMP_LABEL_DISABLE
+};
+
+struct module;
+
+#ifdef HAVE_JUMP_LABEL
+
+extern struct jump_entry __start___jump_table[];
+extern struct jump_entry __stop___jump_table[];
+
+extern void arch_jump_label_transform(struct jump_entry *entry,
+				 enum jump_label_type type);
+extern void jump_label_update(unsigned long key, enum jump_label_type type);
+extern void jump_label_apply_nops(struct module *mod);
+extern void arch_jump_label_text_poke_early(jump_label_t addr);
+
+#define enable_jump_label(key) \
+	jump_label_update((unsigned long)key, JUMP_LABEL_ENABLE);
+
+#define disable_jump_label(key) \
+	jump_label_update((unsigned long)key, JUMP_LABEL_DISABLE);
+
+#else
+
+#define JUMP_LABEL(key, label)			\
+do {						\
+	if (unlikely(*key))			\
+		goto label;			\
+} while (0)
+
+#define enable_jump_label(cond_var)	\
+do {					\
+       *(cond_var) = 1;			\
+} while (0)
+
+#define disable_jump_label(cond_var)	\
+do {					\
+       *(cond_var) = 0;			\
+} while (0)
+
+static inline int jump_label_apply_nops(struct module *mod)
+{
+	return 0;
+}
+
+#endif
+
+#endif
