commit 466f966b1f4545acad150331911784d3f49ab1c3
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Thu May 28 09:35:11 2020 -0500

    dmaengine: Replace zero-length array with flexible-array
    
    There is a regular need in the kernel to provide a way to declare having a
    dynamically sized set of trailing elements in a structure. Kernel code should
    always use “flexible array members”[1] for these cases. The older style of
    one-element or zero-length arrays should no longer be used[2].
    
    [1] https://en.wikipedia.org/wiki/Flexible_array_member
    [2] https://github.com/KSPP/linux/issues/21
    
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index e1c03339918f..6283917edd90 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -153,7 +153,7 @@ struct dma_interleaved_template {
 	bool dst_sgl;
 	size_t numf;
 	size_t frame_size;
-	struct data_chunk sgl[0];
+	struct data_chunk sgl[];
 };
 
 /**
@@ -535,7 +535,7 @@ struct dmaengine_unmap_data {
 	struct device *dev;
 	struct kref kref;
 	size_t len;
-	dma_addr_t addr[0];
+	dma_addr_t addr[];
 };
 
 struct dma_async_tx_descriptor;

commit 0821009445a8261ac4d32a6df4b83938e007c765
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Mon Apr 13 10:40:12 2020 -0700

    dmaengine: fix channel index enumeration
    
    When the channel register code was changed to allow hotplug operations,
    dynamic indexing wasn't taken into account. When channels are randomly
    plugged and unplugged out of order, the serial indexing breaks. Convert
    channel indexing to using IDA tracking in order to allow dynamic
    assignment. The previous code does not cause any regression bug for
    existing channel allocation besides idxd driver since the hotplug usage
    case is only used by idxd at this point.
    
    With this change, the chan->idr_ref is also not needed any longer. We can
    have a device with no channels registered due to hot plug. The channel
    device release code no longer should attempt to free the dma device id on
    the last channel release.
    
    Fixes: e81274cd6b52 ("dmaengine: add support to dynamic register/unregister of channels")
    
    Reported-by: Yixin Zhang <yixin.zhang@intel.com>
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Tested-by: Yixin Zhang <yixin.zhang@intel.com>
    Link: https://lore.kernel.org/r/158679961260.7674.8485924270472851852.stgit@djiang5-desk3.ch.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 31e58ec9f741..e1c03339918f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -341,13 +341,11 @@ struct dma_chan {
  * @chan: driver channel device
  * @device: sysfs device
  * @dev_id: parent dma_device dev_id
- * @idr_ref: reference count to gate release of dma_device dev_id
  */
 struct dma_chan_dev {
 	struct dma_chan *chan;
 	struct device device;
 	int dev_id;
-	atomic_t *idr_ref;
 };
 
 /**
@@ -835,6 +833,8 @@ struct dma_device {
 	int dev_id;
 	struct device *dev;
 	struct module *owner;
+	struct ida chan_ida;
+	struct mutex chan_mutex;	/* to protect chan_ida */
 
 	u32 src_addr_widths;
 	u32 dst_addr_widths;

commit 20d60f6364474a978ab2a2146fb4c2bd9b6bbe3f
Author: Maciej Grochowski <maciej.grochowski@pm.me>
Date:   Tue Apr 14 00:17:03 2020 -0400

    include/linux/dmaengine: Typos fixes in API documentation
    
    Signed-off-by: Maciej Grochowski <maciej.grochowski@pm.me>
    Link: https://lore.kernel.org/r/20200414041703.6661-1-maciek.grochowski@gmail.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 21065c04c4ac..31e58ec9f741 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -83,9 +83,9 @@ enum dma_transfer_direction {
 /**
  * Interleaved Transfer Request
  * ----------------------------
- * A chunk is collection of contiguous bytes to be transfered.
+ * A chunk is collection of contiguous bytes to be transferred.
  * The gap(in bytes) between two chunks is called inter-chunk-gap(ICG).
- * ICGs may or maynot change between chunks.
+ * ICGs may or may not change between chunks.
  * A FRAME is the smallest series of contiguous {chunk,icg} pairs,
  *  that when repeated an integral number of times, specifies the transfer.
  * A transfer template is specification of a Frame, the number of times
@@ -1069,7 +1069,7 @@ static inline int dmaengine_terminate_all(struct dma_chan *chan)
  * dmaengine_synchronize() needs to be called before it is safe to free
  * any memory that is accessed by previously submitted descriptors or before
  * freeing any resources accessed from within the completion callback of any
- * perviously submitted descriptors.
+ * previously submitted descriptors.
  *
  * This function can be called from atomic context as well as from within a
  * complete callback of a descriptor submitted on the same channel.
@@ -1091,7 +1091,7 @@ static inline int dmaengine_terminate_async(struct dma_chan *chan)
  *
  * Synchronizes to the DMA channel termination to the current context. When this
  * function returns it is guaranteed that all transfers for previously issued
- * descriptors have stopped and and it is safe to free the memory assoicated
+ * descriptors have stopped and it is safe to free the memory associated
  * with them. Furthermore it is guaranteed that all complete callback functions
  * for a previously submitted descriptor have finished running and it is safe to
  * free resources accessed from within the complete callbacks.

commit 26cf132de6f79c06025706ddc61e045d591d404d
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Fri Mar 6 16:28:39 2020 +0200

    dmaengine: Create debug directories for DMA devices
    
    Create a placeholder directory for each registered DMA device.
    
    DMA drivers can use the dmaengine_get_debugfs_root() call to get their
    debugfs root and can populate with custom files to aim debugging.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20200306142839.17910-4-peter.ujfalusi@ti.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 72920b5cf2d7..21065c04c4ac 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -902,6 +902,7 @@ struct dma_device {
 	/* debugfs support */
 #ifdef CONFIG_DEBUG_FS
 	void (*dbg_summary_show)(struct seq_file *s, struct dma_device *dev);
+	struct dentry *dbg_dev_root;
 #endif
 };
 

commit e937cc1dd7966df33a478943817302502a164e25
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Fri Mar 6 16:28:37 2020 +0200

    dmaengine: Add basic debugfs support
    
    Via the /sys/kernel/debug/dmaengine/summary users can get information
    about the DMA devices and the used channels.
    
    Example output on am654-evm with audio using two channels and after running
    dmatest on 4 channels:
    
    dma0 (285c0000.dma-controller): number of channels: 96
    
    dma1 (31150000.dma-controller): number of channels: 267
     dma1chan0    | 2b00000.mcasp:tx
     dma1chan1    | 2b00000.mcasp:rx
     dma1chan2    | in-use
     dma1chan3    | in-use
     dma1chan4    | in-use
     dma1chan5    | in-use
    
    For slave channels we can show the device and the channel name a given
    channel is requested.
    For non slave devices the only information we know is that the channel is
    in use.
    
    DMA drivers can implement the optional dbg_summary_show callback to
    provide controller specific information instead of the generic one.
    
    It is easy to extend the generic dmaengine_summary_show() to print
    additional information about the used channels.
    
    I have taken the idea from gpiolib and clk subsystems.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20200306142839.17910-2-peter.ujfalusi@ti.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d3672f065a64..72920b5cf2d7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -300,6 +300,8 @@ struct dma_router {
  * @chan_id: channel ID for sysfs
  * @dev: class device for sysfs
  * @name: backlink name for sysfs
+ * @dbg_client_name: slave name for debugfs in format:
+ *	dev_name(requester's dev):channel name, for example: "2b00000.mcasp:tx"
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client_count: how many clients are using this channel
@@ -318,6 +320,9 @@ struct dma_chan {
 	int chan_id;
 	struct dma_chan_dev *dev;
 	const char *name;
+#ifdef CONFIG_DEBUG_FS
+	char *dbg_client_name;
+#endif
 
 	struct list_head device_node;
 	struct dma_chan_percpu __percpu *local;
@@ -806,7 +811,9 @@ struct dma_filter {
  *     called and there are no further references to this structure. This
  *     must be implemented to free resources however many existing drivers
  *     do not and are therefore not safe to unbind while in use.
- *
+ * @dbg_summary_show: optional routine to show contents in debugfs; default code
+ *     will be used when this is omitted, but custom code can show extra,
+ *     controller specific information.
  */
 struct dma_device {
 	struct kref ref;
@@ -892,6 +899,10 @@ struct dma_device {
 					    struct dma_tx_state *txstate);
 	void (*device_issue_pending)(struct dma_chan *chan);
 	void (*device_release)(struct dma_device *dev);
+	/* debugfs support */
+#ifdef CONFIG_DEBUG_FS
+	void (*dbg_summary_show)(struct seq_file *s, struct dma_device *dev);
+#endif
 };
 
 static inline int dmaengine_slave_config(struct dma_chan *chan,

commit 1873300afa6147a1882aeba1e8bc9a13c5487571
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Wed Feb 26 12:18:41 2020 +0200

    dmaengine: consistently return string literal from switch-case
    
    There is no need to have 'break;' statement in the default case followed by
    return certain string literal when all other cases have returned the string
    literals. So, refactor it accordingly.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Reviewed-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20200226101842.29426-4-andriy.shevchenko@linux.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1bb5477ef7ec..d3672f065a64 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1560,9 +1560,7 @@ dmaengine_get_direction_text(enum dma_transfer_direction dir)
 	case DMA_DEV_TO_DEV:
 		return "DEV_TO_DEV";
 	default:
-		break;
+		return "invalid";
 	}
-
-	return "invalid";
 }
 #endif /* DMAENGINE_H */

commit 5f77dd850c0a32d4d5047d139077718ee7f1a8fe
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Wed Feb 26 12:18:40 2020 +0200

    dmaengine: Drop redundant 'else' keyword
    
    It's obvious that 'else' keyword is redundant in the code like
    
            if (foo)
                    return bar;
            else if (baz)
                    ...
    
    Drop it for good.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Link: https://lore.kernel.org/r/20200226101842.29426-3-andriy.shevchenko@linux.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ae56a91c2a05..1bb5477ef7ec 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1230,9 +1230,9 @@ static inline int dma_maxpq(struct dma_device *dma, enum dma_ctrl_flags flags)
 {
 	if (dma_dev_has_pq_continue(dma) || !dmaf_continue(flags))
 		return dma_dev_to_maxpq(dma);
-	else if (dmaf_p_disabled_continue(flags))
+	if (dmaf_p_disabled_continue(flags))
 		return dma_dev_to_maxpq(dma) - 1;
-	else if (dmaf_continue(flags))
+	if (dmaf_continue(flags))
 		return dma_dev_to_maxpq(dma) - 3;
 	BUG();
 }
@@ -1243,7 +1243,7 @@ static inline size_t dmaengine_get_icg(bool inc, bool sgl, size_t icg,
 	if (inc) {
 		if (dir_icg)
 			return dir_icg;
-		else if (sgl)
+		if (sgl)
 			return icg;
 	}
 

commit 3a92063be16873a10648a81be0b1be42a9d54ee9
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Wed Feb 26 12:18:39 2020 +0200

    dmaengine: Use negative condition for better readability
    
    When negative condition is in use we may decrease indentation level
    and make the main part of logic better visible.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Reviewed-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20200226101842.29426-2-andriy.shevchenko@linux.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9f3f5582816a..ae56a91c2a05 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -618,10 +618,11 @@ static inline void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)
 
 static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
 {
-	if (tx->unmap) {
-		dmaengine_unmap_put(tx->unmap);
-		tx->unmap = NULL;
-	}
+	if (!tx->unmap)
+		return;
+
+	dmaengine_unmap_put(tx->unmap);
+	tx->unmap = NULL;
 }
 
 #ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
@@ -1408,11 +1409,12 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 static inline void
 dma_set_tx_state(struct dma_tx_state *st, dma_cookie_t last, dma_cookie_t used, u32 residue)
 {
-	if (st) {
-		st->last = last;
-		st->used = used;
-		st->residue = residue;
-	}
+	if (!st)
+		return;
+
+	st->last = last;
+	st->used = used;
+	st->residue = residue;
 }
 
 #ifdef CONFIG_DMA_ENGINE
@@ -1489,12 +1491,11 @@ static inline int dmaengine_desc_set_reuse(struct dma_async_tx_descriptor *tx)
 	if (ret)
 		return ret;
 
-	if (caps.descriptor_reuse) {
-		tx->flags |= DMA_CTRL_REUSE;
-		return 0;
-	} else {
+	if (!caps.descriptor_reuse)
 		return -EPERM;
-	}
+
+	tx->flags |= DMA_CTRL_REUSE;
+	return 0;
 }
 
 static inline void dmaengine_desc_clear_reuse(struct dma_async_tx_descriptor *tx)
@@ -1510,10 +1511,10 @@ static inline bool dmaengine_desc_test_reuse(struct dma_async_tx_descriptor *tx)
 static inline int dmaengine_desc_free(struct dma_async_tx_descriptor *desc)
 {
 	/* this is supported for reusable desc, so check that */
-	if (dmaengine_desc_test_reuse(desc))
-		return desc->desc_free(desc);
-	else
+	if (!dmaengine_desc_test_reuse(desc))
 		return -EPERM;
+
+	return desc->desc_free(desc);
 }
 
 /* --- DMA device --- */

commit 88ac039cbed125bd9ed132d27ec9f689c6442748
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Wed Feb 26 12:18:38 2020 +0200

    dmaengine: Refactor dmaengine_check_align() to be bit operations only
    
    There is no need to have branch and temporary variable in the function.
    Simple convert it to be a set of bit and arithmetic operations.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Reviewed-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20200226101842.29426-1-andriy.shevchenko@linux.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 64461fc64e1b..9f3f5582816a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1155,14 +1155,7 @@ static inline dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc
 static inline bool dmaengine_check_align(enum dmaengine_alignment align,
 					 size_t off1, size_t off2, size_t len)
 {
-	size_t mask;
-
-	if (!align)
-		return true;
-	mask = (1 << align) - 1;
-	if (mask & (off1 | off2 | len))
-		return false;
-	return true;
+	return !(((1 << align) - 1) & (off1 | off2 | len));
 }
 
 static inline bool is_dma_copy_aligned(struct dma_device *dev, size_t off1,

commit a5b871c91d470326eed3ae0ebd2fc07f3aee9050
Merge: 715d12856953 71723a96b8b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 10:55:50 2020 -0800

    Merge tag 'dmaengine-5.6-rc1' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull dmaengine updates from Vinod Koul:
     "This time we have a bunch of core changes to support dynamic channels,
      hotplug of controllers, new apis for metadata ops etc along with new
      drivers for Intel data accelerators, TI K3 UDMA, PLX DMA engine and
      hisilicon Kunpeng DMA engine. Also usual assorted updates to drivers.
    
      Core:
       - Support for dynamic channels
       - Removal of various slave wrappers
       - Make few slave request APIs as private to dmaengine
       - Symlinks between channels and slaves
       - Support for hotplug of controllers
       - Support for metadata_ops for dma_async_tx_descriptor
       - Reporting DMA cached data amount
       - Virtual dma channel locking updates
    
      New drivers/device/feature support support:
       - Driver for Intel data accelerators
       - Driver for TI K3 UDMA
       - Driver for PLX DMA engine
       - Driver for hisilicon Kunpeng DMA engine
       - Support for eDMA support for QorIQ LS1028A in fsl edma driver
       - Support for cyclic dma in sun4i driver
       - Support for X1830 in JZ4780 driver"
    
    * tag 'dmaengine-5.6-rc1' of git://git.infradead.org/users/vkoul/slave-dma: (62 commits)
      dmaengine: Create symlinks between DMA channels and slaves
      dmaengine: hisilicon: Add Kunpeng DMA engine support
      dmaengine: idxd: add char driver to expose submission portal to userland
      dmaengine: idxd: connect idxd to dmaengine subsystem
      dmaengine: idxd: add descriptor manipulation routines
      dmaengine: idxd: add sysfs ABI for idxd driver
      dmaengine: idxd: add configuration component of driver
      dmaengine: idxd: Init and probe for Intel data accelerators
      dmaengine: add support to dynamic register/unregister of channels
      dmaengine: break out channel registration
      x86/asm: add iosubmit_cmds512() based on MOVDIR64B CPU instruction
      dmaengine: ti: k3-udma: fix spelling mistake "limted" -> "limited"
      dmaengine: s3c24xx-dma: fix spelling mistake "to" -> "too"
      dmaengine: Move dma_get_{,any_}slave_channel() to private dmaengine.h
      dmaengine: Remove dma_request_slave_channel_compat() wrapper
      dmaengine: Remove dma_device_satisfies_mask() wrapper
      dt-bindings: fsl-imx-sdma: Add i.MX8MM/i.MX8MN/i.MX8MP compatible string
      dmaengine: zynqmp_dma: fix burst length configuration
      dmaengine: sun4i: Add support for cyclic requests with dedicated DMA
      dmaengine: fsl-qdma: fix duplicated argument to &&
      ...

commit 71723a96b8b1367fefc18f60025dae792477d602
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Fri Jan 17 16:30:56 2020 +0100

    dmaengine: Create symlinks between DMA channels and slaves
    
    Currently it is not easy to find out which DMA channels are in use, and
    which slave devices are using which channels.
    
    Fix this by creating two symlinks between the DMA channel and the actual
    slave device when a channel is requested:
      1. A "slave" symlink from DMA channel to slave device,
      2. A "dma:<name>" symlink slave device to DMA channel.
    When the channel is released, the symlinks are removed again.
    The latter requires keeping track of the slave device and the channel
    name in the dma_chan structure.
    
    Note that this is limited to channel request functions for requesting an
    exclusive slave channel that take a device pointer (dma_request_chan()
    and dma_request_slave_channel*()).
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Niklas Söderlund <niklas.soderlund@ragnatech.se>
    Link: https://lore.kernel.org/r/20200117153056.31363-1-geert+renesas@glider.be
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index f52f274773ed..fef69a9c5824 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -294,10 +294,12 @@ struct dma_router {
 /**
  * struct dma_chan - devices supply DMA channels, clients use them
  * @device: ptr to the dma device who supplies this channel, always !%NULL
+ * @slave: ptr to the device using this channel
  * @cookie: last cookie value returned to client
  * @completed_cookie: last completed cookie for this channel
  * @chan_id: channel ID for sysfs
  * @dev: class device for sysfs
+ * @name: backlink name for sysfs
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client_count: how many clients are using this channel
@@ -308,12 +310,14 @@ struct dma_router {
  */
 struct dma_chan {
 	struct dma_device *device;
+	struct device *slave;
 	dma_cookie_t cookie;
 	dma_cookie_t completed_cookie;
 
 	/* sysfs */
 	int chan_id;
 	struct dma_chan_dev *dev;
+	const char *name;
 
 	struct list_head device_node;
 	struct dma_chan_percpu __percpu *local;

commit e81274cd6b5264809384066e09a5253708822522
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Jan 21 16:43:53 2020 -0700

    dmaengine: add support to dynamic register/unregister of channels
    
    With the channel registration routines broken out, now add support code to
    allow independent registering and unregistering of channels in a hotplug fashion.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Link: https://lore.kernel.org/r/157965023364.73301.7821862091077299040.stgit@djiang5-desk3.ch.intel.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9cc0e70e7c35..f52f274773ed 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1521,6 +1521,10 @@ static inline int dmaengine_desc_free(struct dma_async_tx_descriptor *desc)
 int dma_async_device_register(struct dma_device *device);
 int dmaenginem_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
+int dma_async_device_channel_register(struct dma_device *device,
+				      struct dma_chan *chan);
+void dma_async_device_channel_unregister(struct dma_device *device,
+					 struct dma_chan *chan);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 #define dma_request_channel(mask, x, y) \
 	__dma_request_channel(&(mask), x, y, NULL)

commit c3c431de99c068e3f64d01335c1532b22e4b1d1b
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Tue Jan 21 10:33:11 2020 +0100

    dmaengine: Move dma_get_{,any_}slave_channel() to private dmaengine.h
    
    The functions dma_get_slave_channel() and dma_get_any_slave_channel()
    are called from DMA engine drivers only.  Hence move their declarations
    from the public header file <linux/dmaengine.h> to the private header
    file drivers/dma/dmaengine.h.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Link: https://lore.kernel.org/r/20200121093311.28639-4-geert+renesas@glider.be
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 230d50ef7360..9cc0e70e7c35 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1522,8 +1522,6 @@ int dma_async_device_register(struct dma_device *device);
 int dmaenginem_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
-struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
-struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
 #define dma_request_channel(mask, x, y) \
 	__dma_request_channel(&(mask), x, y, NULL)
 

commit 71ca5b78235e79c36f774773491064d7921d1942
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Tue Jan 21 10:33:10 2020 +0100

    dmaengine: Remove dma_request_slave_channel_compat() wrapper
    
    At its original introduction, dma_request_slave_channel_compat() used a
    wrapper, to accommodate filter functions that modify the mask passed.
    Filter functions can no longer modify masks, and the mask parameter was
    made const in commit a53e28da574a40bc ("dma: Make the 'mask' parameter
    of __dma_request_channel const") consecutively.
    
    Hence remove the wrapper, and rename __dma_request_slave_channel_compat()
    to dma_request_slave_channel_compat(), to get rid of one more function
    name starting with a double underscore.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Link: https://lore.kernel.org/r/20200121093311.28639-3-geert+renesas@glider.be
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 62225d46908b..230d50ef7360 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1526,11 +1526,9 @@ struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
 struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
 #define dma_request_channel(mask, x, y) \
 	__dma_request_channel(&(mask), x, y, NULL)
-#define dma_request_slave_channel_compat(mask, x, y, dev, name) \
-	__dma_request_slave_channel_compat(&(mask), x, y, dev, name)
 
 static inline struct dma_chan
-*__dma_request_slave_channel_compat(const dma_cap_mask_t *mask,
+*dma_request_slave_channel_compat(const dma_cap_mask_t mask,
 				  dma_filter_fn fn, void *fn_param,
 				  struct device *dev, const char *name)
 {
@@ -1543,7 +1541,7 @@ static inline struct dma_chan
 	if (!fn || !fn_param)
 		return NULL;
 
-	return __dma_request_channel(mask, fn, fn_param, NULL);
+	return __dma_request_channel(&mask, fn, fn_param, NULL);
 }
 
 static inline char *

commit 816ebf48442eef1c61db26d2ec055f5c8ac83b21
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Mon Dec 23 13:04:46 2019 +0200

    dmaengine: Add helper function to convert direction value to text
    
    dmaengine_get_direction_text() can be useful when the direction is printed
    out. The text is easier to comprehend than the number.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Link: https://lore.kernel.org/r/20191223110458.30766-7-peter.ujfalusi@ti.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b44b9c608709..62225d46908b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1545,4 +1545,23 @@ static inline struct dma_chan
 
 	return __dma_request_channel(mask, fn, fn_param, NULL);
 }
+
+static inline char *
+dmaengine_get_direction_text(enum dma_transfer_direction dir)
+{
+	switch (dir) {
+	case DMA_DEV_TO_MEM:
+		return "DEV_TO_MEM";
+	case DMA_MEM_TO_DEV:
+		return "MEM_TO_DEV";
+	case DMA_MEM_TO_MEM:
+		return "MEM_TO_MEM";
+	case DMA_DEV_TO_DEV:
+		return "DEV_TO_DEV";
+	default:
+		break;
+	}
+
+	return "invalid";
+}
 #endif /* DMAENGINE_H */

commit 6755ec06d1333765d2b935e4e4a5bd011332bac6
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Mon Dec 23 13:04:45 2019 +0200

    dmaengine: Add support for reporting DMA cached data amount
    
    A DMA hardware can have big cache or FIFO and the amount of data sitting in
    the DMA fabric can be an interest for the clients.
    
    For example in audio we want to know the delay in the data flow and in case
    the DMA have significantly large FIFO/cache, it can affect the latenc/delay
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Reviewed-by: Tero Kristo <t-kristo@ti.com>
    Tested-by: Keerthy <j-keerthy@ti.com>
    Reviewed-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Link: https://lore.kernel.org/r/20191223110458.30766-6-peter.ujfalusi@ti.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 68b361891a61..b44b9c608709 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -686,11 +686,13 @@ static inline struct dma_async_tx_descriptor *txd_next(struct dma_async_tx_descr
  * @residue: the remaining number of bytes left to transmit
  *	on the selected transfer for states DMA_IN_PROGRESS and
  *	DMA_PAUSED if this is implemented in the driver, else 0
+ * @in_flight_bytes: amount of data in bytes cached by the DMA.
  */
 struct dma_tx_state {
 	dma_cookie_t last;
 	dma_cookie_t used;
 	u32 residue;
+	u32 in_flight_bytes;
 };
 
 /**

commit 4db8fd32ed2be7cc510e51e43ec3349aa64074a9
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Mon Dec 23 13:04:44 2019 +0200

    dmaengine: Add metadata_ops for dma_async_tx_descriptor
    
    The metadata is best described as side band data or parameters traveling
    alongside the data DMAd by the DMA engine. It is data
    which is understood by the peripheral and the peripheral driver only, the
    DMA engine see it only as data block and it is not interpreting it in any
    way.
    
    The metadata can be different per descriptor as it is a parameter for the
    data being transferred.
    
    If the DMA supports per descriptor metadata it can implement the attach,
    get_ptr/set_len callbacks.
    
    Client drivers must only use either attach or get_ptr/set_len to avoid
    misconfiguration.
    
    Client driver can check if a given metadata mode is supported by the
    channel during probe time with
    dmaengine_is_metadata_mode_supported(chan, DESC_METADATA_CLIENT);
    dmaengine_is_metadata_mode_supported(chan, DESC_METADATA_ENGINE);
    
    and based on this information can use either mode.
    
    Wrappers are also added for the metadata_ops.
    
    To be used in DESC_METADATA_CLIENT mode:
    dmaengine_desc_attach_metadata()
    
    To be used in DESC_METADATA_ENGINE mode:
    dmaengine_desc_get_metadata_ptr()
    dmaengine_desc_set_metadata_len()
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Reviewed-by: Tero Kristo <t-kristo@ti.com>
    Tested-by: Keerthy <j-keerthy@ti.com>
    Reviewed-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Link: https://lore.kernel.org/r/20191223110458.30766-5-peter.ujfalusi@ti.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 7927731e3716..68b361891a61 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -219,6 +219,62 @@ typedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;
  * @bytes_transferred: byte counter
  */
 
+/**
+ * enum dma_desc_metadata_mode - per descriptor metadata mode types supported
+ * @DESC_METADATA_CLIENT - the metadata buffer is allocated/provided by the
+ *  client driver and it is attached (via the dmaengine_desc_attach_metadata()
+ *  helper) to the descriptor.
+ *
+ * Client drivers interested to use this mode can follow:
+ * - DMA_MEM_TO_DEV / DEV_MEM_TO_MEM:
+ *   1. prepare the descriptor (dmaengine_prep_*)
+ *	construct the metadata in the client's buffer
+ *   2. use dmaengine_desc_attach_metadata() to attach the buffer to the
+ *	descriptor
+ *   3. submit the transfer
+ * - DMA_DEV_TO_MEM:
+ *   1. prepare the descriptor (dmaengine_prep_*)
+ *   2. use dmaengine_desc_attach_metadata() to attach the buffer to the
+ *	descriptor
+ *   3. submit the transfer
+ *   4. when the transfer is completed, the metadata should be available in the
+ *	attached buffer
+ *
+ * @DESC_METADATA_ENGINE - the metadata buffer is allocated/managed by the DMA
+ *  driver. The client driver can ask for the pointer, maximum size and the
+ *  currently used size of the metadata and can directly update or read it.
+ *  dmaengine_desc_get_metadata_ptr() and dmaengine_desc_set_metadata_len() is
+ *  provided as helper functions.
+ *
+ *  Note: the metadata area for the descriptor is no longer valid after the
+ *  transfer has been completed (valid up to the point when the completion
+ *  callback returns if used).
+ *
+ * Client drivers interested to use this mode can follow:
+ * - DMA_MEM_TO_DEV / DEV_MEM_TO_MEM:
+ *   1. prepare the descriptor (dmaengine_prep_*)
+ *   2. use dmaengine_desc_get_metadata_ptr() to get the pointer to the engine's
+ *	metadata area
+ *   3. update the metadata at the pointer
+ *   4. use dmaengine_desc_set_metadata_len()  to tell the DMA engine the amount
+ *	of data the client has placed into the metadata buffer
+ *   5. submit the transfer
+ * - DMA_DEV_TO_MEM:
+ *   1. prepare the descriptor (dmaengine_prep_*)
+ *   2. submit the transfer
+ *   3. on transfer completion, use dmaengine_desc_get_metadata_ptr() to get the
+ *	pointer to the engine's metadata area
+ *   4. Read out the metadata from the pointer
+ *
+ * Note: the two mode is not compatible and clients must use one mode for a
+ * descriptor.
+ */
+enum dma_desc_metadata_mode {
+	DESC_METADATA_NONE = 0,
+	DESC_METADATA_CLIENT = BIT(0),
+	DESC_METADATA_ENGINE = BIT(1),
+};
+
 struct dma_chan_percpu {
 	/* stats */
 	unsigned long memcpy_count;
@@ -475,6 +531,18 @@ struct dmaengine_unmap_data {
 	dma_addr_t addr[0];
 };
 
+struct dma_async_tx_descriptor;
+
+struct dma_descriptor_metadata_ops {
+	int (*attach)(struct dma_async_tx_descriptor *desc, void *data,
+		      size_t len);
+
+	void *(*get_ptr)(struct dma_async_tx_descriptor *desc,
+			 size_t *payload_len, size_t *max_len);
+	int (*set_len)(struct dma_async_tx_descriptor *desc,
+		       size_t payload_len);
+};
+
 /**
  * struct dma_async_tx_descriptor - async transaction descriptor
  * ---dma generic offload fields---
@@ -488,6 +556,11 @@ struct dmaengine_unmap_data {
  * descriptor pending. To be pushed on .issue_pending() call
  * @callback: routine to call after this operation is complete
  * @callback_param: general parameter to pass to the callback routine
+ * @desc_metadata_mode: core managed metadata mode to protect mixed use of
+ *	DESC_METADATA_CLIENT or DESC_METADATA_ENGINE. Otherwise
+ *	DESC_METADATA_NONE
+ * @metadata_ops: DMA driver provided metadata mode ops, need to be set by the
+ *	DMA driver if metadata mode is supported with the descriptor
  * ---async_tx api specific fields---
  * @next: at completion submit this descriptor
  * @parent: pointer to the next level up in the dependency chain
@@ -504,6 +577,8 @@ struct dma_async_tx_descriptor {
 	dma_async_tx_callback_result callback_result;
 	void *callback_param;
 	struct dmaengine_unmap_data *unmap;
+	enum dma_desc_metadata_mode desc_metadata_mode;
+	struct dma_descriptor_metadata_ops *metadata_ops;
 #ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 	struct dma_async_tx_descriptor *next;
 	struct dma_async_tx_descriptor *parent;
@@ -666,6 +741,7 @@ struct dma_filter {
  * @global_node: list_head for global dma_device_list
  * @filter: information for device/slave to filter function/param mapping
  * @cap_mask: one or more dma_capability flags
+ * @desc_metadata_modes: supported metadata modes by the DMA device
  * @max_xor: maximum number of xor sources, 0 if no capability
  * @max_pq: maximum number of PQ sources and PQ-continue capability
  * @copy_align: alignment shift for memcpy operations
@@ -733,6 +809,7 @@ struct dma_device {
 	struct list_head global_node;
 	struct dma_filter filter;
 	dma_cap_mask_t  cap_mask;
+	enum dma_desc_metadata_mode desc_metadata_modes;
 	unsigned short max_xor;
 	unsigned short max_pq;
 	enum dmaengine_alignment copy_align;
@@ -910,6 +987,41 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memcpy(
 						    len, flags);
 }
 
+static inline bool dmaengine_is_metadata_mode_supported(struct dma_chan *chan,
+		enum dma_desc_metadata_mode mode)
+{
+	if (!chan)
+		return false;
+
+	return !!(chan->device->desc_metadata_modes & mode);
+}
+
+#ifdef CONFIG_DMA_ENGINE
+int dmaengine_desc_attach_metadata(struct dma_async_tx_descriptor *desc,
+				   void *data, size_t len);
+void *dmaengine_desc_get_metadata_ptr(struct dma_async_tx_descriptor *desc,
+				      size_t *payload_len, size_t *max_len);
+int dmaengine_desc_set_metadata_len(struct dma_async_tx_descriptor *desc,
+				    size_t payload_len);
+#else /* CONFIG_DMA_ENGINE */
+static inline int dmaengine_desc_attach_metadata(
+		struct dma_async_tx_descriptor *desc, void *data, size_t len)
+{
+	return -EINVAL;
+}
+static inline void *dmaengine_desc_get_metadata_ptr(
+		struct dma_async_tx_descriptor *desc, size_t *payload_len,
+		size_t *max_len)
+{
+	return NULL;
+}
+static inline int dmaengine_desc_set_metadata_len(
+		struct dma_async_tx_descriptor *desc, size_t payload_len)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_DMA_ENGINE */
+
 /**
  * dmaengine_terminate_all() - Terminate all active DMA transfers
  * @chan: The channel for which to terminate the transfers

commit 8ad342a863590b24ce77681b7e081363fb3333f7
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Mon Dec 16 12:01:19 2019 -0700

    dmaengine: Add reference counting to dma_device struct
    
    Adding a reference count helps drivers to properly implement the unbind
    while in use case.
    
    References are taken and put every time a channel is allocated or freed.
    
    Once the final reference is put, the device is removed from the
    dma_device_list and a release callback function is called to signal
    the driver to free the memory.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Link: https://lore.kernel.org/r/20191216190120.21374-5-logang@deltatee.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 11b15a2e97a0..7927731e3716 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -719,9 +719,14 @@ struct dma_filter {
  *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
  * @descriptor_reuse: a submitted transfer can be resubmitted after completion
+ * @device_release: called sometime atfer dma_async_device_unregister() is
+ *     called and there are no further references to this structure. This
+ *     must be implemented to free resources however many existing drivers
+ *     do not and are therefore not safe to unbind while in use.
+ *
  */
 struct dma_device {
-
+	struct kref ref;
 	unsigned int chancnt;
 	unsigned int privatecnt;
 	struct list_head channels;
@@ -802,6 +807,7 @@ struct dma_device {
 					    dma_cookie_t cookie,
 					    struct dma_tx_state *txstate);
 	void (*device_issue_pending)(struct dma_chan *chan);
+	void (*device_release)(struct dma_device *dev);
 };
 
 static inline int dmaengine_slave_config(struct dma_chan *chan,

commit dae7a589c18a4d979d5f14b09374e871b995ceb1
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Mon Dec 16 12:01:16 2019 -0700

    dmaengine: Store module owner in dma_device struct
    
    dma_chan_to_owner() dereferences the driver from the struct device to
    obtain the owner and call module_[get|put](). However, if the backing
    device is unbound before the dma_device is unregistered, the driver
    will be cleared and this will cause a NULL pointer dereference.
    
    Instead, store a pointer to the owner module in the dma_device struct
    so the module reference can be properly put when the channel is put, even
    if the backing device was destroyed first.
    
    This change helps to support a safer unbind of DMA engines.
    If the dma_device is unregistered in the driver's remove function,
    there's no guarantee that there are no existing clients and a users
    action may trigger the WARN_ONCE in dma_async_device_unregister()
    which is unlikely to leave the system in a consistent state.
    Instead, a better approach is to allow the backing driver to go away
    and fail any subsequent requests to it.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Link: https://lore.kernel.org/r/20191216190120.21374-2-logang@deltatee.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index dfd2d35b64af..11b15a2e97a0 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -674,6 +674,7 @@ struct dma_filter {
  * @fill_align: alignment shift for memset operations
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
+ * @owner: owner module (automatically set based on the provided dev)
  * @src_addr_widths: bit mask of src addr widths the device supports
  *	Width is specified in bytes, e.g. for a device supporting
  *	a width of 4 the mask should have BIT(4) set.
@@ -737,6 +738,7 @@ struct dma_device {
 
 	int dev_id;
 	struct device *dev;
+	struct module *owner;
 
 	u32 src_addr_widths;
 	u32 dst_addr_widths;

commit 53a256a9b925b47c7e67fc1f16ca41561a7b877c
Author: Lukas Wunner <lukas@wunner.de>
Date:   Thu Dec 5 12:54:49 2019 +0100

    dmaengine: Fix access to uninitialized dma_slave_caps
    
    dmaengine_desc_set_reuse() allocates a struct dma_slave_caps on the
    stack, populates it using dma_get_slave_caps() and then accesses one
    of its members.
    
    However dma_get_slave_caps() may fail and this isn't accounted for,
    leading to a legitimate warning of gcc-4.9 (but not newer versions):
    
       In file included from drivers/spi/spi-bcm2835.c:19:0:
       drivers/spi/spi-bcm2835.c: In function 'dmaengine_desc_set_reuse':
    >> include/linux/dmaengine.h:1370:10: warning: 'caps.descriptor_reuse' is used uninitialized in this function [-Wuninitialized]
         if (caps.descriptor_reuse) {
    
    Fix it, thereby also silencing the gcc-4.9 warning.
    
    The issue has been present for 4 years but surfaces only now that
    the first caller of dmaengine_desc_set_reuse() has been added in
    spi-bcm2835.c. Another user of reusable DMA descriptors has existed
    for a while in pxa_camera.c, but it sets the DMA_CTRL_REUSE flag
    directly instead of calling dmaengine_desc_set_reuse(). Nevertheless,
    tag this commit for stable in case there are out-of-tree users.
    
    Fixes: 272420214d26 ("dmaengine: Add DMA_CTRL_REUSE")
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Lukas Wunner <lukas@wunner.de>
    Cc: stable@vger.kernel.org # v4.3+
    Link: https://lore.kernel.org/r/ca92998ccc054b4f2bfd60ef3adbab2913171eac.1575546234.git.lukas@wunner.de
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8fcdee1c0cf9..dad4a68fa009 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1364,8 +1364,11 @@ static inline int dma_get_slave_caps(struct dma_chan *chan,
 static inline int dmaengine_desc_set_reuse(struct dma_async_tx_descriptor *tx)
 {
 	struct dma_slave_caps caps;
+	int ret;
 
-	dma_get_slave_caps(tx->chan, &caps);
+	ret = dma_get_slave_caps(tx->chan, &caps);
+	if (ret)
+		return ret;
 
 	if (caps.descriptor_reuse) {
 		tx->flags |= DMA_CTRL_REUSE;

commit dda510890498b9a2f4b2142192f6d516c6c1e2e5
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Fri Dec 6 14:24:35 2019 +0100

    dmaengine: Remove spaces before TABs
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Link: https://lore.kernel.org/r/20191206132435.29139-1-geert+renesas@glider.be
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8fcdee1c0cf9..dfd2d35b64af 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -481,7 +481,7 @@ struct dmaengine_unmap_data {
  * @cookie: tracking cookie for this transaction, set to -EBUSY if
  *	this tx is sitting on a dependency list
  * @flags: flags to augment operation preparation, control completion, and
- * 	communicate status
+ *	communicate status
  * @phys: physical address of the descriptor
  * @chan: target channel for this operation
  * @tx_submit: accept the descriptor, assign ordered cookie and mark the

commit 47ebe00b684c2bc183a766bc33c8b5943bc0df85
Merge: fa121bb3fed6 5c274ca4cfb2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 17 09:55:43 2019 -0700

    Merge tag 'dmaengine-5.3-rc1' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull dmaengine updates from Vinod Koul:
    
     - Add support in dmaengine core to do device node checks for DT devices
       and update bunch of drivers to use that and remove open coding from
       drivers
    
     - New driver/driver support for new hardware, namely:
         - MediaTek UART APDMA
         - Freescale i.mx7ulp edma2
         - Synopsys eDMA IP core version 0
         - Allwinner H6 DMA
    
     - Updates to axi-dma and support for interleaved cyclic transfers
    
     - Greg's debugfs return value check removals on drivers
    
     - Updates to stm32-dma, hsu, dw, pl330, tegra drivers
    
    * tag 'dmaengine-5.3-rc1' of git://git.infradead.org/users/vkoul/slave-dma: (68 commits)
      dmaengine: Revert "dmaengine: fsl-edma: add i.mx7ulp edma2 version support"
      dmaengine: at_xdmac: check for non-empty xfers_list before invoking callback
      Documentation: dmaengine: clean up description of dmatest usage
      dmaengine: tegra210-adma: remove PM_CLK dependency
      dmaengine: fsl-edma: add i.mx7ulp edma2 version support
      dt-bindings: dma: fsl-edma: add new i.mx7ulp-edma
      dmaengine: fsl-edma-common: version check for v2 instead
      dmaengine: fsl-edma-common: move dmamux register to another single function
      dmaengine: fsl-edma: add drvdata for fsl-edma
      dmaengine: Revert "dmaengine: fsl-edma: support little endian for edma driver"
      dmaengine: rcar-dmac: Reject zero-length slave DMA requests
      dmaengine: dw: Enable iDMA 32-bit on Intel Elkhart Lake
      dmaengine: dw-edma: fix semicolon.cocci warnings
      dmaengine: sh: usb-dmac: Use [] to denote a flexible array member
      dmaengine: dmatest: timeout value of -1 should specify infinite wait
      dmaengine: dw: Distinguish ->remove() between DW and iDMA 32-bit
      dmaengine: fsl-edma: support little endian for edma driver
      dmaengine: hsu: Revert "set HSU_CH_MTSR to memory width"
      dmagengine: pl330: add code to get reset property
      dt-bindings: pl330: document the optional resets property
      ...

commit f5151311c3f37f6edc85b2253ccf6d3e2a4c4c26
Author: Baolin Wang <baolin.wang@linaro.org>
Date:   Mon May 20 19:32:14 2019 +0800

    dmaengine: Add matching device node validation in __dma_request_channel()
    
    When user try to request one DMA channel by __dma_request_channel(), it won't
    validate if it is the correct DMA device to request, that will lead each DMA
    engine driver to validate the correct device node in their filter function
    if it is necessary.
    
    Thus we can add the matching device node validation in the DMA engine core,
    to remove all of device node validation in the drivers.
    
    Tested-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Baolin Wang <baolin.wang@linaro.org>
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d49ec5c31944..504085b2bf21 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1314,7 +1314,8 @@ enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
-					dma_filter_fn fn, void *fn_param);
+				       dma_filter_fn fn, void *fn_param,
+				       struct device_node *np);
 struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
 
 struct dma_chan *dma_request_chan(struct device *dev, const char *name);
@@ -1339,7 +1340,9 @@ static inline void dma_issue_pending_all(void)
 {
 }
 static inline struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
-					      dma_filter_fn fn, void *fn_param)
+						     dma_filter_fn fn,
+						     void *fn_param,
+						     struct device_node *np)
 {
 	return NULL;
 }
@@ -1411,7 +1414,8 @@ void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
 struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
-#define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
+#define dma_request_channel(mask, x, y) \
+	__dma_request_channel(&(mask), x, y, NULL)
 #define dma_request_slave_channel_compat(mask, x, y, dev, name) \
 	__dma_request_slave_channel_compat(&(mask), x, y, dev, name)
 
@@ -1429,6 +1433,6 @@ static inline struct dma_chan
 	if (!fn || !fn_param)
 		return NULL;
 
-	return __dma_request_channel(mask, fn, fn_param);
+	return __dma_request_channel(mask, fn, fn_param, NULL);
 }
 #endif /* DMAENGINE_H */

commit 9ab65aff02e842b09fbdcd7a7df02b63ed63442a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 15:51:37 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 7
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details the full
      gnu general public license is included in this distribution in the
      file called copying
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 9 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Jilayne Lovejoy <opensource@jilayne.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190519154041.244154651@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d49ec5c31944..c952f987ee57 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1,18 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Copyright(c) 2004 - 2006 Intel Corporation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License as published by the Free
- * Software Foundation; either version 2 of the License, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * The full GNU General Public License is included in this distribution in the
- * file called COPYING.
  */
 #ifndef LINUX_DMAENGINE_H
 #define LINUX_DMAENGINE_H

commit f39b948dbeaf9da0dfd17e68704f38fe4237788f
Author: Huang Shijie <sjhuang@iluvatar.ai>
Date:   Thu Jul 26 14:45:53 2018 +0800

    dmaengine: add a new helper dmaenginem_async_device_register
    
    This patch adds the dmaenginem_async_device_register for DMA code.
    Use the Devres to call the release for the DMA engine driver.
    
    Signed-off-by: Huang Shijie <sjhuang@iluvatar.ai>
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c8c3a7a93802..d49ec5c31944 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1406,6 +1406,7 @@ static inline int dmaengine_desc_free(struct dma_async_tx_descriptor *desc)
 /* --- DMA device --- */
 
 int dma_async_device_register(struct dma_device *device);
+int dmaenginem_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);

commit d8095f94e19581057bcad35b8a725aa739e77595
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 2 15:08:10 2018 +0200

    dmaengine: add support for reporting pause and resume separately
    
    'cmd_pause' DMA channel capability means that respective DMA engine
    supports both pausing and resuming given DMA channel. However, in some
    cases it is important to know if DMA channel can be paused without the
    need to resume it. This is a typical requirement for proper residue
    reading on transfer timeout in UART drivers. There are also some DMA
    engines with limited hardware, which doesn't really support resuming.
    
    Reporting pause and resume capabilities separately allows UART drivers to
    properly check for the really required capabilities and operate in DMA
    mode also in systems with limited DMA hardware. On the other hand drivers,
    which rely on full channel suspend/resume support, should now check for
    both 'pause' and 'resume' features.
    
    Existing clients of dma_get_slave_caps() have been checked and the only
    driver which rely on proper channel resuming is soc-generic-dmaengine-pcm
    driver, which has been updated to check the newly added capability.
    Existing 'cmd_pause' now only indicates that DMA engine support pausing
    given DMA channel.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 861be5cab1df..c8c3a7a93802 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -415,7 +415,9 @@ enum dma_residue_granularity {
  *	each type, the dma controller should set BIT(<TYPE>) and same
  *	should be checked by controller as well
  * @max_burst: max burst capability per-transfer
- * @cmd_pause: true, if pause and thereby resume is supported
+ * @cmd_pause: true, if pause is supported (i.e. for reading residue or
+ *	       for resume later)
+ * @cmd_resume: true, if resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
  * @residue_granularity: granularity of the reported transfer residue
  * @descriptor_reuse: if a descriptor can be reused by client and
@@ -427,6 +429,7 @@ struct dma_slave_caps {
 	u32 directions;
 	u32 max_burst;
 	bool cmd_pause;
+	bool cmd_resume;
 	bool cmd_terminate;
 	enum dma_residue_granularity residue_granularity;
 	bool descriptor_reuse;

commit 0c0eb4caf03bb6d3d92c70560e0530c8fdf62284
Author: Zi Yan <zi.yan@cs.rutgers.edu>
Date:   Mon Jan 8 10:50:50 2018 -0500

    dmaengine: avoid map_cnt overflow with CONFIG_DMA_ENGINE_RAID
    
    When CONFIG_DMA_ENGINE_RAID is enabled, unmap pool size can reach to
    256. But in struct dmaengine_unmap_data, map_cnt is only u8, wrapping
    to 0, if the unmap pool is maximally used. This triggers BUG() when
    struct dmaengine_unmap_data is freed. Use u16 to fix the problem.
    
    Signed-off-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index f838764993eb..861be5cab1df 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -470,7 +470,11 @@ typedef void (*dma_async_tx_callback_result)(void *dma_async_param,
 				const struct dmaengine_result *result);
 
 struct dmaengine_unmap_data {
+#if IS_ENABLED(CONFIG_DMA_ENGINE_RAID)
+	u16 map_cnt;
+#else
 	u8 map_cnt;
+#endif
 	u8 to_cnt;
 	u8 from_cnt;
 	u8 bidi_cnt;

commit c2cbd4276eea1d3b204754653dd74d6e10ca207f
Author: Stefan Brüns <stefan.bruens@rwth-aachen.de>
Date:   Tue Sep 12 01:44:45 2017 +0200

    dmaengine: Mark struct dma_slave_caps kernel-doc correctly, clarify
    
    struct dma_slave_caps documentation omitted the correct kernel-doc
    opening comment mark.
    
    Document byte granularity and interpretation of the src/dst_addr_widths
    bit flag fields used by struct dma_slave_caps and struct dma_device.
    
    Add punctuation to their "directions" member documentations, and cleanup
    wording of the description.
    
    Signed-off-by: Stefan Brüns <stefan.bruens@rwth-aachen.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2910e7dadc7f..f838764993eb 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -404,14 +404,16 @@ enum dma_residue_granularity {
 	DMA_RESIDUE_GRANULARITY_BURST = 2,
 };
 
-/* struct dma_slave_caps - expose capabilities of a slave channel only
- *
- * @src_addr_widths: bit mask of src addr widths the channel supports
- * @dst_addr_widths: bit mask of dstn addr widths the channel supports
- * @directions: bit mask of slave direction the channel supported
- * 	since the enum dma_transfer_direction is not defined as bits for each
- * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
- * 	should be checked by controller as well
+/**
+ * struct dma_slave_caps - expose capabilities of a slave channel only
+ * @src_addr_widths: bit mask of src addr widths the channel supports.
+ *	Width is specified in bytes, e.g. for a channel supporting
+ *	a width of 4 the mask should have BIT(4) set.
+ * @dst_addr_widths: bit mask of dst addr widths the channel supports
+ * @directions: bit mask of slave directions the channel supports.
+ *	Since the enum dma_transfer_direction is not defined as bit flag for
+ *	each type, the dma controller should set BIT(<TYPE>) and same
+ *	should be checked by controller as well
  * @max_burst: max burst capability per-transfer
  * @cmd_pause: true, if pause and thereby resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
@@ -678,11 +680,13 @@ struct dma_filter {
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @src_addr_widths: bit mask of src addr widths the device supports
+ *	Width is specified in bytes, e.g. for a device supporting
+ *	a width of 4 the mask should have BIT(4) set.
  * @dst_addr_widths: bit mask of dst addr widths the device supports
- * @directions: bit mask of slave direction the device supports since
- * 	the enum dma_transfer_direction is not defined as bits for
- * 	each type of direction, the dma controller should fill (1 <<
- * 	<TYPE>) and same should be checked by controller as well
+ * @directions: bit mask of slave directions the device supports.
+ *	Since the enum dma_transfer_direction is not defined as bit flag for
+ *	each type, the dma controller should set BIT(<TYPE>) and same
+ *	should be checked by controller as well
  * @max_burst: max burst capability per-transfer
  * @residue_granularity: granularity of the transfer residue reported
  *	by tx_status

commit 3f7632e1ba2c6f5ca2499d1ee8acf95599d4b7b6
Author: Stefan Brüns <stefan.bruens@rwth-aachen.de>
Date:   Tue Sep 12 01:44:44 2017 +0200

    dmaengine: List all allowed values for src/dst_addr_width in kernel doc
    
    Commit 93c6ee94c140 ("dma: Support for 3 bytes word size") and
    commit 534a729866f9 ("dmaengine: Add 16 bytes, 32 bytes and 64 bytes
    bus widths") added additional values for the allowed word size, but
    omitted these from the struct dma_slave_config documentation.
    
    Signed-off-by: Stefan Brüns <stefan.bruens@rwth-aachen.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8319101170fc..2910e7dadc7f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -329,7 +329,7 @@ enum dma_slave_buswidth {
  * @src_addr_width: this is the width in bytes of the source (RX)
  * register where DMA data shall be read. If the source
  * is memory this may be ignored depending on architecture.
- * Legal values: 1, 2, 4, 8.
+ * Legal values: 1, 2, 3, 4, 8, 16, 32, 64.
  * @dst_addr_width: same as src_addr_width but for destination
  * target (TX) mutatis mutandis.
  * @src_maxburst: the maximum number of words (note: words, as in

commit 41bd0314fa3a458bee7ad768d079e681316332e7
Merge: 346ea25e811b 3eeb5156362b
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Sep 6 21:55:10 2017 +0530

    Merge branch 'topic/dmatest' into for-linus

commit 3e00ab4ac51c2ed47c28fd5000c47399f1a11cf5
Author: Abhishek Sahu <absahu@codeaurora.org>
Date:   Tue Aug 1 19:41:42 2017 +0530

    dmaengine: add DMA_PREP_CMD for non-Data descriptors.
    
    Some of the DMA controllers are capable of issuing the commands
    to peripheral by the DMA. These commands can be list of register
    reads/writes and its different from normal data reads/writes.
    This patch adds new flag DMA_PREP_CMD in DMA API which tells
    the driver that the data passed to DMA API is command data
    and DMA controller driver will form descriptor in the required
    format.
    
    This flag can be used by any DMA controller driver which requires
    the descriptor in different format for non-Data descriptors.
    
    Signed-off-by: Abhishek Sahu <absahu@codeaurora.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 533680860865..dd4de1d40166 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -186,6 +186,9 @@ struct dma_interleaved_template {
  *  on the result of this operation
  * @DMA_CTRL_REUSE: client can reuse the descriptor and submit again till
  *  cleared or freed
+ * @DMA_PREP_CMD: tell the driver that the data passed to DMA API is command
+ *  data and the descriptor should be in different format from normal
+ *  data descriptors.
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
@@ -195,6 +198,7 @@ enum dma_ctrl_flags {
 	DMA_PREP_CONTINUE = (1 << 4),
 	DMA_PREP_FENCE = (1 << 5),
 	DMA_CTRL_REUSE = (1 << 6),
+	DMA_PREP_CMD = (1 << 7),
 };
 
 /**

commit c678fa66341c7b82a57cfed0ba3656162e970f99
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Mon Aug 21 10:23:13 2017 -0700

    dmaengine: remove DMA_SG as it is dead code in kernel
    
    There are no in kernel consumers for DMA_SG op. Removing operation,
    dead code, and test code in dmatest.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Cc: Gary Hook <gary.hook@amd.com>
    Cc: Ludovic Desroches <ludovic.desroches@microchip.com>
    Cc: Kedareswara rao Appana <appana.durga.rao@xilinx.com>
    Cc: Li Yang <leoyang.li@nxp.com>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 533680860865..64fbd380c430 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -68,7 +68,6 @@ enum dma_transaction_type {
 	DMA_MEMSET,
 	DMA_MEMSET_SG,
 	DMA_INTERRUPT,
-	DMA_SG,
 	DMA_PRIVATE,
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
@@ -771,11 +770,6 @@ struct dma_device {
 		unsigned int nents, int value, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
-	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(
-		struct dma_chan *chan,
-		struct scatterlist *dst_sg, unsigned int dst_nents,
-		struct scatterlist *src_sg, unsigned int src_nents,
-		unsigned long flags);
 
 	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(
 		struct dma_chan *chan, struct scatterlist *sgl,
@@ -905,19 +899,6 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memcpy(
 						    len, flags);
 }
 
-static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
-		struct dma_chan *chan,
-		struct scatterlist *dst_sg, unsigned int dst_nents,
-		struct scatterlist *src_sg, unsigned int src_nents,
-		unsigned long flags)
-{
-	if (!chan || !chan->device || !chan->device->device_prep_dma_sg)
-		return NULL;
-
-	return chan->device->device_prep_dma_sg(chan, dst_sg, dst_nents,
-			src_sg, src_nents, flags);
-}
-
 /**
  * dmaengine_terminate_all() - Terminate all active DMA transfers
  * @chan: The channel for which to terminate the transfers

commit 77d65d6f3d60cebb2dc24cf05408255a21bb6409
Author: Boris Brezillon <boris.brezillon@free-electrons.com>
Date:   Fri Jan 27 17:42:01 2017 +0100

    dmaengine: Provide a wrapper for memcpy operations
    
    Almost all ->device_prep_dma_xx() methods have a wrapper defined in
    dmaengine.h. Add one for  ->device_prep_dma_memcpy().
    
    Signed-off-by: Boris Brezillon <boris.brezillon@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index feee6ec6a13b..533680860865 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -894,6 +894,17 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memset(
 						    len, flags);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memcpy(
+		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		size_t len, unsigned long flags)
+{
+	if (!chan || !chan->device || !chan->device->device_prep_dma_memcpy)
+		return NULL;
+
+	return chan->device->device_prep_dma_memcpy(chan, dest, src,
+						    len, flags);
+}
+
 static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 		struct dma_chan *chan,
 		struct scatterlist *dst_sg, unsigned int dst_nents,

commit 54cd255808761ecfd0e000eb78eb74dde8cd0c96
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Tue Nov 29 16:23:41 2016 +0200

    dmaengine: dma_slave_config: add support for slave port window
    
    Some slave devices uses address window instead of single register for read
    and/or write of data. With the src/dst_port_window_size the address window
    can be specified and the DMAengine driver should use this information to
    correctly set up the transfer to loop within the provided window.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cc535a478bae..feee6ec6a13b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -336,6 +336,12 @@ enum dma_slave_buswidth {
  * may or may not be applicable on memory sources.
  * @dst_maxburst: same as src_maxburst but for destination target
  * mutatis mutandis.
+ * @src_port_window_size: The length of the register area in words the data need
+ * to be accessed on the device side. It is only used for devices which is using
+ * an area instead of a single register to receive the data. Typically the DMA
+ * loops in this area in order to transfer the data.
+ * @dst_port_window_size: same as src_port_window_size but for the destination
+ * port.
  * @device_fc: Flow Controller Settings. Only valid for slave channels. Fill
  * with 'true' if peripheral should be flow controller. Direction will be
  * selected at Runtime.
@@ -363,6 +369,8 @@ struct dma_slave_config {
 	enum dma_slave_buswidth dst_addr_width;
 	u32 src_maxburst;
 	u32 dst_maxburst;
+	u32 src_port_window_size;
+	u32 dst_port_window_size;
 	bool device_fc;
 	unsigned int slave_id;
 };

commit f067025bc676ba8d18fba5f959598339e39b86db
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Jul 20 13:13:50 2016 -0700

    dmaengine: add support to provide error result from a DMA transation
    
    Adding a new callback that will provide the error result for a transaction.
    The result is allocated on the stack and the callback should create a copy
    if it wishes to retain the information after exiting. The result parameter
    is now defined and takes over the dummy void pointer we placed in the
    helper functions previously. dmaengine drivers should start converting
    to the new "callback_result" callback in order to receive transaction
    results.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Reviewed-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 30de0197263a..cc535a478bae 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -441,6 +441,21 @@ typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
 
 typedef void (*dma_async_tx_callback)(void *dma_async_param);
 
+enum dmaengine_tx_result {
+	DMA_TRANS_NOERROR = 0,		/* SUCCESS */
+	DMA_TRANS_READ_FAILED,		/* Source DMA read failed */
+	DMA_TRANS_WRITE_FAILED,		/* Destination DMA write failed */
+	DMA_TRANS_ABORTED,		/* Op never submitted / aborted */
+};
+
+struct dmaengine_result {
+	enum dmaengine_tx_result result;
+	u32 residue;
+};
+
+typedef void (*dma_async_tx_callback_result)(void *dma_async_param,
+				const struct dmaengine_result *result);
+
 struct dmaengine_unmap_data {
 	u8 map_cnt;
 	u8 to_cnt;
@@ -478,6 +493,7 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	int (*desc_free)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
+	dma_async_tx_callback_result callback_result;
 	void *callback_param;
 	struct dmaengine_unmap_data *unmap;
 #ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH

commit 757d12e5849be549076901b0d33c60d5f360269c
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Tue Apr 12 21:07:06 2016 +0530

    dmaengine: ensure dmaengine helpers check valid callback
    
    dmaengine has various device callbacks and exposes helper
    functions to invoke these. These helpers should check if channel,
    device and callback is valid or not before invoking them.
    
    Reported-by: Jon Hunter <jonathanh@nvidia.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 017433712833..30de0197263a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -804,6 +804,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
 	sg_dma_address(&sg) = buf;
 	sg_dma_len(&sg) = len;
 
+	if (!chan || !chan->device || !chan->device->device_prep_slave_sg)
+		return NULL;
+
 	return chan->device->device_prep_slave_sg(chan, &sg, 1,
 						  dir, flags, NULL);
 }
@@ -812,6 +815,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
 	struct dma_chan *chan, struct scatterlist *sgl,	unsigned int sg_len,
 	enum dma_transfer_direction dir, unsigned long flags)
 {
+	if (!chan || !chan->device || !chan->device->device_prep_slave_sg)
+		return NULL;
+
 	return chan->device->device_prep_slave_sg(chan, sgl, sg_len,
 						  dir, flags, NULL);
 }
@@ -823,6 +829,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_rio_sg(
 	enum dma_transfer_direction dir, unsigned long flags,
 	struct rio_dma_ext *rio_ext)
 {
+	if (!chan || !chan->device || !chan->device->device_prep_slave_sg)
+		return NULL;
+
 	return chan->device->device_prep_slave_sg(chan, sgl, sg_len,
 						  dir, flags, rio_ext);
 }
@@ -833,6 +842,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		size_t period_len, enum dma_transfer_direction dir,
 		unsigned long flags)
 {
+	if (!chan || !chan->device || !chan->device->device_prep_dma_cyclic)
+		return NULL;
+
 	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
 						period_len, dir, flags);
 }
@@ -841,6 +853,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags)
 {
+	if (!chan || !chan->device || !chan->device->device_prep_interleaved_dma)
+		return NULL;
+
 	return chan->device->device_prep_interleaved_dma(chan, xt, flags);
 }
 
@@ -848,7 +863,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memset(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags)
 {
-	if (!chan || !chan->device)
+	if (!chan || !chan->device || !chan->device->device_prep_dma_memset)
 		return NULL;
 
 	return chan->device->device_prep_dma_memset(chan, dest, value,
@@ -861,6 +876,9 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 		struct scatterlist *src_sg, unsigned int src_nents,
 		unsigned long flags)
 {
+	if (!chan || !chan->device || !chan->device->device_prep_dma_sg)
+		return NULL;
+
 	return chan->device->device_prep_dma_sg(chan, dst_sg, dst_nents,
 			src_sg, src_nents, flags);
 }

commit 8bce4c87657af3dc4625e873ec1201205e44375b
Merge: 805dd3508b23 0a18f9b268dd
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Mar 14 11:18:12 2016 +0530

    Merge branch 'topic/pl330' into for-linus

commit 9575632052bacc2fda38d845eb17b0fb808e13eb
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Feb 15 22:27:02 2016 +0530

    dmaengine: make slave address physical
    
    The slave dmaengine semantics required the client to map dma
    addresses and pass DMA address to dmaengine drivers. This
    was a convenient notion coming from generic dma offload cases
    where dmaengines are interchangeable and client is not aware of
    which engine to map to.
    
    But in case of slave, we know the dmaengine and always use a
    specific one. Further the IOMMU cases can lead to failure of this
    notion, so make this as physical address and now dmaengine driver
    will do the required mapping.
    
    Original-patch-by: Linus Walleij <linus.walleij@linaro.org>
    Original-patch-Acked-by: Lee Jones <lee.jones@linaro.org>
    Reviewed-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Acked-by: Wolfram Sang <wsa+renesas@sang-engineering.com>
    Tested-by: Wolfram Sang <wsa+renesas@sang-engineering.com>
    Tested-by: Niklas Söderlund <niklas.soderlund+renesas@ragnatech.se>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 16a1cad30c33..d85ecd20af50 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -357,8 +357,8 @@ enum dma_slave_buswidth {
  */
 struct dma_slave_config {
 	enum dma_transfer_direction direction;
-	dma_addr_t src_addr;
-	dma_addr_t dst_addr;
+	phys_addr_t src_addr;
+	phys_addr_t dst_addr;
 	enum dma_slave_buswidth src_addr_width;
 	enum dma_slave_buswidth dst_addr_width;
 	u32 src_maxburst;

commit 6d5bbed30f89acd2ae0d23b3fff5b13b307525d9
Author: Shawn Lin <shawn.lin@rock-chips.com>
Date:   Fri Jan 22 19:06:50 2016 +0800

    dmaengine: core: expose max burst capability to clients
    
    This patch add max_burst to dma_get_slave_caps for clients
    to get the burst capability of slave dma controller.
    
    Signed-off-by: Shawn Lin <shawn.lin@rock-chips.com>
    Signed-off-by: Caesar Wang <wxt@rock-chips.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 16a1cad30c33..0a9a0ba1998b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -401,6 +401,7 @@ enum dma_residue_granularity {
  * 	since the enum dma_transfer_direction is not defined as bits for each
  * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
  * 	should be checked by controller as well
+ * @max_burst: max burst capability per-transfer
  * @cmd_pause: true, if pause and thereby resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
  * @residue_granularity: granularity of the reported transfer residue
@@ -411,6 +412,7 @@ struct dma_slave_caps {
 	u32 src_addr_widths;
 	u32 dst_addr_widths;
 	u32 directions;
+	u32 max_burst;
 	bool cmd_pause;
 	bool cmd_terminate;
 	enum dma_residue_granularity residue_granularity;
@@ -654,6 +656,7 @@ struct dma_filter {
  * 	the enum dma_transfer_direction is not defined as bits for
  * 	each type of direction, the dma controller should fill (1 <<
  * 	<TYPE>) and same should be checked by controller as well
+ * @max_burst: max burst capability per-transfer
  * @residue_granularity: granularity of the transfer residue reported
  *	by tx_status
  * @device_alloc_chan_resources: allocate resources and return the
@@ -712,6 +715,7 @@ struct dma_device {
 	u32 src_addr_widths;
 	u32 dst_addr_widths;
 	u32 directions;
+	u32 max_burst;
 	bool descriptor_reuse;
 	enum dma_residue_granularity residue_granularity;
 

commit d3f1e93ce8e00be19711c35f0c67c54a58aea559
Merge: 7c7b680fa6b0 b1d6ab1aa8cd
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Jan 6 15:17:47 2016 +0530

    Merge branch 'topic/async' into for-linus

commit 7c7b680fa6b0866af2c4876da261bbfe710315d6
Merge: 5eec94388db4 020c62ae3894
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Jan 6 15:17:32 2016 +0530

    Merge branch 'topic/univ_api' into for-linus

commit a8135d0d79e9d0ad3a4ff494fceeaae838becf38
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Mon Dec 14 22:47:40 2015 +0200

    dmaengine: core: Introduce new, universal API to request a channel
    
    The two API function can cover most, if not all current APIs used to
    request a channel. With minimal effort dmaengine drivers, platforms and
    dmaengine user drivers can be converted to use the two function.
    
    struct dma_chan *dma_request_chan_by_mask(const dma_cap_mask_t *mask);
    
    To request any channel matching with the requested capabilities, can be
    used to request channel for memcpy, memset, xor, etc where no hardware
    synchronization is needed.
    
    struct dma_chan *dma_request_chan(struct device *dev, const char *name);
    To request a slave channel. The dma_request_chan() will try to find the
    channel via DT, ACPI or in case if the kernel booted in non DT/ACPI mode
    it will use a filter lookup table and retrieves the needed information from
    the dma_slave_map provided by the DMA drivers.
    This legacy mode needs changes in platform code, in dmaengine drivers and
    finally the dmaengine user drivers can be converted:
    
    For each dmaengine driver an array of DMA device, slave and the parameter
    for the filter function needs to be added:
    
    static const struct dma_slave_map da830_edma_map[] = {
            { "davinci-mcasp.0", "rx", EDMA_FILTER_PARAM(0, 0) },
            { "davinci-mcasp.0", "tx", EDMA_FILTER_PARAM(0, 1) },
            { "davinci-mcasp.1", "rx", EDMA_FILTER_PARAM(0, 2) },
            { "davinci-mcasp.1", "tx", EDMA_FILTER_PARAM(0, 3) },
            { "davinci-mcasp.2", "rx", EDMA_FILTER_PARAM(0, 4) },
            { "davinci-mcasp.2", "tx", EDMA_FILTER_PARAM(0, 5) },
            { "spi_davinci.0", "rx", EDMA_FILTER_PARAM(0, 14) },
            { "spi_davinci.0", "tx", EDMA_FILTER_PARAM(0, 15) },
            { "da830-mmc.0", "rx", EDMA_FILTER_PARAM(0, 16) },
            { "da830-mmc.0", "tx", EDMA_FILTER_PARAM(0, 17) },
            { "spi_davinci.1", "rx", EDMA_FILTER_PARAM(0, 18) },
            { "spi_davinci.1", "tx", EDMA_FILTER_PARAM(0, 19) },
    };
    
    This information is going to be needed by the dmaengine driver, so
    modification to the platform_data is needed, and the driver map should be
    added to the pdata of the DMA driver:
    
    da8xx_edma0_pdata.slave_map = da830_edma_map;
    da8xx_edma0_pdata.slavecnt = ARRAY_SIZE(da830_edma_map);
    
    The DMA driver then needs to configure the needed device -> filter_fn
    mapping before it registers with dma_async_device_register() :
    
    ecc->dma_slave.filter_map.map = info->slave_map;
    ecc->dma_slave.filter_map.mapcnt = info->slavecnt;
    ecc->dma_slave.filter_map.fn = edma_filter_fn;
    
    When neither DT or ACPI lookup is available the dma_request_chan() will
    try to match the requester's device name with the filter_map's list of
    device names, when a match found it will use the information from the
    dma_slave_map to get the channel with the dma_get_channel() internal
    function.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c47c68e535e8..d50a6b51a73d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -606,12 +606,39 @@ enum dmaengine_alignment {
 	DMAENGINE_ALIGN_64_BYTES = 6,
 };
 
+/**
+ * struct dma_slave_map - associates slave device and it's slave channel with
+ * parameter to be used by a filter function
+ * @devname: name of the device
+ * @slave: slave channel name
+ * @param: opaque parameter to pass to struct dma_filter.fn
+ */
+struct dma_slave_map {
+	const char *devname;
+	const char *slave;
+	void *param;
+};
+
+/**
+ * struct dma_filter - information for slave device/channel to filter_fn/param
+ * mapping
+ * @fn: filter function callback
+ * @mapcnt: number of slave device/channel in the map
+ * @map: array of channel to filter mapping data
+ */
+struct dma_filter {
+	dma_filter_fn fn;
+	int mapcnt;
+	const struct dma_slave_map *map;
+};
+
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported
  * @privatecnt: how many DMA channels are requested by dma_request_channel
  * @channels: the list of struct dma_chan
  * @global_node: list_head for global dma_device_list
+ * @filter: information for device/slave to filter function/param mapping
  * @cap_mask: one or more dma_capability flags
  * @max_xor: maximum number of xor sources, 0 if no capability
  * @max_pq: maximum number of PQ sources and PQ-continue capability
@@ -666,6 +693,7 @@ struct dma_device {
 	unsigned int privatecnt;
 	struct list_head channels;
 	struct list_head global_node;
+	struct dma_filter filter;
 	dma_cap_mask_t  cap_mask;
 	unsigned short max_xor;
 	unsigned short max_pq;
@@ -1140,9 +1168,11 @@ enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 					dma_filter_fn fn, void *fn_param);
-struct dma_chan *dma_request_slave_channel_reason(struct device *dev,
-						  const char *name);
 struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
+
+struct dma_chan *dma_request_chan(struct device *dev, const char *name);
+struct dma_chan *dma_request_chan_by_mask(const dma_cap_mask_t *mask);
+
 void dma_release_channel(struct dma_chan *chan);
 int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps);
 #else
@@ -1166,16 +1196,21 @@ static inline struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 {
 	return NULL;
 }
-static inline struct dma_chan *dma_request_slave_channel_reason(
-					struct device *dev, const char *name)
-{
-	return ERR_PTR(-ENODEV);
-}
 static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
 							 const char *name)
 {
 	return NULL;
 }
+static inline struct dma_chan *dma_request_chan(struct device *dev,
+						const char *name)
+{
+	return ERR_PTR(-ENODEV);
+}
+static inline struct dma_chan *dma_request_chan_by_mask(
+						const dma_cap_mask_t *mask)
+{
+	return ERR_PTR(-ENODEV);
+}
 static inline void dma_release_channel(struct dma_chan *chan)
 {
 }
@@ -1186,6 +1221,8 @@ static inline int dma_get_slave_caps(struct dma_chan *chan,
 }
 #endif
 
+#define dma_request_slave_channel_reason(dev, name) dma_request_chan(dev, name)
+
 static inline int dmaengine_desc_set_reuse(struct dma_async_tx_descriptor *tx)
 {
 	struct dma_slave_caps caps;

commit b1d6ab1aa8cdc23b89bcd578ea8d5e3c501a13d9
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Mon Nov 23 11:06:43 2015 +0100

    dmaengine: Add might_sleep() to dmaengine_synchronize()
    
    Implementations of dmaengine_synchronize() are allowed to sleep, hence the
    function must not be called to from atomic context. Add might_sleep() to
    dmaengine_synchronize() to make it easier to detect non-compliant callers.
    
    Suggested-by: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 4662d9aa6d5a..2f69e1d93f92 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -895,6 +895,8 @@ static inline int dmaengine_terminate_async(struct dma_chan *chan)
  */
 static inline void dmaengine_synchronize(struct dma_chan *chan)
 {
+	might_sleep();
+
 	if (chan->device->device_synchronize)
 		chan->device->device_synchronize(chan);
 }

commit 9eeacd3a2f17438d9d286ff2f78c4709a4148be7
Author: Robert Jarzmik <robert.jarzmik@free.fr>
Date:   Tue Oct 13 21:54:29 2015 +0200

    dmaengine: enable DMA_CTRL_REUSE
    
    In the current state, the capability of transfer reuse can neither be
    set by a slave dmaengine driver, nor used by a client driver, because
    the capability is not available to dma_get_slave_caps().
    
    Fix this by adding a way to declare the capability.
    
    Fixes: 272420214d26 ("dmaengine: Add DMA_CTRL_REUSE")
    Signed-off-by: Robert Jarzmik <robert.jarzmik@free.fr>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c47c68e535e8..6f94b5cbd97c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -659,6 +659,7 @@ enum dmaengine_alignment {
  *	struct with auxiliary transfer status information, otherwise the call
  *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
+ * @descriptor_reuse: a submitted transfer can be resubmitted after completion
  */
 struct dma_device {
 
@@ -681,6 +682,7 @@ struct dma_device {
 	u32 src_addr_widths;
 	u32 dst_addr_widths;
 	u32 directions;
+	bool descriptor_reuse;
 	enum dma_residue_granularity residue_granularity;
 
 	int (*device_alloc_chan_resources)(struct dma_chan *chan);

commit b36f09c3c441a6e59eab9315032e7d546571de3f
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Tue Oct 20 11:46:28 2015 +0200

    dmaengine: Add transfer termination synchronization support
    
    The DMAengine API has a long standing race condition that is inherent to
    the API itself. Calling dmaengine_terminate_all() is supposed to stop and
    abort any pending or active transfers that have previously been submitted.
    Unfortunately it is possible that this operation races against a currently
    running (or with some drivers also scheduled) completion callback.
    
    Since the API allows dmaengine_terminate_all() to be called from atomic
    context as well as from within a completion callback it is not possible to
    synchronize to the execution of the completion callback from within
    dmaengine_terminate_all() itself.
    
    This means that a user of the DMAengine API does not know when it is safe
    to free resources used in the completion callback, which can result in a
    use-after-free race condition.
    
    This patch addresses the issue by introducing an explicit synchronization
    primitive to the DMAengine API called dmaengine_synchronize().
    
    The existing dmaengine_terminate_all() is deprecated in favor of
    dmaengine_terminate_sync() and dmaengine_terminate_async(). The former
    aborts all pending and active transfers and synchronizes to the current
    context, meaning it will wait until all running completion callbacks have
    finished. This means it is only possible to call this function from
    non-atomic context. The later function does not synchronize, but can still
    be used in atomic context or from within a complete callback. It has to be
    followed up by dmaengine_synchronize() before a client can free the
    resources used in a completion callback.
    
    In addition to this the semantics of the device_terminate_all() callback
    are slightly relaxed by this patch. It is now OK for a driver to only
    schedule the termination of the active transfer, but does not necessarily
    have to wait until the DMA controller has completely stopped. The driver
    must ensure though that the controller has stopped and no longer accesses
    any memory when the device_synchronize() callback returns.
    
    This was in part done since most drivers do not pay attention to this
    anyway at the moment and to emphasize that this needs to be done when the
    device_synchronize() callback is implemented. But it also helps with
    implementing support for devices where stopping the controller can require
    operations that may sleep.
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c47c68e535e8..4662d9aa6d5a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -654,6 +654,8 @@ enum dmaengine_alignment {
  *	paused. Returns 0 or an error code
  * @device_terminate_all: Aborts all transfers on a channel. Returns 0
  *	or an error code
+ * @device_synchronize: Synchronizes the termination of a transfers to the
+ *  current context.
  * @device_tx_status: poll for transaction completion, the optional
  *	txstate parameter can be supplied with a pointer to get a
  *	struct with auxiliary transfer status information, otherwise the call
@@ -737,6 +739,7 @@ struct dma_device {
 	int (*device_pause)(struct dma_chan *chan);
 	int (*device_resume)(struct dma_chan *chan);
 	int (*device_terminate_all)(struct dma_chan *chan);
+	void (*device_synchronize)(struct dma_chan *chan);
 
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,
@@ -828,6 +831,13 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 			src_sg, src_nents, flags);
 }
 
+/**
+ * dmaengine_terminate_all() - Terminate all active DMA transfers
+ * @chan: The channel for which to terminate the transfers
+ *
+ * This function is DEPRECATED use either dmaengine_terminate_sync() or
+ * dmaengine_terminate_async() instead.
+ */
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	if (chan->device->device_terminate_all)
@@ -836,6 +846,86 @@ static inline int dmaengine_terminate_all(struct dma_chan *chan)
 	return -ENOSYS;
 }
 
+/**
+ * dmaengine_terminate_async() - Terminate all active DMA transfers
+ * @chan: The channel for which to terminate the transfers
+ *
+ * Calling this function will terminate all active and pending descriptors
+ * that have previously been submitted to the channel. It is not guaranteed
+ * though that the transfer for the active descriptor has stopped when the
+ * function returns. Furthermore it is possible the complete callback of a
+ * submitted transfer is still running when this function returns.
+ *
+ * dmaengine_synchronize() needs to be called before it is safe to free
+ * any memory that is accessed by previously submitted descriptors or before
+ * freeing any resources accessed from within the completion callback of any
+ * perviously submitted descriptors.
+ *
+ * This function can be called from atomic context as well as from within a
+ * complete callback of a descriptor submitted on the same channel.
+ *
+ * If none of the two conditions above apply consider using
+ * dmaengine_terminate_sync() instead.
+ */
+static inline int dmaengine_terminate_async(struct dma_chan *chan)
+{
+	if (chan->device->device_terminate_all)
+		return chan->device->device_terminate_all(chan);
+
+	return -EINVAL;
+}
+
+/**
+ * dmaengine_synchronize() - Synchronize DMA channel termination
+ * @chan: The channel to synchronize
+ *
+ * Synchronizes to the DMA channel termination to the current context. When this
+ * function returns it is guaranteed that all transfers for previously issued
+ * descriptors have stopped and and it is safe to free the memory assoicated
+ * with them. Furthermore it is guaranteed that all complete callback functions
+ * for a previously submitted descriptor have finished running and it is safe to
+ * free resources accessed from within the complete callbacks.
+ *
+ * The behavior of this function is undefined if dma_async_issue_pending() has
+ * been called between dmaengine_terminate_async() and this function.
+ *
+ * This function must only be called from non-atomic context and must not be
+ * called from within a complete callback of a descriptor submitted on the same
+ * channel.
+ */
+static inline void dmaengine_synchronize(struct dma_chan *chan)
+{
+	if (chan->device->device_synchronize)
+		chan->device->device_synchronize(chan);
+}
+
+/**
+ * dmaengine_terminate_sync() - Terminate all active DMA transfers
+ * @chan: The channel for which to terminate the transfers
+ *
+ * Calling this function will terminate all active and pending transfers
+ * that have previously been submitted to the channel. It is similar to
+ * dmaengine_terminate_async() but guarantees that the DMA transfer has actually
+ * stopped and that all complete callbacks have finished running when the
+ * function returns.
+ *
+ * This function must only be called from non-atomic context and must not be
+ * called from within a complete callback of a descriptor submitted on the same
+ * channel.
+ */
+static inline int dmaengine_terminate_sync(struct dma_chan *chan)
+{
+	int ret;
+
+	ret = dmaengine_terminate_async(chan);
+	if (ret)
+		return ret;
+
+	dmaengine_synchronize(chan);
+
+	return 0;
+}
+
 static inline int dmaengine_pause(struct dma_chan *chan)
 {
 	if (chan->device->device_pause)

commit ff39988abd70bcd1b14a4c81f2d102e67b8db580
Author: Siva Yerramreddy <yshivakrishna@gmail.com>
Date:   Tue Sep 29 18:09:37 2015 -0700

    dma: Add support to program MIC x100 status descriptiors
    
    The MIC X100 DMA engine has a special status descriptor which writes
    an 8 byte value to a destination location.  This is used to signal
    completion of all DMA descriptors prior to the status descriptor.
    This patch add a new DMA engine API which enables updating a
    destination address with an 8 byte immediate data value.
    
    Reviewed-by: Nikhil Rao <nikhil.rao@intel.com>
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Lawrynowicz, Jacek <jacek.lawrynowicz@intel.com>
    Signed-off-by: Sudeep Dutt <sudeep.dutt@intel.com>
    Signed-off-by: Siva Yerramreddy <yshivakrishna@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 7ea9184eaa13..c47c68e535e8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -645,6 +645,7 @@ enum dmaengine_alignment {
  *	The function takes a buffer of size buf_len. The callback function will
  *	be called after period_len bytes have been transferred.
  * @device_prep_interleaved_dma: Transfer expression in a generic way.
+ * @device_prep_dma_imm_data: DMA's 8 byte immediate data to the dst address
  * @device_config: Pushes a new configuration to a channel, return 0 or an error
  *	code
  * @device_pause: Pauses any transfer happening on a channel. Returns
@@ -727,6 +728,9 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_imm_data)(
+		struct dma_chan *chan, dma_addr_t dst, u64 data,
+		unsigned long flags);
 
 	int (*device_config)(struct dma_chan *chan,
 			     struct dma_slave_config *config);

commit 7dfffb9541bca80bbf8df1869564f9220ee150d2
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Mon Aug 17 15:08:55 2015 +0200

    dmaengine: Stricter legacy checking in dma_request_slave_channel_compat()
    
    dma_request_slave_channel_compat() is meant for drivers that support
    both DT and legacy platform device based probing: if DT channel DMA
    setup fails, it will fall back to platform data based DMA channel setup,
    using hardcoded DMA channel IDs and a filter function.
    
    However, if the DTS doesn't provide a "dmas" property for the device,
    the fallback is also used. If the legacy filter function is not
    hardcoded in the DMA slave driver, but comes from platform data, it will
    be NULL. Then dma_request_slave_channel_compat() will succeed
    incorrectly, and return a DMA channel, as a NULL legacy filter function
    actually means "all channels are OK", not "do not match".
    
    Later, when trying to use that DMA channel, it will fail with:
    
        rcar-dmac e6700000.dma-controller: rcar_dmac_prep_slave_sg: bad parameter: len=1, id=-22
    
    To fix this, ensure that both the filter function and the DMA channel ID
    are not NULL before using the legacy fallback.
    
    Note that some DMA slave drivers can handle this failure, and will fall
    back to PIO.
    
    See also commit 056f6c87028544de ("dmaengine: shdma: Make dummy
    shdma_chan_filter() always return false"), which fixed the same issue
    for the case where shdma_chan_filter() is hardcoded in a DMA slave
    driver.
    
    Suggested-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3a732ac95285..7ea9184eaa13 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1237,6 +1237,9 @@ static inline struct dma_chan
 	if (chan)
 		return chan;
 
+	if (!fn || !fn_param)
+		return NULL;
+
 	return __dma_request_channel(mask, fn, fn_param);
 }
 #endif /* DMAENGINE_H */

commit 1dc042885456dff457d0b758b69209dcafa688ec
Author: Jarkko Nikula <jarkko.nikula@linux.intel.com>
Date:   Wed Aug 12 11:30:59 2015 +0300

    dmaengine: Make __dma_request_slave_channel_compat() name argument constant
    
    Inline function __dma_request_slave_channel_compat() doesn't modify "name"
    argument but passes it to dma_request_slave_channel() which already takes
    it as a constant.
    
    Signed-off-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1b866e9a6ed1..3a732ac95285 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1229,7 +1229,7 @@ struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
 static inline struct dma_chan
 *__dma_request_slave_channel_compat(const dma_cap_mask_t *mask,
 				  dma_filter_fn fn, void *fn_param,
-				  struct device *dev, char *name)
+				  struct device *dev, const char *name)
 {
 	struct dma_chan *chan;
 

commit 272420214d261e97f08a4c555defb3924de06ae8
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Aug 5 08:42:05 2015 +0530

    dmaengine: Add DMA_CTRL_REUSE
    
    This adds new descriptor flag for reusing a descriptor by submitting
    multiple times by a client, for example video buffer.
    Add helper APIs for this as well
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>
    Acked-by:Robert Jarzmik <robert.jarzmik@free.fr>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8ad9a4e839f6..1b866e9a6ed1 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -184,6 +184,8 @@ struct dma_interleaved_template {
  *  operation it continues the calculation with new sources
  * @DMA_PREP_FENCE - tell the driver that subsequent operations depend
  *  on the result of this operation
+ * @DMA_CTRL_REUSE: client can reuse the descriptor and submit again till
+ *  cleared or freed
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
@@ -192,6 +194,7 @@ enum dma_ctrl_flags {
 	DMA_PREP_PQ_DISABLE_Q = (1 << 3),
 	DMA_PREP_CONTINUE = (1 << 4),
 	DMA_PREP_FENCE = (1 << 5),
+	DMA_CTRL_REUSE = (1 << 6),
 };
 
 /**
@@ -401,6 +404,8 @@ enum dma_residue_granularity {
  * @cmd_pause: true, if pause and thereby resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
  * @residue_granularity: granularity of the reported transfer residue
+ * @descriptor_reuse: if a descriptor can be reused by client and
+ * resubmitted multiple times
  */
 struct dma_slave_caps {
 	u32 src_addr_widths;
@@ -409,6 +414,7 @@ struct dma_slave_caps {
 	bool cmd_pause;
 	bool cmd_terminate;
 	enum dma_residue_granularity residue_granularity;
+	bool descriptor_reuse;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)
@@ -468,6 +474,7 @@ struct dma_async_tx_descriptor {
 	dma_addr_t phys;
 	struct dma_chan *chan;
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
+	int (*desc_free)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
 	void *callback_param;
 	struct dmaengine_unmap_data *unmap;
@@ -1175,6 +1182,39 @@ static inline int dma_get_slave_caps(struct dma_chan *chan,
 }
 #endif
 
+static inline int dmaengine_desc_set_reuse(struct dma_async_tx_descriptor *tx)
+{
+	struct dma_slave_caps caps;
+
+	dma_get_slave_caps(tx->chan, &caps);
+
+	if (caps.descriptor_reuse) {
+		tx->flags |= DMA_CTRL_REUSE;
+		return 0;
+	} else {
+		return -EPERM;
+	}
+}
+
+static inline void dmaengine_desc_clear_reuse(struct dma_async_tx_descriptor *tx)
+{
+	tx->flags &= ~DMA_CTRL_REUSE;
+}
+
+static inline bool dmaengine_desc_test_reuse(struct dma_async_tx_descriptor *tx)
+{
+	return (tx->flags & DMA_CTRL_REUSE) == DMA_CTRL_REUSE;
+}
+
+static inline int dmaengine_desc_free(struct dma_async_tx_descriptor *desc)
+{
+	/* this is supported for reusable desc, so check that */
+	if (dmaengine_desc_test_reuse(desc))
+		return desc->desc_free(desc);
+	else
+		return -EPERM;
+}
+
 /* --- DMA device --- */
 
 int dma_async_device_register(struct dma_device *device);

commit 50c7cd2bd3786258606c6c7c8356064c08ab2383
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Jul 6 12:19:23 2015 +0200

    dmaengine: Add scatter-gathered memset
    
    The current API allows the driver to accelerate memset by using the DMA
    controller.
    
    However, it does so over a contiguous memory area, which might proves
    inefficient when you have to do it over a non-contiguous yet repititive
    pattern, since you have to create a number of descriptors and then submit
    each other.
    
    Add a memset operation going over a scatter list to handle such cases in a
    single call.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Ludovic Desroches <ludovic.desroches@atmel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 03ed832adbc2..8ad9a4e839f6 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -66,6 +66,7 @@ enum dma_transaction_type {
 	DMA_XOR_VAL,
 	DMA_PQ_VAL,
 	DMA_MEMSET,
+	DMA_MEMSET_SG,
 	DMA_INTERRUPT,
 	DMA_SG,
 	DMA_PRIVATE,
@@ -630,6 +631,7 @@ enum dmaengine_alignment {
  * @device_prep_dma_pq: prepares a pq operation
  * @device_prep_dma_pq_val: prepares a pqzero_sum operation
  * @device_prep_dma_memset: prepares a memset operation
+ * @device_prep_dma_memset_sg: prepares a memset operation over a scatter list
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
  * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
@@ -696,6 +698,9 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_memset_sg)(
+		struct dma_chan *chan, struct scatterlist *sg,
+		unsigned int nents, int value, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(

commit 77a68e56aae141d3e9c740a0ac43362af75d4890
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Jul 20 10:41:32 2015 +0200

    dmaengine: Add an enum for the dmaengine alignment constraints
    
    Most drivers need to set constraints on the buffer alignment for async tx
    operations. However, even though it is documented, some drivers either use
    a defined constant that is not matching what the alignment variable expects
    (like DMA_BUSWIDTH_* constants) or fill the alignment in bytes instead of
    power of two.
    
    Add a new enum for these alignments that matches what the framework
    expects, and convert the drivers to it.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index e2f5eb419976..03ed832adbc2 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -584,6 +584,20 @@ struct dma_tx_state {
 	u32 residue;
 };
 
+/**
+ * enum dmaengine_alignment - defines alignment of the DMA async tx
+ * buffers
+ */
+enum dmaengine_alignment {
+	DMAENGINE_ALIGN_1_BYTE = 0,
+	DMAENGINE_ALIGN_2_BYTES = 1,
+	DMAENGINE_ALIGN_4_BYTES = 2,
+	DMAENGINE_ALIGN_8_BYTES = 3,
+	DMAENGINE_ALIGN_16_BYTES = 4,
+	DMAENGINE_ALIGN_32_BYTES = 5,
+	DMAENGINE_ALIGN_64_BYTES = 6,
+};
+
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported
@@ -645,10 +659,10 @@ struct dma_device {
 	dma_cap_mask_t  cap_mask;
 	unsigned short max_xor;
 	unsigned short max_pq;
-	u8 copy_align;
-	u8 xor_align;
-	u8 pq_align;
-	u8 fill_align;
+	enum dmaengine_alignment copy_align;
+	enum dmaengine_alignment xor_align;
+	enum dmaengine_alignment pq_align;
+	enum dmaengine_alignment fill_align;
 	#define DMA_HAS_PQ_CONTINUE (1 << 15)
 
 	int dev_id;
@@ -833,7 +847,8 @@ static inline dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc
 	return desc->tx_submit(desc);
 }
 
-static inline bool dmaengine_check_align(u8 align, size_t off1, size_t off2, size_t len)
+static inline bool dmaengine_check_align(enum dmaengine_alignment align,
+					 size_t off1, size_t off2, size_t len)
 {
 	size_t mask;
 

commit 4fb9c15b4f2371b8640c411ceff2c100857aee2c
Merge: 0e0fa66e39db 5abecfa5e969
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Thu Jun 25 09:21:49 2015 +0530

    Merge branch 'topic/xdmac' into for-linus

commit 0e0fa66e39db6b2c72dbc0d8975fc2a45533a8eb
Merge: 9324fdf5267b a074ae38f859
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Thu Jun 25 09:21:43 2015 +0530

    Merge branch 'topic/omap' into for-linus

commit 9324fdf5267b12f6db660fe52e882bbfffcc109a
Merge: 4983a501afed 5f88d9706fa4
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Thu Jun 25 09:21:37 2015 +0530

    Merge branch 'topic/core' into for-linus

commit 4983a501afede12f95d26e1e213f8f2e9eda1871
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon May 18 13:46:15 2015 +0200

    dmaengine: Revert "drivers/dma: remove unused support for MEMSET operations"
    
    This reverts commit 48a9db462d99494583dad829969616ac90a8df4e.
    
    Some platforms actually need support for the memset operations. Bring it back.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ad419757241f..19face3168b4 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -65,6 +65,7 @@ enum dma_transaction_type {
 	DMA_PQ,
 	DMA_XOR_VAL,
 	DMA_PQ_VAL,
+	DMA_MEMSET,
 	DMA_INTERRUPT,
 	DMA_SG,
 	DMA_PRIVATE,
@@ -570,6 +571,7 @@ struct dma_tx_state {
  * @copy_align: alignment shift for memcpy operations
  * @xor_align: alignment shift for xor operations
  * @pq_align: alignment shift for pq operations
+ * @fill_align: alignment shift for memset operations
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @src_addr_widths: bit mask of src addr widths the device supports
@@ -588,6 +590,7 @@ struct dma_tx_state {
  * @device_prep_dma_xor_val: prepares a xor validation operation
  * @device_prep_dma_pq: prepares a pq operation
  * @device_prep_dma_pq_val: prepares a pqzero_sum operation
+ * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
  * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
@@ -620,6 +623,7 @@ struct dma_device {
 	u8 copy_align;
 	u8 xor_align;
 	u8 pq_align;
+	u8 fill_align;
 	#define DMA_HAS_PQ_CONTINUE (1 << 15)
 
 	int dev_id;
@@ -650,6 +654,9 @@ struct dma_device {
 		struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
 		unsigned int src_cnt, const unsigned char *scf, size_t len,
 		enum sum_check_flags *pqres, unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
+		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
+		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(
@@ -745,6 +752,17 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
 	return chan->device->device_prep_interleaved_dma(chan, xt, flags);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memset(
+		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
+		unsigned long flags)
+{
+	if (!chan || !chan->device)
+		return NULL;
+
+	return chan->device->device_prep_dma_memset(chan, dest, value,
+						    len, flags);
+}
+
 static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 		struct dma_chan *chan,
 		struct scatterlist *dst_sg, unsigned int dst_nents,
@@ -820,6 +838,12 @@ static inline bool is_dma_pq_aligned(struct dma_device *dev, size_t off1,
 	return dmaengine_check_align(dev->pq_align, off1, off2, len);
 }
 
+static inline bool is_dma_fill_aligned(struct dma_device *dev, size_t off1,
+				       size_t off2, size_t len)
+{
+	return dmaengine_check_align(dev->fill_align, off1, off2, len);
+}
+
 static inline void
 dma_set_maxpq(struct dma_device *dma, int maxpq, int has_pq_continue)
 {

commit 87d001ef5366c4a24f7a1340246c4ce68190581c
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Wed May 27 16:01:52 2015 +0200

    dmaengine: Move icg helpers to global header
    
    Now that we can have ICGs set for both the source and destination (using
    the icg field of struct data_chunk) or for only the source or the
    destination (using the dst_icg or src_icg respectively), and that these
    fields can be ignored depending on other parameters (src_inc, src_sgl,
    etc.), the logic to get the actual ICG value can be quite tricky.
    
    The XDMAC driver was already implementing it, but since we will need it in
    other drivers, we can move it to the main header file.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Ludovic Desroches <ludovic.desroches@atmel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ad419757241f..499c530bcbaa 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -874,6 +874,33 @@ static inline int dma_maxpq(struct dma_device *dma, enum dma_ctrl_flags flags)
 	BUG();
 }
 
+static inline size_t dmaengine_get_icg(bool inc, bool sgl, size_t icg,
+				      size_t dir_icg)
+{
+	if (inc) {
+		if (dir_icg)
+			return dir_icg;
+		else if (sgl)
+			return icg;
+	}
+
+	return 0;
+}
+
+static inline size_t dmaengine_get_dst_icg(struct dma_interleaved_template *xt,
+					   struct data_chunk *chunk)
+{
+	return dmaengine_get_icg(xt->dst_inc, xt->dst_sgl,
+				 chunk->icg, chunk->dst_icg);
+}
+
+static inline size_t dmaengine_get_src_icg(struct dma_interleaved_template *xt,
+					   struct data_chunk *chunk)
+{
+	return dmaengine_get_icg(xt->src_inc, xt->src_sgl,
+				 chunk->icg, chunk->src_icg);
+}
+
 /* --- public DMA engine API --- */
 
 #ifdef CONFIG_DMA_ENGINE

commit e1031dc1f7ba5c8724ba211062134076df292791
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Thu May 7 17:38:07 2015 +0200

    dmaengine: Support different source and destination stride
    
    In interleaved mode, we can expect to have different source and destination
    strides.
    
    Add support for such case to dmaengine.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ad419757241f..5d63acb09813 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -122,10 +122,18 @@ enum dma_transfer_direction {
  *	 chunk and before first src/dst address for next chunk.
  *	 Ignored for dst(assumed 0), if dst_inc is true and dst_sgl is false.
  *	 Ignored for src(assumed 0), if src_inc is true and src_sgl is false.
+ * @dst_icg: Number of bytes to jump after last dst address of this
+ *	 chunk and before the first dst address for next chunk.
+ *	 Ignored if dst_inc is true and dst_sgl is false.
+ * @src_icg: Number of bytes to jump after last src address of this
+ *	 chunk and before the first src address for next chunk.
+ *	 Ignored if src_inc is true and src_sgl is false.
  */
 struct data_chunk {
 	size_t size;
 	size_t icg;
+	size_t dst_icg;
+	size_t src_icg;
 };
 
 /**

commit 56f13c0d9524c5816f5dc9c91b9d766d6b1064ca
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Thu Apr 9 12:35:47 2015 +0300

    dmaengine: of_dma: Support for DMA routers
    
    DMA routers are transparent devices used to mux DMA requests from
    peripherals to DMA controllers. They are used when the SoC integrates more
    devices with DMA requests then their controller can handle.
    DRA7x is one example of such SoC, where the sDMA can hanlde 128 DMA request
    lines, but in SoC level it has 205 DMA requests.
    
    The of_dma_router will be registered as of_dma_controller with special
    xlate function and additional parameters. The driver for the router is
    responsible to craft the dma_spec (in the of_dma_route_allocate callback)
    which can be used to requests a DMA channel from the real DMA controller.
    This way the router can be transparent for the system while remaining generic
    enough to be used in different environments.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ad419757241f..abf63ceabef9 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -221,6 +221,16 @@ struct dma_chan_percpu {
 	unsigned long bytes_transferred;
 };
 
+/**
+ * struct dma_router - DMA router structure
+ * @dev: pointer to the DMA router device
+ * @route_free: function to be called when the route can be disconnected
+ */
+struct dma_router {
+	struct device *dev;
+	void (*route_free)(struct device *dev, void *route_data);
+};
+
 /**
  * struct dma_chan - devices supply DMA channels, clients use them
  * @device: ptr to the dma device who supplies this channel, always !%NULL
@@ -232,6 +242,8 @@ struct dma_chan_percpu {
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client_count: how many clients are using this channel
  * @table_count: number of appearances in the mem-to-mem allocation table
+ * @router: pointer to the DMA router structure
+ * @route_data: channel specific data for the router
  * @private: private data for certain client-channel associations
  */
 struct dma_chan {
@@ -247,6 +259,11 @@ struct dma_chan {
 	struct dma_chan_percpu __percpu *local;
 	int client_count;
 	int table_count;
+
+	/* DMA router */
+	struct dma_router *router;
+	void *route_data;
+
 	void *private;
 };
 

commit 3b62286d0ef785815994e2558e8cfb686597b0cd
Author: Jarkko Nikula <jarkko.nikula@linux.intel.com>
Date:   Mon Mar 16 09:37:24 2015 +0200

    dmaengine: Remove FSF mailing addresses
    
    Free Software Foundation mailing address has been moved in the past and some
    of the addresses here are outdated. Remove them from file headers since the
    COPYING file in the kernel sources includes it.
    
    Signed-off-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2bff9abc162a..ad419757241f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -11,10 +11,6 @@
  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
  * more details.
  *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59
- * Temple Place - Suite 330, Boston, MA  02111-1307, USA.
- *
  * The full GNU General Public License is included in this distribution in the
  * file called COPYING.
  */

commit bfde98bd762346639f0a5a557e02c4828dd6273b
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Thu Mar 5 11:48:50 2015 +0100

    dmaengine: Remove net_dma_find_channel
    
    Since commit 7bced397510a ("net_dma: simple removal") removed the net_dma
    support entirely, net_dma_find_channel has no users left. Remove the function
    entirely.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index f5cc5d4f1ad5..2bff9abc162a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1090,7 +1090,6 @@ void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
 struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
-struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 #define dma_request_slave_channel_compat(mask, x, y, dev, name) \
 	__dma_request_slave_channel_compat(&(mask), x, y, dev, name)

commit 68c062eaa87b7b85b65f20f25c54524437715a95
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Thu Mar 5 11:42:43 2015 +0100

    dmaengine: Remove net_dma leftovers
    
    Commit 7bce d397 510a ("net_dma: simple removal") removed the net_dma support
    entirely but left some functions and prototypes in the dmaengine header.
    Remove them.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index db0104b0da4d..f5cc5d4f1ad5 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1108,27 +1108,4 @@ static inline struct dma_chan
 
 	return __dma_request_channel(mask, fn, fn_param);
 }
-
-/* --- Helper iov-locking functions --- */
-
-struct dma_page_list {
-	char __user *base_address;
-	int nr_pages;
-	struct page **pages;
-};
-
-struct dma_pinned_list {
-	int nr_iovecs;
-	struct dma_page_list page_list[0];
-};
-
-struct dma_pinned_list *dma_pin_iovec_pages(struct iovec *iov, size_t len);
-void dma_unpin_iovec_pages(struct dma_pinned_list* pinned_list);
-
-dma_cookie_t dma_memcpy_to_iovec(struct dma_chan *chan, struct iovec *iov,
-	struct dma_pinned_list *pinned_list, unsigned char *kdata, size_t len);
-dma_cookie_t dma_memcpy_pg_to_iovec(struct dma_chan *chan, struct iovec *iov,
-	struct dma_pinned_list *pinned_list, struct page *page,
-	unsigned int offset, size_t len);
-
 #endif /* DMAENGINE_H */

commit e921eea8e7d4457f424bc3f821cb836e35b91f88
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Tue Mar 3 12:05:00 2015 +0100

    dmaengine: Remove memset leftovers
    
    Commit 48a9db462d99 ("drivers/dma: remove unused support for MEMSET
    operations") removed support for the memset operation in dmaengine, but left
    the fill_aligned field that was supposed to set the buffer alignment for the
    memset operations.
    
    Remove that field too.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b6997a0cb528..db0104b0da4d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -574,7 +574,6 @@ struct dma_tx_state {
  * @copy_align: alignment shift for memcpy operations
  * @xor_align: alignment shift for xor operations
  * @pq_align: alignment shift for pq operations
- * @fill_align: alignment shift for memset operations
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @src_addr_widths: bit mask of src addr widths the device supports
@@ -625,7 +624,6 @@ struct dma_device {
 	u8 copy_align;
 	u8 xor_align;
 	u8 pq_align;
-	u8 fill_align;
 	#define DMA_HAS_PQ_CONTINUE (1 << 15)
 
 	int dev_id;
@@ -826,12 +824,6 @@ static inline bool is_dma_pq_aligned(struct dma_device *dev, size_t off1,
 	return dmaengine_check_align(dev->pq_align, off1, off2, len);
 }
 
-static inline bool is_dma_fill_aligned(struct dma_device *dev, size_t off1,
-				       size_t off2, size_t len)
-{
-	return dmaengine_check_align(dev->fill_align, off1, off2, len);
-}
-
 static inline void
 dma_set_maxpq(struct dma_device *dma, int maxpq, int has_pq_continue)
 {

commit 46c2eb645991c00bcbdf991c2f74389f7efce918
Merge: 2cd6f7928ca4 ee4b876bbee2
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Feb 2 16:55:43 2015 -0800

    Merge branch 'topic/rcar' into for-linus

commit fdb8df9933632e177621daf60da74fc693a8c7d1
Author: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
Date:   Mon Jan 19 13:54:27 2015 +0200

    dmaengine: Add dma_get_slave_caps() inline stub when !CONFIG_DMA_ENGINE
    
    Commit 0d5484b1c3db8a38 ("dmaengine: Move dma_get_slave_caps()
    implementation to dmaengine.c") turned the inline dma_get_slave_caps()
    function into an external function without adding an inline stub for the
    cases where CONFIG_DMA_ENGINE isn't set. This breaks compilation of
    drivers using the DMA engine API when CONFIG_DMA_ENGINE isn't set.
    
    Add an inline stub to fix compilation.
    
    Signed-off-by: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
    Fixes: 0d5484b1c3db ("dmaengine: Move dma_get_slave_caps() implementation to dmaengine.c")
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1b4842bb3890..50745e3a8a3f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -758,8 +758,6 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 			src_sg, src_nents, flags);
 }
 
-int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps);
-
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	if (chan->device->device_terminate_all)
@@ -1048,6 +1046,7 @@ struct dma_chan *dma_request_slave_channel_reason(struct device *dev,
 						  const char *name);
 struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
 void dma_release_channel(struct dma_chan *chan);
+int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps);
 #else
 static inline struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type)
 {
@@ -1082,6 +1081,11 @@ static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
 static inline void dma_release_channel(struct dma_chan *chan)
 {
 }
+static inline int dma_get_slave_caps(struct dma_chan *chan,
+				     struct dma_slave_caps *caps)
+{
+	return -ENXIO;
+}
 #endif
 
 /* --- DMA device --- */

commit 0d5484b1c3db8a3870c6100deeb4678594433b2c
Author: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
Date:   Wed Oct 29 00:30:58 2014 +0200

    dmaengine: Move dma_get_slave_caps() implementation to dmaengine.c
    
    The function is too big to be a static inline.
    
    Signed-off-by: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6d34ce91036c..1b4842bb3890 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -758,37 +758,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 			src_sg, src_nents, flags);
 }
 
-static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps)
-{
-	struct dma_device *device;
-
-	if (!chan || !caps)
-		return -EINVAL;
-
-	device = chan->device;
-
-	/* check if the channel supports slave transactions */
-	if (!test_bit(DMA_SLAVE, device->cap_mask.bits))
-		return -ENXIO;
-
-	/*
-	 * Check whether it reports it uses the generic slave
-	 * capabilities, if not, that means it doesn't support any
-	 * kind of slave capabilities reporting.
-	 */
-	if (!device->directions)
-		return -ENXIO;
-
-	caps->src_addr_widths = device->src_addr_widths;
-	caps->dst_addr_widths = device->dst_addr_widths;
-	caps->directions = device->directions;
-	caps->residue_granularity = device->residue_granularity;
-
-	caps->cmd_pause = !!device->device_pause;
-	caps->cmd_terminate = !!device->device_terminate_all;
-
-	return 0;
-}
+int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps);
 
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {

commit 534a729866f9edc9264340c5b96cb94878ffda00
Author: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
Date:   Wed Aug 6 10:52:41 2014 +0200

    dmaengine: Add 16 bytes, 32 bytes and 64 bytes bus widths
    
    The widths are missing, add them.
    
    Signed-off-by: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
    Tested-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Tested-by: Wolfram Sang <wsa+renesas@sang-engineering.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6d34ce91036c..b7724a5d4661 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -279,6 +279,9 @@ enum dma_slave_buswidth {
 	DMA_SLAVE_BUSWIDTH_3_BYTES = 3,
 	DMA_SLAVE_BUSWIDTH_4_BYTES = 4,
 	DMA_SLAVE_BUSWIDTH_8_BYTES = 8,
+	DMA_SLAVE_BUSWIDTH_16_BYTES = 16,
+	DMA_SLAVE_BUSWIDTH_32_BYTES = 32,
+	DMA_SLAVE_BUSWIDTH_64_BYTES = 64,
 };
 
 /**

commit 2c44ad914c56f4e53ef43285b5e4fe3459109769
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:42:54 2014 +0100

    dmaengine: Remove device_control and device_slave_caps
    
    Now that device_control has been split into several functions, and
    device_slave_caps rendered useless, we can safely remove them.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index adf22089cc93..6d34ce91036c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -188,25 +188,6 @@ enum dma_ctrl_flags {
 	DMA_PREP_FENCE = (1 << 5),
 };
 
-/**
- * enum dma_ctrl_cmd - DMA operations that can optionally be exercised
- * on a running channel.
- * @DMA_TERMINATE_ALL: terminate all ongoing transfers
- * @DMA_PAUSE: pause ongoing transfers
- * @DMA_RESUME: resume paused transfer
- * @DMA_SLAVE_CONFIG: this command is only implemented by DMA controllers
- * that need to runtime reconfigure the slave channels (as opposed to passing
- * configuration data in statically from the platform). An additional
- * argument of struct dma_slave_config must be passed in with this
- * command.
- */
-enum dma_ctrl_cmd {
-	DMA_TERMINATE_ALL,
-	DMA_PAUSE,
-	DMA_RESUME,
-	DMA_SLAVE_CONFIG,
-};
-
 /**
  * enum sum_check_bits - bit position of pq_check_flags
  */
@@ -336,9 +317,8 @@ enum dma_slave_buswidth {
  * This struct is passed in as configuration data to a DMA engine
  * in order to set up a certain channel for DMA transport at runtime.
  * The DMA device/engine has to provide support for an additional
- * command in the channel config interface, DMA_SLAVE_CONFIG
- * and this struct will then be passed in as an argument to the
- * DMA engine device_control() function.
+ * callback in the dma_device structure, device_config and this struct
+ * will then be passed in as an argument to the function.
  *
  * The rationale for adding configuration information to this struct is as
  * follows: if it is likely that more than one DMA slave controllers in
@@ -618,8 +598,6 @@ struct dma_tx_state {
  * @device_prep_interleaved_dma: Transfer expression in a generic way.
  * @device_config: Pushes a new configuration to a channel, return 0 or an error
  *	code
- * @device_control: manipulate all pending operations on a channel, returns
- *	zero or error code
  * @device_pause: Pauses any transfer happening on a channel. Returns
  *	0 or an error code
  * @device_resume: Resumes any transfer on a channel previously
@@ -631,7 +609,6 @@ struct dma_tx_state {
  *	struct with auxiliary transfer status information, otherwise the call
  *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
- * @device_slave_caps: return the slave channel capabilities
  */
 struct dma_device {
 
@@ -698,8 +675,6 @@ struct dma_device {
 
 	int (*device_config)(struct dma_chan *chan,
 			     struct dma_slave_config *config);
-	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
-		unsigned long arg);
 	int (*device_pause)(struct dma_chan *chan);
 	int (*device_resume)(struct dma_chan *chan);
 	int (*device_terminate_all)(struct dma_chan *chan);
@@ -708,27 +683,15 @@ struct dma_device {
 					    dma_cookie_t cookie,
 					    struct dma_tx_state *txstate);
 	void (*device_issue_pending)(struct dma_chan *chan);
-	int (*device_slave_caps)(struct dma_chan *chan, struct dma_slave_caps *caps);
 };
 
-static inline int dmaengine_device_control(struct dma_chan *chan,
-					   enum dma_ctrl_cmd cmd,
-					   unsigned long arg)
-{
-	if (chan->device->device_control)
-		return chan->device->device_control(chan, cmd, arg);
-
-	return -ENOSYS;
-}
-
 static inline int dmaengine_slave_config(struct dma_chan *chan,
 					  struct dma_slave_config *config)
 {
 	if (chan->device->device_config)
 		return chan->device->device_config(chan, config);
 
-	return dmaengine_device_control(chan, DMA_SLAVE_CONFIG,
-			(unsigned long)config);
+	return -ENOSYS;
 }
 
 static inline bool is_slave_direction(enum dma_transfer_direction direction)
@@ -808,9 +771,6 @@ static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_cap
 	if (!test_bit(DMA_SLAVE, device->cap_mask.bits))
 		return -ENXIO;
 
-	if (device->device_slave_caps)
-		return device->device_slave_caps(chan, caps);
-
 	/*
 	 * Check whether it reports it uses the generic slave
 	 * capabilities, if not, that means it doesn't support any
@@ -835,7 +795,7 @@ static inline int dmaengine_terminate_all(struct dma_chan *chan)
 	if (chan->device->device_terminate_all)
 		return chan->device->device_terminate_all(chan);
 
-	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);
+	return -ENOSYS;
 }
 
 static inline int dmaengine_pause(struct dma_chan *chan)
@@ -843,7 +803,7 @@ static inline int dmaengine_pause(struct dma_chan *chan)
 	if (chan->device->device_pause)
 		return chan->device->device_pause(chan);
 
-	return dmaengine_device_control(chan, DMA_PAUSE, 0);
+	return -ENOSYS;
 }
 
 static inline int dmaengine_resume(struct dma_chan *chan)
@@ -851,7 +811,7 @@ static inline int dmaengine_resume(struct dma_chan *chan)
 	if (chan->device->device_resume)
 		return chan->device->device_resume(chan);
 
-	return dmaengine_device_control(chan, DMA_RESUME, 0);
+	return -ENOSYS;
 }
 
 static inline enum dma_status dmaengine_tx_status(struct dma_chan *chan,

commit cb8cea513c80db1dfe2dce468d2d0772005bb9a1
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:42:04 2014 +0100

    dmaengine: Create a generic dma_slave_caps callback
    
    dma_slave_caps is very important to the generic layers that might interact with
    dmaengine, such as ASoC. Unfortunately, it has been added as yet another
    dma_device callback, and most of the existing drivers haven't implemented it,
    reducing its reliability.
    
    Introduce a generic behaviour to implement this, that rely on both the split of
    device_control to derive which functions are supported and on new variables to
    be set in the dma_device structure.
    
    These variables holds what used to be the capabilities, that were set
    per-channel. However, this proved to be a bit overkill, since every driver
    filling these so far were hardcoding it, disregarding which channel was
    actually given.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ded5161653aa..adf22089cc93 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -594,6 +594,14 @@ struct dma_tx_state {
  * @fill_align: alignment shift for memset operations
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
+ * @src_addr_widths: bit mask of src addr widths the device supports
+ * @dst_addr_widths: bit mask of dst addr widths the device supports
+ * @directions: bit mask of slave direction the device supports since
+ * 	the enum dma_transfer_direction is not defined as bits for
+ * 	each type of direction, the dma controller should fill (1 <<
+ * 	<TYPE>) and same should be checked by controller as well
+ * @residue_granularity: granularity of the transfer residue reported
+ *	by tx_status
  * @device_alloc_chan_resources: allocate resources and return the
  *	number of allocated descriptors
  * @device_free_chan_resources: release DMA channel's resources
@@ -643,6 +651,11 @@ struct dma_device {
 	int dev_id;
 	struct device *dev;
 
+	u32 src_addr_widths;
+	u32 dst_addr_widths;
+	u32 directions;
+	enum dma_residue_granularity residue_granularity;
+
 	int (*device_alloc_chan_resources)(struct dma_chan *chan);
 	void (*device_free_chan_resources)(struct dma_chan *chan);
 
@@ -784,17 +797,37 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
 
 static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps)
 {
+	struct dma_device *device;
+
 	if (!chan || !caps)
 		return -EINVAL;
 
+	device = chan->device;
+
 	/* check if the channel supports slave transactions */
-	if (!test_bit(DMA_SLAVE, chan->device->cap_mask.bits))
+	if (!test_bit(DMA_SLAVE, device->cap_mask.bits))
+		return -ENXIO;
+
+	if (device->device_slave_caps)
+		return device->device_slave_caps(chan, caps);
+
+	/*
+	 * Check whether it reports it uses the generic slave
+	 * capabilities, if not, that means it doesn't support any
+	 * kind of slave capabilities reporting.
+	 */
+	if (!device->directions)
 		return -ENXIO;
 
-	if (chan->device->device_slave_caps)
-		return chan->device->device_slave_caps(chan, caps);
+	caps->src_addr_widths = device->src_addr_widths;
+	caps->dst_addr_widths = device->dst_addr_widths;
+	caps->directions = device->directions;
+	caps->residue_granularity = device->residue_granularity;
+
+	caps->cmd_pause = !!device->device_pause;
+	caps->cmd_terminate = !!device->device_terminate_all;
 
-	return -ENXIO;
+	return 0;
 }
 
 static inline int dmaengine_terminate_all(struct dma_chan *chan)

commit 7fa0cf462daa6f6121b332b87833d7f5bdb515c0
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:42:02 2014 +0100

    dmaengine: Add device_terminate_all callback
    
    Split out the terminate_all command from device_control to a dma_device
    callback. In order to preserve backward capability, still rely on
    device_control if no such callback has been implemented.
    
    Eventually, this will allow to create a generic dma_slave_caps callback.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 01f27e8a69b7..ded5161653aa 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -616,6 +616,8 @@ struct dma_tx_state {
  *	0 or an error code
  * @device_resume: Resumes any transfer on a channel previously
  *	paused. Returns 0 or an error code
+ * @device_terminate_all: Aborts all transfers on a channel. Returns 0
+ *	or an error code
  * @device_tx_status: poll for transaction completion, the optional
  *	txstate parameter can be supplied with a pointer to get a
  *	struct with auxiliary transfer status information, otherwise the call
@@ -687,6 +689,7 @@ struct dma_device {
 		unsigned long arg);
 	int (*device_pause)(struct dma_chan *chan);
 	int (*device_resume)(struct dma_chan *chan);
+	int (*device_terminate_all)(struct dma_chan *chan);
 
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,
@@ -796,6 +799,9 @@ static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_cap
 
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
+	if (chan->device->device_terminate_all)
+		return chan->device->device_terminate_all(chan);
+
 	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);
 }
 

commit 23a3ea2f5bead4d3b16e119e9127a66234f41d53
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:42:01 2014 +0100

    dmaengine: split out pause/resume operations from device_control
    
    Split out the pause and resume operations to callbacks of their own. In order
    to preserve some backwark compatibility, the dmaengine_pause/dmaengine_resume
    are still falling back on dmaengine_device_control.
    
    Eventually, that will allow to get the device capabilities in a generic way,
    removing the need to implement device_slave_caps.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cf7c85d408ad..01f27e8a69b7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -612,6 +612,10 @@ struct dma_tx_state {
  *	code
  * @device_control: manipulate all pending operations on a channel, returns
  *	zero or error code
+ * @device_pause: Pauses any transfer happening on a channel. Returns
+ *	0 or an error code
+ * @device_resume: Resumes any transfer on a channel previously
+ *	paused. Returns 0 or an error code
  * @device_tx_status: poll for transaction completion, the optional
  *	txstate parameter can be supplied with a pointer to get a
  *	struct with auxiliary transfer status information, otherwise the call
@@ -681,6 +685,8 @@ struct dma_device {
 			     struct dma_slave_config *config);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
+	int (*device_pause)(struct dma_chan *chan);
+	int (*device_resume)(struct dma_chan *chan);
 
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,
@@ -795,11 +801,17 @@ static inline int dmaengine_terminate_all(struct dma_chan *chan)
 
 static inline int dmaengine_pause(struct dma_chan *chan)
 {
+	if (chan->device->device_pause)
+		return chan->device->device_pause(chan);
+
 	return dmaengine_device_control(chan, DMA_PAUSE, 0);
 }
 
 static inline int dmaengine_resume(struct dma_chan *chan)
 {
+	if (chan->device->device_resume)
+		return chan->device->device_resume(chan);
+
 	return dmaengine_device_control(chan, DMA_RESUME, 0);
 }
 

commit 94a73e30dfe6722e9f4ef19f7892901d7d00eab1
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:42:00 2014 +0100

    dmaengine: Introduce a device_config callback
    
    The fact that the channel configuration is done in device_control is rather
    misleading, since it's not really advertised as such, plus, the fact that the
    framework exposes a function of its own makes it not really intuitive, while
    we're losing the type checking whenever we pass that unsigned long argument.
    
    Add a device_config callback to dma_device, with a fallback on the old
    behaviour for now for existing drivers to opt in.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 03a1febe8740..cf7c85d408ad 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -608,6 +608,8 @@ struct dma_tx_state {
  *	The function takes a buffer of size buf_len. The callback function will
  *	be called after period_len bytes have been transferred.
  * @device_prep_interleaved_dma: Transfer expression in a generic way.
+ * @device_config: Pushes a new configuration to a channel, return 0 or an error
+ *	code
  * @device_control: manipulate all pending operations on a channel, returns
  *	zero or error code
  * @device_tx_status: poll for transaction completion, the optional
@@ -674,6 +676,9 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags);
+
+	int (*device_config)(struct dma_chan *chan,
+			     struct dma_slave_config *config);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
 
@@ -697,6 +702,9 @@ static inline int dmaengine_device_control(struct dma_chan *chan,
 static inline int dmaengine_slave_config(struct dma_chan *chan,
 					  struct dma_slave_config *config)
 {
+	if (chan->device->device_config)
+		return chan->device->device_config(chan, config);
+
 	return dmaengine_device_control(chan, DMA_SLAVE_CONFIG,
 			(unsigned long)config);
 }

commit ceacbdbf65c4cf48a130db6152c6e03432c85ed1
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Mon Nov 17 14:41:57 2014 +0100

    dmaengine: Make the destination abbreviation coherent
    
    The dmaengine header abbreviates destination as at least two different strings.
    Make a coherent use of a single one.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Mark Brown <broonie@kernel.org>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Acked-by: Stephen Warren <swarren@wwwdotorg.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 40cd75e21ea2..03a1febe8740 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -387,7 +387,7 @@ enum dma_residue_granularity {
 /* struct dma_slave_caps - expose capabilities of a slave channel only
  *
  * @src_addr_widths: bit mask of src addr widths the channel supports
- * @dstn_addr_widths: bit mask of dstn addr widths the channel supports
+ * @dst_addr_widths: bit mask of dstn addr widths the channel supports
  * @directions: bit mask of slave direction the channel supported
  * 	since the enum dma_transfer_direction is not defined as bits for each
  * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
@@ -398,7 +398,7 @@ enum dma_residue_granularity {
  */
 struct dma_slave_caps {
 	u32 src_addr_widths;
-	u32 dstn_addr_widths;
+	u32 dst_addr_widths;
 	u32 directions;
 	bool cmd_pause;
 	bool cmd_terminate;
@@ -639,10 +639,10 @@ struct dma_device {
 	void (*device_free_chan_resources)(struct dma_chan *chan);
 
 	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(
-		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		struct dma_chan *chan, dma_addr_t dst, dma_addr_t src,
 		size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(
-		struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
+		struct dma_chan *chan, dma_addr_t dst, dma_addr_t *src,
 		unsigned int src_cnt, size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(
 		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,

commit aba96bada40d19a0afbc3bfcb3a47e29e23df7ea
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Fri Dec 5 20:49:07 2014 +0530

    dmaengine: clarify the issue_pending expectations
    
    Although Documentation explicitly mentions the expectations, the comment in
    header can be lead to different expectation so clear up the confusion
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 653a1fd07ae8..40cd75e21ea2 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -447,7 +447,8 @@ struct dmaengine_unmap_data {
  * 	communicate status
  * @phys: physical address of the descriptor
  * @chan: target channel for this operation
- * @tx_submit: set the prepared descriptor(s) to be executed by the engine
+ * @tx_submit: accept the descriptor, assign ordered cookie and mark the
+ * descriptor pending. To be pushed on .issue_pending() call
  * @callback: routine to call after this operation is complete
  * @callback_param: general parameter to pass to the callback routine
  * ---async_tx api specific fields---

commit 52d589a01d4545ce1dc5c3892bb8c7b55edfe714
Merge: 0a582821d4f8 6b997bab2044
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 18 18:11:04 2014 -0700

    Merge branch 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine updates from Vinod Koul:
     "For dmaengine contributions we have:
       - designware cleanup by Andy
       - my series moving device_control users to dmanegine_xxx APIs for
         later removal of device_control API
       - minor fixes spread over drivers mainly mv_xor, pl330, mmp, imx-sdma
         etc"
    
    * 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma: (60 commits)
      serial: atmel: add missing dmaengine header
      dmaengine: remove FSLDMA_EXTERNAL_START
      dmaengine: freescale: remove FSLDMA_EXTERNAL_START control method
      carma-fpga: move to fsl_dma_external_start()
      carma-fpga: use dmaengine_xxx() API
      dmaengine: freescale: add and export fsl_dma_external_start()
      dmaengine: add dmaengine_prep_dma_sg() helper
      video: mx3fb: use dmaengine_terminate_all() API
      serial: sh-sci: use dmaengine_terminate_all() API
      net: ks8842: use dmaengine_terminate_all() API
      mtd: sh_flctl: use dmaengine_terminate_all() API
      mtd: fsmc_nand: use dmaengine_terminate_all() API
      V4L2: mx3_camer: use dmaengine_pause() API
      dmaengine: coh901318: use dmaengine_terminate_all() API
      pata_arasan_cf: use dmaengine_terminate_all() API
      dmaengine: edma: check for echan->edesc => NULL in edma_dma_pause()
      dmaengine: dw: export probe()/remove() and Co to users
      dmaengine: dw: enable and disable controller when needed
      dmaengine: dw: always export dw_dma_{en,dis}able
      dmaengine: dw: introduce dw_dma_on() helper
      ...

commit cf6c0ab54daeb614ae79bdd0168e8b7ec2650134
Merge: 2856fcdc1f7b b80719b6bd08
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Oct 15 21:39:09 2014 +0530

    Merge branch 'topic/dma_control_fsl_acks' into for-linus

commit b80719b6bd083130c112cb4d3e5329a164eef4c3
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Sat Oct 11 21:16:48 2014 +0530

    dmaengine: remove FSLDMA_EXTERNAL_START
    
    as users have been converted, so no need of this custom method
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 7e6b3a281da8..f8e5a9ea461a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -199,15 +199,12 @@ enum dma_ctrl_flags {
  * configuration data in statically from the platform). An additional
  * argument of struct dma_slave_config must be passed in with this
  * command.
- * @FSLDMA_EXTERNAL_START: this command will put the Freescale DMA controller
- * into external start mode.
  */
 enum dma_ctrl_cmd {
 	DMA_TERMINATE_ALL,
 	DMA_PAUSE,
 	DMA_RESUME,
 	DMA_SLAVE_CONFIG,
-	FSLDMA_EXTERNAL_START,
 };
 
 /**

commit b65612a868768cd0431084ccf376d0946c12132d
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Sat Oct 11 21:16:43 2014 +0530

    dmaengine: add dmaengine_prep_dma_sg() helper
    
    This was only prep API which didnt have an helper
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1f9e642c66ad..7e6b3a281da8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -755,6 +755,16 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
 	return chan->device->device_prep_interleaved_dma(chan, xt, flags);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_sg(
+		struct dma_chan *chan,
+		struct scatterlist *dst_sg, unsigned int dst_nents,
+		struct scatterlist *src_sg, unsigned int src_nents,
+		unsigned long flags)
+{
+	return chan->device->device_prep_dma_sg(chan, dst_sg, dst_nents,
+			src_sg, src_nents, flags);
+}
+
 static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps)
 {
 	if (!chan || !caps)

commit d0cd84817c745655428dbfdb1e3f754230b46bef
Merge: bdf428feb225 3f3340785672
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 7 20:39:25 2014 -0400

    Merge tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine updates from Dan Williams:
     "Even though this has fixes marked for -stable, given the size and the
      needed conflict resolutions this is 3.18-rc1/merge-window material.
    
      These patches have been languishing in my tree for a long while.  The
      fact that I do not have the time to do proper/prompt maintenance of
      this tree is a primary factor in the decision to step down as
      dmaengine maintainer.  That and the fact that the bulk of drivers/dma/
      activity is going through Vinod these days.
    
      The net_dma removal has not been in -next.  It has developed simple
      conflicts against mainline and net-next (for-3.18).
    
      Continuing thanks to Vinod for staying on top of drivers/dma/.
    
      Summary:
    
       1/ Step down as dmaengine maintainer see commit 08223d80df38
          "dmaengine maintainer update"
    
       2/ Removal of net_dma, as it has been marked 'broken' since 3.13
          (commit 77873803363c "net_dma: mark broken"), without reports of
          performance regression.
    
       3/ Miscellaneous fixes"
    
    * tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      net: make tcp_cleanup_rbuf private
      net_dma: revert 'copied_early'
      net_dma: simple removal
      dmaengine maintainer update
      dmatest: prevent memory leakage on error path in thread
      ioat: Use time_before_jiffies()
      dmaengine: fix xor sources continuation
      dma: mv_xor: Rename __mv_xor_slot_cleanup() to mv_xor_slot_cleanup()
      dma: mv_xor: Remove all callers of mv_xor_slot_cleanup()
      dma: mv_xor: Remove unneeded mv_xor_clean_completed_slots() call
      ioat: Use pci_enable_msix_exact() instead of pci_enable_msix()
      drivers: dma: Include appropriate header file in dca.c
      drivers: dma: Mark functions as static in dma_v3.c
      dma: mv_xor: Add DMA API error checks
      ioat/dca: Use dev_is_pci() to check whether it is pci device

commit 7bced397510ab569d31de4c70b39e13355046387
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 12:37:29 2013 -0800

    net_dma: simple removal
    
    Per commit "77873803363c net_dma: mark broken" net_dma is no longer used
    and there is no plan to fix it.
    
    This is the mechanical removal of bits in CONFIG_NET_DMA ifdef guards.
    Reverting the remainder of the net_dma induced changes is deferred to
    subsequent patches.
    
    Marked for stable due to Roman's report of a memory leak in
    dma_pin_iovec_pages():
    
        https://lkml.org/lkml/2014/9/3/177
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: David Whipple <whipple@securedatainnovations.ch>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c5c92d59e531..3e382ecd1927 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -903,18 +903,6 @@ static inline void dmaengine_put(void)
 }
 #endif
 
-#ifdef CONFIG_NET_DMA
-#define net_dmaengine_get()	dmaengine_get()
-#define net_dmaengine_put()	dmaengine_put()
-#else
-static inline void net_dmaengine_get(void)
-{
-}
-static inline void net_dmaengine_put(void)
-{
-}
-#endif
-
 #ifdef CONFIG_ASYNC_TX_DMA
 #define async_dmaengine_get()	dmaengine_get()
 #define async_dmaengine_put()	dmaengine_put()
@@ -936,16 +924,8 @@ async_dma_find_channel(enum dma_transaction_type type)
 	return NULL;
 }
 #endif /* CONFIG_ASYNC_TX_DMA */
-
-dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
-	void *dest, void *src, size_t len);
-dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
-	struct page *page, unsigned int offset, void *kdata, size_t len);
-dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
-	struct page *dest_pg, unsigned int dest_off, struct page *src_pg,
-	unsigned int src_off, size_t len);
 void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
-	struct dma_chan *chan);
+				  struct dma_chan *chan);
 
 static inline void async_tx_ack(struct dma_async_tx_descriptor *tx)
 {

commit d9ff958bb34aabdce08d11b0db24123c093d87cd
Author: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
Date:   Wed Aug 20 19:20:53 2014 +0200

    dmaengine: Mark the struct dma_slave_config direction field deprecated
    
    The direction passed to the device_prep_slave_sg, device_prep_dma_cyclic
    or device_prep_interleaved_dma (through struct dma_interleaved_template)
    should be used instead.
    
    Signed-off-by: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1f9e642c66ad..3d291f59acd8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -307,7 +307,9 @@ enum dma_slave_buswidth {
  * struct dma_slave_config - dma slave channel runtime config
  * @direction: whether the data shall go in or out on this slave
  * channel, right now. DMA_MEM_TO_DEV and DMA_DEV_TO_MEM are
- * legal values.
+ * legal values. DEPRECATED, drivers should use the direction argument
+ * to the device_prep_slave_sg and device_prep_dma_cyclic functions or
+ * the dir field in the dma_interleaved_template structure.
  * @src_addr: this is the physical address where DMA slave data
  * should be read (RX), if the source is memory this argument is
  * ignored.

commit c7a19c795b4b0a3232c157ed29eea85077e95da6
Merge: 5fd41f2a10b3 a0bbe990c161
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 11 07:14:01 2014 -0700

    Merge branch 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dma updates from Vinod Koul:
     "Some notable changes are:
       - new driver for AMBA AXI NBPF by Guennadi
       - new driver for sun6i controller by Maxime
       - pl330 drivers fixes from Lar's
       - sh-dma updates and fixes from Laurent, Geert and Kuninori
       - Documentation updates from Geert
       - drivers fixes and updates spread over dw, edma, freescale, mpc512x
         etc.."
    
    * 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma: (72 commits)
      dmaengine: sun6i: depends on RESET_CONTROLLER
      dma: at_hdmac: fix invalid remaining bytes detection
      dmaengine: nbpfaxi: don't build this driver where it cannot be used
      dmaengine: nbpf_error_get_channel() can be static
      dma: pl08x: Use correct specifier for size_t values
      dmaengine: Remove the context argument to the prep_dma_cyclic operation
      dmaengine: nbpfaxi: convert to tasklet
      dmaengine: nbpfaxi: fix a theoretical race
      dmaengine: add a driver for AMBA AXI NBPF DMAC IP cores
      dmaengine: add device tree binding documentation for the nbpfaxi driver
      dmaengine: edma: Do not register second device when booted with DT
      dmaengine: edma: Do not change the error code returned from edma_alloc_slot
      dmaengine: rcar-dmac: Add device tree bindings documentation
      dmaengine: shdma: Allocate cyclic sg list dynamically
      dmaengine: shdma: Make channel filter ignore unrelated devices
      dmaengine: sh: Rework Kconfig and Makefile
      dmaengine: sun6i: Fix memory leaks
      dmaengine: sun6i: Free the interrupt before killing the tasklet
      dmaengine: sun6i: Remove switch statement from buswidth convertion routine
      dmaengine: of: kconfig: select DMA_ENGINE when DMA_OF is selected
      ...

commit 31c1e5a1350ae8d1bc2018f5de8264266d9773e1
Author: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
Date:   Fri Aug 1 12:20:10 2014 +0200

    dmaengine: Remove the context argument to the prep_dma_cyclic operation
    
    The argument is always set to NULL and never used. Remove it.
    
    Signed-off-by: Laurent Pinchart <laurent.pinchart+renesas@ideasonboard.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 4eb2f82aed1d..94ddccd706fc 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -669,7 +669,7 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
 		size_t period_len, enum dma_transfer_direction direction,
-		unsigned long flags, void *context);
+		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags);
@@ -744,7 +744,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		unsigned long flags)
 {
 	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
-						period_len, dir, flags, NULL);
+						period_len, dir, flags);
 }
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(

commit 0c9dbebdb6611d2cd75d025ec09035c3e8ce2160
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Fri Jul 11 18:18:26 2014 +0200

    dmaengine: Remove unused definition of DMA_MAX_COOKIE
    
    As of commit commit f04cd40701deace2efb9edd7120e59366bda2118 ("fsldma: fix
    controller lockups"), its last (and only ever) user is gone.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d2c5cc7c583c..4eb2f82aed1d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -37,7 +37,6 @@
  */
 typedef s32 dma_cookie_t;
 #define DMA_MIN_COOKIE	1
-#define DMA_MAX_COOKIE	INT_MAX
 
 static inline int dma_submit_error(dma_cookie_t cookie)
 {

commit 93c6ee94c140eefb6f9d5b6e2ad1acc2e138e44c
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Thu Jul 3 07:51:52 2014 +0300

    dma: Support for 3 bytes word size
    
    Add DMA_SLAVE_BUSWIDTH_3_BYTES to dma_slave_buswidth for engines and users
    to select 3 bytes as bus width.
    For example eDMA can be configured to use 3bytes mode and in audio we have
    formats stored on 3bytes in memory (_XXX_3LE) where this new bus width can
    be used.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Acked-by: Vinod Koul <vinod.koul@intel.com>
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Mark Brown <broonie@linaro.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d2c5cc7c583c..3d1c2aa51530 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -299,6 +299,7 @@ enum dma_slave_buswidth {
 	DMA_SLAVE_BUSWIDTH_UNDEFINED = 0,
 	DMA_SLAVE_BUSWIDTH_1_BYTE = 1,
 	DMA_SLAVE_BUSWIDTH_2_BYTES = 2,
+	DMA_SLAVE_BUSWIDTH_3_BYTES = 3,
 	DMA_SLAVE_BUSWIDTH_4_BYTES = 4,
 	DMA_SLAVE_BUSWIDTH_8_BYTES = 8,
 };

commit 77c32bbbe0d0e963ba5723b8d1f6c42c5d56858b
Merge: fad0701eaa09 06822788faa2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 10 10:28:45 2014 -0700

    Merge branch 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine updates from Vinod Koul:
     - new Xilixn VDMA driver from Srikanth
     - bunch of updates for edma driver by Thomas, Joel and Peter
     - fixes and updates on dw, ste_dma, freescale, mpc512x, sudmac etc
    
    * 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma: (45 commits)
      dmaengine: sh: don't use dynamic static allocation
      dmaengine: sh: fix print specifier warnings
      dmaengine: sh: make shdma_prep_dma_cyclic static
      dmaengine: Kconfig: Update MXS_DMA help text to include MX6Q/MX6DL
      of: dma: Grammar s/requests/request/, s/used required/required/
      dmaengine: shdma: Enable driver compilation with COMPILE_TEST
      dmaengine: rcar-hpbdma: Include linux/err.h
      dmaengine: sudmac: Include linux/err.h
      dmaengine: sudmac: Keep #include sorted alphabetically
      dmaengine: shdmac: Include linux/err.h
      dmaengine: shdmac: Keep #include sorted alphabetically
      dmaengine: s3c24xx-dma: Add cyclic transfer support
      dmaengine: s3c24xx-dma: Process whole SG chain
      dmaengine: imx: correct sdmac->status for cyclic dma tx
      dmaengine: pch: fix compilation for alpha target
      dmaengine: dw: check return code of dma_async_device_register()
      dmaengine: dw: fix regression in dw_probe() function
      dmaengine: dw: enable clock before access
      dma: pch_dma: Fix Kconfig dependencies
      dmaengine: mpc512x: add support for peripheral transfers
      ...

commit ba730340f96c01160b5f26f81e8fb38f8cb1821c
Author: Alexander Popov <a13xp0p0v88@gmail.com>
Date:   Thu May 15 18:15:31 2014 +0400

    dmaengine: fix comment typo
    
    Fix comment typo.
    
    Signed-off-by: Alexander Popov <a13xp0p0v88@gmail.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8300fb87b84a..cbb168e04dc1 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -292,7 +292,7 @@ struct dma_chan_dev {
 };
 
 /**
- * enum dma_slave_buswidth - defines bus with of the DMA slave
+ * enum dma_slave_buswidth - defines bus width of the DMA slave
  * device, source or target buses
  */
 enum dma_slave_buswidth {

commit c1f43dd9c20d85e66c4d77e284f64ac114abe3f8
Author: Xuelin Shi <xuelin.shi@freescale.com>
Date:   Wed May 21 14:02:37 2014 -0700

    dmaengine: fix dmaengine_unmap failure
    
    The count which is used to get_unmap_data maybe not the same as the
    count computed in dmaengine_unmap which causes to free data in a
    wrong pool.
    
    This patch fixes this issue by keeping the map count with unmap_data
    structure and use this count to get the pool.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Xuelin Shi <xuelin.shi@freescale.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8300fb87b84a..72cb0ddb9678 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -429,6 +429,7 @@ typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
 typedef void (*dma_async_tx_callback)(void *dma_async_param);
 
 struct dmaengine_unmap_data {
+	u8 map_cnt;
 	u8 to_cnt;
 	u8 from_cnt;
 	u8 bidi_cnt;

commit 7cbccb55f04bef306bc2840185ec8f986bd0df3c
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Sun Feb 16 14:21:22 2014 +0100

    dma: Remove comment about embedding dma_slave_config into custom structs
    
    The documentation for the dma_slave_config struct recommends that if a DMA
    controller has special configuration options, which can not be configured
    through the dma_slave_config struct, the driver should create its own custom
    config struct and embed the dma_slave_config struct in it and pass the custom
    config struct to dmaengine_slave_config(). This overloads the generic
    dmaengine_slave_config() API with custom semantics and any caller of the
    dmaengine_slave_config() that is not aware of these special semantics will cause
    undefined behavior. This means that it is impossible for generic code to make
    use of dmaengine_slave_config(). Such a restriction contradicts the very idea of
    having a generic API.
    
    E.g. consider the following case of a DMA controller that has an option to
    reverse the field polarity of the DMA transfer with the following implementation
    for setting the configuration:
    
            struct my_slave_config {
                    struct dma_slave_config config;
                    unsigned int field_polarity;
            };
    
            static int my_dma_controller_slave_config(struct dma_chan *chan,
                    struct dma_slave_config *config)
            {
                    struct my_slave_config *my_cfg = container_of(config,
                                    struct my_slave_config, config);
    
                    ...
                    my_dma_set_field_polarity(chan, my_cfg->field_polarity);
                    ...
            }
    
    Now a generic user of the dmaengine API might want to configure a DMA channel
    for this DMA controller that it obtained using the following code:
    
            struct dma_slave_config config;
    
            config.src_addr = ...;
            ...
            dmaengine_slave_config(chan, &config);
    
    The call to dmaengine_slave_config() will eventually call into
    my_dma_controller_slave_config() which will cast from dma_slave_config to
    my_slave_config and then tries to access the field_polarity member. Since the
    dma_slave_config struct that was passed in was never embedded into a
    my_slave_config struct this attempt will just read random stack garbage and use
    that to configure the DMA controller. This is bad. Instead, if a DMA controller
    really needs to have custom configuration options, the driver should create a
    custom API for it. This makes it very clear that there is a direct dependency
    of a user of such an API and the implementer. E.g.:
    
            int my_dma_set_field_polarity(struct dma_chan *chan,
                    unsigned int field_polarity) {
                    if (chan->device->dev->driver != &my_dma_controller_driver.driver)
                            return -EINVAL;
                    ...
            }
            EXPORT_SYMBOL_GPL(my_dma_set_field_polarity);
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c5c92d59e531..8300fb87b84a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -341,15 +341,11 @@ enum dma_slave_buswidth {
  * and this struct will then be passed in as an argument to the
  * DMA engine device_control() function.
  *
- * The rationale for adding configuration information to this struct
- * is as follows: if it is likely that most DMA slave controllers in
- * the world will support the configuration option, then make it
- * generic. If not: if it is fixed so that it be sent in static from
- * the platform data, then prefer to do that. Else, if it is neither
- * fixed at runtime, nor generic enough (such as bus mastership on
- * some CPU family and whatnot) then create a custom slave config
- * struct and pass that, then make this config a member of that
- * struct, if applicable.
+ * The rationale for adding configuration information to this struct is as
+ * follows: if it is likely that more than one DMA slave controllers in
+ * the world will support the configuration option, then make it generic.
+ * If not: if it is fixed so that it be sent in static from the platform
+ * data, then prefer to do that.
  */
 struct dma_slave_config {
 	enum dma_transfer_direction direction;

commit ca2a650f3dfdc30d71d21bcbb04d2d057779f3f9
Merge: e9e352e9100b 15cec530e4bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 29 20:27:23 2014 -0800

    Merge branch 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dma updates from Vinod Koul:
     - new driver for BCM2835 used in R-pi
     - new driver for MOXA ART
     - dma_get_any_slave_channel API for DT based systems
     - minor fixes and updates spread acrooss driver
    
    [ The fsl-ssi dual fifo mode support addition clashed badly with the
      other changes to fsl-ssi that came in through the sound merge.  I did
      a very rough cut at fixing up the conflict, but Nicolin Chen (author
      of both sides) will need to verify and check things ]
    
    * 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma: (36 commits)
      dmaengine: mmp_pdma: fix mismerge
      dma: pl08x: Export pl08x_filter_id
      acpi-dma: align documentation with kernel-doc format
      dma: fix vchan_cookie_complete() debug print
      DMA: dmatest: extend the "device" module parameter to 32 characters
      drivers/dma: fix error return code
      dma: omap: Set debug level to debugging messages
      dmaengine: fix kernel-doc style typos for few comments
      dma: tegra: add support for Tegra148/124
      dma: dw: use %pad instead of casting dma_addr_t
      dma: dw: join split up messages
      dma: dw: fix style of multiline comment
      dmaengine: k3dma: fix sparse warnings
      dma: pl330: Use dma_get_slave_channel() in the of xlate callback
      dma: pl330: Differentiate between submitted and issued descriptors
      dmaengine: sirf: Add device_slave_caps interface
      DMA: Freescale: change BWC from 256 bytes to 1024 bytes
      dmaengine: Add MOXA ART DMA engine driver
      dmaengine: Add DMA_PRIVATE to BCM2835 driver
      dma: imx-sdma: Assign a default script number for ROM firmware cases
      ...

commit f2c73464d7b399cf4e0c601c1c7d7b079080fa52
Merge: 93abdb778550 273c2279ca50
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 23 18:36:55 2014 -0800

    Merge tag 'cleanup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC cleanups from Olof Johansson:
     "This is the branch where we usually queue up cleanup efforts, moving
      drivers out of the architecture directory, header file restructuring,
      etc.  Sometimes they tangle with new development so it's hard to keep
      it strictly to cleanups.
    
      Some of the things included in this branch are:
    
       * Atmel SAMA5 conversion to common clock
       * Reset framework conversion for tegra platforms
        - Some of this depends on tegra clock driver reworks that are shared
          with Mike Turquette's clk tree.
       * Tegra DMA refactoring, which are shared branches with the DMA tree.
       * Removal of some header files on exynos to prepare for
         multiplatform"
    
    * tag 'cleanup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (169 commits)
      ARM: mvebu: move Armada 370/XP specific definitions to armada-370-xp.h
      ARM: mvebu: remove prototypes of non-existing functions from common.h
      ARM: mvebu: move ARMADA_XP_MAX_CPUS to armada-370-xp.h
      serial: sh-sci: Rework baud rate calculation
      serial: sh-sci: Compute overrun_bit without using baud rate algo
      serial: sh-sci: Remove unused GPIO request code
      serial: sh-sci: Move overrun_bit and error_mask fields out of pdata
      serial: sh-sci: Support resources passed through platform resources
      serial: sh-sci: Don't check IRQ in verify port operation
      serial: sh-sci: Set the UPF_FIXED_PORT flag
      serial: sh-sci: Remove duplicate interrupt check in verify port op
      serial: sh-sci: Simplify baud rate calculation algorithms
      serial: sh-sci: Remove baud rate calculation algorithm 5
      serial: sh-sci: Sort headers alphabetically
      ARM: EXYNOS: Kill exynos_pm_late_initcall()
      ARM: EXYNOS: Consolidate selection of PM_GENERIC_DOMAINS for Exynos4
      ARM: at91: switch Calao QIL-A9260 board to DT
      clk: at91: fix pmc_clk_ids data type attriubte
      PM / devfreq: use inclusion <mach/map.h> instead of <plat/map-s5p.h>
      ARM: EXYNOS: remove <mach/regs-clock.h> for exynos
      ...

commit 868d2ee252918e7640df80156df9e1299f8118f5
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Dec 18 21:39:39 2013 +0530

    dmaengine: fix kernel-doc style typos for few comments
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index fa6b4285d8d2..188108c36313 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -257,7 +257,7 @@ struct dma_chan_percpu {
  * @dev: class device for sysfs
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
- * @client-count: how many clients are using this channel
+ * @client_count: how many clients are using this channel
  * @table_count: number of appearances in the mem-to-mem allocation table
  * @private: private data for certain client-channel associations
  */
@@ -279,10 +279,10 @@ struct dma_chan {
 
 /**
  * struct dma_chan_dev - relate sysfs device node to backing channel device
- * @chan - driver channel device
- * @device - sysfs device
- * @dev_id - parent dma_device dev_id
- * @idr_ref - reference count to gate release of dma_device dev_id
+ * @chan: driver channel device
+ * @device: sysfs device
+ * @dev_id: parent dma_device dev_id
+ * @idr_ref: reference count to gate release of dma_device dev_id
  */
 struct dma_chan_dev {
 	struct dma_chan *chan;

commit 507205632dd12636cfe4af4322dace263dca0c21
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Sat Jan 11 14:02:16 2014 +0100

    dma: Indicate residue granularity in dma_slave_caps
    
    This patch adds a new field to the dma_slave_caps struct which indicates the
    granularity with which the driver is able to update the residue field of the
    dma_tx_state struct. Making this information available to dmaengine users allows
    them to make better decisions on how to operate. E.g. for audio certain features
    like wakeup less operation or timer based scheduling only make sense and work
    correctly if the reported residue is fine-grained enough.
    
    Right now four different levels of granularity are supported:
            * DESCRIPTOR: The DMA channel is only able to tell whether a descriptor has
              been completed or not, which means residue reporting is not supported by
              this channel. The residue field of the dma_tx_state field will always be
              0.
            * SEGMENT: The DMA channel updates the residue field after each successfully
              completed segment of the transfer (For cyclic transfers this is after each
              period). This is typically implemented by having the hardware generate an
              interrupt after each transferred segment and then the drivers updates the
              outstanding residue by the size of the segment. Another possibility is if
              the hardware supports SG and the segment descriptor has a field which gets
              set after the segment has been completed. The driver then counts the
              number of segments without the flag set to compute the residue.
            * BURST: The DMA channel updates the residue field after each transferred
              burst. This is typically only supported if the hardware has a progress
              register of some sort (E.g. a register with the current read/write address
              or a register with the amount of bursts/beats/bytes that have been
              transferred or still need to be transferred).
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Acked-by: Vinod Koul <vinod.koul@intel.com>
    Signed-off-by: Mark Brown <broonie@linaro.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ed92b30a02fd..ba5f96db0754 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -364,6 +364,32 @@ struct dma_slave_config {
 	unsigned int slave_id;
 };
 
+/**
+ * enum dma_residue_granularity - Granularity of the reported transfer residue
+ * @DMA_RESIDUE_GRANULARITY_DESCRIPTOR: Residue reporting is not support. The
+ *  DMA channel is only able to tell whether a descriptor has been completed or
+ *  not, which means residue reporting is not supported by this channel. The
+ *  residue field of the dma_tx_state field will always be 0.
+ * @DMA_RESIDUE_GRANULARITY_SEGMENT: Residue is updated after each successfully
+ *  completed segment of the transfer (For cyclic transfers this is after each
+ *  period). This is typically implemented by having the hardware generate an
+ *  interrupt after each transferred segment and then the drivers updates the
+ *  outstanding residue by the size of the segment. Another possibility is if
+ *  the hardware supports scatter-gather and the segment descriptor has a field
+ *  which gets set after the segment has been completed. The driver then counts
+ *  the number of segments without the flag set to compute the residue.
+ * @DMA_RESIDUE_GRANULARITY_BURST: Residue is updated after each transferred
+ *  burst. This is typically only supported if the hardware has a progress
+ *  register of some sort (E.g. a register with the current read/write address
+ *  or a register with the amount of bursts/beats/bytes that have been
+ *  transferred or still need to be transferred).
+ */
+enum dma_residue_granularity {
+	DMA_RESIDUE_GRANULARITY_DESCRIPTOR = 0,
+	DMA_RESIDUE_GRANULARITY_SEGMENT = 1,
+	DMA_RESIDUE_GRANULARITY_BURST = 2,
+};
+
 /* struct dma_slave_caps - expose capabilities of a slave channel only
  *
  * @src_addr_widths: bit mask of src addr widths the channel supports
@@ -374,6 +400,7 @@ struct dma_slave_config {
  * 	should be checked by controller as well
  * @cmd_pause: true, if pause and thereby resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
+ * @residue_granularity: granularity of the reported transfer residue
  */
 struct dma_slave_caps {
 	u32 src_addr_widths;
@@ -381,6 +408,7 @@ struct dma_slave_caps {
 	u32 directions;
 	bool cmd_pause;
 	bool cmd_terminate;
+	enum dma_residue_granularity residue_granularity;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)

commit 0adcdeed6f87ac7230c9a0364ac785b8e70ad275
Merge: 4fce628f6859 8010dad55a0a
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Tue Jan 7 21:36:24 2014 +0530

    Merge branch 'topic/of' into for-linus
    
    Conflicts:
            drivers/dma/mmp_pdma.c
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

commit 4fce628f685945df4fd6fa1cab6f7eb397dc9267
Merge: 1080411c6bf9 0ad7c00057dc
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Tue Jan 7 21:32:28 2014 +0530

    Merge branch 'topic/defer_probe' into for-linus

commit 1c7af42fe579b5cf8c942319cbed38801305dda4
Merge: 509633c8366a e9036c2a60f3 8010dad55a0a 62ce7cd62f53
Author: Olof Johansson <olof@lixom.net>
Date:   Thu Dec 26 10:32:35 2013 -0800

    Merge branches 'depends/asoc-dma', 'depends/dma-of' and 'depends/tegra-clk' into next/cleanup
    
    Merging in external dependencies for the Tegra DMA and reset controller
    refactoring from external trees.
    
    Per Stephen Warren, the stability of these branches have been negotiated
    with the relevant parties (Vinod/Mark/Mike)
    
    * depends/asoc-dma:
      ASoC: dmaengine: fix deferred probe detection
      ASoC: dmaengine: support deferred probe for DMA channels
      dma: add channel request API that supports deferred probe
      ASoC: dmaengine: add custom DMA config to snd_dmaengine_pcm_config
      ASoC: don't leak on error in snd_dmaengine_pcm_register
      ASoC: restructure dmaengine_pcm_request_chan_of()
      ASoC: generic-dmaengine-pcm: Set BATCH flag when residue reporting is not supported
      ASoC: Add resource managed snd_dmaengine_pcm_register()
    
    * depends/dma-of:
      dma: add dma_get_any_slave_channel(), for use in of_xlate()
    
    * depends/tegra-clk: (42 commits)
      clk: tegra: fix __clk_lookup() return value checks
      clk: tegra: Do not print errors for clk_round_rate()
      clk: tegra: Initialize DSI low-power clocks
      clk: tegra: add FUSE clock device
      clk: tegra: Properly setup PWM clock on Tegra30
      clk: tegra: Initialize secondary gr3d clock on Tegra30
      clk: tegra114: Initialize clocks needed for HDMI
      clk: tegra124: add suspend/resume function for tegra_cpu_car_ops
      clk: tegra124: add wait_for_reset and disable_clock for tegra_cpu_car_ops
      clk: tegra124: Add support for Tegra124 clocks
      clk: tegra124: Add new peripheral clocks
      clk: tegra124: Add common clk IDs to clk-id.h
      clk: tegra: add TEGRA_PERIPH_NO_GATE
      clk: tegra: add locking to periph clks
      clk: tegra: Add periph regs bank X
      clk: tegra: Add support for PLLSS
      clk: tegra: move tegra20 to common infra
      clk: tegra: move tegra30 to common infra
      clk: tegra: introduce common gen4 super clock
      clk: tegra: move PMC, fixed clocks to common files
      ...
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 397321f45d31159c84982978106f3165be44bc2b
Author: Alexander Popov <a13xp0p0v88@gmail.com>
Date:   Mon Dec 16 12:12:17 2013 +0400

    dmaengine: fix incorrect kernel-doc comment for struct dma_slave_config
    
    The 'direction' member of 'struct dma_slave_config' is of data
    type 'enum dma_transfer_direction', so update the kernel-doc comment
    for 'struct dma_slave_config' to refer to appropriate values.
    
    Signed-off-by: Alexander Popov <a13xp0p0v88@gmail.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 41cf0c399288..bd6b88222ced 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -305,9 +305,8 @@ enum dma_slave_buswidth {
 /**
  * struct dma_slave_config - dma slave channel runtime config
  * @direction: whether the data shall go in or out on this slave
- * channel, right now. DMA_TO_DEVICE and DMA_FROM_DEVICE are
- * legal values, DMA_BIDIRECTIONAL is not acceptable since we
- * need to differentiate source and target addresses.
+ * channel, right now. DMA_MEM_TO_DEV and DMA_DEV_TO_MEM are
+ * legal values.
  * @src_addr: this is the physical address where DMA slave data
  * should be read (RX), if the source is memory this argument is
  * ignored.

commit 8010dad55a0ab0e829f3733854e5235eef4e2734
Author: Stephen Warren <swarren@nvidia.com>
Date:   Tue Nov 26 12:40:51 2013 -0700

    dma: add dma_get_any_slave_channel(), for use in of_xlate()
    
    mmp_pdma.c implements a custom of_xlate() function that is 95% identical
    to what Tegra will need. Create a function to implement the common part,
    so everyone doesn't just cut/paste the implementation.
    
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Lars-Peter Clausen <lars@metafoo.de>
    Cc: dmaengine@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 41cf0c399288..09ef23ee8bce 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1079,6 +1079,7 @@ int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
+struct dma_chan *dma_get_any_slave_channel(struct dma_device *device);
 struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 #define dma_request_slave_channel_compat(mask, x, y, dev, name) \

commit 0ad7c00057dc1640647c1dc81ccbd009de17a767
Author: Stephen Warren <swarren@nvidia.com>
Date:   Tue Nov 26 10:04:22 2013 -0700

    dma: add channel request API that supports deferred probe
    
    dma_request_slave_channel() simply returns NULL whenever DMA channel
    lookup fails. Lookup could fail for two distinct reasons:
    
    a) No DMA specification exists for the channel name.
       This includes situations where no DMA specifications exist at all, or
       other general lookup problems.
    
    b) A DMA specification does exist, yet the driver for that channel is not
       yet registered.
    
    Case (b) should trigger deferred probe in client drivers. However, since
    they have no way to differentiate the two situations, it cannot.
    
    Implement new function dma_request_slave_channel_reason(), which performs
    identically to dma_request_slave_channel(), except that it returns an
    error-pointer rather than NULL, which allows callers to detect when
    deferred probe should occur.
    
    Eventually, all drivers should be converted to this new API, the old API
    removed, and the new API renamed to the more desirable name. This patch
    doesn't convert the existing API and all drivers in one go, since some
    drivers call dma_request_slave_channel() then dma_request_channel() if
    that fails. That would require either modifying dma_request_channel() in
    the same way, or adding extra error-handling code to all affected
    drivers, and there are close to 100 drivers using the other API, rather
    than just the 15-20 or so that use dma_request_slave_channel(), which
    might be tenable in a single patch.
    
    acpi_dma_request_slave_chan_by_name() doesn't currently implement
    deferred probe. It should, but this will be addressed later.
    
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 41cf0c399288..ed92b30a02fd 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -22,6 +22,7 @@
 #define LINUX_DMAENGINE_H
 
 #include <linux/device.h>
+#include <linux/err.h>
 #include <linux/uio.h>
 #include <linux/bug.h>
 #include <linux/scatterlist.h>
@@ -1040,6 +1041,8 @@ enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 					dma_filter_fn fn, void *fn_param);
+struct dma_chan *dma_request_slave_channel_reason(struct device *dev,
+						  const char *name);
 struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
 void dma_release_channel(struct dma_chan *chan);
 #else
@@ -1063,6 +1066,11 @@ static inline struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 {
 	return NULL;
 }
+static inline struct dma_chan *dma_request_slave_channel_reason(
+					struct device *dev, const char *name)
+{
+	return ERR_PTR(-ENODEV);
+}
 static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
 							 const char *name)
 {

commit df12a3178d340319b1955be6b973a4eb84aff754
Merge: 2f986ec6fa57 82a1402eaee5
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Sat Nov 16 11:54:17 2013 +0530

    Merge commit 'dmaengine-3.13-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine changes from Dan
    
    1/ Bartlomiej and Dan finalized a rework of the dma address unmap
       implementation.
    
    2/ In the course of testing 1/ a collection of enhancements to dmatest
       fell out.  Notably basic performance statistics, and fixed / enhanced
       test control through new module parameters 'run', 'wait', 'noverify',
       and 'verbose'.  Thanks to Andriy and Linus for their review.
    
    3/ Testing the raid related corner cases of 1/ triggered bugs in the
       recently added 16-source operation support in the ioatdma driver.
    
    4/ Some minor fixes / cleanups to mv_xor and ioatdma.
    
    Conflicts:
            drivers/dma/dmatest.c
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

commit 0776ae7b89782124ddd72eafe0b1e0fdcdabe32e
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Fri Oct 18 19:35:33 2013 +0200

    dmaengine: remove DMA unmap flags
    
    Remove no longer needed DMA unmap flags:
    - DMA_COMPL_SKIP_SRC_UNMAP
    - DMA_COMPL_SKIP_DEST_UNMAP
    - DMA_COMPL_SRC_UNMAP_SINGLE
    - DMA_COMPL_DEST_UNMAP_SINGLE
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Jon Mason <jon.mason@intel.com>
    Acked-by: Mark Brown <broonie@linaro.org>
    [djbw: clean up straggling skip unmap flags in ntb]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3782cdb782a8..491072cb5ba0 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -171,12 +171,6 @@ struct dma_interleaved_template {
  * @DMA_CTRL_ACK - if clear, the descriptor cannot be reused until the client
  *  acknowledges receipt, i.e. has has a chance to establish any dependency
  *  chains
- * @DMA_COMPL_SKIP_SRC_UNMAP - set to disable dma-unmapping the source buffer(s)
- * @DMA_COMPL_SKIP_DEST_UNMAP - set to disable dma-unmapping the destination(s)
- * @DMA_COMPL_SRC_UNMAP_SINGLE - set to do the source dma-unmapping as single
- * 	(if not set, do the source dma-unmapping as page)
- * @DMA_COMPL_DEST_UNMAP_SINGLE - set to do the destination dma-unmapping as single
- * 	(if not set, do the destination dma-unmapping as page)
  * @DMA_PREP_PQ_DISABLE_P - prevent generation of P while generating Q
  * @DMA_PREP_PQ_DISABLE_Q - prevent generation of Q while generating P
  * @DMA_PREP_CONTINUE - indicate to a driver that it is reusing buffers as
@@ -188,14 +182,10 @@ struct dma_interleaved_template {
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
 	DMA_CTRL_ACK = (1 << 1),
-	DMA_COMPL_SKIP_SRC_UNMAP = (1 << 2),
-	DMA_COMPL_SKIP_DEST_UNMAP = (1 << 3),
-	DMA_COMPL_SRC_UNMAP_SINGLE = (1 << 4),
-	DMA_COMPL_DEST_UNMAP_SINGLE = (1 << 5),
-	DMA_PREP_PQ_DISABLE_P = (1 << 6),
-	DMA_PREP_PQ_DISABLE_Q = (1 << 7),
-	DMA_PREP_CONTINUE = (1 << 8),
-	DMA_PREP_FENCE = (1 << 9),
+	DMA_PREP_PQ_DISABLE_P = (1 << 2),
+	DMA_PREP_PQ_DISABLE_Q = (1 << 3),
+	DMA_PREP_CONTINUE = (1 << 4),
+	DMA_PREP_FENCE = (1 << 5),
 };
 
 /**

commit 8971646294bda65f8666b60cb2cb3d5e172c99bf
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Oct 18 19:35:25 2013 +0200

    async_memcpy: convert to dmaengine_unmap_data
    
    Use the generic unmap object to unmap dma buffers.
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Reported-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    [bzolnier: add missing unmap->len initialization]
    [bzolnier: fix whitespace damage]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    [djbw: add DMA_ENGINE=n support]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2fe855a7cab1..3782cdb782a8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -457,6 +457,7 @@ struct dma_async_tx_descriptor {
 #endif
 };
 
+#ifdef CONFIG_DMA_ENGINE
 static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
 				 struct dmaengine_unmap_data *unmap)
 {
@@ -464,7 +465,23 @@ static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
 	tx->unmap = unmap;
 }
 
+struct dmaengine_unmap_data *
+dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags);
 void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap);
+#else
+static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
+				 struct dmaengine_unmap_data *unmap)
+{
+}
+static inline struct dmaengine_unmap_data *
+dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)
+{
+	return NULL;
+}
+static inline void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)
+{
+}
+#endif
 
 static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
 {

commit 45c463ae924c62af4aa64ded1ca831f334a1db65
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Oct 18 19:35:24 2013 +0200

    dmaengine: reference counted unmap data
    
    Hang a common 'unmap' object off of dma descriptors for the purpose of
    providing a unified unmapping interface.  The lifetime of a mapping may
    span multiple descriptors, so these unmap objects are reference counted
    by related descriptor.
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    [bzolnier: fix IS_ENABLED() check]
    [bzolnier: fix release ordering in dmaengine_destroy_unmap_pool()]
    [bzolnier: fix check for success in dmaengine_init_unmap_pool()]
    [bzolnier: use mempool_free() instead of kmem_cache_free()]
    [bzolnier: add missing unmap->len initializations]
    [bzolnier: add __init tag to dmaengine_init_unmap_pool()]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    [djbw: move DMAENGINE=n support to this patch for async_tx]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9070050fbcd8..2fe855a7cab1 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -464,9 +464,12 @@ static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
 	tx->unmap = unmap;
 }
 
+void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap);
+
 static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
 {
 	if (tx->unmap) {
+		dmaengine_unmap_put(tx->unmap);
 		tx->unmap = NULL;
 	}
 }

commit d38a8c622a1b382336c3e152c6caf4e11d1f1b2a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Oct 18 19:35:23 2013 +0200

    dmaengine: prepare for generic 'unmap' data
    
    Add a hook for a common dma unmap implementation to enable removal of
    the per driver custom unmap code.  (A reworked version of Bartlomiej
    Zolnierkiewicz's patches to remove the custom callbacks and the size
    increase of dma_async_tx_descriptor for drivers that don't care about
    raid).
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    [bzolnier: prepare pl330 driver for adding missing unmap while at it]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 0bc727534108..9070050fbcd8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -413,6 +413,17 @@ void dma_chan_cleanup(struct kref *kref);
 typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
 
 typedef void (*dma_async_tx_callback)(void *dma_async_param);
+
+struct dmaengine_unmap_data {
+	u8 to_cnt;
+	u8 from_cnt;
+	u8 bidi_cnt;
+	struct device *dev;
+	struct kref kref;
+	size_t len;
+	dma_addr_t addr[0];
+};
+
 /**
  * struct dma_async_tx_descriptor - async transaction descriptor
  * ---dma generic offload fields---
@@ -438,6 +449,7 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
 	void *callback_param;
+	struct dmaengine_unmap_data *unmap;
 #ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 	struct dma_async_tx_descriptor *next;
 	struct dma_async_tx_descriptor *parent;
@@ -445,6 +457,20 @@ struct dma_async_tx_descriptor {
 #endif
 };
 
+static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
+				 struct dmaengine_unmap_data *unmap)
+{
+	kref_get(&unmap->kref);
+	tx->unmap = unmap;
+}
+
+static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
+{
+	if (tx->unmap) {
+		tx->unmap = NULL;
+	}
+}
+
 #ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 static inline void txd_lock(struct dma_async_tx_descriptor *txd)
 {

commit 7db5f7274a0b065abdc358be2a44b4a911d75707
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Thu Oct 17 07:29:57 2013 +0530

    dmaengine: remove unused DMA_SUCCESS
    
    after all the users are converted
    
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 120e64c96478..4b460a683968 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -51,7 +51,7 @@ static inline int dma_submit_error(dma_cookie_t cookie)
  * @DMA_ERROR: transaction failed
  */
 enum dma_status {
-	DMA_SUCCESS = 0, DMA_COMPLETE = 0,
+	DMA_COMPLETE,
 	DMA_IN_PROGRESS,
 	DMA_PAUSED,
 	DMA_ERROR,

commit adfedd9a32e4e3490c0060576fd824881572b72a
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Oct 16 13:29:02 2013 +0530

    dmaengine: use DMA_COMPLETE for dma completion status
    
    the DMA_SUCCESS is a misnomer as dmaengine indicates the transfer is complete and
    gives no guarantee of the transfer success. Hence we should use DMA_COMPLTE
    instead of DMA_SUCCESS
    
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 0bc727534108..120e64c96478 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -45,13 +45,13 @@ static inline int dma_submit_error(dma_cookie_t cookie)
 
 /**
  * enum dma_status - DMA transaction status
- * @DMA_SUCCESS: transaction completed successfully
+ * @DMA_COMPLETE: transaction completed
  * @DMA_IN_PROGRESS: transaction not yet processed
  * @DMA_PAUSED: transaction is paused
  * @DMA_ERROR: transaction failed
  */
 enum dma_status {
-	DMA_SUCCESS,
+	DMA_SUCCESS = 0, DMA_COMPLETE = 0,
 	DMA_IN_PROGRESS,
 	DMA_PAUSED,
 	DMA_ERROR,
@@ -979,10 +979,10 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 {
 	if (last_complete <= last_used) {
 		if ((cookie <= last_complete) || (cookie > last_used))
-			return DMA_SUCCESS;
+			return DMA_COMPLETE;
 	} else {
 		if ((cookie <= last_complete) && (cookie > last_used))
-			return DMA_SUCCESS;
+			return DMA_COMPLETE;
 	}
 	return DMA_IN_PROGRESS;
 }
@@ -1013,11 +1013,11 @@ static inline struct dma_chan *dma_find_channel(enum dma_transaction_type tx_typ
 }
 static inline enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie)
 {
-	return DMA_SUCCESS;
+	return DMA_COMPLETE;
 }
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
 {
-	return DMA_SUCCESS;
+	return DMA_COMPLETE;
 }
 static inline void dma_issue_pending_all(void)
 {

commit ec5b103ecfde929004b691f29183255aeeadecd5
Merge: d0048f0b91ee 5622ff1a4dd7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 10 13:37:36 2013 -0700

    Merge branch 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine updates from Vinod Koul:
     "This pull brings:
       - Andy's DW driver updates
       - Guennadi's sh driver updates
       - Pl08x driver fixes from Tomasz & Alban
       - Improvements to mmp_pdma by Daniel
       - TI EDMA fixes by Joel
       - New drivers:
         - Hisilicon k3dma driver
         - Renesas rcar dma driver
      - New API for publishing slave driver capablities
      - Various fixes across the subsystem by Andy, Jingoo, Sachin etc..."
    
    * 'for-linus' of git://git.infradead.org/users/vkoul/slave-dma: (94 commits)
      dma: edma: Remove limits on number of slots
      dma: edma: Leave linked to Null slot instead of DUMMY slot
      dma: edma: Find missed events and issue them
      ARM: edma: Add function to manually trigger an EDMA channel
      dma: edma: Write out and handle MAX_NR_SG at a given time
      dma: edma: Setup parameters to DMA MAX_NR_SG at a time
      dmaengine: pl330: use dma_set_max_seg_size to set the sg limit
      dmaengine: dma_slave_caps: remove sg entries
      dma: replace devm_request_and_ioremap by devm_ioremap_resource
      dma: ste_dma40: Fix potential null pointer dereference
      dma: ste_dma40: Remove duplicate const
      dma: imx-dma: Remove redundant NULL check
      dma: dmagengine: fix function names in comments
      dma: add driver for R-Car HPB-DMAC
      dma: k3dma: use devm_ioremap_resource() instead of devm_request_and_ioremap()
      dma: imx-sdma: Staticize sdma_driver_data structures
      pch_dma: Add MODULE_DEVICE_TABLE
      dmaengine: PL08x: Add cyclic transfer support
      dmaengine: PL08x: Fix reading the byte count in cctl
      dmaengine: PL08x: Add support for different maximum transfer size
      ...

commit 4a43f394a08214eaf92cdd8ce3eae75e555323d8
Author: Jon Mason <jon.mason@intel.com>
Date:   Mon Sep 9 16:51:59 2013 -0700

    dmaengine: dma_sync_wait and dma_find_channel undefined
    
    dma_sync_wait and dma_find_channel are declared regardless of whether
    CONFIG_DMA_ENGINE is enabled, but calling the function without
    CONFIG_DMA_ENGINE enabled results "undefined reference" errors.
    
    To get around this, declare dma_sync_wait and dma_find_channel as inline
    functions if CONFIG_DMA_ENGINE is undefined.
    
    Signed-off-by: Jon Mason <jon.mason@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b3ba7e410943..0c72b89a172c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -961,8 +961,9 @@ dma_set_tx_state(struct dma_tx_state *st, dma_cookie_t last, dma_cookie_t used,
 	}
 }
 
-enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 #ifdef CONFIG_DMA_ENGINE
+struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
+enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
@@ -970,6 +971,14 @@ struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
 void dma_release_channel(struct dma_chan *chan);
 #else
+static inline struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type)
+{
+	return NULL;
+}
+static inline enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie)
+{
+	return DMA_SUCCESS;
+}
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
 {
 	return DMA_SUCCESS;
@@ -997,7 +1006,6 @@ static inline void dma_release_channel(struct dma_chan *chan)
 int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
-struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
 struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 #define dma_request_slave_channel_compat(mask, x, y, dev, name) \

commit bd127639f43ed00f721b403c7c252caa19d0f613
Merge: 265d9c673d47 dbaf6d85114b
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Wed Sep 4 18:36:53 2013 +0530

    Merge branch 'topic/api_caps' into for-linus

commit 14f00c74f787a263e443b6901083187ffae641de
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Sep 2 17:47:33 2013 +0530

    dmaengine: dma_slave_caps: remove sg entries
    
    As pointed by Russell in [1], the sg properties are already availble in struct device,
    so no need to duplicate here.
    
    [1]: http://marc.info/?l=linux-omap&m=137416733628831
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 5692bc3afd39..4310b8972867 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -380,11 +380,6 @@ struct dma_slave_config {
  * 	should be checked by controller as well
  * @cmd_pause: true, if pause and thereby resume is supported
  * @cmd_terminate: true, if terminate cmd is supported
- *
- * @max_sg_nr: maximum number of SG segments supported
- * 	0 for no maximum
- * @max_sg_len: maximum length of a SG segment supported
- * 	0 for no maximum
  */
 struct dma_slave_caps {
 	u32 src_addr_widths;
@@ -392,9 +387,6 @@ struct dma_slave_caps {
 	u32 directions;
 	bool cmd_pause;
 	bool cmd_terminate;
-
-	u32 max_sg_nr;
-	u32 max_sg_len;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)

commit 355cdafe14d72c616dc804a756f3af4f4df4fe8c
Merge: 4770ee44359a ca38ff133eb8
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Sep 2 17:40:40 2013 +0530

    Merge branch 'topic/api_caps' into for-linus

commit 71ea148370f8b6c745a8a42f6fd983cf5ebade18
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Sat Aug 10 10:46:50 2013 +0300

    dmaengine: make dma_submit_error() return an error code
    
    The problem here is that the dma_xfer() functions in
    drivers/ata/pata_arasan_cf.c and drivers/mtd/nand/fsmc_nand.c expect
    dma_submit_error() to return an error code so they return 1 when they
    intended to return a negative.
    
    So far as I can tell, none of the ->tx_submit() functions ever do
    return error codes so this patch should have no effect in the current
    code.
    
    I also changed it from a define to an inline.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Dan Williams <djbw@fb.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cb286b1acdb6..b3ba7e410943 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -38,7 +38,10 @@ typedef s32 dma_cookie_t;
 #define DMA_MIN_COOKIE	1
 #define DMA_MAX_COOKIE	INT_MAX
 
-#define dma_submit_error(cookie) ((cookie) < 0 ? 1 : 0)
+static inline int dma_submit_error(dma_cookie_t cookie)
+{
+	return cookie < 0 ? cookie : 0;
+}
 
 /**
  * enum dma_status - DMA transaction status

commit 7bb587f4eef8f71ce589f360ab99bb54ab0fc85d
Author: Zhangfei Gao <zhangfei.gao@linaro.org>
Date:   Fri Jun 28 20:39:12 2013 +0800

    dmaengine: add interface of dma_get_slave_channel
    
    Suggested by Arnd, add dma_get_slave_channel interface
    Dma host driver could get specific channel specificied by request line, rather than filter.
    
    host example:
    static struct dma_chan *xx_of_dma_simple_xlate(struct of_phandle_args *dma_spec,
                    struct of_dma *ofdma)
    {
            struct xx_dma_dev *d = ofdma->of_dma_data;
            unsigned int request = dma_spec->args[0];
    
            if (request > d->dma_requests)
                    return NULL;
    
            return dma_get_slave_channel(&(d->chans[request].vc.chan));
    }
    
    probe:
    of_dma_controller_register((&op->dev)->of_node, xx_of_dma_simple_xlate, d);
    
    Signed-off-by: Zhangfei Gao <zhangfei.gao@linaro.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cb286b1acdb6..c271608e862e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -995,6 +995,7 @@ int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
+struct dma_chan *dma_get_slave_channel(struct dma_chan *chan);
 struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 #define dma_request_slave_channel_compat(mask, x, y, dev, name) \

commit 221a27c76033a3a4196b3da09848bc5f237f3f94
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Jul 8 14:15:25 2013 +0530

    dmaengine: add dma_slave_get_caps api
    
    add new device callback .device_slave_caps api which can be used by clients to
    query the dma channel capablties before they program the channel. This can help
    is removing errors during the channel programming. Also add helper
    dma_slave_get_caps API
    
    This patch folds the work done by Matt earlier
    https://patchwork.kernel.org/patch/2094891/
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cb286b1acdb6..5692bc3afd39 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -370,6 +370,33 @@ struct dma_slave_config {
 	unsigned int slave_id;
 };
 
+/* struct dma_slave_caps - expose capabilities of a slave channel only
+ *
+ * @src_addr_widths: bit mask of src addr widths the channel supports
+ * @dstn_addr_widths: bit mask of dstn addr widths the channel supports
+ * @directions: bit mask of slave direction the channel supported
+ * 	since the enum dma_transfer_direction is not defined as bits for each
+ * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
+ * 	should be checked by controller as well
+ * @cmd_pause: true, if pause and thereby resume is supported
+ * @cmd_terminate: true, if terminate cmd is supported
+ *
+ * @max_sg_nr: maximum number of SG segments supported
+ * 	0 for no maximum
+ * @max_sg_len: maximum length of a SG segment supported
+ * 	0 for no maximum
+ */
+struct dma_slave_caps {
+	u32 src_addr_widths;
+	u32 dstn_addr_widths;
+	u32 directions;
+	bool cmd_pause;
+	bool cmd_terminate;
+
+	u32 max_sg_nr;
+	u32 max_sg_len;
+};
+
 static inline const char *dma_chan_name(struct dma_chan *chan)
 {
 	return dev_name(&chan->dev->device);
@@ -532,6 +559,7 @@ struct dma_tx_state {
  *	struct with auxiliary transfer status information, otherwise the call
  *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
+ * @device_slave_caps: return the slave channel capabilities
  */
 struct dma_device {
 
@@ -597,6 +625,7 @@ struct dma_device {
 					    dma_cookie_t cookie,
 					    struct dma_tx_state *txstate);
 	void (*device_issue_pending)(struct dma_chan *chan);
+	int (*device_slave_caps)(struct dma_chan *chan, struct dma_slave_caps *caps);
 };
 
 static inline int dmaengine_device_control(struct dma_chan *chan,
@@ -670,6 +699,21 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
 	return chan->device->device_prep_interleaved_dma(chan, xt, flags);
 }
 
+static inline int dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps)
+{
+	if (!chan || !caps)
+		return -EINVAL;
+
+	/* check if the channel supports slave transactions */
+	if (!test_bit(DMA_SLAVE, chan->device->cap_mask.bits))
+		return -ENXIO;
+
+	if (chan->device->device_slave_caps)
+		return chan->device->device_slave_caps(chan, caps);
+
+	return -ENXIO;
+}
+
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);

commit 48a9db462d99494583dad829969616ac90a8df4e
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Wed Jul 3 15:05:06 2013 -0700

    drivers/dma: remove unused support for MEMSET operations
    
    There have never been any real users of MEMSET operations since they
    have been introduced in January 2007 by commit 7405f74badf4 ("dmaengine:
    refactor dmaengine around dma_async_tx_descriptor").  Therefore remove
    support for them for now, it can be always brought back when needed.
    
    [sebastian.hesselbarth@gmail.com: fix drivers/dma/mv_xor]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Sebastian Hesselbarth <sebastian.hesselbarth@gmail.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 96d3e4ab11a9..cb286b1acdb6 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -66,7 +66,6 @@ enum dma_transaction_type {
 	DMA_PQ,
 	DMA_XOR_VAL,
 	DMA_PQ_VAL,
-	DMA_MEMSET,
 	DMA_INTERRUPT,
 	DMA_SG,
 	DMA_PRIVATE,
@@ -520,7 +519,6 @@ struct dma_tx_state {
  * @device_prep_dma_xor_val: prepares a xor validation operation
  * @device_prep_dma_pq: prepares a pq operation
  * @device_prep_dma_pq_val: prepares a pqzero_sum operation
- * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
  * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
@@ -573,9 +571,6 @@ struct dma_device {
 		struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
 		unsigned int src_cnt, const unsigned char *scf, size_t len,
 		enum sum_check_flags *pqres, unsigned long flags);
-	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
-		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
-		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(

commit b2396f7984ea09e83d489cfca6d5da62cc22945a
Merge: 42361f20f290 de61608acf89
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Thu May 2 21:52:26 2013 +0530

    Merge branch 'topic/of' into for-linus
    
    Conflicts:
            include/linux/dmaengine.h
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

commit bef29ec508e58bf8b9ec0915de5b0739fb800c91
Author: Markus Pargmann <mpa@pengutronix.de>
Date:   Sun Feb 24 16:36:09 2013 +0100

    DMA: of: Constant names
    
    No DMA of-function alters the name, so this patch changes the name arguments
    to be constant. Most drivers will probably request DMA channels using a
    constant name.
    
    Signed-off-by: Markus Pargmann <mpa@pengutronix.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 91ac8da25020..274071ca6f04 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -968,7 +968,7 @@ enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
-struct dma_chan *dma_request_slave_channel(struct device *dev, char *name);
+struct dma_chan *dma_request_slave_channel(struct device *dev, const char *name);
 void dma_release_channel(struct dma_chan *chan);
 #else
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
@@ -984,7 +984,7 @@ static inline struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask,
 	return NULL;
 }
 static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
-							 char *name)
+							 const char *name)
 {
 	return NULL;
 }

commit a53e28da574a40bcc9f78f5d0b0b60570182595b
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Mon Mar 25 13:23:52 2013 +0100

    dma: Make the 'mask' parameter of __dma_request_channel const
    
    The 'mask' parameter is not modified in __dma_request_channel and really
    shouldn't be. Make this explicit by making the parameter const.
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 91ac8da25020..dd6d21b335c8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -967,7 +967,8 @@ enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 #ifdef CONFIG_DMA_ENGINE
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
-struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
+struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
+					dma_filter_fn fn, void *fn_param);
 struct dma_chan *dma_request_slave_channel(struct device *dev, char *name);
 void dma_release_channel(struct dma_chan *chan);
 #else
@@ -978,7 +979,7 @@ static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descript
 static inline void dma_issue_pending_all(void)
 {
 }
-static inline struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask,
+static inline struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,
 					      dma_filter_fn fn, void *fn_param)
 {
 	return NULL;
@@ -1005,9 +1006,9 @@ struct dma_chan *net_dma_find_channel(void);
 	__dma_request_slave_channel_compat(&(mask), x, y, dev, name)
 
 static inline struct dma_chan
-*__dma_request_slave_channel_compat(dma_cap_mask_t *mask, dma_filter_fn fn,
-				  void *fn_param, struct device *dev,
-				  char *name)
+*__dma_request_slave_channel_compat(const dma_cap_mask_t *mask,
+				  dma_filter_fn fn, void *fn_param,
+				  struct device *dev, char *name)
 {
 	struct dma_chan *chan;
 

commit 864ef69b2d9b34e7c85baa9c5c601d5e735b208a
Author: Matt Porter <mporter@ti.com>
Date:   Fri Feb 1 18:22:52 2013 +0000

    dmaengine: add dma_request_slave_channel_compat()
    
    Adds a dma_request_slave_channel_compat() wrapper which accepts
    both the arguments from dma_request_channel() and
    dma_request_slave_channel(). Based on whether the driver is
    instantiated via DT, the appropriate channel request call will be
    made.
    
    This allows for a much cleaner migration of drivers to the
    dmaengine DT API as platforms continue to be mixed between those
    that boot using DT and those that do not.
    
    Suggested-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Matt Porter <mporter@ti.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index f5939999cb65..91ac8da25020 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -1001,6 +1001,22 @@ void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
 struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
+#define dma_request_slave_channel_compat(mask, x, y, dev, name) \
+	__dma_request_slave_channel_compat(&(mask), x, y, dev, name)
+
+static inline struct dma_chan
+*__dma_request_slave_channel_compat(dma_cap_mask_t *mask, dma_filter_fn fn,
+				  void *fn_param, struct device *dev,
+				  char *name)
+{
+	struct dma_chan *chan;
+
+	chan = dma_request_slave_channel(dev, name);
+	if (chan)
+		return chan;
+
+	return __dma_request_channel(mask, fn, fn_param);
+}
 
 /* --- Helper iov-locking functions --- */
 

commit 978c4172af48f0adc082f8b1d94acb817d947730
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Thu Feb 14 11:00:16 2013 +0200

    dmaengine.h: remove redundant else keyword
    
    dmaengine_device_control returns -ENOSYS in case the dma driver doesn't have
    such functionality.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index bfcdecb5d87a..f5939999cb65 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -610,8 +610,8 @@ static inline int dmaengine_device_control(struct dma_chan *chan,
 {
 	if (chan->device->device_control)
 		return chan->device->device_control(chan, cmd, arg);
-	else
-		return -ENOSYS;
+
+	return -ENOSYS;
 }
 
 static inline int dmaengine_slave_config(struct dma_chan *chan,

commit 61cc13a51bcff737ce02d2047834171c0365b00d
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Thu Jan 10 10:52:56 2013 +0200

    dmaengine: introduce is_slave_direction function
    
    This function helps to distinguish the slave type of transfer by checking the
    direction parameter.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Cc: Nicolas Ferre <nicolas.ferre@atmel.com>
    Cc: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 4ca9cf73ad3f..bfcdecb5d87a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -621,6 +621,11 @@ static inline int dmaengine_slave_config(struct dma_chan *chan,
 			(unsigned long)config);
 }
 
+static inline bool is_slave_direction(enum dma_transfer_direction direction)
+{
+	return (direction == DMA_MEM_TO_DEV) || (direction == DMA_DEV_TO_MEM);
+}
+
 static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
 	struct dma_chan *chan, dma_addr_t buf, size_t len,
 	enum dma_transfer_direction dir, unsigned long flags)

commit e239345f642e6a255d0ba1e3d92c2f9ec5a44fbe
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Thu Nov 8 10:01:01 2012 +0000

    dmaengine: remove dma_async_memcpy_complete() macro
    
    Just use dma_async_is_tx_complete() directly.
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Dan Williams <djbw@fb.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cd15958d4d1d..4ca9cf73ad3f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -926,16 +926,13 @@ static inline enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,
 	return status;
 }
 
-#define dma_async_memcpy_complete(chan, cookie, last, used)\
-	dma_async_is_tx_complete(chan, cookie, last, used)
-
 /**
  * dma_async_is_complete - test a cookie against chan state
  * @cookie: transaction identifier to test status of
  * @last_complete: last know completed transaction
  * @last_used: last cookie value handed out
  *
- * dma_async_is_complete() is used in dma_async_memcpy_complete()
+ * dma_async_is_complete() is used in dma_async_is_tx_complete()
  * the test logic is separated for lightweight testing of multiple cookies
  */
 static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,

commit b9ee86830f34737a08deead93872a79a37419a13
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Thu Nov 8 09:59:54 2012 +0000

    dmaengine: remove dma_async_memcpy_pending() macro
    
    Just use dma_async_issue_pending() directly.
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Dan Williams <djbw@fb.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index be6e95395b11..cd15958d4d1d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -901,8 +901,6 @@ static inline void dma_async_issue_pending(struct dma_chan *chan)
 	chan->device->device_issue_pending(chan);
 }
 
-#define dma_async_memcpy_issue_pending(chan) dma_async_issue_pending(chan)
-
 /**
  * dma_async_is_tx_complete - poll for transaction completion
  * @chan: DMA channel

commit 944ea4dd38b8575e30a5699633c81945bff1864d
Author: Jon Mason <jon.mason@intel.com>
Date:   Sun Nov 11 23:03:20 2012 +0000

    dmatest: Fix NULL pointer dereference on ioat
    
    device_control is an optional and not implemented in all DMA drivers.
    Any calls to these will result in a NULL pointer dereference.  dmatest
    makes two of these calls when completing the kernel thread and removing
    the module.  These are corrected by calling the dmaengine_device_control
    wrapper and checking for a non-existant device_control function pointer
    there.
    
    Signed-off-by: Jon Mason <jon.mason@intel.com>
    CC: Vinod Koul <vinod.koul@intel.com>
    CC: Dan Williams <djbw@fb.com>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3aa76dbed166..be6e95395b11 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -608,7 +608,10 @@ static inline int dmaengine_device_control(struct dma_chan *chan,
 					   enum dma_ctrl_cmd cmd,
 					   unsigned long arg)
 {
-	return chan->device->device_control(chan, cmd, arg);
+	if (chan->device->device_control)
+		return chan->device->device_control(chan, cmd, arg);
+	else
+		return -ENOSYS;
 }
 
 static inline int dmaengine_slave_config(struct dma_chan *chan,

commit a14acb4ac2a1486f6633c55eb7f7ded07f3ec9fc
Author: Barry Song <Baohua.Song@csr.com>
Date:   Tue Nov 6 21:32:39 2012 +0800

    DMAEngine: add dmaengine_prep_interleaved_dma wrapper for interleaved api
    
    commit b14dab792dee(DMAEngine: Define interleaved transfer request api) adds
    interleaved request api, this patch adds the dmaengine_prep_interleaved_dma
    just like we have dmaengine_prep_ for other modes to avoid drivers call:
    xxx_chan->device->device_prep_interleaved_dma().
    
    Signed-off-by: Barry Song <Baohua.Song@csr.com>
    Cc: Jassi Brar <jaswinder.singh@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 4c8643794e0d..3aa76dbed166 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -660,6 +660,13 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 						period_len, dir, flags, NULL);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(
+		struct dma_chan *chan, struct dma_interleaved_template *xt,
+		unsigned long flags)
+{
+	return chan->device->device_prep_interleaved_dma(chan, xt, flags);
+}
+
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);

commit e5a087fdc1ebe5bba40bcecb53c28a0af70e3b47
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Oct 26 23:35:15 2012 +0900

    dmaengine: use for_each_set_bit
    
    Use for_each_set_bit() to implement for_each_dma_cap_mask() and
    remove unused first_dma_cap() and next_dma_cap().
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Dan Williams <djbw@fb.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c88f302d91c9..4c8643794e0d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -849,20 +849,6 @@ static inline bool async_tx_test_ack(struct dma_async_tx_descriptor *tx)
 	return (tx->flags & DMA_CTRL_ACK) == DMA_CTRL_ACK;
 }
 
-#define first_dma_cap(mask) __first_dma_cap(&(mask))
-static inline int __first_dma_cap(const dma_cap_mask_t *srcp)
-{
-	return min_t(int, DMA_TX_TYPE_END,
-		find_first_bit(srcp->bits, DMA_TX_TYPE_END));
-}
-
-#define next_dma_cap(n, mask) __next_dma_cap((n), &(mask))
-static inline int __next_dma_cap(int n, const dma_cap_mask_t *srcp)
-{
-	return min_t(int, DMA_TX_TYPE_END,
-		find_next_bit(srcp->bits, DMA_TX_TYPE_END, n+1));
-}
-
 #define dma_cap_set(tx, mask) __dma_cap_set((tx), &(mask))
 static inline void
 __dma_cap_set(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)
@@ -891,9 +877,7 @@ __dma_has_cap(enum dma_transaction_type tx_type, dma_cap_mask_t *srcp)
 }
 
 #define for_each_dma_cap_mask(cap, mask) \
-	for ((cap) = first_dma_cap(mask);	\
-		(cap) < DMA_TX_TYPE_END;	\
-		(cap) = next_dma_cap((cap), (mask)))
+	for_each_set_bit(cap, mask.bits, DMA_TX_TYPE_END)
 
 /**
  * dma_async_issue_pending - flush pending transactions to HW

commit d18d5f59797009d86a0ee46feb2a56d53707455a
Author: Vinod Koul <vinod.koul@linux.intel.com>
Date:   Tue Sep 25 16:18:55 2012 +0530

    dmaengine: fix build failure due to missing semi-colon
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8cd0e2556d04..c88f302d91c9 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -992,7 +992,7 @@ static inline struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask,
 static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
 							 char *name)
 {
-	return NULL
+	return NULL;
 }
 static inline void dma_release_channel(struct dma_chan *chan)
 {

commit 9a6cecc846169159bfce511f4c0034bb96eea1ca
Author: Jon Hunter <jon-hunter@ti.com>
Date:   Fri Sep 14 17:41:57 2012 -0500

    dmaengine: add helper function to request a slave DMA channel
    
    Currently slave DMA channels are requested by calling dma_request_channel()
    and requires DMA clients to pass various filter parameters to obtain the
    appropriate channel.
    
    With device-tree being used by architectures such as arm and the addition of
    device-tree helper functions to extract the relevant DMA client information
    from device-tree, add a new function to request a slave DMA channel using
    device-tree. This function is currently a simple wrapper that calls the
    device-tree of_dma_request_slave_channel() function.
    
    Cc: Nicolas Ferre <nicolas.ferre@atmel.com>
    Cc: Benoit Cousson <b-cousson@ti.com>
    Cc: Stephen Warren <swarren@nvidia.com>
    Cc: Grant Likely <grant.likely@secretlab.ca>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Dan Williams <djbw@fb.com>
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jon Hunter <jon-hunter@ti.com>
    Reviewed-by: Stephen Warren <swarren@wwwdotorg.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d3201e438d16..8cd0e2556d04 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -974,6 +974,7 @@ enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
 struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
+struct dma_chan *dma_request_slave_channel(struct device *dev, char *name);
 void dma_release_channel(struct dma_chan *chan);
 #else
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
@@ -988,6 +989,11 @@ static inline struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask,
 {
 	return NULL;
 }
+static inline struct dma_chan *dma_request_slave_channel(struct device *dev,
+							 char *name)
+{
+	return NULL
+}
 static inline void dma_release_channel(struct dma_chan *chan)
 {
 }

commit e7736cdea223f3a5b867c359fb35cf08250dd771
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Mon Sep 24 10:58:04 2012 +0300

    dmaengine: Add flags parameter to dmaengine_prep_dma_cyclic()
    
    With this parameter added to dmaengine_prep_dma_cyclic() the API will be in
    sync with other dmaengine_prep_*() functions.
    The dmaengine_prep_dma_cyclic() function primarily used by audio for cyclic
    transfer required by ALSA, we use the from audio to ask dma drivers to
    suppress interrupts (if DMA_PREP_INTERRUPT is cleared) when it is supported
    on the platform.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    CC: Lars-Peter Clausen <lars@metafoo.de>
    Acked-by: Vinod Koul <vinod.koul@linux.intel.com>
    Signed-off-by: Takashi Iwai <tiwai@suse.de>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 09da4e565297..d3201e438d16 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -653,7 +653,8 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_rio_sg(
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
-		size_t period_len, enum dma_transfer_direction dir)
+		size_t period_len, enum dma_transfer_direction dir,
+		unsigned long flags)
 {
 	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
 						period_len, dir, flags, NULL);

commit ec8b5e48c03790a68cb875fe5064007a9cbdfdd0
Author: Peter Ujfalusi <peter.ujfalusi@ti.com>
Date:   Fri Sep 14 15:05:47 2012 +0300

    dmaengine: Pass flags via device_prep_dma_cyclic() callback
    
    Change the parameter list of device_prep_dma_cyclic() so the DMA drivers
    can receive the flags coming from clients.
    This feature can be used during audio operation to disable all audio
    related interrupts when the DMA_PREP_INTERRUPT is cleared from the flags.
    
    Signed-off-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
    Acked-by: Nicolas Ferre <nicolas.ferre@atmel.com>
    Acked-by: Shawn Guo <shawn.guo@linaro.org>
    Acked-by: Vinod Koul <vinod.koul@linux.intel.com>
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9c02a4508b25..09da4e565297 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -591,7 +591,7 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
 		size_t period_len, enum dma_transfer_direction direction,
-		void *context);
+		unsigned long flags, void *context);
 	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags);
@@ -656,7 +656,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		size_t period_len, enum dma_transfer_direction dir)
 {
 	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
-						period_len, dir, NULL);
+						period_len, dir, flags, NULL);
 }
 
 static inline int dmaengine_terminate_all(struct dma_chan *chan)

commit 3052cc2c92f11875d111d5b7b9b3ad535b3128b9
Author: Lars-Peter Clausen <lars@metafoo.de>
Date:   Mon Jun 11 20:11:40 2012 +0200

    dmaengine: Add wrapper for device_tx_status callback
    
    This patch adds a small inline wrapper for the devivce_tx_status callback of a
    dma device. This makes the source code of users of this function a bit more
    compact and a bit more legible.
    
    E.g.:
    -status = chan->device->device_tx_status(chan, cookie, &state)
    +status = dmaengine_tx_status(chan, cookie, &state)
    
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ccec62f8e501..9c02a4508b25 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -674,6 +674,12 @@ static inline int dmaengine_resume(struct dma_chan *chan)
 	return dmaengine_device_control(chan, DMA_RESUME, 0);
 }
 
+static inline enum dma_status dmaengine_tx_status(struct dma_chan *chan,
+	dma_cookie_t cookie, struct dma_tx_state *state)
+{
+	return chan->device->device_tx_status(chan, cookie, state);
+}
+
 static inline dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc)
 {
 	return desc->tx_submit(desc);

commit 4fd1e324b7b5f80bd521b58593ada74ef89e80c4
Author: Laxman Dewangan <ldewangan@nvidia.com>
Date:   Wed Jun 6 10:55:26 2012 +0530

    dma: dmaengine: add slave req id in slave_config
    
    The DMA controller like Nvidia's Tegra Dma controller
    supports the different slave requestor id from different slave.
    This need to be configure in dma controller to handle the request
    properly.
    
    Adding the slave-id in the slave configuration so that information
    can be passed from client when configuring for slave.
    
    Signed-off-by: Laxman Dewangan <ldewangan@nvidia.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 56377df39124..ccec62f8e501 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -338,6 +338,9 @@ enum dma_slave_buswidth {
  * @device_fc: Flow Controller Settings. Only valid for slave channels. Fill
  * with 'true' if peripheral should be flow controller. Direction will be
  * selected at Runtime.
+ * @slave_id: Slave requester id. Only valid for slave channels. The dma
+ * slave peripheral will have unique id as dma requester which need to be
+ * pass as slave config.
  *
  * This struct is passed in as configuration data to a DMA engine
  * in order to set up a certain channel for DMA transport at runtime.
@@ -365,6 +368,7 @@ struct dma_slave_config {
 	u32 src_maxburst;
 	u32 dst_maxburst;
 	bool device_fc;
+	unsigned int slave_id;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)

commit e42d98ebe7d754a2c9fbccd6186721d3ca8679f6
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Thu May 31 16:26:38 2012 -0700

    rapidio: add DMA engine support for RIO data transfers
    
    Adds DMA Engine framework support into RapidIO subsystem.
    
    Uses DMA Engine DMA_SLAVE interface to generate data transfers to/from
    remote RapidIO target devices.
    
    Introduces RapidIO-specific wrapper for prep_slave_sg() interface with an
    extra parameter to pass target specific information.
    
    Uses scatterlist to describe local data buffer.  Address flat data buffer
    on a remote side.
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Vinod Koul <vinod.koul@linux.intel.com>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d3fec584e8c3..56377df39124 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -635,6 +635,18 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
 						  dir, flags, NULL);
 }
 
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+struct rio_dma_ext;
+static inline struct dma_async_tx_descriptor *dmaengine_prep_rio_sg(
+	struct dma_chan *chan, struct scatterlist *sgl,	unsigned int sg_len,
+	enum dma_transfer_direction dir, unsigned long flags,
+	struct rio_dma_ext *rio_ext)
+{
+	return chan->device->device_prep_slave_sg(chan, sgl, sg_len,
+						  dir, flags, rio_ext);
+}
+#endif
+
 static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
 		size_t period_len, enum dma_transfer_direction dir)

commit d5adf235adc8d8d67c10afd43922c92753f6be3c
Merge: d484864dd96e 1dd1ea8eb46a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 25 09:31:59 2012 -0700

    Merge branch 'next' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine updates from Vinod Koul:
     "Nothing exciting this time, odd fixes in a bunch of drivers"
    
    * 'next' of git://git.infradead.org/users/vkoul/slave-dma:
      dmaengine: at_hdmac: take maxburst from slave configuration
      dmaengine: at_hdmac: remove ATC_DEFAULT_CTRLA constant
      dmaengine: at_hdmac: remove some at_dma_slave comments
      dma: imx-sdma: make channel0 operations atomic
      dmaengine: Fixup dmaengine_prep_slave_single() to be actually useful
      dmaengine: Use dma_sg_len(sg) instead of sg->length
      dmaengine: Use sg_dma_address instead of sg_phys
      DMA: PL330: Remove duplicate header file inclusion
      dma: imx-sdma: keep the callbacks invoked in the tasklet
      dmaengine: dw_dma: add Device Tree probing capability
      dmaengine: dw_dmac: Add clk_{un}prepare() support
      dma/amba-pl08x: add support for the Nomadik variant
      dma/amba-pl08x: check for terminal count status only

commit 922ee08baad2052d0759f100e026d49798c51fef
Author: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
Date:   Wed Apr 25 20:50:53 2012 +0200

    dmaengine: Fixup dmaengine_prep_slave_single() to be actually useful
    
    dmaengine_prep_slave_single() is a helper function which is supposed to be used
    to prepare a transfer of a single contingous buffer. Currently the function
    takes a pointer to such a buffer from which it builds a scatterlist and passes
    it on to device_prep_slave_sg. The dmaengine framework requires that any
    scatterlist that is passed to device_prep_slave_sg is mapped and it may not be
    unmapped until the DMA operation has completed. This is not the here and any use
    of dmaengine_prep_slave_single() will lead to undefined behaviour (Most likely a
    system crash).
    
    This patch changes dmaengine_prep_slave_single() to take a dma_addr_t instead of
    a pointer to a buffer and moves the responsibility of mapping and unmapping the
    buffer up to the caller.
    
    Signed-off-by: Kuninori Morimoto <kuninori.morimoto.gx@renesas.com>
    Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 676f967390ae..0e6b595e95c8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -615,11 +615,13 @@ static inline int dmaengine_slave_config(struct dma_chan *chan,
 }
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
-	struct dma_chan *chan, void *buf, size_t len,
+	struct dma_chan *chan, dma_addr_t buf, size_t len,
 	enum dma_transfer_direction dir, unsigned long flags)
 {
 	struct scatterlist sg;
-	sg_init_one(&sg, buf, len);
+	sg_init_table(&sg, 1);
+	sg_dma_address(&sg) = buf;
+	sg_dma_len(&sg) = len;
 
 	return chan->device->device_prep_slave_sg(chan, &sg, 1,
 						  dir, flags, NULL);

commit 94fb175c0414902ad9dbd956addf3a5feafbc85b
Merge: a9e1e53bcfb2 a2bd1140a264
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 10 15:30:16 2012 -0700

    Merge tag 'dmaengine-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine fixes from Dan Williams:
    
    1/ regression fix for Xen as it now trips over a broken assumption
       about the dma address size on 32-bit builds
    
    2/ new quirk for netdma to ignore dma channels that cannot meet
       netdma alignment requirements
    
    3/ fixes for two long standing issues in ioatdma (ring size overflow)
       and iop-adma (potential stack corruption)
    
    * tag 'dmaengine-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      netdma: adding alignment check for NETDMA ops
      ioatdma: DMA copy alignment needed to address IOAT DMA silicon errata
      ioat: ring size variables need to be 32bit to avoid overflow
      iop-adma: Corrected array overflow in RAID6 Xscale(R) test.
      ioat: fix size of 'completion' for Xen

commit a2bd1140a264b561e38d99e656cd843c2d840e86
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Apr 4 16:10:46 2012 -0700

    netdma: adding alignment check for NETDMA ops
    
    This is the fallout from adding memcpy alignment workaround for certain
    IOATDMA hardware. NetDMA will only use DMA engine that can handle byte align
    ops.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 679b349d9b66..a5bb3ad5c7a5 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -948,6 +948,7 @@ int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
+struct dma_chan *net_dma_find_channel(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 
 /* --- Helper iov-locking functions --- */

commit ef08e78268423fc4d7fbc3e54bd9a67fc8da7cc5
Merge: 71db34fc4330 5b2e02e401de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 15:34:57 2012 -0700

    Merge branch 'next' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine update from Vinod Koul:
     "This includes the cookie cleanup by Russell, the addition of context
      parameter for dmaengine APIs, more arm dmaengine driver cleanup by
      moving code to dmaengine, this time for imx by Javier and pl330 by
      Boojin along with the usual driver fixes."
    
    Fix up some fairly trivial conflicts with various other cleanups.
    
    * 'next' of git://git.infradead.org/users/vkoul/slave-dma: (67 commits)
      dmaengine: imx: fix the build failure on x86_64
      dmaengine: i.MX: Fix merge of cookie branch.
      dmaengine: i.MX: Add support for interleaved transfers.
      dmaengine: imx-dma: use 'dev_dbg' and 'dev_warn' for messages.
      dmaengine: imx-dma: remove 'imx_dmav1_baseaddr' and 'dma_clk'.
      dmaengine: imx-dma: remove unused arg of imxdma_sg_next.
      dmaengine: imx-dma: remove internal structure.
      dmaengine: imx-dma: remove 'resbytes' field of 'internal' structure.
      dmaengine: imx-dma: remove 'in_use' field of 'internal' structure.
      dmaengine: imx-dma: remove sg member from internal structure.
      dmaengine: imx-dma: remove 'imxdma_setup_sg_hw' function.
      dmaengine: imx-dma: remove 'imxdma_config_channel_hw' function.
      dmaengine: imx-dma: remove 'imxdma_setup_mem2mem_hw' function.
      dmaengine: imx-dma: remove dma_mode member of internal structure.
      dmaengine: imx-dma: remove data member from internal structure.
      dmaengine: imx-dma: merge old dma-v1.c with imx-dma.c
      dmaengine: at_hdmac: add slave config operation
      dmaengine: add context parameter to prep_slave_sg and prep_dma_cyclic
      dmaengine/dma_slave: introduce inline wrappers
      dma: imx-sdma: Treat firmware messages as warnings instead of erros
      ...

commit 185ecb5f4fd43911c35956d4cc7d94a1da30417f
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Thu Mar 8 15:35:13 2012 -0500

    dmaengine: add context parameter to prep_slave_sg and prep_dma_cyclic
    
    Add context parameter to device_prep_slave_sg() and device_prep_dma_cyclic()
    interfaces to allow passing client/target specific information associated
    with the data transfer.
    Modify all affected DMA engine drivers.
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 03d68b7e5705..b3b5b38776f0 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -582,10 +582,11 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(
 		struct dma_chan *chan, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_transfer_direction direction,
-		unsigned long flags);
+		unsigned long flags, void *context);
 	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
-		size_t period_len, enum dma_transfer_direction direction);
+		size_t period_len, enum dma_transfer_direction direction,
+		void *context);
 	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
 		struct dma_chan *chan, struct dma_interleaved_template *xt,
 		unsigned long flags);
@@ -619,7 +620,8 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
 	struct scatterlist sg;
 	sg_init_one(&sg, buf, len);
 
-	return chan->device->device_prep_slave_sg(chan, &sg, 1, dir, flags);
+	return chan->device->device_prep_slave_sg(chan, &sg, 1,
+						  dir, flags, NULL);
 }
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
@@ -627,7 +629,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
 	enum dma_transfer_direction dir, unsigned long flags)
 {
 	return chan->device->device_prep_slave_sg(chan, sgl, sg_len,
-						  dir, flags);
+						  dir, flags, NULL);
 }
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
@@ -635,7 +637,7 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
 		size_t period_len, enum dma_transfer_direction dir)
 {
 	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
-							period_len, dir);
+						period_len, dir, NULL);
 }
 
 static inline int dmaengine_terminate_all(struct dma_chan *chan)

commit 16052827d98fbc13c31ebad560af4bd53e2b4dd5
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Thu Mar 8 16:11:18 2012 -0500

    dmaengine/dma_slave: introduce inline wrappers
    
    Add inline wrappers for device_prep_slave_sg() and device_prep_dma_cyclic()
    interfaces to hide new parameter from current users of affected interfaces.
    Convert current users to use new wrappers instead of direct calls.
    Suggested by Russell King [https://lkml.org/lkml/2012/2/3/269].
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 715babf4bffe..03d68b7e5705 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -622,6 +622,22 @@ static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
 	return chan->device->device_prep_slave_sg(chan, &sg, 1, dir, flags);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(
+	struct dma_chan *chan, struct scatterlist *sgl,	unsigned int sg_len,
+	enum dma_transfer_direction dir, unsigned long flags)
+{
+	return chan->device->device_prep_slave_sg(chan, sgl, sg_len,
+						  dir, flags);
+}
+
+static inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(
+		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
+		size_t period_len, enum dma_transfer_direction dir)
+{
+	return chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,
+							period_len, dir);
+}
+
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);

commit d2ebfb335b0426deb1a4fb14e4e926d81ecd8235
Author: Russell King - ARM Linux <linux@arm.linux.org.uk>
Date:   Tue Mar 6 22:34:26 2012 +0000

    dmaengine: add private header file
    
    Add a local private header file to contain definitions and declarations
    which should only be used by DMA engine drivers.
    
    We also fix linux/dmaengine.h to use LINUX_DMAENGINE_H to guard against
    multiple inclusion.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Jassi Brar <jassisinghbrar@gmail.com>
    [imx-sdma.c & mxs-dma.c]
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c59c4f0c2cc9..715babf4bffe 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -18,8 +18,8 @@
  * The full GNU General Public License is included in this distribution in the
  * file called COPYING.
  */
-#ifndef DMAENGINE_H
-#define DMAENGINE_H
+#ifndef LINUX_DMAENGINE_H
+#define LINUX_DMAENGINE_H
 
 #include <linux/device.h>
 #include <linux/uio.h>

commit 4d4e58de32a192fea65ab84509d17d199bd291c8
Author: Russell King - ARM Linux <linux@arm.linux.org.uk>
Date:   Tue Mar 6 22:34:06 2012 +0000

    dmaengine: move last completed cookie into generic dma_chan structure
    
    Every DMA engine implementation declares a last completed dma cookie
    in their private dma channel structures.  This is pointless, and
    forces driver specific code.  Move this out into the common dma_chan
    structure.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Jassi Brar <jassisinghbrar@gmail.com>
    [imx-sdma.c & mxs-dma.c]
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 7e640bf27d2d..c59c4f0c2cc9 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -258,6 +258,7 @@ struct dma_chan_percpu {
  * struct dma_chan - devices supply DMA channels, clients use them
  * @device: ptr to the dma device who supplies this channel, always !%NULL
  * @cookie: last cookie value returned to client
+ * @completed_cookie: last completed cookie for this channel
  * @chan_id: channel ID for sysfs
  * @dev: class device for sysfs
  * @device_node: used to add this to the device chan list
@@ -269,6 +270,7 @@ struct dma_chan_percpu {
 struct dma_chan {
 	struct dma_device *device;
 	dma_cookie_t cookie;
+	dma_cookie_t completed_cookie;
 
 	/* sysfs */
 	int chan_id;

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 679b349d9b66..a5966f691ef8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -23,6 +23,7 @@
 
 #include <linux/device.h>
 #include <linux/uio.h>
+#include <linux/bug.h>
 #include <linux/scatterlist.h>
 #include <linux/bitmap.h>
 #include <asm/page.h>

commit dcc043dc0c60046cf6b75ca04a462314cf64e2ba
Author: Viresh Kumar <viresh.kumar@st.com>
Date:   Wed Feb 1 16:12:18 2012 +0530

    dmaengine: Add flow controller information to dma_slave_config
    
    Flow controller is programmable for few controllers and there are few
    intelligent peripherals like, Synopsys JPEG controller, that needs to be a flow
    controller of DMA transfers on dest side.
    
    For this, currently two drivers, pl08x and dw_dmac, support flow controller to
    be passed from platform to these drivers.
    
    Perhaps, this should be a part of struct dma_slave_config. This patch adds
    another field device_fc to this structure. User drivers must pass this as true
    if they want to be flow controller of certain transfers.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@st.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 679b349d9b66..7e640bf27d2d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -25,6 +25,7 @@
 #include <linux/uio.h>
 #include <linux/scatterlist.h>
 #include <linux/bitmap.h>
+#include <linux/types.h>
 #include <asm/page.h>
 
 /**
@@ -331,6 +332,9 @@ enum dma_slave_buswidth {
  * may or may not be applicable on memory sources.
  * @dst_maxburst: same as src_maxburst but for destination target
  * mutatis mutandis.
+ * @device_fc: Flow Controller Settings. Only valid for slave channels. Fill
+ * with 'true' if peripheral should be flow controller. Direction will be
+ * selected at Runtime.
  *
  * This struct is passed in as configuration data to a DMA engine
  * in order to set up a certain channel for DMA transport at runtime.
@@ -357,6 +361,7 @@ struct dma_slave_config {
 	enum dma_slave_buswidth dst_addr_width;
 	u32 src_maxburst;
 	u32 dst_maxburst;
+	bool device_fc;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)

commit 62268ce9170c5466332c046ff6ddafcb67751502
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Tue Dec 13 23:48:03 2011 +0800

    dmaengine: add DMA_TRANS_NONE to dma_transfer_direction
    
    Before dma_transfer_direction was introduced to replace
    dma_data_direction, some dmaengine device uses DMA_NONE of
    dma_data_direction for some talk with its client drivers.
    The mxs-dma and its clients mxs-mmc and gpmi-nand are such case.
    
    This patch adds DMA_TRANS_NONE to dma_transfer_direction and
    migrate the DMA_NONE use in mxs-dma to it.
    
    It also fixes the compile warning below.
    
    CC      drivers/dma/mxs-dma.o
    drivers/dma/mxs-dma.c: In function ‘mxs_dma_prep_slave_sg’:
    drivers/dma/mxs-dma.c:420:16: warning: comparison between ‘enum dma_transfer_direction’ and ‘enum dma_data_direction’
    
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 5532bb8b500c..679b349d9b66 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -88,6 +88,7 @@ enum dma_transfer_direction {
 	DMA_MEM_TO_DEV,
 	DMA_DEV_TO_MEM,
 	DMA_DEV_TO_DEV,
+	DMA_TRANS_NONE,
 };
 
 /**

commit b14dab792dee3245b628e046d80a7fad5573fea6
Author: Jassi Brar <jaswinder.singh@linaro.org>
Date:   Thu Oct 13 12:33:30 2011 +0530

    DMAEngine: Define interleaved transfer request api
    
    Define a new api that could be used for doing fancy data transfers
    like interleaved to contiguous copy and vice-versa.
    Traditional SG_list based transfers tend to be very inefficient in
    such cases as where the interleave and chunk are only a few bytes,
    which call for a very condensed api to convey pattern of the transfer.
    This api supports all 4 variants of scatter-gather and contiguous transfer.
    
    Of course, neither can this api help transfers that don't lend to DMA by
    nature, i.e, scattered tiny read/writes with no periodic pattern.
    
    Also since now we support SLAVE channels that might not provide
    device_prep_slave_sg callback but device_prep_interleaved_dma,
    remove the BUG_ON check.
    
    Signed-off-by: Jassi Brar <jaswinder.singh@linaro.org>
    Acked-by: Barry Song <Baohua.Song@csr.com>
    [renamed dmaxfer_template to dma_interleaved_template
     did fixup after the enum dma_transfer_merge]
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index a865b3a354cd..5532bb8b500c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -71,10 +71,10 @@ enum dma_transaction_type {
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
 	DMA_CYCLIC,
-};
-
+	DMA_INTERLEAVE,
 /* last transaction type for creation of the capabilities mask */
-#define DMA_TX_TYPE_END (DMA_CYCLIC + 1)
+	DMA_TX_TYPE_END,
+};
 
 /**
  * enum dma_transfer_direction - dma transfer mode and direction indicator
@@ -90,6 +90,74 @@ enum dma_transfer_direction {
 	DMA_DEV_TO_DEV,
 };
 
+/**
+ * Interleaved Transfer Request
+ * ----------------------------
+ * A chunk is collection of contiguous bytes to be transfered.
+ * The gap(in bytes) between two chunks is called inter-chunk-gap(ICG).
+ * ICGs may or maynot change between chunks.
+ * A FRAME is the smallest series of contiguous {chunk,icg} pairs,
+ *  that when repeated an integral number of times, specifies the transfer.
+ * A transfer template is specification of a Frame, the number of times
+ *  it is to be repeated and other per-transfer attributes.
+ *
+ * Practically, a client driver would have ready a template for each
+ *  type of transfer it is going to need during its lifetime and
+ *  set only 'src_start' and 'dst_start' before submitting the requests.
+ *
+ *
+ *  |      Frame-1        |       Frame-2       | ~ |       Frame-'numf'  |
+ *  |====....==.===...=...|====....==.===...=...| ~ |====....==.===...=...|
+ *
+ *    ==  Chunk size
+ *    ... ICG
+ */
+
+/**
+ * struct data_chunk - Element of scatter-gather list that makes a frame.
+ * @size: Number of bytes to read from source.
+ *	  size_dst := fn(op, size_src), so doesn't mean much for destination.
+ * @icg: Number of bytes to jump after last src/dst address of this
+ *	 chunk and before first src/dst address for next chunk.
+ *	 Ignored for dst(assumed 0), if dst_inc is true and dst_sgl is false.
+ *	 Ignored for src(assumed 0), if src_inc is true and src_sgl is false.
+ */
+struct data_chunk {
+	size_t size;
+	size_t icg;
+};
+
+/**
+ * struct dma_interleaved_template - Template to convey DMAC the transfer pattern
+ *	 and attributes.
+ * @src_start: Bus address of source for the first chunk.
+ * @dst_start: Bus address of destination for the first chunk.
+ * @dir: Specifies the type of Source and Destination.
+ * @src_inc: If the source address increments after reading from it.
+ * @dst_inc: If the destination address increments after writing to it.
+ * @src_sgl: If the 'icg' of sgl[] applies to Source (scattered read).
+ *		Otherwise, source is read contiguously (icg ignored).
+ *		Ignored if src_inc is false.
+ * @dst_sgl: If the 'icg' of sgl[] applies to Destination (scattered write).
+ *		Otherwise, destination is filled contiguously (icg ignored).
+ *		Ignored if dst_inc is false.
+ * @numf: Number of frames in this template.
+ * @frame_size: Number of chunks in a frame i.e, size of sgl[].
+ * @sgl: Array of {chunk,icg} pairs that make up a frame.
+ */
+struct dma_interleaved_template {
+	dma_addr_t src_start;
+	dma_addr_t dst_start;
+	enum dma_transfer_direction dir;
+	bool src_inc;
+	bool dst_inc;
+	bool src_sgl;
+	bool dst_sgl;
+	size_t numf;
+	size_t frame_size;
+	struct data_chunk sgl[0];
+};
+
 /**
  * enum dma_ctrl_flags - DMA flags to augment operation preparation,
  *  control completion, and communicate status.
@@ -445,6 +513,7 @@ struct dma_tx_state {
  * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
  *	The function takes a buffer of size buf_len. The callback function will
  *	be called after period_len bytes have been transferred.
+ * @device_prep_interleaved_dma: Transfer expression in a generic way.
  * @device_control: manipulate all pending operations on a channel, returns
  *	zero or error code
  * @device_tx_status: poll for transaction completion, the optional
@@ -509,6 +578,9 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
 		size_t period_len, enum dma_transfer_direction direction);
+	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(
+		struct dma_chan *chan, struct dma_interleaved_template *xt,
+		unsigned long flags);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
 

commit e0d23ef29ed637dc6bd739f590985746d9ad9caa
Merge: ca7fe2db892d 55ba4e5ed4ac
Author: Vinod Koul <vinod.koul@linux.intel.com>
Date:   Thu Nov 17 14:54:38 2011 +0530

    Merge branch 'dma_slave_direction' into next_test_dirn
    
    resolved conflicts:
            drivers/media/video/mx3_camera.c

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit a8efa9d6bf00fbe9597dd3352dc062a998bf9b15
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 29 16:55:11 2011 +1000

    linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
    
    The implicit presence of module.h and all its sub-includes was
    masking these implicit header usages:
    
    include/linux/dmaengine.h:684: warning: 'struct page' declared inside parameter list
    include/linux/dmaengine.h:684: warning: its scope is only this definition or declaration, which is probably not what you want
    include/linux/dmaengine.h:687: warning: 'struct page' declared inside parameter list
    include/linux/dmaengine.h:736:2: error: implicit declaration of function 'bitmap_zero'
    
    With input from Stephen Rothwell <sfr@canb.auug.org.au>
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8fbf40e0713c..1ceff5ae9d31 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -24,6 +24,8 @@
 #include <linux/device.h>
 #include <linux/uio.h>
 #include <linux/dma-direction.h>
+#include <linux/bitmap.h>
+#include <asm/page.h>
 
 struct scatterlist;
 

commit 49920bc66984a512f4bcc7735a61642cd0e4d6f2
Author: Vinod Koul <vinod.koul@linux.intel.com>
Date:   Thu Oct 13 15:15:27 2011 +0530

    dmaengine: add new enum dma_transfer_direction
    
    This new enum removes usage of dma_data_direction for dma direction. The new
    enum cleans tells the DMA direction and mode
    This further paves way for merging the dmaengine _prep operations and also for
    interleaved dma
    
    Suggested-by: Jassi Brar <jaswinder.singh@linaro.org>
    Reviewed-by: Barry Song <Baohua.Song@csr.com>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ace51af4369f..d946ef7f5e67 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -23,7 +23,6 @@
 
 #include <linux/device.h>
 #include <linux/uio.h>
-#include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
 
 /**
@@ -75,6 +74,19 @@ enum dma_transaction_type {
 /* last transaction type for creation of the capabilities mask */
 #define DMA_TX_TYPE_END (DMA_CYCLIC + 1)
 
+/**
+ * enum dma_transfer_direction - dma transfer mode and direction indicator
+ * @DMA_MEM_TO_MEM: Async/Memcpy mode
+ * @DMA_MEM_TO_DEV: Slave mode & From Memory to Device
+ * @DMA_DEV_TO_MEM: Slave mode & From Device to Memory
+ * @DMA_DEV_TO_DEV: Slave mode & From Device to Device
+ */
+enum dma_transfer_direction {
+	DMA_MEM_TO_MEM,
+	DMA_MEM_TO_DEV,
+	DMA_DEV_TO_MEM,
+	DMA_DEV_TO_DEV,
+};
 
 /**
  * enum dma_ctrl_flags - DMA flags to augment operation preparation,
@@ -267,7 +279,7 @@ enum dma_slave_buswidth {
  * struct, if applicable.
  */
 struct dma_slave_config {
-	enum dma_data_direction direction;
+	enum dma_transfer_direction direction;
 	dma_addr_t src_addr;
 	dma_addr_t dst_addr;
 	enum dma_slave_buswidth src_addr_width;
@@ -490,11 +502,11 @@ struct dma_device {
 
 	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(
 		struct dma_chan *chan, struct scatterlist *sgl,
-		unsigned int sg_len, enum dma_data_direction direction,
+		unsigned int sg_len, enum dma_transfer_direction direction,
 		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
 		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
-		size_t period_len, enum dma_data_direction direction);
+		size_t period_len, enum dma_transfer_direction direction);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
 
@@ -520,7 +532,7 @@ static inline int dmaengine_slave_config(struct dma_chan *chan,
 
 static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
 	struct dma_chan *chan, void *buf, size_t len,
-	enum dma_data_direction dir, unsigned long flags)
+	enum dma_transfer_direction dir, unsigned long flags)
 {
 	struct scatterlist sg;
 	sg_init_one(&sg, buf, len);

commit a16e470caa173d323ef68dcac98c899b95fa4f84
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Tue Aug 9 10:08:10 2011 +0530

    dmaengine: remove struct scatterlist for header
    
    Commit 90b44f8 introduces dmaengine_prep_slave_single API which adds
    scatterlist.h in dmaengine.h, so defining struct scatterlist is not required
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 0d738c95fe4e..ace51af4369f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -26,8 +26,6 @@
 #include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
 
-struct scatterlist;
-
 /**
  * typedef dma_cookie_t - an opaque DMA cookie
  *

commit 90b44f8ffdf6c66d190ee71b330009bf7f11a208
Author: Vinod Koul <vinod.koul@intel.com>
Date:   Mon Jul 25 19:57:52 2011 +0530

    dmaengine: add helper function for slave_single
    
    For clients which require a single slave transfer and dont want to be bothered
    about the scatterlist api, this helper gives simple API for this transfer and
    creates single scatterlist for DMA API
    
    Idea from Russell King
    
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 8fbf40e0713c..0d738c95fe4e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -24,6 +24,7 @@
 #include <linux/device.h>
 #include <linux/uio.h>
 #include <linux/dma-direction.h>
+#include <linux/scatterlist.h>
 
 struct scatterlist;
 
@@ -519,6 +520,16 @@ static inline int dmaengine_slave_config(struct dma_chan *chan,
 			(unsigned long)config);
 }
 
+static inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(
+	struct dma_chan *chan, void *buf, size_t len,
+	enum dma_data_direction dir, unsigned long flags)
+{
+	struct scatterlist sg;
+	sg_init_one(&sg, buf, len);
+
+	return chan->device->device_prep_slave_sg(chan, &sg, 1, dir, flags);
+}
+
 static inline int dmaengine_terminate_all(struct dma_chan *chan)
 {
 	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);

commit b7f080cfe223b3b7424872639d153695615a9255
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Jun 16 11:01:34 2011 +0000

    net: remove mm.h inclusion from netdevice.h
    
    Remove linux/mm.h inclusion from netdevice.h -- it's unused (I've checked manually).
    
    To prevent mm.h inclusion via other channels also extract "enum dma_data_direction"
    definition into separate header. This tiny piece is what gluing netdevice.h with mm.h
    via "netdevice.h => dmaengine.h => dma-mapping.h => scatterlist.h => mm.h".
    Removal of mm.h from scatterlist.h was tried and was found not feasible
    on most archs, so the link was cutoff earlier.
    
    Hope people are OK with tiny include file.
    
    Note, that mm_types.h is still dragged in, but it is a separate story.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index eee7addec282..8fbf40e0713c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -23,7 +23,9 @@
 
 #include <linux/device.h>
 #include <linux/uio.h>
-#include <linux/dma-mapping.h>
+#include <linux/dma-direction.h>
+
+struct scatterlist;
 
 /**
  * typedef dma_cookie_t - an opaque DMA cookie

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9bebd7f16ef1..eee7addec282 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -434,7 +434,7 @@ struct dma_tx_state {
  *	zero or error code
  * @device_tx_status: poll for transaction completion, the optional
  *	txstate parameter can be supplied with a pointer to get a
- *	struct with auxilary transfer status information, otherwise the call
+ *	struct with auxiliary transfer status information, otherwise the call
  *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
  */

commit e1288cd72f54e7fc16ae9ebb4d0647537ef848d4
Merge: e78bf5e6cbe8 94ae85220a07
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 17 10:54:41 2011 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx: (63 commits)
      ARM: PL08x: cleanup comments
      Update CONFIG_MD_RAID6_PQ to CONFIG_RAID6_PQ in drivers/dma/iop-adma.c
      ARM: PL08x: fix a warning
      Fix dmaengine_submit() return type
      dmaengine: at_hdmac: fix race while monitoring channel status
      dmaengine: at_hdmac: flags located in first descriptor
      dmaengine: at_hdmac: use subsys_initcall instead of module_init
      dmaengine: at_hdmac: no need set ACK in new descriptor
      dmaengine: at_hdmac: trivial add precision to unmapping comment
      dmaengine: at_hdmac: use dma_address to program DMA hardware
      pch_dma: support new device ML7213 IOH
      ARM: PL08x: prevent dma_set_runtime_config() reconfiguring memcpy channels
      ARM: PL08x: allow dma_set_runtime_config() to return errors
      ARM: PL08x: fix locking between prepare function and submit function
      ARM: PL08x: introduce 'phychan_hold' to hold on to physical channels
      ARM: PL08x: put txd's on the pending list in pl08x_tx_submit()
      ARM: PL08x: rename 'desc_list' as 'pend_list'
      ARM: PL08x: implement unmapping of memcpy buffers
      ARM: PL08x: store prep_* flags in async_tx structure
      ARM: PL08x: shrink srcbus/dstbus in txd structure
      ...

commit 98d530fe246b65fbd3cdeeeca319a80c46cb4793
Author: Russell King - ARM Linux <linux@arm.linux.org.uk>
Date:   Sat Jan 1 23:00:23 2011 +0000

    Fix dmaengine_submit() return type
    
    desc->tx_submit's return type is dma_cookie_t, not int.  Therefore,
    dmaengine_submit() should match this return type as it's just
    wrapping this detail.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9d8688b92d8b..830935b7c49c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -532,7 +532,7 @@ static inline int dmaengine_resume(struct dma_chan *chan)
 	return dmaengine_device_control(chan, DMA_RESUME, 0);
 }
 
-static inline int dmaengine_submit(struct dma_async_tx_descriptor *desc)
+static inline dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc)
 {
 	return desc->tx_submit(desc);
 }

commit 8f33d5277fada0291ea495f7fd44a3e7b7aa41d3
Author: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
Date:   Wed Dec 22 14:46:46 2010 +0100

    dmaengine: provide dummy functions for DMA_ENGINE=n
    
    This lets drivers, optionally using the dmaengine, build with DMA_ENGINE
    unselected.
    
    Signed-off-by: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9d8688b92d8b..8cd00ad98d37 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -824,6 +824,8 @@ enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 #ifdef CONFIG_DMA_ENGINE
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void dma_issue_pending_all(void);
+struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
+void dma_release_channel(struct dma_chan *chan);
 #else
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
 {
@@ -831,7 +833,14 @@ static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descript
 }
 static inline void dma_issue_pending_all(void)
 {
-	do { } while (0);
+}
+static inline struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask,
+					      dma_filter_fn fn, void *fn_param)
+{
+	return NULL;
+}
+static inline void dma_release_channel(struct dma_chan *chan)
+{
 }
 #endif
 
@@ -842,8 +851,6 @@ void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
-struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
-void dma_release_channel(struct dma_chan *chan);
 
 /* --- Helper iov-locking functions --- */
 

commit 5fc6d897fde352bad5db5767e7260741a8cdd9e9
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Oct 7 16:44:50 2010 -0700

    async_tx: make async_tx channel switching opt-in
    
    The majority of drivers in drivers/dma/ will never establish cross
    channel operation chains and do not need the extra overhead in struct
    dma_async_tx_descriptor.  Make channel switching opt-in by default.
    
    Cc: Anatolij Gustschin <agust@denx.de>
    Cc: Ira Snyder <iws@ovro.caltech.edu>
    Cc: Linus Walleij <linus.walleij@stericsson.com>
    Cc: Saeed Bishara <saeed@marvell.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3934ebdd85c2..9d8688b92d8b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -321,14 +321,14 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
 	void *callback_param;
-#ifndef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
+#ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 	struct dma_async_tx_descriptor *next;
 	struct dma_async_tx_descriptor *parent;
 	spinlock_t lock;
 #endif
 };
 
-#ifdef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
+#ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 static inline void txd_lock(struct dma_async_tx_descriptor *txd)
 {
 }
@@ -656,11 +656,11 @@ static inline void net_dmaengine_put(void)
 #ifdef CONFIG_ASYNC_TX_DMA
 #define async_dmaengine_get()	dmaengine_get()
 #define async_dmaengine_put()	dmaengine_put()
-#ifdef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
+#ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
 #define async_dma_find_channel(type) dma_find_channel(DMA_ASYNC_TX)
 #else
 #define async_dma_find_channel(type) dma_find_channel(type)
-#endif /* CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH */
+#endif /* CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH */
 #else
 static inline void async_dmaengine_get(void)
 {

commit 6391987d6f8ced7d0fafaa1440dcc57bb4b34d8f
Merge: 9646b7985e90 e8689e63d4d2 0d688662aab9 1f1846c6ceed 20dd63900d23
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Oct 7 15:19:01 2010 -0700

    Merge branches 'dma40', 'pl08x', 'fsldma', 'imx' and 'intel-mid' into dmaengine

commit 968f19ae802fdc6b6b6b5af6fe79cf23d281be0f
Author: Ira Snyder <iws@ovro.caltech.edu>
Date:   Thu Sep 30 11:46:46 2010 +0000

    fsldma: improved DMA_SLAVE support
    
    Now that the generic DMAEngine API has support for scatterlist to
    scatterlist copying, the device_prep_slave_sg() portion of the
    DMA_SLAVE API is no longer necessary and has been removed.
    
    However, the device_control() portion of the DMA_SLAVE API is still
    useful to control device specific parameters, such as externally
    controlled DMA transfers and maximum burst length.
    
    A special dma_ctrl_cmd has been added to enable externally controlled
    DMA transfers. This is currently specific to the Freescale DMA
    controller, but can easily be made generic when another user is found.
    
    Signed-off-by: Ira W. Snyder <iws@ovro.caltech.edu>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2c9ee98f6c77..885f35211675 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -120,12 +120,15 @@ enum dma_ctrl_flags {
  * configuration data in statically from the platform). An additional
  * argument of struct dma_slave_config must be passed in with this
  * command.
+ * @FSLDMA_EXTERNAL_START: this command will put the Freescale DMA controller
+ * into external start mode.
  */
 enum dma_ctrl_cmd {
 	DMA_TERMINATE_ALL,
 	DMA_PAUSE,
 	DMA_RESUME,
 	DMA_SLAVE_CONFIG,
+	FSLDMA_EXTERNAL_START,
 };
 
 /**

commit a86ee03ce6f279ebe581a7a8c0c4393eaeb789ee
Author: Ira Snyder <iws@ovro.caltech.edu>
Date:   Thu Sep 30 11:46:44 2010 +0000

    dma: add support for scatterlist to scatterlist copy
    
    This adds support for scatterlist to scatterlist DMA transfers. A
    similar interface is exposed by the fsldma driver (through the DMA_SLAVE
    API) and by the ste_dma40 driver (through an exported function).
    
    This patch paves the way for making this type of copy operation a part
    of the generic DMAEngine API. Futher patches will add support in
    individual drivers.
    
    Signed-off-by: Ira W. Snyder <iws@ovro.caltech.edu>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index e2106495cc11..2c9ee98f6c77 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -64,6 +64,7 @@ enum dma_transaction_type {
 	DMA_PQ_VAL,
 	DMA_MEMSET,
 	DMA_INTERRUPT,
+	DMA_SG,
 	DMA_PRIVATE,
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
@@ -473,6 +474,11 @@ struct dma_device {
 		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(
+		struct dma_chan *chan,
+		struct scatterlist *dst_sg, unsigned int dst_nents,
+		struct scatterlist *src_sg, unsigned int src_nents,
+		unsigned long flags);
 
 	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(
 		struct dma_chan *chan, struct scatterlist *sgl,

commit 6e3ecaf0ad49de0bed829d409a164e7107c02993
Author: Sascha Hauer <s.hauer@pengutronix.de>
Date:   Thu Sep 30 13:56:33 2010 +0000

    dmaengine: add wrapper functions for device control functions
    
    Add wrapper functions around the dma_device->device_control function
    to bring back type safety. Also, add a wrapper function around
    dma_async_tx_descriptor->tx_submit. This is named dmaengine_submit
    instead of dmaengine_tx_submit to get rid of the confusing 'tx' in the
    function name
    
    Signed-off-by: Sascha Hauer <s.hauer@pengutronix.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 32cd84b47478..2218fdcbe8a9 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -494,6 +494,40 @@ struct dma_device {
 	void (*device_issue_pending)(struct dma_chan *chan);
 };
 
+static inline int dmaengine_device_control(struct dma_chan *chan,
+					   enum dma_ctrl_cmd cmd,
+					   unsigned long arg)
+{
+	return chan->device->device_control(chan, cmd, arg);
+}
+
+static inline int dmaengine_slave_config(struct dma_chan *chan,
+					  struct dma_slave_config *config)
+{
+	return dmaengine_device_control(chan, DMA_SLAVE_CONFIG,
+			(unsigned long)config);
+}
+
+static inline int dmaengine_terminate_all(struct dma_chan *chan)
+{
+	return dmaengine_device_control(chan, DMA_TERMINATE_ALL, 0);
+}
+
+static inline int dmaengine_pause(struct dma_chan *chan)
+{
+	return dmaengine_device_control(chan, DMA_PAUSE, 0);
+}
+
+static inline int dmaengine_resume(struct dma_chan *chan)
+{
+	return dmaengine_device_control(chan, DMA_RESUME, 0);
+}
+
+static inline int dmaengine_submit(struct dma_async_tx_descriptor *desc)
+{
+	return desc->tx_submit(desc);
+}
+
 static inline bool dmaengine_check_align(u8 align, size_t off1, size_t off2, size_t len)
 {
 	size_t mask;

commit 782bc950d84e404422ba21008fd51ee894c8d231
Author: Sascha Hauer <s.hauer@pengutronix.de>
Date:   Thu Sep 30 13:56:32 2010 +0000

    dmaengine: add possibility for cyclic transfers
    
    Cyclic transfers are useful for audio where a single buffer divided
    in periods has to be transfered endlessly until stopped. After being
    prepared the transfer is started using the dma_async_descriptor->tx_submit
    function. dma_async_descriptor->callback is called after each period.
    The transfer is stopped using the DMA_TERMINATE_ALL callback.
    While being used for cyclic transfers the channel cannot be used
    for other transfer types.
    
    Signed-off-by: Sascha Hauer <s.hauer@pengutronix.de>
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c61d4ca27bcc..32cd84b47478 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -67,10 +67,11 @@ enum dma_transaction_type {
 	DMA_PRIVATE,
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
+	DMA_CYCLIC,
 };
 
 /* last transaction type for creation of the capabilities mask */
-#define DMA_TX_TYPE_END (DMA_SLAVE + 1)
+#define DMA_TX_TYPE_END (DMA_CYCLIC + 1)
 
 
 /**
@@ -422,6 +423,9 @@ struct dma_tx_state {
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
+ * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
+ *	The function takes a buffer of size buf_len. The callback function will
+ *	be called after period_len bytes have been transferred.
  * @device_control: manipulate all pending operations on a channel, returns
  *	zero or error code
  * @device_tx_status: poll for transaction completion, the optional
@@ -478,6 +482,9 @@ struct dma_device {
 		struct dma_chan *chan, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_data_direction direction,
 		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(
+		struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,
+		size_t period_len, enum dma_data_direction direction);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
 

commit d3f3cf859db17cc5f8156c5bfcd032413e44483b
Author: Mathieu Lacage <mathieu.lacage@sophia.inria.fr>
Date:   Sat Aug 14 15:02:44 2010 +0200

    missing inline keyword for static function in linux/dmaengine.h
    
    Add a missing inline keyword for static function in linux/dmaengine.h to
    avoid duplicate symbol definitions.
    
    Signed-off-by: Mathieu Lacage <mathieu.lacage@sophia.inria.fr>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c61d4ca27bcc..e2106495cc11 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -548,7 +548,7 @@ static inline bool dma_dev_has_pq_continue(struct dma_device *dma)
 	return (dma->max_pq & DMA_HAS_PQ_CONTINUE) == DMA_HAS_PQ_CONTINUE;
 }
 
-static unsigned short dma_dev_to_maxpq(struct dma_device *dma)
+static inline unsigned short dma_dev_to_maxpq(struct dma_device *dma)
 {
 	return dma->max_pq & ~DMA_HAS_PQ_CONTINUE;
 }

commit c156d0a5b0c667999e06d0bb52e3d1376faec8bf
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Wed Aug 4 13:37:33 2010 +0200

    DMAENGINE: generic slave channel control v3
    
    This adds an interface to the DMAengine to make it possible to
    reconfigure a slave channel at runtime. We add a few foreseen
    config parameters to the passed struct, with a void * pointer
    for custom per-device or per-platform runtime slave data.
    
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 5204f018931b..c61d4ca27bcc 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -114,11 +114,17 @@ enum dma_ctrl_flags {
  * @DMA_TERMINATE_ALL: terminate all ongoing transfers
  * @DMA_PAUSE: pause ongoing transfers
  * @DMA_RESUME: resume paused transfer
+ * @DMA_SLAVE_CONFIG: this command is only implemented by DMA controllers
+ * that need to runtime reconfigure the slave channels (as opposed to passing
+ * configuration data in statically from the platform). An additional
+ * argument of struct dma_slave_config must be passed in with this
+ * command.
  */
 enum dma_ctrl_cmd {
 	DMA_TERMINATE_ALL,
 	DMA_PAUSE,
 	DMA_RESUME,
+	DMA_SLAVE_CONFIG,
 };
 
 /**
@@ -199,6 +205,71 @@ struct dma_chan_dev {
 	atomic_t *idr_ref;
 };
 
+/**
+ * enum dma_slave_buswidth - defines bus with of the DMA slave
+ * device, source or target buses
+ */
+enum dma_slave_buswidth {
+	DMA_SLAVE_BUSWIDTH_UNDEFINED = 0,
+	DMA_SLAVE_BUSWIDTH_1_BYTE = 1,
+	DMA_SLAVE_BUSWIDTH_2_BYTES = 2,
+	DMA_SLAVE_BUSWIDTH_4_BYTES = 4,
+	DMA_SLAVE_BUSWIDTH_8_BYTES = 8,
+};
+
+/**
+ * struct dma_slave_config - dma slave channel runtime config
+ * @direction: whether the data shall go in or out on this slave
+ * channel, right now. DMA_TO_DEVICE and DMA_FROM_DEVICE are
+ * legal values, DMA_BIDIRECTIONAL is not acceptable since we
+ * need to differentiate source and target addresses.
+ * @src_addr: this is the physical address where DMA slave data
+ * should be read (RX), if the source is memory this argument is
+ * ignored.
+ * @dst_addr: this is the physical address where DMA slave data
+ * should be written (TX), if the source is memory this argument
+ * is ignored.
+ * @src_addr_width: this is the width in bytes of the source (RX)
+ * register where DMA data shall be read. If the source
+ * is memory this may be ignored depending on architecture.
+ * Legal values: 1, 2, 4, 8.
+ * @dst_addr_width: same as src_addr_width but for destination
+ * target (TX) mutatis mutandis.
+ * @src_maxburst: the maximum number of words (note: words, as in
+ * units of the src_addr_width member, not bytes) that can be sent
+ * in one burst to the device. Typically something like half the
+ * FIFO depth on I/O peripherals so you don't overflow it. This
+ * may or may not be applicable on memory sources.
+ * @dst_maxburst: same as src_maxburst but for destination target
+ * mutatis mutandis.
+ *
+ * This struct is passed in as configuration data to a DMA engine
+ * in order to set up a certain channel for DMA transport at runtime.
+ * The DMA device/engine has to provide support for an additional
+ * command in the channel config interface, DMA_SLAVE_CONFIG
+ * and this struct will then be passed in as an argument to the
+ * DMA engine device_control() function.
+ *
+ * The rationale for adding configuration information to this struct
+ * is as follows: if it is likely that most DMA slave controllers in
+ * the world will support the configuration option, then make it
+ * generic. If not: if it is fixed so that it be sent in static from
+ * the platform data, then prefer to do that. Else, if it is neither
+ * fixed at runtime, nor generic enough (such as bus mastership on
+ * some CPU family and whatnot) then create a custom slave config
+ * struct and pass that, then make this config a member of that
+ * struct, if applicable.
+ */
+struct dma_slave_config {
+	enum dma_data_direction direction;
+	dma_addr_t src_addr;
+	dma_addr_t dst_addr;
+	enum dma_slave_buswidth src_addr_width;
+	enum dma_slave_buswidth dst_addr_width;
+	u32 src_maxburst;
+	u32 dst_maxburst;
+};
+
 static inline const char *dma_chan_name(struct dma_chan *chan)
 {
 	return dev_name(&chan->dev->device);

commit 0b28330e39bbe0ffee4c56b09fc415fcec595ea3
Merge: 058276303dbc caa20d974c86
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon May 17 16:30:58 2010 -0700

    Merge branch 'ioat' into dmaengine

commit 058276303dbc4ed089c1f7dad0871810b1f5ddf1
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Mon May 17 16:30:42 2010 -0700

    DMAENGINE: extend the control command to include an arg
    
    This adds an argument to the DMAengine control function, so that
    we can later provide control commands that need some external data
    passed in through an argument akin to the ioctl() operation
    prototype.
    
    [dan.j.williams@intel.com: fix up some missed conversions]
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 50b7b3e0d572..17456571ff7a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -347,7 +347,8 @@ struct dma_device {
 		struct dma_chan *chan, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_data_direction direction,
 		unsigned long flags);
-	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd);
+	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
+		unsigned long arg);
 
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,

commit caa20d974c86af496b419eef70010e63b7fab7ac
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon May 17 16:24:16 2010 -0700

    async_tx: trim dma_async_tx_descriptor in 'no channel switch' case
    
    Saves 24 bytes per descriptor (64-bit) when the channel-switching
    capabilities of async_tx are not required.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 20ea12c86fd0..cb234979fc6b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -230,11 +230,71 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
 	void *callback_param;
+#ifndef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
 	struct dma_async_tx_descriptor *next;
 	struct dma_async_tx_descriptor *parent;
 	spinlock_t lock;
+#endif
 };
 
+#ifdef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
+static inline void txd_lock(struct dma_async_tx_descriptor *txd)
+{
+}
+static inline void txd_unlock(struct dma_async_tx_descriptor *txd)
+{
+}
+static inline void txd_chain(struct dma_async_tx_descriptor *txd, struct dma_async_tx_descriptor *next)
+{
+	BUG();
+}
+static inline void txd_clear_parent(struct dma_async_tx_descriptor *txd)
+{
+}
+static inline void txd_clear_next(struct dma_async_tx_descriptor *txd)
+{
+}
+static inline struct dma_async_tx_descriptor *txd_next(struct dma_async_tx_descriptor *txd)
+{
+	return NULL;
+}
+static inline struct dma_async_tx_descriptor *txd_parent(struct dma_async_tx_descriptor *txd)
+{
+	return NULL;
+}
+
+#else
+static inline void txd_lock(struct dma_async_tx_descriptor *txd)
+{
+	spin_lock_bh(&txd->lock);
+}
+static inline void txd_unlock(struct dma_async_tx_descriptor *txd)
+{
+	spin_unlock_bh(&txd->lock);
+}
+static inline void txd_chain(struct dma_async_tx_descriptor *txd, struct dma_async_tx_descriptor *next)
+{
+	txd->next = next;
+	next->parent = txd;
+}
+static inline void txd_clear_parent(struct dma_async_tx_descriptor *txd)
+{
+	txd->parent = NULL;
+}
+static inline void txd_clear_next(struct dma_async_tx_descriptor *txd)
+{
+	txd->next = NULL;
+}
+static inline struct dma_async_tx_descriptor *txd_parent(struct dma_async_tx_descriptor *txd)
+{
+	return txd->parent;
+}
+static inline struct dma_async_tx_descriptor *txd_next(struct dma_async_tx_descriptor *txd)
+{
+	return txd->next;
+}
+#endif
+
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported

commit bca3469205402d9fb14060d255d8786ae2256640
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Mar 26 16:52:10 2010 -0700

    dmaengine: provide helper for setting txstate
    
    Simple conditional struct filler to cut out some duplicated code.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 55b08e84ac8d..50b7b3e0d572 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -628,6 +628,16 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 	return DMA_IN_PROGRESS;
 }
 
+static inline void
+dma_set_tx_state(struct dma_tx_state *st, dma_cookie_t last, dma_cookie_t used, u32 residue)
+{
+	if (st) {
+		st->last = last;
+		st->used = used;
+		st->residue = residue;
+	}
+}
+
 enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 #ifdef CONFIG_DMA_ENGINE
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);

commit 0793448187643b50af89d36b08470baf45a3cab4
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Fri Mar 26 16:50:49 2010 -0700

    DMAENGINE: generic channel status v2
    
    Convert the device_is_tx_complete() operation on the
    DMA engine to a generic device_tx_status()operation which
    can return three states, DMA_TX_RUNNING, DMA_TX_COMPLETE,
    DMA_TX_PAUSED.
    
    [dan.j.williams@intel.com: update for timberdale]
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Acked-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Cc: Nicolas Ferre <nicolas.ferre@atmel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Cc: Magnus Damm <damm@opensource.se>
    Cc: Liam Girdwood <lrg@slimlogic.co.uk>
    Cc: Joe Perches <joe@perches.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 0731802f876f..55b08e84ac8d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -40,11 +40,13 @@ typedef s32 dma_cookie_t;
  * enum dma_status - DMA transaction status
  * @DMA_SUCCESS: transaction completed successfully
  * @DMA_IN_PROGRESS: transaction not yet processed
+ * @DMA_PAUSED: transaction is paused
  * @DMA_ERROR: transaction failed
  */
 enum dma_status {
 	DMA_SUCCESS,
 	DMA_IN_PROGRESS,
+	DMA_PAUSED,
 	DMA_ERROR,
 };
 
@@ -248,6 +250,21 @@ struct dma_async_tx_descriptor {
 	spinlock_t lock;
 };
 
+/**
+ * struct dma_tx_state - filled in to report the status of
+ * a transfer.
+ * @last: last completed DMA cookie
+ * @used: last issued DMA cookie (i.e. the one in progress)
+ * @residue: the remaining number of bytes left to transmit
+ *	on the selected transfer for states DMA_IN_PROGRESS and
+ *	DMA_PAUSED if this is implemented in the driver, else 0
+ */
+struct dma_tx_state {
+	dma_cookie_t last;
+	dma_cookie_t used;
+	u32 residue;
+};
+
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported
@@ -276,7 +293,10 @@ struct dma_async_tx_descriptor {
  * @device_prep_slave_sg: prepares a slave dma operation
  * @device_control: manipulate all pending operations on a channel, returns
  *	zero or error code
- * @device_is_tx_complete: poll for transaction completion
+ * @device_tx_status: poll for transaction completion, the optional
+ *	txstate parameter can be supplied with a pointer to get a
+ *	struct with auxilary transfer status information, otherwise the call
+ *	will just return a simple status code
  * @device_issue_pending: push pending transactions to hardware
  */
 struct dma_device {
@@ -329,9 +349,9 @@ struct dma_device {
 		unsigned long flags);
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd);
 
-	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
-			dma_cookie_t cookie, dma_cookie_t *last,
-			dma_cookie_t *used);
+	enum dma_status (*device_tx_status)(struct dma_chan *chan,
+					    dma_cookie_t cookie,
+					    struct dma_tx_state *txstate);
 	void (*device_issue_pending)(struct dma_chan *chan);
 };
 
@@ -572,7 +592,15 @@ static inline void dma_async_issue_pending(struct dma_chan *chan)
 static inline enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,
 	dma_cookie_t cookie, dma_cookie_t *last, dma_cookie_t *used)
 {
-	return chan->device->device_is_tx_complete(chan, cookie, last, used);
+	struct dma_tx_state state;
+	enum dma_status status;
+
+	status = chan->device->device_tx_status(chan, cookie, &state);
+	if (last)
+		*last = state.last;
+	if (used)
+		*used = state.used;
+	return status;
 }
 
 #define dma_async_memcpy_complete(chan, cookie, last, used)\

commit c3635c78e500a52c9fcd55de381a72928d9e054d
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Fri Mar 26 16:44:01 2010 -0700

    DMAENGINE: generic slave control v2
    
    Convert the device_terminate_all() operation on the
    DMA engine to a generic device_control() operation
    which can now optionally support also pausing and
    resuming DMA on a certain channel. Implemented for the
    COH 901 318 DMAC as an example.
    
    [dan.j.williams@intel.com: update for timberdale]
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Acked-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Cc: Nicolas Ferre <nicolas.ferre@atmel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Cc: Magnus Damm <damm@opensource.se>
    Cc: Liam Girdwood <lrg@slimlogic.co.uk>
    Cc: Joe Perches <joe@perches.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 20ea12c86fd0..0731802f876f 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -106,6 +106,19 @@ enum dma_ctrl_flags {
 	DMA_PREP_FENCE = (1 << 9),
 };
 
+/**
+ * enum dma_ctrl_cmd - DMA operations that can optionally be exercised
+ * on a running channel.
+ * @DMA_TERMINATE_ALL: terminate all ongoing transfers
+ * @DMA_PAUSE: pause ongoing transfers
+ * @DMA_RESUME: resume paused transfer
+ */
+enum dma_ctrl_cmd {
+	DMA_TERMINATE_ALL,
+	DMA_PAUSE,
+	DMA_RESUME,
+};
+
 /**
  * enum sum_check_bits - bit position of pq_check_flags
  */
@@ -261,7 +274,8 @@ struct dma_async_tx_descriptor {
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
- * @device_terminate_all: terminate all pending operations
+ * @device_control: manipulate all pending operations on a channel, returns
+ *	zero or error code
  * @device_is_tx_complete: poll for transaction completion
  * @device_issue_pending: push pending transactions to hardware
  */
@@ -313,7 +327,7 @@ struct dma_device {
 		struct dma_chan *chan, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_data_direction direction,
 		unsigned long flags);
-	void (*device_terminate_all)(struct dma_chan *chan);
+	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd);
 
 	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
 			dma_cookie_t cookie, dma_cookie_t *last,

commit 9bb676966aa85e56af00b353387d3c274a26e480
Merge: 0f2cc4ecd81d dd58ffcf5a53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 4 08:20:14 2010 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx: (28 commits)
      ioat: cleanup ->timer_fn() and ->cleanup_fn() prototypes
      ioat3: interrupt coalescing
      ioat: close potential BUG_ON race in the descriptor cleanup path
      ioat2: kill pending flag
      ioat3: use ioat2_quiesce()
      ioat3: cleanup, don't enable DCA completion writes
      DMAENGINE: COH 901 318 lli sg offset fix
      DMAENGINE: COH 901 318 configure channel direction
      DMAENGINE: COH 901 318 remove irq counting
      DMAENGINE: COH 901 318 descriptor pool refactoring
      DMAENGINE: COH 901 318 cleanups
      dma: Add MPC512x DMA driver
      Debugging options for the DMA engine subsystem
      iop-adma: redundant/wrong tests in iop_*_count()?
      dmatest: fix handling of an even number of xor_sources
      dmatest: correct raid6 PQ test
      fsldma: Fix cookie issues
      fsldma: Fix cookie issues
      dma: cases IPU_PIX_FMT_BGRA32, BGR32 and ABGR32 are the same in ipu_ch_param_set_size()
      dma: make Open Firmware device id constant
      ...

commit 76bd061f5c7b7550cdaed68ad6219ea7cee288fc
Author: Steven J. Magnani <steve@digidescorp.com>
Date:   Sun Feb 28 22:18:16 2010 -0700

    fsldma: Fix cookie issues
    
    fsl_dma_update_completed_cookie() appears to calculate the last completed
    cookie incorrectly in the corner case where DMA on cookie 1 is in progress
    just following a cookie wrap.
    
    Signed-off-by: Steven J. Magnani <steve@digidescorp.com>
    Acked-by: Ira W. Snyder <iws@ovro.caltech.edu>
    [dan.j.williams@intel.com: fix an integer overflow warning with INT_MAX]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 78784982b33e..4d8d619f28bc 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -31,6 +31,8 @@
  * if dma_cookie_t is >0 it's a DMA request cookie, <0 it's an error code
  */
 typedef s32 dma_cookie_t;
+#define DMA_MIN_COOKIE	1
+#define DMA_MAX_COOKIE	INT_MAX
 
 #define dma_submit_error(cookie) ((cookie) < 0 ? 1 : 0)
 

commit a29d8b8e2d811a24bbe49215a0f0c536b72ebc18
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 2 14:39:15 2010 +0900

    percpu: add __percpu sparse annotations to what's left
    
    Add __percpu sparse annotations to places which didn't make it in one
    of the previous patches.  All converions are trivial.
    
    These annotations are to make sparse consider percpu variables to be
    in a different address space and warn if accessed without going
    through percpu accessors.  This patch doesn't affect normal builds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Neil Brown <neilb@suse.de>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 78784982b33e..21fd9b7c6a40 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -162,7 +162,7 @@ struct dma_chan {
 	struct dma_chan_dev *dev;
 
 	struct list_head device_node;
-	struct dma_chan_percpu *local;
+	struct dma_chan_percpu __percpu *local;
 	int client_count;
 	int table_count;
 	void *private;

commit a88f6667078412e5eff37ead68a043ee0ec9f1da
Author: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
Date:   Thu Dec 10 18:35:15 2009 +0100

    dmaengine: clarify the meaning of the DMA_CTRL_ACK flag
    
    DMA_CTRL_ACK's description applies to its clear state, not to its set state.
    
    Signed-off-by: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2b9f2ac7ed60..78784982b33e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -74,7 +74,7 @@ enum dma_transaction_type {
  *  control completion, and communicate status.
  * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
  *  this transaction
- * @DMA_CTRL_ACK - the descriptor cannot be reused until the client
+ * @DMA_CTRL_ACK - if clear, the descriptor cannot be reused until the client
  *  acknowledges receipt, i.e. has has a chance to establish any dependency
  *  chains
  * @DMA_COMPL_SKIP_SRC_UNMAP - set to disable dma-unmapping the source buffer(s)

commit bbb20089a3275a19e475dbc21320c3742e3ca423
Merge: 3e48e656903e 657a77fa7284
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:55:21 2009 -0700

    Merge branch 'dmaengine' into async-tx-next
    
    Conflicts:
            crypto/async_tx/async_xor.c
            drivers/dma/ioat/dma_v2.h
            drivers/dma/ioat/pci.c
            drivers/md/raid5.c

commit 0803172778901e24a75ab074798d98c2b7411559
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:53:04 2009 -0700

    dmaengine: kill tx_list
    
    The tx_list attribute of struct dma_async_tx_descriptor is common to
    most, but not all dma driver implementations.  None of the upper level
    code (dmaengine/async_tx) uses it, so allow drivers to implement it
    locally if they need it.  This saves sizeof(struct list_head) bytes for
    drivers that do not manage descriptors with a linked list (e.g.: ioatdma
    v2,3).
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ffefba81c818..f114bc7790bc 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -180,8 +180,6 @@ typedef void (*dma_async_tx_callback)(void *dma_async_param);
  * @flags: flags to augment operation preparation, control completion, and
  * 	communicate status
  * @phys: physical address of the descriptor
- * @tx_list: driver common field for operations that require multiple
- *	descriptors
  * @chan: target channel for this operation
  * @tx_submit: set the prepared descriptor(s) to be executed by the engine
  * @callback: routine to call after this operation is complete
@@ -195,7 +193,6 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t cookie;
 	enum dma_ctrl_flags flags; /* not a 'long' to pack with cookie */
 	dma_addr_t phys;
-	struct list_head tx_list;
 	struct dma_chan *chan;
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;

commit 83544ae9f3991bfc7d5e0fe9a3008cd05a8d57b7
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:53 2009 -0700

    dmaengine, async_tx: support alignment checks
    
    Some engines have transfer size and address alignment restrictions.  Add
    a per-operation alignment property to struct dma_device that the async
    routines and dmatest can use to check alignment capabilities.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index db23fd583f98..835b9c7bf1c2 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -245,6 +245,10 @@ struct dma_async_tx_descriptor {
  * @cap_mask: one or more dma_capability flags
  * @max_xor: maximum number of xor sources, 0 if no capability
  * @max_pq: maximum number of PQ sources and PQ-continue capability
+ * @copy_align: alignment shift for memcpy operations
+ * @xor_align: alignment shift for xor operations
+ * @pq_align: alignment shift for pq operations
+ * @fill_align: alignment shift for memset operations
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @device_alloc_chan_resources: allocate resources and return the
@@ -271,6 +275,10 @@ struct dma_device {
 	dma_cap_mask_t  cap_mask;
 	unsigned short max_xor;
 	unsigned short max_pq;
+	u8 copy_align;
+	u8 xor_align;
+	u8 pq_align;
+	u8 fill_align;
 	#define DMA_HAS_PQ_CONTINUE (1 << 15)
 
 	int dev_id;
@@ -314,6 +322,42 @@ struct dma_device {
 	void (*device_issue_pending)(struct dma_chan *chan);
 };
 
+static inline bool dmaengine_check_align(u8 align, size_t off1, size_t off2, size_t len)
+{
+	size_t mask;
+
+	if (!align)
+		return true;
+	mask = (1 << align) - 1;
+	if (mask & (off1 | off2 | len))
+		return false;
+	return true;
+}
+
+static inline bool is_dma_copy_aligned(struct dma_device *dev, size_t off1,
+				       size_t off2, size_t len)
+{
+	return dmaengine_check_align(dev->copy_align, off1, off2, len);
+}
+
+static inline bool is_dma_xor_aligned(struct dma_device *dev, size_t off1,
+				      size_t off2, size_t len)
+{
+	return dmaengine_check_align(dev->xor_align, off1, off2, len);
+}
+
+static inline bool is_dma_pq_aligned(struct dma_device *dev, size_t off1,
+				     size_t off2, size_t len)
+{
+	return dmaengine_check_align(dev->pq_align, off1, off2, len);
+}
+
+static inline bool is_dma_fill_aligned(struct dma_device *dev, size_t off1,
+				       size_t off2, size_t len)
+{
+	return dmaengine_check_align(dev->fill_align, off1, off2, len);
+}
+
 static inline void
 dma_set_maxpq(struct dma_device *dma, int maxpq, int has_pq_continue)
 {

commit 9308add6ea4fedeba37b0d7c4630a542bd34f214
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:52 2009 -0700

    dmaengine: cleanup unused transaction types
    
    No drivers currently implement these operation types, so they can be
    deleted.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 86853ed7970b..db23fd583f98 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -56,12 +56,9 @@ enum dma_transaction_type {
 	DMA_MEMCPY,
 	DMA_XOR,
 	DMA_PQ,
-	DMA_DUAL_XOR,
-	DMA_PQ_UPDATE,
 	DMA_XOR_VAL,
 	DMA_PQ_VAL,
 	DMA_MEMSET,
-	DMA_MEMCPY_CRC32C,
 	DMA_INTERRUPT,
 	DMA_PRIVATE,
 	DMA_ASYNC_TX,

commit 138f4c359d23d2ec38d18bd70dd9613ae515fe93
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:51 2009 -0700

    dmaengine, async_tx: add a "no channel switch" allocator
    
    Channel switching is problematic for some dmaengine drivers as the
    architecture precludes separating the ->prep from ->submit.  In these
    cases the driver can select ASYNC_TX_DISABLE_CHANNEL_SWITCH to modify
    the async_tx allocator to only return channels that support all of the
    required asynchronous operations.
    
    For example MD_RAID456=y selects support for asynchronous xor, xor
    validate, pq, pq validate, and memcpy.  When
    ASYNC_TX_DISABLE_CHANNEL_SWITCH=y any channel with all these
    capabilities is marked DMA_ASYNC_TX allowing async_tx_find_channel() to
    quickly locate compatible channels with the guarantee that dependency
    chains will remain on one channel.  When
    ASYNC_TX_DISABLE_CHANNEL_SWITCH=n async_tx_find_channel() may select
    channels that lead to operation chains that need to cross channel
    boundaries using the async_tx channel switch capability.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 4d6c1c925fd4..86853ed7970b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -48,6 +48,9 @@ enum dma_status {
 
 /**
  * enum dma_transaction_type - DMA transaction types/indexes
+ *
+ * Note: The DMA_ASYNC_TX capability is not to be set by drivers.  It is
+ * automatically set as dma devices are registered.
  */
 enum dma_transaction_type {
 	DMA_MEMCPY,
@@ -61,6 +64,7 @@ enum dma_transaction_type {
 	DMA_MEMCPY_CRC32C,
 	DMA_INTERRUPT,
 	DMA_PRIVATE,
+	DMA_ASYNC_TX,
 	DMA_SLAVE,
 };
 
@@ -396,7 +400,11 @@ static inline void net_dmaengine_put(void)
 #ifdef CONFIG_ASYNC_TX_DMA
 #define async_dmaengine_get()	dmaengine_get()
 #define async_dmaengine_put()	dmaengine_put()
+#ifdef CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH
+#define async_dma_find_channel(type) dma_find_channel(DMA_ASYNC_TX)
+#else
 #define async_dma_find_channel(type) dma_find_channel(type)
+#endif /* CONFIG_ASYNC_TX_DISABLE_CHANNEL_SWITCH */
 #else
 static inline void async_dmaengine_get(void)
 {
@@ -409,7 +417,7 @@ async_dma_find_channel(enum dma_transaction_type type)
 {
 	return NULL;
 }
-#endif
+#endif /* CONFIG_ASYNC_TX_DMA */
 
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);

commit 0403e3827788d878163f9ef0541b748b0f88ca5d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:50 2009 -0700

    dmaengine: add fence support
    
    Some engines optimize operation by reading ahead in the descriptor chain
    such that descriptor2 may start execution before descriptor1 completes.
    If descriptor2 depends on the result from descriptor1 then a fence is
    required (on descriptor2) to disable this optimization.  The async_tx
    api could implicitly identify dependencies via the 'depend_tx'
    parameter, but that would constrain cases where the dependency chain
    only specifies a completion order rather than a data dependency.  So,
    provide an ASYNC_TX_FENCE to explicitly identify data dependencies.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1012f1abcb54..4d6c1c925fd4 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -87,6 +87,8 @@ enum dma_transaction_type {
  * @DMA_PREP_CONTINUE - indicate to a driver that it is reusing buffers as
  *  sources that were the result of a previous operation, in the case of a PQ
  *  operation it continues the calculation with new sources
+ * @DMA_PREP_FENCE - tell the driver that subsequent operations depend
+ *  on the result of this operation
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
@@ -98,6 +100,7 @@ enum dma_ctrl_flags {
 	DMA_PREP_PQ_DISABLE_P = (1 << 6),
 	DMA_PREP_PQ_DISABLE_Q = (1 << 7),
 	DMA_PREP_CONTINUE = (1 << 8),
+	DMA_PREP_FENCE = (1 << 9),
 };
 
 /**

commit f9dd2134374c8de6b911e2b8652c6c9622eaa658
Merge: 4b652f0db3be 07a3b417dc3d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:29 2009 -0700

    Merge branch 'md-raid6-accel' into ioat3.2
    
    Conflicts:
            include/linux/dmaengine.h

commit b2f46fd8ef3dff2ab30f31126833f78b7480283a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 12:20:36 2009 -0700

    async_tx: add support for asynchronous GF multiplication
    
    [ Based on an original patch by Yuri Tikhonov ]
    
    This adds support for doing asynchronous GF multiplication by adding
    two additional functions to the async_tx API:
    
     async_gen_syndrome() does simultaneous XOR and Galois field
        multiplication of sources.
    
     async_syndrome_val() validates the given source buffers against known P
        and Q values.
    
    When a request is made to run async_pq against more than the hardware
    maximum number of supported sources we need to reuse the previous
    generated P and Q values as sources into the next operation.  Care must
    be taken to remove Q from P' and P from Q'.  For example to perform a 5
    source pq op with hardware that only supports 4 sources at a time the
    following approach is taken:
    
    p, q = PQ(src0, src1, src2, src3, COEF({01}, {02}, {04}, {08}))
    p', q' = PQ(p, q, q, src4, COEF({00}, {01}, {00}, {10}))
    
    p' = p + q + q + src4 = p + src4
    q' = {00}*p + {01}*q + {00}*q + {10}*src4 = q + {10}*src4
    
    Note: 4 is the minimum acceptable maxpq otherwise we punt to
    synchronous-software path.
    
    The DMA_PREP_CONTINUE flag indicates to the driver to reuse p and q as
    sources (in the above manner) and fill the remaining slots up to maxpq
    with the new sources/coefficients.
    
    Note1: Some devices have native support for P+Q continuation and can skip
    this extra work.  Devices with this capability can advertise it with
    dma_set_maxpq.  It is up to each driver how to handle the
    DMA_PREP_CONTINUE flag.
    
    Note2: The api supports disabling the generation of P when generating Q,
    this is ignored by the synchronous path but is implemented by some dma
    devices to save unnecessary writes.  In this case the continuation
    algorithm is simplified to only reuse Q as a source.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 02447afcebad..ce010cd991d2 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -52,7 +52,7 @@ enum dma_status {
 enum dma_transaction_type {
 	DMA_MEMCPY,
 	DMA_XOR,
-	DMA_PQ_XOR,
+	DMA_PQ,
 	DMA_DUAL_XOR,
 	DMA_PQ_UPDATE,
 	DMA_XOR_VAL,
@@ -70,20 +70,28 @@ enum dma_transaction_type {
 
 /**
  * enum dma_ctrl_flags - DMA flags to augment operation preparation,
- * 	control completion, and communicate status.
+ *  control completion, and communicate status.
  * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
- * 	this transaction
+ *  this transaction
  * @DMA_CTRL_ACK - the descriptor cannot be reused until the client
- * 	acknowledges receipt, i.e. has has a chance to establish any
- * 	dependency chains
+ *  acknowledges receipt, i.e. has has a chance to establish any dependency
+ *  chains
  * @DMA_COMPL_SKIP_SRC_UNMAP - set to disable dma-unmapping the source buffer(s)
  * @DMA_COMPL_SKIP_DEST_UNMAP - set to disable dma-unmapping the destination(s)
+ * @DMA_PREP_PQ_DISABLE_P - prevent generation of P while generating Q
+ * @DMA_PREP_PQ_DISABLE_Q - prevent generation of Q while generating P
+ * @DMA_PREP_CONTINUE - indicate to a driver that it is reusing buffers as
+ *  sources that were the result of a previous operation, in the case of a PQ
+ *  operation it continues the calculation with new sources
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
 	DMA_CTRL_ACK = (1 << 1),
 	DMA_COMPL_SKIP_SRC_UNMAP = (1 << 2),
 	DMA_COMPL_SKIP_DEST_UNMAP = (1 << 3),
+	DMA_PREP_PQ_DISABLE_P = (1 << 4),
+	DMA_PREP_PQ_DISABLE_Q = (1 << 5),
+	DMA_PREP_CONTINUE = (1 << 6),
 };
 
 /**
@@ -226,6 +234,7 @@ struct dma_async_tx_descriptor {
  * @global_node: list_head for global dma_device_list
  * @cap_mask: one or more dma_capability flags
  * @max_xor: maximum number of xor sources, 0 if no capability
+ * @max_pq: maximum number of PQ sources and PQ-continue capability
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @device_alloc_chan_resources: allocate resources and return the
@@ -234,6 +243,8 @@ struct dma_async_tx_descriptor {
  * @device_prep_dma_memcpy: prepares a memcpy operation
  * @device_prep_dma_xor: prepares a xor operation
  * @device_prep_dma_xor_val: prepares a xor validation operation
+ * @device_prep_dma_pq: prepares a pq operation
+ * @device_prep_dma_pq_val: prepares a pqzero_sum operation
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
@@ -248,7 +259,9 @@ struct dma_device {
 	struct list_head channels;
 	struct list_head global_node;
 	dma_cap_mask_t  cap_mask;
-	int max_xor;
+	unsigned short max_xor;
+	unsigned short max_pq;
+	#define DMA_HAS_PQ_CONTINUE (1 << 15)
 
 	int dev_id;
 	struct device *dev;
@@ -265,6 +278,14 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(
 		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,
 		size_t len, enum sum_check_flags *result, unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_pq)(
+		struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf,
+		size_t len, unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_pq_val)(
+		struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf, size_t len,
+		enum sum_check_flags *pqres, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags);
@@ -283,6 +304,60 @@ struct dma_device {
 	void (*device_issue_pending)(struct dma_chan *chan);
 };
 
+static inline void
+dma_set_maxpq(struct dma_device *dma, int maxpq, int has_pq_continue)
+{
+	dma->max_pq = maxpq;
+	if (has_pq_continue)
+		dma->max_pq |= DMA_HAS_PQ_CONTINUE;
+}
+
+static inline bool dmaf_continue(enum dma_ctrl_flags flags)
+{
+	return (flags & DMA_PREP_CONTINUE) == DMA_PREP_CONTINUE;
+}
+
+static inline bool dmaf_p_disabled_continue(enum dma_ctrl_flags flags)
+{
+	enum dma_ctrl_flags mask = DMA_PREP_CONTINUE | DMA_PREP_PQ_DISABLE_P;
+
+	return (flags & mask) == mask;
+}
+
+static inline bool dma_dev_has_pq_continue(struct dma_device *dma)
+{
+	return (dma->max_pq & DMA_HAS_PQ_CONTINUE) == DMA_HAS_PQ_CONTINUE;
+}
+
+static unsigned short dma_dev_to_maxpq(struct dma_device *dma)
+{
+	return dma->max_pq & ~DMA_HAS_PQ_CONTINUE;
+}
+
+/* dma_maxpq - reduce maxpq in the face of continued operations
+ * @dma - dma device with PQ capability
+ * @flags - to check if DMA_PREP_CONTINUE and DMA_PREP_PQ_DISABLE_P are set
+ *
+ * When an engine does not support native continuation we need 3 extra
+ * source slots to reuse P and Q with the following coefficients:
+ * 1/ {00} * P : remove P from Q', but use it as a source for P'
+ * 2/ {01} * Q : use Q to continue Q' calculation
+ * 3/ {00} * Q : subtract Q from P' to cancel (2)
+ *
+ * In the case where P is disabled we only need 1 extra source:
+ * 1/ {01} * Q : use Q to continue Q' calculation
+ */
+static inline int dma_maxpq(struct dma_device *dma, enum dma_ctrl_flags flags)
+{
+	if (dma_dev_has_pq_continue(dma) || !dmaf_continue(flags))
+		return dma_dev_to_maxpq(dma);
+	else if (dmaf_p_disabled_continue(flags))
+		return dma_dev_to_maxpq(dma) - 1;
+	else if (dmaf_continue(flags))
+		return dma_dev_to_maxpq(dma) - 3;
+	BUG();
+}
+
 /* --- public DMA engine API --- */
 
 #ifdef CONFIG_DMA_ENGINE

commit ad283ea4a3ce82cda2efe33163748a397b31b1eb
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Aug 29 19:09:26 2009 -0700

    async_tx: add sum check flags
    
    Replace the flat zero_sum_result with a collection of flags to contain
    the P (xor) zero-sum result, and the soon to be utilized Q (raid6 reed
    solomon syndrome) zero-sum result.  Use the SUM_CHECK_ namespace instead
    of DMA_ since these flags will be used on non-dma-zero-sum enabled
    platforms.
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6768727d00d7..02447afcebad 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -86,6 +86,25 @@ enum dma_ctrl_flags {
 	DMA_COMPL_SKIP_DEST_UNMAP = (1 << 3),
 };
 
+/**
+ * enum sum_check_bits - bit position of pq_check_flags
+ */
+enum sum_check_bits {
+	SUM_CHECK_P = 0,
+	SUM_CHECK_Q = 1,
+};
+
+/**
+ * enum pq_check_flags - result of async_{xor,pq}_zero_sum operations
+ * @SUM_CHECK_P_RESULT - 1 if xor zero sum error, 0 otherwise
+ * @SUM_CHECK_Q_RESULT - 1 if reed-solomon zero sum error, 0 otherwise
+ */
+enum sum_check_flags {
+	SUM_CHECK_P_RESULT = (1 << SUM_CHECK_P),
+	SUM_CHECK_Q_RESULT = (1 << SUM_CHECK_Q),
+};
+
+
 /**
  * dma_cap_mask_t - capabilities bitmap modeled after cpumask_t.
  * See linux/cpumask.h
@@ -245,7 +264,7 @@ struct dma_device {
 		unsigned int src_cnt, size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(
 		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,
-		size_t len, u32 *result, unsigned long flags);
+		size_t len, enum sum_check_flags *result, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags);

commit 4f005dbe5584fe54c9f6d6d4f0acd3fb29be84da
Author: Maciej Sosnowski <maciej.sosnowski@intel.com>
Date:   Thu Apr 23 12:31:51 2009 +0200

    ioatdma: fix "ioatdma frees DMA memory with wrong function"
    
    as reported by Alexander Beregalov <a.beregalov@gmail.com>
    
    ioatdma 0000:00:08.0: DMA-API: device driver frees DMA memory with
    wrong function [device address=0x000000007f76f800] [size=2000 bytes]
    [map
    ped as single] [unmapped as page]
    
    The ioatdma driver was unmapping all regions
    (either allocated as page or single) using unmap_page.
    This patch lets dma driver recognize if unmap_single or unmap_page should be used.
    It introduces two new dma control flags:
    DMA_COMPL_SRC_UNMAP_SINGLE and DMA_COMPL_DEST_UNMAP_SINGLE.
    They should be set to indicate dma driver to do dma-unmapping as single
    (first one for the source, tha latter for the destination).
    If respective flag is not set, the driver assumes dma-unmapping as page.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Tested-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2e2aa3df170c..ffefba81c818 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -78,12 +78,18 @@ enum dma_transaction_type {
  * 	dependency chains
  * @DMA_COMPL_SKIP_SRC_UNMAP - set to disable dma-unmapping the source buffer(s)
  * @DMA_COMPL_SKIP_DEST_UNMAP - set to disable dma-unmapping the destination(s)
+ * @DMA_COMPL_SRC_UNMAP_SINGLE - set to do the source dma-unmapping as single
+ * 	(if not set, do the source dma-unmapping as page)
+ * @DMA_COMPL_DEST_UNMAP_SINGLE - set to do the destination dma-unmapping as single
+ * 	(if not set, do the destination dma-unmapping as page)
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
 	DMA_CTRL_ACK = (1 << 1),
 	DMA_COMPL_SKIP_SRC_UNMAP = (1 << 2),
 	DMA_COMPL_SKIP_DEST_UNMAP = (1 << 3),
+	DMA_COMPL_SRC_UNMAP_SINGLE = (1 << 4),
+	DMA_COMPL_DEST_UNMAP_SINGLE = (1 << 5),
 };
 
 /**

commit 099f53cb50e45ef617a9f1d63ceec799e489418b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Apr 8 14:28:37 2009 -0700

    async_tx: rename zero_sum to val
    
    'zero_sum' does not properly describe the operation of generating parity
    and checking that it validates against an existing buffer.  Change the
    name of the operation to 'val' (for 'validate').  This is in
    anticipation of the p+q case where it is a requirement to identify the
    target parity buffers separately from the source buffers, because the
    target parity buffers will not have corresponding pq coefficients.
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2e2aa3df170c..6768727d00d7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -55,8 +55,8 @@ enum dma_transaction_type {
 	DMA_PQ_XOR,
 	DMA_DUAL_XOR,
 	DMA_PQ_UPDATE,
-	DMA_ZERO_SUM,
-	DMA_PQ_ZERO_SUM,
+	DMA_XOR_VAL,
+	DMA_PQ_VAL,
 	DMA_MEMSET,
 	DMA_MEMCPY_CRC32C,
 	DMA_INTERRUPT,
@@ -214,7 +214,7 @@ struct dma_async_tx_descriptor {
  * @device_free_chan_resources: release DMA channel's resources
  * @device_prep_dma_memcpy: prepares a memcpy operation
  * @device_prep_dma_xor: prepares a xor operation
- * @device_prep_dma_zero_sum: prepares a zero_sum operation
+ * @device_prep_dma_xor_val: prepares a xor validation operation
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
@@ -243,7 +243,7 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(
 		struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
 		unsigned int src_cnt, size_t len, unsigned long flags);
-	struct dma_async_tx_descriptor *(*device_prep_dma_zero_sum)(
+	struct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(
 		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,
 		size_t len, u32 *result, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(

commit 0f571515c332e00b3515dbe0859ceaa30ab66e00
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Fri Mar 6 20:07:14 2009 +0900

    dmaengine: Add privatecnt to revert DMA_PRIVATE property
    
    Currently dma_request_channel() set DMA_PRIVATE capability but never
    clear it.  So if a public channel was once grabbed by
    dma_request_channel(), the device stay PRIVATE forever.  Add
    privatecnt member to dma_device to correctly revert it.
    
    [lg@denx.de: fix bad usage of 'chan' in dma_async_device_register]
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2afc2c95e42d..2e2aa3df170c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -202,6 +202,7 @@ struct dma_async_tx_descriptor {
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported
+ * @privatecnt: how many DMA channels are requested by dma_request_channel
  * @channels: the list of struct dma_chan
  * @global_node: list_head for global dma_device_list
  * @cap_mask: one or more dma_capability flags
@@ -224,6 +225,7 @@ struct dma_async_tx_descriptor {
 struct dma_device {
 
 	unsigned int chancnt;
+	unsigned int privatecnt;
 	struct list_head channels;
 	struct list_head global_node;
 	dma_cap_mask_t  cap_mask;
@@ -352,6 +354,13 @@ __dma_cap_set(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)
 	set_bit(tx_type, dstp->bits);
 }
 
+#define dma_cap_clear(tx, mask) __dma_cap_clear((tx), &(mask))
+static inline void
+__dma_cap_clear(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)
+{
+	clear_bit(tx_type, dstp->bits);
+}
+
 #define dma_cap_zero(mask) __dma_cap_zero(&(mask))
 static inline void __dma_cap_zero(dma_cap_mask_t *dstp)
 {

commit 729b5d1b8ec72c28e99840b3f300ba67726e3ab9
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Mar 25 09:13:25 2009 -0700

    dmaengine: allow dma support for async_tx to be toggled
    
    Provide a config option for blocking the allocation of dma channels to
    the async_tx api.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 96e676e5bf9b..2afc2c95e42d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -288,6 +288,24 @@ static inline void net_dmaengine_put(void)
 }
 #endif
 
+#ifdef CONFIG_ASYNC_TX_DMA
+#define async_dmaengine_get()	dmaengine_get()
+#define async_dmaengine_put()	dmaengine_put()
+#define async_dma_find_channel(type) dma_find_channel(type)
+#else
+static inline void async_dmaengine_get(void)
+{
+}
+static inline void async_dmaengine_put(void)
+{
+}
+static inline struct dma_chan *
+async_dma_find_channel(enum dma_transaction_type type)
+{
+	return NULL;
+}
+#endif
+
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);
 dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,

commit 54aee6a5f560d0e1bf3f39987c6ebe06daeb0ce1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Mar 25 09:13:24 2009 -0700

    dmaengine: kill some unused headers
    
    The dmaengine redux left some unneeded headers in
    include/linux/dmaengine.h, clean them up.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1956c8d46d32..96e676e5bf9b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -23,9 +23,6 @@
 
 #include <linux/device.h>
 #include <linux/uio.h>
-#include <linux/kref.h>
-#include <linux/completion.h>
-#include <linux/rcupdate.h>
 #include <linux/dma-mapping.h>
 
 /**

commit 5dc18f51a2c06ddab708184e30b7967fb71c1784
Merge: fd6ec5f3acfe 7cbd4877e5b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 8 10:23:05 2009 -0700

    Merge branch 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx
    
    * 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx:
      dmatest: fix use after free in dmatest_exit
      ipu_idmac: fix spinlock type
      iop-adma, mv_xor: fix mem leak on self-test setup failure
      fsldma: fix off by one in dma_halt
      I/OAT: fail self-test if callback test reaches timeout
      I/OAT: update driver version and copyright dates
      I/OAT: list usage cleanup
      I/OAT: set tcp_dma_copybreak to 256k for I/OAT ver.3
      I/OAT: cancel watchdog before dma remove
      I/OAT: fail initialization on zero channels detection
      I/OAT: do not set DCACTRL_CMPL_WRITE_ENABLE for I/OAT ver.3
      I/OAT: add verification for proper APICID_TAG_MAP setting by BIOS
      dmaengine: update kerneldoc

commit 287d859222e0adbc67666a6154aaf42d7d5bbb54
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Feb 18 14:48:26 2009 -0800

    atmel-mci: fix initialization of dma slave data
    
    The conversion of atmel-mci to dma_request_channel missed the
    initialization of the channel dma_slave information.  The filter_fn passed
    to dma_request_channel is responsible for initializing the channel's
    private data.  This implementation has the additional benefit of enabling
    a generic client-channel data passing mechanism.
    
    Reviewed-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3e68469c1885..f0413845f20e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -121,6 +121,7 @@ struct dma_chan_percpu {
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client-count: how many clients are using this channel
  * @table_count: number of appearances in the mem-to-mem allocation table
+ * @private: private data for certain client-channel associations
  */
 struct dma_chan {
 	struct dma_device *device;
@@ -134,6 +135,7 @@ struct dma_chan {
 	struct dma_chan_percpu *local;
 	int client_count;
 	int table_count;
+	void *private;
 };
 
 /**

commit 1d93e52eb48df986a3c4d5ad8a520bf1f6837367
Author: Johannes Weiner <jw@emlix.com>
Date:   Wed Feb 11 08:47:19 2009 -0700

    dmaengine: update kerneldoc
    
    Some of the kerneldoc comments in the dmaengine header describe
    already removed structure members.  Remove them.
    
    Also add a short description for dma_device->device_is_tx_complete.
    
    Signed-off-by: Johannes Weiner <jw@emlix.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3e68469c1885..087e79acf8c7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -97,7 +97,6 @@ typedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;
 
 /**
  * struct dma_chan_percpu - the per-CPU part of struct dma_chan
- * @refcount: local_t used for open-coded "bigref" counting
  * @memcpy_count: transaction counter
  * @bytes_transferred: byte counter
  */
@@ -114,9 +113,6 @@ struct dma_chan_percpu {
  * @cookie: last cookie value returned to client
  * @chan_id: channel ID for sysfs
  * @dev: class device for sysfs
- * @refcount: kref, used in "bigref" slow-mode
- * @slow_ref: indicates that the DMA channel is free
- * @rcu: the DMA channel's RCU head
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client-count: how many clients are using this channel
@@ -211,8 +207,6 @@ struct dma_async_tx_descriptor {
  * @global_node: list_head for global dma_device_list
  * @cap_mask: one or more dma_capability flags
  * @max_xor: maximum number of xor sources, 0 if no capability
- * @refcount: reference count
- * @done: IO completion struct
  * @dev_id: unique device ID
  * @dev: struct device reference for dma mapping api
  * @device_alloc_chan_resources: allocate resources and return the
@@ -225,6 +219,7 @@ struct dma_async_tx_descriptor {
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
  * @device_prep_slave_sg: prepares a slave dma operation
  * @device_terminate_all: terminate all pending operations
+ * @device_is_tx_complete: poll for transaction completion
  * @device_issue_pending: push pending transactions to hardware
  */
 struct dma_device {

commit b4bd07c20ba0c1fa7ad09ba257e0a5cfc2bf6bb3
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 6 22:06:43 2009 -0800

    net_dma: call dmaengine_get only if NET_DMA enabled
    
    Based upon a patch from Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    
    --------------------
    The commit 649274d993212e7c23c0cb734572c2311c200872 ("net_dma:
    acquire/release dma channels on ifup/ifdown") added unconditional call
    of dmaengine_get() to net_dma.  The API should be called only if
    NET_DMA was enabled.
    --------------------
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3e0f64c335c8..3e68469c1885 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -282,6 +282,18 @@ static inline void dmaengine_put(void)
 }
 #endif
 
+#ifdef CONFIG_NET_DMA
+#define net_dmaengine_get()	dmaengine_get()
+#define net_dmaengine_put()	dmaengine_put()
+#else
+static inline void net_dmaengine_get(void)
+{
+}
+static inline void net_dmaengine_put(void)
+{
+}
+#endif
+
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);
 dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,

commit 7954d5cf39ee1ce9bb0a4b19fcf1924885a9cad1
Merge: 37f5fed55559 86528da229a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 26 10:13:08 2009 -0800

    Merge branch 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx
    
    * 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/async_tx:
      i.MX31: framebuffer driver
      i.MX31: Image Processing Unit DMA and IRQ drivers
      dmaengine: add async_tx_clear_ack() macro
      dmaengine: dma_issue_pending_all == nop when CONFIG_DMA_ENGINE=n
      dmaengine: kill some dubious WARN_ONCEs
      fsldma: print correct IRQ on mpc83xx
      fsldma: check for NO_IRQ in fsl_dma_chan_remove()
      dmatest: Use custom map/unmap for destination buffer
      fsldma: use a valid 'device' for dma_pool_create
      dmaengine: fix dependency chaining

commit ef560682a97491f62ef538931a4861b57d66c52c
Author: Guennadi Liakhovetski <lg@denx.de>
Date:   Mon Jan 19 15:36:21 2009 -0700

    dmaengine: add async_tx_clear_ack() macro
    
    To complete the DMA_CTRL_ACK handling API add a async_tx_clear_ack() macro.
    
    Signed-off-by: Guennadi Liakhovetski <lg@denx.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c4a560e72ab7..34f124d7fb94 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -287,6 +287,11 @@ static inline void async_tx_ack(struct dma_async_tx_descriptor *tx)
 	tx->flags |= DMA_CTRL_ACK;
 }
 
+static inline void async_tx_clear_ack(struct dma_async_tx_descriptor *tx)
+{
+	tx->flags &= ~DMA_CTRL_ACK;
+}
+
 static inline bool async_tx_test_ack(struct dma_async_tx_descriptor *tx)
 {
 	return (tx->flags & DMA_CTRL_ACK) == DMA_CTRL_ACK;

commit c50331e8be32eaba5e1949f98c70d50b891262db
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Jan 19 15:33:14 2009 -0700

    dmaengine: dma_issue_pending_all == nop when CONFIG_DMA_ENGINE=n
    
    The device list will always be empty in this configuration, so no need
    to walk the list.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 64dea2ab326c..c4a560e72ab7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -390,11 +390,16 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 #ifdef CONFIG_DMA_ENGINE
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
+void dma_issue_pending_all(void);
 #else
 static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
 {
 	return DMA_SUCCESS;
 }
+static inline void dma_issue_pending_all(void)
+{
+	do { } while (0);
+}
 #endif
 
 /* --- DMA device --- */
@@ -403,7 +408,6 @@ int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
-void dma_issue_pending_all(void);
 #define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
 struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
 void dma_release_channel(struct dma_chan *chan);

commit 649274d993212e7c23c0cb734572c2311c200872
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Jan 11 00:20:39 2009 -0800

    net_dma: acquire/release dma channels on ifup/ifdown
    
    The recent dmaengine rework removed the capability to remove dma device
    driver modules while net_dma is active.  Rather than notify
    dmaengine-clients that channels are trying to be removed, we now rely on
    clients to notify dmaengine when they no longer have a need for
    channels.  Teach net_dma to release channels by taking dmaengine
    references at netdevice open and dropping references at netdevice close.
    
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 64dea2ab326c..c73f1e2b59b7 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -270,8 +270,18 @@ struct dma_device {
 
 /* --- public DMA engine API --- */
 
+#ifdef CONFIG_DMA_ENGINE
 void dmaengine_get(void);
 void dmaengine_put(void);
+#else
+static inline void dmaengine_get(void)
+{
+}
+static inline void dmaengine_put(void)
+{
+}
+#endif
+
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);
 dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,

commit 864498aaa9fef69ee166da023d12413a7776342d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:21 2009 -0700

    dmaengine: use idr for registering dma device numbers
    
    This brings some predictability to dma device numbers, i.e. an rmmod/insmod
    cycle may now result in /sys/class/dma/dma0chan0 being restored rather than
    /sys/class/dma/dma1chan0 appearing.
    
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d6b6bff355f4..64dea2ab326c 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -140,10 +140,14 @@ struct dma_chan {
  * struct dma_chan_dev - relate sysfs device node to backing channel device
  * @chan - driver channel device
  * @device - sysfs device
+ * @dev_id - parent dma_device dev_id
+ * @idr_ref - reference count to gate release of dma_device dev_id
  */
 struct dma_chan_dev {
 	struct dma_chan *chan;
 	struct device device;
+	int dev_id;
+	atomic_t *idr_ref;
 };
 
 static inline const char *dma_chan_name(struct dma_chan *chan)

commit 41d5e59c1299f27983977bcfe3b360600996051c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:21 2009 -0700

    dmaengine: add a release for dma class devices and dependent infrastructure
    
    Resolves:
    WARNING: at drivers/base/core.c:122 device_release+0x4d/0x52()
    Device 'dma0chan0' does not have a release() function, it is broken and must be fixed.
    
    The dma_chan_dev object is introduced to gear-match sysfs kobject and
    dmaengine channel lifetimes.  When a channel is removed access to the
    sysfs entries return -ENODEV until the kobject can be released.
    
    The bulk of the change is updates to existing code to handle the extra
    layer of indirection between a dma_chan and its struct device.
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Acked-by: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1419a5094478..d6b6bff355f4 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -113,7 +113,7 @@ struct dma_chan_percpu {
  * @device: ptr to the dma device who supplies this channel, always !%NULL
  * @cookie: last cookie value returned to client
  * @chan_id: channel ID for sysfs
- * @class_dev: class device for sysfs
+ * @dev: class device for sysfs
  * @refcount: kref, used in "bigref" slow-mode
  * @slow_ref: indicates that the DMA channel is free
  * @rcu: the DMA channel's RCU head
@@ -128,7 +128,7 @@ struct dma_chan {
 
 	/* sysfs */
 	int chan_id;
-	struct device dev;
+	struct dma_chan_dev *dev;
 
 	struct list_head device_node;
 	struct dma_chan_percpu *local;
@@ -136,7 +136,20 @@ struct dma_chan {
 	int table_count;
 };
 
-#define to_dma_chan(p) container_of(p, struct dma_chan, dev)
+/**
+ * struct dma_chan_dev - relate sysfs device node to backing channel device
+ * @chan - driver channel device
+ * @device - sysfs device
+ */
+struct dma_chan_dev {
+	struct dma_chan *chan;
+	struct device device;
+};
+
+static inline const char *dma_chan_name(struct dma_chan *chan)
+{
+	return dev_name(&chan->dev->device);
+}
 
 void dma_chan_cleanup(struct kref *kref);
 

commit 7dd602510128d7a64b11ff3b7d4f30ac8e3946ce
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:19 2009 -0700

    dmaengine: kill enum dma_state_client
    
    DMA_NAK is now useless.  We can just use a bool instead.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index bca2fc758894..1419a5094478 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -28,18 +28,6 @@
 #include <linux/rcupdate.h>
 #include <linux/dma-mapping.h>
 
-/**
- * enum dma_state_client - state of the channel in the client
- * @DMA_ACK: client would like to use, or was using this channel
- * @DMA_DUP: client has already seen this channel, or is not using this channel
- * @DMA_NAK: client does not want to see any more channels
- */
-enum dma_state_client {
-	DMA_ACK,
-	DMA_DUP,
-	DMA_NAK,
-};
-
 /**
  * typedef dma_cookie_t - an opaque DMA cookie
  *
@@ -160,9 +148,10 @@ void dma_chan_cleanup(struct kref *kref);
  * When this optional parameter is specified in a call to dma_request_channel a
  * suitable channel is passed to this routine for further dispositioning before
  * being returned.  Where 'suitable' indicates a non-busy channel that
- * satisfies the given capability mask.
+ * satisfies the given capability mask.  It returns 'true' to indicate that the
+ * channel is suitable.
  */
-typedef enum dma_state_client (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
+typedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
 
 typedef void (*dma_async_tx_callback)(void *dma_async_param);
 /**

commit f27c580c3628d79b17f38976d842a6d7f3616e2e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:18 2009 -0700

    dmaengine: remove 'bigref' infrastructure
    
    Reference counting is done at the module level so clients need not worry
    that a channel will leave while they are actively using dmaengine.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index db050e97d2b4..bca2fc758894 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -142,10 +142,6 @@ struct dma_chan {
 	int chan_id;
 	struct device dev;
 
-	struct kref refcount;
-	int slow_ref;
-	struct rcu_head rcu;
-
 	struct list_head device_node;
 	struct dma_chan_percpu *local;
 	int client_count;
@@ -233,9 +229,6 @@ struct dma_device {
 	dma_cap_mask_t  cap_mask;
 	int max_xor;
 
-	struct kref refcount;
-	struct completion done;
-
 	int dev_id;
 	struct device *dev;
 

commit aa1e6f1a385eb2b04171ec841f3b760091e4a8ee
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:17 2009 -0700

    dmaengine: kill struct dma_client and supporting infrastructure
    
    All users have been converted to either the general-purpose allocator,
    dma_find_channel, or dma_request_channel.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 37d95db156d3..db050e97d2b4 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -28,20 +28,6 @@
 #include <linux/rcupdate.h>
 #include <linux/dma-mapping.h>
 
-/**
- * enum dma_state - resource PNP/power management state
- * @DMA_RESOURCE_SUSPEND: DMA device going into low power state
- * @DMA_RESOURCE_RESUME: DMA device returning to full power
- * @DMA_RESOURCE_AVAILABLE: DMA device available to the system
- * @DMA_RESOURCE_REMOVED: DMA device removed from the system
- */
-enum dma_state {
-	DMA_RESOURCE_SUSPEND,
-	DMA_RESOURCE_RESUME,
-	DMA_RESOURCE_AVAILABLE,
-	DMA_RESOURCE_REMOVED,
-};
-
 /**
  * enum dma_state_client - state of the channel in the client
  * @DMA_ACK: client would like to use, or was using this channel
@@ -170,23 +156,6 @@ struct dma_chan {
 
 void dma_chan_cleanup(struct kref *kref);
 
-/*
- * typedef dma_event_callback - function pointer to a DMA event callback
- * For each channel added to the system this routine is called for each client.
- * If the client would like to use the channel it returns '1' to signal (ack)
- * the dmaengine core to take out a reference on the channel and its
- * corresponding device.  A client must not 'ack' an available channel more
- * than once.  When a channel is removed all clients are notified.  If a client
- * is using the channel it must 'ack' the removal.  A client must not 'ack' a
- * removed channel more than once.
- * @client - 'this' pointer for the client context
- * @chan - channel to be acted upon
- * @state - available or removed
- */
-struct dma_client;
-typedef enum dma_state_client (*dma_event_callback) (struct dma_client *client,
-		struct dma_chan *chan, enum dma_state state);
-
 /**
  * typedef dma_filter_fn - callback filter for dma_request_channel
  * @chan: channel to be reviewed
@@ -199,21 +168,6 @@ typedef enum dma_state_client (*dma_event_callback) (struct dma_client *client,
  */
 typedef enum dma_state_client (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
 
-/**
- * struct dma_client - info on the entity making use of DMA services
- * @event_callback: func ptr to call when something happens
- * @cap_mask: only return channels that satisfy the requested capabilities
- *  a value of zero corresponds to any capability
- * @slave: data for preparing slave transfer. Must be non-NULL iff the
- *  DMA_SLAVE capability is requested.
- * @global_node: list_head for global dma_client_list
- */
-struct dma_client {
-	dma_event_callback	event_callback;
-	dma_cap_mask_t		cap_mask;
-	struct list_head	global_node;
-};
-
 typedef void (*dma_async_tx_callback)(void *dma_async_param);
 /**
  * struct dma_async_tx_descriptor - async transaction descriptor
@@ -285,8 +239,7 @@ struct dma_device {
 	int dev_id;
 	struct device *dev;
 
-	int (*device_alloc_chan_resources)(struct dma_chan *chan,
-			struct dma_client *client);
+	int (*device_alloc_chan_resources)(struct dma_chan *chan);
 	void (*device_free_chan_resources)(struct dma_chan *chan);
 
 	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(
@@ -320,7 +273,6 @@ struct dma_device {
 
 void dmaengine_get(void);
 void dmaengine_put(void);
-void dma_async_client_chan_request(struct dma_client *client);
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);
 dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,

commit 209b84a88fe81341b4d8d465acc4a67cb7c3feb3
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:17 2009 -0700

    dmaengine: replace dma_async_client_register with dmaengine_get
    
    Now that clients no longer need to be notified of channel arrival
    dma_async_client_register can simply increment the dmaengine_ref_count.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d63544cf8a1a..37d95db156d3 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -318,8 +318,8 @@ struct dma_device {
 
 /* --- public DMA engine API --- */
 
-void dma_async_client_register(struct dma_client *client);
-void dma_async_client_unregister(struct dma_client *client);
+void dmaengine_get(void);
+void dmaengine_put(void);
 void dma_async_client_chan_request(struct dma_client *client);
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);

commit 74465b4ff9ac1da503025c0a0042e023bfa6505c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:16 2009 -0700

    atmel-mci: convert to dma_request_channel and down-level dma_slave
    
    dma_request_channel provides an exclusive channel, so we no longer need to
    pass slave data through dmaengine.
    
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6f2d070ac7f3..d63544cf8a1a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -96,17 +96,6 @@ enum dma_transaction_type {
 /* last transaction type for creation of the capabilities mask */
 #define DMA_TX_TYPE_END (DMA_SLAVE + 1)
 
-/**
- * enum dma_slave_width - DMA slave register access width.
- * @DMA_SLAVE_WIDTH_8BIT: Do 8-bit slave register accesses
- * @DMA_SLAVE_WIDTH_16BIT: Do 16-bit slave register accesses
- * @DMA_SLAVE_WIDTH_32BIT: Do 32-bit slave register accesses
- */
-enum dma_slave_width {
-	DMA_SLAVE_WIDTH_8BIT,
-	DMA_SLAVE_WIDTH_16BIT,
-	DMA_SLAVE_WIDTH_32BIT,
-};
 
 /**
  * enum dma_ctrl_flags - DMA flags to augment operation preparation,
@@ -132,32 +121,6 @@ enum dma_ctrl_flags {
  */
 typedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;
 
-/**
- * struct dma_slave - Information about a DMA slave
- * @dev: device acting as DMA slave
- * @dma_dev: required DMA master device. If non-NULL, the client can not be
- *	bound to other masters than this.
- * @tx_reg: physical address of data register used for
- *	memory-to-peripheral transfers
- * @rx_reg: physical address of data register used for
- *	peripheral-to-memory transfers
- * @reg_width: peripheral register width
- *
- * If dma_dev is non-NULL, the client can not be bound to other DMA
- * masters than the one corresponding to this device. The DMA master
- * driver may use this to determine if there is controller-specific
- * data wrapped around this struct. Drivers of platform code that sets
- * the dma_dev field must therefore make sure to use an appropriate
- * controller-specific dma slave structure wrapping this struct.
- */
-struct dma_slave {
-	struct device		*dev;
-	struct device		*dma_dev;
-	dma_addr_t		tx_reg;
-	dma_addr_t		rx_reg;
-	enum dma_slave_width	reg_width;
-};
-
 /**
  * struct dma_chan_percpu - the per-CPU part of struct dma_chan
  * @refcount: local_t used for open-coded "bigref" counting
@@ -248,7 +211,6 @@ typedef enum dma_state_client (*dma_filter_fn)(struct dma_chan *chan, void *filt
 struct dma_client {
 	dma_event_callback	event_callback;
 	dma_cap_mask_t		cap_mask;
-	struct dma_slave	*slave;
 	struct list_head	global_node;
 };
 

commit 33df8ca068123457db56c316946a3c0e4ef787d6
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:15 2009 -0700

    dmatest: convert to dma_request_channel
    
    Replace the client registration infrastructure with a custom loop to
    poll for channels.  Once dma_request_channel returns NULL stop asking
    for channels.  A userspace side effect of this change if that loading
    the dmatest module before loading a dma driver will result in no
    channels being found, previously dmatest would get a callback.  To
    facilitate testing in the built-in case dmatest_init is marked as a
    late_initcall.  Another side effect is that channels under test can not
    be used for any other purpose.
    
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index fe40bc020af6..6f2d070ac7f3 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -400,6 +400,12 @@ __dma_cap_set(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)
 	set_bit(tx_type, dstp->bits);
 }
 
+#define dma_cap_zero(mask) __dma_cap_zero(&(mask))
+static inline void __dma_cap_zero(dma_cap_mask_t *dstp)
+{
+	bitmap_zero(dstp->bits, DMA_TX_TYPE_END);
+}
+
 #define dma_has_cap(tx, mask) __dma_has_cap((tx), &(mask))
 static inline int
 __dma_has_cap(enum dma_transaction_type tx_type, dma_cap_mask_t *srcp)

commit 59b5ec21446b9239d706ab237fb261d525b75e81
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:15 2009 -0700

    dmaengine: introduce dma_request_channel and private channels
    
    This interface is primarily for device-to-memory clients which need to
    search for dma channels with platform-specific characteristics.  The
    prototype is:
    
    struct dma_chan *dma_request_channel(dma_cap_mask_t mask,
                                         dma_filter_fn filter_fn,
                                         void *filter_param);
    
    When the optional 'filter_fn' parameter is set to NULL
    dma_request_channel simply returns the first channel that satisfies the
    capability mask.  Otherwise, when the mask parameter is insufficient for
    specifying the necessary channel, the filter_fn routine can be used to
    disposition the available channels in the system. The filter_fn routine
    is called once for each free channel in the system.  Upon seeing a
    suitable channel filter_fn returns DMA_ACK which flags that channel to
    be the return value from dma_request_channel.  A channel allocated via
    this interface is exclusive to the caller, until dma_release_channel()
    is called.
    
    To ensure that all channels are not consumed by the general-purpose
    allocator the DMA_PRIVATE capability is provided to exclude a dma_device
    from general-purpose (memory-to-memory) consideration.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 57a43adfc39e..fe40bc020af6 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -89,6 +89,7 @@ enum dma_transaction_type {
 	DMA_MEMSET,
 	DMA_MEMCPY_CRC32C,
 	DMA_INTERRUPT,
+	DMA_PRIVATE,
 	DMA_SLAVE,
 };
 
@@ -223,6 +224,18 @@ struct dma_client;
 typedef enum dma_state_client (*dma_event_callback) (struct dma_client *client,
 		struct dma_chan *chan, enum dma_state state);
 
+/**
+ * typedef dma_filter_fn - callback filter for dma_request_channel
+ * @chan: channel to be reviewed
+ * @filter_param: opaque parameter passed through dma_request_channel
+ *
+ * When this optional parameter is specified in a call to dma_request_channel a
+ * suitable channel is passed to this routine for further dispositioning before
+ * being returned.  Where 'suitable' indicates a non-busy channel that
+ * satisfies the given capability mask.
+ */
+typedef enum dma_state_client (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);
+
 /**
  * struct dma_client - info on the entity making use of DMA services
  * @event_callback: func ptr to call when something happens
@@ -472,6 +485,9 @@ void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
 void dma_issue_pending_all(void);
+#define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
+struct dma_chan *__dma_request_channel(dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param);
+void dma_release_channel(struct dma_chan *chan);
 
 /* --- Helper iov-locking functions --- */
 

commit 2ba05622b8b143b0c95968ba59bddfbd6d2f2559
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:14 2009 -0700

    dmaengine: provide a common 'issue_pending_all' implementation
    
    async_tx and net_dma each have open-coded versions of issue_pending_all,
    so provide a common routine in dmaengine.
    
    The implementation needs to walk the global device list, so implement
    rcu to allow dma_issue_pending_all to run lockless.  Clients protect
    themselves from channel removal events by holding a dmaengine reference.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b466f02e2433..57a43adfc39e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -471,6 +471,7 @@ int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
+void dma_issue_pending_all(void);
 
 /* --- Helper iov-locking functions --- */
 

commit bec085134e446577a983f17f57d642a88d1af53b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:14 2009 -0700

    dmaengine: centralize channel allocation, introduce dma_find_channel
    
    Allowing multiple clients to each define their own channel allocation
    scheme quickly leads to a pathological situation.  For memory-to-memory
    offload all clients can share a central allocator.
    
    This simply moves the existing async_tx allocator to dmaengine with
    minimal fixups:
    * async_tx.c:get_chan_ref_by_cap --> dmaengine.c:nth_chan
    * async_tx.c:async_tx_rebalance --> dmaengine.c:dma_channel_rebalance
    * split out common code from async_tx.c:__async_tx_find_channel -->
      dma_find_channel
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d18d37d1015d..b466f02e2433 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -182,6 +182,7 @@ struct dma_chan_percpu {
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
  * @client-count: how many clients are using this channel
+ * @table_count: number of appearances in the mem-to-mem allocation table
  */
 struct dma_chan {
 	struct dma_device *device;
@@ -198,6 +199,7 @@ struct dma_chan {
 	struct list_head device_node;
 	struct dma_chan_percpu *local;
 	int client_count;
+	int table_count;
 };
 
 #define to_dma_chan(p) container_of(p, struct dma_chan, dev)
@@ -468,6 +470,7 @@ static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descript
 int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
+struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);
 
 /* --- Helper iov-locking functions --- */
 

commit 6f49a57aa5a0c6d4e4e27c85f7af6c83325a12d1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:14 2009 -0700

    dmaengine: up-level reference counting to the module level
    
    Simply, if a client wants any dmaengine channel then prevent all dmaengine
    modules from being removed.  Once the clients are done re-enable module
    removal.
    
    Why?, beyond reducing complication:
    1/ Tracking reference counts per-transaction in an efficient manner, as
       is currently done, requires a complicated scheme to avoid cache-line
       bouncing effects.
    2/ Per-transaction ref-counting gives the false impression that a
       dma-driver can be gracefully removed ahead of its user (net, md, or
       dma-slave)
    3/ None of the in-tree dma-drivers talk to hot pluggable hardware, but
       if such an engine were built one day we still would not need to notify
       clients of remove events.  The driver can simply return NULL to a
       ->prep() request, something that is much easier for a client to handle.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index e4ec7e7b8056..d18d37d1015d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -165,7 +165,6 @@ struct dma_slave {
  */
 
 struct dma_chan_percpu {
-	local_t refcount;
 	/* stats */
 	unsigned long memcpy_count;
 	unsigned long bytes_transferred;
@@ -205,26 +204,6 @@ struct dma_chan {
 
 void dma_chan_cleanup(struct kref *kref);
 
-static inline void dma_chan_get(struct dma_chan *chan)
-{
-	if (unlikely(chan->slow_ref))
-		kref_get(&chan->refcount);
-	else {
-		local_inc(&(per_cpu_ptr(chan->local, get_cpu())->refcount));
-		put_cpu();
-	}
-}
-
-static inline void dma_chan_put(struct dma_chan *chan)
-{
-	if (unlikely(chan->slow_ref))
-		kref_put(&chan->refcount, dma_chan_cleanup);
-	else {
-		local_dec(&(per_cpu_ptr(chan->local, get_cpu())->refcount));
-		put_cpu();
-	}
-}
-
 /*
  * typedef dma_event_callback - function pointer to a DMA event callback
  * For each channel added to the system this routine is called for each client.

commit 07f2211e4fbce6990722d78c4f04225da9c0e9cf
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Jan 5 17:14:31 2009 -0700

    dmaengine: remove dependency on async_tx
    
    async_tx.ko is a consumer of dma channels.  A circular dependency arises
    if modules in drivers/dma rely on common code in async_tx.ko.  It
    prevents either module from being unloaded.
    
    Move dma_wait_for_async_tx and async_tx_run_dependencies to dmaeninge.o
    where they should have been from the beginning.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index adb0b084eb5a..e4ec7e7b8056 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -475,11 +475,20 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 }
 
 enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
+#ifdef CONFIG_DMA_ENGINE
+enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
+#else
+static inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
+{
+	return DMA_SUCCESS;
+}
+#endif
 
 /* --- DMA device --- */
 
 int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
+void dma_run_dependencies(struct dma_async_tx_descriptor *tx);
 
 /* --- Helper iov-locking functions --- */
 

commit 0839875e0c197ded56bbae820e699f26d6fa2697
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 17 17:59:56 2008 -0700

    async_tx: make async_tx_test_ack a boolean routine
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 9b91d341e1fa..adb0b084eb5a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -375,16 +375,14 @@ dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
 void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
 	struct dma_chan *chan);
 
-static inline void
-async_tx_ack(struct dma_async_tx_descriptor *tx)
+static inline void async_tx_ack(struct dma_async_tx_descriptor *tx)
 {
 	tx->flags |= DMA_CTRL_ACK;
 }
 
-static inline int
-async_tx_test_ack(struct dma_async_tx_descriptor *tx)
+static inline bool async_tx_test_ack(struct dma_async_tx_descriptor *tx)
 {
-	return tx->flags & DMA_CTRL_ACK;
+	return (tx->flags & DMA_CTRL_ACK) == DMA_CTRL_ACK;
 }
 
 #define first_dma_cap(mask) __first_dma_cap(&(mask))

commit dc0ee6435cb92ccc81b14ff28d163fecc5a7f120
Author: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
Date:   Tue Jul 8 11:59:35 2008 -0700

    dmaengine: Add slave DMA interface
    
    This patch adds the necessary interfaces to the DMA Engine framework
    to use functionality found on most embedded DMA controllers: DMA from
    and to I/O registers with hardware handshaking.
    
    In this context, hardware hanshaking means that the peripheral that
    owns the I/O registers in question is able to tell the DMA controller
    when more data is available for reading, or when there is room for
    more data to be written. This usually happens internally on the chip,
    but these signals may also be exported outside the chip for things
    like IDE DMA, etc.
    
    A new struct dma_slave is introduced. This contains information that
    the DMA engine driver needs to set up slave transfers to and from a
    slave device. Most engines supporting DMA slave transfers will want to
    extend this structure with controller-specific parameters.  This
    additional information is usually passed from the platform/board code
    through the client driver.
    
    A "slave" pointer is added to the dma_client struct. This must point
    to a valid dma_slave structure iff the DMA_SLAVE capability is
    requested.  The DMA engine driver may use this information in its
    device_alloc_chan_resources hook to configure the DMA controller for
    slave transfers from and to the given slave device.
    
    A new operation for preparing slave DMA transfers is added to struct
    dma_device. This takes a scatterlist and returns a single descriptor
    representing the whole transfer.
    
    Another new operation for terminating all pending transfers is added as
    well. The latter is needed because there may be errors outside the scope
    of the DMA Engine framework that may require DMA operations to be
    terminated prematurely.
    
    DMA Engine drivers may extend the dma_device, dma_chan and/or
    dma_slave_descriptor structures to allow controller-specific
    operations. The client driver can detect such extensions by looking at
    the DMA Engine's struct device, or it can request a specific DMA
    Engine device by setting the dma_dev field in struct dma_slave.
    
    dmaslave interface changes since v4:
      * Fix checkpatch errors
      * Fix changelog (there are no slave descriptors anymore)
    
    dmaslave interface changes since v3:
      * Use dma_data_direction instead of a new enum
      * Submit slave transfers as scatterlists
      * Remove the DMA slave descriptor struct
    
    dmaslave interface changes since v2:
      * Add a dma_dev field to struct dma_slave. If set, the client can
        only be bound to the DMA controller that corresponds to this
        device.  This allows controller-specific extensions of the
        dma_slave structure; if the device matches, the controller may
        safely assume its extensions are present.
      * Move reg_width into struct dma_slave as there are currently no
        users that need to be able to set the width on a per-transfer
        basis.
    
    dmaslave interface changes since v1:
      * Drop the set_direction and set_width descriptor hooks. Pass the
        direction and width to the prep function instead.
      * Declare a dma_slave struct with fixed information about a slave,
        i.e. register addresses, handshake interfaces and such.
      * Add pointer to a dma_slave struct to dma_client. Can be NULL if
        the DMA_SLAVE capability isn't requested.
      * Drop the set_slave device hook since the alloc_chan_resources hook
        now has enough information to set up the channel for slave
        transfers.
    
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b058d6360383..9b91d341e1fa 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -89,10 +89,23 @@ enum dma_transaction_type {
 	DMA_MEMSET,
 	DMA_MEMCPY_CRC32C,
 	DMA_INTERRUPT,
+	DMA_SLAVE,
 };
 
 /* last transaction type for creation of the capabilities mask */
-#define DMA_TX_TYPE_END (DMA_INTERRUPT + 1)
+#define DMA_TX_TYPE_END (DMA_SLAVE + 1)
+
+/**
+ * enum dma_slave_width - DMA slave register access width.
+ * @DMA_SLAVE_WIDTH_8BIT: Do 8-bit slave register accesses
+ * @DMA_SLAVE_WIDTH_16BIT: Do 16-bit slave register accesses
+ * @DMA_SLAVE_WIDTH_32BIT: Do 32-bit slave register accesses
+ */
+enum dma_slave_width {
+	DMA_SLAVE_WIDTH_8BIT,
+	DMA_SLAVE_WIDTH_16BIT,
+	DMA_SLAVE_WIDTH_32BIT,
+};
 
 /**
  * enum dma_ctrl_flags - DMA flags to augment operation preparation,
@@ -118,6 +131,32 @@ enum dma_ctrl_flags {
  */
 typedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;
 
+/**
+ * struct dma_slave - Information about a DMA slave
+ * @dev: device acting as DMA slave
+ * @dma_dev: required DMA master device. If non-NULL, the client can not be
+ *	bound to other masters than this.
+ * @tx_reg: physical address of data register used for
+ *	memory-to-peripheral transfers
+ * @rx_reg: physical address of data register used for
+ *	peripheral-to-memory transfers
+ * @reg_width: peripheral register width
+ *
+ * If dma_dev is non-NULL, the client can not be bound to other DMA
+ * masters than the one corresponding to this device. The DMA master
+ * driver may use this to determine if there is controller-specific
+ * data wrapped around this struct. Drivers of platform code that sets
+ * the dma_dev field must therefore make sure to use an appropriate
+ * controller-specific dma slave structure wrapping this struct.
+ */
+struct dma_slave {
+	struct device		*dev;
+	struct device		*dma_dev;
+	dma_addr_t		tx_reg;
+	dma_addr_t		rx_reg;
+	enum dma_slave_width	reg_width;
+};
+
 /**
  * struct dma_chan_percpu - the per-CPU part of struct dma_chan
  * @refcount: local_t used for open-coded "bigref" counting
@@ -208,11 +247,14 @@ typedef enum dma_state_client (*dma_event_callback) (struct dma_client *client,
  * @event_callback: func ptr to call when something happens
  * @cap_mask: only return channels that satisfy the requested capabilities
  *  a value of zero corresponds to any capability
+ * @slave: data for preparing slave transfer. Must be non-NULL iff the
+ *  DMA_SLAVE capability is requested.
  * @global_node: list_head for global dma_client_list
  */
 struct dma_client {
 	dma_event_callback	event_callback;
 	dma_cap_mask_t		cap_mask;
+	struct dma_slave	*slave;
 	struct list_head	global_node;
 };
 
@@ -269,6 +311,8 @@ struct dma_async_tx_descriptor {
  * @device_prep_dma_zero_sum: prepares a zero_sum operation
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
+ * @device_prep_slave_sg: prepares a slave dma operation
+ * @device_terminate_all: terminate all pending operations
  * @device_issue_pending: push pending transactions to hardware
  */
 struct dma_device {
@@ -304,6 +348,12 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
 
+	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(
+		struct dma_chan *chan, struct scatterlist *sgl,
+		unsigned int sg_len, enum dma_data_direction direction,
+		unsigned long flags);
+	void (*device_terminate_all)(struct dma_chan *chan);
+
 	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
 			dma_cookie_t cookie, dma_cookie_t *last,
 			dma_cookie_t *used);

commit e1d181efb14a93cf263d6c588a5395518edf3294
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jul 4 00:13:40 2008 -0700

    dmaengine: add DMA_COMPL_SKIP_{SRC,DEST}_UNMAP flags to control dma unmap
    
    In some cases client code may need the dma-driver to skip the unmap of source
    and/or destination buffers.  Setting these flags indicates to the driver to
    skip the unmap step.  In this regard async_xor is currently broken in that it
    allows the destination buffer to be unmapped while an operation is still in
    progress, i.e. when the number of sources exceeds the hardware channel's
    maximum (fixed in a subsequent patch).
    
    Acked-by: Saeed Bishara <saeed@marvell.com>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Acked-by: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index ba89b0f5056e..b058d6360383 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -102,10 +102,14 @@ enum dma_transaction_type {
  * @DMA_CTRL_ACK - the descriptor cannot be reused until the client
  * 	acknowledges receipt, i.e. has has a chance to establish any
  * 	dependency chains
+ * @DMA_COMPL_SKIP_SRC_UNMAP - set to disable dma-unmapping the source buffer(s)
+ * @DMA_COMPL_SKIP_DEST_UNMAP - set to disable dma-unmapping the destination(s)
  */
 enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
 	DMA_CTRL_ACK = (1 << 1),
+	DMA_COMPL_SKIP_SRC_UNMAP = (1 << 2),
+	DMA_COMPL_SKIP_DEST_UNMAP = (1 << 3),
 };
 
 /**

commit 848c536a37b8db4e461f14ca15fe29850151c822
Author: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
Date:   Tue Jul 8 11:58:58 2008 -0700

    dmaengine: Add dma_client parameter to device_alloc_chan_resources
    
    A DMA controller capable of doing slave transfers may need to know a
    few things about the slave when preparing the channel. We don't want
    to add this information to struct dma_channel since the channel hasn't
    yet been bound to a client at this point.
    
    Instead, pass a reference to the client requesting the channel to the
    driver's device_alloc_chan_resources hook so that it can pick the
    necessary information from the dma_client struct by itself.
    
    [dan.j.williams@intel.com: fixed up fsldma and mv_xor]
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6432b8343220..ba89b0f5056e 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -281,7 +281,8 @@ struct dma_device {
 	int dev_id;
 	struct device *dev;
 
-	int (*device_alloc_chan_resources)(struct dma_chan *chan);
+	int (*device_alloc_chan_resources)(struct dma_chan *chan,
+			struct dma_client *client);
 	void (*device_free_chan_resources)(struct dma_chan *chan);
 
 	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(

commit 7cc5bf9a3a84e5a02e23e5739fb894790b37c101
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 8 11:58:21 2008 -0700

    dmaengine: track the number of clients using a channel
    
    Haavard's dma-slave interface would like to test for exclusive access to a
    channel.  The standard channel refcounting is not sufficient in that it
    tracks more than just client references, it is also inaccurate as reference
    counts are percpu until the channel is removed.
    
    This change also enables a future fix to deallocate resources when a client
    declines to use a capable channel.
    
    Acked-by: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index d08a5c5eb928..6432b8343220 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -139,6 +139,7 @@ struct dma_chan_percpu {
  * @rcu: the DMA channel's RCU head
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
+ * @client-count: how many clients are using this channel
  */
 struct dma_chan {
 	struct dma_device *device;
@@ -154,6 +155,7 @@ struct dma_chan {
 
 	struct list_head device_node;
 	struct dma_chan_percpu *local;
+	int client_count;
 };
 
 #define to_dma_chan(p) container_of(p, struct dma_chan, dev)

commit 8a5703f846e2363fc466aff3f53608340a1ae33f
Author: Sebastian Siewior <bigeasy@tglx.de>
Date:   Mon Apr 21 22:38:45 2008 +0000

    DMA engine: typo fixes
    
    Spelling fixes for dmaengine.[ch]
    
    Signed-off-by: Sebastian Siewior <bigeasy@linutronix.de>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b4d84ed6187d..d08a5c5eb928 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -404,7 +404,7 @@ static inline enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,
  * @last_used: last cookie value handed out
  *
  * dma_async_is_complete() is used in dma_async_memcpy_complete()
- * the test logic is seperated for lightweight testing of multiple cookies
+ * the test logic is separated for lightweight testing of multiple cookies
  */
 static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 			dma_cookie_t last_complete, dma_cookie_t last_used)

commit 636bdeaa1243327501edfd2a597ed7443eb4239a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 17 20:17:26 2008 -0700

    dmaengine: ack to flags: make use of the unused bits in the 'ack' field
    
    'ack' is currently a simple integer that flags whether or not a client is done
    touching fields in the given descriptor.  It is effectively just a single bit
    of information.  Converting this to a flags parameter allows the other bits to
    be put to use to control completion actions, like dma-unmap, and capture
    results, like xor-zero-sum == 0.
    
    Changes are one of:
    1/ convert all open-coded ->ack manipulations to use async_tx_ack
       and async_tx_test_ack.
    2/ set the ack bit at prep time where possible
    3/ make drivers store the flags at prep time
    4/ add flags to the device_prep_dma_interrupt prototype
    
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index cd34df78c6aa..b4d84ed6187d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -95,12 +95,17 @@ enum dma_transaction_type {
 #define DMA_TX_TYPE_END (DMA_INTERRUPT + 1)
 
 /**
- * enum dma_prep_flags - DMA flags to augment operation preparation
+ * enum dma_ctrl_flags - DMA flags to augment operation preparation,
+ * 	control completion, and communicate status.
  * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
  * 	this transaction
+ * @DMA_CTRL_ACK - the descriptor cannot be reused until the client
+ * 	acknowledges receipt, i.e. has has a chance to establish any
+ * 	dependency chains
  */
-enum dma_prep_flags {
+enum dma_ctrl_flags {
 	DMA_PREP_INTERRUPT = (1 << 0),
+	DMA_CTRL_ACK = (1 << 1),
 };
 
 /**
@@ -211,8 +216,8 @@ typedef void (*dma_async_tx_callback)(void *dma_async_param);
  * ---dma generic offload fields---
  * @cookie: tracking cookie for this transaction, set to -EBUSY if
  *	this tx is sitting on a dependency list
- * @ack: the descriptor can not be reused until the client acknowledges
- *	receipt, i.e. has has a chance to establish any dependency chains
+ * @flags: flags to augment operation preparation, control completion, and
+ * 	communicate status
  * @phys: physical address of the descriptor
  * @tx_list: driver common field for operations that require multiple
  *	descriptors
@@ -227,7 +232,7 @@ typedef void (*dma_async_tx_callback)(void *dma_async_param);
  */
 struct dma_async_tx_descriptor {
 	dma_cookie_t cookie;
-	int ack;
+	enum dma_ctrl_flags flags; /* not a 'long' to pack with cookie */
 	dma_addr_t phys;
 	struct list_head tx_list;
 	struct dma_chan *chan;
@@ -290,7 +295,7 @@ struct dma_device {
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
-		struct dma_chan *chan);
+		struct dma_chan *chan, unsigned long flags);
 
 	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
 			dma_cookie_t cookie, dma_cookie_t *last,
@@ -316,7 +321,13 @@ void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
 static inline void
 async_tx_ack(struct dma_async_tx_descriptor *tx)
 {
-	tx->ack = 1;
+	tx->flags |= DMA_CTRL_ACK;
+}
+
+static inline int
+async_tx_test_ack(struct dma_async_tx_descriptor *tx)
+{
+	return tx->flags & DMA_CTRL_ACK;
 }
 
 #define first_dma_cap(mask) __first_dma_cap(&(mask))

commit ce4d65a5db77e1568c82d5151a746f627c4f6ed5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 17 20:17:26 2008 -0700

    async_tx: kill ->device_dependency_added
    
    DMA drivers no longer need to be notified of dependency submission
    events as async_tx_run_dependencies and async_tx_channel_switch will
    handle the scheduling and execution of dependent operations.
    
    [sfr@canb.auug.org.au: extend this for fsldma]
    Acked-by: Shannon Nelson <shannon.nelson@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 91252a7e4d03..cd34df78c6aa 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -258,7 +258,6 @@ struct dma_async_tx_descriptor {
  * @device_prep_dma_zero_sum: prepares a zero_sum operation
  * @device_prep_dma_memset: prepares a memset operation
  * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
- * @device_dependency_added: async_tx notifies the channel about new deps
  * @device_issue_pending: push pending transactions to hardware
  */
 struct dma_device {
@@ -293,7 +292,6 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan);
 
-	void (*device_dependency_added)(struct dma_chan *chan);
 	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
 			dma_cookie_t cookie, dma_cookie_t *last,
 			dma_cookie_t *used);

commit 19242d7233df7d658405d4b7ee1758d21414cfaa
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 17 20:17:25 2008 -0700

    async_tx: fix multiple dependency submission
    
    Shrink struct dma_async_tx_descriptor and introduce
    async_tx_channel_switch to properly inject a channel switch interrupt in
    the descriptor stream.  This simplifies the locking model as drivers no
    longer need to handle dma_async_tx_descriptor.lock.
    
    Acked-by: Shannon Nelson <shannon.nelson@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 34d440698293..91252a7e4d03 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -221,11 +221,9 @@ typedef void (*dma_async_tx_callback)(void *dma_async_param);
  * @callback: routine to call after this operation is complete
  * @callback_param: general parameter to pass to the callback routine
  * ---async_tx api specific fields---
- * @depend_list: at completion this list of transactions are submitted
- * @depend_node: allow this transaction to be executed after another
- *	transaction has completed, possibly on another channel
+ * @next: at completion submit this descriptor
  * @parent: pointer to the next level up in the dependency chain
- * @lock: protect the dependency list
+ * @lock: protect the parent and next pointers
  */
 struct dma_async_tx_descriptor {
 	dma_cookie_t cookie;
@@ -236,8 +234,7 @@ struct dma_async_tx_descriptor {
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
 	dma_async_tx_callback callback;
 	void *callback_param;
-	struct list_head depend_list;
-	struct list_head depend_node;
+	struct dma_async_tx_descriptor *next;
 	struct dma_async_tx_descriptor *parent;
 	spinlock_t lock;
 };

commit b2ddb9019ea13fb7b62d8e45adcc468376af0de7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Mar 29 03:09:38 2008 +0000

    dma_page_list ->base_address is a userland pointer
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 261e43a4c873..34d440698293 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -423,7 +423,7 @@ void dma_async_device_unregister(struct dma_device *device);
 /* --- Helper iov-locking functions --- */
 
 struct dma_page_list {
-	char *base_address;
+	char __user *base_address;
 	int nr_pages;
 	struct page **pages;
 };

commit ec8670f1f795badedaa056a3a3245b9b82201747
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Mar 1 07:51:29 2008 -0700

    dmaengine: fix sparse warning
    
    include/linux/dmaengine.h:364:2: warning: returning void-valued expression
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index acbb364674ff..261e43a4c873 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -366,7 +366,7 @@ __dma_has_cap(enum dma_transaction_type tx_type, dma_cap_mask_t *srcp)
  */
 static inline void dma_async_issue_pending(struct dma_chan *chan)
 {
-	return chan->device->device_issue_pending(chan);
+	chan->device->device_issue_pending(chan);
 }
 
 #define dma_async_memcpy_issue_pending(chan) dma_async_issue_pending(chan)

commit d4c56f97ff21df405d0cebe11f49e3c3c79662b5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Feb 2 19:49:58 2008 -0700

    async_tx: replace 'int_en' with operation preparation flags
    
    Pass a full set of flags to drivers' per-operation 'prep' routines.
    Currently the only flag passed is DMA_PREP_INTERRUPT.  The expectation is
    that arch-specific async_tx_find_channel() implementations can exploit this
    capability to find the best channel for an operation.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Shannon Nelson <shannon.nelson@intel.com>
    Reviewed-by: Haavard Skinnemoen <hskinnemoen@atmel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index b0864f5b729d..acbb364674ff 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -94,6 +94,15 @@ enum dma_transaction_type {
 /* last transaction type for creation of the capabilities mask */
 #define DMA_TX_TYPE_END (DMA_INTERRUPT + 1)
 
+/**
+ * enum dma_prep_flags - DMA flags to augment operation preparation
+ * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
+ * 	this transaction
+ */
+enum dma_prep_flags {
+	DMA_PREP_INTERRUPT = (1 << 0),
+};
+
 /**
  * dma_cap_mask_t - capabilities bitmap modeled after cpumask_t.
  * See linux/cpumask.h
@@ -274,16 +283,16 @@ struct dma_device {
 
 	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(
 		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
-		size_t len, int int_en);
+		size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(
 		struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
-		unsigned int src_cnt, size_t len, int int_en);
+		unsigned int src_cnt, size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_zero_sum)(
 		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,
-		size_t len, u32 *result, int int_en);
+		size_t len, u32 *result, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
-		int int_en);
+		unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan);
 

commit 0036731c88fdb5bf4f04a796a30b5e445fc57f54
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Feb 2 19:49:57 2008 -0700

    async_tx: kill tx_set_src and tx_set_dest methods
    
    The tx_set_src and tx_set_dest methods were originally implemented to allow
    an array of addresses to be passed down from async_xor to the dmaengine
    driver while minimizing stack overhead.  Removing these methods allows
    drivers to have all transaction parameters available at 'prep' time, saves
    two function pointers in struct dma_async_tx_descriptor, and reduces the
    number of indirect branches..
    
    A consequence of moving this data to the 'prep' routine is that
    multi-source routines like async_xor need temporary storage to convert an
    array of linear addresses into an array of dma addresses.  In order to keep
    the same stack footprint of the previous implementation the input array is
    reused as storage for the dma addresses.  This requires that
    sizeof(dma_addr_t) be less than or equal to sizeof(void *).  As a
    consequence CONFIG_DMADEVICES now depends on !CONFIG_HIGHMEM64G.  It also
    requires that drivers be able to make descriptor resources available when
    the 'prep' routine is polled.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Shannon Nelson <shannon.nelson@intel.com>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 5c84bf897593..b0864f5b729d 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -209,8 +209,6 @@ typedef void (*dma_async_tx_callback)(void *dma_async_param);
  *	descriptors
  * @chan: target channel for this operation
  * @tx_submit: set the prepared descriptor(s) to be executed by the engine
- * @tx_set_dest: set a destination address in a hardware descriptor
- * @tx_set_src: set a source address in a hardware descriptor
  * @callback: routine to call after this operation is complete
  * @callback_param: general parameter to pass to the callback routine
  * ---async_tx api specific fields---
@@ -227,10 +225,6 @@ struct dma_async_tx_descriptor {
 	struct list_head tx_list;
 	struct dma_chan *chan;
 	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
-	void (*tx_set_dest)(dma_addr_t addr,
-		struct dma_async_tx_descriptor *tx, int index);
-	void (*tx_set_src)(dma_addr_t addr,
-		struct dma_async_tx_descriptor *tx, int index);
 	dma_async_tx_callback callback;
 	void *callback_param;
 	struct list_head depend_list;
@@ -279,15 +273,17 @@ struct dma_device {
 	void (*device_free_chan_resources)(struct dma_chan *chan);
 
 	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(
-		struct dma_chan *chan, size_t len, int int_en);
+		struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		size_t len, int int_en);
 	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(
-		struct dma_chan *chan, unsigned int src_cnt, size_t len,
-		int int_en);
+		struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
+		unsigned int src_cnt, size_t len, int int_en);
 	struct dma_async_tx_descriptor *(*device_prep_dma_zero_sum)(
-		struct dma_chan *chan, unsigned int src_cnt, size_t len,
-		u32 *result, int int_en);
+		struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,
+		size_t len, u32 *result, int int_en);
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
-		struct dma_chan *chan, int value, size_t len, int int_en);
+		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
+		int int_en);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan);
 

commit fd3f8984f6fa1ad1a6c2283eef48ba6e5242bcc5
Author: Joe Perches <joe@perches.com>
Date:   Sun Feb 3 17:45:46 2008 +0200

    include/linux/: Spelling fixes
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 55c9a6952f44..5c84bf897593 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -29,7 +29,7 @@
 #include <linux/dma-mapping.h>
 
 /**
- * enum dma_state - resource PNP/power managment state
+ * enum dma_state - resource PNP/power management state
  * @DMA_RESOURCE_SUSPEND: DMA device going into low power state
  * @DMA_RESOURCE_RESUME: DMA device returning to full power
  * @DMA_RESOURCE_AVAILABLE: DMA device available to the system

commit 891f78ea833edd4a1e524e15bfe297a7a84d81a0
Author: Tony Jones <tonyj@suse.de>
Date:   Tue Sep 25 02:03:03 2007 +0200

    DMA: Convert from class_device to device for DMA engine
    
    Signed-off-by: Tony Jones <tonyj@suse.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Shannon Nelson <shannon.nelson@intel.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index a3b6035b6c86..55c9a6952f44 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -132,7 +132,7 @@ struct dma_chan {
 
 	/* sysfs */
 	int chan_id;
-	struct class_device class_dev;
+	struct device dev;
 
 	struct kref refcount;
 	int slow_ref;
@@ -142,6 +142,7 @@ struct dma_chan {
 	struct dma_chan_percpu *local;
 };
 
+#define to_dma_chan(p) container_of(p, struct dma_chan, dev)
 
 void dma_chan_cleanup(struct kref *kref);
 

commit d379b01e9087a582d58f4b678208a4f8d8376fe7
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Jul 9 11:56:42 2007 -0700

    dmaengine: make clients responsible for managing channels
    
    The current implementation assumes that a channel will only be used by one
    client at a time.  In order to enable channel sharing the dmaengine core is
    changed to a model where clients subscribe to channel-available-events.
    Instead of tracking how many channels a client wants and how many it has
    received the core just broadcasts the available channels and lets the
    clients optionally take a reference.  The core learns about the clients'
    needs at dma_event_callback time.
    
    In support of multiple operation types, clients can specify a capability
    mask to only be notified of channels that satisfy a certain set of
    capabilities.
    
    Changelog:
    * removed DMA_TX_ARRAY_INIT, no longer needed
    * dma_client_chan_free -> dma_chan_release: switch to global reference
      counting only at device unregistration time, before it was also happening
      at client unregistration time
    * clients now return dma_state_client to dmaengine (ack, dup, nak)
    * checkpatch.pl fixes
    * fixup merge with git-ioat
    
    Cc: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: Shannon Nelson <shannon.nelson@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 3de1cf71031a..a3b6035b6c86 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -29,19 +29,31 @@
 #include <linux/dma-mapping.h>
 
 /**
- * enum dma_event - resource PNP/power managment events
+ * enum dma_state - resource PNP/power managment state
  * @DMA_RESOURCE_SUSPEND: DMA device going into low power state
  * @DMA_RESOURCE_RESUME: DMA device returning to full power
- * @DMA_RESOURCE_ADDED: DMA device added to the system
+ * @DMA_RESOURCE_AVAILABLE: DMA device available to the system
  * @DMA_RESOURCE_REMOVED: DMA device removed from the system
  */
-enum dma_event {
+enum dma_state {
 	DMA_RESOURCE_SUSPEND,
 	DMA_RESOURCE_RESUME,
-	DMA_RESOURCE_ADDED,
+	DMA_RESOURCE_AVAILABLE,
 	DMA_RESOURCE_REMOVED,
 };
 
+/**
+ * enum dma_state_client - state of the channel in the client
+ * @DMA_ACK: client would like to use, or was using this channel
+ * @DMA_DUP: client has already seen this channel, or is not using this channel
+ * @DMA_NAK: client does not want to see any more channels
+ */
+enum dma_state_client {
+	DMA_ACK,
+	DMA_DUP,
+	DMA_NAK,
+};
+
 /**
  * typedef dma_cookie_t - an opaque DMA cookie
  *
@@ -104,7 +116,6 @@ struct dma_chan_percpu {
 
 /**
  * struct dma_chan - devices supply DMA channels, clients use them
- * @client: ptr to the client user of this chan, will be %NULL when unused
  * @device: ptr to the dma device who supplies this channel, always !%NULL
  * @cookie: last cookie value returned to client
  * @chan_id: channel ID for sysfs
@@ -112,12 +123,10 @@ struct dma_chan_percpu {
  * @refcount: kref, used in "bigref" slow-mode
  * @slow_ref: indicates that the DMA channel is free
  * @rcu: the DMA channel's RCU head
- * @client_node: used to add this to the client chan list
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
  */
 struct dma_chan {
-	struct dma_client *client;
 	struct dma_device *device;
 	dma_cookie_t cookie;
 
@@ -129,11 +138,11 @@ struct dma_chan {
 	int slow_ref;
 	struct rcu_head rcu;
 
-	struct list_head client_node;
 	struct list_head device_node;
 	struct dma_chan_percpu *local;
 };
 
+
 void dma_chan_cleanup(struct kref *kref);
 
 static inline void dma_chan_get(struct dma_chan *chan)
@@ -158,26 +167,31 @@ static inline void dma_chan_put(struct dma_chan *chan)
 
 /*
  * typedef dma_event_callback - function pointer to a DMA event callback
+ * For each channel added to the system this routine is called for each client.
+ * If the client would like to use the channel it returns '1' to signal (ack)
+ * the dmaengine core to take out a reference on the channel and its
+ * corresponding device.  A client must not 'ack' an available channel more
+ * than once.  When a channel is removed all clients are notified.  If a client
+ * is using the channel it must 'ack' the removal.  A client must not 'ack' a
+ * removed channel more than once.
+ * @client - 'this' pointer for the client context
+ * @chan - channel to be acted upon
+ * @state - available or removed
  */
-typedef void (*dma_event_callback) (struct dma_client *client,
-		struct dma_chan *chan, enum dma_event event);
+struct dma_client;
+typedef enum dma_state_client (*dma_event_callback) (struct dma_client *client,
+		struct dma_chan *chan, enum dma_state state);
 
 /**
  * struct dma_client - info on the entity making use of DMA services
  * @event_callback: func ptr to call when something happens
- * @chan_count: number of chans allocated
- * @chans_desired: number of chans requested. Can be +/- chan_count
- * @lock: protects access to the channels list
- * @channels: the list of DMA channels allocated
+ * @cap_mask: only return channels that satisfy the requested capabilities
+ *  a value of zero corresponds to any capability
  * @global_node: list_head for global dma_client_list
  */
 struct dma_client {
 	dma_event_callback	event_callback;
-	unsigned int		chan_count;
-	unsigned int		chans_desired;
-
-	spinlock_t		lock;
-	struct list_head	channels;
+	dma_cap_mask_t		cap_mask;
 	struct list_head	global_node;
 };
 
@@ -285,10 +299,9 @@ struct dma_device {
 
 /* --- public DMA engine API --- */
 
-struct dma_client *dma_async_client_register(dma_event_callback event_callback);
+void dma_async_client_register(struct dma_client *client);
 void dma_async_client_unregister(struct dma_client *client);
-void dma_async_client_chan_request(struct dma_client *client,
-		unsigned int number);
+void dma_async_client_chan_request(struct dma_client *client);
 dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len);
 dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
@@ -299,7 +312,6 @@ dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
 void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
 	struct dma_chan *chan);
 
-
 static inline void
 async_tx_ack(struct dma_async_tx_descriptor *tx)
 {

commit 7405f74badf46b5d023c5d2b670b4471525f6c91
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 2 11:10:43 2007 -0700

    dmaengine: refactor dmaengine around dma_async_tx_descriptor
    
    The current dmaengine interface defines mutliple routines per operation,
    i.e. dma_async_memcpy_buf_to_buf, dma_async_memcpy_buf_to_page etc.  Adding
    more operation types (xor, crc, etc) to this model would result in an
    unmanageable number of method permutations.
    
            Are we really going to add a set of hooks for each DMA engine
            whizbang feature?
                    - Jeff Garzik
    
    The descriptor creation process is refactored using the new common
    dma_async_tx_descriptor structure.  Instead of per driver
    do_<operation>_<dest>_to_<src> methods, drivers integrate
    dma_async_tx_descriptor into their private software descriptor and then
    define a 'prep' routine per operation.  The prep routine allocates a
    descriptor and ensures that the tx_set_src, tx_set_dest, tx_submit routines
    are valid.  Descriptor creation and submission becomes:
    
    struct dma_device *dev;
    struct dma_chan *chan;
    struct dma_async_tx_descriptor *tx;
    
    tx = dev->device_prep_dma_<operation>(chan, len, int_flag)
    tx->tx_set_src(dma_addr_t, tx, index /* for multi-source ops */)
    tx->tx_set_dest(dma_addr_t, tx, index)
    tx->tx_submit(tx)
    
    In addition to the refactoring, dma_async_tx_descriptor also lays the
    groundwork for definining cross-channel-operation dependencies, and a
    callback facility for asynchronous notification of operation completion.
    
    Changelog:
    * drop dma mapping methods, suggested by Chris Leech
    * fix ioat_dma_dependency_added, also caught by Andrew Morton
    * fix dma_sync_wait, change from Andrew Morton
    * uninline large functions, change from Andrew Morton
    * add tx->callback = NULL to dmaengine calls to interoperate with async_tx
      calls
    * hookup ioat_tx_submit
    * convert channel capabilities to a 'cpumask_t like' bitmap
    * removed DMA_TX_ARRAY_INIT, no longer needed
    * checkpatch.pl fixes
    * make set_src, set_dest, and tx_submit descriptor specific methods
    * fixup git-ioat merge
    * move group_list and phys to dma_async_tx_descriptor
    
    Cc: Jeff Garzik <jeff@garzik.org>
    Cc: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: Shannon Nelson <shannon.nelson@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index c94d8f1d62e5..3de1cf71031a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -21,13 +21,12 @@
 #ifndef DMAENGINE_H
 #define DMAENGINE_H
 
-#ifdef CONFIG_DMA_ENGINE
-
 #include <linux/device.h>
 #include <linux/uio.h>
 #include <linux/kref.h>
 #include <linux/completion.h>
 #include <linux/rcupdate.h>
+#include <linux/dma-mapping.h>
 
 /**
  * enum dma_event - resource PNP/power managment events
@@ -64,6 +63,31 @@ enum dma_status {
 	DMA_ERROR,
 };
 
+/**
+ * enum dma_transaction_type - DMA transaction types/indexes
+ */
+enum dma_transaction_type {
+	DMA_MEMCPY,
+	DMA_XOR,
+	DMA_PQ_XOR,
+	DMA_DUAL_XOR,
+	DMA_PQ_UPDATE,
+	DMA_ZERO_SUM,
+	DMA_PQ_ZERO_SUM,
+	DMA_MEMSET,
+	DMA_MEMCPY_CRC32C,
+	DMA_INTERRUPT,
+};
+
+/* last transaction type for creation of the capabilities mask */
+#define DMA_TX_TYPE_END (DMA_INTERRUPT + 1)
+
+/**
+ * dma_cap_mask_t - capabilities bitmap modeled after cpumask_t.
+ * See linux/cpumask.h
+ */
+typedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;
+
 /**
  * struct dma_chan_percpu - the per-CPU part of struct dma_chan
  * @refcount: local_t used for open-coded "bigref" counting
@@ -157,48 +181,106 @@ struct dma_client {
 	struct list_head	global_node;
 };
 
+typedef void (*dma_async_tx_callback)(void *dma_async_param);
+/**
+ * struct dma_async_tx_descriptor - async transaction descriptor
+ * ---dma generic offload fields---
+ * @cookie: tracking cookie for this transaction, set to -EBUSY if
+ *	this tx is sitting on a dependency list
+ * @ack: the descriptor can not be reused until the client acknowledges
+ *	receipt, i.e. has has a chance to establish any dependency chains
+ * @phys: physical address of the descriptor
+ * @tx_list: driver common field for operations that require multiple
+ *	descriptors
+ * @chan: target channel for this operation
+ * @tx_submit: set the prepared descriptor(s) to be executed by the engine
+ * @tx_set_dest: set a destination address in a hardware descriptor
+ * @tx_set_src: set a source address in a hardware descriptor
+ * @callback: routine to call after this operation is complete
+ * @callback_param: general parameter to pass to the callback routine
+ * ---async_tx api specific fields---
+ * @depend_list: at completion this list of transactions are submitted
+ * @depend_node: allow this transaction to be executed after another
+ *	transaction has completed, possibly on another channel
+ * @parent: pointer to the next level up in the dependency chain
+ * @lock: protect the dependency list
+ */
+struct dma_async_tx_descriptor {
+	dma_cookie_t cookie;
+	int ack;
+	dma_addr_t phys;
+	struct list_head tx_list;
+	struct dma_chan *chan;
+	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);
+	void (*tx_set_dest)(dma_addr_t addr,
+		struct dma_async_tx_descriptor *tx, int index);
+	void (*tx_set_src)(dma_addr_t addr,
+		struct dma_async_tx_descriptor *tx, int index);
+	dma_async_tx_callback callback;
+	void *callback_param;
+	struct list_head depend_list;
+	struct list_head depend_node;
+	struct dma_async_tx_descriptor *parent;
+	spinlock_t lock;
+};
+
 /**
  * struct dma_device - info on the entity supplying DMA services
  * @chancnt: how many DMA channels are supported
  * @channels: the list of struct dma_chan
  * @global_node: list_head for global dma_device_list
+ * @cap_mask: one or more dma_capability flags
+ * @max_xor: maximum number of xor sources, 0 if no capability
  * @refcount: reference count
  * @done: IO completion struct
  * @dev_id: unique device ID
+ * @dev: struct device reference for dma mapping api
  * @device_alloc_chan_resources: allocate resources and return the
  *	number of allocated descriptors
  * @device_free_chan_resources: release DMA channel's resources
- * @device_memcpy_buf_to_buf: memcpy buf pointer to buf pointer
- * @device_memcpy_buf_to_pg: memcpy buf pointer to struct page
- * @device_memcpy_pg_to_pg: memcpy struct page/offset to struct page/offset
- * @device_memcpy_complete: poll the status of an IOAT DMA transaction
- * @device_memcpy_issue_pending: push appended descriptors to hardware
+ * @device_prep_dma_memcpy: prepares a memcpy operation
+ * @device_prep_dma_xor: prepares a xor operation
+ * @device_prep_dma_zero_sum: prepares a zero_sum operation
+ * @device_prep_dma_memset: prepares a memset operation
+ * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
+ * @device_dependency_added: async_tx notifies the channel about new deps
+ * @device_issue_pending: push pending transactions to hardware
  */
 struct dma_device {
 
 	unsigned int chancnt;
 	struct list_head channels;
 	struct list_head global_node;
+	dma_cap_mask_t  cap_mask;
+	int max_xor;
 
 	struct kref refcount;
 	struct completion done;
 
 	int dev_id;
+	struct device *dev;
 
 	int (*device_alloc_chan_resources)(struct dma_chan *chan);
 	void (*device_free_chan_resources)(struct dma_chan *chan);
-	dma_cookie_t (*device_memcpy_buf_to_buf)(struct dma_chan *chan,
-			void *dest, void *src, size_t len);
-	dma_cookie_t (*device_memcpy_buf_to_pg)(struct dma_chan *chan,
-			struct page *page, unsigned int offset, void *kdata,
-			size_t len);
-	dma_cookie_t (*device_memcpy_pg_to_pg)(struct dma_chan *chan,
-			struct page *dest_pg, unsigned int dest_off,
-			struct page *src_pg, unsigned int src_off, size_t len);
-	enum dma_status (*device_memcpy_complete)(struct dma_chan *chan,
+
+	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(
+		struct dma_chan *chan, size_t len, int int_en);
+	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(
+		struct dma_chan *chan, unsigned int src_cnt, size_t len,
+		int int_en);
+	struct dma_async_tx_descriptor *(*device_prep_dma_zero_sum)(
+		struct dma_chan *chan, unsigned int src_cnt, size_t len,
+		u32 *result, int int_en);
+	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
+		struct dma_chan *chan, int value, size_t len, int int_en);
+	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
+		struct dma_chan *chan);
+
+	void (*device_dependency_added)(struct dma_chan *chan);
+	enum dma_status (*device_is_tx_complete)(struct dma_chan *chan,
 			dma_cookie_t cookie, dma_cookie_t *last,
 			dma_cookie_t *used);
-	void (*device_memcpy_issue_pending)(struct dma_chan *chan);
+	void (*device_issue_pending)(struct dma_chan *chan);
 };
 
 /* --- public DMA engine API --- */
@@ -207,96 +289,72 @@ struct dma_client *dma_async_client_register(dma_event_callback event_callback);
 void dma_async_client_unregister(struct dma_client *client);
 void dma_async_client_chan_request(struct dma_client *client,
 		unsigned int number);
+dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
+	void *dest, void *src, size_t len);
+dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
+	struct page *page, unsigned int offset, void *kdata, size_t len);
+dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
+	struct page *dest_pg, unsigned int dest_off, struct page *src_pg,
+	unsigned int src_off, size_t len);
+void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
+	struct dma_chan *chan);
 
-/**
- * dma_async_memcpy_buf_to_buf - offloaded copy between virtual addresses
- * @chan: DMA channel to offload copy to
- * @dest: destination address (virtual)
- * @src: source address (virtual)
- * @len: length
- *
- * Both @dest and @src must be mappable to a bus address according to the
- * DMA mapping API rules for streaming mappings.
- * Both @dest and @src must stay memory resident (kernel memory or locked
- * user space pages).
- */
-static inline dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
-	void *dest, void *src, size_t len)
-{
-	int cpu = get_cpu();
-	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
-	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
-	put_cpu();
 
-	return chan->device->device_memcpy_buf_to_buf(chan, dest, src, len);
+static inline void
+async_tx_ack(struct dma_async_tx_descriptor *tx)
+{
+	tx->ack = 1;
 }
 
-/**
- * dma_async_memcpy_buf_to_pg - offloaded copy from address to page
- * @chan: DMA channel to offload copy to
- * @page: destination page
- * @offset: offset in page to copy to
- * @kdata: source address (virtual)
- * @len: length
- *
- * Both @page/@offset and @kdata must be mappable to a bus address according
- * to the DMA mapping API rules for streaming mappings.
- * Both @page/@offset and @kdata must stay memory resident (kernel memory or
- * locked user space pages)
- */
-static inline dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
-	struct page *page, unsigned int offset, void *kdata, size_t len)
+#define first_dma_cap(mask) __first_dma_cap(&(mask))
+static inline int __first_dma_cap(const dma_cap_mask_t *srcp)
 {
-	int cpu = get_cpu();
-	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
-	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
-	put_cpu();
+	return min_t(int, DMA_TX_TYPE_END,
+		find_first_bit(srcp->bits, DMA_TX_TYPE_END));
+}
 
-	return chan->device->device_memcpy_buf_to_pg(chan, page, offset,
-	                                             kdata, len);
+#define next_dma_cap(n, mask) __next_dma_cap((n), &(mask))
+static inline int __next_dma_cap(int n, const dma_cap_mask_t *srcp)
+{
+	return min_t(int, DMA_TX_TYPE_END,
+		find_next_bit(srcp->bits, DMA_TX_TYPE_END, n+1));
 }
 
-/**
- * dma_async_memcpy_pg_to_pg - offloaded copy from page to page
- * @chan: DMA channel to offload copy to
- * @dest_pg: destination page
- * @dest_off: offset in page to copy to
- * @src_pg: source page
- * @src_off: offset in page to copy from
- * @len: length
- *
- * Both @dest_page/@dest_off and @src_page/@src_off must be mappable to a bus
- * address according to the DMA mapping API rules for streaming mappings.
- * Both @dest_page/@dest_off and @src_page/@src_off must stay memory resident
- * (kernel memory or locked user space pages).
- */
-static inline dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
-	struct page *dest_pg, unsigned int dest_off, struct page *src_pg,
-	unsigned int src_off, size_t len)
+#define dma_cap_set(tx, mask) __dma_cap_set((tx), &(mask))
+static inline void
+__dma_cap_set(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)
 {
-	int cpu = get_cpu();
-	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
-	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
-	put_cpu();
+	set_bit(tx_type, dstp->bits);
+}
 
-	return chan->device->device_memcpy_pg_to_pg(chan, dest_pg, dest_off,
-	                                            src_pg, src_off, len);
+#define dma_has_cap(tx, mask) __dma_has_cap((tx), &(mask))
+static inline int
+__dma_has_cap(enum dma_transaction_type tx_type, dma_cap_mask_t *srcp)
+{
+	return test_bit(tx_type, srcp->bits);
 }
 
+#define for_each_dma_cap_mask(cap, mask) \
+	for ((cap) = first_dma_cap(mask);	\
+		(cap) < DMA_TX_TYPE_END;	\
+		(cap) = next_dma_cap((cap), (mask)))
+
 /**
- * dma_async_memcpy_issue_pending - flush pending copies to HW
+ * dma_async_issue_pending - flush pending transactions to HW
  * @chan: target DMA channel
  *
  * This allows drivers to push copies to HW in batches,
  * reducing MMIO writes where possible.
  */
-static inline void dma_async_memcpy_issue_pending(struct dma_chan *chan)
+static inline void dma_async_issue_pending(struct dma_chan *chan)
 {
-	return chan->device->device_memcpy_issue_pending(chan);
+	return chan->device->device_issue_pending(chan);
 }
 
+#define dma_async_memcpy_issue_pending(chan) dma_async_issue_pending(chan)
+
 /**
- * dma_async_memcpy_complete - poll for transaction completion
+ * dma_async_is_tx_complete - poll for transaction completion
  * @chan: DMA channel
  * @cookie: transaction identifier to check status of
  * @last: returns last completed cookie, can be NULL
@@ -306,12 +364,15 @@ static inline void dma_async_memcpy_issue_pending(struct dma_chan *chan)
  * internal state and can be used with dma_async_is_complete() to check
  * the status of multiple cookies without re-checking hardware state.
  */
-static inline enum dma_status dma_async_memcpy_complete(struct dma_chan *chan,
+static inline enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,
 	dma_cookie_t cookie, dma_cookie_t *last, dma_cookie_t *used)
 {
-	return chan->device->device_memcpy_complete(chan, cookie, last, used);
+	return chan->device->device_is_tx_complete(chan, cookie, last, used);
 }
 
+#define dma_async_memcpy_complete(chan, cookie, last, used)\
+	dma_async_is_tx_complete(chan, cookie, last, used)
+
 /**
  * dma_async_is_complete - test a cookie against chan state
  * @cookie: transaction identifier to test status of
@@ -334,6 +395,7 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 	return DMA_IN_PROGRESS;
 }
 
+enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);
 
 /* --- DMA device --- */
 
@@ -362,5 +424,4 @@ dma_cookie_t dma_memcpy_pg_to_iovec(struct dma_chan *chan, struct iovec *iov,
 	struct dma_pinned_list *pinned_list, struct page *page,
 	unsigned int offset, size_t len);
 
-#endif /* CONFIG_DMA_ENGINE */
 #endif /* DMAENGINE_H */

commit fe4ada2d6f0b746246e9b5bf0f4f2e4d3a07d26e
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Mon Jul 3 19:44:51 2006 -0700

    [IOAT]: fix header file kernel-doc
    
    Fix kernel-doc problems in include/linux/dmaengine.h:
    - add some fields/parameters
    - expand some descriptions
    - fix typos
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 272010a6078a..c94d8f1d62e5 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -44,7 +44,7 @@ enum dma_event {
 };
 
 /**
- * typedef dma_cookie_t
+ * typedef dma_cookie_t - an opaque DMA cookie
  *
  * if dma_cookie_t is >0 it's a DMA request cookie, <0 it's an error code
  */
@@ -80,14 +80,14 @@ struct dma_chan_percpu {
 
 /**
  * struct dma_chan - devices supply DMA channels, clients use them
- * @client: ptr to the client user of this chan, will be NULL when unused
- * @device: ptr to the dma device who supplies this channel, always !NULL
+ * @client: ptr to the client user of this chan, will be %NULL when unused
+ * @device: ptr to the dma device who supplies this channel, always !%NULL
  * @cookie: last cookie value returned to client
- * @chan_id:
- * @class_dev:
+ * @chan_id: channel ID for sysfs
+ * @class_dev: class device for sysfs
  * @refcount: kref, used in "bigref" slow-mode
- * @slow_ref:
- * @rcu:
+ * @slow_ref: indicates that the DMA channel is free
+ * @rcu: the DMA channel's RCU head
  * @client_node: used to add this to the client chan list
  * @device_node: used to add this to the device chan list
  * @local: per-cpu pointer to a struct dma_chan_percpu
@@ -162,10 +162,17 @@ struct dma_client {
  * @chancnt: how many DMA channels are supported
  * @channels: the list of struct dma_chan
  * @global_node: list_head for global dma_device_list
- * @refcount:
- * @done:
- * @dev_id:
- * Other func ptrs: used to make use of this device's capabilities
+ * @refcount: reference count
+ * @done: IO completion struct
+ * @dev_id: unique device ID
+ * @device_alloc_chan_resources: allocate resources and return the
+ *	number of allocated descriptors
+ * @device_free_chan_resources: release DMA channel's resources
+ * @device_memcpy_buf_to_buf: memcpy buf pointer to buf pointer
+ * @device_memcpy_buf_to_pg: memcpy buf pointer to struct page
+ * @device_memcpy_pg_to_pg: memcpy struct page/offset to struct page/offset
+ * @device_memcpy_complete: poll the status of an IOAT DMA transaction
+ * @device_memcpy_issue_pending: push appended descriptors to hardware
  */
 struct dma_device {
 
@@ -211,7 +218,7 @@ void dma_async_client_chan_request(struct dma_client *client,
  * Both @dest and @src must be mappable to a bus address according to the
  * DMA mapping API rules for streaming mappings.
  * Both @dest and @src must stay memory resident (kernel memory or locked
- * user space pages)
+ * user space pages).
  */
 static inline dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 	void *dest, void *src, size_t len)
@@ -225,7 +232,7 @@ static inline dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
 }
 
 /**
- * dma_async_memcpy_buf_to_pg - offloaded copy
+ * dma_async_memcpy_buf_to_pg - offloaded copy from address to page
  * @chan: DMA channel to offload copy to
  * @page: destination page
  * @offset: offset in page to copy to
@@ -250,18 +257,18 @@ static inline dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
 }
 
 /**
- * dma_async_memcpy_buf_to_pg - offloaded copy
+ * dma_async_memcpy_pg_to_pg - offloaded copy from page to page
  * @chan: DMA channel to offload copy to
- * @dest_page: destination page
+ * @dest_pg: destination page
  * @dest_off: offset in page to copy to
- * @src_page: source page
+ * @src_pg: source page
  * @src_off: offset in page to copy from
  * @len: length
  *
  * Both @dest_page/@dest_off and @src_page/@src_off must be mappable to a bus
  * address according to the DMA mapping API rules for streaming mappings.
  * Both @dest_page/@dest_off and @src_page/@src_off must stay memory resident
- * (kernel memory or locked user space pages)
+ * (kernel memory or locked user space pages).
  */
 static inline dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
 	struct page *dest_pg, unsigned int dest_off, struct page *src_pg,
@@ -278,7 +285,7 @@ static inline dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
 
 /**
  * dma_async_memcpy_issue_pending - flush pending copies to HW
- * @chan:
+ * @chan: target DMA channel
  *
  * This allows drivers to push copies to HW in batches,
  * reducing MMIO writes where possible.

commit 1c0f16e5cdff59f3b132a1b0c0d44a941f8813d2
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Tue Jun 27 02:53:56 2006 -0700

    [PATCH] Remove gratuitous inclusion of <linux/config.h> from <linux/dmaengine.h>
    
    We include config.h on the compiler command line. There's no need for it
    to be included again.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 78b236ca04f8..272010a6078a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -20,7 +20,7 @@
  */
 #ifndef DMAENGINE_H
 #define DMAENGINE_H
-#include <linux/config.h>
+
 #ifdef CONFIG_DMA_ENGINE
 
 #include <linux/device.h>

commit de5506e155276d385712c2aa1c2d9a27cd4ed947
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 17:50:37 2006 -0700

    [I/OAT]: Utility functions for offloading sk_buff to iovec copies
    
    Provides for pinning user space pages in memory, copying to iovecs,
    and copying from sk_buffs including fragmented and chained sk_buffs.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 30781546ac99..78b236ca04f8 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -333,5 +333,27 @@ static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
 int dma_async_device_register(struct dma_device *device);
 void dma_async_device_unregister(struct dma_device *device);
 
+/* --- Helper iov-locking functions --- */
+
+struct dma_page_list {
+	char *base_address;
+	int nr_pages;
+	struct page **pages;
+};
+
+struct dma_pinned_list {
+	int nr_iovecs;
+	struct dma_page_list page_list[0];
+};
+
+struct dma_pinned_list *dma_pin_iovec_pages(struct iovec *iov, size_t len);
+void dma_unpin_iovec_pages(struct dma_pinned_list* pinned_list);
+
+dma_cookie_t dma_memcpy_to_iovec(struct dma_chan *chan, struct iovec *iov,
+	struct dma_pinned_list *pinned_list, unsigned char *kdata, size_t len);
+dma_cookie_t dma_memcpy_pg_to_iovec(struct dma_chan *chan, struct iovec *iov,
+	struct dma_pinned_list *pinned_list, struct page *page,
+	unsigned int offset, size_t len);
+
 #endif /* CONFIG_DMA_ENGINE */
 #endif /* DMAENGINE_H */

commit c13c8260da3155f2cefb63b0d1b7dcdcb405c644
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 17:18:44 2006 -0700

    [I/OAT]: DMA memcpy subsystem
    
    Provides an API for offloading memory copies to DMA devices
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
new file mode 100644
index 000000000000..30781546ac99
--- /dev/null
+++ b/include/linux/dmaengine.h
@@ -0,0 +1,337 @@
+/*
+ * Copyright(c) 2004 - 2006 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; either version 2 of the License, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+ *
+ * The full GNU General Public License is included in this distribution in the
+ * file called COPYING.
+ */
+#ifndef DMAENGINE_H
+#define DMAENGINE_H
+#include <linux/config.h>
+#ifdef CONFIG_DMA_ENGINE
+
+#include <linux/device.h>
+#include <linux/uio.h>
+#include <linux/kref.h>
+#include <linux/completion.h>
+#include <linux/rcupdate.h>
+
+/**
+ * enum dma_event - resource PNP/power managment events
+ * @DMA_RESOURCE_SUSPEND: DMA device going into low power state
+ * @DMA_RESOURCE_RESUME: DMA device returning to full power
+ * @DMA_RESOURCE_ADDED: DMA device added to the system
+ * @DMA_RESOURCE_REMOVED: DMA device removed from the system
+ */
+enum dma_event {
+	DMA_RESOURCE_SUSPEND,
+	DMA_RESOURCE_RESUME,
+	DMA_RESOURCE_ADDED,
+	DMA_RESOURCE_REMOVED,
+};
+
+/**
+ * typedef dma_cookie_t
+ *
+ * if dma_cookie_t is >0 it's a DMA request cookie, <0 it's an error code
+ */
+typedef s32 dma_cookie_t;
+
+#define dma_submit_error(cookie) ((cookie) < 0 ? 1 : 0)
+
+/**
+ * enum dma_status - DMA transaction status
+ * @DMA_SUCCESS: transaction completed successfully
+ * @DMA_IN_PROGRESS: transaction not yet processed
+ * @DMA_ERROR: transaction failed
+ */
+enum dma_status {
+	DMA_SUCCESS,
+	DMA_IN_PROGRESS,
+	DMA_ERROR,
+};
+
+/**
+ * struct dma_chan_percpu - the per-CPU part of struct dma_chan
+ * @refcount: local_t used for open-coded "bigref" counting
+ * @memcpy_count: transaction counter
+ * @bytes_transferred: byte counter
+ */
+
+struct dma_chan_percpu {
+	local_t refcount;
+	/* stats */
+	unsigned long memcpy_count;
+	unsigned long bytes_transferred;
+};
+
+/**
+ * struct dma_chan - devices supply DMA channels, clients use them
+ * @client: ptr to the client user of this chan, will be NULL when unused
+ * @device: ptr to the dma device who supplies this channel, always !NULL
+ * @cookie: last cookie value returned to client
+ * @chan_id:
+ * @class_dev:
+ * @refcount: kref, used in "bigref" slow-mode
+ * @slow_ref:
+ * @rcu:
+ * @client_node: used to add this to the client chan list
+ * @device_node: used to add this to the device chan list
+ * @local: per-cpu pointer to a struct dma_chan_percpu
+ */
+struct dma_chan {
+	struct dma_client *client;
+	struct dma_device *device;
+	dma_cookie_t cookie;
+
+	/* sysfs */
+	int chan_id;
+	struct class_device class_dev;
+
+	struct kref refcount;
+	int slow_ref;
+	struct rcu_head rcu;
+
+	struct list_head client_node;
+	struct list_head device_node;
+	struct dma_chan_percpu *local;
+};
+
+void dma_chan_cleanup(struct kref *kref);
+
+static inline void dma_chan_get(struct dma_chan *chan)
+{
+	if (unlikely(chan->slow_ref))
+		kref_get(&chan->refcount);
+	else {
+		local_inc(&(per_cpu_ptr(chan->local, get_cpu())->refcount));
+		put_cpu();
+	}
+}
+
+static inline void dma_chan_put(struct dma_chan *chan)
+{
+	if (unlikely(chan->slow_ref))
+		kref_put(&chan->refcount, dma_chan_cleanup);
+	else {
+		local_dec(&(per_cpu_ptr(chan->local, get_cpu())->refcount));
+		put_cpu();
+	}
+}
+
+/*
+ * typedef dma_event_callback - function pointer to a DMA event callback
+ */
+typedef void (*dma_event_callback) (struct dma_client *client,
+		struct dma_chan *chan, enum dma_event event);
+
+/**
+ * struct dma_client - info on the entity making use of DMA services
+ * @event_callback: func ptr to call when something happens
+ * @chan_count: number of chans allocated
+ * @chans_desired: number of chans requested. Can be +/- chan_count
+ * @lock: protects access to the channels list
+ * @channels: the list of DMA channels allocated
+ * @global_node: list_head for global dma_client_list
+ */
+struct dma_client {
+	dma_event_callback	event_callback;
+	unsigned int		chan_count;
+	unsigned int		chans_desired;
+
+	spinlock_t		lock;
+	struct list_head	channels;
+	struct list_head	global_node;
+};
+
+/**
+ * struct dma_device - info on the entity supplying DMA services
+ * @chancnt: how many DMA channels are supported
+ * @channels: the list of struct dma_chan
+ * @global_node: list_head for global dma_device_list
+ * @refcount:
+ * @done:
+ * @dev_id:
+ * Other func ptrs: used to make use of this device's capabilities
+ */
+struct dma_device {
+
+	unsigned int chancnt;
+	struct list_head channels;
+	struct list_head global_node;
+
+	struct kref refcount;
+	struct completion done;
+
+	int dev_id;
+
+	int (*device_alloc_chan_resources)(struct dma_chan *chan);
+	void (*device_free_chan_resources)(struct dma_chan *chan);
+	dma_cookie_t (*device_memcpy_buf_to_buf)(struct dma_chan *chan,
+			void *dest, void *src, size_t len);
+	dma_cookie_t (*device_memcpy_buf_to_pg)(struct dma_chan *chan,
+			struct page *page, unsigned int offset, void *kdata,
+			size_t len);
+	dma_cookie_t (*device_memcpy_pg_to_pg)(struct dma_chan *chan,
+			struct page *dest_pg, unsigned int dest_off,
+			struct page *src_pg, unsigned int src_off, size_t len);
+	enum dma_status (*device_memcpy_complete)(struct dma_chan *chan,
+			dma_cookie_t cookie, dma_cookie_t *last,
+			dma_cookie_t *used);
+	void (*device_memcpy_issue_pending)(struct dma_chan *chan);
+};
+
+/* --- public DMA engine API --- */
+
+struct dma_client *dma_async_client_register(dma_event_callback event_callback);
+void dma_async_client_unregister(struct dma_client *client);
+void dma_async_client_chan_request(struct dma_client *client,
+		unsigned int number);
+
+/**
+ * dma_async_memcpy_buf_to_buf - offloaded copy between virtual addresses
+ * @chan: DMA channel to offload copy to
+ * @dest: destination address (virtual)
+ * @src: source address (virtual)
+ * @len: length
+ *
+ * Both @dest and @src must be mappable to a bus address according to the
+ * DMA mapping API rules for streaming mappings.
+ * Both @dest and @src must stay memory resident (kernel memory or locked
+ * user space pages)
+ */
+static inline dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan,
+	void *dest, void *src, size_t len)
+{
+	int cpu = get_cpu();
+	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
+	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
+	put_cpu();
+
+	return chan->device->device_memcpy_buf_to_buf(chan, dest, src, len);
+}
+
+/**
+ * dma_async_memcpy_buf_to_pg - offloaded copy
+ * @chan: DMA channel to offload copy to
+ * @page: destination page
+ * @offset: offset in page to copy to
+ * @kdata: source address (virtual)
+ * @len: length
+ *
+ * Both @page/@offset and @kdata must be mappable to a bus address according
+ * to the DMA mapping API rules for streaming mappings.
+ * Both @page/@offset and @kdata must stay memory resident (kernel memory or
+ * locked user space pages)
+ */
+static inline dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan,
+	struct page *page, unsigned int offset, void *kdata, size_t len)
+{
+	int cpu = get_cpu();
+	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
+	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
+	put_cpu();
+
+	return chan->device->device_memcpy_buf_to_pg(chan, page, offset,
+	                                             kdata, len);
+}
+
+/**
+ * dma_async_memcpy_buf_to_pg - offloaded copy
+ * @chan: DMA channel to offload copy to
+ * @dest_page: destination page
+ * @dest_off: offset in page to copy to
+ * @src_page: source page
+ * @src_off: offset in page to copy from
+ * @len: length
+ *
+ * Both @dest_page/@dest_off and @src_page/@src_off must be mappable to a bus
+ * address according to the DMA mapping API rules for streaming mappings.
+ * Both @dest_page/@dest_off and @src_page/@src_off must stay memory resident
+ * (kernel memory or locked user space pages)
+ */
+static inline dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan,
+	struct page *dest_pg, unsigned int dest_off, struct page *src_pg,
+	unsigned int src_off, size_t len)
+{
+	int cpu = get_cpu();
+	per_cpu_ptr(chan->local, cpu)->bytes_transferred += len;
+	per_cpu_ptr(chan->local, cpu)->memcpy_count++;
+	put_cpu();
+
+	return chan->device->device_memcpy_pg_to_pg(chan, dest_pg, dest_off,
+	                                            src_pg, src_off, len);
+}
+
+/**
+ * dma_async_memcpy_issue_pending - flush pending copies to HW
+ * @chan:
+ *
+ * This allows drivers to push copies to HW in batches,
+ * reducing MMIO writes where possible.
+ */
+static inline void dma_async_memcpy_issue_pending(struct dma_chan *chan)
+{
+	return chan->device->device_memcpy_issue_pending(chan);
+}
+
+/**
+ * dma_async_memcpy_complete - poll for transaction completion
+ * @chan: DMA channel
+ * @cookie: transaction identifier to check status of
+ * @last: returns last completed cookie, can be NULL
+ * @used: returns last issued cookie, can be NULL
+ *
+ * If @last and @used are passed in, upon return they reflect the driver
+ * internal state and can be used with dma_async_is_complete() to check
+ * the status of multiple cookies without re-checking hardware state.
+ */
+static inline enum dma_status dma_async_memcpy_complete(struct dma_chan *chan,
+	dma_cookie_t cookie, dma_cookie_t *last, dma_cookie_t *used)
+{
+	return chan->device->device_memcpy_complete(chan, cookie, last, used);
+}
+
+/**
+ * dma_async_is_complete - test a cookie against chan state
+ * @cookie: transaction identifier to test status of
+ * @last_complete: last know completed transaction
+ * @last_used: last cookie value handed out
+ *
+ * dma_async_is_complete() is used in dma_async_memcpy_complete()
+ * the test logic is seperated for lightweight testing of multiple cookies
+ */
+static inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,
+			dma_cookie_t last_complete, dma_cookie_t last_used)
+{
+	if (last_complete <= last_used) {
+		if ((cookie <= last_complete) || (cookie > last_used))
+			return DMA_SUCCESS;
+	} else {
+		if ((cookie <= last_complete) && (cookie > last_used))
+			return DMA_SUCCESS;
+	}
+	return DMA_IN_PROGRESS;
+}
+
+
+/* --- DMA device --- */
+
+int dma_async_device_register(struct dma_device *device);
+void dma_async_device_unregister(struct dma_device *device);
+
+#endif /* CONFIG_DMA_ENGINE */
+#endif /* DMAENGINE_H */
