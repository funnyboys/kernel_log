commit 764e515f41c82220b20099962f47ec952c9ca6de
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Thu May 28 09:35:11 2020 -0500

    KVM: Replace zero-length array with flexible-array
    
    There is a regular need in the kernel to provide a way to declare having a
    dynamically sized set of trailing elements in a structure. Kernel code should
    always use “flexible array members”[1] for these cases. The older style of
    one-element or zero-length arrays should no longer be used[2].
    
    [1] https://en.wikipedia.org/wiki/Flexible_array_member
    [2] https://github.com/KSPP/linux/issues/21
    
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 62ec926c78a0..d564855243d8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -409,7 +409,7 @@ struct kvm_irq_routing_table {
 	 * Array indexed by gsi. Each entry contains list of irq chips
 	 * the gsi is connected to.
 	 */
-	struct hlist_head map[0];
+	struct hlist_head map[];
 };
 #endif
 

commit 2a18b7e7cd8882f626316c340c6f2fca49b5fa12
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed Jun 10 19:55:32 2020 +0200

    KVM: async_pf: Inject 'page ready' event only if 'page not present' was previously injected
    
    'Page not present' event may or may not get injected depending on
    guest's state. If the event wasn't injected, there is no need to
    inject the corresponding 'page ready' event as the guest may get
    confused. E.g. Linux thinks that the corresponding 'page not present'
    event wasn't delivered *yet* and allocates a 'dummy entry' for it.
    This entry is never freed.
    
    Note, 'wakeup all' events have no corresponding 'page not present'
    event and always get injected.
    
    s390 seems to always be able to inject 'page not present', the
    change is effectively a nop.
    
    Suggested-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200610175532.779793-2-vkuznets@redhat.com>
    Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=208081
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e2f82131bb3e..62ec926c78a0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -206,6 +206,7 @@ struct kvm_async_pf {
 	unsigned long addr;
 	struct kvm_arch_async_pf arch;
 	bool   wakeup_all;
+	bool notpresent_injected;
 };
 
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);

commit e649b3f0188f8fd34dd0dde8d43fd3312b902fb2
Author: Eiichi Tsukata <eiichi.tsukata@nutanix.com>
Date:   Sat Jun 6 13:26:27 2020 +0900

    KVM: x86: Fix APIC page invalidation race
    
    Commit b1394e745b94 ("KVM: x86: fix APIC page invalidation") tried
    to fix inappropriate APIC page invalidation by re-introducing arch
    specific kvm_arch_mmu_notifier_invalidate_range() and calling it from
    kvm_mmu_notifier_invalidate_range_start. However, the patch left a
    possible race where the VMCS APIC address cache is updated *before*
    it is unmapped:
    
      (Invalidator) kvm_mmu_notifier_invalidate_range_start()
      (Invalidator) kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD)
      (KVM VCPU) vcpu_enter_guest()
      (KVM VCPU) kvm_vcpu_reload_apic_access_page()
      (Invalidator) actually unmap page
    
    Because of the above race, there can be a mismatch between the
    host physical address stored in the APIC_ACCESS_PAGE VMCS field and
    the host physical address stored in the EPT entry for the APIC GPA
    (0xfee0000).  When this happens, the processor will not trap APIC
    accesses, and will instead show the raw contents of the APIC-access page.
    Because Windows OS periodically checks for unexpected modifications to
    the LAPIC register, this will show up as a BSOD crash with BugCheck
    CRITICAL_STRUCTURE_CORRUPTION (109) we are currently seeing in
    https://bugzilla.redhat.com/show_bug.cgi?id=1751017.
    
    The root cause of the issue is that kvm_arch_mmu_notifier_invalidate_range()
    cannot guarantee that no additional references are taken to the pages in
    the range before kvm_mmu_notifier_invalidate_range_end().  Fortunately,
    this case is supported by the MMU notifier API, as documented in
    include/linux/mmu_notifier.h:
    
             * If the subsystem
             * can't guarantee that no additional references are taken to
             * the pages in the range, it has to implement the
             * invalidate_range() notifier to remove any references taken
             * after invalidate_range_start().
    
    The fix therefore is to reload the APIC-access page field in the VMCS
    from kvm_mmu_notifier_invalidate_range() instead of ..._range_start().
    
    Cc: stable@vger.kernel.org
    Fixes: b1394e745b94 ("KVM: x86: fix APIC page invalidation")
    Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=197951
    Signed-off-by: Eiichi Tsukata <eiichi.tsukata@nutanix.com>
    Message-Id: <20200606042627.61070-1-eiichi.tsukata@nutanix.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d38d6b9c24be..e2f82131bb3e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1420,8 +1420,8 @@ static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
 }
 #endif /* CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL */
 
-int kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
-		unsigned long start, unsigned long end, bool blockable);
+void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
+					    unsigned long start, unsigned long end);
 
 #ifdef CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE
 int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu);

commit d56f5136b01020155b6b0a29f69d924687529bee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 4 15:16:52 2020 +0200

    KVM: let kvm_destroy_vm_debugfs clean up vCPU debugfs directories
    
    After commit 63d0434 ("KVM: x86: move kvm_create_vcpu_debugfs after
    last failure point") we are creating the pre-vCPU debugfs files
    after the creation of the vCPU file descriptor.  This makes it
    possible for userspace to reach kvm_vcpu_release before
    kvm_create_vcpu_debugfs has finished.  The vcpu->debugfs_dentry
    then does not have any associated inode anymore, and this causes
    a NULL-pointer dereference in debugfs_create_file.
    
    The solution is simply to avoid removing the files; they are
    cleaned up when the VM file descriptor is closed (and that must be
    after KVM_CREATE_VCPU returns).  We can stop storing the dentry
    in struct kvm_vcpu too, because it is not needed anywhere after
    kvm_create_vcpu_debugfs returns.
    
    Reported-by: syzbot+705f4401d5a93a59b87d@syzkaller.appspotmail.com
    Fixes: 63d04348371b ("KVM: x86: move kvm_create_vcpu_debugfs after last failure point")
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f43b59b1294c..d38d6b9c24be 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -318,7 +318,6 @@ struct kvm_vcpu {
 	bool preempted;
 	bool ready;
 	struct kvm_vcpu_arch arch;
-	struct dentry *debugfs_dentry;
 };
 
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
@@ -888,7 +887,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
-void kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
+void kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu, struct dentry *debugfs_dentry);
 #endif
 
 int kvm_arch_hardware_enable(void);

commit 0958f0cefede403037653e44de0e3332d10b0e1a
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon May 25 16:41:19 2020 +0200

    KVM: introduce kvm_read_guest_offset_cached()
    
    We already have kvm_write_guest_offset_cached(), introduce read analogue.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20200525144125.143875-5-vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 161684696610..f43b59b1294c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -734,6 +734,9 @@ int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
 int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len);
+int kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+				 void *data, unsigned int offset,
+				 unsigned long len);
 int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,

commit da4ad88cab5867ee240dfd0585e9d115a8cc47db
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 23 22:48:37 2020 -0700

    kvm: Replace vcpu->swait with rcuwait
    
    The use of any sort of waitqueue (simple or regular) for
    wait/waking vcpus has always been an overkill and semantically
    wrong. Because this is per-vcpu (which is blocked) there is
    only ever a single waiting vcpu, thus no need for any sort of
    queue.
    
    As such, make use of the rcuwait primitive, with the following
    considerations:
    
      - rcuwait already provides the proper barriers that serialize
      concurrent waiter and waker.
    
      - Task wakeup is done in rcu read critical region, with a
      stable task pointer.
    
      - Because there is no concurrency among waiters, we need
      not worry about rcuwait_wait_event() calls corrupting
      the wait->task. As a consequence, this saves the locking
      done in swait when modifying the queue. This also applies
      to per-vcore wait for powerpc kvm-hv.
    
    The x86 tscdeadline_latency test mentioned in 8577370fb0cb
    ("KVM: Use simple waitqueue for vcpu->wq") shows that, on avg,
    latency is reduced by around 15-20% with this change.
    
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-mips@vger.kernel.org
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Message-Id: <20200424054837.5138-6-dave@stgolabs.net>
    [Avoid extra logic changes. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index abfa71cb5d2d..161684696610 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -23,7 +23,7 @@
 #include <linux/irqflags.h>
 #include <linux/context_tracking.h>
 #include <linux/irqbypass.h>
-#include <linux/swait.h>
+#include <linux/rcuwait.h>
 #include <linux/refcount.h>
 #include <linux/nospec.h>
 #include <asm/signal.h>
@@ -277,7 +277,7 @@ struct kvm_vcpu {
 	struct mutex mutex;
 	struct kvm_run *run;
 
-	struct swait_queue_head wq;
+	struct rcuwait wait;
 	struct pid __rcu *pid;
 	int sigset_active;
 	sigset_t sigset;
@@ -960,12 +960,12 @@ static inline bool kvm_arch_has_assigned_device(struct kvm *kvm)
 }
 #endif
 
-static inline struct swait_queue_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+static inline struct rcuwait *kvm_arch_vcpu_get_wait(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_WQP
-	return vcpu->arch.wqp;
+	return vcpu->arch.waitp;
 #else
-	return &vcpu->wq;
+	return &vcpu->wait;
 #endif
 }
 

commit 4aef2ec9022b217f74d0f4c9b84081f07cc223d9
Merge: 7c67f54661fc 37486135d3a7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 12:14:05 2020 -0400

    Merge branch 'kvm-amd-fixes' into HEAD

commit 54163a346d4a0a1b93f2ff6dc1f488419a605fa9
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Wed May 6 08:17:53 2020 -0500

    KVM: Introduce kvm_make_all_cpus_request_except()
    
    This allows making request to all other vcpus except the one
    specified in the parameter.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Message-Id: <1588771076-73790-2-git-send-email-suravee.suthikulpanit@amd.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01276e3d01b9..131cc1527d68 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -813,8 +813,11 @@ void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
 
 bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
+				 struct kvm_vcpu *except,
 				 unsigned long *vcpu_bitmap, cpumask_var_t tmp);
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);
+bool kvm_make_all_cpus_request_except(struct kvm *kvm, unsigned int req,
+				      struct kvm_vcpu *except);
 bool kvm_make_cpus_request_mask(struct kvm *kvm, unsigned int req,
 				unsigned long *vcpu_bitmap);
 

commit acd05785e48c01edb2c4f4d014d28478b5f19fb5
Author: David Matlack <dmatlack@google.com>
Date:   Fri Apr 17 15:14:46 2020 -0700

    kvm: add capability for halt polling
    
    KVM_CAP_HALT_POLL is a per-VM capability that lets userspace
    control the halt-polling time, allowing halt-polling to be tuned or
    disabled on particular VMs.
    
    With dynamic halt-polling, a VM's VCPUs can poll from anywhere from
    [0, halt_poll_ns] on each halt. KVM_CAP_HALT_POLL sets the
    upper limit on the poll time.
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Jon Cargille <jcargill@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Message-Id: <20200417221446.108733-1-jcargill@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5285a5568208..3cc6ccbb1183 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -503,6 +503,7 @@ struct kvm {
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
 	pid_t userspace_pid;
+	unsigned int max_halt_poll_ns;
 };
 
 #define kvm_err(fmt, ...) \

commit 1b94f6f81007b4afaea3480ec018bc9236148961
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Thu Apr 16 13:10:57 2020 +0800

    KVM: Remove redundant argument to kvm_arch_vcpu_ioctl_run
    
    In earlier versions of kvm, 'kvm_run' was an independent structure
    and was not included in the vcpu structure. At present, 'kvm_run'
    is already included in the vcpu structure, so the parameter
    'kvm_run' is redundant.
    
    This patch simplifies the function definition, removes the extra
    'kvm_run' parameter, and extracts it from the 'kvm_vcpu' structure
    if necessary.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Message-Id: <20200416051057.26526-1-tianjia.zhang@linux.alibaba.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7d4f1eb70274..5285a5568208 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -866,7 +866,7 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state);
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 					struct kvm_guest_debug *dbg);
-int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run);
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu);
 
 int kvm_arch_init(void *opaque);
 void kvm_arch_exit(void);

commit c36b71503a2268206ebeda6697094ffb4e7e94c2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 16 09:48:07 2020 -0400

    KVM: x86/mmu: Avoid an extra memslot lookup in try_async_pf() for L2
    
    Create a new function kvm_is_visible_memslot() and use it from
    kvm_is_visible_gfn(); use the new function in try_async_pf() too,
    to avoid an extra memslot lookup.
    
    Opportunistically squish a multi-line comment into a single-line comment.
    
    Note, the end result, KVM_PFN_NOSLOT, is unchanged.
    
    Cc: Jim Mattson <jmattson@google.com>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 658215f6102c..7d4f1eb70274 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1357,6 +1357,12 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 }
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 
+static inline bool kvm_is_visible_memslot(struct kvm_memory_slot *memslot)
+{
+	return (memslot && memslot->id < KVM_USER_MEM_SLOTS &&
+		!(memslot->flags & KVM_MEMSLOT_INVALID));
+}
+
 struct kvm_vcpu *kvm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 

commit 812756a82ea51e3c7ff7ba5e6fa3f34345234bc7
Author: Emanuele Giuseppe Esposito <eesposit@redhat.com>
Date:   Tue Apr 14 17:56:25 2020 +0200

    kvm_host: unify VM_STAT and VCPU_STAT definitions in a single place
    
    The macros VM_STAT and VCPU_STAT are redundantly implemented in multiple
    files, each used by a different architecure to initialize the debugfs
    entries for statistics. Since they all have the same purpose, they can be
    unified in a single common definition in include/linux/kvm_host.h
    
    Signed-off-by: Emanuele Giuseppe Esposito <eesposit@redhat.com>
    Message-Id: <20200414155625.20559-1-eesposit@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01276e3d01b9..658215f6102c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1130,6 +1130,11 @@ struct kvm_stats_debugfs_item {
 #define KVM_DBGFS_GET_MODE(dbgfs_item)                                         \
 	((dbgfs_item)->mode ? (dbgfs_item)->mode : 0644)
 
+#define VM_STAT(n, x, ...) 							\
+	{ n, offsetof(struct kvm, stat.x), KVM_STAT_VM, ## __VA_ARGS__ }
+#define VCPU_STAT(n, x, ...)							\
+	{ n, offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU, ## __VA_ARGS__ }
+
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 

commit b6467ab142b708dd076f6186ca274f14af379c72
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Apr 7 23:40:58 2020 -0700

    KVM: Check validity of resolved slot when searching memslots
    
    Check that the resolved slot (somewhat confusingly named 'start') is a
    valid/allocated slot before doing the final comparison to see if the
    specified gfn resides in the associated slot.  The resolved slot can be
    invalid if the binary search loop terminated because the search index
    was incremented beyond the number of used slots.
    
    This bug has existed since the binary search algorithm was introduced,
    but went unnoticed because KVM statically allocated memory for the max
    number of slots, i.e. the access would only be truly out-of-bounds if
    all possible slots were allocated and the specified gfn was less than
    the base of the lowest memslot.  Commit 36947254e5f98 ("KVM: Dynamically
    size memslot array based on number of used slots") eliminated the "all
    possible slots allocated" condition and made the bug embarrasingly easy
    to hit.
    
    Fixes: 9c1a5d38780e6 ("kvm: optimize GFN to memslot lookup with large slots amount")
    Reported-by: syzbot+d889b59b2bb87d4047a2@syzkaller.appspotmail.com
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200408064059.8957-2-sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6d58beb65454..01276e3d01b9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1048,7 +1048,7 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 			start = slot + 1;
 	}
 
-	if (gfn >= memslots[start].base_gfn &&
+	if (start < slots->used_slots && gfn >= memslots[start].base_gfn &&
 	    gfn < memslots[start].base_gfn + memslots[start].npages) {
 		atomic_set(&slots->lru_slot, start);
 		return &memslots[start];

commit b990408537388e9174b642ad36cdef6c47c64d3a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:25:55 2020 -0700

    KVM: Pass kvm_init()'s opaque param to additional arch funcs
    
    Pass @opaque to kvm_arch_hardware_setup() and
    kvm_arch_check_processor_compat() to allow architecture specific code to
    reference @opaque without having to stash it away in a temporary global
    variable.  This will enable x86 to separate its vendor specific callback
    ops, which are passed via @opaque, into "init" and "runtime" ops without
    having to stash away the "init" ops.
    
    No functional change intended.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Tested-by: Cornelia Huck <cohuck@redhat.com> #s390
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-2-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f6a1905da9bf..6d58beb65454 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -886,9 +886,9 @@ void kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
 
 int kvm_arch_hardware_enable(void);
 void kvm_arch_hardware_disable(void);
-int kvm_arch_hardware_setup(void);
+int kvm_arch_hardware_setup(void *opaque);
 void kvm_arch_hardware_unsetup(void);
-int kvm_arch_check_processor_compat(void);
+int kvm_arch_check_processor_compat(void *opaque);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);

commit cf39d37539068d53e015d8b4f1dcf42c65306b0d
Merge: 830948eb6826 463050599742
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 31 10:44:53 2020 -0400

    Merge tag 'kvmarm-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for Linux 5.7
    
    - GICv4.1 support
    - 32bit host removal

commit 0774a964ef561b7170d8d1b1bfe6f88002b6d219
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Mar 20 13:55:40 2020 -0700

    KVM: Fix out of range accesses to memslots
    
    Reset the LRU slot if it becomes invalid when deleting a memslot to fix
    an out-of-bounds/use-after-free access when searching through memslots.
    
    Explicitly check for there being no used slots in search_memslots(), and
    in the caller of s390's approximation variant.
    
    Fixes: 36947254e5f9 ("KVM: Dynamically size memslot array based on number of used slots")
    Reported-by: Qian Cai <cai@lca.pw>
    Cc: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 35bc52e187a2..b19dee4ed7d9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1032,6 +1032,9 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 	int slot = atomic_read(&slots->lru_slot);
 	struct kvm_memory_slot *memslots = slots->memslots;
 
+	if (unlikely(!slots->used_slots))
+		return NULL;
+
 	if (gfn >= memslots[slot].base_gfn &&
 	    gfn < memslots[slot].base_gfn + memslots[slot].npages)
 		return &memslots[slot];

commit 600087b6146764999949b4a12ce5f7627602c33a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Mar 2 15:57:05 2020 -0800

    KVM: Drop largepages_enabled and its accessor/mutator
    
    Drop largepages_enabled, kvm_largepages_enabled() and
    kvm_disable_largepages() now that all users are gone.
    
    Note, largepages_enabled was an x86-only flag that got left in common
    KVM code when KVM gained support for multiple architectures.
    
    No functional change intended.
    
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0ed394162b68..35bc52e187a2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -693,8 +693,6 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change);
-bool kvm_largepages_enabled(void);
-void kvm_disable_largepages(void);
 /* flush all memory translations */
 void kvm_arch_flush_shadow_all(struct kvm *kvm);
 /* flush memory translations pointing to 'slot' */

commit 2bde08f9f5f13ef2674674a2e3d7420abd08be33
Author: Peter Xu <peterx@redhat.com>
Date:   Wed Mar 4 12:51:52 2020 -0500

    KVM: Drop gfn_to_pfn_atomic()
    
    It's never used anywhere now.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 127cb086ba32..0ed394162b68 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -714,7 +714,6 @@ void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
-kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
 kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);

commit 3c9bd4006bfc2dccda1823db61b3f470ef91cfaa
Author: Jay Zhou <jianjay.zhou@huawei.com>
Date:   Thu Feb 27 09:32:27 2020 +0800

    KVM: x86: enable dirty log gradually in small chunks
    
    It could take kvm->mmu_lock for an extended period of time when
    enabling dirty log for the first time. The main cost is to clear
    all the D-bits of last level SPTEs. This situation can benefit from
    manual dirty log protect as well, which can reduce the mmu_lock
    time taken. The sequence is like this:
    
    1. Initialize all the bits of the dirty bitmap to 1 when enabling
       dirty log for the first time
    2. Only write protect the huge pages
    3. KVM_GET_DIRTY_LOG returns the dirty bitmap info
    4. KVM_CLEAR_DIRTY_LOG will clear D-bit for each of the leaf level
       SPTEs gradually in small chunks
    
    Under the Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz environment,
    I did some tests with a 128G windows VM and counted the time taken
    of memory_global_dirty_log_start, here is the numbers:
    
    VM Size        Before    After optimization
    128G           460ms     10ms
    
    Signed-off-by: Jay Zhou <jianjay.zhou@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4bd5251b4477..127cb086ba32 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -360,6 +360,10 @@ static inline unsigned long *kvm_second_dirty_bitmap(struct kvm_memory_slot *mem
 	return memslot->dirty_bitmap + len / sizeof(*memslot->dirty_bitmap);
 }
 
+#ifndef KVM_DIRTY_LOG_MANUAL_CAPS
+#define KVM_DIRTY_LOG_MANUAL_CAPS KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE
+#endif
+
 struct kvm_s390_adapter_int {
 	u64 ind_addr;
 	u64 summary_addr;
@@ -493,7 +497,7 @@ struct kvm {
 #endif
 	long tlbs_dirty;
 	struct list_head devices;
-	bool manual_dirty_log_protect;
+	u64 manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
@@ -527,6 +531,11 @@ struct kvm {
 #define vcpu_err(vcpu, fmt, ...)					\
 	kvm_err("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 
+static inline bool kvm_dirty_log_manual_protect_and_init_set(struct kvm *kvm)
+{
+	return !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);
+}
+
 static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 {
 	return srcu_dereference_check(kvm->buses[idx], &kvm->srcu,

commit 36947254e5f981aeeedab1c7dfa35fc34d330e80
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:32 2020 -0800

    KVM: Dynamically size memslot array based on number of used slots
    
    Now that the memslot logic doesn't assume memslots are always non-NULL,
    dynamically size the array of memslots instead of unconditionally
    allocating memory for the maximum number of memslots.
    
    Note, because a to-be-deleted memslot must first be invalidated, the
    array size cannot be immediately reduced when deleting a memslot.
    However, consecutive deletions will realize the memory savings, i.e.
    a second deletion will trim the entry.
    
    Tested-by: Christoffer Dall <christoffer.dall@arm.com>
    Tested-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 20763598b13b..4bd5251b4477 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -431,11 +431,11 @@ static inline int kvm_arch_vcpu_memslots_id(struct kvm_vcpu *vcpu)
  */
 struct kvm_memslots {
 	u64 generation;
-	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 	/* The mapping table from slot id to the index in memslots[]. */
 	short id_to_index[KVM_MEM_SLOTS_NUM];
 	atomic_t lru_slot;
 	int used_slots;
+	struct kvm_memory_slot memslots[];
 };
 
 struct kvm {

commit 0577d1abe704c315bb5cdfc71f4ca7b9b5358f59
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:31 2020 -0800

    KVM: Terminate memslot walks via used_slots
    
    Refactor memslot handling to treat the number of used slots as the de
    facto size of the memslot array, e.g. return NULL from id_to_memslot()
    when an invalid index is provided instead of relying on npages==0 to
    detect an invalid memslot.  Rework the sorting and walking of memslots
    in advance of dynamically sizing memslots to aid bisection and debug,
    e.g. with luck, a bug in the refactoring will bisect here and/or hit a
    WARN instead of randomly corrupting memory.
    
    Alternatively, a global null/invalid memslot could be returned, i.e. so
    callers of id_to_memslot() don't have to explicitly check for a NULL
    memslot, but that approach runs the risk of introducing difficult-to-
    debug issues, e.g. if the global null slot is modified.  Constifying
    the return from id_to_memslot() to combat such issues is possible, but
    would require a massive refactoring of arch specific code and would
    still be susceptible to casting shenanigans.
    
    Add function comments to update_memslots() and search_memslots() to
    explicitly (and loudly) state how memslots are sorted.
    
    Opportunistically stuff @hva with a non-canonical value when deleting a
    private memslot on x86 to detect bogus usage of the freed slot.
    
    No functional change intended.
    
    Tested-by: Christoffer Dall <christoffer.dall@arm.com>
    Tested-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 63ce6b21b107..20763598b13b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -572,10 +572,11 @@ static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
 	return vcpu->vcpu_idx;
 }
 
-#define kvm_for_each_memslot(memslot, slots)	\
-	for (memslot = &slots->memslots[0];	\
-	      memslot < slots->memslots + KVM_MEM_SLOTS_NUM && memslot->npages;\
-		memslot++)
+#define kvm_for_each_memslot(memslot, slots)				\
+	for (memslot = &slots->memslots[0];				\
+	     memslot < slots->memslots + slots->used_slots; memslot++)	\
+		if (WARN_ON_ONCE(!memslot->npages)) {			\
+		} else
 
 void kvm_vcpu_destroy(struct kvm_vcpu *vcpu);
 
@@ -635,12 +636,15 @@ static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 	return __kvm_memslots(vcpu->kvm, as_id);
 }
 
-static inline struct kvm_memory_slot *
-id_to_memslot(struct kvm_memslots *slots, int id)
+static inline
+struct kvm_memory_slot *id_to_memslot(struct kvm_memslots *slots, int id)
 {
 	int index = slots->id_to_index[id];
 	struct kvm_memory_slot *slot;
 
+	if (index < 0)
+		return NULL;
+
 	slot = &slots->memslots[index];
 
 	WARN_ON(slot->id != id);
@@ -1012,6 +1016,8 @@ bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args);
  * used in non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c.
  * gfn_to_memslot() itself isn't here as an inline because that would
  * bloat other code too much.
+ *
+ * IMPORTANT: Slots are sorted from highest GFN to lowest GFN!
  */
 static inline struct kvm_memory_slot *
 search_memslots(struct kvm_memslots *slots, gfn_t gfn)

commit 2a49f61dfcdc25ec06b41f7466ccb94a7a9d2624
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:30 2020 -0800

    KVM: Ensure validity of memslot with respect to kvm_get_dirty_log()
    
    Rework kvm_get_dirty_log() so that it "returns" the associated memslot
    on success.  A future patch will rework memslot handling such that
    id_to_memslot() can return NULL, returning the memslot makes it more
    obvious that the validity of the memslot has been verified, i.e.
    precludes the need to add validity checks in the arch code that are
    technically unnecessary.
    
    To maintain ordering in s390, move the call to kvm_arch_sync_dirty_log()
    from s390's kvm_vm_ioctl_get_dirty_log() to the new kvm_get_dirty_log().
    This is a nop for PPC, the only other arch that doesn't select
    KVM_GENERIC_DIRTYLOG_READ_PROTECT, as its sync_dirty_log() is empty.
    
    Ideally, moving the sync_dirty_log() call would be done in a separate
    patch, but it can't be done in a follow-on patch because that would
    temporarily break s390's ordering.  Making the move in a preparatory
    patch would be functionally correct, but would create an odd scenario
    where the moved sync_dirty_log() would operate on a "different" memslot
    due to consuming the result of a different id_to_memslot().  The
    memslot couldn't actually be different as slots_lock is held, but the
    code is confusing enough as it is, i.e. moving sync_dirty_log() in this
    patch is the lesser of all evils.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 35e6975d0a82..63ce6b21b107 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -828,7 +828,7 @@ void kvm_arch_flush_remote_tlbs_memslot(struct kvm *kvm,
 #else /* !CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT */
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log);
 int kvm_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log,
-		      int *is_dirty);
+		      int *is_dirty, struct kvm_memory_slot **memslot);
 #endif
 
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,

commit 0dff084607bd555d6f74db2af8406a9da9f0fc3a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:29 2020 -0800

    KVM: Provide common implementation for generic dirty log functions
    
    Move the implementations of KVM_GET_DIRTY_LOG and KVM_CLEAR_DIRTY_LOG
    for CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT into common KVM code.
    The arch specific implemenations are extremely similar, differing
    only in whether the dirty log needs to be sync'd from hardware (x86)
    and how the TLBs are flushed.  Add new arch hooks to handle sync
    and TLB flush; the sync will also be used for non-generic dirty log
    support in a future patch (s390).
    
    The ulterior motive for providing a common implementation is to
    eliminate the dependency between arch and common code with respect to
    the memslot referenced by the dirty log, i.e. to make it obvious in the
    code that the validity of the memslot is guaranteed, as a future patch
    will rework memslot handling such that id_to_memslot() can return NULL.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5404ef8be291..35e6975d0a82 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -816,23 +816,20 @@ vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);
 
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 
-int kvm_get_dirty_log(struct kvm *kvm,
-			struct kvm_dirty_log *log, int *is_dirty);
-
-int kvm_get_dirty_log_protect(struct kvm *kvm,
-			      struct kvm_dirty_log *log, bool *flush);
-int kvm_clear_dirty_log_protect(struct kvm *kvm,
-				struct kvm_clear_dirty_log *log, bool *flush);
-
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 					struct kvm_memory_slot *slot,
 					gfn_t gfn_offset,
 					unsigned long mask);
-
-int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
-				struct kvm_dirty_log *log);
-int kvm_vm_ioctl_clear_dirty_log(struct kvm *kvm,
-				  struct kvm_clear_dirty_log *log);
+void kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot);
+
+#ifdef CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT
+void kvm_arch_flush_remote_tlbs_memslot(struct kvm *kvm,
+					struct kvm_memory_slot *memslot);
+#else /* !CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT */
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log);
+int kvm_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log,
+		      int *is_dirty);
+#endif
 
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,
 			bool line_status);

commit e96c81ee89d80e1a0fe50a0e9be40c1b77e14aaa
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:27 2020 -0800

    KVM: Simplify kvm_free_memslot() and all its descendents
    
    Now that all callers of kvm_free_memslot() pass NULL for @dont, remove
    the param from the top-level routine and all arch's implementations.
    
    No functional change intended.
    
    Tested-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7827156ec1c9..5404ef8be291 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -669,8 +669,7 @@ int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem);
-void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
-			   struct kvm_memory_slot *dont);
+void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot);
 void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,

commit 9d4c197c0e94c372ceffd2ffc53a23518f301ed9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:24 2020 -0800

    KVM: Drop "const" attribute from old memslot in commit_memory_region()
    
    Drop the "const" attribute from @old in kvm_arch_commit_memory_region()
    to allow arch specific code to free arch specific resources in the old
    memslot without having to cast away the attribute.  Freeing resources in
    kvm_arch_commit_memory_region() paves the way for simplifying
    kvm_free_memslot() by eliminating the last usage of its @dont param.
    
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8f47f6b48444..7827156ec1c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -678,7 +678,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				enum kvm_mr_change change);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
-				const struct kvm_memory_slot *old,
+				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change);
 bool kvm_largepages_enabled(void);

commit 414de7abbf809f046511269797d9f2310b88e036
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:20 2020 -0800

    KVM: Drop kvm_arch_create_memslot()
    
    Remove kvm_arch_create_memslot() now that all arch implementations are
    effectively nops.  Removing kvm_arch_create_memslot() eliminates the
    possibility for arch specific code to allocate memory prior to setting
    a memslot, which sets the stage for simplifying kvm_free_memslot().
    
    Cc: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7944ad6ac10b..8f47f6b48444 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -671,8 +671,6 @@ int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem);
 void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
-int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
-			    unsigned long npages);
 void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,

commit fcd07f9adc7dacc2532695cf9dd2284d49e716ff
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Feb 28 09:49:41 2020 +0100

    KVM: let declaration of kvm_get_running_vcpus match implementation
    
    Sparse notices that declaration and implementation do not match:
    arch/s390/kvm/../../../virt/kvm/kvm_main.c:4435:17: warning: incorrect type in return expression (different address spaces)
    arch/s390/kvm/../../../virt/kvm/kvm_main.c:4435:17:    expected struct kvm_vcpu [noderef] <asn:3> **
    arch/s390/kvm/../../../virt/kvm/kvm_main.c:4435:17:    got struct kvm_vcpu *[noderef] <asn:3> *
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7944ad6ac10b..bcb9b2ac0791 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1344,7 +1344,7 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 
 struct kvm_vcpu *kvm_get_running_vcpu(void);
-struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
+struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 bool kvm_arch_has_irq_bypass(void);

commit d970a325561da5e611596cbb06475db3755ce823
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Feb 13 18:22:55 2020 +0100

    KVM: x86: fix missing prototypes
    
    Reported with "make W=1" due to -Wmissing-prototypes.
    
    Reported-by: Qian Cai <cai@lca.pw>
    Reviewed-by: Miaohe Lin <linmiaohe@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e89eb67356cb..7944ad6ac10b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -889,6 +889,8 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 bool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu);
+int kvm_arch_post_init_vm(struct kvm *kvm);
+void kvm_arch_pre_destroy_vm(struct kvm *kvm);
 
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
 /*

commit 4cbc418a44d5067133271bb6eeac2382f2bf94f7
Merge: 1d5920c306f1 a6bd811f1209
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 30 18:47:38 2020 +0100

    Merge branch 'cve-2019-3016' into kvm-next-5.6
    
    From Boris Ostrovsky:
    
    The KVM hypervisor may provide a guest with ability to defer remote TLB
    flush when the remote VCPU is not running. When this feature is used,
    the TLB flush will happen only when the remote VPCU is scheduled to run
    again. This will avoid unnecessary (and expensive) IPIs.
    
    Under certain circumstances, when a guest initiates such deferred action,
    the hypervisor may miss the request. It is also possible that the guest
    may mistakenly assume that it has already marked remote VCPU as needing
    a flush when in fact that request had already been processed by the
    hypervisor. In both cases this will result in an invalid translation
    being present in a vCPU, potentially allowing accesses to memory locations
    in that guest's address space that should not be accessible.
    
    Note that only intra-guest memory is vulnerable.
    
    The five patches address both of these problems:
    1. The first patch makes sure the hypervisor doesn't accidentally clear
    a guest's remote flush request
    2. The rest of the patches prevent the race between hypervisor
    acknowledging a remote flush request and guest issuing a new one.
    
    Conflicts:
            arch/x86/kvm/x86.c [move from kvm_arch_vcpu_free to kvm_arch_vcpu_destroy]

commit 917248144db5d7320655dbb41d3af0b8a0f3d589
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Dec 5 01:30:51 2019 +0000

    x86/kvm: Cache gfn to pfn translation
    
    __kvm_map_gfn()'s call to gfn_to_pfn_memslot() is
    * relatively expensive
    * in certain cases (such as when done from atomic context) cannot be called
    
    Stashing gfn-to-pfn mapping should help with both cases.
    
    This is part of CVE-2019-3016.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0cb78f55b92c..71cb9cc105f0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -723,6 +723,7 @@ void kvm_set_pfn_dirty(kvm_pfn_t pfn);
 void kvm_set_pfn_accessed(kvm_pfn_t pfn);
 void kvm_get_pfn(kvm_pfn_t pfn);
 
+void kvm_release_pfn(kvm_pfn_t pfn, bool dirty, struct gfn_to_pfn_cache *cache);
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
 int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
@@ -775,10 +776,12 @@ struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
-int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map);
+int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
+		struct gfn_to_pfn_cache *cache, bool atomic);
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
 void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
-int kvm_unmap_gfn(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
+int kvm_unmap_gfn(struct kvm_vcpu *vcpu, struct kvm_host_map *map,
+		  struct gfn_to_pfn_cache *cache, bool dirty, bool atomic);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
 int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,

commit 1eff70a9abd46f175defafd29bc17ad456f398a7
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Tue Nov 12 16:35:06 2019 +0000

    x86/kvm: Introduce kvm_(un)map_gfn()
    
    kvm_vcpu_(un)map operates on gfns from any current address space.
    In certain cases we want to make sure we are not mapping SMRAM
    and for that we can use kvm_(un)map_gfn() that we are introducing
    in this patch.
    
    This is part of CVE-2019-3016.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 538c25e778c0..0cb78f55b92c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -775,8 +775,10 @@ struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
+int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map);
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
 void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
+int kvm_unmap_gfn(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
 int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,

commit f9b84e19221efc5f493156ee0329df3142085f28
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Jan 8 12:24:37 2020 -0800

    KVM: Use vcpu-specific gva->hva translation when querying host page size
    
    Use kvm_vcpu_gfn_to_hva() when retrieving the host page size so that the
    correct set of memslots is used when handling x86 page faults in SMM.
    
    Fixes: 54bf36aac520 ("KVM: x86: use vcpu-specific functions to read/write/translate GFNs")
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 46fdb7533678..6d5331b0d937 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -762,7 +762,7 @@ int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
-unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
+unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);

commit 005ba37cb89bcc0cf63c2029a41f8db165aeb615
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Jan 8 12:24:36 2020 -0800

    mm: thp: KVM: Explicitly check for THP when populating secondary MMU
    
    Add a helper, is_transparent_hugepage(), to explicitly check whether a
    compound page is a THP and use it when populating KVM's secondary MMU.
    The explicit check fixes a bug where a remapped compound page, e.g. for
    an XDP Rx socket, is mapped into a KVM guest and is mistaken for a THP,
    which results in KVM incorrectly creating a huge page in its secondary
    MMU.
    
    Fixes: 936a5fe6e6148 ("thp: kvm mmu transparent hugepage support")
    Reported-by: syzbot+c9d1fb51ac9d0d10c39d@syzkaller.appspotmail.com
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 48e139c293c2..46fdb7533678 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -976,6 +976,7 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 
 bool kvm_is_reserved_pfn(kvm_pfn_t pfn);
 bool kvm_is_zone_device_pfn(kvm_pfn_t pfn);
+bool kvm_is_transparent_hugepage(kvm_pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;

commit 7495e22bb165e7030bae4d9c6e84addb5ea17b29
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 9 09:57:19 2020 -0500

    KVM: Move running VCPU from ARM to common code
    
    For ring-based dirty log tracking, it will be more efficient to account
    writes during schedule-out or schedule-in to the currently running VCPU.
    We would like to do it even if the write doesn't use the current VCPU's
    address space, as is the case for cached writes (see commit 4e335d9e7ddb,
    "Revert "KVM: Support vCPU-based gfn->hva cache"", 2017-05-02).
    
    Therefore, add a mechanism to track the currently-loaded kvm_vcpu struct.
    There is already something similar in KVM/ARM; one important difference
    is that kvm_arch_vcpu_{load,put} have two callers in virt/kvm/kvm_main.c:
    we have to update both the architecture-independent vcpu_{load,put} and
    the preempt notifiers.
    
    Another change made in the process is to allow using kvm_get_running_vcpu()
    in preemptible code.  This is allowed because preempt notifiers ensure
    that the value does not change even after the VCPU thread is migrated.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 83bd60f0af01..48e139c293c2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1335,6 +1335,9 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 }
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 
+struct kvm_vcpu *kvm_get_running_vcpu(void);
+struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
+
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 bool kvm_arch_has_irq_bypass(void);
 int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *,

commit ef82eddc0e3179b4529a67ed102fe4f7efba2e65
Author: Peter Xu <peterx@redhat.com>
Date:   Thu Jan 9 09:57:11 2020 -0500

    KVM: Remove kvm_read_guest_atomic()
    
    Remove kvm_read_guest_atomic() because it's not used anywhere.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a654cf6df078..83bd60f0af01 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -722,8 +722,6 @@ void kvm_get_pfn(kvm_pfn_t pfn);
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
-int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
-			  unsigned long len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
 int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len);

commit ddd259c9aaba08244dba8877687ee856f79c4f45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:28 2019 -0800

    KVM: Drop kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit()
    
    Remove kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit() now that all
    arch specific implementations are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 87ca40f62b06..a654cf6df078 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -864,9 +864,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run);
 int kvm_arch_init(void *opaque);
 void kvm_arch_exit(void);
 
-int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu);
-void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
-
 void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);

commit afede96df55e9cba948c8cc8a682e962244285b4
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:22 2019 -0800

    KVM: Drop kvm_arch_vcpu_setup()
    
    Remove kvm_arch_vcpu_setup() now that all arch specific implementations
    are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 521f17cd2b26..87ca40f62b06 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -873,7 +873,6 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id);
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu);
-int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 

commit aaba298c6bca8d8625880a8016e5b80adc8a11af
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:16 2019 -0800

    KVM: Unexport kvm_vcpu_cache and kvm_vcpu_{un}init()
    
    Unexport kvm_vcpu_cache and kvm_vcpu_{un}init() and make them static
    now that they are referenced only in kvm_main.c.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 405ea07068f1..521f17cd2b26 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -157,8 +157,6 @@ static inline bool is_error_page(struct page *page)
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
 
-extern struct kmem_cache *kvm_vcpu_cache;
-
 extern struct mutex kvm_lock;
 extern struct list_head vm_list;
 
@@ -579,8 +577,6 @@ static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
 	      memslot < slots->memslots + KVM_MEM_SLOTS_NUM && memslot->npages;\
 		memslot++)
 
-int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
-void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 void kvm_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 void vcpu_load(struct kvm_vcpu *vcpu);

commit e529ef66e6b53b34f9b8caac55950c8a55c79dac
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:15 2019 -0800

    KVM: Move vcpu alloc and init invocation to common code
    
    Now that all architectures tightly couple vcpu allocation/free with the
    mandatory calls to kvm_{un}init_vcpu(), move the sequences verbatim to
    common KVM code.
    
    Move both allocation and initialization in a single patch to eliminate
    thrash in arch specific code.  The bisection benefits of moving the two
    pieces in separate patches is marginal at best, whereas the odds of
    introducing a transient arch specific bug are non-zero.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 432827ab7623..405ea07068f1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -876,7 +876,7 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id);
-struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
+int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);

commit 4543bdc08857e8026475a477e7ba88e461f38271
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:14 2019 -0800

    KVM: Introduce kvm_vcpu_destroy()
    
    Add kvm_vcpu_destroy() and wire up all architectures to call the common
    function instead of their arch specific implementation.  The common
    destruction function will be used by future patches to move allocation
    and initialization of vCPUs to common KVM code, i.e. to free resources
    that are allocated by arch agnostic code.
    
    No functional change intended.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 59ac53423361..432827ab7623 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -581,6 +581,7 @@ static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
 
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
+void kvm_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 void vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);

commit 897cc38eaab96d006ab17edd0f50a2f432f584cf
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:09 2019 -0800

    KVM: Add kvm_arch_vcpu_precreate() to handle pre-allocation issues
    
    Add a pre-allocation arch hook to handle checks that are currently done
    by arch specific code prior to allocating the vCPU object.  This paves
    the way for moving the allocation to common KVM code.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4f7c8e2f378d..59ac53423361 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -874,6 +874,7 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
+int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id);
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);

commit fe931f12277186d1a9d38ba6729b42e8edb68988
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:08 2019 -0800

    KVM: Remove kvm_arch_vcpu_free() declaration
    
    Remove KVM's declaration of kvm_arch_vcpu_free() now that the function
    is gone from all architectures (several architectures were relying on
    the forward declaration).
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5e2ec7e295db..4f7c8e2f378d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -872,7 +872,6 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
 
 void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);
 
-void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);

commit 09cbcef6c60e77af11c3f27e62ea3f291a5d436c
Author: Milan Pandurov <milanpa@amazon.de>
Date:   Fri Dec 13 14:07:21 2019 +0100

    kvm: Refactor handling of VM debugfs files
    
    We can store reference to kvm_stats_debugfs_item instead of copying
    its values to kvm_stat_data.
    This allows us to remove duplicated code and usage of temporary
    kvm_stat_data inside vm_stat_get et al.
    
    Signed-off-by: Milan Pandurov <milanpa@amazon.de>
    Reviewed-by: Alexander Graf <graf@amazon.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 528ab7a814ab..5e2ec7e295db 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1108,9 +1108,8 @@ enum kvm_stat_kind {
 };
 
 struct kvm_stat_data {
-	int offset;
-	int mode;
 	struct kvm *kvm;
+	struct kvm_stats_debugfs_item *dbgfs_item;
 };
 
 struct kvm_stats_debugfs_item {
@@ -1119,6 +1118,10 @@ struct kvm_stats_debugfs_item {
 	enum kvm_stat_kind kind;
 	int mode;
 };
+
+#define KVM_DBGFS_GET_MODE(dbgfs_item)                                         \
+	((dbgfs_item)->mode ? (dbgfs_item)->mode : 0644)
+
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 

commit 736c291c9f36b07f8889c61764c28edce20e715d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Dec 6 15:57:14 2019 -0800

    KVM: x86: Use gpa_t for cr2/gpa to fix TDP support on 32-bit KVM
    
    Convert a plethora of parameters and variables in the MMU and page fault
    flows from type gva_t to gpa_t to properly handle TDP on 32-bit KVM.
    
    Thanks to PSE and PAE paging, 32-bit kernels can access 64-bit physical
    addresses.  When TDP is enabled, the fault address is a guest physical
    address and thus can be a 64-bit value, even when both KVM and its guest
    are using 32-bit virtual addressing, e.g. VMX's VMCS.GUEST_PHYSICAL is a
    64-bit field, not a natural width field.
    
    Using a gva_t for the fault address means KVM will incorrectly drop the
    upper 32-bits of the GPA.  Ditto for gva_to_gpa() when it is used to
    translate L2 GPAs to L1 GPAs.
    
    Opportunistically rename variables and parameters to better reflect the
    dual address modes, e.g. use "cr2_or_gpa" for fault addresses and plain
    "addr" instead of "vaddr" when the address may be either a GVA or an L2
    GPA.  Similarly, use "gpa" in the nonpaging_page_fault() flows to avoid
    a confusing "gpa_t gva" declaration; this also sets the stage for a
    future patch to combing nonpaging_page_fault() and tdp_page_fault() with
    minimal churn.
    
    Sprinkle in a few comments to document flows where an address is known
    to be a GVA and thus can be safely truncated to a 32-bit value.  Add
    WARNs in kvm_handle_page_fault() and FNAME(gva_to_gpa_nested)() to help
    document such cases and detect bugs.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0d632a75fce9..528ab7a814ab 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -204,7 +204,7 @@ struct kvm_async_pf {
 	struct list_head queue;
 	struct kvm_vcpu *vcpu;
 	struct mm_struct *mm;
-	gva_t gva;
+	gpa_t cr2_or_gpa;
 	unsigned long addr;
 	struct kvm_arch_async_pf arch;
 	bool   wakeup_all;
@@ -212,8 +212,8 @@ struct kvm_async_pf {
 
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu);
-int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
-		       struct kvm_arch_async_pf *arch);
+int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
+		       unsigned long hva, struct kvm_arch_async_pf *arch);
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 

commit 885f7d6cb87eb15d62613c05d8012e9370fb5e27
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Fri Dec 6 18:45:52 2019 +0800

    KVM: Remove duplicated declaration of kvm_vcpu_kick
    
    There are two declarations of kvm_vcpu_kick() in kvm_host.h where
    one of them is redundant. Remove to keep the git grep a bit cleaner.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 538c25e778c0..0d632a75fce9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -982,7 +982,6 @@ void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
-void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
 bool kvm_is_reserved_pfn(kvm_pfn_t pfn);
 bool kvm_is_zone_device_pfn(kvm_pfn_t pfn);

commit c593642c8be046915ca3a4a300243a68077cd207
Author: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
Date:   Mon Dec 9 10:31:43 2019 -0800

    treewide: Use sizeof_field() macro
    
    Replace all the occurrences of FIELD_SIZEOF() with sizeof_field() except
    at places where these are defined. Later patches will remove the unused
    definition of FIELD_SIZEOF().
    
    This patch is generated using following script:
    
    EXCLUDE_FILES="include/linux/stddef.h|include/linux/kernel.h"
    
    git grep -l -e "\bFIELD_SIZEOF\b" | while read file;
    do
    
            if [[ "$file" =~ $EXCLUDE_FILES ]]; then
                    continue
            fi
            sed -i  -e 's/\bFIELD_SIZEOF\b/sizeof_field/g' $file;
    done
    
    Signed-off-by: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
    Link: https://lore.kernel.org/r/20190924105839.110713-3-pankaj.laxminarayan.bharadiya@intel.com
    Co-developed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: David Miller <davem@davemloft.net> # for net

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7ed1e2f8641e..538c25e778c0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -149,7 +149,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQUEST_ARCH_BASE     8
 
 #define KVM_ARCH_REQ_FLAGS(nr, flags) ({ \
-	BUILD_BUG_ON((unsigned)(nr) >= (FIELD_SIZEOF(struct kvm_vcpu, requests) * 8) - KVM_REQUEST_ARCH_BASE); \
+	BUILD_BUG_ON((unsigned)(nr) >= (sizeof_field(struct kvm_vcpu, requests) * 8) - KVM_REQUEST_ARCH_BASE); \
 	(unsigned)(((nr) + KVM_REQUEST_ARCH_BASE) | (flags)); \
 })
 #define KVM_ARCH_REQ(nr)           KVM_ARCH_REQ_FLAGS(nr, 0)

commit 46f4f0aabc61bfd365e1eb3c8a6d766d1a49cf32
Merge: 14edff88315a b07a5c53d42a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 10:01:51 2019 +0100

    Merge branch 'kvm-tsx-ctrl' into HEAD
    
    Conflicts:
            arch/x86/kvm/vmx/vmx.c

commit 14edff88315add29099fd8eebb9ef989c2e47c18
Merge: 992edeaefed6 cd7056ae34af
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 09:58:35 2019 +0100

    Merge tag 'kvmarm-5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for Linux 5.5:
    
    - Allow non-ISV data aborts to be reported to userspace
    - Allow injection of data aborts from userspace
    - Expose stolen time to guests
    - GICv4 performance improvements
    - vgic ITS emulation fixes
    - Simplify FWB handling
    - Enable halt pool counters
    - Make the emulated timer PREEMPT_RT compliant
    
    Conflicts:
            include/uapi/linux/kvm.h

commit 7ee30bc132c683d06a6d9e360e39e483e3990708
Author: Nitesh Narayan Lal <nitesh@redhat.com>
Date:   Thu Nov 7 07:53:43 2019 -0500

    KVM: x86: deliver KVM IOAPIC scan request to target vCPUs
    
    In IOAPIC fixed delivery mode instead of flushing the scan
    requests to all vCPUs, we should only send the requests to
    vCPUs specified within the destination field.
    
    This patch introduces kvm_get_dest_vcpus_mask() API which
    retrieves an array of target vCPUs by using
    kvm_apic_map_get_dest_lapic() and then based on the
    vcpus_idx, it sets the bit in a bitmap. However, if the above
    fails kvm_get_dest_vcpus_mask() finds the target vCPUs by
    traversing all available vCPUs. Followed by setting the
    bits in the bitmap.
    
    If we had different vCPUs in the previous request for the
    same redirection table entry then bits corresponding to
    these vCPUs are also set. This to done to keep
    ioapic_handled_vectors synchronized.
    
    This bitmap is then eventually passed on to
    kvm_make_vcpus_request_mask() to generate a masked request
    only for the target vCPUs.
    
    This would enable us to reduce the latency overhead on isolated
    vCPUs caused by the IPI to process due to KVM_REQ_IOAPIC_SCAN.
    
    Suggested-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Nitesh Narayan Lal <nitesh@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 70b2296fb2ae..bfe6c6729988 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -786,6 +786,8 @@ void kvm_reload_remote_mmus(struct kvm *kvm);
 bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 				 unsigned long *vcpu_bitmap, cpumask_var_t tmp);
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);
+bool kvm_make_cpus_request_mask(struct kvm *kvm, unsigned int req,
+				unsigned long *vcpu_bitmap);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);

commit 8750e72a79dda2f665ce17b62049f4d62130d991
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Nov 7 07:53:42 2019 -0500

    KVM: remember position in kvm->vcpus array
    
    Fetching an index for any vcpu in kvm->vcpus array by traversing
    the entire array everytime is costly.
    This patch remembers the position of each vcpu in kvm->vcpus array
    by storing it in vcpus_idx under kvm_vcpu structure.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Nitesh Narayan Lal <nitesh@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a817e446c9aa..70b2296fb2ae 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -266,7 +266,8 @@ struct kvm_vcpu {
 	struct preempt_notifier preempt_notifier;
 #endif
 	int cpu;
-	int vcpu_id;
+	int vcpu_id; /* id given by userspace at creation */
+	int vcpu_idx; /* index in kvm->vcpus array */
 	int srcu_idx;
 	int mode;
 	u64 requests;
@@ -570,13 +571,7 @@ static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 
 static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
 {
-	struct kvm_vcpu *tmp;
-	int idx;
-
-	kvm_for_each_vcpu(idx, tmp, vcpu->kvm)
-		if (tmp == vcpu)
-			return idx;
-	BUG();
+	return vcpu->vcpu_idx;
 }
 
 #define kvm_for_each_memslot(memslot, slots)	\

commit 8c5bd25bf42effd194d4b0b43895c42b374e620b
Merge: eb094f06963b a78986aae9b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 12 13:19:15 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm fixes from Paolo Bonzini:
     "Fix unwinding of KVM_CREATE_VM failure, VT-d posted interrupts,
      DAX/ZONE_DEVICE, and module unload/reload"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm:
      KVM: MMU: Do not treat ZONE_DEVICE pages as being reserved
      KVM: VMX: Introduce pi_is_pir_empty() helper
      KVM: VMX: Do not change PID.NDST when loading a blocked vCPU
      KVM: VMX: Consider PID.PIR to determine if vCPU has pending interrupts
      KVM: VMX: Fix comment to specify PID.ON instead of PIR.ON
      KVM: X86: Fix initialization of MSR lists
      KVM: fix placement of refcount initialization
      KVM: Fix NULL-ptr deref after kvm_create_vm fails

commit a78986aae9b2988f8493f9f65a587ee433e83bc3
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Nov 11 14:12:27 2019 -0800

    KVM: MMU: Do not treat ZONE_DEVICE pages as being reserved
    
    Explicitly exempt ZONE_DEVICE pages from kvm_is_reserved_pfn() and
    instead manually handle ZONE_DEVICE on a case-by-case basis.  For things
    like page refcounts, KVM needs to treat ZONE_DEVICE pages like normal
    pages, e.g. put pages grabbed via gup().  But for flows such as setting
    A/D bits or shifting refcounts for transparent huge pages, KVM needs to
    to avoid processing ZONE_DEVICE pages as the flows in question lack the
    underlying machinery for proper handling of ZONE_DEVICE pages.
    
    This fixes a hang reported by Adam Borowski[*] in dev_pagemap_cleanup()
    when running a KVM guest backed with /dev/dax memory, as KVM straight up
    doesn't put any references to ZONE_DEVICE pages acquired by gup().
    
    Note, Dan Williams proposed an alternative solution of doing put_page()
    on ZONE_DEVICE pages immediately after gup() in order to simplify the
    auditing needed to ensure is_zone_device_page() is called if and only if
    the backing device is pinned (via gup()).  But that approach would break
    kvm_vcpu_{un}map() as KVM requires the page to be pinned from map() 'til
    unmap() when accessing guest memory, unlike KVM's secondary MMU, which
    coordinates with mmu_notifier invalidations to avoid creating stale
    page references, i.e. doesn't rely on pages being pinned.
    
    [*] http://lkml.kernel.org/r/20190919115547.GA17963@angband.pl
    
    Reported-by: Adam Borowski <kilobyte@angband.pl>
    Analyzed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Cc: stable@vger.kernel.org
    Fixes: 3565fce3a659 ("mm, x86: get_user_pages() for dax mappings")
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 719fc3e15ea4..290dbe353a47 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -966,6 +966,7 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
 bool kvm_is_reserved_pfn(kvm_pfn_t pfn);
+bool kvm_is_zone_device_pfn(kvm_pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;

commit c57c80467f90e5504c8df9ad3555d2c78800bf94
Author: Junaid Shahid <junaids@google.com>
Date:   Mon Nov 4 12:22:02 2019 +0100

    kvm: Add helper function for creating VM worker threads
    
    Add a function to create a kernel thread associated with a given VM. In
    particular, it ensures that the worker thread inherits the priority and
    cgroups of the calling thread.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 719fc3e15ea4..52ed5f66e8f9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1382,4 +1382,10 @@ static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 }
 #endif /* CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE */
 
+typedef int (*kvm_vm_thread_fn_t)(struct kvm *kvm, uintptr_t data);
+
+int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
+				uintptr_t data, const char *name,
+				struct task_struct **thread_ptr);
+
 #endif

commit 149487bdacde32f5a9a344a49533ae0772fb9db7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Mon Oct 21 15:58:42 2019 -0700

    KVM: Add separate helper for putting borrowed reference to kvm
    
    Add a new helper, kvm_put_kvm_no_destroy(), to handle putting a borrowed
    reference[*] to the VM when installing a new file descriptor fails.  KVM
    expects the refcount to remain valid in this case, as the in-progress
    ioctl() has an explicit reference to the VM.  The primary motiviation
    for the helper is to document that the 'kvm' pointer is still valid
    after putting the borrowed reference, e.g. to document that doing
    mutex(&kvm->lock) immediately after putting a ref to kvm isn't broken.
    
    [*] When exposing a new object to userspace via a file descriptor, e.g.
        a new vcpu, KVM grabs a reference to itself (the VM) prior to making
        the object visible to userspace to avoid prematurely freeing the VM
        in the scenario where userspace immediately closes file descriptor.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d2017302996c..a817e446c9aa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -621,6 +621,7 @@ void kvm_exit(void);
 
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
+void kvm_put_kvm_no_destroy(struct kvm *kvm);
 
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {

commit 78958563d8023db0c6d03a2fe2a64d79b47b4349
Author: Aaron Lewis <aaronlewis@google.com>
Date:   Mon Oct 21 16:30:22 2019 -0700

    KVM: x86: Remove unneeded kvm_vcpu variable, guest_xcr0_loaded
    
    The kvm_vcpu variable, guest_xcr0_loaded, is a waste of an 'int'
    and a conditional branch.  VMX and SVM are the only users, and both
    unconditionally pair kvm_load_guest_xcr0() with kvm_put_guest_xcr0()
    making this check unnecessary. Without this variable, the predicates in
    kvm_load_guest_xcr0 and kvm_put_guest_xcr0 should match.
    
    Suggested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: Aaron Lewis <aaronlewis@google.com>
    Change-Id: I7b1eb9b62969d7bbb2850f27e42f863421641b23
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 719fc3e15ea4..d2017302996c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -278,7 +278,6 @@ struct kvm_vcpu {
 	struct mutex mutex;
 	struct kvm_run *run;
 
-	int guest_xcr0_loaded;
 	struct swait_queue_head wq;
 	struct pid __rcu *pid;
 	int sigset_active;

commit 8538cb22bbce5a988671b68baf0b0f9e86ca1e87
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:19 2019 +0100

    KVM: Allow kvm_device_ops to be const
    
    Currently a kvm_device_ops structure cannot be const without triggering
    compiler warnings. However the structure doesn't need to be written to
    and, by marking it const, it can be read-only in memory. Add some more
    const keywords to allow this.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9907e45f8875..7a26d5513471 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1262,7 +1262,7 @@ extern unsigned int halt_poll_ns_grow_start;
 extern unsigned int halt_poll_ns_shrink;
 
 struct kvm_device {
-	struct kvm_device_ops *ops;
+	const struct kvm_device_ops *ops;
 	struct kvm *kvm;
 	void *private;
 	struct list_head vm_node;
@@ -1315,7 +1315,7 @@ struct kvm_device_ops {
 void kvm_device_get(struct kvm_device *dev);
 void kvm_device_put(struct kvm_device *dev);
 struct kvm_device *kvm_device_from_filp(struct file *filp);
-int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
+int kvm_register_device_ops(const struct kvm_device_ops *ops, u32 type);
 void kvm_unregister_device_ops(u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;

commit cac0f1b7285eaaf9a186c618c3a7304d82ed5493
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:17 2019 +0100

    KVM: Implement kvm_put_guest()
    
    kvm_put_guest() is analogous to put_user() - it writes a single value to
    the guest physical address. The implementation is built upon put_user()
    and so it has the same single copy atomic properties.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 719fc3e15ea4..9907e45f8875 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -746,6 +746,28 @@ int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 				  unsigned long len);
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len);
+
+#define __kvm_put_guest(kvm, gfn, offset, value, type)			\
+({									\
+	unsigned long __addr = gfn_to_hva(kvm, gfn);			\
+	type __user *__uaddr = (type __user *)(__addr + offset);	\
+	int __ret = -EFAULT;						\
+									\
+	if (!kvm_is_error_hva(__addr))					\
+		__ret = put_user(value, __uaddr);			\
+	if (!__ret)							\
+		mark_page_dirty(kvm, gfn);				\
+	__ret;								\
+})
+
+#define kvm_put_guest(kvm, gpa, value, type)				\
+({									\
+	gpa_t __gpa = gpa;						\
+	struct kvm *__kvm = kvm;					\
+	__kvm_put_guest(__kvm, __gpa >> PAGE_SHIFT,			\
+			offset_in_page(__gpa), (value), type);		\
+})
+
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);

commit 833b45de69a6016c4b0cebe6765d526a31a81580
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Sep 30 18:48:44 2019 +0200

    kvm: x86, powerpc: do not allow clearing largepages debugfs entry
    
    The largepages debugfs entry is incremented/decremented as shadow
    pages are created or destroyed.  Clearing it will result in an
    underflow, which is harmless to KVM but ugly (and could be
    misinterpreted by tools that use debugfs information), so make
    this particular statistic read-only.
    
    Cc: kvm-ppc@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fcb46b3374c6..719fc3e15ea4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1090,6 +1090,7 @@ enum kvm_stat_kind {
 
 struct kvm_stat_data {
 	int offset;
+	int mode;
 	struct kvm *kvm;
 };
 
@@ -1097,6 +1098,7 @@ struct kvm_stats_debugfs_item {
 	const char *name;
 	int offset;
 	enum kvm_stat_kind kind;
+	int mode;
 };
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;

commit 3e7093d045196b1016517631645e874fe903db7e
Author: Greg KH <gregkh@linuxfoundation.org>
Date:   Wed Jul 31 20:56:20 2019 +0200

    KVM: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    Also, when doing this, change kvm_arch_create_vcpu_debugfs() to return
    void instead of an integer, as we should not care at all about if this
    function actually does anything or not.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <x86@kernel.org>
    Cc: <kvm@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8d34db3c8bc6..fcb46b3374c6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -862,7 +862,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
-int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
+void kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
 #endif
 
 int kvm_arch_hardware_enable(void);

commit 741cbbae0768b828be2d48331eb371a4f08bbea8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Aug 3 08:14:25 2019 +0200

    KVM: remove kvm_arch_has_vcpu_debugfs()
    
    There is no need for this function as all arches have to implement
    kvm_arch_create_vcpu_debugfs() no matter what.  A #define symbol
    let us actually simplify the code.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9e4c2bb90297..8d34db3c8bc6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -861,8 +861,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
-bool kvm_arch_has_vcpu_debugfs(void);
+#ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
 int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
+#endif
 
 int kvm_arch_hardware_enable(void);
 void kvm_arch_hardware_disable(void);

commit 17e433b54393a6269acbcb792da97791fe1592d8
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Aug 5 10:03:19 2019 +0800

    KVM: Fix leak vCPU's VMCS value into other pCPU
    
    After commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts), a
    five years old bug is exposed. Running ebizzy benchmark in three 80 vCPUs VMs
    on one 80 pCPUs Skylake server, a lot of rcu_sched stall warning splatting
    in the VMs after stress testing:
    
     INFO: rcu_sched detected stalls on CPUs/tasks: { 4 41 57 62 77} (detected by 15, t=60004 jiffies, g=899, c=898, q=15073)
     Call Trace:
       flush_tlb_mm_range+0x68/0x140
       tlb_flush_mmu.part.75+0x37/0xe0
       tlb_finish_mmu+0x55/0x60
       zap_page_range+0x142/0x190
       SyS_madvise+0x3cd/0x9c0
       system_call_fastpath+0x1c/0x21
    
    swait_active() sustains to be true before finish_swait() is called in
    kvm_vcpu_block(), voluntarily preempted vCPUs are taken into account
    by kvm_vcpu_on_spin() loop greatly increases the probability condition
    kvm_arch_vcpu_runnable(vcpu) is checked and can be true, when APICv
    is enabled the yield-candidate vCPU's VMCS RVI field leaks(by
    vmx_sync_pir_to_irr()) into spinning-on-a-taken-lock vCPU's current
    VMCS.
    
    This patch fixes it by checking conservatively a subset of events.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Marc Zyngier <Marc.Zyngier@arm.com>
    Cc: stable@vger.kernel.org
    Fixes: 98f4a1467 (KVM: add kvm_arch_vcpu_runnable() test to kvm_vcpu_on_spin() loop)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5c5b5867024c..9e4c2bb90297 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -872,6 +872,7 @@ int kvm_arch_check_processor_compat(void);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
+bool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu);
 
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
 /*

commit d73eb57b80b98ae147e4e6a7d9877c2ba175f972
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Thu Jul 18 19:39:06 2019 +0800

    KVM: Boost vCPUs that are delivering interrupts
    
    Inspired by commit 9cac38dd5d (KVM/s390: Set preempted flag during
    vcpu wakeup and interrupt delivery), we want to also boost not just
    lock holders but also vCPUs that are delivering interrupts. Most
    smp_call_function_many calls are synchronous, so the IPI target vCPUs
    are also good yield candidates.  This patch introduces vcpu->ready to
    boost vCPUs during wakeup and interrupt delivery time; unlike s390 we do
    not reuse vcpu->preempted so that voluntarily preempted vCPUs are taken
    into account by kvm_vcpu_on_spin, but vmx_vcpu_pi_put is not affected
    (VT-d PI handles voluntary preemption separately, in pi_pre_block).
    
    Testing on 80 HT 2 socket Xeon Skylake server, with 80 vCPUs VM 80GB RAM:
    ebizzy -M
    
                vanilla     boosting    improved
    1VM          21443       23520         9%
    2VM           2800        8000       180%
    3VM           1800        3100        72%
    
    Testing on my Haswell desktop 8 HT, with 8 vCPUs VM 8GB RAM, two VMs,
    one running ebizzy -M, the other running 'stress --cpu 2':
    
    w/ boosting + w/o pv sched yield(vanilla)
    
                vanilla     boosting   improved
                  1570         4000      155%
    
    w/ boosting + w/ pv sched yield(vanilla)
    
                vanilla     boosting   improved
                  1844         5157      179%
    
    w/o boosting, perf top in VM:
    
     72.33%  [kernel]       [k] smp_call_function_many
      4.22%  [kernel]       [k] call_function_i
      3.71%  [kernel]       [k] async_page_fault
    
    w/ boosting, perf top in VM:
    
     38.43%  [kernel]       [k] smp_call_function_many
      6.31%  [kernel]       [k] async_page_fault
      6.13%  libc-2.23.so   [.] __memcpy_avx_unaligned
      4.88%  [kernel]       [k] call_function_interrupt
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c5da875f19e3..5c5b5867024c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -318,6 +318,7 @@ struct kvm_vcpu {
 	} spin_loop;
 #endif
 	bool preempted;
+	bool ready;
 	struct kvm_vcpu_arch arch;
 	struct dentry *debugfs_dentry;
 };

commit a45ff5994c9cde41af627c46abb9f32beae68943
Merge: 429bb83af8bc 1e0cf16cdad1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jul 11 15:14:16 2019 +0200

    Merge tag 'kvm-arm-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 5.3
    
    - Add support for chained PMU counters in guests
    - Improve SError handling
    - Handle Neoverse N1 erratum #1349291
    - Allow side-channel mitigation status to be migrated
    - Standardise most AArch64 system register accesses to msr_s/mrs_s
    - Fix host MPIDR corruption on 32bit

commit cdc238eb72f6b94b6c33b98c07b9fc3ac5e57b18
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Wed Jul 10 08:24:03 2019 +0800

    kvm: x86: Fix -Wmissing-prototypes warnings
    
    We get a warning when build kernel W=1:
    
    arch/x86/kvm/../../../virt/kvm/eventfd.c:48:1: warning: no previous prototype for ‘kvm_arch_irqfd_allowed’ [-Wmissing-prototypes]
     kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
     ^
    
    The reason is kvm_arch_irqfd_allowed() is declared in arch/x86/kvm/irq.h,
    which is not included by eventfd.c. Considering kvm_arch_irqfd_allowed()
    is a weakly defined function in eventfd.c, remove the declaration to
    kvm_host.h can fix this.
    
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index abafddb9fe2c..b91829ee3db1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -993,6 +993,7 @@ void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
+bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args);
 
 /*
  * search_memslots() and __gfn_to_memslot() are here because they are

commit 20c8ccb1975b8d5639789d1025ad6ada38bd6f48
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:32 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 499
    
    Based on 1 normalized pattern(s):
    
      this work is licensed under the terms of the gnu gpl version 2 see
      the copying file in the top level directory
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 35 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.797835076@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 79fa4426509c..d1ad38a3f048 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1,10 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef __KVM_HOST_H
 #define __KVM_HOST_H
 
-/*
- * This work is licensed under the terms of the GNU GPL, version 2.  See
- * the COPYING file in the top-level directory.
- */
 
 #include <linux/types.h>
 #include <linux/hardirq.h>

commit 0d9ce162cf46c99628cc5da9510b959c7976735b
Author: Junaid Shahid <junaids@google.com>
Date:   Thu Jan 3 17:14:28 2019 -0800

    kvm: Convert kvm_lock to a mutex
    
    It doesn't seem as if there is any particular need for kvm_lock to be a
    spinlock, so convert the lock to a mutex so that sleepable functions (in
    particular cond_resched()) can be called while holding it.
    
    Signed-off-by: Junaid Shahid <junaids@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5e9fd7ad8018..abafddb9fe2c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -162,7 +162,7 @@ static inline bool is_error_page(struct page *page)
 
 extern struct kmem_cache *kvm_vcpu_cache;
 
-extern spinlock_t kvm_lock;
+extern struct mutex kvm_lock;
 extern struct list_head vm_list;
 
 struct kvm_io_range {

commit f257d6dcda0187693407e0c2e5dab69bdab3223f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:18:17 2019 -0700

    KVM: Directly return result from kvm_arch_check_processor_compat()
    
    Add a wrapper to invoke kvm_arch_check_processor_compat() so that the
    boilerplate ugliness of checking virtualization support on all CPUs is
    hidden from the arch specific code.  x86's implementation in particular
    is quite heinous, as it unnecessarily propagates the out-param pattern
    into kvm_x86_ops.
    
    While the x86 specific issue could be resolved solely by changing
    kvm_x86_ops, make the change for all architectures as returning a value
    directly is prettier and technically more robust, e.g. s390 doesn't set
    the out param, which could lead to subtle breakage in the (highly
    unlikely) scenario where the out-param was not pre-initialized by the
    caller.
    
    Opportunistically annotate svm_check_processor_compat() with __init.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 79fa4426509c..5e9fd7ad8018 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -870,7 +870,7 @@ int kvm_arch_hardware_enable(void);
 void kvm_arch_hardware_disable(void);
 int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
-void kvm_arch_check_processor_compat(void *rtn);
+int kvm_arch_check_processor_compat(void);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);

commit 59c5c58c5b93285753d5c1de34d2e00039c27bc0
Merge: f93f7ede087f 4894fbcce856
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 15 23:38:42 2019 +0200

    Merge tag 'kvm-ppc-next-5.2-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    PPC KVM update for 5.2
    
    * Support for guests to access the new POWER9 XIVE interrupt controller
      hardware directly, reducing interrupt latency and overhead for guests.
    
    * In-kernel implementation of the H_PAGE_INIT hypercall.
    
    * Reduce memory usage of sparsely-populated IOMMU tables.
    
    * Several bug fixes.
    
    Second PPC KVM update for 5.2
    
    * Fix a bug, fix a spelling mistake, remove some useless code.

commit e45adf665a53df0db37f784ed87c6b57ddd81885
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Thu Jan 31 21:24:34 2019 +0100

    KVM: Introduce a new guest mapping API
    
    In KVM, specially for nested guests, there is a dominant pattern of:
    
            => map guest memory -> do_something -> unmap guest memory
    
    In addition to all this unnecessarily noise in the code due to boiler plate
    code, most of the time the mapping function does not properly handle memory
    that is not backed by "struct page". This new guest mapping API encapsulate
    most of this boiler plate code and also handles guest memory that is not
    backed by "struct page".
    
    The current implementation of this API is using memremap for memory that is
    not backed by a "struct page" which would lead to a huge slow-down if it
    was used for high-frequency mapping operations. The API does not have any
    effect on current setups where guest memory is backed by a "struct page".
    Further patches are going to also introduce a pfn-cache which would
    significantly improve the performance of the memremap case.
    
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 43f87ad83ff4..6f665c16e31d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -227,6 +227,32 @@ enum {
 	READING_SHADOW_PAGE_TABLES,
 };
 
+#define KVM_UNMAPPED_PAGE	((void *) 0x500 + POISON_POINTER_DELTA)
+
+struct kvm_host_map {
+	/*
+	 * Only valid if the 'pfn' is managed by the host kernel (i.e. There is
+	 * a 'struct page' for it. When using mem= kernel parameter some memory
+	 * can be used as guest memory but they are not managed by host
+	 * kernel).
+	 * If 'pfn' is not managed by the host kernel, this field is
+	 * initialized to KVM_UNMAPPED_PAGE.
+	 */
+	struct page *page;
+	void *hva;
+	kvm_pfn_t pfn;
+	kvm_pfn_t gfn;
+};
+
+/*
+ * Used to check if the mapping is valid or not. Never use 'kvm_host_map'
+ * directly to check for that.
+ */
+static inline bool kvm_vcpu_mapped(struct kvm_host_map *map)
+{
+	return !!map->hva;
+}
+
 /*
  * Sometimes a large or cross-page mmio needs to be broken up into separate
  * exits for userspace servicing.
@@ -733,7 +759,9 @@ struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
+void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
 int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,

commit da8f0d97b2a02ebc98eb380d9e59c7fb653d4ad8
Merge: c110ae578ca0 b2d0371d2e37
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 30 21:29:14 2019 +0200

    Merge tag 'kvm-s390-next-5.2-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into HEAD
    
    KVM: s390: Features and fixes for 5.2
    
    - VSIE crypto fixes
    - new guest features for gen15
    - disable halt polling for nested virtualization with overcommit

commit 2bde9b3ec8bdf60788e9e2ce8c07a2f8d6003dbd
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:41 2019 +0200

    KVM: Introduce a 'release' method for KVM devices
    
    When a P9 sPAPR VM boots, the CAS negotiation process determines which
    interrupt mode to use (XICS legacy or XIVE native) and invokes a
    machine reset to activate the chosen mode.
    
    To be able to switch from one interrupt mode to another, we introduce
    the capability to release a KVM device without destroying the VM. The
    KVM device interface is extended with a new 'release' method which is
    called when the file descriptor of the device is closed.
    
    Once 'release' is called, the 'destroy' method will not be called
    anymore as the device is removed from the device list of the VM.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 831d963451d8..722692e2f745 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1240,6 +1240,15 @@ struct kvm_device_ops {
 	 */
 	void (*destroy)(struct kvm_device *dev);
 
+	/*
+	 * Release is an alternative method to free the device. It is
+	 * called when the device file descriptor is closed. Once
+	 * release is called, the destroy method will not be called
+	 * anymore as the device is removed from the device list of
+	 * the VM. kvm->lock is held.
+	 */
+	void (*release)(struct kvm_device *dev);
+
 	int (*set_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
 	int (*get_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
 	int (*has_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);

commit a1cd3f0883f435e5f9ae6530d7e62b361c87a91a
Author: Cédric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:36 2019 +0200

    KVM: Introduce a 'mmap' method for KVM devices
    
    Some KVM devices will want to handle special mappings related to the
    underlying HW. For instance, the XIVE interrupt controller of the
    POWER9 processor has MMIO pages for thread interrupt management and
    for interrupt source control that need to be exposed to the guest when
    the OS has the required support.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Cédric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9d55c63db09b..831d963451d8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1245,6 +1245,7 @@ struct kvm_device_ops {
 	int (*has_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
 	long (*ioctl)(struct kvm_device *dev, unsigned int ioctl,
 		      unsigned long arg);
+	int (*mmap)(struct kvm_device *dev, struct vm_area_struct *vma);
 };
 
 void kvm_device_get(struct kvm_device *dev);

commit cdd6ad3ac63d2fa320baefcf92a02a918375c30f
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Mar 5 05:30:01 2019 -0500

    KVM: polling: add architecture backend to disable polling
    
    There are cases where halt polling is unwanted. For example when running
    KVM on an over committed LPAR we rather want to give back the CPU to
    neighbour LPARs instead of polling. Let us provide a callback that
    allows architectures to disable polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9d55c63db09b..b3aff1a3f633 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1305,6 +1305,16 @@ static inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)
 }
 #endif /* CONFIG_HAVE_KVM_INVALID_WAKEUPS */
 
+#ifdef CONFIG_HAVE_KVM_NO_POLL
+/* Callback that tells if we must not poll */
+bool kvm_arch_no_poll(struct kvm_vcpu *vcpu);
+#else
+static inline bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
+{
+	return false;
+}
+#endif /* CONFIG_HAVE_KVM_NO_POLL */
+
 #ifdef CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL
 long kvm_arch_vcpu_async_ioctl(struct file *filp,
 			       unsigned int ioctl, unsigned long arg);

commit 1d487e9bf8ba66a7174c56a0029c54b1eca8f99c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 11 11:16:47 2019 +0200

    KVM: fix spectrev1 gadgets
    
    These were found with smatch, and then generalized when applicable.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9d55c63db09b..640a03642766 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -28,6 +28,7 @@
 #include <linux/irqbypass.h>
 #include <linux/swait.h>
 #include <linux/refcount.h>
+#include <linux/nospec.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -513,10 +514,10 @@ static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {
-	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu, in case
-	 * the caller has read kvm->online_vcpus before (as is the case
-	 * for kvm_for_each_vcpu, for example).
-	 */
+	int num_vcpus = atomic_read(&kvm->online_vcpus);
+	i = array_index_nospec(i, num_vcpus);
+
+	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
 	return kvm->vcpus[i];
 }
@@ -600,6 +601,7 @@ void kvm_put_kvm(struct kvm *kvm);
 
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
+	as_id = array_index_nospec(as_id, KVM_ADDRESS_SPACE_NUM);
 	return srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,
 			lockdep_is_held(&kvm->slots_lock) ||
 			!refcount_read(&kvm->users_count));

commit 49113d360bdeb4dd916fb6bffbcc3e157422b6fd
Author: Nir Weiner <nir.weiner@oracle.com>
Date:   Sun Jan 27 12:17:15 2019 +0200

    KVM: Expose the initial start value in grow_halt_poll_ns() as a module parameter
    
    The hard-coded value 10000 in grow_halt_poll_ns() stands for the initial
    start value when raising up vcpu->halt_poll_ns.
    It actually sets the first timeout to the first polling session.
    This value has significant effect on how tolerant we are to outliers.
    On the standard case, higher value is better - we will spend more time
    in the polling busyloop, handle events/interrupts faster and result
    in better performance.
    But on outliers it puts us in a busy loop that does nothing.
    Even if the shrink factor is zero, we will still waste time on the first
    iteration.
    The optimal value changes between different workloads. It depends on
    outliers rate and polling sessions length.
    As this value has significant effect on the dynamic halt-polling
    algorithm, it should be configurable and exposed.
    
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Liran Alon <liran.alon@oracle.com>
    Signed-off-by: Nir Weiner <nir.weiner@oracle.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 85c0c00d5159..9d55c63db09b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1203,6 +1203,7 @@ extern bool kvm_rebooting;
 
 extern unsigned int halt_poll_ns;
 extern unsigned int halt_poll_ns_grow;
+extern unsigned int halt_poll_ns_grow_start;
 extern unsigned int halt_poll_ns_shrink;
 
 struct kvm_device {

commit 164bf7e56c5a73f2f819c39ba7e0f20e0f97dc7b
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:18 2019 -0800

    KVM: Move the memslot update in-progress flag to bit 63
    
    ...now that KVM won't explode by moving it out of bit 0.  Using bit 63
    eliminates the need to jump over bit 0, e.g. when calculating a new
    memslots generation or when propagating the memslots generation to an
    MMIO spte.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5e1cb74922b3..85c0c00d5159 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -49,7 +49,7 @@
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
 
 /*
- * Bit 0 of the memslot generation number is an "update in-progress flag",
+ * Bit 63 of the memslot generation number is an "update in-progress flag",
  * e.g. is temporarily set for the duration of install_new_memslots().
  * This flag effectively creates a unique generation number that is used to
  * mark cached memslot data, e.g. MMIO accesses, as potentially being stale,
@@ -67,7 +67,7 @@
  * the actual generation number against accesses that were inserted into the
  * cache *before* the memslots were updated.
  */
-#define KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS	BIT_ULL(0)
+#define KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS	BIT_ULL(63)
 
 /* Two fragments for cross MMIO pages. */
 #define KVM_MAX_MMIO_FRAGMENTS	2

commit 361209e054a2c9f34da090ee1ee4c1e8bfe76a64
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 13:01:14 2019 -0800

    KVM: Explicitly define the "memslot update in-progress" bit
    
    KVM uses bit 0 of the memslots generation as an "update in-progress"
    flag, which is used by x86 to prevent caching MMIO access while the
    memslots are changing.  Although the intended behavior is flag-like,
    e.g. MMIO sptes intentionally drop the in-progress bit so as to avoid
    caching data from in-flux memslots, the implementation oftentimes treats
    the bit as part of the generation number itself, e.g. incrementing the
    generation increments twice, once to set the flag and once to clear it.
    
    Prior to commit 4bd518f1598d ("KVM: use separate generations for
    each address space"), incorporating the "update in-progress" bit into
    the generation number largely made sense, e.g. "real" generations are
    even, "bogus" generations are odd, most code doesn't need to be aware of
    the bit, etc...
    
    Now that unique memslots generation numbers are assigned to each address
    space, stealthing the in-progress status into the generation number
    results in a wide variety of subtle code, e.g. kvm_create_vm() jumps
    over bit 0 when initializing the memslots generation without any hint as
    to why.
    
    Explicitly define the flag and convert as much code as possible (which
    isn't much) to actually treat it like a flag.  This paves the way for
    eventually using a different bit for "update in-progress" so that it can
    be a flag in truth instead of a awkward extension to the generation
    number.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cf761ff58224..5e1cb74922b3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -48,6 +48,27 @@
  */
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
 
+/*
+ * Bit 0 of the memslot generation number is an "update in-progress flag",
+ * e.g. is temporarily set for the duration of install_new_memslots().
+ * This flag effectively creates a unique generation number that is used to
+ * mark cached memslot data, e.g. MMIO accesses, as potentially being stale,
+ * i.e. may (or may not) have come from the previous memslots generation.
+ *
+ * This is necessary because the actual memslots update is not atomic with
+ * respect to the generation number update.  Updating the generation number
+ * first would allow a vCPU to cache a spte from the old memslots using the
+ * new generation number, and updating the generation number after switching
+ * to the new memslots would allow cache hits using the old generation number
+ * to reference the defunct memslots.
+ *
+ * This mechanism is used to prevent getting hits in KVM's caches while a
+ * memslot update is in-progress, and to prevent cache hits *after* updating
+ * the actual generation number against accesses that were inserted into the
+ * cache *before* the memslots were updated.
+ */
+#define KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS	BIT_ULL(0)
+
 /* Two fragments for cross MMIO pages. */
 #define KVM_MAX_MMIO_FRAGMENTS	2
 

commit 152482580a1b0accb60676063a1ac57b2d12daf6
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 5 12:54:17 2019 -0800

    KVM: Call kvm_arch_memslots_updated() before updating memslots
    
    kvm_arch_memslots_updated() is at this point in time an x86-specific
    hook for handling MMIO generation wraparound.  x86 stashes 19 bits of
    the memslots generation number in its MMIO sptes in order to avoid
    full page fault walks for repeat faults on emulated MMIO addresses.
    Because only 19 bits are used, wrapping the MMIO generation number is
    possible, if unlikely.  kvm_arch_memslots_updated() alerts x86 that
    the generation has changed so that it can invalidate all MMIO sptes in
    case the effective MMIO generation has wrapped so as to avoid using a
    stale spte, e.g. a (very) old spte that was created with generation==0.
    
    Given that the purpose of kvm_arch_memslots_updated() is to prevent
    consuming stale entries, it needs to be called before the new generation
    is propagated to memslots.  Invalidating the MMIO sptes after updating
    memslots means that there is a window where a vCPU could dereference
    the new memslots generation, e.g. 0, and incorrectly reuse an old MMIO
    spte that was created with (pre-wrap) generation==0.
    
    Fixes: e59dbe09f8e6 ("KVM: Introduce kvm_arch_memslots_updated()")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c38cc5eb7e73..cf761ff58224 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -634,7 +634,7 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages);
-void kvm_arch_memslots_updated(struct kvm *kvm, struct kvm_memslots *slots);
+void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				const struct kvm_userspace_memory_region *mem,

commit 7a86dab8cf2f0fdf508f3555dddfc236623bff60
Author: Jim Mattson <jmattson@google.com>
Date:   Fri Dec 14 14:34:43 2018 -0800

    kvm: Change offset in kvm_write_guest_offset_cached to unsigned
    
    Since the offset is added directly to the hva from the
    gfn_to_hva_cache, a negative offset could result in an out of bounds
    write. The existing BUG_ON only checks for addresses beyond the end of
    the gfn_to_hva_cache, not for addresses before the start of the
    gfn_to_hva_cache.
    
    Note that all current call sites have non-negative offsets.
    
    Fixes: 4ec6e8636256 ("kvm: Introduce kvm_write_guest_offset_cached()")
    Reported-by: Cfir Cohen <cfir@google.com>
    Signed-off-by: Jim Mattson <jmattson@google.com>
    Reviewed-by: Cfir Cohen <cfir@google.com>
    Reviewed-by: Peter Shier <pshier@google.com>
    Reviewed-by: Krish Sadhukhan <krish.sadhukhan@oracle.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e065aeaae29e..c38cc5eb7e73 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -695,7 +695,8 @@ int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len);
 int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			   void *data, int offset, unsigned long len);
+				  void *data, unsigned int offset,
+				  unsigned long len);
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);

commit 2a31b9db153530df4aa02dac8c32837bf5f47019
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Oct 23 02:36:47 2018 +0200

    kvm: introduce manual dirty log reprotect
    
    There are two problems with KVM_GET_DIRTY_LOG.  First, and less important,
    it can take kvm->mmu_lock for an extended period of time.  Second, its user
    can actually see many false positives in some cases.  The latter is due
    to a benign race like this:
    
      1. KVM_GET_DIRTY_LOG returns a set of dirty pages and write protects
         them.
      2. The guest modifies the pages, causing them to be marked ditry.
      3. Userspace actually copies the pages.
      4. KVM_GET_DIRTY_LOG returns those pages as dirty again, even though
         they were not written to since (3).
    
    This is especially a problem for large guests, where the time between
    (1) and (3) can be substantial.  This patch introduces a new
    capability which, when enabled, makes KVM_GET_DIRTY_LOG not
    write-protect the pages it returns.  Instead, userspace has to
    explicitly clear the dirty log bits just before using the content
    of the page.  The new KVM_CLEAR_DIRTY_LOG ioctl can also operate on a
    64-page granularity rather than requiring to sync a full memslot;
    this way, the mmu_lock is taken for small amounts of time, and
    only a small amount of time will pass between write protection
    of pages and the sending of their content.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8c56b2873b13..e065aeaae29e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -449,6 +449,7 @@ struct kvm {
 #endif
 	long tlbs_dirty;
 	struct list_head devices;
+	bool manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
@@ -754,6 +755,8 @@ int kvm_get_dirty_log(struct kvm *kvm,
 
 int kvm_get_dirty_log_protect(struct kvm *kvm,
 			      struct kvm_dirty_log *log, bool *flush);
+int kvm_clear_dirty_log_protect(struct kvm *kvm,
+				struct kvm_clear_dirty_log *log, bool *flush);
 
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 					struct kvm_memory_slot *slot,
@@ -762,6 +765,8 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				struct kvm_dirty_log *log);
+int kvm_vm_ioctl_clear_dirty_log(struct kvm *kvm,
+				  struct kvm_clear_dirty_log *log);
 
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,
 			bool line_status);

commit 8fe65a8299f9e1f40cb95308ab7b3c4ad80bf801
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Oct 23 02:18:42 2018 +0200

    kvm: rename last argument to kvm_get_dirty_log_protect
    
    When manual dirty log reprotect will be enabled, kvm_get_dirty_log_protect's
    pointer argument will always be false on exit, because no TLB flush is needed
    until the manual re-protection operation.  Rename it from "is_dirty" to "flush",
    which more accurately tells the caller what they have to do with it.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 54cc06dd7e6c..8c56b2873b13 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -753,7 +753,7 @@ int kvm_get_dirty_log(struct kvm *kvm,
 			struct kvm_dirty_log *log, int *is_dirty);
 
 int kvm_get_dirty_log_protect(struct kvm *kvm,
-			struct kvm_dirty_log *log, bool *is_dirty);
+			      struct kvm_dirty_log *log, bool *flush);
 
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 					struct kvm_memory_slot *slot,

commit e5d83c74a5800c2a1fa3ba982c1c4b2b39ae6db2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Feb 16 10:40:56 2017 +0100

    kvm: make KVM_CAP_ENABLE_CAP_VM architecture agnostic
    
    The first such capability to be handled in virt/kvm/ will be manual
    dirty page reprotection.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c926698040e0..54cc06dd7e6c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -765,6 +765,8 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,
 			bool line_status);
+int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
+			    struct kvm_enable_cap *cap);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);
 

commit 822f312d47f0200dc0999c9f006fe94aa43bd0bd
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Sep 12 15:33:45 2018 +0200

    kvm: x86: make kvm_{load|put}_guest_fpu() static
    
    The functions
            kvm_load_guest_fpu()
            kvm_put_guest_fpu()
    
    are only used locally, make them static. This requires also that both
    functions are moved because they are used before their implementation.
    Those functions were exported (via EXPORT_SYMBOL) before commit
    e5bb40251a920 ("KVM: Drop kvm_{load,put}_guest_fpu() exports").
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0205aee44ded..c926698040e0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -733,8 +733,6 @@ bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu, bool usermode_vcpu_not_eligible);
-void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
-void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);

commit 93065ac753e4443840a057bfef4be71ec766fde9
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Aug 21 21:52:33 2018 -0700

    mm, oom: distinguish blockable mode for mmu notifiers
    
    There are several blockable mmu notifiers which might sleep in
    mmu_notifier_invalidate_range_start and that is a problem for the
    oom_reaper because it needs to guarantee a forward progress so it cannot
    depend on any sleepable locks.
    
    Currently we simply back off and mark an oom victim with blockable mmu
    notifiers as done after a short sleep.  That can result in selecting a new
    oom victim prematurely because the previous one still hasn't torn its
    memory down yet.
    
    We can do much better though.  Even if mmu notifiers use sleepable locks
    there is no reason to automatically assume those locks are held.  Moreover
    majority of notifiers only care about a portion of the address space and
    there is absolutely zero reason to fail when we are unmapping an unrelated
    range.  Many notifiers do really block and wait for HW which is harder to
    handle and we have to bail out though.
    
    This patch handles the low hanging fruit.
    __mmu_notifier_invalidate_range_start gets a blockable flag and callbacks
    are not allowed to sleep if the flag is set to false.  This is achieved by
    using trylock instead of the sleepable lock for most callbacks and
    continue as long as we do not block down the call chain.
    
    I think we can improve that even further because there is a common pattern
    to do a range lookup first and then do something about that.  The first
    part can be done without a sleeping lock in most cases AFAICS.
    
    The oom_reaper end then simply retries if there is at least one notifier
    which couldn't make any progress in !blockable mode.  A retry loop is
    already implemented to wait for the mmap_sem and this is basically the
    same thing.
    
    The simplest way for driver developers to test this code path is to wrap
    userspace code which uses these notifiers into a memcg and set the hard
    limit to hit the oom.  This can be done e.g.  after the test faults in all
    the mmu notifier managed memory and set the hard limit to something really
    small.  Then we are looking for a proper process tear down.
    
    [akpm@linux-foundation.org: coding style fixes]
    [akpm@linux-foundation.org: minor code simplification]
    Link: http://lkml.kernel.org/r/20180716115058.5559-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Christian König <christian.koenig@amd.com> # AMD notifiers
    Acked-by: Leon Romanovsky <leonro@mellanox.com> # mlx and umem_odp
    Reported-by: David Rientjes <rientjes@google.com>
    Cc: "David (ChunMing) Zhou" <David1.Zhou@amd.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Cc: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Cc: Sudeep Dutt <sudeep.dutt@intel.com>
    Cc: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: "Jérôme Glisse" <jglisse@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Felix Kuehling <felix.kuehling@amd.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7c7362dd2faa..0205aee44ded 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1289,8 +1289,8 @@ static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
 }
 #endif /* CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL */
 
-void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
-		unsigned long start, unsigned long end);
+int kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
+		unsigned long start, unsigned long end, bool blockable);
 
 #ifdef CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE
 int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu);

commit b08660e59dbdb600c55953787ed2265a0b510f77
Author: Tianyu Lan <Tianyu.Lan@microsoft.com>
Date:   Thu Jul 19 08:40:17 2018 +0000

    KVM: x86: Add tlb remote flush callback in kvm_x86_ops.
    
    This patch is to provide a way for platforms to register hv tlb remote
    flush callback and this helps to optimize operation of tlb flush
    among vcpus for nested virtualization case.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2840ac74a3d7..7c7362dd2faa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -834,6 +834,13 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 }
 #endif
 
+#ifndef __KVM_HAVE_ARCH_FLUSH_REMOTE_TLB
+static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
+{
+	return -ENOTSUPP;
+}
+#endif
+
 #ifdef __KVM_HAVE_ARCH_NONCOHERENT_DMA
 void kvm_arch_register_noncoherent_dma(struct kvm *kvm);
 void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm);

commit 86dafed50e2b9b18198cc663775ece819ff113ba
Author: KarimAllah Ahmed <karahmed@amazon.de>
Date:   Tue Jul 10 11:27:19 2018 +0200

    KVM: Switch 'requests' to be 64-bit (explicitly)
    
    Switch 'requests' to be explicitly 64-bit and update BUILD_BUG_ON check to
    use the size of "requests" instead of the hard-coded '32'.
    
    That gives us a bit more room again for arch-specific requests as we
    already ran out of space for x86 due to the hard-coded check.
    
    The only exception here is ARM32 as it is still 32-bits.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Reviewed-by: Jim Mattson <jmattson@google.com>
    Signed-off-by: KarimAllah Ahmed <karahmed@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5f138b40e433..2840ac74a3d7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -130,7 +130,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQUEST_ARCH_BASE     8
 
 #define KVM_ARCH_REQ_FLAGS(nr, flags) ({ \
-	BUILD_BUG_ON((unsigned)(nr) >= 32 - KVM_REQUEST_ARCH_BASE); \
+	BUILD_BUG_ON((unsigned)(nr) >= (FIELD_SIZEOF(struct kvm_vcpu, requests) * 8) - KVM_REQUEST_ARCH_BASE); \
 	(unsigned)(((nr) + KVM_REQUEST_ARCH_BASE) | (flags)); \
 })
 #define KVM_ARCH_REQ(nr)           KVM_ARCH_REQ_FLAGS(nr, 0)
@@ -224,7 +224,7 @@ struct kvm_vcpu {
 	int vcpu_id;
 	int srcu_idx;
 	int mode;
-	unsigned long requests;
+	u64 requests;
 	unsigned long guest_debug;
 
 	int pre_pcpu;
@@ -1131,7 +1131,7 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	 * caller.  Paired with the smp_mb__after_atomic in kvm_check_request.
 	 */
 	smp_wmb();
-	set_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
+	set_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
 static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
@@ -1141,12 +1141,12 @@ static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
-	return test_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
+	return test_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
 static inline void kvm_clear_request(int req, struct kvm_vcpu *vcpu)
 {
-	clear_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
+	clear_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
 static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)

commit 03133347b4452ef9b1f1456b92f5fafa467c0655
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Mon Apr 30 18:33:24 2018 +0200

    KVM: s390: a utility function for migration
    
    Introduce a utility function that will be used later on for storage
    attributes migration, and use it in kvm_main.c to replace existing code
    that does the same thing.
    
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Message-Id: <1525106005-13931-2-git-send-email-imbrenda@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4ee7bc548a83..5f138b40e433 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -309,6 +309,13 @@ static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memsl
 	return ALIGN(memslot->npages, BITS_PER_LONG) / 8;
 }
 
+static inline unsigned long *kvm_second_dirty_bitmap(struct kvm_memory_slot *memslot)
+{
+	unsigned long len = kvm_dirty_bitmap_bytes(memslot);
+
+	return memslot->dirty_bitmap + len / sizeof(*memslot->dirty_bitmap);
+}
+
 struct kvm_s390_adapter_int {
 	u64 ind_addr;
 	u64 summary_addr;

commit d1e5b0e98ea27b4f17871dc4e8ea4b0447e35221
Author: Marc Orr <marcorr@google.com>
Date:   Tue May 15 04:37:37 2018 -0700

    kvm: Make VM ioctl do valloc for some archs
    
    The kvm struct has been bloating. For example, it's tens of kilo-bytes
    for x86, which turns out to be a large amount of memory to allocate
    contiguously via kzalloc. Thus, this patch does the following:
    1. Uses architecture-specific routines to allocate the kvm struct via
       vzalloc for x86.
    2. Switches arm to __KVM_HAVE_ARCH_VM_ALLOC so that it can use vzalloc
       when has_vhe() is true.
    
    Other architectures continue to default to kalloc, as they have a
    dependency on kalloc or have a small-enough struct kvm.
    
    Signed-off-by: Marc Orr <marcorr@google.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bca28637bcd4..4ee7bc548a83 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -19,6 +19,7 @@
 #include <linux/preempt.h>
 #include <linux/msi.h>
 #include <linux/slab.h>
+#include <linux/vmalloc.h>
 #include <linux/rcupdate.h>
 #include <linux/ratelimit.h>
 #include <linux/err.h>
@@ -811,6 +812,10 @@ bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
+/*
+ * All architectures that want to use vzalloc currently also
+ * need their own kvm_arch_alloc_vm implementation.
+ */
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
 	return kzalloc(sizeof(struct kvm), GFP_KERNEL);

commit 1499fa809e9e6713952ef84a7e9d51606881681f
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Thu Apr 19 00:49:58 2018 +0530

    kvm: Change return type to vm_fault_t
    
    Use new return type vm_fault_t for fault handler. For
    now, this is just documenting that the function returns
    a VM_FAULT value rather than an errno. Once all instances
    are converted, vm_fault_t will become a distinct type.
    
    commit 1c8f422059ae ("mm: change return type to vm_fault_t")
    
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b81769a5a2b7..bca28637bcd4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -739,7 +739,7 @@ long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg);
-int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);
+vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);
 
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 

commit 5eec43a1fa2a7ec5225411c97538fa582d36f579
Merge: 75025cc9d13f e25028c8ded0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 1 19:17:22 2018 +0200

    Merge tag 'kvmarm-for-v4.18' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.18
    
    - Lazy context-switching of FPSIMD registers on arm64
    - Allow virtual redistributors to be part of two or more MMIO ranges

commit 7053df4edb3ae3ae15c316fe49122c0b3936e9dd
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Wed May 16 17:21:28 2018 +0200

    KVM: introduce kvm_make_vcpus_request_mask() API
    
    Hyper-V style PV TLB flush hypercalls inmplementation will use this API.
    To avoid memory allocation in CONFIG_CPUMASK_OFFSTACK case add
    cpumask_var_t argument.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6d6e79c59e68..14e710d639c7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -730,6 +730,9 @@ void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
+
+bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
+				 unsigned long *vcpu_bitmap, cpumask_var_t tmp);
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);
 
 long kvm_arch_dev_ioctl(struct file *filp,

commit bd2a6394fd2d3ea528d4b9c67f829e35f1f5d5dd
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Fri Feb 23 17:23:57 2018 +0100

    KVM: arm/arm64: Introduce kvm_arch_vcpu_run_pid_change
    
    KVM/ARM differs from other architectures in having to maintain an
    additional virtual address space from that of the host and the
    guest, because we split the execution of KVM across both EL1 and
    EL2.
    
    This results in a need to explicitly map data structures into EL2
    (hyp) which are accessed from the hyp code.  As we are about to be
    more clever with our FPSIMD handling on arm64, which stores data in
    the task struct and uses thread_info flags, we will have to map
    parts of the currently executing task struct into the EL2 virtual
    address space.
    
    However, we don't want to do this on every KVM_RUN, because it is a
    fairly expensive operation to walk the page tables, and the common
    execution mode is to map a single thread to a VCPU.  By introducing
    a hook that architectures can select with
    HAVE_KVM_VCPU_RUN_PID_CHANGE, we do not introduce overhead for
    other architectures, but have a simple way to only map the data we
    need when required for arm64.
    
    This patch introduces the framework only, and wires it up in the
    arm/arm64 KVM common code.
    
    No functional change.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6930c63126c7..4268ace60bf1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1276,4 +1276,13 @@ static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
 void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 		unsigned long start, unsigned long end);
 
+#ifdef CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE
+int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu);
+#else
+static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+#endif /* CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE */
+
 #endif

commit ddc9cfb79c1096a0855839631c091aa7e9602052
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Thu Apr 26 17:55:03 2018 -0700

    KVM: Extend MAX_IRQ_ROUTES to 4096 for all archs
    
    Our virtual machines make use of device assignment by configuring
    12 NVMe disks for high I/O performance. Each NVMe device has 129
    MSI-X Table entries:
    Capabilities: [50] MSI-X: Enable+ Count=129 Masked-Vector table: BAR=0 offset=00002000
    The windows virtual machines fail to boot since they will map the number of
    MSI-table entries that the NVMe hardware reported to the bus to msi routing
    table, this will exceed the 1024. This patch extends MAX_IRQ_ROUTES to 4096
    for all archs, in the future this might be extended again if needed.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Cc: Cornelia Huck <cohuck@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Tonny Lu <tonnylu@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6930c63126c7..6d6e79c59e68 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1045,13 +1045,7 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 
 #ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 
-#ifdef CONFIG_S390
-#define KVM_MAX_IRQ_ROUTES 4096 //FIXME: we can have more than that...
-#elif defined(CONFIG_ARM64)
-#define KVM_MAX_IRQ_ROUTES 4096
-#else
-#define KVM_MAX_IRQ_ROUTES 1024
-#endif
+#define KVM_MAX_IRQ_ROUTES 4096 /* might need extension/rework in the future */
 
 bool kvm_arch_can_set_irq_routing(struct kvm *kvm);
 int kvm_set_irq_routing(struct kvm *kvm,

commit f75e4924f0152be747bf04c9d16bb23fd8baf5f9
Author: Sebastian Ott <sebott@linux.vnet.ibm.com>
Date:   Thu Feb 22 13:04:39 2018 +0100

    kvm: fix warning for non-x86 builds
    
    Fix the following sparse warning by moving the prototype
    of kvm_arch_mmu_notifier_invalidate_range() to linux/kvm_host.h .
    
      CHECK   arch/s390/kvm/../../../virt/kvm/kvm_main.c
    arch/s390/kvm/../../../virt/kvm/kvm_main.c:138:13: warning: symbol 'kvm_arch_mmu_notifier_invalidate_range' was not declared. Should it be static?
    
    Signed-off-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 84b9c50693f2..6930c63126c7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1273,4 +1273,7 @@ static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
 }
 #endif /* CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL */
 
+void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
+		unsigned long start, unsigned long end);
+
 #endif

commit 076467490b8176eb96eddc548a14d4135c7b5852
Author: Sebastian Ott <sebott@linux.vnet.ibm.com>
Date:   Thu Feb 22 13:05:41 2018 +0100

    kvm: fix warning for CONFIG_HAVE_KVM_EVENTFD builds
    
    Move the kvm_arch_irq_routing_update() prototype outside of
    ifdef CONFIG_HAVE_KVM_EVENTFD guards to fix the following sparse warning:
    
    arch/s390/kvm/../../../virt/kvm/irqchip.c:171:28: warning: symbol 'kvm_arch_irq_routing_update' was not declared. Should it be static?
    
    Signed-off-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ac0062b74aed..84b9c50693f2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1105,7 +1105,6 @@ static inline void kvm_irq_routing_update(struct kvm *kvm)
 {
 }
 #endif
-void kvm_arch_irq_routing_update(struct kvm *kvm);
 
 static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
@@ -1114,6 +1113,8 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
+void kvm_arch_irq_routing_update(struct kvm *kvm);
+
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	/*

commit 5cb0944c0c66004c0d9006a7f0fba5782ae38f69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Dec 12 17:41:34 2017 +0100

    KVM: introduce kvm_arch_vcpu_async_ioctl
    
    After the vcpu_load/vcpu_put pushdown, the handling of asynchronous VCPU
    ioctl is already much clearer in that it is obvious that they bypass
    vcpu_load and vcpu_put.
    
    However, it is still not perfect in that the different state of the VCPU
    mutex is still hidden in the caller.  Separate those ioctls into a new
    function kvm_arch_vcpu_async_ioctl that returns -ENOIOCTLCMD for more
    "traditional" synchronous ioctls.
    
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Suggested-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 09de0ff3d677..ac0062b74aed 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1260,4 +1260,16 @@ static inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)
 }
 #endif /* CONFIG_HAVE_KVM_INVALID_WAKEUPS */
 
+#ifdef CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL
+long kvm_arch_vcpu_async_ioctl(struct file *filp,
+			       unsigned int ioctl, unsigned long arg);
+#else
+static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
+					     unsigned int ioctl,
+					     unsigned long arg)
+{
+	return -ENOIOCTLCMD;
+}
+#endif /* CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL */
+
 #endif

commit ec7660ccdd2b71d8c7f0243f8590253713e9b75d
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:23 2017 +0100

    KVM: Take vcpu->mutex outside vcpu_load
    
    As we're about to call vcpu_load() from architecture-specific
    implementations of the KVM vcpu ioctls, but yet we access data
    structures protected by the vcpu->mutex in the generic code, factor
    this logic out from vcpu_load().
    
    x86 is the only architecture which calls vcpu_load() outside of the main
    vcpu ioctl function, and these calls will no longer take the vcpu mutex
    following this patch.  However, with the exception of
    kvm_arch_vcpu_postcreate (see below), the callers are either in the
    creation or destruction path of the VCPU, which means there cannot be
    any concurrent access to the data structure, because the file descriptor
    is not yet accessible, or is already gone.
    
    kvm_arch_vcpu_postcreate makes the newly created vcpu potentially
    accessible by other in-kernel threads through the kvm->vcpus array, and
    we therefore take the vcpu mutex in this case directly.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6bdd4b9f6611..09de0ff3d677 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -533,7 +533,7 @@ static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 
-int __must_check vcpu_load(struct kvm_vcpu *vcpu);
+void vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_IOAPIC

commit f775b13eedee2f7f3c6fdd4e90fb79090ce5d339
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Nov 14 16:54:23 2017 -0500

    x86,kvm: move qemu/guest FPU switching out to vcpu_run
    
    Currently, every time a VCPU is scheduled out, the host kernel will
    first save the guest FPU/xstate context, then load the qemu userspace
    FPU context, only to then immediately save the qemu userspace FPU
    context back to memory. When scheduling in a VCPU, the same extraneous
    FPU loads and saves are done.
    
    This could be avoided by moving from a model where the guest FPU is
    loaded and stored with preemption disabled, to a model where the
    qemu userspace FPU is swapped out for the guest FPU context for
    the duration of the KVM_RUN ioctl.
    
    This is done under the VCPU mutex, which is also taken when other
    tasks inspect the VCPU FPU context, so the code should already be
    safe for this change. That should come as no surprise, given that
    s390 already has this optimization.
    
    This can fix a bug where KVM calls get_user_pages while owning the
    FPU, and the file system ends up requesting the FPU again:
    
        [258270.527947]  __warn+0xcb/0xf0
        [258270.527948]  warn_slowpath_null+0x1d/0x20
        [258270.527951]  kernel_fpu_disable+0x3f/0x50
        [258270.527953]  __kernel_fpu_begin+0x49/0x100
        [258270.527955]  kernel_fpu_begin+0xe/0x10
        [258270.527958]  crc32c_pcl_intel_update+0x84/0xb0
        [258270.527961]  crypto_shash_update+0x3f/0x110
        [258270.527968]  crc32c+0x63/0x8a [libcrc32c]
        [258270.527975]  dm_bm_checksum+0x1b/0x20 [dm_persistent_data]
        [258270.527978]  node_prepare_for_write+0x44/0x70 [dm_persistent_data]
        [258270.527985]  dm_block_manager_write_callback+0x41/0x50 [dm_persistent_data]
        [258270.527988]  submit_io+0x170/0x1b0 [dm_bufio]
        [258270.527992]  __write_dirty_buffer+0x89/0x90 [dm_bufio]
        [258270.527994]  __make_buffer_clean+0x4f/0x80 [dm_bufio]
        [258270.527996]  __try_evict_buffer+0x42/0x60 [dm_bufio]
        [258270.527998]  dm_bufio_shrink_scan+0xc0/0x130 [dm_bufio]
        [258270.528002]  shrink_slab.part.40+0x1f5/0x420
        [258270.528004]  shrink_node+0x22c/0x320
        [258270.528006]  do_try_to_free_pages+0xf5/0x330
        [258270.528008]  try_to_free_pages+0xe9/0x190
        [258270.528009]  __alloc_pages_slowpath+0x40f/0xba0
        [258270.528011]  __alloc_pages_nodemask+0x209/0x260
        [258270.528014]  alloc_pages_vma+0x1f1/0x250
        [258270.528017]  do_huge_pmd_anonymous_page+0x123/0x660
        [258270.528021]  handle_mm_fault+0xfd3/0x1330
        [258270.528025]  __get_user_pages+0x113/0x640
        [258270.528027]  get_user_pages+0x4f/0x60
        [258270.528063]  __gfn_to_pfn_memslot+0x120/0x3f0 [kvm]
        [258270.528108]  try_async_pf+0x66/0x230 [kvm]
        [258270.528135]  tdp_page_fault+0x130/0x280 [kvm]
        [258270.528149]  kvm_mmu_page_fault+0x60/0x120 [kvm]
        [258270.528158]  handle_ept_violation+0x91/0x170 [kvm_intel]
        [258270.528162]  vmx_handle_exit+0x1ca/0x1400 [kvm_intel]
    
    No performance changes were detected in quick ping-pong tests on
    my 4 socket system, which is expected since an FPU+xstate load is
    on the order of 0.1us, while ping-ponging between CPUs is on the
    order of 20us, and somewhat noisy.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Suggested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [Fixed a bug where reset_vcpu called put_fpu without preceding load_fpu,
     which happened inside from KVM_CREATE_VCPU ioctl. - Radim]
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 893d6d606cd0..6bdd4b9f6611 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -232,7 +232,7 @@ struct kvm_vcpu {
 	struct mutex mutex;
 	struct kvm_run *run;
 
-	int guest_fpu_loaded, guest_xcr0_loaded;
+	int guest_xcr0_loaded;
 	struct swait_queue_head wq;
 	struct pid __rcu *pid;
 	int sigset_active;

commit 20b7035c66bacc909ae3ffe92c1a1ea7db99fe4f
Author: Jan H. Schönherr <jschoenh@amazon.de>
Date:   Fri Nov 24 22:39:01 2017 +0100

    KVM: Let KVM_SET_SIGNAL_MASK work as advertised
    
    KVM API says for the signal mask you set via KVM_SET_SIGNAL_MASK, that
    "any unblocked signal received [...] will cause KVM_RUN to return with
    -EINTR" and that "the signal will only be delivered if not blocked by
    the original signal mask".
    
    This, however, is only true, when the calling task has a signal handler
    registered for a signal. If not, signal evaluation is short-circuited for
    SIG_IGN and SIG_DFL, and the signal is either ignored without KVM_RUN
    returning or the whole process is terminated.
    
    Make KVM_SET_SIGNAL_MASK behave as advertised by utilizing logic similar
    to that in do_sigtimedwait() to avoid short-circuiting of signals.
    
    Signed-off-by: Jan H. SchÃ¶nherr <jschoenh@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2e754b7c282c..893d6d606cd0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -715,6 +715,9 @@ int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
 			 unsigned long len);
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);
 
+void kvm_sigset_activate(struct kvm_vcpu *vcpu);
+void kvm_sigset_deactivate(struct kvm_vcpu *vcpu);
+
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);

commit f7a6509fe002e3909cb41c09e807b7f3ca4a361b
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Sep 1 17:11:43 2017 +0200

    KVM: s390: vsie: use common code functions for pinning
    
    We will not see -ENOMEM (gfn_to_hva() will return KVM_ERR_PTR_BAD_PAGE
    for all errors). So we can also get rid of special handling in the
    callers of pin_guest_page() and always assume that it is a g2 error.
    
    As also kvm_s390_inject_program_int() should never fail, we can
    simplify pin_scb(), too.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Message-Id: <20170901151143.22714-1-david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6882538eda32..2e754b7c282c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -667,6 +667,7 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool *writable);
 
 void kvm_release_pfn_clean(kvm_pfn_t pfn);
+void kvm_release_pfn_dirty(kvm_pfn_t pfn);
 void kvm_set_pfn_dirty(kvm_pfn_t pfn);
 void kvm_set_pfn_accessed(kvm_pfn_t pfn);
 void kvm_get_pfn(kvm_pfn_t pfn);

commit 199b5763d329b43c88f6ad539db8a6c6b42f8edb
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:32 2017 +0800

    KVM: add spinlock optimization framework
    
    If a vcpu exits due to request a user mode spinlock, then
    the spinlock-holder may be preempted in user mode or kernel mode.
    (Note that not all architectures trap spin loops in user mode,
    only AMD x86 and ARM/ARM64 currently do).
    
    But if a vcpu exits in kernel mode, then the holder must be
    preempted in kernel mode, so we should choose a vcpu in kernel mode
    as a more likely candidate for the lock holder.
    
    This introduces kvm_arch_vcpu_in_kernel() to decide whether the
    vcpu is in kernel-mode when it's preempted.  kvm_vcpu_on_spin's
    new argument says the same of the spinning VCPU.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 28112d7917c1..6882538eda32 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -720,7 +720,7 @@ void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
-void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
+void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu, bool usermode_vcpu_not_eligible);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
@@ -800,6 +800,7 @@ int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
 void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
+bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC

commit 5e2f30b756a37bd80c5b0471d0e10d769ab2eb9a
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Aug 3 18:11:04 2017 +0200

    KVM: nVMX: get rid of nested_get_page()
    
    nested_get_page() just sounds confusing. All we want is a page from G1.
    This is even unrelated to nested.
    
    Let's introduce kvm_vcpu_gpa_to_page() so we don't get too lengthy
    lines.
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    [Squash pasto fix from Wanpeng Li. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 21a6fd6c44af..28112d7917c1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -985,6 +985,12 @@ static inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)
 	return (hpa_t)pfn << PAGE_SHIFT;
 }
 
+static inline struct page *kvm_vcpu_gpa_to_page(struct kvm_vcpu *vcpu,
+						gpa_t gpa)
+{
+	return kvm_vcpu_gfn_to_page(vcpu, gpa_to_gfn(gpa));
+}
+
 static inline bool kvm_is_error_gpa(struct kvm *kvm, gpa_t gpa)
 {
 	unsigned long hva = gfn_to_hva(kvm, gpa_to_gfn(gpa));

commit 3898da947bbaf9e7fd5816e825978d360028bba2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 2 17:55:54 2017 +0200

    KVM: avoid using rcu_dereference_protected
    
    During teardown, accesses to memslots and buses are using
    rcu_dereference_protected with an always-true condition because
    these accesses are done outside the usual mutexes.  This
    is because the last reference is gone and there cannot be any
    concurrent modifications, but rcu_dereference_protected is
    ugly and unobvious.
    
    Instead, check the refcount in kvm_get_bus and __kvm_memslots.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 890b706d1943..21a6fd6c44af 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -477,7 +477,8 @@ struct kvm {
 static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 {
 	return srcu_dereference_check(kvm->buses[idx], &kvm->srcu,
-				      lockdep_is_held(&kvm->slots_lock));
+				      lockdep_is_held(&kvm->slots_lock) ||
+				      !refcount_read(&kvm->users_count));
 }
 
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
@@ -570,7 +571,8 @@ void kvm_put_kvm(struct kvm *kvm);
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
 	return srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,
-			lockdep_is_held(&kvm->slots_lock));
+			lockdep_is_held(&kvm->slots_lock) ||
+			!refcount_read(&kvm->users_count));
 }
 
 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)

commit fdeaf7e3eb37c6dbc4b4ac97dbe1945d239eb788
Author: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
Date:   Mon Jul 24 13:40:03 2017 +0200

    KVM: make pid available for uevents without debugfs
    
    Simplify and improve the code so that the PID is always available in
    the uevent even when debugfs is not available.
    
    This adds a userspace_pid field to struct kvm, as per Radim's
    suggestion, so that the PID can be retrieved on destruction too.
    
    Acked-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Fixes: 286de8f6ac9202 ("KVM: trigger uevents when creating or destroying a VM")
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 648b34cabb38..890b706d1943 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -445,6 +445,7 @@ struct kvm {
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
+	pid_t userspace_pid;
 };
 
 #define kvm_err(fmt, ...) \

commit 7e988b103d0d52190244517edc76e649071284bb
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jul 7 15:49:00 2017 +0200

    KVM: use correct accessor function for __kvm_memslots
    
    kvm memslots are protected by srcu and not by rcu. We must use
    srcu_dereference_check instead of rcu_dereference_check.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b3ca77a96b2d..648b34cabb38 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -568,9 +568,8 @@ void kvm_put_kvm(struct kvm *kvm);
 
 static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
-	return rcu_dereference_check(kvm->memslots[as_id],
-			srcu_read_lock_held(&kvm->srcu)
-			|| lockdep_is_held(&kvm->slots_lock));
+	return srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,
+			lockdep_is_held(&kvm->slots_lock));
 }
 
 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)

commit a80cf7b5f4149753d5f19c872a47e66195b167d4
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jul 6 16:17:14 2017 +0200

    KVM: mark memory slots as rcu
    
    we access the memslots array via srcu. Mark it as such and
    use the right access functions also for the freeing of
    memory slots.
    
    Found by sparse:
    ./include/linux/kvm_host.h:565:16: error: incompatible types in
    comparison expression (different address spaces)
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6a164f9eb02c..b3ca77a96b2d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -390,7 +390,7 @@ struct kvm {
 	spinlock_t mmu_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
-	struct kvm_memslots *memslots[KVM_ADDRESS_SPACE_NUM];
+	struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 
 	/*

commit 4a12f95177280a660bda99e81838919b1cc6a91a
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jul 7 10:51:38 2017 +0200

    KVM: mark kvm->busses as rcu protected
    
    mark kvm->busses as rcu protected and use the correct access
    function everywhere.
    
    found by sparse
    virt/kvm/kvm_main.c:3490:15: error: incompatible types in comparison expression (different address spaces)
    virt/kvm/kvm_main.c:3509:15: error: incompatible types in comparison expression (different address spaces)
    virt/kvm/kvm_main.c:3561:15: error: incompatible types in comparison expression (different address spaces)
    virt/kvm/kvm_main.c:3644:15: error: incompatible types in comparison expression (different address spaces)
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bcd37b855c66..6a164f9eb02c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -404,7 +404,7 @@ struct kvm {
 	int last_boosted_vcpu;
 	struct list_head vm_list;
 	struct mutex lock;
-	struct kvm_io_bus *buses[KVM_NR_BUSES];
+	struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 	struct {
 		spinlock_t        lock;
@@ -473,6 +473,12 @@ struct kvm {
 #define vcpu_err(vcpu, fmt, ...)					\
 	kvm_err("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 
+static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
+{
+	return srcu_dereference_check(kvm->buses[idx], &kvm->srcu,
+				      lockdep_is_held(&kvm->slots_lock));
+}
+
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu, in case

commit 0e4524a5d341e719e8ee9ee7db5d58e2c5a4c10e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jul 6 14:44:28 2017 +0200

    KVM: mark vcpu->pid pointer as rcu protected
    
    We do use rcu to protect the pid pointer. Mark it as such and
    adopt all code to use the proper access methods.
    
    This was detected by sparse.
    "virt/kvm/kvm_main.c:2248:15: error: incompatible types in comparison
    expression (different address spaces)"
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0b50e7b35ed4..bcd37b855c66 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -234,7 +234,7 @@ struct kvm_vcpu {
 
 	int guest_fpu_loaded, guest_xcr0_loaded;
 	struct swait_queue_head wq;
-	struct pid *pid;
+	struct pid __rcu *pid;
 	int sigset_active;
 	sigset_t sigset;
 	struct kvm_vcpu_stat stat;

commit 2fa6e1e12a024b48b2c7ea39f50205246e027da7
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Sun Jun 4 14:43:52 2017 +0200

    KVM: add kvm_request_pending
    
    A first step in vcpu->requests encapsulation.  Additionally, we now
    use READ_ONCE() when accessing vcpu->requests, which ensures we
    always load vcpu->requests when it's accessed.  This is important as
    other threads can change it any time.  Also, READ_ONCE() documents
    that vcpu->requests is used with other threads, likely requiring
    memory barriers, which it does.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    [ Documented the new use of READ_ONCE() and converted another check
      in arch/mips/kvm/vz.c ]
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3724b51aab64..0b50e7b35ed4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1105,6 +1105,11 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	set_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
 }
 
+static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
+{
+	return READ_ONCE(vcpu->requests);
+}
+
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
 	return test_bit(req & KVM_REQUEST_MASK, &vcpu->requests);

commit 2387149eade25f32dcf1398811b3d0293181d005
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:51 2017 +0200

    KVM: improve arch vcpu request defining
    
    Marc Zyngier suggested that we define the arch specific VCPU request
    base, rather than requiring each arch to remember to start from 8.
    That suggestion, along with Radim Krcmar's recent VCPU request flag
    addition, snowballed into defining something of an arch VCPU request
    defining API.
    
    No functional change.
    
    (Looks like x86 is running out of arch VCPU request bits.  Maybe
     someday we'll need to extend to 64.)
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8c0664309815..3724b51aab64 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -126,6 +126,13 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_MMU_RELOAD        (1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_PENDING_TIMER     2
 #define KVM_REQ_UNHALT            3
+#define KVM_REQUEST_ARCH_BASE     8
+
+#define KVM_ARCH_REQ_FLAGS(nr, flags) ({ \
+	BUILD_BUG_ON((unsigned)(nr) >= 32 - KVM_REQUEST_ARCH_BASE); \
+	(unsigned)(((nr) + KVM_REQUEST_ARCH_BASE) | (flags)); \
+})
+#define KVM_ARCH_REQ(nr)           KVM_ARCH_REQ_FLAGS(nr, 0)
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 5ccd414080822d5257c3569f4aeca74f63f4a257
Merge: 29250d301b0c 36c344f3f1ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 10 11:29:23 2017 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Paolo Bonzini:
     "ARM:
       - bugfixes
       - moved shared 32-bit/64-bit files to virt/kvm/arm
       - support for saving/restoring virtual ITS state to userspace
    
      PPC:
       - XIVE (eXternal Interrupt Virtualization Engine) support
    
      x86:
       - nVMX improvements, including emulated page modification logging
         (PML) which brings nice performance improvements on some workloads"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (45 commits)
      KVM: arm/arm64: vgic-its: Cleanup after failed ITT restore
      KVM: arm/arm64: Don't call map_resources when restoring ITS tables
      KVM: arm/arm64: Register ITS iodev when setting base address
      KVM: arm/arm64: Get rid of its->initialized field
      KVM: arm/arm64: Register iodevs when setting redist base and creating VCPUs
      KVM: arm/arm64: Slightly rework kvm_vgic_addr
      KVM: arm/arm64: Make vgic_v3_check_base more broadly usable
      KVM: arm/arm64: Refactor vgic_register_redist_iodevs
      KVM: Add kvm_vcpu_get_idx to get vcpu index in kvm->vcpus
      nVMX: Advertise PML to L1 hypervisor
      nVMX: Implement emulated Page Modification Logging
      kvm: x86: Add a hook for arch specific dirty logging emulation
      kvm: nVMX: Validate CR3 target count on nested VM-entry
      KVM: set no_llseek in stat_fops_per_vm
      KVM: arm/arm64: vgic: Rename kvm_vgic_vcpu_init to kvm_vgic_vcpu_enable
      KVM: arm/arm64: Clarification and relaxation to ITS save/restore ABI
      KVM: arm64: vgic-v3: KVM_DEV_ARM_VGIC_SAVE_PENDING_TABLES
      KVM: arm64: vgic-its: Fix pending table sync
      KVM: arm64: vgic-its: ITT save and restore
      KVM: arm64: vgic-its: Device table save/restore
      ...

commit de4d195308ad589626571dbe5789cebf9695a204
Merge: dc9edaab90de 20652ed6e44f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 10 09:50:55 2017 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes are:
    
       - Debloat RCU headers
    
       - Parallelize SRCU callback handling (plus overlapping patches)
    
       - Improve the performance of Tree SRCU on a CPU-hotplug stress test
    
       - Documentation updates
    
       - Miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (74 commits)
      rcu: Open-code the rcu_cblist_n_lazy_cbs() function
      rcu: Open-code the rcu_cblist_n_cbs() function
      rcu: Open-code the rcu_cblist_empty() function
      rcu: Separately compile large rcu_segcblist functions
      srcu: Debloat the <linux/rcu_segcblist.h> header
      srcu: Adjust default auto-expediting holdoff
      srcu: Specify auto-expedite holdoff time
      srcu: Expedite first synchronize_srcu() when idle
      srcu: Expedited grace periods with reduced memory contention
      srcu: Make rcutorture writer stalls print SRCU GP state
      srcu: Exact tracking of srcu_data structures containing callbacks
      srcu: Make SRCU be built by default
      srcu: Fix Kconfig botch when SRCU not selected
      rcu: Make non-preemptive schedule be Tasks RCU quiescent state
      srcu: Expedite srcu_schedule_cbs_snp() callback invocation
      srcu: Parallelize callback handling
      kvm: Move srcu_struct fields to end of struct kvm
      rcu: Fix typo in PER_RCU_NODE_PERIOD header comment
      rcu: Use true/false in assignment to bool
      rcu: Use bool value directly
      ...

commit 36c344f3f1ffc0b1b20abd237b7401dc6687ee8f
Merge: 03efce6f935f a2b19e6e2d4b
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 9 12:51:49 2017 +0200

    Merge tag 'kvm-arm-for-v4.12-round2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    Second round of KVM/ARM Changes for v4.12.
    
    Changes include:
     - A fix related to the 32-bit idmap stub
     - A fix to the bitmask used to deode the operands of an AArch32 CP
       instruction
     - We have moved the files shared between arch/arm/kvm and
       arch/arm64/kvm to virt/kvm/arm
     - We add support for saving/restoring the virtual ITS state to
       userspace

commit 497d72d80a789501501cccabdad6b145f9e31371
Author: Christoffer Dall <cdall@linaro.org>
Date:   Mon May 8 20:38:40 2017 +0200

    KVM: Add kvm_vcpu_get_idx to get vcpu index in kvm->vcpus
    
    There are occasional needs to use the index of vcpu in the kvm->vcpus
    array to map something related to a VCPU.  For example, unlike the
    vcpu->vcpu_id, the vcpu index is guaranteed to not be sparse across all
    vcpus which is useful when allocating a memory area for each vcpu.
    
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c14ad9809da..12eb26d665e8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -490,6 +490,17 @@ static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 	return NULL;
 }
 
+static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
+{
+	struct kvm_vcpu *tmp;
+	int idx;
+
+	kvm_for_each_vcpu(idx, tmp, vcpu->kvm)
+		if (tmp == vcpu)
+			return idx;
+	BUG();
+}
+
 #define kvm_for_each_memslot(memslot, slots)	\
 	for (memslot = &slots->memslots[0];	\
 	      memslot < slots->memslots + KVM_MEM_SLOTS_NUM && memslot->npages;\

commit 4415b335282591e76762cd9e6dc60932a7595fc3
Merge: 3bed8888edc8 fb7dcf723dd2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 9 11:50:01 2017 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    The main thing here is a new implementation of the in-kernel
    XICS interrupt controller emulation for POWER9 machines, from Ben
    Herrenschmidt.
    
    POWER9 has a new interrupt controller called XIVE (eXternal Interrupt
    Virtualization Engine) which is able to deliver interrupts directly
    to guest virtual CPUs in hardware without hypervisor intervention.
    With this new code, the guest still sees the old XICS interface but
    performance is better because the XICS emulation in the host uses the
    XIVE directly rather than going through a XICS emulation in firmware.
    
    Conflicts:
            arch/powerpc/kernel/cpu_setup_power.S [cherry-picked fix]
            arch/powerpc/kvm/book3s_xive.c [include asm/debugfs.h]

commit bf5f89463f5b3109a72ed13ca62b57e90213387d
Merge: 2d3e4866dea9 4d2b5bcab53f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 8 18:17:56 2017 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - various misc things
    
     - procfs updates
    
     - lib/ updates
    
     - checkpatch updates
    
     - kdump/kexec updates
    
     - add kvmalloc helpers, use them
    
     - time helper updates for Y2038 issues. We're almost ready to remove
       current_fs_time() but that awaits a btrfs merge.
    
     - add tracepoints to DAX
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (114 commits)
      drivers/staging/ccree/ssi_hash.c: fix build with gcc-4.4.4
      selftests/vm: add a test for virtual address range mapping
      dax: add tracepoint to dax_insert_mapping()
      dax: add tracepoint to dax_writeback_one()
      dax: add tracepoints to dax_writeback_mapping_range()
      dax: add tracepoints to dax_load_hole()
      dax: add tracepoints to dax_pfn_mkwrite()
      dax: add tracepoints to dax_iomap_pte_fault()
      mtd: nand: nandsim: convert to memalloc_noreclaim_*()
      treewide: convert PF_MEMALLOC manipulations to new helpers
      mm: introduce memalloc_noreclaim_{save,restore}
      mm: prevent potential recursive reclaim due to clearing PF_MEMALLOC
      mm/huge_memory.c: deposit a pgtable for DAX PMD faults when required
      mm/huge_memory.c: use zap_deposited_table() more
      time: delete CURRENT_TIME_SEC and CURRENT_TIME
      gfs2: replace CURRENT_TIME with current_time
      apparmorfs: replace CURRENT_TIME with current_time()
      lustre: replace CURRENT_TIME macro
      fs: ubifs: replace CURRENT_TIME_SEC with current_time
      fs: ufs: use ktime_get_real_ts64() for birthtime
      ...

commit a7c3e901a46ff54c016d040847eda598a9e3e653
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:09 2017 -0700

    mm: introduce kv[mz]alloc helpers
    
    Patch series "kvmalloc", v5.
    
    There are many open coded kmalloc with vmalloc fallback instances in the
    tree.  Most of them are not careful enough or simply do not care about
    the underlying semantic of the kmalloc/page allocator which means that
    a) some vmalloc fallbacks are basically unreachable because the kmalloc
    part will keep retrying until it succeeds b) the page allocator can
    invoke a really disruptive steps like the OOM killer to move forward
    which doesn't sound appropriate when we consider that the vmalloc
    fallback is available.
    
    As it can be seen implementing kvmalloc requires quite an intimate
    knowledge if the page allocator and the memory reclaim internals which
    strongly suggests that a helper should be implemented in the memory
    subsystem proper.
    
    Most callers, I could find, have been converted to use the helper
    instead.  This is patch 6.  There are some more relying on __GFP_REPEAT
    in the networking stack which I have converted as well and Eric Dumazet
    was not opposed [2] to convert them as well.
    
    [1] http://lkml.kernel.org/r/20170130094940.13546-1-mhocko@kernel.org
    [2] http://lkml.kernel.org/r/1485273626.16328.301.camel@edumazet-glaptop3.roam.corp.google.com
    
    This patch (of 9):
    
    Using kmalloc with the vmalloc fallback for larger allocations is a
    common pattern in the kernel code.  Yet we do not have any common helper
    for that and so users have invented their own helpers.  Some of them are
    really creative when doing so.  Let's just add kv[mz]alloc and make sure
    it is implemented properly.  This implementation makes sure to not make
    a large memory pressure for > PAGE_SZE requests (__GFP_NORETRY) and also
    to not warn about allocation failures.  This also rules out the OOM
    killer as the vmalloc is a more approapriate fallback than a disruptive
    user visible action.
    
    This patch also changes some existing users and removes helpers which
    are specific for them.  In some cases this is not possible (e.g.
    ext4_kvmalloc, libcfs_kvzalloc) because those seems to be broken and
    require GFP_NO{FS,IO} context which is not vmalloc compatible in general
    (note that the page table allocation is GFP_KERNEL).  Those need to be
    fixed separately.
    
    While we are at it, document that __vmalloc{_node} about unsupported gfp
    mask because there seems to be a lot of confusion out there.
    kvmalloc_node will warn about GFP_KERNEL incompatible (which are not
    superset) flags to catch new abusers.  Existing ones would have to die
    slowly.
    
    [sfr@canb.auug.org.au: f2fs fixup]
      Link: http://lkml.kernel.org/r/20170320163735.332e64b7@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170306103032.2540-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Reviewed-by: Andreas Dilger <adilger@dilger.ca> [ext4 part]
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d0250744507a..5d9b2a08e553 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -767,8 +767,6 @@ void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
-void *kvm_kvzalloc(unsigned long size);
-
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {

commit 4e335d9e7ddbcf83d03e7fbe65797ebed2272c18
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 2 16:20:18 2017 +0200

    Revert "KVM: Support vCPU-based gfn->hva cache"
    
    This reverts commit bbd6411513aa8ef3ea02abab61318daf87c1af1e.
    
    I've been sitting on this revert for too long and it unfortunately
    missed 4.11.  It's also the reason why I haven't merged ring-based
    dirty tracking for 4.12.
    
    Using kvm_vcpu_memslots in kvm_gfn_to_hva_cache_init and
    kvm_vcpu_write_guest_offset_cached means that the MSR value can
    now be used to access SMRAM, simply by making it point to an SMRAM
    physical address.  This is problematic because it lets the guest
    OS overwrite memory that it shouldn't be able to touch.
    
    Cc: stable@vger.kernel.org
    Fixes: bbd6411513aa8ef3ea02abab61318daf87c1af1e
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 25cf258a1c9b..3727afdf614d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -650,18 +650,18 @@ int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
 			  unsigned long len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
-int kvm_vcpu_read_guest_cached(struct kvm_vcpu *vcpu, struct gfn_to_hva_cache *ghc,
-			       void *data, unsigned long len);
+int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, unsigned long len);
 int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 		    unsigned long len);
-int kvm_vcpu_write_guest_cached(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
-				void *data, unsigned long len);
-int kvm_vcpu_write_guest_offset_cached(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
-				       void *data, int offset, unsigned long len);
-int kvm_vcpu_gfn_to_hva_cache_init(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
-				   gpa_t gpa, unsigned long len);
+int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, unsigned long len);
+int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, int offset, unsigned long len);
+int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			      gpa_t gpa, unsigned long len);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);

commit 5c0aea0e8d98e38858fbb3a09870ed8487a01da2
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 28 17:06:20 2017 +0200

    KVM: x86: don't hold kvm->lock in KVM_SET_GSI_ROUTING
    
    We needed the lock to avoid racing with creation of the irqchip on x86. As
    kvm_set_irq_routing() calls srcu_synchronize_expedited(), this lock
    might be held for a longer time.
    
    Let's introduce an arch specific callback to check if we can actually
    add irq routes. For x86, all we have to do is check if we have an
    irqchip in the kernel. We don't need kvm->lock at that point as the
    irqchip is marked as inititalized only when actually fully created.
    
    Reported-by: Steve Rutherford <srutherford@google.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Fixes: 1df6ddede10a ("KVM: x86: race between KVM_SET_GSI_ROUTING and KVM_CREATE_IRQCHIP")
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a5bfffa8c8d4..25cf258a1c9b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1018,6 +1018,7 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 #define KVM_MAX_IRQ_ROUTES 1024
 #endif
 
+bool kvm_arch_can_set_irq_routing(struct kvm *kvm);
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,

commit fb7dcf723dd2cb1d5d8f2f49c3023130938848e3
Merge: db4b0dfab7b0 5af50993850a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 28 08:23:16 2017 +1000

    Merge remote-tracking branch 'remotes/powerpc/topic/xive' into kvm-ppc-next
    
    This merges in the powerpc topic/xive branch to bring in the code for
    the in-kernel XICS interrupt controller emulation to use the new XIVE
    (eXternal Interrupt Virtualization Engine) hardware in the POWER9 chip
    directly, rather than via a XICS emulation in firmware.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 7a97cec26b94c909f4cbad2dc3186af3e457a522
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 27 14:33:43 2017 +0200

    KVM: mark requests that need synchronization
    
    kvm_make_all_requests() provides a synchronization that waits until all
    kicked VCPUs have acknowledged the kick.  This is important for
    KVM_REQ_MMU_RELOAD as it prevents freeing while lockless paging is
    underway.
    
    This patch adds the synchronization property into all requests that are
    currently being used with kvm_make_all_requests() in order to preserve
    the current behavior and only introduce a new framework.  Removing it
    from requests where it is not necessary is left for future patches.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4a2c00092f8..a5bfffa8c8d4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -117,14 +117,15 @@ static inline bool is_error_page(struct page *page)
 
 #define KVM_REQUEST_MASK           GENMASK(7,0)
 #define KVM_REQUEST_NO_WAKEUP      BIT(8)
+#define KVM_REQUEST_WAIT           BIT(9)
 /*
  * Architecture-independent vcpu->requests bit members
  * Bits 4-7 are reserved for more arch-independent bits.
  */
-#define KVM_REQ_TLB_FLUSH          (0 | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_MMU_RELOAD         (1 | KVM_REQUEST_NO_WAKEUP)
-#define KVM_REQ_PENDING_TIMER      2
-#define KVM_REQ_UNHALT             3
+#define KVM_REQ_TLB_FLUSH         (0 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_MMU_RELOAD        (1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_PENDING_TIMER     2
+#define KVM_REQ_UNHALT            3
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 178f02ffafafc59d4d4b135242e5cc1515743680
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:26 2017 +0200

    KVM: return if kvm_vcpu_wake_up() did wake up the VCPU
    
    No need to kick a VCPU that we have just woken up.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 84c5396564f7..f4a2c00092f8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -690,7 +690,7 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
-void kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
+bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);

commit cde9af6e79046e12cd08d161139b1d5e57e9fbac
Author: Andrew Jones <drjones@redhat.com>
Date:   Wed Apr 26 22:32:24 2017 +0200

    KVM: add explicit barrier to kvm_vcpu_kick
    
    kvm_vcpu_kick() must issue a general memory barrier prior to reading
    vcpu->mode in order to ensure correctness of the mutual-exclusion
    memory barrier pattern used with vcpu->requests.  While the cmpxchg
    called from kvm_vcpu_kick():
    
     kvm_vcpu_kick
       kvm_arch_vcpu_should_kick
         kvm_vcpu_exiting_guest_mode
           cmpxchg
    
    implies general memory barriers before and after the operation, that
    implication is only valid when cmpxchg succeeds.  We need an explicit
    barrier for when it fails, otherwise a VCPU thread on its entry path
    that reads zero for vcpu->requests does not exclude the possibility
    the requesting thread sees !IN_GUEST_MODE when it reads vcpu->mode.
    
    kvm_make_all_cpus_request already had a barrier, so we remove it, as
    now it would be redundant.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a805ddcb7eb0..84c5396564f7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -270,6 +270,12 @@ struct kvm_vcpu {
 
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * The memory barrier ensures a previous write to vcpu->requests cannot
+	 * be reordered with the read of vcpu->mode.  It pairs with the general
+	 * memory barrier following the write of vcpu->mode in VCPU RUN.
+	 */
+	smp_mb__before_atomic();
 	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
 }
 

commit 930f7fd6da77ed9476a538345513460fd304aaf5
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:22 2017 +0200

    KVM: mark requests that do not need a wakeup
    
    Some operations must ensure that the guest is not running with stale
    data, but if the guest is halted, then the update can wait until another
    event happens.  kvm_make_all_requests() currently doesn't wake up, so we
    can mark all requests used with it.
    
    First 8 bits were arbitrarily reserved for request numbers.
    
    Most uses of requests have the request type as a constant, so a compiler
    will optimize the '&'.
    
    An alternative would be to have an inline function that would return
    whether the request needs a wake-up or not, but I like this one better
    even though it might produce worse assembly.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 374fa92c7657..a805ddcb7eb0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -115,12 +115,14 @@ static inline bool is_error_page(struct page *page)
 	return IS_ERR(page);
 }
 
+#define KVM_REQUEST_MASK           GENMASK(7,0)
+#define KVM_REQUEST_NO_WAKEUP      BIT(8)
 /*
  * Architecture-independent vcpu->requests bit members
  * Bits 4-7 are reserved for more arch-independent bits.
  */
-#define KVM_REQ_TLB_FLUSH          0
-#define KVM_REQ_MMU_RELOAD         1
+#define KVM_REQ_TLB_FLUSH          (0 | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_MMU_RELOAD         (1 | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_PENDING_TIMER      2
 #define KVM_REQ_UNHALT             3
 
@@ -1076,17 +1078,17 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	 * caller.  Paired with the smp_mb__after_atomic in kvm_check_request.
 	 */
 	smp_wmb();
-	set_bit(req, &vcpu->requests);
+	set_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
 }
 
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
-	return test_bit(req, &vcpu->requests);
+	return test_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
 }
 
 static inline void kvm_clear_request(int req, struct kvm_vcpu *vcpu)
 {
-	clear_bit(req, &vcpu->requests);
+	clear_bit(req & KVM_REQUEST_MASK, &vcpu->requests);
 }
 
 static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)

commit 72875d8a4d92f6f37e051be522b2252fd49bd50e
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:19 2017 +0200

    KVM: add kvm_{test,clear}_request to replace {test,clear}_bit
    
    Users were expected to use kvm_check_request() for testing and clearing,
    but request have expanded their use since then and some users want to
    only test or do a faster clear.
    
    Make sure that requests are not directly accessed with bit operations.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 397b7b5b1933..374fa92c7657 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1079,10 +1079,20 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	set_bit(req, &vcpu->requests);
 }
 
+static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
+{
+	return test_bit(req, &vcpu->requests);
+}
+
+static inline void kvm_clear_request(int req, struct kvm_vcpu *vcpu)
+{
+	clear_bit(req, &vcpu->requests);
+}
+
 static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 {
-	if (test_bit(req, &vcpu->requests)) {
-		clear_bit(req, &vcpu->requests);
+	if (kvm_test_request(req, vcpu)) {
+		kvm_clear_request(req, vcpu);
 
 		/*
 		 * Ensure the rest of the request is visible to kvm_check_request's

commit 5af50993850a48ba749b122173d789ea90976c72
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:56 2017 +1000

    KVM: PPC: Book3S HV: Native usage of the XIVE interrupt controller
    
    This patch makes KVM capable of using the XIVE interrupt controller
    to provide the standard PAPR "XICS" style hypercalls. It is necessary
    for proper operations when the host uses XIVE natively.
    
    This has been lightly tested on an actual system, including PCI
    pass-through with a TG3 device.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Cleanup pr_xxx(), unsplit pr_xxx() strings, etc., fix build
     failures by adding KVM_XIVE which depends on KVM_XICS and XIVE, and
     adding empty stubs for the kvm_xive_xxx() routines, fixup subject,
     integrate fixes from Paul for building PR=y HV=n]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c14ad9809da..d1a6e554ee68 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1165,7 +1165,6 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
 void kvm_unregister_device_ops(u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;
-extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
 extern struct kvm_device_ops kvm_arm_vgic_v3_ops;
 

commit 58d30c36d472b75e8e9962d6a640be19d9389128
Merge: 94836ecf1e73 f2094107ac82
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 23 11:12:44 2017 +0200

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
     - Documentation updates.
    
     - Miscellaneous fixes.
    
     - Parallelize SRCU callback handling (plus overlapping patches).
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6ade8694f471d847500c7cec152cc15171cef5d5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 20 17:30:06 2017 -0700

    kvm: Move srcu_struct fields to end of struct kvm
    
    Parallelizing SRCU callback handling will increase the size of
    srcu_struct, which will move the kvm structure's kvm_arch field out
    of reach of powerpc's current assembly code, which will result in the
    following sort of build error:
    
    arch/powerpc/kvm/book3s_hv_rmhandlers.S:617: Error: operand out of range (0x000000000000b328 is not between 0xffffffffffff8000 and 0x0000000000007fff)
    
    This commit moves the srcu_struct fields in the kvm structure to follow
    the kvm_arch field, which will allow powerpc's assembly code to continue
    to be able to reach the kvm_arch field.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Reported-by: Michael Ellerman <michaele@au1.ibm.com>
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    [ paulmck: Moved this commit to precede SRCU callback parallelization,
      and reworded the commit log into future tense, all in the name of
      bisectability. ]

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c14ad9809da..96c8e29c6442 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -375,8 +375,6 @@ struct kvm {
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots[KVM_ADDRESS_SPACE_NUM];
-	struct srcu_struct srcu;
-	struct srcu_struct irq_srcu;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 
 	/*
@@ -429,6 +427,8 @@ struct kvm {
 	struct list_head devices;
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
+	struct srcu_struct srcu;
+	struct srcu_struct irq_srcu;
 };
 
 #define kvm_err(fmt, ...) \

commit 993225adf4af20a0e50e37c3d4894b79c98e01c9
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Apr 7 10:50:33 2017 +0200

    KVM: x86: rename kvm_vcpu_request_scan_ioapic()
    
    Let's rename it into a proper arch specific callback.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7e74ae4d99bb..397b7b5b1933 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -502,10 +502,10 @@ int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_IOAPIC
-void kvm_vcpu_request_scan_ioapic(struct kvm *kvm);
+void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm);
 void kvm_arch_post_irq_routing_update(struct kvm *kvm);
 #else
-static inline void kvm_vcpu_request_scan_ioapic(struct kvm *kvm)
+static inline void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm)
 {
 }
 static inline void kvm_arch_post_irq_routing_update(struct kvm *kvm)

commit 4b4357e02523ec63ad853f927f5d93a25101a1d2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Mar 31 13:53:23 2017 +0200

    kvm: make KVM_COALESCED_MMIO_PAGE_OFFSET public
    
    Its value has never changed; we might as well make it part of the ABI instead
    of using the return value of KVM_CHECK_EXTENSION(KVM_CAP_COALESCED_MMIO).
    
    Because PPC does not always make MMIO available, the code has to be made
    dependent on CONFIG_KVM_MMIO rather than KVM_COALESCED_MMIO_PAGE_OFFSET.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f1339a7756b3..7e74ae4d99bb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -403,7 +403,7 @@ struct kvm {
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
 	refcount_t users_count;
-#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
+#ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
 	spinlock_t ring_lock;
 	struct list_head coalesced_zones;

commit ad6260da1e23cf937806e42c8490af3ff4530474
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Mar 27 14:30:40 2017 +0200

    KVM: x86: drop legacy device assignment
    
    Legacy device assignment has been deprecated since 4.2 (released
    1.5 years ago).  VFIO is better and everyone should have switched to it.
    If they haven't, this should convince them. :)
    
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d0250744507a..f1339a7756b3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -877,22 +877,6 @@ void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
-#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
-int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
-void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
-#else
-static inline int kvm_iommu_map_pages(struct kvm *kvm,
-				      struct kvm_memory_slot *slot)
-{
-	return 0;
-}
-
-static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
-					 struct kvm_memory_slot *slot)
-{
-}
-#endif
-
 /*
  * search_memslots() and __gfn_to_memslot() are here because they are
  * used in non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c.

commit 90db10434b163e46da413d34db8d0e77404cc645
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Mar 23 18:24:19 2017 +0100

    KVM: kvm_io_bus_unregister_dev() should never fail
    
    No caller currently checks the return value of
    kvm_io_bus_unregister_dev(). This is evil, as all callers silently go on
    freeing their device. A stale reference will remain in the io_bus,
    getting at least used again, when the iobus gets teared down on
    kvm_destroy_vm() - leading to use after free errors.
    
    There is nothing the callers could do, except retrying over and over
    again.
    
    So let's simply remove the bus altogether, print an error and make
    sure no one can access this broken bus again (returning -ENOMEM on any
    attempt to access it).
    
    Fixes: e93f8a0f821e ("KVM: convert io_bus to SRCU")
    Cc: stable@vger.kernel.org # 3.4+
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c14ad9809da..d0250744507a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -162,8 +162,8 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		    int len, void *val);
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
-int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
-			      struct kvm_io_device *dev);
+void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
+			       struct kvm_io_device *dev);
 struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 					 gpa_t addr);
 

commit e3736c3eb3a6f7c0966923b629c9f92b558aa9c7
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Mon Feb 20 13:06:21 2017 +0200

    kvm: convert kvm.users_count from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8d69d5150748..2c14ad9809da 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -26,6 +26,7 @@
 #include <linux/context_tracking.h>
 #include <linux/irqbypass.h>
 #include <linux/swait.h>
+#include <linux/refcount.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -401,7 +402,7 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
-	atomic_t users_count;
+	refcount_t users_count;
 #ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
 	spinlock_t ring_lock;

commit bd7e5b0899a429445cc6e3037c13f8b5ae3be903
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 3 21:18:52 2017 -0800

    KVM: x86: remove code for lazy FPU handling
    
    The FPU is always active now when running KVM.
    
    Reviewed-by: David Matlack <dmatlack@google.com>
    Reviewed-by: Bandan Das <bsd@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2db458ee94b0..8d69d5150748 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -221,7 +221,6 @@ struct kvm_vcpu {
 	struct mutex mutex;
 	struct kvm_run *run;
 
-	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
 	struct swait_queue_head wq;
 	struct pid *pid;

commit bbd6411513aa8ef3ea02abab61318daf87c1af1e
Author: Cao, Lei <Lei.Cao@stratus.com>
Date:   Fri Feb 3 20:04:35 2017 +0000

    KVM: Support vCPU-based gfn->hva cache
    
    Provide versions of struct gfn_to_hva_cache functions that
    take vcpu as a parameter instead of struct kvm.  The existing functions
    are not needed anymore, so delete them.  This allows dirty pages to
    be logged in the vcpu dirty ring, instead of the global dirty ring,
    for ring-based dirty memory tracking.
    
    Signed-off-by: Lei Cao <lei.cao@stratus.com>
    Message-Id: <CY1PR08MB19929BD2AC47A291FD680E83F04F0@CY1PR08MB1992.namprd08.prod.outlook.com>
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cda457bcedc1..2db458ee94b0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -641,18 +641,18 @@ int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
 			  unsigned long len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
-int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			   void *data, unsigned long len);
+int kvm_vcpu_read_guest_cached(struct kvm_vcpu *vcpu, struct gfn_to_hva_cache *ghc,
+			       void *data, unsigned long len);
 int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 		    unsigned long len);
-int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			   void *data, unsigned long len);
-int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			   void *data, int offset, unsigned long len);
-int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			      gpa_t gpa, unsigned long len);
+int kvm_vcpu_write_guest_cached(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
+				void *data, unsigned long len);
+int kvm_vcpu_write_guest_offset_cached(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
+				       void *data, int offset, unsigned long len);
+int kvm_vcpu_gfn_to_hva_cache_init(struct kvm_vcpu *v, struct gfn_to_hva_cache *ghc,
+				   gpa_t gpa, unsigned long len);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);

commit 9d93dc1c96ec446bef9c34a189ea24556f1af89a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 25 13:47:43 2017 +0000

    arm/arm64: KVM: Get rid of KVM_MEMSLOT_INCOHERENT
    
    KVM_MEMSLOT_INCOHERENT is not used anymore, as we've killed its
    only use in the arm/arm64 MMU code. Let's remove the last artifacts.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1c5190dab2c1..cda457bcedc1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -45,7 +45,6 @@
  * include/linux/kvm_h.
  */
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
-#define KVM_MEMSLOT_INCOHERENT	(1UL << 17)
 
 /* Two fragments for cross MMIO pages. */
 #define KVM_MAX_MMIO_FRAGMENTS	2

commit 93173b5bf2841da7e3a9b0cb1312ef5c87251524
Merge: 1c59e1edb13d f673b5b2a663
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 15:47:02 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small release, the most interesting stuff is x86 nested virt
      improvements.
    
      x86:
       - userspace can now hide nested VMX features from guests
       - nested VMX can now run Hyper-V in a guest
       - support for AVX512_4VNNIW and AVX512_FMAPS in KVM
       - infrastructure support for virtual Intel GPUs.
    
      PPC:
       - support for KVM guests on POWER9
       - improved support for interrupt polling
       - optimizations and cleanups.
    
      s390:
       - two small optimizations, more stuff is in flight and will be in
         4.11.
    
      ARM:
       - support for the GICv3 ITS on 32bit platforms"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (94 commits)
      arm64: KVM: pmu: Reset PMSELR_EL0.SEL to a sane value before entering the guest
      KVM: arm/arm64: timer: Check for properly initialized timer on init
      KVM: arm/arm64: vgic-v2: Limit ITARGETSR bits to number of VCPUs
      KVM: x86: Handle the kthread worker using the new API
      KVM: nVMX: invvpid handling improvements
      KVM: nVMX: check host CR3 on vmentry and vmexit
      KVM: nVMX: introduce nested_vmx_load_cr3 and call it on vmentry
      KVM: nVMX: propagate errors from prepare_vmcs02
      KVM: nVMX: fix CR3 load if L2 uses PAE paging and EPT
      KVM: nVMX: load GUEST_EFER after GUEST_CR0 during emulated VM-entry
      KVM: nVMX: generate MSR_IA32_CR{0,4}_FIXED1 from guest CPUID
      KVM: nVMX: fix checks on CR{0,4} during virtual VMX operation
      KVM: nVMX: support restore of VMX capability MSRs
      KVM: nVMX: generate non-true VMX MSRs based on true versions
      KVM: x86: Do not clear RFLAGS.TF when a singlestep trap occurs.
      KVM: x86: Add kvm_skip_emulated_instruction and use it.
      KVM: VMX: Move skip_emulated_instruction out of nested_vmx_check_vmcs12
      KVM: VMX: Reorder some skip_emulated_instruction calls
      KVM: x86: Add a return value to kvm_emulate_cpuid
      KVM: PPC: Book3S: Move prototypes for KVM functions into kvm_ppc.h
      ...

commit 518bacf5a569d111e256d58b9fbc8d7b80ec42ea
Merge: 535b2f73f6f6 064e6a8ba61a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 14:27:49 2016 -0800

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 FPU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - do a large round of simplifications after all CPUs do 'eager' FPU
         context switching in v4.9: remove CR0 twiddling, remove leftover
         eager/lazy bts, etc (Andy Lutomirski)
    
       - more FPU code simplifications: remove struct fpu::counter, clarify
         nomenclature, remove unnecessary arguments/functions and better
         structure the code (Rik van Riel)"
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/fpu: Remove clts()
      x86/fpu: Remove stts()
      x86/fpu: Handle #NM without FPU emulation as an error
      x86/fpu, lguest: Remove CR0.TS support
      x86/fpu, kvm: Remove host CR0.TS manipulation
      x86/fpu: Remove irq_ts_save() and irq_ts_restore()
      x86/fpu: Stop saving and restoring CR0.TS in fpu__init_check_bugs()
      x86/fpu: Get rid of two redundant clts() calls
      x86/fpu: Finish excising 'eagerfpu'
      x86/fpu: Split old_fpu & new_fpu handling into separate functions
      x86/fpu: Remove 'cpu' argument from __cpu_invalidate_fpregs_state()
      x86/fpu: Split old & new FPU code paths
      x86/fpu: Remove __fpregs_(de)activate()
      x86/fpu: Rename lazy restore functions to "register state valid"
      x86/fpu, kvm: Remove KVM vcpu->fpu_counter
      x86/fpu: Remove struct fpu::counter
      x86/fpu: Remove use_eager_fpu()
      x86/fpu: Remove the XFEATURE_MASK_EAGER/LAZY distinction
      x86/fpu: Hard-disable lazy FPU mode
      x86/crypto, x86/fpu: Remove X86_FEATURE_EAGER_FPU #ifdef from the crc32c code

commit ffcb09f27f46ea21305c7846de1fd3b76e4e6a6f
Merge: bf65014d0b89 6ccad8cea5bc
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Nov 29 14:26:55 2016 +0100

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    PPC KVM update for 4.10:
    
     * Support for KVM guests on POWER9 using the hashed page table MMU.
     * Updates and improvements to the halt-polling support on PPC, from
       Suraj Jitindar Singh.
     * An optimization to speed up emulated MMIO, from Yongji Xie.
     * Various other minor cleanups.

commit ec76d819d27040e418801d1a57bd3bdfde51019e
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Oct 14 11:53:19 2016 +1100

    KVM: Export kvm module parameter variables
    
    The kvm module has the parameters halt_poll_ns, halt_poll_ns_grow, and
    halt_poll_ns_shrink. Halt polling was recently added to the powerpc kvm-hv
    module and these parameters were essentially duplicated for that. There is
    no benefit to this duplication and it can lead to confusion when trying to
    tune halt polling.
    
    Thus move the definition of these variables to kvm_host.h and export them.
    This will allow the kvm-hv module to use the same module parameters by
    accessing these variables, which will be implemented in the next patch,
    meaning that they will no longer be duplicated.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01c0b9cc3915..29b500a857d1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1107,6 +1107,10 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 
 extern bool kvm_rebooting;
 
+extern unsigned int halt_poll_ns;
+extern unsigned int halt_poll_ns_grow;
+extern unsigned int halt_poll_ns_shrink;
+
 struct kvm_device {
 	struct kvm_device_ops *ops;
 	struct kvm *kvm;

commit ae0f5499511e5b1723792c848e44d661d0d4e22f
Author: Bandan Das <bsd@redhat.com>
Date:   Tue Nov 15 01:36:18 2016 -0500

    kvm: x86: don't print warning messages for unimplemented msrs
    
    Change unimplemented msrs messages to use pr_debug.
    If CONFIG_DYNAMIC_DEBUG is set, then these messages can be
    enabled at run time or else -DDEBUG can be used at compile
    time to enable them. These messages will still be printed if
    ignore_msrs=1.
    
    Signed-off-by: Bandan Das <bsd@redhat.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01c0b9cc3915..274bf343cbd0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -439,6 +439,9 @@ struct kvm {
 	pr_info("kvm [%i]: " fmt, task_pid_nr(current), ## __VA_ARGS__)
 #define kvm_debug(fmt, ...) \
 	pr_debug("kvm [%i]: " fmt, task_pid_nr(current), ## __VA_ARGS__)
+#define kvm_debug_ratelimited(fmt, ...) \
+	pr_debug_ratelimited("kvm [%i]: " fmt, task_pid_nr(current), \
+			     ## __VA_ARGS__)
 #define kvm_pr_unimpl(fmt, ...) \
 	pr_err_ratelimited("kvm [%i]: " fmt, \
 			   task_tgid_nr(current), ## __VA_ARGS__)
@@ -450,6 +453,9 @@ struct kvm {
 
 #define vcpu_debug(vcpu, fmt, ...)					\
 	kvm_debug("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
+#define vcpu_debug_ratelimited(vcpu, fmt, ...)				\
+	kvm_debug_ratelimited("vcpu%i " fmt, (vcpu)->vcpu_id,           \
+			      ## __VA_ARGS__)
 #define vcpu_err(vcpu, fmt, ...)					\
 	kvm_err("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 

commit 4ec6e863625625a54f527464ab91ce1a1cb16c42
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Wed Nov 2 05:08:34 2016 -0400

    kvm: Introduce kvm_write_guest_offset_cached()
    
    It allows us to update some status or field of a structure partially.
    
    We can also save a kvm_read_guest_cached() call if we just update one
    fild of the struct regardless of its current value.
    
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: boqun.feng@gmail.com
    Cc: borntraeger@de.ibm.com
    Cc: bsingharora@gmail.com
    Cc: dave@stgolabs.net
    Cc: jgross@suse.com
    Cc: kernellwp@gmail.com
    Cc: konrad.wilk@oracle.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: paulmck@linux.vnet.ibm.com
    Cc: paulus@samba.org
    Cc: rkrcmar@redhat.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: will.deacon@arm.com
    Cc: xen-devel-request@lists.xenproject.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1478077718-37424-8-git-send-email-xinhui.pan@linux.vnet.ibm.com
    [ Typo fixes. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01c0b9cc3915..6f0023797b33 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -645,6 +645,8 @@ int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 		    unsigned long len);
 int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len);
+int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, int offset, unsigned long len);
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);

commit 4d69f155d58d0f75c5404ea502178b1943a04755
Merge: c474e50711aa 1001354ca341
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Oct 16 13:04:34 2016 +0200

    Merge tag 'v4.9-rc1' into x86/fpu, to resolve conflict
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3d42de25d290fdfe604835d1b389845b8cba5bff
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Oct 4 20:34:35 2016 -0400

    x86/fpu, kvm: Remove KVM vcpu->fpu_counter
    
    With the removal of the lazy FPU code, this field is no longer used.
    Get rid of it.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: pbonzini@redhat.com
    Link: http://lkml.kernel.org/r/1475627678-20788-7-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9c28b4d4c90b..4e6905cd1e8e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -224,7 +224,6 @@ struct kvm_vcpu {
 
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
-	unsigned char fpu_counter;
 	struct swait_queue_head wq;
 	struct pid *pid;
 	int sigset_active;

commit 45b5939e50746b92fd4cb47c02524f79ba8fabe6
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Fri Sep 16 10:27:35 2016 -0400

    kvm: create per-vcpu dirs in debugfs
    
    This commit adds the ability for archs to export
    per-vcpu information via a new per-vcpu dir in
    the VM's debugfs directory.
    
    If kvm_arch_has_vcpu_debugfs() returns true, then KVM
    will create a vcpu dir for each vCPU in the VM's
    debugfs directory. Then kvm_arch_create_vcpu_debugfs()
    is responsible for populating each vcpu directory
    with arch specific entries.
    
    The per-vcpu path in debugfs will look like:
    
    /sys/kernel/debug/kvm/29162-10/vcpu0
    /sys/kernel/debug/kvm/29162-10/vcpu1
    
    This is all arch specific for now because the only
    user of this interface (x86) wants to export x86-specific
    per-vcpu information to user-space.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5486ff9aa71e..01c0b9cc3915 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -265,6 +265,7 @@ struct kvm_vcpu {
 #endif
 	bool preempted;
 	struct kvm_vcpu_arch arch;
+	struct dentry *debugfs_dentry;
 };
 
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)

commit 235539b48a2357da28f52d66d04bec04f3dcb9dd
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Sep 7 14:47:23 2016 -0400

    kvm: add stubs for arch specific debugfs support
    
    Two stubs are added:
    
     o kvm_arch_has_vcpu_debugfs(): must return true if the arch
       supports creating debugfs entries in the vcpu debugfs dir
       (which will be implemented by the next commit)
    
     o kvm_arch_create_vcpu_debugfs(): code that creates debugfs
       entries in the vcpu debugfs dir
    
    For x86, this commit introduces a new file to avoid growing
    arch/x86/kvm/x86.c even more.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9c28b4d4c90b..5486ff9aa71e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -749,6 +749,9 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
+bool kvm_arch_has_vcpu_debugfs(void);
+int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu);
+
 int kvm_arch_hardware_enable(void);
 void kvm_arch_hardware_disable(void);
 int kvm_arch_hardware_setup(void);

commit a28ebea2adc4a2bef5989a5a181ec238f59fbcad
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Aug 9 19:13:01 2016 +0200

    KVM: Protect device ops->create and list_add with kvm->lock
    
    KVM devices were manipulating list data structures without any form of
    synchronization, and some implementations of the create operations also
    suffered from a lack of synchronization.
    
    Now when we've split the xics create operation into create and init, we
    can hold the kvm->lock mutex while calling the create operation and when
    manipulating the devices list.
    
    The error path in the generic code gets slightly ugly because we have to
    take the mutex again and delete the device from the list, but holding
    the mutex during anon_inode_getfd or releasing/locking the mutex in the
    common non-error path seemed wrong.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d3c9b82812c3..9c28b4d4c90b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1113,6 +1113,12 @@ struct kvm_device {
 /* create, destroy, and name are mandatory */
 struct kvm_device_ops {
 	const char *name;
+
+	/*
+	 * create is called holding kvm->lock and any operations not suitable
+	 * to do while holding the lock should be deferred to init (see
+	 * below).
+	 */
 	int (*create)(struct kvm_device *dev, u32 type);
 
 	/*

commit 023e9fddc3616b005c3753fc1bb6526388cd7a30
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Aug 9 19:13:00 2016 +0200

    KVM: PPC: Move xics_debugfs_init out of create
    
    As we are about to hold the kvm->lock during the create operation on KVM
    devices, we should move the call to xics_debugfs_init into its own
    function, since holding a mutex over extended amounts of time might not
    be a good idea.
    
    Introduce an init operation on the kvm_device_ops struct which cannot
    fail and call this, if configured, after the device has been created.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 01e908ac4a39..d3c9b82812c3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1115,6 +1115,12 @@ struct kvm_device_ops {
 	const char *name;
 	int (*create)(struct kvm_device *dev, u32 type);
 
+	/*
+	 * init is called after create if create is successful and is called
+	 * outside of holding kvm->lock.
+	 */
+	void (*init)(struct kvm_device *dev);
+
 	/*
 	 * Destroy is responsible for freeing dev.
 	 *

commit 6f49b2f3414622d3e41135a65dac98968956662b
Merge: abe9efa79be0 89581f06b2bc
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 4 13:59:56 2016 +0200

    Merge tag 'kvm-arm-for-4.8-take2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM Changes for v4.8 - Take 2
    
    Includes GSI routing support to go along with the new VGIC and a small fix that
    has been cooking in -next for a while.

commit 912902ce78b0d48f717f9128e61fb9bffbd65f86
Merge: 61f5dea17965 3a88bded2035
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Jul 22 20:27:26 2016 +0200

    Merge tag 'kvm-arm-for-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into next
    
    KVM/ARM changes for Linux 4.8
    
    - GICv3 ITS emulation
    - Simpler idmap management that fixes potential TLB conflicts
    - Honor the kernel protection in HYP mode
    - Removal of the old vgic implementation

commit 995a0ee9809b6948bb7bfea77f129fe1d5157cc1
Author: Eric Auger <eric.auger@redhat.com>
Date:   Fri Jul 22 16:20:42 2016 +0000

    KVM: arm/arm64: Enable MSI routing
    
    Up to now, only irqchip routing entries could be set. This patch
    adds the capability to insert MSI routing entries.
    
    For ARM64, let's also increase KVM_MAX_IRQ_ROUTES to 4096: this
    include SPI irqchip routes plus MSI routes. In the future this
    might be extended.
    
    Signed-off-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a7eb5c48251e..a318c3b21608 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1048,6 +1048,8 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 
 #ifdef CONFIG_S390
 #define KVM_MAX_IRQ_ROUTES 4096 //FIXME: we can have more than that...
+#elif defined(CONFIG_ARM64)
+#define KVM_MAX_IRQ_ROUTES 4096
 #else
 #define KVM_MAX_IRQ_ROUTES 1024
 #endif

commit d9565a7399d665fa7313122504778cb3d5ef3e19
Author: Eric Auger <eric.auger@redhat.com>
Date:   Fri Jul 22 16:20:40 2016 +0000

    KVM: Move kvm_setup_default/empty_irq_routing declaration in arch specific header
    
    kvm_setup_default_irq_routing and kvm_setup_empty_irq_routing are
    not used by generic code. So let's move the declarations in x86 irq.h
    header instead of kvm_host.h.
    
    Signed-off-by: Eric Auger <eric.auger@redhat.com>
    Suggested-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a15828fe845c..a7eb5c48251e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1052,8 +1052,6 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 #define KVM_MAX_IRQ_ROUTES 1024
 #endif
 
-int kvm_setup_default_irq_routing(struct kvm *kvm);
-int kvm_setup_empty_irq_routing(struct kvm *kvm);
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,

commit 0455e72c9ae9b7e9589f2cc5ba5bc7804be83342
Author: Eric Auger <eric.auger@redhat.com>
Date:   Fri Jul 22 16:20:38 2016 +0000

    KVM: Add devid in kvm_kernel_irq_routing_entry
    
    Enhance kvm_kernel_irq_routing_entry to transport the device id
    field, devid. A new flags field makes possible to indicate the
    devid is valid. Those additions are used for ARM GICv3 ITS MSI
    injection. The original struct msi_msg msi field is replaced by
    a new custom structure that embeds the new fields.
    
    Signed-off-by: Eric Auger <eric.auger@redhat.com>
    Suggested-by: Radim Krčmář <rkrcmar@redhat.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 614a98137c5f..a15828fe845c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -317,7 +317,13 @@ struct kvm_kernel_irq_routing_entry {
 			unsigned irqchip;
 			unsigned pin;
 		} irqchip;
-		struct msi_msg msi;
+		struct {
+			u32 address_lo;
+			u32 address_hi;
+			u32 data;
+			u32 flags;
+			u32 devid;
+		} msi;
 		struct kvm_s390_adapter_int adapter;
 		struct kvm_hv_sint hv_sint;
 	};

commit 8a39d00670f0792c1186e442e1dd28fe0326f2ee
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Jul 15 12:43:26 2016 +0100

    KVM: kvm_io_bus: Add kvm_io_bus_get_dev() call
    
    The kvm_io_bus framework is a nice place of holding information about
    various MMIO regions for kernel emulated devices.
    Add a call to retrieve the kvm_io_device structure which is associated
    with a certain MMIO address. This avoids to duplicate kvm_io_bus'
    knowledge of MMIO regions without having to fake MMIO calls if a user
    needs the device a certain MMIO address belongs to.
    This will be used by the ITS emulation to get the associated ITS device
    when someone triggers an MSI via an ioctl from userspace.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Tested-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0640ee92b978..614a98137c5f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -164,6 +164,8 @@ int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 			      struct kvm_io_device *dev);
+struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,
+					 gpa_t addr);
 
 #ifdef CONFIG_KVM_ASYNC_PF
 struct kvm_async_pf {

commit c63cf538eb4bf6a5ffd3750366d8d56f023645a5
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Tue Jul 12 22:09:26 2016 +0200

    KVM: pass struct kvm to kvm_set_routing_entry
    
    Arch-specific code will use it.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 66b2f6159aad..60d339faa94c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1011,7 +1011,8 @@ int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,
 			unsigned flags);
-int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
+int kvm_set_routing_entry(struct kvm *kvm,
+			  struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue);
 void kvm_free_irq_routing(struct kvm *kvm);
 

commit 6edaa5307f3f51e4e56dc4c63f68a69d88c6ddf5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 15 15:18:26 2016 +0200

    KVM: remove kvm_guest_enter/exit wrappers
    
    Use the functions from context_tracking.h directly.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ffff40522688..66b2f6159aad 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -875,28 +875,6 @@ static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 }
 #endif
 
-/* must be called with irqs disabled */
-static inline void __kvm_guest_enter(void)
-{
-	guest_enter_irqoff();
-}
-
-/* must be called with irqs disabled */
-static inline void __kvm_guest_exit(void)
-{
-	guest_exit_irqoff();
-}
-
-static inline void kvm_guest_enter(void)
-{
-	guest_enter();
-}
-
-static inline void kvm_guest_exit(void)
-{
-	guest_exit();
-}
-
 /*
  * search_memslots() and __gfn_to_memslot() are here because they are
  * used in non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c.

commit ebaac1736245e78109cd47d453a86a18dcfc94b8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 15 15:09:28 2016 +0200

    context_tracking: move rcu_virt_note_context_switch out of kvm_host.h
    
    Make kvm_guest_{enter,exit} and __kvm_guest_{enter,exit} trivial wrappers
    around the code in context_tracking.h.  Name the context_tracking.h functions
    consistently with what those for kernel<->user switch.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0640ee92b978..ffff40522688 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -878,40 +878,23 @@ static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 /* must be called with irqs disabled */
 static inline void __kvm_guest_enter(void)
 {
-	guest_enter();
-	/* KVM does not hold any references to rcu protected data when it
-	 * switches CPU into a guest mode. In fact switching to a guest mode
-	 * is very similar to exiting to userspace from rcu point of view. In
-	 * addition CPU may stay in a guest mode for quite a long time (up to
-	 * one time slice). Lets treat guest mode as quiescent state, just like
-	 * we do with user-mode execution.
-	 */
-	if (!context_tracking_cpu_is_enabled())
-		rcu_virt_note_context_switch(smp_processor_id());
+	guest_enter_irqoff();
 }
 
 /* must be called with irqs disabled */
 static inline void __kvm_guest_exit(void)
 {
-	guest_exit();
+	guest_exit_irqoff();
 }
 
 static inline void kvm_guest_enter(void)
 {
-	unsigned long flags;
-
-	local_irq_save(flags);
-	__kvm_guest_enter();
-	local_irq_restore(flags);
+	guest_enter();
 }
 
 static inline void kvm_guest_exit(void)
 {
-	unsigned long flags;
-
-	local_irq_save(flags);
-	__kvm_guest_exit();
-	local_irq_restore(flags);
+	guest_exit();
 }
 
 /*

commit 557abc40d121358883d2da8bc8bf976d6e8ec332
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 13 14:50:04 2016 +0200

    KVM: remove kvm_vcpu_compatible
    
    The new created_vcpus field makes it possible to avoid the race between
    irqchip and VCPU creation in a much nicer way; just check under kvm->lock
    whether a VCPU has already been created.
    
    We can then remove KVM_APIC_ARCHITECTURE too, because at this point the
    symbol is only governing the default definition of kvm_vcpu_compatible.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 63c6ab30bc81..0640ee92b978 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1105,12 +1105,6 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
-#ifdef CONFIG_KVM_APIC_ARCHITECTURE
-bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu);
-#else
-static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
-#endif
-
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	/*

commit 6c7caebc26c5f0b618f0ef6b851e9f5f27c3812f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 13 14:48:25 2016 +0200

    KVM: introduce kvm->created_vcpus
    
    The race between creating the irqchip and the first VCPU is
    currently fixed by checking the presence of an irqchip before
    updating kvm->online_vcpus, and undoing the whole VCPU creation
    if someone created the irqchip in the meanwhile.
    
    Instead, introduce a new field in struct kvm that will count VCPUs
    under a mutex, without the atomic access and memory ordering that we
    need elsewhere to protect the vcpus array.  This also plugs the race
    and is more easily applicable in all similar circumstances.
    
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1c9c973a7dd9..63c6ab30bc81 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -371,7 +371,15 @@ struct kvm {
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
+
+	/*
+	 * created_vcpus is protected by kvm->lock, and is incremented
+	 * at the beginning of KVM_CREATE_VCPU.  online_vcpus is only
+	 * incremented after storing the kvm_vcpu pointer in vcpus,
+	 * and is accessed atomically.
+	 */
 	atomic_t online_vcpus;
+	int created_vcpus;
 	int last_boosted_vcpu;
 	struct list_head vm_list;
 	struct mutex lock;

commit 536a6f88c49dd739961ffd53774775afed852c83
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Wed May 18 13:26:23 2016 +0200

    KVM: Create debugfs dir and stat files for each VM
    
    This patch adds a kvm debugfs subdirectory for each VM, which is named
    after its pid and file descriptor. The directories contain the same
    kind of files that are already in the kvm debugfs directory, but the
    data exported through them is now VM specific.
    
    This makes the debugfs kvm data a convenient alternative to the
    tracepoints which already have per VM data. The debugfs data is easy
    to read and low overhead.
    
    CC: Dan Carpenter <dan.carpenter@oracle.com> [includes fixes by Dan Carpenter]
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b1fa8f11c95b..1c9c973a7dd9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -412,6 +412,8 @@ struct kvm {
 #endif
 	long tlbs_dirty;
 	struct list_head devices;
+	struct dentry *debugfs_dentry;
+	struct kvm_stat_data **debugfs_stat_data;
 };
 
 #define kvm_err(fmt, ...) \
@@ -991,6 +993,11 @@ enum kvm_stat_kind {
 	KVM_STAT_VCPU,
 };
 
+struct kvm_stat_data {
+	int offset;
+	struct kvm *kvm;
+};
+
 struct kvm_stats_debugfs_item {
 	const char *name;
 	int offset;

commit dd1a4cc1fbdf516bb38ca31b65c76e720d414d0d
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed May 4 14:09:44 2016 -0500

    KVM: split kvm_vcpu_wake_up from kvm_vcpu_kick
    
    AVIC has a use for kvm_vcpu_wake_up.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Tested-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bbcd921d7cb0..b1fa8f11c95b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -657,6 +657,7 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
+void kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);

commit 3491caf2755e9f312666712510d80b00c81ff247
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 13 12:16:35 2016 +0200

    KVM: halt_polling: provide a way to qualify wakeups during poll
    
    Some wakeups should not be considered a sucessful poll. For example on
    s390 I/O interrupts are usually floating, which means that _ALL_ CPUs
    would be considered runnable - letting all vCPUs poll all the time for
    transactional like workload, even if one vCPU would be enough.
    This can result in huge CPU usage for large guests.
    This patch lets architectures provide a way to qualify wakeups if they
    should be considered a good/bad wakeups in regard to polls.
    
    For s390 the implementation will fence of halt polling for anything but
    known good, single vCPU events. The s390 implementation for floating
    interrupts does a wakeup for one vCPU, but the interrupt will be delivered
    by whatever CPU checks first for a pending interrupt. We prefer the
    woken up CPU by marking the poll of this CPU as "good" poll.
    This code will also mark several other wakeup reasons like IPI or
    expired timers as "good". This will of course also mark some events as
    not sucessful. As  KVM on z runs always as a 2nd level hypervisor,
    we prefer to not poll, unless we are really sure, though.
    
    This patch successfully limits the CPU usage for cases like uperf 1byte
    transactional ping pong workload or wakeup heavy workload like OLTP
    while still providing a proper speedup.
    
    This also introduced a new vcpu stat "halt_poll_no_tuning" that marks
    wakeups that are considered not good for polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Radim Krčmář <rkrcmar@redhat.com> (for an earlier version)
    Cc: David Matlack <dmatlack@google.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    [Rename config symbol. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 92a0229044fb..bbcd921d7cb0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -229,6 +229,7 @@ struct kvm_vcpu {
 	sigset_t sigset;
 	struct kvm_vcpu_stat stat;
 	unsigned int halt_poll_ns;
+	bool valid_wakeup;
 
 #ifdef CONFIG_HAS_IOMEM
 	int mmio_needed;
@@ -1196,4 +1197,18 @@ int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				  uint32_t guest_irq, bool set);
 #endif /* CONFIG_HAVE_KVM_IRQ_BYPASS */
 
+#ifdef CONFIG_HAVE_KVM_INVALID_WAKEUPS
+/* If we wakeup during the poll time, was it a sucessful poll? */
+static inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)
+{
+	return vcpu->valid_wakeup;
+}
+
+#else
+static inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)
+{
+	return true;
+}
+#endif /* CONFIG_HAVE_KVM_INVALID_WAKEUPS */
+
 #endif

commit 14717e2031862d9aa2512b24a7df42cf68a977ec
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Thu May 5 11:58:35 2016 -0600

    kvm: Conditionally register IRQ bypass consumer
    
    If we don't support a mechanism for bypassing IRQs, don't register as
    a consumer.  This eliminates meaningless dev_info()s when the connect
    fails between producer and consumer, such as on AMD systems where
    kvm_x86_ops->update_pi_irte is not implemented
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 352889d6e322..92a0229044fb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1185,6 +1185,7 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
+bool kvm_arch_has_irq_bypass(void);
 int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *,
 			   struct irq_bypass_producer *);
 void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *,

commit 0b1b1dfd52a67f4f09a18cb82337199bc90ad7fb
Author: Greg Kurz <gkurz@linux.vnet.ibm.com>
Date:   Mon May 9 18:13:37 2016 +0200

    kvm: introduce KVM_MAX_VCPU_ID
    
    The KVM_MAX_VCPUS define provides the maximum number of vCPUs per guest, and
    also the upper limit for vCPU ids. This is okay for all archs except PowerPC
    which can have higher ids, depending on the cpu/core/thread topology. In the
    worst case (single threaded guest, host with 8 threads per core), it limits
    the maximum number of vCPUS to KVM_MAX_VCPUS / 8.
    
    This patch separates the vCPU numbering from the total number of vCPUs, with
    the introduction of KVM_MAX_VCPU_ID, as the maximal valid value for vCPU ids
    plus one.
    
    The corresponding KVM_CAP_MAX_VCPU_ID allows userspace to validate vCPU ids
    before passing them to KVM_CREATE_VCPU.
    
    This patch only implements KVM_MAX_VCPU_ID with a specific value for PowerPC.
    Other archs continue to return KVM_MAX_VCPUS instead.
    
    Suggested-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0a0e00d9c5da..352889d6e322 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -35,6 +35,10 @@
 
 #include <asm/kvm_host.h>
 
+#ifndef KVM_MAX_VCPU_ID
+#define KVM_MAX_VCPU_ID KVM_MAX_VCPUS
+#endif
+
 /*
  * The bit 16 ~ bit 31 of kvm_memory_region::flags are internally used
  * in kvm, other bits are visible for userspace which are defined in

commit 9b9e3fc4d5a31f6050508f2404369beac4356867
Author: Greg Kurz <gkurz@linux.vnet.ibm.com>
Date:   Mon May 9 18:11:54 2016 +0200

    KVM: remove NULL return path for vcpu ids >= KVM_MAX_VCPUS
    
    Commit c896939f7cff ("KVM: use heuristic for fast VCPU lookup by id") added
    a return path that prevents vcpu ids to exceed KVM_MAX_VCPUS. This is a
    problem for powerpc where vcpu ids can grow up to 8*KVM_MAX_VCPUS.
    
    This patch simply reverses the logic so that we only try fast path if the
    vcpu id can be tried as an index in kvm->vcpus[]. The slow path is not
    affected by the change.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ad40d44784c7..0a0e00d9c5da 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -447,12 +447,13 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 {
-	struct kvm_vcpu *vcpu;
+	struct kvm_vcpu *vcpu = NULL;
 	int i;
 
-	if (id < 0 || id >= KVM_MAX_VCPUS)
+	if (id < 0)
 		return NULL;
-	vcpu = kvm_get_vcpu(kvm, id);
+	if (id < KVM_MAX_VCPUS)
+		vcpu = kvm_get_vcpu(kvm, id);
 	if (vcpu && vcpu->vcpu_id == id)
 		return vcpu;
 	kvm_for_each_vcpu(i, vcpu, kvm)

commit 2e4682ba2ed79d8082b78d292b3b80f54d970b7a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 10 16:30:22 2016 +0100

    KVM: add missing memory barrier in kvm_{make,check}_request
    
    kvm_make_request and kvm_check_request imply a producer-consumer
    relationship; add implicit memory barriers to them.  There was indeed
    already a place that was adding an explicit smp_mb() to order between
    kvm_check_request and the processing of the request.  That memory
    barrier can be removed (as an added benefit, kvm_check_request can use
    smp_mb__after_atomic which is free on x86).
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5276fe0916fc..ad40d44784c7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1091,6 +1091,11 @@ static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
 
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
+	/*
+	 * Ensure the rest of the request is published to kvm_check_request's
+	 * caller.  Paired with the smp_mb__after_atomic in kvm_check_request.
+	 */
+	smp_wmb();
 	set_bit(req, &vcpu->requests);
 }
 
@@ -1098,6 +1103,12 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 {
 	if (test_bit(req, &vcpu->requests)) {
 		clear_bit(req, &vcpu->requests);
+
+		/*
+		 * Ensure the rest of the request is visible to kvm_check_request's
+		 * caller.  Paired with the smp_wmb in kvm_make_request.
+		 */
+		smp_mb__after_atomic();
 		return true;
 	} else {
 		return false;

commit 8577370fb0cbe88266b7583d8d3b9f43ced077a0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Feb 19 09:46:39 2016 +0100

    KVM: Use simple waitqueue for vcpu->wq
    
    The problem:
    
    On -rt, an emulated LAPIC timer instances has the following path:
    
    1) hard interrupt
    2) ksoftirqd is scheduled
    3) ksoftirqd wakes up vcpu thread
    4) vcpu thread is scheduled
    
    This extra context switch introduces unnecessary latency in the
    LAPIC path for a KVM guest.
    
    The solution:
    
    Allow waking up vcpu thread from hardirq context,
    thus avoiding the need for ksoftirqd to be scheduled.
    
    Normal waitqueues make use of spinlocks, which on -RT
    are sleepable locks. Therefore, waking up a waitqueue
    waiter involves locking a sleeping lock, which
    is not allowed from hard interrupt context.
    
    cyclictest command line:
    
    This patch reduces the average latency in my tests from 14us to 11us.
    
    Daniel writes:
    Paolo asked for numbers from kvm-unit-tests/tscdeadline_latency
    benchmark on mainline. The test was run 1000 times on
    tip/sched/core 4.4.0-rc8-01134-g0905f04:
    
      ./x86-run x86/tscdeadline_latency.flat -cpu host
    
    with idle=poll.
    
    The test seems not to deliver really stable numbers though most of
    them are smaller. Paolo write:
    
    "Anything above ~10000 cycles means that the host went to C1 or
    lower---the number means more or less nothing in that case.
    
    The mean shows an improvement indeed."
    
    Before:
    
                   min             max         mean           std
    count  1000.000000     1000.000000  1000.000000   1000.000000
    mean   5162.596000  2019270.084000  5824.491541  20681.645558
    std      75.431231   622607.723969    89.575700   6492.272062
    min    4466.000000    23928.000000  5537.926500    585.864966
    25%    5163.000000  1613252.750000  5790.132275  16683.745433
    50%    5175.000000  2281919.000000  5834.654000  23151.990026
    75%    5190.000000  2382865.750000  5861.412950  24148.206168
    max    5228.000000  4175158.000000  6254.827300  46481.048691
    
    After
                   min            max         mean           std
    count  1000.000000     1000.00000  1000.000000   1000.000000
    mean   5143.511000  2076886.10300  5813.312474  21207.357565
    std      77.668322   610413.09583    86.541500   6331.915127
    min    4427.000000    25103.00000  5529.756600    559.187707
    25%    5148.000000  1691272.75000  5784.889825  17473.518244
    50%    5160.000000  2308328.50000  5832.025000  23464.837068
    75%    5172.000000  2393037.75000  5853.177675  24223.969976
    max    5222.000000  3922458.00000  6186.720500  42520.379830
    
    [Patch was originaly based on the swait implementation found in the -rt
     tree. Daniel ported it to mainline's version and gathered the
     benchmark numbers for tscdeadline_latency test.]
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-rt-users@vger.kernel.org
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1455871601-27484-4-git-send-email-wagi@monom.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 861f690aa791..5276fe0916fc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/irqflags.h>
 #include <linux/context_tracking.h>
 #include <linux/irqbypass.h>
+#include <linux/swait.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -218,7 +219,7 @@ struct kvm_vcpu {
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
 	unsigned char fpu_counter;
-	wait_queue_head_t wq;
+	struct swait_queue_head wq;
 	struct pid *pid;
 	int sigset_active;
 	sigset_t sigset;
@@ -782,7 +783,7 @@ static inline bool kvm_arch_has_assigned_device(struct kvm *kvm)
 }
 #endif
 
-static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+static inline struct swait_queue_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_WQP
 	return vcpu->arch.wqp;

commit ba049e93aef7e8c571567088b1b73f4f5b99272a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:11 2016 -0800

    kvm: rename pfn_t to kvm_pfn_t
    
    To date, we have implemented two I/O usage models for persistent memory,
    PMEM (a persistent "ram disk") and DAX (mmap persistent memory into
    userspace).  This series adds a third, DAX-GUP, that allows DAX mappings
    to be the target of direct-i/o.  It allows userspace to coordinate
    DMA/RDMA from/to persistent memory.
    
    The implementation leverages the ZONE_DEVICE mm-zone that went into
    4.3-rc1 (also discussed at kernel summit) to flag pages that are owned
    and dynamically mapped by a device driver.  The pmem driver, after
    mapping a persistent memory range into the system memmap via
    devm_memremap_pages(), arranges for DAX to distinguish pfn-only versus
    page-backed pmem-pfns via flags in the new pfn_t type.
    
    The DAX code, upon seeing a PFN_DEV+PFN_MAP flagged pfn, flags the
    resulting pte(s) inserted into the process page tables with a new
    _PAGE_DEVMAP flag.  Later, when get_user_pages() is walking ptes it keys
    off _PAGE_DEVMAP to pin the device hosting the page range active.
    Finally, get_page() and put_page() are modified to take references
    against the device driver established page mapping.
    
    Finally, this need for "struct page" for persistent memory requires
    memory capacity to store the memmap array.  Given the memmap array for a
    large pool of persistent may exhaust available DRAM introduce a
    mechanism to allocate the memmap from persistent memory.  The new
    "struct vmem_altmap *" parameter to devm_memremap_pages() enables
    arch_add_memory() to use reserved pmem capacity rather than the page
    allocator.
    
    This patch (of 18):
    
    The core has developed a need for a "pfn_t" type [1].  Move the existing
    pfn_t in KVM to kvm_pfn_t [2].
    
    [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html
    [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f707f74055c3..861f690aa791 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -66,7 +66,7 @@
  * error pfns indicate that the gfn is in slot but faild to
  * translate it to pfn on host.
  */
-static inline bool is_error_pfn(pfn_t pfn)
+static inline bool is_error_pfn(kvm_pfn_t pfn)
 {
 	return !!(pfn & KVM_PFN_ERR_MASK);
 }
@@ -76,13 +76,13 @@ static inline bool is_error_pfn(pfn_t pfn)
  * translated to pfn - it is not in slot or failed to
  * translate it to pfn.
  */
-static inline bool is_error_noslot_pfn(pfn_t pfn)
+static inline bool is_error_noslot_pfn(kvm_pfn_t pfn)
 {
 	return !!(pfn & KVM_PFN_ERR_NOSLOT_MASK);
 }
 
 /* noslot pfn indicates that the gfn is not in slot. */
-static inline bool is_noslot_pfn(pfn_t pfn)
+static inline bool is_noslot_pfn(kvm_pfn_t pfn)
 {
 	return pfn == KVM_PFN_NOSLOT;
 }
@@ -591,19 +591,20 @@ void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
-pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
-pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
-pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
-pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
-			   bool *async, bool write_fault, bool *writable);
+kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool *async, bool write_fault,
+			       bool *writable);
 
-void kvm_release_pfn_clean(pfn_t pfn);
-void kvm_set_pfn_dirty(pfn_t pfn);
-void kvm_set_pfn_accessed(pfn_t pfn);
-void kvm_get_pfn(pfn_t pfn);
+void kvm_release_pfn_clean(kvm_pfn_t pfn);
+void kvm_set_pfn_dirty(kvm_pfn_t pfn);
+void kvm_set_pfn_accessed(kvm_pfn_t pfn);
+void kvm_get_pfn(kvm_pfn_t pfn);
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
@@ -629,8 +630,8 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
-pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
-pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
@@ -811,7 +812,7 @@ void kvm_arch_sync_events(struct kvm *kvm);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
-bool kvm_is_reserved_pfn(pfn_t pfn);
+bool kvm_is_reserved_pfn(kvm_pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;
@@ -965,7 +966,7 @@ static inline gfn_t gpa_to_gfn(gpa_t gpa)
 	return (gfn_t)(gpa >> PAGE_SHIFT);
 }
 
-static inline hpa_t pfn_to_hpa(pfn_t pfn)
+static inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)
 {
 	return (hpa_t)pfn << PAGE_SHIFT;
 }

commit 2860c4b1678646c99f5f1d77d026cd12ffd8a3a9
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 15:05:10 2016 +0100

    KVM: move architecture-dependent requests to arch/
    
    Since the numbers now overlap, it makes sense to enumerate
    them in asm/kvm_host.h rather than linux/kvm_host.h.  Functions
    that refer to architecture-specific requests are also moved
    to arch/.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b0ec0f778192..f707f74055c3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -111,46 +111,14 @@ static inline bool is_error_page(struct page *page)
 }
 
 /*
- * vcpu->requests bit members
+ * Architecture-independent vcpu->requests bit members
+ * Bits 4-7 are reserved for more arch-independent bits.
  */
 #define KVM_REQ_TLB_FLUSH          0
 #define KVM_REQ_MMU_RELOAD         1
 #define KVM_REQ_PENDING_TIMER      2
 #define KVM_REQ_UNHALT             3
 
-/* x86-specific requests */
-#define KVM_REQ_MIGRATE_TIMER      8
-#define KVM_REQ_REPORT_TPR_ACCESS  9
-#define KVM_REQ_TRIPLE_FAULT      10
-#define KVM_REQ_MMU_SYNC          11
-#define KVM_REQ_CLOCK_UPDATE      12
-#define KVM_REQ_DEACTIVATE_FPU    13
-#define KVM_REQ_EVENT             14
-#define KVM_REQ_APF_HALT          15
-#define KVM_REQ_STEAL_UPDATE      16
-#define KVM_REQ_NMI               17
-#define KVM_REQ_PMU               18
-#define KVM_REQ_PMI               19
-#define KVM_REQ_SMI               20
-#define KVM_REQ_MASTERCLOCK_UPDATE 21
-#define KVM_REQ_MCLOCK_INPROGRESS 22
-#define KVM_REQ_SCAN_IOAPIC       23
-#define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
-#define KVM_REQ_APIC_PAGE_RELOAD  25
-#define KVM_REQ_HV_CRASH          26
-#define KVM_REQ_IOAPIC_EOI_EXIT   27
-#define KVM_REQ_HV_RESET          28
-#define KVM_REQ_HV_EXIT           29
-#define KVM_REQ_HV_STIMER         30
-
-/* PPC-specific requests */
-#define KVM_REQ_WATCHDOG           8
-#define KVM_REQ_EPR_EXIT           9
-
-/* s390-specific requests */
-#define KVM_REQ_ENABLE_IBS         8
-#define KVM_REQ_DISABLE_IBS        9
-
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
 
@@ -689,8 +657,6 @@ void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
-void kvm_make_mclock_inprogress_request(struct kvm *kvm);
-void kvm_make_scan_ioapic_request(struct kvm *kvm);
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);
 
 long kvm_arch_dev_ioctl(struct file *filp,
@@ -1011,11 +977,6 @@ static inline bool kvm_is_error_gpa(struct kvm *kvm, gpa_t gpa)
 	return kvm_is_error_hva(hva);
 }
 
-static inline void kvm_migrate_timers(struct kvm_vcpu *vcpu)
-{
-	set_bit(KVM_REQ_MIGRATE_TIMER, &vcpu->requests);
-}
-
 enum kvm_stat_kind {
 	KVM_STAT_VM,
 	KVM_STAT_VCPU,

commit 6662ba347b29b6df0756ffedb167fa4d89bab06f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 15:02:44 2016 +0100

    KVM: renumber vcpu->request bits
    
    Leave room for 4 more arch-independent requests.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 48abf6792286..b0ec0f778192 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -114,43 +114,42 @@ static inline bool is_error_page(struct page *page)
  * vcpu->requests bit members
  */
 #define KVM_REQ_TLB_FLUSH          0
-#define KVM_REQ_MMU_RELOAD         3
-#define KVM_REQ_PENDING_TIMER      5
-#define KVM_REQ_UNHALT             6
+#define KVM_REQ_MMU_RELOAD         1
+#define KVM_REQ_PENDING_TIMER      2
+#define KVM_REQ_UNHALT             3
 
 /* x86-specific requests */
-#define KVM_REQ_MIGRATE_TIMER      1
-#define KVM_REQ_REPORT_TPR_ACCESS  2
-#define KVM_REQ_TRIPLE_FAULT       4
-#define KVM_REQ_MMU_SYNC           7
-#define KVM_REQ_CLOCK_UPDATE       8
-/* 9 is unused */
-#define KVM_REQ_DEACTIVATE_FPU    10
-#define KVM_REQ_EVENT             11
-#define KVM_REQ_APF_HALT          12
-#define KVM_REQ_STEAL_UPDATE      13
-#define KVM_REQ_NMI               14
-#define KVM_REQ_PMU               15
-#define KVM_REQ_PMI               16
-#define KVM_REQ_MASTERCLOCK_UPDATE 18
-#define KVM_REQ_MCLOCK_INPROGRESS 19
-#define KVM_REQ_SCAN_IOAPIC       21
-#define KVM_REQ_GLOBAL_CLOCK_UPDATE 22
+#define KVM_REQ_MIGRATE_TIMER      8
+#define KVM_REQ_REPORT_TPR_ACCESS  9
+#define KVM_REQ_TRIPLE_FAULT      10
+#define KVM_REQ_MMU_SYNC          11
+#define KVM_REQ_CLOCK_UPDATE      12
+#define KVM_REQ_DEACTIVATE_FPU    13
+#define KVM_REQ_EVENT             14
+#define KVM_REQ_APF_HALT          15
+#define KVM_REQ_STEAL_UPDATE      16
+#define KVM_REQ_NMI               17
+#define KVM_REQ_PMU               18
+#define KVM_REQ_PMI               19
+#define KVM_REQ_SMI               20
+#define KVM_REQ_MASTERCLOCK_UPDATE 21
+#define KVM_REQ_MCLOCK_INPROGRESS 22
+#define KVM_REQ_SCAN_IOAPIC       23
+#define KVM_REQ_GLOBAL_CLOCK_UPDATE 24
 #define KVM_REQ_APIC_PAGE_RELOAD  25
-#define KVM_REQ_SMI               26
-#define KVM_REQ_HV_CRASH          27
-#define KVM_REQ_IOAPIC_EOI_EXIT   28
-#define KVM_REQ_HV_RESET          29
-#define KVM_REQ_HV_EXIT           30
-#define KVM_REQ_HV_STIMER         31
+#define KVM_REQ_HV_CRASH          26
+#define KVM_REQ_IOAPIC_EOI_EXIT   27
+#define KVM_REQ_HV_RESET          28
+#define KVM_REQ_HV_EXIT           29
+#define KVM_REQ_HV_STIMER         30
 
 /* PPC-specific requests */
-#define KVM_REQ_WATCHDOG          17
-#define KVM_REQ_EPR_EXIT          20
+#define KVM_REQ_WATCHDOG           8
+#define KVM_REQ_EPR_EXIT           9
 
 /* s390-specific requests */
-#define KVM_REQ_ENABLE_IBS        23
-#define KVM_REQ_DISABLE_IBS       24
+#define KVM_REQ_ENABLE_IBS         8
+#define KVM_REQ_DISABLE_IBS        9
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 0cd310437255be81cd2413407c1d61eb70286fe2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 15:00:53 2016 +0100

    KVM: document which architecture uses each request bit
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5ac775b4dde9..48abf6792286 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -114,12 +114,14 @@ static inline bool is_error_page(struct page *page)
  * vcpu->requests bit members
  */
 #define KVM_REQ_TLB_FLUSH          0
-#define KVM_REQ_MIGRATE_TIMER      1
-#define KVM_REQ_REPORT_TPR_ACCESS  2
 #define KVM_REQ_MMU_RELOAD         3
-#define KVM_REQ_TRIPLE_FAULT       4
 #define KVM_REQ_PENDING_TIMER      5
 #define KVM_REQ_UNHALT             6
+
+/* x86-specific requests */
+#define KVM_REQ_MIGRATE_TIMER      1
+#define KVM_REQ_REPORT_TPR_ACCESS  2
+#define KVM_REQ_TRIPLE_FAULT       4
 #define KVM_REQ_MMU_SYNC           7
 #define KVM_REQ_CLOCK_UPDATE       8
 /* 9 is unused */
@@ -130,14 +132,10 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_NMI               14
 #define KVM_REQ_PMU               15
 #define KVM_REQ_PMI               16
-#define KVM_REQ_WATCHDOG          17
 #define KVM_REQ_MASTERCLOCK_UPDATE 18
 #define KVM_REQ_MCLOCK_INPROGRESS 19
-#define KVM_REQ_EPR_EXIT          20
 #define KVM_REQ_SCAN_IOAPIC       21
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE 22
-#define KVM_REQ_ENABLE_IBS        23
-#define KVM_REQ_DISABLE_IBS       24
 #define KVM_REQ_APIC_PAGE_RELOAD  25
 #define KVM_REQ_SMI               26
 #define KVM_REQ_HV_CRASH          27
@@ -146,6 +144,14 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_HV_EXIT           30
 #define KVM_REQ_HV_STIMER         31
 
+/* PPC-specific requests */
+#define KVM_REQ_WATCHDOG          17
+#define KVM_REQ_EPR_EXIT          20
+
+/* s390-specific requests */
+#define KVM_REQ_ENABLE_IBS        23
+#define KVM_REQ_DISABLE_IBS       24
+
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
 

commit 6c71f8ae155422a030b4c382cb985dde006ccc3f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 7 14:53:46 2016 +0100

    KVM: Remove unused KVM_REQ_KICK to save a bit in vcpu->requests
    
    Suggested-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    [Takuya moved all subsequent constants to fill the void, but that
     is useless in view of the following patches.  So this change looks
     nothing like the original. - Paolo]
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 61c3e6c69f27..5ac775b4dde9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -122,7 +122,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_UNHALT             6
 #define KVM_REQ_MMU_SYNC           7
 #define KVM_REQ_CLOCK_UPDATE       8
-#define KVM_REQ_KICK               9
+/* 9 is unused */
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11
 #define KVM_REQ_APF_HALT          12

commit 671d9ab38097fae45ff4f24562789b98b51d37ec
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Nov 20 19:52:12 2015 +0100

    kvm: Dump guest rIP when the guest tried something unsupported
    
    It looks like this in action:
    
      kvm [5197]: vcpu0, guest rIP: 0xffffffff810187ba unhandled rdmsr: 0xc001102
    
    and helps to pinpoint quickly where in the guest we did the unsupported
    thing.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2969c474a399..61c3e6c69f27 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -447,7 +447,8 @@ struct kvm {
 
 /* The guest did something we don't support. */
 #define vcpu_unimpl(vcpu, fmt, ...)					\
-	kvm_pr_unimpl("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
+	kvm_pr_unimpl("vcpu%i, guest rIP: 0x%lx " fmt,			\
+			(vcpu)->vcpu_id, kvm_rip_read(vcpu), ## __VA_ARGS__)
 
 #define vcpu_debug(vcpu, fmt, ...)					\
 	kvm_debug("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)

commit 1f4b34f825e8cef6f493d06b46605384785b3d16
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Mon Nov 30 19:22:21 2015 +0300

    kvm/x86: Hyper-V SynIC timers
    
    Per Hyper-V specification (and as required by Hyper-V-aware guests),
    SynIC provides 4 per-vCPU timers.  Each timer is programmed via a pair
    of MSRs, and signals expiration by delivering a special format message
    to the configured SynIC message slot and triggering the corresponding
    synthetic interrupt.
    
    Note: as implemented by this patch, all periodic timers are "lazy"
    (i.e. if the vCPU wasn't scheduled for more than the timer period the
    timer events are lost), regardless of the corresponding configuration
    MSR.  If deemed necessary, the "catch up" mode (the timer period is
    shortened until the timer catches up) will be implemented later.
    
    Changes v2:
    * Use remainder to calculate periodic timer expiration time
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: "K. Y. Srinivasan" <kys@microsoft.com>
    CC: Haiyang Zhang <haiyangz@microsoft.com>
    CC: Vitaly Kuznetsov <vkuznets@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f44c24b81b17..2969c474a399 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -144,6 +144,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_IOAPIC_EOI_EXIT   28
 #define KVM_REQ_HV_RESET          29
 #define KVM_REQ_HV_EXIT           30
+#define KVM_REQ_HV_STIMER         31
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 765eaa0f70eaa274ec8b815d8c210c20cf7b6dbc
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Mon Nov 30 19:22:20 2015 +0300

    kvm/x86: Hyper-V SynIC message slot pending clearing at SINT ack
    
    The SynIC message protocol mandates that the message slot is claimed
    by atomically setting message type to something other than HVMSG_NONE.
    If another message is to be delivered while the slot is still busy,
    message pending flag is asserted to indicate to the guest that the
    hypervisor wants to be notified when the slot is released.
    
    To make sure the protocol works regardless of where the message
    sources are (kernel or userspace), clear the pending flag on SINT ACK
    notification, and let the message sources compete for the slot again.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: "K. Y. Srinivasan" <kys@microsoft.com>
    CC: Haiyang Zhang <haiyangz@microsoft.com>
    CC: Vitaly Kuznetsov <vkuznets@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 590c46e672df..f44c24b81b17 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -450,6 +450,8 @@ struct kvm {
 
 #define vcpu_debug(vcpu, fmt, ...)					\
 	kvm_debug("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
+#define vcpu_err(vcpu, fmt, ...)					\
+	kvm_err("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {

commit 4bd33b568855f5483a6c6d7e4706ef507ab8586b
Author: Janosch Frank <frankja@linux.vnet.ibm.com>
Date:   Wed Oct 14 12:37:35 2015 +0200

    KVM: Remove unnecessary debugfs dentry references
    
    KVM creates debugfs files to export VM statistics to userland. To be
    able to remove them on kvm exit it tracks the files' dentries.
    
    Since their parent directory is also tracked and since each parent
    direntry knows its children we can easily remove them by using
    debugfs_remove_recursive(kvm_debugfs_dir). Therefore we don't
    need the extra tracking in the kvm_stats_debugfs_item anymore.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-By: Sascha Silbe <silbe@linux.vnet.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a754fc08e194..590c46e672df 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1016,7 +1016,6 @@ struct kvm_stats_debugfs_item {
 	const char *name;
 	int offset;
 	enum kvm_stat_kind kind;
-	struct dentry *dentry;
 };
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;

commit c896939f7cff767091b5d84587cd144e5d3613b7
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Nov 5 09:55:08 2015 +0100

    KVM: use heuristic for fast VCPU lookup by id
    
    Usually, VCPU ids match the array index. So let's try a fast
    lookup first before falling back to the slow iteration.
    
    Suggested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 29119193a19f..a754fc08e194 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -472,6 +472,11 @@ static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 	struct kvm_vcpu *vcpu;
 	int i;
 
+	if (id < 0 || id >= KVM_MAX_VCPUS)
+		return NULL;
+	vcpu = kvm_get_vcpu(kvm, id);
+	if (vcpu && vcpu->vcpu_id == id)
+		return vcpu;
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		if (vcpu->vcpu_id == id)
 			return vcpu;

commit 33e941547923283f7f1022f3c35359ea9403d9a4
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Sat Nov 14 11:21:06 2015 +0800

    KVM: kvm_is_visible_gfn can be boolean
    
    This patch makes kvm_is_visible_gfn return bool due to this particular
    function only using either one or zero as its return value.
    
    No functional change.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 14f95969b0f3..29119193a19f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -641,7 +641,7 @@ int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
-int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
+bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 

commit db3975717ac5e2c2761bae7b90c4f2e0abb5ef22
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:35 2015 +0300

    kvm/x86: Hyper-V kvm exit
    
    A new vcpu exit is introduced to notify the userspace of the
    changes in Hyper-V SynIC configuration triggered by guest writing to the
    corresponding MSRs.
    
    Changes v4:
    * exit into userspace only if guest writes into SynIC MSR's
    
    Changes v3:
    * added KVM_EXIT_HYPERV types and structs notes into docs
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ebaf2f82f712..14f95969b0f3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -143,6 +143,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_HV_CRASH          27
 #define KVM_REQ_IOAPIC_EOI_EXIT   28
 #define KVM_REQ_HV_RESET          29
+#define KVM_REQ_HV_EXIT           30
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 5c919412fe61c35947816fdbd5f7bd09fe0dd073
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:34 2015 +0300

    kvm/x86: Hyper-V synthetic interrupt controller
    
    SynIC (synthetic interrupt controller) is a lapic extension,
    which is controlled via MSRs and maintains for each vCPU
     - 16 synthetic interrupt "lines" (SINT's); each can be configured to
       trigger a specific interrupt vector optionally with auto-EOI
       semantics
     - a message page in the guest memory with 16 256-byte per-SINT message
       slots
     - an event flag page in the guest memory with 16 2048-bit per-SINT
       event flag areas
    
    The host triggers a SINT whenever it delivers a new message to the
    corresponding slot or flips an event flag bit in the corresponding area.
    The guest informs the host that it can try delivering a message by
    explicitly asserting EOI in lapic or writing to End-Of-Message (EOM)
    MSR.
    
    The userspace (qemu) triggers interrupts and receives EOM notifications
    via irqfd with resampler; for that, a GSI is allocated for each
    configured SINT, and irq_routing api is extended to support GSI-SINT
    mapping.
    
    Changes v4:
    * added activation of SynIC by vcpu KVM_ENABLE_CAP
    * added per SynIC active flag
    * added deactivation of APICv upon SynIC activation
    
    Changes v3:
    * added KVM_CAP_HYPERV_SYNIC and KVM_IRQ_ROUTING_HV_SINT notes into
    docs
    
    Changes v2:
    * do not use posted interrupts for Hyper-V SynIC AutoEOI vectors
    * add Hyper-V SynIC vectors into EOI exit bitmap
    * Hyper-V SyniIC SINT msr write logic simplified
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 23555c0f4f2d..ebaf2f82f712 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -318,6 +318,11 @@ struct kvm_s390_adapter_int {
 	u32 adapter_id;
 };
 
+struct kvm_hv_sint {
+	u32 vcpu;
+	u32 sint;
+};
+
 struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
 	u32 type;
@@ -331,6 +336,7 @@ struct kvm_kernel_irq_routing_entry {
 		} irqchip;
 		struct msi_msg msi;
 		struct kvm_s390_adapter_int adapter;
+		struct kvm_hv_sint hv_sint;
 	};
 	struct hlist_node link;
 };

commit abdb080f7ac8a85547f5e0246362790043bbd3f2
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Tue Nov 10 15:36:31 2015 +0300

    kvm/irqchip: kvm_arch_irq_routing_update renaming split
    
    Actually kvm_arch_irq_routing_update() should be
    kvm_arch_post_irq_routing_update() as it's called at the end
    of irq routing update.
    
    This renaming frees kvm_arch_irq_routing_update function name.
    kvm_arch_irq_routing_update() weak function which will be used
    to update mappings for arch-specific irq routing entries
    (in particular, the upcoming Hyper-V synthetic interrupts).
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Roman Kagan <rkagan@virtuozzo.com>
    CC: Denis V. Lunev <den@openvz.org>
    CC: qemu-devel@nongnu.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c923350ca20a..23555c0f4f2d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -484,12 +484,12 @@ void vcpu_put(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_IOAPIC
 void kvm_vcpu_request_scan_ioapic(struct kvm *kvm);
-void kvm_arch_irq_routing_update(struct kvm *kvm);
+void kvm_arch_post_irq_routing_update(struct kvm *kvm);
 #else
 static inline void kvm_vcpu_request_scan_ioapic(struct kvm *kvm)
 {
 }
-static inline void kvm_arch_irq_routing_update(struct kvm *kvm)
+static inline void kvm_arch_post_irq_routing_update(struct kvm *kvm)
 {
 }
 #endif
@@ -1091,6 +1091,7 @@ static inline void kvm_irq_routing_update(struct kvm *kvm)
 {
 }
 #endif
+void kvm_arch_irq_routing_update(struct kvm *kvm);
 
 static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {

commit db27a7a37aa0b1f8b373f8b0fb72a2ccaafb85b7
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Thu Nov 5 09:03:50 2015 +0100

    KVM: Provide function for VCPU lookup by id
    
    Let's provide a function to lookup a VCPU by id.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [split patch from refactoring patch]

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5706a2108f0a..c923350ca20a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -460,6 +460,17 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 	     (vcpup = kvm_get_vcpu(kvm, idx)) != NULL; \
 	     idx++)
 
+static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
+{
+	struct kvm_vcpu *vcpu;
+	int i;
+
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		if (vcpu->vcpu_id == id)
+			return vcpu;
+	return NULL;
+}
+
 #define kvm_for_each_memslot(memslot, slots)	\
 	for (memslot = &slots->memslots[0];	\
 	      memslot < slots->memslots + KVM_MEM_SLOTS_NUM && memslot->npages;\

commit 35181e86df97e4223f4a28fb33e2bcf3b73de141
Author: Haozhong Zhang <haozhong.zhang@intel.com>
Date:   Tue Oct 20 15:39:03 2015 +0800

    KVM: x86: Add a common TSC scaling function
    
    VMX and SVM calculate the TSC scaling ratio in a similar logic, so this
    patch generalizes it to a common TSC scaling function.
    
    Signed-off-by: Haozhong Zhang <haozhong.zhang@intel.com>
    [Inline the multiplication and shift steps into mul_u64_u64_shr.  Remove
     BUG_ON.  - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 242a6d2b53ff..5706a2108f0a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1183,4 +1183,5 @@ void kvm_arch_irq_bypass_start(struct irq_bypass_consumer *);
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				  uint32_t guest_irq, bool set);
 #endif /* CONFIG_HAVE_KVM_IRQ_BYPASS */
+
 #endif

commit 8a22f234a81ab4d1de5d948c3478608f08a9b844
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 28 18:52:02 2015 +0100

    KVM: x86: move kvm_set_irq_inatomic to legacy device assignment
    
    The function is not used outside device assignment, and
    kvm_arch_set_irq_inatomic has a different prototype.  Move it here and
    make it static to avoid confusion.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 15c78f320678..242a6d2b53ff 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -827,7 +827,6 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin);
 
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status);
-int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level, bool line_status);
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,

commit b97e6de9c96cefaa02a6a7464731ea504b45e150
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 28 19:16:47 2015 +0100

    KVM: x86: merge kvm_arch_set_irq with kvm_set_msi_inatomic
    
    We do not want to do too much work in atomic context, in particular
    not walking all the VCPUs of the virtual machine.  So we want
    to distinguish the architecture-specific injection function for irqfd
    from kvm_set_msi.  Since it's still empty, reuse the newly added
    kvm_arch_set_irq and rename it to kvm_arch_set_irq_inatomic.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 87189a41d904..15c78f320678 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -830,10 +830,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level, bool line_status);
-
-int kvm_arch_set_irq(struct kvm_kernel_irq_routing_entry *irq, struct kvm *kvm,
-		     int irq_source_id, int level, bool line_status);
-
+int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
+			       struct kvm *kvm, int irq_source_id,
+			       int level, bool line_status);
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_notify_acked_gsi(struct kvm *kvm, int gsi);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);

commit 197a4f4b063e4e7a603ff1de56b3cf0400fabc30
Merge: d6cf98e06ea4 26caea7693cb
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Nov 4 16:24:17 2015 +0100

    Merge tag 'kvm-arm-for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM Changes for v4.4-rc1
    
    Includes a number of fixes for the arch-timer, introducing proper
    level-triggered semantics for the arch-timers, a series of patches to
    synchronously halt a guest (prerequisite for IRQ forwarding), some tracepoint
    improvements, a tweak for the EL2 panic handlers, some more VGIC cleanups
    getting rid of redundant state, and finally a stylistic change that gets rid of
    some ctags warnings.
    
    Conflicts:
            arch/x86/include/asm/kvm_host.h

commit 3217f7c25bca66eed9b07f0b8bfd1937169b0736
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Aug 27 16:41:15 2015 +0200

    KVM: Add kvm_arch_vcpu_{un}blocking callbacks
    
    Some times it is useful for architecture implementations of KVM to know
    when the VCPU thread is about to block or when it comes back from
    blocking (arm/arm64 needs to know this to properly implement timers, for
    example).
    
    Therefore provide a generic architecture callback function in line with
    what we do elsewhere for KVM generic-arch interactions.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1bef9e21e725..4a86f5f072c0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -625,6 +625,8 @@ int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);

commit c9a5eccac1abf50649949f15754a7635f263a1ff
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Oct 16 10:07:47 2015 +0300

    kvm/eventfd: add arch-specific set_irq
    
    Allow for arch-specific interrupt types to be set.  For that, add
    kvm_arch_set_irq() which takes interrupt type-specific action if it
    recognizes the interrupt type given, and -EWOULDBLOCK otherwise.
    
    The default implementation always returns -EWOULDBLOCK.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Vitaly Kuznetsov <vkuznets@redhat.com>
    CC: "K. Y. Srinivasan" <kys@microsoft.com>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b66861c297c4..eba9caebc9c1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -828,6 +828,10 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level, bool line_status);
+
+int kvm_arch_set_irq(struct kvm_kernel_irq_routing_entry *irq, struct kvm *kvm,
+		     int irq_source_id, int level, bool line_status);
+
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_notify_acked_gsi(struct kvm *kvm, int gsi);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);

commit ba1aefcd6db5536d3eb3ca3ce7bd6786960140ea
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Oct 16 10:07:46 2015 +0300

    kvm/eventfd: factor out kvm_notify_acked_gsi()
    
    Factor out kvm_notify_acked_gsi() helper to iterate over EOI listeners
    and notify those matching the given gsi.
    
    It will be reused in the upcoming Hyper-V SynIC implementation.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Vitaly Kuznetsov <vkuznets@redhat.com>
    CC: "K. Y. Srinivasan" <kys@microsoft.com>
    CC: Gleb Natapov <gleb@kernel.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9596a2f0977b..b66861c297c4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -829,6 +829,7 @@ int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level)
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level, bool line_status);
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);
+void kvm_notify_acked_gsi(struct kvm *kvm, int gsi);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);

commit bf9f6ac8d74969690df1485b33b7c238ca9f2269
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:55 2015 +0800

    KVM: Update Posted-Interrupts Descriptor when vCPU is blocked
    
    This patch updates the Posted-Interrupts Descriptor when vCPU
    is blocked.
    
    pre-block:
    - Add the vCPU to the blocked per-CPU list
    - Set 'NV' to POSTED_INTR_WAKEUP_VECTOR
    
    post-block:
    - Remove the vCPU from the per-CPU list
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    [Concentrate invocation of pre/post-block hooks to vcpu_block. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5c3f4538807f..9596a2f0977b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -234,6 +234,9 @@ struct kvm_vcpu {
 	unsigned long requests;
 	unsigned long guest_debug;
 
+	int pre_pcpu;
+	struct list_head blocked_vcpu_list;
+
 	struct mutex mutex;
 	struct kvm_run *run;
 

commit f70c20aaf141adb715a2d750c55154073b02a9c3
Author: Feng Wu <feng.wu@intel.com>
Date:   Fri Sep 18 22:29:53 2015 +0800

    KVM: Add an arch specific hooks in 'struct kvm_kernel_irqfd'
    
    This patch adds an arch specific hooks 'arch_update' in
    'struct kvm_kernel_irqfd'. On Intel side, it is used to
    update the IRTE when VT-d posted-interrupts is used.
    
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b8543b02b7bc..5c3f4538807f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1172,5 +1172,7 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *,
 			   struct irq_bypass_producer *);
 void kvm_arch_irq_bypass_stop(struct irq_bypass_consumer *);
 void kvm_arch_irq_bypass_start(struct irq_bypass_consumer *);
+int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
+				  uint32_t guest_irq, bool set);
 #endif /* CONFIG_HAVE_KVM_IRQ_BYPASS */
 #endif

commit 1a02b27035f82091d51ecafcb9ccaac1f31d4eb2
Author: Eric Auger <eric.auger@linaro.org>
Date:   Fri Sep 18 22:29:43 2015 +0800

    KVM: introduce kvm_arch functions for IRQ bypass
    
    This patch introduces
    - kvm_arch_irq_bypass_add_producer
    - kvm_arch_irq_bypass_del_producer
    - kvm_arch_irq_bypass_stop
    - kvm_arch_irq_bypass_start
    
    They make possible to specialize the KVM IRQ bypass consumer in
    case CONFIG_KVM_HAVE_IRQ_BYPASS is set.
    
    Signed-off-by: Eric Auger <eric.auger@linaro.org>
    [Add weak implementations of the callbacks. - Feng]
    Signed-off-by: Feng Wu <feng.wu@intel.com>
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cd0ba2e931e1..b8543b02b7bc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -24,6 +24,7 @@
 #include <linux/err.h>
 #include <linux/irqflags.h>
 #include <linux/context_tracking.h>
+#include <linux/irqbypass.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -1163,4 +1164,13 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 {
 }
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
+
+#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
+int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *,
+			   struct irq_bypass_producer *);
+void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *,
+			   struct irq_bypass_producer *);
+void kvm_arch_irq_bypass_stop(struct irq_bypass_consumer *);
+void kvm_arch_irq_bypass_start(struct irq_bypass_consumer *);
+#endif /* CONFIG_HAVE_KVM_IRQ_BYPASS */
 #endif

commit e516cebb4f2b2057a5a421fea589079502acfff6
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Wed Sep 16 12:29:48 2015 +0300

    kvm/x86: Hyper-V HV_X64_MSR_RESET msr
    
    HV_X64_MSR_RESET msr is used by Hyper-V based Windows guest
    to reset guest VM by hypervisor.
    
    Necessary to support loading of winhv.sys in guest, which in turn is
    required to support Windows VMBus.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d754f2d826e9..cd0ba2e931e1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -141,6 +141,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_SMI               26
 #define KVM_REQ_HV_CRASH          27
 #define KVM_REQ_IOAPIC_EOI_EXIT   28
+#define KVM_REQ_HV_RESET          29
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit b053b2aef25d00773fa6762dcd4b7f5c9c42d171
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:32:35 2015 -0700

    KVM: x86: Add EOI exit bitmap inference
    
    In order to support a userspace IOAPIC interacting with an in kernel
    APIC, the EOI exit bitmaps need to be configurable.
    
    If the IOAPIC is in userspace (i.e. the irqchip has been split), the
    EOI exit bitmaps will be set whenever the GSI Routes are configured.
    In particular, for the low MSI routes are reservable for userspace
    IOAPICs. For these MSI routes, the EOI Exit bit corresponding to the
    destination vector of the route will be set for the destination VCPU.
    
    The intention is for the userspace IOAPICs to use the reservable MSI
    routes to inject interrupts into the guest.
    
    This is a slight abuse of the notion of an MSI Route, given that MSIs
    classically bypass the IOAPIC. It might be worthwhile to add an
    additional route type to improve clarity.
    
    Compile tested for Intel x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3b33215ed447..d754f2d826e9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -330,6 +330,18 @@ struct kvm_kernel_irq_routing_entry {
 	struct hlist_node link;
 };
 
+#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
+struct kvm_irq_routing_table {
+	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
+	u32 nr_rt_entries;
+	/*
+	 * Array indexed by gsi. Each entry contains list of irq chips
+	 * the gsi is connected to.
+	 */
+	struct hlist_head map[0];
+};
+#endif
+
 #ifndef KVM_PRIVATE_MEM_SLOTS
 #define KVM_PRIVATE_MEM_SLOTS 0
 #endif
@@ -456,10 +468,14 @@ void vcpu_put(struct kvm_vcpu *vcpu);
 
 #ifdef __KVM_HAVE_IOAPIC
 void kvm_vcpu_request_scan_ioapic(struct kvm *kvm);
+void kvm_arch_irq_routing_update(struct kvm *kvm);
 #else
 static inline void kvm_vcpu_request_scan_ioapic(struct kvm *kvm)
 {
 }
+static inline void kvm_arch_irq_routing_update(struct kvm *kvm)
+{
+}
 #endif
 
 #ifdef CONFIG_HAVE_KVM_IRQFD

commit 7543a635aa09eb138b2cbf60ac3ff19503ae6954
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:21:41 2015 -0700

    KVM: x86: Add KVM exit for IOAPIC EOIs
    
    Adds KVM_EXIT_IOAPIC_EOI which allows the kernel to EOI
    level-triggered IOAPIC interrupts.
    
    Uses a per VCPU exit bitmap to decide whether or not the IOAPIC needs
    to be informed (which is identical to the EOI_EXIT_BITMAP field used
    by modern x86 processors, but can also be used to elide kvm IOAPIC EOI
    exits on older processors).
    
    [Note: A prototype using ResampleFDs found that decoupling the EOI
    from the VCPU's thread made it possible for the VCPU to not see a
    recent EOI after reentering the guest. This does not match real
    hardware.]
    
    Compile tested for Intel x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 354f147647ab..3b33215ed447 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -140,6 +140,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_APIC_PAGE_RELOAD  25
 #define KVM_REQ_SMI               26
 #define KVM_REQ_HV_CRASH          27
+#define KVM_REQ_IOAPIC_EOI_EXIT   28
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
@@ -1146,4 +1147,3 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 }
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 #endif
-

commit 49df6397edfc5a8ba8ca813b51fb9729d8e94b40
Author: Steve Rutherford <srutherford@google.com>
Date:   Wed Jul 29 23:21:40 2015 -0700

    KVM: x86: Split the APIC from the rest of IRQCHIP.
    
    First patch in a series which enables the relocation of the
    PIC/IOAPIC to userspace.
    
    Adds capability KVM_CAP_SPLIT_IRQCHIP;
    
    KVM_CAP_SPLIT_IRQCHIP enables the construction of LAPICs without the
    rest of the irqchip.
    
    Compile tested for x86.
    
    Signed-off-by: Steve Rutherford <srutherford@google.com>
    Suggested-by: Andrew Honig <ahonig@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1bef9e21e725..354f147647ab 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1002,6 +1002,7 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 #endif
 
 int kvm_setup_default_irq_routing(struct kvm *kvm);
+int kvm_setup_empty_irq_routing(struct kvm *kvm);
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,

commit 19020f8ab83de9dc5a9c8af1f321a526f38bbc40
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Sep 3 22:07:37 2015 +0800

    KVM: make halt_poll_ns per-vCPU
    
    Change halt_poll_ns into per-VCPU variable, seeded from module parameter,
    to allow greater flexibility.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 81089cf1f0c1..1bef9e21e725 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -242,6 +242,7 @@ struct kvm_vcpu {
 	int sigset_active;
 	sigset_t sigset;
 	struct kvm_vcpu_stat stat;
+	unsigned int halt_poll_ns;
 
 #ifdef CONFIG_HAS_IOMEM
 	int mmio_needed;

commit dd489240a21afc3ff3962aba5d987229536cae63
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 29 11:32:20 2015 +0200

    KVM: document memory barriers for kvm->vcpus/kvm->online_vcpus
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bd1097a95704..81089cf1f0c1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -427,6 +427,10 @@ struct kvm {
 
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {
+	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu, in case
+	 * the caller has read kvm->online_vcpus before (as is the case
+	 * for kvm_for_each_vcpu, for example).
+	 */
 	smp_rmb();
 	return kvm->vcpus[i];
 }

commit d71ba788345c2b5646101766e0c52714a9b5ed7f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jul 29 11:56:48 2015 +0200

    KVM: move code related to KVM_SET_BOOT_CPU_ID to x86
    
    This is another remnant of ia64 support.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 51103f0feb7e..bd1097a95704 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -364,9 +364,6 @@ struct kvm {
 	struct kvm_memslots *memslots[KVM_ADDRESS_SPACE_NUM];
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
-#ifdef CONFIG_KVM_APIC_ARCHITECTURE
-	u32 bsp_vcpu_id;
-#endif
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	atomic_t online_vcpus;
 	int last_boosted_vcpu;
@@ -1059,22 +1056,9 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
-static inline bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)
-{
-	return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
-}
-
-static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
-{
-	return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
-}
-
 bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu);
-
 #else
-
 static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
-
 #endif
 
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)

commit 2ce7918990641b07e70e1b25752d666369e2016e
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Jul 3 15:01:41 2015 +0300

    kvm/x86: add sending hyper-v crash notification to user space
    
    Sending of notification is done by exiting vcpu to user space
    if KVM_REQ_HV_CRASH is enabled for vcpu. At exit to user space
    the kvm_run structure contains system_event with type
    KVM_SYSTEM_EVENT_CRASH to notify about guest crash occurred.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1d917d9b7f12..51103f0feb7e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -139,6 +139,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_DISABLE_IBS       24
 #define KVM_REQ_APIC_PAGE_RELOAD  25
 #define KVM_REQ_SMI               26
+#define KVM_REQ_HV_CRASH          27
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit ee86dbc6e327062396748162b95309388c19faab
Author: Andrey Smetanin <asmetanin@virtuozzo.com>
Date:   Fri Jul 3 15:01:35 2015 +0300

    kvm: introduce vcpu_debug = kvm_debug + vcpu context
    
    vcpu_debug is useful macro like kvm_debug but additionally
    includes vcpu context inside output.
    
    Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Reviewed-by: Peter Hornyack <peterhornyack@google.com>
    CC: Paolo Bonzini <pbonzini@redhat.com>
    CC: Gleb Natapov <gleb@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 05e99b8ef465..1d917d9b7f12 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -424,6 +424,9 @@ struct kvm {
 #define vcpu_unimpl(vcpu, fmt, ...)					\
 	kvm_pr_unimpl("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 
+#define vcpu_debug(vcpu, fmt, ...)					\
+	kvm_debug("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
+
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {
 	smp_rmb();

commit 5544eb9b81940647b8fad1f251b37cbe2819ce44
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Jul 7 15:41:58 2015 +0200

    KVM: count number of assigned devices
    
    If there are no assigned devices, the guest PAT are not providing
    any useful information and can be overridden to writeback; VMX
    always does this because it has the "IPAT" bit in its extended
    page table entries, but SVM does not have anything similar.
    Hook into VFIO and legacy device assignment so that they
    provide this information to KVM.
    
    Reviewed-by: Alex Williamson <alex.williamson@redhat.com>
    Tested-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9564fd78c547..05e99b8ef465 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -734,6 +734,24 @@ static inline bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 	return false;
 }
 #endif
+#ifdef __KVM_HAVE_ARCH_ASSIGNED_DEVICE
+void kvm_arch_start_assignment(struct kvm *kvm);
+void kvm_arch_end_assignment(struct kvm *kvm);
+bool kvm_arch_has_assigned_device(struct kvm *kvm);
+#else
+static inline void kvm_arch_start_assignment(struct kvm *kvm)
+{
+}
+
+static inline void kvm_arch_end_assignment(struct kvm *kvm)
+{
+}
+
+static inline bool kvm_arch_has_assigned_device(struct kvm *kvm)
+{
+	return false;
+}
+#endif
 
 static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 {

commit f481b069e674378758c73761827e83ab05c46b52
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun May 17 17:30:37 2015 +0200

    KVM: implement multiple address spaces
    
    Only two ioctls have to be modified; the address space id is
    placed in the higher 16 bits of their slot id argument.
    
    As of this patch, no architecture defines more than one
    address space; x86 will be the first.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ba1ea43998e4..9564fd78c547 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -44,6 +44,10 @@
 /* Two fragments for cross MMIO pages. */
 #define KVM_MAX_MMIO_FRAGMENTS	2
 
+#ifndef KVM_ADDRESS_SPACE_NUM
+#define KVM_ADDRESS_SPACE_NUM	1
+#endif
+
 /*
  * For the normal pfn, the highest 12 bits should be zero,
  * so we can mask bit 62 ~ bit 52  to indicate the error pfn,
@@ -331,6 +335,13 @@ struct kvm_kernel_irq_routing_entry {
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 #endif
 
+#ifndef __KVM_VCPU_MULTIPLE_ADDRESS_SPACE
+static inline int kvm_arch_vcpu_memslots_id(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+#endif
+
 /*
  * Note:
  * memslots are not sorted by id anymore, please use id_to_memslot()
@@ -349,7 +360,7 @@ struct kvm {
 	spinlock_t mmu_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
-	struct kvm_memslots *memslots;
+	struct kvm_memslots *memslots[KVM_ADDRESS_SPACE_NUM];
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
@@ -464,16 +475,23 @@ void kvm_exit(void);
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
 
-static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
+static inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)
 {
-	return rcu_dereference_check(kvm->memslots,
+	return rcu_dereference_check(kvm->memslots[as_id],
 			srcu_read_lock_held(&kvm->srcu)
 			|| lockdep_is_held(&kvm->slots_lock));
 }
 
+static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
+{
+	return __kvm_memslots(kvm, 0);
+}
+
 static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 {
-	return kvm_memslots(vcpu->kvm);
+	int as_id = kvm_arch_vcpu_memslots_id(vcpu);
+
+	return __kvm_memslots(vcpu->kvm, as_id);
 }
 
 static inline struct kvm_memory_slot *

commit 8e73485c7959fd25650761eab04db1e72ea14c23
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun May 17 13:58:53 2015 +0200

    KVM: add vcpu-specific functions to read/write/translate GFNs
    
    We need to hide SMRAM from guests not running in SMM.  Therefore, all
    uses of kvm_read_guest* and kvm_write_guest* must be changed to use
    different address spaces, depending on whether the VCPU is in system
    management mode.  We need to introduce a new family of functions for
    this purpose.
    
    For now, the VCPU-based functions have the same behavior as the
    existing per-VM ones, they just accept a different type for the
    first argument.  Later however they will be changed to use one of many
    "struct kvm_memslots" stored in struct kvm, through an architecture hook.
    VM-based functions will unconditionally use the first memslots pointer.
    
    Whenever possible, this patch introduces slot-based functions with an
    __ prefix, with two wrappers for generic and vcpu-based actions.
    The exceptions are kvm_read_guest and kvm_write_guest, which are copied
    into the new functions kvm_vcpu_read_guest and kvm_vcpu_write_guest.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b019fee6d941..ba1ea43998e4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -471,6 +471,11 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 			|| lockdep_is_held(&kvm->slots_lock));
 }
 
+static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
+{
+	return kvm_memslots(vcpu->kvm);
+}
+
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
@@ -576,6 +581,25 @@ int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
+struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
+struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
+pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
+pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
+unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
+unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
+int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,
+			     int len);
+int kvm_vcpu_read_guest_atomic(struct kvm_vcpu *vcpu, gpa_t gpa, void *data,
+			       unsigned long len);
+int kvm_vcpu_read_guest(struct kvm_vcpu *vcpu, gpa_t gpa, void *data,
+			unsigned long len);
+int kvm_vcpu_write_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, const void *data,
+			      int offset, int len);
+int kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,
+			 unsigned long len);
+void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);
+
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);

commit 64d6067057d9658acb8675afcfba549abdb7fc16
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu May 7 11:36:11 2015 +0200

    KVM: x86: stubs for SMM support
    
    This patch adds the interface between x86.c and the emulator: the
    SMBASE register, a new emulator flag, the RSM instruction.  It also
    adds a new request bit that will be used by the KVM_SMI ioctl.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a8bcbc9c6078..b019fee6d941 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -134,6 +134,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_ENABLE_IBS        23
 #define KVM_REQ_DISABLE_IBS       24
 #define KVM_REQ_APIC_PAGE_RELOAD  25
+#define KVM_REQ_SMI               26
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit d9ef13c2b3983de8dd1373ef670799dbb6498122
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 19 16:01:50 2015 +0200

    KVM: pass kvm_memory_slot to gfn_to_page_many_atomic
    
    The memory slot is already available from gfn_to_memslot_dirty_bitmap.
    Isn't it a shame to look it up again?  Plus, it makes gfn_to_page_many_atomic
    agnostic of multiple VCPU address spaces.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9bd3bc16be87..a8bcbc9c6078 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -526,8 +526,8 @@ void kvm_arch_flush_shadow_all(struct kvm *kvm);
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 				   struct kvm_memory_slot *slot);
 
-int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
-			    int nr_pages);
+int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,
+			    struct page **pages, int nr_pages);
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);

commit f36f3f2846b5578d62910ee0b6dbef59fdd1cfa4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:20:23 2015 +0200

    KVM: add "new" argument to kvm_arch_commit_memory_region
    
    This lets the function access the new memory slot without going through
    kvm_memslots and id_to_memslot.  It will simplify the code when more
    than one address space will be supported.
    
    Unfortunately, the "const"ness of the new argument must be casted
    away in two places.  Fixing KVM to accept const struct kvm_memory_slot
    pointers would require modifications in pretty much all architectures,
    and is left for later.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8815f1dffb77..9bd3bc16be87 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -516,6 +516,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
+				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change);
 bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);

commit 15f46015ee17681b542432df21747f5c51857156
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sun May 17 21:26:08 2015 +0200

    KVM: add memslots argument to kvm_arch_memslots_updated
    
    Prepare for the case of multiple address spaces.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fbced7015ebd..8815f1dffb77 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -508,7 +508,7 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages);
-void kvm_arch_memslots_updated(struct kvm *kvm);
+void kvm_arch_memslots_updated(struct kvm *kvm, struct kvm_memslots *slots);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				const struct kvm_userspace_memory_region *mem,

commit 09170a49422bd786be3eac5cec1955257c5a34b7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:59:39 2015 +0200

    KVM: const-ify uses of struct kvm_userspace_memory_region
    
    Architecture-specific helpers are not supposed to muck with
    struct kvm_userspace_memory_region contents.  Add const to
    enforce this.
    
    In order to eliminate the only write in __kvm_set_memory_region,
    the cleaning of deleted slots is pulled up from update_memslots
    to __kvm_set_memory_region.
    
    Reviewed-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 87fd74a04005..fbced7015ebd 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -501,9 +501,9 @@ enum kvm_mr_change {
 };
 
 int kvm_set_memory_region(struct kvm *kvm,
-			  struct kvm_userspace_memory_region *mem);
+			  const struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,
-			    struct kvm_userspace_memory_region *mem);
+			    const struct kvm_userspace_memory_region *mem);
 void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
@@ -511,10 +511,10 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 void kvm_arch_memslots_updated(struct kvm *kvm);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
-				struct kvm_userspace_memory_region *mem,
+				const struct kvm_userspace_memory_region *mem,
 				enum kvm_mr_change change);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
-				struct kvm_userspace_memory_region *mem,
+				const struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
 				enum kvm_mr_change change);
 bool kvm_largepages_enabled(void);

commit 3520469d65f26a1cd2f610f5d5de976f78db74fe
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 2 11:20:48 2015 +0200

    KVM: export __gfn_to_pfn_memslot, drop gfn_to_pfn_async
    
    gfn_to_pfn_async is used in just one place, and because of x86-specific
    treatment that place will need to look at the memory slot.  Hence inline
    it into try_async_pf and export __gfn_to_pfn_memslot.
    
    The patch also switches the subsequent call to gfn_to_pfn_prot to use
    __gfn_to_pfn_memslot.  This is a small optimization.  Finally, remove
    the now-unused async argument of __gfn_to_pfn.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b7a08cd6f4a8..87fd74a04005 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -539,13 +539,13 @@ void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,
-		       bool write_fault, bool *writable);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
 pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
+pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
+			   bool *async, bool write_fault, bool *writable);
 
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);

commit 653f52c316a49c5ee2701bc13b15879f20790662
Author: Rik van Riel <riel@redhat.com>
Date:   Thu Apr 23 11:52:37 2015 -0400

    kvm,x86: load guest FPU context more eagerly
    
    Currently KVM will clear the FPU bits in CR0.TS in the VMCS, and trap to
    re-load them every time the guest accesses the FPU after a switch back into
    the guest from the host.
    
    This patch copies the x86 task switch semantics for FPU loading, with the
    FPU loaded eagerly after first use if the system uses eager fpu mode,
    or if the guest uses the FPU frequently.
    
    In the latter case, after loading the FPU for 255 times, the fpu_counter
    will roll over, and we will revert to loading the FPU on demand, until
    it has been established that the guest is still actively using the FPU.
    
    This mirrors the x86 task switch policy, which seems to work.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index efc16df1fc5d..b7a08cd6f4a8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -230,6 +230,7 @@ struct kvm_vcpu {
 
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
+	unsigned char fpu_counter;
 	wait_queue_head_t wq;
 	struct pid *pid;
 	int sigset_active;

commit 0097d12e504b3ce57b68810737ad6a5a64a98c68
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Apr 30 13:43:30 2015 +0200

    KVM: provide irq_unsafe kvm_guest_{enter|exit}
    
    Several kvm architectures disable interrupts before kvm_guest_enter.
    kvm_guest_enter then uses local_irq_save/restore to disable interrupts
    again or for the first time. Lets provide underscore versions of
    kvm_guest_{enter|exit} that assume being called locked.
    kvm_guest_enter now disables interrupts for the full function and
    thus we can remove the check for preemptible.
    
    This patch then adopts s390/kvm to use local_irq_disable/enable calls
    which are slighty cheaper that local_irq_save/restore and call these
    new functions.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ad45054309a0..efc16df1fc5d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -762,16 +762,10 @@ static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 }
 #endif
 
-static inline void kvm_guest_enter(void)
+/* must be called with irqs disabled */
+static inline void __kvm_guest_enter(void)
 {
-	unsigned long flags;
-
-	BUG_ON(preemptible());
-
-	local_irq_save(flags);
 	guest_enter();
-	local_irq_restore(flags);
-
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
 	 * is very similar to exiting to userspace from rcu point of view. In
@@ -783,12 +777,27 @@ static inline void kvm_guest_enter(void)
 		rcu_virt_note_context_switch(smp_processor_id());
 }
 
+/* must be called with irqs disabled */
+static inline void __kvm_guest_exit(void)
+{
+	guest_exit();
+}
+
+static inline void kvm_guest_enter(void)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	__kvm_guest_enter();
+	local_irq_restore(flags);
+}
+
 static inline void kvm_guest_exit(void)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
-	guest_exit();
+	__kvm_guest_exit();
 	local_irq_restore(flags);
 }
 

commit e95e7f627062be5e6ce971ce873e6234c91ffc50
Merge: 078838d56574 1524b745406a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 13:58:48 2015 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ changes from Ingo Molnar:
     "This tree adds full dynticks support to KVM guests (support the
      disabling of the timer tick on the guest).  The main missing piece was
      the recognition of guest execution as RCU extended quiescent state and
      related changes"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      kvm,rcu,nohz: use RCU extended quiescent state when running KVM guest
      context_tracking: Export context_tracking_user_enter/exit
      context_tracking: Run vtime_user_enter/exit only when state == CONTEXT_USER
      context_tracking: Add stub context_tracking_is_enabled
      context_tracking: Generalize context tracking APIs to support user and guest
      context_tracking: Rename context symbols to prepare for transition state
      ppc: Remove unused cpp symbols in kvm headers

commit 58d269d8cccc53643f1a0900cfc0940e85ec9691
Author: Nadav Amit <namit@cs.technion.ac.il>
Date:   Thu Apr 2 03:10:36 2015 +0300

    KVM: x86: BSP in MSR_IA32_APICBASE is writable
    
    After reset, the CPU can change the BSP, which will be used upon INIT.  Reset
    should return the BSP which QEMU asked for, and therefore handled accordingly.
    
    To quote: "If the MP protocol has completed and a BSP is chosen, subsequent
    INITs (either to a specific processor or system wide) do not cause the MP
    protocol to be repeated."
    [Intel SDM 8.4.2: MP Initialization Protocol Requirements and Restrictions]
    
    Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
    Message-Id: <1427933438-12782-3-git-send-email-namit@cs.technion.ac.il>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 27bd53b69080..82af5d0b996e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -982,11 +982,16 @@ static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
-static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
+static inline bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)
 {
 	return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
 }
 
+static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
+{
+	return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+}
+
 bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu);
 
 #else

commit bf0fb67cf957fc8ecfaaa2819b7d6a0f795e2ef2
Merge: 8999602d08a8 d44758c0dfc5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 7 18:09:20 2015 +0200

    Merge tag 'kvm-arm-for-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into 'kvm-next'
    
    KVM/ARM changes for v4.1:
    
    - fixes for live migration
    - irqfd support
    - kvm-io-bus & vgic rework to enable ioeventfd
    - page ageing for stage-2 translation
    - various cleanups

commit e32edf4fd0fa4897e12ca66118ab67bf257e16e4
Author: Nikolay Nikolaev <n.nikolaev@virtualopensystems.com>
Date:   Thu Mar 26 14:39:28 2015 +0000

    KVM: Redesign kvm_io_bus_ API to pass VCPU structure to the callbacks.
    
    This is needed in e.g. ARM vGIC emulation, where the MMIO handling
    depends on the VCPU that does the access.
    
    Signed-off-by: Nikolay Nikolaev <n.nikolaev@virtualopensystems.com>
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ae9c72012004..9605e46fce0b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -165,12 +165,12 @@ enum kvm_bus {
 	KVM_NR_BUSES
 };
 
-int kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
+int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val);
-int kvm_io_bus_write_cookie(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
-			    int len, const void *val, long cookie);
-int kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr, int len,
-		    void *val);
+int kvm_io_bus_write_cookie(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx,
+			    gpa_t addr, int len, const void *val, long cookie);
+int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
+		    int len, void *val);
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,

commit 01c94e64f5a6f298774bdbde435e577821119fc0
Author: Eric Auger <eric.auger@linaro.org>
Date:   Wed Mar 4 11:14:33 2015 +0100

    KVM: introduce kvm_arch_intc_initialized and use it in irqfd
    
    Introduce __KVM_HAVE_ARCH_INTC_INITIALIZED define and
    associated kvm_arch_intc_initialized function. This latter
    allows to test whether the virtual interrupt controller is initialized
    and ready to accept virtual IRQ injection. On some architectures,
    the virtual interrupt controller is dynamically instantiated, justifying
    that kind of check.
    
    The new function can now be used by irqfd to check whether the
    virtual interrupt controller is ready on KVM_IRQFD request. If not,
    KVM_IRQFD returns -EAGAIN.
    
    Signed-off-by: Eric Auger <eric.auger@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d12b2104d19b..ae9c72012004 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -700,6 +700,20 @@ static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 #endif
 }
 
+#ifdef __KVM_HAVE_ARCH_INTC_INITIALIZED
+/*
+ * returns true if the virtual interrupt controller is initialized and
+ * ready to accept virtual IRQ. On some architectures the virtual interrupt
+ * controller is dynamically instantiated and this is not always true.
+ */
+bool kvm_arch_intc_initialized(struct kvm *kvm);
+#else
+static inline bool kvm_arch_intc_initialized(struct kvm *kvm)
+{
+	return true;
+}
+#endif
+
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type);
 void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);

commit 548ef28449c0c06f92194c40ff0eaed248cb4b75
Author: Thomas Huth <thuth@linux.vnet.ibm.com>
Date:   Tue Feb 24 21:29:25 2015 +0100

    KVM: Get rid of kvm_kvfree()
    
    kvm_kvfree() provides exactly the same functionality as the
    new common kvfree() function - so let's simply replace the
    kvm function with the common function.
    
    Signed-off-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d12b2104d19b..0f574ebc82f4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -658,7 +658,6 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
 void *kvm_kvzalloc(unsigned long size);
-void kvm_kvfree(const void *addr);
 
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)

commit 126a6a542446f1a49b9f3c69237c87df3eb4e6e1
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 10 15:27:54 2015 -0500

    kvm,rcu,nohz: use RCU extended quiescent state when running KVM guest
    
    The host kernel is not doing anything while the CPU is executing
    a KVM guest VCPU, so it can be marked as being in an extended
    quiescent state, identical to that used when running user space
    code.
    
    The only exception to that rule is when the host handles an
    interrupt, which is already handled by the irq code, which
    calls rcu_irq_enter and rcu_irq_exit.
    
    The guest_enter and guest_exit functions already switch vtime
    accounting independent of context tracking. Leave those calls
    where they are, instead of moving them into the context tracking
    code.
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d12b2104d19b..cc8c61c5459c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -766,7 +766,8 @@ static inline void kvm_guest_enter(void)
 	 * one time slice). Lets treat guest mode as quiescent state, just like
 	 * we do with user-mode execution.
 	 */
-	rcu_virt_note_context_switch(smp_processor_id());
+	if (!context_tracking_cpu_is_enabled())
+		rcu_virt_note_context_switch(smp_processor_id());
 }
 
 static inline void kvm_guest_exit(void)

commit b9085bcbf5f43adf60533f9b635b2e7faeed0fe9
Merge: c7d7b9867155 6557bada461a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 13 09:55:09 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM update from Paolo Bonzini:
     "Fairly small update, but there are some interesting new features.
    
      Common:
         Optional support for adding a small amount of polling on each HLT
         instruction executed in the guest (or equivalent for other
         architectures).  This can improve latency up to 50% on some
         scenarios (e.g. O_DSYNC writes or TCP_RR netperf tests).  This
         also has to be enabled manually for now, but the plan is to
         auto-tune this in the future.
    
      ARM/ARM64:
         The highlights are support for GICv3 emulation and dirty page
         tracking
    
      s390:
         Several optimizations and bugfixes.  Also a first: a feature
         exposed by KVM (UUID and long guest name in /proc/sysinfo) before
         it is available in IBM's hypervisor! :)
    
      MIPS:
         Bugfixes.
    
      x86:
         Support for PML (page modification logging, a new feature in
         Broadwell Xeons that speeds up dirty page tracking), nested
         virtualization improvements (nested APICv---a nice optimization),
         usual round of emulation fixes.
    
         There is also a new option to reduce latency of the TSC deadline
         timer in the guest; this needs to be tuned manually.
    
         Some commits are common between this pull and Catalin's; I see you
         have already included his tree.
    
      Powerpc:
         Nothing yet.
    
         The KVM/PPC changes will come in through the PPC maintainers,
         because I haven't received them yet and I might end up being
         offline for some part of next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: ia64: drop kvm.h from installed user headers
      KVM: x86: fix build with !CONFIG_SMP
      KVM: x86: emulate: correct page fault error code for NoWrite instructions
      KVM: Disable compat ioctl for s390
      KVM: s390: add cpu model support
      KVM: s390: use facilities and cpu_id per KVM
      KVM: s390/CPACF: Choose crypto control block format
      s390/kernel: Update /proc/sysinfo file with Extended Name and UUID
      KVM: s390: reenable LPP facility
      KVM: s390: floating irqs: fix user triggerable endless loop
      kvm: add halt_poll_ns module parameter
      kvm: remove KVM_MMIO_SIZE
      KVM: MIPS: Don't leak FPU/DSP to guest
      KVM: MIPS: Disable HTW while in guest
      KVM: nVMX: Enable nested posted interrupt processing
      KVM: nVMX: Enable nested virtual interrupt delivery
      KVM: nVMX: Enable nested apic register virtualization
      KVM: nVMX: Make nested control MSRs per-cpu
      KVM: nVMX: Enable nested virtualize x2apic mode
      KVM: nVMX: Prepare for using hardware MSR bitmap
      ...

commit 0664e57ff0c68cbca012a45a38288fa277eb6795
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Feb 11 15:27:28 2015 -0800

    mm: gup: kvm use get_user_pages_unlocked
    
    Use the more generic get_user_pages_unlocked which has the additional
    benefit of passing FAULT_FLAG_ALLOW_RETRY at the very first page fault
    (which allows the first page fault in an unmapped area to be always able
    to block indefinitely by being allowed to release the mmap_sem).
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Andres Lagar-Cavilla <andreslc@google.com>
    Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 26f106022c88..d189ee098aa2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -200,17 +200,6 @@ int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 
-/*
- * Carry out a gup that requires IO. Allow the mm to relinquish the mmap
- * semaphore if the filemap/swap has to wait on a page lock. pagep == NULL
- * controls whether we retry the gup one more time to completion in that case.
- * Typically this is called after a FAULT_FLAG_RETRY_NOWAIT in the main tdp
- * handler.
- */
-int kvm_get_user_page_io(struct task_struct *tsk, struct mm_struct *mm,
-			 unsigned long addr, bool write_fault,
-			 struct page **pagep);
-
 enum {
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,

commit 1c2b364b225a5a93dbd1f317bd000d2fec2694be
Author: Tiejun Chen <tiejun.chen@intel.com>
Date:   Thu Feb 5 17:22:26 2015 +0800

    kvm: remove KVM_MMIO_SIZE
    
    After f78146b0f923, "KVM: Fix page-crossing MMIO", and
    87da7e66a405, "KVM: x86: fix vcpu->mmio_fragments overflow",
    actually KVM_MMIO_SIZE is gone.
    
    Signed-off-by: Tiejun Chen <tiejun.chen@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 32d057571bf6..8a82838034f1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -33,10 +33,6 @@
 
 #include <asm/kvm_host.h>
 
-#ifndef KVM_MMIO_SIZE
-#define KVM_MMIO_SIZE 8
-#endif
-
 /*
  * The bit 16 ~ bit 31 of kvm_memory_region::flags are internally used
  * in kvm, other bits are visible for userspace which are defined in

commit 3b0f1d01e501792d8d89ab4371bc9e8cd2a10032
Author: Kai Huang <kai.huang@linux.intel.com>
Date:   Wed Jan 28 10:54:23 2015 +0800

    KVM: Rename kvm_arch_mmu_write_protect_pt_masked to be more generic for log dirty
    
    We don't have to write protect guest memory for dirty logging if architecture
    supports hardware dirty logging, such as PML on VMX, so rename it to be more
    generic.
    
    Signed-off-by: Kai Huang <kai.huang@linux.intel.com>
    Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7d6719522f1f..32d057571bf6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -615,7 +615,7 @@ int kvm_get_dirty_log(struct kvm *kvm,
 int kvm_get_dirty_log_protect(struct kvm *kvm,
 			struct kvm_dirty_log *log, bool *is_dirty);
 
-void kvm_arch_mmu_write_protect_pt_masked(struct kvm *kvm,
+void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 					struct kvm_memory_slot *slot,
 					gfn_t gfn_offset,
 					unsigned long mask);

commit 8fff5e374a2f6047d1bb52288af7da119bc75765
Merge: 1c6007d59a20 0eb135ff9f19
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jan 23 14:33:36 2015 +0100

    Merge tag 'kvm-s390-next-20150122' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into kvm-next
    
    KVM: s390: fixes and features for kvm/next (3.20)
    
    1. Generic
    - sparse warning (make function static)
    - optimize locking
    - bugfixes for interrupt injection
    - fix MVPG addressing modes
    
    2. hrtimer/wakeup fun
    A recent change can cause KVM hangs if adjtime is used in the host.
    The hrtimer might wake up too early or too late. Too early is fatal
    as vcpu_block will see that the wakeup condition is not met and
    sleep again. This CPU might never wake up again.
    This series addresses this problem. adjclock slowing down the host
    clock will result in too late wakeups. This will require more work.
    In addition to that we also change the hrtimer from REALTIME to
    MONOTONIC to avoid similar problems with timedatectl set-time.
    
    3. sigp rework
    We will move all "slow" sigps to QEMU (protected with a capability that
    can be enabled) to avoid several races between concurrent SIGP orders.
    
    4. Optimize the shadow page table
    Provide an interface to announce the maximum guest size. The kernel
    will use that to make the pagetable 2,3,4 (or theoretically) 5 levels.
    
    5. Provide an interface to set the guest TOD
    We now use two vm attributes instead of two oneregs, as oneregs are
    vcpu ioctl and we don't want to call them from other threads.
    
    6. Protected key functions
    The real HMC allows to enable/disable protected key CPACF functions.
    Lets provide an implementation + an interface for QEMU to activate
    this the protected key instructions.

commit 31928aa5863e71535ee942f506ca9ac8ce1c4315
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Dec 4 15:47:07 2014 +0100

    KVM: remove unneeded return value of vcpu_postcreate
    
    The return value of kvm_arch_vcpu_postcreate is not checked in its
    caller.  This is okay, because only x86 provides vcpu_postcreate right
    now and it could only fail if vcpu_load failed.  But that is not
    possible during KVM_CREATE_VCPU (kvm_arch_vcpu_load is void, too), so
    just get rid of the unchecked return value.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 26f106022c88..a82432c710c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -652,7 +652,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
-int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 int kvm_arch_hardware_enable(void);

commit a0675c25d6392c2197b796a60c4a2a0138c86355
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Sat Jun 7 00:54:51 2014 +0200

    arm/arm64: KVM: add virtual GICv3 distributor emulation
    
    With everything separated and prepared, we implement a model of a
    GICv3 distributor and redistributors by using the existing framework
    to provide handler functions for each register group.
    
    Currently we limit the emulation to a model enforcing a single
    security state, with SRE==1 (forcing system register access) and
    ARE==1 (allowing more than 8 VCPUs).
    
    We share some of the functions provided for GICv2 emulation, but take
    the different ways of addressing (v)CPUs into account.
    Save and restore is currently not implemented.
    
    Similar to the split-off of the GICv2 specific code, the new emulation
    code goes into a new file (vgic-v3-emul.c).
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 25d7ce31a5d4..0ef2daa199d8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1052,6 +1052,7 @@ void kvm_unregister_device_ops(u32 type);
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
+extern struct kvm_device_ops kvm_arm_vgic_v3_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit ea2f83a7de9d0abbd145e37177905aab57fdb835
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Sun Oct 26 23:17:00 2014 +0000

    arm/arm64: KVM: move kvm_register_device_ops() into vGIC probing
    
    Currently we unconditionally register the GICv2 emulation device
    during the host's KVM initialization. Since with GICv3 support we
    may end up with only v2 or only v3 or both supported, we move the
    registration into the GIC probing function, where we will later know
    which combination is valid.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3b934cc94cc8..25d7ce31a5d4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1051,6 +1051,7 @@ void kvm_unregister_device_ops(u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
+extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit ba0513b5b8ffbcb0cc89e2f172c0bcb70497ba2e
Author: Mario Smarduch <m.smarduch@samsung.com>
Date:   Thu Jan 15 15:58:53 2015 -0800

    KVM: Add generic support for dirty page logging
    
    kvm_get_dirty_log() provides generic handling of dirty bitmap, currently reused
    by several architectures. Building on that we intrdoduce
    kvm_get_dirty_log_protect() adding write protection to mark these pages dirty
    for future write access, before next KVM_GET_DIRTY_LOG ioctl call from user
    space.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mario Smarduch <m.smarduch@samsung.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 26f106022c88..3b934cc94cc8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -611,6 +611,15 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 
 int kvm_get_dirty_log(struct kvm *kvm,
 			struct kvm_dirty_log *log, int *is_dirty);
+
+int kvm_get_dirty_log_protect(struct kvm *kvm,
+			struct kvm_dirty_log *log, bool *is_dirty);
+
+void kvm_arch_mmu_write_protect_pt_masked(struct kvm *kvm,
+					struct kvm_memory_slot *slot,
+					gfn_t gfn_offset,
+					unsigned long mask);
+
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				struct kvm_dirty_log *log);
 

commit 333bce5aac9e8cb7f6b27e0122a224d17be4dd5d
Merge: ab646f54f4fd 05971120fca4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Dec 15 13:06:40 2014 +0100

    Merge tag 'kvm-arm-for-3.19-take2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    Second round of changes for KVM for arm/arm64 for v3.19; fixes reboot
    problems, clarifies VCPU init, and fixes a regression concerning the
    VGIC init flow.
    
    Conflicts:
            arch/ia64/kvm/kvm-ia64.c [deleted in HEAD and modified in kvmarm]

commit 9c1a5d38780e652275aa55362dbee0d7e827e069
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Mon Dec 1 17:29:27 2014 +0000

    kvm: optimize GFN to memslot lookup with large slots amount
    
    Current linear search doesn't scale well when
    large amount of memslots is used and looked up slot
    is not in the beginning memslots array.
    Taking in account that memslots don't overlap, it's
    possible to switch sorting order of memslots array from
    'npages' to 'base_gfn' and use binary search for
    memslot lookup by GFN.
    
    As result of switching to binary search lookup times
    are reduced with large amount of memslots.
    
    Following is a table of search_memslot() cycles
    during WS2008R2 guest boot.
    
                             boot,          boot + ~10 min
                             mostly same    of using it,
                             slot lookup    randomized lookup
                    max      average        average
                    cycles   cycles         cycles
    
    13 slots      : 1450       28           30
    
    13 slots      : 1400       30           40
    binary search
    
    117 slots     : 13000      30           460
    
    117 slots     : 2000       35           180
    binary search
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1a371447fd45..193bca68372d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -354,6 +354,7 @@ struct kvm_memslots {
 	/* The mapping table from slot id to the index in memslots[]. */
 	short id_to_index[KVM_MEM_SLOTS_NUM];
 	atomic_t lru_slot;
+	int used_slots;
 };
 
 struct kvm {
@@ -791,19 +792,28 @@ static inline void kvm_guest_exit(void)
 static inline struct kvm_memory_slot *
 search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 {
+	int start = 0, end = slots->used_slots;
 	int slot = atomic_read(&slots->lru_slot);
-	struct kvm_memory_slot *memslot = &slots->memslots[slot];
-
-	if (gfn >= memslot->base_gfn &&
-	    gfn < memslot->base_gfn + memslot->npages)
-		return memslot;
-
-	kvm_for_each_memslot(memslot, slots)
-		if (gfn >= memslot->base_gfn &&
-		      gfn < memslot->base_gfn + memslot->npages) {
-			atomic_set(&slots->lru_slot, memslot - slots->memslots);
-			return memslot;
-		}
+	struct kvm_memory_slot *memslots = slots->memslots;
+
+	if (gfn >= memslots[slot].base_gfn &&
+	    gfn < memslots[slot].base_gfn + memslots[slot].npages)
+		return &memslots[slot];
+
+	while (start < end) {
+		slot = start + (end - start) / 2;
+
+		if (gfn >= memslots[slot].base_gfn)
+			end = slot;
+		else
+			start = slot + 1;
+	}
+
+	if (gfn >= memslots[start].base_gfn &&
+	    gfn < memslots[start].base_gfn + memslots[start].npages) {
+		atomic_set(&slots->lru_slot, start);
+		return &memslots[start];
+	}
 
 	return NULL;
 }

commit d4ae84a02bc65cec29608bc417a969fc2ec75449
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Mon Dec 1 17:29:25 2014 +0000

    kvm: search_memslots: add simple LRU memslot caching
    
    In typical guest boot workload only 2-3 memslots are used
    extensively, and at that it's mostly the same memslot
    lookup operation.
    
    Adding LRU cache improves average lookup time from
    46 to 28 cycles (~40%) for this workload.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 231dd9472226..1a371447fd45 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -353,6 +353,7 @@ struct kvm_memslots {
 	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 	/* The mapping table from slot id to the index in memslots[]. */
 	short id_to_index[KVM_MEM_SLOTS_NUM];
+	atomic_t lru_slot;
 };
 
 struct kvm {
@@ -790,12 +791,19 @@ static inline void kvm_guest_exit(void)
 static inline struct kvm_memory_slot *
 search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 {
-	struct kvm_memory_slot *memslot;
+	int slot = atomic_read(&slots->lru_slot);
+	struct kvm_memory_slot *memslot = &slots->memslots[slot];
+
+	if (gfn >= memslot->base_gfn &&
+	    gfn < memslot->base_gfn + memslot->npages)
+		return memslot;
 
 	kvm_for_each_memslot(memslot, slots)
 		if (gfn >= memslot->base_gfn &&
-		      gfn < memslot->base_gfn + memslot->npages)
+		      gfn < memslot->base_gfn + memslot->npages) {
+			atomic_set(&slots->lru_slot, memslot - slots->memslots);
 			return memslot;
+		}
 
 	return NULL;
 }

commit 1050dcda3052912984b26fb6d2695a3f41792000
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 17 14:58:51 2014 +0000

    kvm: add a memslot flag for incoherent memory regions
    
    Memory regions may be incoherent with the caches, typically when the
    guest has mapped a host system RAM backed memory region as uncached.
    Add a flag KVM_MEMSLOT_INCOHERENT so that we can tag these memslots
    and handle them appropriately when mapping them.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a6059bdf7b03..e4d8f705fecd 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -43,6 +43,7 @@
  * include/linux/kvm_h.
  */
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
+#define KVM_MEMSLOT_INCOHERENT	(1UL << 17)
 
 /* Two fragments for cross MMIO pages. */
 #define KVM_MAX_MMIO_FRAGMENTS	2

commit bf4bea8e9a9058319a19f8c2710a6f0ef2459983
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 10 08:33:56 2014 +0000

    kvm: fix kvm_is_mmio_pfn() and rename to kvm_is_reserved_pfn()
    
    This reverts commit 85c8555ff0 ("KVM: check for !is_zero_pfn() in
    kvm_is_mmio_pfn()") and renames the function to kvm_is_reserved_pfn.
    
    The problem being addressed by the patch above was that some ARM code
    based the memory mapping attributes of a pfn on the return value of
    kvm_is_mmio_pfn(), whose name indeed suggests that such pfns should
    be mapped as device memory.
    
    However, kvm_is_mmio_pfn() doesn't do quite what it says on the tin,
    and the existing non-ARM users were already using it in a way which
    suggests that its name should probably have been 'kvm_is_reserved_pfn'
    from the beginning, e.g., whether or not to call get_page/put_page on
    it etc. This means that returning false for the zero page is a mistake
    and the patch above should be reverted.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ea53b04993f2..a6059bdf7b03 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -703,7 +703,7 @@ void kvm_arch_sync_events(struct kvm *kvm);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
-bool kvm_is_mmio_pfn(pfn_t pfn);
+bool kvm_is_reserved_pfn(pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;

commit c9eab58f6466cef3d9cd760a96e4de5e060e5195
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Nov 24 15:27:17 2014 +0100

    KVM: x86: move device assignment out of kvm_host.h
    
    Create a new header, and hide the device assignment functions there.
    Move struct kvm_assigned_dev_kernel to assigned-dev.c by modifying
    arch/x86/kvm/iommu.c to take a PCI device struct.
    
    Based on a patch by Radim Krcmar <rkrcmark@redhat.com>.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index aa56894ce839..231dd9472226 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -718,31 +718,6 @@ struct kvm_irq_ack_notifier {
 	void (*irq_acked)(struct kvm_irq_ack_notifier *kian);
 };
 
-struct kvm_assigned_dev_kernel {
-	struct kvm_irq_ack_notifier ack_notifier;
-	struct list_head list;
-	int assigned_dev_id;
-	int host_segnr;
-	int host_busnr;
-	int host_devfn;
-	unsigned int entries_nr;
-	int host_irq;
-	bool host_irq_disabled;
-	bool pci_2_3;
-	struct msix_entry *host_msix_entries;
-	int guest_irq;
-	struct msix_entry *guest_msix_entries;
-	unsigned long irq_requested_type;
-	int irq_source_id;
-	int flags;
-	struct pci_dev *dev;
-	struct kvm *kvm;
-	spinlock_t intx_lock;
-	spinlock_t intx_mask_lock;
-	char irq_name[32];
-	struct pci_saved_state *pci_saved_state;
-};
-
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi);
 int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin);
@@ -764,10 +739,6 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 #ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
-int kvm_assign_device(struct kvm *kvm,
-		      struct kvm_assigned_dev_kernel *assigned_dev);
-int kvm_deassign_device(struct kvm *kvm,
-			struct kvm_assigned_dev_kernel *assigned_dev);
 #else
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      struct kvm_memory_slot *slot)

commit c274e03af70544506cd7214fcc2d4c4376c2c6f4
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Fri Nov 21 22:21:50 2014 +0100

    kvm: x86: move assigned-dev.c and iommu.c to arch/x86/
    
    Now that ia64 is gone, we can hide deprecated device assignment in x86.
    
    Notable changes:
     - kvm_vm_ioctl_assigned_device() was moved to x86/kvm_arch_vm_ioctl()
    
    The easy parts were removed from generic kvm code, remaining
     - kvm_iommu_(un)map_pages() would require new code to be moved
     - struct kvm_assigned_dev_kernel depends on struct kvm_irq_ack_notifier
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ded64cb3a081..aa56894ce839 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -764,8 +764,6 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 #ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
-int kvm_iommu_map_guest(struct kvm *kvm);
-int kvm_iommu_unmap_guest(struct kvm *kvm);
 int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev);
 int kvm_deassign_device(struct kvm *kvm,
@@ -781,11 +779,6 @@ static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 					 struct kvm_memory_slot *slot)
 {
 }
-
-static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
-{
-	return 0;
-}
 #endif
 
 static inline void kvm_guest_enter(void)
@@ -1005,25 +998,6 @@ static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
 
 #endif
 
-#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
-
-long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
-				  unsigned long arg);
-
-void kvm_free_all_assigned_devices(struct kvm *kvm);
-
-#else
-
-static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
-						unsigned long arg)
-{
-	return -ENOTTY;
-}
-
-static inline void kvm_free_all_assigned_devices(struct kvm *kvm) {}
-
-#endif
-
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	set_bit(req, &vcpu->requests);

commit 6ef768fac9dfe3404d3fdc09909ea203a88f2f38
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 20 13:45:31 2014 +0100

    kvm: x86: move ioapic.c and irq_comm.c back to arch/x86/
    
    ia64 does not need them anymore.  Ack notifiers become x86-specific
    too.
    
    Suggested-by: Gleb Natapov <gleb@kernel.org>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ea53b04993f2..ded64cb3a081 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -395,7 +395,6 @@ struct kvm {
 	 * Update side is protected by irq_lock.
 	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
-	struct hlist_head mask_notifier_list;
 #endif
 #ifdef CONFIG_HAVE_KVM_IRQFD
 	struct hlist_head irq_ack_notifier_list;
@@ -447,6 +446,14 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
+#ifdef __KVM_HAVE_IOAPIC
+void kvm_vcpu_request_scan_ioapic(struct kvm *kvm);
+#else
+static inline void kvm_vcpu_request_scan_ioapic(struct kvm *kvm)
+{
+}
+#endif
+
 #ifdef CONFIG_HAVE_KVM_IRQFD
 int kvm_irqfd_init(void);
 void kvm_irqfd_exit(void);
@@ -736,19 +743,6 @@ struct kvm_assigned_dev_kernel {
 	struct pci_saved_state *pci_saved_state;
 };
 
-struct kvm_irq_mask_notifier {
-	void (*func)(struct kvm_irq_mask_notifier *kimn, bool masked);
-	int irq;
-	struct hlist_node link;
-};
-
-void kvm_register_irq_mask_notifier(struct kvm *kvm, int irq,
-				    struct kvm_irq_mask_notifier *kimn);
-void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
-				      struct kvm_irq_mask_notifier *kimn);
-void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
-			     bool mask);
-
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi);
 int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin);

commit 571ee1b6859869a09ed718d390aac2b9414646a2
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Thu Oct 9 18:30:08 2014 +0800

    kvm: vfio: fix unregister kvm_device_ops of vfio
    
    After commit 80ce163 (KVM: VFIO: register kvm_device_ops dynamically),
    kvm_device_ops of vfio can be registered dynamically. Commit 3c3c29fd
    (kvm-vfio: do not use module_init) move the dynamic register invoked by
    kvm_init in order to fix broke unloading of the kvm module. However,
    kvm_device_ops of vfio is unregistered after rmmod kvm-intel module
    which lead to device type collision detection warning after kvm-intel
    module reinsmod.
    
        WARNING: CPU: 1 PID: 10358 at /root/cathy/kvm/arch/x86/kvm/../../../virt/kvm/kvm_main.c:3289 kvm_init+0x234/0x282 [kvm]()
        Modules linked in: kvm_intel(O+) kvm(O) nfsv3 nfs_acl auth_rpcgss oid_registry nfsv4 dns_resolver nfs fscache lockd sunrpc pci_stub bridge stp llc autofs4 8021q cpufreq_ondemand ipv6 joydev microcode pcspkr igb i2c_algo_bit ehci_pci ehci_hcd e1000e i2c_i801 ixgbe ptp pps_core hwmon mdio tpm_tis tpm ipmi_si ipmi_msghandler acpi_cpufreq isci libsas scsi_transport_sas button dm_mirror dm_region_hash dm_log dm_mod [last unloaded: kvm_intel]
        CPU: 1 PID: 10358 Comm: insmod Tainted: G        W  O   3.17.0-rc1 #2
        Hardware name: Intel Corporation S2600CP/S2600CP, BIOS RMLSDP.86I.00.29.D696.1311111329 11/11/2013
         0000000000000cd9 ffff880ff08cfd18 ffffffff814a61d9 0000000000000cd9
         0000000000000000 ffff880ff08cfd58 ffffffff810417b7 ffff880ff08cfd48
         ffffffffa045bcac ffffffffa049c420 0000000000000040 00000000000000ff
        Call Trace:
         [<ffffffff814a61d9>] dump_stack+0x49/0x60
         [<ffffffff810417b7>] warn_slowpath_common+0x7c/0x96
         [<ffffffffa045bcac>] ? kvm_init+0x234/0x282 [kvm]
         [<ffffffff810417e6>] warn_slowpath_null+0x15/0x17
         [<ffffffffa045bcac>] kvm_init+0x234/0x282 [kvm]
         [<ffffffffa016e995>] vmx_init+0x1bf/0x42a [kvm_intel]
         [<ffffffffa016e7d6>] ? vmx_check_processor_compat+0x64/0x64 [kvm_intel]
         [<ffffffff810002ab>] do_one_initcall+0xe3/0x170
         [<ffffffff811168a9>] ? __vunmap+0xad/0xb8
         [<ffffffff8109c58f>] do_init_module+0x2b/0x174
         [<ffffffff8109d414>] load_module+0x43e/0x569
         [<ffffffff8109c6d8>] ? do_init_module+0x174/0x174
         [<ffffffff8109c75a>] ? copy_module_from_user+0x39/0x82
         [<ffffffff8109b7dd>] ? module_sect_show+0x20/0x20
         [<ffffffff8109d65f>] SyS_init_module+0x54/0x81
         [<ffffffff814a9a12>] system_call_fastpath+0x16/0x1b
        ---[ end trace 0626f4a3ddea56f3 ]---
    
    The bug can be reproduced by:
    
        rmmod kvm_intel.ko
        insmod kvm_intel.ko
    
    without rmmod/insmod kvm.ko
    This patch fixes the bug by unregistering kvm_device_ops of vfio when the
    kvm-intel module is removed.
    
    Reported-by: Liu Rongrong <rongrongx.liu@intel.com>
    Fixes: 3c3c29fd0d7cddc32862c350d0700ce69953e3bd
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 28be31f49250..ea53b04993f2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1080,6 +1080,7 @@ void kvm_device_get(struct kvm_device *dev);
 void kvm_device_put(struct kvm_device *dev);
 struct kvm_device *kvm_device_from_filp(struct file *filp);
 int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
+void kvm_unregister_device_ops(u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;

commit e77d99d4a4ec761ad061f1ec890c71040a92efe3
Merge: bb0ca6acd466 0496daa5cf99
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Sep 27 11:03:33 2014 +0200

    Merge tag 'kvm-arm-for-3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-next
    
    Changes for KVM for arm/arm64 for 3.18
    
    This includes a bunch of changes:
     - Support read-only memory slots on arm/arm64
     - Various changes to fix Sparse warnings
     - Correctly detect write vs. read Stage-2 faults
     - Various VGIC cleanups and fixes
     - Dynamic VGIC data strcuture sizing
     - Fix SGI set_clear_pend offset bug
     - Fix VTTBR_BADDR Mask
     - Correctly report the FSC on Stage-2 faults
    
    Conflicts:
            virt/kvm/eventfd.c
            [duplicate, different patch where the kvm-arm version broke x86.
             The kvm tree instead has the right one]

commit 4256f43f9fab91e1c17b5846a240cf4b66a768a8
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:54 2014 +0800

    kvm: x86: Add request bit to reload APIC access page address
    
    Currently, the APIC access page is pinned by KVM for the entire life
    of the guest.  We want to make it migratable in order to make memory
    hot-unplug available for machines that run KVM.
    
    This patch prepares to handle this in generic code, through a new
    request bit (that will be set by the MMU notifier) and a new hook
    that is called whenever the request bit is processed.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 12c591fc2571..d594f9f34429 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -136,6 +136,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE 22
 #define KVM_REQ_ENABLE_IBS        23
 #define KVM_REQ_DISABLE_IBS       24
+#define KVM_REQ_APIC_PAGE_RELOAD  25
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 445b8236959bfe624a5aa9bce89f44a3bec9b2b1
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:55 2014 +0800

    kvm: Rename make_all_cpus_request() to kvm_make_all_cpus_request() and make it non-static
    
    Different architectures need different requests, and in fact we
    will use this function in architecture-specific code later. This
    will be outside kvm_main.c, so make it non-static and rename it to
    kvm_make_all_cpus_request().
    
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 45aaeb3360c9..12c591fc2571 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -586,6 +586,7 @@ void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
 void kvm_make_mclock_inprogress_request(struct kvm *kvm);
 void kvm_make_scan_ioapic_request(struct kvm *kvm);
+bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);

commit 234b239bea395316d7f78018c672f4a88b3cdf0d
Author: Andres Lagar-Cavilla <andreslc@google.com>
Date:   Wed Sep 17 10:51:48 2014 -0700

    kvm: Faults which trigger IO release the mmap_sem
    
    When KVM handles a tdp fault it uses FOLL_NOWAIT. If the guest memory
    has been swapped out or is behind a filemap, this will trigger async
    readahead and return immediately. The rationale is that KVM will kick
    back the guest with an "async page fault" and allow for some other
    guest process to take over.
    
    If async PFs are enabled the fault is retried asap from an async
    workqueue. If not, it's retried immediately in the same code path. In
    either case the retry will not relinquish the mmap semaphore and will
    block on the IO. This is a bad thing, as other mmap semaphore users
    now stall as a function of swap or filemap latency.
    
    This patch ensures both the regular and async PF path re-enter the
    fault allowing for the mmap semaphore to be relinquished in the case
    of IO wait.
    
    Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Andres Lagar-Cavilla <andreslc@google.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d44a2d640551..45aaeb3360c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -198,6 +198,17 @@ int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 
+/*
+ * Carry out a gup that requires IO. Allow the mm to relinquish the mmap
+ * semaphore if the filemap/swap has to wait on a page lock. pagep == NULL
+ * controls whether we retry the gup one more time to completion in that case.
+ * Typically this is called after a FAULT_FLAG_RETRY_NOWAIT in the main tdp
+ * handler.
+ */
+int kvm_get_user_page_io(struct task_struct *tsk, struct mm_struct *mm,
+			 unsigned long addr, bool write_fault,
+			 struct page **pagep);
+
 enum {
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,

commit a875dafcf9b6b266c855e1f9b0aa060ef585d38a
Merge: 0ba09511ddc3 f51770ed465e
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Sep 18 18:15:32 2014 -0700

    Merge remote-tracking branch 'kvm/next' into queue
    
    Conflicts:
            arch/arm64/include/asm/kvm_host.h
            virt/kvm/arm/vgic.c

commit 80ce1639727e9d38729c34f162378508c307ca25
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 2 10:27:36 2014 +0100

    KVM: VFIO: register kvm_device_ops dynamically
    
    Now that we have a dynamic means to register kvm_device_ops, use that
    for the VFIO kvm device, instead of relying on the static table.
    
    This is achieved by a module_init call to register the ops with KVM.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Alex Williamson <Alex.Williamson@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7ef088bec715..d44a2d640551 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1068,7 +1068,6 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
-extern struct kvm_device_ops kvm_vfio_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit 84877d93336de21a6251db00b841468a83c65906
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Tue Sep 2 10:27:35 2014 +0100

    KVM: s390: register flic ops dynamically
    
    Using the new kvm_register_device_ops() interface makes us get rid of
    an #ifdef in common code.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 601d321f96e7..7ef088bec715 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1069,7 +1069,6 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_vfio_ops;
-extern struct kvm_device_ops kvm_flic_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit c06a841bf36340e9e917ce60d11a6425ac85d0bd
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 2 10:27:34 2014 +0100

    KVM: ARM: vgic: register kvm_device_ops dynamically
    
    Now that we have a dynamic means to register kvm_device_ops, use that
    for the ARM VGIC, instead of relying on the static table.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b6e954742951..601d321f96e7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1069,7 +1069,6 @@ int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_vfio_ops;
-extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
 extern struct kvm_device_ops kvm_flic_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT

commit d60eacb07053142bfb9b41582074a89a790a9d46
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 2 10:27:33 2014 +0100

    KVM: device: add simple registration mechanism for kvm_device_ops
    
    kvm_ioctl_create_device currently has knowledge of all the device types
    and their associated ops. This is fairly inflexible when adding support
    for new in-kernel device emulations, so move what we currently have out
    into a table, which can support dynamic registration of ops by new
    drivers for virtual hardware.
    
    Cc: Alex Williamson <Alex.Williamson@redhat.com>
    Cc: Alex Graf <agraf@suse.de>
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e098dce179df..b6e954742951 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1064,6 +1064,7 @@ struct kvm_device_ops {
 void kvm_device_get(struct kvm_device *dev);
 void kvm_device_put(struct kvm_device *dev);
 struct kvm_device *kvm_device_from_filp(struct file *filp);
+int kvm_register_device_ops(struct kvm_device_ops *ops, u32 type);
 
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim KrÄmÃ¡Å™ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e1cb915a1096..e098dce179df 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -630,8 +630,8 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
-int kvm_arch_hardware_enable(void *garbage);
-void kvm_arch_hardware_disable(void *garbage);
+int kvm_arch_hardware_enable(void);
+void kvm_arch_hardware_disable(void);
 int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
 void kvm_arch_check_processor_compat(void *rtn);

commit 656473003bc7e056c3bbd4a4d9832dad01e86f76
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Aug 29 14:01:17 2014 +0200

    KVM: forward declare structs in kvm_types.h
    
    Opaque KVM structs are useful for prototypes in asm/kvm_host.h, to avoid
    "'struct foo' declared inside parameter list" warnings (and consequent
    breakage due to conflicting types).
    
    Move them from individual files to a generic place in linux/kvm_types.h.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ebd723676633..e1cb915a1096 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -140,8 +140,6 @@ static inline bool is_error_page(struct page *page)
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
 
-struct kvm;
-struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
 extern spinlock_t kvm_lock;
@@ -325,8 +323,6 @@ struct kvm_kernel_irq_routing_entry {
 	struct hlist_node link;
 };
 
-struct kvm_irq_routing_table;
-
 #ifndef KVM_PRIVATE_MEM_SLOTS
 #define KVM_PRIVATE_MEM_SLOTS 0
 #endif
@@ -1036,8 +1032,6 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 
 extern bool kvm_rebooting;
 
-struct kvm_device_ops;
-
 struct kvm_device {
 	struct kvm_device_ops *ops;
 	struct kvm *kvm;

commit 64d831269ccbca1fc6d739a0f3c8aa24afb43a5e
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Aug 19 12:15:00 2014 +0200

    KVM: Introduce gfn_to_hva_memslot_prot
    
    To support read-only memory regions on arm and arm64, we have a need to
    resolve a gfn to an hva given a pointer to a memslot to avoid looping
    through the memslots twice and to reuse the hva error checking of
    gfn_to_hva_prot(), add a new gfn_to_hva_memslot_prot() function and
    refactor gfn_to_hva_prot() to use this function.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ebd723676633..6d8a658ec174 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -528,6 +528,8 @@ struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva_prot(struct kvm *kvm, gfn_t gfn, bool *writable);
 unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
+unsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot, gfn_t gfn,
+				      bool *writable);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);

commit e790d9ef6405633b007339d746b709aed43a928d
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Aug 21 18:08:05 2014 +0200

    KVM: add kvm_arch_sched_in
    
    Introduce preempt notifiers for architecture specific code.
    Advantage over creating a new notifier in every arch is slightly simpler
    code and guaranteed call order with respect to kvm_sched_in.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a4c33b34fe3f..ebd723676633 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -624,6 +624,8 @@ void kvm_arch_exit(void);
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
 
+void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);
+
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);

commit c77dcacb397519b6ade8f08201a4a90a7f4f751e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 6 14:24:45 2014 +0200

    KVM: Move more code under CONFIG_HAVE_KVM_IRQFD
    
    Commits e4d57e1ee1ab (KVM: Move irq notifier implementation into
    eventfd.c, 2014-06-30) included the irq notifier code unconditionally
    in eventfd.c, while it was under CONFIG_HAVE_KVM_IRQCHIP before.
    
    Similarly, commit 297e21053a52 (KVM: Give IRQFD its own separate enabling
    Kconfig option, 2014-06-30) moved code from CONFIG_HAVE_IRQ_ROUTING
    to CONFIG_HAVE_KVM_IRQFD but forgot to move the pieces that used to be
    under CONFIG_HAVE_KVM_IRQCHIP.
    
    Together, this broke compilation without CONFIG_KVM_XICS.  Fix by adding
    or changing the #ifdefs so that they point at CONFIG_HAVE_KVM_IRQFD.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8593d2e61cbf..a4c33b34fe3f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -388,6 +388,8 @@ struct kvm {
 	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
 	struct hlist_head mask_notifier_list;
+#endif
+#ifdef CONFIG_HAVE_KVM_IRQFD
 	struct hlist_head irq_ack_notifier_list;
 #endif
 

commit 297e21053a52f060944e9f0de4c64fad9bcd72fc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 30 20:51:13 2014 +1000

    KVM: Give IRQFD its own separate enabling Kconfig option
    
    Currently, the IRQFD code is conditional on CONFIG_HAVE_KVM_IRQ_ROUTING.
    So that we can have the IRQFD code compiled in without having the
    IRQ routing code, this creates a new CONFIG_HAVE_KVM_IRQFD, makes
    the IRQFD code conditional on it instead of CONFIG_HAVE_KVM_IRQ_ROUTING,
    and makes all the platforms that currently select HAVE_KVM_IRQ_ROUTING
    also select HAVE_KVM_IRQFD.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Eric Auger <eric.auger@linaro.org>
    Tested-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ddd33e1aeee1..8593d2e61cbf 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -437,7 +437,7 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
-#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
+#ifdef CONFIG_HAVE_KVM_IRQFD
 int kvm_irqfd_init(void);
 void kvm_irqfd_exit(void);
 #else
@@ -932,20 +932,20 @@ int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue);
 void kvm_free_irq_routing(struct kvm *kvm);
 
-int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi);
-
 #else
 
 static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 
 #endif
 
+int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi);
+
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 
 void kvm_eventfd_init(struct kvm *kvm);
 int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
 
-#ifdef CONFIG_HAVE_KVM_IRQCHIP
+#ifdef CONFIG_HAVE_KVM_IRQFD
 int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args);
 void kvm_irqfd_release(struct kvm *kvm);
 void kvm_irq_routing_update(struct kvm *);

commit 9957c86d659a4d5a2bed25ccbd3bfc9c3f25e658
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 30 20:51:11 2014 +1000

    KVM: Move all accesses to kvm::irq_routing into irqchip.c
    
    Now that struct _irqfd does not keep a reference to storage pointed
    to by the irq_routing field of struct kvm, we can move the statement
    that updates it out from under the irqfds.lock and put it in
    kvm_set_irq_routing() instead.  That means we then have to take a
    srcu_read_lock on kvm->irq_srcu around the irqfd_update call in
    kvm_irqfd_assign(), since holding the kvm->irqfds.lock no longer
    ensures that that the routing can't change.
    
    Combined with changing kvm_irq_map_gsi() and kvm_irq_map_chip_pin()
    to take a struct kvm * argument instead of the pointer to the routing
    table, this allows us to to move all references to kvm->irq_routing
    into irqchip.c.  That in turn allows us to move the definition of the
    kvm_irq_routing_table struct into irqchip.c as well.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Eric Auger <eric.auger@linaro.org>
    Tested-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4956149e962a..ddd33e1aeee1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -325,24 +325,7 @@ struct kvm_kernel_irq_routing_entry {
 	struct hlist_node link;
 };
 
-#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
-
-struct kvm_irq_routing_table {
-	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
-	struct kvm_kernel_irq_routing_entry *rt_entries;
-	u32 nr_rt_entries;
-	/*
-	 * Array indexed by gsi. Each entry contains list of irq chips
-	 * the gsi is connected to.
-	 */
-	struct hlist_head map[0];
-};
-
-#else
-
-struct kvm_irq_routing_table {};
-
-#endif
+struct kvm_irq_routing_table;
 
 #ifndef KVM_PRIVATE_MEM_SLOTS
 #define KVM_PRIVATE_MEM_SLOTS 0
@@ -401,8 +384,7 @@ struct kvm {
 	struct mutex irq_lock;
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 	/*
-	 * Update side is protected by irq_lock and,
-	 * if configured, irqfds.lock.
+	 * Update side is protected by irq_lock.
 	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
 	struct hlist_head mask_notifier_list;
@@ -752,10 +734,9 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 			     bool mask);
 
-int kvm_irq_map_gsi(struct kvm_kernel_irq_routing_entry *entries,
-		    struct kvm_irq_routing_table *irq_rt, int gsi);
-int kvm_irq_map_chip_pin(struct kvm_irq_routing_table *irq_rt,
-			 unsigned irqchip, unsigned pin);
+int kvm_irq_map_gsi(struct kvm *kvm,
+		    struct kvm_kernel_irq_routing_entry *entries, int gsi);
+int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin);
 
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status);
@@ -967,7 +948,7 @@ int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args);
 void kvm_irqfd_release(struct kvm *kvm);
-void kvm_irq_routing_update(struct kvm *, struct kvm_irq_routing_table *);
+void kvm_irq_routing_update(struct kvm *);
 #else
 static inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -989,10 +970,8 @@ static inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 static inline void kvm_irqfd_release(struct kvm *kvm) {}
 
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
-static inline void kvm_irq_routing_update(struct kvm *kvm,
-					  struct kvm_irq_routing_table *irq_rt)
+static inline void kvm_irq_routing_update(struct kvm *kvm)
 {
-	rcu_assign_pointer(kvm->irq_routing, irq_rt);
 }
 #endif
 

commit 8ba918d488caded2c4368b0b922eb905fe3bb101
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 30 20:51:10 2014 +1000

    KVM: irqchip: Provide and use accessors for irq routing table
    
    This provides accessor functions for the KVM interrupt mappings, in
    order to reduce the amount of code that accesses the fields of the
    kvm_irq_routing_table struct, and restrict that code to one file,
    virt/kvm/irqchip.c.  The new functions are kvm_irq_map_gsi(), which
    maps from a global interrupt number to a set of IRQ routing entries,
    and kvm_irq_map_chip_pin, which maps from IRQ chip and pin numbers to
    a global interrupt number.
    
    This also moves the update of kvm_irq_routing_table::chip[][]
    into irqchip.c, out of the various kvm_set_routing_entry
    implementations.  That means that none of the kvm_set_routing_entry
    implementations need the kvm_irq_routing_table argument anymore,
    so this removes it.
    
    This does not change any locking or data lifetime rules.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Eric Auger <eric.auger@linaro.org>
    Tested-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5065b953e6e8..4956149e962a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -752,6 +752,11 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 			     bool mask);
 
+int kvm_irq_map_gsi(struct kvm_kernel_irq_routing_entry *entries,
+		    struct kvm_irq_routing_table *irq_rt, int gsi);
+int kvm_irq_map_chip_pin(struct kvm_irq_routing_table *irq_rt,
+			 unsigned irqchip, unsigned pin);
+
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status);
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
@@ -942,8 +947,7 @@ int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,
 			unsigned flags);
-int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
-			  struct kvm_kernel_irq_routing_entry *e,
+int kvm_set_routing_entry(struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue);
 void kvm_free_irq_routing(struct kvm *kvm);
 

commit 784aa3d7fb6f729c06d5836c9d9569f58e4d05ae
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 14 18:27:35 2014 +0200

    KVM: Rename and add argument to check_extension
    
    In preparation to make the check_extension function available to VM scope
    we add a struct kvm * argument to the function header and rename the function
    accordingly. It will still be called from the /dev/kvm fd, but with a NULL
    argument for struct kvm *.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ec4e3bd83d47..5065b953e6e8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -602,7 +602,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg);
 int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);
 
-int kvm_dev_ioctl_check_extension(long ext);
+int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 
 int kvm_get_dirty_log(struct kvm *kvm,
 			struct kvm_dirty_log *log, int *is_dirty);

commit b2e09f633a3994ee97fa6bc734b533d9c8e6ea0f
Merge: 3737a1276163 535560d841b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:42:15 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more scheduler updates from Ingo Molnar:
     "Second round of scheduler changes:
       - try-to-wakeup and IPI reduction speedups, from Andy Lutomirski
       - continued power scheduling cleanups and refactorings, from Nicolas
         Pitre
       - misc fixes and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/deadline: Delete extraneous extern for to_ratio()
      sched/idle: Optimize try-to-wake-up IPI
      sched/idle: Simplify wake_up_idle_cpu()
      sched/idle: Clear polling before descheduling the idle thread
      sched, trace: Add a tracepoint for IPI-less remote wakeups
      cpuidle: Set polling in poll_idle
      sched: Remove redundant assignment to "rt_rq" in update_curr_rt(...)
      sched: Rename capacity related flags
      sched: Final power vs. capacity cleanups
      sched: Remove remaining dubious usage of "power"
      sched: Let 'struct sched_group_power' care about CPU capacity
      sched/fair: Disambiguate existing/remaining "capacity" usage
      sched/fair: Change "has_capacity" to "has_free_capacity"
      sched/fair: Remove "power" from 'struct numa_stats'
      sched: Fix signedness bug in yield_to()
      sched/fair: Use time_after() in record_wakee()
      sched/balancing: Reduce the rate of needless idle load balancing
      sched/fair: Fix unlocked reads of some cfs_b->quota/period

commit fa93384f40deeb294fd29f2fdcadbd0ebc2dedf1
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri May 23 13:20:42 2014 +0300

    sched: Fix signedness bug in yield_to()
    
    yield_to() is supposed to return -ESRCH if there is no task to
    yield to, but because the type is bool that is the same as returning
    true.
    
    The only place I see which cares is kvm_vcpu_on_spin().
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Raghavendra <raghavendra.kt@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: kvm@vger.kernel.org
    Link: http://lkml.kernel.org/r/20140523102042.GA7267@mwanda
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7d21cf9f4380..3c4bcf146159 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -584,7 +584,7 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
-bool kvm_vcpu_yield_to(struct kvm_vcpu *target);
+int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);

commit 719d93cd5f5c5c8775b7a38192069e8e1d1ac46e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Jan 16 13:44:20 2014 +0100

    kvm/irqchip: Speed up KVM_SET_GSI_ROUTING
    
    When starting lots of dataplane devices the bootup takes very long on
    Christian's s390 with irqfd patches. With larger setups he is even
    able to trigger some timeouts in some components.  Turns out that the
    KVM_SET_GSI_ROUTING ioctl takes very long (strace claims up to 0.1 sec)
    when having multiple CPUs.  This is caused by the  synchronize_rcu and
    the HZ=100 of s390.  By changing the code to use a private srcu we can
    speed things up.  This patch reduces the boot time till mounting root
    from 8 to 2 seconds on my s390 guest with 100 disks.
    
    Uses of hlist_for_each_entry_rcu, hlist_add_head_rcu, hlist_del_init_rcu
    are fine because they do not have lockdep checks (hlist_for_each_entry_rcu
    uses rcu_dereference_raw rather than rcu_dereference, and write-sides
    do not do rcu lockdep at all).
    
    Note that we're hardly relying on the "sleepable" part of srcu.  We just
    want SRCU's faster detection of grace periods.
    
    Testing was done by Andrew Theurer using netperf tests STREAM, MAERTS
    and RR.  The difference between results "before" and "after" the patch
    has mean -0.2% and standard deviation 0.6%.  Using a paired t-test on the
    data points says that there is a 2.5% probability that the patch is the
    cause of the performance difference (rather than a random fluctuation).
    
    (Restricting the t-test to RR, which is the most likely to be affected,
    changes the numbers to respectively -0.3% mean, 0.7% stdev, and 8%
    probability that the numbers actually say something about the patch.
    The probability increases mostly because there are fewer data points).
    
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com> # s390
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1e125b055327..970c68197c69 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -370,6 +370,7 @@ struct kvm {
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots;
 	struct srcu_struct srcu;
+	struct srcu_struct irq_srcu;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
 	u32 bsp_vcpu_id;
 #endif

commit 8ad357551797b1edc184fb9f6a4f80a6fa626459
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Mar 14 11:00:21 2014 +0100

    KVM: s390: enable IBS for single running VCPUs
    
    This patch enables the IBS facility when a single VCPU is running.
    The facility is dynamically turned on/off as soon as other VCPUs
    enter/leave the stopped state.
    
    When this facility is operating, some instructions can be executed
    faster for single-cpu guests.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Reviewed-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 820fc2e1d9df..1e125b055327 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -134,6 +134,8 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_EPR_EXIT          20
 #define KVM_REQ_SCAN_IOAPIC       21
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE 22
+#define KVM_REQ_ENABLE_IBS        23
+#define KVM_REQ_DISABLE_IBS       24
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit a086f6a1ebc9d8d2d028b99e779ce0dbd9691dea
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Apr 17 17:06:12 2014 +0800

    Revert "KVM: Simplify kvm->tlbs_dirty handling"
    
    This reverts commit 5befdc385ddb2d5ae8995ad89004529a3acf58fc.
    
    Since we will allow flush tlb out of mmu-lock in the later
    patch
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 32d263f683dc..820fc2e1d9df 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -411,9 +411,7 @@ struct kvm {
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 #endif
-	/* Protected by mmu_lock */
-	bool tlbs_dirty;
-
+	long tlbs_dirty;
 	struct list_head devices;
 };
 

commit 63b5cf04f4ede6046cc8771789e5ac40529f30e8
Merge: 5c7411e29374 e325fe69aa37
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Apr 22 10:51:06 2014 -0300

    Merge tag 'kvm-s390-20140422' of git://git.kernel.org/pub/scm/linux/kernel/git/kvms390/linux into queue
    
    Lazy storage key handling
    -------------------------
    Linux does not use the ACC and F bits of the storage key. Newer Linux
    versions also do not use the storage keys for dirty and reference
    tracking. We can optimize the guest handling for those guests for faults
    as well as page-in and page-out by simply not caring about the guest
    visible storage key. We trap guest storage key instruction to enable
    those keys only on demand.
    
    Migration bitmap
    
    Until now s390 never provided a proper dirty bitmap.  Let's provide a
    proper migration bitmap for s390. We also change the user dirty tracking
    to a fault based mechanism. This makes the host completely independent
    from the storage keys. Long term this will allow us to back guest memory
    with large pages.
    
    per-VM device attributes
    ------------------------
    To avoid the introduction of new ioctls, let's provide the
    attribute semanantic also on the VM-"device".
    
    Userspace controlled CMMA
    -------------------------
    The CMMA assist is changed from "always on" to "on if requested" via
    per-VM device attributes. In addition a callback to reset all usage
    states is provided.
    
    Proper guest DAT handling for intercepts
    ----------------------------------------
    While instructions handled by SIE take care of all addressing aspects,
    KVM/s390 currently does not care about guest address translation of
    intercepts. This worked out fine, because
    - the s390 Linux kernel has a 1:1 mapping between kernel virtual<->real
     for all pages up to memory size
    - intercepts happen only for a small amount of cases
    - all of these intercepts happen to be in the kernel text for current
      distros
    
    Of course we need to be better for other intercepts, kernel modules etc.
    We provide the infrastructure and rework all in-kernel intercepts to work
    on logical addresses (paging etc) instead of real ones. The code has
    been running internally for several months now, so it is time for going
    public.
    
    GDB support
    -----------
    We provide breakpoints, single stepping and watchpoints.
    
    Fixes/Cleanups
    --------------
    - Improve program check delivery
    - Factor out the handling of transactional memory  on program checks
    - Use the existing define __LC_PGM_TDB
    - Several cleanups in the lowcore structure
    - Documentation
    
    NOTES
    -----
    - All patches touching base s390 are either ACKed or written by the s390
      maintainers
    - One base KVM patch "KVM: add kvm_is_error_gpa() helper"
    - One patch introduces the notion of VM device attributes
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    
    Conflicts:
            include/uapi/linux/kvm.h

commit dfeec843fb237d73947e818f961e8d6f0df22b01
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 1 16:09:21 2014 +0100

    KVM: add kvm_is_error_gpa() helper
    
    It's quite common (in the s390 guest access code) to test if a guest
    physical address points to a valid guest memory area or not.
    So add a simple helper function in common code, since this might be
    of interest for other architectures as well.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7d21cf9f4380..471d1400c4ac 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -879,6 +879,13 @@ static inline hpa_t pfn_to_hpa(pfn_t pfn)
 	return (hpa_t)pfn << PAGE_SHIFT;
 }
 
+static inline bool kvm_is_error_gpa(struct kvm *kvm, gpa_t gpa)
+{
+	unsigned long hva = gfn_to_hva(kvm, gpa_to_gfn(gpa));
+
+	return kvm_is_error_hva(hva);
+}
+
 static inline void kvm_migrate_timers(struct kvm_vcpu *vcpu)
 {
 	set_bit(KVM_REQ_MIGRATE_TIMER, &vcpu->requests);

commit 68c3b4d1676d870f0453c31d5a52e7e65c7448ae
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Mon Mar 31 21:50:44 2014 +0300

    KVM: VMX: speed up wildcard MMIO EVENTFD
    
    With KVM, MMIO is much slower than PIO, due to the need to
    do page walk and emulation. But with EPT, it does not have to be: we
    know the address from the VMCS so if the address is unique, we can look
    up the eventfd directly, bypassing emulation.
    
    Unfortunately, this only works if userspace does not need to match on
    access length and data.  The implementation adds a separate FAST_MMIO
    bus internally. This serves two purposes:
        - minimize overhead for old userspace that does not use eventfd with lengtth = 0
        - minimize disruption in other code (since we don't know the length,
          devices on the MMIO bus only get a valid address in write, this
          way we don't need to touch all devices to teach them to handle
          an invalid length)
    
    At the moment, this optimization only has effect for EPT on x86.
    
    It will be possible to speed up MMIO for NPT and MMU using the same
    idea in the future.
    
    With this patch applied, on VMX MMIO EVENTFD is essentially as fast as PIO.
    I was unable to detect any measureable slowdown to non-eventfd MMIO.
    
    Making MMIO faster is important for the upcoming virtio 1.0 which
    includes an MMIO signalling capability.
    
    The idea was suggested by Peter Anvin.  Lots of thanks to Gleb for
    pre-review and suggestions.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7d21cf9f4380..6c3c2eb96d06 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -163,6 +163,7 @@ enum kvm_bus {
 	KVM_MMIO_BUS,
 	KVM_PIO_BUS,
 	KVM_VIRTIO_CCW_NOTIFY_BUS,
+	KVM_FAST_MMIO_BUS,
 	KVM_NR_BUSES
 };
 

commit f3f710bc64e121c10c67ce58c893d3bc8c72abe4
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Tue Feb 25 12:48:01 2014 +0100

    KVM: Bump KVM_MAX_IRQ_ROUTES for s390
    
    The maximum number for irq routes is currently 1024, which is a bit on
    the small size for s390: We support up to 4 x 64k virtual devices with
    up to 64 queues, and we need one route for each of the queues if we want
    to operate it via irqfd.
    
    Let's bump this to 4k on s390 for now, as this at least covers the saner
    setups.
    
    We need to find a more general solution, though, as we can't just grow
    the routing table indefinitly.
    
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index da7510b4c6ad..7d21cf9f4380 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -922,7 +922,11 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 
 #ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 
+#ifdef CONFIG_S390
+#define KVM_MAX_IRQ_ROUTES 4096 //FIXME: we can have more than that...
+#else
 #define KVM_MAX_IRQ_ROUTES 1024
+#endif
 
 int kvm_setup_default_irq_routing(struct kvm *kvm);
 int kvm_set_irq_routing(struct kvm *kvm,

commit 84223598778ba08041f4297fda485df83414d57e
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Mon Jul 15 13:36:01 2013 +0200

    KVM: s390: irq routing for adapter interrupts.
    
    Introduce a new interrupt class for s390 adapter interrupts and enable
    irqfds for s390.
    
    This is depending on a new s390 specific vm capability, KVM_CAP_S390_IRQCHIP,
    that needs to be enabled by userspace.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9816b68b085f..da7510b4c6ad 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -297,6 +297,14 @@ static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memsl
 	return ALIGN(memslot->npages, BITS_PER_LONG) / 8;
 }
 
+struct kvm_s390_adapter_int {
+	u64 ind_addr;
+	u64 summary_addr;
+	u64 ind_offset;
+	u32 summary_offset;
+	u32 adapter_id;
+};
+
 struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
 	u32 type;
@@ -309,6 +317,7 @@ struct kvm_kernel_irq_routing_entry {
 			unsigned pin;
 		} irqchip;
 		struct msi_msg msi;
+		struct kvm_s390_adapter_int adapter;
 	};
 	struct hlist_node link;
 };

commit 5befdc385ddb2d5ae8995ad89004529a3acf58fc
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Tue Feb 18 17:22:47 2014 +0900

    KVM: Simplify kvm->tlbs_dirty handling
    
    When this was introduced, kvm_flush_remote_tlbs() could be called
    without holding mmu_lock.  It is now acknowledged that the function
    must be called before releasing mmu_lock, and all callers have already
    been changed to do so.
    
    There is no need to use smp_mb() and cmpxchg() any more.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f5937b8188b4..9816b68b085f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -401,7 +401,9 @@ struct kvm {
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 #endif
-	long tlbs_dirty;
+	/* Protected by mmu_lock */
+	bool tlbs_dirty;
+
 	struct list_head devices;
 };
 

commit e0ead41a6dac09f86675ce07a66e4b253a9b7bd5
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Jun 6 15:32:37 2013 +0200

    KVM: async_pf: Provide additional direct page notification
    
    By setting a Kconfig option, the architecture can control when
    guest notifications will be presented by the apf backend.
    There is the default batch mechanism, working as before, where the vcpu
    thread should pull in this information.
    Opposite to this, there is now the direct mechanism, that will push the
    information to the guest.
    This way s390 can use an already existing architecture interface.
    
    Still the vcpu thread should call check_completion to cleanup leftovers.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c0102ef2de48..f5937b8188b4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -192,7 +192,7 @@ struct kvm_async_pf {
 
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu);
-int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
+int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,
 		       struct kvm_arch_async_pf *arch);
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif

commit c05c4186bbe4e99d64e8a36f7ca7f480da5d109f
Author: Jens Freimann <jfrei@linux.vnet.ibm.com>
Date:   Mon Oct 7 16:13:45 2013 +0200

    KVM: s390: add floating irq controller
    
    This patch adds a floating irq controller as a kvm_device.
    It will be necessary for migration of floating interrupts as well
    as for hardening the reset code by allowing user space to explicitly
    remove all pending floating interrupts.
    
    Signed-off-by: Jens Freimann <jfrei@linux.vnet.ibm.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b8e9a43e501a..c0102ef2de48 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1064,6 +1064,7 @@ extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_vfio_ops;
 extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
+extern struct kvm_device_ops kvm_flic_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit ea0269bc34a7df6bda1ee862ad198dee0839f170
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Sun Dec 29 12:13:08 2013 -0800

    kvm: remove dead code
    
    The function kvm_io_bus_read_cookie is defined but never used
    in current in-tree code.
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4306c5608f6d..b8e9a43e501a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -172,8 +172,6 @@ int kvm_io_bus_write_cookie(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, const void *val, long cookie);
 int kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr, int len,
 		    void *val);
-int kvm_io_bus_read_cookie(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
-			   int len, void *val, long cookie);
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,

commit 7940876e1330671708186ac3386aa521ffb5c182
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Sun Dec 29 12:12:29 2013 -0800

    kvm: make local functions static
    
    Running 'make namespacecheck' found lots of functions that
    should be declared static, since only used in one file.
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1f46f66f60ab..4306c5608f6d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -463,8 +463,6 @@ void kvm_exit(void);
 
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
-void update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new,
-		     u64 last_generation);
 
 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 {
@@ -537,7 +535,6 @@ unsigned long gfn_to_hva_prot(struct kvm *kvm, gfn_t gfn, bool *writable);
 unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
-void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
@@ -549,7 +546,6 @@ pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
 
-void kvm_release_pfn_dirty(pfn_t pfn);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
 void kvm_set_pfn_accessed(pfn_t pfn);
@@ -576,8 +572,6 @@ struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
-void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,
-			     gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
@@ -604,8 +598,6 @@ int kvm_get_dirty_log(struct kvm *kvm,
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				struct kvm_dirty_log *log);
 
-int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
-				   struct kvm_userspace_memory_region *mem);
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,
 			bool line_status);
 long kvm_arch_vm_ioctl(struct file *filp,
@@ -653,8 +645,6 @@ void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
-void kvm_free_physmem(struct kvm *kvm);
-
 void *kvm_kvzalloc(unsigned long size);
 void kvm_kvfree(const void *addr);
 
@@ -1097,12 +1087,6 @@ static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)
 static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 {
 }
-
-static inline bool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)
-{
-	return true;
-}
-
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 #endif
 

commit 7330672befe6269e575f79b924a7068b26c144b4
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Fri Oct 25 17:29:18 2013 +0100

    KVM: arm-vgic: Support KVM_CREATE_DEVICE for VGIC
    
    Support creating the ARM VGIC device through the KVM_CREATE_DEVICE
    ioctl, which can then later be leveraged to use the
    KVM_{GET/SET}_DEVICE_ATTR, which is useful both for setting addresses in
    a more generic API than the ARM-specific one and is useful for
    save/restore of VGIC state.
    
    Adds KVM_CAP_DEVICE_CTRL to ARM capabilities.
    
    Note that we change the check for creating a VGIC from bailing out if
    any VCPUs were created, to bailing out if any VCPUs were ever run.  This
    is an important distinction that shouldn't break anything, but allows
    creating the VGIC after the VCPUs have been created.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4ecf10775c4f..1f46f66f60ab 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1075,6 +1075,7 @@ struct kvm_device *kvm_device_from_filp(struct file *filp);
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
 extern struct kvm_device_ops kvm_vfio_ops;
+extern struct kvm_device_ops kvm_arm_vgic_v2_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit c08ac06ab3f3cdb8d34376c3a8a5e46a31a62c8f
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Fri Dec 13 15:07:21 2013 +0900

    KVM: Use cond_resched() directly and remove useless kvm_resched()
    
    Since the commit 15ad7146 ("KVM: Use the scheduler preemption notifiers
    to make kvm preemptible"), the remaining stuff in this function is a
    simple cond_resched() call with an extra need_resched() check which was
    there to avoid dropping VCPUs unnecessarily.  Now it is meaningless.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9523d2ad7535..4ecf10775c4f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -583,7 +583,6 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
-void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 

commit 95f328d3ad1a8e4e3175a18546fb35c495e31130
Merge: daf727225b8a a78b55d1c021
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Nov 4 10:20:57 2013 +0200

    Merge branch 'kvm-ppc-queue' of git://github.com/agraf/linux-2.6 into queue
    
    Conflicts:
            arch/powerpc/include/asm/processor.h

commit 81e87e26796782e014fd1f2bb9cd8fb6ce4021a8
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Wed Oct 30 21:43:01 2013 +0200

    kvm_host: typo fix
    
    fix up typo in comment.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 92aae88756db..9bb1048d010e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -804,7 +804,7 @@ static inline void kvm_guest_enter(void)
 
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
-	 * is very similar to exiting to userspase from rcu point of view. In
+	 * is very similar to exiting to userspace from rcu point of view. In
 	 * addition CPU may stay in a guest mode for quite a long time (up to
 	 * one time slice). Lets treat guest mode as quiescent state, just like
 	 * we do with user-mode execution.

commit e0f0bbc527f6e9c0261f1d16b2a0b47612b7f235
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Oct 30 11:02:30 2013 -0600

    kvm: Create non-coherent DMA registeration
    
    We currently use some ad-hoc arch variables tied to legacy KVM device
    assignment to manage emulation of instructions that depend on whether
    non-coherent DMA is present.  Create an interface for this, adapting
    legacy KVM device assignment and adding VFIO via the KVM-VFIO device.
    For now we assume that non-coherent DMA is possible any time we have a
    VFIO group.  Eventually an interface can be developed as part of the
    VFIO external user interface to query the coherency of a group.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ed64880e4915..92aae88756db 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -670,6 +670,25 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 }
 #endif
 
+#ifdef __KVM_HAVE_ARCH_NONCOHERENT_DMA
+void kvm_arch_register_noncoherent_dma(struct kvm *kvm);
+void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm);
+bool kvm_arch_has_noncoherent_dma(struct kvm *kvm);
+#else
+static inline void kvm_arch_register_noncoherent_dma(struct kvm *kvm)
+{
+}
+
+static inline void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)
+{
+}
+
+static inline bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
+{
+	return false;
+}
+#endif
+
 static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_WQP

commit d96eb2c6f480769bff32054e78b964860dae4d56
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Oct 30 11:02:23 2013 -0600

    kvm/x86: Convert iommu_flags to iommu_noncoherent
    
    Default to operating in coherent mode.  This simplifies the logic when
    we switch to a model of registering and unregistering noncoherent I/O
    with KVM.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7beddbd38ac7..ed64880e4915 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -746,9 +746,6 @@ void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
-/* For vcpu->arch.iommu_flags */
-#define KVM_IOMMU_CACHE_COHERENCY	0x1
-
 #ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);

commit ec53500fae421e07c5d035918ca454a429732ef4
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Oct 30 11:02:17 2013 -0600

    kvm: Add VFIO device
    
    So far we've succeeded at making KVM and VFIO mostly unaware of each
    other, but areas are cropping up where a connection beyond eventfds
    and irqfds needs to be made.  This patch introduces a KVM-VFIO device
    that is meant to be a gateway for such interaction.  The user creates
    the device and can add and remove VFIO groups to it via file
    descriptors.  When a group is added, KVM verifies the group is valid
    and gets a reference to it via the VFIO external user interface.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c9d4236ab442..7beddbd38ac7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1058,6 +1058,7 @@ struct kvm_device *kvm_device_from_filp(struct file *filp);
 
 extern struct kvm_device_ops kvm_mpic_ops;
 extern struct kvm_device_ops kvm_xics_ops;
+extern struct kvm_device_ops kvm_vfio_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit 5587027ce9d59a57aecaa190be1c8e560aaff45d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:00 2013 +0530

    kvm: Add struct kvm arg to memslot APIs
    
    We will use that in the later patch to find the kvm ops handler
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c9d4236ab442..8b0107dc2067 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -507,9 +507,10 @@ int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem);
-void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
-int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
+int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
+			    unsigned long npages);
 void kvm_arch_memslots_updated(struct kvm *kvm);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,

commit f2e106692d5189303997ad7b96de8d8123aa5613
Author: chai wen <chaiw.fnst@cn.fujitsu.com>
Date:   Mon Oct 14 22:22:33 2013 +0800

    KVM: Drop FOLL_GET in GUP when doing async page fault
    
    Page pinning is not mandatory in kvm async page fault processing since
    after async page fault event is delivered to a guest it accesses page once
    again and does its own GUP.  Drop the FOLL_GET flag in GUP in async_pf
    code, and do some simplifying in check/clear processing.
    
    Suggested-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Gu zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: chai wen <chaiw.fnst@cn.fujitsu.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f6dccde755f6..c9d4236ab442 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -189,7 +189,7 @@ struct kvm_async_pf {
 	gva_t gva;
 	unsigned long addr;
 	struct kvm_arch_async_pf arch;
-	struct page *page;
+	bool   wakeup_all;
 };
 
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);

commit 6d9d41e57440e32a3400f37aa05ef7a1a09ced64
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Oct 2 14:22:28 2013 -0700

    KVM: Move gfn_to_index to x86 specific code
    
    The gfn_to_index function relies on huge page defines which either may
    not make sense on systems that don't support huge pages or are defined
    in an unconvenient way for other architectures.  Since this is
    x86-specific, move the function to arch/x86/include/asm/kvm_host.h.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7c961e1e9270..f6dccde755f6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -841,13 +841,6 @@ static inline int memslot_id(struct kvm *kvm, gfn_t gfn)
 	return gfn_to_memslot(kvm, gfn)->id;
 }
 
-static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
-{
-	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
-	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
-		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
-}
-
 static inline gfn_t
 hva_to_gfn_memslot(unsigned long hva, struct kvm_memory_slot *slot)
 {

commit 2f303b74a62fb74983c0a66e2df353be963c527c
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Sep 25 13:53:07 2013 +0200

    KVM: Convert kvm_lock back to non-raw spinlock
    
    In commit e935b8372cf8 ("KVM: Convert kvm_lock to raw_spinlock"),
    the kvm_lock was made a raw lock.  However, the kvm mmu_shrink()
    function tries to grab the (non-raw) mmu_lock within the scope of
    the raw locked kvm_lock being held.  This leads to the following:
    
    BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
    in_atomic(): 1, irqs_disabled(): 0, pid: 55, name: kswapd0
    Preemption disabled at:[<ffffffffa0376eac>] mmu_shrink+0x5c/0x1b0 [kvm]
    
    Pid: 55, comm: kswapd0 Not tainted 3.4.34_preempt-rt
    Call Trace:
     [<ffffffff8106f2ad>] __might_sleep+0xfd/0x160
     [<ffffffff817d8d64>] rt_spin_lock+0x24/0x50
     [<ffffffffa0376f3c>] mmu_shrink+0xec/0x1b0 [kvm]
     [<ffffffff8111455d>] shrink_slab+0x17d/0x3a0
     [<ffffffff81151f00>] ? mem_cgroup_iter+0x130/0x260
     [<ffffffff8111824a>] balance_pgdat+0x54a/0x730
     [<ffffffff8111fe47>] ? set_pgdat_percpu_threshold+0xa7/0xd0
     [<ffffffff811185bf>] kswapd+0x18f/0x490
     [<ffffffff81070961>] ? get_parent_ip+0x11/0x50
     [<ffffffff81061970>] ? __init_waitqueue_head+0x50/0x50
     [<ffffffff81118430>] ? balance_pgdat+0x730/0x730
     [<ffffffff81060d2b>] kthread+0xdb/0xe0
     [<ffffffff8106e122>] ? finish_task_switch+0x52/0x100
     [<ffffffff817e1e94>] kernel_thread_helper+0x4/0x10
     [<ffffffff81060c50>] ? __init_kthread_worker+0x
    
    After the previous patch, kvm_lock need not be a raw spinlock anymore,
    so change it back.
    
    Reported-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: kvm@vger.kernel.org
    Cc: gleb@redhat.com
    Cc: jan.kiszka@siemens.com
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 749bdb12cd15..7c961e1e9270 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -142,7 +142,7 @@ struct kvm;
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
-extern raw_spinlock_t kvm_lock;
+extern spinlock_t kvm_lock;
 extern struct list_head vm_list;
 
 struct kvm_io_range {

commit 98fda169290b3b28c0f2db2b8f02290c13da50ef
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Wed Sep 4 22:32:24 2013 +0200

    kvm: remove .done from struct kvm_async_pf
    
    '.done' is used to mark the completion of 'async_pf_execute()', but
    'cancel_work_sync()' returns true when the work was canceled, so we
    use it instead.
    
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0fbbc7aa02cb..749bdb12cd15 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -190,7 +190,6 @@ struct kvm_async_pf {
 	unsigned long addr;
 	struct kvm_arch_async_pf arch;
 	struct page *page;
-	bool done;
 };
 
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);

commit ba6a3541545542721ce821d1e7e5ce35752e6fdf
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Sep 9 13:52:33 2013 +0200

    KVM: mmu: allow page tables to be in read-only slots
    
    Page tables in a read-only memory slot will currently cause a triple
    fault because the page walker uses gfn_to_hva and it fails on such a slot.
    
    OVMF uses such a page table; however, real hardware seems to be fine with
    that as long as the accessed/dirty bits are set.  Save whether the slot
    is readonly, and later check it when updating the accessed and dirty bits.
    
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ca645a01d37a..0fbbc7aa02cb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -533,6 +533,7 @@ int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
+unsigned long gfn_to_hva_prot(struct kvm *kvm, gfn_t gfn, bool *writable);
 unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);

commit bf640876e21fe603f7f52b0c27d66b7716da0384
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Jul 26 15:04:07 2013 +0200

    KVM: s390: Make KVM_HVA_ERR_BAD usable on s390
    
    Current common code uses PAGE_OFFSET to indicate a bad host virtual address.
    As this check won't work on architectures that don't map kernel and user memory
    into the same address space (e.g. s390), such architectures can now provide
    their own KVM_HVA_ERR_BAD defines.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c11c7686ae5f..ca645a01d37a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -85,6 +85,12 @@ static inline bool is_noslot_pfn(pfn_t pfn)
 	return pfn == KVM_PFN_NOSLOT;
 }
 
+/*
+ * architectures with KVM_HVA_ERR_BAD other than PAGE_OFFSET (e.g. s390)
+ * provide own defines and kvm_is_error_hva
+ */
+#ifndef KVM_HVA_ERR_BAD
+
 #define KVM_HVA_ERR_BAD		(PAGE_OFFSET)
 #define KVM_HVA_ERR_RO_BAD	(PAGE_OFFSET + PAGE_SIZE)
 
@@ -93,6 +99,8 @@ static inline bool kvm_is_error_hva(unsigned long addr)
 	return addr >= PAGE_OFFSET;
 }
 
+#endif
+
 #define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))
 
 static inline bool is_error_page(struct page *page)

commit e59dbe09f8e6fb8f6ee19dc79d1a2f14299e4cd2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Thu Jul 4 13:40:29 2013 +0900

    KVM: Introduce kvm_arch_memslots_updated()
    
    This is called right after the memslots is updated, i.e. when the result
    of update_memslots() gets installed in install_new_memslots().  Since
    the memslots needs to be updated twice when we delete or move a memslot,
    kvm_arch_commit_memory_region() does not correspond to this exactly.
    
    In the following patch, x86 will use this new API to check if the mmio
    generation has reached its maximum value, in which case mmio sptes need
    to be flushed out.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Acked-by: Alexander Graf <agraf@suse.de>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ec590aece366..c11c7686ae5f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -503,6 +503,7 @@ int __kvm_set_memory_region(struct kvm *kvm,
 void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
+void kvm_arch_memslots_updated(struct kvm *kvm);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				struct kvm_userspace_memory_region *mem,

commit 126a5af520eff9b99a0bb1ca4bb4a0b7973f7d5a
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Wed Jul 3 16:30:53 2013 +0200

    KVM: kvm-io: support cookies
    
    Add new functions kvm_io_bus_{read,write}_cookie() that allows users of
    the kvm io infrastructure to use a cookie value to speed up lookup of a
    device on an io bus.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a63d83ebd151..ec590aece366 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -160,8 +160,12 @@ enum kvm_bus {
 
 int kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val);
+int kvm_io_bus_write_cookie(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
+			    int len, const void *val, long cookie);
 int kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr, int len,
 		    void *val);
+int kvm_io_bus_read_cookie(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
+			   int len, void *val, long cookie);
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,

commit fe489bf4505ae26d3c6d6a1f1d3064c2a9c5cd85
Merge: 3e34131a6512 a3ff5fbc94a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 13:21:40 2013 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM fixes from Paolo Bonzini:
     "On the x86 side, there are some optimizations and documentation
      updates.  The big ARM/KVM change for 3.11, support for AArch64, will
      come through Catalin Marinas's tree.  s390 and PPC have misc cleanups
      and bugfixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (87 commits)
      KVM: PPC: Ignore PIR writes
      KVM: PPC: Book3S PR: Invalidate SLB entries properly
      KVM: PPC: Book3S PR: Allow guest to use 1TB segments
      KVM: PPC: Book3S PR: Don't keep scanning HPTEG after we find a match
      KVM: PPC: Book3S PR: Fix invalidation of SLB entry 0 on guest entry
      KVM: PPC: Book3S PR: Fix proto-VSID calculations
      KVM: PPC: Guard doorbell exception with CONFIG_PPC_DOORBELL
      KVM: Fix RTC interrupt coalescing tracking
      kvm: Add a tracepoint write_tsc_offset
      KVM: MMU: Inform users of mmio generation wraparound
      KVM: MMU: document fast invalidate all mmio sptes
      KVM: MMU: document fast invalidate all pages
      KVM: MMU: document fast page fault
      KVM: MMU: document mmio page fault
      KVM: MMU: document write_flooding_count
      KVM: MMU: document clear_spte_count
      KVM: MMU: drop kvm_mmu_zap_mmio_sptes
      KVM: MMU: init kvm generation close to mmio wrap-around value
      KVM: MMU: add tracepoint for check_mmio_spte
      KVM: MMU: fast invalidate all mmio sptes
      ...

commit 6ea34c9b78c10289846db0abeebd6b84d5aca084
Author: Amos Kong <akong@redhat.com>
Date:   Sat May 25 06:44:15 2013 +0800

    kvm: exclude ioeventfd from counting kvm_io_range limit
    
    We can easily reach the 1000 limit by start VM with a couple
    hundred I/O devices (multifunction=on). The hardcode limit
    already been adjusted 3 times (6 ~ 200 ~ 300 ~ 1000).
    
    In userspace, we already have maximum file descriptor to
    limit ioeventfd count. But kvm_io_bus devices also are used
    for pit, pic, ioapic, coalesced_mmio. They couldn't be limited
    by maximum file descriptor.
    
    Currently only ioeventfds take too much kvm_io_bus devices,
    so just exclude it from counting kvm_io_range limit.
    
    Also fixed one indent issue in kvm_host.h
    
    Signed-off-by: Amos Kong <akong@redhat.com>
    Reviewed-by: Stefan Hajnoczi <stefanha@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d9a3c30eab2e..e3aae6db276f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -145,7 +145,8 @@ struct kvm_io_range {
 #define NR_IOBUS_DEVS 1000
 
 struct kvm_io_bus {
-	int                   dev_count;
+	int dev_count;
+	int ioeventfd_count;
 	struct kvm_io_range range[];
 };
 

commit 521921bad1192fb1b8f9b6a5aa673635848b8b5f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 16 01:21:38 2013 +0200

    kvm: Move guest entry/exit APIs to context_tracking
    
    The kvm_host.h header file doesn't handle well
    inclusion when archs don't support KVM.
    
    This results in build crashes for such archs when they
    want to implement context tracking because this subsystem
    includes kvm_host.h in order to implement the
    guest_enter/exit APIs but it doesn't handle KVM off case.
    
    To fix this, move the guest_enter()/guest_exit()
    declarations and generic implementation to the context
    tracking headers. These generic APIs actually belong to
    this subsystem, besides other domains boundary tracking
    like user_enter() et al.
    
    KVM now properly becomes a user of this library, not the
    other buggy way around.
    
    Reported-by: Kevin Hilman <khilman@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f0eea07d2c2b..8db53cfaccdb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -23,6 +23,7 @@
 #include <linux/ratelimit.h>
 #include <linux/err.h>
 #include <linux/irqflags.h>
+#include <linux/context_tracking.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -760,42 +761,6 @@ static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
 }
 #endif
 
-static inline void __guest_enter(void)
-{
-	/*
-	 * This is running in ioctl context so we can avoid
-	 * the call to vtime_account() with its unnecessary idle check.
-	 */
-	vtime_account_system(current);
-	current->flags |= PF_VCPU;
-}
-
-static inline void __guest_exit(void)
-{
-	/*
-	 * This is running in ioctl context so we can avoid
-	 * the call to vtime_account() with its unnecessary idle check.
-	 */
-	vtime_account_system(current);
-	current->flags &= ~PF_VCPU;
-}
-
-#ifdef CONFIG_CONTEXT_TRACKING
-extern void guest_enter(void);
-extern void guest_exit(void);
-
-#else /* !CONFIG_CONTEXT_TRACKING */
-static inline void guest_enter(void)
-{
-	__guest_enter();
-}
-
-static inline void guest_exit(void)
-{
-	__guest_exit();
-}
-#endif /* !CONFIG_CONTEXT_TRACKING */
-
 static inline void kvm_guest_enter(void)
 {
 	unsigned long flags;

commit 0061d53daf26ff713ab43ab84ae5c44b5edbefa9
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu May 9 20:21:41 2013 -0300

    KVM: x86: limit difference between kvmclock updates
    
    kvmclock updates which are isolated to a given vcpu, such as vcpu->cpu
    migration, should not allow system_timestamp from the rest of the vcpus
    to remain static. Otherwise ntp frequency correction applies to one
    vcpu's system_timestamp but not the others.
    
    So in those cases, request a kvmclock update for all vcpus. The worst
    case for a remote vcpu to update its kvmclock is then bounded by maximum
    nohz sleep latency.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f0eea07d2c2b..d9a3c30eab2e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -124,6 +124,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_MCLOCK_INPROGRESS 19
 #define KVM_REQ_EPR_EXIT          20
 #define KVM_REQ_SCAN_IOAPIC       21
+#define KVM_REQ_GLOBAL_CLOCK_UPDATE 22
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 01227a889ed56ae53aeebb9f93be9d54dd8b2de8
Merge: 9e6879460c8e db6ae6158186
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 14:47:31 2013 -0700

    Merge tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Gleb Natapov:
     "Highlights of the updates are:
    
      general:
       - new emulated device API
       - legacy device assignment is now optional
       - irqfd interface is more generic and can be shared between arches
    
      x86:
       - VMCS shadow support and other nested VMX improvements
       - APIC virtualization and Posted Interrupt hardware support
       - Optimize mmio spte zapping
    
      ppc:
        - BookE: in-kernel MPIC emulation with irqfd support
        - Book3S: in-kernel XICS emulation (incomplete)
        - Book3S: HV: migration fixes
        - BookE: more debug support preparation
        - BookE: e6500 support
    
      ARM:
       - reworking of Hyp idmaps
    
      s390:
       - ioeventfd for virtio-ccw
    
      And many other bug fixes, cleanups and improvements"
    
    * tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      kvm: Add compat_ioctl for device control API
      KVM: x86: Account for failing enable_irq_window for NMI window request
      KVM: PPC: Book3S: Add API for in-kernel XICS emulation
      kvm/ppc/mpic: fix missing unlock in set_base_addr()
      kvm/ppc: Hold srcu lock when calling kvm_io_bus_read/write
      kvm/ppc/mpic: remove users
      kvm/ppc/mpic: fix mmio region lists when multiple guests used
      kvm/ppc/mpic: remove default routes from documentation
      kvm: KVM_CAP_IOMMU only available with device assignment
      ARM: KVM: iterate over all CPUs for CPU compatibility check
      KVM: ARM: Fix spelling in error message
      ARM: KVM: define KVM_ARM_MAX_VCPUS unconditionally
      KVM: ARM: Fix API documentation for ONE_REG encoding
      ARM: KVM: promote vfp_host pointer to generic host cpu context
      ARM: KVM: add architecture specific hook for capabilities
      ARM: KVM: perform HYP initilization for hotplugged CPUs
      ARM: KVM: switch to a dual-step HYP init code
      ARM: KVM: rework HYP page table freeing
      ARM: KVM: enforce maximum size for identity mapped code
      ARM: KVM: move to a KVM provided HYP idmap
      ...

commit 5975a2e0950291a6bfe9fd5880e7952ff87764be
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Apr 27 00:28:37 2013 +0000

    KVM: PPC: Book3S: Add API for in-kernel XICS emulation
    
    This adds the API for userspace to instantiate an XICS device in a VM
    and connect VCPUs to it.  The API consists of a new device type for
    the KVM_CREATE_DEVICE ioctl, a new capability KVM_CAP_IRQ_XICS, which
    functions similarly to KVM_CAP_IRQ_MPIC, and the KVM_IRQ_LINE ioctl,
    which is used to assert and deassert interrupt inputs of the XICS.
    
    The XICS device has one attribute group, KVM_DEV_XICS_GRP_SOURCES.
    Each attribute within this group corresponds to the state of one
    interrupt source.  The attribute number is the same as the interrupt
    source number.
    
    This does not support irq routing or irqfd yet.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 996661eb8e99..7823b6369c5e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1086,6 +1086,7 @@ void kvm_device_put(struct kvm_device *dev);
 struct kvm_device *kvm_device_from_filp(struct file *filp);
 
 extern struct kvm_device_ops kvm_mpic_ops;
+extern struct kvm_device_ops kvm_xics_ops;
 
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 

commit 2a5bab1004729f3302c776e53ee7c895b98bb1ce
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Tue Apr 16 13:49:18 2013 -0600

    kvm: Allow build-time configuration of KVM device assignment
    
    We hope to at some point deprecate KVM legacy device assignment in
    favor of VFIO-based assignment.  Towards that end, allow legacy
    device assignment to be deconfigured.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 309774715897..996661eb8e99 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -667,7 +667,6 @@ static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type);
 void kvm_arch_destroy_vm(struct kvm *kvm);
-void kvm_free_all_assigned_devices(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
@@ -736,7 +735,7 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 /* For vcpu->arch.iommu_flags */
 #define KVM_IOMMU_CACHE_COHERENCY	0x1
 
-#ifdef CONFIG_IOMMU_API
+#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 int kvm_iommu_map_guest(struct kvm *kvm);
@@ -745,7 +744,7 @@ int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev);
 int kvm_deassign_device(struct kvm *kvm,
 			struct kvm_assigned_dev_kernel *assigned_dev);
-#else /* CONFIG_IOMMU_API */
+#else
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      struct kvm_memory_slot *slot)
 {
@@ -757,28 +756,11 @@ static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 {
 }
 
-static inline int kvm_iommu_map_guest(struct kvm *kvm)
-{
-	return -ENODEV;
-}
-
 static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
 {
 	return 0;
 }
-
-static inline int kvm_assign_device(struct kvm *kvm,
-		struct kvm_assigned_dev_kernel *assigned_dev)
-{
-	return 0;
-}
-
-static inline int kvm_deassign_device(struct kvm *kvm,
-		struct kvm_assigned_dev_kernel *assigned_dev)
-{
-	return 0;
-}
-#endif /* CONFIG_IOMMU_API */
+#endif
 
 static inline void __guest_enter(void)
 {
@@ -1032,11 +1014,13 @@ static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
 
 #endif
 
-#ifdef __KVM_HAVE_DEVICE_ASSIGNMENT
+#ifdef CONFIG_KVM_DEVICE_ASSIGNMENT
 
 long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
 				  unsigned long arg);
 
+void kvm_free_all_assigned_devices(struct kvm *kvm);
+
 #else
 
 static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
@@ -1045,6 +1029,8 @@ static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
 	return -ENOTTY;
 }
 
+static inline void kvm_free_all_assigned_devices(struct kvm *kvm) {}
+
 #endif
 
 static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)

commit 064d1afaa5a60fc391d0b4b77599fc8f63f99cd3
Merge: 730dca42c1d3 8b78645c93b5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Apr 28 12:50:07 2013 +0300

    Merge git://github.com/agraf/linux-2.6.git kvm-ppc-next into queue

commit 730dca42c1d363c939da18c1499c7327c66e2b37
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Sun Apr 28 10:50:52 2013 +0200

    KVM: x86: Rework request for immediate exit
    
    The VMX implementation of enable_irq_window raised
    KVM_REQ_IMMEDIATE_EXIT after we checked it in vcpu_enter_guest. This
    caused infinite loops on vmentry. Fix it by letting enable_irq_window
    signal the need for an immediate exit via its return value and drop
    KVM_REQ_IMMEDIATE_EXIT.
    
    This issue only affects nested VMX scenarios.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93a50054d46c..7bde42470e37 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -119,14 +119,13 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_APF_HALT          12
 #define KVM_REQ_STEAL_UPDATE      13
 #define KVM_REQ_NMI               14
-#define KVM_REQ_IMMEDIATE_EXIT    15
-#define KVM_REQ_PMU               16
-#define KVM_REQ_PMI               17
-#define KVM_REQ_WATCHDOG          18
-#define KVM_REQ_MASTERCLOCK_UPDATE 19
-#define KVM_REQ_MCLOCK_INPROGRESS 20
-#define KVM_REQ_EPR_EXIT          21
-#define KVM_REQ_SCAN_IOAPIC       22
+#define KVM_REQ_PMU               15
+#define KVM_REQ_PMI               16
+#define KVM_REQ_WATCHDOG          17
+#define KVM_REQ_MASTERCLOCK_UPDATE 18
+#define KVM_REQ_MCLOCK_INPROGRESS 19
+#define KVM_REQ_EPR_EXIT          20
+#define KVM_REQ_SCAN_IOAPIC       21
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 07f0a7bdec5c4039cfb9b836482c45004d4c21cc
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Apr 25 14:11:23 2013 +0000

    kvm: destroy emulated devices on VM exit
    
    The hassle of getting refcounting right was greater than the hassle
    of keeping a list of devices to destroy on VM exit.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index feffbdaf8986..36c977694741 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -393,6 +393,7 @@ struct kvm {
 	long mmu_notifier_count;
 #endif
 	long tlbs_dirty;
+	struct list_head devices;
 };
 
 #define kvm_err(fmt, ...) \
@@ -1069,8 +1070,8 @@ struct kvm_device_ops;
 struct kvm_device {
 	struct kvm_device_ops *ops;
 	struct kvm *kvm;
-	atomic_t users;
 	void *private;
+	struct list_head vm_node;
 };
 
 /* create, destroy, and name are mandatory */

commit 5df554ad5b7522ea62b0ff9d5be35183494efc21
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:46 2013 +0000

    kvm/ppc/mpic: in-kernel MPIC emulation
    
    Hook the MPIC code up to the KVM interfaces, add locking, etc.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub function for kvmppc_mpic_set_epr, non-booke, 64bit]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6dab6b5b3e3b..feffbdaf8986 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1099,6 +1099,8 @@ void kvm_device_get(struct kvm_device *dev);
 void kvm_device_put(struct kvm_device *dev);
 struct kvm_device *kvm_device_from_filp(struct file *filp);
 
+extern struct kvm_device_ops kvm_mpic_ops;
+
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 
 static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)

commit 852b6d57dc7fa378019786fa84727036e56839ea
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:42 2013 +0000

    kvm: add device control API
    
    Currently, devices that are emulated inside KVM are configured in a
    hardcoded manner based on an assumption that any given architecture
    only has one way to do it.  If there's any need to access device state,
    it is done through inflexible one-purpose-only IOCTLs (e.g.
    KVM_GET/SET_LAPIC).  Defining new IOCTLs for every little thing is
    cumbersome and depletes a limited numberspace.
    
    This API provides a mechanism to instantiate a device of a certain
    type, returning an ID that can be used to set/get attributes of the
    device.  Attributes may include configuration parameters (e.g.
    register base address), device state, operational commands, etc.  It
    is similar to the ONE_REG API, except that it acts on devices rather
    than vcpus.
    
    Both device types and individual attributes can be tested without having
    to create the device or get/set the attribute, without the need for
    separately managing enumerated capabilities.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index dcef724f4ba6..6dab6b5b3e3b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1064,6 +1064,41 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 
 extern bool kvm_rebooting;
 
+struct kvm_device_ops;
+
+struct kvm_device {
+	struct kvm_device_ops *ops;
+	struct kvm *kvm;
+	atomic_t users;
+	void *private;
+};
+
+/* create, destroy, and name are mandatory */
+struct kvm_device_ops {
+	const char *name;
+	int (*create)(struct kvm_device *dev, u32 type);
+
+	/*
+	 * Destroy is responsible for freeing dev.
+	 *
+	 * Destroy may be called before or after destructors are called
+	 * on emulated I/O regions, depending on whether a reference is
+	 * held by a vcpu or other kvm component that gets destroyed
+	 * after the emulated I/O.
+	 */
+	void (*destroy)(struct kvm_device *dev);
+
+	int (*set_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
+	int (*get_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
+	int (*has_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);
+	long (*ioctl)(struct kvm_device *dev, unsigned int ioctl,
+		      unsigned long arg);
+};
+
+void kvm_device_get(struct kvm_device *dev);
+void kvm_device_put(struct kvm_device *dev);
+struct kvm_device *kvm_device_from_filp(struct file *filp);
+
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 
 static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)

commit e8cde0939d8ebe9c8ebe5a4bdf71af51a0d56053
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Apr 15 23:23:21 2013 +0200

    KVM: Move irq routing setup to irqchip.c
    
    Setting up IRQ routes is nothing IOAPIC specific. Extract everything
    that really is generic code into irqchip.c and only leave the ioapic
    specific bits to irq_comm.c.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a7bfe9d3aa5e..dcef724f4ba6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -961,6 +961,9 @@ int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *entries,
 			unsigned nr,
 			unsigned flags);
+int kvm_set_routing_entry(struct kvm_irq_routing_table *rt,
+			  struct kvm_kernel_irq_routing_entry *e,
+			  const struct kvm_irq_routing_entry *ue);
 void kvm_free_irq_routing(struct kvm *kvm);
 
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi);

commit 7eee2efdc6978aa70ab4046394128c1a10bc2d80
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Apr 15 10:50:54 2013 +0200

    KVM: Remove kvm_get_intr_delivery_bitmask
    
    The prototype has been stale for a while, I can't spot any real function
    define behind it. Let's just remove it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4215d4f3d86f..a7bfe9d3aa5e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -719,11 +719,6 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 			     bool mask);
 
-#ifdef __KVM_HAVE_IOAPIC
-void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
-				   union kvm_ioapic_redirect_entry *entry,
-				   unsigned long *deliver_bitmask);
-#endif
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status);
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);

commit a725d56a02ec3582bb5b9756f261fdc6962c79ee
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Apr 17 13:29:30 2013 +0200

    KVM: Introduce CONFIG_HAVE_KVM_IRQ_ROUTING
    
    Quite a bit of code in KVM has been conditionalized on availability of
    IOAPIC emulation. However, most of it is generically applicable to
    platforms that don't have an IOPIC, but a different type of irq chip.
    
    Make code that only relies on IRQ routing, not an APIC itself, on
    CONFIG_HAVE_KVM_IRQ_ROUTING, so that we can reuse it later.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bf3b1dcb8b3d..4215d4f3d86f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -304,7 +304,7 @@ struct kvm_kernel_irq_routing_entry {
 	struct hlist_node link;
 };
 
-#ifdef __KVM_HAVE_IOAPIC
+#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 
 struct kvm_irq_routing_table {
 	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
@@ -432,7 +432,7 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
-#ifdef __KVM_HAVE_IOAPIC
+#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 int kvm_irqfd_init(void);
 void kvm_irqfd_exit(void);
 #else
@@ -957,7 +957,7 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 }
 #endif
 
-#ifdef KVM_CAP_IRQ_ROUTING
+#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
 
 #define KVM_MAX_IRQ_ROUTES 1024
 

commit 8175e5b79c38a1d85225da516fa1a0ecbf2fdbca
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Apr 15 10:42:33 2013 +0200

    KVM: Add KVM_IRQCHIP_NUM_PINS in addition to KVM_IOAPIC_NUM_PINS
    
    The concept of routing interrupt lines to an irqchip is nothing
    that is IOAPIC specific. Every irqchip has a maximum number of pins
    that can be linked to irq lines.
    
    So let's add a new define that allows us to reuse generic code for
    non-IOAPIC platforms.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93a50054d46c..bf3b1dcb8b3d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -307,7 +307,7 @@ struct kvm_kernel_irq_routing_entry {
 #ifdef __KVM_HAVE_IOAPIC
 
 struct kvm_irq_routing_table {
-	int chip[KVM_NR_IRQCHIPS][KVM_IOAPIC_NUM_PINS];
+	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
 	struct kvm_kernel_irq_routing_entry *rt_entries;
 	u32 nr_rt_entries;
 	/*

commit 3d81bc7e96d6bca0b8f8b7d1bf6ea72caa3aac57
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Thu Apr 11 19:25:13 2013 +0800

    KVM: Call common update function when ioapic entry changed.
    
    Both TMR and EOI exit bitmap need to be updated when ioapic changed
    or vcpu's id/ldr/dfr changed. So use common function instead eoi exit
    bitmap specific function.
    
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4a76aca31478..93a50054d46c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -126,7 +126,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_MASTERCLOCK_UPDATE 19
 #define KVM_REQ_MCLOCK_INPROGRESS 20
 #define KVM_REQ_EPR_EXIT          21
-#define KVM_REQ_EOIBITMAP         22
+#define KVM_REQ_SCAN_IOAPIC       22
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
@@ -575,7 +575,7 @@ void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
 void kvm_make_mclock_inprogress_request(struct kvm *kvm);
-void kvm_make_update_eoibitmap_request(struct kvm *kvm);
+void kvm_make_scan_ioapic_request(struct kvm *kvm);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);

commit aa2fbe6d44892070d78995f0df875ce930904e29
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Thu Apr 11 19:21:40 2013 +0800

    KVM: Let ioapic know the irq line status
    
    Userspace may deliver RTC interrupt without query the status. So we
    want to track RTC EOI for this case.
    
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 20d77d24d764..4a76aca31478 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -292,7 +292,8 @@ struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
 	u32 type;
 	int (*set)(struct kvm_kernel_irq_routing_entry *e,
-		   struct kvm *kvm, int irq_source_id, int level);
+		   struct kvm *kvm, int irq_source_id, int level,
+		   bool line_status);
 	union {
 		struct {
 			unsigned irqchip;
@@ -591,7 +592,8 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 
 int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 				   struct kvm_userspace_memory_region *mem);
-int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level);
+int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,
+			bool line_status);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);
 
@@ -722,10 +724,11 @@ void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
 				   union kvm_ioapic_redirect_entry *entry,
 				   unsigned long *deliver_bitmask);
 #endif
-int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level);
+int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
+		bool line_status);
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
-		int irq_source_id, int level);
+		int irq_source_id, int level, bool line_status);
 bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,

commit 8b415dcd762607379cf0a69c9dd25940da1d174e
Author: Geoff Levand <geoff@infradead.org>
Date:   Fri Apr 5 19:20:30 2013 +0000

    KVM: Move kvm_rebooting declaration out of x86
    
    The variable kvm_rebooting is a common kvm variable, so move its
    declaration from arch/x86/include/asm/kvm_host.h to
    include/asm/kvm_host.h.
    
    Fixes this sparse warning when building on arm64:
    
      virt/kvm/kvm_main.c:warning: symbol 'kvm_rebooting' was not declared. Should it be static?
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 71fed38c7200..20d77d24d764 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1061,6 +1061,8 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 	}
 }
 
+extern bool kvm_rebooting;
+
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
 
 static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)

commit fc1b74925f87f6aca5432eb73f6a57eff30afde7
Author: Geoff Levand <geoff@infradead.org>
Date:   Fri Apr 5 19:20:30 2013 +0000

    KVM: Move vm_list kvm_lock declarations out of x86
    
    The variables vm_list and kvm_lock are common to all architectures, so
    move the declarations from arch/x86/include/asm/kvm_host.h to
    include/linux/kvm_host.h.
    
    Fixes sparse warnings like these when building for arm64:
    
      virt/kvm/kvm_main.c: warning: symbol 'kvm_lock' was not declared. Should it be static?
      virt/kvm/kvm_main.c: warning: symbol 'vm_list' was not declared. Should it be static?
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1c0be23f874d..71fed38c7200 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -135,6 +135,9 @@ struct kvm;
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
+extern raw_spinlock_t kvm_lock;
+extern struct list_head vm_list;
+
 struct kvm_io_range {
 	gpa_t addr;
 	int len;

commit 8f964525a121f2ff2df948dac908dcc65be21b5b
Author: Andrew Honig <ahonig@google.com>
Date:   Fri Mar 29 09:35:21 2013 -0700

    KVM: Allow cross page reads and writes from cached translations.
    
    This patch adds support for kvm_gfn_to_hva_cache_init functions for
    reads and writes that will cross a page.  If the range falls within
    the same memslot, then this will be a fast operation.  If the range
    is split between two memslots, then the slower kvm_read_guest and
    kvm_write_guest are used.
    
    Tested: Test against kvm_clock unit tests.
    
    Signed-off-by: Andrew Honig <ahonig@google.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cad77fe09d77..c13958251927 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -518,7 +518,7 @@ int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len);
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
-			      gpa_t gpa);
+			      gpa_t gpa, unsigned long len);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);

commit 09a6e1f4ad32243989b30485f78985c0923284cd
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Mar 22 08:08:06 2013 -0300

    Revert "KVM: allow host header to be included even for !CONFIG_KVM"
    
    This reverts commit f445f11eb2cc265dd47da5b2e864df46cd6e5a82 as
    it breaks PPC with CONFIG_KVM=n.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a9428635c9fd..cad77fe09d77 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1,8 +1,6 @@
 #ifndef __KVM_HOST_H
 #define __KVM_HOST_H
 
-#if IS_ENABLED(CONFIG_KVM)
-
 /*
  * This work is licensed under the terms of the GNU GPL, version 2.  See
  * the COPYING file in the top-level directory.
@@ -1057,8 +1055,5 @@ static inline bool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)
 }
 
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
-#else
-static inline void __guest_enter(void) { return; }
-static inline void __guest_exit(void) { return; }
-#endif /* IS_ENABLED(CONFIG_KVM) */
 #endif
+

commit 2ae33b389601b86a3d0cfe2d09f5e3189d5322fd
Merge: 04b66839d312 2ffdd7e23cde
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Mar 21 11:11:52 2013 -0300

    Merge remote-tracking branch 'upstream/master' into queue
    
    Merge reason:
    
    From: Alexander Graf <agraf@suse.de>
    
    "Just recently this really important patch got pulled into Linus' tree for 3.9:
    
    commit 1674400aaee5b466c595a8fc310488263ce888c7
    Author: Anton Blanchard <anton <at> samba.org>
    Date:   Tue Mar 12 01:51:51 2013 +0000
    
    Without that commit, I can not boot my G5, thus I can't run automated tests on it against my queue.
    
    Could you please merge kvm/next against linus/master, so that I can base my trees against that?"
    
    * upstream/master: (653 commits)
      PCI: Use ROM images from firmware only if no other ROM source available
      sparc: remove unused "config BITS"
      sparc: delete "if !ULTRA_HAS_POPULATION_COUNT"
      KVM: Fix bounds checking in ioapic indirect register reads (CVE-2013-1798)
      KVM: x86: Convert MSR_KVM_SYSTEM_TIME to use gfn_to_hva_cache functions (CVE-2013-1797)
      KVM: x86: fix for buffer overflow in handling of MSR_KVM_SYSTEM_TIME (CVE-2013-1796)
      arm64: Kconfig.debug: Remove unused CONFIG_DEBUG_ERRORS
      arm64: Do not select GENERIC_HARDIRQS_NO_DEPRECATED
      inet: limit length of fragment queue hash table bucket lists
      qeth: Fix scatter-gather regression
      qeth: Fix invalid router settings handling
      qeth: delay feature trace
      sgy-cts1000: Remove __dev* attributes
      KVM: x86: fix deadlock in clock-in-progress request handling
      KVM: allow host header to be included even for !CONFIG_KVM
      hwmon: (lm75) Fix tcn75 prefix
      hwmon: (lm75.h) Update header inclusion
      MAINTAINERS: Remove Mark M. Hoffman
      xfs: ensure we capture IO errors correctly
      xfs: fix xfs_iomap_eof_prealloc_initial_size type
      ...
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit f445f11eb2cc265dd47da5b2e864df46cd6e5a82
Author: Kevin Hilman <khilman@linaro.org>
Date:   Thu Mar 14 17:13:46 2013 -0700

    KVM: allow host header to be included even for !CONFIG_KVM
    
    The new context tracking subsystem unconditionally includes kvm_host.h
    headers for the guest enter/exit macros.  This causes a compile
    failure when KVM is not enabled.
    
    Fix by adding an IS_ENABLED(CONFIG_KVM) check to kvm_host so it can
    be included/compiled even when KVM is not enabled.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cad77fe09d77..a9428635c9fd 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1,6 +1,8 @@
 #ifndef __KVM_HOST_H
 #define __KVM_HOST_H
 
+#if IS_ENABLED(CONFIG_KVM)
+
 /*
  * This work is licensed under the terms of the GNU GPL, version 2.  See
  * the COPYING file in the top-level directory.
@@ -1055,5 +1057,8 @@ static inline bool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)
 }
 
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
+#else
+static inline void __guest_enter(void) { return; }
+static inline void __guest_exit(void) { return; }
+#endif /* IS_ENABLED(CONFIG_KVM) */
 #endif
-

commit 3a08a8f9f0936e182d387afd85fdc5d303381521
Author: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
Date:   Mon Mar 4 23:32:07 2013 +0530

    kvm: Record the preemption status of vcpus using preempt notifiers
    
    Note that we mark as preempted only when vcpu's task state was
    Running during preemption.
    
    Thanks Jiannan, Avi for preemption notifier ideas. Thanks Gleb, PeterZ
    for their precious suggestions. Thanks Srikar for an idea on avoiding
    rcu lock while checking task state that improved overcommit numbers.
    
    Reviewed-by: Chegu Vinod <chegu_vinod@hp.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9fa13ebc3381..0f4941a9c9c8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -253,6 +253,7 @@ struct kvm_vcpu {
 		bool dy_eligible;
 	} spin_loop;
 #endif
+	bool preempted;
 	struct kvm_vcpu_arch arch;
 };
 

commit 060f0ce6ff975decd1e0ee318c08e228bccbee1e
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Feb 28 12:33:19 2013 +0100

    KVM: Introduce KVM_VIRTIO_CCW_NOTIFY_BUS.
    
    Add a new bus type for virtio-ccw devices on s390.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d50fe173028b..9fa13ebc3381 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -149,6 +149,7 @@ struct kvm_io_bus {
 enum kvm_bus {
 	KVM_MMIO_BUS,
 	KVM_PIO_BUS,
+	KVM_VIRTIO_CCW_NOTIFY_BUS,
 	KVM_NR_BUSES
 };
 

commit a0f155e9646d5f1c263f6f9aae880151100243bb
Author: Cornelia Huck <cornelia.huck@de.ibm.com>
Date:   Thu Feb 28 12:33:18 2013 +0100

    KVM: Initialize irqfd from kvm_init().
    
    Currently, eventfd introduces module_init/module_exit functions
    to initialize/cleanup the irqfd workqueue. This only works, however,
    if no other module_init/module_exit functions are built into the
    same module.
    
    Let's just move the initialization and cleanup to kvm_init and kvm_exit.
    This way, it is also clearer where kvm startup may fail.
    
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ac584cc53581..d50fe173028b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -424,6 +424,19 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
+#ifdef __KVM_HAVE_IOAPIC
+int kvm_irqfd_init(void);
+void kvm_irqfd_exit(void);
+#else
+static inline int kvm_irqfd_init(void)
+{
+	return 0;
+}
+
+static inline void kvm_irqfd_exit(void)
+{
+}
+#endif
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module);
 void kvm_exit(void);

commit 8482644aea11e0647867732319ccf35879a9acc2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:45:25 2013 +0900

    KVM: set_memory_region: Refactor commit_memory_region()
    
    This patch makes the parameter old a const pointer to the old memory
    slot and adds a new parameter named change to know the change being
    requested: the former is for removing extra copying and the latter is
    for cleaning up the code.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index caa72cf7e8e7..ac584cc53581 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -483,7 +483,8 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				enum kvm_mr_change change);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old);
+				const struct kvm_memory_slot *old,
+				enum kvm_mr_change change);
 bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);
 /* flush all memory translations */

commit 7b6195a91d60909a2834ab7181e2b9476e6fe749
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:44:34 2013 +0900

    KVM: set_memory_region: Refactor prepare_memory_region()
    
    This patch drops the parameter old, a copy of the old memory slot, and
    adds a new parameter named change to know the change being requested.
    
    This not only cleans up the code but also removes extra copying of the
    memory slot structure.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8eaf61f7b02d..caa72cf7e8e7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -479,8 +479,8 @@ void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
-				struct kvm_memory_slot old,
-				struct kvm_userspace_memory_region *mem);
+				struct kvm_userspace_memory_region *mem,
+				enum kvm_mr_change change);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old);

commit 74d0727cb7aaaea48a6353209093be26abc8d160
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:43:44 2013 +0900

    KVM: set_memory_region: Make kvm_mr_change available to arch code
    
    This will be used for cleaning up prepare/commit_memory_region() later.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 84a994c7a5c5..8eaf61f7b02d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -452,6 +452,24 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 	return slot;
 }
 
+/*
+ * KVM_SET_USER_MEMORY_REGION ioctl allows the following operations:
+ * - create a new memory slot
+ * - delete an existing memory slot
+ * - modify an existing memory slot
+ *   -- move it in the guest physical memory space
+ *   -- just change its flags
+ *
+ * Since flags can be changed by some of these operations, the following
+ * differentiation is the best we can do for __kvm_set_memory_region():
+ */
+enum kvm_mr_change {
+	KVM_MR_CREATE,
+	KVM_MR_DELETE,
+	KVM_MR_MOVE,
+	KVM_MR_FLAGS_ONLY,
+};
+
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,

commit 47ae31e257c548abdb199e0d26723139a9a967ba
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:43:00 2013 +0900

    KVM: set_memory_region: Drop user_alloc from set_memory_region()
    
    Except ia64's stale code, KVM_SET_MEMORY_REGION support, this is only
    used for sanity checks in __kvm_set_memory_region() which can easily
    be changed to use slot id instead.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b4757a1cc4c4..84a994c7a5c5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -453,11 +453,9 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 }
 
 int kvm_set_memory_region(struct kvm *kvm,
-			  struct kvm_userspace_memory_region *mem,
-			  bool user_alloc);
+			  struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,
-			    struct kvm_userspace_memory_region *mem,
-			    bool user_alloc);
+			    struct kvm_userspace_memory_region *mem);
 void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
@@ -553,9 +551,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				struct kvm_dirty_log *log);
 
 int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
-				   struct
-				   kvm_userspace_memory_region *mem,
-				   bool user_alloc);
+				   struct kvm_userspace_memory_region *mem);
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);

commit 462fce46065ec4b200c08619c047b9e5a8fd154a
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:41:56 2013 +0900

    KVM: set_memory_region: Drop user_alloc from prepare/commit_memory_region()
    
    X86 does not use this any more.  The remaining user, s390's !user_alloc
    check, can be simply removed since KVM_SET_MEMORY_REGION ioctl is no
    longer supported.
    
    Note: fixed powerpc's indentations with spaces to suppress checkpatch
    errors.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cad77fe09d77..b4757a1cc4c4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -464,12 +464,10 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				struct kvm_memory_slot old,
-				struct kvm_userspace_memory_region *mem,
-				bool user_alloc);
+				struct kvm_userspace_memory_region *mem);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
-				struct kvm_memory_slot old,
-				bool user_alloc);
+				struct kvm_memory_slot old);
 bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);
 /* flush all memory translations */

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit 7a905b1485adf863607b5fc9e32a3fa3838bcc23
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Thu Feb 7 18:55:57 2013 +0900

    KVM: Remove user_alloc from struct kvm_memory_slot
    
    This field was needed to differentiate memory slots created by the new
    API, KVM_SET_USER_MEMORY_REGION, from those by the old equivalent,
    KVM_SET_MEMORY_REGION, whose support was dropped long before:
    
      commit b74a07beed0e64bfba413dcb70dd6749c57f43dc
      KVM: Remove kernel-allocated memory regions
    
    Although we also have private memory slots to which KVM allocates
    memory with vm_mmap(), !user_alloc slots in other words, the slot id
    should be enough for differentiating them.
    
    Note: corresponding function parameters will be removed later.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0350e0d5e031..722cae78bbc4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -273,7 +273,6 @@ struct kvm_memory_slot {
 	unsigned long userspace_addr;
 	u32 flags;
 	short id;
-	bool user_alloc;
 };
 
 static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memslot)

commit c7c9c56ca26f7b9458711b2d78b60b60e0d38ba7
Author: Yang Zhang <yang.z.zhang@Intel.com>
Date:   Fri Jan 25 10:18:51 2013 +0800

    x86, apicv: add virtual interrupt delivery support
    
    Virtual interrupt delivery avoids KVM to inject vAPIC interrupts
    manually, which is fully taken care of by the hardware. This needs
    some special awareness into existing interrupr injection path:
    
    - for pending interrupt, instead of direct injection, we may need
      update architecture specific indicators before resuming to guest.
    
    - A pending interrupt, which is masked by ISR, should be also
      considered in above update action, since hardware will decide
      when to inject it at right time. Current has_interrupt and
      get_interrupt only returns a valid vector from injection p.o.v.
    
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Yang Zhang <yang.z.zhang@Intel.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4dd7d7531e69..0350e0d5e031 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -123,6 +123,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_MASTERCLOCK_UPDATE 19
 #define KVM_REQ_MCLOCK_INPROGRESS 20
 #define KVM_REQ_EPR_EXIT          21
+#define KVM_REQ_EOIBITMAP         22
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
@@ -538,6 +539,7 @@ void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
 void kvm_make_mclock_inprogress_request(struct kvm *kvm);
+void kvm_make_update_eoibitmap_request(struct kvm *kvm);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);
@@ -691,6 +693,7 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level);
+bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4fe2396401da..b7996a768eb2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -741,7 +741,7 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 }
 #endif /* CONFIG_IOMMU_API */
 
-static inline void guest_enter(void)
+static inline void __guest_enter(void)
 {
 	/*
 	 * This is running in ioctl context so we can avoid
@@ -751,7 +751,7 @@ static inline void guest_enter(void)
 	current->flags |= PF_VCPU;
 }
 
-static inline void guest_exit(void)
+static inline void __guest_exit(void)
 {
 	/*
 	 * This is running in ioctl context so we can avoid
@@ -761,6 +761,22 @@ static inline void guest_exit(void)
 	current->flags &= ~PF_VCPU;
 }
 
+#ifdef CONFIG_CONTEXT_TRACKING
+extern void guest_enter(void);
+extern void guest_exit(void);
+
+#else /* !CONFIG_CONTEXT_TRACKING */
+static inline void guest_enter(void)
+{
+	__guest_enter();
+}
+
+static inline void guest_exit(void)
+{
+	__guest_exit();
+}
+#endif /* !CONFIG_CONTEXT_TRACKING */
+
 static inline void kvm_guest_enter(void)
 {
 	unsigned long flags;

commit c11f11fcbdb5be790c565aed46411486a7586afc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jan 21 00:50:22 2013 +0100

    kvm: Prepare to add generic guest entry/exit callbacks
    
    Do some ground preparatory work before adding guest_enter()
    and guest_exit() context tracking callbacks. Those will
    be later used to read the guest cputime safely when we
    run in full dynticks mode.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c497ab0d03d..4fe2396401da 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -22,6 +22,7 @@
 #include <linux/rcupdate.h>
 #include <linux/ratelimit.h>
 #include <linux/err.h>
+#include <linux/irqflags.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -740,15 +741,36 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 }
 #endif /* CONFIG_IOMMU_API */
 
-static inline void kvm_guest_enter(void)
+static inline void guest_enter(void)
 {
-	BUG_ON(preemptible());
 	/*
 	 * This is running in ioctl context so we can avoid
 	 * the call to vtime_account() with its unnecessary idle check.
 	 */
-	vtime_account_system_irqsafe(current);
+	vtime_account_system(current);
 	current->flags |= PF_VCPU;
+}
+
+static inline void guest_exit(void)
+{
+	/*
+	 * This is running in ioctl context so we can avoid
+	 * the call to vtime_account() with its unnecessary idle check.
+	 */
+	vtime_account_system(current);
+	current->flags &= ~PF_VCPU;
+}
+
+static inline void kvm_guest_enter(void)
+{
+	unsigned long flags;
+
+	BUG_ON(preemptible());
+
+	local_irq_save(flags);
+	guest_enter();
+	local_irq_restore(flags);
+
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
 	 * is very similar to exiting to userspase from rcu point of view. In
@@ -761,12 +783,11 @@ static inline void kvm_guest_enter(void)
 
 static inline void kvm_guest_exit(void)
 {
-	/*
-	 * This is running in ioctl context so we can avoid
-	 * the call to vtime_account() with its unnecessary idle check.
-	 */
-	vtime_account_system_irqsafe(current);
-	current->flags &= ~PF_VCPU;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	guest_exit();
+	local_irq_restore(flags);
 }
 
 /*

commit 1c810636556c8d53a37406b34a64d9b9b0161aa6
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 4 18:12:48 2013 +0100

    KVM: PPC: BookE: Implement EPR exit
    
    The External Proxy Facility in FSL BookE chips allows the interrupt
    controller to automatically acknowledge an interrupt as soon as a
    core gets its pending external interrupt delivered.
    
    Today, user space implements the interrupt controller, so we need to
    check on it during such a cycle.
    
    This patch implements logic for user space to enable EPR exiting,
    disable EPR exiting and EPR exiting itself, so that user space can
    acknowledge an interrupt when an external interrupt has successfully
    been delivered into the guest vcpu.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cbe0d683e2e5..4dd7d7531e69 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -122,6 +122,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_WATCHDOG          18
 #define KVM_REQ_MASTERCLOCK_UPDATE 19
 #define KVM_REQ_MCLOCK_INPROGRESS 20
+#define KVM_REQ_EPR_EXIT          21
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit 116c14c0191f3378e6567af296529ac287e85aa2
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri Dec 21 08:20:16 2012 -0700

    kvm: Fix memory slot generation updates
    
    Previous patch "kvm: Minor memory slot optimization" (b7f69c555ca43)
    overlooked the generation field of the memory slots.  Re-using the
    original memory slots left us with with two slightly different memory
    slots with the same generation.  To fix this, make update_memslots()
    take a new parameter to specify the last generation.  This also makes
    generation management more explicit to avoid such problems in the future.
    
    Reported-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 32fdc45ca35e..cbe0d683e2e5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -428,7 +428,8 @@ void kvm_exit(void);
 
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
-void update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new);
+void update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new,
+		     u64 last_generation);
 
 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 {

commit 1e702d9af5d633cf0eca76f6340b3c50fbb5a4e5
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:32 2012 -0700

    KVM: struct kvm_memory_slot.id -> short
    
    We're currently offering a whopping 32 memory slots to user space, an
    int is a bit excessive for storing this.  We would like to increase
    our memslots, but SHRT_MAX should be more than enough.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fec607537fa3..32fdc45ca35e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -270,7 +270,7 @@ struct kvm_memory_slot {
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
 	u32 flags;
-	int id;
+	short id;
 	bool user_alloc;
 };
 
@@ -330,7 +330,7 @@ struct kvm_memslots {
 	u64 generation;
 	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 	/* The mapping table from slot id to the index in memslots[]. */
-	int id_to_index[KVM_MEM_SLOTS_NUM];
+	short id_to_index[KVM_MEM_SLOTS_NUM];
 };
 
 struct kvm {

commit 6104f472a5ea287fbdcf4644e74867dfd905a018
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:26 2012 -0700

    KVM: struct kvm_memory_slot.flags -> u32
    
    struct kvm_userspace_memory_region.flags is a u32 with a comment that
    bits 0 ~ 15 are visible to userspace and the other bits are reserved
    for kvm internal use.  KVM_MEMSLOT_INVALID is the only internal use
    flag and it has a comment that bits 16 ~ 31 are internally used and
    the other bits are visible to userspace.
    
    Therefore, let's define this as a u32 so we don't waste bytes on LP64
    systems.  Move to the end of the struct for alignment.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d897f035749f..fec607537fa3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -266,10 +266,10 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
-	unsigned long flags;
 	unsigned long *dirty_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
+	u32 flags;
 	int id;
 	bool user_alloc;
 };

commit f82a8cfe9354f5cdea55ebeceba3fd19051d3ee8
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:21 2012 -0700

    KVM: struct kvm_memory_slot.user_alloc -> bool
    
    There's no need for this to be an int, it holds a boolean.
    Move to the end of the struct for alignment.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5a3581ceb036..d897f035749f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -270,8 +270,8 @@ struct kvm_memory_slot {
 	unsigned long *dirty_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
-	int user_alloc;
 	int id;
+	bool user_alloc;
 };
 
 static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memslot)
@@ -451,10 +451,10 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,
-			  int user_alloc);
+			  bool user_alloc);
 int __kvm_set_memory_region(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem,
-			    int user_alloc);
+			    bool user_alloc);
 void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont);
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
@@ -462,11 +462,11 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				struct kvm_memory_slot old,
 				struct kvm_userspace_memory_region *mem,
-				int user_alloc);
+				bool user_alloc);
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
-				int user_alloc);
+				bool user_alloc);
 bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);
 /* flush all memory translations */
@@ -553,7 +553,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 				   struct
 				   kvm_userspace_memory_region *mem,
-				   int user_alloc);
+				   bool user_alloc);
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);

commit 0743247fbf0c4a27185b2aa1fdda91d0745dfed1
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:15 2012 -0700

    KVM: Make KVM_PRIVATE_MEM_SLOTS optional
    
    Seems like everyone copied x86 and defined 4 private memory slots
    that never actually get used.  Even x86 only uses 3 of the 4.  These
    aren't exposed so there's no need to add padding.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index abad7f30771e..5a3581ceb036 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -313,6 +313,10 @@ struct kvm_irq_routing_table {};
 
 #endif
 
+#ifndef KVM_PRIVATE_MEM_SLOTS
+#define KVM_PRIVATE_MEM_SLOTS 0
+#endif
+
 #ifndef KVM_MEM_SLOTS_NUM
 #define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 #endif

commit bbacc0c111c3c5d1f3192b8cc1642b9c3954f80d
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:09 2012 -0700

    KVM: Rename KVM_MEMORY_SLOTS -> KVM_USER_MEM_SLOTS
    
    It's easy to confuse KVM_MEMORY_SLOTS and KVM_MEM_SLOTS_NUM.  One is
    the user accessible slots and the other is user + private.  Make this
    more obvious.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c497ab0d03d..abad7f30771e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -314,7 +314,7 @@ struct kvm_irq_routing_table {};
 #endif
 
 #ifndef KVM_MEM_SLOTS_NUM
-#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+#define KVM_MEM_SLOTS_NUM (KVM_USER_MEM_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 #endif
 
 /*

commit 66cdd0ceaf65a18996f561b770eedde1d123b019
Merge: 896ea17d3da5 58b7825bc324
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 15:31:08 2012 -0800

    Merge tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "Considerable KVM/PPC work, x86 kvmclock vsyscall support,
      IA32_TSC_ADJUST MSR emulation, amongst others."
    
    Fix up trivial conflict in kernel/sched/core.c due to cross-cpu
    migration notifier added next to rq migration call-back.
    
    * tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (156 commits)
      KVM: emulator: fix real mode segment checks in address linearization
      VMX: remove unneeded enable_unrestricted_guest check
      KVM: VMX: fix DPL during entry to protected mode
      x86/kexec: crash_vmclear_local_vmcss needs __rcu
      kvm: Fix irqfd resampler list walk
      KVM: VMX: provide the vmclear function and a bitmap to support VMCLEAR in kdump
      x86/kexec: VMCLEAR VMCSs loaded on all cpus if necessary
      KVM: MMU: optimize for set_spte
      KVM: PPC: booke: Get/set guest EPCR register using ONE_REG interface
      KVM: PPC: bookehv: Add EPCR support in mtspr/mfspr emulation
      KVM: PPC: bookehv: Add guest computation mode for irq delivery
      KVM: PPC: Make EPCR a valid field for booke64 and bookehv
      KVM: PPC: booke: Extend MAS2 EPN mask for 64-bit
      KVM: PPC: e500: Mask MAS2 EPN high 32-bits in 32/64 tlbwe emulation
      KVM: PPC: Mask ea's high 32-bits in 32/64 instr emulation
      KVM: PPC: e500: Add emulation helper for getting instruction ea
      KVM: PPC: bookehv64: Add support for interrupt handling
      KVM: PPC: bookehv: Remove GET_VCPU macro from exception handler
      KVM: PPC: booke: Fix get_tb() compile error on 64-bit
      KVM: PPC: e500: Silence bogus GCC warning in tlb code
      ...

commit d2ff4fc557a4c5248b2d99b0d48e47a246d994b2
Merge: 8f536b7697a0 352df1deb2e3
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Sun Dec 9 18:44:10 2012 -0200

    Merge branch 'for-upstream' of https://github.com/agraf/linux-2.6 into queue
    
    * 'for-upstream' of https://github.com/agraf/linux-2.6: (28 commits)
      KVM: PPC: booke: Get/set guest EPCR register using ONE_REG interface
      KVM: PPC: bookehv: Add EPCR support in mtspr/mfspr emulation
      KVM: PPC: bookehv: Add guest computation mode for irq delivery
      KVM: PPC: Make EPCR a valid field for booke64 and bookehv
      KVM: PPC: booke: Extend MAS2 EPN mask for 64-bit
      KVM: PPC: e500: Mask MAS2 EPN high 32-bits in 32/64 tlbwe emulation
      KVM: PPC: Mask ea's high 32-bits in 32/64 instr emulation
      KVM: PPC: e500: Add emulation helper for getting instruction ea
      KVM: PPC: bookehv64: Add support for interrupt handling
      KVM: PPC: bookehv: Remove GET_VCPU macro from exception handler
      KVM: PPC: booke: Fix get_tb() compile error on 64-bit
      KVM: PPC: e500: Silence bogus GCC warning in tlb code
      KVM: PPC: Book3S HV: Handle guest-caused machine checks on POWER7 without panicking
      KVM: PPC: Book3S HV: Improve handling of local vs. global TLB invalidations
      MAINTAINERS: Add git tree link for PPC KVM
      KVM: PPC: Book3S PR: MSR_DE doesn't exist on Book 3S
      KVM: PPC: Book3S PR: Fix VSX handling
      KVM: PPC: Book3S PR: Emulate PURR, SPURR and DSCR registers
      KVM: PPC: Book3S HV: Don't give the guest RW access to RO pages
      KVM: PPC: Book3S HV: Report correct HPT entry index when reading HPT
      ...

commit 38130ec08716ae2ece8060eca01607b58da7258c
Merge: e783377e93d4 1b2852b152be
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Dec 8 15:44:43 2012 +0100

    Merge tag 'sched-cputime-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into sched/core
    
    Pull more cputime cleanups from Frederic Weisbecker:
    
     * Get rid of underscores polluting the vtime namespace
    
     * Consolidate context switch and tick handling
    
     * Improve debuggability by detecting irq unsafe callers
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 222e82bef4bd520a31d48c31ab24e49dd46daa46
Merge: 38ca9c927c7d 18a2f371f5ed
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 7 12:15:33 2012 +0100

    Merge branch 'linus' into sched/core
    
    Pick up the autogroups fix and other fixes.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 914daba865cb5c38cd5fdee024ca38029315b38f
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Oct 9 00:22:59 2012 +0200

    KVM: Distangle eventfd code from irqchip
    
    The current eventfd code assumes that when we have eventfd, we also have
    irqfd for in-kernel interrupt delivery. This is not necessarily true. On
    PPC we don't have an in-kernel irqchip yet, but we can still support easily
    support eventfd.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8e5c7b651655..c823e47c3641 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -900,10 +900,20 @@ static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 
 void kvm_eventfd_init(struct kvm *kvm);
+int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
+
+#ifdef CONFIG_HAVE_KVM_IRQCHIP
 int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args);
 void kvm_irqfd_release(struct kvm *kvm);
 void kvm_irq_routing_update(struct kvm *, struct kvm_irq_routing_table *);
-int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
+#else
+static inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
+{
+	return -EINVAL;
+}
+
+static inline void kvm_irqfd_release(struct kvm *kvm) {}
+#endif
 
 #else
 

commit 01f218803757c9ec1152ac2fd39d03c27c452634
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Wed Oct 17 18:06:02 2012 +0200

    kvm: add kvm_set_irq_inatomic
    
    Add an API to inject IRQ from atomic context.
    Return EWOULDBLOCK if impossible (e.g. for multicast).
    Only MSI is supported ATM.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8e5c7b651655..36c3704bfa7c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -693,6 +693,7 @@ void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
 				   unsigned long *deliver_bitmask);
 #endif
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level);
+int kvm_set_irq_inatomic(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
 		int irq_source_id, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);

commit 42897d866b120547777ae1fd316680ec53356d9c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:02 2012 -0200

    KVM: x86: add kvm_arch_vcpu_postcreate callback, move TSC initialization
    
    TSC initialization will soon make use of online_vcpus.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c94c9985dee0..8e5c7b651655 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -596,6 +596,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
+int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 int kvm_arch_hardware_enable(void *garbage);

commit d828199e84447795c6669ff0e6c6d55eb9beeff6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:01 2012 -0200

    KVM: x86: implement PVCLOCK_TSC_STABLE_BIT pvclock flag
    
    KVM added a global variable to guarantee monotonicity in the guest.
    One of the reasons for that is that the time between
    
            1. ktime_get_ts(&timespec);
            2. rdtscll(tsc);
    
    Is variable. That is, given a host with stable TSC, suppose that
    two VCPUs read the same time via ktime_get_ts() above.
    
    The time required to execute 2. is not the same on those two instances
    executing in different VCPUS (cache misses, interrupts...).
    
    If the TSC value that is used by the host to interpolate when
    calculating the monotonic time is the same value used to calculate
    the tsc_timestamp value stored in the pvclock data structure, and
    a single <system_timestamp, tsc_timestamp> tuple is visible to all
    vcpus simultaneously, this problem disappears. See comment on top
    of pvclock_update_vm_gtod_copy for details.
    
    Monotonicity is then guaranteed by synchronicity of the host TSCs
    and guest TSCs.
    
    Set TSC stable pvclock flag in that case, allowing the guest to read
    clock from userspace.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 99a47627e046..c94c9985dee0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -131,6 +131,8 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_PMU               16
 #define KVM_REQ_PMI               17
 #define KVM_REQ_WATCHDOG          18
+#define KVM_REQ_MASTERCLOCK_UPDATE 19
+#define KVM_REQ_MCLOCK_INPROGRESS 20
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
@@ -540,6 +542,7 @@ void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
+void kvm_make_mclock_inprogress_request(struct kvm *kvm);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);

commit fd25b4c2f226de818e1d2b71e3e681d28bcaf5ba
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 18:21:22 2012 +0100

    vtime: Remove the underscore prefix invasion
    
    Prepending irq-unsafe vtime APIs with underscores was actually
    a bad idea as the result is a big mess in the API namespace that
    is even waiting to be further extended. Also these helpers
    are always called from irq safe callers except kvm. Just
    provide a vtime_account_system_irqsafe() for this specific
    case so that we can remove the underscore prefix on other
    vtime functions.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0e2212fe4784..f17158bdd4fc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -741,7 +741,7 @@ static inline void kvm_guest_enter(void)
 	 * This is running in ioctl context so we can avoid
 	 * the call to vtime_account() with its unnecessary idle check.
 	 */
-	vtime_account_system(current);
+	vtime_account_system_irqsafe(current);
 	current->flags |= PF_VCPU;
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
@@ -759,7 +759,7 @@ static inline void kvm_guest_exit(void)
 	 * This is running in ioctl context so we can avoid
 	 * the call to vtime_account() with its unnecessary idle check.
 	 */
-	vtime_account_system(current);
+	vtime_account_system_irqsafe(current);
 	current->flags &= ~PF_VCPU;
 }
 

commit 87da7e66a40532b743cd50972fcf85a1f15b14ea
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Wed Oct 24 14:07:59 2012 +0800

    KVM: x86: fix vcpu->mmio_fragments overflow
    
    After commit b3356bf0dbb349 (KVM: emulator: optimize "rep ins" handling),
    the pieces of io data can be collected and write them to the guest memory
    or MMIO together
    
    Unfortunately, kvm splits the mmio access into 8 bytes and store them to
    vcpu->mmio_fragments. If the guest uses "rep ins" to move large data, it
    will cause vcpu->mmio_fragments overflow
    
    The bug can be exposed by isapc (-M isapc):
    
    [23154.818733] general protection fault: 0000 [#1] SMP DEBUG_PAGEALLOC
    [ ......]
    [23154.858083] Call Trace:
    [23154.859874]  [<ffffffffa04f0e17>] kvm_get_cr8+0x1d/0x28 [kvm]
    [23154.861677]  [<ffffffffa04fa6d4>] kvm_arch_vcpu_ioctl_run+0xcda/0xe45 [kvm]
    [23154.863604]  [<ffffffffa04f5a1a>] ? kvm_arch_vcpu_load+0x17b/0x180 [kvm]
    
    Actually, we can use one mmio_fragment to store a large mmio access then
    split it when we pass the mmio-exit-info to userspace. After that, we only
    need two entries to store mmio info for the cross-mmio pages access
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93bfc9f9815c..ecc554374e44 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -42,19 +42,8 @@
  */
 #define KVM_MEMSLOT_INVALID	(1UL << 16)
 
-/*
- * If we support unaligned MMIO, at most one fragment will be split into two:
- */
-#ifdef KVM_UNALIGNED_MMIO
-#  define KVM_EXTRA_MMIO_FRAGMENTS 1
-#else
-#  define KVM_EXTRA_MMIO_FRAGMENTS 0
-#endif
-
-#define KVM_USER_MMIO_SIZE 8
-
-#define KVM_MAX_MMIO_FRAGMENTS \
-	(KVM_MMIO_SIZE / KVM_USER_MMIO_SIZE + KVM_EXTRA_MMIO_FRAGMENTS)
+/* Two fragments for cross MMIO pages. */
+#define KVM_MAX_MMIO_FRAGMENTS	2
 
 /*
  * For the normal pfn, the highest 12 bits should be zero,

commit 81c52c56e2b43589091ee29038bcf793d3f184ab
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Oct 16 20:10:59 2012 +0800

    KVM: do not treat noslot pfn as a error pfn
    
    This patch filters noslot pfn out from error pfns based on Marcelo comment:
    noslot pfn is not a error pfn
    
    After this patch,
    - is_noslot_pfn indicates that the gfn is not in slot
    - is_error_pfn indicates that the gfn is in slot but the error is occurred
      when translate the gfn to pfn
    - is_error_noslot_pfn indicates that the pfn either it is error pfns or it
      is noslot pfn
    And is_invalid_pfn can be removed, it makes the code more clean
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 82e2c783a21e..99a47627e046 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -58,28 +58,40 @@
 
 /*
  * For the normal pfn, the highest 12 bits should be zero,
- * so we can mask these bits to indicate the error.
+ * so we can mask bit 62 ~ bit 52  to indicate the error pfn,
+ * mask bit 63 to indicate the noslot pfn.
  */
-#define KVM_PFN_ERR_MASK	(0xfffULL << 52)
+#define KVM_PFN_ERR_MASK	(0x7ffULL << 52)
+#define KVM_PFN_ERR_NOSLOT_MASK	(0xfffULL << 52)
+#define KVM_PFN_NOSLOT		(0x1ULL << 63)
 
 #define KVM_PFN_ERR_FAULT	(KVM_PFN_ERR_MASK)
 #define KVM_PFN_ERR_HWPOISON	(KVM_PFN_ERR_MASK + 1)
-#define KVM_PFN_ERR_BAD		(KVM_PFN_ERR_MASK + 2)
-#define KVM_PFN_ERR_RO_FAULT	(KVM_PFN_ERR_MASK + 3)
+#define KVM_PFN_ERR_RO_FAULT	(KVM_PFN_ERR_MASK + 2)
 
+/*
+ * error pfns indicate that the gfn is in slot but faild to
+ * translate it to pfn on host.
+ */
 static inline bool is_error_pfn(pfn_t pfn)
 {
 	return !!(pfn & KVM_PFN_ERR_MASK);
 }
 
-static inline bool is_noslot_pfn(pfn_t pfn)
+/*
+ * error_noslot pfns indicate that the gfn can not be
+ * translated to pfn - it is not in slot or failed to
+ * translate it to pfn.
+ */
+static inline bool is_error_noslot_pfn(pfn_t pfn)
 {
-	return pfn == KVM_PFN_ERR_BAD;
+	return !!(pfn & KVM_PFN_ERR_NOSLOT_MASK);
 }
 
-static inline bool is_invalid_pfn(pfn_t pfn)
+/* noslot pfn indicates that the gfn is not in slot. */
+static inline bool is_noslot_pfn(pfn_t pfn)
 {
-	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
+	return pfn == KVM_PFN_NOSLOT;
 }
 
 #define KVM_HVA_ERR_BAD		(PAGE_OFFSET)

commit b080935c8638e08134629d0a9ebdf35669bec14d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 5 23:07:19 2012 +0200

    kvm: Directly account vtime to system on guest switch
    
    Switching to or from guest context is done on ioctl context.
    So by the time we call kvm_guest_enter() or kvm_guest_exit()
    we know we are not running the idle task.
    
    As a result, we can directly account the cputime using
    vtime_account_system().
    
    There are two good reasons to do this:
    
    * We avoid some useless checks on guest switch. It optimizes
    a bit this fast path.
    
    * In the case of CONFIG_IRQ_TIME_ACCOUNTING, calling vtime_account()
    checks for irq time to account. This is pointless since we know
    we are not in an irq on guest switch. This is wasting cpu cycles
    for no good reason. vtime_account_system() OTOH is a no-op in
    this config option.
    
    * We can remove the irq disable/enable around kvm guest switch in s390.
    
    A further optimization may consist in introducing a vtime_account_guest()
    that directly calls account_guest_time().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Joerg Roedel <joerg.roedel@amd.com>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Xiantao Zhang <xiantao.zhang@intel.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Cornelia Huck <cornelia.huck@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93bfc9f9815c..0e2212fe4784 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -737,7 +737,11 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 static inline void kvm_guest_enter(void)
 {
 	BUG_ON(preemptible());
-	vtime_account(current);
+	/*
+	 * This is running in ioctl context so we can avoid
+	 * the call to vtime_account() with its unnecessary idle check.
+	 */
+	vtime_account_system(current);
 	current->flags |= PF_VCPU;
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
@@ -751,7 +755,11 @@ static inline void kvm_guest_enter(void)
 
 static inline void kvm_guest_exit(void)
 {
-	vtime_account(current);
+	/*
+	 * This is running in ioctl context so we can avoid
+	 * the call to vtime_account() with its unnecessary idle check.
+	 */
+	vtime_account_system(current);
 	current->flags &= ~PF_VCPU;
 }
 

commit 8ca40a70a70988c0bdea106c894843f763ca2989
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Sun Oct 14 23:10:18 2012 -0400

    KVM: Take kvm instead of vcpu to mmu_notifier_retry
    
    The mmu_notifier_retry is not specific to any vcpu (and never will be)
    so only take struct kvm as a parameter.
    
    The motivation is the ARM mmu code that needs to call this from
    somewhere where we long let go of the vcpu pointer.
    
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6afc5be2615e..82e2c783a21e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -841,9 +841,9 @@ extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 
 #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
-static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_seq)
+static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 {
-	if (unlikely(vcpu->kvm->mmu_notifier_count))
+	if (unlikely(kvm->mmu_notifier_count))
 		return 1;
 	/*
 	 * Ensure the read of mmu_notifier_count happens before the read
@@ -856,7 +856,7 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 	 * can't rely on kvm->mmu_lock to keep things ordered.
 	 */
 	smp_rmb();
-	if (vcpu->kvm->mmu_notifier_seq != mmu_seq)
+	if (kvm->mmu_notifier_seq != mmu_seq)
 		return 1;
 	return 0;
 }

commit 03604b31142058362db13e7881385806977893f5
Merge: 87cac8f879a5 12ecd9570d89
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Oct 10 19:03:54 2012 -0300

    Merge branch 'for-upstream' of http://github.com/agraf/linux-2.6 into queue
    
    * 'for-upstream' of http://github.com/agraf/linux-2.6: (56 commits)
      arch/powerpc/kvm/e500_tlb.c: fix error return code
      KVM: PPC: Book3S HV: Provide a way for userspace to get/set per-vCPU areas
      KVM: PPC: Book3S: Get/set guest FP regs using the GET/SET_ONE_REG interface
      KVM: PPC: Book3S: Get/set guest SPRs using the GET/SET_ONE_REG interface
      KVM: PPC: set IN_GUEST_MODE before checking requests
      KVM: PPC: e500: MMU API: fix leak of shared_tlb_pages
      KVM: PPC: e500: fix allocation size error on g2h_tlb1_map
      KVM: PPC: Book3S HV: Fix calculation of guest phys address for MMIO emulation
      KVM: PPC: Book3S HV: Remove bogus update of physical thread IDs
      KVM: PPC: Book3S HV: Fix updates of vcpu->cpu
      KVM: Move some PPC ioctl definitions to the correct place
      KVM: PPC: Book3S HV: Handle memory slot deletion and modification correctly
      KVM: PPC: Move kvm->arch.slot_phys into memslot.arch
      KVM: PPC: Book3S HV: Take the SRCU read lock before looking up memslots
      KVM: PPC: bookehv: Allow duplicate calls of DO_KVM macro
      KVM: PPC: BookE: Support FPU on non-hv systems
      KVM: PPC: 440: Implement mfdcrx
      KVM: PPC: 440: Implement mtdcrx
      Document IACx/DACx registers access using ONE_REG API
      KVM: PPC: E500: Remove E500_TLB_DIRTY flag
      ...

commit 8b6e4547e0e4b6aac11df6d8d4e71ea2ab159b5e
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Thu Sep 20 07:43:08 2012 +0200

    KVM: x86: Convert kvm_arch_vcpu_reset into private kvm_vcpu_reset
    
    There are no external callers of this function as there is no concept of
    resetting a vcpu from generic code.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93bfc9f9815c..c35b1c08c004 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -582,7 +582,6 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
 int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
-int kvm_arch_vcpu_reset(struct kvm_vcpu *vcpu);
 int kvm_arch_hardware_enable(void *garbage);
 void kvm_arch_hardware_disable(void *garbage);
 int kvm_arch_hardware_setup(void);

commit f61c94bb99ca4253ac5dd57750e1af209a4beb7a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Aug 8 20:38:19 2012 +0000

    KVM: PPC: booke: Add watchdog emulation
    
    This patch adds the watchdog emulation in KVM. The watchdog
    emulation is enabled by KVM_ENABLE_CAP(KVM_CAP_PPC_BOOKE_WATCHDOG) ioctl.
    The kernel timer are used for watchdog emulation and emulates
    h/w watchdog state machine. On watchdog timer expiry, it exit to QEMU
    if TCR.WRC is non ZERO. QEMU can reset/shutdown etc depending upon how
    it is configured.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [bharat.bhushan@freescale.com: reworked patch]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    [agraf: adjust to new request framework]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2850656e2e96..0ca3663206f8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -118,6 +118,7 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_IMMEDIATE_EXIT    15
 #define KVM_REQ_PMU               16
 #define KVM_REQ_PMI               17
+#define KVM_REQ_WATCHDOG          18
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID		0
 #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1

commit ecefbd94b834fa32559d854646d777c56749ef1c
Merge: ce57e981f2b9 3d11df7abbff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 4 09:30:33 2012 -0700

    Merge tag 'kvm-3.7-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Avi Kivity:
     "Highlights of the changes for this release include support for vfio
      level triggered interrupts, improved big real mode support on older
      Intels, a streamlines guest page table walker, guest APIC speedups,
      PIO optimizations, better overcommit handling, and read-only memory."
    
    * tag 'kvm-3.7-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (138 commits)
      KVM: s390: Fix vcpu_load handling in interrupt code
      KVM: x86: Fix guest debug across vcpu INIT reset
      KVM: Add resampling irqfds for level triggered interrupts
      KVM: optimize apic interrupt delivery
      KVM: MMU: Eliminate pointless temporary 'ac'
      KVM: MMU: Avoid access/dirty update loop if all is well
      KVM: MMU: Eliminate eperm temporary
      KVM: MMU: Optimize is_last_gpte()
      KVM: MMU: Simplify walk_addr_generic() loop
      KVM: MMU: Optimize pte permission checks
      KVM: MMU: Update accessed and dirty bits after guest pagetable walk
      KVM: MMU: Move gpte_access() out of paging_tmpl.h
      KVM: MMU: Optimize gpte_access() slightly
      KVM: MMU: Push clean gpte write protection out of gpte_access()
      KVM: clarify kvmclock documentation
      KVM: make processes waiting on vcpu mutex killable
      KVM: SVM: Make use of asm.h
      KVM: VMX: Make use of asm.h
      KVM: VMX: Make lto-friendly
      KVM: x86: lapic: Clean up find_highest_vector() and count_vectors()
      ...
    
    Conflicts:
            arch/s390/include/asm/processor.h
            arch/x86/kvm/i8259.c

commit bf9fae9f5e4ca8dce4708812f9ad6281e61df109
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 15:23:11 2012 +0200

    cputime: Use a proper subsystem naming for vtime related APIs
    
    Use a naming based on vtime as a prefix for virtual based
    cputime accounting APIs:
    
    - account_system_vtime() -> vtime_account()
    - account_switch_vtime() -> vtime_task_switch()
    
    It makes it easier to allow for further declension such
    as vtime_account_system(), vtime_account_idle(), ... if we
    want to find out the context we account to from generic code.
    
    This also make it better to know on which subsystem these APIs
    refer to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b70b48b01098..8a59e0abe5fa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -685,7 +685,7 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 static inline void kvm_guest_enter(void)
 {
 	BUG_ON(preemptible());
-	account_system_vtime(current);
+	vtime_account(current);
 	current->flags |= PF_VCPU;
 	/* KVM does not hold any references to rcu protected data when it
 	 * switches CPU into a guest mode. In fact switching to a guest mode
@@ -699,7 +699,7 @@ static inline void kvm_guest_enter(void)
 
 static inline void kvm_guest_exit(void)
 {
-	account_system_vtime(current);
+	vtime_account(current);
 	current->flags &= ~PF_VCPU;
 }
 

commit 7a84428af7ca6a847f058c9ff244a18a2664fd1b
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri Sep 21 11:58:03 2012 -0600

    KVM: Add resampling irqfds for level triggered interrupts
    
    To emulate level triggered interrupts, add a resample option to
    KVM_IRQFD.  When specified, a new resamplefd is provided that notifies
    the user when the irqchip has been resampled by the VM.  This may, for
    instance, indicate an EOI.  Also in this mode, posting of an interrupt
    through an irqfd only asserts the interrupt.  On resampling, the
    interrupt is automatically de-asserted prior to user notification.
    This enables level triggered interrupts to be posted and re-enabled
    from vfio with no userspace intervention.
    
    All resampling irqfds can make use of a single irq source ID, so we
    reserve a new one for this interface.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 80bfc880921e..2850656e2e96 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -119,7 +119,8 @@ static inline bool is_error_page(struct page *page)
 #define KVM_REQ_PMU               16
 #define KVM_REQ_PMI               17
 
-#define KVM_USERSPACE_IRQ_SOURCE_ID	0
+#define KVM_USERSPACE_IRQ_SOURCE_ID		0
+#define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID	1
 
 struct kvm;
 struct kvm_vcpu;
@@ -343,6 +344,8 @@ struct kvm {
 	struct {
 		spinlock_t        lock;
 		struct list_head  items;
+		struct list_head  resampler_list;
+		struct mutex      resampler_lock;
 	} irqfds;
 	struct list_head ioeventfds;
 #endif

commit 9fc77441e5e1bf80b794cc546d2243ee9f4afb75
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Sep 16 11:50:30 2012 +0300

    KVM: make processes waiting on vcpu mutex killable
    
    vcpu mutex can be held for unlimited time so
    taking it with mutex_lock on an ioctl is wrong:
    one process could be passed a vcpu fd and
    call this ioctl on the vcpu used by another process,
    it will then be unkillable until the owner exits.
    
    Call mutex_lock_killable instead and return status.
    Note: mutex_lock_interruptible would be even nicer,
    but I am not sure all users are prepared to handle EINTR
    from these ioctls. They might misinterpret it as an error.
    
    Cleanup paths expect a vcpu that can't be used by
    any userspace so this will always succeed - catch bugs
    by calling BUG_ON.
    
    Catch callers that don't check return state by adding
    __must_check.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 40791930bc15..80bfc880921e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -408,7 +408,7 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 
-void vcpu_load(struct kvm_vcpu *vcpu);
+int __must_check vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,

commit 2df72e9bc4c505d8357012f2924589f3d16f9d44
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Aug 24 15:54:57 2012 -0300

    KVM: split kvm_arch_flush_shadow
    
    Introducing kvm_arch_flush_shadow_memslot, to invalidate the
    translations of a single memory slot.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9c0b3c3ae0a5..40791930bc15 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -458,7 +458,11 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				int user_alloc);
 bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);
-void kvm_arch_flush_shadow(struct kvm *kvm);
+/* flush all memory translations */
+void kvm_arch_flush_shadow_all(struct kvm *kvm);
+/* flush memory translations pointing to 'slot' */
+void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
+				   struct kvm_memory_slot *slot);
 
 int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
 			    int nr_pages);

commit 66a03505a7fcc70187319ef2318832f4d3c451a6
Author: Gavin Shan <shangw@linux.vnet.ibm.com>
Date:   Fri Aug 24 16:50:28 2012 +0800

    KVM: PPC: book3s: fix build error caused by gfn_to_hva_memslot()
    
    The build error was caused by that builtin functions are calling
    the functions implemented in modules. This error was introduced by
    commit 4d8b81abc4 ("KVM: introduce readonly memslot").
    
    The patch fixes the build error by moving function __gfn_to_hva_memslot()
    from kvm_main.c to kvm_host.h and making that "inline" so that the
    builtin function (kvmppc_h_enter) can use that.
    
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Gavin Shan <shangw@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5972c9845ddb..9c0b3c3ae0a5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -773,6 +773,12 @@ __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 	return search_memslots(slots, gfn);
 }
 
+static inline unsigned long
+__gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
+{
+	return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
+}
+
 static inline int memslot_id(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn_to_memslot(kvm, gfn)->id;

commit 4d8b81abc47b83a1939e59df2fdb0e98dfe0eedd
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 11:02:51 2012 +0800

    KVM: introduce readonly memslot
    
    In current code, if we map a readonly memory space from host to guest
    and the page is not currently mapped in the host, we will get a fault
    pfn and async is not allowed, then the vm will crash
    
    We introduce readonly memory region to map ROM/ROMD to the guest, read access
    is happy for readonly memslot, write access on readonly memslot will cause
    KVM_EXIT_MMIO exit
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a913ac709a9d..5972c9845ddb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -465,6 +465,7 @@ int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
+unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_dirty(struct page *page);
@@ -792,12 +793,6 @@ hva_to_gfn_memslot(unsigned long hva, struct kvm_memory_slot *slot)
 	return slot->base_gfn + gfn_offset;
 }
 
-static inline unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
-					       gfn_t gfn)
-{
-	return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
-}
-
 static inline gpa_t gfn_to_gpa(gfn_t gfn)
 {
 	return (gpa_t)gfn << PAGE_SHIFT;

commit 7068d0971524dd47a38f44f6020ba133432871ca
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 11:02:22 2012 +0800

    KVM: introduce KVM_HVA_ERR_RO_BAD
    
    In the later patch, it indicates failure when we try to get a writable
    hva from the readonly memslot
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b9bba60c298b..a913ac709a9d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -82,11 +82,12 @@ static inline bool is_invalid_pfn(pfn_t pfn)
 	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
 }
 
-#define KVM_HVA_ERR_BAD	(PAGE_OFFSET)
+#define KVM_HVA_ERR_BAD		(PAGE_OFFSET)
+#define KVM_HVA_ERR_RO_BAD	(PAGE_OFFSET + PAGE_SIZE)
 
 static inline bool kvm_is_error_hva(unsigned long addr)
 {
-	return addr == PAGE_OFFSET;
+	return addr >= PAGE_OFFSET;
 }
 
 #define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))

commit ca3a490c7de8472b514e2d635c052b1e0f8e1c9d
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 11:01:50 2012 +0800

    KVM: introduce KVM_HVA_ERR_BAD
    
    Then, remove bad_hva and inline kvm_is_error_hva
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4fd33e03d5dc..b9bba60c298b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -82,6 +82,13 @@ static inline bool is_invalid_pfn(pfn_t pfn)
 	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
 }
 
+#define KVM_HVA_ERR_BAD	(PAGE_OFFSET)
+
+static inline bool kvm_is_error_hva(unsigned long addr)
+{
+	return addr == PAGE_OFFSET;
+}
+
 #define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))
 
 static inline bool is_error_page(struct page *page)
@@ -430,7 +437,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 	return slot;
 }
 
-int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,
 			  int user_alloc);

commit 69552c295e3df1431801526b5b705a0e130c920d
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 11:01:19 2012 +0800

    KVM: introduce KVM_PFN_ERR_RO_FAULT
    
    In the later patch, it indicates failure when we try to get a writable
    pfn from the readonly memslot
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 52c86e4f6d8c..4fd33e03d5dc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -65,6 +65,7 @@
 #define KVM_PFN_ERR_FAULT	(KVM_PFN_ERR_MASK)
 #define KVM_PFN_ERR_HWPOISON	(KVM_PFN_ERR_MASK + 1)
 #define KVM_PFN_ERR_BAD		(KVM_PFN_ERR_MASK + 2)
+#define KVM_PFN_ERR_RO_FAULT	(KVM_PFN_ERR_MASK + 3)
 
 static inline bool is_error_pfn(pfn_t pfn)
 {

commit 037d92dc5d4691ae7cf44699c55ca83b1b441992
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 10:59:12 2012 +0800

    KVM: introduce gfn_to_pfn_memslot_atomic
    
    It can instead of hva_to_pfn_atomic
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d4bd4d41e355..52c86e4f6d8c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -462,7 +462,6 @@ void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
-pfn_t hva_to_pfn_atomic(unsigned long addr);
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,
 		       bool write_fault, bool *writable);
@@ -470,6 +469,8 @@ pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
 pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
+pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
+
 void kvm_release_pfn_dirty(pfn_t pfn);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);

commit 67b29204c8c9ecb4b2799a06ab646eeb363a0fe6
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Aug 21 10:58:45 2012 +0800

    KVM: hide KVM_MEMSLOT_INVALID from userspace
    
    Quote Avi's comment:
    | KVM_MEMSLOT_INVALID is actually an internal symbol, not used by
    | userspace.  Please move it to kvm_host.h.
    
    Also, we divide the memlsot->flags into two parts, the lower 16 bits
    are visible for userspace, the higher 16 bits are internally used in
    kvm
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d2b897ee3ac4..d4bd4d41e355 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -35,6 +35,13 @@
 #define KVM_MMIO_SIZE 8
 #endif
 
+/*
+ * The bit 16 ~ bit 31 of kvm_memory_region::flags are internally used
+ * in kvm, other bits are visible for userspace which are defined in
+ * include/linux/kvm_h.
+ */
+#define KVM_MEMSLOT_INVALID	(1UL << 16)
+
 /*
  * If we support unaligned MMIO, at most one fragment will be split into two:
  */

commit 9c5b11728344e1085593f494ddc8838497e7ffde
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:43:51 2012 +0800

    KVM: let the error pfn not depend on error code
    
    Currently, we use the error code as error pfn to indicat the error
    condition, it is not straightforward and it will not work on PAE
    32-bit cpu with huge memory, since the valid physical address
    can be at most 52 bits
    
    For the normal pfn, the highest 12 bits should be zero, so we can
    mask these bits to indicate the error.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 07226f820e6c..d2b897ee3ac4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -49,28 +49,34 @@
 #define KVM_MAX_MMIO_FRAGMENTS \
 	(KVM_MMIO_SIZE / KVM_USER_MMIO_SIZE + KVM_EXTRA_MMIO_FRAGMENTS)
 
-#define KVM_PFN_ERR_FAULT	(-EFAULT)
-#define KVM_PFN_ERR_HWPOISON	(-EHWPOISON)
-#define KVM_PFN_ERR_BAD		(-ENOENT)
+/*
+ * For the normal pfn, the highest 12 bits should be zero,
+ * so we can mask these bits to indicate the error.
+ */
+#define KVM_PFN_ERR_MASK	(0xfffULL << 52)
+
+#define KVM_PFN_ERR_FAULT	(KVM_PFN_ERR_MASK)
+#define KVM_PFN_ERR_HWPOISON	(KVM_PFN_ERR_MASK + 1)
+#define KVM_PFN_ERR_BAD		(KVM_PFN_ERR_MASK + 2)
 
-static inline int is_error_pfn(pfn_t pfn)
+static inline bool is_error_pfn(pfn_t pfn)
 {
-	return IS_ERR_VALUE(pfn);
+	return !!(pfn & KVM_PFN_ERR_MASK);
 }
 
-static inline int is_noslot_pfn(pfn_t pfn)
+static inline bool is_noslot_pfn(pfn_t pfn)
 {
-	return pfn == -ENOENT;
+	return pfn == KVM_PFN_ERR_BAD;
 }
 
-static inline int is_invalid_pfn(pfn_t pfn)
+static inline bool is_invalid_pfn(pfn_t pfn)
 {
 	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
 }
 
 #define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))
 
-static inline int is_error_page(struct page *page)
+static inline bool is_error_page(struct page *page)
 {
 	return IS_ERR(page);
 }

commit 32cad84f44d186654492f1a50a1424c8906ccbd9
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:42:52 2012 +0800

    KVM: do not release the error page
    
    After commit a2766325cf9f9, the error page is replaced by the
    error code, it need not be released anymore
    
    [ The patch has been compiling tested for powerpc ]
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ce7c32950f4e..07226f820e6c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -457,7 +457,7 @@ pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
 pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
-void kvm_release_pfn_dirty(pfn_t);
+void kvm_release_pfn_dirty(pfn_t pfn);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
 void kvm_set_pfn_accessed(pfn_t pfn);

commit 6cede2e6794be6b0649f62d3681e0c4aff5a9270
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:41:22 2012 +0800

    KVM: introduce KVM_ERR_PTR_BAD_PAGE
    
    It is used to eliminate the overload of function call and cleanup
    the code
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e2dcc7cb2284..ce7c32950f4e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -68,6 +68,13 @@ static inline int is_invalid_pfn(pfn_t pfn)
 	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
 }
 
+#define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))
+
+static inline int is_error_page(struct page *page)
+{
+	return IS_ERR(page);
+}
+
 /*
  * vcpu->requests bit members
  */
@@ -409,7 +416,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 	return slot;
 }
 
-int is_error_page(struct page *page);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,
@@ -436,7 +442,6 @@ void kvm_arch_flush_shadow(struct kvm *kvm);
 int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
 			    int nr_pages);
 
-struct page *get_bad_page(void);
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);

commit 9a592a953880fd6981955e69c1476ce541d9bd16
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:40:39 2012 +0800

    KVM: remove the unused declare
    
    Remove it since it is not used anymore
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 19d91ceaf5e6..e2dcc7cb2284 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -409,8 +409,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 	return slot;
 }
 
-extern struct page *bad_page;
-
 int is_error_page(struct page *page);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,

commit 83f09228d068911ac8797ae8d6febef886698936
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:39:59 2012 +0800

    KVM: inline is_*_pfn functions
    
    These functions are exported and can not inline, move them
    to kvm_host.h to eliminate the overload of function call
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b2cf3109822e..19d91ceaf5e6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -21,6 +21,7 @@
 #include <linux/slab.h>
 #include <linux/rcupdate.h>
 #include <linux/ratelimit.h>
+#include <linux/err.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -52,6 +53,21 @@
 #define KVM_PFN_ERR_HWPOISON	(-EHWPOISON)
 #define KVM_PFN_ERR_BAD		(-ENOENT)
 
+static inline int is_error_pfn(pfn_t pfn)
+{
+	return IS_ERR_VALUE(pfn);
+}
+
+static inline int is_noslot_pfn(pfn_t pfn)
+{
+	return pfn == -ENOENT;
+}
+
+static inline int is_invalid_pfn(pfn_t pfn)
+{
+	return !is_noslot_pfn(pfn) && is_error_pfn(pfn);
+}
+
 /*
  * vcpu->requests bit members
  */
@@ -396,9 +412,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 extern struct page *bad_page;
 
 int is_error_page(struct page *page);
-int is_error_pfn(pfn_t pfn);
-int is_noslot_pfn(pfn_t pfn);
-int is_invalid_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,

commit 950e95097b1c6573ef5e21061ccb56964278c45b
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:39:19 2012 +0800

    KVM: introduce KVM_PFN_ERR_BAD
    
    Then, remove get_bad_pfn
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 840f44a096c9..b2cf3109822e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -50,6 +50,7 @@
 
 #define KVM_PFN_ERR_FAULT	(-EFAULT)
 #define KVM_PFN_ERR_HWPOISON	(-EHWPOISON)
+#define KVM_PFN_ERR_BAD		(-ENOENT)
 
 /*
  * vcpu->requests bit members

commit e6c1502b3f933ace20c711ce34ab696f5a67086d
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:38:36 2012 +0800

    KVM: introduce KVM_PFN_ERR_HWPOISON
    
    Then, get_hwpoison_pfn and is_hwpoison_pfn can be removed
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ef5554f47486..840f44a096c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -49,6 +49,7 @@
 	(KVM_MMIO_SIZE / KVM_USER_MMIO_SIZE + KVM_EXTRA_MMIO_FRAGMENTS)
 
 #define KVM_PFN_ERR_FAULT	(-EFAULT)
+#define KVM_PFN_ERR_HWPOISON	(-EHWPOISON)
 
 /*
  * vcpu->requests bit members
@@ -395,7 +396,6 @@ extern struct page *bad_page;
 
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);
-int is_hwpoison_pfn(pfn_t pfn);
 int is_noslot_pfn(pfn_t pfn);
 int is_invalid_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);

commit 6c8ee57be9350c5c2cafdd6a99d0462d528676e2
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Aug 3 15:37:54 2012 +0800

    KVM: introduce KVM_PFN_ERR_FAULT
    
    After that, the exported and un-inline function, get_fault_pfn,
    can be removed
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3c16f0f1fe35..ef5554f47486 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -48,6 +48,8 @@
 #define KVM_MAX_MMIO_FRAGMENTS \
 	(KVM_MMIO_SIZE / KVM_USER_MMIO_SIZE + KVM_EXTRA_MMIO_FRAGMENTS)
 
+#define KVM_PFN_ERR_FAULT	(-EFAULT)
+
 /*
  * vcpu->requests bit members
  */
@@ -443,7 +445,6 @@ void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
 void kvm_set_pfn_accessed(pfn_t pfn);
 void kvm_get_pfn(pfn_t pfn);
-pfn_t get_fault_pfn(void);
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);

commit d89cc617b954aff4030fce178f7d86f59aaf713d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Aug 1 18:03:28 2012 +0900

    KVM: Push rmap into kvm_arch_memory_slot
    
    Two reasons:
     - x86 can integrate rmap and rmap_pde and remove heuristics in
       __gfn_to_rmap().
     - Some architectures do not need rmap.
    
    Since rmap is one of the most memory consuming stuff in KVM, ppc'd
    better restrict the allocation to Book3S HV.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index dbc65f9d6a2b..3c16f0f1fe35 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -213,7 +213,6 @@ struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
 	unsigned long flags;
-	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;

commit 23d43cf998275bc97437931c0cdee1df2c1aa3ca
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Tue Jul 24 08:51:20 2012 -0400

    KVM: Move KVM_IRQ_LINE to arch-generic code
    
    Handle KVM_IRQ_LINE and KVM_IRQ_LINE_STATUS in the generic
    kvm_vm_ioctl() function and call into kvm_vm_ioctl_irq_line().
    
    This is even more relevant when KVM/ARM also uses this ioctl.
    
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4e60d3695e4e..dbc65f9d6a2b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -498,6 +498,7 @@ int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 				   struct
 				   kvm_userspace_memory_region *mem,
 				   int user_alloc);
+int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);
 

commit a2766325cf9f9e36d1225145f1ce1b066f001837
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Jul 26 11:58:59 2012 +0800

    KVM: remove dummy pages
    
    Currently, kvm allocates some pages and use them as error indicators,
    it wastes memory and is not good for scalability
    
    Base on Avi's suggestion, we use the error codes instead of these pages
    to indicate the error conditions
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1993eb1cb2cd..4e60d3695e4e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -423,6 +423,7 @@ void kvm_arch_flush_shadow(struct kvm *kvm);
 int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
 			    int nr_pages);
 
+struct page *get_bad_page(void);
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
@@ -576,7 +577,7 @@ void kvm_arch_sync_events(struct kvm *kvm);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
-int kvm_is_mmio_pfn(pfn_t pfn);
+bool kvm_is_mmio_pfn(pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;

commit e9bda6f6f902e6b55d9baceb5523468a048cbe56
Merge: bdc0077af574 06e48c510aa3
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Jul 26 11:54:21 2012 +0300

    Merge branch 'queue' into next
    
    Merge patches queued during the run-up to the merge window.
    
    * queue: (25 commits)
      KVM: Choose better candidate for directed yield
      KVM: Note down when cpu relax intercepted or pause loop exited
      KVM: Add config to support ple or cpu relax optimzation
      KVM: switch to symbolic name for irq_states size
      KVM: x86: Fix typos in pmu.c
      KVM: x86: Fix typos in lapic.c
      KVM: x86: Fix typos in cpuid.c
      KVM: x86: Fix typos in emulate.c
      KVM: x86: Fix typos in x86.c
      KVM: SVM: Fix typos
      KVM: VMX: Fix typos
      KVM: remove the unused parameter of gfn_to_pfn_memslot
      KVM: remove is_error_hpa
      KVM: make bad_pfn static to kvm_main.c
      KVM: using get_fault_pfn to get the fault pfn
      KVM: MMU: track the refcount when unmap the page
      KVM: x86: remove unnecessary mark_page_dirty
      KVM: MMU: Avoid handling same rmap_pde in kvm_handle_hva_range()
      KVM: MMU: Push trace_kvm_age_page() into kvm_age_rmapp()
      KVM: MMU: Add memslot parameter to hva handlers
      ...
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit 5fecc9d8f59e765c2a48379dd7c6f5cf88c7d75a
Merge: 3c4cfadef6a1 1a577b72475d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 12:01:20 2012 -0700

    Merge tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Avi Kivity:
     "Highlights include
      - full big real mode emulation on pre-Westmere Intel hosts (can be
        disabled with emulate_invalid_guest_state=0)
      - relatively small ppc and s390 updates
      - PCID/INVPCID support in guests
      - EOI avoidance; 3.6 guests should perform better on 3.6 hosts on
        interrupt intensive workloads)
      - Lockless write faults during live migration
      - EPT accessed/dirty bits support for new Intel processors"
    
    Fix up conflicts in:
     - Documentation/virtual/kvm/api.txt:
    
       Stupid subchapter numbering, added next to each other.
    
     - arch/powerpc/kvm/booke_interrupts.S:
    
       PPC asm changes clashing with the KVM fixes
    
     - arch/s390/include/asm/sigp.h, arch/s390/kvm/sigp.c:
    
       Duplicated commits through the kvm tree and the s390 tree, with
       subsequent edits in the KVM tree.
    
    * tag 'kvm-3.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (93 commits)
      KVM: fix race with level interrupts
      x86, hyper: fix build with !CONFIG_KVM_GUEST
      Revert "apic: fix kvm build on UP without IOAPIC"
      KVM guest: switch to apic_set_eoi_write, apic_write
      apic: add apic_set_eoi_write for PV use
      KVM: VMX: Implement PCID/INVPCID for guests with EPT
      KVM: Add x86_hyper_kvm to complete detect_hypervisor_platform check
      KVM: PPC: Critical interrupt emulation support
      KVM: PPC: e500mc: Fix tlbilx emulation for 64-bit guests
      KVM: PPC64: booke: Set interrupt computation mode for 64-bit host
      KVM: PPC: bookehv: Add ESR flag to Data Storage Interrupt
      KVM: PPC: bookehv64: Add support for std/ld emulation.
      booke: Added crit/mc exception handler for e500v2
      booke/bookehv: Add host crit-watchdog exception support
      KVM: MMU: document mmu-lock and fast page fault
      KVM: MMU: fix kvm_mmu_pagetable_walk tracepoint
      KVM: MMU: trace fast page fault
      KVM: MMU: fast path of handling guest page fault
      KVM: MMU: introduce SPTE_MMU_WRITEABLE bit
      KVM: MMU: fold tlb flush judgement into mmu_spte_update
      ...

commit 06e48c510aa37f6e791602e6420422ea7071fe94
Author: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
Date:   Thu Jul 19 15:17:52 2012 +0530

    KVM: Choose better candidate for directed yield
    
    Currently, on a large vcpu guests, there is a high probability of
    yielding to the same vcpu who had recently done a pause-loop exit or
    cpu relax intercepted. Such a yield can lead to the vcpu spinning
    again and hence degrade the performance.
    
    The patchset keeps track of the pause loop exit/cpu relax interception
    and gives chance to a vcpu which:
     (a) Has not done pause loop exit or cpu relax intercepted at all
         (probably he is preempted lock-holder)
     (b) Was skipped in last iteration because it did pause loop exit or
         cpu relax intercepted, and probably has become eligible now
         (next eligible lock holder)
    
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com> # on s390x
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 361b36fe7ecc..74a78d09c454 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -931,6 +931,11 @@ static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
 {
 }
 
+static inline bool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)
+{
+	return true;
+}
+
 #endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 #endif
 

commit 4c088493c8d07e4e27bad53a99dcfdc14cdf45f8
Author: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
Date:   Wed Jul 18 19:07:46 2012 +0530

    KVM: Note down when cpu relax intercepted or pause loop exited
    
    Noting pause loop exited vcpu or cpu relax intercepted helps in
    filtering right candidate to yield. Wrong selection of vcpu;
    i.e., a vcpu that just did a pl-exit or cpu relax intercepted may
    contribute to performance degradation.
    
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com> # on s390x
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index db9aa917840a..361b36fe7ecc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -183,6 +183,18 @@ struct kvm_vcpu {
 	} async_pf;
 #endif
 
+#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
+	/*
+	 * Cpu relax intercept or pause loop exit optimization
+	 * in_spin_loop: set when a vcpu does a pause loop exit
+	 *  or cpu relax intercepted.
+	 * dy_eligible: indicates whether vcpu is eligible for directed yield.
+	 */
+	struct {
+		bool in_spin_loop;
+		bool dy_eligible;
+	} spin_loop;
+#endif
 	struct kvm_vcpu_arch arch;
 };
 
@@ -898,5 +910,27 @@ static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 	}
 }
 
+#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
+
+static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)
+{
+	vcpu->spin_loop.in_spin_loop = val;
+}
+static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
+{
+	vcpu->spin_loop.dy_eligible = val;
+}
+
+#else /* !CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
+
+static inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)
+{
+}
+
+static inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)
+{
+}
+
+#endif /* CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT */
 #endif
 

commit d566104853361cc377c61f70e41c1ad3d44b86c6
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Jul 17 21:56:16 2012 +0800

    KVM: remove the unused parameter of gfn_to_pfn_memslot
    
    The parameter, 'kvm', is not used in gfn_to_pfn_memslot, we can happily remove
    it
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e8d13a072d24..db9aa917840a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -418,15 +418,14 @@ void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
-pfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr);
+pfn_t hva_to_pfn_atomic(unsigned long addr);
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,
 		       bool write_fault, bool *writable);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
-pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
-			 struct kvm_memory_slot *slot, gfn_t gfn);
+pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_release_pfn_dirty(pfn_t);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);

commit f340a51b7e41abbe92ae3a327c0020974a059f95
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Jul 17 21:55:34 2012 +0800

    KVM: remove is_error_hpa
    
    Remove them since they are not used anymore
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5f956cde1374..e8d13a072d24 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -378,10 +378,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 	return slot;
 }
 
-#define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
-#define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
-static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
-
 extern struct page *bad_page;
 
 int is_error_page(struct page *page);

commit ca0565f5736e67af3172d188577b57e303dd082a
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Jul 17 21:54:52 2012 +0800

    KVM: make bad_pfn static to kvm_main.c
    
    bad_pfn is not used out of kvm_main.c, so mark it static, also move it near
    hwpoison_pfn and fault_pfn
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1a7f838d30c6..5f956cde1374 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -383,7 +383,6 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
 
 extern struct page *bad_page;
-extern pfn_t bad_pfn;
 
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);

commit 903816fa4d016e20ec71a1a97700cfcdda115580
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Tue Jul 17 21:54:11 2012 +0800

    KVM: using get_fault_pfn to get the fault pfn
    
    Using get_fault_pfn to cleanup the code
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6f6c18a03c50..1a7f838d30c6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -383,15 +383,11 @@ id_to_memslot(struct kvm_memslots *slots, int id)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
 
 extern struct page *bad_page;
-extern struct page *fault_page;
-
 extern pfn_t bad_pfn;
-extern pfn_t fault_pfn;
 
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);
 int is_hwpoison_pfn(pfn_t pfn);
-int is_fault_pfn(pfn_t pfn);
 int is_noslot_pfn(pfn_t pfn);
 int is_invalid_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
@@ -441,6 +437,7 @@ void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
 void kvm_set_pfn_accessed(pfn_t pfn);
 void kvm_get_pfn(pfn_t pfn);
+pfn_t get_fault_pfn(void);
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);

commit d19a748b1c42b133e9263e9023c1d162efa6f4ad
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Jul 2 17:54:30 2012 +0900

    KVM: Introduce hva_to_gfn_memslot() for kvm_handle_hva()
    
    This restricts hva handling in mmu code and makes it easier to extend
    kvm_handle_hva() so that it can treat a range of addresses later in this
    patch series.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e3c86f8c86c9..6f6c18a03c50 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -740,6 +740,14 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+static inline gfn_t
+hva_to_gfn_memslot(unsigned long hva, struct kvm_memory_slot *slot)
+{
+	gfn_t gfn_offset = (hva - slot->userspace_addr) >> PAGE_SHIFT;
+
+	return slot->base_gfn + gfn_offset;
+}
+
 static inline unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
 					       gfn_t gfn)
 {

commit 36c1ed821bd11fb9a3f99a060b1553c114dc2d07
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 15 15:07:24 2012 -0400

    KVM: Guard mmu_notifier specific code with CONFIG_MMU_NOTIFIER
    
    In order to avoid compilation failure when KVM is not compiled in,
    guard the mmu_notifier specific sections with both CONFIG_MMU_NOTIFIER
    and KVM_ARCH_WANT_MMU_NOTIFIER, like it is being done in the rest of
    the KVM code.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c7f77876c9b3..e3c86f8c86c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -306,7 +306,7 @@ struct kvm {
 	struct hlist_head irq_ack_notifier_list;
 #endif
 
-#ifdef KVM_ARCH_WANT_MMU_NOTIFIER
+#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 	struct mmu_notifier mmu_notifier;
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
@@ -780,7 +780,7 @@ struct kvm_stats_debugfs_item {
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 
-#ifdef KVM_ARCH_WANT_MMU_NOTIFIER
+#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_seq)
 {
 	if (unlikely(vcpu->kvm->mmu_notifier_count))

commit d4db2935e4fffeba42540b0dc9d85e3036701221
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri Jun 29 09:56:08 2012 -0600

    KVM: Pass kvm_irqfd to functions
    
    Prune this down to just the struct kvm_irqfd so we can avoid
    changing function definition for every flag or field we use.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c4464356b35b..96c158a37d3e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -815,7 +815,7 @@ static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 
 void kvm_eventfd_init(struct kvm *kvm);
-int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags);
+int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args);
 void kvm_irqfd_release(struct kvm *kvm);
 void kvm_irq_routing_update(struct kvm *, struct kvm_irq_routing_table *);
 int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
@@ -824,7 +824,7 @@ int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
 
 static inline void kvm_eventfd_init(struct kvm *kvm) {}
 
-static inline int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags)
+static inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	return -EINVAL;
 }

commit 9900b4b48b095895cf962054e45aafa49ef70f74
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 15 15:07:02 2012 -0400

    KVM: use KVM_CAP_IRQ_ROUTING to protect the routing related code
    
    The KVM code sometimes uses CONFIG_HAVE_KVM_IRQCHIP to protect
    code that is related to IRQ routing, which not all in-kernel
    irqchips may support.
    
    Use KVM_CAP_IRQ_ROUTING instead.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 27ac8a4767fa..c7f77876c9b3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -802,7 +802,7 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 }
 #endif
 
-#ifdef CONFIG_HAVE_KVM_IRQCHIP
+#ifdef KVM_CAP_IRQ_ROUTING
 
 #define KVM_MAX_IRQ_ROUTES 1024
 

commit a737f256bf14adf94920aa70d150ab4dcd145109
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Sun Jun 3 21:17:48 2012 +0300

    KVM: Cleanup the kvm_print functions and introduce pr_XX wrappers
    
    Introduces a couple of print functions, which are essentially wrappers
    around standard printk functions, with a KVM: prefix.
    
    Functions introduced or modified are:
     - kvm_err(fmt, ...)
     - kvm_info(fmt, ...)
     - kvm_debug(fmt, ...)
     - kvm_pr_unimpl(fmt, ...)
     - pr_unimpl(vcpu, fmt, ...) -> vcpu_unimpl(vcpu, fmt, ...)
    
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 19b83f6efa49..27ac8a4767fa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -314,13 +314,19 @@ struct kvm {
 	long tlbs_dirty;
 };
 
-/* The guest did something we don't support. */
-#define pr_unimpl(vcpu, fmt, ...)					\
-	pr_err_ratelimited("kvm: %i: cpu%i " fmt,			\
-			   current->tgid, (vcpu)->vcpu_id , ## __VA_ARGS__)
+#define kvm_err(fmt, ...) \
+	pr_err("kvm [%i]: " fmt, task_pid_nr(current), ## __VA_ARGS__)
+#define kvm_info(fmt, ...) \
+	pr_info("kvm [%i]: " fmt, task_pid_nr(current), ## __VA_ARGS__)
+#define kvm_debug(fmt, ...) \
+	pr_debug("kvm [%i]: " fmt, task_pid_nr(current), ## __VA_ARGS__)
+#define kvm_pr_unimpl(fmt, ...) \
+	pr_err_ratelimited("kvm [%i]: " fmt, \
+			   task_tgid_nr(current), ## __VA_ARGS__)
 
-#define kvm_printf(kvm, fmt ...) printk(KERN_DEBUG fmt)
-#define vcpu_printf(vcpu, fmt...) kvm_printf(vcpu->kvm, fmt)
+/* The guest did something we don't support. */
+#define vcpu_unimpl(vcpu, fmt, ...)					\
+	kvm_pr_unimpl("vcpu%i " fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)
 
 static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 {

commit c1a7b32a14138f908df52d7c53b5ce3415ec6b50
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Sun May 20 13:15:07 2012 +0900

    KVM: Avoid wasting pages for small lpage_info arrays
    
    lpage_info is created for each large level even when the memory slot is
    not for RAM.  This means that when we add one slot for a PCI device, we
    end up allocating at least KVM_NR_PAGE_SIZES - 1 pages by vmalloc().
    
    To make things worse, there is an increasing number of devices which
    would result in more pages being wasted this way.
    
    This patch mitigates this problem by using kvm_kvzalloc().
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c4464356b35b..19b83f6efa49 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -535,6 +535,9 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
 void kvm_free_physmem(struct kvm *kvm);
 
+void *kvm_kvzalloc(unsigned long size);
+void kvm_kvfree(const void *addr);
+
 #ifndef __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {

commit c142786c6291189b5c85f53d91743e1eefbd8fe0
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 14 15:44:06 2012 +0300

    KVM: MMU: Don't use RCU for lockless shadow walking
    
    Using RCU for lockless shadow walking can increase the amount of memory
    in use by the system, since RCU grace periods are unpredictable.  We also
    have an unconditional write to a shared variable (reader_counter), which
    isn't good for scaling.
    
    Replace that with a scheme similar to x86's get_user_pages_fast(): disable
    interrupts during lockless shadow walk to force the freer
    (kvm_mmu_commit_zap_page()) to wait for the TLB flush IPI to find the
    processor with interrupts enabled.
    
    We also add a new vcpu->mode, READING_SHADOW_PAGE_TABLES, to prevent
    kvm_flush_remote_tlbs() from avoiding the IPI.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cae342d29d1b..c4464356b35b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -128,7 +128,8 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 enum {
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,
-	EXITING_GUEST_MODE
+	EXITING_GUEST_MODE,
+	READING_SHADOW_PAGE_TABLES,
 };
 
 /*

commit 41628d334361670d825fb03c04568f5ef9f084dc
Author: Konstantin Weitz <WEITZKON@de.ibm.com>
Date:   Wed Apr 25 15:30:38 2012 +0200

    KVM: s390: Implement the directed yield (diag 9c) hypervisor call for KVM
    
    This patch implements the directed yield hypercall found on other
    System z hypervisors. It delegates execution time to the virtual cpu
    specified in the instruction's parameter.
    
    Useful to avoid long spinlock waits in the guest.
    
    Christian Borntraeger: moved common code in virt/kvm/
    
    Signed-off-by: Konstantin Weitz <WEITZKON@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6f343307d72b..cae342d29d1b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -461,6 +461,7 @@ void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
+bool kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
 void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);

commit 07975ad3b30579ca27d880491ad992326b930c63
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Thu Mar 29 21:14:12 2012 +0200

    KVM: Introduce direct MSI message injection for in-kernel irqchips
    
    Currently, MSI messages can only be injected to in-kernel irqchips by
    defining a corresponding IRQ route for each message. This is not only
    unhandy if the MSI messages are generated "on the fly" by user space,
    IRQ routes are a limited resource that user space has to manage
    carefully.
    
    By providing a direct injection path, we can both avoid using up limited
    resources and simplify the necessary steps for user land.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 186ffab0b9f0..6f343307d72b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -802,6 +802,8 @@ int kvm_set_irq_routing(struct kvm *kvm,
 			unsigned flags);
 void kvm_free_irq_routing(struct kvm *kvm);
 
+int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi);
+
 #else
 
 static inline void kvm_free_irq_routing(struct kvm *kvm) {}

commit f78146b0f9230765c6315b2e14f56112513389ad
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 18 19:22:47 2012 +0300

    KVM: Fix page-crossing MMIO
    
    MMIO that are split across a page boundary are currently broken - the
    code does not expect to be aborted by the exit to userspace for the
    first MMIO fragment.
    
    This patch fixes the problem by generalizing the current code for handling
    16-byte MMIOs to handle a number of "fragments", and changes the MMIO
    code to create those fragments.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a2d00b1bbf54..186ffab0b9f0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -34,6 +34,20 @@
 #define KVM_MMIO_SIZE 8
 #endif
 
+/*
+ * If we support unaligned MMIO, at most one fragment will be split into two:
+ */
+#ifdef KVM_UNALIGNED_MMIO
+#  define KVM_EXTRA_MMIO_FRAGMENTS 1
+#else
+#  define KVM_EXTRA_MMIO_FRAGMENTS 0
+#endif
+
+#define KVM_USER_MMIO_SIZE 8
+
+#define KVM_MAX_MMIO_FRAGMENTS \
+	(KVM_MMIO_SIZE / KVM_USER_MMIO_SIZE + KVM_EXTRA_MMIO_FRAGMENTS)
+
 /*
  * vcpu->requests bit members
  */
@@ -117,6 +131,16 @@ enum {
 	EXITING_GUEST_MODE
 };
 
+/*
+ * Sometimes a large or cross-page mmio needs to be broken up into separate
+ * exits for userspace servicing.
+ */
+struct kvm_mmio_fragment {
+	gpa_t gpa;
+	void *data;
+	unsigned len;
+};
+
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
@@ -144,10 +168,9 @@ struct kvm_vcpu {
 	int mmio_needed;
 	int mmio_read_completed;
 	int mmio_is_write;
-	int mmio_size;
-	int mmio_index;
-	unsigned char mmio_data[KVM_MMIO_SIZE];
-	gpa_t mmio_phys_addr;
+	int mmio_cur_fragment;
+	int mmio_nr_fragments;
+	struct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS];
 #endif
 
 #ifdef CONFIG_KVM_ASYNC_PF

commit eac0556750e727ff39144a9a9e59d5ccf1fc0e2a
Merge: f71fa31f9f7a 19853301ef32
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Apr 19 17:06:26 2012 -0300

    Merge branch 'linus' into queue
    
    Merge reason: development work has dependency on kvm patches merged
    upstream.
    
    Conflicts:
            Documentation/feature-removal-schedule.txt
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit 32f6daad4651a748a58a3ab6da0611862175722f
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Apr 11 09:51:49 2012 -0600

    KVM: unmap pages from the iommu when slots are removed
    
    We've been adding new mappings, but not destroying old mappings.
    This can lead to a page leak as pages are pinned using
    get_user_pages, but only unpinned with put_page if they still
    exist in the memslots list on vm shutdown.  A memslot that is
    destroyed while an iommu domain is enabled for the guest will
    therefore result in an elevated page reference count that is
    never cleared.
    
    Additionally, without this fix, the iommu is only programmed
    with the first translation for a gpa.  This can result in
    peer-to-peer errors if a mapping is destroyed and replaced by a
    new mapping at the same gpa as the iommu will still be pointing
    to the original, pinned memory address.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 665a260c7e09..72cbf08d45fb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -596,6 +596,7 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
 #ifdef CONFIG_IOMMU_API
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
+void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 int kvm_iommu_map_guest(struct kvm *kvm);
 int kvm_iommu_unmap_guest(struct kvm *kvm);
 int kvm_assign_device(struct kvm *kvm,
@@ -609,6 +610,11 @@ static inline int kvm_iommu_map_pages(struct kvm *kvm,
 	return 0;
 }
 
+static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
+					 struct kvm_memory_slot *slot)
+{
+}
+
 static inline int kvm_iommu_map_guest(struct kvm *kvm)
 {
 	return -ENODEV;

commit 93474b25af1eabf5b14743793156e8d307bfcd6b
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Thu Mar 1 19:34:45 2012 +0900

    KVM: Remove unused dirty_bitmap_head and nr_dirty_pages
    
    Now that we do neither double buffering nor heuristic selection of the
    write protection method these are not needed anymore.
    
    Note: some drivers have their own implementation of set_bit_le() and
    making it generic needs a bit of work; so we use test_and_set_bit_le()
    and will later replace it with generic set_bit_le().
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5184817e714a..49c2f2fd281f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -179,8 +179,6 @@ struct kvm_memory_slot {
 	unsigned long flags;
 	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
-	unsigned long *dirty_bitmap_head;
-	unsigned long nr_dirty_pages;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
 	int user_alloc;

commit 2246f8b56315befa30f3d3d2800e0734c774f70e
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Mar 13 22:35:01 2012 +0100

    KVM: PPC: Rework wqp conditional code
    
    On PowerPC, we sometimes use a waitqueue per core, not per thread,
    so we can't always use the vcpu internal waitqueue.
    
    This code has been generalized by Christoffer Dall recently, but
    unfortunately broke compilation for PowerPC. At the time the helper
    function is defined, struct kvm_vcpu is not declared yet, so we can't
    dereference it.
    
    This patch moves all logic into the generic inline function, at which
    time we have all information necessary.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5b624e1ff814..5184817e714a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -524,12 +524,14 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 }
 #endif
 
-#ifndef __KVM_HAVE_ARCH_VCPU_GET_WQ
 static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 {
+#ifdef __KVM_HAVE_ARCH_WQP
+	return vcpu->arch.wqp;
+#else
 	return &vcpu->wq;
-}
 #endif
+}
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type);
 void kvm_arch_destroy_vm(struct kvm *kvm);

commit b6d33834bd4e8bdf4a199812e31b3e36da53c794
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Thu Mar 8 16:44:24 2012 -0500

    KVM: Factor out kvm_vcpu_kick to arch-generic code
    
    The kvm_vcpu_kick function performs roughly the same funcitonality on
    most all architectures, so we shouldn't have separate copies.
    
    PowerPC keeps a pointer to interchanging waitqueues on the vcpu_arch
    structure and to accomodate this special need a
    __KVM_HAVE_ARCH_VCPU_GET_WQ define and accompanying function
    kvm_arch_vcpu_wq have been defined. For all other architectures this
    is a generic inline that just returns &vcpu->wq;
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3a2cea616283..5b624e1ff814 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -439,6 +439,7 @@ void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,
 			     gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
+void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
 void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
@@ -507,6 +508,7 @@ int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
 void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
+int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);
 
 void kvm_free_physmem(struct kvm *kvm);
 
@@ -522,6 +524,13 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 }
 #endif
 
+#ifndef __KVM_HAVE_ARCH_VCPU_GET_WQ
+static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+{
+	return &vcpu->wq;
+}
+#endif
+
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type);
 void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);

commit 786a9f888bfbe70a36d0592b26037ca1e8c8da7f
Author: Amos Kong <akong@redhat.com>
Date:   Fri Mar 9 12:17:40 2012 +0800

    KVM: set upper bounds for iobus dev to limit userspace
    
    kvm_io_bus devices are used for ioevent, pit, pic, ioapic,
    coalesced_mmio.
    
    Currently Qemu only emulates one PCI bus, it contains 32 slots,
    one slot contains 8 functions, maximum of supported PCI devices:
     1 * 32 * 8 = 256. One virtio-blk takes one iobus device,
    one virtio-net(vhost=on) takes two iobus devices.
    The maximum of coalesced mmio zone is 100, each zone
    has an iobus devices. So 300 io_bus devices are not enough.
    
    Set an upper bounds for kvm_io_range to limit userspace.
    1000 is a very large limit and not bloat the typical user.
    
    Signed-off-by: Amos Kong <akong@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ba9fb4a9762d..3a2cea616283 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -68,7 +68,7 @@ struct kvm_io_range {
 	struct kvm_io_device *dev;
 };
 
-#define NR_IOBUS_DEVS 300
+#define NR_IOBUS_DEVS 1000
 
 struct kvm_io_bus {
 	int                   dev_count;

commit a13007160f1b9ec7c67e28ec9254f197c5c08d7d
Author: Amos Kong <akong@redhat.com>
Date:   Fri Mar 9 12:17:32 2012 +0800

    KVM: resize kvm_io_range array dynamically
    
    This patch makes the kvm_io_range array can be resized dynamically.
    
    Signed-off-by: Amos Kong <akong@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 665a260c7e09..ba9fb4a9762d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -68,10 +68,11 @@ struct kvm_io_range {
 	struct kvm_io_device *dev;
 };
 
+#define NR_IOBUS_DEVS 300
+
 struct kvm_io_bus {
 	int                   dev_count;
-#define NR_IOBUS_DEVS 300
-	struct kvm_io_range range[NR_IOBUS_DEVS];
+	struct kvm_io_range range[];
 };
 
 enum kvm_bus {

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit cf9eeac46350b8b43730b7dc5e999757bed089a4
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Wed Mar 14 11:02:11 2012 +0100

    KVM: Convert intx_mask_lock to spin lock
    
    As kvm_notify_acked_irq calls kvm_assigned_dev_ack_irq under
    rcu_read_lock, we cannot use a mutex in the latter function. Switch to a
    spin lock to address this.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ec171c1d0878..40bb1c661a6e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -556,7 +556,7 @@ struct kvm_assigned_dev_kernel {
 	struct pci_dev *dev;
 	struct kvm *kvm;
 	spinlock_t intx_lock;
-	struct mutex intx_mask_lock;
+	spinlock_t intx_mask_lock;
 	char irq_name[32];
 	struct pci_saved_state *pci_saved_state;
 };

commit 07700a94b00a4fcbbfb07d1b72dc112a0e036735
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Feb 28 14:19:54 2012 +0100

    KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
    
    PCI 2.3 allows to generically disable IRQ sources at device level. This
    enables us to share legacy IRQs of such devices with other host devices
    when passing them to a guest.
    
    The new IRQ sharing feature introduced here is optional, user space has
    to request it explicitly. Moreover, user space can inform us about its
    view of PCI_COMMAND_INTX_DISABLE so that we can avoid unmasking the
    interrupt and signaling it if the guest masked it via the virtualized
    PCI config space.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e42d85ae8541..ec171c1d0878 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -546,6 +546,7 @@ struct kvm_assigned_dev_kernel {
 	unsigned int entries_nr;
 	int host_irq;
 	bool host_irq_disabled;
+	bool pci_2_3;
 	struct msix_entry *host_msix_entries;
 	int guest_irq;
 	struct msix_entry *guest_msix_entries;
@@ -555,6 +556,7 @@ struct kvm_assigned_dev_kernel {
 	struct pci_dev *dev;
 	struct kvm *kvm;
 	spinlock_t intx_lock;
+	struct mutex intx_mask_lock;
 	char irq_name[32];
 	struct pci_saved_state *pci_saved_state;
 };

commit 3e515705a1f46beb1c942bb8043c16f8ac7b1e9e
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Mar 5 14:23:29 2012 +0200

    KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
    
    If some vcpus are created before KVM_CREATE_IRQCHIP, then
    irqchip_in_kernel() and vcpu->arch.apic will be inconsistent, leading
    to potential NULL pointer dereferences.
    
    Fix by:
    - ensuring that no vcpus are installed when KVM_CREATE_IRQCHIP is called
    - ensuring that a vcpu has an apic if it is installed after KVM_CREATE_IRQCHIP
    
    This is somewhat long winded because vcpu->arch.apic is created without
    kvm->lock held.
    
    Based on earlier patch by Michael Ellerman.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 355e44555c39..e42d85ae8541 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -805,6 +805,13 @@ static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
 	return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
 }
+
+bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu);
+
+#else
+
+static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }
+
 #endif
 
 #ifdef __KVM_HAVE_DEVICE_ASSIGNMENT

commit db3fe4eb45f3555d91a7124e18cf3a2f2a30eb90
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 13:02:18 2012 +0900

    KVM: Introduce kvm_memory_slot::arch and move lpage_info into it
    
    Some members of kvm_memory_slot are not used by every architecture.
    
    This patch is the first step to make this difference clear by
    introducing kvm_memory_slot::arch;  lpage_info is moved into it.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7a08496b974a..355e44555c39 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -171,11 +171,6 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
  */
 #define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)
 
-struct kvm_lpage_info {
-	unsigned long rmap_pde;
-	int write_count;
-};
-
 struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
@@ -184,7 +179,7 @@ struct kvm_memory_slot {
 	unsigned long *dirty_bitmap;
 	unsigned long *dirty_bitmap_head;
 	unsigned long nr_dirty_pages;
-	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
 	int user_alloc;
 	int id;
@@ -376,6 +371,9 @@ int kvm_set_memory_region(struct kvm *kvm,
 int __kvm_set_memory_region(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem,
 			    int user_alloc);
+void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+			   struct kvm_memory_slot *dont);
+int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages);
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *memslot,
 				struct kvm_memory_slot old,
@@ -385,6 +383,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
 				int user_alloc);
+bool kvm_largepages_enabled(void);
 void kvm_disable_largepages(void);
 void kvm_arch_flush_shadow(struct kvm *kvm);
 

commit fb03cb6f44236f4bef62a0dda8e025ff5ca51417
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 12:59:10 2012 +0900

    KVM: Introduce gfn_to_index() which returns the index for a given level
    
    This patch cleans up the code and removes the "(void)level;" warning
    suppressor.
    
    Note that we can also use this for PT_PAGE_TABLE_LEVEL to treat every
    level uniformly later.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9698080c902b..7a08496b974a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -681,6 +681,13 @@ static inline int memslot_id(struct kvm *kvm, gfn_t gfn)
 	return gfn_to_memslot(kvm, gfn)->id;
 }
 
+static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
+{
+	/* KVM_HPAGE_GFN_SHIFT(PT_PAGE_TABLE_LEVEL) must be 0. */
+	return (gfn >> KVM_HPAGE_GFN_SHIFT(level)) -
+		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
+}
+
 static inline unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
 					       gfn_t gfn)
 {

commit 9d4cba7f93c52d4121ab9c6f289e582d368a6979
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jan 12 20:09:51 2012 +0000

    KVM: Move gfn_to_memslot() to kvm_host.h
    
    This moves __gfn_to_memslot() and search_memslots() from kvm_main.c to
    kvm_host.h to reduce the code duplication caused by the need for
    non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c to call
    gfn_to_memslot() in real mode.
    
    Rather than putting gfn_to_memslot() itself in a header, which would
    lead to increased code size, this puts __gfn_to_memslot() in a header.
    Then, the non-modular uses of gfn_to_memslot() are changed to call
    __gfn_to_memslot() instead.  This way there is only one place in the
    source code that needs to be changed should the gfn_to_memslot()
    implementation need to be modified.
    
    On powerpc, the Book3S HV style of KVM has code that is called from
    real mode which needs to call gfn_to_memslot() and thus needs this.
    (Module code is allocated in the vmalloc region, which can't be
    accessed in real mode.)
    
    With this, we can remove builtin_gfn_to_memslot() from book3s_hv_rm_mmu.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index eada8e69fe58..9698080c902b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -651,6 +651,31 @@ static inline void kvm_guest_exit(void)
 	current->flags &= ~PF_VCPU;
 }
 
+/*
+ * search_memslots() and __gfn_to_memslot() are here because they are
+ * used in non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c.
+ * gfn_to_memslot() itself isn't here as an inline because that would
+ * bloat other code too much.
+ */
+static inline struct kvm_memory_slot *
+search_memslots(struct kvm_memslots *slots, gfn_t gfn)
+{
+	struct kvm_memory_slot *memslot;
+
+	kvm_for_each_memslot(memslot, slots)
+		if (gfn >= memslot->base_gfn &&
+		      gfn < memslot->base_gfn + memslot->npages)
+			return memslot;
+
+	return NULL;
+}
+
+static inline struct kvm_memory_slot *
+__gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
+{
+	return search_memslots(slots, gfn);
+}
+
 static inline int memslot_id(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn_to_memslot(kvm, gfn)->id;

commit a355aa54f1d25dff83c0feef8863d83a76988fdb
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:37:21 2011 +0000

    KVM: Add barriers to allow mmu_notifier_retry to be used locklessly
    
    This adds an smp_wmb in kvm_mmu_notifier_invalidate_range_end() and an
    smp_rmb in mmu_notifier_retry() so that mmu_notifier_retry() will give
    the correct answer when called without kvm->mmu_lock being held.
    PowerPC Book3S HV KVM wants to use a bitlock per guest page rather than
    a single global spinlock in order to improve the scalability of updates
    to the guest MMU hashed page table, and so needs this.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d4d4d7092110..eada8e69fe58 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -702,12 +702,16 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 	if (unlikely(vcpu->kvm->mmu_notifier_count))
 		return 1;
 	/*
-	 * Both reads happen under the mmu_lock and both values are
-	 * modified under mmu_lock, so there's no need of smb_rmb()
-	 * here in between, otherwise mmu_notifier_count should be
-	 * read before mmu_notifier_seq, see
-	 * mmu_notifier_invalidate_range_end write side.
+	 * Ensure the read of mmu_notifier_count happens before the read
+	 * of mmu_notifier_seq.  This interacts with the smp_wmb() in
+	 * mmu_notifier_invalidate_range_end to make sure that the caller
+	 * either sees the old (non-zero) value of mmu_notifier_count or
+	 * the new (incremented) value of mmu_notifier_seq.
+	 * PowerPC Book3s HV KVM calls this under a per-page lock
+	 * rather than under kvm->mmu_lock, for scalability, so
+	 * can't rely on kvm->mmu_lock to keep things ordered.
 	 */
+	smp_rmb();
 	if (vcpu->kvm->mmu_notifier_seq != mmu_seq)
 		return 1;
 	return 0;

commit 5b1c1493afe8d69909f9df3221bb2fffdf479f4a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:23 2012 +0100

    KVM: s390: ucontrol: export SIE control block to user
    
    This patch exports the s390 SIE hardware control block to userspace
    via the mapping of the vcpu file descriptor. In order to do so,
    a new arch callback named kvm_arch_vcpu_fault  is introduced for all
    architectures. It allows to map architecture specific pages.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 82375e145e64..d4d4d7092110 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -450,6 +450,7 @@ long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg);
+int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);
 
 int kvm_dev_ioctl_check_extension(long ext);
 

commit e08b96371625aaa84cb03f51acc4c8e0be27403a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:20 2012 +0100

    KVM: s390: add parameter for KVM_CREATE_VM
    
    This patch introduces a new config option for user controlled kernel
    virtual machines. It introduces a parameter to KVM_CREATE_VM that
    allows to set bits that alter the capabilities of the newly created
    virtual machine.
    The parameter is passed to kvm_arch_init_vm for all architectures.
    The only valid modifier bit for now is KVM_VM_S390_UCONTROL.
    This requires CAP_SYS_ADMIN privileges and creates a user controlled
    virtual machine on s390 architectures.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 900c76337e8f..82375e145e64 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -520,7 +520,7 @@ static inline void kvm_arch_free_vm(struct kvm *kvm)
 }
 #endif
 
-int kvm_arch_init_vm(struct kvm *kvm);
+int kvm_arch_init_vm(struct kvm *kvm, unsigned long type);
 void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 900c76337e8f..ca1b153585d3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -13,6 +13,7 @@
 #include <linux/spinlock.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
+#include <linux/bug.h>
 #include <linux/mm.h>
 #include <linux/mmu_notifier.h>
 #include <linux/preempt.h>

commit f5132b01386b5a67f1ff673bb2b96a507a3f7e41
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Nov 10 14:57:22 2011 +0200

    KVM: Expose a version 2 architectural PMU to a guests
    
    Use perf_events to emulate an architectural PMU, version 2.
    
    Based on PMU version 1 emulation by Avi Kivity.
    
    [avi: adjust for cpuid.c]
    [jan: fix anonymous field initialization for older gcc]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7a080383935f..900c76337e8f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -52,6 +52,8 @@
 #define KVM_REQ_STEAL_UPDATE      13
 #define KVM_REQ_NMI               14
 #define KVM_REQ_IMMEDIATE_EXIT    15
+#define KVM_REQ_PMU               16
+#define KVM_REQ_PMI               17
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit d546cb406ea0d83e2d39ec14221957a24f88a622
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Dec 15 12:38:40 2011 +0200

    KVM: drop bsp_vcpu pointer from kvm struct
    
    Drop bsp_vcpu pointer from kvm struct since its only use is incorrect
    anyway.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8c5c30361b0d..7a080383935f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -251,7 +251,6 @@ struct kvm {
 	struct srcu_struct srcu;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
 	u32 bsp_vcpu_id;
-	struct kvm_vcpu *bsp_vcpu;
 #endif
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	atomic_t online_vcpus;

commit f85e2cb5dbaf905e9470d3fe099b31619da431fc
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:41:54 2011 +0800

    KVM: introduce a table to map slot id to index in memslots array
    
    The operation of getting dirty log is frequent when framebuffer-based
    displays are used(for example, Xwindow), so, we introduce a mapping table
    to speed up id_to_memslot()
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9efdf5c703a5..8c5c30361b0d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -239,6 +239,8 @@ struct kvm_irq_routing_table {};
 struct kvm_memslots {
 	u64 generation;
 	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
+	/* The mapping table from slot id to the index in memslots[]. */
+	int id_to_index[KVM_MEM_SLOTS_NUM];
 };
 
 struct kvm {
@@ -341,14 +343,13 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
-	int i;
+	int index = slots->id_to_index[id];
+	struct kvm_memory_slot *slot;
 
-	for (i = 0; i < KVM_MEM_SLOTS_NUM; i++)
-		if (slots->memslots[i].id == id)
-			return &slots->memslots[i];
+	slot = &slots->memslots[index];
 
-	WARN_ON(1);
-	return NULL;
+	WARN_ON(slot->id != id);
+	return slot;
 }
 
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)

commit bf3e05bc1e2781d5d8d3ddb2d8bf2d6ec207e5cb
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:40:57 2011 +0800

    KVM: sort memslots by its size and use line search
    
    Sort memslots base on its size and use line search to find it, so that the
    larger memslots have better fit
    
    The idea is from Avi
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 123925cd3ae8..9efdf5c703a5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -231,8 +231,12 @@ struct kvm_irq_routing_table {};
 #define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 #endif
 
+/*
+ * Note:
+ * memslots are not sorted by id anymore, please use id_to_memslot()
+ * to get the memslot by its id.
+ */
 struct kvm_memslots {
-	int nmemslots;
 	u64 generation;
 	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 };
@@ -310,7 +314,8 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 #define kvm_for_each_memslot(memslot, slots)	\
 	for (memslot = &slots->memslots[0];	\
-	      memslot < slots->memslots + (slots)->nmemslots; memslot++)
+	      memslot < slots->memslots + KVM_MEM_SLOTS_NUM && memslot->npages;\
+		memslot++)
 
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
@@ -336,7 +341,14 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 static inline struct kvm_memory_slot *
 id_to_memslot(struct kvm_memslots *slots, int id)
 {
-	return &slots->memslots[id];
+	int i;
+
+	for (i = 0; i < KVM_MEM_SLOTS_NUM; i++)
+		if (slots->memslots[i].id == id)
+			return &slots->memslots[i];
+
+	WARN_ON(1);
+	return NULL;
 }
 
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)

commit 28a37544fb0223eb9805d2567b88f7360edec52a
Author: Xiao Guangrong <xiaoguangrong.eric@gmail.com>
Date:   Thu Nov 24 19:04:35 2011 +0800

    KVM: introduce id_to_memslot function
    
    Introduce id_to_memslot to get memslot by slot id
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 392af47a4353..123925cd3ae8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -333,6 +333,12 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 			|| lockdep_is_held(&kvm->slots_lock));
 }
 
+static inline struct kvm_memory_slot *
+id_to_memslot(struct kvm_memslots *slots, int id)
+{
+	return &slots->memslots[id];
+}
+
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
 #define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }

commit be6ba0f0962a39091c52eb9167ddea201fe80716
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:39:18 2011 +0800

    KVM: introduce kvm_for_each_memslot macro
    
    Introduce kvm_for_each_memslot to walk all valid memslot
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 23f795c66220..392af47a4353 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -308,6 +308,10 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 	     (vcpup = kvm_get_vcpu(kvm, idx)) != NULL; \
 	     idx++)
 
+#define kvm_for_each_memslot(memslot, slots)	\
+	for (memslot = &slots->memslots[0];	\
+	      memslot < slots->memslots + (slots)->nmemslots; memslot++)
+
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 

commit be593d6286075801bba6d60fa466a39c24cc7616
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:38:24 2011 +0800

    KVM: introduce update_memslots function
    
    Introduce update_memslots to update slot which will be update to
    kvm->memslots
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 924df0d7ac5f..23f795c66220 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -320,6 +320,7 @@ void kvm_exit(void);
 
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
+void update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new);
 
 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 {

commit 93a5cef07d686a0341d056b0f930a762c7174a13
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Thu Nov 24 17:37:48 2011 +0800

    KVM: introduce KVM_MEM_SLOTS_NUM macro
    
    Introduce KVM_MEM_SLOTS_NUM macro to instead of
    KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7c654aa46b6c..924df0d7ac5f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -227,11 +227,14 @@ struct kvm_irq_routing_table {};
 
 #endif
 
+#ifndef KVM_MEM_SLOTS_NUM
+#define KVM_MEM_SLOTS_NUM (KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
+#endif
+
 struct kvm_memslots {
 	int nmemslots;
 	u64 generation;
-	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
-					KVM_PRIVATE_MEM_SLOTS];
+	struct kvm_memory_slot memslots[KVM_MEM_SLOTS_NUM];
 };
 
 struct kvm {

commit 7850ac5420803996e2960d15b924021f28e0dffc
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Nov 14 18:23:34 2011 +0900

    KVM: Count the number of dirty pages for dirty logging
    
    Needed for the next patch which uses this number to decide how to write
    protect a slot.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c6a2ec925846..7c654aa46b6c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -181,6 +181,7 @@ struct kvm_memory_slot {
 	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
 	unsigned long *dirty_bitmap_head;
+	unsigned long nr_dirty_pages;
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned long userspace_addr;
 	int user_alloc;

commit b297e672e24687546ac74af5ae5e8c4a022b9806
Author: Eric B Munson <emunson@mgebm.net>
Date:   Mon Oct 10 11:46:15 2011 -0400

    KVM: Fix include dependency for mmu_notifier
    
    The kvm_host struct can include an mmu_notifier struct but mmu_notifier.h is
    not included directly.
    
    Signed-off-by: Eric B Munson <emunson@mgebm.net>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9fedeb356b95..c6a2ec925846 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -14,6 +14,7 @@
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
+#include <linux/mmu_notifier.h>
 #include <linux/preempt.h>
 #include <linux/msi.h>
 #include <linux/slab.h>

commit d6185f20a0efbf175e12831d0de330e4f21725aa
Author: Nadav Har'El <nyh@il.ibm.com>
Date:   Thu Sep 22 13:52:56 2011 +0300

    KVM: nVMX: Add KVM_REQ_IMMEDIATE_EXIT
    
    This patch adds a new vcpu->requests bit, KVM_REQ_IMMEDIATE_EXIT.
    This bit requests that when next entering the guest, we should run it only
    for as little as possible, and exit again.
    
    We use this new option in nested VMX: When L1 launches L2, but L0 wishes L1
    to continue running so it can inject an event to it, we unfortunately cannot
    just pretend to have run L2 for a little while - We must really launch L2,
    otherwise certain one-off vmcs12 parameters (namely, L1 injection into L2)
    will be lost. So the existing code runs L2 in this case.
    But L2 could potentially run for a long time until it exits, and the
    injection into L1 will be delayed. The new KVM_REQ_IMMEDIATE_EXIT allows us
    to request that L2 will be entered, as necessary, but will exit as soon as
    possible after entry.
    
    Our implementation of this request uses smp_send_reschedule() to send a
    self-IPI, with interrupts disabled. The interrupts remain disabled until the
    guest is entered, and then, after the entry is complete (often including
    processing an injection and jumping to the relevant handler), the physical
    interrupt is noticed and causes an exit.
    
    On recent Intel processors, we could have achieved the same goal by using
    MTF instead of a self-IPI. Another technique worth considering in the future
    is to use VM_EXIT_ACK_INTR_ON_EXIT and a highest-priority vector IPI - to
    slightly improve performance by avoiding the useless interrupt handler
    which ends up being called when smp_send_reschedule() is used.
    
    Signed-off-by: Nadav Har'El <nyh@il.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d52623199978..9fedeb356b95 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -50,6 +50,7 @@
 #define KVM_REQ_APF_HALT          12
 #define KVM_REQ_STEAL_UPDATE      13
 #define KVM_REQ_NMI               14
+#define KVM_REQ_IMMEDIATE_EXIT    15
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit 7460fb4a340033107530df19e7e125bd0969bfb2
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Sep 20 13:43:14 2011 +0300

    KVM: Fix simultaneous NMIs
    
    If simultaneous NMIs happen, we're supposed to queue the second
    and next (collapsing them), but currently we sometimes collapse
    the second into the first.
    
    Fix by using a counter for pending NMIs instead of a bool; since
    the counter limit depends on whether the processor is currently
    in an NMI handler, which can only be checked in vcpu context
    (via the NMI mask), we add a new KVM_REQ_NMI to request recalculation
    of the counter.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2a414f66af28..d52623199978 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -49,6 +49,7 @@
 #define KVM_REQ_EVENT             11
 #define KVM_REQ_APF_HALT          12
 #define KVM_REQ_STEAL_UPDATE      13
+#define KVM_REQ_NMI               14
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit bd80158aff71a80292f96d9baea1a65bc0ce87b3
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Sep 12 11:26:22 2011 +0200

    KVM: Clean up and extend rate-limited output
    
    The use of printk_ratelimit is discouraged, replace it with
    pr*_ratelimited or __ratelimit. While at it, convert remaining
    guest-triggerable printks to rate-limited variants.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d0e42f30edf6..2a414f66af28 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -18,6 +18,7 @@
 #include <linux/msi.h>
 #include <linux/slab.h>
 #include <linux/rcupdate.h>
+#include <linux/ratelimit.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -282,11 +283,8 @@ struct kvm {
 
 /* The guest did something we don't support. */
 #define pr_unimpl(vcpu, fmt, ...)					\
- do {									\
-	if (printk_ratelimit())						\
-		printk(KERN_ERR "kvm: %i: cpu%i " fmt,			\
-		       current->tgid, (vcpu)->vcpu_id , ## __VA_ARGS__); \
- } while (0)
+	pr_err_ratelimited("kvm: %i: cpu%i " fmt,			\
+			   current->tgid, (vcpu)->vcpu_id , ## __VA_ARGS__)
 
 #define kvm_printf(kvm, fmt ...) printk(KERN_DEBUG fmt)
 #define vcpu_printf(vcpu, fmt...) kvm_printf(vcpu->kvm, fmt)

commit 743eeb0b01d2fbf4154bf87bff1ebb6fb18aeb7a
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Wed Jul 27 16:00:48 2011 +0300

    KVM: Intelligent device lookup on I/O bus
    
    Currently the method of dealing with an IO operation on a bus (PIO/MMIO)
    is to call the read or write callback for each device registered
    on the bus until we find a device which handles it.
    
    Since the number of devices on a bus can be significant due to ioeventfds
    and coalesced MMIO zones, this leads to a lot of overhead on each IO
    operation.
    
    Instead of registering devices, we now register ranges which points to
    a device. Lookup is done using an efficient bsearch instead of a linear
    search.
    
    Performance test was conducted by comparing exit count per second with
    200 ioeventfds created on one byte and the guest is trying to access a
    different byte continuously (triggering usermode exits).
    Before the patch the guest has achieved 259k exits per second, after the
    patch the guest does 274k exits per second.
    
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ff4d4062af9d..d0e42f30edf6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -55,16 +55,16 @@ struct kvm;
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
-/*
- * It would be nice to use something smarter than a linear search, TBD...
- * Thankfully we dont expect many devices to register (famous last words :),
- * so until then it will suffice.  At least its abstracted so we can change
- * in one place.
- */
+struct kvm_io_range {
+	gpa_t addr;
+	int len;
+	struct kvm_io_device *dev;
+};
+
 struct kvm_io_bus {
 	int                   dev_count;
 #define NR_IOBUS_DEVS 300
-	struct kvm_io_device *devs[NR_IOBUS_DEVS];
+	struct kvm_io_range range[NR_IOBUS_DEVS];
 };
 
 enum kvm_bus {
@@ -77,8 +77,8 @@ int kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val);
 int kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr, int len,
 		    void *val);
-int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx,
-			    struct kvm_io_device *dev);
+int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
+			    int len, struct kvm_io_device *dev);
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 			      struct kvm_io_device *dev);
 

commit 2b3c246a682c50f5415c71fc5387a114a6f0d643
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Wed Jul 20 20:59:00 2011 +0300

    KVM: Make coalesced mmio use a device per zone
    
    This patch changes coalesced mmio to create one mmio device per
    zone instead of handling all zones in one device.
    
    Doing so enables us to take advantage of existing locking and prevents
    a race condition between coalesced mmio registration/unregistration
    and lookups.
    
    Suggested-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index eabb21a30c34..ff4d4062af9d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -63,7 +63,7 @@ extern struct kmem_cache *kvm_vcpu_cache;
  */
 struct kvm_io_bus {
 	int                   dev_count;
-#define NR_IOBUS_DEVS 200
+#define NR_IOBUS_DEVS 300
 	struct kvm_io_device *devs[NR_IOBUS_DEVS];
 };
 
@@ -256,8 +256,9 @@ struct kvm {
 	struct kvm_arch arch;
 	atomic_t users_count;
 #ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
-	struct kvm_coalesced_mmio_dev *coalesced_mmio_dev;
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
+	spinlock_t ring_lock;
+	struct list_head coalesced_zones;
 #endif
 
 	struct mutex irq_lock;

commit fce92dce79dbf5fff39c7ac2fb149729d79b7a39
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Jul 12 03:28:54 2011 +0800

    KVM: MMU: filter out the mmio pfn from the fault pfn
    
    If the page fault is caused by mmio, the gfn can not be found in memslots, and
    'bad_pfn' is returned on gfn_to_hva path, so we can use 'bad_pfn' to identify
    the mmio page fault.
    And, to clarify the meaning of mmio pfn, we return fault page instead of bad
    page when the gfn is not allowd to prefetch
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c8e023902f79..eabb21a30c34 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -327,12 +327,17 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
 
 extern struct page *bad_page;
+extern struct page *fault_page;
+
 extern pfn_t bad_pfn;
+extern pfn_t fault_pfn;
 
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);
 int is_hwpoison_pfn(pfn_t pfn);
 int is_fault_pfn(pfn_t pfn);
+int is_noslot_pfn(pfn_t pfn);
+int is_invalid_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,

commit c9aaa8957f203bd6df83b002fb40b98390bed078
Author: Glauber Costa <glommer@redhat.com>
Date:   Mon Jul 11 15:28:14 2011 -0400

    KVM: Steal time implementation
    
    To implement steal time, we need the hypervisor to pass the guest
    information about how much time was spent running other processes
    outside the VM, while the vcpu had meaningful work to do - halt
    time does not count.
    
    This information is acquired through the run_delay field of
    delayacct/schedstats infrastructure, that counts time spent in a
    runqueue but not running.
    
    Steal time is a per-cpu information, so the traditional MSR-based
    infrastructure is used. A new msr, KVM_MSR_STEAL_TIME, holds the
    memory area address containing information about steal time
    
    This patch contains the hypervisor part of the steal time infrasructure,
    and can be backported independently of the guest portion.
    
    [avi, yongjie: export delayacct_on, to avoid build failures in some configs]
    
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Tested-by: Eric B Munson <emunson@mgebm.net>
    CC: Rik van Riel <riel@redhat.com>
    CC: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Yongjie Ren <yongjie.ren@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f7df0a3b031d..c8e023902f79 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -47,6 +47,7 @@
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11
 #define KVM_REQ_APF_HALT          12
+#define KVM_REQ_STEAL_UPDATE      13
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit e03b644fe68b1c6401465b02724d261538dba10f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Jul 11 15:28:11 2011 -0400

    KVM: introduce kvm_read_guest_cached
    
    Introduce kvm_read_guest_cached() function in addition to write one we
    already have.
    
    [ by glauber: export function signature in kvm header ]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Glauber Costa <glommer@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Tested-by: Eric Munson <emunson@mgebm.net>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 31ebb59cbd2f..f7df0a3b031d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -381,6 +381,8 @@ int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
 			  unsigned long len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
+int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, unsigned long len);
 int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,

commit 5e152b4c9e0fce6149c74406346a7ae7e7a17727
Merge: a77febbef105 9251bac97d47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 15:39:34 2011 -0700

    Merge branch 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6
    
    * 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6: (27 commits)
      PCI: Don't use dmi_name_in_vendors in quirk
      PCI: remove unused AER functions
      PCI/sysfs: move bus cpuaffinity to class dev_attrs
      PCI: add rescan to /sys/.../pci_bus/.../
      PCI: update bridge resources to get more big ranges when allocating space (again)
      KVM: Use pci_store/load_saved_state() around VM device usage
      PCI: Add interfaces to store and load the device saved state
      PCI: Track the size of each saved capability data area
      PCI/e1000e: Add and use pci_disable_link_state_locked()
      x86/PCI: derive pcibios_last_bus from ACPI MCFG
      PCI: add latency tolerance reporting enable/disable support
      PCI: add OBFF enable/disable support
      PCI: add ID-based ordering enable/disable support
      PCI hotplug: acpiphp: assume device is in state D0 after powering on a slot.
      PCI: Set PCIE maxpayload for card during hotplug insertion
      PCI/ACPI: Report _OSC control mask returned on failure to get control
      x86/PCI: irq and pci_ids patch for Intel Panther Point DeviceIDs
      PCI: handle positive error codes
      PCI: check pci_vpd_pci22_wait() return
      PCI: Use ICH6_GPIO_EN in ich6_lpc_acpi_gpio
      ...
    
    Fix up trivial conflicts in include/linux/pci_ids.h: commit a6e5e2be4461
    moved the intel SMBUS ID definitons to the i2c-i801.c driver.

commit 8fa2206821953a50a3a02ea33fcfb3ced2fd9997
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed May 4 16:31:04 2011 +0300

    KVM: make guest mode entry to be rcu quiescent state
    
    KVM does not hold any references to rcu protected data when it switches
    CPU into a guest mode. In fact switching to a guest mode is very similar
    to exiting to userspase from rcu point of view. In addition CPU may stay
    in a guest mode for quite a long time (up to one time slice). Lets treat
    guest mode as quiescent state, just like we do with user-mode execution.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0bc3d372e3cb..b9c3299c6a55 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -591,8 +591,17 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 
 static inline void kvm_guest_enter(void)
 {
+	BUG_ON(preemptible());
 	account_system_vtime(current);
 	current->flags |= PF_VCPU;
+	/* KVM does not hold any references to rcu protected data when it
+	 * switches CPU into a guest mode. In fact switching to a guest mode
+	 * is very similar to exiting to userspase from rcu point of view. In
+	 * addition CPU may stay in a guest mode for quite a long time (up to
+	 * one time slice). Lets treat guest mode as quiescent state, just like
+	 * we do with user-mode execution.
+	 */
+	rcu_virt_note_context_switch(smp_processor_id());
 }
 
 static inline void kvm_guest_exit(void)

commit f8fcfd775523347afe460dc3a0f45d0479e784a2
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Tue May 10 10:02:39 2011 -0600

    KVM: Use pci_store/load_saved_state() around VM device usage
    
    Store the device saved state so that we can reload the device back
    to the original state when it's unassigned.  This has the benefit
    that the state survives across pci_reset_function() calls via
    the PCI sysfs reset interface while the VM is using the device.
    
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Jesse Barnes <jbarnes@virtuousgeek.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ab428552af8e..9272db03a3e5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -513,6 +513,7 @@ struct kvm_assigned_dev_kernel {
 	struct kvm *kvm;
 	spinlock_t intx_lock;
 	char irq_name[32];
+	struct pci_saved_state *pci_saved_state;
 };
 
 struct kvm_irq_mask_notifier {

commit b42fc3cbc3d6e284463e93896679379443e19d56
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Tue Apr 12 21:30:17 2011 -0400

    KVM: Fix off by one in kvm_for_each_vcpu iteration
    
    This patch avoids gcc issuing the following warning when KVM_MAX_VCPUS=1:
    warning: array subscript is above array bounds
    
    kvm_for_each_vcpu currently checks to see if the index for the vcpu is
    valid /after/ loading it. We don't run into problems because the address
    is still inside the enclosing struct kvm and we never deference or write
    to it, so this isn't a security issue.
    
    The warning occurs when KVM_MAX_VCPUS=1 because the increment portion of
    the loop will *always* cause the loop to load an invalid location since
    ++idx will always be > 0.
    
    This patch moves the load so that the check occurs before the load and
    we don't run into the compiler warning.
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d1f507567068..0bc3d372e3cb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -296,9 +296,10 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 }
 
 #define kvm_for_each_vcpu(idx, vcpup, kvm) \
-	for (idx = 0, vcpup = kvm_get_vcpu(kvm, idx); \
-	     idx < atomic_read(&kvm->online_vcpus) && vcpup; \
-	     vcpup = kvm_get_vcpu(kvm, ++idx))
+	for (idx = 0; \
+	     idx < atomic_read(&kvm->online_vcpus) && \
+	     (vcpup = kvm_get_vcpu(kvm, idx)) != NULL; \
+	     idx++)
 
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);

commit cef4dea07f6720b36cc93e18a2e68be4bdb71a92
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Jan 20 12:01:20 2010 +0200

    KVM: 16-byte mmio support
    
    Since sse instructions can issue 16-byte mmios, we need to support them.  We
    can't increase the kvm_run mmio buffer size to 16 bytes without breaking
    compatibility, so instead we break the large mmios into two smaller 8-byte
    ones.  Since the bus is 64-bit we aren't breaking any atomicity guarantees.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7ca831e55186..d1f507567068 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -27,6 +27,10 @@
 
 #include <asm/kvm_host.h>
 
+#ifndef KVM_MMIO_SIZE
+#define KVM_MMIO_SIZE 8
+#endif
+
 /*
  * vcpu->requests bit members
  */
@@ -132,7 +136,8 @@ struct kvm_vcpu {
 	int mmio_read_completed;
 	int mmio_is_write;
 	int mmio_size;
-	unsigned char mmio_data[8];
+	int mmio_index;
+	unsigned char mmio_data[KVM_MMIO_SIZE];
 	gpa_t mmio_phys_addr;
 #endif
 

commit c761e5868e6737abe0464636ebd7fcbb6814c626
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Apr 1 11:25:03 2011 -0300

    Revert "KVM: Fix race between nmi injection and enabling nmi window"
    
    This reverts commit f86368493ec038218e8663cc1b6e5393cd8e008a.
    
    Simpler fix to follow.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 57d7092d7717..7ca831e55186 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -43,7 +43,6 @@
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11
 #define KVM_REQ_APF_HALT          12
-#define KVM_REQ_NMI               13
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit 0ee8dcb87e403397e575674d0e79272b06dea12e
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Mar 9 15:41:59 2011 +0800

    KVM: cleanup memslot_id function
    
    We can get memslot id from memslot->id directly
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ab428552af8e..57d7092d7717 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -365,7 +365,6 @@ pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);
-int memslot_id(struct kvm *kvm, gfn_t gfn);
 void kvm_release_pfn_dirty(pfn_t);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
@@ -597,6 +596,11 @@ static inline void kvm_guest_exit(void)
 	current->flags &= ~PF_VCPU;
 }
 
+static inline int memslot_id(struct kvm *kvm, gfn_t gfn)
+{
+	return gfn_to_memslot(kvm, gfn)->id;
+}
+
 static inline unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
 					       gfn_t gfn)
 {

commit f86368493ec038218e8663cc1b6e5393cd8e008a
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Feb 3 15:07:07 2011 +0200

    KVM: Fix race between nmi injection and enabling nmi window
    
    The interrupt injection logic looks something like
    
      if an nmi is pending, and nmi injection allowed
        inject nmi
      if an nmi is pending
        request exit on nmi window
    
    the problem is that "nmi is pending" can be set asynchronously by
    the PIT; if it happens to fire between the two if statements, we
    will request an nmi window even though nmi injection is allowed.  On
    SVM, this has disasterous results, since it causes eflags.TF to be
    set in random guest code.
    
    The fix is simple; make nmi_pending synchronous using the standard
    vcpu->requests mechanism; this ensures the code above is completely
    synchronous wrt nmi_pending.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3751ea0d1f92..ab428552af8e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -43,6 +43,7 @@
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11
 #define KVM_REQ_APF_HALT          12
+#define KVM_REQ_NMI               13
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit 217ece6129f2d3b4fdd18d9e79be9e43d8d14a42
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 1 09:53:28 2011 -0500

    KVM: use yield_to instead of sleep in kvm_vcpu_on_spin
    
    Instead of sleeping in kvm_vcpu_on_spin, which can cause gigantic
    slowdowns of certain workloads, we instead use yield_to to get
    another VCPU in the same KVM guest to run sooner.
    
    This seems to give a 10-15% speedup in certain workloads.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4721b11b922a..3751ea0d1f92 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -235,6 +235,7 @@ struct kvm {
 #endif
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	atomic_t online_vcpus;
+	int last_boosted_vcpu;
 	struct list_head vm_list;
 	struct mutex lock;
 	struct kvm_io_bus *buses[KVM_NR_BUSES];

commit 34bb10b79de7df118de832f6832efb630e646577
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 1 09:52:41 2011 -0500

    KVM: keep track of which task is running a KVM vcpu
    
    Keep track of which task is running a KVM vcpu.  This helps us
    figure out later what task to wake up if we want to boost a
    vcpu that got preempted.
    
    Unfortunately there are no guarantees that the same task
    always keeps the same vcpu, so we can only track the task
    across a single "run" of the vcpu.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c8dee22b1945..4721b11b922a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -122,6 +122,7 @@ struct kvm_vcpu {
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
 	wait_queue_head_t wq;
+	struct pid *pid;
 	int sigset_active;
 	sigset_t sigset;
 	struct kvm_vcpu_stat stat;

commit 3cba41307a2b1344ab8c1b9f55202d1e9d7bf81b
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Jan 12 15:41:22 2011 +0800

    KVM: make make_all_cpus_request() lockless
    
    Now, we have 'vcpu->mode' to judge whether need to send ipi to other
    cpus, this way is very exact, so checking request bit is needless,
    then we can drop the spinlock let it's collateral
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b99eacd988ab..c8dee22b1945 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -224,7 +224,6 @@ struct kvm_memslots {
 
 struct kvm {
 	spinlock_t mmu_lock;
-	raw_spinlock_t requests_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots;
@@ -731,11 +730,6 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	set_bit(req, &vcpu->requests);
 }
 
-static inline bool kvm_make_check_request(int req, struct kvm_vcpu *vcpu)
-{
-	return test_and_set_bit(req, &vcpu->requests);
-}
-
 static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 {
 	if (test_bit(req, &vcpu->requests)) {

commit 6b7e2d0991489559a1df4500d77f7b76c4607ed0
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Jan 12 15:40:31 2011 +0800

    KVM: Add "exiting guest mode" state
    
    Currently we keep track of only two states: guest mode and host
    mode.  This patch adds an "exiting guest mode" state that tells
    us that an IPI will happen soon, so unless we need to wait for the
    IPI, we can avoid it completely.
    
    Also
    1: No need atomically to read/write ->mode in vcpu's thread
    
    2: reorganize struct kvm_vcpu to make ->mode and ->requests
       in the same cache line explicitly
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b5021db21858..b99eacd988ab 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -98,19 +98,26 @@ int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 
+enum {
+	OUTSIDE_GUEST_MODE,
+	IN_GUEST_MODE,
+	EXITING_GUEST_MODE
+};
+
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	struct preempt_notifier preempt_notifier;
 #endif
+	int cpu;
 	int vcpu_id;
-	struct mutex mutex;
-	int   cpu;
-	atomic_t guest_mode;
-	struct kvm_run *run;
+	int srcu_idx;
+	int mode;
 	unsigned long requests;
 	unsigned long guest_debug;
-	int srcu_idx;
+
+	struct mutex mutex;
+	struct kvm_run *run;
 
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
@@ -140,6 +147,11 @@ struct kvm_vcpu {
 	struct kvm_vcpu_arch arch;
 };
 
+static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
+{
+	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+}
+
 /*
  * Some of the bitops functions do not support too long bitmaps.
  * This number must be determined not to exceed such limits.

commit 5c663a1534d27d817e17eed06a83d08f497f9f4f
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Dec 8 18:04:51 2010 +0200

    KVM: Fix build error on s390 due to missing tlbs_dirty
    
    Make it available for all archs.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bd0da8f12500..b5021db21858 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -256,8 +256,8 @@ struct kvm {
 	struct mmu_notifier mmu_notifier;
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
-	long tlbs_dirty;
 #endif
+	long tlbs_dirty;
 };
 
 /* The guest did something we don't support. */

commit d4dbf470096c51cb4785167ea59fdbdea87ccbe4
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Tue Dec 7 12:59:07 2010 +0900

    KVM: MMU: Make the way of accessing lpage_info more generic
    
    Large page information has two elements but one of them, write_count, alone
    is accessed by a helper function.
    
    This patch replaces this helper function with more generic one which returns
    newly named kvm_lpage_info structure and use it to access the other element
    rmap_pde.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ac4e83a1a10d..bd0da8f12500 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -146,6 +146,11 @@ struct kvm_vcpu {
  */
 #define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)
 
+struct kvm_lpage_info {
+	unsigned long rmap_pde;
+	int write_count;
+};
+
 struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;
@@ -153,10 +158,7 @@ struct kvm_memory_slot {
 	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
 	unsigned long *dirty_bitmap_head;
-	struct {
-		unsigned long rmap_pde;
-		int write_count;
-	} *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned long userspace_addr;
 	int user_alloc;
 	int id;

commit a4ee1ca4a36e7857d90ae8c2b85f1bde9a042c10
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Tue Nov 23 11:13:00 2010 +0800

    KVM: MMU: delay flush all tlbs on sync_page path
    
    Quote from Avi:
    | I don't think we need to flush immediately; set a "tlb dirty" bit somewhere
    | that is cleareded when we flush the tlb.  kvm_mmu_notifier_invalidate_page()
    | can consult the bit and force a flush if set.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index da0794f707f6..ac4e83a1a10d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -254,6 +254,7 @@ struct kvm {
 	struct mmu_notifier mmu_notifier;
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
+	long tlbs_dirty;
 #endif
 };
 
@@ -382,6 +383,7 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
 void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
+
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 void kvm_reload_remote_mmus(struct kvm *kvm);
 

commit 27923eb19c5d1197bd9d1472abdc2e749f21387a
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Nov 25 10:25:44 2010 +0100

    KVM: PPC: Fix compile warning
    
    KVM compilation fails with the following warning:
    
    include/linux/kvm_host.h: In function 'kvm_irq_routing_update':
    include/linux/kvm_host.h:679:2: error: 'struct kvm' has no member named 'irq_routing'
    
    That function is only used and reasonable to have on systems that implement
    an in-kernel interrupt chip. PPC doesn't.
    
    Fix by #ifdef'ing it out when no irqchip is available.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f17beae3cca0..da0794f707f6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -673,11 +673,13 @@ static inline int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags)
 
 static inline void kvm_irqfd_release(struct kvm *kvm) {}
 
+#ifdef CONFIG_HAVE_KVM_IRQCHIP
 static inline void kvm_irq_routing_update(struct kvm *kvm,
 					  struct kvm_irq_routing_table *irq_rt)
 {
 	rcu_assign_pointer(kvm->irq_routing, irq_rt);
 }
+#endif
 
 static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {

commit bd2b53b20fcd0d6c4c815b54e6d464e34429d3a4
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Thu Nov 18 19:09:08 2010 +0200

    KVM: fast-path msi injection with irqfd
    
    Store irq routing table pointer in the irqfd object,
    and use that to inject MSI directly without bouncing out to
    a kernel thread.
    
    While we touch this structure, rearrange irqfd fields to make fastpath
    better packed for better cache utilization.
    
    This also adds some comments about locking rules and rcu usage in code.
    
    Some notes on the design:
    - Use pointer into the rt instead of copying an entry,
      to make it possible to use rcu, thus side-stepping
      locking complexities.  We also save some memory this way.
    - Old workqueue code is still used for level irqs.
      I don't think we DTRT with level anyway, however,
      it seems easier to keep the code around as
      it has been thought through and debugged, and fix level later than
      rip out and re-instate it later.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4bd663d6443d..f17beae3cca0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -17,6 +17,7 @@
 #include <linux/preempt.h>
 #include <linux/msi.h>
 #include <linux/slab.h>
+#include <linux/rcupdate.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -240,6 +241,10 @@ struct kvm {
 
 	struct mutex irq_lock;
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
+	/*
+	 * Update side is protected by irq_lock and,
+	 * if configured, irqfds.lock.
+	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
 	struct hlist_head mask_notifier_list;
 	struct hlist_head irq_ack_notifier_list;
@@ -511,6 +516,8 @@ void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
 				   unsigned long *deliver_bitmask);
 #endif
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level);
+int kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,
+		int irq_source_id, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
@@ -652,17 +659,26 @@ static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 void kvm_eventfd_init(struct kvm *kvm);
 int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags);
 void kvm_irqfd_release(struct kvm *kvm);
+void kvm_irq_routing_update(struct kvm *, struct kvm_irq_routing_table *);
 int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
 
 #else
 
 static inline void kvm_eventfd_init(struct kvm *kvm) {}
+
 static inline int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags)
 {
 	return -EINVAL;
 }
 
 static inline void kvm_irqfd_release(struct kvm *kvm) {}
+
+static inline void kvm_irq_routing_update(struct kvm *kvm,
+					  struct kvm_irq_routing_table *irq_rt)
+{
+	rcu_assign_pointer(kvm->irq_routing, irq_rt);
+}
+
 static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
 	return -ENOSYS;

commit 1e001d49f9f9a0e3eb72939ad49d9a2c7754e9c1
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Nov 16 22:30:04 2010 +0100

    KVM: Refactor IRQ names of assigned devices
    
    Cosmetic change, but it helps to correlate IRQs with PCI devices.
    
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9fe7fefe76b1..4bd663d6443d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -489,6 +489,7 @@ struct kvm_assigned_dev_kernel {
 	struct pci_dev *dev;
 	struct kvm *kvm;
 	spinlock_t intx_lock;
+	char irq_name[32];
 };
 
 struct kvm_irq_mask_notifier {

commit 0645211c43df0b96c51e12980066b3227e10b164
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Nov 16 22:30:03 2010 +0100

    KVM: Switch assigned device IRQ forwarding to threaded handler
    
    This improves the IRQ forwarding for assigned devices: By using the
    kernel's threaded IRQ scheme, we can get rid of the latency-prone work
    queue and simplify the code in the same run.
    
    Moreover, we no longer have to hold assigned_dev_lock while raising the
    guest IRQ, which can be a lenghty operation as we may have to iterate
    over all VCPUs. The lock is now only used for synchronizing masking vs.
    unmasking of INTx-type IRQs, thus is renames to intx_lock.
    
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2d63f2c0137c..9fe7fefe76b1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -470,16 +470,8 @@ struct kvm_irq_ack_notifier {
 	void (*irq_acked)(struct kvm_irq_ack_notifier *kian);
 };
 
-#define KVM_ASSIGNED_MSIX_PENDING		0x1
-struct kvm_guest_msix_entry {
-	u32 vector;
-	u16 entry;
-	u16 flags;
-};
-
 struct kvm_assigned_dev_kernel {
 	struct kvm_irq_ack_notifier ack_notifier;
-	struct work_struct interrupt_work;
 	struct list_head list;
 	int assigned_dev_id;
 	int host_segnr;
@@ -490,13 +482,13 @@ struct kvm_assigned_dev_kernel {
 	bool host_irq_disabled;
 	struct msix_entry *host_msix_entries;
 	int guest_irq;
-	struct kvm_guest_msix_entry *guest_msix_entries;
+	struct msix_entry *guest_msix_entries;
 	unsigned long irq_requested_type;
 	int irq_source_id;
 	int flags;
 	struct pci_dev *dev;
 	struct kvm *kvm;
-	spinlock_t assigned_dev_lock;
+	spinlock_t intx_lock;
 };
 
 struct kvm_irq_mask_notifier {

commit d89f5eff70a31237ffa1e21c51d23ca532110aea
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Nov 9 17:02:49 2010 +0100

    KVM: Clean up vm creation and release
    
    IA64 support forces us to abstract the allocation of the kvm structure.
    But instead of mixing this up with arch-specific initialization and
    doing the same on destruction, split both steps. This allows to move
    generic destruction calls into generic code.
    
    It also fixes error clean-up on failures of kvm_create_vm for IA64.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bcf71c7730f0..2d63f2c0137c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -16,6 +16,7 @@
 #include <linux/mm.h>
 #include <linux/preempt.h>
 #include <linux/msi.h>
+#include <linux/slab.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -441,7 +442,19 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
 
 void kvm_free_physmem(struct kvm *kvm);
 
-struct  kvm *kvm_arch_create_vm(void);
+#ifndef __KVM_HAVE_ARCH_VM_ALLOC
+static inline struct kvm *kvm_arch_alloc_vm(void)
+{
+	return kzalloc(sizeof(struct kvm), GFP_KERNEL);
+}
+
+static inline void kvm_arch_free_vm(struct kvm *kvm)
+{
+	kfree(kvm);
+}
+#endif
+
+int kvm_arch_init_vm(struct kvm *kvm);
 void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);

commit 515a01279a187415322a80736800a7d6325876ab
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Oct 27 18:23:54 2010 +0900

    KVM: pre-allocate one more dirty bitmap to avoid vmalloc()
    
    Currently x86's kvm_vm_ioctl_get_dirty_log() needs to allocate a bitmap by
    vmalloc() which will be used in the next logging and this has been causing
    bad effect to VGA and live-migration: vmalloc() consumes extra systime,
    triggers tlb flush, etc.
    
    This patch resolves this issue by pre-allocating one more bitmap and switching
    between two bitmaps during dirty logging.
    
    Performance improvement:
      I measured performance for the case of VGA update by trace-cmd.
      The result was 1.5 times faster than the original one.
    
      In the case of live migration, the improvement ratio depends on the workload
      and the guest memory size. In general, the larger the memory size is the more
      benefits we get.
    
    Note:
      This does not change other architectures's logic but the allocation size
      becomes twice. This will increase the actual memory consumption only when
      the new size changes the number of pages allocated by vmalloc().
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 462b982fedfb..bcf71c7730f0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -150,6 +150,7 @@ struct kvm_memory_slot {
 	unsigned long flags;
 	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
+	unsigned long *dirty_bitmap_head;
 	struct {
 		unsigned long rmap_pde;
 		int write_count;

commit 612819c3c6e67bac8fceaa7cc402f13b1b63f7e4
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Oct 22 14:18:18 2010 -0200

    KVM: propagate fault r/w information to gup(), allow read-only memory
    
    As suggested by Andrea, pass r/w error code to gup(), upgrading read fault
    to writable if host pte allows it.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ee4314e15ead..462b982fedfb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -334,8 +334,11 @@ void kvm_set_page_accessed(struct page *page);
 
 pfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr);
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async);
+pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,
+		       bool write_fault, bool *writable);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+		      bool *writable);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);
 int memslot_id(struct kvm *kvm, gfn_t gfn);

commit 344d9588a9df06182684168be4f1408b55c7da3e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:50 2010 +0200

    KVM: Add PV MSR to enable asynchronous page faults delivery.
    
    Guest enables async PF vcpu functionality using this MSR.
    
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e6748204cd56..ee4314e15ead 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -93,6 +93,7 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu);
 int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
 		       struct kvm_arch_async_pf *arch);
+int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 #endif
 
 struct kvm_vcpu {

commit 49c7754ce57063b819b01eb8a4290841ad0886c4
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Oct 18 15:22:23 2010 +0200

    KVM: Add memory slot versioning and use it to provide fast guest write interface
    
    Keep track of memslots changes by keeping generation number in memslots
    structure. Provide kvm_write_guest_cached() function that skips
    gfn_to_hva() translation if memslots was not changed since previous
    invocation.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e56acc7857e2..e6748204cd56 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -199,6 +199,7 @@ struct kvm_irq_routing_table {};
 
 struct kvm_memslots {
 	int nmemslots;
+	u64 generation;
 	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
 					KVM_PRIVATE_MEM_SLOTS];
 };
@@ -352,12 +353,18 @@ int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);
 int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
 		    unsigned long len);
+int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			   void *data, unsigned long len);
+int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
+			      gpa_t gpa);
 int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
+void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,
+			     gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);

commit af585b921e5d1e919947c4b1164b59507fe7cd7b
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Oct 14 11:22:46 2010 +0200

    KVM: Halt vcpu if page it tries to access is swapped out
    
    If a guest accesses swapped out memory do not swap it in from vcpu thread
    context. Schedule work to do swapping and put vcpu into halted state
    instead.
    
    Interrupts will still be delivered to the guest and if interrupt will
    cause reschedule guest will continue to run another task.
    
    [avi: remove call to get_user_pages_noio(), nacked by Linus; this
          makes everything synchrnous again]
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a0557422715e..e56acc7857e2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -40,6 +40,7 @@
 #define KVM_REQ_KICK               9
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11
+#define KVM_REQ_APF_HALT          12
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 
@@ -74,6 +75,26 @@ int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 			      struct kvm_io_device *dev);
 
+#ifdef CONFIG_KVM_ASYNC_PF
+struct kvm_async_pf {
+	struct work_struct work;
+	struct list_head link;
+	struct list_head queue;
+	struct kvm_vcpu *vcpu;
+	struct mm_struct *mm;
+	gva_t gva;
+	unsigned long addr;
+	struct kvm_arch_async_pf arch;
+	struct page *page;
+	bool done;
+};
+
+void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);
+void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu);
+int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
+		       struct kvm_arch_async_pf *arch);
+#endif
+
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
@@ -104,6 +125,15 @@ struct kvm_vcpu {
 	gpa_t mmio_phys_addr;
 #endif
 
+#ifdef CONFIG_KVM_ASYNC_PF
+	struct {
+		u32 queued;
+		struct list_head queue;
+		struct list_head done;
+		spinlock_t lock;
+	} async_pf;
+#endif
+
 	struct kvm_vcpu_arch arch;
 };
 
@@ -302,6 +332,7 @@ void kvm_set_page_accessed(struct page *page);
 
 pfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr);
 pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
+pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);

commit 1765a1fe5d6f82c0eceb1ad10594cfc83759b6d0
Merge: bdaf12b41235 2a31339aa014
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 24 12:47:25 2010 -0700

    Merge branch 'kvm-updates/2.6.37' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.37' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (321 commits)
      KVM: Drop CONFIG_DMAR dependency around kvm_iommu_map_pages
      KVM: Fix signature of kvm_iommu_map_pages stub
      KVM: MCE: Send SRAR SIGBUS directly
      KVM: MCE: Add MCG_SER_P into KVM_MCE_CAP_SUPPORTED
      KVM: fix typo in copyright notice
      KVM: Disable interrupts around get_kernel_ns()
      KVM: MMU: Avoid sign extension in mmu_alloc_direct_roots() pae root address
      KVM: MMU: move access code parsing to FNAME(walk_addr) function
      KVM: MMU: audit: check whether have unsync sps after root sync
      KVM: MMU: audit: introduce audit_printk to cleanup audit code
      KVM: MMU: audit: unregister audit tracepoints before module unloaded
      KVM: MMU: audit: fix vcpu's spte walking
      KVM: MMU: set access bit for direct mapping
      KVM: MMU: cleanup for error mask set while walk guest page table
      KVM: MMU: update 'root_hpa' out of loop in PAE shadow path
      KVM: x86 emulator: Eliminate compilation warning in x86_decode_insn()
      KVM: x86: Fix constant type in kvm_get_time_scale
      KVM: VMX: Add AX to list of registers clobbered by guest switch
      KVM guest: Move a printk that's using the clock before it's ready
      KVM: x86: TSC catchup mode
      ...

commit d7a79b6c80fdbe4366484805ee07a4735fc427d8
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Thu Oct 14 13:59:04 2010 +0200

    KVM: Fix signature of kvm_iommu_map_pages stub
    
    Breaks otherwise if CONFIG_IOMMU_API is not set.
    
    KVM-Stable-Tag.
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0b89d008db65..866ed3084363 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -483,8 +483,7 @@ int kvm_deassign_device(struct kvm *kvm,
 			struct kvm_assigned_dev_kernel *assigned_dev);
 #else /* CONFIG_IOMMU_API */
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
-				      gfn_t base_gfn,
-				      unsigned long npages)
+				      struct kvm_memory_slot *slot)
 {
 	return 0;
 }

commit 34c238a1d1832d7b1f655641f52782e86396b30a
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Sat Sep 18 14:38:14 2010 -1000

    KVM: x86: Rename timer function
    
    This just changes some names to better reflect the usage they
    will be given.  Separated out to keep confusion to a minimum.
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6022da1490e4..0b89d008db65 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -36,7 +36,7 @@
 #define KVM_REQ_PENDING_TIMER      5
 #define KVM_REQ_UNHALT             6
 #define KVM_REQ_MMU_SYNC           7
-#define KVM_REQ_KVMCLOCK_UPDATE    8
+#define KVM_REQ_CLOCK_UPDATE       8
 #define KVM_REQ_KICK               9
 #define KVM_REQ_DEACTIVATE_FPU    10
 #define KVM_REQ_EVENT             11

commit 3842d135ff246b6543f1df77f5600e12094a6845
Author: Avi Kivity <avi@redhat.com>
Date:   Tue Jul 27 12:30:24 2010 +0300

    KVM: Check for pending events before attempting injection
    
    Instead of blindly attempting to inject an event before each guest entry,
    check for a possible event first in vcpu->requests.  Sites that can trigger
    event injection are modified to set KVM_REQ_EVENT:
    
    - interrupt, nmi window opening
    - ppr updates
    - i8259 output changes
    - local apic irr changes
    - rflags updates
    - gif flag set
    - event set on exit
    
    This improves non-injecting entry performance, and sets the stage for
    non-atomic injection.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 917e68ff5ed2..6022da1490e4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -39,6 +39,7 @@
 #define KVM_REQ_KVMCLOCK_UPDATE    8
 #define KVM_REQ_KICK               9
 #define KVM_REQ_DEACTIVATE_FPU    10
+#define KVM_REQ_EVENT             11
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit c30a358d33e0e111f06e54a4a4125371e6b6693c
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Sep 10 17:30:48 2010 +0200

    KVM: MMU: Add infrastructure for two-level page walker
    
    This patch introduces a mmu-callback to translate gpa
    addresses in the walk_addr code. This is later used to
    translate l2_gpa addresses into l1_gpa addresses.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f2ecdd52032b..917e68ff5ed2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -534,6 +534,11 @@ static inline gpa_t gfn_to_gpa(gfn_t gfn)
 	return (gpa_t)gfn << PAGE_SHIFT;
 }
 
+static inline gfn_t gpa_to_gfn(gpa_t gpa)
+{
+	return (gfn_t)(gpa >> PAGE_SHIFT);
+}
+
 static inline hpa_t pfn_to_hpa(pfn_t pfn)
 {
 	return (hpa_t)pfn << PAGE_SHIFT;

commit 365fb3fdf6769d3553999d8eb6cc2a8c56c747c1
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sat Aug 28 19:24:13 2010 +0800

    KVM: MMU: rewrite audit_mappings_page() function
    
    There is a bugs in this function, we call gfn_to_pfn() and kvm_mmu_gva_to_gpa_read() in
    atomic context(kvm_mmu_audit() is called under the spinlock(mmu_lock)'s protection).
    
    This patch fix it by:
    - introduce gfn_to_pfn_atomic instead of gfn_to_pfn
    - get the mapping gfn from kvm_mmu_page_get_gfn()
    
    And it adds 'notrap' ptes check in unsync/direct sps
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b837ec80885d..f2ecdd52032b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -300,6 +300,7 @@ void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
 pfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr);
+pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);

commit 48987781eb1d1e8ded41f55cd5806615fda92c6e
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sun Aug 22 19:11:43 2010 +0800

    KVM: MMU: introduce gfn_to_page_many_atomic() function
    
    Introduce this function to get consecutive gfn's pages, it can reduce
    gup's overload, used by later patch
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 307d0e2c0f59..b837ec80885d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -289,6 +289,9 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 void kvm_disable_largepages(void);
 void kvm_arch_flush_shadow(struct kvm *kvm);
 
+int gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,
+			    int nr_pages);
+
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);

commit 887c08ac191efb103e33e589aacbc2ce1a3f131e
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Sun Aug 22 19:10:28 2010 +0800

    KVM: MMU: introduce hva_to_pfn_atomic function
    
    Introduce hva_to_pfn_atomic(), it's the fast path and can used in atomic
    context, the later patch will use it
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c13cc48697aa..307d0e2c0f59 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -296,6 +296,7 @@ void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
+pfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);
@@ -518,6 +519,12 @@ static inline void kvm_guest_exit(void)
 	current->flags &= ~PF_VCPU;
 }
 
+static inline unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot,
+					       gfn_t gfn)
+{
+	return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
+}
+
 static inline gpa_t gfn_to_gpa(gfn_t gfn)
 {
 	return (gpa_t)gfn << PAGE_SHIFT;

commit 4b6a2872a2a00042ee50024822ab706e5456aad8
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Mar 4 15:59:23 2010 +0100

    kvm: add __rcu annotations
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c13cc48697aa..ac740b26eb10 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -205,7 +205,7 @@ struct kvm {
 
 	struct mutex irq_lock;
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
-	struct kvm_irq_routing_table *irq_routing;
+	struct kvm_irq_routing_table __rcu *irq_routing;
 	struct hlist_head mask_notifier_list;
 	struct hlist_head irq_ack_notifier_list;
 #endif

commit 4a994358b919c3b14de61be5e30d9edc9089ba3f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Jul 11 15:32:23 2010 +0300

    KVM: Convert mask notifiers to use irqchip/pin instead of gsi
    
    Devices register mask notifier using gsi, but irqchip knows about
    irqchip/pin, so conversion from irqchip/pin to gsi should be done before
    looking for mask notifier to call.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8055067b6bec..c13cc48697aa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -447,7 +447,8 @@ void kvm_register_irq_mask_notifier(struct kvm *kvm, int irq,
 				    struct kvm_irq_mask_notifier *kimn);
 void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
-void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
+void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
+			     bool mask);
 
 #ifdef __KVM_HAVE_IOAPIC
 void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,

commit edba23e51578f7cb6781461568489fc1825db4ac
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Jul 7 20:16:45 2010 +0300

    KVM: Return EFAULT from kvm ioctl when guest accesses bad area
    
    Currently if guest access address that belongs to memory slot but is not
    backed up by page or page is read only KVM treats it like MMIO access.
    Remove that capability. It was never part of the interface and should
    not be relied upon.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e796326f3646..8055067b6bec 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -269,6 +269,7 @@ extern pfn_t bad_pfn;
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);
 int is_hwpoison_pfn(pfn_t pfn);
+int is_fault_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,

commit e36d96f7cfaa71870c407131eb4fbd38ea285c01
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 10:56:36 2010 +0300

    KVM: Keep slot ID in memory slot structure
    
    May be used for distinguishing between internal and user slots, or for sorting
    slots in size order.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e820eb579108..e796326f3646 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -124,6 +124,7 @@ struct kvm_memory_slot {
 	} *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned long userspace_addr;
 	int user_alloc;
+	int id;
 };
 
 static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memslot)

commit 0719837c0832a7b305e42327caa7d330462360ea
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 10 13:08:26 2010 +0300

    KVM: Reduce atomic operations on vcpu->requests
    
    Usually the vcpu->requests bitmap is sparse, so a test_and_clear_bit() for
    each request generates a large number of unneeded atomics if a bit is set.
    
    Replace with a separate test/clear sequence.  This is safe since there is
    no clear_bit() outside the vcpu thread.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c8a9d628898e..e820eb579108 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -636,7 +636,12 @@ static inline bool kvm_make_check_request(int req, struct kvm_vcpu *vcpu)
 
 static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
 {
-	return test_and_clear_bit(req, &vcpu->requests);
+	if (test_bit(req, &vcpu->requests)) {
+		clear_bit(req, &vcpu->requests);
+		return true;
+	} else {
+		return false;
+	}
 }
 
 #endif

commit a8eeb04a44dd6dc4c8158953d9bae48849c9a188
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 10 12:34:53 2010 +0300

    KVM: Add mini-API for vcpu->requests
    
    Makes it a little more readable and hackable.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 240e460777bc..c8a9d628898e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -624,5 +624,20 @@ static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
 
 #endif
 
+static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
+{
+	set_bit(req, &vcpu->requests);
+}
+
+static inline bool kvm_make_check_request(int req, struct kvm_vcpu *vcpu)
+{
+	return test_and_set_bit(req, &vcpu->requests);
+}
+
+static inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)
+{
+	return test_and_clear_bit(req, &vcpu->requests);
+}
+
 #endif
 

commit a1f4d39500ad8ed61825eff061debff42386ab5b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 11:44:20 2010 +0300

    KVM: Remove memory alias support
    
    As advertised in feature-removal-schedule.txt.  Equivalent support is provided
    by overlapping memory regions.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2d96555cd4ed..240e460777bc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -286,8 +286,6 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				int user_alloc);
 void kvm_disable_largepages(void);
 void kvm_arch_flush_shadow(struct kvm *kvm);
-gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
-gfn_t unalias_gfn_instantiation(struct kvm *kvm, gfn_t gfn);
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
@@ -564,10 +562,6 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 }
 #endif
 
-#ifndef KVM_ARCH_HAS_UNALIAS_INSTANTIATION
-#define unalias_gfn_instantiation unalias_gfn
-#endif
-
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 
 #define KVM_MAX_IRQ_ROUTES 1024

commit 2acf923e38fb6a4ce0c57115decbb38d334902ac
Author: Dexuan Cui <dexuan.cui@intel.com>
Date:   Thu Jun 10 11:27:12 2010 +0800

    KVM: VMX: Enable XSAVE/XRSTOR for guest
    
    This patch enable guest to use XSAVE/XRSTOR instructions.
    
    We assume that host_xcr0 would use all possible bits that OS supported.
    
    And we loaded xcr0 in the same way we handled fpu - do it as late as we can.
    
    Signed-off-by: Dexuan Cui <dexuan.cui@intel.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c62319727ef..2d96555cd4ed 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -88,7 +88,7 @@ struct kvm_vcpu {
 	int srcu_idx;
 
 	int fpu_active;
-	int guest_fpu_loaded;
+	int guest_fpu_loaded, guest_xcr0_loaded;
 	wait_queue_head_t wq;
 	int sigset_active;
 	sigset_t sigset;

commit d94e1dc9af60e3431a586c3edfbe42d8a0d3932b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon May 3 16:54:48 2010 +0300

    KVM: Get rid of KVM_REQ_KICK
    
    KVM_REQ_KICK poisons vcpu->requests by having a bit set during normal
    operation.  This causes the fast path check for a clear vcpu->requests
    to fail all the time, triggering tons of atomic operations.
    
    Fix by replacing KVM_REQ_KICK with a vcpu->guest_mode atomic.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a0e019769f5d..2c62319727ef 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -81,6 +81,7 @@ struct kvm_vcpu {
 	int vcpu_id;
 	struct mutex mutex;
 	int   cpu;
+	atomic_t guest_mode;
 	struct kvm_run *run;
 	unsigned long requests;
 	unsigned long guest_debug;

commit bf998156d24bcb127318ad5bf531ac3bdfcd6449
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon May 31 14:28:19 2010 +0800

    KVM: Avoid killing userspace through guest SRAO MCE on unmapped pages
    
    In common cases, guest SRAO MCE will cause corresponding poisoned page
    be un-mapped and SIGBUS be sent to QEMU-KVM, then QEMU-KVM will relay
    the MCE to guest OS.
    
    But it is reported that if the poisoned page is accessed in guest
    after unmapping and before MCE is relayed to guest OS, userspace will
    be killed.
    
    The reason is as follows. Because poisoned page has been un-mapped,
    guest access will cause guest exit and kvm_mmu_page_fault will be
    called. kvm_mmu_page_fault can not get the poisoned page for fault
    address, so kernel and user space MMIO processing is tried in turn. In
    user MMIO processing, poisoned page is accessed again, then userspace
    is killed by force_sig_info.
    
    To fix the bug, kvm_mmu_page_fault send HWPOISON signal to QEMU-KVM
    and do not try kernel and user space MMIO processing for poisoned
    page.
    
    [xiao: fix warning introduced by avi]
    
    Reported-by: Max Asbock <masbock@linux.vnet.ibm.com>
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7cb116afa1cd..a0e019769f5d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -266,6 +266,7 @@ extern pfn_t bad_pfn;
 
 int is_error_page(struct page *page);
 int is_error_pfn(pfn_t pfn);
+int is_hwpoison_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,

commit 0ee75bead83da4791e5cbf659806c54d8ee40f12
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Apr 28 15:39:01 2010 +0300

    KVM: Let vcpu structure alignment be determined at runtime
    
    vmx and svm vcpus have different contents and therefore may have different
    alignmment requirements.  Let each specify its required alignment.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ce027d518096..7cb116afa1cd 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -243,7 +243,7 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 void vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
-int kvm_init(void *opaque, unsigned int vcpu_size,
+int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module);
 void kvm_exit(void);
 

commit 2a059bf444dd7758ccf48f217cd981570132be85
Author: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
Date:   Fri Apr 16 17:19:48 2010 +0800

    KVM: Get rid of dead function gva_to_page()
    
    Nobody use gva_to_page() anymore, get rid of it.
    
    Signed-off-by: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1ed030bad59e..ce027d518096 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -260,7 +260,6 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
 #define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
-struct page *gva_to_page(struct kvm_vcpu *vcpu, gva_t gva);
 
 extern struct page *bad_page;
 extern pfn_t bad_pfn;

commit 90d83dc3d49f5101addae962ccc1b4aff66b68d8
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Apr 19 17:41:23 2010 +0800

    KVM: use the correct RCU API for PROVE_RCU=y
    
    The RCU/SRCU API have already changed for proving RCU usage.
    
    I got the following dmesg when PROVE_RCU=y because we used incorrect API.
    This patch coverts rcu_deference() to srcu_dereference() or family API.
    
    ===================================================
    [ INFO: suspicious rcu_dereference_check() usage. ]
    ---------------------------------------------------
    arch/x86/kvm/mmu.c:3020 invoked rcu_dereference_check() without protection!
    
    other info that might help us debug this:
    
    rcu_scheduler_active = 1, debug_locks = 0
    2 locks held by qemu-system-x86/8550:
     #0:  (&kvm->slots_lock){+.+.+.}, at: [<ffffffffa011a6ac>] kvm_set_memory_region+0x29/0x50 [kvm]
     #1:  (&(&kvm->mmu_lock)->rlock){+.+...}, at: [<ffffffffa012262d>] kvm_arch_commit_memory_region+0xa6/0xe2 [kvm]
    
    stack backtrace:
    Pid: 8550, comm: qemu-system-x86 Not tainted 2.6.34-rc4-tip-01028-g939eab1 #27
    Call Trace:
     [<ffffffff8106c59e>] lockdep_rcu_dereference+0xaa/0xb3
     [<ffffffffa012f6c1>] kvm_mmu_calculate_mmu_pages+0x44/0x7d [kvm]
     [<ffffffffa012263e>] kvm_arch_commit_memory_region+0xb7/0xe2 [kvm]
     [<ffffffffa011a5d7>] __kvm_set_memory_region+0x636/0x6e2 [kvm]
     [<ffffffffa011a6ba>] kvm_set_memory_region+0x37/0x50 [kvm]
     [<ffffffffa015e956>] vmx_set_tss_addr+0x46/0x5a [kvm_intel]
     [<ffffffffa0126592>] kvm_arch_vm_ioctl+0x17a/0xcf8 [kvm]
     [<ffffffff810a8692>] ? unlock_page+0x27/0x2c
     [<ffffffff810bf879>] ? __do_fault+0x3a9/0x3e1
     [<ffffffffa011b12f>] kvm_vm_ioctl+0x364/0x38d [kvm]
     [<ffffffff81060cfa>] ? up_read+0x23/0x3d
     [<ffffffff810f3587>] vfs_ioctl+0x32/0xa6
     [<ffffffff810f3b19>] do_vfs_ioctl+0x495/0x4db
     [<ffffffff810e6b2f>] ? fget_light+0xc2/0x241
     [<ffffffff810e416c>] ? do_sys_open+0x104/0x116
     [<ffffffff81382d6d>] ? retint_swapgs+0xe/0x13
     [<ffffffff810f3ba6>] sys_ioctl+0x47/0x6a
     [<ffffffff810021db>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 55830638aeb1..1ed030bad59e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -250,6 +250,13 @@ void kvm_exit(void);
 void kvm_get_kvm(struct kvm *kvm);
 void kvm_put_kvm(struct kvm *kvm);
 
+static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
+{
+	return rcu_dereference_check(kvm->memslots,
+			srcu_read_lock_held(&kvm->srcu)
+			|| lockdep_is_held(&kvm->slots_lock));
+}
+
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
 #define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }

commit 660c22c425cbe14badfb3b0a0206862577701ab7
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Tue Apr 13 22:47:24 2010 +0900

    KVM: limit the number of pages per memory slot
    
    This patch limits the number of pages per memory slot to make
    us free from extra care about type issues.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 169d07758ee5..55830638aeb1 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -105,6 +105,12 @@ struct kvm_vcpu {
 	struct kvm_vcpu_arch arch;
 };
 
+/*
+ * Some of the bitops functions do not support too long bitmaps.
+ * This number must be determined not to exceed such limits.
+ */
+#define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)
+
 struct kvm_memory_slot {
 	gfn_t base_gfn;
 	unsigned long npages;

commit e80e2a60ff7914dae691345a976c80bbbff3ec74
Author: Sridhar Samudrala <sri@us.ibm.com>
Date:   Tue Mar 30 16:48:25 2010 -0700

    KVM: Increase NR_IOBUS_DEVS limit to 200
    
    This patch increases the current hardcoded limit of NR_IOBUS_DEVS
    from 6 to 200. We are hitting this limit when creating a guest with more
    than 1 virtio-net device using vhost-net backend. Each virtio-net
    device requires 2 such devices to service notifications from rx/tx queues.
    
    Signed-off-by: Sridhar Samudrala <sri@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9ad825e1c79b..169d07758ee5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -54,7 +54,7 @@ extern struct kmem_cache *kvm_vcpu_cache;
  */
 struct kvm_io_bus {
 	int                   dev_count;
-#define NR_IOBUS_DEVS 6
+#define NR_IOBUS_DEVS 200
 	struct kvm_io_device *devs[NR_IOBUS_DEVS];
 };
 

commit 87bf6e7de1134f48681fd2ce4b7c1ec45458cb6d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Mon Apr 12 19:35:35 2010 +0900

    KVM: fix the handling of dirty bitmaps to avoid overflows
    
    Int is not long enough to store the size of a dirty bitmap.
    
    This patch fixes this problem with the introduction of a wrapper
    function to calculate the sizes of dirty bitmaps.
    
    Note: in mark_page_dirty(), we have to consider the fact that
      __set_bit() takes the offset as int, not long.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a3fd0f91d943..9ad825e1c79b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -119,6 +119,11 @@ struct kvm_memory_slot {
 	int user_alloc;
 };
 
+static inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memslot)
+{
+	return ALIGN(memslot->npages, BITS_PER_LONG) / 8;
+}
+
 struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
 	u32 type;

commit 70e335e16882df5b5d6971022e63c3603a1e8c23
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Feb 18 11:25:22 2010 +0200

    KVM: Convert kvm->requests_lock to raw_spinlock_t
    
    The code relies on kvm->requests_lock inhibiting preemption.
    
    Noted by Jan Kiszka.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3145b281de9d..a3fd0f91d943 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -161,7 +161,7 @@ struct kvm_memslots {
 
 struct kvm {
 	spinlock_t mmu_lock;
-	spinlock_t requests_lock;
+	raw_spinlock_t requests_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots;

commit 8f0b1ab6fb045a1324d9435ba00c2940783b0041
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Thu Jan 28 12:37:56 2010 +0100

    KVM: Introduce kvm_host_page_size
    
    This patch introduces a generic function to find out the
    host page size for a given gfn. This function is needed by
    the kvm iommu code. This patch also simplifies the x86
    host_mapping_level function.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 665c37063f30..3145b281de9d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -300,6 +300,7 @@ int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
+unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);

commit ab9f4ecbb6d39a18e300a0d10a4968c37404aa76
Author: Zhai, Edwin <edwin.zhai@intel.com>
Date:   Fri Jan 29 14:38:44 2010 +0800

    KVM: enable PCI multiple-segments for pass-through device
    
    Enable optional parameter (default 0) - PCI segment (or domain) besides
    BDF, when assigning PCI device to guest.
    
    Signed-off-by: Zhai Edwin <edwin.zhai@intel.com>
    Acked-by: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index dfde04b0d453..665c37063f30 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -400,6 +400,7 @@ struct kvm_assigned_dev_kernel {
 	struct work_struct interrupt_work;
 	struct list_head list;
 	int assigned_dev_id;
+	int host_segnr;
 	int host_busnr;
 	int host_devfn;
 	unsigned int entries_nr;

commit 02daab21d94dc4cf01b2fd09863d59a436900322
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Dec 30 12:40:26 2009 +0200

    KVM: Lazify fpu activation and deactivation
    
    Defer fpu deactivation as much as possible - if the guest fpu is loaded, keep
    it loaded until the next heavyweight exit (where we are forced to unload it).
    This reduces unnecessary exits.
    
    We also defer fpu activation on clts; while clts signals the intent to use the
    fpu, we can't be sure the guest will actually use it.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bb0314ea9267..dfde04b0d453 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -38,6 +38,7 @@
 #define KVM_REQ_MMU_SYNC           7
 #define KVM_REQ_KVMCLOCK_UPDATE    8
 #define KVM_REQ_KICK               9
+#define KVM_REQ_DEACTIVATE_FPU    10
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit 79fac95ecfa3969aab8119d37ccd7226165f933a
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:26 2009 -0200

    KVM: convert slots_lock to a mutex
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0bb9aa295e6c..bb0314ea9267 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -161,7 +161,7 @@ struct kvm_memslots {
 struct kvm {
 	spinlock_t mmu_lock;
 	spinlock_t requests_lock;
-	struct rw_semaphore slots_lock;
+	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots;
 	struct srcu_struct srcu;

commit f656ce0185cabbbb0cf96877306879661297c7ad
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:25 2009 -0200

    KVM: switch vcpu context to use SRCU
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5e9cb902550b..0bb9aa295e6c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -83,6 +83,8 @@ struct kvm_vcpu {
 	struct kvm_run *run;
 	unsigned long requests;
 	unsigned long guest_debug;
+	int srcu_idx;
+
 	int fpu_active;
 	int guest_fpu_loaded;
 	wait_queue_head_t wq;

commit e93f8a0f821e290ac5149830110a5f704db7a1fc
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:24 2009 -0200

    KVM: convert io_bus to SRCU
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 20941c0f4045..5e9cb902550b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -57,20 +57,20 @@ struct kvm_io_bus {
 	struct kvm_io_device *devs[NR_IOBUS_DEVS];
 };
 
-void kvm_io_bus_init(struct kvm_io_bus *bus);
-void kvm_io_bus_destroy(struct kvm_io_bus *bus);
-int kvm_io_bus_write(struct kvm_io_bus *bus, gpa_t addr, int len,
-		     const void *val);
-int kvm_io_bus_read(struct kvm_io_bus *bus, gpa_t addr, int len,
+enum kvm_bus {
+	KVM_MMIO_BUS,
+	KVM_PIO_BUS,
+	KVM_NR_BUSES
+};
+
+int kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
+		     int len, const void *val);
+int kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr, int len,
 		    void *val);
-int __kvm_io_bus_register_dev(struct kvm_io_bus *bus,
-			       struct kvm_io_device *dev);
-int kvm_io_bus_register_dev(struct kvm *kvm, struct kvm_io_bus *bus,
+int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 			    struct kvm_io_device *dev);
-void __kvm_io_bus_unregister_dev(struct kvm_io_bus *bus,
-				 struct kvm_io_device *dev);
-void kvm_io_bus_unregister_dev(struct kvm *kvm, struct kvm_io_bus *bus,
-			       struct kvm_io_device *dev);
+int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,
+			      struct kvm_io_device *dev);
 
 struct kvm_vcpu {
 	struct kvm *kvm;
@@ -171,8 +171,7 @@ struct kvm {
 	atomic_t online_vcpus;
 	struct list_head vm_list;
 	struct mutex lock;
-	struct kvm_io_bus mmio_bus;
-	struct kvm_io_bus pio_bus;
+	struct kvm_io_bus *buses[KVM_NR_BUSES];
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 	struct {
 		spinlock_t        lock;

commit a983fb238728e1123177e8058d4f644b949a7d05
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:23 2009 -0200

    KVM: x86: switch kvm_set_memory_alias to SRCU update
    
    Using a similar two-step procedure as for memslots.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 93bd30701ca7..20941c0f4045 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -266,6 +266,8 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 void kvm_disable_largepages(void);
 void kvm_arch_flush_shadow(struct kvm *kvm);
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
+gfn_t unalias_gfn_instantiation(struct kvm *kvm, gfn_t gfn);
+
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
@@ -539,6 +541,10 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 }
 #endif
 
+#ifndef KVM_ARCH_HAS_UNALIAS_INSTANTIATION
+#define unalias_gfn_instantiation unalias_gfn
+#endif
+
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 
 #define KVM_MAX_IRQ_ROUTES 1024

commit bc6678a33d9b952981a8e44a4f876c3ad64ca4d8
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:21 2009 -0200

    KVM: introduce kvm->srcu and convert kvm_set_memory_region to SRCU update
    
    Use two steps for memslot deletion: mark the slot invalid (which stops
    instantiation of new shadow pages for that slot, but allows destruction),
    then instantiate the new empty slot.
    
    Also simplifies kvm_handle_hva locking.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9af240387fe6..93bd30701ca7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -162,6 +162,7 @@ struct kvm {
 	struct rw_semaphore slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots *memslots;
+	struct srcu_struct srcu;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
 	u32 bsp_vcpu_id;
 	struct kvm_vcpu *bsp_vcpu;
@@ -275,6 +276,7 @@ void kvm_set_page_accessed(struct page *page);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);
+int memslot_id(struct kvm *kvm, gfn_t gfn);
 void kvm_release_pfn_dirty(pfn_t);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);
@@ -490,11 +492,6 @@ static inline void kvm_guest_exit(void)
 	current->flags &= ~PF_VCPU;
 }
 
-static inline int memslot_id(struct kvm *kvm, struct kvm_memory_slot *slot)
-{
-	return slot - kvm->memslots->memslots;
-}
-
 static inline gpa_t gfn_to_gpa(gfn_t gfn)
 {
 	return (gpa_t)gfn << PAGE_SHIFT;

commit 3ad26d8139a82b0510b1e0435ee82ae461d33401
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:20 2009 -0200

    KVM: use gfn_to_pfn_memslot in kvm_iommu_map_pages
    
    So its possible to iommu map a memslot before making it visible to
    kvm.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f1f78deece10..9af240387fe6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -440,8 +440,7 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 #define KVM_IOMMU_CACHE_COHERENCY	0x1
 
 #ifdef CONFIG_IOMMU_API
-int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,
-			unsigned long npages);
+int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
 int kvm_iommu_map_guest(struct kvm *kvm);
 int kvm_iommu_unmap_guest(struct kvm *kvm);
 int kvm_assign_device(struct kvm *kvm,

commit 506f0d6f9c40ae7d9634acf3c26358810f42c24a
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:19 2009 -0200

    KVM: introduce gfn_to_pfn_memslot
    
    Which takes a memslot pointer instead of using kvm->memslots.
    
    To be used by SRCU convertion later.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3c44687b3425..f1f78deece10 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -273,6 +273,8 @@ void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
+			 struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_release_pfn_dirty(pfn_t);
 void kvm_release_pfn_clean(pfn_t pfn);
 void kvm_set_pfn_dirty(pfn_t pfn);

commit f7784b8ec9b6a041fa828cfbe9012fe51933f5ac
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:18 2009 -0200

    KVM: split kvm_arch_set_memory_region into prepare and commit
    
    Required for SRCU convertion later.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 782bfb185f8a..3c44687b3425 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -253,7 +253,12 @@ int kvm_set_memory_region(struct kvm *kvm,
 int __kvm_set_memory_region(struct kvm *kvm,
 			    struct kvm_userspace_memory_region *mem,
 			    int user_alloc);
-int kvm_arch_set_memory_region(struct kvm *kvm,
+int kvm_arch_prepare_memory_region(struct kvm *kvm,
+				struct kvm_memory_slot *memslot,
+				struct kvm_memory_slot old,
+				struct kvm_userspace_memory_region *mem,
+				int user_alloc);
+void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
 				int user_alloc);

commit 46a26bf55714c1e2f17e34683292a389acb8e601
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:16 2009 -0200

    KVM: modify memslots layout in struct kvm
    
    Have a pointer to an allocated region inside struct kvm.
    
    [alex: fix ppc book 3s]
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bd5a616d9373..782bfb185f8a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -150,14 +150,18 @@ struct kvm_irq_routing_table {};
 
 #endif
 
+struct kvm_memslots {
+	int nmemslots;
+	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
+					KVM_PRIVATE_MEM_SLOTS];
+};
+
 struct kvm {
 	spinlock_t mmu_lock;
 	spinlock_t requests_lock;
 	struct rw_semaphore slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
-	int nmemslots;
-	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
-					KVM_PRIVATE_MEM_SLOTS];
+	struct kvm_memslots *memslots;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
 	u32 bsp_vcpu_id;
 	struct kvm_vcpu *bsp_vcpu;
@@ -482,7 +486,7 @@ static inline void kvm_guest_exit(void)
 
 static inline int memslot_id(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
-	return slot - kvm->memslots;
+	return slot - kvm->memslots->memslots;
 }
 
 static inline gpa_t gfn_to_gpa(gfn_t gfn)

commit d255f4f2bac81eb798fcf76938147f1f6c756ae2
Author: Zhai, Edwin <edwin.zhai@intel.com>
Date:   Fri Oct 9 18:03:20 2009 +0800

    KVM: introduce kvm_vcpu_on_spin
    
    Introduce kvm_vcpu_on_spin, to be used by VMX/SVM to yield processing
    once the cpu detects pause-based looping.
    
    Signed-off-by: "Zhai, Edwin" <edwin.zhai@intel.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b985a29d8175..bd5a616d9373 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -286,6 +286,7 @@ int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 void kvm_vcpu_block(struct kvm_vcpu *vcpu);
+void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu);
 void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);

commit 10474ae8945ce08622fd1f3464e55bd817bf2376
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Sep 15 11:37:46 2009 +0200

    KVM: Activate Virtualization On Demand
    
    X86 CPUs need to have some magic happening to enable the virtualization
    extensions on them. This magic can result in unpleasant results for
    users, like blocking other VMMs from working (vmx) or using invalid TLB
    entries (svm).
    
    Currently KVM activates virtualization when the respective kernel module
    is loaded. This blocks us from autoloading KVM modules without breaking
    other VMMs.
    
    To circumvent this problem at least a bit, this patch introduces on
    demand activation of virtualization. This means, that instead
    virtualization is enabled on creation of the first virtual machine
    and disabled on destruction of the last one.
    
    So using this, KVM can be easily autoloaded, while keeping other
    hypervisors usable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c0a1cc35f080..b985a29d8175 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -345,7 +345,7 @@ int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
 
 int kvm_arch_vcpu_reset(struct kvm_vcpu *vcpu);
-void kvm_arch_hardware_enable(void *garbage);
+int kvm_arch_hardware_enable(void *garbage);
 void kvm_arch_hardware_disable(void *garbage);
 int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);

commit bfd99ff5d483b11c32bca49fbff7a5ac59038b0a
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Aug 26 14:57:50 2009 +0300

    KVM: Move assigned device code to own file
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4aa5e1d9a797..c0a1cc35f080 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -577,4 +577,21 @@ static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 	return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
 }
 #endif
+
+#ifdef __KVM_HAVE_DEVICE_ASSIGNMENT
+
+long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
+				  unsigned long arg);
+
+#else
+
+static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
+						unsigned long arg)
+{
+	return -ENOTTY;
+}
+
 #endif
+
+#endif
+

commit 136bdfeee7b5bc986fc94af3a40d7d13ea37bb95
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:23 2009 +0300

    KVM: Move irq ack notifier list to arch independent code
    
    Mask irq notifier list is already there.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cc2d7493598b..4aa5e1d9a797 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -187,6 +187,7 @@ struct kvm {
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 	struct kvm_irq_routing_table *irq_routing;
 	struct hlist_head mask_notifier_list;
+	struct hlist_head irq_ack_notifier_list;
 #endif
 
 #ifdef KVM_ARCH_WANT_MMU_NOTIFIER

commit 3e71f88bc90792a187703860cf22fbed7c12cbd9
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:21 2009 +0300

    KVM: Maintain back mapping from irqchip/pin to gsi
    
    Maintain back mapping from irqchip/pin to gsi to speedup
    interrupt acknowledgment notifications.
    
    [avi: build fix on non-x86/ia64]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f403e66557fb..cc2d7493598b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -131,7 +131,10 @@ struct kvm_kernel_irq_routing_entry {
 	struct hlist_node link;
 };
 
+#ifdef __KVM_HAVE_IOAPIC
+
 struct kvm_irq_routing_table {
+	int chip[KVM_NR_IRQCHIPS][KVM_IOAPIC_NUM_PINS];
 	struct kvm_kernel_irq_routing_entry *rt_entries;
 	u32 nr_rt_entries;
 	/*
@@ -141,6 +144,12 @@ struct kvm_irq_routing_table {
 	struct hlist_head map[0];
 };
 
+#else
+
+struct kvm_irq_routing_table {};
+
+#endif
+
 struct kvm {
 	spinlock_t mmu_lock;
 	spinlock_t requests_lock;

commit 46e624b95c36d729bdf24010fff11d16f6fe94fa
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:20 2009 +0300

    KVM: Change irq routing table to use gsi indexed array
    
    Use gsi indexed array instead of scanning all entries on each interrupt
    injection.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1c7f8c49e4e8..f403e66557fb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -128,7 +128,17 @@ struct kvm_kernel_irq_routing_entry {
 		} irqchip;
 		struct msi_msg msi;
 	};
-	struct list_head link;
+	struct hlist_node link;
+};
+
+struct kvm_irq_routing_table {
+	struct kvm_kernel_irq_routing_entry *rt_entries;
+	u32 nr_rt_entries;
+	/*
+	 * Array indexed by gsi. Each entry contains list of irq chips
+	 * the gsi is connected to.
+	 */
+	struct hlist_head map[0];
 };
 
 struct kvm {
@@ -166,7 +176,7 @@ struct kvm {
 
 	struct mutex irq_lock;
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
-	struct list_head irq_routing; /* of kvm_kernel_irq_routing_entry */
+	struct kvm_irq_routing_table *irq_routing;
 	struct hlist_head mask_notifier_list;
 #endif
 
@@ -390,7 +400,12 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
-int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
+#ifdef __KVM_HAVE_IOAPIC
+void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
+				   union kvm_ioapic_redirect_entry *entry,
+				   unsigned long *deliver_bitmask);
+#endif
+int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);

commit 1a6e4a8c276e122dbeb6f9c610f29735e4236bfd
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Aug 24 11:54:19 2009 +0300

    KVM: Move irq sharing information to irqchip level
    
    This removes assumptions that max GSIs is smaller than number of pins.
    Sharing is tracked on pin level not GSI level.
    
    [avi: no PIC on ia64]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b7bbb5ddd7ae..1c7f8c49e4e8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -120,7 +120,7 @@ struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
 	u32 type;
 	int (*set)(struct kvm_kernel_irq_routing_entry *e,
-		    struct kvm *kvm, int level);
+		   struct kvm *kvm, int irq_source_id, int level);
 	union {
 		struct {
 			unsigned irqchip;

commit fc5377668c3d808e1d53c4aee152c836f55c3490
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 17 19:35:28 2009 +0200

    tracing: Remove markers
    
    Now that the last users of markers have migrated to the event
    tracer we can kill off the (now orphan) support code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20090917173527.GA1699@lst.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4af56036a6bf..b7bbb5ddd7ae 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -15,7 +15,6 @@
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/preempt.h>
-#include <linux/marker.h>
 #include <linux/msi.h>
 #include <asm/signal.h>
 

commit a1b37100d9e29c1f8dc3e2f5490a205c80180e01
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Jul 9 15:33:52 2009 +0300

    KVM: Reduce runnability interface with arch support code
    
    Remove kvm_cpu_has_interrupt() and kvm_arch_interrupt_allowed() from
    interface between general code and arch code. kvm_arch_vcpu_runnable()
    checks for interrupts instead.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2c6a5f008746..4af56036a6bf 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -332,7 +332,6 @@ int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
 void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
-int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 
 void kvm_free_physmem(struct kvm *kvm);
 
@@ -341,7 +340,6 @@ void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);
 
-int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 

commit 0b71785dc05f1f66e6268022b9953c0d6a9985c6
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Jul 9 15:33:53 2009 +0300

    KVM: Move kvm_cpu_get_interrupt() declaration to x86 code
    
    It is implemented only by x86.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6ec9fc56a49e..2c6a5f008746 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -341,7 +341,6 @@ void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);
 void kvm_arch_sync_events(struct kvm *kvm);
 
-int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);

commit d34e6b175e61821026893ec5298cc8e7558df43a
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Tue Jul 7 17:08:49 2009 -0400

    KVM: add ioeventfd support
    
    ioeventfd is a mechanism to register PIO/MMIO regions to trigger an eventfd
    signal when written to by a guest.  Host userspace can register any
    arbitrary IO address with a corresponding eventfd and then pass the eventfd
    to a specific end-point of interest for handling.
    
    Normal IO requires a blocking round-trip since the operation may cause
    side-effects in the emulated model or may return data to the caller.
    Therefore, an IO in KVM traps from the guest to the host, causes a VMX/SVM
    "heavy-weight" exit back to userspace, and is ultimately serviced by qemu's
    device model synchronously before returning control back to the vcpu.
    
    However, there is a subclass of IO which acts purely as a trigger for
    other IO (such as to kick off an out-of-band DMA request, etc).  For these
    patterns, the synchronous call is particularly expensive since we really
    only want to simply get our notification transmitted asychronously and
    return as quickly as possible.  All the sychronous infrastructure to ensure
    proper data-dependencies are met in the normal IO case are just unecessary
    overhead for signalling.  This adds additional computational load on the
    system, as well as latency to the signalling path.
    
    Therefore, we provide a mechanism for registration of an in-kernel trigger
    point that allows the VCPU to only require a very brief, lightweight
    exit just long enough to signal an eventfd.  This also means that any
    clients compatible with the eventfd interface (which includes userspace
    and kernelspace equally well) can now register to be notified. The end
    result should be a more flexible and higher performance notification API
    for the backend KVM hypervisor and perhipheral components.
    
    To test this theory, we built a test-harness called "doorbell".  This
    module has a function called "doorbell_ring()" which simply increments a
    counter for each time the doorbell is signaled.  It supports signalling
    from either an eventfd, or an ioctl().
    
    We then wired up two paths to the doorbell: One via QEMU via a registered
    io region and through the doorbell ioctl().  The other is direct via
    ioeventfd.
    
    You can download this test harness here:
    
    ftp://ftp.novell.com/dev/ghaskins/doorbell.tar.bz2
    
    The measured results are as follows:
    
    qemu-mmio:       110000 iops, 9.09us rtt
    ioeventfd-mmio: 200100 iops, 5.00us rtt
    ioeventfd-pio:  367300 iops, 2.72us rtt
    
    I didn't measure qemu-pio, because I have to figure out how to register a
    PIO region with qemu's device model, and I got lazy.  However, for now we
    can extrapolate based on the data from the NULLIO runs of +2.56us for MMIO,
    and -350ns for HC, we get:
    
    qemu-pio:      153139 iops, 6.53us rtt
    ioeventfd-hc: 412585 iops, 2.37us rtt
    
    these are just for fun, for now, until I can gather more data.
    
    Here is a graph for your convenience:
    
    http://developer.novell.com/wiki/images/7/76/Iofd-chart.png
    
    The conclusion to draw is that we save about 4us by skipping the userspace
    hop.
    
    --------------------
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 983b0bdeb3ff..6ec9fc56a49e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -155,6 +155,7 @@ struct kvm {
 		spinlock_t        lock;
 		struct list_head  items;
 	} irqfds;
+	struct list_head ioeventfds;
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
@@ -528,19 +529,24 @@ static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 
 #ifdef CONFIG_HAVE_KVM_EVENTFD
 
-void kvm_irqfd_init(struct kvm *kvm);
+void kvm_eventfd_init(struct kvm *kvm);
 int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags);
 void kvm_irqfd_release(struct kvm *kvm);
+int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);
 
 #else
 
-static inline void kvm_irqfd_init(struct kvm *kvm) {}
+static inline void kvm_eventfd_init(struct kvm *kvm) {}
 static inline int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags)
 {
 	return -EINVAL;
 }
 
 static inline void kvm_irqfd_release(struct kvm *kvm) {}
+static inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
+{
+	return -ENOSYS;
+}
 
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 

commit 090b7aff27120cdae76a346a70db394844fea598
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Tue Jul 7 17:08:44 2009 -0400

    KVM: make io_bus interface more robust
    
    Today kvm_io_bus_regsiter_dev() returns void and will internally BUG_ON
    if it fails.  We want to create dynamic MMIO/PIO entries driven from
    userspace later in the series, so we need to enhance the code to be more
    robust with the following changes:
    
       1) Add a return value to the registration function
       2) Fix up all the callsites to check the return code, handle any
          failures, and percolate the error up to the caller.
       3) Add an unregister function that collapses holes in the array
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 077e8bb875a9..983b0bdeb3ff 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -64,10 +64,14 @@ int kvm_io_bus_write(struct kvm_io_bus *bus, gpa_t addr, int len,
 		     const void *val);
 int kvm_io_bus_read(struct kvm_io_bus *bus, gpa_t addr, int len,
 		    void *val);
-void __kvm_io_bus_register_dev(struct kvm_io_bus *bus,
+int __kvm_io_bus_register_dev(struct kvm_io_bus *bus,
+			       struct kvm_io_device *dev);
+int kvm_io_bus_register_dev(struct kvm *kvm, struct kvm_io_bus *bus,
+			    struct kvm_io_device *dev);
+void __kvm_io_bus_unregister_dev(struct kvm_io_bus *bus,
+				 struct kvm_io_device *dev);
+void kvm_io_bus_unregister_dev(struct kvm *kvm, struct kvm_io_bus *bus,
 			       struct kvm_io_device *dev);
-void kvm_io_bus_register_dev(struct kvm *kvm, struct kvm_io_bus *bus,
-			     struct kvm_io_device *dev);
 
 struct kvm_vcpu {
 	struct kvm *kvm;

commit bda9020e2463ec94db9f97e8615f3bae22069838
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Mon Jun 29 22:24:32 2009 +0300

    KVM: remove in_range from io devices
    
    This changes bus accesses to use high-level kvm_io_bus_read/kvm_io_bus_write
    functions. in_range now becomes unused so it is removed from device ops in
    favor of read/write callbacks performing range checks internally.
    
    This allows aliasing (mostly for in-kernel virtio), as well as better error
    handling by making it possible to pass errors up to userspace.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 96c8c0b01929..077e8bb875a9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -60,8 +60,10 @@ struct kvm_io_bus {
 
 void kvm_io_bus_init(struct kvm_io_bus *bus);
 void kvm_io_bus_destroy(struct kvm_io_bus *bus);
-struct kvm_io_device *kvm_io_bus_find_dev(struct kvm_io_bus *bus,
-					  gpa_t addr, int len, int is_write);
+int kvm_io_bus_write(struct kvm_io_bus *bus, gpa_t addr, int len,
+		     const void *val);
+int kvm_io_bus_read(struct kvm_io_bus *bus, gpa_t addr, int len,
+		    void *val);
 void __kvm_io_bus_register_dev(struct kvm_io_bus *bus,
 			       struct kvm_io_device *dev);
 void kvm_io_bus_register_dev(struct kvm *kvm, struct kvm_io_bus *bus,

commit 6c474694530f377507f9aca438c17206e051e6e7
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Mon Jun 29 22:24:26 2009 +0300

    KVM: convert bus to slots_lock
    
    Use slots_lock to protect device list on the bus.  slots_lock is already
    taken for read everywhere, so we only need to take it for write when
    registering devices.  This is in preparation to removing in_range and
    kvm->lock around it.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4ea42c950539..96c8c0b01929 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -42,6 +42,7 @@
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 
+struct kvm;
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
@@ -61,7 +62,9 @@ void kvm_io_bus_init(struct kvm_io_bus *bus);
 void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 struct kvm_io_device *kvm_io_bus_find_dev(struct kvm_io_bus *bus,
 					  gpa_t addr, int len, int is_write);
-void kvm_io_bus_register_dev(struct kvm_io_bus *bus,
+void __kvm_io_bus_register_dev(struct kvm_io_bus *bus,
+			       struct kvm_io_device *dev);
+void kvm_io_bus_register_dev(struct kvm *kvm, struct kvm_io_bus *bus,
 			     struct kvm_io_device *dev);
 
 struct kvm_vcpu {

commit d3efc8efdbaa81e9db3089810fcd526fccba234d
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jun 17 10:07:59 2009 -0300

    KVM: use vcpu_id instead of bsp_vcpu pointer in kvm_vcpu_is_bsp
    
    Change kvm_vcpu_is_bsp to use vcpu_id instead of bsp_vcpu pointer, which
    is only initialized at the end of kvm_vm_ioctl_create_vcpu.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0604d56f6eed..4ea42c950539 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -538,7 +538,7 @@ static inline void kvm_irqfd_release(struct kvm *kvm) {}
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
 static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
-	return vcpu->kvm->bsp_vcpu == vcpu;
+	return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
 }
 #endif
 #endif

commit 2023a29cbe34139afcea8f65f8aef78c325c5dc0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jun 18 11:47:28 2009 -0300

    KVM: remove old KVMTRACE support code
    
    Return EOPNOTSUPP for KVM_TRACE_ENABLE/PAUSE/DISABLE ioctls.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 06af936a250a..0604d56f6eed 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -482,37 +482,6 @@ struct kvm_stats_debugfs_item {
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 
-#define KVMTRACE_5D(evt, vcpu, d1, d2, d3, d4, d5, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 5, d1, d2, d3, d4, d5)
-#define KVMTRACE_4D(evt, vcpu, d1, d2, d3, d4, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 4, d1, d2, d3, d4, 0)
-#define KVMTRACE_3D(evt, vcpu, d1, d2, d3, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 3, d1, d2, d3, 0, 0)
-#define KVMTRACE_2D(evt, vcpu, d1, d2, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 2, d1, d2, 0, 0, 0)
-#define KVMTRACE_1D(evt, vcpu, d1, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 1, d1, 0, 0, 0, 0)
-#define KVMTRACE_0D(evt, vcpu, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
-						vcpu, 0, 0, 0, 0, 0, 0)
-
-#ifdef CONFIG_KVM_TRACE
-int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg);
-void kvm_trace_cleanup(void);
-#else
-static inline
-int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg)
-{
-	return -EINVAL;
-}
-#define kvm_trace_cleanup() ((void)0)
-#endif
-
 #ifdef KVM_ARCH_WANT_MMU_NOTIFIER
 static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_seq)
 {

commit ec04b2604c3707a46db1d26d98f82b11d0844669
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Fri Jun 19 15:16:23 2009 +0200

    KVM: Prepare memslot data structures for multiple hugepage sizes
    
    [avi: fix build on non-x86]
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6988858dc56e..06af936a250a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -103,7 +103,7 @@ struct kvm_memory_slot {
 	struct {
 		unsigned long rmap_pde;
 		int write_count;
-	} *lpage_info;
+	} *lpage_info[KVM_NR_PAGE_SIZES - 1];
 	unsigned long userspace_addr;
 	int user_alloc;
 };

commit 54dee9933e8d93589ad63ec3d6be39f1921b0767
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jun 11 12:07:44 2009 -0300

    KVM: VMX: conditionally disable 2M pages
    
    Disable usage of 2M pages if VMX_EPT_2MB_PAGE_BIT (bit 16) is clear
    in MSR_IA32_VMX_EPT_VPID_CAP and EPT is enabled.
    
    [avi: s/largepages_disabled/largepages_enabled/ to avoid negative logic]
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c6e4d02067fe..6988858dc56e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -224,6 +224,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
 				int user_alloc);
+void kvm_disable_largepages(void);
 void kvm_arch_flush_shadow(struct kvm *kvm);
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);

commit 988a2cae6a3c0dea6df59808a935a9a697bfc28c
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Jun 9 15:56:29 2009 +0300

    KVM: Use macro to iterate over vcpus.
    
    [christian: remove unused variables on s390]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d3fdf1a738c9..c6e4d02067fe 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -179,6 +179,17 @@ struct kvm {
 #define kvm_printf(kvm, fmt ...) printk(KERN_DEBUG fmt)
 #define vcpu_printf(vcpu, fmt...) kvm_printf(vcpu->kvm, fmt)
 
+static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
+{
+	smp_rmb();
+	return kvm->vcpus[i];
+}
+
+#define kvm_for_each_vcpu(idx, vcpup, kvm) \
+	for (idx = 0, vcpup = kvm_get_vcpu(kvm, idx); \
+	     idx < atomic_read(&kvm->online_vcpus) && vcpup; \
+	     vcpup = kvm_get_vcpu(kvm, ++idx))
+
 int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
 void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 

commit 73880c80aa9c8dc353cd0ad26579023213cd5314
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Jun 9 15:56:28 2009 +0300

    KVM: Break dependency between vcpu index in vcpus array and vcpu_id.
    
    Archs are free to use vcpu_id as they see fit. For x86 it is used as
    vcpu's apic id. New ioctl is added to configure boot vcpu id that was
    assumed to be 0 till now.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a5bd429e9bd3..d3fdf1a738c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -131,8 +131,12 @@ struct kvm {
 	int nmemslots;
 	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
 					KVM_PRIVATE_MEM_SLOTS];
+#ifdef CONFIG_KVM_APIC_ARCHITECTURE
+	u32 bsp_vcpu_id;
 	struct kvm_vcpu *bsp_vcpu;
+#endif
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
+	atomic_t online_vcpus;
 	struct list_head vm_list;
 	struct mutex lock;
 	struct kvm_io_bus mmio_bus;
@@ -550,8 +554,10 @@ static inline void kvm_irqfd_release(struct kvm *kvm) {}
 
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
+#ifdef CONFIG_KVM_APIC_ARCHITECTURE
 static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
 	return vcpu->kvm->bsp_vcpu == vcpu;
 }
 #endif
+#endif

commit c5af89b68abb26eea5e745f33228f4d672f115e5
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Jun 9 15:56:26 2009 +0300

    KVM: Introduce kvm_vcpu_is_bsp() function.
    
    Use it instead of open code "vcpu_id zero is BSP" assumption.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a29ea030dd8e..a5bd429e9bd3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -131,6 +131,7 @@ struct kvm {
 	int nmemslots;
 	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
 					KVM_PRIVATE_MEM_SLOTS];
+	struct kvm_vcpu *bsp_vcpu;
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	struct list_head vm_list;
 	struct mutex lock;
@@ -549,4 +550,8 @@ static inline void kvm_irqfd_release(struct kvm *kvm) {}
 
 #endif /* CONFIG_HAVE_KVM_EVENTFD */
 
+static inline bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
+{
+	return vcpu->kvm->bsp_vcpu == vcpu;
+}
 #endif

commit fa40a8214bb9bcae8d49c234c19d8b4a6c1f37ff
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jun 4 15:08:24 2009 -0300

    KVM: switch irq injection/acking data structures to irq_lock
    
    Protect irq injection/acking data structures with a separate irq_lock
    mutex. This fixes the following deadlock:
    
    CPU A                               CPU B
    kvm_vm_ioctl_deassign_dev_irq()
      mutex_lock(&kvm->lock);            worker_thread()
      -> kvm_deassign_irq()                -> kvm_assigned_dev_interrupt_work_handler()
        -> deassign_host_irq()               mutex_lock(&kvm->lock);
          -> cancel_work_sync() [blocked]
    
    [gleb: fix ia64 path]
    
    Reported-by: Alex Williamson <alex.williamson@hp.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0c71688b1ee3..a29ea030dd8e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -371,7 +371,8 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
-void kvm_unregister_irq_ack_notifier(struct kvm_irq_ack_notifier *kian);
+void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
+				   struct kvm_irq_ack_notifier *kian);
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 

commit 60eead79ad8750f80384cbe48fc44edcc78a0305
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jun 4 15:08:23 2009 -0300

    KVM: introduce irq_lock, use it to protect ioapic
    
    Introduce irq_lock, and use to protect ioapic data structures.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 19240feefe6f..0c71688b1ee3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -124,7 +124,6 @@ struct kvm_kernel_irq_routing_entry {
 };
 
 struct kvm {
-	struct mutex lock; /* protects the vcpus array and APIC accesses */
 	spinlock_t mmu_lock;
 	spinlock_t requests_lock;
 	struct rw_semaphore slots_lock;
@@ -134,6 +133,7 @@ struct kvm {
 					KVM_PRIVATE_MEM_SLOTS];
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	struct list_head vm_list;
+	struct mutex lock;
 	struct kvm_io_bus mmio_bus;
 	struct kvm_io_bus pio_bus;
 #ifdef CONFIG_HAVE_KVM_EVENTFD
@@ -150,6 +150,7 @@ struct kvm {
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
 #endif
 
+	struct mutex irq_lock;
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 	struct list_head irq_routing; /* of kvm_kernel_irq_routing_entry */
 	struct hlist_head mask_notifier_list;

commit b188d2d365702cfc660a56d66f4bf77ebf14a5c2
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Fri May 29 12:58:50 2009 +0200

    KVM: remove redundant declarations
    
    Changing s390 code in kvm_arch_vcpu_load/put come across this header
    declarations. They are complete duplicates, not even useful forward
    declarations as nothing using it is in between (maybe it was that in
    the past).
    
    This patch removes the two dispensable lines.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7724dcb6ff76..19240feefe6f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -249,8 +249,6 @@ long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg);
-void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
-void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
 
 int kvm_dev_ioctl_check_extension(long ext);
 

commit 721eecbf4fe995ca94a9edec0c9843b1cc0eaaf3
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Wed May 20 10:30:49 2009 -0400

    KVM: irqfd
    
    KVM provides a complete virtual system environment for guests, including
    support for injecting interrupts modeled after the real exception/interrupt
    facilities present on the native platform (such as the IDT on x86).
    Virtual interrupts can come from a variety of sources (emulated devices,
    pass-through devices, etc) but all must be injected to the guest via
    the KVM infrastructure.  This patch adds a new mechanism to inject a specific
    interrupt to a guest using a decoupled eventfd mechnanism:  Any legal signal
    on the irqfd (using eventfd semantics from either userspace or kernel) will
    translate into an injected interrupt in the guest at the next available
    interrupt window.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3060bdc35ffe..7724dcb6ff76 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -136,6 +136,12 @@ struct kvm {
 	struct list_head vm_list;
 	struct kvm_io_bus mmio_bus;
 	struct kvm_io_bus pio_bus;
+#ifdef CONFIG_HAVE_KVM_EVENTFD
+	struct {
+		spinlock_t        lock;
+		struct list_head  items;
+	} irqfds;
+#endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
 	atomic_t users_count;
@@ -525,4 +531,22 @@ static inline void kvm_free_irq_routing(struct kvm *kvm) {}
 
 #endif
 
+#ifdef CONFIG_HAVE_KVM_EVENTFD
+
+void kvm_irqfd_init(struct kvm *kvm);
+int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags);
+void kvm_irqfd_release(struct kvm *kvm);
+
+#else
+
+static inline void kvm_irqfd_init(struct kvm *kvm) {}
+static inline int kvm_irqfd(struct kvm *kvm, int fd, int gsi, int flags)
+{
+	return -EINVAL;
+}
+
+static inline void kvm_irqfd_release(struct kvm *kvm) {}
+
+#endif /* CONFIG_HAVE_KVM_EVENTFD */
+
 #endif

commit 5116d8f6b977970ebefc1932c0f313163a6ec91f
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Jul 26 17:10:01 2009 +0300

    KVM: fix ack not being delivered when msi present
    
    kvm_notify_acked_irq does not check irq type, so that it sometimes
    interprets msi vector as irq.  As a result, ack notifiers are not
    called, which typially hangs the guest.  The fix is to track and
    check irq type.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 16713dc672e4..3060bdc35ffe 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -110,6 +110,7 @@ struct kvm_memory_slot {
 
 struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
+	u32 type;
 	int (*set)(struct kvm_kernel_irq_routing_entry *e,
 		    struct kvm *kvm, int level);
 	union {

commit 84261923d3dddb766736023bead6fa07b7e218d5
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jun 17 10:53:47 2009 -0300

    KVM: protect concurrent make_all_cpus_request
    
    make_all_cpus_request contains a race condition which can
    trigger false request completed status, as follows:
    
    CPU0                                              CPU1
    
    if (test_and_set_bit(req,&vcpu->requests))
       ....                                            if (test_and_set_bit(req,&vcpu->requests))
       ..                                                  return
    proceed to smp_call_function_many(wait=1)
    
    Use a spinlock to serialize concurrent CPUs.
    
    Cc: stable@kernel.org
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index aacc5449f586..16713dc672e4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -125,6 +125,7 @@ struct kvm_kernel_irq_routing_entry {
 struct kvm {
 	struct mutex lock; /* protects the vcpus array and APIC accesses */
 	spinlock_t mmu_lock;
+	spinlock_t requests_lock;
 	struct rw_semaphore slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	int nmemslots;

commit 547de29e5b1662deb05b5f90917902dc0e9ac182
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu May 7 17:55:13 2009 -0300

    KVM: protect assigned dev workqueue, int handler and irq acker
    
    kvm_assigned_dev_ack_irq is vulnerable to a race condition with the
    interrupt handler function. It does:
    
            if (dev->host_irq_disabled) {
                    enable_irq(dev->host_irq);
                    dev->host_irq_disabled = false;
            }
    
    If an interrupt triggers before the host->dev_irq_disabled assignment,
    it will disable the interrupt and set dev->host_irq_disabled to true.
    
    On return to kvm_assigned_dev_ack_irq, dev->host_irq_disabled is set to
    false, and the next kvm_assigned_dev_ack_irq call will fail to reenable
    it.
    
    Other than that, having the interrupt handler and work handlers run in
    parallel sounds like asking for trouble (could not spot any obvious
    problem, but better not have to, its fragile).
    
    CC: sheng.yang@intel.com
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 161816284192..aacc5449f586 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -345,6 +345,7 @@ struct kvm_assigned_dev_kernel {
 	int flags;
 	struct pci_dev *dev;
 	struct kvm *kvm;
+	spinlock_t assigned_dev_lock;
 };
 
 struct kvm_irq_mask_notifier {

commit 32f8840064d88cc3f6e85203aec7b6b57bebcb97
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu May 7 17:55:12 2009 -0300

    KVM: use smp_send_reschedule in kvm_vcpu_kick
    
    KVM uses a function call IPI to cause the exit of a guest running on a
    physical cpu. For virtual interrupt notification there is no need to
    wait on IPI receival, or to execute any function.
    
    This is exactly what the reschedule IPI does, without the overhead
    of function IPI. So use it instead of smp_call_function_single in
    kvm_vcpu_kick.
    
    Also change the "guest_mode" variable to a bit in vcpu->requests, and
    use that to collapse multiple IPI's that would be issued between the
    first one and zeroing of guest mode.
    
    This allows kvm_vcpu_kick to called with interrupts disabled.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bdce8e1303c9..161816284192 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -38,6 +38,7 @@
 #define KVM_REQ_UNHALT             6
 #define KVM_REQ_MMU_SYNC           7
 #define KVM_REQ_KVMCLOCK_UPDATE    8
+#define KVM_REQ_KICK               9
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 
@@ -72,7 +73,6 @@ struct kvm_vcpu {
 	struct mutex mutex;
 	int   cpu;
 	struct kvm_run *run;
-	int guest_mode;
 	unsigned long requests;
 	unsigned long guest_debug;
 	int fpu_active;

commit 522c68c4416de3cd3e11a9ff10d58e776a69ae1e
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Apr 27 20:35:43 2009 +0800

    KVM: Enable snooping control for supported hardware
    
    Memory aliases with different memory type is a problem for guest. For the guest
    without assigned device, the memory type of guest memory would always been the
    same as host(WB); but for the assigned device, some part of memory may be used
    as DMA and then set to uncacheable memory type(UC/WC), which would be a conflict of
    host memory type then be a potential issue.
    
    Snooping control can guarantee the cache correctness of memory go through the
    DMA engine of VT-d.
    
    [avi: fix build on ia64]
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 72d56844f388..bdce8e1303c9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -367,6 +367,9 @@ void kvm_unregister_irq_ack_notifier(struct kvm_irq_ack_notifier *kian);
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
+/* For vcpu->arch.iommu_flags */
+#define KVM_IOMMU_CACHE_COHERENCY	0x1
+
 #ifdef CONFIG_IOMMU_API
 int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,
 			unsigned long npages);

commit 78646121e9a2fcf7977cc15966420e572a450bc3
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Mar 23 12:12:11 2009 +0200

    KVM: Fix interrupt unhalting a vcpu when it shouldn't
    
    kvm_vcpu_block() unhalts vpu on an interrupt/timer without checking
    if interrupt window is actually opened.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 40e49ede8f91..72d56844f388 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -298,6 +298,7 @@ int kvm_arch_hardware_setup(void);
 void kvm_arch_hardware_unsetup(void);
 void kvm_arch_check_processor_compat(void *rtn);
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
+int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);
 
 void kvm_free_physmem(struct kvm *kvm);
 

commit e56d532f20c890a06bbe7cd479f4201e3a03cd73
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Thu Mar 12 21:45:39 2009 +0800

    KVM: Device assignment framework rework
    
    After discussion with Marcelo, we decided to rework device assignment framework
    together. The old problems are kernel logic is unnecessary complex. So Marcelo
    suggest to split it into a more elegant way:
    
    1. Split host IRQ assign and guest IRQ assign. And userspace determine the
    combination. Also discard msi2intx parameter, userspace can specific
    KVM_DEV_IRQ_HOST_MSI | KVM_DEV_IRQ_GUEST_INTX in assigned_irq->flags to
    enable MSI to INTx convertion.
    
    2. Split assign IRQ and deassign IRQ. Import two new ioctls:
    KVM_ASSIGN_DEV_IRQ and KVM_DEASSIGN_DEV_IRQ.
    
    This patch also fixed the reversed _IOR vs _IOW in definition(by deprecated the
    old interface).
    
    [avi: replace homemade bitcount() by hweight_long()]
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index fb60f31c4fb3..40e49ede8f91 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -339,11 +339,6 @@ struct kvm_assigned_dev_kernel {
 	struct msix_entry *host_msix_entries;
 	int guest_irq;
 	struct kvm_guest_msix_entry *guest_msix_entries;
-#define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)
-#define KVM_ASSIGNED_DEV_GUEST_MSI	(1 << 1)
-#define KVM_ASSIGNED_DEV_HOST_INTX	(1 << 8)
-#define KVM_ASSIGNED_DEV_HOST_MSI	(1 << 9)
-#define KVM_ASSIGNED_DEV_MSIX		((1 << 2) | (1 << 10))
 	unsigned long irq_requested_type;
 	int irq_source_id;
 	int flags;

commit 343f94fe4d16ec898da77720c03da9e09f8523d2
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 5 16:34:54 2009 +0200

    KVM: consolidate ioapic/ipi interrupt delivery logic
    
    Use kvm_apic_match_dest() in kvm_get_intr_delivery_bitmask() instead
    of duplicating the same code. Use kvm_get_intr_delivery_bitmask() in
    apic_send_ipi() to figure out ipi destination instead of reimplementing
    the logic.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ec9d078b1e8e..fb60f31c4fb3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -363,11 +363,6 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
-#ifdef __KVM_HAVE_IOAPIC
-void kvm_get_intr_delivery_bitmask(struct kvm *kvm,
-				   union kvm_ioapic_redirect_entry *entry,
-				   unsigned long *deliver_bitmask);
-#endif
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,

commit a53c17d21c46a752f5ac6695376481bc27865b04
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Mar 5 16:34:49 2009 +0200

    KVM: ioapic/msi interrupt delivery consolidation
    
    ioapic_deliver() and kvm_set_msi() have code duplication. Move
    the code into ioapic_deliver_entry() function and call it from
    both places.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3b91ec9982c2..ec9d078b1e8e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -364,7 +364,7 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
 #ifdef __KVM_HAVE_IOAPIC
-void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
+void kvm_get_intr_delivery_bitmask(struct kvm *kvm,
 				   union kvm_ioapic_redirect_entry *entry,
 				   unsigned long *deliver_bitmask);
 #endif

commit b95b51d580bff9376850eef29d34c3aa08c26db7
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Feb 26 13:55:33 2009 +0100

    KVM: declare ioapic functions only on affected hardware
    
    Since "KVM: Unify the delivery of IOAPIC and MSI interrupts"
    I get the following warnings:
    
      CC [M]  arch/s390/kvm/kvm-s390.o
    In file included from arch/s390/kvm/kvm-s390.c:22:
    include/linux/kvm_host.h:357: warning: 'struct kvm_ioapic' declared inside parameter list
    include/linux/kvm_host.h:357: warning: its scope is only this definition or declaration, which is probably not what you want
    
    This patch limits IOAPIC functions for architectures that have one.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3832243625d4..3b91ec9982c2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -363,9 +363,11 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
+#ifdef __KVM_HAVE_IOAPIC
 void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
 				   union kvm_ioapic_redirect_entry *entry,
 				   unsigned long *deliver_bitmask);
+#endif
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,

commit 2350bd1f62c8706c22b8e58c3bfff10806c0a31b
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Feb 25 17:22:27 2009 +0800

    KVM: Add MSI-X interrupt injection logic
    
    We have to handle more than one interrupt with one handler for MSI-X. Avi
    suggested to use a flag to indicate the pending. So here is it.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 432edc27e82b..3832243625d4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -319,6 +319,7 @@ struct kvm_irq_ack_notifier {
 	void (*irq_acked)(struct kvm_irq_ack_notifier *kian);
 };
 
+#define KVM_ASSIGNED_MSIX_PENDING		0x1
 struct kvm_guest_msix_entry {
 	u32 vector;
 	u16 entry;

commit c1e01514296e8a4a43ff0c88dcff635cb90feb5f
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Feb 25 17:22:26 2009 +0800

    KVM: Ioctls for init MSI-X entry
    
    Introduce KVM_SET_MSIX_NR and KVM_SET_MSIX_ENTRY two ioctls.
    
    This two ioctls are used by userspace to specific guest device MSI-X entry
    number and correlate MSI-X entry with GSI during the initialization stage.
    
    MSI-X should be well initialzed before enabling.
    
    Don't support change MSI-X entry number for now.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 1a2f98fbecea..432edc27e82b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -319,6 +319,12 @@ struct kvm_irq_ack_notifier {
 	void (*irq_acked)(struct kvm_irq_ack_notifier *kian);
 };
 
+struct kvm_guest_msix_entry {
+	u32 vector;
+	u16 entry;
+	u16 flags;
+};
+
 struct kvm_assigned_dev_kernel {
 	struct kvm_irq_ack_notifier ack_notifier;
 	struct work_struct interrupt_work;
@@ -326,13 +332,17 @@ struct kvm_assigned_dev_kernel {
 	int assigned_dev_id;
 	int host_busnr;
 	int host_devfn;
+	unsigned int entries_nr;
 	int host_irq;
 	bool host_irq_disabled;
+	struct msix_entry *host_msix_entries;
 	int guest_irq;
+	struct kvm_guest_msix_entry *guest_msix_entries;
 #define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)
 #define KVM_ASSIGNED_DEV_GUEST_MSI	(1 << 1)
 #define KVM_ASSIGNED_DEV_HOST_INTX	(1 << 8)
 #define KVM_ASSIGNED_DEV_HOST_MSI	(1 << 9)
+#define KVM_ASSIGNED_DEV_MSIX		((1 << 2) | (1 << 10))
 	unsigned long irq_requested_type;
 	int irq_source_id;
 	int flags;

commit 116191b69b608d0f1513e3abe71d6a46800f2bd6
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Feb 11 16:03:37 2009 +0800

    KVM: Unify the delivery of IOAPIC and MSI interrupts
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 894a56e365e8..1a2f98fbecea 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -352,6 +352,9 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
+void kvm_get_intr_delivery_bitmask(struct kvm_ioapic *ioapic,
+				   union kvm_ioapic_redirect_entry *entry,
+				   unsigned long *deliver_bitmask);
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,

commit 4925663a079c77d95d8685228ad6675fc5639c8e
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Feb 4 17:28:14 2009 +0200

    KVM: Report IRQ injection status to userspace.
    
    IRQ injection status is either -1 (if there was no CPU found
    that should except the interrupt because IRQ was masked or
    ioapic was misconfigured or ...) or >= 0 in that case the
    number indicates to how many CPUs interrupt was injected.
    If the value is 0 it means that the interrupt was coalesced
    and probably should be reinjected.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 18b4df8264cf..894a56e365e8 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -110,7 +110,7 @@ struct kvm_memory_slot {
 
 struct kvm_kernel_irq_routing_entry {
 	u32 gsi;
-	void (*set)(struct kvm_kernel_irq_routing_entry *e,
+	int (*set)(struct kvm_kernel_irq_routing_entry *e,
 		    struct kvm *kvm, int level);
 	union {
 		struct {
@@ -352,7 +352,7 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 				      struct kvm_irq_mask_notifier *kimn);
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
-void kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
+int kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);

commit c807660407a695f390034e402edfe544a1d2e40c
Author: Gerd Hoffmann <kraxel@redhat.com>
Date:   Wed Feb 4 17:52:04 2009 +0100

    KVM: Fix kvmclock on !constant_tsc boxes
    
    kvmclock currently falls apart on machines without constant tsc.
    This patch fixes it.  Changes:
    
      * keep tsc frequency in a per-cpu variable.
      * handle kvmclock update using a new request flag, thus checking
        whenever we need an update each time we enter guest context.
      * use a cpufreq notifier to track frequency changes and force
        kvmclock updates.
      * send ipis to kick cpu out of guest context if needed to make
        sure the guest doesn't see stale values.
    
    Signed-off-by: Gerd Hoffmann <kraxel@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 339eda3ca6ee..18b4df8264cf 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -37,6 +37,7 @@
 #define KVM_REQ_PENDING_TIMER      5
 #define KVM_REQ_UNHALT             6
 #define KVM_REQ_MMU_SYNC           7
+#define KVM_REQ_KVMCLOCK_UPDATE    8
 
 #define KVM_USERSPACE_IRQ_SOURCE_ID	0
 

commit 79950e1073150909619b7c0f9a39a2fea83a42d8
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Feb 10 13:57:06 2009 +0800

    KVM: Use irq routing API for MSI
    
    Merge MSI userspace interface with IRQ routing table. Notice the API have been
    changed, and using IRQ routing table would be the only interface kvm-userspace
    supported.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c03a0a9a8584..339eda3ca6ee 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -116,6 +116,7 @@ struct kvm_kernel_irq_routing_entry {
 			unsigned irqchip;
 			unsigned pin;
 		} irqchip;
+		struct msi_msg msi;
 	};
 	struct list_head link;
 };
@@ -327,7 +328,6 @@ struct kvm_assigned_dev_kernel {
 	int host_irq;
 	bool host_irq_disabled;
 	int guest_irq;
-	struct msi_msg guest_msi;
 #define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)
 #define KVM_ASSIGNED_DEV_GUEST_MSI	(1 << 1)
 #define KVM_ASSIGNED_DEV_HOST_INTX	(1 << 8)

commit 44882eed2ebe7f75f8cdae5671ab1d6e0fa40dbc
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 27 15:12:38 2009 -0200

    KVM: make irq ack notifications aware of routing table
    
    IRQ ack notifications assume an identity mapping between pin->gsi,
    which might not be the case with, for example, HPET.
    
    Translate before acking.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ce285e01bd57..c03a0a9a8584 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -352,7 +352,7 @@ void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
 void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
 
 void kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
-void kvm_notify_acked_irq(struct kvm *kvm, unsigned gsi);
+void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
 void kvm_unregister_irq_ack_notifier(struct kvm_irq_ack_notifier *kian);

commit 399ec807ddc38ecccf8c06dbde04531cbdc63e11
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Nov 19 13:58:46 2008 +0200

    KVM: Userspace controlled irq routing
    
    Currently KVM has a static routing from GSI numbers to interrupts (namely,
    0-15 are mapped 1:1 to both PIC and IOAPIC, and 16:23 are mapped 1:1 to
    the IOAPIC).  This is insufficient for several reasons:
    
    - HPET requires non 1:1 mapping for the timer interrupt
    - MSIs need a new method to assign interrupt numbers and dispatch them
    - ACPI APIC mode needs to be able to reassign the PCI LINK interrupts to the
      ioapics
    
    This patch implements an interrupt routing table (as a linked list, but this
    can be easily changed) and a userspace interface to replace the table.  The
    routing table is initialized according to the current hardwired mapping.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 99963f36a6db..ce285e01bd57 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -107,6 +107,19 @@ struct kvm_memory_slot {
 	int user_alloc;
 };
 
+struct kvm_kernel_irq_routing_entry {
+	u32 gsi;
+	void (*set)(struct kvm_kernel_irq_routing_entry *e,
+		    struct kvm *kvm, int level);
+	union {
+		struct {
+			unsigned irqchip;
+			unsigned pin;
+		} irqchip;
+	};
+	struct list_head link;
+};
+
 struct kvm {
 	struct mutex lock; /* protects the vcpus array and APIC accesses */
 	spinlock_t mmu_lock;
@@ -128,6 +141,7 @@ struct kvm {
 #endif
 
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
+	struct list_head irq_routing; /* of kvm_kernel_irq_routing_entry */
 	struct hlist_head mask_notifier_list;
 #endif
 
@@ -480,4 +494,21 @@ static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_se
 }
 #endif
 
+#ifdef CONFIG_HAVE_KVM_IRQCHIP
+
+#define KVM_MAX_IRQ_ROUTES 1024
+
+int kvm_setup_default_irq_routing(struct kvm *kvm);
+int kvm_set_irq_routing(struct kvm *kvm,
+			const struct kvm_irq_routing_entry *entries,
+			unsigned nr,
+			unsigned flags);
+void kvm_free_irq_routing(struct kvm *kvm);
+
+#else
+
+static inline void kvm_free_irq_routing(struct kvm *kvm) {}
+
+#endif
+
 #endif

commit 75858a84a6207f5e60196f6bbd18fde4250e5759
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Jan 4 17:10:50 2009 +0200

    KVM: Interrupt mask notifiers for ioapic
    
    Allow clients to request notifications when the guest masks or unmasks a
    particular irq line.  This complements irq ack notifications, as the guest
    will not ack an irq line that is masked.
    
    Currently implemented for the ioapic only.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3cf0ede3fd73..99963f36a6db 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -127,6 +127,10 @@ struct kvm {
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
 #endif
 
+#ifdef CONFIG_HAVE_KVM_IRQCHIP
+	struct hlist_head mask_notifier_list;
+#endif
+
 #ifdef KVM_ARCH_WANT_MMU_NOTIFIER
 	struct mmu_notifier mmu_notifier;
 	unsigned long mmu_notifier_seq;
@@ -320,6 +324,19 @@ struct kvm_assigned_dev_kernel {
 	struct pci_dev *dev;
 	struct kvm *kvm;
 };
+
+struct kvm_irq_mask_notifier {
+	void (*func)(struct kvm_irq_mask_notifier *kimn, bool masked);
+	int irq;
+	struct hlist_node link;
+};
+
+void kvm_register_irq_mask_notifier(struct kvm *kvm, int irq,
+				    struct kvm_irq_mask_notifier *kimn);
+void kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,
+				      struct kvm_irq_mask_notifier *kimn);
+void kvm_fire_mask_notifiers(struct kvm *kvm, int irq, bool mask);
+
 void kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned gsi);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,

commit 67346440e83d2a2f2e9801f370b6240317c7d9bd
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jan 6 10:03:01 2009 +0800

    KVM: Remove duplicated prototype of kvm_arch_destroy_vm
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e92212f970db..3cf0ede3fd73 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -237,7 +237,6 @@ int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 				   int user_alloc);
 long kvm_arch_vm_ioctl(struct file *filp,
 		       unsigned int ioctl, unsigned long arg);
-void kvm_arch_destroy_vm(struct kvm *kvm);
 
 int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);
 int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);

commit d0bfb940ecabf0b44fb1fd80d8d60594e569e5ec
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Dec 15 13:52:10 2008 +0100

    KVM: New guest debug interface
    
    This rips out the support for KVM_DEBUG_GUEST and introduces a new IOCTL
    instead: KVM_SET_GUEST_DEBUG. The IOCTL payload consists of a generic
    part, controlling the "main switch" and the single-step feature. The
    arch specific part adds an x86 interface for intercepting both types of
    debug exceptions separately and re-injecting them when the host was not
    interested. Moveover, the foundation for guest debugging via debug
    registers is layed.
    
    To signal breakpoint events properly back to userland, an arch-specific
    data block is now returned along KVM_EXIT_DEBUG. For x86, the arch block
    contains the PC, the debug exception, and relevant debug registers to
    tell debug events properly apart.
    
    The availability of this new interface is signaled by
    KVM_CAP_SET_GUEST_DEBUG. Empty stubs for not yet supported archs are
    provided.
    
    Note that both SVM and VTX are supported, but only the latter was tested
    yet. Based on the experience with all those VTX corner case, I would be
    fairly surprised if SVM will work out of the box.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bf6f703642fc..e92212f970db 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -73,7 +73,7 @@ struct kvm_vcpu {
 	struct kvm_run *run;
 	int guest_mode;
 	unsigned long requests;
-	struct kvm_guest_debug guest_debug;
+	unsigned long guest_debug;
 	int fpu_active;
 	int guest_fpu_loaded;
 	wait_queue_head_t wq;
@@ -255,8 +255,8 @@ int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state);
 int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state);
-int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
-				    struct kvm_debug_guest *dbg);
+int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
+					struct kvm_guest_debug *dbg);
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run);
 
 int kvm_arch_init(void *opaque);

commit ad8ba2cd44d4d39fb3fe55d5dcc565b19fc3a7fb
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jan 6 10:03:02 2009 +0800

    KVM: Add kvm_arch_sync_events to sync with asynchronize events
    
    kvm_arch_sync_events is introduced to quiet down all other events may happen
    contemporary with VM destroy process, like IRQ handler and work struct for
    assigned device.
    
    For kvm_arch_sync_events is called at the very beginning of kvm_destroy_vm(), so
    the state of KVM here is legal and can provide a environment to quiet down other
    events.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ec49d0be7f52..bf6f703642fc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -285,6 +285,7 @@ void kvm_free_physmem(struct kvm *kvm);
 struct  kvm *kvm_arch_create_vm(void);
 void kvm_arch_destroy_vm(struct kvm *kvm);
 void kvm_free_all_assigned_devices(struct kvm *kvm);
+void kvm_arch_sync_events(struct kvm *kvm);
 
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v);

commit 19de40a8472fa64693eab844911eec277d489f6c
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Wed Dec 3 14:43:34 2008 +0100

    KVM: change KVM to use IOMMU API
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index e62a4629e51c..ec49d0be7f52 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -328,7 +328,7 @@ void kvm_unregister_irq_ack_notifier(struct kvm_irq_ack_notifier *kian);
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
-#ifdef CONFIG_DMAR
+#ifdef CONFIG_IOMMU_API
 int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,
 			unsigned long npages);
 int kvm_iommu_map_guest(struct kvm *kvm);
@@ -337,7 +337,7 @@ int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev);
 int kvm_deassign_device(struct kvm *kvm,
 			struct kvm_assigned_dev_kernel *assigned_dev);
-#else /* CONFIG_DMAR */
+#else /* CONFIG_IOMMU_API */
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      gfn_t base_gfn,
 				      unsigned long npages)
@@ -366,7 +366,7 @@ static inline int kvm_deassign_device(struct kvm *kvm,
 {
 	return 0;
 }
-#endif /* CONFIG_DMAR */
+#endif /* CONFIG_IOMMU_API */
 
 static inline void kvm_guest_enter(void)
 {

commit b653574a7d14b663cc812cb20be6a114939ba186
Author: Weidong Han <weidong.han@intel.com>
Date:   Mon Dec 8 23:29:53 2008 +0800

    Deassign device in kvm_free_assgined_device
    
    In kvm_iommu_unmap_memslots(), assigned_dev_head is already empty.
    
    Signed-off-by: Weidong Han <weidong.han@intel.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ce5d1c17ce26..e62a4629e51c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -316,6 +316,7 @@ struct kvm_assigned_dev_kernel {
 #define KVM_ASSIGNED_DEV_HOST_MSI	(1 << 9)
 	unsigned long irq_requested_type;
 	int irq_source_id;
+	int flags;
 	struct pci_dev *dev;
 	struct kvm *kvm;
 };

commit 0a920356748df4fb06e86c21c23d2ed6d31d37ad
Author: Weidong Han <weidong.han@intel.com>
Date:   Tue Dec 2 21:24:23 2008 +0800

    KVM: support device deassignment
    
    Support device deassignment, it can be used in device hotplug.
    
    Signed-off-by: Weidong Han <weidong.han@intel.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c96739b4b7a3..ce5d1c17ce26 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -334,6 +334,8 @@ int kvm_iommu_map_guest(struct kvm *kvm);
 int kvm_iommu_unmap_guest(struct kvm *kvm);
 int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev);
+int kvm_deassign_device(struct kvm *kvm,
+			struct kvm_assigned_dev_kernel *assigned_dev);
 #else /* CONFIG_DMAR */
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      gfn_t base_gfn,
@@ -357,6 +359,12 @@ static inline int kvm_assign_device(struct kvm *kvm,
 {
 	return 0;
 }
+
+static inline int kvm_deassign_device(struct kvm *kvm,
+		struct kvm_assigned_dev_kernel *assigned_dev)
+{
+	return 0;
+}
 #endif /* CONFIG_DMAR */
 
 static inline void kvm_guest_enter(void)

commit 260782bcfdaaa7850f29d6bb2ec6603019168c57
Author: Weidong Han <weidong.han@intel.com>
Date:   Tue Dec 2 21:03:39 2008 +0800

    KVM: use the new intel iommu APIs
    
    intel iommu APIs are updated, use the new APIs.
    
    In addition, change kvm_iommu_map_guest() to just create the domain, let kvm_iommu_assign_device() assign device.
    
    Signed-off-by: Weidong Han <weidong.han@intel.com>
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index eafabd5c66b2..c96739b4b7a3 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -330,9 +330,10 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 #ifdef CONFIG_DMAR
 int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,
 			unsigned long npages);
-int kvm_iommu_map_guest(struct kvm *kvm,
-			struct kvm_assigned_dev_kernel *assigned_dev);
+int kvm_iommu_map_guest(struct kvm *kvm);
 int kvm_iommu_unmap_guest(struct kvm *kvm);
+int kvm_assign_device(struct kvm *kvm,
+		      struct kvm_assigned_dev_kernel *assigned_dev);
 #else /* CONFIG_DMAR */
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      gfn_t base_gfn,
@@ -341,9 +342,7 @@ static inline int kvm_iommu_map_pages(struct kvm *kvm,
 	return 0;
 }
 
-static inline int kvm_iommu_map_guest(struct kvm *kvm,
-				      struct kvm_assigned_dev_kernel
-				      *assigned_dev)
+static inline int kvm_iommu_map_guest(struct kvm *kvm)
 {
 	return -ENODEV;
 }
@@ -352,6 +351,12 @@ static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
 {
 	return 0;
 }
+
+static inline int kvm_assign_device(struct kvm *kvm,
+		struct kvm_assigned_dev_kernel *assigned_dev)
+{
+	return 0;
+}
 #endif /* CONFIG_DMAR */
 
 static inline void kvm_guest_enter(void)

commit defaf1587c5d7dff828f6f11c8941e5bcef00f50
Author: Mark McLoughlin <markmc@redhat.com>
Date:   Tue Dec 2 12:16:33 2008 +0000

    KVM: fix handling of ACK from shared guest IRQ
    
    If an assigned device shares a guest irq with an emulated
    device then we currently interpret an ack generated by the
    emulated device as originating from the assigned device
    leading to e.g. "Unbalanced enable for IRQ 4347" from the
    enable_irq() in kvm_assigned_dev_ack_irq().
    
    The fix is fairly simple - don't enable the physical device
    irq unless it was previously disabled.
    
    Of course, this can still lead to a situation where a
    non-assigned device ACK can cause the physical device irq to
    be reenabled before the device was serviced. However, being
    level sensitive, the interrupt will merely be regenerated.
    
    Signed-off-by: Mark McLoughlin <markmc@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8091a4d90ddf..eafabd5c66b2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -307,6 +307,7 @@ struct kvm_assigned_dev_kernel {
 	int host_busnr;
 	int host_devfn;
 	int host_irq;
+	bool host_irq_disabled;
 	int guest_irq;
 	struct msi_msg guest_msi;
 #define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)

commit 0937c48d075ddd59ae2c12a6fa8308b9c7a63753
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Nov 24 14:32:53 2008 +0800

    KVM: Add fields for MSI device assignment
    
    Prepared for kvm_arch_assigned_device_msi_dispatch().
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c3d4b96a08fa..8091a4d90ddf 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -16,6 +16,7 @@
 #include <linux/mm.h>
 #include <linux/preempt.h>
 #include <linux/marker.h>
+#include <linux/msi.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -307,8 +308,11 @@ struct kvm_assigned_dev_kernel {
 	int host_devfn;
 	int host_irq;
 	int guest_irq;
+	struct msi_msg guest_msi;
 #define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)
+#define KVM_ASSIGNED_DEV_GUEST_MSI	(1 << 1)
 #define KVM_ASSIGNED_DEV_HOST_INTX	(1 << 8)
+#define KVM_ASSIGNED_DEV_HOST_MSI	(1 << 9)
 	unsigned long irq_requested_type;
 	int irq_source_id;
 	struct pci_dev *dev;

commit 4f906c19ae29397409bedabf7bbe5cb42ad90332
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Nov 24 14:32:51 2008 +0800

    KVM: Replace irq_requested with more generic irq_requested_type
    
    Separate guest irq type and host irq type, for we can support guest using INTx
    with host using MSI (but not opposite combination).
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3a0fb77d1f6a..c3d4b96a08fa 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -307,7 +307,9 @@ struct kvm_assigned_dev_kernel {
 	int host_devfn;
 	int host_irq;
 	int guest_irq;
-	int irq_requested;
+#define KVM_ASSIGNED_DEV_GUEST_INTX	(1 << 0)
+#define KVM_ASSIGNED_DEV_HOST_INTX	(1 << 8)
+	unsigned long irq_requested_type;
 	int irq_source_id;
 	struct pci_dev *dev;
 	struct kvm *kvm;

commit e19e30effac03f5a005a8e42ed941a2a5dc62654
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Mon Oct 20 16:07:10 2008 +0800

    KVM: IRQ ACK notifier should be used with in-kernel irqchip
    
    Also remove unnecessary parameter of unregister irq ack notifier.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bb92be2153bc..3a0fb77d1f6a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -316,8 +316,7 @@ void kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned gsi);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
-void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
-				     struct kvm_irq_ack_notifier *kian);
+void kvm_unregister_irq_ack_notifier(struct kvm_irq_ack_notifier *kian);
 int kvm_request_irq_source_id(struct kvm *kvm);
 void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 

commit 5550af4df179e52753d3a43a788a113ad8cd95cd
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Wed Oct 15 20:15:06 2008 +0800

    KVM: Fix guest shared interrupt with in-kernel irqchip
    
    Every call of kvm_set_irq() should offer an irq_source_id, which is
    allocated by kvm_request_irq_source_id(). Based on irq_source_id, we
    identify the irq source and implement logical OR for shared level
    interrupts.
    
    The allocated irq_source_id can be freed by kvm_free_irq_source_id().
    
    Currently, we support at most sizeof(unsigned long) different irq sources.
    
    [Amit: - rebase to kvm.git HEAD
           - move definition of KVM_USERSPACE_IRQ_SOURCE_ID to common file
           - move kvm_request_irq_source_id to the update_irq ioctl]
    
    [Xiantao: - Add kvm/ia64 stuff and make it work for kvm/ia64 guests]
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Amit Shah <amit.shah@redhat.com>
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 3833c48fae3a..bb92be2153bc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -37,6 +37,8 @@
 #define KVM_REQ_UNHALT             6
 #define KVM_REQ_MMU_SYNC           7
 
+#define KVM_USERSPACE_IRQ_SOURCE_ID	0
+
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
@@ -306,15 +308,18 @@ struct kvm_assigned_dev_kernel {
 	int host_irq;
 	int guest_irq;
 	int irq_requested;
+	int irq_source_id;
 	struct pci_dev *dev;
 	struct kvm *kvm;
 };
-void kvm_set_irq(struct kvm *kvm, int irq, int level);
+void kvm_set_irq(struct kvm *kvm, int irq_source_id, int irq, int level);
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned gsi);
 void kvm_register_irq_ack_notifier(struct kvm *kvm,
 				   struct kvm_irq_ack_notifier *kian);
 void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
 				     struct kvm_irq_ack_notifier *kian);
+int kvm_request_irq_source_id(struct kvm *kvm);
+void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 
 #ifdef CONFIG_DMAR
 int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,

commit 3de42dc094ecd313dc7d551e007a134b52f8663d
Author: Xiantao Zhang <xiantao.zhang@intel.com>
Date:   Mon Oct 6 13:48:45 2008 +0800

    KVM: Separate irq ack notification out of arch/x86/kvm/irq.c
    
    Moving irq ack notification logic as common, and make
    it shared with ia64 side.
    
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b3b7598b4d94..3833c48fae3a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -309,6 +309,12 @@ struct kvm_assigned_dev_kernel {
 	struct pci_dev *dev;
 	struct kvm *kvm;
 };
+void kvm_set_irq(struct kvm *kvm, int irq, int level);
+void kvm_notify_acked_irq(struct kvm *kvm, unsigned gsi);
+void kvm_register_irq_ack_notifier(struct kvm *kvm,
+				   struct kvm_irq_ack_notifier *kian);
+void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
+				     struct kvm_irq_ack_notifier *kian);
 
 #ifdef CONFIG_DMAR
 int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,

commit c77fb9dc7a0383c86eabef30272a763a482403e1
Author: Xiantao Zhang <xiantao.zhang@intel.com>
Date:   Sat Sep 27 10:55:40 2008 +0800

    KVM: Change is_mmio_pfn to kvm_is_mmio_pfn, and make it common for all archs
    
    Add a kvm_ prefix to avoid polluting kernel's name space.
    
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 10c1146cd009..b3b7598b4d94 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -288,6 +288,8 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
+int kvm_is_mmio_pfn(pfn_t pfn);
+
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;
 	unsigned gsi;

commit 8a98f6648a2b0756d8f26d6c13332f5526355fec
Author: Xiantao Zhang <xiantao.zhang@intel.com>
Date:   Mon Oct 6 13:47:38 2008 +0800

    KVM: Move device assignment logic to common code
    
    To share with other archs, this patch moves device assignment
    logic to common parts.
    
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 73b7c52b9493..10c1146cd009 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -281,6 +281,7 @@ void kvm_free_physmem(struct kvm *kvm);
 
 struct  kvm *kvm_arch_create_vm(void);
 void kvm_arch_destroy_vm(struct kvm *kvm);
+void kvm_free_all_assigned_devices(struct kvm *kvm);
 
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v);

commit 4731d4c7a07769cf2926c327177b97bb8c68cafc
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Sep 23 13:18:39 2008 -0300

    KVM: MMU: out of sync shadow core
    
    Allow guest pagetables to go out of sync.  Instead of emulating write
    accesses to guest pagetables, or unshadowing them, we un-write-protect
    the page table and allow the guest to modify it at will.  We rely on
    invlpg executions to synchronize individual ptes, and will synchronize
    the entire pagetable on tlb flushes.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 6252802c3cc0..73b7c52b9493 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -35,6 +35,7 @@
 #define KVM_REQ_TRIPLE_FAULT       4
 #define KVM_REQ_PENDING_TIMER      5
 #define KVM_REQ_UNHALT             6
+#define KVM_REQ_MMU_SYNC           7
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;

commit 62c476c7c7f25a5b245b9902a935636e6316e58c
Author: Ben-Ami Yassour <benami@il.ibm.com>
Date:   Sun Sep 14 03:48:28 2008 +0300

    KVM: Device Assignment with VT-d
    
    Based on a patch by: Kay, Allen M <allen.m.kay@intel.com>
    
    This patch enables PCI device assignment based on VT-d support.
    When a device is assigned to the guest, the guest memory is pinned and
    the mapping is updated in the VT-d IOMMU.
    
    [Amit: Expose KVM_CAP_IOMMU so we can check if an IOMMU is present
    and also control enable/disable from userspace]
    
    Signed-off-by: Kay, Allen M <allen.m.kay@intel.com>
    Signed-off-by: Weidong Han <weidong.han@intel.com>
    Signed-off-by: Ben-Ami Yassour <benami@il.ibm.com>
    Signed-off-by: Amit Shah <amit.shah@qumranet.com>
    
    Acked-by: Mark Gross <mgross@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4b036430ea23..6252802c3cc0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -286,6 +286,53 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
+struct kvm_irq_ack_notifier {
+	struct hlist_node link;
+	unsigned gsi;
+	void (*irq_acked)(struct kvm_irq_ack_notifier *kian);
+};
+
+struct kvm_assigned_dev_kernel {
+	struct kvm_irq_ack_notifier ack_notifier;
+	struct work_struct interrupt_work;
+	struct list_head list;
+	int assigned_dev_id;
+	int host_busnr;
+	int host_devfn;
+	int host_irq;
+	int guest_irq;
+	int irq_requested;
+	struct pci_dev *dev;
+	struct kvm *kvm;
+};
+
+#ifdef CONFIG_DMAR
+int kvm_iommu_map_pages(struct kvm *kvm, gfn_t base_gfn,
+			unsigned long npages);
+int kvm_iommu_map_guest(struct kvm *kvm,
+			struct kvm_assigned_dev_kernel *assigned_dev);
+int kvm_iommu_unmap_guest(struct kvm *kvm);
+#else /* CONFIG_DMAR */
+static inline int kvm_iommu_map_pages(struct kvm *kvm,
+				      gfn_t base_gfn,
+				      unsigned long npages)
+{
+	return 0;
+}
+
+static inline int kvm_iommu_map_guest(struct kvm *kvm,
+				      struct kvm_assigned_dev_kernel
+				      *assigned_dev)
+{
+	return -ENODEV;
+}
+
+static inline int kvm_iommu_unmap_guest(struct kvm *kvm)
+{
+	return 0;
+}
+#endif /* CONFIG_DMAR */
+
 static inline void kvm_guest_enter(void)
 {
 	account_system_vtime(current);
@@ -308,6 +355,11 @@ static inline gpa_t gfn_to_gpa(gfn_t gfn)
 	return (gpa_t)gfn << PAGE_SHIFT;
 }
 
+static inline hpa_t pfn_to_hpa(pfn_t pfn)
+{
+	return (hpa_t)pfn << PAGE_SHIFT;
+}
+
 static inline void kvm_migrate_timers(struct kvm_vcpu *vcpu)
 {
 	set_bit(KVM_REQ_MIGRATE_TIMER, &vcpu->requests);

commit d76901750ab9f71091d33ef3d2b5909d8a9a4ad4
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Sep 8 15:23:48 2008 -0300

    KVM: x86: do not execute halted vcpus
    
    Offline or uninitialized vcpu's can be executed if requested to perform
    userspace work.
    
    Follow Avi's suggestion to handle halted vcpu's in the main loop,
    simplifying kvm_emulate_halt(). Introduce a new vcpu->requests bit to
    indicate events that promote state from halted to running.
    
    Also standardize vcpu wake sites.
    
    Signed-off-by: Marcelo Tosatti <mtosatti <at> redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a18aaad2ab79..4b036430ea23 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -34,6 +34,7 @@
 #define KVM_REQ_MMU_RELOAD         3
 #define KVM_REQ_TRIPLE_FAULT       4
 #define KVM_REQ_PENDING_TIMER      5
+#define KVM_REQ_UNHALT             6
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;

commit d98e6346350ac909f095768beb28b82368bd126f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Jul 1 16:23:49 2008 -0500

    KVM: Move KVM TRACE DEFINITIONS to common header
    
    Move KVM trace definitions from x86 specific kvm headers to common kvm
    headers to create a cross-architecture numbering scheme for trace
    events. This means the kvmtrace_format userspace tool won't need to know
    which architecture produced the log file being processed.
    
    Signed-off-by: Jerone Young <jyoung5@us.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8525afc53107..a18aaad2ab79 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -326,6 +326,25 @@ struct kvm_stats_debugfs_item {
 extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 
+#define KVMTRACE_5D(evt, vcpu, d1, d2, d3, d4, d5, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 5, d1, d2, d3, d4, d5)
+#define KVMTRACE_4D(evt, vcpu, d1, d2, d3, d4, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 4, d1, d2, d3, d4, 0)
+#define KVMTRACE_3D(evt, vcpu, d1, d2, d3, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 3, d1, d2, d3, 0, 0)
+#define KVMTRACE_2D(evt, vcpu, d1, d2, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 2, d1, d2, 0, 0, 0)
+#define KVMTRACE_1D(evt, vcpu, d1, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 1, d1, 0, 0, 0, 0)
+#define KVMTRACE_0D(evt, vcpu, name) \
+	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+						vcpu, 0, 0, 0, 0, 0, 0)
+
 #ifdef CONFIG_KVM_TRACE
 int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg);
 void kvm_trace_cleanup(void);

commit e930bffe95e1e886a1ede80726ea38df5838d067
Author: Andrea Arcangeli <andrea@qumranet.com>
Date:   Fri Jul 25 16:24:52 2008 +0200

    KVM: Synchronize guest physical memory map to host virtual memory map
    
    Synchronize changes to host virtual addresses which are part of
    a KVM memory slot to the KVM shadow mmu.  This allows pte operations
    like swapping, page migration, and madvise() to transparently work
    with KVM.
    
    Signed-off-by: Andrea Arcangeli <andrea@qumranet.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 07d68a8ae8e9..8525afc53107 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -121,6 +121,12 @@ struct kvm {
 	struct kvm_coalesced_mmio_dev *coalesced_mmio_dev;
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
 #endif
+
+#ifdef KVM_ARCH_WANT_MMU_NOTIFIER
+	struct mmu_notifier mmu_notifier;
+	unsigned long mmu_notifier_seq;
+	long mmu_notifier_count;
+#endif
 };
 
 /* The guest did something we don't support. */
@@ -332,4 +338,22 @@ int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg)
 #define kvm_trace_cleanup() ((void)0)
 #endif
 
+#ifdef KVM_ARCH_WANT_MMU_NOTIFIER
+static inline int mmu_notifier_retry(struct kvm_vcpu *vcpu, unsigned long mmu_seq)
+{
+	if (unlikely(vcpu->kvm->mmu_notifier_count))
+		return 1;
+	/*
+	 * Both reads happen under the mmu_lock and both values are
+	 * modified under mmu_lock, so there's no need of smb_rmb()
+	 * here in between, otherwise mmu_notifier_count should be
+	 * read before mmu_notifier_seq, see
+	 * mmu_notifier_invalidate_range_end write side.
+	 */
+	if (vcpu->kvm->mmu_notifier_seq != mmu_seq)
+		return 1;
+	return 0;
+}
+#endif
+
 #endif

commit 34d4cb8fca1f2a31be152b74797e6cd160ec9de6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jul 10 20:49:31 2008 -0300

    KVM: MMU: nuke shadowed pgtable pages and ptes on memslot destruction
    
    Flush the shadow mmu before removing regions to avoid stale entries.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d220b4926c4a..07d68a8ae8e9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -168,6 +168,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				struct kvm_memory_slot old,
 				int user_alloc);
+void kvm_arch_flush_shadow(struct kvm *kvm);
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);

commit 5f94c1741bdc7a336553122036e8a779e616ccbf
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Fri May 30 16:05:54 2008 +0200

    KVM: Add coalesced MMIO support (common part)
    
    This patch adds all needed structures to coalesce MMIOs.
    Until an architecture uses it, it is not compiled.
    
    Coalesced MMIO introduces two ioctl() to define where are the MMIO zones that
    can be coalesced:
    
    - KVM_REGISTER_COALESCED_MMIO registers a coalesced MMIO zone.
      It requests one parameter (struct kvm_coalesced_mmio_zone) which defines
      a memory area where MMIOs can be coalesced until the next switch to
      user space. The maximum number of MMIO zones is KVM_COALESCED_MMIO_ZONE_MAX.
    
    - KVM_UNREGISTER_COALESCED_MMIO cancels all registered zones inside
      the given bounds (bounds are also given by struct kvm_coalesced_mmio_zone).
    
    The userspace client can check kernel coalesced MMIO availability by asking
    ioctl(KVM_CHECK_EXTENSION) for the KVM_CAP_COALESCED_MMIO capability.
    The ioctl() call to KVM_CAP_COALESCED_MMIO will return 0 if not supported,
    or the page offset where will be stored the ring buffer.
    The page offset depends on the architecture.
    
    After an ioctl(KVM_RUN), the first page of the KVM memory mapped points to
    a kvm_run structure. The offset given by KVM_CAP_COALESCED_MMIO is
    an offset to the coalesced MMIO ring expressed in PAGE_SIZE relatively
    to the address of the start of th kvm_run structure. The MMIO ring buffer
    is defined by the structure kvm_coalesced_mmio_ring.
    
    [akio: fix oops during guest shutdown]
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Signed-off-by: Akio Takebe <takebe_akio@jp.fujitsu.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 499ff0604234..d220b4926c4a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -117,6 +117,10 @@ struct kvm {
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
 	atomic_t users_count;
+#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
+	struct kvm_coalesced_mmio_dev *coalesced_mmio_dev;
+	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
+#endif
 };
 
 /* The guest did something we don't support. */

commit 92760499d01ef91518119908eb9b8798b6c9bd3f
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Fri May 30 16:05:53 2008 +0200

    KVM: kvm_io_device: extend in_range() to manage len and write attribute
    
    Modify member in_range() of structure kvm_io_device to pass length and the type
    of the I/O (write or read).
    
    This modification allows to use kvm_io_device with coalesced MMIO.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 865dcbcb891f..499ff0604234 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -52,7 +52,8 @@ struct kvm_io_bus {
 
 void kvm_io_bus_init(struct kvm_io_bus *bus);
 void kvm_io_bus_destroy(struct kvm_io_bus *bus);
-struct kvm_io_device *kvm_io_bus_find_dev(struct kvm_io_bus *bus, gpa_t addr);
+struct kvm_io_device *kvm_io_bus_find_dev(struct kvm_io_bus *bus,
+					  gpa_t addr, int len, int is_write);
 void kvm_io_bus_register_dev(struct kvm_io_bus *bus,
 			     struct kvm_io_device *dev);
 

commit 7cc8883074b040aa8c1ebd3a17463b0ea3a9ef16
Author: Avi Kivity <avi@qumranet.com>
Date:   Tue May 13 16:29:20 2008 +0300

    KVM: Remove decache_vcpus_on_cpu() and related callbacks
    
    Obsoleted by the vmx-specific per-cpu list.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index de9d1df4bba2..865dcbcb891f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -135,9 +135,6 @@ void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
 void vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
-void decache_vcpus_on_cpu(int cpu);
-
-
 int kvm_init(void *opaque, unsigned int vcpu_size,
 		  struct module *module);
 void kvm_exit(void);

commit 06e05645661211b9eaadaf6344c335d2e80f0ba2
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Jun 6 16:37:36 2008 -0300

    KVM: close timer injection race window in __vcpu_run
    
    If a timer fires after kvm_inject_pending_timer_irqs() but before
    local_irq_disable() the code will enter guest mode and only inject such
    timer interrupt the next time an unrelated event causes an exit.
    
    It would be simpler if the timer->pending irq conversion could be done
    with IRQ's disabled, so that the above problem cannot happen.
    
    For now introduce a new vcpu requests bit to cancel guest entry.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 092b1b25291d..de9d1df4bba2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -33,6 +33,7 @@
 #define KVM_REQ_REPORT_TPR_ACCESS  2
 #define KVM_REQ_MMU_RELOAD         3
 #define KVM_REQ_TRIPLE_FAULT       4
+#define KVM_REQ_PENDING_TIMER      5
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;

commit 2f5997140f22f68f6390c49941150d3fa8a95cb7
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue May 27 12:10:20 2008 -0300

    KVM: migrate PIT timer
    
    Migrate the PIT timer to the physical CPU which vcpu0 is scheduled on,
    similarly to what is done for the LAPIC timers, otherwise PIT interrupts
    will be delayed until an unrelated event causes an exit.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 398978972b7a..092b1b25291d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -297,7 +297,7 @@ static inline gpa_t gfn_to_gpa(gfn_t gfn)
 	return (gpa_t)gfn << PAGE_SHIFT;
 }
 
-static inline void kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
+static inline void kvm_migrate_timers(struct kvm_vcpu *vcpu)
 {
 	set_bit(KVM_REQ_MIGRATE_TIMER, &vcpu->requests);
 }

commit 66c0b394f08fd89236515c1c84485ea712a157be
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Sat Apr 19 20:33:56 2008 +0100

    KVM: kill file->f_count abuse in kvm
    
    Use kvm own refcounting instead of playing with ->filp->f_count.
    That will allow to get rid of a lot of crap in anon_inode_getfd() and
    kill a race in kvm_dev_ioctl_create_vm() (file might have been closed
    immediately by another thread, so ->filp might point to already freed
    struct file when we get around to setting it).
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 4e16682ee8bb..398978972b7a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -110,7 +110,6 @@ struct kvm {
 					KVM_PRIVATE_MEM_SLOTS];
 	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
 	struct list_head vm_list;
-	struct file *filp;
 	struct kvm_io_bus mmio_bus;
 	struct kvm_io_bus pio_bus;
 	struct kvm_vm_stat stat;

commit 76f7c87902fd2c2de9eb57168adbf9bc5ec2047d
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Apr 15 16:05:42 2008 -0500

    KVM: Rename debugfs_dir to kvm_debugfs_dir
    
    It's a globally exported symbol now.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 81d4c3305a28..4e16682ee8bb 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -315,7 +315,7 @@ struct kvm_stats_debugfs_item {
 	struct dentry *dentry;
 };
 extern struct kvm_stats_debugfs_item debugfs_entries[];
-extern struct dentry *debugfs_dir;
+extern struct dentry *kvm_debugfs_dir;
 
 #ifdef CONFIG_KVM_TRACE
 int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg);

commit 62d9f0dbc92d7e398fde53fc6021338393522e68
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Apr 11 13:24:45 2008 -0300

    KVM: add ioctls to save/store mpstate
    
    So userspace can save/restore the mpstate during migration.
    
    [avi: export the #define constants describing the value]
    [christian: add s390 stubs]
    [avi: ditto for ia64]
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 0bc400387cae..81d4c3305a28 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -237,6 +237,10 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs);
 int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 				  struct kvm_sregs *sregs);
+int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
+				    struct kvm_mp_state *mp_state);
+int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
+				    struct kvm_mp_state *mp_state);
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
 				    struct kvm_debug_guest *dbg);
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run);

commit 3d80840d96127401ba6aeadd813c3a15b84e70fe
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Apr 11 14:53:26 2008 -0300

    KVM: hlt emulation should take in-kernel APIC/PIT timers into account
    
    Timers that fire between guest hlt and vcpu_block's add_wait_queue() are
    ignored, possibly resulting in hangs.
    
    Also make sure that atomic_inc and waitqueue_active tests happen in the
    specified order, otherwise the following race is open:
    
    CPU0                                        CPU1
                                                if (waitqueue_active(wq))
    add_wait_queue()
    if (!atomic_read(pit_timer->pending))
        schedule()
                                                atomic_inc(pit_timer->pending)
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index bd0c2d2d840f..0bc400387cae 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -269,6 +269,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm);
 
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
+int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
 static inline void kvm_guest_enter(void)

commit d4c9ff2d1b78e385471b3f4d80c0596909926ef7
Author: Feng(Eric) Liu <eric.e.liu@intel.com>
Date:   Thu Apr 10 08:47:53 2008 -0400

    KVM: Add kvm trace userspace interface
    
    This interface allows user a space application to read the trace of kvm
    related events through relayfs.
    
    Signed-off-by: Feng (Eric) Liu <eric.e.liu@intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 578c3638bbba..bd0c2d2d840f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/preempt.h>
+#include <linux/marker.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -309,5 +310,18 @@ struct kvm_stats_debugfs_item {
 	struct dentry *dentry;
 };
 extern struct kvm_stats_debugfs_item debugfs_entries[];
+extern struct dentry *debugfs_dir;
+
+#ifdef CONFIG_KVM_TRACE
+int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg);
+void kvm_trace_cleanup(void);
+#else
+static inline
+int kvm_trace_ioctl(unsigned int ioctl, unsigned long arg)
+{
+	return -EINVAL;
+}
+#define kvm_trace_cleanup() ((void)0)
+#endif
 
 #endif

commit 35149e2129fe34fc8cb5917e1ecf5156b0fa3415
Author: Anthony Liguori <aliguori@us.ibm.com>
Date:   Wed Apr 2 14:46:56 2008 -0500

    KVM: MMU: Don't assume struct page for x86
    
    This patch introduces a gfn_to_pfn() function and corresponding functions like
    kvm_release_pfn_dirty().  Using these new functions, we can modify the x86
    MMU to no longer assume that it can always get a struct page for any given gfn.
    
    We don't want to eliminate gfn_to_page() entirely because a number of places
    assume they can do gfn_to_page() and then kmap() the results.  When we support
    IO memory, gfn_to_page() will fail for IO pages although gfn_to_pfn() will
    succeed.
    
    This does not implement support for avoiding reference counting for reserved
    RAM or for IO memory.  However, it should make those things pretty straight
    forward.
    
    Since we're only introducing new common symbols, I don't think it will break
    the non-x86 architectures but I haven't tested those.  I've tested Intel,
    AMD, NPT, and hugetlbfs with Windows and Linux guests.
    
    [avi: fix overflow when shifting left pfns by adding casts]
    
    Signed-off-by: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a2ceb51b4274..578c3638bbba 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -150,8 +150,10 @@ static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
 struct page *gva_to_page(struct kvm_vcpu *vcpu, gva_t gva);
 
 extern struct page *bad_page;
+extern pfn_t bad_pfn;
 
 int is_error_page(struct page *page);
+int is_error_pfn(pfn_t pfn);
 int kvm_is_error_hva(unsigned long addr);
 int kvm_set_memory_region(struct kvm *kvm,
 			  struct kvm_userspace_memory_region *mem,
@@ -168,6 +170,16 @@ struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
+void kvm_set_page_dirty(struct page *page);
+void kvm_set_page_accessed(struct page *page);
+
+pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+void kvm_release_pfn_dirty(pfn_t);
+void kvm_release_pfn_clean(pfn_t pfn);
+void kvm_set_pfn_dirty(pfn_t pfn);
+void kvm_set_pfn_accessed(pfn_t pfn);
+void kvm_get_pfn(pfn_t pfn);
+
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
 int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,

commit d39f13b0da7fa7f705fbe6c80995205d0380bc7a
Author: Izik Eidus <izike@qumranet.com>
Date:   Sun Mar 30 16:01:25 2008 +0300

    KVM: add vm refcounting
    
    the main purpose of adding this functions is the abilaty to release the
    spinlock that protect the kvm list while still be able to do operations
    on a specific kvm in a safe way.
    
    Signed-off-by: Izik Eidus <izike@qumranet.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4e143621e35..a2ceb51b4274 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -114,6 +114,7 @@ struct kvm {
 	struct kvm_io_bus pio_bus;
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	atomic_t users_count;
 };
 
 /* The guest did something we don't support. */
@@ -140,6 +141,9 @@ int kvm_init(void *opaque, unsigned int vcpu_size,
 		  struct module *module);
 void kvm_exit(void);
 
+void kvm_get_kvm(struct kvm *kvm);
+void kvm_put_kvm(struct kvm *kvm);
+
 #define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
 #define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
 static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }

commit 69a9f69bb24d6d3dbf3d2ba542ddceeda40536d5
Author: Avi Kivity <avi@qumranet.com>
Date:   Fri Mar 21 12:38:23 2008 +0200

    KVM: Move some x86 specific constants and structures to include/asm-x86
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 958e00371516..f4e143621e35 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -24,13 +24,6 @@
 
 #include <asm/kvm_host.h>
 
-#define KVM_MAX_VCPUS 16
-#define KVM_MEMORY_SLOTS 32
-/* memory slots that does not exposed to userspace */
-#define KVM_PRIVATE_MEM_SLOTS 4
-
-#define KVM_PIO_PAGE_OFFSET 1
-
 /*
  * vcpu->requests bit members
  */
@@ -43,12 +36,6 @@
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
 
-struct kvm_guest_debug {
-	int enabled;
-	unsigned long bp[4];
-	int singlestep;
-};
-
 /*
  * It would be nice to use something smarter than a linear search, TBD...
  * Thankfully we dont expect many devices to register (famous last words :),

commit 71c4dfafc0932d92cc99c7e839d25174b0ce10a1
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Tue Feb 26 16:49:16 2008 +0100

    KVM: detect if VCPU triple faults
    
    In the current inject_page_fault path KVM only checks if there is another PF
    pending and injects a DF then. But it has to check for a pending DF too to
    detect a shutdown condition in the VCPU.  If this is not detected the VCPU goes
    to a PF -> DF -> PF loop when it should triple fault. This patch detects this
    condition and handles it with an KVM_SHUTDOWN exit to userspace.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9750bb3c5a75..958e00371516 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -38,6 +38,7 @@
 #define KVM_REQ_MIGRATE_TIMER      1
 #define KVM_REQ_REPORT_TPR_ACCESS  2
 #define KVM_REQ_MMU_RELOAD         3
+#define KVM_REQ_TRIPLE_FAULT       4
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;

commit 05da45583de9b383dc81dd695fe248431d6c9f2b
Author: Marcelo Tosatti <marcelo@kvack.org>
Date:   Sat Feb 23 11:44:30 2008 -0300

    KVM: MMU: large page support
    
    Create large pages mappings if the guest PTE's are marked as such and
    the underlying memory is hugetlbfs backed.  If the largepage contains
    write-protected pages, a large pte is not used.
    
    Gives a consistent 2% improvement for data copies on ram mounted
    filesystem, without NPT/EPT.
    
    Anthony measures a 4% improvement on 4-way kernbench, with NPT.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 994278fb5883..9750bb3c5a75 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -103,6 +103,10 @@ struct kvm_memory_slot {
 	unsigned long flags;
 	unsigned long *rmap;
 	unsigned long *dirty_bitmap;
+	struct {
+		unsigned long rmap_pde;
+		int write_count;
+	} *lpage_info;
 	unsigned long userspace_addr;
 	int user_alloc;
 };
@@ -169,6 +173,7 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 				int user_alloc);
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
+unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,

commit 2e53d63acba75795aa226febd140f67c58c6a353
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Feb 20 14:47:24 2008 -0500

    KVM: MMU: ignore zapped root pagetables
    
    Mark zapped root pagetables as invalid and ignore such pages during lookup.
    
    This is a problem with the cr3-target feature, where a zapped root table fools
    the faulting code into creating a read-only mapping. The result is a lockup
    if the instruction can't be emulated.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Anthony Liguori <aliguori@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index eb88d32dd5c7..994278fb5883 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -37,6 +37,7 @@
 #define KVM_REQ_TLB_FLUSH          0
 #define KVM_REQ_MIGRATE_TIMER      1
 #define KVM_REQ_REPORT_TPR_ACCESS  2
+#define KVM_REQ_MMU_RELOAD         3
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;
@@ -190,6 +191,7 @@ void kvm_resched(struct kvm_vcpu *vcpu);
 void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_flush_remote_tlbs(struct kvm *kvm);
+void kvm_reload_remote_mmus(struct kvm *kvm);
 
 long kvm_arch_dev_ioctl(struct file *filp,
 			unsigned int ioctl, unsigned long arg);

commit ef2979bd98dac86ea6a4cd9bdd6820a466108017
Author: Avi Kivity <avi@qumranet.com>
Date:   Wed Feb 20 12:04:47 2008 +0200

    KVM: Increase the number of user memory slots per vm
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4deb9992625..eb88d32dd5c7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -25,7 +25,7 @@
 #include <asm/kvm_host.h>
 
 #define KVM_MAX_VCPUS 16
-#define KVM_MEMORY_SLOTS 8
+#define KVM_MEMORY_SLOTS 32
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4
 

commit edbe6c325da48e707a3b31310c5ff5783cf6c0be
Author: Avi Kivity <avi@qumranet.com>
Date:   Wed Feb 20 11:56:51 2008 +0200

    KVM: Increase vcpu count to 16
    
    With NPT support, scalability is much improved.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index b90ca368dcf6..f4deb9992625 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -24,7 +24,7 @@
 
 #include <asm/kvm_host.h>
 
-#define KVM_MAX_VCPUS 4
+#define KVM_MAX_VCPUS 16
 #define KVM_MEMORY_SLOTS 8
 /* memory slots that does not exposed to userspace */
 #define KVM_PRIVATE_MEM_SLOTS 4

commit 31bb117eb48f2629e030ca547ca89a1c34150183
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Mon Jan 28 17:42:34 2008 -0600

    KVM: Use CONFIG_PREEMPT_NOTIFIERS around struct preempt_notifier
    
    This allows kvm_host.h to be #included even when struct preempt_notifier is
    undefined. This is needed to build ppc asm-offsets.h.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 928b0d59e9ba..b90ca368dcf6 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -67,7 +67,9 @@ void kvm_io_bus_register_dev(struct kvm_io_bus *bus,
 
 struct kvm_vcpu {
 	struct kvm *kvm;
+#ifdef CONFIG_PREEMPT_NOTIFIERS
 	struct preempt_notifier preempt_notifier;
+#endif
 	int vcpu_id;
 	struct mutex mutex;
 	int   cpu;

commit 72dc67a69690288538142df73a7e3ac66fea68dc
Author: Izik Eidus <izike@qumranet.com>
Date:   Sun Feb 10 18:04:15 2008 +0200

    KVM: remove the usage of the mmap_sem for the protection of the memory slots.
    
    This patch replaces the mmap_sem lock for the memory slots with a new
    kvm private lock, it is needed beacuse untill now there were cases where
    kvm accesses user memory while holding the mmap semaphore.
    
    Signed-off-by: Izik Eidus <izike@qumranet.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index ea4764b0a2f4..928b0d59e9ba 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -107,6 +107,7 @@ struct kvm_memory_slot {
 struct kvm {
 	struct mutex lock; /* protects the vcpus array and APIC accesses */
 	spinlock_t mmu_lock;
+	struct rw_semaphore slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	int nmemslots;
 	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +

commit 2f52d58c92d971bf421f461ad06eb93fb4f34981
Author: Avi Kivity <avi@qumranet.com>
Date:   Wed Jan 16 12:49:30 2008 +0200

    KVM: Move apic timer migration away from critical section
    
    Migrating the apic timer in the critical section is not very nice, and is
    absolutely horrible with the real-time port.  Move migration to the regular
    vcpu execution path, triggered by a new bitflag.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 2714068ee8bc..ea4764b0a2f4 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -35,6 +35,7 @@
  * vcpu->requests bit members
  */
 #define KVM_REQ_TLB_FLUSH          0
+#define KVM_REQ_MIGRATE_TIMER      1
 #define KVM_REQ_REPORT_TPR_ACCESS  2
 
 struct kvm_vcpu;
@@ -277,6 +278,11 @@ static inline gpa_t gfn_to_gpa(gfn_t gfn)
 	return (gpa_t)gfn << PAGE_SHIFT;
 }
 
+static inline void kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
+{
+	set_bit(KVM_REQ_MIGRATE_TIMER, &vcpu->requests);
+}
+
 enum kvm_stat_kind {
 	KVM_STAT_VM,
 	KVM_STAT_VCPU,

commit aaee2c94f7a1f7726e360a6cfb40173bd552bcff
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Dec 20 19:18:26 2007 -0500

    KVM: MMU: Switch to mmu spinlock
    
    Convert the synchronization of the shadow handling to a separate mmu_lock
    spinlock.
    
    Also guard fetch() by mmap_sem in read-mode to protect against alias
    and memslot changes.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a020fb280540..2714068ee8bc 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -104,7 +104,8 @@ struct kvm_memory_slot {
 };
 
 struct kvm {
-	struct mutex lock; /* protects everything except vcpus */
+	struct mutex lock; /* protects the vcpus array and APIC accesses */
+	spinlock_t mmu_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	int nmemslots;
 	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +

commit 7ec54588210df29ea637e6054489bc942c0ef371
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Dec 20 19:18:23 2007 -0500

    KVM: Add kvm_read_guest_atomic()
    
    In preparation for a mmu spinlock, add kvm_read_guest_atomic()
    and use it in fetch() and prefetch_page().
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9ff5904c5072..a020fb280540 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -167,6 +167,8 @@ void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
+int kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,
+			  unsigned long len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
 int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
 			 int offset, int len);

commit b209749f528488c4c0d20a42c0fbcbf49e6933b3
Author: Avi Kivity <avi@qumranet.com>
Date:   Mon Oct 22 16:50:39 2007 +0200

    KVM: local APIC TPR access reporting facility
    
    Add a facility to report on accesses to the local apic tpr even if the
    local apic is emulated in the kernel.  This is basically a hack that
    allows userspace to patch Windows which tends to bang on the tpr a lot.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 953b50aa0e61..9ff5904c5072 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -35,7 +35,7 @@
  * vcpu->requests bit members
  */
 #define KVM_REQ_TLB_FLUSH          0
-
+#define KVM_REQ_REPORT_TPR_ACCESS  2
 
 struct kvm_vcpu;
 extern struct kmem_cache *kvm_vcpu_cache;

commit 5736199afba8a8bb60a1ea282ab72857d6b16400
Author: Zhang Xiantao <xiantao.zhang@intel.com>
Date:   Mon Dec 17 14:21:40 2007 +0800

    KVM: Move kvm_vcpu_kick() to x86.c
    
    Moving kvm_vcpu_kick() to x86.c. Since it should be
    common for all archs, put its declarations in <linux/kvm_host.h>
    
    Signed-off-by: Zhang Xiantao <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index a85d5b6943de..953b50aa0e61 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -250,6 +250,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm);
 
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
+void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
 static inline void kvm_guest_enter(void)
 {

commit edf884172e9828c6234b254208af04655855038d
Author: Avi Kivity <avi@qumranet.com>
Date:   Sun Dec 16 11:02:48 2007 +0200

    KVM: Move arch dependent files to new directory arch/x86/kvm/
    
    This paves the way for multiple architecture support.  Note that while
    ioapic.c could potentially be shared with ia64, it is also moved.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
new file mode 100644
index 000000000000..a85d5b6943de
--- /dev/null
+++ b/include/linux/kvm_host.h
@@ -0,0 +1,289 @@
+#ifndef __KVM_HOST_H
+#define __KVM_HOST_H
+
+/*
+ * This work is licensed under the terms of the GNU GPL, version 2.  See
+ * the COPYING file in the top-level directory.
+ */
+
+#include <linux/types.h>
+#include <linux/hardirq.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/preempt.h>
+#include <asm/signal.h>
+
+#include <linux/kvm.h>
+#include <linux/kvm_para.h>
+
+#include <linux/kvm_types.h>
+
+#include <asm/kvm_host.h>
+
+#define KVM_MAX_VCPUS 4
+#define KVM_MEMORY_SLOTS 8
+/* memory slots that does not exposed to userspace */
+#define KVM_PRIVATE_MEM_SLOTS 4
+
+#define KVM_PIO_PAGE_OFFSET 1
+
+/*
+ * vcpu->requests bit members
+ */
+#define KVM_REQ_TLB_FLUSH          0
+
+
+struct kvm_vcpu;
+extern struct kmem_cache *kvm_vcpu_cache;
+
+struct kvm_guest_debug {
+	int enabled;
+	unsigned long bp[4];
+	int singlestep;
+};
+
+/*
+ * It would be nice to use something smarter than a linear search, TBD...
+ * Thankfully we dont expect many devices to register (famous last words :),
+ * so until then it will suffice.  At least its abstracted so we can change
+ * in one place.
+ */
+struct kvm_io_bus {
+	int                   dev_count;
+#define NR_IOBUS_DEVS 6
+	struct kvm_io_device *devs[NR_IOBUS_DEVS];
+};
+
+void kvm_io_bus_init(struct kvm_io_bus *bus);
+void kvm_io_bus_destroy(struct kvm_io_bus *bus);
+struct kvm_io_device *kvm_io_bus_find_dev(struct kvm_io_bus *bus, gpa_t addr);
+void kvm_io_bus_register_dev(struct kvm_io_bus *bus,
+			     struct kvm_io_device *dev);
+
+struct kvm_vcpu {
+	struct kvm *kvm;
+	struct preempt_notifier preempt_notifier;
+	int vcpu_id;
+	struct mutex mutex;
+	int   cpu;
+	struct kvm_run *run;
+	int guest_mode;
+	unsigned long requests;
+	struct kvm_guest_debug guest_debug;
+	int fpu_active;
+	int guest_fpu_loaded;
+	wait_queue_head_t wq;
+	int sigset_active;
+	sigset_t sigset;
+	struct kvm_vcpu_stat stat;
+
+#ifdef CONFIG_HAS_IOMEM
+	int mmio_needed;
+	int mmio_read_completed;
+	int mmio_is_write;
+	int mmio_size;
+	unsigned char mmio_data[8];
+	gpa_t mmio_phys_addr;
+#endif
+
+	struct kvm_vcpu_arch arch;
+};
+
+struct kvm_memory_slot {
+	gfn_t base_gfn;
+	unsigned long npages;
+	unsigned long flags;
+	unsigned long *rmap;
+	unsigned long *dirty_bitmap;
+	unsigned long userspace_addr;
+	int user_alloc;
+};
+
+struct kvm {
+	struct mutex lock; /* protects everything except vcpus */
+	struct mm_struct *mm; /* userspace tied to this vm */
+	int nmemslots;
+	struct kvm_memory_slot memslots[KVM_MEMORY_SLOTS +
+					KVM_PRIVATE_MEM_SLOTS];
+	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
+	struct list_head vm_list;
+	struct file *filp;
+	struct kvm_io_bus mmio_bus;
+	struct kvm_io_bus pio_bus;
+	struct kvm_vm_stat stat;
+	struct kvm_arch arch;
+};
+
+/* The guest did something we don't support. */
+#define pr_unimpl(vcpu, fmt, ...)					\
+ do {									\
+	if (printk_ratelimit())						\
+		printk(KERN_ERR "kvm: %i: cpu%i " fmt,			\
+		       current->tgid, (vcpu)->vcpu_id , ## __VA_ARGS__); \
+ } while (0)
+
+#define kvm_printf(kvm, fmt ...) printk(KERN_DEBUG fmt)
+#define vcpu_printf(vcpu, fmt...) kvm_printf(vcpu->kvm, fmt)
+
+int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id);
+void kvm_vcpu_uninit(struct kvm_vcpu *vcpu);
+
+void vcpu_load(struct kvm_vcpu *vcpu);
+void vcpu_put(struct kvm_vcpu *vcpu);
+
+void decache_vcpus_on_cpu(int cpu);
+
+
+int kvm_init(void *opaque, unsigned int vcpu_size,
+		  struct module *module);
+void kvm_exit(void);
+
+#define HPA_MSB ((sizeof(hpa_t) * 8) - 1)
+#define HPA_ERR_MASK ((hpa_t)1 << HPA_MSB)
+static inline int is_error_hpa(hpa_t hpa) { return hpa >> HPA_MSB; }
+struct page *gva_to_page(struct kvm_vcpu *vcpu, gva_t gva);
+
+extern struct page *bad_page;
+
+int is_error_page(struct page *page);
+int kvm_is_error_hva(unsigned long addr);
+int kvm_set_memory_region(struct kvm *kvm,
+			  struct kvm_userspace_memory_region *mem,
+			  int user_alloc);
+int __kvm_set_memory_region(struct kvm *kvm,
+			    struct kvm_userspace_memory_region *mem,
+			    int user_alloc);
+int kvm_arch_set_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem,
+				struct kvm_memory_slot old,
+				int user_alloc);
+gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn);
+struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
+void kvm_release_page_clean(struct page *page);
+void kvm_release_page_dirty(struct page *page);
+int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
+			int len);
+int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
+int kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,
+			 int offset, int len);
+int kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,
+		    unsigned long len);
+int kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len);
+int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
+struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
+int kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
+void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
+
+void kvm_vcpu_block(struct kvm_vcpu *vcpu);
+void kvm_resched(struct kvm_vcpu *vcpu);
+void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
+void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
+void kvm_flush_remote_tlbs(struct kvm *kvm);
+
+long kvm_arch_dev_ioctl(struct file *filp,
+			unsigned int ioctl, unsigned long arg);
+long kvm_arch_vcpu_ioctl(struct file *filp,
+			 unsigned int ioctl, unsigned long arg);
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
+
+int kvm_dev_ioctl_check_extension(long ext);
+
+int kvm_get_dirty_log(struct kvm *kvm,
+			struct kvm_dirty_log *log, int *is_dirty);
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
+				struct kvm_dirty_log *log);
+
+int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
+				   struct
+				   kvm_userspace_memory_region *mem,
+				   int user_alloc);
+long kvm_arch_vm_ioctl(struct file *filp,
+		       unsigned int ioctl, unsigned long arg);
+void kvm_arch_destroy_vm(struct kvm *kvm);
+
+int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);
+int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);
+
+int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,
+				    struct kvm_translation *tr);
+
+int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs);
+int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs);
+int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
+				  struct kvm_sregs *sregs);
+int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
+				  struct kvm_sregs *sregs);
+int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
+				    struct kvm_debug_guest *dbg);
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run);
+
+int kvm_arch_init(void *opaque);
+void kvm_arch_exit(void);
+
+int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
+
+void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);
+struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id);
+int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);
+
+int kvm_arch_vcpu_reset(struct kvm_vcpu *vcpu);
+void kvm_arch_hardware_enable(void *garbage);
+void kvm_arch_hardware_disable(void *garbage);
+int kvm_arch_hardware_setup(void);
+void kvm_arch_hardware_unsetup(void);
+void kvm_arch_check_processor_compat(void *rtn);
+int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);
+
+void kvm_free_physmem(struct kvm *kvm);
+
+struct  kvm *kvm_arch_create_vm(void);
+void kvm_arch_destroy_vm(struct kvm *kvm);
+
+int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
+int kvm_cpu_has_interrupt(struct kvm_vcpu *v);
+
+static inline void kvm_guest_enter(void)
+{
+	account_system_vtime(current);
+	current->flags |= PF_VCPU;
+}
+
+static inline void kvm_guest_exit(void)
+{
+	account_system_vtime(current);
+	current->flags &= ~PF_VCPU;
+}
+
+static inline int memslot_id(struct kvm *kvm, struct kvm_memory_slot *slot)
+{
+	return slot - kvm->memslots;
+}
+
+static inline gpa_t gfn_to_gpa(gfn_t gfn)
+{
+	return (gpa_t)gfn << PAGE_SHIFT;
+}
+
+enum kvm_stat_kind {
+	KVM_STAT_VM,
+	KVM_STAT_VCPU,
+};
+
+struct kvm_stats_debugfs_item {
+	const char *name;
+	int offset;
+	enum kvm_stat_kind kind;
+	struct dentry *dentry;
+};
+extern struct kvm_stats_debugfs_item debugfs_entries[];
+
+#endif
