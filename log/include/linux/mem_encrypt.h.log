commit ae7eb82a92fae4d255a0caa9f7c0f99e3babfec1
Author: Thiago Jung Bauermann <bauerman@linux.ibm.com>
Date:   Tue Aug 6 01:49:18 2019 -0300

    fs/core/vmcore: Move sev_active() reference to x86 arch code
    
    Secure Encrypted Virtualization is an x86-specific feature, so it shouldn't
    appear in generic kernel code because it forces non-x86 architectures to
    define the sev_active() function, which doesn't make a lot of sense.
    
    To solve this problem, add an x86 elfcorehdr_read() function to override
    the generic weak implementation. To do that, it's necessary to make
    read_from_oldmem() public so that it can be used outside of vmcore.c.
    
    Also, remove the export for sev_active() since it's only used in files that
    won't be built as modules.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Lianbo Jiang <lijiang@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190806044919.10622-6-bauerman@linux.ibm.com

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 0c5b0ff9eb29..5c4a18a91f89 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -19,7 +19,6 @@
 #else	/* !CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
 static inline bool mem_encrypt_active(void) { return false; }
-static inline bool sev_active(void) { return false; }
 
 #endif	/* CONFIG_ARCH_HAS_MEM_ENCRYPT */
 

commit 284e21fab2cfcf90dacce565e0b12f29e5df00c1
Author: Thiago Jung Bauermann <bauerman@linux.ibm.com>
Date:   Tue Aug 6 01:49:17 2019 -0300

    x86, s390/mm: Move sme_active() and sme_me_mask to x86-specific header
    
    Now that generic code doesn't reference them, move sme_active() and
    sme_me_mask to x86's <asm/mem_encrypt.h>.
    
    Also remove the export for sme_active() since it's only used in files that
    won't be built as modules. sme_me_mask on the other hand is used in
    arch/x86/kvm/svm.c (via __sme_set() and __psp_pa()) which can be built as a
    module so its export needs to stay.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190806044919.10622-5-bauerman@linux.ibm.com

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 470bd53a89df..0c5b0ff9eb29 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -18,23 +18,11 @@
 
 #else	/* !CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
-#define sme_me_mask	0ULL
-
-static inline bool sme_active(void) { return false; }
+static inline bool mem_encrypt_active(void) { return false; }
 static inline bool sev_active(void) { return false; }
 
 #endif	/* CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
-static inline bool mem_encrypt_active(void)
-{
-	return sme_me_mask;
-}
-
-static inline u64 sme_get_me_mask(void)
-{
-	return sme_me_mask;
-}
-
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 /*
  * The __sme_set() and __sme_clr() macros are useful for adding or removing

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index b310a9c18113..470bd53a89df 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -1,13 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * AMD Memory Encryption Support
  *
  * Copyright (C) 2016 Advanced Micro Devices, Inc.
  *
  * Author: Tom Lendacky <thomas.lendacky@amd.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #ifndef __MEM_ENCRYPT_H__

commit d8aa7eea78a1401cce39b3bb61ead0150044a3df
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Fri Oct 20 09:30:44 2017 -0500

    x86/mm: Add Secure Encrypted Virtualization (SEV) support
    
    Provide support for Secure Encrypted Virtualization (SEV). This initial
    support defines a flag that is used by the kernel to determine if it is
    running with SEV active.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: kvm@vger.kernel.org
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Link: https://lkml.kernel.org/r/20171020143059.3291-3-brijesh.singh@amd.com

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 265a9cd21cb4..b310a9c18113 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -23,11 +23,14 @@
 
 #define sme_me_mask	0ULL
 
+static inline bool sme_active(void) { return false; }
+static inline bool sev_active(void) { return false; }
+
 #endif	/* CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
-static inline bool sme_active(void)
+static inline bool mem_encrypt_active(void)
 {
-	return !!sme_me_mask;
+	return sme_me_mask;
 }
 
 static inline u64 sme_get_me_mask(void)

commit 21d9bb4a05bac50fb4f850517af4030baecd00f6
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Sep 7 11:38:37 2017 +0200

    x86/mm: Make the SME mask a u64
    
    The SME encryption mask is for masking 64-bit pagetable entries. It
    being an unsigned long works fine on X86_64 but on 32-bit builds in
    truncates bits leading to Xen guests crashing very early.
    
    And regardless, the whole SME mask handling shouldnt've leaked into
    32-bit because SME is X86_64-only feature. So, first make the mask u64.
    And then, add trivial 32-bit versions of the __sme_* macros so that
    nothing happens there.
    
    Reported-and-tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Tested-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Tom Lendacky <Thomas.Lendacky@amd.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas <Thomas.Lendacky@amd.com>
    Fixes: 21729f81ce8a ("x86/mm: Provide general kernel support for memory encryption")
    Link: http://lkml.kernel.org/r/20170907093837.76zojtkgebwtqc74@pd.tnic
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 1255f09f5e42..265a9cd21cb4 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -21,7 +21,7 @@
 
 #else	/* !CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
-#define sme_me_mask	0UL
+#define sme_me_mask	0ULL
 
 #endif	/* CONFIG_ARCH_HAS_MEM_ENCRYPT */
 
@@ -30,18 +30,23 @@ static inline bool sme_active(void)
 	return !!sme_me_mask;
 }
 
-static inline unsigned long sme_get_me_mask(void)
+static inline u64 sme_get_me_mask(void)
 {
 	return sme_me_mask;
 }
 
+#ifdef CONFIG_AMD_MEM_ENCRYPT
 /*
  * The __sme_set() and __sme_clr() macros are useful for adding or removing
  * the encryption mask from a value (e.g. when dealing with pagetable
  * entries).
  */
-#define __sme_set(x)		((unsigned long)(x) | sme_me_mask)
-#define __sme_clr(x)		((unsigned long)(x) & ~sme_me_mask)
+#define __sme_set(x)		((x) | sme_me_mask)
+#define __sme_clr(x)		((x) & ~sme_me_mask)
+#else
+#define __sme_set(x)		(x)
+#define __sme_clr(x)		(x)
+#endif
 
 #endif	/* __ASSEMBLY__ */
 

commit 21729f81ce8ae76a6995681d40e16f7ce8075db4
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:07 2017 -0500

    x86/mm: Provide general kernel support for memory encryption
    
    Changes to the existing page table macros will allow the SME support to
    be enabled in a simple fashion with minimal changes to files that use these
    macros.  Since the memory encryption mask will now be part of the regular
    pagetable macros, we introduce two new macros (_PAGE_TABLE_NOENC and
    _KERNPG_TABLE_NOENC) to allow for early pagetable creation/initialization
    without the encryption mask before SME becomes active.  Two new pgprot()
    macros are defined to allow setting or clearing the page encryption mask.
    
    The FIXMAP_PAGE_NOCACHE define is introduced for use with MMIO.  SME does
    not support encryption for MMIO areas so this define removes the encryption
    mask from the page attribute.
    
    Two new macros are introduced (__sme_pa() / __sme_pa_nodebug()) to allow
    creating a physical address with the encryption mask.  These are used when
    working with the cr3 register so that the PGD can be encrypted. The current
    __va() macro is updated so that the virtual address is generated based off
    of the physical address without the encryption mask thus allowing the same
    virtual address to be generated regardless of whether encryption is enabled
    for that physical location or not.
    
    Also, an early initialization function is added for SME.  If SME is active,
    this function:
    
     - Updates the early_pmd_flags so that early page faults create mappings
       with the encryption mask.
    
     - Updates the __supported_pte_mask to include the encryption mask.
    
     - Updates the protection_map entries to include the encryption mask so
       that user-space allocations will automatically have the encryption mask
       applied.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 570f4fcff13f..1255f09f5e42 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -35,6 +35,14 @@ static inline unsigned long sme_get_me_mask(void)
 	return sme_me_mask;
 }
 
+/*
+ * The __sme_set() and __sme_clr() macros are useful for adding or removing
+ * the encryption mask from a value (e.g. when dealing with pagetable
+ * entries).
+ */
+#define __sme_set(x)		((unsigned long)(x) | sme_me_mask)
+#define __sme_clr(x)		((unsigned long)(x) & ~sme_me_mask)
+
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* __MEM_ENCRYPT_H__ */

commit 5868f3651fa0dff96a57f94d49247d3ef320ebe2
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:05 2017 -0500

    x86/mm: Add support to enable SME in early boot processing
    
    Add support to the early boot code to use Secure Memory Encryption (SME).
    Since the kernel has been loaded into memory in a decrypted state, encrypt
    the kernel in place and update the early pagetables with the memory
    encryption mask so that new pagetable entries will use memory encryption.
    
    The routines to set the encryption mask and perform the encryption are
    stub routines for now with functionality to be added in a later patch.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/e52ad781f085224bf835b3caff9aa3aee6febccb.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
index 59769f7287e4..570f4fcff13f 100644
--- a/include/linux/mem_encrypt.h
+++ b/include/linux/mem_encrypt.h
@@ -30,6 +30,11 @@ static inline bool sme_active(void)
 	return !!sme_me_mask;
 }
 
+static inline unsigned long sme_get_me_mask(void)
+{
+	return sme_me_mask;
+}
+
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* __MEM_ENCRYPT_H__ */

commit 7744ccdbc16f0ac4adae21b3678af93775b3a386
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:03 2017 -0500

    x86/mm: Add Secure Memory Encryption (SME) support
    
    Add support for Secure Memory Encryption (SME). This initial support
    provides a Kconfig entry to build the SME support into the kernel and
    defines the memory encryption mask that will be used in subsequent
    patches to mark pages as encrypted.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/a6c34d16caaed3bc3e2d6f0987554275bd291554.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h
new file mode 100644
index 000000000000..59769f7287e4
--- /dev/null
+++ b/include/linux/mem_encrypt.h
@@ -0,0 +1,35 @@
+/*
+ * AMD Memory Encryption Support
+ *
+ * Copyright (C) 2016 Advanced Micro Devices, Inc.
+ *
+ * Author: Tom Lendacky <thomas.lendacky@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __MEM_ENCRYPT_H__
+#define __MEM_ENCRYPT_H__
+
+#ifndef __ASSEMBLY__
+
+#ifdef CONFIG_ARCH_HAS_MEM_ENCRYPT
+
+#include <asm/mem_encrypt.h>
+
+#else	/* !CONFIG_ARCH_HAS_MEM_ENCRYPT */
+
+#define sme_me_mask	0UL
+
+#endif	/* CONFIG_ARCH_HAS_MEM_ENCRYPT */
+
+static inline bool sme_active(void)
+{
+	return !!sme_me_mask;
+}
+
+#endif	/* __ASSEMBLY__ */
+
+#endif	/* __MEM_ENCRYPT_H__ */
