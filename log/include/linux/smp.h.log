commit 8c4890d1c3358fb8023d46e1e554c41d54f02878
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 22 12:01:25 2020 +0200

    smp, irq_work: Continue smp_call_function*() and irq_work*() integration
    
    Instead of relying on BUG_ON() to ensure the various data structures
    line up, use a bunch of horrible unions to make it all automatic.
    
    Much of the union magic is to ensure irq_work and smp_call_function do
    not (yet) see the members of their respective data structures change
    name.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20200622100825.844455025@infradead.org

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 7ee202ad21a6..80d557ef8a11 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -12,29 +12,22 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
-#include <linux/llist.h>
+#include <linux/smp_types.h>
 
 typedef void (*smp_call_func_t)(void *info);
 typedef bool (*smp_cond_func_t)(int cpu, void *info);
 
-enum {
-	CSD_FLAG_LOCK		= 0x01,
-
-	/* IRQ_WORK_flags */
-
-	CSD_TYPE_ASYNC		= 0x00,
-	CSD_TYPE_SYNC		= 0x10,
-	CSD_TYPE_IRQ_WORK	= 0x20,
-	CSD_TYPE_TTWU		= 0x30,
-	CSD_FLAG_TYPE_MASK	= 0xF0,
-};
-
 /*
  * structure shares (partial) layout with struct irq_work
  */
 struct __call_single_data {
-	struct llist_node llist;
-	unsigned int flags;
+	union {
+		struct __call_single_node node;
+		struct {
+			struct llist_node llist;
+			unsigned int flags;
+		};
+	};
 	smp_call_func_t func;
 	void *info;
 };

commit d479c5a1919b4e569dcd3ae9c84ed74a675d0b94
Merge: f6aee505c71b 25de110d1486
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 13:06:42 2020 -0700

    Merge tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The changes in this cycle are:
    
       - Optimize the task wakeup CPU selection logic, to improve
         scalability and reduce wakeup latency spikes
    
       - PELT enhancements
    
       - CFS bandwidth handling fixes
    
       - Optimize the wakeup path by remove rq->wake_list and replacing it
         with ->ttwu_pending
    
       - Optimize IPI cross-calls by making flush_smp_call_function_queue()
         process sync callbacks first.
    
       - Misc fixes and enhancements"
    
    * tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      irq_work: Define irq_work_single() on !CONFIG_IRQ_WORK too
      sched/headers: Split out open-coded prototypes into kernel/sched/smp.h
      sched: Replace rq::wake_list
      sched: Add rq::ttwu_pending
      irq_work, smp: Allow irq_work on call_single_queue
      smp: Optimize send_call_function_single_ipi()
      smp: Move irq_work_run() out of flush_smp_call_function_queue()
      smp: Optimize flush_smp_call_function_queue()
      sched: Fix smp_call_function_single_async() usage for ILB
      sched/core: Offload wakee task activation if it the wakee is descheduling
      sched/core: Optimize ttwu() spinning on p->on_cpu
      sched: Defend cfs and rt bandwidth quota against overflow
      sched/cpuacct: Fix charge cpuacct.usage_sys
      sched/fair: Replace zero-length array with flexible-array
      sched/pelt: Sync util/runnable_sum with PELT window when propagating
      sched/cpuacct: Use __this_cpu_add() instead of this_cpu_ptr()
      sched/fair: Optimize enqueue_task_fair()
      sched: Make scheduler_ipi inline
      sched: Clean up scheduler_ipi()
      sched/core: Simplify sched_init()
      ...

commit a148866489fbe243c936fe43e4525d8dbfa0318f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:04 2020 +0200

    sched: Replace rq::wake_list
    
    The recent commit: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    got smp_call_function_single_async() subtly wrong. Even though it will
    return -EBUSY when trying to re-use a csd, that condition is not
    atomic and still requires external serialization.
    
    The change in ttwu_queue_remote() got this wrong.
    
    While on first reading ttwu_queue_remote() has an atomic test-and-set
    that appears to serialize the use, the matching 'release' is not in
    the right place to actually guarantee this serialization.
    
    The actual race is vs the sched_ttwu_pending() call in the idle loop;
    that can run the wakeup-list without consuming the CSD.
    
    Instead of trying to chain the lists, merge them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.129371594@infradead.org

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 45ad6e30f398..84f90e24ed6f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -25,6 +25,7 @@ enum {
 	CSD_TYPE_ASYNC		= 0x00,
 	CSD_TYPE_SYNC		= 0x10,
 	CSD_TYPE_IRQ_WORK	= 0x20,
+	CSD_TYPE_TTWU		= 0x30,
 	CSD_FLAG_TYPE_MASK	= 0xF0,
 };
 

commit 4b44a21dd640b692d4e9b12d3e37c24825f90baa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:02 2020 +0200

    irq_work, smp: Allow irq_work on call_single_queue
    
    Currently irq_work_queue_on() will issue an unconditional
    arch_send_call_function_single_ipi() and has the handler do
    irq_work_run().
    
    This is unfortunate in that it makes the IPI handler look at a second
    cacheline and it misses the opportunity to avoid the IPI. Instead note
    that struct irq_work and struct __call_single_data are very similar in
    layout, so use a few bits in the flags word to encode a type and stick
    the irq_work on the call_single_queue list.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.011635912@infradead.org

diff --git a/include/linux/smp.h b/include/linux/smp.h
index cbc9162689d0..45ad6e30f398 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -16,17 +16,38 @@
 
 typedef void (*smp_call_func_t)(void *info);
 typedef bool (*smp_cond_func_t)(int cpu, void *info);
+
+enum {
+	CSD_FLAG_LOCK		= 0x01,
+
+	/* IRQ_WORK_flags */
+
+	CSD_TYPE_ASYNC		= 0x00,
+	CSD_TYPE_SYNC		= 0x10,
+	CSD_TYPE_IRQ_WORK	= 0x20,
+	CSD_FLAG_TYPE_MASK	= 0xF0,
+};
+
+/*
+ * structure shares (partial) layout with struct irq_work
+ */
 struct __call_single_data {
 	struct llist_node llist;
+	unsigned int flags;
 	smp_call_func_t func;
 	void *info;
-	unsigned int flags;
 };
 
 /* Use __aligned() to avoid to use 2 cache lines for 1 csd */
 typedef struct __call_single_data call_single_data_t
 	__aligned(sizeof(struct __call_single_data));
 
+/*
+ * Enqueue a llist_node on the call_single_queue; be very careful, read
+ * flush_smp_call_function_queue() in detail.
+ */
+extern void __smp_call_single_queue(int cpu, struct llist_node *node);
+
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 

commit 565558558985b1d7cd43b21f18c1ad6b232788d0
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Thu Apr 30 12:40:03 2020 +0100

    cpu/hotplug: Remove disable_nonboot_cpus()
    
    The single user could have called freeze_secondary_cpus() directly.
    
    Since this function was a source of confusion, remove it as it's
    just a pointless wrapper.
    
    While at it, rename enable_nonboot_cpus() to thaw_secondary_cpus() to
    preserve the naming symmetry.
    
    Done automatically via:
    
            git grep -l enable_nonboot_cpus | xargs sed -i 's/enable_nonboot_cpus/thaw_secondary_cpus/g'
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Link: https://lkml.kernel.org/r/20200430114004.17477-1-qais.yousef@arm.com

diff --git a/include/linux/smp.h b/include/linux/smp.h
index cbc9162689d0..04019872c7bc 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -227,8 +227,8 @@ static inline int get_boot_cpu_id(void)
  */
 extern void arch_disable_smp_support(void);
 
-extern void arch_enable_nonboot_cpus_begin(void);
-extern void arch_enable_nonboot_cpus_end(void);
+extern void arch_thaw_secondary_cpus_begin(void);
+extern void arch_thaw_secondary_cpus_end(void);
 
 void smp_setup_processor_id(void);
 

commit cb923159bbb8cc8fe09c19a3435ee11fd546f3d3
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jan 17 10:01:37 2020 +0100

    smp: Remove allocation mask from on_each_cpu_cond.*()
    
    The allocation mask is no longer used by on_each_cpu_cond() and
    on_each_cpu_cond_mask() and can be removed.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200117090137.1205765-4-bigeasy@linutronix.de

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 4734416855aa..cbc9162689d0 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -51,11 +51,10 @@ void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
  * processor.
  */
 void on_each_cpu_cond(smp_cond_func_t cond_func, smp_call_func_t func,
-		      void *info, bool wait, gfp_t gfp_flags);
+		      void *info, bool wait);
 
 void on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,
-			   void *info, bool wait, gfp_t gfp_flags,
-			   const struct cpumask *mask);
+			   void *info, bool wait, const struct cpumask *mask);
 
 int smp_call_function_single_async(int cpu, call_single_data_t *csd);
 

commit 5671d814dbd204b4ecc705045b5f1a647bff6f3b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jan 17 10:01:35 2020 +0100

    smp: Use smp_cond_func_t as type for the conditional function
    
    Use a typdef for the conditional function instead defining it each time in
    the function prototype.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200117090137.1205765-2-bigeasy@linutronix.de

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 6fc856c9eda5..4734416855aa 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -15,6 +15,7 @@
 #include <linux/llist.h>
 
 typedef void (*smp_call_func_t)(void *info);
+typedef bool (*smp_cond_func_t)(int cpu, void *info);
 struct __call_single_data {
 	struct llist_node llist;
 	smp_call_func_t func;
@@ -49,13 +50,12 @@ void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
  * cond_func returns a positive value. This may include the local
  * processor.
  */
-void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
-		smp_call_func_t func, void *info, bool wait,
-		gfp_t gfp_flags);
+void on_each_cpu_cond(smp_cond_func_t cond_func, smp_call_func_t func,
+		      void *info, bool wait, gfp_t gfp_flags);
 
-void on_each_cpu_cond_mask(bool (*cond_func)(int cpu, void *info),
-		smp_call_func_t func, void *info, bool wait,
-		gfp_t gfp_flags, const struct cpumask *mask);
+void on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,
+			   void *info, bool wait, gfp_t gfp_flags,
+			   const struct cpumask *mask);
 
 int smp_call_function_single_async(int cpu, call_single_data_t *csd);
 

commit e1928328699a582a540b105e5f4c160832a7fdcb
Merge: 46f1ec23a469 9156e545765e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:12:03 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - rwsem scalability improvements, phase #2, by Waiman Long, which are
         rather impressive:
    
           "On a 2-socket 40-core 80-thread Skylake system with 40 reader
            and writer locking threads, the min/mean/max locking operations
            done in a 5-second testing window before the patchset were:
    
             40 readers, Iterations Min/Mean/Max = 1,807/1,808/1,810
             40 writers, Iterations Min/Mean/Max = 1,807/50,344/151,255
    
            After the patchset, they became:
    
             40 readers, Iterations Min/Mean/Max = 30,057/31,359/32,741
             40 writers, Iterations Min/Mean/Max = 94,466/95,845/97,098"
    
         There's a lot of changes to the locking implementation that makes
         it similar to qrwlock, including owner handoff for more fair
         locking.
    
         Another microbenchmark shows how across the spectrum the
         improvements are:
    
           "With a locking microbenchmark running on 5.1 based kernel, the
            total locking rates (in kops/s) on a 2-socket Skylake system
            with equal numbers of readers and writers (mixed) before and
            after this patchset were:
    
            # of Threads   Before Patch      After Patch
            ------------   ------------      -----------
                 2            2,618             4,193
                 4            1,202             3,726
                 8              802             3,622
                16              729             3,359
                32              319             2,826
                64              102             2,744"
    
         The changes are extensive and the patch-set has been through
         several iterations addressing various locking workloads. There
         might be more regressions, but unless they are pathological I
         believe we want to use this new implementation as the baseline
         going forward.
    
       - jump-label optimizations by Daniel Bristot de Oliveira: the primary
         motivation was to remove IPI disturbance of isolated RT-workload
         CPUs, which resulted in the implementation of batched jump-label
         updates. Beyond the improvement of the real-time characteristics
         kernel, in one test this patchset improved static key update
         overhead from 57 msecs to just 1.4 msecs - which is a nice speedup
         as well.
    
       - atomic64_t cross-arch type cleanups by Mark Rutland: over the last
         ~10 years of atomic64_t existence the various types used by the
         APIs only had to be self-consistent within each architecture -
         which means they became wildly inconsistent across architectures.
         Mark puts and end to this by reworking all the atomic64
         implementations to use 's64' as the base type for atomic64_t, and
         to ensure that this type is consistently used for parameters and
         return values in the API, avoiding further problems in this area.
    
       - A large set of small improvements to lockdep by Yuyang Du: type
         cleanups, output cleanups, function return type and othr cleanups
         all around the place.
    
       - A set of percpu ops cleanups and fixes by Peter Zijlstra.
    
       - Misc other changes - please see the Git log for more details"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (82 commits)
      locking/lockdep: increase size of counters for lockdep statistics
      locking/atomics: Use sed(1) instead of non-standard head(1) option
      locking/lockdep: Move mark_lock() inside CONFIG_TRACE_IRQFLAGS && CONFIG_PROVE_LOCKING
      x86/jump_label: Make tp_vec_nr static
      x86/percpu: Optimize raw_cpu_xchg()
      x86/percpu, sched/fair: Avoid local_clock()
      x86/percpu, x86/irq: Relax {set,get}_irq_regs()
      x86/percpu: Relax smp_processor_id()
      x86/percpu: Differentiate this_cpu_{}() and __this_cpu_{}()
      locking/rwsem: Guard against making count negative
      locking/rwsem: Adaptive disabling of reader optimistic spinning
      locking/rwsem: Enable time-based spinning on reader-owned rwsem
      locking/rwsem: Make rwsem->owner an atomic_long_t
      locking/rwsem: Enable readers spinning on writer
      locking/rwsem: Clarify usage of owner's nonspinaable bit
      locking/rwsem: Wake up almost all readers in wait queue
      locking/rwsem: More optimal RT task handling of null owner
      locking/rwsem: Always release wait_lock before waking up tasks
      locking/rwsem: Implement lock handoff to prevent lock starvation
      locking/rwsem: Make rwsem_spin_on_owner() return owner state
      ...

commit caa759323c73676b3e48c8d9c86093c88b4aba97
Author: Nadav Amit <namit@vmware.com>
Date:   Wed Jun 12 23:48:05 2019 -0700

    smp: Remove smp_call_function() and on_each_cpu() return values
    
    The return value is fixed. Remove it and amend the callers.
    
    [ tglx: Fixup arm/bL_switcher and powerpc/rtas ]
    
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20190613064813.8102-2-namit@vmware.com

diff --git a/include/linux/smp.h b/include/linux/smp.h
index a56f08ff3097..bb8b451ab01f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -35,7 +35,7 @@ int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
 /*
  * Call a function on all processors
  */
-int on_each_cpu(smp_call_func_t func, void *info, int wait);
+void on_each_cpu(smp_call_func_t func, void *info, int wait);
 
 /*
  * Call a function on processors specified by mask, which might include
@@ -101,7 +101,7 @@ extern void smp_cpus_done(unsigned int max_cpus);
 /*
  * Call a function on all other processors
  */
-int smp_call_function(smp_call_func_t func, void *info, int wait);
+void smp_call_function(smp_call_func_t func, void *info, int wait);
 void smp_call_function_many(const struct cpumask *mask,
 			    smp_call_func_t func, void *info, bool wait);
 
@@ -144,9 +144,8 @@ static inline void smp_send_stop(void) { }
  *	These macros fold the SMP functionality into a single CPU system
  */
 #define raw_smp_processor_id()			0
-static inline int up_smp_call_function(smp_call_func_t func, void *info)
+static inline void up_smp_call_function(smp_call_func_t func, void *info)
 {
-	return 0;
 }
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))

commit 9ed7d75b2f09d836e71d597cd5879abb1a44e7a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 27 09:48:51 2019 +0100

    x86/percpu: Relax smp_processor_id()
    
    Nadav reported that since this_cpu_read() became asm-volatile, many
    smp_processor_id() users generated worse code due to the extra
    constraints.
    
    However since smp_processor_id() is reading a stable value, we can use
    __this_cpu_read().
    
    While this does reduce text size somewhat, this mostly results in code
    movement to .text.unlikely as a result of more/larger .cold.
    subfunctions. Less text on the hotpath is good for I$.
    
      $ ./compare.sh defconfig-build1 defconfig-build2 vmlinux.o
      setup_APIC_ibs                                             90         98   -12,+20
      force_ibs_eilvt_setup                                     400        413   -57,+70
      pci_serr_error                                            109        104   -54,+49
      pci_serr_error                                            109        104   -54,+49
      unknown_nmi_error                                         125        120   -76,+71
      unknown_nmi_error                                         125        120   -76,+71
      io_check_error                                            125        132   -97,+104
      intel_thermal_interrupt                                   730        822   +92,+0
      intel_init_thermal                                        951        945   -6,+0
      generic_get_mtrr                                          301        294   -7,+0
      generic_get_mtrr                                          301        294   -7,+0
      generic_set_all                                           749        754   -44,+49
      get_fixed_ranges                                          352        360   -41,+49
      x86_acpi_suspend_lowlevel                                 369        363   -6,+0
      check_tsc_sync_source                                     412        412   -71,+71
      irq_migrate_all_off_this_cpu                              662        674   -14,+26
      clocksource_watchdog                                      748        748   -113,+113
      __perf_event_account_interrupt                            204        197   -7,+0
      attempt_merge                                            1748       1741   -7,+0
      intel_guc_send_ct                                        1424       1409   -15,+0
      __fini_doorbell                                           235        231   -4,+0
      bdw_set_cdclk                                             928        923   -5,+0
      gen11_dsi_disable                                        1571       1556   -15,+0
      gmbus_wait                                                493        488   -5,+0
      md_make_request                                           376        369   -7,+0
      __split_and_process_bio                                   543        536   -7,+0
      delay_tsc                                                  96         89   -7,+0
      hsw_disable_pc8                                           696        691   -5,+0
      tsc_verify_tsc_adjust                                     215        228   -22,+35
      cpuidle_driver_unref                                       56         49   -7,+0
      blk_account_io_completion                                 159        148   -11,+0
      mtrr_wrmsr                                                 95         99   -29,+33
      __intel_wait_for_register_fw                              401        419   +18,+0
      cpuidle_driver_ref                                         43         36   -7,+0
      cpuidle_get_driver                                         15          8   -7,+0
      blk_account_io_done                                       535        528   -7,+0
      irq_migrate_all_off_this_cpu                              662        674   -14,+26
      check_tsc_sync_source                                     412        412   -71,+71
      irq_wait_for_poll                                         170        163   -7,+0
      generic_end_io_acct                                       329        322   -7,+0
      x86_acpi_suspend_lowlevel                                 369        363   -6,+0
      nohz_balance_enter_idle                                   198        191   -7,+0
      generic_start_io_acct                                     254        247   -7,+0
      blk_account_io_start                                      341        334   -7,+0
      perf_event_task_tick                                      682        675   -7,+0
      intel_init_thermal                                        951        945   -6,+0
      amd_e400_c1e_apic_setup                                    47         51   -28,+32
      setup_APIC_eilvt                                          350        328   -22,+0
      hsw_enable_pc8                                           1611       1605   -6,+0
                                                   total   12985947   12985892   -994,+939
    
    Reported-by: Nadav Amit <nadav.amit@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index a56f08ff3097..aa9e5e82d8c3 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -181,29 +181,46 @@ static inline int get_boot_cpu_id(void)
 
 #endif /* !SMP */
 
-/*
- * smp_processor_id(): get the current CPU ID.
+/**
+ * raw_processor_id() - get the current (unstable) CPU id
+ *
+ * For then you know what you are doing and need an unstable
+ * CPU id.
+ */
+
+/**
+ * smp_processor_id() - get the current (stable) CPU id
+ *
+ * This is the normal accessor to the CPU id and should be used
+ * whenever possible.
+ *
+ * The CPU id is stable when:
  *
- * if DEBUG_PREEMPT is enabled then we check whether it is
- * used in a preemption-safe way. (smp_processor_id() is safe
- * if it's used in a preemption-off critical section, or in
- * a thread that is bound to the current CPU.)
+ *  - IRQs are disabled;
+ *  - preemption is disabled;
+ *  - the task is CPU affine.
  *
- * NOTE: raw_smp_processor_id() is for internal use only
- * (smp_processor_id() is the preferred variant), but in rare
- * instances it might also be used to turn off false positives
- * (i.e. smp_processor_id() use that the debugging code reports but
- * which use for some reason is legal). Don't use this to hack around
- * the warning message, as your code might not work under PREEMPT.
+ * When CONFIG_DEBUG_PREEMPT; we verify these assumption and WARN
+ * when smp_processor_id() is used when the CPU id is not stable.
  */
+
+/*
+ * Allow the architecture to differentiate between a stable and unstable read.
+ * For example, x86 uses an IRQ-safe asm-volatile read for the unstable but a
+ * regular asm read for the stable.
+ */
+#ifndef __smp_processor_id
+#define __smp_processor_id(x) raw_smp_processor_id(x)
+#endif
+
 #ifdef CONFIG_DEBUG_PREEMPT
   extern unsigned int debug_smp_processor_id(void);
 # define smp_processor_id() debug_smp_processor_id()
 #else
-# define smp_processor_id() raw_smp_processor_id()
+# define smp_processor_id() __smp_processor_id()
 #endif
 
-#define get_cpu()		({ preempt_disable(); smp_processor_id(); })
+#define get_cpu()		({ preempt_disable(); __smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
 /*

commit 7d49b28a80b830c3ca876d33bedc58d62a78e16f
Author: Rik van Riel <riel@surriel.com>
Date:   Tue Sep 25 23:58:41 2018 -0400

    smp,cpumask: introduce on_each_cpu_cond_mask
    
    Introduce a variant of on_each_cpu_cond that iterates only over the
    CPUs in a cpumask, in order to avoid making callbacks for every single
    CPU in the system when we only need to test a subset.
    
    Cc: npiggin@gmail.com
    Cc: mingo@kernel.org
    Cc: will.deacon@arm.com
    Cc: songliubraving@fb.com
    Cc: kernel-team@fb.com
    Cc: hpa@zytor.com
    Cc: luto@kernel.org
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180926035844.1420-5-riel@surriel.com

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9fb239e12b82..a56f08ff3097 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -53,6 +53,10 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
+void on_each_cpu_cond_mask(bool (*cond_func)(int cpu, void *info),
+		smp_call_func_t func, void *info, bool wait,
+		gfp_t gfp_flags, const struct cpumask *mask);
+
 int smp_call_function_single_async(int cpu, call_single_data_t *csd);
 
 #ifdef CONFIG_SMP

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 98b1fe027fc9..9fb239e12b82 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __LINUX_SMP_H
 #define __LINUX_SMP_H
 

commit 966a967116e699762dbf4af7f9e0d1955c25aa37
Author: Ying Huang <ying.huang@intel.com>
Date:   Tue Aug 8 12:30:00 2017 +0800

    smp: Avoid using two cache lines for struct call_single_data
    
    struct call_single_data is used in IPIs to transfer information between
    CPUs.  Its size is bigger than sizeof(unsigned long) and less than
    cache line size.  Currently it is not allocated with any explicit alignment
    requirements.  This makes it possible for allocated call_single_data to
    cross two cache lines, which results in double the number of the cache lines
    that need to be transferred among CPUs.
    
    This can be fixed by requiring call_single_data to be aligned with the
    size of call_single_data. Currently the size of call_single_data is the
    power of 2.  If we add new fields to call_single_data, we may need to
    add padding to make sure the size of new definition is the power of 2
    as well.
    
    Fortunately, this is enforced by GCC, which will report bad sizes.
    
    To set alignment requirements of call_single_data to the size of
    call_single_data, a struct definition and a typedef is used.
    
    To test the effect of the patch, I used the vm-scalability multiple
    thread swap test case (swap-w-seq-mt).  The test will create multiple
    threads and each thread will eat memory until all RAM and part of swap
    is used, so that huge number of IPIs are triggered when unmapping
    memory.  In the test, the throughput of memory writing improves ~5%
    compared with misaligned call_single_data, because of faster IPIs.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    [ Add call_single_data_t and align with size of call_single_data. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 68123c1fe549..98b1fe027fc9 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -14,13 +14,17 @@
 #include <linux/llist.h>
 
 typedef void (*smp_call_func_t)(void *info);
-struct call_single_data {
+struct __call_single_data {
 	struct llist_node llist;
 	smp_call_func_t func;
 	void *info;
 	unsigned int flags;
 };
 
+/* Use __aligned() to avoid to use 2 cache lines for 1 csd */
+typedef struct __call_single_data call_single_data_t
+	__aligned(sizeof(struct __call_single_data));
+
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 
@@ -48,7 +52,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
-int smp_call_function_single_async(int cpu, struct call_single_data *csd);
+int smp_call_function_single_async(int cpu, call_single_data_t *csd);
 
 #ifdef CONFIG_SMP
 

commit 8ce371f9846ef1e8b3cc8f6865766cb5c1f17e40
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 20 12:26:55 2017 +0100

    lockdep: Fix per-cpu static objects
    
    Since commit 383776fa7527 ("locking/lockdep: Handle statically initialized
    PER_CPU locks properly") we try to collapse per-cpu locks into a single
    class by giving them all the same key. For this key we choose the canonical
    address of the per-cpu object, which would be the offset into the per-cpu
    area.
    
    This has two problems:
    
     - there is a case where we run !0 lock->key through static_obj() and
       expect this to pass; it doesn't for canonical pointers.
    
     - 0 is a valid canonical address.
    
    Cure both issues by redefining the canonical address as the address of the
    per-cpu variable on the boot CPU.
    
    Since I didn't want to rely on CPU0 being the boot-cpu, or even existing at
    all, track the boot CPU in a variable.
    
    Fixes: 383776fa7527 ("locking/lockdep: Handle statically initialized PER_CPU locks properly")
    Reported-by: kernel test robot <fengguang.wu@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Borislav Petkov <bp@suse.de>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: linux-mm@kvack.org
    Cc: wfg@linux.intel.com
    Cc: kernel test robot <fengguang.wu@intel.com>
    Cc: LKP <lkp@01.org>
    Link: http://lkml.kernel.org/r/20170320114108.kbvcsuepem45j5cr@hirez.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 8e0cb7a0f836..68123c1fe549 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -120,6 +120,13 @@ extern unsigned int setup_max_cpus;
 extern void __init setup_nr_cpu_ids(void);
 extern void __init smp_init(void);
 
+extern int __boot_cpu_id;
+
+static inline int get_boot_cpu_id(void)
+{
+	return __boot_cpu_id;
+}
+
 #else /* !SMP */
 
 static inline void smp_send_stop(void) { }
@@ -158,6 +165,11 @@ static inline void smp_init(void) { up_late_init(); }
 static inline void smp_init(void) { }
 #endif
 
+static inline int get_boot_cpu_id(void)
+{
+	return 0;
+}
+
 #endif /* !SMP */
 
 /*

commit df8ce9d78a4e7fbe7ddfd8ccee3ecaaa0013e883
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 29 08:48:44 2016 +0200

    smp: Add function to execute a function synchronously on a CPU
    
    On some hardware models (e.g. Dell Studio 1555 laptop) some hardware
    related functions (e.g. SMIs) are to be executed on physical CPU 0
    only. Instead of open coding such a functionality multiple times in
    the kernel add a service function for this purpose. This will enable
    the possibility to take special measures in virtualized environments
    like Xen, too.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Douglas_Warzecha@dell.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: chrisw@sous-sol.org
    Cc: david.vrabel@citrix.com
    Cc: hpa@zytor.com
    Cc: jdelvare@suse.com
    Cc: jeremy@goop.org
    Cc: linux@roeck-us.net
    Cc: pali.rohar@gmail.com
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1472453327-19050-4-git-send-email-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index eccae4690f41..8e0cb7a0f836 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -196,6 +196,9 @@ extern void arch_enable_nonboot_cpus_end(void);
 
 void smp_setup_processor_id(void);
 
+int smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par,
+		    bool phys);
+
 /* SMP core functions */
 int smpcfd_prepare_cpu(unsigned int cpu);
 int smpcfd_dead_cpu(unsigned int cpu);

commit 31487f8328f20fdb302430b020a5d6e8446c1971
Author: Richard Weinberger <richard@nod.at>
Date:   Wed Jul 13 17:17:01 2016 +0000

    smp/cfd: Convert core to hotplug state machine
    
    Install the callbacks via the state machine. They are installed at runtime so
    smpcfd_prepare_cpu() needs to be invoked by the boot-CPU.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>
    [ Added the dropped CPU dying case back in. ]
    Signed-off-by: Richard Cochran <rcochran@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Davidlohr Bueso <dave@stgolabs>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153337.818376366@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c4414074bd88..eccae4690f41 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -196,4 +196,9 @@ extern void arch_enable_nonboot_cpus_end(void);
 
 void smp_setup_processor_id(void);
 
+/* SMP core functions */
+int smpcfd_prepare_cpu(unsigned int cpu);
+int smpcfd_dead_cpu(unsigned int cpu);
+int smpcfd_dying_cpu(unsigned int cpu);
+
 #endif /* __LINUX_SMP_H */

commit f4d03bd143c628b00f66cc2ec2c013767bdd1518
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 20 09:08:49 2015 -0700

    smp: don't use 16-bit words for atomic accesses
    
    Yes, it should work, but it's a bad idea.  Not only did ARM64 not have
    the 16-bit access code (there's a separate patch to add it), it's just
    not a good atomic type.  Some architectures fundamentally don't do
    atomic accesses in them (alpha), and it's not like it saves any space
    here anyway because of structure packing issues.
    
    We normally should aim for flags to be "unsigned int" or "unsigned
    long".  And if space is at a premium, use a single byte (although that
    causes problems on alpha again).  There might be very special cases
    where a 16-byte entity is really wanted, but this is not one of them.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index be91db2a7017..c4414074bd88 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -18,7 +18,7 @@ struct call_single_data {
 	struct llist_node llist;
 	smp_call_func_t func;
 	void *info;
-	u16 flags;
+	unsigned int flags;
 };
 
 /* total number of cpus in this system (may exceed NR_CPUS) */

commit 30b8b0066cafef274fc92462578ee346211ce7cb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 15 21:22:39 2015 +0000

    init: Get rid of x86isms
    
    The UP local API support can be set up from an early initcall. No need
    for horrible hackery in the init code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20150115211703.827943883@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 93dff5fff524..be91db2a7017 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -151,6 +151,13 @@ smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 static inline void kick_all_cpus_sync(void) {  }
 static inline void wake_up_all_idle_cpus(void) {  }
 
+#ifdef CONFIG_UP_LATE_INIT
+extern void __init up_late_init(void);
+static inline void smp_init(void) { up_late_init(); }
+#else
+static inline void smp_init(void) { }
+#endif
+
 #endif /* !SMP */
 
 /*

commit c6f4459fc3ba532e896cb678e29b45cb985f82bf
Author: Chuansheng Liu <chuansheng.liu@intel.com>
Date:   Thu Sep 4 15:17:54 2014 +0800

    smp: Add new wake_up_all_idle_cpus() function
    
    Currently kick_all_cpus_sync() can break non-polling idle cpus
    thru IPI interrupts.
    
    But sometimes we need to break the polling idle cpus immediately
    to reselect the suitable c-state, also for non-idle cpus, we need
    to do nothing if we try to wake up them.
    
    Here adding one new function wake_up_all_idle_cpus() to let all cpus
    out of idle based on function wake_up_if_idle().
    
    Signed-off-by: Chuansheng Liu <chuansheng.liu@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: daniel.lezcano@linaro.org
    Cc: rjw@rjwysocki.net
    Cc: linux-pm@vger.kernel.org
    Cc: changcheng.liu@intel.com
    Cc: xiaoming.wang@intel.com
    Cc: souvik.k.chakravarty@intel.com
    Cc: luto@amacapital.net
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Roman Gushchin <klamm@yandex-team.ru>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1409815075-4180-2-git-send-email-chuansheng.liu@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 34347f26be9b..93dff5fff524 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -100,6 +100,7 @@ int smp_call_function_any(const struct cpumask *mask,
 			  smp_call_func_t func, void *info, int wait);
 
 void kick_all_cpus_sync(void);
+void wake_up_all_idle_cpus(void);
 
 /*
  * Generic and arch helpers
@@ -148,6 +149,7 @@ smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 }
 
 static inline void kick_all_cpus_sync(void) {  }
+static inline void wake_up_all_idle_cpus(void) {  }
 
 #endif /* !SMP */
 

commit ae022622ae9447bd70e59db7c91efa25c99a90d5
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Fri Jun 6 14:38:31 2014 -0700

    idle: remove cpu_idle() forward declarations
    
    After all architectures were converted to the generic idle framework,
    commit d190e8195b90 ("idle: Remove GENERIC_IDLE_LOOP config switch")
    removed the last caller of cpu_idle().  The forward declarations in
    header files were forgotten.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 633f5edd7470..34347f26be9b 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -13,8 +13,6 @@
 #include <linux/init.h>
 #include <linux/llist.h>
 
-extern void cpu_idle(void);
-
 typedef void (*smp_call_func_t)(void *info);
 struct call_single_data {
 	struct llist_node llist;

commit c46fff2a3b29794b35d717b5680a27f31a6a6bc0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Feb 24 16:40:02 2014 +0100

    smp: Rename __smp_call_function_single() to smp_call_function_single_async()
    
    The name __smp_call_function_single() doesn't tell much about the
    properties of this function, especially when compared to
    smp_call_function_single().
    
    The comments above the implementation are also misleading. The main
    point of this function is actually not to be able to embed the csd
    in an object. This is actually a requirement that result from the
    purpose of this function which is to raise an IPI asynchronously.
    
    As such it can be called with interrupts disabled. And this feature
    comes at the cost of the caller who then needs to serialize the
    IPIs on this csd.
    
    Lets rename the function and enhance the comments so that they reflect
    these properties.
    
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@fb.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index b410a1f23281..633f5edd7470 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -50,7 +50,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
-int __smp_call_function_single(int cpu, struct call_single_data *csd);
+int smp_call_function_single_async(int cpu, struct call_single_data *csd);
 
 #ifdef CONFIG_SMP
 

commit fce8ad1568c57e7f334018dec4fa1744c926c135
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Feb 24 16:40:01 2014 +0100

    smp: Remove wait argument from __smp_call_function_single()
    
    The main point of calling __smp_call_function_single() is to send
    an IPI in a pure asynchronous way. By embedding a csd in an object,
    a caller can send the IPI without waiting for a previous one to complete
    as is required by smp_call_function_single() for example. As such,
    sending this kind of IPI can be safe even when irqs are disabled.
    
    This flexibility comes at the expense of the caller who then needs to
    synchronize the csd lifecycle by himself and make sure that IPIs on a
    single csd are serialized.
    
    This is how __smp_call_function_single() works when wait = 0 and this
    usecase is relevant.
    
    Now there don't seem to be any usecase with wait = 1 that can't be
    covered by smp_call_function_single() instead, which is safer. Lets look
    at the two possible scenario:
    
    1) The user calls __smp_call_function_single(wait = 1) on a csd embedded
       in an object. It looks like a nice and convenient pattern at the first
       sight because we can then retrieve the object from the IPI handler easily.
    
       But actually it is a waste of memory space in the object since the csd
       can be allocated from the stack by smp_call_function_single(wait = 1)
       and the object can be passed an the IPI argument.
    
       Besides that, embedding the csd in an object is more error prone
       because the caller must take care of the serialization of the IPIs
       for this csd.
    
    2) The user calls __smp_call_function_single(wait = 1) on a csd that
       is allocated on the stack. It's ok but smp_call_function_single()
       can do it as well and it already takes care of the allocation on the
       stack. Again it's more simple and less error prone.
    
    Therefore, using the underscore prepend API version with wait = 1
    is a bad pattern and a sign that the caller can do safer and more
    simple.
    
    There was a single user of that which has just been converted.
    So lets remove this option to discourage further users.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@fb.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c39074c794c5..b410a1f23281 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -50,7 +50,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
-int __smp_call_function_single(int cpu, struct call_single_data *csd, int wait);
+int __smp_call_function_single(int cpu, struct call_single_data *csd);
 
 #ifdef CONFIG_SMP
 

commit 08eed44c7249d381a099bc55577e55c6bb533160
Author: Jan Kara <jack@suse.cz>
Date:   Mon Feb 24 16:39:57 2014 +0100

    smp: Teach __smp_call_function_single() to check for offline cpus
    
    Align __smp_call_function_single() with smp_call_function_single() so
    that it also checks whether requested cpu is still online.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jens Axboe <axboe@fb.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 4991c6b12831..c39074c794c5 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -50,8 +50,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
-void __smp_call_function_single(int cpuid, struct call_single_data *data,
-				int wait);
+int __smp_call_function_single(int cpu, struct call_single_data *csd, int wait);
 
 #ifdef CONFIG_SMP
 

commit 0ebeb79ce920bed3a4a55113534951d95a444fbb
Author: Jan Kara <jack@suse.cz>
Date:   Mon Feb 24 16:39:56 2014 +0100

    smp: Remove unused list_head from csd
    
    Now that we got rid of all the remaining code which fiddled with csd.list,
    lets remove it.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jens Axboe <axboe@fb.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 6ae004e437ea..4991c6b12831 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -17,10 +17,7 @@ extern void cpu_idle(void);
 
 typedef void (*smp_call_func_t)(void *info);
 struct call_single_data {
-	union {
-		struct list_head list;
-		struct llist_node llist;
-	};
+	struct llist_node llist;
 	smp_call_func_t func;
 	void *info;
 	u16 flags;

commit fb37bb04d6c8d6c44e61d9da189dcfa6aa0f135e
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Feb 10 14:25:49 2014 -0800

    smp.h: fix x86+cpu.c sparse warnings about arch nonboot CPU calls
    
    Use what we already do for arch_disable_smp_support() to fix these:
    
      arch/x86/kernel/smpboot.c:1155:6: warning: symbol 'arch_enable_nonboot_cpus_begin' was not declared. Should it be static?
      arch/x86/kernel/smpboot.c:1160:6: warning: symbol 'arch_enable_nonboot_cpus_end' was not declared. Should it be static?
      kernel/cpu.c:512:13: warning: symbol 'arch_enable_nonboot_cpus_begin' was not declared. Should it be static?
      kernel/cpu.c:516:13: warning: symbol 'arch_enable_nonboot_cpus_end' was not declared. Should it be static?
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3834f43f9993..6ae004e437ea 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -188,6 +188,9 @@ static inline void kick_all_cpus_sync(void) {  }
  */
 extern void arch_disable_smp_support(void);
 
+extern void arch_enable_nonboot_cpus_begin(void);
+extern void arch_enable_nonboot_cpus_end(void);
+
 void smp_setup_processor_id(void);
 
 #endif /* __LINUX_SMP_H */

commit 6897fc22ea01b562b55c6168592bcbd3ee62b006
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Jan 30 15:45:47 2014 -0800

    kernel: use lockless list for smp_call_function_single
    
    Make smp_call_function_single and friends more efficient by using a
    lockless list.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 5da22ee42e16..3834f43f9993 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -11,12 +11,16 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
+#include <linux/llist.h>
 
 extern void cpu_idle(void);
 
 typedef void (*smp_call_func_t)(void *info);
 struct call_single_data {
-	struct list_head list;
+	union {
+		struct list_head list;
+		struct llist_node llist;
+	};
 	smp_call_func_t func;
 	void *info;
 	u16 flags;

commit 7cf64f861b7a43b395f0855994003254b06a7e5a
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Nov 14 14:32:09 2013 -0800

    kernel-provide-a-__smp_call_function_single-stub-for-config_smp-fix
    
    x86_64 allnoconfig:
    
      kernel/up.c:25: error: redefinition of '__smp_call_function_single'
      include/linux/smp.h:154: note: previous definition of '__smp_call_function_single' was here
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 78851513030d..5da22ee42e16 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -49,6 +49,9 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
+void __smp_call_function_single(int cpuid, struct call_single_data *data,
+				int wait);
+
 #ifdef CONFIG_SMP
 
 #include <linux/preempt.h>
@@ -95,9 +98,6 @@ int smp_call_function(smp_call_func_t func, void *info, int wait);
 void smp_call_function_many(const struct cpumask *mask,
 			    smp_call_func_t func, void *info, bool wait);
 
-void __smp_call_function_single(int cpuid, struct call_single_data *data,
-				int wait);
-
 int smp_call_function_any(const struct cpumask *mask,
 			  smp_call_func_t func, void *info, int wait);
 
@@ -151,12 +151,6 @@ smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 
 static inline void kick_all_cpus_sync(void) {  }
 
-static inline void __smp_call_function_single(int cpuid,
-		struct call_single_data *data, int wait)
-{
-	on_each_cpu(data->func, data->info, wait);
-}
-
 #endif /* !SMP */
 
 /*

commit 0a06ff068f1255bcd7965ab07bc0f4adc3eb639a
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Nov 14 14:32:07 2013 -0800

    kernel: remove CONFIG_USE_GENERIC_SMP_HELPERS
    
    We've switched over every architecture that supports SMP to it, so
    remove the new useless config variable.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 731f5237d5f4..78851513030d 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -106,14 +106,10 @@ void kick_all_cpus_sync(void);
 /*
  * Generic and arch helpers
  */
-#ifdef CONFIG_USE_GENERIC_SMP_HELPERS
 void __init call_function_init(void);
 void generic_smp_call_function_single_interrupt(void);
 #define generic_smp_call_function_interrupt \
 	generic_smp_call_function_single_interrupt
-#else
-static inline void call_function_init(void) { }
-#endif
 
 /*
  * Mark the boot cpu "online" so that it can call console drivers in

commit 9809b18fcf6b8d8ec4d3643677345907e6b50eca
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Sep 24 15:27:30 2013 -0700

    watchdog: update watchdog_thresh properly
    
    watchdog_tresh controls how often nmi perf event counter checks per-cpu
    hrtimer_interrupts counter and blows up if the counter hasn't changed
    since the last check.  The counter is updated by per-cpu
    watchdog_hrtimer hrtimer which is scheduled with 2/5 watchdog_thresh
    period which guarantees that hrtimer is scheduled 2 times per the main
    period.  Both hrtimer and perf event are started together when the
    watchdog is enabled.
    
    So far so good.  But...
    
    But what happens when watchdog_thresh is updated from sysctl handler?
    
    proc_dowatchdog will set a new sampling period and hrtimer callback
    (watchdog_timer_fn) will use the new value in the next round.  The
    problem, however, is that nobody tells the perf event that the sampling
    period has changed so it is ticking with the period configured when it
    has been set up.
    
    This might result in an ear ripping dissonance between perf and hrtimer
    parts if the watchdog_thresh is increased.  And even worse it might lead
    to KABOOM if the watchdog is configured to panic on such a spurious
    lockup.
    
    This patch fixes the issue by updating both nmi perf even counter and
    hrtimers if the threshold value has changed.
    
    The nmi one is disabled and then reinitialized from scratch.  This has
    an unpleasant side effect that the allocation of the new event might
    fail theoretically so the hard lockup detector would be disabled for
    such cpus.  On the other hand such a memory allocation failure is very
    unlikely because the original event is deallocated right before.
    
    It would be much nicer if we just changed perf event period but there
    doesn't seem to be any API to do that right now.  It is also unfortunate
    that perf_event_alloc uses GFP_KERNEL allocation unconditionally so we
    cannot use on_each_cpu() and do the same thing from the per-cpu context.
    The update from the current CPU should be safe because
    perf_event_disable removes the event atomically before it clears the
    per-cpu watchdog_ev so it cannot change anything under running handler
    feet.
    
    The hrtimer is simply restarted (thanks to Don Zickus who has pointed
    this out) if it is queued because we cannot rely it will fire&adopt to
    the new sampling period before a new nmi event triggers (when the
    treshold is decreased).
    
    [akpm@linux-foundation.org: the UP version of __smp_call_function_single ended up in the wrong place]
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Fabio Estevam <festevam@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index cfb7ca094b38..731f5237d5f4 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -155,6 +155,12 @@ smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 
 static inline void kick_all_cpus_sync(void) {  }
 
+static inline void __smp_call_function_single(int cpuid,
+		struct call_single_data *data, int wait)
+{
+	on_each_cpu(data->func, data->info, wait);
+}
+
 #endif /* !SMP */
 
 /*

commit bff2dc42bcafdd75c0296987747f782965d691a0
Author: David Daney <david.daney@cavium.com>
Date:   Wed Sep 11 14:23:26 2013 -0700

    smp.h: move !SMP version of on_each_cpu() out-of-line
    
    All of the other non-trivial !SMP versions of functions in smp.h are
    out-of-line in up.c.  Move on_each_cpu() there as well.
    
    This allows us to get rid of the #include <linux/irqflags.h>.  The
    drawback is that this makes both the x86_64 and i386 defconfig !SMP
    kernels about 200 bytes larger each.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3724a9070907..cfb7ca094b38 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -11,7 +11,6 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
-#include <linux/irqflags.h>
 
 extern void cpu_idle(void);
 
@@ -29,6 +28,11 @@ extern unsigned int total_cpus;
 int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
 			     int wait);
 
+/*
+ * Call a function on all processors
+ */
+int on_each_cpu(smp_call_func_t func, void *info, int wait);
+
 /*
  * Call a function on processors specified by mask, which might include
  * the local one.
@@ -111,11 +115,6 @@ void generic_smp_call_function_single_interrupt(void);
 static inline void call_function_init(void) { }
 #endif
 
-/*
- * Call a function on all processors
- */
-int on_each_cpu(smp_call_func_t func, void *info, int wait);
-
 /*
  * Mark the boot cpu "online" so that it can call console drivers in
  * printk() and can access its per-cpu storage.
@@ -141,16 +140,6 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
 
-static inline int on_each_cpu(smp_call_func_t func, void *info, int wait)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	func(info);
-	local_irq_restore(flags);
-	return 0;
-}
-
 static inline void smp_send_reschedule(int cpu) { }
 #define smp_prepare_boot_cpu()			do {} while (0)
 #define smp_call_function_many(mask, func, info, wait) \

commit fa688207c9db48b64ab6538abc3fcdf26110b9ec
Author: David Daney <david.daney@cavium.com>
Date:   Wed Sep 11 14:23:24 2013 -0700

    smp: quit unconditionally enabling irq in on_each_cpu_mask and on_each_cpu_cond
    
    As in commit f21afc25f9ed ("smp.h: Use local_irq_{save,restore}() in
    !SMP version of on_each_cpu()"), we don't want to enable irqs if they
    are not already enabled.  There are currently no known problematical
    callers of these functions, but since it is a known failure pattern, we
    preemptively fix them.
    
    Since they are not trivial functions, make them non-inline by moving
    them to up.c.  This also makes it so we don't have to fix #include
    dependancies for preempt_{disable,enable}.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c8488763277f..3724a9070907 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -29,6 +29,22 @@ extern unsigned int total_cpus;
 int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
 			     int wait);
 
+/*
+ * Call a function on processors specified by mask, which might include
+ * the local one.
+ */
+void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
+		void *info, bool wait);
+
+/*
+ * Call a function on each processor for which the supplied function
+ * cond_func returns a positive value. This may include the local
+ * processor.
+ */
+void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
+		smp_call_func_t func, void *info, bool wait,
+		gfp_t gfp_flags);
+
 #ifdef CONFIG_SMP
 
 #include <linux/preempt.h>
@@ -100,22 +116,6 @@ static inline void call_function_init(void) { }
  */
 int on_each_cpu(smp_call_func_t func, void *info, int wait);
 
-/*
- * Call a function on processors specified by mask, which might include
- * the local one.
- */
-void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
-		void *info, bool wait);
-
-/*
- * Call a function on each processor for which the supplied function
- * cond_func returns a positive value. This may include the local
- * processor.
- */
-void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
-		smp_call_func_t func, void *info, bool wait,
-		gfp_t gfp_flags);
-
 /*
  * Mark the boot cpu "online" so that it can call console drivers in
  * printk() and can access its per-cpu storage.
@@ -151,36 +151,6 @@ static inline int on_each_cpu(smp_call_func_t func, void *info, int wait)
 	return 0;
 }
 
-/*
- * Note we still need to test the mask even for UP
- * because we actually can get an empty mask from
- * code that on SMP might call us without the local
- * CPU in the mask.
- */
-#define on_each_cpu_mask(mask, func, info, wait) \
-	do {						\
-		if (cpumask_test_cpu(0, (mask))) {	\
-			local_irq_disable();		\
-			(func)(info);			\
-			local_irq_enable();		\
-		}					\
-	} while (0)
-/*
- * Preemption is disabled here to make sure the cond_func is called under the
- * same condtions in UP and SMP.
- */
-#define on_each_cpu_cond(cond_func, func, info, wait, gfp_flags)\
-	do {							\
-		void *__info = (info);				\
-		preempt_disable();				\
-		if ((cond_func)(0, __info)) {			\
-			local_irq_disable();			\
-			(func)(__info);				\
-			local_irq_enable();			\
-		}						\
-		preempt_enable();				\
-	} while (0)
-
 static inline void smp_send_reschedule(int cpu) { }
 #define smp_prepare_boot_cpu()			do {} while (0)
 #define smp_call_function_many(mask, func, info, wait) \

commit 3b8967d713d7426e9dd107d065208b84adface91
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Sep 11 14:19:37 2013 -0700

    include/linux/smp.h:on_each_cpu(): switch back to a C function
    
    Revert commit c846ef7deba2 ("include/linux/smp.h:on_each_cpu(): switch
    back to a macro").  It turns out that the problematic linux/irqflags.h
    include was fixed within ia64 and mn10300.
    
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: David Daney <david.daney@cavium.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c181399f2c20..c8488763277f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -11,6 +11,7 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
+#include <linux/irqflags.h>
 
 extern void cpu_idle(void);
 
@@ -139,14 +140,17 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 }
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
-#define on_each_cpu(func, info, wait)		\
-	({					\
-		unsigned long __flags;		\
-		local_irq_save(__flags);	\
-		func(info);			\
-		local_irq_restore(__flags);	\
-		0;				\
-	})
+
+static inline int on_each_cpu(smp_call_func_t func, void *info, int wait)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	func(info);
+	local_irq_restore(flags);
+	return 0;
+}
+
 /*
  * Note we still need to test the mask even for UP
  * because we actually can get an empty mask from

commit c846ef7deba2d4f75138cf6a4b137b7e0e7659af
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Jul 3 15:00:41 2013 -0700

    include/linux/smp.h:on_each_cpu(): switch back to a macro
    
    Commit f21afc25f9ed ("smp.h: Use local_irq_{save,restore}() in !SMP
    version of on_each_cpu()") converted on_each_cpu() to a C function.
    
    This required inclusion of irqflags.h, which broke ia64 and mn10300 (at
    least) due to header ordering hell.
    
    Switch on_each_cpu() back to a macro to fix this.
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>    [3.10.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c8488763277f..c181399f2c20 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -11,7 +11,6 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
-#include <linux/irqflags.h>
 
 extern void cpu_idle(void);
 
@@ -140,17 +139,14 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 }
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
-
-static inline int on_each_cpu(smp_call_func_t func, void *info, int wait)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	func(info);
-	local_irq_restore(flags);
-	return 0;
-}
-
+#define on_each_cpu(func, info, wait)		\
+	({					\
+		unsigned long __flags;		\
+		local_irq_save(__flags);	\
+		func(info);			\
+		local_irq_restore(__flags);	\
+		0;				\
+	})
 /*
  * Note we still need to test the mask even for UP
  * because we actually can get an empty mask from

commit f21afc25f9ed45b8ffe200d0f071b0caec3ed2ef
Author: David Daney <david.daney@cavium.com>
Date:   Fri Jun 14 11:13:59 2013 -0700

    smp.h: Use local_irq_{save,restore}() in !SMP version of on_each_cpu().
    
    Thanks to commit f91eb62f71b3 ("init: scream bloody murder if interrupts
    are enabled too early"), "bloody murder" is now being screamed.
    
    With a MIPS OCTEON config, we use on_each_cpu() in our
    irq_chip.irq_bus_sync_unlock() function.  This gets called in early as a
    result of the time_init() call.  Because the !SMP version of
    on_each_cpu() unconditionally enables irqs, we get:
    
        WARNING: at init/main.c:560 start_kernel+0x250/0x410()
        Interrupts were enabled early
        CPU: 0 PID: 0 Comm: swapper Not tainted 3.10.0-rc5-Cavium-Octeon+ #801
        Call Trace:
          show_stack+0x68/0x80
          warn_slowpath_common+0x78/0xb0
          warn_slowpath_fmt+0x38/0x48
          start_kernel+0x250/0x410
    
    Suggested fix: Do what we already do in the SMP version of
    on_each_cpu(), and use local_irq_save/local_irq_restore.  Because we
    need a flags variable, make it a static inline to avoid name space
    issues.
    
    [ Change from v1: Convert on_each_cpu to a static inline function, add
      #include <linux/irqflags.h> to avoid build breakage on some files.
    
      on_each_cpu_mask() and on_each_cpu_cond() suffer the same problem as
      on_each_cpu(), but they are not causing !SMP bugs for me, so I will
      defer changing them to a less urgent patch. ]
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index e6564c1dc552..c8488763277f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -11,6 +11,7 @@
 #include <linux/list.h>
 #include <linux/cpumask.h>
 #include <linux/init.h>
+#include <linux/irqflags.h>
 
 extern void cpu_idle(void);
 
@@ -139,13 +140,17 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 }
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
-#define on_each_cpu(func,info,wait)		\
-	({					\
-		local_irq_disable();		\
-		func(info);			\
-		local_irq_enable();		\
-		0;				\
-	})
+
+static inline int on_each_cpu(smp_call_func_t func, void *info, int wait)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	func(info);
+	local_irq_restore(flags);
+	return 0;
+}
+
 /*
  * Note we still need to test the mask even for UP
  * because we actually can get an empty mask from

commit 3440a1ca99707f093e9568ba9762764d3162dd8f
Author: liguang <lig.fnst@cn.fujitsu.com>
Date:   Tue Apr 30 15:27:26 2013 -0700

    kernel/smp.c: remove 'priv' of call_single_data
    
    The 'priv' field is redundant; we can pass data via 'info'.
    
    Signed-off-by: liguang <lig.fnst@cn.fujitsu.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3e07a7df6478..e6564c1dc552 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -20,7 +20,6 @@ struct call_single_data {
 	smp_call_func_t func;
 	void *info;
 	u16 flags;
-	u16 priv;
 };
 
 /* total number of cpus in this system (may exceed NR_CPUS) */

commit 9a46ad6d6df3b547d057c39db13f69d7170a99e9
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Feb 21 16:43:03 2013 -0800

    smp: make smp_call_function_many() use logic similar to smp_call_function_single()
    
    I'm testing swapout workload in a two-socket Xeon machine.  The workload
    has 10 threads, each thread sequentially accesses separate memory
    region.  TLB flush overhead is very big in the workload.  For each page,
    page reclaim need move it from active lru list and then unmap it.  Both
    need a TLB flush.  And this is a multthread workload, TLB flush happens
    in 10 CPUs.  In X86, TLB flush uses generic smp_call)function.  So this
    workload stress smp_call_function_many heavily.
    
    Without patch, perf shows:
    +  24.49%  [k] generic_smp_call_function_interrupt
    -  21.72%  [k] _raw_spin_lock
       - _raw_spin_lock
          + 79.80% __page_check_address
          + 6.42% generic_smp_call_function_interrupt
          + 3.31% get_swap_page
          + 2.37% free_pcppages_bulk
          + 1.75% handle_pte_fault
          + 1.54% put_super
          + 1.41% grab_super_passive
          + 1.36% __swap_duplicate
          + 0.68% blk_flush_plug_list
          + 0.62% swap_info_get
    +   6.55%  [k] flush_tlb_func
    +   6.46%  [k] smp_call_function_many
    +   5.09%  [k] call_function_interrupt
    +   4.75%  [k] default_send_IPI_mask_sequence_phys
    +   2.18%  [k] find_next_bit
    
    swapout throughput is around 1300M/s.
    
    With the patch, perf shows:
    -  27.23%  [k] _raw_spin_lock
       - _raw_spin_lock
          + 80.53% __page_check_address
          + 8.39% generic_smp_call_function_single_interrupt
          + 2.44% get_swap_page
          + 1.76% free_pcppages_bulk
          + 1.40% handle_pte_fault
          + 1.15% __swap_duplicate
          + 1.05% put_super
          + 0.98% grab_super_passive
          + 0.86% blk_flush_plug_list
          + 0.57% swap_info_get
    +   8.25%  [k] default_send_IPI_mask_sequence_phys
    +   7.55%  [k] call_function_interrupt
    +   7.47%  [k] smp_call_function_many
    +   7.25%  [k] flush_tlb_func
    +   3.81%  [k] _raw_spin_lock_irqsave
    +   3.78%  [k] generic_smp_call_function_single_interrupt
    
    swapout throughput is around 1400M/s.  So there is around a 7%
    improvement, and total cpu utilization doesn't change.
    
    Without the patch, cfd_data is shared by all CPUs.
    generic_smp_call_function_interrupt does read/write cfd_data several times
    which will create a lot of cache ping-pong.  With the patch, the data
    becomes per-cpu.  The ping-pong is avoided.  And from the perf data, this
    doesn't make call_single_queue lock contend.
    
    Next step is to remove generic_smp_call_function_interrupt() from arch
    code.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index dd6f06be3c9f..3e07a7df6478 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -89,7 +89,8 @@ void kick_all_cpus_sync(void);
 #ifdef CONFIG_USE_GENERIC_SMP_HELPERS
 void __init call_function_init(void);
 void generic_smp_call_function_single_interrupt(void);
-void generic_smp_call_function_interrupt(void);
+#define generic_smp_call_function_interrupt \
+	generic_smp_call_function_single_interrupt
 #else
 static inline void call_function_init(void) { }
 #endif

commit 43cc7e86f3200b094e2960b732623aeec00b482d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 15 17:26:16 2012 +0200

    smp: Remove num_booting_cpus()
    
    No users.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index a34d4f15430d..dd6f06be3c9f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -177,7 +177,6 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 	} while (0)
 
 static inline void smp_send_reschedule(int cpu) { }
-#define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
 #define smp_call_function_many(mask, func, info, wait) \
 			(up_smp_call_function(func, info))

commit 8feb8e896d77439146d2e2ab3d0ab55bb5baf5fc
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Tue May 29 15:16:05 2012 +0800

    smp: Remove ipi_call_lock[_irq]()/ipi_call_unlock[_irq]()
    
    There is no user of those APIs anymore, just remove it.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: ralf@linux-mips.org
    Cc: sshtylyov@mvista.com
    Cc: david.daney@cavium.com
    Cc: nikunj@linux.vnet.ibm.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: axboe@kernel.dk
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1338275765-3217-11-git-send-email-yong.zhang0@gmail.com
    Acked-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 717fb746c9a8..a34d4f15430d 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -90,10 +90,6 @@ void kick_all_cpus_sync(void);
 void __init call_function_init(void);
 void generic_smp_call_function_single_interrupt(void);
 void generic_smp_call_function_interrupt(void);
-void ipi_call_lock(void);
-void ipi_call_unlock(void);
-void ipi_call_lock_irq(void);
-void ipi_call_unlock_irq(void);
 #else
 static inline void call_function_init(void) { }
 #endif

commit f37f435f33717dcf15fd4bb422da739da7fc2052
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 7 17:59:48 2012 +0000

    smp: Implement kick_all_cpus_sync()
    
    Will replace the misnomed cpu_idle_wait() function which is copied a
    gazillion times all over arch/*
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120507175652.049316594@linutronix.de

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 24360de6c968..717fb746c9a8 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -81,6 +81,8 @@ void __smp_call_function_single(int cpuid, struct call_single_data *data,
 int smp_call_function_any(const struct cpumask *mask,
 			  smp_call_func_t func, void *info, int wait);
 
+void kick_all_cpus_sync(void);
+
 /*
  * Generic and arch helpers
  */
@@ -192,6 +194,8 @@ smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 	return smp_call_function_single(0, func, info, wait);
 }
 
+static inline void kick_all_cpus_sync(void) {  }
+
 #endif /* !SMP */
 
 /*

commit 8239c25f47d2b318156993b15f33900a86ea5e17
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:42 2012 +0000

    smp: Add task_struct argument to __cpu_up()
    
    Preparatory patch to make the idle thread allocation for secondary
    cpus generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124556.964170564@linutronix.de

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 10530d92c04b..24360de6c968 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -61,7 +61,7 @@ extern void smp_prepare_cpus(unsigned int max_cpus);
 /*
  * Bring a CPU up
  */
-extern int __cpu_up(unsigned int cpunum);
+extern int __cpu_up(unsigned int cpunum, struct task_struct *tidle);
 
 /*
  * Final polishing of CPUs

commit b3a7e98e024ffa9f7e4554dd720c508015c4a831
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Wed Mar 28 14:42:43 2012 -0700

    smp: add func to IPI cpus based on parameter func
    
    Add the on_each_cpu_cond() function that wraps on_each_cpu_mask() and
    calculates the cpumask of cpus to IPI by calling a function supplied as a
    parameter in order to determine whether to IPI each specific cpu.
    
    The function works around allocation failure of cpumask variable in
    CONFIG_CPUMASK_OFFSTACK=y by itereating over cpus sending an IPI a time
    via smp_call_function_single().
    
    The function is useful since it allows to seperate the specific code that
    decided in each case whether to IPI a specific cpu for a specific request
    from the common boilerplate code of handling creating the mask, handling
    failures etc.
    
    [akpm@linux-foundation.org: s/gfpflags/gfp_flags/]
    [akpm@linux-foundation.org: avoid double-evaluation of `info' (per Michal), parenthesise evaluation of `cond_func']
    [akpm@linux-foundation.org: s/CPU/CPUs, use all 80 cols in comment]
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Sasha Levin <levinsasha928@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.org>
    Cc: Kosaki Motohiro <kosaki.motohiro@gmail.com>
    Cc: Milton Miller <miltonm@bga.com>
    Reviewed-by: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index d0adb7898d54..10530d92c04b 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -108,6 +108,15 @@ int on_each_cpu(smp_call_func_t func, void *info, int wait);
 void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
 		void *info, bool wait);
 
+/*
+ * Call a function on each processor for which the supplied function
+ * cond_func returns a positive value. This may include the local
+ * processor.
+ */
+void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
+		smp_call_func_t func, void *info, bool wait,
+		gfp_t gfp_flags);
+
 /*
  * Mark the boot cpu "online" so that it can call console drivers in
  * printk() and can access its per-cpu storage.
@@ -153,6 +162,21 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 			local_irq_enable();		\
 		}					\
 	} while (0)
+/*
+ * Preemption is disabled here to make sure the cond_func is called under the
+ * same condtions in UP and SMP.
+ */
+#define on_each_cpu_cond(cond_func, func, info, wait, gfp_flags)\
+	do {							\
+		void *__info = (info);				\
+		preempt_disable();				\
+		if ((cond_func)(0, __info)) {			\
+			local_irq_disable();			\
+			(func)(__info);				\
+			local_irq_enable();			\
+		}						\
+		preempt_enable();				\
+	} while (0)
 
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1

commit 3fc498f165304dc913f1d13b5ac9ab4c758ee7ab
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Wed Mar 28 14:42:43 2012 -0700

    smp: introduce a generic on_each_cpu_mask() function
    
    We have lots of infrastructure in place to partition multi-core systems
    such that we have a group of CPUs that are dedicated to specific task:
    cgroups, scheduler and interrupt affinity, and cpuisol= boot parameter.
    Still, kernel code will at times interrupt all CPUs in the system via IPIs
    for various needs.  These IPIs are useful and cannot be avoided
    altogether, but in certain cases it is possible to interrupt only specific
    CPUs that have useful work to do and not the entire system.
    
    This patch set, inspired by discussions with Peter Zijlstra and Frederic
    Weisbecker when testing the nohz task patch set, is a first stab at trying
    to explore doing this by locating the places where such global IPI calls
    are being made and turning the global IPI into an IPI for a specific group
    of CPUs.  The purpose of the patch set is to get feedback if this is the
    right way to go for dealing with this issue and indeed, if the issue is
    even worth dealing with at all.  Based on the feedback from this patch set
    I plan to offer further patches that address similar issue in other code
    paths.
    
    This patch creates an on_each_cpu_mask() and on_each_cpu_cond()
    infrastructure API (the former derived from existing arch specific
    versions in Tile and Arm) and uses them to turn several global IPI
    invocation to per CPU group invocations.
    
    Core kernel:
    
    on_each_cpu_mask() calls a function on processors specified by cpumask,
    which may or may not include the local processor.
    
    You must not call this function with disabled interrupts or from a
    hardware interrupt handler or from a bottom half handler.
    
    arch/arm:
    
    Note that the generic version is a little different then the Arm one:
    
    1. It has the mask as first parameter
    2. It calls the function on the calling CPU with interrupts disabled,
       but this should be OK since the function is called on the other CPUs
       with interrupts disabled anyway.
    
    arch/tile:
    
    The API is the same as the tile private one, but the generic version
    also calls the function on the with interrupts disabled in UP case
    
    This is OK since the function is called on the other CPUs
    with interrupts disabled.
    
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Sasha Levin <levinsasha928@gmail.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.org>
    Cc: Kosaki Motohiro <kosaki.motohiro@gmail.com>
    Cc: Milton Miller <miltonm@bga.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 8cc38d3bab0c..d0adb7898d54 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -101,6 +101,13 @@ static inline void call_function_init(void) { }
  */
 int on_each_cpu(smp_call_func_t func, void *info, int wait);
 
+/*
+ * Call a function on processors specified by mask, which might include
+ * the local one.
+ */
+void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
+		void *info, bool wait);
+
 /*
  * Mark the boot cpu "online" so that it can call console drivers in
  * printk() and can access its per-cpu storage.
@@ -132,6 +139,21 @@ static inline int up_smp_call_function(smp_call_func_t func, void *info)
 		local_irq_enable();		\
 		0;				\
 	})
+/*
+ * Note we still need to test the mask even for UP
+ * because we actually can get an empty mask from
+ * code that on SMP might call us without the local
+ * CPU in the mask.
+ */
+#define on_each_cpu_mask(mask, func, info, wait) \
+	do {						\
+		if (cpumask_test_cpu(0, (mask))) {	\
+			local_irq_disable();		\
+			(func)(info);			\
+			local_irq_enable();		\
+		}					\
+	} while (0)
+
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)

commit d8ad7d1123a960cc9f276bd499f9325c6f5e1bd1
Author: Takao Indoh <indou.takao@jp.fujitsu.com>
Date:   Tue Mar 29 12:35:04 2011 -0400

    generic-ipi: Fix kexec boot crash by initializing call_single_queue before enabling interrupts
    
    There is a problem that kdump(2nd kernel) sometimes hangs up due
    to a pending IPI from 1st kernel. Kernel panic occurs because IPI
    comes before call_single_queue is initialized.
    
    To fix the crash, rename init_call_single_data() to call_function_init()
    and call it in start_kernel() so that call_single_queue can be
    initialized before enabling interrupts.
    
    The details of the crash are:
    
     (1) 2nd kernel boots up
    
     (2) A pending IPI from 1st kernel comes when irqs are first enabled
         in start_kernel().
    
     (3) Kernel tries to handle the interrupt, but call_single_queue
         is not initialized yet at this point. As a result, in the
         generic_smp_call_function_single_interrupt(), NULL pointer
         dereference occurs when list_replace_init() tries to access
         &q->list.next.
    
    Therefore this patch changes the name of init_call_single_data()
    to call_function_init() and calls it before local_irq_enable()
    in start_kernel().
    
    Signed-off-by: Takao Indoh <indou.takao@jp.fujitsu.com>
    Reviewed-by: WANG Cong <xiyou.wangcong@gmail.com>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Milton Miller <miltonm@bga.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: kexec@lists.infradead.org
    Link: http://lkml.kernel.org/r/D6CBEE2F420741indou.takao@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 7ad824d510a2..8cc38d3bab0c 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -85,12 +85,15 @@ int smp_call_function_any(const struct cpumask *mask,
  * Generic and arch helpers
  */
 #ifdef CONFIG_USE_GENERIC_SMP_HELPERS
+void __init call_function_init(void);
 void generic_smp_call_function_single_interrupt(void);
 void generic_smp_call_function_interrupt(void);
 void ipi_call_lock(void);
 void ipi_call_unlock(void);
 void ipi_call_lock_irq(void);
 void ipi_call_unlock_irq(void);
+#else
+static inline void call_function_init(void) { }
 #endif
 
 /*
@@ -134,7 +137,7 @@ static inline void smp_send_reschedule(int cpu) { }
 #define smp_prepare_boot_cpu()			do {} while (0)
 #define smp_call_function_many(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
-static inline void init_call_single_data(void) { }
+static inline void call_function_init(void) { }
 
 static inline int
 smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,

commit ce2a40458ebf9c2e47fa0806fec31f845bfcb9d5
Author: Milton Miller <miltonm@bga.com>
Date:   Tue May 10 19:29:13 2011 +0000

    Remove unused MSG_ flags in linux/smp.h
    
    Now that powerpc has removed its use of MSG_ALL_BUT_SELF and MSG_ALL
    all these MSG_ flags are unused.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 74243c86ba39..7ad824d510a2 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -98,16 +98,6 @@ void ipi_call_unlock_irq(void);
  */
 int on_each_cpu(smp_call_func_t func, void *info, int wait);
 
-#define MSG_ALL_BUT_SELF	0x8000	/* Assume <32768 CPU's */
-#define MSG_ALL			0x8001
-
-#define MSG_INVALIDATE_TLB	0x0001	/* Remote processor TLB invalidate */
-#define MSG_STOP_CPU		0x0002	/* Sent to shut down slave CPU's
-					 * when rebooting
-					 */
-#define MSG_RESCHEDULE		0x0003	/* Reschedule request from master CPU*/
-#define MSG_CALL_FUNCTION       0x0004  /* Call function on all other CPUs */
-
 /*
  * Mark the boot cpu "online" so that it can call console drivers in
  * printk() and can access its per-cpu storage.

commit 04948c7f80b9446009c1c4791bb93e79729724fb
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Mar 23 08:24:58 2011 +0100

    smp: add missing init.h include
    
    Commit 34db18a054c6 ("smp: move smp setup functions to kernel/smp.c")
    causes this build error on s390 because of a missing init.h include:
    
      CC      arch/s390/kernel/asm-offsets.s
      In file included from /home2/heicarst/linux-2.6/arch/s390/include/asm/spinlock.h:14:0,
      from include/linux/spinlock.h:87,
      from include/linux/seqlock.h:29,
      from include/linux/time.h:8,
      from include/linux/timex.h:56,
      from include/linux/sched.h:57,
      from arch/s390/kernel/asm-offsets.c:10:
      include/linux/smp.h:117:20: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'setup_nr_cpu_ids'
      include/linux/smp.h:118:20: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'smp_init'
    
    Fix it by adding the include statement.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: WANG Cong <amwang@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 48159dd320d0..74243c86ba39 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -10,6 +10,7 @@
 #include <linux/types.h>
 #include <linux/list.h>
 #include <linux/cpumask.h>
+#include <linux/init.h>
 
 extern void cpu_idle(void);
 

commit 34db18a054c600b6f81787165669dc572fe4de25
Author: Amerigo Wang <amwang@redhat.com>
Date:   Tue Mar 22 16:34:06 2011 -0700

    smp: move smp setup functions to kernel/smp.c
    
    Move setup_nr_cpu_ids(), smp_init() and some other SMP boot parameter
    setup functions from init/main.c to kenrel/smp.c, saves some #ifdef
    CONFIG_SMP.
    
    Signed-off-by: WANG Cong <amwang@redhat.com>
    Cc: Rakib Mullick <rakib.mullick@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 6dc95cac6b3d..48159dd320d0 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -114,6 +114,8 @@ int on_each_cpu(smp_call_func_t func, void *info, int wait);
 void smp_prepare_boot_cpu(void);
 
 extern unsigned int setup_max_cpus;
+extern void __init setup_nr_cpu_ids(void);
+extern void __init smp_init(void);
 
 #else /* !SMP */
 

commit 3a5f65df5a0fcbaa35e5417c0420d691fee4ac56
Author: David Howells <dhowells@redhat.com>
Date:   Wed Oct 27 17:28:36 2010 +0100

    Typedef SMP call function pointer
    
    Typedef the pointer to the function to be called by smp_call_function() and
    friends:
    
            typedef void (*smp_call_func_t)(void *info);
    
    as it is used in a fair number of places.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-arch@vger.kernel.org

diff --git a/include/linux/smp.h b/include/linux/smp.h
index cfa2d20e35f1..6dc95cac6b3d 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -13,9 +13,10 @@
 
 extern void cpu_idle(void);
 
+typedef void (*smp_call_func_t)(void *info);
 struct call_single_data {
 	struct list_head list;
-	void (*func) (void *info);
+	smp_call_func_t func;
 	void *info;
 	u16 flags;
 	u16 priv;
@@ -24,8 +25,8 @@ struct call_single_data {
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 
-int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
-				int wait);
+int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
+			     int wait);
 
 #ifdef CONFIG_SMP
 
@@ -69,15 +70,15 @@ extern void smp_cpus_done(unsigned int max_cpus);
 /*
  * Call a function on all other processors
  */
-int smp_call_function(void(*func)(void *info), void *info, int wait);
+int smp_call_function(smp_call_func_t func, void *info, int wait);
 void smp_call_function_many(const struct cpumask *mask,
-			    void (*func)(void *info), void *info, bool wait);
+			    smp_call_func_t func, void *info, bool wait);
 
 void __smp_call_function_single(int cpuid, struct call_single_data *data,
 				int wait);
 
 int smp_call_function_any(const struct cpumask *mask,
-			  void (*func)(void *info), void *info, int wait);
+			  smp_call_func_t func, void *info, int wait);
 
 /*
  * Generic and arch helpers
@@ -94,7 +95,7 @@ void ipi_call_unlock_irq(void);
 /*
  * Call a function on all processors
  */
-int on_each_cpu(void (*func) (void *info), void *info, int wait);
+int on_each_cpu(smp_call_func_t func, void *info, int wait);
 
 #define MSG_ALL_BUT_SELF	0x8000	/* Assume <32768 CPU's */
 #define MSG_ALL			0x8001
@@ -122,7 +123,7 @@ static inline void smp_send_stop(void) { }
  *	These macros fold the SMP functionality into a single CPU system
  */
 #define raw_smp_processor_id()			0
-static inline int up_smp_call_function(void (*func)(void *), void *info)
+static inline int up_smp_call_function(smp_call_func_t func, void *info)
 {
 	return 0;
 }
@@ -143,7 +144,7 @@ static inline void smp_send_reschedule(int cpu) { }
 static inline void init_call_single_data(void) { }
 
 static inline int
-smp_call_function_any(const struct cpumask *mask, void (*func)(void *info),
+smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
 		      void *info, int wait)
 {
 	return smp_call_function_single(0, func, info, wait);

commit cfd8d6c0ed89ba387609419e3d8d4c6b92a5d446
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Fri Mar 5 13:42:45 2010 -0800

    smp: fix documentation in include/linux/smp.h
    
    smp: Fix documentation.
    
    Fix documentation in include/linux/smp.h: smp_processor_id()
    
    Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 7a0570e6a596..cfa2d20e35f1 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -154,7 +154,7 @@ smp_call_function_any(const struct cpumask *mask, void (*func)(void *info),
 /*
  * smp_processor_id(): get the current CPU ID.
  *
- * if DEBUG_PREEMPT is enabled the we check whether it is
+ * if DEBUG_PREEMPT is enabled then we check whether it is
  * used in a preemption-safe way. (smp_processor_id() is safe
  * if it's used in a preemption-off critical section, or in
  * a thread that is bound to the current CPU.)

commit 2ea6dec4a22a6f66f6633876212fd4d195cf8277
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 17 14:27:27 2009 -0800

    generic-ipi: Add smp_call_function_any()
    
    Andrew points out that acpi-cpufreq uses cpumask_any, when it really
    would prefer to use the same CPU if possible (to avoid an IPI).  In
    general, this seems a good idea to offer.
    
    [ tglx: Documented selection preference and Inlined the UP case to
            avoid the copy of smp_call_function_single() and the extra
            EXPORT ]
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Zhao Yakui <yakui.zhao@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 39c64bae776d..7a0570e6a596 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -76,6 +76,9 @@ void smp_call_function_many(const struct cpumask *mask,
 void __smp_call_function_single(int cpuid, struct call_single_data *data,
 				int wait);
 
+int smp_call_function_any(const struct cpumask *mask,
+			  void (*func)(void *info), void *info, int wait);
+
 /*
  * Generic and arch helpers
  */
@@ -137,9 +140,15 @@ static inline void smp_send_reschedule(int cpu) { }
 #define smp_prepare_boot_cpu()			do {} while (0)
 #define smp_call_function_many(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
-static inline void init_call_single_data(void)
+static inline void init_call_single_data(void) { }
+
+static inline int
+smp_call_function_any(const struct cpumask *mask, void (*func)(void *info),
+		      void *info, int wait)
 {
+	return smp_call_function_single(0, func, info, wait);
 }
+
 #endif /* !SMP */
 
 /*

commit fe71a3c7dc8cfe0f239c04b4fc6501f4aa56aa0a
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:40 2009 -0600

    cpumask: remove the deprecated smp_call_function_mask()
    
    Everyone is now using smp_call_function_many().
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9e3d8af09207..39c64bae776d 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -73,15 +73,6 @@ int smp_call_function(void(*func)(void *info), void *info, int wait);
 void smp_call_function_many(const struct cpumask *mask,
 			    void (*func)(void *info), void *info, bool wait);
 
-/* Deprecated: Use smp_call_function_many which takes a pointer to the mask. */
-static inline int
-smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
-		       int wait)
-{
-	smp_call_function_many(&mask, func, info, wait);
-	return 0;
-}
-
 void __smp_call_function_single(int cpuid, struct call_single_data *data,
 				int wait);
 
@@ -144,8 +135,6 @@ static inline int up_smp_call_function(void (*func)(void *), void *info)
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
-#define smp_call_function_mask(mask, func, info, wait) \
-			(up_smp_call_function(func, info))
 #define smp_call_function_many(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
 static inline void init_call_single_data(void)

commit 8b0b1db0133e4218a9b45c09e53793c039edebe1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 16 15:33:39 2009 -0700

    remove put_cpu_no_resched()
    
    put_cpu_no_resched() is an optimization of put_cpu() which unfortunately
    can cause high latencies.
    
    The nfs iostats code uses put_cpu_no_resched() in a code sequence where a
    reschedule request caused by an interrupt between the get_cpu() and the
    put_cpu_no_resched() can delay the reschedule for at least HZ.
    
    The other users of put_cpu_no_resched() optimize correctly in interrupt
    code, but there is no real harm in using the put_cpu() function which is
    an alias for preempt_enable().  The extra check of the preemmpt count is
    not as critical as the potential source of missing a reschedule.
    
    Debugged in the preempt-rt tree and verified in mainline.
    
    Impact: remove a high latency source
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: "J. Bruce Fields" <bfields@fieldses.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index a69db820eed6..9e3d8af09207 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -177,7 +177,6 @@ static inline void init_call_single_data(void)
 
 #define get_cpu()		({ preempt_disable(); smp_processor_id(); })
 #define put_cpu()		preempt_enable()
-#define put_cpu_no_resched()	preempt_enable_no_resched()
 
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the

commit d1dedb52acd98bd5e13e1ff4c4d045d58bbd16fe
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 11:14:06 2009 +0100

    panic, smp: provide smp_send_stop() wrapper on UP too
    
    Impact: cleanup, no code changed
    
    Remove an ugly #ifdef CONFIG_SMP from panic(), by providing
    an smp_send_stop() wrapper on UP too.
    
    LKML-Reference: <49B91A7E.76E4.0078.0@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 2d3bcb6b37ff..a69db820eed6 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -38,7 +38,7 @@ int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
 /*
  * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
  * (defined in asm header):
- */ 
+ */
 
 /*
  * stops all CPUs but the current one:
@@ -122,6 +122,8 @@ extern unsigned int setup_max_cpus;
 
 #else /* !SMP */
 
+static inline void smp_send_stop(void) { }
+
 /*
  *	These macros fold the SMP functionality into a single CPU system
  */

commit cd80a8142efa3468c2cd9fb52845f334c3220d54
Merge: 641cd4cfcdc7 a98fe7f3425c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 11:05:58 2009 +0100

    Merge branch 'x86/core' into core/ipi

commit 6e2756376c706e4da3454a272947983f92e80a7e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 25 13:59:48 2009 +0100

    generic-ipi: remove CSD_FLAG_WAIT
    
    Oleg noticed that we don't strictly need CSD_FLAG_WAIT, rework
    the code so that we can use CSD_FLAG_LOCK for both purposes.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 715196b09d67..00866d7fdf34 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -82,7 +82,8 @@ smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
 	return 0;
 }
 
-void __smp_call_function_single(int cpuid, struct call_single_data *data);
+void __smp_call_function_single(int cpuid, struct call_single_data *data,
+				int wait);
 
 /*
  * Generic and arch helpers

commit a146649bc19d5eba4f5bfac6720c5f252d517a71
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 14:09:06 2009 +0100

    smp, generic: introduce arch_disable_smp_support(), build fix
    
    This function should be provided on UP too.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index d41a3a865fe3..bbacb7baa446 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -66,12 +66,6 @@ extern int __cpu_up(unsigned int cpunum);
  */
 extern void smp_cpus_done(unsigned int max_cpus);
 
-/*
- * Callback to arch code if there's nosmp or maxcpus=0 on the
- * boot command line:
- */
-extern void arch_disable_smp_support(void);
-
 /*
  * Call a function on all other processors
  */
@@ -182,6 +176,12 @@ static inline void init_call_single_data(void)
 #define put_cpu()		preempt_enable()
 #define put_cpu_no_resched()	preempt_enable_no_resched()
 
+/*
+ * Callback to arch code if there's nosmp or maxcpus=0 on the
+ * boot command line:
+ */
+extern void arch_disable_smp_support(void);
+
 void smp_setup_processor_id(void);
 
 #endif /* __LINUX_SMP_H */

commit 65a4e574d2382d83f71b30ea92f86d2e40a6ef8d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 03:36:17 2009 +0100

    smp, generic: introduce arch_disable_smp_support() instead of disable_ioapic_setup()
    
    Impact: cleanup
    
    disable_ioapic_setup() in init/main.c is ugly as the function is
    x86-specific. The #ifdef inline prototype there is ugly too.
    
    Replace it with a generic arch_disable_smp_support() function - which
    has a weak alias for non-x86 architectures and for non-ioapic x86 builds.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 715196b09d67..d41a3a865fe3 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -66,6 +66,12 @@ extern int __cpu_up(unsigned int cpunum);
  */
 extern void smp_cpus_done(unsigned int max_cpus);
 
+/*
+ * Callback to arch code if there's nosmp or maxcpus=0 on the
+ * boot command line:
+ */
+extern void arch_disable_smp_support(void);
+
 /*
  * Call a function on all other processors
  */

commit 53ce3d9564908794ae7dd32969089b57df5fc098
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Jan 9 12:27:08 2009 -0800

    smp_call_function_single(): be slightly less stupid
    
    If you do
    
            smp_call_function_single(expression-with-side-effects, ...)
    
    then expression-with-side-effects never gets evaluated on UP builds.
    
    As always, implementing it in C is the correct thing to do.
    
    While we're there, uninline it for size and possible header dependency
    reasons.
    
    And create a new kernel/up.c, as a place in which to put
    uniprocessor-specific code and storage.  It should mirror kernel/smp.c.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index b82466968101..715196b09d67 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -24,6 +24,9 @@ struct call_single_data {
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 
+int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
+				int wait);
+
 #ifdef CONFIG_SMP
 
 #include <linux/preempt.h>
@@ -79,8 +82,6 @@ smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
 	return 0;
 }
 
-int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
-				int wait);
 void __smp_call_function_single(int cpuid, struct call_single_data *data);
 
 /*
@@ -140,14 +141,6 @@ static inline int up_smp_call_function(void (*func)(void *), void *info)
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
-#define smp_call_function_single(cpuid, func, info, wait) \
-({ \
-	WARN_ON(cpuid != 0);	\
-	local_irq_disable();	\
-	(func)(info);		\
-	local_irq_enable();	\
-	0;			\
-})
 #define smp_call_function_mask(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
 #define smp_call_function_many(mask, func, info, wait) \

commit 54b11e6d57a10aa9d0009efd93873e17bffd5d30
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Dec 30 09:05:16 2008 +1030

    cpumask: smp_call_function_many()
    
    Impact: Implementation change to remove cpumask_t from stack.
    
    Actually change smp_call_function_mask() to smp_call_function_many().
    We avoid cpumasks on the stack in this version.
    
    (S390 has its own version, but that's going away apparently).
    
    We have to do some dancing to figure out if 0 or 1 other cpus are in
    the mask supplied and the online mask without allocating a tmp
    cpumask.  It's still fairly cheap.
    
    We allocate the cpumask at the end of the call_function_data
    structure: if allocation fails we fallback to smp_call_function_single
    rather than using the baroque quiescing code (which needs a cpumask on
    stack).
    
    (Thanks to Hiroshi Shimamoto for spotting several bugs in previous versions!)
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: npiggin@suse.de
    Cc: axboe@kernel.dk

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 2f85f3b04bc4..b82466968101 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -67,15 +67,16 @@ extern void smp_cpus_done(unsigned int max_cpus);
  * Call a function on all other processors
  */
 int smp_call_function(void(*func)(void *info), void *info, int wait);
-/* Deprecated: use smp_call_function_many() which uses a cpumask ptr. */
-int smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
-				int wait);
+void smp_call_function_many(const struct cpumask *mask,
+			    void (*func)(void *info), void *info, bool wait);
 
-static inline void smp_call_function_many(const struct cpumask *mask,
-					  void (*func)(void *info), void *info,
-					  int wait)
+/* Deprecated: Use smp_call_function_many which takes a pointer to the mask. */
+static inline int
+smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
+		       int wait)
 {
-	smp_call_function_mask(*mask, func, info, wait);
+	smp_call_function_many(&mask, func, info, wait);
+	return 0;
 }
 
 int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,

commit 33edcf133ba93ecba2e4b6472e97b689895d805c
Merge: be4d638c1597 3c92ec8ae91e
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Dec 30 08:02:35 2008 +1030

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6

commit e057d7aea9d8f2a46cd440d8bfb72245d4e72d79
Author: Mike Travis <travis@sgi.com>
Date:   Mon Dec 15 20:26:48 2008 -0800

    cpumask: add sysfs displays for configured and disabled cpu maps
    
    Impact: add new sysfs files.
    
    Add sysfs files "kernel_max" and "offline" to display the max CPU index
    allowed (NR_CPUS-1), and the map of cpus that are offline.
    
    Cpus can be offlined via HOTPLUG, disabled by the BIOS ACPI tables, or
    if they exceed the number of cpus allowed by the NR_CPUS config option,
    or the "maxcpus=NUM" kernel start parameter.
    
    The "possible_cpus=NUM" parameter can also extend the number of possible
    cpus allowed, in which case the cpus not present at startup will be
    in the offline state.  (These cpus can be HOTPLUGGED ON after system
    startup [pending a follow-on patch to provide the capability via the
    /sys/devices/sys/cpu/cpuN/online mechanism to bring them online.])
    
    By design, the "offlined cpus > possible cpus" display will always
    use the following formats:
    
      * all possible cpus online:   "x$"    or "x-y$"
      * some possible cpus offline: ".*,x$" or ".*,x-y$"
    
    where:
      x == number of possible cpus (nr_cpu_ids); and
      y == number of cpus >= NR_CPUS or maxcpus (if y > x).
    
    One use of this feature is for distros to select (or configure) the
    appropriate kernel to install for the resident system.
    
    Notes:
      * cpus offlined <= possible cpus will be printed for all architectures.
      * cpus offlined >  possible cpus will only be printed for arches that
            set 'total_cpus' [X86 only in this patch].
    
    Based on tip/cpus4096 + .../rusty/linux-2.6-for-ingo.git/master +
             x86-only-patches sent 12/15.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3f9a60043a97..0d5770c2e43a 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -21,6 +21,9 @@ struct call_single_data {
 	u16 priv;
 };
 
+/* total number of cpus in this system (may exceed NR_CPUS) */
+extern unsigned int total_cpus;
+
 #ifdef CONFIG_SMP
 
 #include <linux/preempt.h>

commit d2ff911882b6bc693d86ca9566daac70aacbb2b3
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Dec 15 19:04:35 2008 +1030

    Define smp_call_function_many for UP
    
    Otherwise those using it in transition patches (eg. kvm) can't compile
    with CONFIG_SMP=n:
    
    arch/x86/kvm/../../../virt/kvm/kvm_main.c: In function 'make_all_cpus_request':
    arch/x86/kvm/../../../virt/kvm/kvm_main.c:380: error: implicit declaration of function 'smp_call_function_many'
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3f9a60043a97..6e7ba16ff454 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -146,6 +146,8 @@ static inline void smp_send_reschedule(int cpu) { }
 })
 #define smp_call_function_mask(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
+#define smp_call_function_many(mask, func, info, wait) \
+			(up_smp_call_function(func, info))
 static inline void init_call_single_data(void)
 {
 }

commit 2d3854a37e8b767a51aba38ed6d22817b0631e33
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Wed Nov 5 13:39:10 2008 +1100

    cpumask: introduce new API, without changing anything
    
    Impact: introduce new APIs
    
    We want to deprecate cpumasks on the stack, as we are headed for
    gynormous numbers of CPUs.  Eventually, we want to head towards an
    undefined 'struct cpumask' so they can never be declared on stack.
    
    1) New cpumask functions which take pointers instead of copies.
       (cpus_* -> cpumask_*)
    
    2) Several new helpers to reduce requirements for temporary cpumasks
       (cpumask_first_and, cpumask_next_and, cpumask_any_and)
    
    3) Helpers for declaring cpumasks on or offstack for large NR_CPUS
       (cpumask_var_t, alloc_cpumask_var and free_cpumask_var)
    
    4) 'struct cpumask' for explicitness and to mark new-style code.
    
    5) Make iterator functions stop at nr_cpu_ids (a runtime constant),
       not NR_CPUS for time efficiency and for smaller dynamic allocations
       in future.
    
    6) cpumask_copy() so we can allocate less than a full cpumask eventually
       (for alloc_cpumask_var), and so we can eliminate the 'struct cpumask'
       definition eventually.
    
    7) work_on_cpu() helper for doing task on a CPU, rather than saving old
       cpumask for current thread and manipulating it.
    
    8) smp_call_function_many() which is smp_call_function_mask() except
       taking a cpumask pointer.
    
    Note that this patch simply introduces the new functions and leaves
    the obsolescent ones in place.  This is to simplify the transition
    patches.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 2e4d58b26c06..3f9a60043a97 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -64,8 +64,17 @@ extern void smp_cpus_done(unsigned int max_cpus);
  * Call a function on all other processors
  */
 int smp_call_function(void(*func)(void *info), void *info, int wait);
+/* Deprecated: use smp_call_function_many() which uses a cpumask ptr. */
 int smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
 				int wait);
+
+static inline void smp_call_function_many(const struct cpumask *mask,
+					  void (*func)(void *info), void *info,
+					  int wait)
+{
+	smp_call_function_mask(*mask, func, info, wait);
+}
+
 int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
 				int wait);
 void __smp_call_function_single(int cpuid, struct call_single_data *data);

commit 54514a70adefe356afe854e2d3912d46668068e6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 23 22:15:57 2008 -0700

    softirq: Add support for triggering softirq work on softirqs.
    
    This is basically a genericization of Jens Axboe's block layer
    remote softirq changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 66484d4a8459..2e4d58b26c06 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -7,6 +7,7 @@
  */
 
 #include <linux/errno.h>
+#include <linux/types.h>
 #include <linux/list.h>
 #include <linux/cpumask.h>
 
@@ -16,7 +17,8 @@ struct call_single_data {
 	struct list_head list;
 	void (*func) (void *info);
 	void *info;
-	unsigned int flags;
+	u16 flags;
+	u16 priv;
 };
 
 #ifdef CONFIG_SMP

commit 7babe8db99d305340cf4828ce1f5a1481d5622ef
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Fri Jul 25 19:45:11 2008 -0700

    Full conversion to early_initcall() interface, remove old interface
    
    A previous patch added the early_initcall(), to allow a cleaner hooking of
    pre-SMP initcalls.  Now we remove the older interface, converting all
    existing users to the new one.
    
    [akpm@linux-foundation.org: cleanups]
    [akpm@linux-foundation.org: build fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 48262f86c969..66484d4a8459 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -74,15 +74,10 @@ void __smp_call_function_single(int cpuid, struct call_single_data *data);
 #ifdef CONFIG_USE_GENERIC_SMP_HELPERS
 void generic_smp_call_function_single_interrupt(void);
 void generic_smp_call_function_interrupt(void);
-void init_call_single_data(void);
 void ipi_call_lock(void);
 void ipi_call_unlock(void);
 void ipi_call_lock_irq(void);
 void ipi_call_unlock_irq(void);
-#else
-static inline void init_call_single_data(void)
-{
-}
 #endif
 
 /*

commit ba8dd03ac09f51a69c154b8cb508b701d713a2cd
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jul 4 11:26:40 2008 +0200

    generic-ipi: fix s390 build bug
    
    forgot to remove #include <linux/spinlock.h> from linux/smp.h while
    fixing the original s390 build bug.
    
    Patch below fixes this build bug caused by header inclusion dependencies:
    
      CC      kernel/timer.o
    In file included from include/linux/spinlock.h:87,
                     from include/linux/smp.h:11,
                     from include/linux/kernel_stat.h:4,
                     from kernel/timer.c:22:
    include/asm/spinlock.h: In function '__raw_spin_lock':
    include/asm/spinlock.h:69: error: implicit declaration of function 'smp_processor_id'
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 55261101d09a..48262f86c969 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -8,7 +8,6 @@
 
 #include <linux/errno.h>
 #include <linux/list.h>
-#include <linux/spinlock.h>
 #include <linux/cpumask.h>
 
 extern void cpu_idle(void);

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 338cad1b9548..55261101d09a 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -89,7 +89,7 @@ static inline void init_call_single_data(void)
 /*
  * Call a function on all processors
  */
-int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait);
+int on_each_cpu(void (*func) (void *info), void *info, int wait);
 
 #define MSG_ALL_BUT_SELF	0x8000	/* Assume <32768 CPU's */
 #define MSG_ALL			0x8001
@@ -121,7 +121,7 @@ static inline int up_smp_call_function(void (*func)(void *), void *info)
 }
 #define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
-#define on_each_cpu(func,info,retry,wait)	\
+#define on_each_cpu(func,info,wait)		\
 	({					\
 		local_irq_disable();		\
 		func(info);			\

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index eac3e062250f..338cad1b9548 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -62,11 +62,11 @@ extern void smp_cpus_done(unsigned int max_cpus);
 /*
  * Call a function on all other processors
  */
-int smp_call_function(void(*func)(void *info), void *info, int retry, int wait);
+int smp_call_function(void(*func)(void *info), void *info, int wait);
 int smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
 				int wait);
 int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
-				int retry, int wait);
+				int wait);
 void __smp_call_function_single(int cpuid, struct call_single_data *data);
 
 /*
@@ -119,7 +119,7 @@ static inline int up_smp_call_function(void (*func)(void *), void *info)
 {
 	return 0;
 }
-#define smp_call_function(func, info, retry, wait) \
+#define smp_call_function(func, info, wait) \
 			(up_smp_call_function(func, info))
 #define on_each_cpu(func,info,retry,wait)	\
 	({					\
@@ -131,7 +131,7 @@ static inline int up_smp_call_function(void (*func)(void *), void *info)
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
-#define smp_call_function_single(cpuid, func, info, retry, wait) \
+#define smp_call_function_single(cpuid, func, info, wait) \
 ({ \
 	WARN_ON(cpuid != 0);	\
 	local_irq_disable();	\

commit 3d4422332711ef48ef0f132f1fcbfcbd56c7f3d1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 26 11:21:34 2008 +0200

    Add generic helpers for arch IPI function calls
    
    This adds kernel/smp.c which contains helpers for IPI function calls. In
    addition to supporting the existing smp_call_function() in a more efficient
    manner, it also adds a more scalable variant called smp_call_function_single()
    for calling a given function on a single CPU only.
    
    The core of this is based on the x86-64 patch from Nick Piggin, lots of
    changes since then. "Alan D. Brunelle" <Alan.Brunelle@hp.com> has
    contributed lots of fixes and suggestions as well. Also thanks to
    Paul E. McKenney <paulmck@linux.vnet.ibm.com> for reviewing RCU usage
    and getting rid of the data allocation fallback deadlock.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 55232ccf9cfd..eac3e062250f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -7,9 +7,19 @@
  */
 
 #include <linux/errno.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/cpumask.h>
 
 extern void cpu_idle(void);
 
+struct call_single_data {
+	struct list_head list;
+	void (*func) (void *info);
+	void *info;
+	unsigned int flags;
+};
+
 #ifdef CONFIG_SMP
 
 #include <linux/preempt.h>
@@ -53,9 +63,28 @@ extern void smp_cpus_done(unsigned int max_cpus);
  * Call a function on all other processors
  */
 int smp_call_function(void(*func)(void *info), void *info, int retry, int wait);
-
+int smp_call_function_mask(cpumask_t mask, void(*func)(void *info), void *info,
+				int wait);
 int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
 				int retry, int wait);
+void __smp_call_function_single(int cpuid, struct call_single_data *data);
+
+/*
+ * Generic and arch helpers
+ */
+#ifdef CONFIG_USE_GENERIC_SMP_HELPERS
+void generic_smp_call_function_single_interrupt(void);
+void generic_smp_call_function_interrupt(void);
+void init_call_single_data(void);
+void ipi_call_lock(void);
+void ipi_call_unlock(void);
+void ipi_call_lock_irq(void);
+void ipi_call_unlock_irq(void);
+#else
+static inline void init_call_single_data(void)
+{
+}
+#endif
 
 /*
  * Call a function on all processors
@@ -112,7 +141,9 @@ static inline void smp_send_reschedule(int cpu) { }
 })
 #define smp_call_function_mask(mask, func, info, wait) \
 			(up_smp_call_function(func, info))
-
+static inline void init_call_single_data(void)
+{
+}
 #endif /* !SMP */
 
 /*

commit ca74a6f84e68b44867022f4a4f3ec17c087c864e
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 30 13:33:17 2008 +0100

    x86: optimize lock prefix switching to run less frequently
    
    On VMs implemented using JITs that cache translated code changing the lock
    prefixes is a quite costly operation that forces the JIT to throw away and
    retranslate a lot of code.
    
    Previously a SMP kernel would rewrite the locks once for each CPU which
    is quite unnecessary. This patch changes the code to never switch at boot in
     the normal case (SMP kernel booting with >1 CPU) or only once for SMP kernel
    on UP.
    
    This makes a significant difference in boot up performance on AMD SimNow!
    Also I expect it to be a little faster on native systems too because a smp
    switch does a lot of text_poke()s which each synchronize the pipeline.
    
    v1->v2: Rename max_cpus
    v1->v2: Fix off by one in UP check (Thomas Gleixner)
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c25e66bcecf3..55232ccf9cfd 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -78,6 +78,8 @@ int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait);
  */
 void smp_prepare_boot_cpu(void);
 
+extern unsigned int setup_max_cpus;
+
 #else /* !SMP */
 
 /*

commit a5fbb6d1064be885d2a6b82f625186753cf74848
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Nov 9 22:39:38 2007 +0100

    KVM: fix !SMP build error
    
    fix a !SMP build error:
    
    drivers/kvm/kvm_main.c: In function 'kvm_flush_remote_tlbs':
    drivers/kvm/kvm_main.c:220: error: implicit declaration of function 'smp_call_function_mask'
    
    (and also avoid unused function warning related to up_smp_call_function()
    not making use of the 'func' parameter.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 259a13c3bd98..c25e66bcecf3 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -84,11 +84,12 @@ void smp_prepare_boot_cpu(void);
  *	These macros fold the SMP functionality into a single CPU system
  */
 #define raw_smp_processor_id()			0
-static inline int up_smp_call_function(void)
+static inline int up_smp_call_function(void (*func)(void *), void *info)
 {
 	return 0;
 }
-#define smp_call_function(func,info,retry,wait)	(up_smp_call_function())
+#define smp_call_function(func, info, retry, wait) \
+			(up_smp_call_function(func, info))
 #define on_each_cpu(func,info,retry,wait)	\
 	({					\
 		local_irq_disable();		\
@@ -107,6 +108,8 @@ static inline void smp_send_reschedule(int cpu) { }
 	local_irq_enable();	\
 	0;			\
 })
+#define smp_call_function_mask(mask, func, info, wait) \
+			(up_smp_call_function(func, info))
 
 #endif /* !SMP */
 

commit 8dfd588c3180b7403c402b4545164ee4543f8f86
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Tue Jul 17 22:29:46 2007 +0100

    smp_call_function_single() should be a macro on UP
    
    ... or we end up with header include order problems from hell.
    
    E.g. on m68k this is 100% fatal - local_irq_enable() there
    wants preempt_count(), which wants task_struct fields, which
    we won't have when we are in smp.h pulled from sched.h.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 8039daced688..259a13c3bd98 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -7,7 +7,6 @@
  */
 
 #include <linux/errno.h>
-#include <asm/system.h>
 
 extern void cpu_idle(void);
 
@@ -100,15 +99,14 @@ static inline int up_smp_call_function(void)
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
-static inline int smp_call_function_single(int cpuid, void (*func) (void *info),
-					   void *info, int retry, int wait)
-{
-	WARN_ON(cpuid != 0);
-	local_irq_disable();
-	func(info);
-	local_irq_enable();
-	return 0;
-}
+#define smp_call_function_single(cpuid, func, info, retry, wait) \
+({ \
+	WARN_ON(cpuid != 0);	\
+	local_irq_disable();	\
+	(func)(info);		\
+	local_irq_enable();	\
+	0;			\
+})
 
 #endif /* !SMP */
 

commit a52b1752c077cb919b71167c54968a0b91673281
Author: Avi Kivity <avi@qumranet.com>
Date:   Mon Jul 9 17:11:49 2007 +0300

    SMP: Allow smp_call_function_single() to current cpu
    
    This removes the requirement for callers to get_cpu() to check in simple
    cases.  This patch is for !CONFIG_SMP.
    
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 96ac21f8dd73..8039daced688 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -7,6 +7,7 @@
  */
 
 #include <linux/errno.h>
+#include <asm/system.h>
 
 extern void cpu_idle(void);
 
@@ -102,7 +103,11 @@ static inline void smp_send_reschedule(int cpu) { }
 static inline int smp_call_function_single(int cpuid, void (*func) (void *info),
 					   void *info, int retry, int wait)
 {
-	return -EBUSY;
+	WARN_ON(cpuid != 0);
+	local_irq_disable();
+	func(info);
+	local_irq_enable();
+	return 0;
 }
 
 #endif /* !SMP */

commit 79974a0e4c6be6e9a3717b4c5a5d5c44c36b1653
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed May 16 22:11:09 2007 -0700

    Let smp_call_function_single return -EBUSY on UP
    
    All architectures that have an implementation of smp_call_function_single
    let it return -EBUSY if it is asked to execute func on the current cpu.
    (akpm: except for x86_64).  Therefore the UP version must always return
    -EBUSY.
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 3f70149eabbb..96ac21f8dd73 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -6,6 +6,7 @@
  *		Alan Cox. <alan@redhat.com>
  */
 
+#include <linux/errno.h>
 
 extern void cpu_idle(void);
 
@@ -99,11 +100,9 @@ static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
 static inline int smp_call_function_single(int cpuid, void (*func) (void *info),
-				void *info, int retry, int wait)
+					   void *info, int retry, int wait)
 {
-	/* Disable interrupts here? */
-	func(info);
-	return 0;
+	return -EBUSY;
 }
 
 #endif /* !SMP */

commit 2f4dfe206a2fc07099dfad77a8ea2f4b4ae2140f
Author: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
Date:   Wed May 9 02:33:25 2007 -0700

    Remove hardcoding of hard_smp_processor_id on UP systems
    
    With the advent of kdump, the assumption that the boot CPU when booting an UP
    kernel is always the CPU with a particular hardware ID (often 0) (usually
    referred to as BSP on some architectures) is not valid anymore.  The reason
    being that the dump capture kernel boots on the crashed CPU (the CPU that
    invoked crash_kexec), which may be or may not be that particular CPU.
    
    Move definition of hard_smp_processor_id for the UP case to
    architecture-specific code ("asm/smp.h") where it belongs, so that each
    architecture can provide its own implementation.
    
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Acked-by: Andi Kleen <ak@suse.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 7ba23ec8211b..3f70149eabbb 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -83,7 +83,6 @@ void smp_prepare_boot_cpu(void);
  *	These macros fold the SMP functionality into a single CPU system
  */
 #define raw_smp_processor_id()			0
-#define hard_smp_processor_id()			0
 static inline int up_smp_call_function(void)
 {
 	return 0;

commit 83df8db9e62129975fab6d800fb381faf0dfee74
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Wed Dec 6 20:38:14 2006 -0800

    [PATCH] declare smp_call_function_single in generic code
    
    smp_call_function_single() needs to be visible in non-SMP builds, to fix:
    
    arch/x86_64/kernel/vsyscall.c:283: warning: implicit declaration of function 'smp_call_function_single'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 51649987f691..7ba23ec8211b 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -99,6 +99,13 @@ static inline int up_smp_call_function(void)
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)
+static inline int smp_call_function_single(int cpuid, void (*func) (void *info),
+				void *info, int retry, int wait)
+{
+	/* Disable interrupts here? */
+	func(info);
+	return 0;
+}
 
 #endif /* !SMP */
 

commit a3bc0dbc81d36fd38991b4373f6de8e1a507605a
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Sep 25 23:32:33 2006 -0700

    [PATCH] smp_call_function_single() cleanup
    
    If we're going to implement smp_call_function_single() on three architecture
    with the same prototype then it should have a declaration in a
    non-arch-specific header file.
    
    Move it into <linux/smp.h>.
    
    Cc: Stephane Eranian <eranian@hpl.hp.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 837e8bce1349..51649987f691 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -53,6 +53,9 @@ extern void smp_cpus_done(unsigned int max_cpus);
  */
 int smp_call_function(void(*func)(void *info), void *info, int retry, int wait);
 
+int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
+				int retry, int wait);
+
 /*
  * Call a function on all processors
  */

commit 033ab7f8e5c655f49ec8039930b2efd412abbbd7
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Jun 30 01:55:50 2006 -0700

    [PATCH] add smp_setup_processor_id()
    
    Presently, smp_processor_id() isn't necessarily set up until setup_arch().
    But it's used in boot_cpu_init() and printk() and perhaps in other places,
    prior to setup_arch() being called.
    
    So provide a new smp_setup_processor_id() which is called before anything
    else, wire it up for Voyager (which boots on a CPU other than #0, and broke).
    
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index c93c3fe4308c..837e8bce1349 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -125,4 +125,6 @@ static inline void smp_send_reschedule(int cpu) { }
 #define put_cpu()		preempt_enable()
 #define put_cpu_no_resched()	preempt_enable_no_resched()
 
+void smp_setup_processor_id(void);
+
 #endif /* __LINUX_SMP_H */

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index e2fa3ab4afc5..c93c3fe4308c 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -6,7 +6,6 @@
  *		Alan Cox. <alan@redhat.com>
  */
 
-#include <linux/config.h>
 
 extern void cpu_idle(void);
 

commit 3c30b06df404c8892c225a99ecfd3f02789c0513
Author: Con Kolivas <kernel@kolivas.org>
Date:   Sun Mar 26 01:37:19 2006 -0800

    [PATCH] cleanup smp_call_function UP build
    
    net/core/flow.c: In function 'flow_cache_flush':
    net/core/flow.c:299: warning: statement with no effect
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index d699a16b0cb2..e2fa3ab4afc5 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -82,7 +82,11 @@ void smp_prepare_boot_cpu(void);
  */
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
-#define smp_call_function(func,info,retry,wait)	({ 0; })
+static inline int up_smp_call_function(void)
+{
+	return 0;
+}
+#define smp_call_function(func,info,retry,wait)	(up_smp_call_function())
 #define on_each_cpu(func,info,retry,wait)	\
 	({					\
 		local_irq_disable();		\

commit 78eef01b0fae087c5fadbd85dd4fe2918c3a015f
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Mar 22 00:08:16 2006 -0800

    [PATCH] on_each_cpu(): disable local interrupts
    
    When on_each_cpu() runs the callback on other CPUs, it runs with local
    interrupts disabled.  So we should run the function with local interrupts
    disabled on this CPU, too.
    
    And do the same for UP, so the callback is run in the same environment on both
    UP and SMP.  (strictly it should do preempt_disable() too, but I think
    local_irq_disable is sufficiently equivalent).
    
    Also uninlines on_each_cpu().  softirq.c was the most appropriate file I could
    find, but it doesn't seem to justify creating a new file.
    
    Oh, and fix up that comment over (under?) x86's smp_call_function().  It
    drives me nuts.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 44153fdf73fc..d699a16b0cb2 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -52,23 +52,12 @@ extern void smp_cpus_done(unsigned int max_cpus);
 /*
  * Call a function on all other processors
  */
-extern int smp_call_function (void (*func) (void *info), void *info,
-			      int retry, int wait);
+int smp_call_function(void(*func)(void *info), void *info, int retry, int wait);
 
 /*
  * Call a function on all processors
  */
-static inline int on_each_cpu(void (*func) (void *info), void *info,
-			      int retry, int wait)
-{
-	int ret = 0;
-
-	preempt_disable();
-	ret = smp_call_function(func, info, retry, wait);
-	func(info);
-	preempt_enable();
-	return ret;
-}
+int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait);
 
 #define MSG_ALL_BUT_SELF	0x8000	/* Assume <32768 CPU's */
 #define MSG_ALL			0x8001
@@ -94,7 +83,13 @@ void smp_prepare_boot_cpu(void);
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
 #define smp_call_function(func,info,retry,wait)	({ 0; })
-#define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
+#define on_each_cpu(func,info,retry,wait)	\
+	({					\
+		local_irq_disable();		\
+		func(info);			\
+		local_irq_enable();		\
+		0;				\
+	})
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
 #define smp_prepare_boot_cpu()			do {} while (0)

commit 1b8623545b42c03eb92e51b28c84acf4b8ba00a3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Dec 15 01:07:03 2005 -0500

    [PATCH] remove bogus asm/bug.h includes.
    
    A bunch of asm/bug.h includes are both not needed (since it will get
    pulled anyway) and bogus (since they are done too early).  Removed.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9dfa3ee769ae..44153fdf73fc 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -17,7 +17,6 @@ extern void cpu_idle(void);
 #include <linux/compiler.h>
 #include <linux/thread_info.h>
 #include <asm/smp.h>
-#include <asm/bug.h>
 
 /*
  * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.

commit 2d0ebb36038c0626cde662a3b06da9787cfb68c3
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Wed Nov 23 08:44:05 2005 -0800

    Revert "[NET]: Shut up warnings in net/core/flow.c"
    
    This reverts commit af2b4079ab154bd12e8c12b02db5f31b31babe63
    
    Changing the #define to an inline function breaks on non-SMP builds,
    since wuite a few places in the kernel do not implement the ipi handler
    when compiling for UP.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index b6069c8e1f04..9dfa3ee769ae 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -94,13 +94,7 @@ void smp_prepare_boot_cpu(void);
  */
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
-
-static inline int smp_call_function(void (*func) (void *info), void *info,
-				    int retry, int wait)
-{
-	return 0;
-}
-
+#define smp_call_function(func,info,retry,wait)	({ 0; })
 #define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1

commit af2b4079ab154bd12e8c12b02db5f31b31babe63
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 22 14:38:04 2005 -0800

    [NET]: Shut up warnings in net/core/flow.c
    
    Not really a network problem, more a !SMP issue.
    
    net/core/flow.c:295: warning: statement with no effect
    
    flow.c:295:        smp_call_function(flow_cache_flush_per_cpu, &info, 1, 0);
    
    Fix this by converting the macro to an inline function, which
    also increases the typechecking for !SMP builds.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9dfa3ee769ae..b6069c8e1f04 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -94,7 +94,13 @@ void smp_prepare_boot_cpu(void);
  */
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
-#define smp_call_function(func,info,retry,wait)	({ 0; })
+
+static inline int smp_call_function(void (*func) (void *info), void *info,
+				    int retry, int wait)
+{
+	return 0;
+}
+
 #define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
 static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1

commit 2ac6608c41f8c45371ea9dddae7f99bc2c15d5cf
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Jul 28 10:34:47 2005 -0700

    Revert broken "statement with no effect" warning fix
    
    It may shut up gcc, but it also incorrectly changes the semantics of the
    smp_call_function() helpers.
    
    You can fix the warning other ways if you are interested (create another
    inline function that takes no arguments and returns zero), but
    preferably gcc just shouldn't complain about unused return values from
    statement expressions in the first place.

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 22b451d1b93f..9dfa3ee769ae 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -94,23 +94,11 @@ void smp_prepare_boot_cpu(void);
  */
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
-#define num_booting_cpus()			1
-
-static inline int smp_call_function(void (*func) (void *), void *info,
-				    int retry, int wait)
-{
-	return 0;
-}
-
-static inline int on_each_cpu(void (*func) (void *), void *info,
-			      int retry, int wait)
-{
-	func(info);
-	return 0;
-}
-
+#define smp_call_function(func,info,retry,wait)	({ 0; })
+#define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
 static inline void smp_send_reschedule(int cpu) { }
-static inline void smp_prepare_boot_cpu(void) { }
+#define num_booting_cpus()			1
+#define smp_prepare_boot_cpu()			do {} while (0)
 
 #endif /* !SMP */
 

commit 79a8810221ee9ea96c4e5a5817afb88f22ea698c
Author: Richard Henderson <rth@twiddle.net>
Date:   Thu Jul 28 01:07:41 2005 -0700

    [PATCH] alpha: fix "statement with no effect" warnings
    
    Apparently gcc 4.0 complains about "({ 0; });", which leads to -Werror
    breakage in one of the alpha oprofile modules.
    
    One might could argue that this is a gcc bug, in that statement-expressions
    should be considered to be function-like rather than statement-like for the
    purposes of this warning.  But it's just as easy to use an inline function
    in the first place, side-stepping the issue.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index 9dfa3ee769ae..22b451d1b93f 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -94,11 +94,23 @@ void smp_prepare_boot_cpu(void);
  */
 #define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
-#define smp_call_function(func,info,retry,wait)	({ 0; })
-#define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
-static inline void smp_send_reschedule(int cpu) { }
 #define num_booting_cpus()			1
-#define smp_prepare_boot_cpu()			do {} while (0)
+
+static inline int smp_call_function(void (*func) (void *), void *info,
+				    int retry, int wait)
+{
+	return 0;
+}
+
+static inline int on_each_cpu(void (*func) (void *), void *info,
+			      int retry, int wait)
+{
+	func(info);
+	return 0;
+}
+
+static inline void smp_send_reschedule(int cpu) { }
+static inline void smp_prepare_boot_cpu(void) { }
 
 #endif /* !SMP */
 

commit 39c715b71740c4a78ba4769fb54826929bac03cb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 21 17:14:34 2005 -0700

    [PATCH] smp_processor_id() cleanup
    
    This patch implements a number of smp_processor_id() cleanup ideas that
    Arjan van de Ven and I came up with.
    
    The previous __smp_processor_id/_smp_processor_id/smp_processor_id API
    spaghetti was hard to follow both on the implementational and on the
    usage side.
    
    Some of the complexity arose from picking wrong names, some of the
    complexity comes from the fact that not all architectures defined
    __smp_processor_id.
    
    In the new code, there are two externally visible symbols:
    
     - smp_processor_id(): debug variant.
    
     - raw_smp_processor_id(): nondebug variant. Replaces all existing
       uses of _smp_processor_id() and __smp_processor_id(). Defined
       by every SMP architecture in include/asm-*/smp.h.
    
    There is one new internal symbol, dependent on DEBUG_PREEMPT:
    
     - debug_smp_processor_id(): internal debug variant, mapped to
                                 smp_processor_id().
    
    Also, i moved debug_smp_processor_id() from lib/kernel_lock.c into a new
    lib/smp_processor_id.c file.  All related comments got updated and/or
    clarified.
    
    I have build/boot tested the following 8 .config combinations on x86:
    
     {SMP,UP} x {PREEMPT,!PREEMPT} x {DEBUG_PREEMPT,!DEBUG_PREEMPT}
    
    I have also build/boot tested x64 on UP/PREEMPT/DEBUG_PREEMPT.  (Other
    architectures are untested, but should work just fine.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/smp.h b/include/linux/smp.h
index dcf1db3b35d3..9dfa3ee769ae 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -92,10 +92,7 @@ void smp_prepare_boot_cpu(void);
 /*
  *	These macros fold the SMP functionality into a single CPU system
  */
-
-#if !defined(__smp_processor_id) || !defined(CONFIG_PREEMPT)
-# define smp_processor_id()			0
-#endif
+#define raw_smp_processor_id()			0
 #define hard_smp_processor_id()			0
 #define smp_call_function(func,info,retry,wait)	({ 0; })
 #define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
@@ -106,30 +103,25 @@ static inline void smp_send_reschedule(int cpu) { }
 #endif /* !SMP */
 
 /*
- * DEBUG_PREEMPT support: check whether smp_processor_id() is being
- * used in a preemption-safe way.
+ * smp_processor_id(): get the current CPU ID.
  *
- * An architecture has to enable this debugging code explicitly.
- * It can do so by renaming the smp_processor_id() macro to
- * __smp_processor_id().  This should only be done after some minimal
- * testing, because usually there are a number of false positives
- * that an architecture will trigger.
+ * if DEBUG_PREEMPT is enabled the we check whether it is
+ * used in a preemption-safe way. (smp_processor_id() is safe
+ * if it's used in a preemption-off critical section, or in
+ * a thread that is bound to the current CPU.)
  *
- * To fix a false positive (i.e. smp_processor_id() use that the
- * debugging code reports but which use for some reason is legal),
- * change the smp_processor_id() reference to _smp_processor_id(),
- * which is the nondebug variant.  NOTE: don't use this to hack around
- * real bugs.
+ * NOTE: raw_smp_processor_id() is for internal use only
+ * (smp_processor_id() is the preferred variant), but in rare
+ * instances it might also be used to turn off false positives
+ * (i.e. smp_processor_id() use that the debugging code reports but
+ * which use for some reason is legal). Don't use this to hack around
+ * the warning message, as your code might not work under PREEMPT.
  */
-#ifdef __smp_processor_id
-# if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-   extern unsigned int smp_processor_id(void);
-# else
-#  define smp_processor_id() __smp_processor_id()
-# endif
-# define _smp_processor_id() __smp_processor_id()
+#ifdef CONFIG_DEBUG_PREEMPT
+  extern unsigned int debug_smp_processor_id(void);
+# define smp_processor_id() debug_smp_processor_id()
 #else
-# define _smp_processor_id() smp_processor_id()
+# define smp_processor_id() raw_smp_processor_id()
 #endif
 
 #define get_cpu()		({ preempt_disable(); smp_processor_id(); })

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/smp.h b/include/linux/smp.h
new file mode 100644
index 000000000000..dcf1db3b35d3
--- /dev/null
+++ b/include/linux/smp.h
@@ -0,0 +1,139 @@
+#ifndef __LINUX_SMP_H
+#define __LINUX_SMP_H
+
+/*
+ *	Generic SMP support
+ *		Alan Cox. <alan@redhat.com>
+ */
+
+#include <linux/config.h>
+
+extern void cpu_idle(void);
+
+#ifdef CONFIG_SMP
+
+#include <linux/preempt.h>
+#include <linux/kernel.h>
+#include <linux/compiler.h>
+#include <linux/thread_info.h>
+#include <asm/smp.h>
+#include <asm/bug.h>
+
+/*
+ * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
+ * (defined in asm header):
+ */ 
+
+/*
+ * stops all CPUs but the current one:
+ */
+extern void smp_send_stop(void);
+
+/*
+ * sends a 'reschedule' event to another CPU:
+ */
+extern void smp_send_reschedule(int cpu);
+
+
+/*
+ * Prepare machine for booting other CPUs.
+ */
+extern void smp_prepare_cpus(unsigned int max_cpus);
+
+/*
+ * Bring a CPU up
+ */
+extern int __cpu_up(unsigned int cpunum);
+
+/*
+ * Final polishing of CPUs
+ */
+extern void smp_cpus_done(unsigned int max_cpus);
+
+/*
+ * Call a function on all other processors
+ */
+extern int smp_call_function (void (*func) (void *info), void *info,
+			      int retry, int wait);
+
+/*
+ * Call a function on all processors
+ */
+static inline int on_each_cpu(void (*func) (void *info), void *info,
+			      int retry, int wait)
+{
+	int ret = 0;
+
+	preempt_disable();
+	ret = smp_call_function(func, info, retry, wait);
+	func(info);
+	preempt_enable();
+	return ret;
+}
+
+#define MSG_ALL_BUT_SELF	0x8000	/* Assume <32768 CPU's */
+#define MSG_ALL			0x8001
+
+#define MSG_INVALIDATE_TLB	0x0001	/* Remote processor TLB invalidate */
+#define MSG_STOP_CPU		0x0002	/* Sent to shut down slave CPU's
+					 * when rebooting
+					 */
+#define MSG_RESCHEDULE		0x0003	/* Reschedule request from master CPU*/
+#define MSG_CALL_FUNCTION       0x0004  /* Call function on all other CPUs */
+
+/*
+ * Mark the boot cpu "online" so that it can call console drivers in
+ * printk() and can access its per-cpu storage.
+ */
+void smp_prepare_boot_cpu(void);
+
+#else /* !SMP */
+
+/*
+ *	These macros fold the SMP functionality into a single CPU system
+ */
+
+#if !defined(__smp_processor_id) || !defined(CONFIG_PREEMPT)
+# define smp_processor_id()			0
+#endif
+#define hard_smp_processor_id()			0
+#define smp_call_function(func,info,retry,wait)	({ 0; })
+#define on_each_cpu(func,info,retry,wait)	({ func(info); 0; })
+static inline void smp_send_reschedule(int cpu) { }
+#define num_booting_cpus()			1
+#define smp_prepare_boot_cpu()			do {} while (0)
+
+#endif /* !SMP */
+
+/*
+ * DEBUG_PREEMPT support: check whether smp_processor_id() is being
+ * used in a preemption-safe way.
+ *
+ * An architecture has to enable this debugging code explicitly.
+ * It can do so by renaming the smp_processor_id() macro to
+ * __smp_processor_id().  This should only be done after some minimal
+ * testing, because usually there are a number of false positives
+ * that an architecture will trigger.
+ *
+ * To fix a false positive (i.e. smp_processor_id() use that the
+ * debugging code reports but which use for some reason is legal),
+ * change the smp_processor_id() reference to _smp_processor_id(),
+ * which is the nondebug variant.  NOTE: don't use this to hack around
+ * real bugs.
+ */
+#ifdef __smp_processor_id
+# if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
+   extern unsigned int smp_processor_id(void);
+# else
+#  define smp_processor_id() __smp_processor_id()
+# endif
+# define _smp_processor_id() __smp_processor_id()
+#else
+# define _smp_processor_id() smp_processor_id()
+#endif
+
+#define get_cpu()		({ preempt_disable(); smp_processor_id(); })
+#define put_cpu()		preempt_enable()
+#define put_cpu_no_resched()	preempt_enable_no_resched()
+
+#endif /* __LINUX_SMP_H */
