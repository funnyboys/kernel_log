commit df1e849ae4559544ff00ff5052eefe2479750539
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Wed Nov 27 16:36:45 2019 -0800

    rcu: Enable tick for nohz_full CPUs slow to provide expedited QS
    
    An expedited grace period can be stalled by a nohz_full CPU looping
    in kernel context.  This possibility is currently handled by some
    carefully crafted checks in rcu_read_unlock_special() that enlist help
    from ksoftirqd when permitted by the scheduler.  However, it is exactly
    these checks that require the scheduler avoid holding any of its rq or
    pi locks across rcu_read_unlock() without also having held them across
    the entire RCU read-side critical section.
    
    It would therefore be very nice if expedited grace periods could
    handle nohz_full CPUs looping in kernel context without such checks.
    This commit therefore adds code to the expedited grace period's wait
    and cleanup code that forces the scheduler-clock interrupt on for CPUs
    that fail to quickly supply a quiescent state.  "Quickly" is currently
    a hard-coded single-jiffy delay.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 7896f792d3b0..7340613c7eff 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -109,8 +109,10 @@ enum tick_dep_bits {
 	TICK_DEP_BIT_PERF_EVENTS	= 1,
 	TICK_DEP_BIT_SCHED		= 2,
 	TICK_DEP_BIT_CLOCK_UNSTABLE	= 3,
-	TICK_DEP_BIT_RCU		= 4
+	TICK_DEP_BIT_RCU		= 4,
+	TICK_DEP_BIT_RCU_EXP		= 5
 };
+#define TICK_DEP_BIT_MAX TICK_DEP_BIT_RCU_EXP
 
 #define TICK_DEP_MASK_NONE		0
 #define TICK_DEP_MASK_POSIX_TIMER	(1 << TICK_DEP_BIT_POSIX_TIMER)
@@ -118,6 +120,7 @@ enum tick_dep_bits {
 #define TICK_DEP_MASK_SCHED		(1 << TICK_DEP_BIT_SCHED)
 #define TICK_DEP_MASK_CLOCK_UNSTABLE	(1 << TICK_DEP_BIT_CLOCK_UNSTABLE)
 #define TICK_DEP_MASK_RCU		(1 << TICK_DEP_BIT_RCU)
+#define TICK_DEP_MASK_RCU_EXP		(1 << TICK_DEP_BIT_RCU_EXP)
 
 #ifdef CONFIG_NO_HZ_COMMON
 extern bool tick_nohz_enabled;

commit 1ae78780eda54023a0fb49ee743dbba39da148e0
Merge: 77a05940eee7 43e0ae7ae0f5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 15:42:43 2019 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Dynamic tick (nohz) updates, perhaps most notably changes to force
         the tick on when needed due to lengthy in-kernel execution on CPUs
         on which RCU is waiting.
    
       - Linux-kernel memory consistency model updates.
    
       - Replace rcu_swap_protected() with rcu_prepace_pointer().
    
       - Torture-test updates.
    
       - Documentation updates.
    
       - Miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (51 commits)
      security/safesetid: Replace rcu_swap_protected() with rcu_replace_pointer()
      net/sched: Replace rcu_swap_protected() with rcu_replace_pointer()
      net/netfilter: Replace rcu_swap_protected() with rcu_replace_pointer()
      net/core: Replace rcu_swap_protected() with rcu_replace_pointer()
      bpf/cgroup: Replace rcu_swap_protected() with rcu_replace_pointer()
      fs/afs: Replace rcu_swap_protected() with rcu_replace_pointer()
      drivers/scsi: Replace rcu_swap_protected() with rcu_replace_pointer()
      drm/i915: Replace rcu_swap_protected() with rcu_replace_pointer()
      x86/kvm/pmu: Replace rcu_swap_protected() with rcu_replace_pointer()
      rcu: Upgrade rcu_swap_protected() to rcu_replace_pointer()
      rcu: Suppress levelspread uninitialized messages
      rcu: Fix uninitialized variable in nocb_gp_wait()
      rcu: Update descriptions for rcu_future_grace_period tracepoint
      rcu: Update descriptions for rcu_nocb_wake tracepoint
      rcu: Remove obsolete descriptions for rcu_barrier tracepoint
      rcu: Ensure that ->rcu_urgent_qs is set before resched IPI
      workqueue: Convert for_each_wq to use built-in list check
      rcu: Several rcu_segcblist functions can be static
      rcu: Remove unused function hlist_bl_del_init_rcu()
      Documentation: Rename rcu_node_context_switch() to rcu_note_context_switch()
      ...

commit 74c578759f15cb5a0d0107759bdad671d7b52ab9
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:51 2019 +0200

    context_tracking: Rename context_tracking_is_enabled() => context_tracking_enabled()
    
    Remove the superfluous "is" in the middle of the name. We want to
    standardize the naming so that it can be expanded through suffixes:
    
            context_tracking_enabled()
            context_tracking_enabled_cpu()
            context_tracking_enabled_this_cpu()
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-6-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f92a10b5e112..7e050a356cc5 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -174,7 +174,7 @@ extern cpumask_var_t tick_nohz_full_mask;
 
 static inline bool tick_nohz_full_enabled(void)
 {
-	if (!context_tracking_is_enabled())
+	if (!context_tracking_enabled())
 		return false;
 
 	return tick_nohz_full_running;

commit 01b4c39901e087ceebae2733857248de81476bd8
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Jul 24 15:22:59 2019 +0200

    nohz: Add TICK_DEP_BIT_RCU
    
    If a nohz_full CPU is looping in the kernel, the scheduling-clock tick
    might nevertheless remain disabled.  In !PREEMPT kernels, this can
    prevent RCU's attempts to enlist the aid of that CPU's executions of
    cond_resched(), which can in turn result in an arbitrarily delayed grace
    period and thus an OOM.  RCU therefore needs a way to enable a holdout
    nohz_full CPU's scheduler-clock interrupt.
    
    This commit therefore provides a new TICK_DEP_BIT_RCU value which RCU can
    pass to tick_dep_set_cpu() and friends to force on the scheduler-clock
    interrupt for a specified CPU or task.  In some cases, rcutorture needs
    to turn on the scheduler-clock tick, so this commit also exports the
    relevant symbols to GPL-licensed modules.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f92a10b5e112..39eb44564058 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -108,7 +108,8 @@ enum tick_dep_bits {
 	TICK_DEP_BIT_POSIX_TIMER	= 0,
 	TICK_DEP_BIT_PERF_EVENTS	= 1,
 	TICK_DEP_BIT_SCHED		= 2,
-	TICK_DEP_BIT_CLOCK_UNSTABLE	= 3
+	TICK_DEP_BIT_CLOCK_UNSTABLE	= 3,
+	TICK_DEP_BIT_RCU		= 4
 };
 
 #define TICK_DEP_MASK_NONE		0
@@ -116,6 +117,7 @@ enum tick_dep_bits {
 #define TICK_DEP_MASK_PERF_EVENTS	(1 << TICK_DEP_BIT_PERF_EVENTS)
 #define TICK_DEP_MASK_SCHED		(1 << TICK_DEP_BIT_SCHED)
 #define TICK_DEP_MASK_CLOCK_UNSTABLE	(1 << TICK_DEP_BIT_CLOCK_UNSTABLE)
+#define TICK_DEP_MASK_RCU		(1 << TICK_DEP_BIT_RCU)
 
 #ifdef CONFIG_NO_HZ_COMMON
 extern bool tick_nohz_enabled;
@@ -268,6 +270,9 @@ static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
 
+static inline void tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit) { }
+static inline void tick_nohz_dep_clear_cpu(int cpu, enum tick_dep_bits bit) { }
+
 static inline void tick_dep_set(enum tick_dep_bits bit) { }
 static inline void tick_dep_clear(enum tick_dep_bits bit) { }
 static inline void tick_dep_set_cpu(int cpu, enum tick_dep_bits bit) { }

commit 8f5e823f9131a430b12f73e9436d7486e20c16f5
Merge: 59df1c2bdecb e07095c9bbcd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 19:40:31 2019 -0700

    Merge tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These fix the (Intel-specific) Performance and Energy Bias Hint (EPB)
      handling and expose it to user space via sysfs, fix and clean up
      several cpufreq drivers, add support for two new chips to the qoriq
      cpufreq driver, fix, simplify and clean up the cpufreq core and the
      schedutil governor, add support for "CPU" domains to the generic power
      domains (genpd) framework and provide low-level PSCI firmware support
      for that feature, fix the exynos cpuidle driver and fix a couple of
      issues in the devfreq subsystem and clean it up.
    
      Specifics:
    
       - Fix the handling of Performance and Energy Bias Hint (EPB) on Intel
         processors and expose it to user space via sysfs to avoid having to
         access it through the generic MSR I/F (Rafael Wysocki).
    
       - Improve the handling of global turbo changes made by the platform
         firmware in the intel_pstate driver (Rafael Wysocki).
    
       - Convert some slow-path static_cpu_has() callers to boot_cpu_has()
         in cpufreq (Borislav Petkov).
    
       - Fix the frequency calculation loop in the armada-37xx cpufreq
         driver (Gregory CLEMENT).
    
       - Fix possible object reference leaks in multuple cpufreq drivers
         (Wen Yang).
    
       - Fix kerneldoc comment in the centrino cpufreq driver (dongjian).
    
       - Clean up the ACPI and maple cpufreq drivers (Viresh Kumar, Mohan
         Kumar).
    
       - Add support for lx2160a and ls1028a to the qoriq cpufreq driver
         (Vabhav Sharma, Yuantian Tang).
    
       - Fix kobject memory leak in the cpufreq core (Viresh Kumar).
    
       - Simplify the IOwait boosting in the schedutil cpufreq governor and
         rework the TSC cpufreq notifier on x86 (Rafael Wysocki).
    
       - Clean up the cpufreq core and statistics code (Yue Hu, Kyle Lin).
    
       - Improve the cpufreq documentation, add SPDX license tags to some PM
         documentation files and unify copyright notices in them (Rafael
         Wysocki).
    
       - Add support for "CPU" domains to the generic power domains (genpd)
         framework and provide low-level PSCI firmware support for that
         feature (Ulf Hansson).
    
       - Rearrange the PSCI firmware support code and add support for
         SYSTEM_RESET2 to it (Ulf Hansson, Sudeep Holla).
    
       - Improve genpd support for devices in multiple power domains (Ulf
         Hansson).
    
       - Unify target residency for the AFTR and coupled AFTR states in the
         exynos cpuidle driver (Marek Szyprowski).
    
       - Introduce new helper routine in the operating performance points
         (OPP) framework (Andrew-sh.Cheng).
    
       - Add support for passing on-die termination (ODT) and auto power
         down parameters from the kernel to Trusted Firmware-A (TF-A) to the
         rk3399_dmc devfreq driver (Enric Balletbo i Serra).
    
       - Add tracing to devfreq (Lukasz Luba).
    
       - Make the exynos-bus devfreq driver suspend all devices on system
         shutdown (Marek Szyprowski).
    
       - Fix a few minor issues in the devfreq subsystem and clean it up
         somewhat (Enric Balletbo i Serra, MyungJoo Ham, Rob Herring,
         Saravana Kannan, Yangtao Li).
    
       - Improve system wakeup diagnostics (Stephen Boyd).
    
       - Rework filesystem sync messages emitted during system suspend and
         hibernation (Harry Pan)"
    
    * tag 'pm-5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (72 commits)
      cpufreq: Fix kobject memleak
      cpufreq: armada-37xx: fix frequency calculation for opp
      cpufreq: centrino: Fix centrino_setpolicy() kerneldoc comment
      cpufreq: qoriq: add support for lx2160a
      x86: tsc: Rework time_cpufreq_notifier()
      PM / Domains: Allow to attach a CPU via genpd_dev_pm_attach_by_id|name()
      PM / Domains: Search for the CPU device outside the genpd lock
      PM / Domains: Drop unused in-parameter to some genpd functions
      PM / Domains: Use the base device for driver_deferred_probe_check_state()
      cpufreq: qoriq: Add ls1028a chip support
      PM / Domains: Enable genpd_dev_pm_attach_by_id|name() for single PM domain
      PM / Domains: Allow OF lookup for multi PM domain case from ->attach_dev()
      PM / Domains: Don't kfree() the virtual device in the error path
      cpufreq: Move ->get callback check outside of __cpufreq_get()
      PM / Domains: remove unnecessary unlikely()
      cpufreq: Remove needless bios_limit check in show_bios_limit()
      drivers/cpufreq/acpi-cpufreq.c: This fixes the following checkpatch warning
      firmware/psci: add support for SYSTEM_RESET2
      PM / devfreq: add tracing for scheduling work
      trace: events: add devfreq trace event file
      ...

commit 6f9b83ac877fb5558d76b9f78590f3afd1bdf421
Author: Ulf Hansson <ulf.hansson@linaro.org>
Date:   Wed Mar 27 15:35:47 2019 +0100

    cpuidle: Export the next timer expiration for CPUs
    
    To be able to predict the sleep duration for a CPU entering idle, it
    is essential to know the expiration time of the next timer.  Both the
    teo and the menu cpuidle governors already use this information for
    CPU idle state selection.
    
    Moving forward, a similar prediction needs to be made for a group of
    idle CPUs rather than for a single one and the following changes
    implement a new genpd governor for that purpose.
    
    In order to support that feature, add a new function called
    tick_nohz_get_next_hrtimer() that will return the next hrtimer
    expiration time of a given CPU to be invoked after deciding
    whether or not to stop the scheduler tick on that CPU.
    
    Make the cpuidle core call tick_nohz_get_next_hrtimer() right
    before invoking the ->enter() callback provided by the cpuidle
    driver for the given state and store its return value in the
    per-CPU struct cpuidle_device, so as to make it available to code
    outside of cpuidle.
    
    Note that at the point when cpuidle calls tick_nohz_get_next_hrtimer(),
    the governor's ->select() callback has already returned and indicated
    whether or not the tick should be stopped, so in fact the value
    returned by tick_nohz_get_next_hrtimer() always is the next hrtimer
    expiration time for the given CPU, possibly including the tick (if
    it hasn't been stopped).
    
    Co-developed-by: Lina Iyer <lina.iyer@linaro.org>
    Co-developed-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 55388ab45fd4..8891b5ac3e40 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -122,6 +122,7 @@ extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern bool tick_nohz_idle_got_tick(void);
+extern ktime_t tick_nohz_get_next_hrtimer(void);
 extern ktime_t tick_nohz_get_sleep_length(ktime_t *delta_next);
 extern unsigned long tick_nohz_get_idle_calls(void);
 extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);
@@ -145,7 +146,11 @@ static inline void tick_nohz_idle_restart_tick(void) { }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 static inline bool tick_nohz_idle_got_tick(void) { return false; }
-
+static inline ktime_t tick_nohz_get_next_hrtimer(void)
+{
+	/* Next wake up is the tick period, assume it starts now */
+	return ktime_add(ktime_get(), TICK_NSEC);
+}
 static inline ktime_t tick_nohz_get_sleep_length(ktime_t *delta_next)
 {
 	*delta_next = TICK_NSEC;

commit 1b72d43237980eab9b6ae6bb8181e51c840377e6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 16:39:20 2019 +0100

    tick: Remove outgoing CPU from broadcast masks
    
    Valentin reported that unplugging a CPU occasionally results in a warning
    in the tick broadcast code which is triggered when an offline CPU is in the
    broadcast mask.
    
    This happens because the outgoing CPU is not removing itself from the
    broadcast masks, especially not from the broadcast_force_mask. The removal
    happens on the control CPU after the outgoing CPU is dead. It's a long
    standing issue, but the warning is harmless.
    
    Rework the hotplug mechanism so that the outgoing CPU removes itself from
    the broadcast masks after disabling interrupts and removing itself from the
    online mask.
    
    Reported-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1903211540180.1784@nanos.tec.linutronix.de

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 55388ab45fd4..76acb48acdb7 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -68,6 +68,12 @@ extern void tick_broadcast_control(enum tick_broadcast_mode mode);
 static inline void tick_broadcast_control(enum tick_broadcast_mode mode) { }
 #endif /* BROADCAST */
 
+#if defined(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST) && defined(CONFIG_HOTPLUG_CPU)
+extern void tick_offline_cpu(unsigned int cpu);
+#else
+static inline void tick_offline_cpu(unsigned int cpu) { }
+#endif
+
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 extern int tick_broadcast_oneshot_control(enum tick_broadcast_state state);
 #else

commit 296bb1e51a4838a6488ec5ce676607093482ecbc
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 5 19:12:34 2018 +0200

    cpuidle: menu: Refine idle state selection for running tick
    
    If the tick isn't stopped, the target residency of the state selected
    by the menu governor may be greater than the actual time to the next
    tick and that means lost energy.
    
    To avoid that, make tick_nohz_get_sleep_length() return the current
    time to the next event (before stopping the tick) in addition to the
    estimated one via an extra pointer argument and make menu_select()
    use that value to refine the state selection when necessary.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index e8e7ff16b929..55388ab45fd4 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -122,7 +122,7 @@ extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern bool tick_nohz_idle_got_tick(void);
-extern ktime_t tick_nohz_get_sleep_length(void);
+extern ktime_t tick_nohz_get_sleep_length(ktime_t *delta_next);
 extern unsigned long tick_nohz_get_idle_calls(void);
 extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
@@ -146,9 +146,10 @@ static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 static inline bool tick_nohz_idle_got_tick(void) { return false; }
 
-static inline ktime_t tick_nohz_get_sleep_length(void)
+static inline ktime_t tick_nohz_get_sleep_length(ktime_t *delta_next)
 {
-	return NSEC_PER_SEC / HZ;
+	*delta_next = TICK_NSEC;
+	return *delta_next;
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }

commit 554c8aa8ecade210d58a252173bb8f2106552a44
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Apr 3 23:17:11 2018 +0200

    sched: idle: Select idle state before stopping the tick
    
    In order to address the issue with short idle duration predictions
    by the idle governor after the scheduler tick has been stopped,
    reorder the code in cpuidle_idle_call() so that the governor idle
    state selection runs before tick_nohz_idle_go_idle() and use the
    "nohz" hint returned by cpuidle_select() to decide whether or not
    to stop the tick.
    
    This isn't straightforward, because menu_select() invokes
    tick_nohz_get_sleep_length() to get the time to the next timer
    event and the number returned by the latter comes from
    __tick_nohz_idle_stop_tick().  Fortunately, however, it is possible
    to compute that number without actually stopping the tick and with
    the help of the existing code.
    
    Namely, tick_nohz_get_sleep_length() can be made call
    tick_nohz_next_event(), introduced earlier, to get the time to the
    next non-highres timer event.  If that happens, tick_nohz_next_event()
    need not be called by __tick_nohz_idle_stop_tick() again.
    
    If it turns out that the scheduler tick cannot be stopped going
    forward or the next timer event is too close for the tick to be
    stopped, tick_nohz_get_sleep_length() can simply return the time to
    the next event currently programmed into the corresponding clock
    event device.
    
    In addition to knowing the return value of tick_nohz_next_event(),
    however, tick_nohz_get_sleep_length() needs to know the time to the
    next highres timer event, but with the scheduler tick timer excluded,
    which can be computed with the help of hrtimer_get_next_event().
    
    That minimum of that number and the tick_nohz_next_event() return
    value is the total time to the next timer event with the assumption
    that the tick will be stopped.  It can be returned to the idle
    governor which can use it for predicting idle duration (under the
    assumption that the tick will be stopped) and deciding whether or
    not it makes sense to stop the tick before putting the CPU into the
    selected idle state.
    
    With the above, the sleep_length field in struct tick_sched is not
    necessary any more, so drop it.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=199227
    Reported-by: Doug Smythies <dsmythies@telus.net>
    Reported-by: Thomas Ilsche <thomas.ilsche@tu-dresden.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index ef0717e5e526..e8e7ff16b929 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -116,6 +116,7 @@ extern bool tick_nohz_enabled;
 extern bool tick_nohz_tick_stopped(void);
 extern bool tick_nohz_tick_stopped_cpu(int cpu);
 extern void tick_nohz_idle_stop_tick(void);
+extern void tick_nohz_idle_retain_tick(void);
 extern void tick_nohz_idle_restart_tick(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
@@ -139,6 +140,7 @@ static inline void tick_nohz_idle_stop_tick_protected(void)
 static inline int tick_nohz_tick_stopped(void) { return 0; }
 static inline int tick_nohz_tick_stopped_cpu(int cpu) { return 0; }
 static inline void tick_nohz_idle_stop_tick(void) { }
+static inline void tick_nohz_idle_retain_tick(void) { }
 static inline void tick_nohz_idle_restart_tick(void) { }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }

commit 45f1ff59e27ca59d33cc1a317e669d90022ccf7d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 22 17:50:49 2018 +0100

    cpuidle: Return nohz hint from cpuidle_select()
    
    Add a new pointer argument to cpuidle_select() and to the ->select
    cpuidle governor callback to allow a boolean value indicating
    whether or not the tick should be stopped before entering the
    selected state to be returned from there.
    
    Make the ladder governor ignore that pointer (to preserve its
    current behavior) and make the menu governor return 'false" through
    it if:
     (1) the idle exit latency is constrained at 0, or
     (2) the selected state is a polling one, or
     (3) the expected idle period duration is within the tick period
         range.
    
    In addition to that, the correction factor computations in the menu
    governor need to take the possibility that the tick may not be
    stopped into account to avoid artificially small correction factor
    values.  To that end, add a mechanism to record tick wakeups, as
    suggested by Peter Zijlstra, and use it to modify the menu_update()
    behavior when tick wakeup occurs.  Namely, if the CPU is woken up by
    the tick and the return value of tick_nohz_get_sleep_length() is not
    within the tick boundary, the predicted idle duration is likely too
    short, so make menu_update() try to compensate for that by updating
    the governor statistics as though the CPU was idle for a long time.
    
    Since the value returned through the new argument pointer of
    cpuidle_select() is not used by its caller yet, this change by
    itself is not expected to alter the functionality of the code.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index fccebfba167e..ef0717e5e526 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -120,6 +120,7 @@ extern void tick_nohz_idle_restart_tick(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
+extern bool tick_nohz_idle_got_tick(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern unsigned long tick_nohz_get_idle_calls(void);
 extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);
@@ -141,6 +142,7 @@ static inline void tick_nohz_idle_stop_tick(void) { }
 static inline void tick_nohz_idle_restart_tick(void) { }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
+static inline bool tick_nohz_idle_got_tick(void) { return false; }
 
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {

commit 2aaf709a518d26563b80fd7a42379d7aa7ffed4a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 15 23:05:50 2018 +0100

    sched: idle: Do not stop the tick upfront in the idle loop
    
    Push the decision whether or not to stop the tick somewhat deeper
    into the idle loop.
    
    Stopping the tick upfront leads to unpleasant outcomes in case the
    idle governor doesn't agree with the nohz code on the duration of the
    upcoming idle period.  Specifically, if the tick has been stopped and
    the idle governor predicts short idle, the situation is bad regardless
    of whether or not the prediction is accurate.  If it is accurate, the
    tick has been stopped unnecessarily which means excessive overhead.
    If it is not accurate, the CPU is likely to spend too much time in
    the (shallow, because short idle has been predicted) idle state
    selected by the governor [1].
    
    As the first step towards addressing this problem, change the code
    to make the tick stopping decision inside of the loop in do_idle().
    In particular, do not stop the tick in the cpu_idle_poll() code path.
    Also don't do that in tick_nohz_irq_exit() which doesn't really have
    enough information on whether or not to stop the tick.
    
    Link: https://marc.info/?l=linux-pm&m=150116085925208&w=2 # [1]
    Link: https://tu-dresden.de/zih/forschung/ressourcen/dateien/projekte/haec/powernightmares.pdf
    Suggested-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 1d253df9ea3c..fccebfba167e 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -116,6 +116,7 @@ extern bool tick_nohz_enabled;
 extern bool tick_nohz_tick_stopped(void);
 extern bool tick_nohz_tick_stopped_cpu(int cpu);
 extern void tick_nohz_idle_stop_tick(void);
+extern void tick_nohz_idle_restart_tick(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
@@ -137,6 +138,7 @@ static inline void tick_nohz_idle_stop_tick_protected(void)
 static inline int tick_nohz_tick_stopped(void) { return 0; }
 static inline int tick_nohz_tick_stopped_cpu(int cpu) { return 0; }
 static inline void tick_nohz_idle_stop_tick(void) { }
+static inline void tick_nohz_idle_restart_tick(void) { }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 

commit 0e7767687fdabfc58d5046e7488632bf2ecd4d0c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 5 18:58:27 2018 +0200

    time: tick-sched: Reorganize idle tick management code
    
    Prepare the scheduler tick code for reworking the idle loop to
    avoid stopping the tick in some cases.
    
    The idea is to split the nohz idle entry call to decouple the idle
    time stats accounting and preparatory work from the actual tick stop
    code, in order to later be able to delay the tick stop once we reach
    more power-knowledgeable callers.
    
    Move away the tick_nohz_start_idle() invocation from
    __tick_nohz_idle_enter(), rename the latter to
    __tick_nohz_idle_stop_tick() and define tick_nohz_idle_stop_tick()
    as a wrapper around it for calling it from the outside.
    
    Make tick_nohz_idle_enter() only call tick_nohz_start_idle() instead
    of calling the entire __tick_nohz_idle_enter(), add another wrapper
    disabling and enabling interrupts around tick_nohz_idle_stop_tick()
    and make the current callers of tick_nohz_idle_enter() call it too
    to retain their current functionality.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 7f8c9a127f5a..1d253df9ea3c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -115,6 +115,7 @@ enum tick_dep_bits {
 extern bool tick_nohz_enabled;
 extern bool tick_nohz_tick_stopped(void);
 extern bool tick_nohz_tick_stopped_cpu(int cpu);
+extern void tick_nohz_idle_stop_tick(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
@@ -123,10 +124,19 @@ extern unsigned long tick_nohz_get_idle_calls(void);
 extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
+
+static inline void tick_nohz_idle_stop_tick_protected(void)
+{
+	local_irq_disable();
+	tick_nohz_idle_stop_tick();
+	local_irq_enable();
+}
+
 #else /* !CONFIG_NO_HZ_COMMON */
 #define tick_nohz_enabled (0)
 static inline int tick_nohz_tick_stopped(void) { return 0; }
 static inline int tick_nohz_tick_stopped_cpu(int cpu) { return 0; }
+static inline void tick_nohz_idle_stop_tick(void) { }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 
@@ -136,6 +146,8 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
+
+static inline void tick_nohz_idle_stop_tick_protected(void) { }
 #endif /* !CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_FULL

commit 22ab8bc02a5f6e8ffc418759894f7a6b0b632331
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Feb 21 05:17:25 2018 +0100

    nohz: Allow to check if remote CPU tick is stopped
    
    This check is racy but provides a good heuristic to determine whether
    a CPU may need a remote tick or not.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1519186649-3242-4-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 86576d9d2311..7f8c9a127f5a 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -114,6 +114,7 @@ enum tick_dep_bits {
 #ifdef CONFIG_NO_HZ_COMMON
 extern bool tick_nohz_enabled;
 extern bool tick_nohz_tick_stopped(void);
+extern bool tick_nohz_tick_stopped_cpu(int cpu);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
@@ -125,6 +126,7 @@ extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 #else /* !CONFIG_NO_HZ_COMMON */
 #define tick_nohz_enabled (0)
 static inline int tick_nohz_tick_stopped(void) { return 0; }
+static inline int tick_nohz_tick_stopped_cpu(int cpu) { return 0; }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 

commit a364298359e74a414857bbbf3b725564feb22d09
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Feb 21 05:17:24 2018 +0100

    nohz: Convert tick_nohz_tick_stopped() to bool
    
    It makes this function more self-explanatory about what it does and how
    to use it.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1519186649-3242-3-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 7cc35921218e..86576d9d2311 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -113,7 +113,7 @@ enum tick_dep_bits {
 
 #ifdef CONFIG_NO_HZ_COMMON
 extern bool tick_nohz_enabled;
-extern int tick_nohz_tick_stopped(void);
+extern bool tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);

commit 466a2b42d67644447a1765276259a3ea5531ddff
Author: Joel Fernandes <joelaf@google.com>
Date:   Thu Dec 21 02:22:45 2017 +0100

    cpufreq: schedutil: Use idle_calls counter of the remote CPU
    
    Since the recent remote cpufreq callback work, its possible that a cpufreq
    update is triggered from a remote CPU. For single policies however, the current
    code uses the local CPU when trying to determine if the remote sg_cpu entered
    idle or is busy. This is incorrect. To remedy this, compare with the nohz tick
    idle_calls counter of the remote CPU.
    
    Fixes: 674e75411fc2 (sched: cpufreq: Allow remote cpufreq callbacks)
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Cc: 4.14+ <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f442d1a42025..7cc35921218e 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -119,6 +119,7 @@ extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern unsigned long tick_nohz_get_idle_calls(void);
+extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 #else /* !CONFIG_NO_HZ_COMMON */

commit 8a103df440afea30c91ebd42e61dc644e647f4bd
Merge: a9903f04e0a4 fbc3edf7d773
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 8 10:17:15 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index fe01e68bf520..cf413b344ddb 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Tick related global functions
  */

commit 6f1982fedd59856bcc42a9b521be4c3ffd2f60a7
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Fri Oct 27 04:42:36 2017 +0200

    sched/isolation: Handle the nohz_full= parameter
    
    We want to centralize the isolation management, done by the housekeeping
    subsystem. Therefore we need to handle the nohz_full= parameter from
    there.
    
    Since nohz_full= so far has involved unbound timers, watchdog, RCU
    and tilegx NAPI isolation, we keep that default behaviour.
    
    nohz_full= will be deprecated in the future. We want to control
    the isolation features from the isolcpus= parameter.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1509072159-31808-10-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 68afc09aa8ac..e2a163a9f96c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -228,6 +228,7 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void __tick_nohz_task_switch(void);
+extern void __init tick_nohz_full_setup(cpumask_var_t cpumask);
 #else
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
@@ -248,6 +249,7 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void __tick_nohz_task_switch(void) { }
+static inline void tick_nohz_full_setup(cpumask_var_t cpumask) { }
 #endif
 
 static inline void tick_nohz_task_switch(void)

commit 7863406143d8bbbbda07a61285c5f4c217908dfd
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Fri Oct 27 04:42:28 2017 +0200

    sched/isolation: Move housekeeping related code to its own file
    
    The housekeeping code is currently tied to the NOHZ code. As we are
    planning to make housekeeping independent from it, start with moving
    the relevant code to its own file.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1509072159-31808-2-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index fe01e68bf520..68afc09aa8ac 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -137,7 +137,6 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 #ifdef CONFIG_NO_HZ_FULL
 extern bool tick_nohz_full_running;
 extern cpumask_var_t tick_nohz_full_mask;
-extern cpumask_var_t housekeeping_mask;
 
 static inline bool tick_nohz_full_enabled(void)
 {
@@ -161,11 +160,6 @@ static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask)
 		cpumask_or(mask, mask, tick_nohz_full_mask);
 }
 
-static inline int housekeeping_any_cpu(void)
-{
-	return cpumask_any_and(housekeeping_mask, cpu_online_mask);
-}
-
 extern void tick_nohz_dep_set(enum tick_dep_bits bit);
 extern void tick_nohz_dep_clear(enum tick_dep_bits bit);
 extern void tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit);
@@ -235,10 +229,6 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void __tick_nohz_task_switch(void);
 #else
-static inline int housekeeping_any_cpu(void)
-{
-	return smp_processor_id();
-}
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
@@ -260,33 +250,6 @@ static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void __tick_nohz_task_switch(void) { }
 #endif
 
-static inline const struct cpumask *housekeeping_cpumask(void)
-{
-#ifdef CONFIG_NO_HZ_FULL
-	if (tick_nohz_full_enabled())
-		return housekeeping_mask;
-#endif
-	return cpu_possible_mask;
-}
-
-static inline bool is_housekeeping_cpu(int cpu)
-{
-#ifdef CONFIG_NO_HZ_FULL
-	if (tick_nohz_full_enabled())
-		return cpumask_test_cpu(cpu, housekeeping_mask);
-#endif
-	return true;
-}
-
-static inline void housekeeping_affine(struct task_struct *t)
-{
-#ifdef CONFIG_NO_HZ_FULL
-	if (tick_nohz_full_enabled())
-		set_cpus_allowed_ptr(t, housekeeping_mask);
-
-#endif
-}
-
 static inline void tick_nohz_task_switch(void)
 {
 	if (tick_nohz_full_enabled())

commit b7eaf1aab9f8bd2e49fceed77ebc66c1b5800718
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Mar 22 00:08:50 2017 +0100

    cpufreq: schedutil: Avoid reducing frequency of busy CPUs prematurely
    
    The way the schedutil governor uses the PELT metric causes it to
    underestimate the CPU utilization in some cases.
    
    That can be easily demonstrated by running kernel compilation on
    a Sandy Bridge Intel processor, running turbostat in parallel with
    it and looking at the values written to the MSR_IA32_PERF_CTL
    register.  Namely, the expected result would be that when all CPUs
    were 100% busy, all of them would be requested to run in the maximum
    P-state, but observation shows that this clearly isn't the case.
    The CPUs run in the maximum P-state for a while and then are
    requested to run slower and go back to the maximum P-state after
    a while again.  That causes the actual frequency of the processor to
    visibly oscillate below the sustainable maximum in a jittery fashion
    which clearly is not desirable.
    
    That has been attributed to CPU utilization metric updates on task
    migration that cause the total utilization value for the CPU to be
    reduced by the utilization of the migrated task.  If that happens,
    the schedutil governor may see a CPU utilization reduction and will
    attempt to reduce the CPU frequency accordingly right away.  That
    may be premature, though, for example if the system is generally
    busy and there are other runnable tasks waiting to be run on that
    CPU already.
    
    This is unlikely to be an issue on systems where cpufreq policies are
    shared between multiple CPUs, because in those cases the policy
    utilization is computed as the maximum of the CPU utilization values
    over the whole policy and if that turns out to be low, reducing the
    frequency for the policy most likely is a good idea anyway.  On
    systems with one CPU per policy, however, it may affect performance
    adversely and even lead to increased energy consumption in some cases.
    
    On those systems it may be addressed by taking another utilization
    metric into consideration, like whether or not the CPU whose
    frequency is about to be reduced has been idle recently, because if
    that's not the case, the CPU is likely to be busy in the near future
    and its frequency should not be reduced.
    
    To that end, use the counter of idle calls in the timekeeping code.
    Namely, make the schedutil governor look at that counter for the
    current CPU every time before its frequency is about to be reduced.
    If the counter has not changed since the previous iteration of the
    governor computations for that CPU, the CPU has been busy for all
    that time and its frequency should not be decreased, so if the new
    frequency would be lower than the one set previously, the governor
    will skip the frequency update.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Joel Fernandes <joelaf@google.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index a04fea19676f..fe01e68bf520 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -117,6 +117,7 @@ extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
+extern unsigned long tick_nohz_get_idle_calls(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 #else /* !CONFIG_NO_HZ_COMMON */

commit 2456e855354415bfaeb7badaa14e11b3e02c8466
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 25 11:38:40 2016 +0100

    ktime: Get rid of the union
    
    ktime is a union because the initial implementation stored the time in
    scalar nanoseconds on 64 bit machine and in a endianess optimized timespec
    variant for 32bit machines. The Y2038 cleanup removed the timespec variant
    and switched everything to scalar nanoseconds. The union remained, but
    become completely pointless.
    
    Get rid of the union and just keep ktime_t as simple typedef of type s64.
    
    The conversion was done with coccinelle and some manual mopping up.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 62be0786d6d0..a04fea19676f 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -127,9 +127,7 @@ static inline void tick_nohz_idle_exit(void) { }
 
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {
-	ktime_t len = { .tv64 = NSEC_PER_SEC/HZ };
-
-	return len;
+	return NSEC_PER_SEC / HZ;
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }

commit 4cc7ecb7f2a60e8deb783b8fbf7c1ae467acb920
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Mar 17 14:23:00 2016 -0700

    param: convert some "on"/"off" users to strtobool
    
    This changes several users of manual "on"/"off" parsing to use
    strtobool.
    
    Some side-effects:
    - these uses will now parse y/n/1/0 meaningfully too
    - the early_param uses will now bubble up parse errors
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Amitkumar Karwar <akarwar@marvell.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Joe Perches <joe@perches.com>
    Cc: Kalle Valo <kvalo@codeaurora.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nishant Sarmukadam <nishants@marvell.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Steve French <sfrench@samba.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 21f73649a4dc..62be0786d6d0 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -111,7 +111,7 @@ enum tick_dep_bits {
 #define TICK_DEP_MASK_CLOCK_UNSTABLE	(1 << TICK_DEP_BIT_CLOCK_UNSTABLE)
 
 #ifdef CONFIG_NO_HZ_COMMON
-extern int tick_nohz_enabled;
+extern bool tick_nohz_enabled;
 extern int tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);

commit b78783000d5cb7c5994e6742e1d1ce594bfea15b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 17 22:25:49 2015 +0200

    posix-cpu-timers: Migrate to use new tick dependency mask model
    
    Instead of providing asynchronous checks for the nohz subsystem to verify
    posix cpu timers tick dependency, migrate the latter to the new mask.
    
    In order to keep track of the running timers and expose the tick
    dependency accordingly, we must probe the timers queuing and dequeuing
    on threads and process lists.
    
    Unfortunately it implies both task and signal level dependencies. We
    should be able to further optimize this and merge all that on the task
    level dependency, at the cost of a bit of complexity and may be overhead.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0e63db4bc662..21f73649a4dc 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -234,7 +234,6 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 }
 
 extern void tick_nohz_full_kick_cpu(int cpu);
-extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(void);
 #else
 static inline int housekeeping_any_cpu(void)
@@ -259,7 +258,6 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 					 enum tick_dep_bits bit) { }
 
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
-static inline void tick_nohz_full_kick_all(void) { }
 static inline void __tick_nohz_task_switch(void) { }
 #endif
 

commit 555e0c1ef7ff49ee5ac3a1eb12de4a2e4722f63d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 16 17:42:29 2015 +0200

    perf: Migrate perf to use new tick dependency mask model
    
    Instead of providing asynchronous checks for the nohz subsystem to verify
    perf event tick dependency, migrate perf to the new mask.
    
    Perf needs the tick for two situations:
    
    1) Freq events. We could set the tick dependency when those are
    installed on a CPU context. But setting a global dependency on top of
    the global freq events accounting is much easier. If people want that
    to be optimized, we can still refine that on the per-CPU tick dependency
    level. This patch dooesn't change the current behaviour anyway.
    
    2) Throttled events: this is a per-cpu dependency.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 96d276a923bf..0e63db4bc662 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -233,7 +233,6 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 		tick_nohz_dep_clear_signal(signal, bit);
 }
 
-extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(void);
@@ -260,7 +259,6 @@ static inline void tick_dep_clear_signal(struct signal_struct *signal,
 					 enum tick_dep_bits bit) { }
 
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
-static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
 static inline void __tick_nohz_task_switch(void) { }
 #endif

commit e6e6cc22e067a6f44449aa8fd0328404079c3ba5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Dec 11 03:27:25 2015 +0100

    nohz: Use enum code for tick stop failure tracing message
    
    It makes nohz tracing more lightweight, standard and easier to parse.
    
    Examples:
    
           user_loop-2904  [007] d..1   517.701126: tick_stop: success=1 dependency=NONE
           user_loop-2904  [007] dn.1   518.021181: tick_stop: success=0 dependency=SCHED
        posix_timers-6142  [007] d..1  1739.027400: tick_stop: success=0 dependency=POSIX_TIMER
           user_loop-5463  [007] dN.1  1185.931939: tick_stop: success=0 dependency=PERF_EVENTS
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index d60e5df8ec28..96d276a923bf 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -104,6 +104,7 @@ enum tick_dep_bits {
 	TICK_DEP_BIT_CLOCK_UNSTABLE	= 3
 };
 
+#define TICK_DEP_MASK_NONE		0
 #define TICK_DEP_MASK_POSIX_TIMER	(1 << TICK_DEP_BIT_POSIX_TIMER)
 #define TICK_DEP_MASK_PERF_EVENTS	(1 << TICK_DEP_BIT_PERF_EVENTS)
 #define TICK_DEP_MASK_SCHED		(1 << TICK_DEP_BIT_SCHED)

commit d027d45d8a17a4145eab2d841140e9acbb7feb59
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jun 7 15:54:30 2015 +0200

    nohz: New tick dependency mask
    
    The tick dependency is evaluated on every IRQ and context switch. This
    consists is a batch of checks which determine whether it is safe to
    stop the tick or not. These checks are often split in many details:
    posix cpu timers, scheduler, sched clock, perf events.... each of which
    are made of smaller details: posix cpu timer involves checking process
    wide timers then thread wide timers. Perf involves checking freq events
    then more per cpu details.
    
    Checking these informations asynchronously every time we update the full
    dynticks state bring avoidable overhead and a messy layout.
    
    Let's introduce instead tick dependency masks: one for system wide
    dependency (unstable sched clock, freq based perf events), one for CPU
    wide dependency (sched, throttling perf events), and task/signal level
    dependencies (posix cpu timers). The subsystems are responsible
    for setting and clearing their dependency through a set of APIs that will
    take care of concurrent dependency mask modifications and kick targets
    to restart the relevant CPU tick whenever needed.
    
    This new dependency engine stays beside the old one until all subsystems
    having a tick dependency are converted to it.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 97fd4e543846..d60e5df8ec28 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -97,6 +97,18 @@ static inline void tick_broadcast_exit(void)
 	tick_broadcast_oneshot_control(TICK_BROADCAST_EXIT);
 }
 
+enum tick_dep_bits {
+	TICK_DEP_BIT_POSIX_TIMER	= 0,
+	TICK_DEP_BIT_PERF_EVENTS	= 1,
+	TICK_DEP_BIT_SCHED		= 2,
+	TICK_DEP_BIT_CLOCK_UNSTABLE	= 3
+};
+
+#define TICK_DEP_MASK_POSIX_TIMER	(1 << TICK_DEP_BIT_POSIX_TIMER)
+#define TICK_DEP_MASK_PERF_EVENTS	(1 << TICK_DEP_BIT_PERF_EVENTS)
+#define TICK_DEP_MASK_SCHED		(1 << TICK_DEP_BIT_SCHED)
+#define TICK_DEP_MASK_CLOCK_UNSTABLE	(1 << TICK_DEP_BIT_CLOCK_UNSTABLE)
+
 #ifdef CONFIG_NO_HZ_COMMON
 extern int tick_nohz_enabled;
 extern int tick_nohz_tick_stopped(void);
@@ -154,6 +166,72 @@ static inline int housekeeping_any_cpu(void)
 	return cpumask_any_and(housekeeping_mask, cpu_online_mask);
 }
 
+extern void tick_nohz_dep_set(enum tick_dep_bits bit);
+extern void tick_nohz_dep_clear(enum tick_dep_bits bit);
+extern void tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit);
+extern void tick_nohz_dep_clear_cpu(int cpu, enum tick_dep_bits bit);
+extern void tick_nohz_dep_set_task(struct task_struct *tsk,
+				   enum tick_dep_bits bit);
+extern void tick_nohz_dep_clear_task(struct task_struct *tsk,
+				     enum tick_dep_bits bit);
+extern void tick_nohz_dep_set_signal(struct signal_struct *signal,
+				     enum tick_dep_bits bit);
+extern void tick_nohz_dep_clear_signal(struct signal_struct *signal,
+				       enum tick_dep_bits bit);
+
+/*
+ * The below are tick_nohz_[set,clear]_dep() wrappers that optimize off-cases
+ * on top of static keys.
+ */
+static inline void tick_dep_set(enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_set(bit);
+}
+
+static inline void tick_dep_clear(enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_clear(bit);
+}
+
+static inline void tick_dep_set_cpu(int cpu, enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_cpu(cpu))
+		tick_nohz_dep_set_cpu(cpu, bit);
+}
+
+static inline void tick_dep_clear_cpu(int cpu, enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_cpu(cpu))
+		tick_nohz_dep_clear_cpu(cpu, bit);
+}
+
+static inline void tick_dep_set_task(struct task_struct *tsk,
+				     enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_set_task(tsk, bit);
+}
+static inline void tick_dep_clear_task(struct task_struct *tsk,
+				       enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_clear_task(tsk, bit);
+}
+static inline void tick_dep_set_signal(struct signal_struct *signal,
+				       enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_set_signal(signal, bit);
+}
+static inline void tick_dep_clear_signal(struct signal_struct *signal,
+					 enum tick_dep_bits bit)
+{
+	if (tick_nohz_full_enabled())
+		tick_nohz_dep_clear_signal(signal, bit);
+}
+
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
@@ -166,6 +244,20 @@ static inline int housekeeping_any_cpu(void)
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
+
+static inline void tick_dep_set(enum tick_dep_bits bit) { }
+static inline void tick_dep_clear(enum tick_dep_bits bit) { }
+static inline void tick_dep_set_cpu(int cpu, enum tick_dep_bits bit) { }
+static inline void tick_dep_clear_cpu(int cpu, enum tick_dep_bits bit) { }
+static inline void tick_dep_set_task(struct task_struct *tsk,
+				     enum tick_dep_bits bit) { }
+static inline void tick_dep_clear_task(struct task_struct *tsk,
+				       enum tick_dep_bits bit) { }
+static inline void tick_dep_set_signal(struct signal_struct *signal,
+				       enum tick_dep_bits bit) { }
+static inline void tick_dep_clear_signal(struct signal_struct *signal,
+					 enum tick_dep_bits bit) { }
+
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }

commit 46373a15f65fe862f31c19a484acdf551f2b442f
Author: Jean Delvare <jdelvare@suse.de>
Date:   Mon Jan 11 17:40:31 2016 +0100

    time: nohz: Expose tick_nohz_enabled
    
    The cpuidle subsystem needs it.
    
    Signed-off-by: Jean Delvare <jdelvare@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index e312219ff823..97fd4e543846 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -98,6 +98,7 @@ static inline void tick_broadcast_exit(void)
 }
 
 #ifdef CONFIG_NO_HZ_COMMON
+extern int tick_nohz_enabled;
 extern int tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
@@ -106,6 +107,7 @@ extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 #else /* !CONFIG_NO_HZ_COMMON */
+#define tick_nohz_enabled (0)
 static inline int tick_nohz_tick_stopped(void) { return 0; }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }

commit 9642d18eee2cd169b60c6ac0f20bda745b5a3d1e
Author: Vatika Harlalka <vatikaharlalka@gmail.com>
Date:   Tue Sep 1 16:50:59 2015 +0200

    nohz: Affine unpinned timers to housekeepers
    
    The problem addressed in this patch is about affining unpinned
    timers. Adaptive or Full Dynticks CPUs are currently disturbed
    by unnecessary jitter due to firing of such timers on them.
    
    This patch will affine timers to online CPUs which are not full
    dynticks in NOHZ_FULL configured systems. It should not
    introduce overhead in nohz full off case due to static keys.
    
    Signed-off-by: Vatika Harlalka <vatikaharlalka@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1441119060-2230-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 48d901f83f92..e312219ff823 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -147,11 +147,20 @@ static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask)
 		cpumask_or(mask, mask, tick_nohz_full_mask);
 }
 
+static inline int housekeeping_any_cpu(void)
+{
+	return cpumask_any_and(housekeeping_mask, cpu_online_mask);
+}
+
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(void);
 #else
+static inline int housekeeping_any_cpu(void)
+{
+	return smp_processor_id();
+}
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }

commit de734f89b67c2df30e35a09e7e56a3659e5b6ac6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 11 18:07:12 2015 +0200

    nohz: Remove useless argument on tick_nohz_task_switch()
    
    Leftover from early code.
    
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 7d35b0fec399..48d901f83f92 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -150,7 +150,7 @@ static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask)
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
-extern void __tick_nohz_task_switch(struct task_struct *tsk);
+extern void __tick_nohz_task_switch(void);
 #else
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
@@ -158,7 +158,7 @@ static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
-static inline void __tick_nohz_task_switch(struct task_struct *tsk) { }
+static inline void __tick_nohz_task_switch(void) { }
 #endif
 
 static inline const struct cpumask *housekeeping_cpumask(void)
@@ -188,10 +188,10 @@ static inline void housekeeping_affine(struct task_struct *t)
 #endif
 }
 
-static inline void tick_nohz_task_switch(struct task_struct *tsk)
+static inline void tick_nohz_task_switch(void)
 {
 	if (tick_nohz_full_enabled())
-		__tick_nohz_task_switch(tsk);
+		__tick_nohz_task_switch();
 }
 
 #endif

commit 73738a95d00467812664b7f86ba3052f5faf96d7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed May 27 19:22:08 2015 +0200

    nohz: Restart nohz full tick from irq exit
    
    Restart the tick when necessary from the irq exit path. It makes nohz
    full more flexible, simplify the related IPIs and doesn't bring
    significant overhead on irq exit.
    
    In a longer term view, it will allow us to piggyback the nohz kick
    on the scheduler IPI in the future instead of sending a dedicated IPI
    that often doubles the scheduler IPI on task wakeup. This will require
    more changes though including careful review of resched_curr() callers
    to include nohz full needs.
    
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 1ca93f2de6f5..7d35b0fec399 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -147,7 +147,6 @@ static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask)
 		cpumask_or(mask, mask, tick_nohz_full_mask);
 }
 
-extern void __tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
@@ -156,7 +155,6 @@ extern void __tick_nohz_task_switch(struct task_struct *tsk);
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
-static inline void __tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
@@ -190,12 +188,6 @@ static inline void housekeeping_affine(struct task_struct *t)
 #endif
 }
 
-static inline void tick_nohz_full_check(void)
-{
-	if (tick_nohz_full_enabled())
-		__tick_nohz_full_check();
-}
-
 static inline void tick_nohz_task_switch(struct task_struct *tsk)
 {
 	if (tick_nohz_full_enabled())

commit 03f6199a359e460714b6bd08c10b566760f150a6
Author: Chris Metcalf <cmetcalf@ezchip.com>
Date:   Fri Jul 10 15:37:25 2015 -0400

    nohz: Prevent tilegx network driver interrupts
    
    Normally the tilegx networking shim sends irqs to all the cores
    to distribute the load of processing incoming-packet interrupts,
    so that you can get to multiple Gb's of traffic inbound.
    
    However, in nohz_full mode we don't want to interrupt the
    nohz_full cores by default, so we limit the set of cores we use
    to only the online housekeeping cores.
    
    To make client code easier to read, we introduce a new nohz_full
    accessor, housekeeping_cpumask(), which returns a pointer to the
    housekeeping_mask if nohz_full is enabled, and otherwise returns
    the cpu_possible_mask.
    
    Signed-off-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index edbfc9a5293e..1ca93f2de6f5 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -163,6 +163,15 @@ static inline void tick_nohz_full_kick_all(void) { }
 static inline void __tick_nohz_task_switch(struct task_struct *tsk) { }
 #endif
 
+static inline const struct cpumask *housekeeping_cpumask(void)
+{
+#ifdef CONFIG_NO_HZ_FULL
+	if (tick_nohz_full_enabled())
+		return housekeeping_mask;
+#endif
+	return cpu_possible_mask;
+}
+
 static inline bool is_housekeeping_cpu(int cpu)
 {
 #ifdef CONFIG_NO_HZ_FULL

commit 37b64a42067a04a22468c4e52c12af00d72e462b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 21:56:34 2015 +0200

    tick/broadcast: Unbreak CONFIG_GENERIC_CLOCKEVENTS=n build
    
    Making tick_broadcast_oneshot_control() independent from
    CONFIG_GENERIC_CLOCKEVENTS_BROADCAST broke the build for
    CONFIG_GENERIC_CLOCKEVENTS=n because the function is not defined
    there.
    
    Provide a proper stub inline.
    
    Fixes: f32dd1170511 'tick/broadcast: Make idle check independent from mode and config'
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 6916dcb61857..edbfc9a5293e 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -67,7 +67,14 @@ extern void tick_broadcast_control(enum tick_broadcast_mode mode);
 static inline void tick_broadcast_control(enum tick_broadcast_mode mode) { }
 #endif /* BROADCAST */
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS
 extern int tick_broadcast_oneshot_control(enum tick_broadcast_state state);
+#else
+static inline int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
+{
+	return 0;
+}
+#endif
 
 static inline void tick_broadcast_enable(void)
 {

commit f32dd117051185da6e923b35491a44d7debeeea5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:29:38 2015 +0200

    tick/broadcast: Make idle check independent from mode and config
    
    Currently the broadcast busy check, which prevents the idle code from
    going into deep idle, works only in one shot mode.
    
    If NOHZ and HIGHRES are off (config or command line) there is no
    sanity check at all, so under certain conditions cpus are allowed to
    go into deep idle, where the local timer stops, and are not woken up
    again because there is no broadcast timer installed or a hrtimer based
    broadcast device is not evaluated.
    
    Move tick_broadcast_oneshot_control() into the common code and provide
    proper subfunctions for the various config combinations.
    
    The common check in tick_broadcast_oneshot_control() is for the C3STOP
    misfeature flag of the local clock event device. If its not set, idle
    can proceed. If set, further checks are necessary.
    
    Provide checks for the trivial cases:
    
     - If broadcast is disabled in the config, then return busy
    
     - If oneshot mode (NOHZ/HIGHES) is disabled in the config, return
       busy if the broadcast device is hrtimer based.
    
     - If oneshot mode is enabled in the config call the original
       tick_broadcast_oneshot_control() function. That function needs
       extra checks which will be implemented in seperate patches.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 3741ba1a652c..6916dcb61857 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -67,11 +67,7 @@ extern void tick_broadcast_control(enum tick_broadcast_mode mode);
 static inline void tick_broadcast_control(enum tick_broadcast_mode mode) { }
 #endif /* BROADCAST */
 
-#if defined(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST) && defined(CONFIG_TICK_ONESHOT)
 extern int tick_broadcast_oneshot_control(enum tick_broadcast_state state);
-#else
-static inline int tick_broadcast_oneshot_control(enum tick_broadcast_state state) { return 0; }
-#endif
 
 static inline void tick_broadcast_enable(void)
 {

commit 43c9fad942b5afb9e03801c0721d83160fa5b0dd
Merge: cb8a4deaf9b2 d461003574eb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 23 14:18:07 2015 -0700

    Merge tag 'pm+acpi-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "The rework of backlight interface selection API from Hans de Goede
      stands out from the number of commits and the number of affected
      places perspective.  The cpufreq core fixes from Viresh Kumar are
      quite significant too as far as the number of commits goes and because
      they should reduce CPU online/offline overhead quite a bit in the
      majority of cases.
    
      From the new featues point of view, the ACPICA update (to upstream
      revision 20150515) adding support for new ACPI 6 material to ACPICA is
      the one that matters the most as some new significant features will be
      based on it going forward.  Also included is an update of the ACPI
      device power management core to follow ACPI 6 (which in turn reflects
      the Windows' device PM implementation), a PM core extension to support
      wakeup interrupts in a more generic way and support for the ACPI _CCA
      device configuration object.
    
      The rest is mostly fixes and cleanups all over and some documentation
      updates, including new DT bindings for Operating Performance Points.
    
      There is one fix for a regression introduced in the 4.1 cycle, but it
      adds quite a number of lines of code, it wasn't really ready before
      Thursday and you were on vacation, so I refrained from pushing it on
      the last minute for 4.1.
    
      Specifics:
    
       - ACPICA update to upstream revision 20150515 including basic support
         for ACPI 6 features: new ACPI tables introduced by ACPI 6 (STAO,
         XENV, WPBT, NFIT, IORT), changes related to the other tables (DTRM,
         FADT, LPIT, MADT), new predefined names (_BTH, _CR3, _DSD, _LPI,
         _MTL, _PRR, _RDI, _RST, _TFP, _TSN), fixes and cleanups (Bob Moore,
         Lv Zheng).
    
       - ACPI device power management core code update to follow ACPI 6
         which reflects the ACPI device power management implementation in
         Windows (Rafael J Wysocki).
    
       - rework of the backlight interface selection logic to reduce the
         number of kernel command line options and improve the handling of
         DMI quirks that may be involved in that and to make the code
         generally more straightforward (Hans de Goede).
    
       - fixes for the ACPI Embedded Controller (EC) driver related to the
         handling of EC transactions (Lv Zheng).
    
       - fix for a regression related to the ACPI resources management and
         resulting from a recent change of ACPI initialization code ordering
         (Rafael J Wysocki).
    
       - fix for a system initialization regression related to ACPI
         introduced during the 3.14 cycle and caused by running the code
         that switches the platform over to the ACPI mode too early in the
         initialization sequence (Rafael J Wysocki).
    
       - support for the ACPI _CCA device configuration object related to
         DMA cache coherence (Suravee Suthikulpanit).
    
       - ACPI/APEI fixes and cleanups (Jiri Kosina, Borislav Petkov).
    
       - ACPI battery driver cleanups (Luis Henriques, Mathias Krause).
    
       - ACPI processor driver cleanups (Hanjun Guo).
    
       - cleanups and documentation update related to the ACPI device
         properties interface based on _DSD (Rafael J Wysocki).
    
       - ACPI device power management fixes (Rafael J Wysocki).
    
       - assorted cleanups related to ACPI (Dominik Brodowski, Fabian
         Frederick, Lorenzo Pieralisi, Mathias Krause, Rafael J Wysocki).
    
       - fix for a long-standing issue causing General Protection Faults to
         be generated occasionally on return to user space after resume from
         ACPI-based suspend-to-RAM on 32-bit x86 (Ingo Molnar).
    
       - fix to make the suspend core code return -EBUSY consistently in all
         cases when system suspend is aborted due to wakeup detection (Ruchi
         Kandoi).
    
       - support for automated device wakeup IRQ handling allowing drivers
         to make their PM support more starightforward (Tony Lindgren).
    
       - new tracepoints for suspend-to-idle tracing and rework of the
         prepare/complete callbacks tracing in the PM core (Todd E Brandt,
         Rafael J Wysocki).
    
       - wakeup sources framework enhancements (Jin Qian).
    
       - new macro for noirq system PM callbacks (Grygorii Strashko).
    
       - assorted cleanups related to system suspend (Rafael J Wysocki).
    
       - cpuidle core cleanups to make the code more efficient (Rafael J
         Wysocki).
    
       - powernv/pseries cpuidle driver update (Shilpasri G Bhat).
    
       - cpufreq core fixes related to CPU online/offline that should reduce
         the overhead of these operations quite a bit, unless the CPU in
         question is physically going away (Viresh Kumar, Saravana Kannan).
    
       - serialization of cpufreq governor callbacks to avoid race
         conditions in some cases (Viresh Kumar).
    
       - intel_pstate driver fixes and cleanups (Doug Smythies, Prarit
         Bhargava, Joe Konno).
    
       - cpufreq driver (arm_big_little, cpufreq-dt, qoriq) updates (Sudeep
         Holla, Felipe Balbi, Tang Yuantian).
    
       - assorted cleanups in cpufreq drivers and core (Shailendra Verma,
         Fabian Frederick, Wang Long).
    
       - new Device Tree bindings for representing Operating Performance
         Points (Viresh Kumar).
    
       - updates for the common clock operations support code in the PM core
         (Rajendra Nayak, Geert Uytterhoeven).
    
       - PM domains core code update (Geert Uytterhoeven).
    
       - Intel Knights Landing support for the RAPL (Running Average Power
         Limit) power capping driver (Dasaratharaman Chandramouli).
    
       - fixes related to the floor frequency setting on Atom SoCs in the
         RAPL power capping driver (Ajay Thomas).
    
       - runtime PM framework documentation update (Ben Dooks).
    
       - cpupower tool fix (Herton R Krzesinski)"
    
    * tag 'pm+acpi-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (194 commits)
      cpuidle: powernv/pseries: Auto-promotion of snooze to deeper idle state
      x86: Load __USER_DS into DS/ES after resume
      PM / OPP: Add binding for 'opp-suspend'
      PM / OPP: Allow multiple OPP tables to be passed via DT
      PM / OPP: Add new bindings to address shortcomings of existing bindings
      ACPI: Constify ACPI device IDs in documentation
      ACPI / enumeration: Document the rules regarding the PRP0001 device ID
      ACPI / video: Make acpi_video_unregister_backlight() private
      acpi-video-detect: Remove old API
      toshiba-acpi: Port to new backlight interface selection API
      thinkpad-acpi: Port to new backlight interface selection API
      sony-laptop: Port to new backlight interface selection API
      samsung-laptop: Port to new backlight interface selection API
      msi-wmi: Port to new backlight interface selection API
      msi-laptop: Port to new backlight interface selection API
      intel-oaktrail: Port to new backlight interface selection API
      ideapad-laptop: Port to new backlight interface selection API
      fujitsu-laptop: Port to new backlight interface selection API
      eeepc-laptop: Port to new backlight interface selection API
      dell-wmi: Port to new backlight interface selection API
      ...

commit 87e9b9f1d86c2ee9a10c2a4186a72d0af4cc963e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat May 16 01:38:15 2015 +0200

    PM / sleep: Make suspend-to-idle-specific code depend on CONFIG_SUSPEND
    
    Since idle_should_freeze() is defined to always return 'false'
    for CONFIG_SUSPEND unset, all of the code depending on it in
    cpuidle_idle_call() is not necessary in that case.
    
    Make that code depend on CONFIG_SUSPEND too to avoid building it
    when it is not going to be used.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f8492da57ad3..ec6e8bc992bf 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -13,8 +13,6 @@
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 extern void __init tick_init(void);
-extern void tick_freeze(void);
-extern void tick_unfreeze(void);
 /* Should be core only, but ARM BL switcher requires it */
 extern void tick_suspend_local(void);
 /* Should be core only, but XEN resume magic and ARM BL switcher require it */
@@ -23,14 +21,20 @@ extern void tick_handover_do_timer(void);
 extern void tick_cleanup_dead_cpu(int cpu);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
-static inline void tick_freeze(void) { }
-static inline void tick_unfreeze(void) { }
 static inline void tick_suspend_local(void) { }
 static inline void tick_resume_local(void) { }
 static inline void tick_handover_do_timer(void) { }
 static inline void tick_cleanup_dead_cpu(int cpu) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
+#if defined(CONFIG_GENERIC_CLOCKEVENTS) && defined(CONFIG_SUSPEND)
+extern void tick_freeze(void);
+extern void tick_unfreeze(void);
+#else
+static inline void tick_freeze(void) { }
+static inline void tick_unfreeze(void) { }
+#endif
+
 #ifdef CONFIG_TICK_ONESHOT
 extern void tick_irq_enter(void);
 #  ifndef arch_needs_cpu

commit 83dedea8a07fb4bf91863764b15c1c4ec00330f9
Author: Chris Metcalf <cmetcalf@ezchip.com>
Date:   Wed May 6 18:04:25 2015 +0200

    nohz: Add tick_nohz_full_add_cpus_to() API
    
    This API is useful to modify a cpumask indicating some special
    nohz-type functionality so that the nohz cores are automatically
    added to that set.
    
    Signed-off-by: Chris Metcalf <cmetcalf@ezchip.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Jones <davej@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1429024675-18938-1-git-send-email-cmetcalf@ezchip.com
    Link: http://lkml.kernel.org/r/1430928266-24888-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f8492da57ad3..4191b5623a28 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -134,6 +134,12 @@ static inline bool tick_nohz_full_cpu(int cpu)
 	return cpumask_test_cpu(cpu, tick_nohz_full_mask);
 }
 
+static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask)
+{
+	if (tick_nohz_full_enabled())
+		cpumask_or(mask, mask, tick_nohz_full_mask);
+}
+
 extern void __tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
@@ -142,6 +148,7 @@ extern void __tick_nohz_task_switch(struct task_struct *tsk);
 #else
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
+static inline void tick_nohz_full_add_cpus_to(struct cpumask *mask) { }
 static inline void __tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void tick_nohz_full_kick(void) { }

commit a49b116dcb1265f238f3169507424257b0519069
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:38:05 2015 +0200

    clockevents: Cleanup dead cpu explicitely
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the cleanup function for a dead cpu and invoke it
    directly from the cpu down code. Make it conditional on
    CPU_HOTPLUG as well.
    
    Temporary change, will be refined in the future.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased, added clockevents_notify() removal ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1735025.raBZdQHM3m@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 2c68fa3b9436..f8492da57ad3 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -20,6 +20,7 @@ extern void tick_suspend_local(void);
 /* Should be core only, but XEN resume magic and ARM BL switcher require it */
 extern void tick_resume_local(void);
 extern void tick_handover_do_timer(void);
+extern void tick_cleanup_dead_cpu(int cpu);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_freeze(void) { }
@@ -27,6 +28,7 @@ static inline void tick_unfreeze(void) { }
 static inline void tick_suspend_local(void) { }
 static inline void tick_resume_local(void) { }
 static inline void tick_handover_do_timer(void) { }
+static inline void tick_cleanup_dead_cpu(int cpu) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 #ifdef CONFIG_TICK_ONESHOT

commit 52c063d1adbc16c76e70fffa20727fcd4e9343b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:37:24 2015 +0200

    clockevents: Make tick handover explicit
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the tick_handover call and invoke it explicitely from
    the hotplug code. Temporary solution will be cleaned up in later
    patches.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebase ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1658173.RkEEILFiQZ@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 6119321fc3be..2c68fa3b9436 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -19,12 +19,14 @@ extern void tick_unfreeze(void);
 extern void tick_suspend_local(void);
 /* Should be core only, but XEN resume magic and ARM BL switcher require it */
 extern void tick_resume_local(void);
+extern void tick_handover_do_timer(void);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_freeze(void) { }
 static inline void tick_unfreeze(void) { }
 static inline void tick_suspend_local(void) { }
 static inline void tick_resume_local(void) { }
+static inline void tick_handover_do_timer(void) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 #ifdef CONFIG_TICK_ONESHOT

commit 1fe5d5c3c9ba0c4ade18e3325cba0ffe35127941
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:05:15 2015 +0200

    clockevents: Provide explicit broadcast oneshot control functions
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the broadcast oneshot control into a separate function
    and provide inline helpers. Switch clockevents_notify() over.
    This will go away once all callers are converted.
    
    This also gets rid of the nested locking of clockevents_lock and
    broadcast_lock. The broadcast oneshot control functions do not
    require clockevents_lock. Only the managing functions
    (setup/shutdown/suspend/resume of the broadcast device require
    clockevents_lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Alexandre Courbot <gnurou@gmail.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Warren <swarren@wwwdotorg.org>
    Cc: Thierry Reding <thierry.reding@gmail.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Link: http://lkml.kernel.org/r/13000649.8qZuEDV0OA@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 4bb236357b03..6119321fc3be 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -48,12 +48,23 @@ enum tick_broadcast_mode {
 	TICK_BROADCAST_FORCE,
 };
 
+enum tick_broadcast_state {
+	TICK_BROADCAST_EXIT,
+	TICK_BROADCAST_ENTER,
+};
+
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 extern void tick_broadcast_control(enum tick_broadcast_mode mode);
 #else
 static inline void tick_broadcast_control(enum tick_broadcast_mode mode) { }
 #endif /* BROADCAST */
 
+#if defined(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST) && defined(CONFIG_TICK_ONESHOT)
+extern int tick_broadcast_oneshot_control(enum tick_broadcast_state state);
+#else
+static inline int tick_broadcast_oneshot_control(enum tick_broadcast_state state) { return 0; }
+#endif
+
 static inline void tick_broadcast_enable(void)
 {
 	tick_broadcast_control(TICK_BROADCAST_ON);
@@ -66,6 +77,14 @@ static inline void tick_broadcast_force(void)
 {
 	tick_broadcast_control(TICK_BROADCAST_FORCE);
 }
+static inline int tick_broadcast_enter(void)
+{
+	return tick_broadcast_oneshot_control(TICK_BROADCAST_ENTER);
+}
+static inline void tick_broadcast_exit(void)
+{
+	tick_broadcast_oneshot_control(TICK_BROADCAST_EXIT);
+}
 
 #ifdef CONFIG_NO_HZ_COMMON
 extern int tick_nohz_tick_stopped(void);

commit 592a438ff3fea61d303c5784c209b3f1fd3e16df
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:01:10 2015 +0200

    clockevents: Provide explicit broadcast control functions
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the broadcast control into a separate function and
    provide inline helpers. Switch clockevents_notify() over. This
    will go away once all callers are converted.
    
    This also gets rid of the nested locking of clockevents_lock and
    broadcast_lock. The broadcast control functions do not require
    clockevents_lock. Only the managing functions
    (setup/shutdown/suspend/resume of the broadcast device require
    clockevents_lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tony Lindgren <tony@atomide.com>
    Link: http://lkml.kernel.org/r/8086559.ttsuS0n1Xr@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f9ff225d53c0..4bb236357b03 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -42,6 +42,31 @@ extern void hotplug_cpu__broadcast_tick_pull(int dead_cpu);
 static inline void hotplug_cpu__broadcast_tick_pull(int dead_cpu) { }
 #endif
 
+enum tick_broadcast_mode {
+	TICK_BROADCAST_OFF,
+	TICK_BROADCAST_ON,
+	TICK_BROADCAST_FORCE,
+};
+
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+extern void tick_broadcast_control(enum tick_broadcast_mode mode);
+#else
+static inline void tick_broadcast_control(enum tick_broadcast_mode mode) { }
+#endif /* BROADCAST */
+
+static inline void tick_broadcast_enable(void)
+{
+	tick_broadcast_control(TICK_BROADCAST_ON);
+}
+static inline void tick_broadcast_disable(void)
+{
+	tick_broadcast_control(TICK_BROADCAST_OFF);
+}
+static inline void tick_broadcast_force(void)
+{
+	tick_broadcast_control(TICK_BROADCAST_FORCE);
+}
+
 #ifdef CONFIG_NO_HZ_COMMON
 extern int tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);

commit 345527b1edce8df719e0884500c76832a18211c3
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Mon Mar 30 14:59:19 2015 +0530

    clockevents: Fix cpu_down() race for hrtimer based broadcasting
    
    It was found when doing a hotplug stress test on POWER, that the
    machine either hit softlockups or rcu_sched stall warnings.  The
    issue was traced to commit:
    
      7cba160ad789 ("powernv/cpuidle: Redesign idle states management")
    
    which exposed the cpu_down() race with hrtimer based broadcast mode:
    
      5d1638acb9f6 ("tick: Introduce hrtimer based broadcast")
    
    The race is the following:
    
    Assume CPU1 is the CPU which holds the hrtimer broadcasting duty
    before it is taken down.
    
            CPU0                                    CPU1
    
            cpu_down()                              take_cpu_down()
                                                    disable_interrupts()
    
            cpu_die()
    
            while (CPU1 != CPU_DEAD) {
                    msleep(100);
                    switch_to_idle();
                    stop_cpu_timer();
                    schedule_broadcast();
            }
    
            tick_cleanup_cpu_dead()
                    take_over_broadcast()
    
    So after CPU1 disabled interrupts it cannot handle the broadcast
    hrtimer anymore, so CPU0 will be stuck forever.
    
    Fix this by explicitly taking over broadcast duty before cpu_die().
    
    This is a temporary workaround. What we really want is a callback
    in the clockevent device which allows us to do that from the dying
    CPU by pushing the hrtimer onto a different cpu. That might involve
    an IPI and is definitely more complex than this immediate fix.
    
    Changelog was picked up from:
    
        https://lkml.org/lkml/2015/2/16/213
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: nicolas.pitre@linaro.org
    Cc: peterz@infradead.org
    Cc: rjw@rjwysocki.net
    Fixes: http://linuxppc.10917.n7.nabble.com/offlining-cpus-breakage-td88619.html
    Link: http://lkml.kernel.org/r/20150330092410.24979.59887.stgit@preeti.in.ibm.com
    [ Merged it to the latest timer tree, renamed the callback, tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 589868b09aff..f9ff225d53c0 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -36,6 +36,12 @@ extern void tick_irq_enter(void);
 static inline void tick_irq_enter(void) { }
 #endif
 
+#if defined(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST) && defined(CONFIG_TICK_ONESHOT)
+extern void hotplug_cpu__broadcast_tick_pull(int dead_cpu);
+#else
+static inline void hotplug_cpu__broadcast_tick_pull(int dead_cpu) { }
+#endif
+
 #ifdef CONFIG_NO_HZ_COMMON
 extern int tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);

commit 7270d11c56f594af4d166b2988421cd8ed933dc1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:11:52 2015 +0100

    arm/bL_switcher: Kill tick suspend hackery
    
    Use the new tick_suspend/resume_local() and get rid of the
    homebrewn implementation of these in the ARM bL switcher.  The
    check for the cpumask is completely pointless.  There is no harm
    to suspend a per cpu tick device unconditionally.  If that's a
    real issue then we fix it proper at the core level and not with
    some completely undocumented hacks in some random core code.
    
    Move the tick internals to the core code, now that this nuisance
    is gone.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ rjw: Rebase, changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Link: http://lkml.kernel.org/r/1655112.Ws17YsMfN7@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index a3d4d2840e7f..589868b09aff 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -11,30 +11,19 @@
 #include <linux/cpumask.h>
 #include <linux/sched.h>
 
-/* ARM BL switcher abuse support */
-#ifdef CONFIG_GENERIC_CLOCKEVENTS
-enum tick_device_mode {
-	TICKDEV_MODE_PERIODIC,
-	TICKDEV_MODE_ONESHOT,
-};
-
-struct tick_device {
-	struct clock_event_device *evtdev;
-	enum tick_device_mode mode;
-};
-extern struct tick_device *tick_get_device(int cpu);
-#endif
-
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 extern void __init tick_init(void);
 extern void tick_freeze(void);
 extern void tick_unfreeze(void);
-/* Should be core only, but XEN resume magic requires this */
+/* Should be core only, but ARM BL switcher requires it */
+extern void tick_suspend_local(void);
+/* Should be core only, but XEN resume magic and ARM BL switcher require it */
 extern void tick_resume_local(void);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_freeze(void) { }
 static inline void tick_unfreeze(void) { }
+static inline void tick_suspend_local(void) { }
 static inline void tick_resume_local(void) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 

commit f46481d0a7cb942b84145acb80ad43bdb1ff8eb4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:11:04 2015 +0100

    tick/xen: Provide and use tick_suspend_local() and tick_resume_local()
    
    Xen calls on every cpu into tick_resume() which is just wrong.
    tick_resume() is for the syscore global suspend/resume
    invocation. What XEN really wants is a per cpu local resume
    function.
    
    Provide a tick_resume_local() function and use it in XEN.
    
    Also provide a complementary tick_suspend_local() and modify
    tick_unfreeze() and tick_freeze(), respectively, to use the
    new local tick resume/suspend functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Combined two patches, rebased, modified subject/changelog. ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1698741.eezk9tnXtG@vostro.rjw.lan
    [ Merged to latest timers/core. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 7e07e0e3d898..a3d4d2840e7f 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -29,13 +29,13 @@ extern struct tick_device *tick_get_device(int cpu);
 extern void __init tick_init(void);
 extern void tick_freeze(void);
 extern void tick_unfreeze(void);
-/* Should be core only, but XEN resume magic abuses this interface */
-extern void tick_resume(void);
+/* Should be core only, but XEN resume magic requires this */
+extern void tick_resume_local(void);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_freeze(void) { }
 static inline void tick_unfreeze(void) { }
-static inline void tick_resume(void) { }
+static inline void tick_resume_local(void) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 #ifdef CONFIG_TICK_ONESHOT

commit 4ffee521f36390c7720d493591b764ca35c8030b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:09:16 2015 +0100

    clockevents: Make suspend/resume calls explicit
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call.
    
    We are way better off to have explicit calls instead of this
    monstrosity. Split out the suspend/resume() calls and invoke
    them directly from the call sites.
    
    No locking required at this point because these calls happen
    with interrupts disabled and a single cpu online.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased on top of 4.0-rc5. ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/713674030.jVm1qaHuPf@vostro.rjw.lan
    [ Rebased on top of latest timers/core. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f9a2d2687a46..7e07e0e3d898 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -29,10 +29,13 @@ extern struct tick_device *tick_get_device(int cpu);
 extern void __init tick_init(void);
 extern void tick_freeze(void);
 extern void tick_unfreeze(void);
+/* Should be core only, but XEN resume magic abuses this interface */
+extern void tick_resume(void);
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_freeze(void) { }
 static inline void tick_unfreeze(void) { }
+static inline void tick_resume(void) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 #ifdef CONFIG_TICK_ONESHOT

commit c1797baf6880174f899ce3960d0598f5bbeeb7ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:07:37 2015 +0100

    tick: Move core only declarations and functions to core
    
    No point to expose everything to the world. People just believe
    such functions can be abused for whatever purposes. Sigh.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased on top of 4.0-rc5 ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/28017337.VbCUc39Gme@vostro.rjw.lan
    [ Merged to latest timers/core ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9c085dc12ae9..f9a2d2687a46 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -1,7 +1,5 @@
-/*  linux/include/linux/tick.h
- *
- *  This file contains the structure definitions for tick related functions
- *
+/*
+ * Tick related global functions
  */
 #ifndef _LINUX_TICK_H
 #define _LINUX_TICK_H
@@ -9,13 +7,12 @@
 #include <linux/clockchips.h>
 #include <linux/irqflags.h>
 #include <linux/percpu.h>
-#include <linux/hrtimer.h>
 #include <linux/context_tracking_state.h>
 #include <linux/cpumask.h>
 #include <linux/sched.h>
 
+/* ARM BL switcher abuse support */
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
-
 enum tick_device_mode {
 	TICKDEV_MODE_PERIODIC,
 	TICKDEV_MODE_ONESHOT,
@@ -25,133 +22,38 @@ struct tick_device {
 	struct clock_event_device *evtdev;
 	enum tick_device_mode mode;
 };
-
-enum tick_nohz_mode {
-	NOHZ_MODE_INACTIVE,
-	NOHZ_MODE_LOWRES,
-	NOHZ_MODE_HIGHRES,
-};
-
-/**
- * struct tick_sched - sched tick emulation and no idle tick control/stats
- * @sched_timer:	hrtimer to schedule the periodic tick in high
- *			resolution mode
- * @last_tick:		Store the last tick expiry time when the tick
- *			timer is modified for nohz sleeps. This is necessary
- *			to resume the tick timer operation in the timeline
- *			when the CPU returns from nohz sleep.
- * @tick_stopped:	Indicator that the idle tick has been stopped
- * @idle_jiffies:	jiffies at the entry to idle for idle time accounting
- * @idle_calls:		Total number of idle calls
- * @idle_sleeps:	Number of idle calls, where the sched tick was stopped
- * @idle_entrytime:	Time when the idle call was entered
- * @idle_waketime:	Time when the idle was interrupted
- * @idle_exittime:	Time when the idle state was left
- * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
- * @iowait_sleeptime:	Sum of the time slept in idle with sched tick stopped, with IO outstanding
- * @sleep_length:	Duration of the current idle sleep
- * @do_timer_lst:	CPU was the last one doing do_timer before going idle
- */
-struct tick_sched {
-	struct hrtimer			sched_timer;
-	unsigned long			check_clocks;
-	enum tick_nohz_mode		nohz_mode;
-	ktime_t				last_tick;
-	int				inidle;
-	int				tick_stopped;
-	unsigned long			idle_jiffies;
-	unsigned long			idle_calls;
-	unsigned long			idle_sleeps;
-	int				idle_active;
-	ktime_t				idle_entrytime;
-	ktime_t				idle_waketime;
-	ktime_t				idle_exittime;
-	ktime_t				idle_sleeptime;
-	ktime_t				iowait_sleeptime;
-	ktime_t				sleep_length;
-	unsigned long			last_jiffies;
-	unsigned long			next_jiffies;
-	ktime_t				idle_expires;
-	int				do_timer_last;
-};
-
-extern void __init tick_init(void);
-extern int tick_is_oneshot_available(void);
 extern struct tick_device *tick_get_device(int cpu);
+#endif
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS
+extern void __init tick_init(void);
 extern void tick_freeze(void);
 extern void tick_unfreeze(void);
+#else /* CONFIG_GENERIC_CLOCKEVENTS */
+static inline void tick_init(void) { }
+static inline void tick_freeze(void) { }
+static inline void tick_unfreeze(void) { }
+#endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
-# ifdef CONFIG_HIGH_RES_TIMERS
-extern int tick_init_highres(void);
-extern int tick_program_event(ktime_t expires, int force);
-extern void tick_setup_sched_timer(void);
-# endif
-
-# if defined CONFIG_NO_HZ_COMMON || defined CONFIG_HIGH_RES_TIMERS
-extern void tick_cancel_sched_timer(int cpu);
-# else
-static inline void tick_cancel_sched_timer(int cpu) { }
-# endif
-
-# ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
-extern struct tick_device *tick_get_broadcast_device(void);
-extern struct cpumask *tick_get_broadcast_mask(void);
-
-#  ifdef CONFIG_TICK_ONESHOT
-extern struct cpumask *tick_get_broadcast_oneshot_mask(void);
-#  endif
-
-# endif /* BROADCAST */
-
-# ifdef CONFIG_TICK_ONESHOT
-extern void tick_clock_notify(void);
-extern int tick_check_oneshot_change(int allow_nohz);
-extern struct tick_sched *tick_get_tick_sched(int cpu);
+#ifdef CONFIG_TICK_ONESHOT
 extern void tick_irq_enter(void);
-extern int tick_oneshot_mode_active(void);
 #  ifndef arch_needs_cpu
 #   define arch_needs_cpu() (0)
 #  endif
 # else
-static inline void tick_clock_notify(void) { }
-static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
-static inline void tick_irq_enter(void) { }
-static inline int tick_oneshot_mode_active(void) { return 0; }
-# endif
-
-#else /* CONFIG_GENERIC_CLOCKEVENTS */
-static inline void tick_init(void) { }
-static inline void tick_freeze(void) { }
-static inline void tick_unfreeze(void) { }
-static inline void tick_cancel_sched_timer(int cpu) { }
-static inline void tick_clock_notify(void) { }
-static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 static inline void tick_irq_enter(void) { }
-static inline int tick_oneshot_mode_active(void) { return 0; }
-#endif /* !CONFIG_GENERIC_CLOCKEVENTS */
-
-# ifdef CONFIG_NO_HZ_COMMON
-DECLARE_PER_CPU(struct tick_sched, tick_cpu_sched);
-
-static inline int tick_nohz_tick_stopped(void)
-{
-	return __this_cpu_read(tick_cpu_sched.tick_stopped);
-}
+#endif
 
+#ifdef CONFIG_NO_HZ_COMMON
+extern int tick_nohz_tick_stopped(void);
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
-
-# else /* !CONFIG_NO_HZ_COMMON */
-static inline int tick_nohz_tick_stopped(void)
-{
-	return 0;
-}
-
+#else /* !CONFIG_NO_HZ_COMMON */
+static inline int tick_nohz_tick_stopped(void) { return 0; }
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 
@@ -163,7 +65,7 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
-# endif /* !CONFIG_NO_HZ_COMMON */
+#endif /* !CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool tick_nohz_full_running;

commit 124cf9117c5f93cc5b324530b7e105b09c729d5d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 13 23:50:43 2015 +0100

    PM / sleep: Make it possible to quiesce timers during suspend-to-idle
    
    The efficiency of suspend-to-idle depends on being able to keep CPUs
    in the deepest available idle states for as much time as possible.
    Ideally, they should only be brought out of idle by system wakeup
    interrupts.
    
    However, timer interrupts occurring periodically prevent that from
    happening and it is not practical to chase all of the "misbehaving"
    timers in a whack-a-mole fashion.  A much more effective approach is
    to suspend the local ticks for all CPUs and the entire timekeeping
    along the lines of what is done during full suspend, which also
    helps to keep suspend-to-idle and full suspend reasonably similar.
    
    The idea is to suspend the local tick on each CPU executing
    cpuidle_enter_freeze() and to make the last of them suspend the
    entire timekeeping.  That should prevent timer interrupts from
    triggering until an IO interrupt wakes up one of the CPUs.  It
    needs to be done with interrupts disabled on all of the CPUs,
    though, because otherwise the suspended clocksource might be
    accessed by an interrupt handler which might lead to fatal
    consequences.
    
    Unfortunately, the existing ->enter callbacks provided by cpuidle
    drivers generally cannot be used for implementing that, because some
    of them re-enable interrupts temporarily and some idle entry methods
    cause interrupts to be re-enabled automatically on exit.  Also some
    of these callbacks manipulate local clock event devices of the CPUs
    which really shouldn't be done after suspending their ticks.
    
    To overcome that difficulty, introduce a new cpuidle state callback,
    ->enter_freeze, that will be guaranteed (1) to keep interrupts
    disabled all the time (and return with interrupts disabled) and (2)
    not to touch the CPU timer devices.  Modify cpuidle_enter_freeze() to
    look for the deepest available idle state with ->enter_freeze present
    and to make the CPU execute that callback with suspended tick (and the
    last of the online CPUs to execute it with suspended timekeeping).
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index eda850ca757a..9c085dc12ae9 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -79,6 +79,9 @@ extern void __init tick_init(void);
 extern int tick_is_oneshot_available(void);
 extern struct tick_device *tick_get_device(int cpu);
 
+extern void tick_freeze(void);
+extern void tick_unfreeze(void);
+
 # ifdef CONFIG_HIGH_RES_TIMERS
 extern int tick_init_highres(void);
 extern int tick_program_event(ktime_t expires, int force);
@@ -119,6 +122,8 @@ static inline int tick_oneshot_mode_active(void) { return 0; }
 
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
+static inline void tick_freeze(void) { }
+static inline void tick_unfreeze(void) { }
 static inline void tick_cancel_sched_timer(int cpu) { }
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
@@ -226,5 +231,4 @@ static inline void tick_nohz_task_switch(struct task_struct *tsk)
 		__tick_nohz_task_switch(tsk);
 }
 
-
 #endif

commit 1ee07ef6b5db7235b133ee257a3adf507697e6b3
Merge: 77654908ff1a 0cccdda8d151
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 03:47:00 2014 +0200

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "This patch set contains the main portion of the changes for 3.18 in
      regard to the s390 architecture.  It is a bit bigger than usual,
      mainly because of a new driver and the vector extension patches.
    
      The interesting bits are:
       - Quite a bit of work on the tracing front.  Uprobes is enabled and
         the ftrace code is reworked to get some of the lost performance
         back if CONFIG_FTRACE is enabled.
       - To improve boot time with CONFIG_DEBIG_PAGEALLOC, support for the
         IPTE range facility is added.
       - The rwlock code is re-factored to improve writer fairness and to be
         able to use the interlocked-access instructions.
       - The kernel part for the support of the vector extension is added.
       - The device driver to access the CD/DVD on the HMC is added, this
         will hopefully come in handy to improve the installation process.
       - Add support for control-unit initiated reconfiguration.
       - The crypto device driver is enhanced to enable the additional AP
         domains and to allow the new crypto hardware to be used.
       - Bug fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (39 commits)
      s390/ftrace: simplify enabling/disabling of ftrace_graph_caller
      s390/ftrace: remove 31 bit ftrace support
      s390/kdump: add support for vector extension
      s390/disassembler: add vector instructions
      s390: add support for vector extension
      s390/zcrypt: Toleration of new crypto hardware
      s390/idle: consolidate idle functions and definitions
      s390/nohz: use a per-cpu flag for arch_needs_cpu
      s390/vtime: do not reset idle data on CPU hotplug
      s390/dasd: add support for control unit initiated reconfiguration
      s390/dasd: fix infinite loop during format
      s390/mm: make use of ipte range facility
      s390/setup: correct 4-level kernel page table detection
      s390/topology: call set_sched_topology early
      s390/uprobes: architecture backend for uprobes
      s390/uprobes: common library for kprobes and uprobes
      s390/rwlock: use the interlocked-access facility 1 instructions
      s390/rwlock: improve writer fairness
      s390/rwlock: remove interrupt-enabling rwlock variant.
      s390/mm: remove change bit override support
      ...

commit fe0f49768d807a8fe6336b097feb8c4441951710
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Sep 30 17:37:52 2014 +0200

    s390/nohz: use a per-cpu flag for arch_needs_cpu
    
    Move the nohz_delay bit from the s390_idle data structure to the
    per-cpu flags. Clear the nohz delay flag in __cpu_disable and
    remove the cpu hotplug notifier that used to do this.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9a82c7dc3fdd..e5832d03da19 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -108,7 +108,7 @@ extern struct tick_sched *tick_get_tick_sched(int cpu);
 extern void tick_irq_enter(void);
 extern int tick_oneshot_mode_active(void);
 #  ifndef arch_needs_cpu
-#   define arch_needs_cpu(cpu) (0)
+#   define arch_needs_cpu() (0)
 #  endif
 # else
 static inline void tick_clock_notify(void) { }

commit a80e49e2cc3145af014a8ae44f575829cc236192
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 16 17:47:18 2014 +0200

    nohz: Move nohz full init call to tick init
    
    This way we unbloat a bit main.c and more importantly we initialize
    nohz full after init_IRQ(). This dependency will be needed in further
    patches because nohz full needs irq work to raise its own IRQ.
    Information about the support for this ability on ARM64 is obtained on
    init_IRQ() which initialize the pointer to __smp_call_function.
    
    Since tick_init() is called right after init_IRQ(), this is a good place
    to call tick_nohz_init() and prepare for that dependency.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9a82c7dc3fdd..595ee86f5e0d 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -181,14 +181,12 @@ static inline bool tick_nohz_full_cpu(int cpu)
 	return cpumask_test_cpu(cpu, tick_nohz_full_mask);
 }
 
-extern void tick_nohz_init(void);
 extern void __tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
 extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(struct task_struct *tsk);
 #else
-static inline void tick_nohz_init(void) { }
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void __tick_nohz_full_check(void) { }

commit 40bea039593dfc7f3f9814dab844f6db43ae580b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 13 18:50:16 2014 +0200

    nohz: Restore NMI safe local irq work for local nohz kick
    
    The local nohz kick is currently used by perf which needs it to be
    NMI-safe. Recent commit though (7d1311b93e58ed55f3a31cc8f94c4b8fe988a2b9)
    changed its implementation to fire the local kick using the remote kick
    API. It was convenient to make the code more generic but the remote kick
    isn't NMI-safe.
    
    As a result:
    
            WARNING: CPU: 3 PID: 18062 at kernel/irq_work.c:72 irq_work_queue_on+0x11e/0x140()
            CPU: 3 PID: 18062 Comm: trinity-subchil Not tainted 3.16.0+ #34
            0000000000000009 00000000903774d1 ffff880244e06c00 ffffffff9a7f1e37
            0000000000000000 ffff880244e06c38 ffffffff9a0791dd ffff880244fce180
            0000000000000003 ffff880244e06d58 ffff880244e06ef8 0000000000000000
            Call Trace:
            <NMI>  [<ffffffff9a7f1e37>] dump_stack+0x4e/0x7a
            [<ffffffff9a0791dd>] warn_slowpath_common+0x7d/0xa0
            [<ffffffff9a07930a>] warn_slowpath_null+0x1a/0x20
            [<ffffffff9a17ca1e>] irq_work_queue_on+0x11e/0x140
            [<ffffffff9a10a2c7>] tick_nohz_full_kick_cpu+0x57/0x90
            [<ffffffff9a186cd5>] __perf_event_overflow+0x275/0x350
            [<ffffffff9a184f80>] ? perf_event_task_disable+0xa0/0xa0
            [<ffffffff9a01a4cf>] ? x86_perf_event_set_period+0xbf/0x150
            [<ffffffff9a187934>] perf_event_overflow+0x14/0x20
            [<ffffffff9a020386>] intel_pmu_handle_irq+0x206/0x410
            [<ffffffff9a0b54d3>] ? arch_vtime_task_switch+0x63/0x130
            [<ffffffff9a01937b>] perf_event_nmi_handler+0x2b/0x50
            [<ffffffff9a007b72>] nmi_handle+0xd2/0x390
            [<ffffffff9a007aa5>] ? nmi_handle+0x5/0x390
            [<ffffffff9a0d131b>] ? lock_release+0xab/0x330
            [<ffffffff9a008062>] default_do_nmi+0x72/0x1c0
            [<ffffffff9a0c925f>] ? cpuacct_account_field+0xcf/0x200
            [<ffffffff9a008268>] do_nmi+0xb8/0x100
    
    Lets fix this by restoring the use of local irq work for the nohz local
    kick.
    
    Reported-by: Catalin Iacob <iacobcatalin@gmail.com>
    Reported-and-tested-by: Dave Jones <davej@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 059052306831..9a82c7dc3fdd 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -183,13 +183,8 @@ static inline bool tick_nohz_full_cpu(int cpu)
 
 extern void tick_nohz_init(void);
 extern void __tick_nohz_full_check(void);
+extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_cpu(int cpu);
-
-static inline void tick_nohz_full_kick(void)
-{
-	tick_nohz_full_kick_cpu(smp_processor_id());
-}
-
 extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(struct task_struct *tsk);
 #else

commit 98959948a7ba33cf8c708626e0d2a1456397e1c6
Merge: ef35ad26f8ff cd3bd4e628a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 16:23:30 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Move the nohz kick code out of the scheduler tick to a dedicated IPI,
       from Frederic Weisbecker.
    
      This necessiated quite some background infrastructure rework,
      including:
    
       * Clean up some irq-work internals
       * Implement remote irq-work
       * Implement nohz kick on top of remote irq-work
       * Move full dynticks timer enqueue notification to new kick
       * Move multi-task notification to new kick
       * Remove unecessary barriers on multi-task notification
    
     - Remove proliferation of wait_on_bit() action functions and allow
       wait_on_bit_action() functions to support a timeout.  (Neil Brown)
    
     - Another round of sched/numa improvements, cleanups and fixes.  (Rik
       van Riel)
    
     - Implement fast idling of CPUs when the system is partially loaded,
       for better scalability.  (Tim Chen)
    
     - Restructure and fix the CPU hotplug handling code that may leave
       cfs_rq and rt_rq's throttled when tasks are migrated away from a dead
       cpu.  (Kirill Tkhai)
    
     - Robustify the sched topology setup code.  (Peterz Zijlstra)
    
     - Improve sched_feat() handling wrt.  static_keys (Jason Baron)
    
     - Misc fixes.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      sched/fair: Fix 'make xmldocs' warning caused by missing description
      sched: Use macro for magic number of -1 for setparam
      sched: Robustify topology setup
      sched: Fix sched_setparam() policy == -1 logic
      sched: Allow wait_on_bit_action() functions to support a timeout
      sched: Remove proliferation of wait_on_bit() action functions
      sched/numa: Revert "Use effective_load() to balance NUMA loads"
      sched: Fix static_key race with sched_feat()
      sched: Remove extra static_key*() function indirection
      sched/rt: Fix replenish_dl_entity() comments to match the current upstream code
      sched: Transform resched_task() into resched_curr()
      sched/deadline: Kill task_struct->pi_top_task
      sched: Rework check_for_tasks()
      sched/rt: Enqueue just unthrottled rt_rq back on the stack in __disable_runtime()
      sched/fair: Disable runtime_enabled on dying rq
      sched/numa: Change scan period code to match intent
      sched/numa: Rework best node setting in task_numa_migrate()
      sched/numa: Examine a task move when examining a task swap
      sched/numa: Simplify task_numa_compare()
      sched/numa: Use effective_load() to balance NUMA loads
      ...

commit c0f489d2c6fec8994c642c2ec925eb858727dc7b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 4 13:46:03 2014 -0700

    rcu: Bind grace-period kthreads to non-NO_HZ_FULL CPUs
    
    Binding the grace-period kthreads to the timekeeping CPU resulted in
    significant performance decreases for some workloads.  For more detail,
    see:
    
    https://lkml.org/lkml/2014/6/3/395 for benchmark numbers
    
    https://lkml.org/lkml/2014/6/4/218 for CPU statistics
    
    It turns out that it is necessary to bind the grace-period kthreads
    to the timekeeping CPU only when all but CPU 0 is a nohz_full CPU
    on the one hand or if CONFIG_NO_HZ_FULL_SYSIDLE=y on the other.
    In other cases, it suffices to bind the grace-period kthreads to the
    set of non-nohz_full CPUs.
    
    This commit therefore creates a tick_nohz_not_full_mask that is the
    complement of tick_nohz_full_mask, and then binds the grace-period
    kthread to the set of CPUs indicated by this new mask, which covers
    the CONFIG_NO_HZ_FULL_SYSIDLE=n case.  The CONFIG_NO_HZ_FULL_SYSIDLE=y
    case still binds the grace-period kthreads to the timekeeping CPU.
    This commit also includes the tick_nohz_full_enabled() check suggested
    by Frederic Weisbecker.
    
    Reported-by: Jet Chen <jet.chen@intel.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Created housekeeping_affine() and housekeeping_mask per
      fweisbec feedback. ]

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b84773cb9f4c..06cc093ab7ad 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -12,6 +12,7 @@
 #include <linux/hrtimer.h>
 #include <linux/context_tracking_state.h>
 #include <linux/cpumask.h>
+#include <linux/sched.h>
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 
@@ -162,6 +163,7 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 #ifdef CONFIG_NO_HZ_FULL
 extern bool tick_nohz_full_running;
 extern cpumask_var_t tick_nohz_full_mask;
+extern cpumask_var_t housekeeping_mask;
 
 static inline bool tick_nohz_full_enabled(void)
 {
@@ -194,6 +196,24 @@ static inline void tick_nohz_full_kick_all(void) { }
 static inline void __tick_nohz_task_switch(struct task_struct *tsk) { }
 #endif
 
+static inline bool is_housekeeping_cpu(int cpu)
+{
+#ifdef CONFIG_NO_HZ_FULL
+	if (tick_nohz_full_enabled())
+		return cpumask_test_cpu(cpu, housekeeping_mask);
+#endif
+	return true;
+}
+
+static inline void housekeeping_affine(struct task_struct *t)
+{
+#ifdef CONFIG_NO_HZ_FULL
+	if (tick_nohz_full_enabled())
+		set_cpus_allowed_ptr(t, housekeeping_mask);
+
+#endif
+}
+
 static inline void tick_nohz_full_check(void)
 {
 	if (tick_nohz_full_enabled())

commit 3d36aebc2e78923095575df954f3f3b430ac0a30
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 4 16:17:33 2014 +0200

    nohz: Support nohz full remote kick
    
    Remotely kicking a full nohz CPU in order to make it re-evaluate its
    next tick is currently implemented using the scheduler IPI.
    
    However this bloats a scheduler fast path with an off-topic feature.
    The scheduler tick was abused here for its cool "callable
    anywhere/anytime" properties.
    
    But now that the irq work subsystem can queue remote callbacks, it's
    a perfect fit to safely queue IPIs when interrupts are disabled
    without worrying about concurrent callers.
    
    So lets implement remote kick on top of irq work. This is going to
    be used when a new event requires the next tick to be recalculated:
    more than 1 task competing on the CPU, timer armed, ...
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b84773cb9f4c..8a4987f2294a 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -181,7 +181,13 @@ static inline bool tick_nohz_full_cpu(int cpu)
 
 extern void tick_nohz_init(void);
 extern void __tick_nohz_full_check(void);
-extern void tick_nohz_full_kick(void);
+extern void tick_nohz_full_kick_cpu(int cpu);
+
+static inline void tick_nohz_full_kick(void)
+{
+	tick_nohz_full_kick_cpu(smp_processor_id());
+}
+
 extern void tick_nohz_full_kick_all(void);
 extern void __tick_nohz_task_switch(struct task_struct *tsk);
 #else
@@ -189,6 +195,7 @@ static inline void tick_nohz_init(void) { }
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void __tick_nohz_full_check(void) { }
+static inline void tick_nohz_full_kick_cpu(int cpu) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
 static inline void __tick_nohz_task_switch(struct task_struct *tsk) { }

commit 5acac1be499d979e3aa463ea73a498888faefcbe
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 4 18:28:20 2013 +0100

    tick: Rename tick_check_idle() to tick_irq_enter()
    
    This makes the code more symetric against the existing tick functions
    called on irq exit: tick_irq_exit() and tick_nohz_irq_exit().
    
    These function are also symetric as they mirror each other's action:
    we start to account idle time on irq exit and we stop this accounting
    on irq entry. Also the tick is stopped on irq exit and timekeeping
    catches up with the tickless time elapsed until we reach irq entry.
    
    This rename was suggested by Peter Zijlstra a long while ago but it
    got forgotten in the mass.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Link: http://lkml.kernel.org/r/1387320692-28460-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0175d8663b6c..b84773cb9f4c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -104,7 +104,7 @@ extern struct cpumask *tick_get_broadcast_oneshot_mask(void);
 extern void tick_clock_notify(void);
 extern int tick_check_oneshot_change(int allow_nohz);
 extern struct tick_sched *tick_get_tick_sched(int cpu);
-extern void tick_check_idle(void);
+extern void tick_irq_enter(void);
 extern int tick_oneshot_mode_active(void);
 #  ifndef arch_needs_cpu
 #   define arch_needs_cpu(cpu) (0)
@@ -112,7 +112,7 @@ extern int tick_oneshot_mode_active(void);
 # else
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
-static inline void tick_check_idle(void) { }
+static inline void tick_irq_enter(void) { }
 static inline int tick_oneshot_mode_active(void) { return 0; }
 # endif
 
@@ -121,7 +121,7 @@ static inline void tick_init(void) { }
 static inline void tick_cancel_sched_timer(int cpu) { }
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
-static inline void tick_check_idle(void) { }
+static inline void tick_irq_enter(void) { }
 static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 

commit 58135f574f1b791c926622387780ed3d090116d6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 6 14:45:57 2013 +0100

    context_tracking: Wrap static key check into more intuitive function name
    
    Use a function with a meaningful name to check the global context
    tracking state. static_key_false() is a bit confusing for reviewers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index a004f66a6cf0..0175d8663b6c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -165,7 +165,7 @@ extern cpumask_var_t tick_nohz_full_mask;
 
 static inline bool tick_nohz_full_enabled(void)
 {
-	if (!static_key_false(&context_tracking_enabled))
+	if (!context_tracking_is_enabled())
 		return false;
 
 	return tick_nohz_full_running;

commit e8fcaa5c54e3b0371230e5d43a6f650c667da9c5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 7 22:28:01 2013 +0200

    nohz: Convert a few places to use local per cpu accesses
    
    A few functions use remote per CPU access APIs when they
    deal with local values.
    
    Just do the right conversion to improve performance, code
    readability and debug checks.
    
    While at it, lets extend some of these function names with *_this_cpu()
    suffix in order to display their purpose more clearly.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 5128d33bbb39..a004f66a6cf0 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -104,7 +104,7 @@ extern struct cpumask *tick_get_broadcast_oneshot_mask(void);
 extern void tick_clock_notify(void);
 extern int tick_check_oneshot_change(int allow_nohz);
 extern struct tick_sched *tick_get_tick_sched(int cpu);
-extern void tick_check_idle(int cpu);
+extern void tick_check_idle(void);
 extern int tick_oneshot_mode_active(void);
 #  ifndef arch_needs_cpu
 #   define arch_needs_cpu(cpu) (0)
@@ -112,7 +112,7 @@ extern int tick_oneshot_mode_active(void);
 # else
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
-static inline void tick_check_idle(int cpu) { }
+static inline void tick_check_idle(void) { }
 static inline int tick_oneshot_mode_active(void) { return 0; }
 # endif
 
@@ -121,7 +121,7 @@ static inline void tick_init(void) { }
 static inline void tick_cancel_sched_timer(int cpu) { }
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
-static inline void tick_check_idle(int cpu) { }
+static inline void tick_check_idle(void) { }
 static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 

commit 6f1d657668ac3041b65265d3653d7e9172a0d603
Merge: d4e4ab86bcba d13508f9440e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Aug 14 17:58:56 2013 +0200

    Merge branch 'timers/nohz-v3' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/nohz
    
    Pull nohz improvements from Frederic Weisbecker:
    
     " It mostly contains fixes and full dynticks off-case optimizations. I believe that
       distros want to enable this feature so it seems important to optimize the case
       where the "nohz_full=" parameter is empty. ie: I'm trying to remove any performance
       regression that comes with NO_HZ_FULL=y when the feature is not used.
    
       This patchset improves the current situation a lot (off-case appears to be around 11% faster
       with hackbench, although I guess it may vary depending on the configuration but it should be
       significantly faster in any case) now there is still some work to do: I can still observe a
       remaining loss of 1.6% throughput seen with hackbench compared to CONFIG_NO_HZ_FULL=n. "
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d13508f9440e46dccac6a2dd48d51a73b2207482
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 24 23:52:27 2013 +0200

    nohz: Optimize full dynticks's sched hooks with static keys
    
    Scheduler IPIs and task context switches are serious fast path.
    Let's try to hide as much as we can the impact of full
    dynticks APIs' off case that are called on these sites
    through the use of static keys.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index c60b079e1b37..a7ef1d6fceb6 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -180,20 +180,32 @@ static inline bool tick_nohz_full_cpu(int cpu)
 }
 
 extern void tick_nohz_init(void);
-extern void tick_nohz_full_check(void);
+extern void __tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_all(void);
-extern void tick_nohz_task_switch(struct task_struct *tsk);
+extern void __tick_nohz_task_switch(struct task_struct *tsk);
 #else
 static inline void tick_nohz_init(void) { }
 static inline bool tick_nohz_full_enabled(void) { return false; }
 static inline bool tick_nohz_full_cpu(int cpu) { return false; }
-static inline void tick_nohz_full_check(void) { }
+static inline void __tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
-static inline void tick_nohz_task_switch(struct task_struct *tsk) { }
+static inline void __tick_nohz_task_switch(struct task_struct *tsk) { }
 #endif
 
+static inline void tick_nohz_full_check(void)
+{
+	if (tick_nohz_full_enabled())
+		__tick_nohz_full_check();
+}
+
+static inline void tick_nohz_task_switch(struct task_struct *tsk)
+{
+	if (tick_nohz_full_enabled())
+		__tick_nohz_task_switch(tsk);
+}
+
 
 # ifdef CONFIG_CPU_IDLE_GOV_MENU
 extern void menu_hrtimer_cancel(void);

commit 460775df4680b4593d8449bc171008578625a850
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 24 23:52:27 2013 +0200

    nohz: Optimize full dynticks state checks with static keys
    
    These APIs are frequenctly accessed and priority is given
    to optimize the full dynticks off-case in order to let
    distros enable this feature without suffering from
    significant performance regressions.
    
    Let's inline these APIs and optimize them with static keys.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9180f4b85e6d..c60b079e1b37 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -10,6 +10,8 @@
 #include <linux/irqflags.h>
 #include <linux/percpu.h>
 #include <linux/hrtimer.h>
+#include <linux/context_tracking_state.h>
+#include <linux/cpumask.h>
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 
@@ -158,15 +160,34 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_FULL
+extern bool tick_nohz_full_running;
+extern cpumask_var_t tick_nohz_full_mask;
+
+static inline bool tick_nohz_full_enabled(void)
+{
+	if (!static_key_false(&context_tracking_enabled))
+		return false;
+
+	return tick_nohz_full_running;
+}
+
+static inline bool tick_nohz_full_cpu(int cpu)
+{
+	if (!tick_nohz_full_enabled())
+		return false;
+
+	return cpumask_test_cpu(cpu, tick_nohz_full_mask);
+}
+
 extern void tick_nohz_init(void);
-extern int tick_nohz_full_cpu(int cpu);
 extern void tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_all(void);
 extern void tick_nohz_task_switch(struct task_struct *tsk);
 #else
 static inline void tick_nohz_init(void) { }
-static inline int tick_nohz_full_cpu(int cpu) { return 0; }
+static inline bool tick_nohz_full_enabled(void) { return false; }
+static inline bool tick_nohz_full_cpu(int cpu) { return false; }
 static inline void tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }

commit 148519120c6d1f19ad53349683aeae9f228b0b8d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jul 27 01:41:34 2013 +0200

    Revert "cpuidle: Quickly notice prediction failure for repeat mode"
    
    Revert commit 69a37bea (cpuidle: Quickly notice prediction failure for
    repeat mode), because it has been identified as the source of a
    significant performance regression in v3.8 and later as explained by
    Jeremy Eder:
    
      We believe we've identified a particular commit to the cpuidle code
      that seems to be impacting performance of variety of workloads.
      The simplest way to reproduce is using netperf TCP_RR test, so
      we're using that, on a pair of Sandy Bridge based servers.  We also
      have data from a large database setup where performance is also
      measurably/positively impacted, though that test data isn't easily
      share-able.
    
      Included below are test results from 3 test kernels:
    
      kernel       reverts
      -----------------------------------------------------------
      1) vanilla   upstream (no reverts)
    
      2) perfteam2 reverts e11538d1f03914eb92af5a1a378375c05ae8520c
    
      3) test      reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
                           e11538d1f03914eb92af5a1a378375c05ae8520c
    
      In summary, netperf TCP_RR numbers improve by approximately 4%
      after reverting 69a37beabf1f0a6705c08e879bdd5d82ff6486c4.  When
      69a37beabf1f0a6705c08e879bdd5d82ff6486c4 is included, C0 residency
      never seems to get above 40%.  Taking that patch out gets C0 near
      100% quite often, and performance increases.
    
      The below data are histograms representing the %c0 residency @
      1-second sample rates (using turbostat), while under netperf test.
    
      - If you look at the first 4 histograms, you can see %c0 residency
        almost entirely in the 30,40% bin.
      - The last pair, which reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4,
        shows %c0 in the 80,90,100% bins.
    
      Below each kernel name are netperf TCP_RR trans/s numbers for the
      particular kernel that can be disclosed publicly, comparing the 3
      test kernels.  We ran a 4th test with the vanilla kernel where
      we've also set /dev/cpu_dma_latency=0 to show overall impact
      boosting single-threaded TCP_RR performance over 11% above
      baseline.
    
      3.10-rc2 vanilla RX + c0 lock (/dev/cpu_dma_latency=0):
      TCP_RR trans/s 54323.78
    
      -----------------------------------------------------------
      3.10-rc2 vanilla RX (no reverts)
      TCP_RR trans/s 48192.47
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    59]:
      ***********************************************************
         40.0000 -    50.0000 [     1]: *
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      Sender %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    11]: ***********
         40.0000 -    50.0000 [    49]:
      *************************************************
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      -----------------------------------------------------------
      3.10-rc2 perfteam2 RX (reverts commit
      e11538d1f03914eb92af5a1a378375c05ae8520c)
      TCP_RR trans/s 49698.69
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     1]: *
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    59]:
      ***********************************************************
         40.0000 -    50.0000 [     0]:
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      Sender %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [     2]: **
         40.0000 -    50.0000 [    58]:
      **********************************************************
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      -----------------------------------------------------------
      3.10-rc2 test RX (reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
      and e11538d1f03914eb92af5a1a378375c05ae8520c)
      TCP_RR trans/s 47766.95
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     1]: *
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    27]: ***************************
         40.0000 -    50.0000 [     2]: **
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     2]: **
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [    28]: ****************************
    
      Sender:
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    11]: ***********
         40.0000 -    50.0000 [     0]:
         50.0000 -    60.0000 [     1]: *
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     3]: ***
         80.0000 -    90.0000 [     7]: *******
         90.0000 -   100.0000 [    38]: **************************************
    
      These results demonstrate gaining back the tendency of the CPU to
      stay in more responsive, performant C-states (and thus yield
      measurably better performance), by reverting commit
      69a37beabf1f0a6705c08e879bdd5d82ff6486c4.
    
    Requested-by: Jeremy Eder <jeder@redhat.com>
    Tested-by: Len Brown <len.brown@intel.com>
    Cc: 3.8+ <stable@vger.kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9180f4b85e6d..62bd8b72873c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -174,10 +174,4 @@ static inline void tick_nohz_task_switch(struct task_struct *tsk) { }
 #endif
 
 
-# ifdef CONFIG_CPU_IDLE_GOV_MENU
-extern void menu_hrtimer_cancel(void);
-# else
-static inline void menu_hrtimer_cancel(void) {}
-# endif /* CONFIG_CPU_IDLE_GOV_MENU */
-
 #endif

commit 99e5ada9407cc19d7c4c05ce2165f20dc46fc093
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 17:11:50 2013 +0200

    nohz: Re-evaluate the tick for the new task after a context switch
    
    When a task is scheduled in, it may have some properties
    of its own that could make the CPU reconsider the need for
    the tick: posix cpu timers, perf events, ...
    
    So notify the full dynticks subsystem when a task gets
    scheduled in and re-check the tick dependency at this
    stage. This is done through a self IPI to avoid messing
    up with any current lock scenario.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index e31e67623ea1..9180f4b85e6d 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -163,12 +163,14 @@ extern int tick_nohz_full_cpu(int cpu);
 extern void tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_all(void);
+extern void tick_nohz_task_switch(struct task_struct *tsk);
 #else
 static inline void tick_nohz_init(void) { }
 static inline int tick_nohz_full_cpu(int cpu) { return 0; }
 static inline void tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
+static inline void tick_nohz_task_switch(struct task_struct *tsk) { }
 #endif
 
 

commit ff442c51f6543378cf23107c75b7949dc64a9119
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 15:27:08 2013 +0200

    nohz: Re-evaluate the tick from the scheduler IPI
    
    The scheduler IPI is used by the scheduler to kick
    full dynticks CPUs asynchronously when more than one
    task are running or when a new timer list timer is
    enqueued. This way the destination CPU can decide
    to restart the tick to handle this new situation.
    
    Now let's call that kick in the scheduler IPI.
    
    (Reusing the scheduler IPI rather than implementing
    a new IPI was suggested by Peter Zijlstra a while ago)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index d290168335bc..e31e67623ea1 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -160,11 +160,13 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 #ifdef CONFIG_NO_HZ_FULL
 extern void tick_nohz_init(void);
 extern int tick_nohz_full_cpu(int cpu);
+extern void tick_nohz_full_check(void);
 extern void tick_nohz_full_kick(void);
 extern void tick_nohz_full_kick_all(void);
 #else
 static inline void tick_nohz_init(void) { }
 static inline int tick_nohz_full_cpu(int cpu) { return 0; }
+static inline void tick_nohz_full_check(void) { }
 static inline void tick_nohz_full_kick(void) { }
 static inline void tick_nohz_full_kick_all(void) { }
 #endif

commit a166fcf04d848ffa09f0e831805553089f190cf4
Merge: 2727872dfe5d 555347f6c080
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 21 11:05:47 2013 +0200

    Merge branch 'timers/nohz-posix-timers-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/nohz
    
    Pull posix cpu timers handling on full dynticks from Frederic Weisbecker.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d1e43fa5f8bb25f83a86a29f11fcfb57ed4d7566
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Mar 26 23:47:24 2013 +0100

    nohz: Ensure full dynticks CPUs are RCU nocbs
    
    We need full dynticks CPU to also be RCU nocb so
    that we don't have to keep the tick to handle RCU
    callbacks.
    
    Make sure the range passed to nohz_full= boot
    parameter is a subset of rcu_nocbs=
    
    The CPUs that fail to meet this requirement will be
    excluded from the nohz_full range. This is checked
    early in boot time, before any CPU has the opportunity
    to stop its tick.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b4e3b0c9639e..0b6873cbf512 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -158,8 +158,10 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_FULL
+extern void tick_nohz_init(void);
 extern int tick_nohz_full_cpu(int cpu);
 #else
+static inline void tick_nohz_init(void) { }
 static inline int tick_nohz_full_cpu(int cpu) { return 0; }
 #endif
 

commit 76c24fb054b52b34af4dcde589cbb9e2b98fc74c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Apr 18 00:15:40 2013 +0200

    nohz: New APIs to re-evaluate the tick on full dynticks CPUs
    
    Provide two new helpers in order to notify the full dynticks CPUs about
    some internal system changes against which they may reconsider the state
    of their tick. Some practical examples include: posix cpu timers, perf tick
    and sched clock tick.
    
    For now the notifying handler, implemented through IPIs, is a stub
    that will be implemented when we get the tick stop/restart infrastructure
    in.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b4e3b0c9639e..c2dcfb18f65b 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -159,8 +159,12 @@ static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 
 #ifdef CONFIG_NO_HZ_FULL
 extern int tick_nohz_full_cpu(int cpu);
+extern void tick_nohz_full_kick(void);
+extern void tick_nohz_full_kick_all(void);
 #else
 static inline int tick_nohz_full_cpu(int cpu) { return 0; }
+static inline void tick_nohz_full_kick(void) { }
+static inline void tick_nohz_full_kick_all(void) { }
 #endif
 
 

commit c5bfece2d6129131b4ade985e63bc35ddf5868d4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Apr 12 16:45:34 2013 +0200

    nohz: Switch from "extended nohz" to "full nohz" based naming
    
    "Extended nohz" was used as a naming base for the full dynticks
    API and Kconfig symbols. It reflects the fact the system tries
    to stop the tick in more places than just idle.
    
    But that "extended" name is a bit opaque and vague. Rename it to
    "full" makes it clearer what the system tries to do under this
    config: try to shutdown the tick anytime it can. The various
    constraints that prevent that to happen shouldn't be considered
    as fundamental properties of this feature but rather technical
    issues that may be solved in the future.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 5e403339ee14..b4e3b0c9639e 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -157,10 +157,10 @@ static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !CONFIG_NO_HZ_COMMON */
 
-#ifdef CONFIG_NO_HZ_EXTENDED
-extern int tick_nohz_extended_cpu(int cpu);
+#ifdef CONFIG_NO_HZ_FULL
+extern int tick_nohz_full_cpu(int cpu);
 #else
-static inline int tick_nohz_extended_cpu(int cpu) { return 0; }
+static inline int tick_nohz_full_cpu(int cpu) { return 0; }
 #endif
 
 

commit 3451d0243c3cdfd729b36f9684a14659d4895ca3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 10 23:21:01 2011 +0200

    nohz: Rename CONFIG_NO_HZ to CONFIG_NO_HZ_COMMON
    
    We are planning to convert the dynticks Kconfig options layout
    into a choice menu. The user must be able to easily pick
    any of the following implementations: constant periodic tick,
    idle dynticks, full dynticks.
    
    As this implies a mutual exclusion, the two dynticks implementions
    need to converge on the selection of a common Kconfig option in order
    to ease the sharing of a common infrastructure.
    
    It would thus seem pretty natural to reuse CONFIG_NO_HZ to
    that end. It already implements all the idle dynticks code
    and the full dynticks depends on all that code for now.
    So ideally the choice menu would propose CONFIG_NO_HZ_IDLE and
    CONFIG_NO_HZ_EXTENDED then both would select CONFIG_NO_HZ.
    
    On the other hand we want to stay backward compatible: if
    CONFIG_NO_HZ is set in an older config file, we want to
    enable CONFIG_NO_HZ_IDLE by default.
    
    But we can't afford both at the same time or we run into
    a circular dependency:
    
    1) CONFIG_NO_HZ_IDLE and CONFIG_NO_HZ_EXTENDED both select
       CONFIG_NO_HZ
    2) If CONFIG_NO_HZ is set, we default to CONFIG_NO_HZ_IDLE
    
    We might be able to support that from Kconfig/Kbuild but it
    may not be wise to introduce such a confusing behaviour.
    
    So to solve this, create a new CONFIG_NO_HZ_COMMON option
    which gathers the common code between idle and full dynticks
    (that common code for now is simply the idle dynticks code)
    and select it from their referring Kconfig.
    
    Then we'll later create CONFIG_NO_HZ_IDLE and map CONFIG_NO_HZ
    to it for backward compatibility.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 44bfa8aa439f..5e403339ee14 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -82,7 +82,7 @@ extern int tick_program_event(ktime_t expires, int force);
 extern void tick_setup_sched_timer(void);
 # endif
 
-# if defined CONFIG_NO_HZ || defined CONFIG_HIGH_RES_TIMERS
+# if defined CONFIG_NO_HZ_COMMON || defined CONFIG_HIGH_RES_TIMERS
 extern void tick_cancel_sched_timer(int cpu);
 # else
 static inline void tick_cancel_sched_timer(int cpu) { }
@@ -123,7 +123,7 @@ static inline void tick_check_idle(int cpu) { }
 static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
-# ifdef CONFIG_NO_HZ
+# ifdef CONFIG_NO_HZ_COMMON
 DECLARE_PER_CPU(struct tick_sched, tick_cpu_sched);
 
 static inline int tick_nohz_tick_stopped(void)
@@ -138,7 +138,7 @@ extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 
-# else /* !CONFIG_NO_HZ */
+# else /* !CONFIG_NO_HZ_COMMON */
 static inline int tick_nohz_tick_stopped(void)
 {
 	return 0;
@@ -155,7 +155,7 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
-# endif /* !NO_HZ */
+# endif /* !CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_EXTENDED
 extern int tick_nohz_extended_cpu(int cpu);

commit a831881be220358a1d28c5d95d69449fb6d623ca
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Dec 18 17:32:19 2012 +0100

    nohz: Basic full dynticks interface
    
    For extreme usecases such as Real Time or HPC, having
    the ability to shutdown the tick when a single task runs
    on a CPU is a desired feature:
    
    * Reducing the amount of interrupts improves throughput
    for CPU-bound tasks. The CPU is less distracted from its
    real job, from an execution time and from the cache point
    of views.
    
    * This also improve latency response as we have less critical
    sections.
    
    Start with introducing a very simple interface to define
    full dynticks CPU: use a boot time option defined cpumask
    through the "nohz_extended=" kernel parameter. CPUs that
    are part of this range will have their tick shutdown
    whenever possible: provided they run a single task and
    they don't do kernel activity that require the periodic
    tick. These details will be later documented in
    Documentation/*
    
    An online CPU must be kept outside this range to handle the
    timekeeping.
    
    Suggested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 553272e6af55..44bfa8aa439f 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -157,6 +157,13 @@ static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !NO_HZ */
 
+#ifdef CONFIG_NO_HZ_EXTENDED
+extern int tick_nohz_extended_cpu(int cpu);
+#else
+static inline int tick_nohz_extended_cpu(int cpu) { return 0; }
+#endif
+
+
 # ifdef CONFIG_CPU_IDLE_GOV_MENU
 extern void menu_hrtimer_cancel(void);
 # else

commit 077931446b85e7858bf9dc0927cd116669b965d2
Merge: f7c819c020db 74876a98a87a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Feb 5 00:48:46 2013 +0100

    Merge branch 'nohz/printk-v8' into irq/core
    
    Conflicts:
            kernel/irq_work.c
    
    Add support for printk in full dynticks CPU.
    
    * Don't stop tick with irq works pending. This
    fix is generally useful and concerns archs that
    can't raise self IPIs.
    
    * Flush irq works before CPU offlining.
    
    * Introduce "lazy" irq works that can wait for the
    next tick to be executed, unless it's stopped.
    
    * Implement klogd wake up using irq work. This
    removes the ad-hoc printk_tick()/printk_needs_cpu()
    hooks and make it working even in dynticks mode.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 33a5f6261a61af28f7b4c86f9f958da0f206c914
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 11 17:52:56 2012 +0200

    nohz: Add API to check tick state
    
    We need some quick way to check if the CPU has stopped
    its tick. This will be useful to implement the printk tick
    using the irq work subsystem.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f37fceb69b73..2307dd31b966 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -8,6 +8,8 @@
 
 #include <linux/clockchips.h>
 #include <linux/irqflags.h>
+#include <linux/percpu.h>
+#include <linux/hrtimer.h>
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 
@@ -122,13 +124,26 @@ static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
+DECLARE_PER_CPU(struct tick_sched, tick_cpu_sched);
+
+static inline int tick_nohz_tick_stopped(void)
+{
+	return __this_cpu_read(tick_cpu_sched.tick_stopped);
+}
+
 extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
-# else
+
+# else /* !CONFIG_NO_HZ */
+static inline int tick_nohz_tick_stopped(void)
+{
+	return 0;
+}
+
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
 

commit 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
Author: Youquan Song <youquan.song@intel.com>
Date:   Fri Oct 26 12:26:41 2012 +0200

    cpuidle: Quickly notice prediction failure for repeat mode
    
    The prediction for future is difficult and when the cpuidle governor prediction
    fails and govenor possibly choose the shallower C-state than it should. How to
    quickly notice and find the failure becomes important for power saving.
    
    cpuidle menu governor has a method to predict the repeat pattern if there are 8
    C-states residency which are continuous and the same or very close, so it will
    predict the next C-states residency will keep same residency time.
    
    There is a real case that turbostat utility (tools/power/x86/turbostat)
    at kernel 3.3 or early. turbostat utility will read 10 registers one by one at
    Sandybridge, so it will generate 10 IPIs to wake up idle CPUs. So cpuidle menu
     governor will predict it is repeat mode and there is another IPI wake up idle
     CPU soon, so it keeps idle CPU stay at C1 state even though CPU is totally
    idle. However, in the turbostat, following 10 registers reading is sleep 5
    seconds by default, so the idle CPU will keep at C1 for a long time though it is
     idle until break event occurs.
    In a idle Sandybridge system, run "./turbostat -v", we will notice that deep
    C-state dangles between "70% ~ 99%". After patched the kernel, we will notice
    deep C-state stays at >99.98%.
    
    In the patch, a timer is added when menu governor detects a repeat mode and
    choose a shallow C-state. The timer is set to a time out value that greater
    than predicted time, and we conclude repeat mode prediction failure if timer is
    triggered. When repeat mode happens as expected, the timer is not triggered
    and CPU waken up from C-states and it will cancel the timer initiatively.
    When repeat mode does not happen, the timer will be time out and menu governor
    will quickly notice that the repeat mode prediction fails and then re-evaluates
    deeper C-states possibility.
    
    Below is another case which will clearly show the patch much benefit:
    
    #include <stdlib.h>
    #include <stdio.h>
    #include <unistd.h>
    #include <signal.h>
    #include <sys/time.h>
    #include <time.h>
    #include <pthread.h>
    
    volatile int * shutdown;
    volatile long * count;
    int delay = 20;
    int loop = 8;
    
    void usage(void)
    {
            fprintf(stderr,
                    "Usage: idle_predict [options]\n"
                    "  --help       -h  Print this help\n"
                    "  --thread     -n  Thread number\n"
                    "  --loop       -l  Loop times in shallow Cstate\n"
                    "  --delay      -t  Sleep time (uS)in shallow Cstate\n");
    }
    
    void *simple_loop() {
            int idle_num = 1;
            while (!(*shutdown)) {
                    *count = *count + 1;
    
                    if (idle_num % loop)
                            usleep(delay);
                    else {
                            /* sleep 1 second */
                            usleep(1000000);
                            idle_num = 0;
                    }
                    idle_num++;
            }
    
    }
    
    static void sighand(int sig)
    {
            *shutdown = 1;
    }
    
    int main(int argc, char *argv[])
    {
            sigset_t sigset;
            int signum = SIGALRM;
            int i, c, er = 0, thread_num = 8;
            pthread_t pt[1024];
    
            static char optstr[] = "n:l:t:h:";
    
            while ((c = getopt(argc, argv, optstr)) != EOF)
                    switch (c) {
                            case 'n':
                                    thread_num = atoi(optarg);
                                    break;
                            case 'l':
                                    loop = atoi(optarg);
                                    break;
                            case 't':
                                    delay = atoi(optarg);
                                    break;
                            case 'h':
                            default:
                                    usage();
                                    exit(1);
                    }
    
            printf("thread=%d,loop=%d,delay=%d\n",thread_num,loop,delay);
            count = malloc(sizeof(long));
            shutdown = malloc(sizeof(int));
            *count = 0;
            *shutdown = 0;
    
            sigemptyset(&sigset);
            sigaddset(&sigset, signum);
            sigprocmask (SIG_BLOCK, &sigset, NULL);
            signal(SIGINT, sighand);
            signal(SIGTERM, sighand);
    
            for(i = 0; i < thread_num ; i++)
                    pthread_create(&pt[i], NULL, simple_loop, NULL);
    
            for (i = 0; i < thread_num; i++)
                    pthread_join(pt[i], NULL);
    
            exit(0);
    }
    
    Get powertop V2 from git://github.com/fenrus75/powertop, build powertop.
    After build the above test application, then run it.
    Test plaform can be Intel Sandybridge or other recent platforms.
    #./idle_predict -l 10 &
    #./powertop
    
    We will find that deep C-state will dangle between 40%~100% and much time spent
    on C1 state. It is because menu governor wrongly predict that repeat mode
    is kept, so it will choose the C1 shallow C-state even though it has chance to
    sleep 1 second in deep C-state.
    
    While after patched the kernel, we find that deep C-state will keep >99.6%.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Youquan Song <youquan.song@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f37fceb69b73..1a6567b48492 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -142,4 +142,10 @@ static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !NO_HZ */
 
+# ifdef CONFIG_CPU_IDLE_GOV_MENU
+extern void menu_hrtimer_cancel(void);
+# else
+static inline void menu_hrtimer_cancel(void) {}
+# endif /* CONFIG_CPU_IDLE_GOV_MENU */
+
 #endif

commit f5d411c91ede162240f34e05a233f2759412988e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jul 31 17:44:12 2011 +0200

    nohz: Rename ts->idle_tick to ts->last_tick
    
    Now that idle and nohz logics are going to be independant each others,
    ts->idle_tick becomes too much a biased name to describe the field that
    saves the last scheduled tick on top of which we re-calculate the next
    tick to schedule when the timer is restarted.
    
    We want to reuse this even to stop the tick outside idle cases. So let's
    rename it to some more generic name: ts->last_tick.
    
    This changes a bit the timer list stat export so we need to increase its
    version.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index ab8be90b5cc9..f37fceb69b73 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -31,10 +31,10 @@ enum tick_nohz_mode {
  * struct tick_sched - sched tick emulation and no idle tick control/stats
  * @sched_timer:	hrtimer to schedule the periodic tick in high
  *			resolution mode
- * @idle_tick:		Store the last idle tick expiry time when the tick
- *			timer is modified for idle sleeps. This is necessary
+ * @last_tick:		Store the last tick expiry time when the tick
+ *			timer is modified for nohz sleeps. This is necessary
  *			to resume the tick timer operation in the timeline
- *			when the CPU returns from idle
+ *			when the CPU returns from nohz sleep.
  * @tick_stopped:	Indicator that the idle tick has been stopped
  * @idle_jiffies:	jiffies at the entry to idle for idle time accounting
  * @idle_calls:		Total number of idle calls
@@ -51,7 +51,7 @@ struct tick_sched {
 	struct hrtimer			sched_timer;
 	unsigned long			check_clocks;
 	enum tick_nohz_mode		nohz_mode;
-	ktime_t				idle_tick;
+	ktime_t				last_tick;
 	int				inidle;
 	int				tick_stopped;
 	unsigned long			idle_jiffies;

commit 1268fbc746ea1cd279886a740dcbad4ba5232225
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 17 18:48:14 2011 +0100

    nohz: Remove tick_nohz_idle_enter_norcu() / tick_nohz_idle_exit_norcu()
    
    Those two APIs were provided to optimize the calls of
    tick_nohz_idle_enter() and rcu_idle_enter() into a single
    irq disabled section. This way no interrupt happening in-between would
    needlessly process any RCU job.
    
    Now we are talking about an optimization for which benefits
    have yet to be measured. Let's start simple and completely decouple
    idle rcu and dyntick idle logics to simplify.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 327434a05757..ab8be90b5cc9 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -122,45 +122,8 @@ static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
-extern void __tick_nohz_idle_enter(void);
-static inline void tick_nohz_idle_enter(void)
-{
-	local_irq_disable();
-	__tick_nohz_idle_enter();
-	local_irq_enable();
-}
+extern void tick_nohz_idle_enter(void);
 extern void tick_nohz_idle_exit(void);
-
-/*
- * Call this pair of function if the arch doesn't make any use
- * of RCU in-between. You won't need to call rcu_idle_enter() and
- * rcu_idle_exit().
- * Otherwise you need to call tick_nohz_idle_enter() and tick_nohz_idle_exit()
- * and explicitly tell RCU about the window around the place the CPU enters low
- * power mode where no RCU use is made. This is done by calling rcu_idle_enter()
- * after the last use of RCU before the CPU is put to sleep and by calling
- * rcu_idle_exit() before the first use of RCU after the CPU woke up.
- */
-static inline void tick_nohz_idle_enter_norcu(void)
-{
-	/*
-	 * Also call rcu_idle_enter() in the irq disabled section even
-	 * if it disables irq itself.
-	 * Just an optimization that prevents from an interrupt happening
-	 * between it and __tick_nohz_idle_enter() to lose time to help
-	 * completing a grace period while we could be in extended grace
-	 * period already.
-	 */
-	local_irq_disable();
-	__tick_nohz_idle_enter();
-	rcu_idle_enter();
-	local_irq_enable();
-}
-static inline void tick_nohz_idle_exit_norcu(void)
-{
-	rcu_idle_exit();
-	tick_nohz_idle_exit();
-}
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
@@ -168,14 +131,6 @@ extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 # else
 static inline void tick_nohz_idle_enter(void) { }
 static inline void tick_nohz_idle_exit(void) { }
-static inline void tick_nohz_idle_enter_norcu(void)
-{
-	rcu_idle_enter();
-}
-static inline void tick_nohz_idle_exit_norcu(void)
-{
-	rcu_idle_exit();
-}
 
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {

commit 2bbb6817c0ac1b5f2a68d720f364f98eeb1ac4fd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 8 16:01:00 2011 +0200

    nohz: Allow rcu extended quiescent state handling seperately from tick stop
    
    It is assumed that rcu won't be used once we switch to tickless
    mode and until we restart the tick. However this is not always
    true, as in x86-64 where we dereference the idle notifiers after
    the tick is stopped.
    
    To prepare for fixing this, add two new APIs:
    tick_nohz_idle_enter_norcu() and tick_nohz_idle_exit_norcu().
    
    If no use of RCU is made in the idle loop between
    tick_nohz_enter_idle() and tick_nohz_exit_idle() calls, the arch
    must instead call the new *_norcu() version such that the arch doesn't
    need to call rcu_idle_enter() and rcu_idle_exit().
    
    Otherwise the arch must call tick_nohz_enter_idle() and
    tick_nohz_exit_idle() and also call explicitly:
    
    - rcu_idle_enter() after its last use of RCU before the CPU is put
    to sleep.
    - rcu_idle_exit() before the first use of RCU after the CPU is woken
    up.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0df1d50a408a..327434a05757 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -7,6 +7,7 @@
 #define _LINUX_TICK_H
 
 #include <linux/clockchips.h>
+#include <linux/irqflags.h>
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 
@@ -121,18 +122,57 @@ static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
-extern void tick_nohz_idle_enter(void);
+extern void __tick_nohz_idle_enter(void);
+static inline void tick_nohz_idle_enter(void)
+{
+	local_irq_disable();
+	__tick_nohz_idle_enter();
+	local_irq_enable();
+}
 extern void tick_nohz_idle_exit(void);
+
+/*
+ * Call this pair of function if the arch doesn't make any use
+ * of RCU in-between. You won't need to call rcu_idle_enter() and
+ * rcu_idle_exit().
+ * Otherwise you need to call tick_nohz_idle_enter() and tick_nohz_idle_exit()
+ * and explicitly tell RCU about the window around the place the CPU enters low
+ * power mode where no RCU use is made. This is done by calling rcu_idle_enter()
+ * after the last use of RCU before the CPU is put to sleep and by calling
+ * rcu_idle_exit() before the first use of RCU after the CPU woke up.
+ */
+static inline void tick_nohz_idle_enter_norcu(void)
+{
+	/*
+	 * Also call rcu_idle_enter() in the irq disabled section even
+	 * if it disables irq itself.
+	 * Just an optimization that prevents from an interrupt happening
+	 * between it and __tick_nohz_idle_enter() to lose time to help
+	 * completing a grace period while we could be in extended grace
+	 * period already.
+	 */
+	local_irq_disable();
+	__tick_nohz_idle_enter();
+	rcu_idle_enter();
+	local_irq_enable();
+}
+static inline void tick_nohz_idle_exit_norcu(void)
+{
+	rcu_idle_exit();
+	tick_nohz_idle_exit();
+}
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 # else
-static inline void tick_nohz_idle_enter(void)
+static inline void tick_nohz_idle_enter(void) { }
+static inline void tick_nohz_idle_exit(void) { }
+static inline void tick_nohz_idle_enter_norcu(void)
 {
 	rcu_idle_enter();
 }
-static inline void tick_nohz_idle_exit(void)
+static inline void tick_nohz_idle_exit_norcu(void)
 {
 	rcu_idle_exit();
 }

commit 280f06774afedf849f0b34248ed6aff57d0f6908
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 18:22:06 2011 +0200

    nohz: Separate out irq exit and idle loop dyntick logic
    
    The tick_nohz_stop_sched_tick() function, which tries to delay
    the next timer tick as long as possible, can be called from two
    places:
    
    - From the idle loop to start the dytick idle mode
    - From interrupt exit if we have interrupted the dyntick
    idle mode, so that we reprogram the next tick event in
    case the irq changed some internal state that requires this
    action.
    
    There are only few minor differences between both that
    are handled by that function, driven by the ts->inidle
    cpu variable and the inidle parameter. The whole guarantees
    that we only update the dyntick mode on irq exit if we actually
    interrupted the dyntick idle mode, and that we enter in RCU extended
    quiescent state from idle loop entry only.
    
    Split this function into:
    
    - tick_nohz_idle_enter(), which sets ts->inidle to 1, enters
    dynticks idle mode unconditionally if it can, and enters into RCU
    extended quiescent state.
    
    - tick_nohz_irq_exit() which only updates the dynticks idle mode
    when ts->inidle is set (ie: if tick_nohz_idle_enter() has been called).
    
    To maintain symmetry, tick_nohz_restart_sched_tick() has been renamed
    into tick_nohz_idle_exit().
    
    This simplifies the code and micro-optimize the irq exit path (no need
    for local_irq_save there). This also prepares for the split between
    dynticks and rcu extended quiescent state logics. We'll need this split to
    further fix illegal uses of RCU in extended quiescent states in the idle
    loop.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index ca40838fdfb7..0df1d50a408a 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -121,21 +121,22 @@ static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
-extern void tick_nohz_stop_sched_tick(int inidle);
-extern void tick_nohz_restart_sched_tick(void);
+extern void tick_nohz_idle_enter(void);
+extern void tick_nohz_idle_exit(void);
+extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 # else
-static inline void tick_nohz_stop_sched_tick(int inidle)
+static inline void tick_nohz_idle_enter(void)
 {
-	if (inidle)
-		rcu_idle_enter();
+	rcu_idle_enter();
 }
-static inline void tick_nohz_restart_sched_tick(void)
+static inline void tick_nohz_idle_exit(void)
 {
 	rcu_idle_exit();
 }
+
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {
 	ktime_t len = { .tv64 = NSEC_PER_SEC/HZ };

commit 9b2e4f1880b789be1f24f9684f7a54b90310b5c0
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Sep 30 12:10:22 2011 -0700

    rcu: Track idleness independent of idle tasks
    
    Earlier versions of RCU used the scheduling-clock tick to detect idleness
    by checking for the idle task, but handled idleness differently for
    CONFIG_NO_HZ=y.  But there are now a number of uses of RCU read-side
    critical sections in the idle task, for example, for tracing.  A more
    fine-grained detection of idleness is therefore required.
    
    This commit presses the old dyntick-idle code into full-time service,
    so that rcu_idle_enter(), previously known as rcu_enter_nohz(), is
    always invoked at the beginning of an idle loop iteration.  Similarly,
    rcu_idle_exit(), previously known as rcu_exit_nohz(), is always invoked
    at the end of an idle-loop iteration.  This allows the idle task to
    use RCU everywhere except between consecutive rcu_idle_enter() and
    rcu_idle_exit() calls, in turn allowing architecture maintainers to
    specify exactly where in the idle loop that RCU may be used.
    
    Because some of the userspace upcall uses can result in what looks
    to RCU like half of an interrupt, it is not possible to expect that
    the irq_enter() and irq_exit() hooks will give exact counts.  This
    patch therefore expands the ->dynticks_nesting counter to 64 bits
    and uses two separate bitfields to count process/idle transitions
    and interrupt entry/exit transitions.  It is presumed that userspace
    upcalls do not happen in the idle loop or from usermode execution
    (though usermode might do a system call that results in an upcall).
    The counter is hard-reset on each process/idle transition, which
    avoids the interrupt entry/exit error from accumulating.  Overflow
    is avoided by the 64-bitness of the ->dyntick_nesting counter.
    
    This commit also adds warnings if a non-idle task asks RCU to enter
    idle state (and these checks will need some adjustment before applying
    Frederic's OS-jitter patches (http://lkml.org/lkml/2011/10/7/246).
    In addition, validation of ->dynticks and ->dynticks_nesting is added.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b232ccc0ee29..ca40838fdfb7 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -127,8 +127,15 @@ extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 # else
-static inline void tick_nohz_stop_sched_tick(int inidle) { }
-static inline void tick_nohz_restart_sched_tick(void) { }
+static inline void tick_nohz_stop_sched_tick(int inidle)
+{
+	if (inidle)
+		rcu_idle_enter();
+}
+static inline void tick_nohz_restart_sched_tick(void)
+{
+	rcu_idle_exit();
+}
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {
 	ktime_t len = { .tv64 = NSEC_PER_SEC/HZ };

commit 0224cf4c5ee0d7faec83956b8e21f7d89e3df3bd
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Sun May 9 08:25:23 2010 -0700

    sched: Intoduce get_cpu_iowait_time_us()
    
    For the ondemand cpufreq governor, it is desired that the iowait
    time is microaccounted in a similar way as idle time is.
    
    This patch introduces the infrastructure to account and expose
    this information via the get_cpu_iowait_time_us() function.
    
    [akpm@linux-foundation.org: fix CONFIG_NO_HZ=n build]
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: davej@redhat.com
    LKML-Reference: <20100509082523.284feab6@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0343eed40619..b232ccc0ee29 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -42,6 +42,7 @@ enum tick_nohz_mode {
  * @idle_waketime:	Time when the idle was interrupted
  * @idle_exittime:	Time when the idle state was left
  * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
+ * @iowait_sleeptime:	Sum of the time slept in idle with sched tick stopped, with IO outstanding
  * @sleep_length:	Duration of the current idle sleep
  * @do_timer_lst:	CPU was the last one doing do_timer before going idle
  */
@@ -60,6 +61,7 @@ struct tick_sched {
 	ktime_t				idle_waketime;
 	ktime_t				idle_exittime;
 	ktime_t				idle_sleeptime;
+	ktime_t				iowait_sleeptime;
 	ktime_t				sleep_length;
 	unsigned long			last_jiffies;
 	unsigned long			next_jiffies;
@@ -123,6 +125,7 @@ extern void tick_nohz_stop_sched_tick(int inidle);
 extern void tick_nohz_restart_sched_tick(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
+extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 # else
 static inline void tick_nohz_stop_sched_tick(int inidle) { }
 static inline void tick_nohz_restart_sched_tick(void) { }
@@ -133,6 +136,7 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 	return len;
 }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
+static inline u64 get_cpu_iowait_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !NO_HZ */
 
 #endif

commit e0e37c200f1357db0dd986edb359c41c57d24f6e
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Sun May 9 08:24:39 2010 -0700

    sched: Eliminate the ts->idle_lastupdate field
    
    Now that the only user of ts->idle_lastupdate is
    update_ts_time_stats(), the entire field can be eliminated.
    
    In update_ts_time_stats(), idle_lastupdate is first set to
    "now", and a few lines later, the only user is an if() statement
    that assigns a variable either to "now" or to
    ts->idle_lastupdate, which has the value of "now" at that point.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: davej@redhat.com
    LKML-Reference: <20100509082439.2fab0b4f@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index d2ae79e21be3..0343eed40619 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -60,7 +60,6 @@ struct tick_sched {
 	ktime_t				idle_waketime;
 	ktime_t				idle_exittime;
 	ktime_t				idle_sleeptime;
-	ktime_t				idle_lastupdate;
 	ktime_t				sleep_length;
 	unsigned long			last_jiffies;
 	unsigned long			next_jiffies;

commit 27185016b806d5a1181ff501cae120582b2b27dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Nov 12 22:12:06 2009 +0100

    nohz: Track last do_timer() cpu
    
    The previous patch which limits the sleep time to the maximum
    deferment time of the time keeping clocksource has some limitations on
    SMP machines: if all CPUs are idle then for all CPUs the maximum sleep
    time is limited.
    
    Solve this by keeping track of which cpu had the do_timer() duty
    assigned last and limit the sleep time only for this cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Cc: Jon Hunter <jon-hunter@ti.com>
    Cc: John Stultz <johnstul@us.ibm.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 8dc082194b22..d2ae79e21be3 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -43,6 +43,7 @@ enum tick_nohz_mode {
  * @idle_exittime:	Time when the idle state was left
  * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
  * @sleep_length:	Duration of the current idle sleep
+ * @do_timer_lst:	CPU was the last one doing do_timer before going idle
  */
 struct tick_sched {
 	struct hrtimer			sched_timer;
@@ -64,6 +65,7 @@ struct tick_sched {
 	unsigned long			last_jiffies;
 	unsigned long			next_jiffies;
 	ktime_t				idle_expires;
+	int				do_timer_last;
 };
 
 extern void __init tick_init(void);

commit 3c5d92a0cfb5103c0d5ab74d4ae6373d3af38148
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Sep 29 14:25:16 2009 +0200

    nohz: Introduce arch_needs_cpu
    
    Allow the architecture to request a normal jiffy tick when the system
    goes idle and tick_nohz_stop_sched_tick is called . On s390 the hook is
    used to prevent the system going fully idle if there has been an
    interrupt other than a clock comparator interrupt since the last wakeup.
    
    On s390 the HiperSockets response time for 1 connection ping-pong goes
    down from 42 to 34 microseconds. The CPU cost decreases by 27%.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    LKML-Reference: <20090929122533.402715150@de.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0482229c07db..8dc082194b22 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -98,6 +98,9 @@ extern int tick_check_oneshot_change(int allow_nohz);
 extern struct tick_sched *tick_get_tick_sched(int cpu);
 extern void tick_check_idle(int cpu);
 extern int tick_oneshot_mode_active(void);
+#  ifndef arch_needs_cpu
+#   define arch_needs_cpu(cpu) (0)
+#  endif
 # else
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }

commit cd6d95d8449b7c9f415f26041e9ae173d387b6bd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 12 11:29:27 2009 +0200

    clocksource: prevent selection of low resolution clocksourse also for nohz=on
    
    commit 3f68535adad (clocksource: sanity check sysfs clocksource
    changes) prevents selection of non high resolution capable
    clocksources when high resolution mode is active, but did not take
    into account that the same rules apply for highres=off nohz=on.
    
    Check the tick device mode instead of hrtimer_hres_active() to verify
    whether the system needs to be protected from a switch to jiffies or
    other non highres capable clock sources.
    
    Reported-by: Luming Yu <luming.yu@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 469b82d88b3b..0482229c07db 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -97,10 +97,12 @@ extern void tick_clock_notify(void);
 extern int tick_check_oneshot_change(int allow_nohz);
 extern struct tick_sched *tick_get_tick_sched(int cpu);
 extern void tick_check_idle(int cpu);
+extern int tick_oneshot_mode_active(void);
 # else
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 static inline void tick_check_idle(int cpu) { }
+static inline int tick_oneshot_mode_active(void) { return 0; }
 # endif
 
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
@@ -109,6 +111,7 @@ static inline void tick_cancel_sched_timer(int cpu) { }
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 static inline void tick_check_idle(int cpu) { }
+static inline int tick_oneshot_mode_active(void) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ

commit 6b954823c24f04ed026a8517f6bab5abda279db8
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:25 2009 +1030

    cpumask: convert kernel time functions
    
    Impact: Use new APIs
    
    Convert kernel/time functions to use struct cpumask *.
    
    Note the ugly bitmap declarations in tick-broadcast.c.  These should
    be cpumask_var_t, but there was no obvious initialization function to
    put the alloc_cpumask_var() calls in.  This was safe.
    
    (Eventually 'struct cpumask' will be undefined for CONFIG_CPUMASK_OFFSTACK,
    so we use a bitmap here to show we really mean it).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index b6ec8189ac0c..469b82d88b3b 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -84,10 +84,10 @@ static inline void tick_cancel_sched_timer(int cpu) { }
 
 # ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 extern struct tick_device *tick_get_broadcast_device(void);
-extern cpumask_t *tick_get_broadcast_mask(void);
+extern struct cpumask *tick_get_broadcast_mask(void);
 
 #  ifdef CONFIG_TICK_ONESHOT
-extern cpumask_t *tick_get_broadcast_oneshot_mask(void);
+extern struct cpumask *tick_get_broadcast_oneshot_mask(void);
 #  endif
 
 # endif /* BROADCAST */

commit 719254faa17ffedc87ba0fadb9b34e535c9758d5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Oct 17 09:59:47 2008 +0200

    NOHZ: unify the nohz function calls in irq_enter()
    
    We have two separate nohz function calls in irq_enter() for no good
    reason. Just call a single NOHZ function from irq_enter() and call
    the bits in the tick code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 98921a3e1aa8..b6ec8189ac0c 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -96,9 +96,11 @@ extern cpumask_t *tick_get_broadcast_oneshot_mask(void);
 extern void tick_clock_notify(void);
 extern int tick_check_oneshot_change(int allow_nohz);
 extern struct tick_sched *tick_get_tick_sched(int cpu);
+extern void tick_check_idle(int cpu);
 # else
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
+static inline void tick_check_idle(int cpu) { }
 # endif
 
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
@@ -106,26 +108,23 @@ static inline void tick_init(void) { }
 static inline void tick_cancel_sched_timer(int cpu) { }
 static inline void tick_clock_notify(void) { }
 static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
+static inline void tick_check_idle(int cpu) { }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
 extern void tick_nohz_stop_sched_tick(int inidle);
 extern void tick_nohz_restart_sched_tick(void);
-extern void tick_nohz_update_jiffies(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
-extern void tick_nohz_stop_idle(int cpu);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 # else
 static inline void tick_nohz_stop_sched_tick(int inidle) { }
 static inline void tick_nohz_restart_sched_tick(void) { }
-static inline void tick_nohz_update_jiffies(void) { }
 static inline ktime_t tick_nohz_get_sleep_length(void)
 {
 	ktime_t len = { .tv64 = NSEC_PER_SEC/HZ };
 
 	return len;
 }
-static inline void tick_nohz_stop_idle(int cpu) { }
 static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !NO_HZ */
 

commit 8083e4ad970e4eb567e31037060cdd4ba346f0c0
Author: venkatesh.pallipadi@intel.com <venkatesh.pallipadi@intel.com>
Date:   Mon Aug 4 11:59:11 2008 -0700

    [CPUFREQ][5/6] cpufreq: Changes to get_cpu_idle_time_us(), used by ondemand governor
    
    export get_cpu_idle_time_us() for it to be used in ondemand governor.
    Last update time can be current time when the CPU is currently non-idle,
    accounting for the busy time since last idle.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 8cf8cfe2cc97..98921a3e1aa8 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -126,7 +126,7 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 	return len;
 }
 static inline void tick_nohz_stop_idle(int cpu) { }
-static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return 0; }
+static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return -1; }
 # endif /* !NO_HZ */
 
 #endif

commit 3c4fbe5e01d7e5309be5045e7ae0db20a049e6dc
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Aug 20 16:37:38 2008 -0700

    nohz: fix wrong event handler after online an offlined cpu
    
    On the tickless system(CONFIG_NO_HZ=y and CONFIG_HIGH_RES_TIMERS=n), after
    I made an offlined cpu online, I found this cpu's event handler was
    tick_handle_periodic, not tick_nohz_handler.
    
    After debuging, I found this bug was caused by the wrong tick mode.  the
    tick mode is not changed to NOHZ_MODE_INACTIVE when the cpu is offline.
    
    This patch fixes this bug.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index d3c02695dc5d..8cf8cfe2cc97 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -74,10 +74,13 @@ extern struct tick_device *tick_get_device(int cpu);
 extern int tick_init_highres(void);
 extern int tick_program_event(ktime_t expires, int force);
 extern void tick_setup_sched_timer(void);
+# endif
+
+# if defined CONFIG_NO_HZ || defined CONFIG_HIGH_RES_TIMERS
 extern void tick_cancel_sched_timer(int cpu);
 # else
 static inline void tick_cancel_sched_timer(int cpu) { }
-# endif /* HIGHRES */
+# endif
 
 # ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 extern struct tick_device *tick_get_broadcast_device(void);

commit b8f8c3cf0a4ac0632ec3f0e15e9dc0c29de917af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 18 17:27:28 2008 +0200

    nohz: prevent tick stop outside of the idle loop
    
    Jack Ren and Eric Miao tracked down the following long standing
    problem in the NOHZ code:
    
            scheduler switch to idle task
            enable interrupts
    
    Window starts here
    
            ----> interrupt happens (does not set NEED_RESCHED)
                    irq_exit() stops the tick
    
            ----> interrupt happens (does set NEED_RESCHED)
    
            return from schedule()
    
            cpu_idle(): preempt_disable();
    
    Window ends here
    
    The interrupts can happen at any point inside the race window. The
    first interrupt stops the tick, the second one causes the scheduler to
    rerun and switch away from idle again and we end up with the tick
    disabled.
    
    The fact that it needs two interrupts where the first one does not set
    NEED_RESCHED and the second one does made the bug obscure and extremly
    hard to reproduce and analyse. Kudos to Jack and Eric.
    
    Solution: Limit the NOHZ functionality to the idle loop to make sure
    that we can not run into such a situation ever again.
    
    cpu_idle()
    {
            preempt_disable();
    
            while(1) {
                     tick_nohz_stop_sched_tick(1); <- tell NOHZ code that we
                                                      are in the idle loop
    
                     while (!need_resched())
                           halt();
    
                     tick_nohz_restart_sched_tick(); <- disables NOHZ mode
                     preempt_enable_no_resched();
                     schedule();
                     preempt_disable();
            }
    }
    
    In hindsight we should have done this forever, but ...
    
    /me grabs a large brown paperbag.
    
    Debugged-by: Jack Ren <jack.ren@marvell.com>,
    Debugged-by: eric miao <eric.y.miao@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index a881c652f7e9..d3c02695dc5d 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -49,6 +49,7 @@ struct tick_sched {
 	unsigned long			check_clocks;
 	enum tick_nohz_mode		nohz_mode;
 	ktime_t				idle_tick;
+	int				inidle;
 	int				tick_stopped;
 	unsigned long			idle_jiffies;
 	unsigned long			idle_calls;
@@ -105,14 +106,14 @@ static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 #endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
 # ifdef CONFIG_NO_HZ
-extern void tick_nohz_stop_sched_tick(void);
+extern void tick_nohz_stop_sched_tick(int inidle);
 extern void tick_nohz_restart_sched_tick(void);
 extern void tick_nohz_update_jiffies(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern void tick_nohz_stop_idle(int cpu);
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 # else
-static inline void tick_nohz_stop_sched_tick(void) { }
+static inline void tick_nohz_stop_sched_tick(int inidle) { }
 static inline void tick_nohz_restart_sched_tick(void) { }
 static inline void tick_nohz_update_jiffies(void) { }
 static inline ktime_t tick_nohz_get_sleep_length(void)

commit 5df7fa1c62146a0933767d040d400013310dbcc7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 1 17:45:14 2008 +0100

    tick-sched: add more debug information
    
    To allow better diagnosis of tick-sched related, especially NOHZ
    related problems, we need to know when the last wakeup via an irq
    happened and when the CPU left the idle state.
    
    Add two fields (idle_waketime, idle_exittime) to the tick_sched
    structure and add them to the timer_list output.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 0fadf95debe1..a881c652f7e9 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -39,6 +39,8 @@ enum tick_nohz_mode {
  * @idle_calls:		Total number of idle calls
  * @idle_sleeps:	Number of idle calls, where the sched tick was stopped
  * @idle_entrytime:	Time when the idle call was entered
+ * @idle_waketime:	Time when the idle was interrupted
+ * @idle_exittime:	Time when the idle state was left
  * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
  * @sleep_length:	Duration of the current idle sleep
  */
@@ -53,6 +55,8 @@ struct tick_sched {
 	unsigned long			idle_sleeps;
 	int				idle_active;
 	ktime_t				idle_entrytime;
+	ktime_t				idle_waketime;
+	ktime_t				idle_exittime;
 	ktime_t				idle_sleeptime;
 	ktime_t				idle_lastupdate;
 	ktime_t				sleep_length;

commit 6378ddb592158db4b42197f1bc8666228800e379
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Wed Jan 30 13:30:04 2008 +0100

    time: track accurate idle time with tick_sched.idle_sleeptime
    
    Current idle time in kstat is based on jiffies and is coarse grained.
    tick_sched.idle_sleeptime is making some attempt to keep track of idle time
    in a fine grained manner.  But, it is not handling the time spent in
    interrupts fully.
    
    Make tick_sched.idle_sleeptime accurate with respect to time spent on
    handling interrupts and also add tick_sched.idle_lastupdate, which keeps
    track of last time when idle_sleeptime was updated.
    
    This statistics will be crucial for cpufreq-ondemand governor, which can
    shed some conservative gaurd band that is uses today while setting the
    frequency.  The ondemand changes that uses the exact idle time is coming
    soon.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index f4a1395e05ff..0fadf95debe1 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -51,8 +51,10 @@ struct tick_sched {
 	unsigned long			idle_jiffies;
 	unsigned long			idle_calls;
 	unsigned long			idle_sleeps;
+	int				idle_active;
 	ktime_t				idle_entrytime;
 	ktime_t				idle_sleeptime;
+	ktime_t				idle_lastupdate;
 	ktime_t				sleep_length;
 	unsigned long			last_jiffies;
 	unsigned long			next_jiffies;
@@ -103,6 +105,8 @@ extern void tick_nohz_stop_sched_tick(void);
 extern void tick_nohz_restart_sched_tick(void);
 extern void tick_nohz_update_jiffies(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
+extern void tick_nohz_stop_idle(int cpu);
+extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 # else
 static inline void tick_nohz_stop_sched_tick(void) { }
 static inline void tick_nohz_restart_sched_tick(void) { }
@@ -113,6 +117,8 @@ static inline ktime_t tick_nohz_get_sleep_length(void)
 
 	return len;
 }
+static inline void tick_nohz_stop_idle(int cpu) { }
+static inline u64 get_cpu_idle_time_us(int cpu, u64 *unused) { return 0; }
 # endif /* !NO_HZ */
 
 #endif

commit 4f86d3a8e297205780cca027e974fd5f81064780
Author: Len Brown <len.brown@intel.com>
Date:   Wed Oct 3 18:58:00 2007 -0400

    cpuidle: consolidate 2.6.22 cpuidle branch into one patch
    
    commit e5a16b1f9eec0af7cfa0830304b41c1c0833cf9f
    Author: Len Brown <len.brown@intel.com>
    Date:   Tue Oct 2 23:44:44 2007 -0400
    
        cpuidle: shrink diff
    
        processor_idle.c |  440 +++++++++++++++++++++++++++++++++++++++++--
        1 file changed, 429 insertions(+), 11 deletions(-)
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit dfbb9d5aedfb18848a3e0d6f6e3e4969febb209c
    Author: Len Brown <len.brown@intel.com>
    Date:   Wed Sep 26 02:17:55 2007 -0400
    
        cpuidle: reduce diff size
    
        Reduces the cpuidle processor_idle.c diff vs 2.6.22 from this
         processor_idle.c | 2006 ++++++++++++++++++++++++++-----------------
         1 file changed, 1219 insertions(+), 787 deletions(-)
    
        to this:
         processor_idle.c |  502 +++++++++++++++++++++++++++++++++++++++----
         1 file changed, 458 insertions(+), 44 deletions(-)
    
        ...for the purpose of making the cpuilde patch less invasive
        and easier to review.
    
        no functional changes.  build tested only.
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 889172fc915f5a7fe20f35b133cbd205ce69bf6c
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Sep 13 13:40:05 2007 -0700
    
        cpuidle: Retain old ACPI policy for !CONFIG_CPU_IDLE
    
        Retain the old policy in processor_idle, so that when CPU_IDLE is not
        configured, old C-state policy will still be used. This provides a
        clean gradual migration path from old ACPI policy to new cpuidle
        based policy.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 9544a8181edc7ecc33b3bfd69271571f98ed08bc
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Sep 13 13:39:17 2007 -0700
    
        cpuidle: Configure governors by default
    
        Quoting Len "Do not give an option to users to shoot themselves in the foot".
    
        Remove the configurability of ladder and menu governors as they are
        needed for default policy of cpuidle. That way users will not be able to
        have cpuidle without any policy loosing all C-state power savings.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8975059a2c1e56cfe83d1bcf031bcf4cb39be743
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:27:07 2007 -0400
    
        CPUIDLE: load ACPI properly when CPUIDLE is disabled
    
        Change the registration return codes for when CPUIDLE
        support is not compiled into the kernel.  As a result, the ACPI
        processor driver will load properly even if CPUIDLE is unavailable.
        However, it may be possible to cleanup the ACPI processor driver further
        and eliminate some dead code paths.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit e0322e2b58dd1b12ec669bf84693efe0dc2414a8
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:26:06 2007 -0400
    
        CPUIDLE: remove cpuidle_get_bm_activity()
    
        Remove cpuidle_get_bm_activity() and updates governors
        accordingly.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 18a6e770d5c82ba26653e53d240caa617e09e9ab
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:58 2007 -0400
    
        CPUIDLE: max_cstate fix
    
        Currently max_cstate is limited to 0, resulting in no idle processor
        power management on ACPI platforms.  This patch restores the value to
        the array size.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1fdc0887286179b40ce24bcdbde663172e205ef0
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:40 2007 -0400
    
        CPUIDLE: handle BM detection inside the ACPI Processor driver
    
        Update the ACPI processor driver to detect BM activity and
        limit state entry depth internally, rather than exposing such
        requirements to CPUIDLE.  As a result, CPUIDLE can drop this
        ACPI-specific interface and become more platform independent.  BM
        activity is now handled much more aggressively than it was in the
        original implementation, so some testing coverage may be needed to
        verify that this doesn't introduce any DMA buffer under-run issues.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0ef38840db666f48e3cdd2b769da676c57228dd9
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:14 2007 -0400
    
        CPUIDLE: menu governor updates
    
        Tweak the menu governor to more effectively handle non-timer
        break events.  Non-timer break events are detected by comparing the
        actual sleep time to the expected sleep time.  In future revisions, it
        may be more reliable to use the timer data structures directly.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit bb4d74fca63fa96cf3ace644b15ae0f12b7df5a1
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:24:40 2007 -0400
    
        CPUIDLE: fix 'current_governor' sysfs entry
    
        Allow the "current_governor" sysfs entry to properly handle
        input terminated with '\n'.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit df3c71559bb69b125f1a48971bf0d17f78bbdf47
    Author: Len Brown <len.brown@intel.com>
    Date:   Sun Aug 12 02:00:45 2007 -0400
    
        cpuidle: fix IA64 build (again)
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit a02064579e3f9530fd31baae16b1fc46b5a7bca8
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:39:27 2007 -0400
    
        cpuidle: Remove support for runtime changing of max_cstate
    
        Remove support for runtime changeability of max_cstate. Drivers can use
        use latency APIs.
    
        max_cstate can still be used as a boot time option and dmi override.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0912a44b13adf22f5e3f607d263aed23b4910d7e
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:39:16 2007 -0400
    
        cpuidle: Remove ACPI cstate_limit calls from ipw2100
    
        ipw2100 already has code to use accetable_latency interfaces to limit the
        C-state. Remove the calls to acpi_set_cstate_limit and acpi_get_cstate_limit
        as they are redundant.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c649a76e76be6bff1fd770d0a775798813a3f6e0
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:35:39 2007 -0400
    
        cpuidle: compile fix for pause and resume functions
    
        Fix the compilation failure when cpuidle is not compiled in.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Acked-by: Adam Belay <adam.belay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 2305a5920fb8ee6ccec1c62ade05aa8351091d71
    Author: Adam Belay <abelay@novell.com>
    Date:   Thu Jul 19 00:49:00 2007 -0400
    
        cpuidle: re-write
    
        Some portions have been rewritten to make the code cleaner and lighter
        weight.  The following is a list of changes:
    
        1.) the state name is now included in the sysfs interface
        2.) detection, hotplug, and available state modifications are handled by
        CPUIDLE drivers directly
        3.) the CPUIDLE idle handler is only ever installed when at least one
        cpuidle_device is enabled and ready
        4.) the menu governor BM code no longer overflows
        5.) the sysfs attributes are now printed as unsigned integers, avoiding
        negative values
        6.) a variety of other small cleanups
    
        Also, Idle drivers are no longer swappable during runtime through the
        CPUIDLE sysfs inteface.  On i386 and x86_64 most idle handlers (e.g.
        poll, mwait, halt, etc.) don't benefit from an infrastructure that
        supports multiple states, so I think using a more general case idle
        handler selection mechanism would be cleaner.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Acked-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit df25b6b56955714e6e24b574d88d1fd11f0c3ee5
    Author: Len Brown <len.brown@intel.com>
    Date:   Tue Jul 24 17:08:21 2007 -0400
    
        cpuidle: fix IA64 buid
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit fd6ada4c14488755ff7068860078c437431fbccd
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Mon Jul 9 11:33:13 2007 -0700
    
        cpuidle: static
    
        make cpuidle_replace_governor() static
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c1d4a2cebcadf2429c0c72e1d29aa2a9684c32e0
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Tue Jul 3 00:54:40 2007 -0400
    
        cpuidle: static
    
        This patch makes the needlessly global struct menu_governor static.
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit dbf8780c6e8d572c2c273da97ed1cca7608fd999
    Author: Andrew Morton <akpm@linux-foundation.org>
    Date:   Tue Jul 3 00:49:14 2007 -0400
    
        export symbol tick_nohz_get_sleep_length
    
        ERROR: "tick_nohz_get_sleep_length" [drivers/cpuidle/governors/menu.ko] undefined!
        ERROR: "tick_nohz_get_idle_jiffies" [drivers/cpuidle/governors/menu.ko] undefined!
    
        And please be sure to get your changes to core kernel suitably reviewed.
    
        Cc: Adam Belay <abelay@novell.com>
        Cc: Venki Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Ingo Molnar <mingo@elte.hu>
        Cc: Thomas Gleixner <tglx@linutronix.de>
        Cc: john stultz <johnstul@us.ibm.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 29f0e248e7017be15f99febf9143a2cef00b2961
    Author: Andrew Morton <akpm@linux-foundation.org>
    Date:   Tue Jul 3 00:43:04 2007 -0400
    
        tick.h needs hrtimer.h
    
        It uses hrtimers.
    
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit e40cede7d63a029e92712a3fe02faee60cc38fb4
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:40:34 2007 -0400
    
        cpuidle: first round of documentation updates
    
        Documentation changes based on Pavel's feedback.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 83b42be2efece386976507555c29e7773a0dfcd1
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:39:25 2007 -0400
    
        cpuidle: add rating to the governors and pick the one with highest rating by default
    
        Introduce a governor rating scheme to pick the right governor by default.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d2a74b8c5e8f22def4709330d4bfc4a29209b71c
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:38:08 2007 -0400
    
        cpuidle: make cpuidle sysfs driver governor switch off by default
    
        Make default cpuidle sysfs to show current_governor and current_driver in
        read-only mode.  More elaborate available_governors and available_drivers with
        writeable current_governor and current_driver interface only appear with
        "cpuidle_sysfs_switch" boot parameter.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1f60a0e80bf83cf6b55c8845bbe5596ed8f6307b
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:37:00 2007 -0400
    
        cpuidle: menu governor: change the early break condition
    
        Change the C-state early break out algorithm in menu governor.
    
        We only look at early breakouts that result in wakeups shorter than idle
        state's target_residency.  If such a breakout is frequent enough, eliminate
        the particular idle state upto a timeout period.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 45a42095cf64b003b4a69be3ce7f434f97d7af51
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:35:38 2007 -0400
    
        cpuidle: fix uninitialized variable in sysfs routine
    
        Fix the uninitialized usage of ret.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 80dca7cdba3e6ee13eae277660873ab9584eb3be
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:34:16 2007 -0400
    
        cpuidle: reenable /proc/acpi//power interface for the time being
    
        Keep /proc/acpi/processor/CPU*/power around for a while as powertop depends
        on it. It will be marked deprecated and removed in future. powertop can use
        cpuidle interfaces instead.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 589c37c2646c5e3813a51255a5ee1159cb4c33fc
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:32:37 2007 -0400
    
        cpuidle: menu governor and hrtimer compile fix
    
        Compile fix for menu governor.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0ba80bd9ab3ed304cb4f19b722e4cc6740588b5e
    Author: Len Brown <len.brown@intel.com>
    Date:   Thu May 31 22:51:43 2007 -0400
    
        cpuidle: build fix - cpuidle vs ipw2100 module
    
        ERROR: "acpi_set_cstate_limit" [drivers/net/wireless/ipw2100.ko] undefined!
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d7d8fa7f96a7f7682be7c6cc0cc53fa7a18c3b58
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:47:07 2007 -0400
    
        cpuidle: add the 'menu' governor
    
        Here is my first take at implementing an idle PM governor that takes
        full advantage of NO_HZ.  I call it the 'menu' governor because it
        considers the full list of idle states before each entry.
    
        I've kept the implementation fairly simple.  It attempts to guess the
        next residency time and then chooses a state that would meet at least
        the break-even point between power savings and entry cost.  To this end,
        it selects the deepest idle state that satisfies the following
        constraints:
             1. If the idle time elapsed since bus master activity was detected
                is below a threshold (currently 20 ms), then limit the selection
                to C2-type or above.
             2. Do not choose a state with a break-even residency that exceeds
                the expected time remaining until the next timer interrupt.
             3. Do not choose a state with a break-even residency that exceeds
                the elapsed time between the last pair of break events,
                excluding timer interrupts.
    
        This governor has an advantage over "ladder" governor because it
        proactively checks how much time remains until the next timer interrupt
        using the tick infrastructure.  Also, it handles device interrupt
        activity more intelligently by not including timer interrupts in break
        event calculations.  Finally, it doesn't make policy decisions using the
        number of state entries, which can have variable residency times (NO_HZ
        makes these potentially very large), and instead only considers sleep
        time deltas.
    
        The menu governor can be selected during runtime using the cpuidle sysfs
        interface like so:
        "echo "menu" > /sys/devices/system/cpu/cpuidle/current_governor"
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit a4bec7e65aa3b7488b879d971651cc99a6c410fe
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:47:03 2007 -0400
    
        cpuidle: export time until next timer interrupt using NO_HZ
    
        Expose information about the time remaining until the next
        timer interrupt expires by utilizing the dynticks infrastructure.
        Also modify the main idle loop to allow dynticks to handle
        non-interrupt break events (e.g. DMA).  Finally, expose sleep ticks
        information to external code.  Thomas Gleixner is responsible for much
        of the code in this patch.  However, I've made some additional changes,
        so I'm probably responsible if there are any bugs or oversights :)
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 2929d8996fbc77f41a5ff86bb67cdde3ca7d2d72
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:46:58 2007 -0400
    
        cpuidle: governor API changes
    
        This patch prepares cpuidle for the menu governor.  It adds an optional
        stage after idle state entry to give the governor an opportunity to
        check why the state was exited.  Also it makes sure the idle loop
        returns after each state entry, allowing the appropriate dynticks code
        to run.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 3a7fd42f9825c3b03e364ca59baa751bb350775f
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Apr 26 00:03:59 2007 -0700
    
        cpuidle: hang fix
    
        Prevent hang on x86-64, when ACPI processor driver is added as a module on
        a system that does not support C-states.
    
        x86-64 expects all idle handlers to enable interrupts before returning from
        idle handler.  This is due to enter_idle(), exit_idle() races.  Make
        cpuidle_idle_call() confirm to this when there is no pm_idle_old.
    
        Also, cpuidle look at the return values of attch_driver() and set
        current_driver to NULL if attach fails on all CPUs.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 4893339a142afbd5b7c01ffadfd53d14746e858e
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:09 2007 +0800
    
        cpuidle: add support for max_cstate limit
    
        With CPUIDLE framework, the max_cstate (to limit max cpu c-state)
        parameter is ingored. Some systems require it to ignore C2/C3
        and some drivers like ipw require it too.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 43bbbbe1cb998cbd2df656f55bb3bfe30f30e7d1
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:13 2007 +0800
    
        cpuidle: add cpuidle_fore_redetect_devices API
    
        add cpuidle_force_redetect_devices API,
        which forces all CPU redetect idle states.
        Next patch will use it.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d1edadd608f24836def5ec483d2edccfb37b1d19
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:01 2007 +0800
    
        cpuidle: fix sysfs related issue
    
        Fix the cpuidle sysfs issue.
        a. make kobject dynamicaly allocated
        b. fixed sysfs init issue to avoid suspend/resume issue
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 7169a5cc0d67b263978859672e86c13c23a5570d
    Author: Randy Dunlap <randy.dunlap@oracle.com>
    Date:   Wed Mar 28 22:52:53 2007 -0400
    
        cpuidle: 1-bit field must be unsigned
    
        A 1-bit bitfield has no room for a sign bit.
        drivers/cpuidle/governors/ladder.c:54:16: error: dubious bitfield without explicit `signed' or `unsigned'
    
        Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 4658620158dc2fbd9e4bcb213c5b6fb5d05ba7d4
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Wed Mar 28 22:52:41 2007 -0400
    
        cpuidle: fix boot hang
    
        Patch for cpuidle boot hang reported by Larry Finger here.
        http://www.ussg.iu.edu/hypermail/linux/kernel/0703.2/2025.html
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Larry Finger <larry.finger@lwfinger.net>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c17e168aa6e5fe3851baaae8df2fbc1cf11443a9
    Author: Len Brown <len.brown@intel.com>
    Date:   Wed Mar 7 04:37:53 2007 -0500
    
        cpuidle: ladder does not depend on ACPI
    
        build fix for CONFIG_ACPI=n
    
        In file included from drivers/cpuidle/governors/ladder.c:21:
        include/acpi/processor.h:88: error: expected specifier-qualifier-list before acpi_integer
        include/acpi/processor.h:106: error: expected specifier-qualifier-list before acpi_integer
        include/acpi/processor.h:168: error: expected specifier-qualifier-list before acpi_handle
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8c91d958246bde68db0c3f0c57b535962ce861cb
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Tue Mar 6 02:29:40 2007 -0800
    
        cpuidle: make code static
    
        This patch makes the following needlessly global code static:
        - driver.c: __cpuidle_find_driver()
        - governor.c: __cpuidle_find_governor()
        - ladder.c: struct ladder_governor
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Adam Belay <abelay@novell.com>
        Cc: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0c39dc3187094c72c33ab65a64d2017b21f372d2
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Wed Mar 7 02:38:22 2007 -0500
    
        cpu_idle: fix build break
    
        This patch fixes a build breakage with !CONFIG_HOTPLUG_CPU and
        CONFIG_CPU_IDLE.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8112e3b115659b07df340ef170515799c0105f82
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Mar 6 02:29:39 2007 -0800
    
        cpuidle: build fix for !CPU_IDLE
    
        Fix the compile issues when CPU_IDLE is not configured.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Adam Belay <abelay@novell.com>
        Cc: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1eb4431e9599cd25e0d9872f3c2c8986821839dd
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:54:57 2007 -0800
    
        cpuidle take2: Basic documentation for cpuidle
    
        Documentation for cpuidle infrastructure
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit ef5f15a8b79123a047285ec2e3899108661df779
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:54:03 2007 -0800
    
        cpuidle take2: Hookup ACPI C-states driver with cpuidle
    
        Hookup ACPI C-states onto generic cpuidle infrastructure.
    
        drivers/acpi/procesor_idle.c is now a ACPI C-states driver that registers as
        a driver in cpuidle infrastructure and the policy part is removed from
        drivers/acpi/processor_idle.c. We use governor in cpuidle instead.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 987196fa82d4db52c407e8c9d5dec884ba602183
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:52:57 2007 -0800
    
        cpuidle take2: Core cpuidle infrastructure
    
        Announcing 'cpuidle', a new CPU power management infrastructure to manage
        idle CPUs in a clean and efficient manner.
        cpuidle separates out the drivers that can provide support for multiple types
        of idle states and policy governors that decide on what idle state to use
        at run time.
        A cpuidle driver can support multiple idle states based on parameters like
        varying power consumption, wakeup latency, etc (ACPI C-states for example).
        A cpuidle governor can be usage model specific (laptop, server,
        laptop on battery etc).
        Main advantage of the infrastructure being, it allows independent development
        of drivers and governors and allows for better CPU power management.
    
        A huge thanks to Adam Belay and Shaohua Li who were part of this mini-project
        since its beginning and are greatly responsible for this patchset.
    
        This patch:
    
        Core cpuidle infrastructure.
        Introduces a new abstraction layer for cpuidle:
        * which manages drivers that can support multiple idles states. Drivers
          can be generic or particular to specific hardware/platform
        * allows pluging in multiple policy governors that can take idle state policy
          decision
        * The core also has a set of sysfs interfaces with which administrato can know
          about supported drivers and governors and switch them at run time.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index 9a7252e089b9..f4a1395e05ff 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -40,6 +40,7 @@ enum tick_nohz_mode {
  * @idle_sleeps:	Number of idle calls, where the sched tick was stopped
  * @idle_entrytime:	Time when the idle call was entered
  * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
+ * @sleep_length:	Duration of the current idle sleep
  */
 struct tick_sched {
 	struct hrtimer			sched_timer;
@@ -52,6 +53,7 @@ struct tick_sched {
 	unsigned long			idle_sleeps;
 	ktime_t				idle_entrytime;
 	ktime_t				idle_sleeptime;
+	ktime_t				sleep_length;
 	unsigned long			last_jiffies;
 	unsigned long			next_jiffies;
 	ktime_t				idle_expires;
@@ -100,10 +102,17 @@ static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 extern void tick_nohz_stop_sched_tick(void);
 extern void tick_nohz_restart_sched_tick(void);
 extern void tick_nohz_update_jiffies(void);
+extern ktime_t tick_nohz_get_sleep_length(void);
 # else
 static inline void tick_nohz_stop_sched_tick(void) { }
 static inline void tick_nohz_restart_sched_tick(void) { }
 static inline void tick_nohz_update_jiffies(void) { }
+static inline ktime_t tick_nohz_get_sleep_length(void)
+{
+	ktime_t len = { .tv64 = NSEC_PER_SEC/HZ };
+
+	return len;
+}
 # endif /* !NO_HZ */
 
 #endif

commit 289f480af87e45f7a6de6ba9b4c061c2e259fe98
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 16 01:28:15 2007 -0800

    [PATCH] Add debugging feature /proc/timer_list
    
    add /proc/timer_list, which prints all currently pending (high-res) timers,
    all clock-event sources and their parameters in a human-readable form.
    
    Sample output:
    
    Timer List Version: v0.1
    HRTIMER_MAX_CLOCK_BASES: 2
    now at 4246046273872 nsecs
    
    cpu: 0
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_stop_sched_tick, swapper/0
     # expires at 4246432689566 nsecs [in 386415694 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, pcscd/2050
     # expires at 4247018194689 nsecs [in 971920817 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, irqbalance/1909
     # expires at 4247351358392 nsecs [in 1305084520 nsecs]
     #3: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, crond/2157
     # expires at 4249097614968 nsecs [in 3051341096 nsecs]
     #4: <f5a90ec8>, it_real_fn, do_setitimer, syslogd/1888
     # expires at 4251329900926 nsecs [in 5283627054 nsecs]
      .expires_next   : 4246432689566 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 31306
      .idle_tick      : 4246020791890 nsecs
      .tick_stopped   : 1
      .idle_jiffies   : 986504
      .idle_calls     : 40700
      .idle_sleeps    : 36014
      .idle_entrytime : 4246019418883 nsecs
      .idle_sleeptime : 4178181972709 nsecs
    
    cpu: 1
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_restart_sched_tick, swapper/0
     # expires at 4246050084568 nsecs [in 3810696 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, atd/2227
     # expires at 4261010635003 nsecs [in 14964361131 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, smartd/2332
     # expires at 5469485798970 nsecs [in 1223439525098 nsecs]
      .expires_next   : 4246050084568 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 24043
      .idle_tick      : 4246046084568 nsecs
      .tick_stopped   : 0
      .idle_jiffies   : 986510
      .idle_calls     : 26360
      .idle_sleeps    : 22551
      .idle_entrytime : 4246043874339 nsecs
      .idle_sleeptime : 4170763761184 nsecs
    
    tick_broadcast_mask: 00000003
    event_broadcast_mask: 00000001
    
    CPU#0's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246432689566 nsecs
    
    CPU#1's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246050084568 nsecs
    
    Clock Event Device: hpet
     capabilities:   00000007
     max_delta_ns:   2147483647
     min_delta_ns:   3352
     mult:           61496110
     shift:          32
     set_next_event: hpet_next_event
     set_mode:       hpet_set_mode
     event_handler:  handle_nextevt_broadcast
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index cf435e459598..9a7252e089b9 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -59,6 +59,7 @@ struct tick_sched {
 
 extern void __init tick_init(void);
 extern int tick_is_oneshot_available(void);
+extern struct tick_device *tick_get_device(int cpu);
 
 # ifdef CONFIG_HIGH_RES_TIMERS
 extern int tick_init_highres(void);
@@ -69,6 +70,16 @@ extern void tick_cancel_sched_timer(int cpu);
 static inline void tick_cancel_sched_timer(int cpu) { }
 # endif /* HIGHRES */
 
+# ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+extern struct tick_device *tick_get_broadcast_device(void);
+extern cpumask_t *tick_get_broadcast_mask(void);
+
+#  ifdef CONFIG_TICK_ONESHOT
+extern cpumask_t *tick_get_broadcast_oneshot_mask(void);
+#  endif
+
+# endif /* BROADCAST */
+
 # ifdef CONFIG_TICK_ONESHOT
 extern void tick_clock_notify(void);
 extern int tick_check_oneshot_change(int allow_nohz);

commit 79bf2bb335b85db25d27421c798595a2fa2a0e82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:03 2007 -0800

    [PATCH] tick-management: dyntick / highres functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add functions to provide dynamic ticks and high resolution timers.  The code
    which keeps track of jiffies and handles the long idle periods is shared
    between tick based and high resolution timer based dynticks.  The dyntick
    functionality can be disabled on the kernel commandline.  Provide also the
    infrastructure to support high resolution timers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
index e5c0a4e22706..cf435e459598 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -20,12 +20,79 @@ struct tick_device {
 	enum tick_device_mode mode;
 };
 
+enum tick_nohz_mode {
+	NOHZ_MODE_INACTIVE,
+	NOHZ_MODE_LOWRES,
+	NOHZ_MODE_HIGHRES,
+};
+
+/**
+ * struct tick_sched - sched tick emulation and no idle tick control/stats
+ * @sched_timer:	hrtimer to schedule the periodic tick in high
+ *			resolution mode
+ * @idle_tick:		Store the last idle tick expiry time when the tick
+ *			timer is modified for idle sleeps. This is necessary
+ *			to resume the tick timer operation in the timeline
+ *			when the CPU returns from idle
+ * @tick_stopped:	Indicator that the idle tick has been stopped
+ * @idle_jiffies:	jiffies at the entry to idle for idle time accounting
+ * @idle_calls:		Total number of idle calls
+ * @idle_sleeps:	Number of idle calls, where the sched tick was stopped
+ * @idle_entrytime:	Time when the idle call was entered
+ * @idle_sleeptime:	Sum of the time slept in idle with sched tick stopped
+ */
+struct tick_sched {
+	struct hrtimer			sched_timer;
+	unsigned long			check_clocks;
+	enum tick_nohz_mode		nohz_mode;
+	ktime_t				idle_tick;
+	int				tick_stopped;
+	unsigned long			idle_jiffies;
+	unsigned long			idle_calls;
+	unsigned long			idle_sleeps;
+	ktime_t				idle_entrytime;
+	ktime_t				idle_sleeptime;
+	unsigned long			last_jiffies;
+	unsigned long			next_jiffies;
+	ktime_t				idle_expires;
+};
+
 extern void __init tick_init(void);
+extern int tick_is_oneshot_available(void);
+
+# ifdef CONFIG_HIGH_RES_TIMERS
+extern int tick_init_highres(void);
+extern int tick_program_event(ktime_t expires, int force);
+extern void tick_setup_sched_timer(void);
+extern void tick_cancel_sched_timer(int cpu);
+# else
+static inline void tick_cancel_sched_timer(int cpu) { }
+# endif /* HIGHRES */
 
-#else
+# ifdef CONFIG_TICK_ONESHOT
+extern void tick_clock_notify(void);
+extern int tick_check_oneshot_change(int allow_nohz);
+extern struct tick_sched *tick_get_tick_sched(int cpu);
+# else
+static inline void tick_clock_notify(void) { }
+static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
+# endif
 
+#else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
+static inline void tick_cancel_sched_timer(int cpu) { }
+static inline void tick_clock_notify(void) { }
+static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
+#endif /* !CONFIG_GENERIC_CLOCKEVENTS */
 
-#endif
+# ifdef CONFIG_NO_HZ
+extern void tick_nohz_stop_sched_tick(void);
+extern void tick_nohz_restart_sched_tick(void);
+extern void tick_nohz_update_jiffies(void);
+# else
+static inline void tick_nohz_stop_sched_tick(void) { }
+static inline void tick_nohz_restart_sched_tick(void) { }
+static inline void tick_nohz_update_jiffies(void) { }
+# endif /* !NO_HZ */
 
 #endif

commit 906568c9c668ff994f4078932ec6ae1e3950d1af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:01 2007 -0800

    [PATCH] tick-management: core functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    The tick-management code is the first user of the clockevents layer.  It takes
    clock event devices from the clock events core and uses them to provide the
    periodic tick.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/tick.h b/include/linux/tick.h
new file mode 100644
index 000000000000..e5c0a4e22706
--- /dev/null
+++ b/include/linux/tick.h
@@ -0,0 +1,31 @@
+/*  linux/include/linux/tick.h
+ *
+ *  This file contains the structure definitions for tick related functions
+ *
+ */
+#ifndef _LINUX_TICK_H
+#define _LINUX_TICK_H
+
+#include <linux/clockchips.h>
+
+#ifdef CONFIG_GENERIC_CLOCKEVENTS
+
+enum tick_device_mode {
+	TICKDEV_MODE_PERIODIC,
+	TICKDEV_MODE_ONESHOT,
+};
+
+struct tick_device {
+	struct clock_event_device *evtdev;
+	enum tick_device_mode mode;
+};
+
+extern void __init tick_init(void);
+
+#else
+
+static inline void tick_init(void) { }
+
+#endif
+
+#endif
