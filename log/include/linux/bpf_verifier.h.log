commit 457f44363a8894135c85b7a9afd2bd8196db24ab
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 29 00:54:20 2020 -0700

    bpf: Implement BPF ring buffer and verifier support for it
    
    This commit adds a new MPSC ring buffer implementation into BPF ecosystem,
    which allows multiple CPUs to submit data to a single shared ring buffer. On
    the consumption side, only single consumer is assumed.
    
    Motivation
    ----------
    There are two distinctive motivators for this work, which are not satisfied by
    existing perf buffer, which prompted creation of a new ring buffer
    implementation.
      - more efficient memory utilization by sharing ring buffer across CPUs;
      - preserving ordering of events that happen sequentially in time, even
      across multiple CPUs (e.g., fork/exec/exit events for a task).
    
    These two problems are independent, but perf buffer fails to satisfy both.
    Both are a result of a choice to have per-CPU perf ring buffer.  Both can be
    also solved by having an MPSC implementation of ring buffer. The ordering
    problem could technically be solved for perf buffer with some in-kernel
    counting, but given the first one requires an MPSC buffer, the same solution
    would solve the second problem automatically.
    
    Semantics and APIs
    ------------------
    Single ring buffer is presented to BPF programs as an instance of BPF map of
    type BPF_MAP_TYPE_RINGBUF. Two other alternatives considered, but ultimately
    rejected.
    
    One way would be to, similar to BPF_MAP_TYPE_PERF_EVENT_ARRAY, make
    BPF_MAP_TYPE_RINGBUF could represent an array of ring buffers, but not enforce
    "same CPU only" rule. This would be more familiar interface compatible with
    existing perf buffer use in BPF, but would fail if application needed more
    advanced logic to lookup ring buffer by arbitrary key. HASH_OF_MAPS addresses
    this with current approach. Additionally, given the performance of BPF
    ringbuf, many use cases would just opt into a simple single ring buffer shared
    among all CPUs, for which current approach would be an overkill.
    
    Another approach could introduce a new concept, alongside BPF map, to
    represent generic "container" object, which doesn't necessarily have key/value
    interface with lookup/update/delete operations. This approach would add a lot
    of extra infrastructure that has to be built for observability and verifier
    support. It would also add another concept that BPF developers would have to
    familiarize themselves with, new syntax in libbpf, etc. But then would really
    provide no additional benefits over the approach of using a map.
    BPF_MAP_TYPE_RINGBUF doesn't support lookup/update/delete operations, but so
    doesn't few other map types (e.g., queue and stack; array doesn't support
    delete, etc).
    
    The approach chosen has an advantage of re-using existing BPF map
    infrastructure (introspection APIs in kernel, libbpf support, etc), being
    familiar concept (no need to teach users a new type of object in BPF program),
    and utilizing existing tooling (bpftool). For common scenario of using
    a single ring buffer for all CPUs, it's as simple and straightforward, as
    would be with a dedicated "container" object. On the other hand, by being
    a map, it can be combined with ARRAY_OF_MAPS and HASH_OF_MAPS map-in-maps to
    implement a wide variety of topologies, from one ring buffer for each CPU
    (e.g., as a replacement for perf buffer use cases), to a complicated
    application hashing/sharding of ring buffers (e.g., having a small pool of
    ring buffers with hashed task's tgid being a look up key to preserve order,
    but reduce contention).
    
    Key and value sizes are enforced to be zero. max_entries is used to specify
    the size of ring buffer and has to be a power of 2 value.
    
    There are a bunch of similarities between perf buffer
    (BPF_MAP_TYPE_PERF_EVENT_ARRAY) and new BPF ring buffer semantics:
      - variable-length records;
      - if there is no more space left in ring buffer, reservation fails, no
        blocking;
      - memory-mappable data area for user-space applications for ease of
        consumption and high performance;
      - epoll notifications for new incoming data;
      - but still the ability to do busy polling for new data to achieve the
        lowest latency, if necessary.
    
    BPF ringbuf provides two sets of APIs to BPF programs:
      - bpf_ringbuf_output() allows to *copy* data from one place to a ring
        buffer, similarly to bpf_perf_event_output();
      - bpf_ringbuf_reserve()/bpf_ringbuf_commit()/bpf_ringbuf_discard() APIs
        split the whole process into two steps. First, a fixed amount of space is
        reserved. If successful, a pointer to a data inside ring buffer data area
        is returned, which BPF programs can use similarly to a data inside
        array/hash maps. Once ready, this piece of memory is either committed or
        discarded. Discard is similar to commit, but makes consumer ignore the
        record.
    
    bpf_ringbuf_output() has disadvantage of incurring extra memory copy, because
    record has to be prepared in some other place first. But it allows to submit
    records of the length that's not known to verifier beforehand. It also closely
    matches bpf_perf_event_output(), so will simplify migration significantly.
    
    bpf_ringbuf_reserve() avoids the extra copy of memory by providing a memory
    pointer directly to ring buffer memory. In a lot of cases records are larger
    than BPF stack space allows, so many programs have use extra per-CPU array as
    a temporary heap for preparing sample. bpf_ringbuf_reserve() avoid this needs
    completely. But in exchange, it only allows a known constant size of memory to
    be reserved, such that verifier can verify that BPF program can't access
    memory outside its reserved record space. bpf_ringbuf_output(), while slightly
    slower due to extra memory copy, covers some use cases that are not suitable
    for bpf_ringbuf_reserve().
    
    The difference between commit and discard is very small. Discard just marks
    a record as discarded, and such records are supposed to be ignored by consumer
    code. Discard is useful for some advanced use-cases, such as ensuring
    all-or-nothing multi-record submission, or emulating temporary malloc()/free()
    within single BPF program invocation.
    
    Each reserved record is tracked by verifier through existing
    reference-tracking logic, similar to socket ref-tracking. It is thus
    impossible to reserve a record, but forget to submit (or discard) it.
    
    bpf_ringbuf_query() helper allows to query various properties of ring buffer.
    Currently 4 are supported:
      - BPF_RB_AVAIL_DATA returns amount of unconsumed data in ring buffer;
      - BPF_RB_RING_SIZE returns the size of ring buffer;
      - BPF_RB_CONS_POS/BPF_RB_PROD_POS returns current logical possition of
        consumer/producer, respectively.
    Returned values are momentarily snapshots of ring buffer state and could be
    off by the time helper returns, so this should be used only for
    debugging/reporting reasons or for implementing various heuristics, that take
    into account highly-changeable nature of some of those characteristics.
    
    One such heuristic might involve more fine-grained control over poll/epoll
    notifications about new data availability in ring buffer. Together with
    BPF_RB_NO_WAKEUP/BPF_RB_FORCE_WAKEUP flags for output/commit/discard helpers,
    it allows BPF program a high degree of control and, e.g., more efficient
    batched notifications. Default self-balancing strategy, though, should be
    adequate for most applications and will work reliable and efficiently already.
    
    Design and implementation
    -------------------------
    This reserve/commit schema allows a natural way for multiple producers, either
    on different CPUs or even on the same CPU/in the same BPF program, to reserve
    independent records and work with them without blocking other producers. This
    means that if BPF program was interruped by another BPF program sharing the
    same ring buffer, they will both get a record reserved (provided there is
    enough space left) and can work with it and submit it independently. This
    applies to NMI context as well, except that due to using a spinlock during
    reservation, in NMI context, bpf_ringbuf_reserve() might fail to get a lock,
    in which case reservation will fail even if ring buffer is not full.
    
    The ring buffer itself internally is implemented as a power-of-2 sized
    circular buffer, with two logical and ever-increasing counters (which might
    wrap around on 32-bit architectures, that's not a problem):
      - consumer counter shows up to which logical position consumer consumed the
        data;
      - producer counter denotes amount of data reserved by all producers.
    
    Each time a record is reserved, producer that "owns" the record will
    successfully advance producer counter. At that point, data is still not yet
    ready to be consumed, though. Each record has 8 byte header, which contains
    the length of reserved record, as well as two extra bits: busy bit to denote
    that record is still being worked on, and discard bit, which might be set at
    commit time if record is discarded. In the latter case, consumer is supposed
    to skip the record and move on to the next one. Record header also encodes
    record's relative offset from the beginning of ring buffer data area (in
    pages). This allows bpf_ringbuf_commit()/bpf_ringbuf_discard() to accept only
    the pointer to the record itself, without requiring also the pointer to ring
    buffer itself. Ring buffer memory location will be restored from record
    metadata header. This significantly simplifies verifier, as well as improving
    API usability.
    
    Producer counter increments are serialized under spinlock, so there is
    a strict ordering between reservations. Commits, on the other hand, are
    completely lockless and independent. All records become available to consumer
    in the order of reservations, but only after all previous records where
    already committed. It is thus possible for slow producers to temporarily hold
    off submitted records, that were reserved later.
    
    Reservation/commit/consumer protocol is verified by litmus tests in
    Documentation/litmus-test/bpf-rb.
    
    One interesting implementation bit, that significantly simplifies (and thus
    speeds up as well) implementation of both producers and consumers is how data
    area is mapped twice contiguously back-to-back in the virtual memory. This
    allows to not take any special measures for samples that have to wrap around
    at the end of the circular buffer data area, because the next page after the
    last data page would be first data page again, and thus the sample will still
    appear completely contiguous in virtual memory. See comment and a simple ASCII
    diagram showing this visually in bpf_ringbuf_area_alloc().
    
    Another feature that distinguishes BPF ringbuf from perf ring buffer is
    a self-pacing notifications of new data being availability.
    bpf_ringbuf_commit() implementation will send a notification of new record
    being available after commit only if consumer has already caught up right up
    to the record being committed. If not, consumer still has to catch up and thus
    will see new data anyways without needing an extra poll notification.
    Benchmarks (see tools/testing/selftests/bpf/benchs/bench_ringbuf.c) show that
    this allows to achieve a very high throughput without having to resort to
    tricks like "notify only every Nth sample", which are necessary with perf
    buffer. For extreme cases, when BPF program wants more manual control of
    notifications, commit/discard/output helpers accept BPF_RB_NO_WAKEUP and
    BPF_RB_FORCE_WAKEUP flags, which give full control over notifications of data
    availability, but require extra caution and diligence in using this API.
    
    Comparison to alternatives
    --------------------------
    Before considering implementing BPF ring buffer from scratch existing
    alternatives in kernel were evaluated, but didn't seem to meet the needs. They
    largely fell into few categores:
      - per-CPU buffers (perf, ftrace, etc), which don't satisfy two motivations
        outlined above (ordering and memory consumption);
      - linked list-based implementations; while some were multi-producer designs,
        consuming these from user-space would be very complicated and most
        probably not performant; memory-mapping contiguous piece of memory is
        simpler and more performant for user-space consumers;
      - io_uring is SPSC, but also requires fixed-sized elements. Naively turning
        SPSC queue into MPSC w/ lock would have subpar performance compared to
        locked reserve + lockless commit, as with BPF ring buffer. Fixed sized
        elements would be too limiting for BPF programs, given existing BPF
        programs heavily rely on variable-sized perf buffer already;
      - specialized implementations (like a new printk ring buffer, [0]) with lots
        of printk-specific limitations and implications, that didn't seem to fit
        well for intended use with BPF programs.
    
      [0] https://lwn.net/Articles/779550/
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200529075424.3139988-2-andriin@fb.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index ea833087e853..ca08db4ffb5f 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -54,6 +54,8 @@ struct bpf_reg_state {
 
 		u32 btf_id; /* for PTR_TO_BTF_ID */
 
+		u32 mem_size; /* for PTR_TO_MEM | PTR_TO_MEM_OR_NULL */
+
 		/* Max size from any of the above. */
 		unsigned long raw;
 	};
@@ -63,6 +65,8 @@ struct bpf_reg_state {
 	 * offset, so they can share range knowledge.
 	 * For PTR_TO_MAP_VALUE_OR_NULL this is used to share which map value we
 	 * came from, when one is tested for != NULL.
+	 * For PTR_TO_MEM_OR_NULL this is used to identify memory allocation
+	 * for the purpose of tracking that it's freed.
 	 * For PTR_TO_SOCKET this is used to share which pointers retain the
 	 * same reference to the socket, to determine proper reference freeing.
 	 */

commit 2c78ee898d8f10ae6fb2fa23a3fbaec96b1b7366
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed May 13 16:03:54 2020 -0700

    bpf: Implement CAP_BPF
    
    Implement permissions as stated in uapi/linux/capability.h
    In order to do that the verifier allow_ptr_leaks flag is split
    into four flags and they are set as:
      env->allow_ptr_leaks = bpf_allow_ptr_leaks();
      env->bypass_spec_v1 = bpf_bypass_spec_v1();
      env->bypass_spec_v4 = bpf_bypass_spec_v4();
      env->bpf_capable = bpf_capable();
    
    The first three currently equivalent to perfmon_capable(), since leaking kernel
    pointers and reading kernel memory via side channel attacks is roughly
    equivalent to reading kernel memory with cap_perfmon.
    
    'bpf_capable' enables bounded loops, precision tracking, bpf to bpf calls and
    other verifier features. 'allow_ptr_leaks' enable ptr leaks, ptr conversions,
    subtraction of pointers. 'bypass_spec_v1' disables speculative analysis in the
    verifier, run time mitigations in bpf array, and enables indirect variable
    access in bpf programs. 'bypass_spec_v4' disables emission of sanitation code
    by the verifier.
    
    That means that the networking BPF program loaded with CAP_BPF + CAP_NET_ADMIN
    will have speculative checks done by the verifier and other spectre mitigation
    applied. Such networking BPF program will not be able to leak kernel pointers
    and will not be able to access arbitrary kernel memory.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200513230355.7858-3-alexei.starovoitov@gmail.com

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 6abd5a778fcd..ea833087e853 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -375,6 +375,9 @@ struct bpf_verifier_env {
 	u32 used_map_cnt;		/* number of used maps */
 	u32 id_gen;			/* used to generate unique reg IDs */
 	bool allow_ptr_leaks;
+	bool bpf_capable;
+	bool bypass_spec_v1;
+	bool bypass_spec_v4;
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 	const struct bpf_line_info *prev_linfo;

commit 3f50f132d8400e129fc9eb68b5020167ef80a244
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Mar 30 14:36:39 2020 -0700

    bpf: Verifier, do explicit ALU32 bounds tracking
    
    It is not possible for the current verifier to track ALU32 and JMP ops
    correctly. This can result in the verifier aborting with errors even though
    the program should be verifiable. BPF codes that hit this can work around
    it by changin int variables to 64-bit types, marking variables volatile,
    etc. But this is all very ugly so it would be better to avoid these tricks.
    
    But, the main reason to address this now is do_refine_retval_range() was
    assuming return values could not be negative. Once we fixed this code that
    was previously working will no longer work. See do_refine_retval_range()
    patch for details. And we don't want to suddenly cause programs that used
    to work to fail.
    
    The simplest example code snippet that illustrates the problem is likely
    this,
    
     53: w8 = w0                    // r8 <- [0, S32_MAX],
                                    // w8 <- [-S32_MIN, X]
     54: w8 <s 0                    // r8 <- [0, U32_MAX]
                                    // w8 <- [0, X]
    
    The expected 64-bit and 32-bit bounds after each line are shown on the
    right. The current issue is without the w* bounds we are forced to use
    the worst case bound of [0, U32_MAX]. To resolve this type of case,
    jmp32 creating divergent 32-bit bounds from 64-bit bounds, we add explicit
    32-bit register bounds s32_{min|max}_value and u32_{min|max}_value. Then
    from branch_taken logic creating new bounds we can track 32-bit bounds
    explicitly.
    
    The next case we observed is ALU ops after the jmp32,
    
     53: w8 = w0                    // r8 <- [0, S32_MAX],
                                    // w8 <- [-S32_MIN, X]
     54: w8 <s 0                    // r8 <- [0, U32_MAX]
                                    // w8 <- [0, X]
     55: w8 += 1                    // r8 <- [0, U32_MAX+1]
                                    // w8 <- [0, X+1]
    
    In order to keep the bounds accurate at this point we also need to track
    ALU32 ops. To do this we add explicit ALU32 logic for each of the ALU
    ops, mov, add, sub, etc.
    
    Finally there is a question of how and when to merge bounds. The cases
    enumerate here,
    
    1. MOV ALU32   - zext 32-bit -> 64-bit
    2. MOV ALU64   - copy 64-bit -> 32-bit
    3. op  ALU32   - zext 32-bit -> 64-bit
    4. op  ALU64   - n/a
    5. jmp ALU32   - 64-bit: var32_off | upper_32_bits(var64_off)
    6. jmp ALU64   - 32-bit: (>> (<< var64_off))
    
    Details for each case,
    
    For "MOV ALU32" BPF arch zero extends so we simply copy the bounds
    from 32-bit into 64-bit ensuring we truncate var_off and 64-bit
    bounds correctly. See zext_32_to_64.
    
    For "MOV ALU64" copy all bounds including 32-bit into new register. If
    the src register had 32-bit bounds the dst register will as well.
    
    For "op ALU32" zero extend 32-bit into 64-bit the same as move,
    see zext_32_to_64.
    
    For "op ALU64" calculate both 32-bit and 64-bit bounds no merging
    is done here. Except we have a special case. When RSH or ARSH is
    done we can't simply ignore shifting bits from 64-bit reg into the
    32-bit subreg. So currently just push bounds from 64-bit into 32-bit.
    This will be correct in the sense that they will represent a valid
    state of the register. However we could lose some accuracy if an
    ARSH is following a jmp32 operation. We can handle this special
    case in a follow up series.
    
    For "jmp ALU32" mark 64-bit reg unknown and recalculate 64-bit bounds
    from tnum by setting var_off to ((<<(>>var_off)) | var32_off). We
    special case if 64-bit bounds has zero'd upper 32bits at which point
    we can simply copy 32-bit bounds into 64-bit register. This catches
    a common compiler trick where upper 32-bits are zeroed and then
    32-bit ops are used followed by a 64-bit compare or 64-bit op on
    a pointer. See __reg_combine_64_into_32().
    
    For "jmp ALU64" cast the bounds of the 64bit to their 32-bit
    counterpart. For example s32_min_value = (s32)reg->smin_value. For
    tnum use only the lower 32bits via, (>>(<<var_off)). See
    __reg_combine_64_into_32().
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/158560419880.10843.11448220440809118343.stgit@john-Precision-5820-Tower

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 5406e6e96585..6abd5a778fcd 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -123,6 +123,10 @@ struct bpf_reg_state {
 	s64 smax_value; /* maximum possible (s64)value */
 	u64 umin_value; /* minimum possible (u64)value */
 	u64 umax_value; /* maximum possible (u64)value */
+	s32 s32_min_value; /* minimum possible (s32)value */
+	s32 s32_max_value; /* maximum possible (s32)value */
+	u32 u32_min_value; /* minimum possible (u32)value */
+	u32 u32_max_value; /* maximum possible (u32)value */
 	/* parentage chain for liveness checking */
 	struct bpf_reg_state *parent;
 	/* Inside the callee two registers can be both PTR_TO_STACK like

commit 51c39bb1d5d105a02e29aa7960f0a395086e6342
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 9 22:41:20 2020 -0800

    bpf: Introduce function-by-function verification
    
    New llvm and old llvm with libbpf help produce BTF that distinguish global and
    static functions. Unlike arguments of static function the arguments of global
    functions cannot be removed or optimized away by llvm. The compiler has to use
    exactly the arguments specified in a function prototype. The argument type
    information allows the verifier validate each global function independently.
    For now only supported argument types are pointer to context and scalars. In
    the future pointers to structures, sizes, pointer to packet data can be
    supported as well. Consider the following example:
    
    static int f1(int ...)
    {
      ...
    }
    
    int f3(int b);
    
    int f2(int a)
    {
      f1(a) + f3(a);
    }
    
    int f3(int b)
    {
      ...
    }
    
    int main(...)
    {
      f1(...) + f2(...) + f3(...);
    }
    
    The verifier will start its safety checks from the first global function f2().
    It will recursively descend into f1() because it's static. Then it will check
    that arguments match for the f3() invocation inside f2(). It will not descend
    into f3(). It will finish f2() that has to be successfully verified for all
    possible values of 'a'. Then it will proceed with f3(). That function also has
    to be safe for all possible values of 'b'. Then it will start subprog 0 (which
    is main() function). It will recursively descend into f1() and will skip full
    check of f2() and f3(), since they are global. The order of processing global
    functions doesn't affect safety, since all global functions must be proven safe
    based on their arguments only.
    
    Such function by function verification can drastically improve speed of the
    verification and reduce complexity.
    
    Note that the stack limit of 512 still applies to the call chain regardless whether
    functions were static or global. The nested level of 8 also still applies. The
    same recursion prevention checks are in place as well.
    
    The type information and static/global kind is preserved after the verification
    hence in the above example global function f2() and f3() can be replaced later
    by equivalent functions with the same types that are loaded and verified later
    without affecting safety of this main() program. Such replacement (re-linking)
    of global functions is a subject of future patches.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200110064124.1760511-3-ast@kernel.org

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 26e40de9ef55..5406e6e96585 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -304,11 +304,13 @@ struct bpf_insn_aux_data {
 	u64 map_key_state; /* constant (32 bit) key tracking for maps */
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	int sanitize_stack_off; /* stack slot to be cleared */
-	bool seen; /* this insn was processed by the verifier */
+	u32 seen; /* this insn was processed by the verifier at env->pass_cnt */
 	bool zext_dst; /* this insn zero extends dst reg */
 	u8 alu_state; /* used in combination with alu_limit */
-	bool prune_point;
+
+	/* below fields are initialized once */
 	unsigned int orig_idx; /* original instruction index */
+	bool prune_point;
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
@@ -379,6 +381,7 @@ struct bpf_verifier_env {
 		int *insn_stack;
 		int cur_stack;
 	} cfg;
+	u32 pass_cnt; /* number of times do_check() was called */
 	u32 subprog_cnt;
 	/* number of instructions analyzed by the verifier */
 	u32 prev_insn_processed, insn_processed;
@@ -428,4 +431,7 @@ bpf_prog_offload_replace_insn(struct bpf_verifier_env *env, u32 off,
 void
 bpf_prog_offload_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt);
 
+int check_ctx_reg(struct bpf_verifier_env *env,
+		  const struct bpf_reg_state *reg, int regno);
+
 #endif /* _LINUX_BPF_VERIFIER_H */

commit d2e4c1e6c2947269346054ac8937ccfe9e0bcc6b
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 22 21:07:59 2019 +0100

    bpf: Constant map key tracking for prog array pokes
    
    Add tracking of constant keys into tail call maps. The signature of
    bpf_tail_call_proto is that arg1 is ctx, arg2 map pointer and arg3
    is a index key. The direct call approach for tail calls can be enabled
    if the verifier asserted that for all branches leading to the tail call
    helper invocation, the map pointer and index key were both constant
    and the same.
    
    Tracking of map pointers we already do from prior work via c93552c443eb
    ("bpf: properly enforce index mask to prevent out-of-bounds speculation")
    and 09772d92cd5a ("bpf: avoid retpoline for lookup/update/ delete calls
    on maps").
    
    Given the tail call map index key is not on stack but directly in the
    register, we can add similar tracking approach and later in fixup_bpf_calls()
    add a poke descriptor to the progs poke_tab with the relevant information
    for the JITing phase.
    
    We internally reuse insn->imm for the rewritten BPF_JMP | BPF_TAIL_CALL
    instruction in order to point into the prog's poke_tab, and keep insn->imm
    as 0 as indicator that current indirect tail call emission must be used.
    Note that publishing to the tracker must happen at the end of fixup_bpf_calls()
    since adding elements to the poke_tab reallocates its memory, so we need
    to wait until its in final state.
    
    Future work can generalize and add similar approach to optimize plain
    array map lookups. Difference there is that we need to look into the key
    value that sits on stack. For clarity in bpf_insn_aux_data, map_state
    has been renamed into map_ptr_state, so we get map_{ptr,key}_state as
    trackers.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/e8db37f6b2ae60402fa40216c96738ee9b316c32.1574452833.git.daniel@iogearbox.net

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index cdd08bf0ec06..26e40de9ef55 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -293,7 +293,7 @@ struct bpf_verifier_state_list {
 struct bpf_insn_aux_data {
 	union {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
-		unsigned long map_state;	/* pointer/poison value for maps */
+		unsigned long map_ptr_state;	/* pointer/poison value for maps */
 		s32 call_imm;			/* saved imm field of call insn */
 		u32 alu_limit;			/* limit for add/sub register with pointer */
 		struct {
@@ -301,6 +301,7 @@ struct bpf_insn_aux_data {
 			u32 map_off;		/* offset from value base address */
 		};
 	};
+	u64 map_key_state; /* constant (32 bit) key tracking for maps */
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */

commit 8c1b6e69dcc1e11bd24111e3734dd740aaf3fda1
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:16 2019 -0800

    bpf: Compare BTF types of functions arguments with actual types
    
    Make the verifier check that BTF types of function arguments match actual types
    passed into top-level BPF program and into BPF-to-BPF calls. If types match
    such BPF programs and sub-programs will have full support of BPF trampoline. If
    types mismatch the trampoline has to be conservative. It has to save/restore
    five program arguments and assume 64-bit scalars.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-17-ast@kernel.org

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 6e7284ea1468..cdd08bf0ec06 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -343,6 +343,7 @@ static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 #define BPF_MAX_SUBPROGS 256
 
 struct bpf_subprog_info {
+	/* 'start' has to be the first field otherwise find_subprog() won't work */
 	u32 start; /* insn idx of function entry point */
 	u32 linfo_idx; /* The idx to the main_prog->aux->linfo */
 	u16 stack_depth; /* max. stack depth used by this function */

commit 9e15db66136a14cde3f35691f1d839d950118826
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:25:00 2019 -0700

    bpf: Implement accurate raw_tp context access via BTF
    
    libbpf analyzes bpf C program, searches in-kernel BTF for given type name
    and stores it into expected_attach_type.
    The kernel verifier expects this btf_id to point to something like:
    typedef void (*btf_trace_kfree_skb)(void *, struct sk_buff *skb, void *loc);
    which represents signature of raw_tracepoint "kfree_skb".
    
    Then btf_ctx_access() matches ctx+0 access in bpf program with 'skb'
    and 'ctx+8' access with 'loc' arguments of "kfree_skb" tracepoint.
    In first case it passes btf_id of 'struct sk_buff *' back to the verifier core
    and 'void *' in second case.
    
    Then the verifier tracks PTR_TO_BTF_ID as any other pointer type.
    Like PTR_TO_SOCKET points to 'struct bpf_sock',
    PTR_TO_TCP_SOCK points to 'struct bpf_tcp_sock', and so on.
    PTR_TO_BTF_ID points to in-kernel structs.
    If 1234 is btf_id of 'struct sk_buff' in vmlinux's BTF
    then PTR_TO_BTF_ID#1234 points to one of in kernel skbs.
    
    When PTR_TO_BTF_ID#1234 is dereferenced (like r2 = *(u64 *)r1 + 32)
    the btf_struct_access() checks which field of 'struct sk_buff' is
    at offset 32. Checks that size of access matches type definition
    of the field and continues to track the dereferenced type.
    If that field was a pointer to 'struct net_device' the r2's type
    will be PTR_TO_BTF_ID#456. Where 456 is btf_id of 'struct net_device'
    in vmlinux's BTF.
    
    Such verifier analysis prevents "cheating" in BPF C program.
    The program cannot cast arbitrary pointer to 'struct sk_buff *'
    and access it. C compiler would allow type cast, of course,
    but the verifier will notice type mismatch based on BPF assembly
    and in-kernel BTF.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-7-ast@kernel.org

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 713efae62e96..6e7284ea1468 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -52,6 +52,8 @@ struct bpf_reg_state {
 		 */
 		struct bpf_map *map_ptr;
 
+		u32 btf_id; /* for PTR_TO_BTF_ID */
+
 		/* Max size from any of the above. */
 		unsigned long raw;
 	};
@@ -399,6 +401,8 @@ __printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,
 				      const char *fmt, va_list args);
 __printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,
 					   const char *fmt, ...);
+__printf(2, 3) void bpf_log(struct bpf_verifier_log *log,
+			    const char *fmt, ...);
 
 static inline struct bpf_func_state *cur_func(struct bpf_verifier_env *env)
 {

commit 8580ac9404f6240668a026785d7d8856f0530409
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:24:57 2019 -0700

    bpf: Process in-kernel BTF
    
    If in-kernel BTF exists parse it and prepare 'struct btf *btf_vmlinux'
    for further use by the verifier.
    In-kernel BTF is trusted just like kallsyms and other build artifacts
    embedded into vmlinux.
    Yet run this BTF image through BTF verifier to make sure
    that it is valid and it wasn't mangled during the build.
    If in-kernel BTF is incorrect it means either gcc or pahole or kernel
    are buggy. In such case disallow loading BPF programs.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-4-ast@kernel.org

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 26a6d58ca78c..713efae62e96 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -330,10 +330,12 @@ static inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)
 #define BPF_LOG_STATS	4
 #define BPF_LOG_LEVEL	(BPF_LOG_LEVEL1 | BPF_LOG_LEVEL2)
 #define BPF_LOG_MASK	(BPF_LOG_LEVEL | BPF_LOG_STATS)
+#define BPF_LOG_KERNEL	(BPF_LOG_MASK + 1) /* kernel internal flag */
 
 static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 {
-	return log->level && log->ubuf && !bpf_verifier_log_full(log);
+	return (log->level && log->ubuf && !bpf_verifier_log_full(log)) ||
+		log->level == BPF_LOG_KERNEL;
 }
 
 #define BPF_MAX_SUBPROGS 256

commit 10d274e880eb208ec6a76261a9f8f8155020f771
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Aug 22 22:52:12 2019 -0700

    bpf: introduce verifier internal test flag
    
    Introduce BPF_F_TEST_STATE_FREQ flag to stress test parentage chain
    and state pruning.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 5fe99f322b1c..26a6d58ca78c 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -355,6 +355,7 @@ struct bpf_verifier_env {
 	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */
 	int stack_size;			/* number of states to be processed */
 	bool strict_alignment;		/* perform strict pointer alignment checks */
+	bool test_state_freq;		/* test verifier with different pruning frequency */
 	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
 	struct bpf_verifier_state_list *free_list;

commit dca73a65a68329ee386d3ff473152bac66eaab39
Merge: 497ad9f5b2dc 94079b64255f
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 20 00:06:27 2019 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf-next 2019-06-19
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) new SO_REUSEPORT_DETACH_BPF setsocktopt, from Martin.
    
    2) BTF based map definition, from Andrii.
    
    3) support bpf_map_lookup_elem for xskmap, from Jonathan.
    
    4) bounded loops and scalar precision logic in the verifier, from Alexei.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b5dc0163d8fd78e64a7e21f309cf932fda34353e
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sat Jun 15 12:12:25 2019 -0700

    bpf: precise scalar_value tracking
    
    Introduce precision tracking logic that
    helps cilium programs the most:
                      old clang  old clang    new clang  new clang
                              with all patches         with all patches
    bpf_lb-DLB_L3.o      1838     2283         1923       1863
    bpf_lb-DLB_L4.o      3218     2657         3077       2468
    bpf_lb-DUNKNOWN.o    1064     545          1062       544
    bpf_lxc-DDROP_ALL.o  26935    23045        166729     22629
    bpf_lxc-DUNKNOWN.o   34439    35240        174607     28805
    bpf_netdev.o         9721     8753         8407       6801
    bpf_overlay.o        6184     7901         5420       4754
    bpf_lxc_jit.o        39389    50925        39389      50925
    
    Consider code:
    654: (85) call bpf_get_hash_recalc#34
    655: (bf) r7 = r0
    656: (15) if r8 == 0x0 goto pc+29
    657: (bf) r2 = r10
    658: (07) r2 += -48
    659: (18) r1 = 0xffff8881e41e1b00
    661: (85) call bpf_map_lookup_elem#1
    662: (15) if r0 == 0x0 goto pc+23
    663: (69) r1 = *(u16 *)(r0 +0)
    664: (15) if r1 == 0x0 goto pc+21
    665: (bf) r8 = r7
    666: (57) r8 &= 65535
    667: (bf) r2 = r8
    668: (3f) r2 /= r1
    669: (2f) r2 *= r1
    670: (bf) r1 = r8
    671: (1f) r1 -= r2
    672: (57) r1 &= 255
    673: (25) if r1 > 0x1e goto pc+12
     R0=map_value(id=0,off=0,ks=20,vs=64,imm=0) R1_w=inv(id=0,umax_value=30,var_off=(0x0; 0x1f))
    674: (67) r1 <<= 1
    675: (0f) r0 += r1
    
    At this point the verifier will notice that scalar R1 is used in map pointer adjustment.
    R1 has to be precise for later operations on R0 to be validated properly.
    
    The verifier will backtrack the above code in the following way:
    last_idx 675 first_idx 664
    regs=2 stack=0 before 675: (0f) r0 += r1         // started backtracking R1 regs=2 is a bitmask
    regs=2 stack=0 before 674: (67) r1 <<= 1
    regs=2 stack=0 before 673: (25) if r1 > 0x1e goto pc+12
    regs=2 stack=0 before 672: (57) r1 &= 255
    regs=2 stack=0 before 671: (1f) r1 -= r2         // now both R1 and R2 has to be precise -> regs=6 mask
    regs=6 stack=0 before 670: (bf) r1 = r8          // after this insn R8 and R2 has to be precise
    regs=104 stack=0 before 669: (2f) r2 *= r1       // after this one R8, R2, and R1
    regs=106 stack=0 before 668: (3f) r2 /= r1
    regs=106 stack=0 before 667: (bf) r2 = r8
    regs=102 stack=0 before 666: (57) r8 &= 65535
    regs=102 stack=0 before 665: (bf) r8 = r7
    regs=82 stack=0 before 664: (15) if r1 == 0x0 goto pc+21
     // this is the end of verifier state. The following regs will be marked precised:
     R1_rw=invP(id=0,umax_value=65535,var_off=(0x0; 0xffff)) R7_rw=invP(id=0)
    parent didn't have regs=82 stack=0 marks         // so backtracking continues into parent state
    last_idx 663 first_idx 655
    regs=82 stack=0 before 663: (69) r1 = *(u16 *)(r0 +0)   // R1 was assigned no need to track it further
    regs=80 stack=0 before 662: (15) if r0 == 0x0 goto pc+23    // keep tracking R7
    regs=80 stack=0 before 661: (85) call bpf_map_lookup_elem#1  // keep tracking R7
    regs=80 stack=0 before 659: (18) r1 = 0xffff8881e41e1b00
    regs=80 stack=0 before 658: (07) r2 += -48
    regs=80 stack=0 before 657: (bf) r2 = r10
    regs=80 stack=0 before 656: (15) if r8 == 0x0 goto pc+29
    regs=80 stack=0 before 655: (bf) r7 = r0                // here the assignment into R7
     // mark R0 to be precise:
     R0_rw=invP(id=0)
    parent didn't have regs=1 stack=0 marks                 // regs=1 -> tracking R0
    last_idx 654 first_idx 644
    regs=1 stack=0 before 654: (85) call bpf_get_hash_recalc#34 // and in the parent frame it was a return value
      // nothing further to backtrack
    
    Two scalar registers not marked precise are equivalent from state pruning point of view.
    More details in the patch comments.
    
    It doesn't support bpf2bpf calls yet and enabled for root only.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 03037373b447..19393b0964a8 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -139,6 +139,8 @@ struct bpf_reg_state {
 	 */
 	s32 subreg_def;
 	enum bpf_reg_liveness live;
+	/* if (!precise && SCALAR_VALUE) min/max/tnum don't affect safety */
+	bool precise;
 };
 
 enum bpf_stack_slot_type {
@@ -190,6 +192,11 @@ struct bpf_func_state {
 	struct bpf_stack_state *stack;
 };
 
+struct bpf_idx_pair {
+	u32 prev_idx;
+	u32 idx;
+};
+
 #define MAX_CALL_FRAMES 8
 struct bpf_verifier_state {
 	/* call stack tracking */
@@ -245,6 +252,17 @@ struct bpf_verifier_state {
 	u32 curframe;
 	u32 active_spin_lock;
 	bool speculative;
+
+	/* first and last insn idx of this verifier state */
+	u32 first_insn_idx;
+	u32 last_insn_idx;
+	/* jmp history recorded from first to last.
+	 * backtracking is using it to go from last to first.
+	 * For most states jmp_history_cnt is [0-3].
+	 * For loops can go up to ~40.
+	 */
+	struct bpf_idx_pair *jmp_history;
+	u32 jmp_history_cnt;
 };
 
 #define bpf_get_spilled_reg(slot, frame)				\

commit 2589726d12a1b12eaaa93c7f1ea64287e383c7a5
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sat Jun 15 12:12:20 2019 -0700

    bpf: introduce bounded loops
    
    Allow the verifier to validate the loops by simulating their execution.
    Exisiting programs have used '#pragma unroll' to unroll the loops
    by the compiler. Instead let the verifier simulate all iterations
    of the loop.
    In order to do that introduce parentage chain of bpf_verifier_state and
    'branches' counter for the number of branches left to explore.
    See more detailed algorithm description in bpf_verifier.h
    
    This algorithm borrows the key idea from Edward Cree approach:
    https://patchwork.ozlabs.org/patch/877222/
    Additional state pruning heuristics make such brute force loop walk
    practical even for large loops.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 704ed7971472..03037373b447 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -194,6 +194,53 @@ struct bpf_func_state {
 struct bpf_verifier_state {
 	/* call stack tracking */
 	struct bpf_func_state *frame[MAX_CALL_FRAMES];
+	struct bpf_verifier_state *parent;
+	/*
+	 * 'branches' field is the number of branches left to explore:
+	 * 0 - all possible paths from this state reached bpf_exit or
+	 * were safely pruned
+	 * 1 - at least one path is being explored.
+	 * This state hasn't reached bpf_exit
+	 * 2 - at least two paths are being explored.
+	 * This state is an immediate parent of two children.
+	 * One is fallthrough branch with branches==1 and another
+	 * state is pushed into stack (to be explored later) also with
+	 * branches==1. The parent of this state has branches==1.
+	 * The verifier state tree connected via 'parent' pointer looks like:
+	 * 1
+	 * 1
+	 * 2 -> 1 (first 'if' pushed into stack)
+	 * 1
+	 * 2 -> 1 (second 'if' pushed into stack)
+	 * 1
+	 * 1
+	 * 1 bpf_exit.
+	 *
+	 * Once do_check() reaches bpf_exit, it calls update_branch_counts()
+	 * and the verifier state tree will look:
+	 * 1
+	 * 1
+	 * 2 -> 1 (first 'if' pushed into stack)
+	 * 1
+	 * 1 -> 1 (second 'if' pushed into stack)
+	 * 0
+	 * 0
+	 * 0 bpf_exit.
+	 * After pop_stack() the do_check() will resume at second 'if'.
+	 *
+	 * If is_state_visited() sees a state with branches > 0 it means
+	 * there is a loop. If such state is exactly equal to the current state
+	 * it's an infinite loop. Note states_equal() checks for states
+	 * equvalency, so two states being 'states_equal' does not mean
+	 * infinite loop. The exact comparison is provided by
+	 * states_maybe_looping() function. It's a stronger pre-check and
+	 * much faster than states_equal().
+	 *
+	 * This algorithm may not find all possible infinite loops or
+	 * loop iteration count may be too high.
+	 * In such cases BPF_COMPLEXITY_LIMIT_INSNS limit kicks in.
+	 */
+	u32 branches;
 	u32 insn_idx;
 	u32 curframe;
 	u32 active_spin_lock;
@@ -312,7 +359,9 @@ struct bpf_verifier_env {
 	} cfg;
 	u32 subprog_cnt;
 	/* number of instructions analyzed by the verifier */
-	u32 insn_processed;
+	u32 prev_insn_processed, insn_processed;
+	/* number of jmps, calls, exits analyzed so far */
+	u32 prev_jmps_processed, jmps_processed;
 	/* total verification time */
 	u64 verification_time;
 	/* maximum number of verifier states kept in 'branching' instructions */

commit a6cdeeb16bff89c8486324f53577db058cbe81ba
Merge: 96524ea4be04 1e1d92636954
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 7 11:00:14 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Some ISDN files that got removed in net-next had some changes
    done in mainline, take the removals.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 25763b3c864cf517d686661012d184ee47a49b4c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:09 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 206
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of version 2 of the gnu general public license as
      published by the free software foundation
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 107 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171438.615055994@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 1305ccbd8fe6..519aafabc40c 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -1,8 +1,5 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of version 2 of the GNU General Public
- * License as published by the Free Software Foundation.
  */
 #ifndef _LINUX_BPF_VERIFIER_H
 #define _LINUX_BPF_VERIFIER_H 1

commit 5327ed3d44b754f5cc51d5b3f18e442eaebacff5
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Fri May 24 23:25:12 2019 +0100

    bpf: verifier: mark verified-insn with sub-register zext flag
    
    eBPF ISA specification requires high 32-bit cleared when low 32-bit
    sub-register is written. This applies to destination register of ALU32 etc.
    JIT back-ends must guarantee this semantic when doing code-gen. x86_64 and
    AArch64 ISA has the same semantics, so the corresponding JIT back-end
    doesn't need to do extra work.
    
    However, 32-bit arches (arm, x86, nfp etc.) and some other 64-bit arches
    (PowerPC, SPARC etc) need to do explicit zero extension to meet this
    requirement, otherwise code like the following will fail.
    
      u64_value = (u64) u32_value
      ... other uses of u64_value
    
    This is because compiler could exploit the semantic described above and
    save those zero extensions for extending u32_value to u64_value, these JIT
    back-ends are expected to guarantee this through inserting extra zero
    extensions which however could be a significant increase on the code size.
    Some benchmarks show there could be ~40% sub-register writes out of total
    insns, meaning at least ~40% extra code-gen.
    
    One observation is these extra zero extensions are not always necessary.
    Take above code snippet for example, it is possible u32_value will never be
    casted into a u64, the value of high 32-bit of u32_value then could be
    ignored and extra zero extension could be eliminated.
    
    This patch implements this idea, insns defining sub-registers will be
    marked when the high 32-bit of the defined sub-register matters. For
    those unmarked insns, it is safe to eliminate high 32-bit clearnace for
    them.
    
    Algo:
     - Split read flags into READ32 and READ64.
    
     - Record index of insn that does sub-register write. Keep the index inside
       reg state and update it during verifier insn walking.
    
     - A full register read on a sub-register marks its definition insn as
       needing zero extension on dst register.
    
       A new sub-register write overrides the old one.
    
     - When propagating read64 during path pruning, also mark any insn defining
       a sub-register that is read in the pruned path as full-register.
    
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 405b502283c5..704ed7971472 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -36,9 +36,11 @@
  */
 enum bpf_reg_liveness {
 	REG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */
-	REG_LIVE_READ, /* reg was read, so we're sensitive to initial value */
-	REG_LIVE_WRITTEN, /* reg was written first, screening off later reads */
-	REG_LIVE_DONE = 4, /* liveness won't be updating this register anymore */
+	REG_LIVE_READ32 = 0x1, /* reg was read, so we're sensitive to initial value */
+	REG_LIVE_READ64 = 0x2, /* likewise, but full 64-bit content matters */
+	REG_LIVE_READ = REG_LIVE_READ32 | REG_LIVE_READ64,
+	REG_LIVE_WRITTEN = 0x4, /* reg was written first, screening off later reads */
+	REG_LIVE_DONE = 0x8, /* liveness won't be updating this register anymore */
 };
 
 struct bpf_reg_state {
@@ -131,6 +133,11 @@ struct bpf_reg_state {
 	 * pointing to bpf_func_state.
 	 */
 	u32 frameno;
+	/* Tracks subreg definition. The stored value is the insn_idx of the
+	 * writing insn. This is safe because subreg_def is used before any insn
+	 * patching which only happens after main verification finished.
+	 */
+	s32 subreg_def;
 	enum bpf_reg_liveness live;
 };
 
@@ -233,6 +240,7 @@ struct bpf_insn_aux_data {
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */
+	bool zext_dst; /* this insn zero extends dst reg */
 	u8 alu_state; /* used in combination with alu_limit */
 	bool prune_point;
 	unsigned int orig_idx; /* original instruction index */

commit dc2a4ebc0b44a212fcf72242210e56aa17e7317b
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue May 21 20:17:07 2019 -0700

    bpf: convert explored_states to hash table
    
    All prune points inside a callee bpf function most likely will have
    different callsites. For example, if function foo() is called from
    two callsites the half of explored states in all prune points in foo()
    will be useless for subsequent walking of one of those callsites.
    Fortunately explored_states pruning heuristics keeps the number of states
    per prune point small, but walking these states is still a waste of cpu
    time when the callsite of the current state is different from the callsite
    of the explored state.
    
    To improve pruning logic convert explored_states into hash table and
    use simple insn_idx ^ callsite hash to select hash bucket.
    This optimization has no effect on programs without bpf2bpf calls
    and drastically improves programs with calls.
    In the later case it reduces total memory consumption in 1M scale tests
    by almost 3 times (peak_states drops from 5752 to 2016).
    
    Care should be taken when comparing the states for equivalency.
    Since the same hash bucket can now contain states with different indices
    the insn_idx has to be part of verifier_state and compared.
    
    Different hash table sizes and different hash functions were explored,
    but the results were not significantly better vs this patch.
    They can be improved in the future.
    
    Hit/miss heuristic is not counting index miscompare as a miss.
    Otherwise verifier stats become unstable when experimenting
    with different hash functions.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 02bba09a0ea1..405b502283c5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -187,6 +187,7 @@ struct bpf_func_state {
 struct bpf_verifier_state {
 	/* call stack tracking */
 	struct bpf_func_state *frame[MAX_CALL_FRAMES];
+	u32 insn_idx;
 	u32 curframe;
 	u32 active_spin_lock;
 	bool speculative;

commit a8f500af0ccffc3d2aaf9018537981cb173865a1
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue May 21 20:17:06 2019 -0700

    bpf: split explored_states
    
    split explored_states into prune_point boolean mark
    and link list of explored states.
    This removes STATE_LIST_MARK hack and allows marks to be separate from states.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 1305ccbd8fe6..02bba09a0ea1 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -233,6 +233,7 @@ struct bpf_insn_aux_data {
 	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */
 	u8 alu_state; /* used in combination with alu_limit */
+	bool prune_point;
 	unsigned int orig_idx; /* original instruction index */
 };
 

commit 7df737e991069d75eec1ded1c8b37e81b8c54df9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Fri Apr 19 07:44:54 2019 -0700

    bpf: remove global variables
    
    Move three global variables protected by bpf_verifier_lock into
    'struct bpf_verifier_env' to allow parallel verification.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index b3ab61fe1932..1305ccbd8fe6 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -295,6 +295,11 @@ struct bpf_verifier_env {
 	const struct bpf_line_info *prev_linfo;
 	struct bpf_verifier_log log;
 	struct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];
+	struct {
+		int *insn_state;
+		int *insn_stack;
+		int cur_stack;
+	} cfg;
 	u32 subprog_cnt;
 	/* number of instructions analyzed by the verifier */
 	u32 insn_processed;

commit d8eca5bbb2be9bc7546f9e733786fa2f1a594c67
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:03 2019 +0200

    bpf: implement lookup-free direct value access for maps
    
    This generic extension to BPF maps allows for directly loading
    an address residing inside a BPF map value as a single BPF
    ldimm64 instruction!
    
    The idea is similar to what BPF_PSEUDO_MAP_FD does today, which
    is a special src_reg flag for ldimm64 instruction that indicates
    that inside the first part of the double insns's imm field is a
    file descriptor which the verifier then replaces as a full 64bit
    address of the map into both imm parts. For the newly added
    BPF_PSEUDO_MAP_VALUE src_reg flag, the idea is the following:
    the first part of the double insns's imm field is again a file
    descriptor corresponding to the map, and the second part of the
    imm field is an offset into the value. The verifier will then
    replace both imm parts with an address that points into the BPF
    map value at the given value offset for maps that support this
    operation. Currently supported is array map with single entry.
    It is possible to support more than just single map element by
    reusing both 16bit off fields of the insns as a map index, so
    full array map lookup could be expressed that way. It hasn't
    been implemented here due to lack of concrete use case, but
    could easily be done so in future in a compatible way, since
    both off fields right now have to be 0 and would correctly
    denote a map index 0.
    
    The BPF_PSEUDO_MAP_VALUE is a distinct flag as otherwise with
    BPF_PSEUDO_MAP_FD we could not differ offset 0 between load of
    map pointer versus load of map's value at offset 0, and changing
    BPF_PSEUDO_MAP_FD's encoding into off by one to differ between
    regular map pointer and map value pointer would add unnecessary
    complexity and increases barrier for debugability thus less
    suitable. Using the second part of the imm field as an offset
    into the value does /not/ come with limitations since maximum
    possible value size is in u32 universe anyway.
    
    This optimization allows for efficiently retrieving an address
    to a map value memory area without having to issue a helper call
    which needs to prepare registers according to calling convention,
    etc, without needing the extra NULL test, and without having to
    add the offset in an additional instruction to the value base
    pointer. The verifier then treats the destination register as
    PTR_TO_MAP_VALUE with constant reg->off from the user passed
    offset from the second imm field, and guarantees that this is
    within bounds of the map value. Any subsequent operations are
    normally treated as typical map value handling without anything
    extra needed from verification side.
    
    The two map operations for direct value access have been added to
    array map for now. In future other types could be supported as
    well depending on the use case. The main use case for this commit
    is to allow for BPF loader support for global variables that
    reside in .data/.rodata/.bss sections such that we can directly
    load the address of them with minimal additional infrastructure
    required. Loader support has been added in subsequent commits for
    libbpf library.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index fc8254d6b569..b3ab61fe1932 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -224,6 +224,10 @@ struct bpf_insn_aux_data {
 		unsigned long map_state;	/* pointer/poison value for maps */
 		s32 call_imm;			/* saved imm field of call insn */
 		u32 alu_limit;			/* limit for add/sub register with pointer */
+		struct {
+			u32 map_index;		/* index into used_maps[] */
+			u32 map_off;		/* offset from value base address */
+		};
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	int sanitize_stack_off; /* stack slot to be cleared */

commit 9f4686c41bdff051f557accb531af79dd1773687
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Apr 1 21:27:41 2019 -0700

    bpf: improve verification speed by droping states
    
    Branch instructions, branch targets and calls in a bpf program are
    the places where the verifier remembers states that led to successful
    verification of the program.
    These states are used to prune brute force program analysis.
    For unprivileged programs there is a limit of 64 states per such
    'branching' instructions (maximum length is tracked by max_states_per_insn
    counter introduced in the previous patch).
    Simply reducing this threshold to 32 or lower increases insn_processed
    metric to the point that small valid programs get rejected.
    For root programs there is no limit and cilium programs can have
    max_states_per_insn to be 100 or higher.
    Walking 100+ states multiplied by number of 'branching' insns during
    verification consumes significant amount of cpu time.
    Turned out simple LRU-like mechanism can be used to remove states
    that unlikely will be helpful in future search pruning.
    This patch introduces hit_cnt and miss_cnt counters:
    hit_cnt - this many times this state successfully pruned the search
    miss_cnt - this many times this state was not equivalent to other states
    (and that other states were added to state list)
    
    The heuristic introduced in this patch is:
    if (sl->miss_cnt > sl->hit_cnt * 3 + 3)
      /* drop this state from future considerations */
    
    Higher numbers increase max_states_per_insn (allow more states to be
    considered for pruning) and slow verification speed, but do not meaningfully
    reduce insn_processed metric.
    Lower numbers drop too many states and insn_processed increases too much.
    Many different formulas were considered.
    This one is simple and works well enough in practice.
    (the analysis was done on selftests/progs/* and on cilium programs)
    
    The end result is this heuristic improves verification speed by 10 times.
    Large synthetic programs that used to take a second more now take
    1/10 of a second.
    In cases where max_states_per_insn used to be 100 or more, now it's ~10.
    
    There is a slight increase in insn_processed for cilium progs:
                           before   after
    bpf_lb-DLB_L3.o         1831    1838
    bpf_lb-DLB_L4.o         3029    3218
    bpf_lb-DUNKNOWN.o       1064    1064
    bpf_lxc-DDROP_ALL.o     26309   26935
    bpf_lxc-DUNKNOWN.o      33517   34439
    bpf_netdev.o            9713    9721
    bpf_overlay.o           6184    6184
    bpf_lcx_jit.o           37335   39389
    And 2-3 times improvement in the verification speed.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index f7e15eeb60bb..fc8254d6b569 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -207,6 +207,7 @@ struct bpf_verifier_state {
 struct bpf_verifier_state_list {
 	struct bpf_verifier_state state;
 	struct bpf_verifier_state_list *next;
+	int miss_cnt, hit_cnt;
 };
 
 /* Possible states for alu_state member. */
@@ -280,6 +281,7 @@ struct bpf_verifier_env {
 	bool strict_alignment;		/* perform strict pointer alignment checks */
 	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
+	struct bpf_verifier_state_list *free_list;
 	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
 	u32 used_map_cnt;		/* number of used maps */
 	u32 id_gen;			/* used to generate unique reg IDs */

commit 06ee7115b0d1742de745ad143fb5e06d77d27fba
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Apr 1 21:27:40 2019 -0700

    bpf: add verifier stats and log_level bit 2
    
    In order to understand the verifier bottlenecks add various stats
    and extend log_level:
    log_level 1 and 2 are kept as-is:
    bit 0 - level=1 - print every insn and verifier state at branch points
    bit 1 - level=2 - print every insn and verifier state at every insn
    bit 2 - level=4 - print verifier error and stats at the end of verification
    
    When verifier rejects the program the libbpf is trying to load the program twice.
    Once with log_level=0 (no messages, only error code is reported to user space)
    and second time with log_level=1 to tell the user why the verifier rejected it.
    
    With introduction of bit 2 - level=4 the libbpf can choose to always use that
    level and load programs once, since the verification speed is not affected and
    in case of error the verbose message will be available.
    
    Note that the verifier stats are not part of uapi just like all other
    verbose messages. They're expected to change in the future.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7d8228d1c898..f7e15eeb60bb 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -248,6 +248,12 @@ static inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)
 	return log->len_used >= log->len_total - 1;
 }
 
+#define BPF_LOG_LEVEL1	1
+#define BPF_LOG_LEVEL2	2
+#define BPF_LOG_STATS	4
+#define BPF_LOG_LEVEL	(BPF_LOG_LEVEL1 | BPF_LOG_LEVEL2)
+#define BPF_LOG_MASK	(BPF_LOG_LEVEL | BPF_LOG_STATS)
+
 static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 {
 	return log->level && log->ubuf && !bpf_verifier_log_full(log);
@@ -284,6 +290,21 @@ struct bpf_verifier_env {
 	struct bpf_verifier_log log;
 	struct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;
+	/* number of instructions analyzed by the verifier */
+	u32 insn_processed;
+	/* total verification time */
+	u64 verification_time;
+	/* maximum number of verifier states kept in 'branching' instructions */
+	u32 max_states_per_insn;
+	/* total number of allocated verifier states */
+	u32 total_states;
+	/* some states are freed during program analysis.
+	 * this is peak number of states. this number dominates kernel
+	 * memory consumption during verification
+	 */
+	u32 peak_states;
+	/* longest register parentage chain walked for liveness marking */
+	u32 longest_mark_read_walk;
 };
 
 __printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,

commit 1b986589680a2a5b6fc1ac196ea69925a93d9dd9
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue Mar 12 10:23:02 2019 -0700

    bpf: Fix bpf_tcp_sock and bpf_sk_fullsock issue related to bpf_sk_release
    
    Lorenz Bauer [thanks!] reported that a ptr returned by bpf_tcp_sock(sk)
    can still be accessed after bpf_sk_release(sk).
    Both bpf_tcp_sock() and bpf_sk_fullsock() have the same issue.
    This patch addresses them together.
    
    A simple reproducer looks like this:
    
            sk = bpf_sk_lookup_tcp();
            /* if (!sk) ... */
            tp = bpf_tcp_sock(sk);
            /* if (!tp) ... */
            bpf_sk_release(sk);
            snd_cwnd = tp->snd_cwnd; /* oops! The verifier does not complain. */
    
    The problem is the verifier did not scrub the register's states of
    the tcp_sock ptr (tp) after bpf_sk_release(sk).
    
    [ Note that when calling bpf_tcp_sock(sk), the sk is not always
      refcount-acquired. e.g. bpf_tcp_sock(skb->sk). The verifier works
      fine for this case. ]
    
    Currently, the verifier does not track if a helper's return ptr (in REG_0)
    is "carry"-ing one of its argument's refcount status. To carry this info,
    the reg1->id needs to be stored in reg0.
    
    One approach was tried, like "reg0->id = reg1->id", when calling
    "bpf_tcp_sock()".  The main idea was to avoid adding another "ref_obj_id"
    for the same reg.  However, overlapping the NULL marking and ref
    tracking purpose in one "id" does not work well:
    
            ref_sk = bpf_sk_lookup_tcp();
            fullsock = bpf_sk_fullsock(ref_sk);
            tp = bpf_tcp_sock(ref_sk);
            if (!fullsock) {
                 bpf_sk_release(ref_sk);
                 return 0;
            }
            /* fullsock_reg->id is marked for NOT-NULL.
             * Same for tp_reg->id because they have the same id.
             */
    
            /* oops. verifier did not complain about the missing !tp check */
            snd_cwnd = tp->snd_cwnd;
    
    Hence, a new "ref_obj_id" is needed in "struct bpf_reg_state".
    With a new ref_obj_id, when bpf_sk_release(sk) is called, the verifier can
    scrub all reg states which has a ref_obj_id match.  It is done with the
    changes in release_reg_references() in this patch.
    
    While fixing it, sk_to_full_sk() is removed from bpf_tcp_sock() and
    bpf_sk_fullsock() to avoid these helpers from returning
    another ptr. It will make bpf_sk_release(tp) possible:
    
            sk = bpf_sk_lookup_tcp();
            /* if (!sk) ... */
            tp = bpf_tcp_sock(sk);
            /* if (!tp) ... */
            bpf_sk_release(tp);
    
    A separate helper "bpf_get_listener_sock()" will be added in a later
    patch to do sk_to_full_sk().
    
    Misc change notes:
    - To allow bpf_sk_release(tp), the arg of bpf_sk_release() is changed
      from ARG_PTR_TO_SOCKET to ARG_PTR_TO_SOCK_COMMON.  ARG_PTR_TO_SOCKET
      is removed from bpf.h since no helper is using it.
    
    - arg_type_is_refcounted() is renamed to arg_type_may_be_refcounted()
      because ARG_PTR_TO_SOCK_COMMON is the only one and skb->sk is not
      refcounted.  All bpf_sk_release(), bpf_sk_fullsock() and bpf_tcp_sock()
      take ARG_PTR_TO_SOCK_COMMON.
    
    - check_refcount_ok() ensures is_acquire_function() cannot take
      arg_type_may_be_refcounted() as its argument.
    
    - The check_func_arg() can only allow one refcount-ed arg.  It is
      guaranteed by check_refcount_ok() which ensures at most one arg can be
      refcounted.  Hence, it is a verifier internal error if >1 refcount arg
      found in check_func_arg().
    
    - In release_reference(), release_reference_state() is called
      first to ensure a match on "reg->ref_obj_id" can be found before
      scrubbing the reg states with release_reg_references().
    
    - reg_is_refcounted() is no longer needed.
      1. In mark_ptr_or_null_regs(), its usage is replaced by
         "ref_obj_id && ref_obj_id == id" because,
         when is_null == true, release_reference_state() should only be
         called on the ref_obj_id obtained by a acquire helper (i.e.
         is_acquire_function() == true).  Otherwise, the following
         would happen:
    
            sk = bpf_sk_lookup_tcp();
            /* if (!sk) { ... } */
            fullsock = bpf_sk_fullsock(sk);
            if (!fullsock) {
                    /*
                     * release_reference_state(fullsock_reg->ref_obj_id)
                     * where fullsock_reg->ref_obj_id == sk_reg->ref_obj_id.
                     *
                     * Hence, the following bpf_sk_release(sk) will fail
                     * because the ref state has already been released in the
                     * earlier release_reference_state(fullsock_reg->ref_obj_id).
                     */
                    bpf_sk_release(sk);
            }
    
      2. In release_reg_references(), the current reg_is_refcounted() call
         is unnecessary because the id check is enough.
    
    - The type_is_refcounted() and type_is_refcounted_or_null()
      are no longer needed also because reg_is_refcounted() is removed.
    
    Fixes: 655a51e536c0 ("bpf: Add struct bpf_tcp_sock and BPF_FUNC_tcp_sock")
    Reported-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 69f7a3449eda..7d8228d1c898 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -66,6 +66,46 @@ struct bpf_reg_state {
 	 * same reference to the socket, to determine proper reference freeing.
 	 */
 	u32 id;
+	/* PTR_TO_SOCKET and PTR_TO_TCP_SOCK could be a ptr returned
+	 * from a pointer-cast helper, bpf_sk_fullsock() and
+	 * bpf_tcp_sock().
+	 *
+	 * Consider the following where "sk" is a reference counted
+	 * pointer returned from "sk = bpf_sk_lookup_tcp();":
+	 *
+	 * 1: sk = bpf_sk_lookup_tcp();
+	 * 2: if (!sk) { return 0; }
+	 * 3: fullsock = bpf_sk_fullsock(sk);
+	 * 4: if (!fullsock) { bpf_sk_release(sk); return 0; }
+	 * 5: tp = bpf_tcp_sock(fullsock);
+	 * 6: if (!tp) { bpf_sk_release(sk); return 0; }
+	 * 7: bpf_sk_release(sk);
+	 * 8: snd_cwnd = tp->snd_cwnd;  // verifier will complain
+	 *
+	 * After bpf_sk_release(sk) at line 7, both "fullsock" ptr and
+	 * "tp" ptr should be invalidated also.  In order to do that,
+	 * the reg holding "fullsock" and "sk" need to remember
+	 * the original refcounted ptr id (i.e. sk_reg->id) in ref_obj_id
+	 * such that the verifier can reset all regs which have
+	 * ref_obj_id matching the sk_reg->id.
+	 *
+	 * sk_reg->ref_obj_id is set to sk_reg->id at line 1.
+	 * sk_reg->id will stay as NULL-marking purpose only.
+	 * After NULL-marking is done, sk_reg->id can be reset to 0.
+	 *
+	 * After "fullsock = bpf_sk_fullsock(sk);" at line 3,
+	 * fullsock_reg->ref_obj_id is set to sk_reg->ref_obj_id.
+	 *
+	 * After "tp = bpf_tcp_sock(fullsock);" at line 5,
+	 * tp_reg->ref_obj_id is set to fullsock_reg->ref_obj_id
+	 * which is the same as sk_reg->ref_obj_id.
+	 *
+	 * From the verifier perspective, if sk, fullsock and tp
+	 * are not NULL, they are the same ptr with different
+	 * reg->type.  In particular, bpf_sk_release(tp) is also
+	 * allowed and has the same effect as bpf_sk_release(sk).
+	 */
+	u32 ref_obj_id;
 	/* For scalar types (SCALAR_VALUE), this represents our knowledge of
 	 * the actual value.
 	 * For pointer types, this represents the variable part of the offset

commit d83525ca62cf8ebe3271d14c36fb900c294274a2
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 31 15:40:04 2019 -0800

    bpf: introduce bpf_spin_lock
    
    Introduce 'struct bpf_spin_lock' and bpf_spin_lock/unlock() helpers to let
    bpf program serialize access to other variables.
    
    Example:
    struct hash_elem {
        int cnt;
        struct bpf_spin_lock lock;
    };
    struct hash_elem * val = bpf_map_lookup_elem(&hash_map, &key);
    if (val) {
        bpf_spin_lock(&val->lock);
        val->cnt++;
        bpf_spin_unlock(&val->lock);
    }
    
    Restrictions and safety checks:
    - bpf_spin_lock is only allowed inside HASH and ARRAY maps.
    - BTF description of the map is mandatory for safety analysis.
    - bpf program can take one bpf_spin_lock at a time, since two or more can
      cause dead locks.
    - only one 'struct bpf_spin_lock' is allowed per map element.
      It drastically simplifies implementation yet allows bpf program to use
      any number of bpf_spin_locks.
    - when bpf_spin_lock is taken the calls (either bpf2bpf or helpers) are not allowed.
    - bpf program must bpf_spin_unlock() before return.
    - bpf program can access 'struct bpf_spin_lock' only via
      bpf_spin_lock()/bpf_spin_unlock() helpers.
    - load/store into 'struct bpf_spin_lock lock;' field is not allowed.
    - to use bpf_spin_lock() helper the BTF description of map value must be
      a struct and have 'struct bpf_spin_lock anyname;' field at the top level.
      Nested lock inside another struct is not allowed.
    - syscall map_lookup doesn't copy bpf_spin_lock field to user space.
    - syscall map_update and program map_update do not update bpf_spin_lock field.
    - bpf_spin_lock cannot be on the stack or inside networking packet.
      bpf_spin_lock can only be inside HASH or ARRAY map value.
    - bpf_spin_lock is available to root only and to all program types.
    - bpf_spin_lock is not allowed in inner maps of map-in-map.
    - ld_abs is not allowed inside spin_lock-ed region.
    - tracing progs and socket filter progs cannot use bpf_spin_lock due to
      insufficient preemption checks
    
    Implementation details:
    - cgroup-bpf class of programs can nest with xdp/tc programs.
      Hence bpf_spin_lock is equivalent to spin_lock_irqsave.
      Other solutions to avoid nested bpf_spin_lock are possible.
      Like making sure that all networking progs run with softirq disabled.
      spin_lock_irqsave is the simplest and doesn't add overhead to the
      programs that don't use it.
    - arch_spinlock_t is used when its implemented as queued_spin_lock
    - archs can force their own arch_spinlock_t
    - on architectures where queued_spin_lock is not available and
      sizeof(arch_spinlock_t) != sizeof(__u32) trivial lock is used.
    - presence of bpf_spin_lock inside map value could have been indicated via
      extra flag during map_create, but specifying it via BTF is cleaner.
      It provides introspection for map key/value and reduces user mistakes.
    
    Next steps:
    - allow bpf_spin_lock in other map types (like cgroup local storage)
    - introduce BPF_F_LOCK flag for bpf_map_update() syscall and helper
      to request kernel to grab bpf_spin_lock before rewriting the value.
      That will serialize access to map elements.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 0620e418dde5..69f7a3449eda 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -148,6 +148,7 @@ struct bpf_verifier_state {
 	/* call stack tracking */
 	struct bpf_func_state *frame[MAX_CALL_FRAMES];
 	u32 curframe;
+	u32 active_spin_lock;
 	bool speculative;
 };
 

commit 08ca90afba255d05dc3253caa44056e7aecbe8c5
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Jan 22 22:45:24 2019 -0800

    bpf: notify offload JITs about optimizations
    
    Let offload JITs know when instructions are replaced and optimized
    out, so they can update their state appropriately.  The optimizations
    are best effort, if JIT returns an error from any callback verifier
    will stop notifying it as state may now be out of sync, but the
    verifier continues making progress.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index f3ae00ee5516..0620e418dde5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -266,5 +266,10 @@ int bpf_prog_offload_verifier_prep(struct bpf_prog *prog);
 int bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,
 				 int insn_idx, int prev_insn_idx);
 int bpf_prog_offload_finalize(struct bpf_verifier_env *env);
+void
+bpf_prog_offload_replace_insn(struct bpf_verifier_env *env, u32 off,
+			      struct bpf_insn *insn);
+void
+bpf_prog_offload_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt);
 
 #endif /* _LINUX_BPF_VERIFIER_H */

commit 9e4c24e7ee7dfd3898269519103e823892b730d8
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Jan 22 22:45:23 2019 -0800

    bpf: verifier: record original instruction index
    
    The communication between the verifier and advanced JITs is based
    on instruction indexes.  We have to keep them stable throughout
    the optimizations otherwise referring to a particular instruction
    gets messy quickly.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 573cca00a0e6..f3ae00ee5516 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -187,6 +187,7 @@ struct bpf_insn_aux_data {
 	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */
 	u8 alu_state; /* used in combination with alu_limit */
+	unsigned int orig_idx; /* original instruction index */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit d3bd7413e0ca40b60cf60d4003246d067cafdeda
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Jan 6 00:54:37 2019 +0100

    bpf: fix sanitation of alu op with pointer / scalar type from different paths
    
    While 979d63d50c0c ("bpf: prevent out of bounds speculation on pointer
    arithmetic") took care of rejecting alu op on pointer when e.g. pointer
    came from two different map values with different map properties such as
    value size, Jann reported that a case was not covered yet when a given
    alu op is used in both "ptr_reg += reg" and "numeric_reg += reg" from
    different branches where we would incorrectly try to sanitize based
    on the pointer's limit. Catch this corner case and reject the program
    instead.
    
    Fixes: 979d63d50c0c ("bpf: prevent out of bounds speculation on pointer arithmetic")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 27b74947cd2b..573cca00a0e6 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -172,6 +172,7 @@ struct bpf_verifier_state_list {
 #define BPF_ALU_SANITIZE_SRC		1U
 #define BPF_ALU_SANITIZE_DST		2U
 #define BPF_ALU_NEG_VALUE		(1U << 2)
+#define BPF_ALU_NON_POINTER		(1U << 3)
 #define BPF_ALU_SANITIZE		(BPF_ALU_SANITIZE_SRC | \
 					 BPF_ALU_SANITIZE_DST)
 

commit 979d63d50c0c0f7bc537bf821e056cc9fe5abd38
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jan 3 00:58:34 2019 +0100

    bpf: prevent out of bounds speculation on pointer arithmetic
    
    Jann reported that the original commit back in b2157399cc98
    ("bpf: prevent out-of-bounds speculation") was not sufficient
    to stop CPU from speculating out of bounds memory access:
    While b2157399cc98 only focussed on masking array map access
    for unprivileged users for tail calls and data access such
    that the user provided index gets sanitized from BPF program
    and syscall side, there is still a more generic form affected
    from BPF programs that applies to most maps that hold user
    data in relation to dynamic map access when dealing with
    unknown scalars or "slow" known scalars as access offset, for
    example:
    
      - Load a map value pointer into R6
      - Load an index into R7
      - Do a slow computation (e.g. with a memory dependency) that
        loads a limit into R8 (e.g. load the limit from a map for
        high latency, then mask it to make the verifier happy)
      - Exit if R7 >= R8 (mispredicted branch)
      - Load R0 = R6[R7]
      - Load R0 = R6[R0]
    
    For unknown scalars there are two options in the BPF verifier
    where we could derive knowledge from in order to guarantee
    safe access to the memory: i) While </>/<=/>= variants won't
    allow to derive any lower or upper bounds from the unknown
    scalar where it would be safe to add it to the map value
    pointer, it is possible through ==/!= test however. ii) another
    option is to transform the unknown scalar into a known scalar,
    for example, through ALU ops combination such as R &= <imm>
    followed by R |= <imm> or any similar combination where the
    original information from the unknown scalar would be destroyed
    entirely leaving R with a constant. The initial slow load still
    precedes the latter ALU ops on that register, so the CPU
    executes speculatively from that point. Once we have the known
    scalar, any compare operation would work then. A third option
    only involving registers with known scalars could be crafted
    as described in [0] where a CPU port (e.g. Slow Int unit)
    would be filled with many dependent computations such that
    the subsequent condition depending on its outcome has to wait
    for evaluation on its execution port and thereby executing
    speculatively if the speculated code can be scheduled on a
    different execution port, or any other form of mistraining
    as described in [1], for example. Given this is not limited
    to only unknown scalars, not only map but also stack access
    is affected since both is accessible for unprivileged users
    and could potentially be used for out of bounds access under
    speculation.
    
    In order to prevent any of these cases, the verifier is now
    sanitizing pointer arithmetic on the offset such that any
    out of bounds speculation would be masked in a way where the
    pointer arithmetic result in the destination register will
    stay unchanged, meaning offset masked into zero similar as
    in array_index_nospec() case. With regards to implementation,
    there are three options that were considered: i) new insn
    for sanitation, ii) push/pop insn and sanitation as inlined
    BPF, iii) reuse of ax register and sanitation as inlined BPF.
    
    Option i) has the downside that we end up using from reserved
    bits in the opcode space, but also that we would require
    each JIT to emit masking as native arch opcodes meaning
    mitigation would have slow adoption till everyone implements
    it eventually which is counter-productive. Option ii) and iii)
    have both in common that a temporary register is needed in
    order to implement the sanitation as inlined BPF since we
    are not allowed to modify the source register. While a push /
    pop insn in ii) would be useful to have in any case, it
    requires once again that every JIT needs to implement it
    first. While possible, amount of changes needed would also
    be unsuitable for a -stable patch. Therefore, the path which
    has fewer changes, less BPF instructions for the mitigation
    and does not require anything to be changed in the JITs is
    option iii) which this work is pursuing. The ax register is
    already mapped to a register in all JITs (modulo arm32 where
    it's mapped to stack as various other BPF registers there)
    and used in constant blinding for JITs-only so far. It can
    be reused for verifier rewrites under certain constraints.
    The interpreter's tmp "register" has therefore been remapped
    into extending the register set with hidden ax register and
    reusing that for a number of instructions that needed the
    prior temporary variable internally (e.g. div, mod). This
    allows for zero increase in stack space usage in the interpreter,
    and enables (restricted) generic use in rewrites otherwise as
    long as such a patchlet does not make use of these instructions.
    The sanitation mask is dynamic and relative to the offset the
    map value or stack pointer currently holds.
    
    There are various cases that need to be taken under consideration
    for the masking, e.g. such operation could look as follows:
    ptr += val or val += ptr or ptr -= val. Thus, the value to be
    sanitized could reside either in source or in destination
    register, and the limit is different depending on whether
    the ALU op is addition or subtraction and depending on the
    current known and bounded offset. The limit is derived as
    follows: limit := max_value_size - (smin_value + off). For
    subtraction: limit := umax_value + off. This holds because
    we do not allow any pointer arithmetic that would
    temporarily go out of bounds or would have an unknown
    value with mixed signed bounds where it is unclear at
    verification time whether the actual runtime value would
    be either negative or positive. For example, we have a
    derived map pointer value with constant offset and bounded
    one, so limit based on smin_value works because the verifier
    requires that statically analyzed arithmetic on the pointer
    must be in bounds, and thus it checks if resulting
    smin_value + off and umax_value + off is still within map
    value bounds at time of arithmetic in addition to time of
    access. Similarly, for the case of stack access we derive
    the limit as follows: MAX_BPF_STACK + off for subtraction
    and -off for the case of addition where off := ptr_reg->off +
    ptr_reg->var_off.value. Subtraction is a special case for
    the masking which can be in form of ptr += -val, ptr -= -val,
    or ptr -= val. In the first two cases where we know that
    the value is negative, we need to temporarily negate the
    value in order to do the sanitation on a positive value
    where we later swap the ALU op, and restore original source
    register if the value was in source.
    
    The sanitation of pointer arithmetic alone is still not fully
    sufficient as is, since a scenario like the following could
    happen ...
    
      PTR += 0x1000 (e.g. K-based imm)
      PTR -= BIG_NUMBER_WITH_SLOW_COMPARISON
      PTR += 0x1000
      PTR -= BIG_NUMBER_WITH_SLOW_COMPARISON
      [...]
    
    ... which under speculation could end up as ...
    
      PTR += 0x1000
      PTR -= 0 [ truncated by mitigation ]
      PTR += 0x1000
      PTR -= 0 [ truncated by mitigation ]
      [...]
    
    ... and therefore still access out of bounds. To prevent such
    case, the verifier is also analyzing safety for potential out
    of bounds access under speculative execution. Meaning, it is
    also simulating pointer access under truncation. We therefore
    "branch off" and push the current verification state after the
    ALU operation with known 0 to the verification stack for later
    analysis. Given the current path analysis succeeded it is
    likely that the one under speculation can be pruned. In any
    case, it is also subject to existing complexity limits and
    therefore anything beyond this point will be rejected. In
    terms of pruning, it needs to be ensured that the verification
    state from speculative execution simulation must never prune
    a non-speculative execution path, therefore, we mark verifier
    state accordingly at the time of push_stack(). If verifier
    detects out of bounds access under speculative execution from
    one of the possible paths that includes a truncation, it will
    reject such program.
    
    Given we mask every reg-based pointer arithmetic for
    unprivileged programs, we've been looking into how it could
    affect real-world programs in terms of size increase. As the
    majority of programs are targeted for privileged-only use
    case, we've unconditionally enabled masking (with its alu
    restrictions on top of it) for privileged programs for the
    sake of testing in order to check i) whether they get rejected
    in its current form, and ii) by how much the number of
    instructions and size will increase. We've tested this by
    using Katran, Cilium and test_l4lb from the kernel selftests.
    For Katran we've evaluated balancer_kern.o, Cilium bpf_lxc.o
    and an older test object bpf_lxc_opt_-DUNKNOWN.o and l4lb
    we've used test_l4lb.o as well as test_l4lb_noinline.o. We
    found that none of the programs got rejected by the verifier
    with this change, and that impact is rather minimal to none.
    balancer_kern.o had 13,904 bytes (1,738 insns) xlated and
    7,797 bytes JITed before and after the change. Most complex
    program in bpf_lxc.o had 30,544 bytes (3,817 insns) xlated
    and 18,538 bytes JITed before and after and none of the other
    tail call programs in bpf_lxc.o had any changes either. For
    the older bpf_lxc_opt_-DUNKNOWN.o object we found a small
    increase from 20,616 bytes (2,576 insns) and 12,536 bytes JITed
    before to 20,664 bytes (2,582 insns) and 12,558 bytes JITed
    after the change. Other programs from that object file had
    similar small increase. Both test_l4lb.o had no change and
    remained at 6,544 bytes (817 insns) xlated and 3,401 bytes
    JITed and for test_l4lb_noinline.o constant at 5,080 bytes
    (634 insns) xlated and 3,313 bytes JITed. This can be explained
    in that LLVM typically optimizes stack based pointer arithmetic
    by using K-based operations and that use of dynamic map access
    is not overly frequent. However, in future we may decide to
    optimize the algorithm further under known guarantees from
    branch and value speculation. Latter seems also unclear in
    terms of prediction heuristics that today's CPUs apply as well
    as whether there could be collisions in e.g. the predictor's
    Value History/Pattern Table for triggering out of bounds access,
    thus masking is performed unconditionally at this point but could
    be subject to relaxation later on. We were generally also
    brainstorming various other approaches for mitigation, but the
    blocker was always lack of available registers at runtime and/or
    overhead for runtime tracking of limits belonging to a specific
    pointer. Thus, we found this to be minimally intrusive under
    given constraints.
    
    With that in place, a simple example with sanitized access on
    unprivileged load at post-verification time looks as follows:
    
      # bpftool prog dump xlated id 282
      [...]
      28: (79) r1 = *(u64 *)(r7 +0)
      29: (79) r2 = *(u64 *)(r7 +8)
      30: (57) r1 &= 15
      31: (79) r3 = *(u64 *)(r0 +4608)
      32: (57) r3 &= 1
      33: (47) r3 |= 1
      34: (2d) if r2 > r3 goto pc+19
      35: (b4) (u32) r11 = (u32) 20479  |
      36: (1f) r11 -= r2                | Dynamic sanitation for pointer
      37: (4f) r11 |= r2                | arithmetic with registers
      38: (87) r11 = -r11               | containing bounded or known
      39: (c7) r11 s>>= 63              | scalars in order to prevent
      40: (5f) r11 &= r2                | out of bounds speculation.
      41: (0f) r4 += r11                |
      42: (71) r4 = *(u8 *)(r4 +0)
      43: (6f) r4 <<= r1
      [...]
    
    For the case where the scalar sits in the destination register
    as opposed to the source register, the following code is emitted
    for the above example:
    
      [...]
      16: (b4) (u32) r11 = (u32) 20479
      17: (1f) r11 -= r2
      18: (4f) r11 |= r2
      19: (87) r11 = -r11
      20: (c7) r11 s>>= 63
      21: (5f) r2 &= r11
      22: (0f) r2 += r0
      23: (61) r0 = *(u32 *)(r2 +0)
      [...]
    
    JIT blinding example with non-conflicting use of r10:
    
      [...]
       d5:  je     0x0000000000000106    _
       d7:  mov    0x0(%rax),%edi       |
       da:  mov    $0xf153246,%r10d     | Index load from map value and
       e0:  xor    $0xf153259,%r10      | (const blinded) mask with 0x1f.
       e7:  and    %r10,%rdi            |_
       ea:  mov    $0x2f,%r10d          |
       f0:  sub    %rdi,%r10            | Sanitized addition. Both use r10
       f3:  or     %rdi,%r10            | but do not interfere with each
       f6:  neg    %r10                 | other. (Neither do these instructions
       f9:  sar    $0x3f,%r10           | interfere with the use of ax as temp
       fd:  and    %r10,%rdi            | in interpreter.)
      100:  add    %rax,%rdi            |_
      103:  mov    0x0(%rdi),%eax
     [...]
    
    Tested that it fixes Jann's reproducer, and also checked that test_verifier
    and test_progs suite with interpreter, JIT and JIT with hardening enabled
    on x86-64 and arm64 runs successfully.
    
      [0] Speculose: Analyzing the Security Implications of Speculative
          Execution in CPUs, Giorgi Maisuradze and Christian Rossow,
          https://arxiv.org/pdf/1801.04084.pdf
    
      [1] A Systematic Evaluation of Transient Execution Attacks and
          Defenses, Claudio Canella, Jo Van Bulck, Michael Schwarz,
          Moritz Lipp, Benjamin von Berg, Philipp Ortner, Frank Piessens,
          Dmitry Evtyushkin, Daniel Gruss,
          https://arxiv.org/pdf/1811.05441.pdf
    
    Fixes: b2157399cc98 ("bpf: prevent out-of-bounds speculation")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 3f84f3e87704..27b74947cd2b 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -148,6 +148,7 @@ struct bpf_verifier_state {
 	/* call stack tracking */
 	struct bpf_func_state *frame[MAX_CALL_FRAMES];
 	u32 curframe;
+	bool speculative;
 };
 
 #define bpf_get_spilled_reg(slot, frame)				\
@@ -167,15 +168,24 @@ struct bpf_verifier_state_list {
 	struct bpf_verifier_state_list *next;
 };
 
+/* Possible states for alu_state member. */
+#define BPF_ALU_SANITIZE_SRC		1U
+#define BPF_ALU_SANITIZE_DST		2U
+#define BPF_ALU_NEG_VALUE		(1U << 2)
+#define BPF_ALU_SANITIZE		(BPF_ALU_SANITIZE_SRC | \
+					 BPF_ALU_SANITIZE_DST)
+
 struct bpf_insn_aux_data {
 	union {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 		unsigned long map_state;	/* pointer/poison value for maps */
 		s32 call_imm;			/* saved imm field of call insn */
+		u32 alu_limit;			/* limit for add/sub register with pointer */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */
+	u8 alu_state; /* used in combination with alu_limit */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit c08435ec7f2bc8f4109401f696fd55159b4b40cb
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jan 3 00:58:27 2019 +0100

    bpf: move {prev_,}insn_idx into verifier env
    
    Move prev_insn_idx and insn_idx from the do_check() function into
    the verifier environment, so they can be read inside the various
    helper functions for handling the instructions. It's easier to put
    this into the environment rather than changing all call-sites only
    to pass it along. insn_idx is useful in particular since this later
    on allows to hold state in env->insn_aux_data[env->insn_idx].
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c233efc106c6..3f84f3e87704 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -212,6 +212,8 @@ struct bpf_subprog_info {
  * one verifier_env per bpf_check() call
  */
 struct bpf_verifier_env {
+	u32 insn_idx;
+	u32 prev_insn_idx;
 	struct bpf_prog *prog;		/* eBPF program being verified */
 	const struct bpf_verifier_ops *ops;
 	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */

commit 9242b5f5615c823bfc1e9aea284617ff25a55f10
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Dec 13 11:42:34 2018 -0800

    bpf: add self-check logic to liveness analysis
    
    Introduce REG_LIVE_DONE to check the liveness propagation
    and prepare the states for merging.
    See algorithm description in clean_live_states().
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 548dcbdb7111..c233efc106c6 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -38,6 +38,7 @@ enum bpf_reg_liveness {
 	REG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */
 	REG_LIVE_READ, /* reg was read, so we're sensitive to initial value */
 	REG_LIVE_WRITTEN, /* reg was written first, screening off later reads */
+	REG_LIVE_DONE = 4, /* liveness won't be updating this register anymore */
 };
 
 struct bpf_reg_state {

commit d9762e84ede3eae9636f5dbbe0c8f0390d37e114
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Dec 13 10:41:48 2018 -0800

    bpf: verbose log bpf_line_info in verifier
    
    This patch adds bpf_line_info during the verifier's verbose.
    It can give error context for debug purpose.
    
    ~~~~~~~~~~
    Here is the verbose log for backedge:
            while (a) {
                    a += bpf_get_smp_processor_id();
                    bpf_trace_printk(fmt, sizeof(fmt), a);
            }
    
    ~> bpftool prog load ./test_loop.o /sys/fs/bpf/test_loop type tracepoint
    13: while (a) {
    3: a += bpf_get_smp_processor_id();
    back-edge from insn 13 to 3
    
    ~~~~~~~~~~
    Here is the verbose log for invalid pkt access:
    Modification to test_xdp_noinline.c:
    
            data = (void *)(long)xdp->data;
            data_end = (void *)(long)xdp->data_end;
    /*
            if (data + 4 > data_end)
                    return XDP_DROP;
    */
            *(u32 *)data = dst->dst;
    
    ~> bpftool prog load ./test_xdp_noinline.o /sys/fs/bpf/test_xdp_noinline type xdp
    ; data = (void *)(long)xdp->data;
    224: (79) r2 = *(u64 *)(r10 -112)
    225: (61) r2 = *(u32 *)(r2 +0)
    ; *(u32 *)data = dst->dst;
    226: (63) *(u32 *)(r2 +0) = r1
    invalid access to packet, off=0 size=4, R2(id=0,off=0,r=0)
    R2 offset is outside of the packet
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c736945be7c5..548dcbdb7111 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -224,6 +224,7 @@ struct bpf_verifier_env {
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
+	const struct bpf_line_info *prev_linfo;
 	struct bpf_verifier_log log;
 	struct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;

commit c454a46b5efd8eff8880e88ece2976e60a26bf35
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Dec 7 16:42:25 2018 -0800

    bpf: Add bpf_line_info support
    
    This patch adds bpf_line_info support.
    
    It accepts an array of bpf_line_info objects during BPF_PROG_LOAD.
    The "line_info", "line_info_cnt" and "line_info_rec_size" are added
    to the "union bpf_attr".  The "line_info_rec_size" makes
    bpf_line_info extensible in the future.
    
    The new "check_btf_line()" ensures the userspace line_info is valid
    for the kernel to use.
    
    When the verifier is translating/patching the bpf_prog (through
    "bpf_patch_insn_single()"), the line_infos' insn_off is also
    adjusted by the newly added "bpf_adj_linfo()".
    
    If the bpf_prog is jited, this patch also provides the jited addrs (in
    aux->jited_linfo) for the corresponding line_info.insn_off.
    "bpf_prog_fill_jited_linfo()" is added to fill the aux->jited_linfo.
    It is currently called by the x86 jit.  Other jits can also use
    "bpf_prog_fill_jited_linfo()" and it will be done in the followup patches.
    In the future, if it deemed necessary, a particular jit could also provide
    its own "bpf_prog_fill_jited_linfo()" implementation.
    
    A few "*line_info*" fields are added to the bpf_prog_info such
    that the user can get the xlated line_info back (i.e. the line_info
    with its insn_off reflecting the translated prog).  The jited_line_info
    is available if the prog is jited.  It is an array of __u64.
    If the prog is not jited, jited_line_info_cnt is 0.
    
    The verifier's verbose log with line_info will be done in
    a follow up patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 11f5df1092d9..c736945be7c5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -203,6 +203,7 @@ static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 
 struct bpf_subprog_info {
 	u32 start; /* insn idx of function entry point */
+	u32 linfo_idx; /* The idx to the main_prog->aux->linfo */
 	u16 stack_depth; /* max. stack depth used by this function */
 };
 

commit ba64e7d8525236aa56ab58ba3a3a71615c4ee289
Author: Yonghong Song <yhs@fb.com>
Date:   Sat Nov 24 23:20:44 2018 -0800

    bpf: btf: support proper non-jit func info
    
    Commit 838e96904ff3 ("bpf: Introduce bpf_func_info")
    added bpf func info support. The userspace is able
    to get better ksym's for bpf programs with jit, and
    is able to print out func prototypes.
    
    For a program containing func-to-func calls, the existing
    implementation returns user specified number of function
    calls and BTF types if jit is enabled. If the jit is not
    enabled, it only returns the type for the main function.
    
    This is undesirable. Interpreter may still be used
    and we should keep feature identical regardless of
    whether jit is enabled or not.
    This patch fixed this discrepancy.
    
    Fixes: 838e96904ff3 ("bpf: Introduce bpf_func_info")
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 204382f46fd8..11f5df1092d9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -204,7 +204,6 @@ static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 struct bpf_subprog_info {
 	u32 start; /* insn idx of function entry point */
 	u16 stack_depth; /* max. stack depth used by this function */
-	u32 type_id; /* btf type_id for this subprog */
 };
 
 /* single container for all structs

commit 838e96904ff3fc6c30e5ebbc611474669856e3c0
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Nov 19 15:29:11 2018 -0800

    bpf: Introduce bpf_func_info
    
    This patch added interface to load a program with the following
    additional information:
       . prog_btf_fd
       . func_info, func_info_rec_size and func_info_cnt
    where func_info will provide function range and type_id
    corresponding to each function.
    
    The func_info_rec_size is introduced in the UAPI to specify
    struct bpf_func_info size passed from user space. This
    intends to make bpf_func_info structure growable in the future.
    If the kernel gets a different bpf_func_info size from userspace,
    it will try to handle user request with part of bpf_func_info
    it can understand. In this patch, kernel can understand
      struct bpf_func_info {
           __u32   insn_offset;
           __u32   type_id;
      };
    If user passed a bpf func_info record size of 16 bytes, the
    kernel can still handle part of records with the above definition.
    
    If verifier agrees with function range provided by the user,
    the bpf_prog ksym for each function will use the func name
    provided in the type_id, which is supposed to provide better
    encoding as it is not limited by 16 bytes program name
    limitation and this is better for bpf program which contains
    multiple subprograms.
    
    The bpf_prog_info interface is also extended to
    return btf_id, func_info, func_info_rec_size and func_info_cnt
    to userspace, so userspace can print out the function prototype
    for each xlated function. The insn_offset in the returned
    func_info corresponds to the insn offset for xlated functions.
    With other jit related fields in bpf_prog_info, userspace can also
    print out function prototypes for each jited function.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 11f5df1092d9..204382f46fd8 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -204,6 +204,7 @@ static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 struct bpf_subprog_info {
 	u32 start; /* insn idx of function entry point */
 	u16 stack_depth; /* max. stack depth used by this function */
+	u32 type_id; /* btf type_id for this subprog */
 };
 
 /* single container for all structs

commit a40a26322a83d4a26a99ad2616cbd77394c19587
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Fri Nov 9 13:03:31 2018 +0000

    bpf: pass prog instead of env to bpf_prog_offload_verifier_prep()
    
    Function bpf_prog_offload_verifier_prep(), called from the kernel BPF
    verifier to run a driver-specific callback for preparing for the
    verification step for offloaded programs, takes a pointer to a struct
    bpf_verifier_env object. However, no driver callback needs the whole
    structure at this time: the two drivers supporting this, nfp and
    netdevsim, only need a pointer to the struct bpf_prog instance held by
    env.
    
    Update the callback accordingly, on kernel side and in these two
    drivers.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index d93e89761a8b..11f5df1092d9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -245,7 +245,7 @@ static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 	return cur_func(env)->regs;
 }
 
-int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);
+int bpf_prog_offload_verifier_prep(struct bpf_prog *prog);
 int bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,
 				 int insn_idx, int prev_insn_idx);
 int bpf_prog_offload_finalize(struct bpf_verifier_env *env);

commit 0962590e553331db2cc0aef2dc35c57f6300dbbe
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Nov 1 00:05:52 2018 +0100

    bpf: fix partial copy of map_ptr when dst is scalar
    
    ALU operations on pointers such as scalar_reg += map_value_ptr are
    handled in adjust_ptr_min_max_vals(). Problem is however that map_ptr
    and range in the register state share a union, so transferring state
    through dst_reg->range = ptr_reg->range is just buggy as any new
    map_ptr in the dst_reg is then truncated (or null) for subsequent
    checks. Fix this by adding a raw member and use it for copying state
    over to dst_reg.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Edward Cree <ecree@solarflare.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 9e8056ec20fa..d93e89761a8b 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -51,6 +51,9 @@ struct bpf_reg_state {
 		 *   PTR_TO_MAP_VALUE_OR_NULL
 		 */
 		struct bpf_map *map_ptr;
+
+		/* Max size from any of the above. */
+		unsigned long raw;
 	};
 	/* Fixed part of pointer offset, pointer types only */
 	s32 off;

commit c941ce9c282cc606e6517356fcc186a9da2b4ab9
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Sun Oct 7 12:56:47 2018 +0100

    bpf: add verifier callback to get stack usage info for offloaded progs
    
    In preparation for BPF-to-BPF calls in offloaded programs, add a new
    function attribute to the struct bpf_prog_offload_ops so that drivers
    supporting eBPF offload can hook at the end of program verification, and
    potentially extract information collected by the verifier.
    
    Implement a minimal callback (returning 0) in the drivers providing the
    structs, namely netdevsim and nfp.
    
    This will be useful in the nfp driver, in later commits, to extract the
    number of subprograms as well as the stack depth for those subprograms.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jiong Wang <jiong.wang@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7b6fd2ab3263..9e8056ec20fa 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -245,5 +245,6 @@ static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);
 int bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,
 				 int insn_idx, int prev_insn_idx);
+int bpf_prog_offload_finalize(struct bpf_verifier_env *env);
 
 #endif /* _LINUX_BPF_VERIFIER_H */

commit fd978bf7fd312581a7ca454a991f0ffb34c4204b
Author: Joe Stringer <joe@wand.net.nz>
Date:   Tue Oct 2 13:35:35 2018 -0700

    bpf: Add reference tracking to verifier
    
    Allow helper functions to acquire a reference and return it into a
    register. Specific pointer types such as the PTR_TO_SOCKET will
    implicitly represent such a reference. The verifier must ensure that
    these references are released exactly once in each path through the
    program.
    
    To achieve this, this commit assigns an id to the pointer and tracks it
    in the 'bpf_func_state', then when the function or program exits,
    verifies that all of the acquired references have been freed. When the
    pointer is passed to a function that frees the reference, it is removed
    from the 'bpf_func_state` and all existing copies of the pointer in
    registers are marked invalid.
    
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index a411363098a5..7b6fd2ab3263 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -104,6 +104,17 @@ struct bpf_stack_state {
 	u8 slot_type[BPF_REG_SIZE];
 };
 
+struct bpf_reference_state {
+	/* Track each reference created with a unique id, even if the same
+	 * instruction creates the reference multiple times (eg, via CALL).
+	 */
+	int id;
+	/* Instruction where the allocation of this reference occurred. This
+	 * is used purely to inform the user of a reference leak.
+	 */
+	int insn_idx;
+};
+
 /* state of the program:
  * type of all registers and stack info
  */
@@ -121,7 +132,9 @@ struct bpf_func_state {
 	 */
 	u32 subprogno;
 
-	/* should be second to last. See copy_func_state() */
+	/* The following fields should be last. See copy_func_state() */
+	int acquired_refs;
+	struct bpf_reference_state *refs;
 	int allocated_stack;
 	struct bpf_stack_state *stack;
 };
@@ -217,11 +230,16 @@ __printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,
 __printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,
 					   const char *fmt, ...);
 
-static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
+static inline struct bpf_func_state *cur_func(struct bpf_verifier_env *env)
 {
 	struct bpf_verifier_state *cur = env->cur_state;
 
-	return cur->frame[cur->curframe]->regs;
+	return cur->frame[cur->curframe];
+}
+
+static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
+{
+	return cur_func(env)->regs;
 }
 
 int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);

commit c64b7983288e636356f7f5f652de4813e1cfedac
Author: Joe Stringer <joe@wand.net.nz>
Date:   Tue Oct 2 13:35:33 2018 -0700

    bpf: Add PTR_TO_SOCKET verifier type
    
    Teach the verifier a little bit about a new type of pointer, a
    PTR_TO_SOCKET. This pointer type is accessed from BPF through the
    'struct bpf_sock' structure.
    
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index d0e7f97e8b60..a411363098a5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -58,6 +58,8 @@ struct bpf_reg_state {
 	 * offset, so they can share range knowledge.
 	 * For PTR_TO_MAP_VALUE_OR_NULL this is used to share which map value we
 	 * came from, when one is tested for != NULL.
+	 * For PTR_TO_SOCKET this is used to share which pointers retain the
+	 * same reference to the socket, to determine proper reference freeing.
 	 */
 	u32 id;
 	/* For scalar types (SCALAR_VALUE), this represents our knowledge of

commit f3709f69b7c5cba6323cc03c29b64293b93be817
Author: Joe Stringer <joe@wand.net.nz>
Date:   Tue Oct 2 13:35:29 2018 -0700

    bpf: Add iterator for spilled registers
    
    Add this iterator for spilled registers, it concentrates the details of
    how to get the current frame's spilled registers into a single macro
    while clarifying the intention of the code which is calling the macro.
    
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index b42b60a83e19..d0e7f97e8b60 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -131,6 +131,17 @@ struct bpf_verifier_state {
 	u32 curframe;
 };
 
+#define bpf_get_spilled_reg(slot, frame)				\
+	(((slot < frame->allocated_stack / BPF_REG_SIZE) &&		\
+	  (frame->stack[slot].slot_type[0] == STACK_SPILL))		\
+	 ? &frame->stack[slot].spilled_ptr : NULL)
+
+/* Iterate over 'frame', setting 'reg' to either NULL or a spilled register. */
+#define bpf_for_each_spilled_reg(iter, frame, reg)			\
+	for (iter = 0, reg = bpf_get_spilled_reg(iter, frame);		\
+	     iter < frame->allocated_stack / BPF_REG_SIZE;		\
+	     iter++, reg = bpf_get_spilled_reg(iter, frame))
+
 /* linked list of verifier states used to prune search */
 struct bpf_verifier_state_list {
 	struct bpf_verifier_state state;

commit 679c782de14bd48c19dd74cd1af20a2bc05dd936
Author: Edward Cree <ecree@solarflare.com>
Date:   Wed Aug 22 20:02:19 2018 +0100

    bpf/verifier: per-register parent pointers
    
    By giving each register its own liveness chain, we elide the skip_callee()
     logic.  Instead, each register's parent is the state it inherits from;
     both check_func_call() and prepare_func_exit() automatically connect
     reg states to the correct chain since when they copy the reg state across
     (r1-r5 into the callee as args, and r0 out as the return value) they also
     copy the parent pointer.
    
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 38b04f559ad3..b42b60a83e19 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -41,6 +41,7 @@ enum bpf_reg_liveness {
 };
 
 struct bpf_reg_state {
+	/* Ordering of fields matters.  See states_equal() */
 	enum bpf_reg_type type;
 	union {
 		/* valid when type == PTR_TO_PACKET */
@@ -59,7 +60,6 @@ struct bpf_reg_state {
 	 * came from, when one is tested for != NULL.
 	 */
 	u32 id;
-	/* Ordering of fields matters.  See states_equal() */
 	/* For scalar types (SCALAR_VALUE), this represents our knowledge of
 	 * the actual value.
 	 * For pointer types, this represents the variable part of the offset
@@ -76,15 +76,15 @@ struct bpf_reg_state {
 	s64 smax_value; /* maximum possible (s64)value */
 	u64 umin_value; /* minimum possible (u64)value */
 	u64 umax_value; /* maximum possible (u64)value */
+	/* parentage chain for liveness checking */
+	struct bpf_reg_state *parent;
 	/* Inside the callee two registers can be both PTR_TO_STACK like
 	 * R1=fp-8 and R2=fp-8, but one of them points to this function stack
 	 * while another to the caller's stack. To differentiate them 'frameno'
 	 * is used which is an index in bpf_verifier_state->frame[] array
 	 * pointing to bpf_func_state.
-	 * This field must be second to last, for states_equal() reasons.
 	 */
 	u32 frameno;
-	/* This field must be last, for states_equal() reasons. */
 	enum bpf_reg_liveness live;
 };
 
@@ -107,7 +107,6 @@ struct bpf_stack_state {
  */
 struct bpf_func_state {
 	struct bpf_reg_state regs[MAX_BPF_REG];
-	struct bpf_verifier_state *parent;
 	/* index of call instruction that called into this func */
 	int callsite;
 	/* stack frame number of this function state from pov of
@@ -129,7 +128,6 @@ struct bpf_func_state {
 struct bpf_verifier_state {
 	/* call stack tracking */
 	struct bpf_func_state *frame[MAX_CALL_FRAMES];
-	struct bpf_verifier_state *parent;
 	u32 curframe;
 };
 

commit 5b79c2af667c0e2684f2a6dbf6439074b78f490c
Merge: e52cde717093 bc2dbc5420e8
Author: David S. Miller <davem@davemloft.net>
Date:   Sat May 26 19:46:15 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Lots of easy overlapping changes in the confict
    resolutions here.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 03250e1028057173b212341015d5fbf53327042c
Merge: 62d18ecfa641 eb110410b9f6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 25 19:54:42 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Pull networking fixes from David Miller:
     "Let's begin the holiday weekend with some networking fixes:
    
       1) Whoops need to restrict cfg80211 wiphy names even more to 64
          bytes. From Eric Biggers.
    
       2) Fix flags being ignored when using kernel_connect() with SCTP,
          from Xin Long.
    
       3) Use after free in DCCP, from Alexey Kodanev.
    
       4) Need to check rhltable_init() return value in ipmr code, from Eric
          Dumazet.
    
       5) XDP handling fixes in virtio_net from Jason Wang.
    
       6) Missing RTA_TABLE in rtm_ipv4_policy[], from Roopa Prabhu.
    
       7) Need to use IRQ disabling spinlocks in mlx4_qp_lookup(), from Jack
          Morgenstein.
    
       8) Prevent out-of-bounds speculation using indexes in BPF, from
          Daniel Borkmann.
    
       9) Fix regression added by AF_PACKET link layer cure, from Willem de
          Bruijn.
    
      10) Correct ENIC dma mask, from Govindarajulu Varadarajan.
    
      11) Missing config options for PMTU tests, from Stefano Brivio"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net: (48 commits)
      ibmvnic: Fix partial success login retries
      selftests/net: Add missing config options for PMTU tests
      mlx4_core: allocate ICM memory in page size chunks
      enic: set DMA mask to 47 bit
      ppp: remove the PPPIOCDETACH ioctl
      ipv4: remove warning in ip_recv_error
      net : sched: cls_api: deal with egdev path only if needed
      vhost: synchronize IOTLB message with dev cleanup
      packet: fix reserve calculation
      net/mlx5: IPSec, Fix a race between concurrent sandbox QP commands
      net/mlx5e: When RXFCS is set, add FCS data into checksum calculation
      bpf: properly enforce index mask to prevent out-of-bounds speculation
      net/mlx4: Fix irq-unsafe spinlock usage
      net: phy: broadcom: Fix bcm_write_exp()
      net: phy: broadcom: Fix auxiliary control register reads
      net: ipv4: add missing RTA_TABLE to rtm_ipv4_policy
      net/mlx4: fix spelling mistake: "Inrerface" -> "Interface" and rephrase message
      ibmvnic: Only do H_EOI for mobility events
      tuntap: correctly set SOCKWQ_ASYNC_NOSPACE
      virtio-net: fix leaking page for gso packet during mergeable XDP
      ...

commit c93552c443ebc63b14e26e46d2e76941c88e0d71
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu May 24 02:32:53 2018 +0200

    bpf: properly enforce index mask to prevent out-of-bounds speculation
    
    While reviewing the verifier code, I recently noticed that the
    following two program variants in relation to tail calls can be
    loaded.
    
    Variant 1:
    
      # bpftool p d x i 15
        0: (15) if r1 == 0x0 goto pc+3
        1: (18) r2 = map[id:5]
        3: (05) goto pc+2
        4: (18) r2 = map[id:6]
        6: (b7) r3 = 7
        7: (35) if r3 >= 0xa0 goto pc+2
        8: (54) (u32) r3 &= (u32) 255
        9: (85) call bpf_tail_call#12
       10: (b7) r0 = 1
       11: (95) exit
    
      # bpftool m s i 5
        5: prog_array  flags 0x0
            key 4B  value 4B  max_entries 4  memlock 4096B
      # bpftool m s i 6
        6: prog_array  flags 0x0
            key 4B  value 4B  max_entries 160  memlock 4096B
    
    Variant 2:
    
      # bpftool p d x i 20
        0: (15) if r1 == 0x0 goto pc+3
        1: (18) r2 = map[id:8]
        3: (05) goto pc+2
        4: (18) r2 = map[id:7]
        6: (b7) r3 = 7
        7: (35) if r3 >= 0x4 goto pc+2
        8: (54) (u32) r3 &= (u32) 3
        9: (85) call bpf_tail_call#12
       10: (b7) r0 = 1
       11: (95) exit
    
      # bpftool m s i 8
        8: prog_array  flags 0x0
            key 4B  value 4B  max_entries 160  memlock 4096B
      # bpftool m s i 7
        7: prog_array  flags 0x0
            key 4B  value 4B  max_entries 4  memlock 4096B
    
    In both cases the index masking inserted by the verifier in order
    to control out of bounds speculation from a CPU via b2157399cc98
    ("bpf: prevent out-of-bounds speculation") seems to be incorrect
    in what it is enforcing. In the 1st variant, the mask is applied
    from the map with the significantly larger number of entries where
    we would allow to a certain degree out of bounds speculation for
    the smaller map, and in the 2nd variant where the mask is applied
    from the map with the smaller number of entries, we get buggy
    behavior since we truncate the index of the larger map.
    
    The original intent from commit b2157399cc98 is to reject such
    occasions where two or more different tail call maps are used
    in the same tail call helper invocation. However, the check on
    the BPF_MAP_PTR_POISON is never hit since we never poisoned the
    saved pointer in the first place! We do this explicitly for map
    lookups but in case of tail calls we basically used the tail
    call map in insn_aux_data that was processed in the most recent
    path which the verifier walked. Thus any prior path that stored
    a pointer in insn_aux_data at the helper location was always
    overridden.
    
    Fix it by moving the map pointer poison logic into a small helper
    that covers both BPF helpers with the same logic. After that in
    fixup_bpf_calls() the poison check is then hit for tail calls
    and the program rejected. Latter only happens in unprivileged
    case since this is the *only* occasion where a rewrite needs to
    happen, and where such rewrite is specific to the map (max_entries,
    index_mask). In the privileged case the rewrite is generic for
    the insn->imm / insn->code update so multiple maps from different
    paths can be handled just fine since all the remaining logic
    happens in the instruction processing itself. This is similar
    to the case of map lookups: in case there is a collision of
    maps in fixup_bpf_calls() we must skip the inlined rewrite since
    this will turn the generic instruction sequence into a non-
    generic one. Thus the patch_call_imm will simply update the
    insn->imm location where the bpf_map_lookup_elem() will later
    take care of the dispatch. Given we need this 'poison' state
    as a check, the information of whether a map is an unpriv_array
    gets lost, so enforcing it prior to that needs an additional
    state. In general this check is needed since there are some
    complex and tail call intensive BPF programs out there where
    LLVM tends to generate such code occasionally. We therefore
    convert the map_ptr rather into map_state to store all this
    w/o extra memory overhead, and the bit whether one of the maps
    involved in the collision was from an unpriv_array thus needs
    to be retained as well there.
    
    Fixes: b2157399cc98 ("bpf: prevent out-of-bounds speculation")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7e61c395fddf..52fb077d3c45 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -142,7 +142,7 @@ struct bpf_verifier_state_list {
 struct bpf_insn_aux_data {
 	union {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
-		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
+		unsigned long map_state;	/* pointer/poison value for maps */
 		s32 call_imm;			/* saved imm field of call insn */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */

commit af86ca4e3088fe5eacf2f7e58c01fa68ca067672
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue May 15 09:27:05 2018 -0700

    bpf: Prevent memory disambiguation attack
    
    Detect code patterns where malicious 'speculative store bypass' can be used
    and sanitize such patterns.
    
     39: (bf) r3 = r10
     40: (07) r3 += -216
     41: (79) r8 = *(u64 *)(r7 +0)   // slow read
     42: (7a) *(u64 *)(r10 -72) = 0  // verifier inserts this instruction
     43: (7b) *(u64 *)(r8 +0) = r3   // this store becomes slow due to r8
     44: (79) r1 = *(u64 *)(r6 +0)   // cpu speculatively executes this load
     45: (71) r2 = *(u8 *)(r1 +0)    // speculatively arbitrary 'load byte'
                                     // is now sanitized
    
    Above code after x86 JIT becomes:
     e5: mov    %rbp,%rdx
     e8: add    $0xffffffffffffff28,%rdx
     ef: mov    0x0(%r13),%r14
     f3: movq   $0x0,-0x48(%rbp)
     fb: mov    %rdx,0x0(%r14)
     ff: mov    0x0(%rbx),%rdi
    103: movzbq 0x0(%rdi),%rsi
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7e61c395fddf..65cfc2f59db9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -146,6 +146,7 @@ struct bpf_insn_aux_data {
 		s32 call_imm;			/* saved imm field of call insn */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
+	int sanitize_stack_off; /* stack slot to be cleared */
 	bool seen; /* this insn was processed by the verifier */
 };
 

commit be2d04d11fd33bd46622f94619aae1596d9f9303
Author: Mathieu Malaterre <malat@debian.org>
Date:   Wed May 16 22:27:41 2018 +0200

    bpf: add __printf verification to bpf_verifier_vlog
    
    __printf is useful to verify format and arguments. ‘bpf_verifier_vlog’
    function is used twice in verifier.c in both cases the caller function
    already uses the __printf gcc attribute.
    
    Remove the following warning, triggered with W=1:
    
      kernel/bpf/verifier.c:176:2: warning: function might be possible candidate for ‘gnu_printf’ format attribute [-Wsuggest-attribute=format]
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 8f70dc181e23..c286813deaeb 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -200,8 +200,8 @@ struct bpf_verifier_env {
 	u32 subprog_cnt;
 };
 
-void bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,
-		       va_list args);
+__printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,
+				      const char *fmt, va_list args);
 __printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,
 					   const char *fmt, ...);
 

commit 9c8105bd4402236b1bb0f8f10709c5cec1440a0c
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Wed May 2 16:17:18 2018 -0400

    bpf: centre subprog information fields
    
    It is better to centre all subprog information fields into one structure.
    This structure could later serve as function node in call graph.
    
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index f655b926e432..8f70dc181e23 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -173,6 +173,11 @@ static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
 
 #define BPF_MAX_SUBPROGS 256
 
+struct bpf_subprog_info {
+	u32 start; /* insn idx of function entry point */
+	u16 stack_depth; /* max. stack depth used by this function */
+};
+
 /* single container for all structs
  * one verifier_env per bpf_check() call
  */
@@ -191,9 +196,7 @@ struct bpf_verifier_env {
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 	struct bpf_verifier_log log;
-	u32 subprog_starts[BPF_MAX_SUBPROGS + 1];
-	/* computes the stack depth of each bpf function */
-	u16 subprog_stack_depth[BPF_MAX_SUBPROGS + 1];
+	struct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;
 };
 

commit f910cefa32b6cdabc96b126bcfc46d8940b1dc45
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Wed May 2 16:17:17 2018 -0400

    bpf: unify main prog and subprog
    
    Currently, verifier treat main prog and subprog differently. All subprogs
    detected are kept in env->subprog_starts while main prog is not kept there.
    Instead, main prog is implicitly defined as the prog start at 0.
    
    There is actually no difference between main prog and subprog, it is better
    to unify them, and register all progs detected into env->subprog_starts.
    
    This could also help simplifying some code logic.
    
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7e61c395fddf..f655b926e432 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -191,7 +191,7 @@ struct bpf_verifier_env {
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 	struct bpf_verifier_log log;
-	u32 subprog_starts[BPF_MAX_SUBPROGS];
+	u32 subprog_starts[BPF_MAX_SUBPROGS + 1];
 	/* computes the stack depth of each bpf function */
 	u16 subprog_stack_depth[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;

commit 77d2e05abd45886dcad2b632c738cf46b9f7c19e
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Sat Mar 24 11:44:23 2018 -0700

    bpf: Add bpf_verifier_vlog() and bpf_verifier_log_needed()
    
    The BTF (BPF Type Format) verifier needs to reuse the current
    BPF verifier log.  Hence, it requires the following changes:
    
    (1) Expose log_write() in verifier.c for other users.
        Its name is renamed to bpf_verifier_vlog().
    
    (2) The BTF verifier also needs to check
    'log->level && log->ubuf && !bpf_verifier_log_full(log);'
    independently outside of the current log_write().  It is
    because the BTF verifier will do one-check before
    making multiple calls to btf_verifier_vlog to log
    the details of a type.
    
    Hence, this check is also re-factored to a new function
    bpf_verifier_log_needed().  Since it is re-factored,
    we can check it before va_start() in the current
    bpf_verifier_log_write() and verbose().
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c30668414b22..7e61c395fddf 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -166,6 +166,11 @@ static inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)
 	return log->len_used >= log->len_total - 1;
 }
 
+static inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)
+{
+	return log->level && log->ubuf && !bpf_verifier_log_full(log);
+}
+
 #define BPF_MAX_SUBPROGS 256
 
 /* single container for all structs
@@ -192,6 +197,8 @@ struct bpf_verifier_env {
 	u32 subprog_cnt;
 };
 
+void bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,
+		       va_list args);
 __printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,
 					   const char *fmt, ...);
 

commit b9193c1b61ddb97da4713155b0d580e41fb544ac
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Sat Mar 24 11:44:22 2018 -0700

    bpf: Rename bpf_verifer_log
    
    bpf_verifer_log =>
    bpf_verifier_log
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 6b66cd1aa0b9..c30668414b22 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -153,7 +153,7 @@ struct bpf_insn_aux_data {
 
 #define BPF_VERIFIER_TMP_LOG_SIZE	1024
 
-struct bpf_verifer_log {
+struct bpf_verifier_log {
 	u32 level;
 	char kbuf[BPF_VERIFIER_TMP_LOG_SIZE];
 	char __user *ubuf;
@@ -161,7 +161,7 @@ struct bpf_verifer_log {
 	u32 len_total;
 };
 
-static inline bool bpf_verifier_log_full(const struct bpf_verifer_log *log)
+static inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)
 {
 	return log->len_used >= log->len_total - 1;
 }
@@ -185,7 +185,7 @@ struct bpf_verifier_env {
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
-	struct bpf_verifer_log log;
+	struct bpf_verifier_log log;
 	u32 subprog_starts[BPF_MAX_SUBPROGS];
 	/* computes the stack depth of each bpf function */
 	u16 subprog_stack_depth[BPF_MAX_SUBPROGS + 1];

commit 430e68d10baf55e4c40d4dd1de8201c1caf5dddd
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Wed Jan 10 12:26:06 2018 +0000

    bpf: export function to write into verifier log buffer
    
    Rename the BPF verifier `verbose()` to `bpf_verifier_log_write()` and
    export it, so that other components (in particular, drivers for BPF
    offload) can reuse the user buffer log to dump error messages at
    verification time.
    
    Renaming `verbose()` was necessary in order to avoid a name so generic
    to be exported to the global namespace. However to prevent too much pain
    for backports, the calls to `verbose()` in the kernel BPF verifier were
    not changed. Instead, use function aliasing to make `verbose` point to
    `bpf_verifier_log_write`. Another solution could consist in making a
    wrapper around `verbose()`, but since it is a variadic function, I don't
    see a clean way without creating two identical wrappers, one for the
    verifier and one to export.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 2feb218c001d..6b66cd1aa0b9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -192,6 +192,9 @@ struct bpf_verifier_env {
 	u32 subprog_cnt;
 };
 
+__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,
+					   const char *fmt, ...);
+
 static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 {
 	struct bpf_verifier_state *cur = env->cur_state;

commit cae1927c0b4a93ae15de824faca1f6f611a44fcd
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Dec 27 18:39:05 2017 -0800

    bpf: offload: allow netdev to disappear while verifier is running
    
    To allow verifier instruction callbacks without any extra locking
    NETDEV_UNREGISTER notification would wait on a waitqueue for verifier
    to finish.  This design decision was made when rtnl lock was providing
    all the locking.  Use the read/write lock instead and remove the
    workqueue.
    
    Verifier will now call into the offload code, so dev_ops are moved
    to offload structure.  Since verifier calls are all under
    bpf_prog_is_dev_bound() we no longer need static inline implementations
    to please builds with CONFIG_NET=n.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 883a35d50cd5..2feb218c001d 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -166,12 +166,6 @@ static inline bool bpf_verifier_log_full(const struct bpf_verifer_log *log)
 	return log->len_used >= log->len_total - 1;
 }
 
-struct bpf_verifier_env;
-struct bpf_ext_analyzer_ops {
-	int (*insn_hook)(struct bpf_verifier_env *env,
-			 int insn_idx, int prev_insn_idx);
-};
-
 #define BPF_MAX_SUBPROGS 256
 
 /* single container for all structs
@@ -185,7 +179,6 @@ struct bpf_verifier_env {
 	bool strict_alignment;		/* perform strict pointer alignment checks */
 	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
-	const struct bpf_ext_analyzer_ops *dev_ops; /* device analyzer ops */
 	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
 	u32 used_map_cnt;		/* number of used maps */
 	u32 id_gen;			/* used to generate unique reg IDs */
@@ -206,13 +199,8 @@ static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 	return cur->frame[cur->curframe]->regs;
 }
 
-#if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)
 int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);
-#else
-static inline int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env)
-{
-	return -EOPNOTSUPP;
-}
-#endif
+int bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,
+				 int insn_idx, int prev_insn_idx);
 
 #endif /* _LINUX_BPF_VERIFIER_H */

commit fcffe2edbd390cad499b27d20512ef000d7ecf54
Merge: 4f83435ad777 624588d9d6cc
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Dec 27 20:40:32 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2017-12-28
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Fix incorrect state pruning related to recognition of zero initialized
       stack slots, where stacksafe exploration would mistakenly return a
       positive pruning verdict too early ignoring other slots, from Gianluca.
    
    2) Various BPF to BPF calls related follow-up fixes. Fix an off-by-one
       in maximum call depth check, and rework maximum stack depth tracking
       logic to fix a bypass of the total stack size check reported by Jann.
       Also fix a bug in arm64 JIT where prog->jited_len was uninitialized.
       Addition of various test cases to BPF selftests, from Alexei.
    
    3) Addition of a BPF selftest to test_verifier that is related to BPF to
       BPF calls which demonstrates a late caller stack size increase and
       thus out of bounds access. Fixed above in 2). Test case from Jann.
    
    4) Addition of correlating BPF helper calls, BPF to BPF calls as well
       as BPF maps to bpftool xlated dump in order to allow for better
       BPF program introspection and debugging, from Daniel.
    
    5) Fixing several bugs in BPF to BPF calls kallsyms handling in order
       to get it actually to work for subprogs, from Daniel.
    
    6) Extending sparc64 JIT support for BPF to BPF calls and fix a couple
       of build errors for libbpf on sparc64, from David.
    
    7) Allow narrower context access for BPF dev cgroup typed programs in
       order to adapt to LLVM code generation. Also adjust memlock rlimit
       in the test_dev_cgroup BPF selftest, from Yonghong.
    
    8) Add netdevsim Kconfig entry to BPF selftests since test_offload.py
       relies on netdevsim device being available, from Jakub.
    
    9) Reduce scope of xdp_do_generic_redirect_map() to being static,
       from Xiongwei.
    
    10) Minor cleanups and spelling fixes in BPF verifier, from Colin.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 70a87ffea8acc390ae315fa1930e849f656bdb88
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Dec 25 13:15:40 2017 -0800

    bpf: fix maximum stack depth tracking logic
    
    Instead of computing max stack depth for current call chain
    during the main verifier pass track stack depth of each
    function independently and after do_check() is done do
    another pass over all instructions analyzing depth
    of all possible call stacks.
    
    Fixes: f4d7e40a5b71 ("bpf: introduce function calls (verification)")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index aaac589e490c..94a02ceb1246 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -194,6 +194,7 @@ struct bpf_verifier_env {
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 	struct bpf_verifer_log log;
 	u32 subprog_starts[BPF_MAX_SUBPROGS];
+	/* computes the stack depth of each bpf function */
 	u16 subprog_stack_depth[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;
 };

commit fba961ab29e5ffb055592442808bb0f7962e05da
Merge: 0a80f0c26bf5 ead68f216110
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 22 11:16:31 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Lots of overlapping changes.  Also on the net-next side
    the XDP state management is handled more in the generic
    layers so undo the 'net' nfp fix which isn't applicable
    in net-next.
    
    Include a necessary change by Jakub Kicinski, with log message:
    
    ====================
    cls_bpf no longer takes care of offload tracking.  Make sure
    netdevsim performs necessary checks.  This fixes a warning
    caused by TC trying to remove a filter it has not added.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit bb7f0f989ca7de1153bd128a40a71709e339fa03
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Dec 18 20:12:00 2017 -0800

    bpf: fix integer overflows
    
    There were various issues related to the limited size of integers used in
    the verifier:
     - `off + size` overflow in __check_map_access()
     - `off + reg->off` overflow in check_mem_access()
     - `off + reg->var_off.value` overflow or 32-bit truncation of
       `reg->var_off.value` in check_mem_access()
     - 32-bit truncation in check_stack_boundary()
    
    Make sure that any integer math cannot overflow by not allowing
    pointer math with large values.
    
    Also reduce the scope of "scalar op scalar" tracking.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c561b986bab0..1632bb13ad8a 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -15,11 +15,11 @@
  * In practice this is far bigger than any realistic pointer offset; this limit
  * ensures that umax_value + (int)off + (int)size cannot overflow a u64.
  */
-#define BPF_MAX_VAR_OFF	(1ULL << 31)
+#define BPF_MAX_VAR_OFF	(1 << 29)
 /* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures
  * that converting umax_value to int cannot overflow.
  */
-#define BPF_MAX_VAR_SIZ	INT_MAX
+#define BPF_MAX_VAR_SIZ	(1 << 29)
 
 /* Liveness marks, used for registers and spilled-regs (in stack slots).
  * Read marks propagate upwards until they find a write mark; they record that

commit 1c2a088a6626d4f51d2f2c97b0cbedbfbf3637f6
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:15 2017 -0800

    bpf: x64: add JIT support for multi-function programs
    
    Typical JIT does several passes over bpf instructions to
    compute total size and relative offsets of jumps and calls.
    With multitple bpf functions calling each other all relative calls
    will have invalid offsets intially therefore we need to additional
    last pass over the program to emit calls with correct offsets.
    For example in case of three bpf functions:
    main:
      call foo
      call bpf_map_lookup
      exit
    foo:
      call bar
      exit
    bar:
      exit
    
    We will call bpf_int_jit_compile() indepedently for main(), foo() and bar()
    x64 JIT typically does 4-5 passes to converge.
    After these initial passes the image for these 3 functions
    will be good except call targets, since start addresses of
    foo() and bar() are unknown when we were JITing main()
    (note that call bpf_map_lookup will be resolved properly
    during initial passes).
    Once start addresses of 3 functions are known we patch
    call_insn->imm to point to right functions and call
    bpf_int_jit_compile() again which needs only one pass.
    Additional safety checks are done to make sure this
    last pass doesn't produce image that is larger or smaller
    than previous pass.
    
    When constant blinding is on it's applied to all functions
    at the first pass, since doing it once again at the last
    pass can change size of the JITed code.
    
    Tested on x64 and arm64 hw with JIT on/off, blinding on/off.
    x64 jits bpf-to-bpf calls correctly while arm64 falls back to interpreter.
    All other JITs that support normal BPF_CALL will behave the same way
    since bpf-to-bpf call is equivalent to bpf-to-kernel call from
    JITs point of view.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 585d4e17ea88..aaac589e490c 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -143,6 +143,7 @@ struct bpf_insn_aux_data {
 	union {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
+		s32 call_imm;			/* saved imm field of call insn */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 	bool seen; /* this insn was processed by the verifier */

commit cc2b14d51053eb055c06f45e1a5cdbfcf2b79e94
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:08 2017 -0800

    bpf: teach verifier to recognize zero initialized stack
    
    programs with function calls are often passing various
    pointers via stack. When all calls are inlined llvm
    flattens stack accesses and optimizes away extra branches.
    When functions are not inlined it becomes the job of
    the verifier to recognize zero initialized stack to avoid
    exploring paths that program will not take.
    The following program would fail otherwise:
    
    ptr = &buffer_on_stack;
    *ptr = 0;
    ...
    func_call(.., ptr, ...) {
      if (..)
        *ptr = bpf_map_lookup();
    }
    ...
    if (*ptr != 0) {
      // Access (*ptr)->field is valid.
      // Without stack_zero tracking such (*ptr)->field access
      // will be rejected
    }
    
    since stack slots are no longer uniform invalid | spill | misc
    add liveness marking to all slots, but do it in 8 byte chunks.
    So if nothing was read or written in [fp-16, fp-9] range
    it will be marked as LIVE_NONE.
    If any byte in that range was read, it will be marked LIVE_READ
    and stacksafe() check will perform byte-by-byte verification.
    If all bytes in the range were written the slot will be
    marked as LIVE_WRITTEN.
    This significantly speeds up state equality comparison
    and reduces total number of states processed.
    
                        before   after
    bpf_lb-DLB_L3.o       2051    2003
    bpf_lb-DLB_L4.o       3287    3164
    bpf_lb-DUNKNOWN.o     1080    1080
    bpf_lxc-DDROP_ALL.o   24980   12361
    bpf_lxc-DUNKNOWN.o    34308   16605
    bpf_netdev.o          15404   10962
    bpf_overlay.o         7191    6679
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 1f23408024ee..585d4e17ea88 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -91,7 +91,8 @@ struct bpf_reg_state {
 enum bpf_stack_slot_type {
 	STACK_INVALID,    /* nothing was stored in this stack slot */
 	STACK_SPILL,      /* register spilled into stack */
-	STACK_MISC	  /* BPF program wrote some data into this slot */
+	STACK_MISC,	  /* BPF program wrote some data into this slot */
+	STACK_ZERO,	  /* BPF program wrote constant zero */
 };
 
 #define BPF_REG_SIZE 8	/* size of eBPF register in bytes */

commit f4d7e40a5b7157e1329c3c5b10f60d8289fc2941
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:06 2017 -0800

    bpf: introduce function calls (verification)
    
    Allow arbitrary function calls from bpf function to another bpf function.
    
    To recognize such set of bpf functions the verifier does:
    1. runs control flow analysis to detect function boundaries
    2. proceeds with verification of all functions starting from main(root) function
    It recognizes that the stack of the caller can be accessed by the callee
    (if the caller passed a pointer to its stack to the callee) and the callee
    can store map_value and other pointers into the stack of the caller.
    3. keeps track of the stack_depth of each function to make sure that total
    stack depth is still less than 512 bytes
    4. disallows pointers to the callee stack to be stored into the caller stack,
    since they will be invalid as soon as the callee returns
    5. to reuse all of the existing state_pruning logic each function call
    is considered to be independent call from the verifier point of view.
    The verifier pretends to inline all function calls it sees are being called.
    It stores the callsite instruction index as part of the state to make sure
    that two calls to the same callee from two different places in the caller
    will be different from state pruning point of view
    6. more safety checks are added to liveness analysis
    
    Implementation details:
    . struct bpf_verifier_state is now consists of all stack frames that
      led to this function
    . struct bpf_func_state represent one stack frame. It consists of
      registers in the given frame and its stack
    . propagate_liveness() logic had a premature optimization where
      mark_reg_read() and mark_stack_slot_read() were manually inlined
      with loop iterating over parents for each register or stack slot.
      Undo this optimization to reuse more complex mark_*_read() logic
    . skip_callee() logic is not necessary from safety point of view,
      but without it mark_*_read() markings become too conservative,
      since after returning from the funciton call a read of r6-r9
      will incorrectly propagate the read marks into callee causing
      inefficient pruning later
    . mark_*_read() logic is now aware of control flow which makes it
      more complex. In the future the plan is to rewrite liveness
      to be hierarchical. So that liveness can be done within
      basic block only and control flow will be responsible for
      propagation of liveness information along cfg and between calls.
    . tail_calls and ld_abs insns are not allowed in the programs with
      bpf-to-bpf calls
    . returning stack pointers to the caller or storing them into stack
      frame of the caller is not allowed
    
    Testing:
    . no difference in cilium processed_insn numbers
    . large number of tests follows in next patches
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 91a583bb3fa7..1f23408024ee 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -76,6 +76,14 @@ struct bpf_reg_state {
 	s64 smax_value; /* maximum possible (s64)value */
 	u64 umin_value; /* minimum possible (u64)value */
 	u64 umax_value; /* maximum possible (u64)value */
+	/* Inside the callee two registers can be both PTR_TO_STACK like
+	 * R1=fp-8 and R2=fp-8, but one of them points to this function stack
+	 * while another to the caller's stack. To differentiate them 'frameno'
+	 * is used which is an index in bpf_verifier_state->frame[] array
+	 * pointing to bpf_func_state.
+	 * This field must be second to last, for states_equal() reasons.
+	 */
+	u32 frameno;
 	/* This field must be last, for states_equal() reasons. */
 	enum bpf_reg_liveness live;
 };
@@ -96,13 +104,34 @@ struct bpf_stack_state {
 /* state of the program:
  * type of all registers and stack info
  */
-struct bpf_verifier_state {
+struct bpf_func_state {
 	struct bpf_reg_state regs[MAX_BPF_REG];
 	struct bpf_verifier_state *parent;
+	/* index of call instruction that called into this func */
+	int callsite;
+	/* stack frame number of this function state from pov of
+	 * enclosing bpf_verifier_state.
+	 * 0 = main function, 1 = first callee.
+	 */
+	u32 frameno;
+	/* subprog number == index within subprog_stack_depth
+	 * zero == main subprog
+	 */
+	u32 subprogno;
+
+	/* should be second to last. See copy_func_state() */
 	int allocated_stack;
 	struct bpf_stack_state *stack;
 };
 
+#define MAX_CALL_FRAMES 8
+struct bpf_verifier_state {
+	/* call stack tracking */
+	struct bpf_func_state *frame[MAX_CALL_FRAMES];
+	struct bpf_verifier_state *parent;
+	u32 curframe;
+};
+
 /* linked list of verifier states used to prune search */
 struct bpf_verifier_state_list {
 	struct bpf_verifier_state state;
@@ -163,12 +192,15 @@ struct bpf_verifier_env {
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 	struct bpf_verifer_log log;
 	u32 subprog_starts[BPF_MAX_SUBPROGS];
+	u16 subprog_stack_depth[BPF_MAX_SUBPROGS + 1];
 	u32 subprog_cnt;
 };
 
 static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 {
-	return env->cur_state->regs;
+	struct bpf_verifier_state *cur = env->cur_state;
+
+	return cur->frame[cur->curframe]->regs;
 }
 
 #if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)

commit cc8b0b92a1699bc32f7fec71daa2bfc90de43a4d
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:05 2017 -0800

    bpf: introduce function calls (function boundaries)
    
    Allow arbitrary function calls from bpf function to another bpf function.
    
    Since the beginning of bpf all bpf programs were represented as a single function
    and program authors were forced to use always_inline for all functions
    in their C code. That was causing llvm to unnecessary inflate the code size
    and forcing developers to move code to header files with little code reuse.
    
    With a bit of additional complexity teach verifier to recognize
    arbitrary function calls from one bpf function to another as long as
    all of functions are presented to the verifier as a single bpf program.
    New program layout:
    r6 = r1    // some code
    ..
    r1 = ..    // arg1
    r2 = ..    // arg2
    call pc+1  // function call pc-relative
    exit
    .. = r1    // access arg1
    .. = r2    // access arg2
    ..
    call pc+20 // second level of function call
    ...
    
    It allows for better optimized code and finally allows to introduce
    the core bpf libraries that can be reused in different projects,
    since programs are no longer limited by single elf file.
    With function calls bpf can be compiled into multiple .o files.
    
    This patch is the first step. It detects programs that contain
    multiple functions and checks that calls between them are valid.
    It splits the sequence of bpf instructions (one program) into a set
    of bpf functions that call each other. Calls to only known
    functions are allowed. In the future the verifier may allow
    calls to unresolved functions and will do dynamic linking.
    This logic supports statically linked bpf functions only.
    
    Such function boundary detection could have been done as part of
    control flow graph building in check_cfg(), but it's cleaner to
    separate function boundary detection vs control flow checks within
    a subprogram (function) into logically indepedent steps.
    Follow up patches may split check_cfg() further, but not check_subprogs().
    
    Only allow bpf-to-bpf calls for root only and for non-hw-offloaded programs.
    These restrictions can be relaxed in the future.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c561b986bab0..91a583bb3fa7 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -141,6 +141,8 @@ struct bpf_ext_analyzer_ops {
 			 int insn_idx, int prev_insn_idx);
 };
 
+#define BPF_MAX_SUBPROGS 256
+
 /* single container for all structs
  * one verifier_env per bpf_check() call
  */
@@ -159,8 +161,9 @@ struct bpf_verifier_env {
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
-
 	struct bpf_verifer_log log;
+	u32 subprog_starts[BPF_MAX_SUBPROGS];
+	u32 subprog_cnt;
 };
 
 static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)

commit c131187db2d3fa2f8bf32fdf4e9a4ef805168467
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Nov 22 16:42:05 2017 -0800

    bpf: fix branch pruning logic
    
    when the verifier detects that register contains a runtime constant
    and it's compared with another constant it will prune exploration
    of the branch that is guaranteed not to be taken at runtime.
    This is all correct, but malicious program may be constructed
    in such a way that it always has a constant comparison and
    the other branch is never taken under any conditions.
    In this case such path through the program will not be explored
    by the verifier. It won't be taken at run-time either, but since
    all instructions are JITed the malicious program may cause JITs
    to complain about using reserved fields, etc.
    To fix the issue we have to track the instructions explored by
    the verifier and sanitize instructions that are dead at run time
    with NOPs. We cannot reject such dead code, since llvm generates
    it for valid C code, since it doesn't do as much data flow
    analysis as the verifier does.
    
    Fixes: 17a5267067f3 ("bpf: verifier (add verifier core)")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index b61482d354a2..c561b986bab0 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -115,7 +115,7 @@ struct bpf_insn_aux_data {
 		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
 	};
 	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
-	int converted_op_size; /* the valid value width after perceived conversion */
+	bool seen; /* this insn was processed by the verifier */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit 1438019479349d262b76f8767ace3273d11b6dcb
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 20 15:22:00 2017 -0800

    bpf: make bpf_prog_offload_verifier_prep() static inline
    
    Header implementation of bpf_prog_offload_verifier_prep() which
    is used if CONFIG_NET=n should be a static inline.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 07b96aaca256..b61482d354a2 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -171,7 +171,7 @@ static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 #if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)
 int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);
 #else
-int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env)
+static inline int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env)
 {
 	return -EOPNOTSUPP;
 }

commit b37a530613104aa3f592376c67a462823298759c
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:30 2017 -0700

    bpf: remove old offload/analyzer
    
    Thanks to the ability to load a program for a specific device,
    running verifier twice is no longer needed.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index e45011dbc02d..07b96aaca256 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -152,9 +152,7 @@ struct bpf_verifier_env {
 	bool strict_alignment;		/* perform strict pointer alignment checks */
 	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
-	const struct bpf_ext_analyzer_ops *analyzer_ops; /* external analyzer ops */
 	const struct bpf_ext_analyzer_ops *dev_ops; /* device analyzer ops */
-	void *analyzer_priv; /* pointer to external analyzer's private data */
 	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
 	u32 used_map_cnt;		/* number of used maps */
 	u32 id_gen;			/* used to generate unique reg IDs */
@@ -179,7 +177,4 @@ int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env)
 }
 #endif
 
-int bpf_analyzer(struct bpf_prog *prog, const struct bpf_ext_analyzer_ops *ops,
-		 void *priv);
-
 #endif /* _LINUX_BPF_VERIFIER_H */

commit ab3f0063c48c26c927851b6767824e35a716d878
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:17 2017 -0700

    bpf: offload: add infrastructure for loading programs for a specific netdev
    
    The fact that we don't know which device the program is going
    to be used on is quite limiting in current eBPF infrastructure.
    We have to reverse or limit the changes which kernel makes to
    the loaded bytecode if we want it to be offloaded to a networking
    device.  We also have to invent new APIs for debugging and
    troubleshooting support.
    
    Make it possible to load programs for a specific netdev.  This
    helps us to bring the debug information closer to the core
    eBPF infrastructure (e.g. we will be able to reuse the verifer
    log in device JIT).  It allows device JITs to perform translation
    on the original bytecode.
    
    __bpf_prog_get() when called to get a reference for an attachment
    point will now refuse to give it if program has a device assigned.
    Following patches will add a version of that function which passes
    the expected netdev in. @type argument in __bpf_prog_get() is
    renamed to attach_type to make it clearer that it's only set on
    attachment.
    
    All calls to ndo_bpf are protected by rtnl, only verifier callbacks
    are not.  We need a wait queue to make sure netdev doesn't get
    destroyed while verifier is still running and calling its driver.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 3b0976aaac75..e45011dbc02d 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -153,6 +153,7 @@ struct bpf_verifier_env {
 	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
 	const struct bpf_ext_analyzer_ops *analyzer_ops; /* external analyzer ops */
+	const struct bpf_ext_analyzer_ops *dev_ops; /* device analyzer ops */
 	void *analyzer_priv; /* pointer to external analyzer's private data */
 	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
 	u32 used_map_cnt;		/* number of used maps */
@@ -169,6 +170,15 @@ static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
 	return env->cur_state->regs;
 }
 
+#if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)
+int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env);
+#else
+int bpf_prog_offload_verifier_prep(struct bpf_verifier_env *env)
+{
+	return -EOPNOTSUPP;
+}
+#endif
+
 int bpf_analyzer(struct bpf_prog *prog, const struct bpf_ext_analyzer_ops *ops,
 		 void *priv);
 

commit 638f5b90d46016372a8e3e0a434f199cc5e12b8c
Author: Alexei Starovoitov <ast@fb.com>
Date:   Tue Oct 31 18:16:05 2017 -0700

    bpf: reduce verifier memory consumption
    
    the verifier got progressively smarter over time and size of its internal
    state grew as well. Time to reduce the memory consumption.
    
    Before:
    sizeof(struct bpf_verifier_state) = 6520
    After:
    sizeof(struct bpf_verifier_state) = 896
    
    It's done by observing that majority of BPF programs use little to
    no stack whereas verifier kept all of 512 stack slots ready always.
    Instead dynamically reallocate struct verifier state when stack
    access is detected.
    Runtime difference before vs after is within a noise.
    The number of processed instructions stays the same.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index feeaea93d959..3b0976aaac75 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -88,14 +88,19 @@ enum bpf_stack_slot_type {
 
 #define BPF_REG_SIZE 8	/* size of eBPF register in bytes */
 
+struct bpf_stack_state {
+	struct bpf_reg_state spilled_ptr;
+	u8 slot_type[BPF_REG_SIZE];
+};
+
 /* state of the program:
  * type of all registers and stack info
  */
 struct bpf_verifier_state {
 	struct bpf_reg_state regs[MAX_BPF_REG];
-	u8 stack_slot_type[MAX_BPF_STACK];
-	struct bpf_reg_state spilled_regs[MAX_BPF_STACK / BPF_REG_SIZE];
 	struct bpf_verifier_state *parent;
+	int allocated_stack;
+	struct bpf_stack_state *stack;
 };
 
 /* linked list of verifier states used to prune search */
@@ -145,7 +150,7 @@ struct bpf_verifier_env {
 	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */
 	int stack_size;			/* number of states to be processed */
 	bool strict_alignment;		/* perform strict pointer alignment checks */
-	struct bpf_verifier_state cur_state; /* current verifier state */
+	struct bpf_verifier_state *cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
 	const struct bpf_ext_analyzer_ops *analyzer_ops; /* external analyzer ops */
 	void *analyzer_priv; /* pointer to external analyzer's private data */
@@ -159,6 +164,11 @@ struct bpf_verifier_env {
 	struct bpf_verifer_log log;
 };
 
+static inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)
+{
+	return env->cur_state->regs;
+}
+
 int bpf_analyzer(struct bpf_prog *prog, const struct bpf_ext_analyzer_ops *ops,
 		 void *priv);
 

commit 00176a34d9e27ab1e77db75fe13abc005cffe0ca
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 16 16:40:54 2017 -0700

    bpf: remove the verifier ops from program structure
    
    Since the verifier ops don't have to be associated with
    the program for its entire lifetime we can move it to
    verifier's struct bpf_verifier_env.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index f00ef751c1c5..feeaea93d959 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -141,6 +141,7 @@ struct bpf_ext_analyzer_ops {
  */
 struct bpf_verifier_env {
 	struct bpf_prog *prog;		/* eBPF program being verified */
+	const struct bpf_verifier_ops *ops;
 	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */
 	int stack_size;			/* number of states to be processed */
 	bool strict_alignment;		/* perform strict pointer alignment checks */

commit a2a7d5701052542cd2260e7659b12443e0a74733
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 9 10:30:15 2017 -0700

    bpf: write back the verifier log buffer as it gets filled
    
    Verifier log buffer can be quite large (up to 16MB currently).
    As Eric Dumazet points out if we allow multiple verification
    requests to proceed simultaneously, malicious user may use the
    verifier as a way of allocating large amounts of unswappable
    memory to OOM the host.
    
    Switch to a strategy of allocating a smaller buffer (1024B)
    and writing it out into the user buffer after every print.
    
    While at it remove the old BUG_ON().
    
    This is in preparation of the global verifier lock removal.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 5ddb9a626a51..f00ef751c1c5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -115,9 +115,11 @@ struct bpf_insn_aux_data {
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 
+#define BPF_VERIFIER_TMP_LOG_SIZE	1024
+
 struct bpf_verifer_log {
 	u32 level;
-	char *kbuf;
+	char kbuf[BPF_VERIFIER_TMP_LOG_SIZE];
 	char __user *ubuf;
 	u32 len_used;
 	u32 len_total;

commit 61bd5218eef349fcacc4976a251bc83a4748b4af
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 9 10:30:11 2017 -0700

    bpf: move global verifier log into verifier environment
    
    The biggest piece of global state protected by the verifier lock
    is the verifier_log.  Move that log to struct bpf_verifier_env.
    struct bpf_verifier_env has to be passed now to all invocations
    of verbose().
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 163541ba70d9..5ddb9a626a51 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -152,6 +152,8 @@ struct bpf_verifier_env {
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
+
+	struct bpf_verifer_log log;
 };
 
 int bpf_analyzer(struct bpf_prog *prog, const struct bpf_ext_analyzer_ops *ops,

commit e7bf8249e8f1bac64885eeccb55bcf6111901a81
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 9 10:30:10 2017 -0700

    bpf: encapsulate verifier log state into a structure
    
    Put the loose log_* variables into a structure.  This will make
    it simpler to remove the global verifier state in following patches.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index b8d200f60a40..163541ba70d9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -115,6 +115,19 @@ struct bpf_insn_aux_data {
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 
+struct bpf_verifer_log {
+	u32 level;
+	char *kbuf;
+	char __user *ubuf;
+	u32 len_used;
+	u32 len_total;
+};
+
+static inline bool bpf_verifier_log_full(const struct bpf_verifer_log *log)
+{
+	return log->len_used >= log->len_total - 1;
+}
+
 struct bpf_verifier_env;
 struct bpf_ext_analyzer_ops {
 	int (*insn_hook)(struct bpf_verifier_env *env,

commit 8e9cd9ce90d48369b2c5ddd79fe3d4a4cb1ccb56
Author: Edward Cree <ecree@solarflare.com>
Date:   Wed Aug 23 15:11:21 2017 +0100

    bpf/verifier: document liveness analysis
    
    The liveness tracking algorithm is quite subtle; add comments to explain it.
    
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index d8f131a36fd0..b8d200f60a40 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -21,6 +21,19 @@
  */
 #define BPF_MAX_VAR_SIZ	INT_MAX
 
+/* Liveness marks, used for registers and spilled-regs (in stack slots).
+ * Read marks propagate upwards until they find a write mark; they record that
+ * "one of this state's descendants read this reg" (and therefore the reg is
+ * relevant for states_equal() checks).
+ * Write marks collect downwards and do not propagate; they record that "the
+ * straight-line code that reached this state (from its parent) wrote this reg"
+ * (and therefore that reads propagated from this state or its descendants
+ * should not propagate to its parent).
+ * A state with a write mark can receive read marks; it just won't propagate
+ * them to its parent, since the write mark is a property, not of the state,
+ * but of the link between it and its parent.  See mark_reg_read() and
+ * mark_stack_slot_read() in kernel/bpf/verifier.c.
+ */
 enum bpf_reg_liveness {
 	REG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */
 	REG_LIVE_READ, /* reg was read, so we're sensitive to initial value */

commit 1b688a19a92223cf2d1892c9d05d64dc397b33e3
Author: Edward Cree <ecree@solarflare.com>
Date:   Wed Aug 23 15:10:50 2017 +0100

    bpf/verifier: remove varlen_map_value_access flag
    
    The optimisation it does is broken when the 'new' register value has a
     variable offset and the 'old' was constant.  I broke it with my pointer
     types unification (see Fixes tag below), before which the 'new' value
     would have type PTR_TO_MAP_VALUE_ADJ and would thus not compare equal;
     other changes in that patch mean that its original behaviour (ignore
     min/max values) cannot be restored.
    Tests on a sample set of cilium programs show no change in count of
     processed instructions.
    
    Fixes: f1174f77b50c ("bpf/verifier: rework value tracking")
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 91d07efed2ba..d8f131a36fd0 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -125,7 +125,6 @@ struct bpf_verifier_env {
 	u32 id_gen;			/* used to generate unique reg IDs */
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
-	bool varlen_map_value_access;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 };
 

commit dc503a8ad98474ea0073a1c5c4d9f18cb8dd0dbf
Author: Edward Cree <ecree@solarflare.com>
Date:   Tue Aug 15 20:34:35 2017 +0100

    bpf/verifier: track liveness for pruning
    
    State of a register doesn't matter if it wasn't read in reaching an exit;
     a write screens off all reads downstream of it from all explored_states
     upstream of it.
    This allows us to prune many more branches; here are some processed insn
     counts for some Cilium programs:
    Program                  before  after
    bpf_lb_opt_-DLB_L3.o       6515   3361
    bpf_lb_opt_-DLB_L4.o       8976   5176
    bpf_lb_opt_-DUNKNOWN.o     2960   1137
    bpf_lxc_opt_-DDROP_ALL.o  95412  48537
    bpf_lxc_opt_-DUNKNOWN.o  141706  78718
    bpf_netdev.o              24251  17995
    bpf_overlay.o             10999   9385
    
    The runtime is also improved; here are 'time' results in ms:
    Program                  before  after
    bpf_lb_opt_-DLB_L3.o         24      6
    bpf_lb_opt_-DLB_L4.o         26     11
    bpf_lb_opt_-DUNKNOWN.o       11      2
    bpf_lxc_opt_-DDROP_ALL.o   1288    139
    bpf_lxc_opt_-DUNKNOWN.o    1768    234
    bpf_netdev.o                 62     31
    bpf_overlay.o                15     13
    
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c61c3033522e..91d07efed2ba 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -21,6 +21,12 @@
  */
 #define BPF_MAX_VAR_SIZ	INT_MAX
 
+enum bpf_reg_liveness {
+	REG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */
+	REG_LIVE_READ, /* reg was read, so we're sensitive to initial value */
+	REG_LIVE_WRITTEN, /* reg was written first, screening off later reads */
+};
+
 struct bpf_reg_state {
 	enum bpf_reg_type type;
 	union {
@@ -40,7 +46,7 @@ struct bpf_reg_state {
 	 * came from, when one is tested for != NULL.
 	 */
 	u32 id;
-	/* These five fields must be last.  See states_equal() */
+	/* Ordering of fields matters.  See states_equal() */
 	/* For scalar types (SCALAR_VALUE), this represents our knowledge of
 	 * the actual value.
 	 * For pointer types, this represents the variable part of the offset
@@ -57,6 +63,8 @@ struct bpf_reg_state {
 	s64 smax_value; /* maximum possible (s64)value */
 	u64 umin_value; /* minimum possible (u64)value */
 	u64 umax_value; /* maximum possible (u64)value */
+	/* This field must be last, for states_equal() reasons. */
+	enum bpf_reg_liveness live;
 };
 
 enum bpf_stack_slot_type {
@@ -74,6 +82,7 @@ struct bpf_verifier_state {
 	struct bpf_reg_state regs[MAX_BPF_REG];
 	u8 stack_slot_type[MAX_BPF_STACK];
 	struct bpf_reg_state spilled_regs[MAX_BPF_STACK / BPF_REG_SIZE];
+	struct bpf_verifier_state *parent;
 };
 
 /* linked list of verifier states used to prune search */

commit b03c9f9fdc37dab81ea04d5dacdc5995d4c224c2
Author: Edward Cree <ecree@solarflare.com>
Date:   Mon Aug 7 15:26:36 2017 +0100

    bpf/verifier: track signed and unsigned min/max values
    
    Allows us to, sometimes, combine information from a signed check of one
     bound and an unsigned check of the other.
    We now track the full range of possible values, rather than restricting
     ourselves to [0, 1<<30) and considering anything beyond that as
     unknown.  While this is probably not necessary, it makes the code more
     straightforward and symmetrical between signed and unsigned bounds.
    
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 85936fa92d12..c61c3033522e 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -11,11 +11,15 @@
 #include <linux/filter.h> /* for MAX_BPF_STACK */
 #include <linux/tnum.h>
 
- /* Just some arbitrary values so we can safely do math without overflowing and
-  * are obviously wrong for any sort of memory access.
-  */
-#define BPF_REGISTER_MAX_RANGE (1024 * 1024 * 1024)
-#define BPF_REGISTER_MIN_RANGE -1
+/* Maximum variable offset umax_value permitted when resolving memory accesses.
+ * In practice this is far bigger than any realistic pointer offset; this limit
+ * ensures that umax_value + (int)off + (int)size cannot overflow a u64.
+ */
+#define BPF_MAX_VAR_OFF	(1ULL << 31)
+/* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures
+ * that converting umax_value to int cannot overflow.
+ */
+#define BPF_MAX_VAR_SIZ	INT_MAX
 
 struct bpf_reg_state {
 	enum bpf_reg_type type;
@@ -36,7 +40,7 @@ struct bpf_reg_state {
 	 * came from, when one is tested for != NULL.
 	 */
 	u32 id;
-	/* These three fields must be last.  See states_equal() */
+	/* These five fields must be last.  See states_equal() */
 	/* For scalar types (SCALAR_VALUE), this represents our knowledge of
 	 * the actual value.
 	 * For pointer types, this represents the variable part of the offset
@@ -49,9 +53,10 @@ struct bpf_reg_state {
 	 * These refer to the same value as var_off, not necessarily the actual
 	 * contents of the register.
 	 */
-	s64 min_value;
-	u64 max_value;
-	bool value_from_signed;
+	s64 smin_value; /* minimum possible (s64)value */
+	s64 smax_value; /* maximum possible (s64)value */
+	u64 umin_value; /* minimum possible (u64)value */
+	u64 umax_value; /* maximum possible (u64)value */
 };
 
 enum bpf_stack_slot_type {

commit f1174f77b50c94eecaa658fdc56fa69b421de4b8
Author: Edward Cree <ecree@solarflare.com>
Date:   Mon Aug 7 15:26:19 2017 +0100

    bpf/verifier: rework value tracking
    
    Unifies adjusted and unadjusted register value types (e.g. FRAME_POINTER is
     now just a PTR_TO_STACK with zero offset).
    Tracks value alignment by means of tracking known & unknown bits.  This
     also replaces the 'reg->imm' (leading zero bits) calculations for (what
     were) UNKNOWN_VALUEs.
    If pointer leaks are allowed, and adjust_ptr_min_max_vals returns -EACCES,
     treat the pointer as an unknown scalar and try again, because we might be
     able to conclude something about the result (e.g. pointer & 0x40 is either
     0 or 0x40).
    Verifier hooks in the netronome/nfp driver were changed to match the new
     data structures.
    
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 8e5d31f6faef..85936fa92d12 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -9,6 +9,7 @@
 
 #include <linux/bpf.h> /* for enum bpf_reg_type */
 #include <linux/filter.h> /* for MAX_BPF_STACK */
+#include <linux/tnum.h>
 
  /* Just some arbitrary values so we can safely do math without overflowing and
   * are obviously wrong for any sort of memory access.
@@ -19,30 +20,37 @@
 struct bpf_reg_state {
 	enum bpf_reg_type type;
 	union {
-		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
-		s64 imm;
-
-		/* valid when type == PTR_TO_PACKET* */
-		struct {
-			u16 off;
-			u16 range;
-		};
+		/* valid when type == PTR_TO_PACKET */
+		u16 range;
 
 		/* valid when type == CONST_PTR_TO_MAP | PTR_TO_MAP_VALUE |
 		 *   PTR_TO_MAP_VALUE_OR_NULL
 		 */
 		struct bpf_map *map_ptr;
 	};
+	/* Fixed part of pointer offset, pointer types only */
+	s32 off;
+	/* For PTR_TO_PACKET, used to find other pointers with the same variable
+	 * offset, so they can share range knowledge.
+	 * For PTR_TO_MAP_VALUE_OR_NULL this is used to share which map value we
+	 * came from, when one is tested for != NULL.
+	 */
 	u32 id;
+	/* These three fields must be last.  See states_equal() */
+	/* For scalar types (SCALAR_VALUE), this represents our knowledge of
+	 * the actual value.
+	 * For pointer types, this represents the variable part of the offset
+	 * from the pointed-to object, and is shared with all bpf_reg_states
+	 * with the same id as us.
+	 */
+	struct tnum var_off;
 	/* Used to determine if any memory access using this register will
-	 * result in a bad access. These two fields must be last.
-	 * See states_equal()
+	 * result in a bad access.
+	 * These refer to the same value as var_off, not necessarily the actual
+	 * contents of the register.
 	 */
 	s64 min_value;
 	u64 max_value;
-	u32 min_align;
-	u32 aux_off;
-	u32 aux_off_align;
 	bool value_from_signed;
 };
 

commit 4cabc5b186b5427b9ee5a7495172542af105f02b
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jul 21 00:00:21 2017 +0200

    bpf: fix mixed signed/unsigned derived min/max value bounds
    
    Edward reported that there's an issue in min/max value bounds
    tracking when signed and unsigned compares both provide hints
    on limits when having unknown variables. E.g. a program such
    as the following should have been rejected:
    
       0: (7a) *(u64 *)(r10 -8) = 0
       1: (bf) r2 = r10
       2: (07) r2 += -8
       3: (18) r1 = 0xffff8a94cda93400
       5: (85) call bpf_map_lookup_elem#1
       6: (15) if r0 == 0x0 goto pc+7
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R10=fp
       7: (7a) *(u64 *)(r10 -16) = -8
       8: (79) r1 = *(u64 *)(r10 -16)
       9: (b7) r2 = -1
      10: (2d) if r1 > r2 goto pc+3
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=0
      R2=imm-1,max_value=18446744073709551615,min_align=1 R10=fp
      11: (65) if r1 s> 0x1 goto pc+2
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=0,max_value=1
      R2=imm-1,max_value=18446744073709551615,min_align=1 R10=fp
      12: (0f) r0 += r1
      13: (72) *(u8 *)(r0 +0) = 0
      R0=map_value_adj(ks=8,vs=8,id=0),min_value=0,max_value=1 R1=inv,min_value=0,max_value=1
      R2=imm-1,max_value=18446744073709551615,min_align=1 R10=fp
      14: (b7) r0 = 0
      15: (95) exit
    
    What happens is that in the first part ...
    
       8: (79) r1 = *(u64 *)(r10 -16)
       9: (b7) r2 = -1
      10: (2d) if r1 > r2 goto pc+3
    
    ... r1 carries an unsigned value, and is compared as unsigned
    against a register carrying an immediate. Verifier deduces in
    reg_set_min_max() that since the compare is unsigned and operation
    is greater than (>), that in the fall-through/false case, r1's
    minimum bound must be 0 and maximum bound must be r2. Latter is
    larger than the bound and thus max value is reset back to being
    'invalid' aka BPF_REGISTER_MAX_RANGE. Thus, r1 state is now
    'R1=inv,min_value=0'. The subsequent test ...
    
      11: (65) if r1 s> 0x1 goto pc+2
    
    ... is a signed compare of r1 with immediate value 1. Here,
    verifier deduces in reg_set_min_max() that since the compare
    is signed this time and operation is greater than (>), that
    in the fall-through/false case, we can deduce that r1's maximum
    bound must be 1, meaning with prior test, we result in r1 having
    the following state: R1=inv,min_value=0,max_value=1. Given that
    the actual value this holds is -8, the bounds are wrongly deduced.
    When this is being added to r0 which holds the map_value(_adj)
    type, then subsequent store access in above case will go through
    check_mem_access() which invokes check_map_access_adj(), that
    will then probe whether the map memory is in bounds based
    on the min_value and max_value as well as access size since
    the actual unknown value is min_value <= x <= max_value; commit
    fce366a9dd0d ("bpf, verifier: fix alu ops against map_value{,
    _adj} register types") provides some more explanation on the
    semantics.
    
    It's worth to note in this context that in the current code,
    min_value and max_value tracking are used for two things, i)
    dynamic map value access via check_map_access_adj() and since
    commit 06c1c049721a ("bpf: allow helpers access to variable memory")
    ii) also enforced at check_helper_mem_access() when passing a
    memory address (pointer to packet, map value, stack) and length
    pair to a helper and the length in this case is an unknown value
    defining an access range through min_value/max_value in that
    case. The min_value/max_value tracking is /not/ used in the
    direct packet access case to track ranges. However, the issue
    also affects case ii), for example, the following crafted program
    based on the same principle must be rejected as well:
    
       0: (b7) r2 = 0
       1: (bf) r3 = r10
       2: (07) r3 += -512
       3: (7a) *(u64 *)(r10 -16) = -8
       4: (79) r4 = *(u64 *)(r10 -16)
       5: (b7) r6 = -1
       6: (2d) if r4 > r6 goto pc+5
      R1=ctx R2=imm0,min_value=0,max_value=0,min_align=2147483648 R3=fp-512
      R4=inv,min_value=0 R6=imm-1,max_value=18446744073709551615,min_align=1 R10=fp
       7: (65) if r4 s> 0x1 goto pc+4
      R1=ctx R2=imm0,min_value=0,max_value=0,min_align=2147483648 R3=fp-512
      R4=inv,min_value=0,max_value=1 R6=imm-1,max_value=18446744073709551615,min_align=1
      R10=fp
       8: (07) r4 += 1
       9: (b7) r5 = 0
      10: (6a) *(u16 *)(r10 -512) = 0
      11: (85) call bpf_skb_load_bytes#26
      12: (b7) r0 = 0
      13: (95) exit
    
    Meaning, while we initialize the max_value stack slot that the
    verifier thinks we access in the [1,2] range, in reality we
    pass -7 as length which is interpreted as u32 in the helper.
    Thus, this issue is relevant also for the case of helper ranges.
    Resetting both bounds in check_reg_overflow() in case only one
    of them exceeds limits is also not enough as similar test can be
    created that uses values which are within range, thus also here
    learned min value in r1 is incorrect when mixed with later signed
    test to create a range:
    
       0: (7a) *(u64 *)(r10 -8) = 0
       1: (bf) r2 = r10
       2: (07) r2 += -8
       3: (18) r1 = 0xffff880ad081fa00
       5: (85) call bpf_map_lookup_elem#1
       6: (15) if r0 == 0x0 goto pc+7
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R10=fp
       7: (7a) *(u64 *)(r10 -16) = -8
       8: (79) r1 = *(u64 *)(r10 -16)
       9: (b7) r2 = 2
      10: (3d) if r2 >= r1 goto pc+3
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=3
      R2=imm2,min_value=2,max_value=2,min_align=2 R10=fp
      11: (65) if r1 s> 0x4 goto pc+2
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0
      R1=inv,min_value=3,max_value=4 R2=imm2,min_value=2,max_value=2,min_align=2 R10=fp
      12: (0f) r0 += r1
      13: (72) *(u8 *)(r0 +0) = 0
      R0=map_value_adj(ks=8,vs=8,id=0),min_value=3,max_value=4
      R1=inv,min_value=3,max_value=4 R2=imm2,min_value=2,max_value=2,min_align=2 R10=fp
      14: (b7) r0 = 0
      15: (95) exit
    
    This leaves us with two options for fixing this: i) to invalidate
    all prior learned information once we switch signed context, ii)
    to track min/max signed and unsigned boundaries separately as
    done in [0]. (Given latter introduces major changes throughout
    the whole verifier, it's rather net-next material, thus this
    patch follows option i), meaning we can derive bounds either
    from only signed tests or only unsigned tests.) There is still the
    case of adjust_reg_min_max_vals(), where we adjust bounds on ALU
    operations, meaning programs like the following where boundaries
    on the reg get mixed in context later on when bounds are merged
    on the dst reg must get rejected, too:
    
       0: (7a) *(u64 *)(r10 -8) = 0
       1: (bf) r2 = r10
       2: (07) r2 += -8
       3: (18) r1 = 0xffff89b2bf87ce00
       5: (85) call bpf_map_lookup_elem#1
       6: (15) if r0 == 0x0 goto pc+6
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R10=fp
       7: (7a) *(u64 *)(r10 -16) = -8
       8: (79) r1 = *(u64 *)(r10 -16)
       9: (b7) r2 = 2
      10: (3d) if r2 >= r1 goto pc+2
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=3
      R2=imm2,min_value=2,max_value=2,min_align=2 R10=fp
      11: (b7) r7 = 1
      12: (65) if r7 s> 0x0 goto pc+2
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=3
      R2=imm2,min_value=2,max_value=2,min_align=2 R7=imm1,max_value=0 R10=fp
      13: (b7) r0 = 0
      14: (95) exit
    
      from 12 to 15: R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0
      R1=inv,min_value=3 R2=imm2,min_value=2,max_value=2,min_align=2 R7=imm1,min_value=1 R10=fp
      15: (0f) r7 += r1
      16: (65) if r7 s> 0x4 goto pc+2
      R0=map_value(ks=8,vs=8,id=0),min_value=0,max_value=0 R1=inv,min_value=3
      R2=imm2,min_value=2,max_value=2,min_align=2 R7=inv,min_value=4,max_value=4 R10=fp
      17: (0f) r0 += r7
      18: (72) *(u8 *)(r0 +0) = 0
      R0=map_value_adj(ks=8,vs=8,id=0),min_value=4,max_value=4 R1=inv,min_value=3
      R2=imm2,min_value=2,max_value=2,min_align=2 R7=inv,min_value=4,max_value=4 R10=fp
      19: (b7) r0 = 0
      20: (95) exit
    
    Meaning, in adjust_reg_min_max_vals() we must also reset range
    values on the dst when src/dst registers have mixed signed/
    unsigned derived min/max value bounds with one unbounded value
    as otherwise they can be added together deducing false boundaries.
    Once both boundaries are established from either ALU ops or
    compare operations w/o mixing signed/unsigned insns, then they
    can safely be added to other regs also having both boundaries
    established. Adding regs with one unbounded side to a map value
    where the bounded side has been learned w/o mixing ops is
    possible, but the resulting map value won't recover from that,
    meaning such op is considered invalid on the time of actual
    access. Invalid bounds are set on the dst reg in case i) src reg,
    or ii) in case dst reg already had them. The only way to recover
    would be to perform i) ALU ops but only 'add' is allowed on map
    value types or ii) comparisons, but these are disallowed on
    pointers in case they span a range. This is fine as only BPF_JEQ
    and BPF_JNE may be performed on PTR_TO_MAP_VALUE_OR_NULL registers
    which potentially turn them into PTR_TO_MAP_VALUE type depending
    on the branch, so only here min/max value cannot be invalidated
    for them.
    
    In terms of state pruning, value_from_signed is considered
    as well in states_equal() when dealing with adjusted map values.
    With regards to breaking existing programs, there is a small
    risk, but use-cases are rather quite narrow where this could
    occur and mixing compares probably unlikely.
    
    Joint work with Josef and Edward.
    
      [0] https://lists.iovisor.org/pipermail/iovisor-dev/2017-June/000822.html
    
    Fixes: 484611357c19 ("bpf: allow access into map value arrays")
    Reported-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 621076f56251..8e5d31f6faef 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -43,6 +43,7 @@ struct bpf_reg_state {
 	u32 min_align;
 	u32 aux_off;
 	u32 aux_off_align;
+	bool value_from_signed;
 };
 
 enum bpf_stack_slot_type {

commit 239946314e57711d7da546b67964d0b387a3ee42
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Jun 22 15:07:39 2017 -0700

    bpf: possibly avoid extra masking for narrower load in verifier
    
    Commit 31fd85816dbe ("bpf: permits narrower load from bpf program
    context fields") permits narrower load for certain ctx fields.
    The commit however will already generate a masking even if
    the prog-specific ctx conversion produces the result with
    narrower size.
    
    For example, for __sk_buff->protocol, the ctx conversion
    loads the data into register with 2-byte load.
    A narrower 2-byte load should not generate masking.
    For __sk_buff->vlan_present, the conversion function
    set the result as either 0 or 1, essentially a byte.
    The narrower 2-byte or 1-byte load should not generate masking.
    
    To avoid unnecessary masking, prog-specific *_is_valid_access
    now passes converted_op_size back to verifier, which indicates
    the valid data width after perceived future conversion.
    Based on this information, verifier is able to avoid
    unnecessary marking.
    
    Since we want more information back from prog-specific
    *_is_valid_access checking, all of them are packed into
    one data structure for more clarity.
    
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 189741c0da85..621076f56251 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -73,7 +73,8 @@ struct bpf_insn_aux_data {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
 	};
-	int ctx_field_size; /* the ctx field size for load/store insns, maybe 0 */
+	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
+	int converted_op_size; /* the valid value width after perceived conversion */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit 31fd85816dbe3a714bcc3f67c17c3dd87011f79e
Author: Yonghong Song <yhs@fb.com>
Date:   Tue Jun 13 15:52:13 2017 -0700

    bpf: permits narrower load from bpf program context fields
    
    Currently, verifier will reject a program if it contains an
    narrower load from the bpf context structure. For example,
            __u8 h = __sk_buff->hash, or
            __u16 p = __sk_buff->protocol
            __u32 sample_period = bpf_perf_event_data->sample_period
    which are narrower loads of 4-byte or 8-byte field.
    
    This patch solves the issue by:
      . Introduce a new parameter ctx_field_size to carry the
        field size of narrower load from prog type
        specific *__is_valid_access validator back to verifier.
      . The non-zero ctx_field_size for a memory access indicates
        (1). underlying prog type specific convert_ctx_accesses
             supporting non-whole-field access
        (2). the current insn is a narrower or whole field access.
      . In verifier, for such loads where load memory size is
        less than ctx_field_size, verifier transforms it
        to a full field load followed by proper masking.
      . Currently, __sk_buff and bpf_perf_event_data->sample_period
        are supporting narrowing loads.
      . Narrower stores are still not allowed as typical ctx stores
        are just normal stores.
    
    Because of this change, some tests in verifier will fail and
    these tests are removed. As a bonus, rename some out of bound
    __sk_buff->cb access to proper field name and remove two
    redundant "skb cb oob" tests.
    
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index d5093b52b485..189741c0da85 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -73,6 +73,7 @@ struct bpf_insn_aux_data {
 		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
 	};
+	int ctx_field_size; /* the ctx field size for load/store insns, maybe 0 */
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit e07b98d9bffe410019dfcf62c3428d4a96c56a2c
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 10 11:38:07 2017 -0700

    bpf: Add strict alignment flag for BPF_PROG_LOAD.
    
    Add a new field, "prog_flags", and an initial flag value
    BPF_F_STRICT_ALIGNMENT.
    
    When set, the verifier will enforce strict pointer alignment
    regardless of the setting of CONFIG_EFFICIENT_UNALIGNED_ACCESS.
    
    The verifier, in this mode, will also use a fixed value of "2" in
    place of NET_IP_ALIGN.
    
    This facilitates test cases that will exercise and validate this part
    of the verifier even when run on architectures where alignment doesn't
    matter.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7c6a51924afc..d5093b52b485 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -90,6 +90,7 @@ struct bpf_verifier_env {
 	struct bpf_prog *prog;		/* eBPF program being verified */
 	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */
 	int stack_size;			/* number of states to be processed */
+	bool strict_alignment;		/* perform strict pointer alignment checks */
 	struct bpf_verifier_state cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
 	const struct bpf_ext_analyzer_ops *analyzer_ops; /* external analyzer ops */

commit d1174416747d790d750742d0514915deeed93acf
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 10 11:22:52 2017 -0700

    bpf: Track alignment of register values in the verifier.
    
    Currently if we add only constant values to pointers we can fully
    validate the alignment, and properly check if we need to reject the
    program on !CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS architectures.
    
    However, once an unknown value is introduced we only allow byte sized
    memory accesses which is too restrictive.
    
    Add logic to track the known minimum alignment of register values,
    and propagate this state into registers containing pointers.
    
    The most common paradigm that makes use of this new logic is computing
    the transport header using the IP header length field.  For example:
    
            struct ethhdr *ep = skb->data;
            struct iphdr *iph = (struct iphdr *) (ep + 1);
            struct tcphdr *th;
     ...
            n = iph->ihl;
            th = ((void *)iph + (n * 4));
            port = th->dest;
    
    The existing code will reject the load of th->dest because it cannot
    validate that the alignment is at least 2 once "n * 4" is added the
    the packet pointer.
    
    In the new code, the register holding "n * 4" will have a reg->min_align
    value of 4, because any value multiplied by 4 will be at least 4 byte
    aligned.  (actually, the eBPF code emitted by the compiler in this case
    is most likely to use a shift left by 2, but the end result is identical)
    
    At the critical addition:
    
            th = ((void *)iph + (n * 4));
    
    The register holding 'th' will start with reg->off value of 14.  The
    pointer addition will transform that reg into something that looks like:
    
            reg->aux_off = 14
            reg->aux_off_align = 4
    
    Next, the verifier will look at the th->dest load, and it will see
    a load offset of 2, and first check:
    
            if (reg->aux_off_align % size)
    
    which will pass because aux_off_align is 4.  reg_off will be computed:
    
            reg_off = reg->off;
     ...
                    reg_off += reg->aux_off;
    
    plus we have off==2, and it will thus check:
    
            if ((NET_IP_ALIGN + reg_off + off) % size != 0)
    
    which evaluates to:
    
            if ((NET_IP_ALIGN + 14 + 2) % size != 0)
    
    On strict alignment architectures, NET_IP_ALIGN is 2, thus:
    
            if ((2 + 14 + 2) % size != 0)
    
    which passes.
    
    These pointer transformations and checks work regardless of whether
    the constant offset or the variable with known alignment is added
    first to the pointer register.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 5efb4db44e1e..7c6a51924afc 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -40,6 +40,9 @@ struct bpf_reg_state {
 	 */
 	s64 min_value;
 	u64 max_value;
+	u32 min_align;
+	u32 aux_off;
+	u32 aux_off_align;
 };
 
 enum bpf_stack_slot_type {

commit 81ed18ab3098b6519274545e80a29caacb77d160
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Mar 15 18:26:42 2017 -0700

    bpf: add helper inlining infra and optimize map_array lookup
    
    Optimize bpf_call -> bpf_map_lookup_elem() -> array_map_lookup_elem()
    into a sequence of bpf instructions.
    When JIT is on the sequence of bpf instructions is the sequence
    of native cpu instructions with significantly faster performance
    than indirect call and two function's prologue/epilogue.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index a13b031dc6b8..5efb4db44e1e 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -66,7 +66,10 @@ struct bpf_verifier_state_list {
 };
 
 struct bpf_insn_aux_data {
-	enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
+	union {
+		enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
+		struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
+	};
 };
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */

commit d2a4dd37f6b41fbcad76efbf63124eb3126c66fe
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Dec 7 10:57:59 2016 -0800

    bpf: fix state equivalence
    
    Commmits 57a09bf0a416 ("bpf: Detect identical PTR_TO_MAP_VALUE_OR_NULL registers")
    and 484611357c19 ("bpf: allow access into map value arrays") by themselves
    are correct, but in combination they make state equivalence ignore 'id' field
    of the register state which can lead to accepting invalid program.
    
    Fixes: 57a09bf0a416 ("bpf: Detect identical PTR_TO_MAP_VALUE_OR_NULL registers")
    Fixes: 484611357c19 ("bpf: allow access into map value arrays")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Thomas Graf <tgraf@suug.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7453c1281531..a13b031dc6b8 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -18,13 +18,6 @@
 
 struct bpf_reg_state {
 	enum bpf_reg_type type;
-	/*
-	 * Used to determine if any memory access using this register will
-	 * result in a bad access.
-	 */
-	s64 min_value;
-	u64 max_value;
-	u32 id;
 	union {
 		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
 		s64 imm;
@@ -40,6 +33,13 @@ struct bpf_reg_state {
 		 */
 		struct bpf_map *map_ptr;
 	};
+	u32 id;
+	/* Used to determine if any memory access using this register will
+	 * result in a bad access. These two fields must be last.
+	 * See states_equal()
+	 */
+	s64 min_value;
+	u64 max_value;
 };
 
 enum bpf_stack_slot_type {

commit f9aa9dc7d2d00e6eb02168ffc64ef614b89d7998
Merge: 06b37b650cf8 3b404a519815
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Nov 22 11:29:28 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    All conflicts were simple overlapping changes except perhaps
    for the Thunder driver.
    
    That driver has a change_mtu method explicitly for sending
    a message to the hardware.  If that fails it returns an
    error.
    
    Normally a driver doesn't need an ndo_change_mtu method becuase those
    are usually just range changes, which are now handled generically.
    But since this extra operation is needed in the Thunder driver, it has
    to stay.
    
    However, if the message send fails we have to restore the original
    MTU before the change because the entire call chain expects that if
    an error is thrown by ndo_change_mtu then the MTU did not change.
    Therefore code is added to nicvf_change_mtu to remember the original
    MTU, and to restore it upon nicvf_update_hw_max_frs() failue.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f23cc643f9baec7f71f2b74692da3cf03abbbfda
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Nov 14 15:45:36 2016 -0500

    bpf: fix range arithmetic for bpf map access
    
    I made some invalid assumptions with BPF_AND and BPF_MOD that could result in
    invalid accesses to bpf map entries.  Fix this up by doing a few things
    
    1) Kill BPF_MOD support.  This doesn't actually get used by the compiler in real
    life and just adds extra complexity.
    
    2) Fix the logic for BPF_AND, don't allow AND of negative numbers and set the
    minimum value to 0 for positive AND's.
    
    3) Don't do operations on the ranges if they are set to the limits, as they are
    by definition undefined, and allowing arithmetic operations on those values
    could make them appear valid when they really aren't.
    
    This fixes the testcase provided by Jann as well as a few other theoretical
    problems.
    
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7035b997aaa5..6aaf425cebc3 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -14,7 +14,7 @@
   * are obviously wrong for any sort of memory access.
   */
 #define BPF_REGISTER_MAX_RANGE (1024 * 1024 * 1024)
-#define BPF_REGISTER_MIN_RANGE -(1024 * 1024 * 1024)
+#define BPF_REGISTER_MIN_RANGE -1
 
 struct bpf_reg_state {
 	enum bpf_reg_type type;
@@ -22,7 +22,8 @@ struct bpf_reg_state {
 	 * Used to determine if any memory access using this register will
 	 * result in a bad access.
 	 */
-	u64 min_value, max_value;
+	s64 min_value;
+	u64 max_value;
 	union {
 		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
 		s64 imm;

commit 57a09bf0a416700676e77102c28f9cfcb48267e0
Author: Thomas Graf <tgraf@suug.ch>
Date:   Tue Oct 18 19:51:19 2016 +0200

    bpf: Detect identical PTR_TO_MAP_VALUE_OR_NULL registers
    
    A BPF program is required to check the return register of a
    map_elem_lookup() call before accessing memory. The verifier keeps
    track of this by converting the type of the result register from
    PTR_TO_MAP_VALUE_OR_NULL to PTR_TO_MAP_VALUE after a conditional
    jump ensures safety. This check is currently exclusively performed
    for the result register 0.
    
    In the event the compiler reorders instructions, BPF_MOV64_REG
    instructions may be moved before the conditional jump which causes
    them to keep their type PTR_TO_MAP_VALUE_OR_NULL to which the
    verifier objects when the register is accessed:
    
    0: (b7) r1 = 10
    1: (7b) *(u64 *)(r10 -8) = r1
    2: (bf) r2 = r10
    3: (07) r2 += -8
    4: (18) r1 = 0x59c00000
    6: (85) call 1
    7: (bf) r4 = r0
    8: (15) if r0 == 0x0 goto pc+1
     R0=map_value(ks=8,vs=8) R4=map_value_or_null(ks=8,vs=8) R10=fp
    9: (7a) *(u64 *)(r4 +0) = 0
    R4 invalid mem access 'map_value_or_null'
    
    This commit extends the verifier to keep track of all identical
    PTR_TO_MAP_VALUE_OR_NULL registers after a map_elem_lookup() by
    assigning them an ID and then marking them all when the conditional
    jump is observed.
    
    Signed-off-by: Thomas Graf <tgraf@suug.ch>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 7035b997aaa5..ac5b393ee6b2 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -23,13 +23,13 @@ struct bpf_reg_state {
 	 * result in a bad access.
 	 */
 	u64 min_value, max_value;
+	u32 id;
 	union {
 		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
 		s64 imm;
 
 		/* valid when type == PTR_TO_PACKET* */
 		struct {
-			u32 id;
 			u16 off;
 			u16 range;
 		};

commit 484611357c19f9e19ef742ebef4505a07d243cc9
Author: Josef Bacik <jbacik@fb.com>
Date:   Wed Sep 28 10:54:32 2016 -0400

    bpf: allow access into map value arrays
    
    Suppose you have a map array value that is something like this
    
    struct foo {
            unsigned iter;
            int array[SOME_CONSTANT];
    };
    
    You can easily insert this into an array, but you cannot modify the contents of
    foo->array[] after the fact.  This is because we have no way to verify we won't
    go off the end of the array at verification time.  This patch provides a start
    for this work.  We accomplish this by keeping track of a minimum and maximum
    value a register could be while we're checking the code.  Then at the time we
    try to do an access into a MAP_VALUE we verify that the maximum offset into that
    region is a valid access into that memory region.  So in practice, code such as
    this
    
    unsigned index = 0;
    
    if (foo->iter >= SOME_CONSTANT)
            foo->iter = index;
    else
            index = foo->iter++;
    foo->array[index] = bar;
    
    would be allowed, as we can verify that index will always be between 0 and
    SOME_CONSTANT-1.  If you wish to use signed values you'll have to have an extra
    check to make sure the index isn't less than 0, or do something like index %=
    SOME_CONSTANT.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index c5cb661712c9..7035b997aaa5 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -10,8 +10,19 @@
 #include <linux/bpf.h> /* for enum bpf_reg_type */
 #include <linux/filter.h> /* for MAX_BPF_STACK */
 
+ /* Just some arbitrary values so we can safely do math without overflowing and
+  * are obviously wrong for any sort of memory access.
+  */
+#define BPF_REGISTER_MAX_RANGE (1024 * 1024 * 1024)
+#define BPF_REGISTER_MIN_RANGE -(1024 * 1024 * 1024)
+
 struct bpf_reg_state {
 	enum bpf_reg_type type;
+	/*
+	 * Used to determine if any memory access using this register will
+	 * result in a bad access.
+	 */
+	u64 min_value, max_value;
 	union {
 		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
 		s64 imm;
@@ -81,6 +92,7 @@ struct bpf_verifier_env {
 	u32 id_gen;			/* used to generate unique reg IDs */
 	bool allow_ptr_leaks;
 	bool seen_direct_write;
+	bool varlen_map_value_access;
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 };
 

commit 13a27dfc669724564aafa2699976ee756029fed2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Sep 21 11:43:58 2016 +0100

    bpf: enable non-core use of the verfier
    
    Advanced JIT compilers and translators may want to use
    eBPF verifier as a base for parsers or to perform custom
    checks and validations.
    
    Add ability for external users to invoke the verifier
    and provide callbacks to be invoked for every intruction
    checked.  For now only add most basic callback for
    per-instruction pre-interpretation checks is added.  More
    advanced users may also like to have per-instruction post
    callback and state comparison callback.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
index 9457a22fc6e0..c5cb661712c9 100644
--- a/include/linux/bpf_verifier.h
+++ b/include/linux/bpf_verifier.h
@@ -59,6 +59,12 @@ struct bpf_insn_aux_data {
 
 #define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 
+struct bpf_verifier_env;
+struct bpf_ext_analyzer_ops {
+	int (*insn_hook)(struct bpf_verifier_env *env,
+			 int insn_idx, int prev_insn_idx);
+};
+
 /* single container for all structs
  * one verifier_env per bpf_check() call
  */
@@ -68,6 +74,8 @@ struct bpf_verifier_env {
 	int stack_size;			/* number of states to be processed */
 	struct bpf_verifier_state cur_state; /* current verifier state */
 	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
+	const struct bpf_ext_analyzer_ops *analyzer_ops; /* external analyzer ops */
+	void *analyzer_priv; /* pointer to external analyzer's private data */
 	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
 	u32 used_map_cnt;		/* number of used maps */
 	u32 id_gen;			/* used to generate unique reg IDs */
@@ -76,4 +84,7 @@ struct bpf_verifier_env {
 	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
 };
 
+int bpf_analyzer(struct bpf_prog *prog, const struct bpf_ext_analyzer_ops *ops,
+		 void *priv);
+
 #endif /* _LINUX_BPF_VERIFIER_H */

commit 58e2af8b3a6b587e4ac8414343581da4349d3c0f
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Sep 21 11:43:57 2016 +0100

    bpf: expose internal verfier structures
    
    Move verifier's internal structures to a header file and
    prefix their names with bpf_ to avoid potential namespace
    conflicts.  Those structures will soon be used by external
    analyzers.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_verifier.h b/include/linux/bpf_verifier.h
new file mode 100644
index 000000000000..9457a22fc6e0
--- /dev/null
+++ b/include/linux/bpf_verifier.h
@@ -0,0 +1,79 @@
+/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ */
+#ifndef _LINUX_BPF_VERIFIER_H
+#define _LINUX_BPF_VERIFIER_H 1
+
+#include <linux/bpf.h> /* for enum bpf_reg_type */
+#include <linux/filter.h> /* for MAX_BPF_STACK */
+
+struct bpf_reg_state {
+	enum bpf_reg_type type;
+	union {
+		/* valid when type == CONST_IMM | PTR_TO_STACK | UNKNOWN_VALUE */
+		s64 imm;
+
+		/* valid when type == PTR_TO_PACKET* */
+		struct {
+			u32 id;
+			u16 off;
+			u16 range;
+		};
+
+		/* valid when type == CONST_PTR_TO_MAP | PTR_TO_MAP_VALUE |
+		 *   PTR_TO_MAP_VALUE_OR_NULL
+		 */
+		struct bpf_map *map_ptr;
+	};
+};
+
+enum bpf_stack_slot_type {
+	STACK_INVALID,    /* nothing was stored in this stack slot */
+	STACK_SPILL,      /* register spilled into stack */
+	STACK_MISC	  /* BPF program wrote some data into this slot */
+};
+
+#define BPF_REG_SIZE 8	/* size of eBPF register in bytes */
+
+/* state of the program:
+ * type of all registers and stack info
+ */
+struct bpf_verifier_state {
+	struct bpf_reg_state regs[MAX_BPF_REG];
+	u8 stack_slot_type[MAX_BPF_STACK];
+	struct bpf_reg_state spilled_regs[MAX_BPF_STACK / BPF_REG_SIZE];
+};
+
+/* linked list of verifier states used to prune search */
+struct bpf_verifier_state_list {
+	struct bpf_verifier_state state;
+	struct bpf_verifier_state_list *next;
+};
+
+struct bpf_insn_aux_data {
+	enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
+};
+
+#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
+
+/* single container for all structs
+ * one verifier_env per bpf_check() call
+ */
+struct bpf_verifier_env {
+	struct bpf_prog *prog;		/* eBPF program being verified */
+	struct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */
+	int stack_size;			/* number of states to be processed */
+	struct bpf_verifier_state cur_state; /* current verifier state */
+	struct bpf_verifier_state_list **explored_states; /* search pruning optimization */
+	struct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */
+	u32 used_map_cnt;		/* number of used maps */
+	u32 id_gen;			/* used to generate unique reg IDs */
+	bool allow_ptr_leaks;
+	bool seen_direct_write;
+	struct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */
+};
+
+#endif /* _LINUX_BPF_VERIFIER_H */
