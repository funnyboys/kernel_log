commit bd93f003b7462ae39a43c531abca37fe7073b866
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Jun 4 16:50:30 2020 -0700

    include/linux/bitops.h: avoid clang shift-count-overflow warnings
    
    Clang normally does not warn about certain issues in inline functions when
    it only happens in an eliminated code path. However if something else
    goes wrong, it does tend to complain about the definition of hweight_long()
    on 32-bit targets:
    
      include/linux/bitops.h:75:41: error: shift count >= width of type [-Werror,-Wshift-count-overflow]
              return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
                                                     ^~~~~~~~~~~~
      include/asm-generic/bitops/const_hweight.h:29:49: note: expanded from macro 'hweight64'
       define hweight64(w) (__builtin_constant_p(w) ? __const_hweight64(w) : __arch_hweight64(w))
                                                      ^~~~~~~~~~~~~~~~~~~~
      include/asm-generic/bitops/const_hweight.h:21:76: note: expanded from macro '__const_hweight64'
       define __const_hweight64(w) (__const_hweight32(w) + __const_hweight32((w) >> 32))
                                                                                 ^  ~~
      include/asm-generic/bitops/const_hweight.h:20:49: note: expanded from macro '__const_hweight32'
       define __const_hweight32(w) (__const_hweight16(w) + __const_hweight16((w) >> 16))
                                                      ^
      include/asm-generic/bitops/const_hweight.h:19:72: note: expanded from macro '__const_hweight16'
       define __const_hweight16(w) (__const_hweight8(w)  + __const_hweight8((w)  >> 8 ))
                                                                             ^
      include/asm-generic/bitops/const_hweight.h:12:9: note: expanded from macro '__const_hweight8'
                (!!((w) & (1ULL << 2))) +     \
    
    Adding an explicit cast to __u64 avoids that warning and makes it easier
    to read other output.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Link: http://lkml.kernel.org/r/20200505135513.65265-1-arnd@arndb.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 9acf654f0b19..99f2ac30b1d9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -72,7 +72,7 @@ static inline int get_bitmask_order(unsigned int count)
 
 static __always_inline unsigned long hweight_long(unsigned long w)
 {
-	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
+	return sizeof(w) == 4 ? hweight32(w) : hweight64((__u64)w);
 }
 
 /**

commit f80ac98a641a03097cbc9fdfd4b6a41a8dd3b7ae
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Apr 6 20:09:43 2020 -0700

    bitops: always inline sign extension helpers
    
    With CONFIG_CC_OPTIMIZE_FOR_SIZE, objtool reports:
    
      drivers/gpu/drm/i915/gem/i915_gem_execbuffer.o: warning: objtool: i915_gem_execbuffer2_ioctl()+0x5b7: call to gen8_canonical_addr() with UACCESS enabled
    
    This means i915_gem_execbuffer2_ioctl() is calling gen8_canonical_addr()
    from the user_access_begin/end critical region (i.e, with SMAP disabled).
    
    While it's probably harmless in this case, in general we like to avoid
    extra function calls in SMAP-disabled regions because it can open up
    inadvertent security holes.
    
    Fix the warning by changing the sign extension helpers to __always_inline.
    This convinces GCC to inline gen8_canonical_addr().
    
    The sign extension functions are trivial anyway, so it makes sense to
    always inline them.  With my test optimize-for-size-based config, this
    actually shrinks the text size of i915_gem_execbuffer.o by 45 bytes -- and
    no change for vmlinux.
    
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Link: http://lkml.kernel.org/r/740179324b2b18b750b16295c48357f00b5fa9ed.1582982020.git.jpoimboe@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 47f54b459c26..9acf654f0b19 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -162,7 +162,7 @@ static inline __u8 ror8(__u8 word, unsigned int shift)
  *
  * This is safe to use for 16- and 8-bit types as well.
  */
-static inline __s32 sign_extend32(__u32 value, int index)
+static __always_inline __s32 sign_extend32(__u32 value, int index)
 {
 	__u8 shift = 31 - index;
 	return (__s32)(value << shift) >> shift;
@@ -173,7 +173,7 @@ static inline __s32 sign_extend32(__u32 value, int index)
  * @value: value to sign extend
  * @index: 0 based bit index (0<=index<64) to sign bit
  */
-static inline __s64 sign_extend64(__u64 value, int index)
+static __always_inline __s64 sign_extend64(__u64 value, int index)
 {
 	__u8 shift = 63 - index;
 	return (__s64)(value << shift) >> shift;

commit 0bddc1bd05d6973fee9303005abab6567f743ea7
Author: Yury Norov <yury.norov@gmail.com>
Date:   Mon Feb 3 17:37:24 2020 -0800

    bitops: more BITS_TO_* macros
    
    Introduce BITS_TO_U64, BITS_TO_U32 and BITS_TO_BYTES as they are handy in
    the following patches (BITS_TO_U32 specifically).  Reimplement tools/
    version of the macros according to the kernel implementation.
    
    Also fix indentation for BITS_PER_TYPE definition.
    
    Link: http://lkml.kernel.org/r/20200102043031.30357-3-yury.norov@gmail.com
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Reviewed-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Amritha Nambiar <amritha.nambiar@intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: "Tobin C . Harding" <tobin@kernel.org>
    Cc: Vineet Gupta <vineet.gupta1@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 6c7c4133c25c..47f54b459c26 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -11,8 +11,10 @@
 #  define aligned_byte_mask(n) (~0xffUL << (BITS_PER_LONG - 8 - 8*(n)))
 #endif
 
-#define BITS_PER_TYPE(type) (sizeof(type) * BITS_PER_BYTE)
+#define BITS_PER_TYPE(type)	(sizeof(type) * BITS_PER_BYTE)
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(long))
+#define BITS_TO_U64(nr)		DIV_ROUND_UP(nr, BITS_PER_TYPE(u64))
+#define BITS_TO_U32(nr)		DIV_ROUND_UP(nr, BITS_PER_TYPE(u32))
 #define BITS_TO_BYTES(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(char))
 
 extern unsigned int __sw_hweight8(unsigned int w);

commit dd3e7cba16274831f5a69f071ed3cf13ffb352ea
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Thu Jan 30 22:11:47 2020 -0800

    ocfs2/dlm: move BITS_TO_BYTES() to bitops.h for wider use
    
    There are users already and will be more of BITS_TO_BYTES() macro.  Move
    it to bitops.h for wider use.
    
    In the case of ocfs2 the replacement is identical.
    
    As for bnx2x, there are two places where floor version is used.  In the
    first case to calculate the amount of structures that can fit one memory
    page.  In this case obviously the ceiling variant is correct and
    original code might have a potential bug, if amount of bits % 8 is not
    0.  In the second case the macro is used to calculate bytes transmitted
    in one microsecond.  This will work for all speeds which is multiply of
    1Gbps without any change, for the rest new code will give ceiling value,
    for instance 100Mbps will give 13 bytes, while old code gives 12 bytes
    and the arithmetically correct one is 12.5 bytes.  Further the value is
    used to setup timer threshold which in any case has its own margins due
    to certain resolution.  I don't see here an issue with slightly shifting
    thresholds for low speed connections, the card is supposed to utilize
    highest available rate, which is usually 10Gbps.
    
    Link: http://lkml.kernel.org/r/20200108121316.22411-1-andriy.shevchenko@linux.intel.com
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Reviewed-by: Joseph Qi <joseph.qi@linux.alibaba.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Cc: Mark Fasheh <mark@fasheh.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Changwei Ge <gechangwei@live.cn>
    Cc: Gang He <ghe@suse.com>
    Cc: Jun Piao <piaojun@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index e479067c202c..6c7c4133c25c 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -13,6 +13,7 @@
 
 #define BITS_PER_TYPE(type) (sizeof(type) * BITS_PER_BYTE)
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(long))
+#define BITS_TO_BYTES(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(char))
 
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);

commit 169c474fb22d8a5e909e172f177b957546d0519d
Author: William Breathitt Gray <vilhelm.gray@gmail.com>
Date:   Wed Dec 4 16:50:57 2019 -0800

    bitops: introduce the for_each_set_clump8 macro
    
    Pach series "Introduce the for_each_set_clump8 macro", v18.
    
    While adding GPIO get_multiple/set_multiple callback support for various
    drivers, I noticed a pattern of looping manifesting that would be useful
    standardized as a macro.
    
    This patchset introduces the for_each_set_clump8 macro and utilizes it
    in several GPIO drivers.  The for_each_set_clump macro8 facilitates a
    for-loop syntax that iterates over a memory region entire groups of set
    bits at a time.
    
    For example, suppose you would like to iterate over a 32-bit integer 8
    bits at a time, skipping over 8-bit groups with no set bit, where
    XXXXXXXX represents the current 8-bit group:
    
        Example:        10111110 00000000 11111111 00110011
        First loop:     10111110 00000000 11111111 XXXXXXXX
        Second loop:    10111110 00000000 XXXXXXXX 00110011
        Third loop:     XXXXXXXX 00000000 11111111 00110011
    
    Each iteration of the loop returns the next 8-bit group that has at
    least one set bit.
    
    The for_each_set_clump8 macro has four parameters:
    
        * start: set to the bit offset of the current clump
        * clump: set to the current clump value
        * bits: bitmap to search within
        * size: bitmap size in number of bits
    
    In this version of the patchset, the for_each_set_clump macro has been
    reimplemented and simplified based on the suggestions provided by Rasmus
    Villemoes and Andy Shevchenko in the version 4 submission.
    
    In particular, the function of the for_each_set_clump macro has been
    restricted to handle only 8-bit clumps; the drivers that use the
    for_each_set_clump macro only handle 8-bit ports so a generic
    for_each_set_clump implementation is not necessary.  Thus, a solution
    for large clumps (i.e.  those larger than the width of a bitmap word)
    can be postponed until a driver appears that actually requires such a
    generic for_each_set_clump implementation.
    
    For what it's worth, a semi-generic for_each_set_clump (i.e.  for clumps
    smaller than the width of a bitmap word) can be implemented by simply
    replacing the hardcoded '8' and '0xFF' instances with respective
    variables.  I have not yet had a need for such an implementation, and
    since it falls short of a true generic for_each_set_clump function, I
    have decided to forgo such an implementation for now.
    
    In addition, the bitmap_get_value8 and bitmap_set_value8 functions are
    introduced to get and set 8-bit values respectively.  Their use is based
    on the behavior suggested in the patchset version 4 review.
    
    This patch (of 14):
    
    This macro iterates for each 8-bit group of bits (clump) with set bits,
    within a bitmap memory region.  For each iteration, "start" is set to
    the bit offset of the found clump, while the respective clump value is
    stored to the location pointed by "clump".  Additionally, the
    bitmap_get_value8 and bitmap_set_value8 functions are introduced to
    respectively get and set an 8-bit value in a bitmap memory region.
    
    [gustavo@embeddedor.com: fix potential sign-extension overflow]
      Link: http://lkml.kernel.org/r/20191015184657.GA26541@embeddedor
    [akpm@linux-foundation.org: s/ULL/UL/, per Joe]
    [vilhelm.gray@gmail.com: add for_each_set_clump8 documentation]
      Link: http://lkml.kernel.org/r/20191016161825.301082-1-vilhelm.gray@gmail.com
    Link: http://lkml.kernel.org/r/893c3b4f03266c9496137cc98ac2b1bd27f92c73.1570641097.git.vilhelm.gray@gmail.com
    Signed-off-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Suggested-by: Andy Shevchenko <andy.shevchenko@gmail.com>
    Suggested-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Suggested-by: Lukas Wunner <lukas@wunner.de>
    Tested-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Cc: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Phil Reid <preid@electromag.com.au>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Mathias Duckeck <m.duckeck@kunbus.de>
    Cc: Morten Hein Tiljeset <morten.tiljeset@prevas.dk>
    Cc: Sean Nyekjaer <sean.nyekjaer@prevas.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index c94a9ff9f082..e479067c202c 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -47,6 +47,18 @@ extern unsigned long __sw_hweight64(__u64 w);
 	     (bit) < (size);					\
 	     (bit) = find_next_zero_bit((addr), (size), (bit) + 1))
 
+/**
+ * for_each_set_clump8 - iterate over bitmap for each 8-bit clump with set bits
+ * @start: bit offset to start search and to store the current iteration offset
+ * @clump: location to store copy of current 8-bit clump
+ * @bits: bitmap address to base the search on
+ * @size: bitmap size in number of bits
+ */
+#define for_each_set_clump8(start, clump, bits, size) \
+	for ((start) = find_first_clump8(&(clump), (bits), (size)); \
+	     (start) < (size); \
+	     (start) = find_next_clump8(&(clump), (bits), (size), (start) + 8))
+
 static inline int get_bitmask_order(unsigned int count)
 {
 	int order;

commit f5a1a536fa14895ccff4e94e6a5af90901ce86aa
Author: Aleksa Sarai <cyphar@cyphar.com>
Date:   Tue Oct 1 11:10:52 2019 +1000

    lib: introduce copy_struct_from_user() helper
    
    A common pattern for syscall extensions is increasing the size of a
    struct passed from userspace, such that the zero-value of the new fields
    result in the old kernel behaviour (allowing for a mix of userspace and
    kernel vintages to operate on one another in most cases).
    
    While this interface exists for communication in both directions, only
    one interface is straightforward to have reasonable semantics for
    (userspace passing a struct to the kernel). For kernel returns to
    userspace, what the correct semantics are (whether there should be an
    error if userspace is unaware of a new extension) is very
    syscall-dependent and thus probably cannot be unified between syscalls
    (a good example of this problem is [1]).
    
    Previously there was no common lib/ function that implemented
    the necessary extension-checking semantics (and different syscalls
    implemented them slightly differently or incompletely[2]). Future
    patches replace common uses of this pattern to make use of
    copy_struct_from_user().
    
    Some in-kernel selftests that insure that the handling of alignment and
    various byte patterns are all handled identically to memchr_inv() usage.
    
    [1]: commit 1251201c0d34 ("sched/core: Fix uclamp ABI bug, clean up and
         robustify sched_read_attr() ABI logic and code")
    
    [2]: For instance {sched_setattr,perf_event_open,clone3}(2) all do do
         similar checks to copy_struct_from_user() while rt_sigprocmask(2)
         always rejects differently-sized struct arguments.
    
    Suggested-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Aleksa Sarai <cyphar@cyphar.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20191001011055.19283-2-cyphar@cyphar.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index cf074bce3eb3..c94a9ff9f082 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -4,6 +4,13 @@
 #include <asm/types.h>
 #include <linux/bits.h>
 
+/* Set bits in the first 'n' bytes when loaded from memory */
+#ifdef __LITTLE_ENDIAN
+#  define aligned_byte_mask(n) ((1UL << 8*(n))-1)
+#else
+#  define aligned_byte_mask(n) (~0xffUL << (BITS_PER_LONG - 8 - 8*(n)))
+#endif
+
 #define BITS_PER_TYPE(type) (sizeof(type) * BITS_PER_BYTE)
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(long))
 

commit ef4d6f6b275c498f8e5626c99dbeefdc5027f843
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Tue May 14 15:43:27 2019 -0700

    include/linux/bitops.h: sanitize rotate primitives
    
    The ror32 implementation (word >> shift) | (word << (32 - shift) has
    undefined behaviour if shift is outside the [1, 31] range.  Similarly
    for the 64 bit variants.  Most callers pass a compile-time constant
    (naturally in that range), but there's an UBSAN report that these may
    actually be called with a shift count of 0.
    
    Instead of special-casing that, we can make them DTRT for all values of
    shift while also avoiding UB.  For some reason, this was already partly
    done for rol32 (which was well-defined for [0, 31]).  gcc 8 recognizes
    these patterns as rotates, so for example
    
      __u32 rol32(__u32 word, unsigned int shift)
      {
            return (word << (shift & 31)) | (word >> ((-shift) & 31));
      }
    
    compiles to
    
    0000000000000020 <rol32>:
      20:   89 f8                   mov    %edi,%eax
      22:   89 f1                   mov    %esi,%ecx
      24:   d3 c0                   rol    %cl,%eax
      26:   c3                      retq
    
    Older compilers unfortunately do not do as well, but this only affects
    the small minority of users that don't pass constants.
    
    Due to integer promotions, ro[lr]8 were already well-defined for shifts
    in [0, 8], and ro[lr]16 were mostly well-defined for shifts in [0, 16]
    (only mostly - u16 gets promoted to _signed_ int, so if bit 15 is set,
    word << 16 is undefined).  For consistency, update those as well.
    
    Link: http://lkml.kernel.org/r/20190410211906.2190-1-linux@rasmusvillemoes.dk
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Reported-by: Ido Schimmel <idosch@mellanox.com>
    Tested-by: Ido Schimmel <idosch@mellanox.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Vadim Pasternak <vadimp@mellanox.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 602af23b98c7..cf074bce3eb3 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -60,7 +60,7 @@ static __always_inline unsigned long hweight_long(unsigned long w)
  */
 static inline __u64 rol64(__u64 word, unsigned int shift)
 {
-	return (word << shift) | (word >> (64 - shift));
+	return (word << (shift & 63)) | (word >> ((-shift) & 63));
 }
 
 /**
@@ -70,7 +70,7 @@ static inline __u64 rol64(__u64 word, unsigned int shift)
  */
 static inline __u64 ror64(__u64 word, unsigned int shift)
 {
-	return (word >> shift) | (word << (64 - shift));
+	return (word >> (shift & 63)) | (word << ((-shift) & 63));
 }
 
 /**
@@ -80,7 +80,7 @@ static inline __u64 ror64(__u64 word, unsigned int shift)
  */
 static inline __u32 rol32(__u32 word, unsigned int shift)
 {
-	return (word << shift) | (word >> ((-shift) & 31));
+	return (word << (shift & 31)) | (word >> ((-shift) & 31));
 }
 
 /**
@@ -90,7 +90,7 @@ static inline __u32 rol32(__u32 word, unsigned int shift)
  */
 static inline __u32 ror32(__u32 word, unsigned int shift)
 {
-	return (word >> shift) | (word << (32 - shift));
+	return (word >> (shift & 31)) | (word << ((-shift) & 31));
 }
 
 /**
@@ -100,7 +100,7 @@ static inline __u32 ror32(__u32 word, unsigned int shift)
  */
 static inline __u16 rol16(__u16 word, unsigned int shift)
 {
-	return (word << shift) | (word >> (16 - shift));
+	return (word << (shift & 15)) | (word >> ((-shift) & 15));
 }
 
 /**
@@ -110,7 +110,7 @@ static inline __u16 rol16(__u16 word, unsigned int shift)
  */
 static inline __u16 ror16(__u16 word, unsigned int shift)
 {
-	return (word >> shift) | (word << (16 - shift));
+	return (word >> (shift & 15)) | (word << ((-shift) & 15));
 }
 
 /**
@@ -120,7 +120,7 @@ static inline __u16 ror16(__u16 word, unsigned int shift)
  */
 static inline __u8 rol8(__u8 word, unsigned int shift)
 {
-	return (word << shift) | (word >> (8 - shift));
+	return (word << (shift & 7)) | (word >> ((-shift) & 7));
 }
 
 /**
@@ -130,7 +130,7 @@ static inline __u8 rol8(__u8 word, unsigned int shift)
  */
 static inline __u8 ror8(__u8 word, unsigned int shift)
 {
-	return (word >> shift) | (word << (8 - shift));
+	return (word >> (shift & 7)) | (word << ((-shift) & 7));
 }
 
 /**

commit 1db604f676b2edb7b18de7881f4d5988e97be616
Author: Vineet Gupta <vineet.gupta1@synopsys.com>
Date:   Thu Mar 7 16:28:14 2019 -0800

    include/linux/bitops.h: set_mask_bits() to return old value
    
    | > Also, set_mask_bits is used in fs quite a bit and we can possibly come up
    | > with a generic llsc based implementation (w/o the cmpxchg loop)
    |
    | May I also suggest changing the return value of set_mask_bits() to old.
    |
    | You can compute the new value given old, but you cannot compute the old
    | value given new, therefore old is the better return value. Also, no
    | current user seems to use the return value, so changing it is without
    | risk.
    
    Link: http://lkml.kernel.org/g/20150807110955.GH16853@twins.programming.kicks-ass.net
    Link: http://lkml.kernel.org/r/1548275584-18096-4-git-send-email-vgupta@synopsys.com
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Anthony Yznaga <anthony.yznaga@oracle.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jani Nikula <jani.nikula@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 705f7c442691..602af23b98c7 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -246,7 +246,7 @@ static __always_inline void __assign_bit(long nr, volatile unsigned long *addr,
 		new__ = (old__ & ~mask__) | bits__;		\
 	} while (cmpxchg(ptr, old__, new__) != old__);		\
 								\
-	new__;							\
+	old__;							\
 })
 #endif
 

commit edfa87281f4fa1b78a21f6db999935a2faa2f6b8
Author: Miklos Szeredi <mszeredi@redhat.com>
Date:   Mon Oct 15 15:43:06 2018 +0200

    bitops: protect variables in bit_clear_unless() macro
    
    Unprotected naming of local variables within bit_clear_unless() can easily
    lead to using the wrong scope.
    
    Noticed this by code review after having hit this issue in set_mask_bits()
    
    Signed-off-by: Miklos Szeredi <mszeredi@redhat.com>
    Fixes: 85ad1d13ee9b ("md: set MD_CHANGE_PENDING in a atomic region")
    Cc: Guoqing Jiang <gqjiang@suse.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index d18ee0e63c32..705f7c442691 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -251,18 +251,18 @@ static __always_inline void __assign_bit(long nr, volatile unsigned long *addr,
 #endif
 
 #ifndef bit_clear_unless
-#define bit_clear_unless(ptr, _clear, _test)	\
+#define bit_clear_unless(ptr, clear, test)	\
 ({								\
-	const typeof(*ptr) clear = (_clear), test = (_test);	\
-	typeof(*ptr) old, new;					\
+	const typeof(*(ptr)) clear__ = (clear), test__ = (test);\
+	typeof(*(ptr)) old__, new__;				\
 								\
 	do {							\
-		old = READ_ONCE(*ptr);			\
-		new = old & ~clear;				\
-	} while (!(old & test) &&				\
-		 cmpxchg(ptr, old, new) != old);		\
+		old__ = READ_ONCE(*(ptr));			\
+		new__ = old__ & ~clear__;			\
+	} while (!(old__ & test__) &&				\
+		 cmpxchg(ptr, old__, new__) != old__);		\
 								\
-	!(old & test);						\
+	!(old__ & test__);					\
 })
 #endif
 

commit 18127429a854e7607b859484880b8e26cee9ddab
Author: Miklos Szeredi <mszeredi@redhat.com>
Date:   Mon Oct 15 15:43:06 2018 +0200

    bitops: protect variables in set_mask_bits() macro
    
    Unprotected naming of local variables within the set_mask_bits() can easily
    lead to using the wrong scope.
    
    Noticed this when "set_mask_bits(&foo->bar, 0, mask)" behaved as no-op.
    
    Signed-off-by: Miklos Szeredi <mszeredi@redhat.com>
    Fixes: 00a1a053ebe5 ("ext4: atomically set inode->i_flags in ext4_set_inode_flags()")
    Cc: Theodore Ts'o <tytso@mit.edu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 7ddb1349394d..d18ee0e63c32 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -236,17 +236,17 @@ static __always_inline void __assign_bit(long nr, volatile unsigned long *addr,
 #ifdef __KERNEL__
 
 #ifndef set_mask_bits
-#define set_mask_bits(ptr, _mask, _bits)	\
+#define set_mask_bits(ptr, mask, bits)	\
 ({								\
-	const typeof(*ptr) mask = (_mask), bits = (_bits);	\
-	typeof(*ptr) old, new;					\
+	const typeof(*(ptr)) mask__ = (mask), bits__ = (bits);	\
+	typeof(*(ptr)) old__, new__;				\
 								\
 	do {							\
-		old = READ_ONCE(*ptr);			\
-		new = (old & ~mask) | bits;			\
-	} while (cmpxchg(ptr, old, new) != old);		\
+		old__ = READ_ONCE(*(ptr));			\
+		new__ = (old__ & ~mask__) | bits__;		\
+	} while (cmpxchg(ptr, old__, new__) != old__);		\
 								\
-	new;							\
+	new__;							\
 })
 #endif
 

commit 9144d75e22cad3c89e6b2ccab551db9ee28d250a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Aug 21 21:57:03 2018 -0700

    include/linux/bitops.h: introduce BITS_PER_TYPE
    
    net_dim.h has a rather useful extension to BITS_PER_BYTE to compute the
    number of bits in a type (BITS_PER_BYTE * sizeof(T)), so promote the macro
    to bitops.h, alongside BITS_PER_BYTE, for wider usage.
    
    Link: http://lkml.kernel.org/r/20180706094458.14116-1-chris@chris-wilson.co.uk
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Jani Nikula <jani.nikula@intel.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Andy Gospodarek <gospo@broadcom.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index af419012d77d..7ddb1349394d 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -4,7 +4,8 @@
 #include <asm/types.h>
 #include <linux/bits.h>
 
-#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
+#define BITS_PER_TYPE(type) (sizeof(type) * BITS_PER_BYTE)
+#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_TYPE(long))
 
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);

commit 8bd9cb51daac89337295b6f037b0486911e1b408
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jun 19 13:53:08 2018 +0100

    locking/atomics, asm-generic: Move some macros from <linux/bitops.h> to a new <linux/bits.h> file
    
    In preparation for implementing the asm-generic atomic bitops in terms
    of atomic_long_*(), we need to prevent <asm/atomic.h> implementations from
    pulling in <linux/bitops.h>. A common reason for this include is for the
    BITS_PER_BYTE definition, so move this and some other BIT() and masking
    macros into a new header file, <linux/bits.h>.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: yamada.masahiro@socionext.com
    Link: https://lore.kernel.org/lkml/1529412794-17720-4-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 4cac4e1a72ff..af419012d77d 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -2,29 +2,9 @@
 #ifndef _LINUX_BITOPS_H
 #define _LINUX_BITOPS_H
 #include <asm/types.h>
+#include <linux/bits.h>
 
-#ifdef	__KERNEL__
-#define BIT(nr)			(1UL << (nr))
-#define BIT_ULL(nr)		(1ULL << (nr))
-#define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
-#define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
-#define BIT_ULL_MASK(nr)	(1ULL << ((nr) % BITS_PER_LONG_LONG))
-#define BIT_ULL_WORD(nr)	((nr) / BITS_PER_LONG_LONG)
-#define BITS_PER_BYTE		8
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
-#endif
-
-/*
- * Create a contiguous bitmask starting at bit position @l and ending at
- * position @h. For example
- * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
- */
-#define GENMASK(h, l) \
-	(((~0UL) - (1UL << (l)) + 1) & (~0UL >> (BITS_PER_LONG - 1 - (h))))
-
-#define GENMASK_ULL(h, l) \
-	(((~0ULL) - (1ULL << (l)) + 1) & \
-	 (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
 
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);

commit 6aa2f9441f1ef21f10c41f45e6453b135e9cd736
Merge: e37e0ee01900 24f0966c3e3f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 17:23:44 2017 -0800

    Merge tag 'gpio-v4.15-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/linusw/linux-gpio
    
    Pull GPIO updates from Linus Walleij:
     "This is the bulk of GPIO changes for the v4.15 kernel cycle:
    
      Core:
    
       - Fix the semantics of raw GPIO to actually be raw. No inversion
         semantics as before, but also no open draining, and allow the raw
         operations to affect lines used for interrupts as the caller
         supposedly knows what they are doing if they are getting the big
         hammer.
    
       - Rewrote the __inner_function() notation calls to names that make
         more sense. I just find this kind of code disturbing.
    
       - Drop the .irq_base() field from the gpiochip since now all IRQs are
         mapped dynamically. This is nice.
    
       - Support for .get_multiple() in the core driver API. This allows us
         to read several GPIO lines with a single register read. This has
         high value for some usecases: it can be used to create
         oscilloscopes and signal analyzers and other things that rely on
         reading several lines at exactly the same instant. Also a generally
         nice optimization. This uses the new assign_bit() macro from the
         bitops lib that was ACKed by Andrew Morton and is implemented for
         two drivers, one of them being the generic MMIO driver so everyone
         using that will be able to benefit from this.
    
       - Do not allow requests of Open Drain and Open Source setting of a
         GPIO line simultaneously. If the hardware actually supports
         enabling both at the same time the electrical result would be
         disastrous.
    
       - A new interrupt chip core helper. This will be helpful to deal with
         "banked" GPIOs, which means GPIO controllers with several logical
         blocks of GPIO inside them. This is several gpiochips per device in
         the device model, in contrast to the case when there is a 1-to-1
         relationship between a device and a gpiochip.
    
      New drivers:
    
       - Maxim MAX3191x industrial serializer, a very interesting piece of
         professional I/O hardware.
    
       - Uniphier GPIO driver. This is the GPIO block from the recent
         Socionext (ex Fujitsu and Panasonic) platform.
    
       - Tegra 186 driver. This is based on the new banked GPIO
         infrastructure.
    
      Other improvements:
    
       - Some documentation improvements.
    
       - Wakeup support for the DesignWare DWAPB GPIO controller.
    
       - Reset line support on the DesignWare DWAPB GPIO controller.
    
       - Several non-critical bug fixes and improvements for the Broadcom
         BRCMSTB driver.
    
       - Misc non-critical bug fixes like exotic errorpaths, removal of dead
         code etc.
    
       - Explicit comments on fall-through switch() statements"
    
    * tag 'gpio-v4.15-1' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/linusw/linux-gpio: (65 commits)
      gpio: tegra186: Remove tegra186_gpio_lock_class
      gpio: rcar: Add r8a77995 (R-Car D3) support
      pinctrl: bcm2835: Fix some merge fallout
      gpio: Fix undefined lock_dep_class
      gpio: Automatically add lockdep keys
      gpio: Introduce struct gpio_irq_chip.first
      gpio: Disambiguate struct gpio_irq_chip.nested
      gpio: Add Tegra186 support
      gpio: Export gpiochip_irq_{map,unmap}()
      gpio: Implement tighter IRQ chip integration
      gpio: Move lock_key into struct gpio_irq_chip
      gpio: Move irq_valid_mask into struct gpio_irq_chip
      gpio: Move irq_nested into struct gpio_irq_chip
      gpio: Move irq_chained_parent to struct gpio_irq_chip
      gpio: Move irq_default_type to struct gpio_irq_chip
      gpio: Move irq_handler to struct gpio_irq_chip
      gpio: Move irqdomain into struct gpio_irq_chip
      gpio: Move irqchip into struct gpio_irq_chip
      gpio: Introduce struct gpio_irq_chip
      pinctrl: armada-37xx: remove unused variable
      ...

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 8fbe259b197c..d03c5dd6185d 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_BITOPS_H
 #define _LINUX_BITOPS_H
 #include <asm/types.h>

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 8fbe259b197c..0a7ce668f8e0 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -236,7 +236,7 @@ static inline unsigned long __ffs64(u64 word)
 	typeof(*ptr) old, new;					\
 								\
 	do {							\
-		old = ACCESS_ONCE(*ptr);			\
+		old = READ_ONCE(*ptr);			\
 		new = (old & ~mask) | bits;			\
 	} while (cmpxchg(ptr, old, new) != old);		\
 								\
@@ -251,7 +251,7 @@ static inline unsigned long __ffs64(u64 word)
 	typeof(*ptr) old, new;					\
 								\
 	do {							\
-		old = ACCESS_ONCE(*ptr);			\
+		old = READ_ONCE(*ptr);			\
 		new = old & ~clear;				\
 	} while (!(old & test) &&				\
 		 cmpxchg(ptr, old, new) != old);		\

commit 5307e2ad69ab3b0e0622fdf8b254c1d4565eb924
Author: Lukas Wunner <lukas@wunner.de>
Date:   Thu Oct 12 12:40:10 2017 +0200

    bitops: Introduce assign_bit()
    
    A common idiom is to assign a value to a bit with:
    
        if (value)
            set_bit(nr, addr);
        else
            clear_bit(nr, addr);
    
    Likewise common is the one-line expression variant:
    
        value ? set_bit(nr, addr) : clear_bit(nr, addr);
    
    Commit 9a8ac3ae682e ("dm mpath: cleanup QUEUE_IF_NO_PATH bit
    manipulation by introducing assign_bit()") introduced assign_bit()
    to the md subsystem for brevity.
    
    Make it available to others, specifically gpiolib and the upcoming
    driver for Maxim MAX3191x industrial serializer chips.
    
    As requested by Peter Zijlstra, change the argument order to reflect
    traditional "dst = src" in C, hence "assign_bit(nr, addr, value)".
    
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Cc: Neil Brown <neilb@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Lukas Wunner <lukas@wunner.de>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 8fbe259b197c..9a874deee6e2 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -227,6 +227,30 @@ static inline unsigned long __ffs64(u64 word)
 	return __ffs((unsigned long)word);
 }
 
+/**
+ * assign_bit - Assign value to a bit in memory
+ * @nr: the bit to set
+ * @addr: the address to start counting from
+ * @value: the value to assign
+ */
+static __always_inline void assign_bit(long nr, volatile unsigned long *addr,
+				       bool value)
+{
+	if (value)
+		set_bit(nr, addr);
+	else
+		clear_bit(nr, addr);
+}
+
+static __always_inline void __assign_bit(long nr, volatile unsigned long *addr,
+					 bool value)
+{
+	if (value)
+		__set_bit(nr, addr);
+	else
+		__clear_bit(nr, addr);
+}
+
 #ifdef __KERNEL__
 
 #ifndef set_mask_bits

commit c32ee3d9abd284b4fcaacc250b101f93829c7bae
Author: Matthias Kaehlcke <mka@chromium.org>
Date:   Fri Sep 8 16:14:33 2017 -0700

    bitops: avoid integer overflow in GENMASK(_ULL)
    
    GENMASK(_ULL) performs a left-shift of ~0UL(L), which technically
    results in an integer overflow.  clang raises a warning if the overflow
    occurs in a preprocessor expression.  Clear the low-order bits through a
    substraction instead of the left-shift to avoid the overflow.
    
    (akpm: no change in .text size in my testing)
    
    Link: http://lkml.kernel.org/r/20170803212020.24939-1-mka@chromium.org
    Signed-off-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a83c822c35c2..8fbe259b197c 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -19,10 +19,11 @@
  * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
  */
 #define GENMASK(h, l) \
-	(((~0UL) << (l)) & (~0UL >> (BITS_PER_LONG - 1 - (h))))
+	(((~0UL) - (1UL << (l)) + 1) & (~0UL >> (BITS_PER_LONG - 1 - (h))))
 
 #define GENMASK_ULL(h, l) \
-	(((~0ULL) << (l)) & (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
+	(((~0ULL) - (1ULL << (l)) + 1) & \
+	 (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
 
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);

commit 252e5c6e2e5b4557599ef86ea5d02b0395e9056c
Author: zijun_hu <zijun_hu@htc.com>
Date:   Fri Oct 7 16:57:26 2016 -0700

    mm/vmalloc.c: fix align value calculation error
    
    It causes double align requirement for __get_vm_area_node() if parameter
    size is power of 2 and VM_IOREMAP is set in parameter flags, for example
    size=0x10000 -> fls_long(0x10000)=17 -> align=0x20000
    
    get_count_order_long() is implemented and can be used instead of
    fls_long() for fixing the bug, for example size=0x10000 ->
    get_count_order_long(0x10000)=16 -> align=0x10000
    
    [akpm@linux-foundation.org: s/get_order_long()/get_count_order_long()/]
    [zijun_hu@zoho.com: fixes]
     Link: http://lkml.kernel.org/r/57AABC8B.1040409@zoho.com
    [akpm@linux-foundation.org: locate get_count_order_long() next to get_count_order()]
    [akpm@linux-foundation.org: move get_count_order[_long] definitions to pick up fls_long()]
    [zijun_hu@htc.com: move out get_count_order[_long]() from __KERNEL__ scope]
     Link: http://lkml.kernel.org/r/57B2C4CE.80303@zoho.com
    Link: http://lkml.kernel.org/r/fc045ecf-20fa-0722-b3ac-9a6140488fad@zoho.com
    Signed-off-by: zijun_hu <zijun_hu@htc.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: zijun_hu <zijun_hu@htc.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 299e76b59fe9..a83c822c35c2 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -65,16 +65,6 @@ static inline int get_bitmask_order(unsigned int count)
 	return order;	/* We could be slightly more clever with -1 here... */
 }
 
-static inline int get_count_order(unsigned int count)
-{
-	int order;
-
-	order = fls(count) - 1;
-	if (count & (count - 1))
-		order++;
-	return order;
-}
-
 static __always_inline unsigned long hweight_long(unsigned long w)
 {
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
@@ -191,6 +181,32 @@ static inline unsigned fls_long(unsigned long l)
 	return fls64(l);
 }
 
+static inline int get_count_order(unsigned int count)
+{
+	int order;
+
+	order = fls(count) - 1;
+	if (count & (count - 1))
+		order++;
+	return order;
+}
+
+/**
+ * get_count_order_long - get order after rounding @l up to power of 2
+ * @l: parameter
+ *
+ * it is same as get_count_order() but with long type parameter
+ */
+static inline int get_count_order_long(unsigned long l)
+{
+	if (l == 0UL)
+		return -1;
+	else if (l & (l - 1UL))
+		return (int)fls_long(l);
+	else
+		return (int)fls_long(l) - 1;
+}
+
 /**
  * __ffs64 - find first set bit in a 64 bit word
  * @word: The 64 bit word

commit 85ad1d13ee9b3db00615ea24b031c15e5ba14fd1
Author: Guoqing Jiang <gqjiang@suse.com>
Date:   Tue May 3 22:22:13 2016 -0400

    md: set MD_CHANGE_PENDING in a atomic region
    
    Some code waits for a metadata update by:
    
    1. flagging that it is needed (MD_CHANGE_DEVS or MD_CHANGE_CLEAN)
    2. setting MD_CHANGE_PENDING and waking the management thread
    3. waiting for MD_CHANGE_PENDING to be cleared
    
    If the first two are done without locking, the code in md_update_sb()
    which checks if it needs to repeat might test if an update is needed
    before step 1, then clear MD_CHANGE_PENDING after step 2, resulting
    in the wait returning early.
    
    So make sure all places that set MD_CHANGE_PENDING are atomicial, and
    bit_clear_unless (suggested by Neil) is introduced for the purpose.
    
    Cc: Martin Kepplinger <martink@posteo.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: <linux-kernel@vger.kernel.org>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Guoqing Jiang <gqjiang@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index defeaac0745f..299e76b59fe9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -227,6 +227,22 @@ static inline unsigned long __ffs64(u64 word)
 })
 #endif
 
+#ifndef bit_clear_unless
+#define bit_clear_unless(ptr, _clear, _test)	\
+({								\
+	const typeof(*ptr) clear = (_clear), test = (_test);	\
+	typeof(*ptr) old, new;					\
+								\
+	do {							\
+		old = ACCESS_ONCE(*ptr);			\
+		new = old & ~clear;				\
+	} while (!(old & test) &&				\
+		 cmpxchg(ptr, old, new) != old);		\
+								\
+	!(old & test);						\
+})
+#endif
+
 #ifndef find_last_bit
 /**
  * find_last_bit - find the last set bit in a memory region

commit d7e35dfa2531b53618b9e6edcd8752ce988ac555
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Thu Dec 3 22:04:01 2015 -0500

    bitops.h: correctly handle rol32 with 0 byte shift
    
    ROL on a 32 bit integer with a shift of 32 or more is undefined and the
    result is arch-dependent. Avoid this by handling the trivial case of
    roling by 0 correctly.
    
    The trivial solution of checking if shift is 0 breaks gcc's detection
    of this code as a ROL instruction, which is unacceptable.
    
    This bug was reported and fixed in GCC
    (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=57157):
    
            The standard rotate idiom,
    
              (x << n) | (x >> (32 - n))
    
            is recognized by gcc (for concreteness, I discuss only the case that x
            is an uint32_t here).
    
            However, this is portable C only for n in the range 0 < n < 32. For n
            == 0, we get x >> 32 which gives undefined behaviour according to the
            C standard (6.5.7, Bitwise shift operators). To portably support n ==
            0, one has to write the rotate as something like
    
              (x << n) | (x >> ((-n) & 31))
    
            And this is apparently not recognized by gcc.
    
    Note that this is broken on older GCCs and will result in slower ROL.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 2b8ed123ad36..defeaac0745f 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -107,7 +107,7 @@ static inline __u64 ror64(__u64 word, unsigned int shift)
  */
 static inline __u32 rol32(__u32 word, unsigned int shift)
 {
-	return (word << shift) | (word >> (32 - shift));
+	return (word << shift) | (word >> ((-shift) & 31));
 }
 
 /**

commit 48e203e21b29cd4b2c58403fe8bca68e2e854895
Author: Martin Kepplinger <martink@posteo.de>
Date:   Fri Nov 6 16:31:02 2015 -0800

    bitops.h: add sign_extend64()
    
    Months back, this was discussed, see https://lkml.org/lkml/2015/1/18/289
    The result was the 64-bit version being "likely fine", "valuable" and
    "correct".  The discussion fell asleep but since there are possible users,
    let's add it.
    
    Signed-off-by: Martin Kepplinger <martin.kepplinger@theobroma-systems.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: George Spelvin <linux@horizon.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Maxime Coquelin <maxime.coquelin@st.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Yury Norov <yury.norov@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 5629923a8701..2b8ed123ad36 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -173,6 +173,17 @@ static inline __s32 sign_extend32(__u32 value, int index)
 	return (__s32)(value << shift) >> shift;
 }
 
+/**
+ * sign_extend64 - sign extend a 64-bit value using specified bit as sign-bit
+ * @value: value to sign extend
+ * @index: 0 based bit index (0<=index<64) to sign bit
+ */
+static inline __s64 sign_extend64(__u64 value, int index)
+{
+	__u8 shift = 63 - index;
+	return (__s64)(value << shift) >> shift;
+}
+
 static inline unsigned fls_long(unsigned long l)
 {
 	if (sizeof(l) == 4)

commit e2eb53aa96754b97d158eff884dde88abbad925e
Author: Martin Kepplinger <martink@posteo.de>
Date:   Fri Nov 6 16:30:58 2015 -0800

    bitops.h: improve sign_extend32()'s documentation
    
    It is often overlooked that sign_extend32(), despite its name, is safe to
    use for 16 and 8 bit types as well.  This should help prevent sign
    extension being done manually some other way.
    
    Signed-off-by: Martin Kepplinger <martin.kepplinger@theobroma-systems.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: George Spelvin <linux@horizon.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Maxime Coquelin <maxime.coquelin@st.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Yury Norov <yury.norov@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index e63553386ae7..5629923a8701 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -164,6 +164,8 @@ static inline __u8 ror8(__u8 word, unsigned int shift)
  * sign_extend32 - sign extend a 32-bit value using specified bit as sign-bit
  * @value: value to sign extend
  * @index: 0 based bit index (0<=index<32) to sign bit
+ *
+ * This is safe to use for 16- and 8-bit types as well.
  */
 static inline __s32 sign_extend32(__u32 value, int index)
 {

commit 1a1d48a4a8fde49aedc045d894efe67173d59fe0
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Tue Aug 4 16:15:14 2015 +0200

    linux/bitmap: Force inlining of bitmap weight functions
    
    With this config:
    
      http://busybox.net/~vda/kernel_config_OPTIMIZE_INLINING_and_Os
    
    gcc-4.7.2 generates many copies of these tiny functions:
    
            bitmap_weight (55 copies):
            55                      push   %rbp
            48 89 e5                mov    %rsp,%rbp
            e8 3f 3a 8b 00          callq  __bitmap_weight
            5d                      pop    %rbp
            c3                      retq
    
            hweight_long (23 copies):
            55                      push   %rbp
            e8 b5 65 8e 00          callq  __sw_hweight64
            48 89 e5                mov    %rsp,%rbp
            5d                      pop    %rbp
            c3                      retq
    
    See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66122
    
    This patch fixes this via s/inline/__always_inline/
    
    While at it, replaced two "__inline__" with usual "inline"
    (the rest of the source file uses the latter).
    
                text     data      bss       dec  filename
            86971357 17195880 36659200 140826437  vmlinux.before
            86971120 17195912 36659200 140826232  vmlinux
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Graf <tgraf@suug.ch>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1438697716-28121-1-git-send-email-dvlasenk@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 297f5bda4fdf..e63553386ae7 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -57,7 +57,7 @@ extern unsigned long __sw_hweight64(__u64 w);
 	     (bit) < (size);					\
 	     (bit) = find_next_zero_bit((addr), (size), (bit) + 1))
 
-static __inline__ int get_bitmask_order(unsigned int count)
+static inline int get_bitmask_order(unsigned int count)
 {
 	int order;
 
@@ -65,7 +65,7 @@ static __inline__ int get_bitmask_order(unsigned int count)
 	return order;	/* We could be slightly more clever with -1 here... */
 }
 
-static __inline__ int get_count_order(unsigned int count)
+static inline int get_count_order(unsigned int count)
 {
 	int order;
 
@@ -75,7 +75,7 @@ static __inline__ int get_count_order(unsigned int count)
 	return order;
 }
 
-static inline unsigned long hweight_long(unsigned long w)
+static __always_inline unsigned long hweight_long(unsigned long w)
 {
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }

commit 2c57a0e233d72f8c2e2404560dcf0188ac3cf5d7
Author: Yury Norov <yury.norov@gmail.com>
Date:   Thu Apr 16 12:43:13 2015 -0700

    lib: find_*_bit reimplementation
    
    This patchset does rework to find_bit function family to achieve better
    performance, and decrease size of text.  All rework is done in patch 1.
    Patches 2 and 3 are about code moving and renaming.
    
    It was boot-tested on x86_64 and MIPS (big-endian) machines.
    Performance tests were ran on userspace with code like this:
    
            /* addr[] is filled from /dev/urandom */
            start = clock();
            while (ret < nbits)
                    ret = find_next_bit(addr, nbits, ret + 1);
    
            end = clock();
            printf("%ld\t", (unsigned long) end - start);
    
    On Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz measurements are: (for
    find_next_bit, nbits is 8M, for find_first_bit - 80K)
    
            find_next_bit:          find_first_bit:
            new     current         new     current
            26932   43151           14777   14925
            26947   43182           14521   15423
            26507   43824           15053   14705
            27329   43759           14473   14777
            26895   43367           14847   15023
            26990   43693           15103   15163
            26775   43299           15067   15232
            27282   42752           14544   15121
            27504   43088           14644   14858
            26761   43856           14699   15193
            26692   43075           14781   14681
            27137   42969           14451   15061
            ...                     ...
    
    find_next_bit performance gain is 35-40%;
    find_first_bit - no measurable difference.
    
    On ARM machine, there is arch-specific implementation for find_bit.
    
    Thanks a lot to George Spelvin and Rasmus Villemoes for hints and
    helpful discussions.
    
    This patch (of 3):
    
    New implementations takes less space in source file (see diffstat) and in
    object.  For me it's 710 vs 453 bytes of text.  It also shows better
    performance.
    
    find_last_bit description fixed due to obvious typo.
    
    [akpm@linux-foundation.org: include linux/bitmap.h, per Rasmus]
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Reviewed-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Reviewed-by: George Spelvin <linux@horizon.com>
    Cc: Alexey Klimov <klimov.linux@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Thomas Graf <tgraf@suug.ch>
    Cc: Valentin Rothberg <valentinrothberg@gmail.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 5d858e02997f..297f5bda4fdf 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -218,9 +218,9 @@ static inline unsigned long __ffs64(u64 word)
 /**
  * find_last_bit - find the last set bit in a memory region
  * @addr: The address to start the search at
- * @size: The maximum size to search
+ * @size: The number of bits to search
  *
- * Returns the bit number of the first set bit, or size.
+ * Returns the bit number of the last set bit, or size.
  */
 extern unsigned long find_last_bit(const unsigned long *addr,
 				   unsigned long size);

commit 00b4d9a14125f1e51874def2b9de6092e007412d
Author: Maxime COQUELIN <maxime.coquelin@st.com>
Date:   Thu Nov 6 10:54:19 2014 +0100

    bitops: Fix shift overflow in GENMASK macros
    
    On some 32 bits architectures, including x86, GENMASK(31, 0) returns 0
    instead of the expected ~0UL.
    
    This is the same on some 64 bits architectures with GENMASK_ULL(63, 0).
    
    This is due to an overflow in the shift operand, 1 << 32 for GENMASK,
    1 << 64 for GENMASK_ULL.
    
    Reported-by: Eric Paire <eric.paire@st.com>
    Suggested-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Maxime Coquelin <maxime.coquelin@st.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org> # v3.13+
    Cc: linux@rasmusvillemoes.dk
    Cc: gong.chen@linux.intel.com
    Cc: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Fixes: 10ef6b0dffe4 ("bitops: Introduce a more generic BITMASK macro")
    Link: http://lkml.kernel.org/r/1415267659-10563-1-git-send-email-maxime.coquelin@st.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index be5fd38bd5a0..5d858e02997f 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -18,8 +18,11 @@
  * position @h. For example
  * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
  */
-#define GENMASK(h, l)		(((U32_C(1) << ((h) - (l) + 1)) - 1) << (l))
-#define GENMASK_ULL(h, l)	(((U64_C(1) << ((h) - (l) + 1)) - 1) << (l))
+#define GENMASK(h, l) \
+	(((~0UL) << (l)) & (~0UL >> (BITS_PER_LONG - 1 - (h))))
+
+#define GENMASK_ULL(h, l) \
+	(((~0ULL) << (l)) & (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
 
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);

commit 2e39465abc4b7856a0ea6fcf4f6b4668bb5db877
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 4 12:07:15 2014 +0200

    locking: Remove deprecated smp_mb__() barriers
    
    Its been a while and there are no in-tree users left, so remove the
    deprecated barriers.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chen, Gong <gong.chen@linux.intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index cbc5833fb221..be5fd38bd5a0 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -32,26 +32,6 @@ extern unsigned long __sw_hweight64(__u64 w);
  */
 #include <asm/bitops.h>
 
-/*
- * Provide __deprecated wrappers for the new interface, avoid flag day changes.
- * We need the ugly external functions to break header recursion hell.
- */
-#ifndef smp_mb__before_clear_bit
-static inline void __deprecated smp_mb__before_clear_bit(void)
-{
-	extern void __smp_mb__before_atomic(void);
-	__smp_mb__before_atomic();
-}
-#endif
-
-#ifndef smp_mb__after_clear_bit
-static inline void __deprecated smp_mb__after_clear_bit(void)
-{
-	extern void __smp_mb__after_atomic(void);
-	__smp_mb__after_atomic();
-}
-#endif
-
 #define for_each_set_bit(bit, addr, size) \
 	for ((bit) = find_first_bit((addr), (size));		\
 	     (bit) < (size);					\

commit febdbfe8a91ce0d11939d4940b592eb0dba8d663
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 6 18:16:07 2014 +0100

    arch: Prepare for smp_mb__{before,after}_atomic()
    
    Since the smp_mb__{before,after}*() ops are fundamentally dependent on
    how an arch can implement atomics it doesn't make sense to have 3
    variants of them. They must all be the same.
    
    Furthermore, the 3 variants suggest they're only valid for those 3
    atomic ops, while we have many more where they could be applied.
    
    So move away from
    smp_mb__{before,after}_{atomic,clear}_{dec,inc,bit}() and reduce the
    interface to just the two: smp_mb__{before,after}_atomic().
    
    This patch prepares the way by introducing default implementations in
    asm-generic/barrier.h that default to a full barrier and providing
    __deprecated inlines for the previous 6 barriers if they're not
    provided by the arch.
    
    This should allow for a mostly painless transition (lots of deprecated
    warns in the interim).
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-wr59327qdyi9mbzn6x937s4e@git.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Chen, Gong" <gong.chen@linux.intel.com>
    Cc: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mauro Carvalho Chehab <m.chehab@samsung.com>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index be5fd38bd5a0..cbc5833fb221 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -32,6 +32,26 @@ extern unsigned long __sw_hweight64(__u64 w);
  */
 #include <asm/bitops.h>
 
+/*
+ * Provide __deprecated wrappers for the new interface, avoid flag day changes.
+ * We need the ugly external functions to break header recursion hell.
+ */
+#ifndef smp_mb__before_clear_bit
+static inline void __deprecated smp_mb__before_clear_bit(void)
+{
+	extern void __smp_mb__before_atomic(void);
+	__smp_mb__before_atomic();
+}
+#endif
+
+#ifndef smp_mb__after_clear_bit
+static inline void __deprecated smp_mb__after_clear_bit(void)
+{
+	extern void __smp_mb__after_atomic(void);
+	__smp_mb__after_atomic();
+}
+#endif
+
 #define for_each_set_bit(bit, addr, size) \
 	for ((bit) = find_first_bit((addr), (size));		\
 	     (bit) < (size);					\

commit 00a1a053ebe5febcfc2ec498bd894f035ad2aa06
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Sun Mar 30 10:20:01 2014 -0400

    ext4: atomically set inode->i_flags in ext4_set_inode_flags()
    
    Use cmpxchg() to atomically set i_flags instead of clearing out the
    S_IMMUTABLE, S_APPEND, etc. flags and then setting them from the
    EXT4_IMMUTABLE_FL, EXT4_APPEND_FL flags, since this opens up a race
    where an immutable file has the immutable flag cleared for a brief
    window of time.
    
    Reported-by: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index abc9ca778456..be5fd38bd5a0 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -196,6 +196,21 @@ static inline unsigned long __ffs64(u64 word)
 
 #ifdef __KERNEL__
 
+#ifndef set_mask_bits
+#define set_mask_bits(ptr, _mask, _bits)	\
+({								\
+	const typeof(*ptr) mask = (_mask), bits = (_bits);	\
+	typeof(*ptr) old, new;					\
+								\
+	do {							\
+		old = ACCESS_ONCE(*ptr);			\
+		new = (old & ~mask) | bits;			\
+	} while (cmpxchg(ptr, old, new) != old);		\
+								\
+	new;							\
+})
+#endif
+
 #ifndef find_last_bit
 /**
  * find_last_bit - find the last set bit in a memory region

commit f9300eaaac1ca300083ad41937923a90cc3a2394
Merge: 7f2dc5c4bcbf faddf2f5d278
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 14 13:41:48 2013 +0900

    Merge tag 'pm+acpi-3.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management updates from Rafael J Wysocki:
    
     - New power capping framework and the the Intel Running Average Power
       Limit (RAPL) driver using it from Srinivas Pandruvada and Jacob Pan.
    
     - Addition of the in-kernel switching feature to the arm_big_little
       cpufreq driver from Viresh Kumar and Nicolas Pitre.
    
     - cpufreq support for iMac G5 from Aaro Koskinen.
    
     - Baytrail processors support for intel_pstate from Dirk Brandewie.
    
     - cpufreq support for Midway/ECX-2000 from Mark Langsdorf.
    
     - ARM vexpress/TC2 cpufreq support from Sudeep KarkadaNagesha.
    
     - ACPI power management support for the I2C and SPI bus types from Mika
       Westerberg and Lv Zheng.
    
     - cpufreq core fixes and cleanups from Viresh Kumar, Srivatsa S Bhat,
       Stratos Karafotis, Xiaoguang Chen, Lan Tianyu.
    
     - cpufreq drivers updates (mostly fixes and cleanups) from Viresh
       Kumar, Aaro Koskinen, Jungseok Lee, Sudeep KarkadaNagesha, Lukasz
       Majewski, Manish Badarkhe, Hans-Christian Egtvedt, Evgeny Kapaev.
    
     - intel_pstate updates from Dirk Brandewie and Adrian Huang.
    
     - ACPICA update to version 20130927 includig fixes and cleanups and
       some reduction of divergences between the ACPICA code in the kernel
       and ACPICA upstream in order to improve the automatic ACPICA patch
       generation process.  From Bob Moore, Lv Zheng, Tomasz Nowicki, Naresh
       Bhat, Bjorn Helgaas, David E Box.
    
     - ACPI IPMI driver fixes and cleanups from Lv Zheng.
    
     - ACPI hotplug fixes and cleanups from Bjorn Helgaas, Toshi Kani, Zhang
       Yanfei, Rafael J Wysocki.
    
     - Conversion of the ACPI AC driver to the platform bus type and
       multiple driver fixes and cleanups related to ACPI from Zhang Rui.
    
     - ACPI processor driver fixes and cleanups from Hanjun Guo, Jiang Liu,
       Bartlomiej Zolnierkiewicz, Mathieu Rhéaume, Rafael J Wysocki.
    
     - Fixes and cleanups and new blacklist entries related to the ACPI
       video support from Aaron Lu, Felipe Contreras, Lennart Poettering,
       Kirill Tkhai.
    
     - cpuidle core cleanups from Viresh Kumar and Lorenzo Pieralisi.
    
     - cpuidle drivers fixes and cleanups from Daniel Lezcano, Jingoo Han,
       Bartlomiej Zolnierkiewicz, Prarit Bhargava.
    
     - devfreq updates from Sachin Kamat, Dan Carpenter, Manish Badarkhe.
    
     - Operation Performance Points (OPP) core updates from Nishanth Menon.
    
     - Runtime power management core fix from Rafael J Wysocki and update
       from Ulf Hansson.
    
     - Hibernation fixes from Aaron Lu and Rafael J Wysocki.
    
     - Device suspend/resume lockup detection mechanism from Benoit Goby.
    
     - Removal of unused proc directories created for various ACPI drivers
       from Lan Tianyu.
    
     - ACPI LPSS driver fix and new device IDs for the ACPI platform scan
       handler from Heikki Krogerus and Jarkko Nikula.
    
     - New ACPI _OSI blacklist entry for Toshiba NB100 from Levente Kurusa.
    
     - Assorted fixes and cleanups related to ACPI from Andy Shevchenko, Al
       Stone, Bartlomiej Zolnierkiewicz, Colin Ian King, Dan Carpenter,
       Felipe Contreras, Jianguo Wu, Lan Tianyu, Yinghai Lu, Mathias Krause,
       Liu Chuansheng.
    
     - Assorted PM fixes and cleanups from Andy Shevchenko, Thierry Reding,
       Jean-Christophe Plagniol-Villard.
    
    * tag 'pm+acpi-3.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (386 commits)
      cpufreq: conservative: fix requested_freq reduction issue
      ACPI / hotplug: Consolidate deferred execution of ACPI hotplug routines
      PM / runtime: Use pm_runtime_put_sync() in __device_release_driver()
      ACPI / event: remove unneeded NULL pointer check
      Revert "ACPI / video: Ignore BIOS initial backlight value for HP 250 G1"
      ACPI / video: Quirk initial backlight level 0
      ACPI / video: Fix initial level validity test
      intel_pstate: skip the driver if ACPI has power mgmt option
      PM / hibernate: Avoid overflow in hibernate_preallocate_memory()
      ACPI / hotplug: Do not execute "insert in progress" _OST
      ACPI / hotplug: Carry out PCI root eject directly
      ACPI / hotplug: Merge device hot-removal routines
      ACPI / hotplug: Make acpi_bus_hot_remove_device() internal
      ACPI / hotplug: Simplify device ejection routines
      ACPI / hotplug: Fix handle_root_bridge_removal()
      ACPI / hotplug: Refuse to hot-remove all objects with disabled hotplug
      ACPI / scan: Start matching drivers after trying scan handlers
      ACPI: Remove acpi_pci_slot_init() headers from internal.h
      ACPI / blacklist: fix name of ThinkPad Edge E530
      PowerCap: Fix build error with option -Werror=format-security
      ...
    
    Conflicts:
            arch/arm/mach-omap2/opp.c
            drivers/Kconfig
            drivers/spi/spi.c

commit 10ef6b0dffe404bcc54e94cb2ca1a5b18445a66b
Author: Chen, Gong <gong.chen@linux.intel.com>
Date:   Fri Oct 18 14:29:07 2013 -0700

    bitops: Introduce a more generic BITMASK macro
    
    GENMASK is used to create a contiguous bitmask([hi:lo]). It is
    implemented twice in current kernel. One is in EDAC driver, the other
    is in SiS/XGI FB driver. Move it to a more generic place for other
    usage.
    
    Signed-off-by: Chen, Gong <gong.chen@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Winischhofer <thomas@winischhofer.net>
    Cc: Jean-Christophe Plagniol-Villard <plagnioj@jcrosoft.com>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Mauro Carvalho Chehab <m.chehab@samsung.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a3b6b82108b9..bd0c4598d03b 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -10,6 +10,14 @@
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
 #endif
 
+/*
+ * Create a contiguous bitmask starting at bit position @l and ending at
+ * position @h. For example
+ * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
+ */
+#define GENMASK(h, l)		(((U32_C(1) << ((h) - (l) + 1)) - 1) << (l))
+#define GENMASK_ULL(h, l)	(((U64_C(1) << ((h) - (l) + 1)) - 1) << (l))
+
 extern unsigned int __sw_hweight8(unsigned int w);
 extern unsigned int __sw_hweight16(unsigned int w);
 extern unsigned int __sw_hweight32(unsigned int w);

commit bfd1ff6375c82930bfb3b401eee2c96720fa8e84
Author: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
Date:   Fri Oct 11 16:54:59 2013 -0700

    bitops: Introduce BIT_ULL
    
    Adding BIT(x) equivalent for unsigned long long type, BIT_ULL(x). Also
    added BIT_ULL_MASK and BIT_ULL_WORD.
    
    Suggested-by: Joe Perches <joe@perches.com>
    Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a3b6b82108b9..5a1c8b71ccd8 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -4,8 +4,11 @@
 
 #ifdef	__KERNEL__
 #define BIT(nr)			(1UL << (nr))
+#define BIT_ULL(nr)		(1ULL << (nr))
 #define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
 #define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
+#define BIT_ULL_MASK(nr)	(1ULL << ((nr) % BITS_PER_LONG_LONG))
+#define BIT_ULL_WORD(nr)	((nr) / BITS_PER_LONG_LONG)
 #define BITS_PER_BYTE		8
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
 #endif

commit 03f4a8226c2f9c14361f75848d1e93139bab90c4
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Mar 23 15:02:04 2012 -0700

    bitops: introduce for_each_clear_bit()
    
    Introduce for_each_clear_bit() and for_each_clear_bit_from().  They are
    similar to for_each_set_bit() and list_for_each_set_bit_from(), but they
    iterate over all the cleared bits in a memory region.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Stefano Panella <stefano.panella@csr.com>
    Cc: David Vrabel <david.vrabel@csr.com>
    Cc: Sergei Shtylyov <sshtylyov@mvista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 348b1dca477a..a3b6b82108b9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -32,6 +32,17 @@ extern unsigned long __sw_hweight64(__u64 w);
 	     (bit) < (size);					\
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
+#define for_each_clear_bit(bit, addr, size) \
+	for ((bit) = find_first_zero_bit((addr), (size));	\
+	     (bit) < (size);					\
+	     (bit) = find_next_zero_bit((addr), (size), (bit) + 1))
+
+/* same as for_each_clear_bit() but use bit as value to start with */
+#define for_each_clear_bit_from(bit, addr, size) \
+	for ((bit) = find_next_zero_bit((addr), (size), (bit));	\
+	     (bit) < (size);					\
+	     (bit) = find_next_zero_bit((addr), (size), (bit) + 1))
+
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit 0a329d2d5a1dd75273597538cdc33512ee38855e
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Mar 23 15:02:04 2012 -0700

    bitops: remove for_each_set_bit_cont()
    
    Remove for_each_set_bit_cont() after confirming that no one uses
    for_each_set_bit_cont() anymore.
    
    [sfr@canb.auug.org.au: regmap: cope with bitops API change]
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a78e358f0c17..348b1dca477a 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -32,9 +32,6 @@ extern unsigned long __sw_hweight64(__u64 w);
 	     (bit) < (size);					\
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
-#define for_each_set_bit_cont(bit, addr, size) \
-	for_each_set_bit_from(bit, addr, size)
-
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit 307b1cd7ecd7f3dc5ce3d3860957f034f0abe4df
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Mar 23 15:02:03 2012 -0700

    bitops: rename for_each_set_bit_cont() in favor of analogous list.h function
    
    This renames for_each_set_bit_cont() to for_each_set_bit_from() because
    it is analogous to list_for_each_entry_from() in list.h rather than
    list_for_each_entry_continue().
    
    This doesn't remove for_each_set_bit_cont() for now.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 94300fe46cce..a78e358f0c17 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -27,11 +27,14 @@ extern unsigned long __sw_hweight64(__u64 w);
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
 /* same as for_each_set_bit() but use bit as value to start with */
-#define for_each_set_bit_cont(bit, addr, size) \
+#define for_each_set_bit_from(bit, addr, size) \
 	for ((bit) = find_next_bit((addr), (size), (bit));	\
 	     (bit) < (size);					\
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
+#define for_each_set_bit_cont(bit, addr, size) \
+	for_each_set_bit_from(bit, addr, size)
+
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit a18d3afefa0104419b5e069af5922bb57a302426
Merge: 34ddc81a230b f2ea0f5f04c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 18 15:24:05 2012 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6:
      crypto: sha512 - use standard ror64()

commit f2ea0f5f04c97b48c88edccba52b0682fbe45087
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Jan 14 21:44:49 2012 +0300

    crypto: sha512 - use standard ror64()
    
    Use standard ror64() instead of hand-written.
    There is no standard ror64, so create it.
    
    The difference is shift value being "unsigned int" instead of uint64_t
    (for which there is no reason). gcc starts to emit native ROR instructions
    which it doesn't do for some reason currently. This should make the code
    faster.
    
    Patch survives in-tree crypto test and ping flood with hmac(sha512) on.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a3ef66a2a083..fc8a3ffce320 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -49,6 +49,26 @@ static inline unsigned long hweight_long(unsigned long w)
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
+/**
+ * rol64 - rotate a 64-bit value left
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u64 rol64(__u64 word, unsigned int shift)
+{
+	return (word << shift) | (word >> (64 - shift));
+}
+
+/**
+ * ror64 - rotate a 64-bit value right
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u64 ror64(__u64 word, unsigned int shift)
+{
+	return (word >> shift) | (word << (64 - shift));
+}
+
 /**
  * rol32 - rotate a 32-bit value left
  * @word: value to rotate

commit 1e2ad28f80b4e155678259238f51edebc19e4014
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Nov 18 12:35:21 2011 +0100

    perf, x86: Implement event scheduler helper functions
    
    This patch introduces x86 perf scheduler code helper functions. We
    need this to later add more complex functionality to support
    overlapping counter constraints (next patch).
    
    The algorithm is modified so that the range of weight values is now
    generated from the constraints. There shouldn't be other functional
    changes.
    
    With the helper functions the scheduler is controlled. There are
    functions to initialize, traverse the event list, find unused counters
    etc. The scheduler keeps its own state.
    
    V3:
    * Added macro for_each_set_bit_cont().
    * Changed functions interfaces of perf_sched_find_counter() and
      perf_sched_next_event() to use bool as return value.
    * Added some comments to make code better understandable.
    
    V4:
    * Fix broken event assignment if weight of the first event is not
      wmin (perf_sched_init()).
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1321616122-1533-2-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index a3ef66a2a083..3c1063acb2ab 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -22,8 +22,14 @@ extern unsigned long __sw_hweight64(__u64 w);
 #include <asm/bitops.h>
 
 #define for_each_set_bit(bit, addr, size) \
-	for ((bit) = find_first_bit((addr), (size)); \
-	     (bit) < (size); \
+	for ((bit) = find_first_bit((addr), (size));		\
+	     (bit) < (size);					\
+	     (bit) = find_next_bit((addr), (size), (bit) + 1))
+
+/* same as for_each_set_bit() but use bit as value to start with */
+#define for_each_set_bit_cont(bit, addr, size) \
+	for ((bit) = find_next_bit((addr), (size), (bit));	\
+	     (bit) < (size);					\
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
 static __inline__ int get_bitmask_order(unsigned int count)

commit 63e424c84429903c92a0f1e9654c31ccaf6694d0
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Thu May 26 16:26:10 2011 -0700

    arch: remove CONFIG_GENERIC_FIND_{NEXT_BIT,BIT_LE,LAST_BIT}
    
    By the previous style change, CONFIG_GENERIC_FIND_NEXT_BIT,
    CONFIG_GENERIC_FIND_BIT_LE, and CONFIG_GENERIC_FIND_LAST_BIT are not used
    to test for existence of find bitops anymore.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Acked-by: Greg Ungerer <gerg@uclinux.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 4829252d7cfa..a3ef66a2a083 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -148,7 +148,6 @@ static inline unsigned long __ffs64(u64 word)
 
 #ifdef __KERNEL__
 
-#ifdef CONFIG_GENERIC_FIND_LAST_BIT
 #ifndef find_last_bit
 /**
  * find_last_bit - find the last set bit in a memory region
@@ -160,7 +159,6 @@ static inline unsigned long __ffs64(u64 word)
 extern unsigned long find_last_bit(const unsigned long *addr,
 				   unsigned long size);
 #endif
-#endif /* CONFIG_GENERIC_FIND_LAST_BIT */
 
 #endif /* __KERNEL__ */
 #endif

commit 19de85ef574c3a2182e3ccad9581805052f14946
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Thu May 26 16:26:09 2011 -0700

    bitops: add #ifndef for each of find bitops
    
    The style that we normally use in asm-generic is to test the macro itself
    for existence, so in asm-generic, do:
    
            #ifndef find_next_zero_bit_le
            extern unsigned long find_next_zero_bit_le(const void *addr,
                    unsigned long size, unsigned long offset);
            #endif
    
    and in the architectures, write
    
            static inline unsigned long find_next_zero_bit_le(const void *addr,
                    unsigned long size, unsigned long offset)
            #define find_next_zero_bit_le find_next_zero_bit_le
    
    This adds the #ifndef for each of the find bitops in the generic header
    and source files.
    
    Suggested-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Greg Ungerer <gerg@uclinux.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 2184c6b97aeb..4829252d7cfa 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -149,6 +149,7 @@ static inline unsigned long __ffs64(u64 word)
 #ifdef __KERNEL__
 
 #ifdef CONFIG_GENERIC_FIND_LAST_BIT
+#ifndef find_last_bit
 /**
  * find_last_bit - find the last set bit in a memory region
  * @addr: The address to start the search at
@@ -158,6 +159,7 @@ static inline unsigned long __ffs64(u64 word)
  */
 extern unsigned long find_last_bit(const unsigned long *addr,
 				   unsigned long size);
+#endif
 #endif /* CONFIG_GENERIC_FIND_LAST_BIT */
 
 #endif /* __KERNEL__ */

commit 7919a57bc608140aa8614c19eac40c6916fb61d2
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Mon Aug 30 19:04:01 2010 +0000

    bitops: Provide generic sign_extend32 function
    
    This patch moves code out from wireless drivers where two different
    functions are defined in three code locations for the same purpose and
    provides a common function to sign extend a 32-bit value.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 827cc95711ef..2184c6b97aeb 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -109,6 +109,17 @@ static inline __u8 ror8(__u8 word, unsigned int shift)
 	return (word >> shift) | (word << (8 - shift));
 }
 
+/**
+ * sign_extend32 - sign extend a 32-bit value using specified bit as sign-bit
+ * @value: value to sign extend
+ * @index: 0 based bit index (0<=index<32) to sign bit
+ */
+static inline __s32 sign_extend32(__u32 value, int index)
+{
+	__u8 shift = 31 - index;
+	return (__s32)(value << shift) >> shift;
+}
+
 static inline unsigned fls_long(unsigned long l)
 {
 	if (sizeof(l) == 4)

commit d852a6afd91fc928128f59ebff381838c365e358
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Wed Sep 29 18:08:51 2010 +0900

    bitops: remove duplicated extern declarations
    
    If CONFIG_GENERIC_FIND_NEXT_BIT is enabled, find_next_bit() and
    find_next_zero_bit() are doubly declared in asm-generic/bitops/find.h
    and linux/bitops.h.
    
    asm/bitops.h includes asm-generic/bitops/find.h if and only if the
    architecture enables CONFIG_GENERIC_FIND_NEXT_BIT. And asm/bitops.h
    is included by linux/bitops.h
    
    So we can just remove the extern declarations of find_next_bit() and
    find_next_zero_bit() in linux/bitops.h.
    
    Also we can remove unneeded #ifndef CONFIG_GENERIC_FIND_NEXT_BIT in
    asm-generic/bitops/find.h.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index adb0f113f571..827cc95711ef 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -149,28 +149,5 @@ extern unsigned long find_last_bit(const unsigned long *addr,
 				   unsigned long size);
 #endif /* CONFIG_GENERIC_FIND_LAST_BIT */
 
-#ifdef CONFIG_GENERIC_FIND_NEXT_BIT
-
-/**
- * find_next_bit - find the next set bit in a memory region
- * @addr: The address to base the search on
- * @offset: The bitnumber to start searching at
- * @size: The bitmap size in bits
- */
-extern unsigned long find_next_bit(const unsigned long *addr,
-				   unsigned long size, unsigned long offset);
-
-/**
- * find_next_zero_bit - find the next cleared bit in a memory region
- * @addr: The address to base the search on
- * @offset: The bitnumber to start searching at
- * @size: The bitmap size in bits
- */
-
-extern unsigned long find_next_zero_bit(const unsigned long *addr,
-					unsigned long size,
-					unsigned long offset);
-
-#endif /* CONFIG_GENERIC_FIND_NEXT_BIT */
 #endif /* __KERNEL__ */
 #endif

commit 708ff2a0097b02d32d375b66996661f36cd4d6d1
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Wed Sep 29 18:08:50 2010 +0900

    bitops: make asm-generic/bitops/find.h more generic
    
    asm-generic/bitops/find.h has the extern declarations of find_next_bit()
    and find_next_zero_bit() and the macro definitions of find_first_bit()
    and find_first_zero_bit(). It is only usable by the architectures which
    enables CONFIG_GENERIC_FIND_NEXT_BIT and disables
    CONFIG_GENERIC_FIND_FIRST_BIT.
    
    x86 and tile enable both CONFIG_GENERIC_FIND_NEXT_BIT and
    CONFIG_GENERIC_FIND_FIRST_BIT. These architectures cannot include
    asm-generic/bitops/find.h in their asm/bitops.h. So ifdefed extern
    declarations of find_first_bit and find_first_zero_bit() are put in
    linux/bitops.h.
    
    This makes asm-generic/bitops/find.h usable by these architectures
    and use it. Also this change is needed for the forthcoming duplicated
    extern declarations cleanup.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: Chris Metcalf <cmetcalf@tilera.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index fc68053378ce..adb0f113f571 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -136,28 +136,6 @@ static inline unsigned long __ffs64(u64 word)
 }
 
 #ifdef __KERNEL__
-#ifdef CONFIG_GENERIC_FIND_FIRST_BIT
-
-/**
- * find_first_bit - find the first set bit in a memory region
- * @addr: The address to start the search at
- * @size: The maximum size to search
- *
- * Returns the bit number of the first set bit.
- */
-extern unsigned long find_first_bit(const unsigned long *addr,
-				    unsigned long size);
-
-/**
- * find_first_zero_bit - find the first cleared bit in a memory region
- * @addr: The address to start the search at
- * @size: The maximum size to search
- *
- * Returns the bit number of the first cleared bit.
- */
-extern unsigned long find_first_zero_bit(const unsigned long *addr,
-					 unsigned long size);
-#endif /* CONFIG_GENERIC_FIND_FIRST_BIT */
 
 #ifdef CONFIG_GENERIC_FIND_LAST_BIT
 /**

commit cb41838bbc4403f7270a94b93a9a0d9fc9c2e7ea
Merge: 98f01720cbe3 c59bd5688299
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 09:17:01 2010 -0700

    Merge branch 'core-hweight-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-hweight-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, hweight: Use a 32-bit popcnt for __arch_hweight32()
      arch, hweight: Fix compilation errors
      x86: Add optimized popcnt variants
      bitops: Optimize hweight() by making use of compile-time evaluation

commit 4677d4a53e0d565742277e8913e91c821453e63e
Author: Borislav Petkov <borislav.petkov@amd.com>
Date:   Mon May 3 14:57:11 2010 +0200

    arch, hweight: Fix compilation errors
    
    Fix function prototype visibility issues when compiling for non-x86
    architectures. Tested with crosstool
    (ftp://ftp.kernel.org/pub/tools/crosstool/) with alpha, ia64 and sparc
    targets.
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100503130736.GD26107@aftab>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index c55d5bc4ee58..26caa608ccd9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -10,6 +10,11 @@
 #define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
 #endif
 
+extern unsigned int __sw_hweight8(unsigned int w);
+extern unsigned int __sw_hweight16(unsigned int w);
+extern unsigned int __sw_hweight32(unsigned int w);
+extern unsigned long __sw_hweight64(__u64 w);
+
 /*
  * Include this here because some architectures need generic_ffs/fls in
  * scope

commit b01d0942c2b7a3026d2b7d38b5773d3d00420e06
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Apr 6 14:34:41 2010 -0700

    bitops: remove temporary for_each_bit()
    
    Migration has been completed so remove this now.  There's one straggler in
    linux-next's drivers/mtd/sm_ftl.c.  A patch has been sent.
    
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index b79389879238..b796eab5ca75 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -21,9 +21,6 @@
 	     (bit) < (size); \
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
-/* Temporary */
-#define for_each_bit(bit, addr, size) for_each_set_bit(bit, addr, size)
-
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit 1527bc8b928dd1399c3d3467dd47d9ede210978a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Feb 1 15:03:07 2010 +0100

    bitops: Optimize hweight() by making use of compile-time evaluation
    
    Rename the extisting runtime hweight() implementations to
    __arch_hweight(), rename the compile-time versions to __const_hweight()
    and then have hweight() pick between them.
    
    Suggested-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100318111929.GB11152@aftab>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    LKML-Reference: <1265028224.24455.154.camel@laptop>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index b79389879238..c55d5bc4ee58 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -47,31 +47,6 @@ static inline unsigned long hweight_long(unsigned long w)
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
-/*
- * Clearly slow versions of the hweightN() functions, their benefit is
- * of course compile time evaluation of constant arguments.
- */
-#define HWEIGHT8(w)					\
-      (	BUILD_BUG_ON_ZERO(!__builtin_constant_p(w)) +	\
-	(!!((w) & (1ULL << 0))) +			\
-	(!!((w) & (1ULL << 1))) +			\
-	(!!((w) & (1ULL << 2))) +			\
-	(!!((w) & (1ULL << 3))) +			\
-	(!!((w) & (1ULL << 4))) +			\
-	(!!((w) & (1ULL << 5))) +			\
-	(!!((w) & (1ULL << 6))) +			\
-	(!!((w) & (1ULL << 7)))	)
-
-#define HWEIGHT16(w) (HWEIGHT8(w)  + HWEIGHT8((w) >> 8))
-#define HWEIGHT32(w) (HWEIGHT16(w) + HWEIGHT16((w) >> 16))
-#define HWEIGHT64(w) (HWEIGHT32(w) + HWEIGHT32((w) >> 32))
-
-/*
- * Type invariant version that simply casts things to the
- * largest type.
- */
-#define HWEIGHT(w)   HWEIGHT64((u64)(w))
-
 /**
  * rol32 - rotate a 32-bit value left
  * @word: value to rotate

commit 984b3f5746ed2cde3d184651dabf26980f2b66e5
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Mar 5 13:41:37 2010 -0800

    bitops: rename for_each_bit() to for_each_set_bit()
    
    Rename for_each_bit to for_each_set_bit in the kernel source tree.  To
    permit for_each_clear_bit(), should that ever be added.
    
    The patch includes a macro to map the old for_each_bit() onto the new
    for_each_set_bit().  This is a (very) temporary thing to ease the migration.
    
    [akpm@linux-foundation.org: add temporary for_each_bit()]
    Suggested-by: Alexey Dobriyan <adobriyan@gmail.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Artem Bityutskiy <dedekind@infradead.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 25b8b2f33ae9..b79389879238 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -16,11 +16,13 @@
  */
 #include <asm/bitops.h>
 
-#define for_each_bit(bit, addr, size) \
+#define for_each_set_bit(bit, addr, size) \
 	for ((bit) = find_first_bit((addr), (size)); \
 	     (bit) < (size); \
 	     (bit) = find_next_bit((addr), (size), (bit) + 1))
 
+/* Temporary */
+#define for_each_bit(bit, addr, size) for_each_set_bit(bit, addr, size)
 
 static __inline__ int get_bitmask_order(unsigned int count)
 {

commit fce877e3a429940a986e085a41e8b57f2d922e36
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 29 13:25:12 2010 +0100

    bitops: Ensure the compile time HWEIGHT is only used for such
    
    Avoid accidental misuse by failing to compile things
    
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index ba0fd1eb4af7..25b8b2f33ae9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -45,19 +45,30 @@ static inline unsigned long hweight_long(unsigned long w)
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
-#define HWEIGHT8(w)			\
-      (	(!!((w) & (1ULL << 0))) +	\
-	(!!((w) & (1ULL << 1))) +	\
-	(!!((w) & (1ULL << 2))) +	\
-	(!!((w) & (1ULL << 3))) +	\
-	(!!((w) & (1ULL << 4))) +	\
-	(!!((w) & (1ULL << 5))) +	\
-	(!!((w) & (1ULL << 6))) +	\
+/*
+ * Clearly slow versions of the hweightN() functions, their benefit is
+ * of course compile time evaluation of constant arguments.
+ */
+#define HWEIGHT8(w)					\
+      (	BUILD_BUG_ON_ZERO(!__builtin_constant_p(w)) +	\
+	(!!((w) & (1ULL << 0))) +			\
+	(!!((w) & (1ULL << 1))) +			\
+	(!!((w) & (1ULL << 2))) +			\
+	(!!((w) & (1ULL << 3))) +			\
+	(!!((w) & (1ULL << 4))) +			\
+	(!!((w) & (1ULL << 5))) +			\
+	(!!((w) & (1ULL << 6))) +			\
 	(!!((w) & (1ULL << 7)))	)
 
-#define HWEIGHT16(w) (HWEIGHT8(w)  + HWEIGHT8(w >> 8))
-#define HWEIGHT32(w) (HWEIGHT16(w) + HWEIGHT16(w >> 16))
-#define HWEIGHT64(w) (HWEIGHT32(w) + HWEIGHT32(w >> 32))
+#define HWEIGHT16(w) (HWEIGHT8(w)  + HWEIGHT8((w) >> 8))
+#define HWEIGHT32(w) (HWEIGHT16(w) + HWEIGHT16((w) >> 16))
+#define HWEIGHT64(w) (HWEIGHT32(w) + HWEIGHT32((w) >> 32))
+
+/*
+ * Type invariant version that simply casts things to the
+ * largest type.
+ */
+#define HWEIGHT(w)   HWEIGHT64((u64)(w))
 
 /**
  * rol32 - rotate a 32-bit value left

commit 9f41699ed067fa695faff8e2e9981b2550abec62
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 22 15:59:29 2010 +0100

    bitops: Provide compile time HWEIGHT{8,16,32,64}
    
    Provide compile time versions of hweight.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <20100122155535.797688466@chello.nl>
    [ Remove some whitespace damage while we are at it ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index c05a29cb9bb2..ba0fd1eb4af7 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -25,7 +25,7 @@
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;
-	
+
 	order = fls(count);
 	return order;	/* We could be slightly more clever with -1 here... */
 }
@@ -33,7 +33,7 @@ static __inline__ int get_bitmask_order(unsigned int count)
 static __inline__ int get_count_order(unsigned int count)
 {
 	int order;
-	
+
 	order = fls(count) - 1;
 	if (count & (count - 1))
 		order++;
@@ -45,6 +45,20 @@ static inline unsigned long hweight_long(unsigned long w)
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
+#define HWEIGHT8(w)			\
+      (	(!!((w) & (1ULL << 0))) +	\
+	(!!((w) & (1ULL << 1))) +	\
+	(!!((w) & (1ULL << 2))) +	\
+	(!!((w) & (1ULL << 3))) +	\
+	(!!((w) & (1ULL << 4))) +	\
+	(!!((w) & (1ULL << 5))) +	\
+	(!!((w) & (1ULL << 6))) +	\
+	(!!((w) & (1ULL << 7)))	)
+
+#define HWEIGHT16(w) (HWEIGHT8(w)  + HWEIGHT8(w >> 8))
+#define HWEIGHT32(w) (HWEIGHT16(w) + HWEIGHT16(w >> 16))
+#define HWEIGHT64(w) (HWEIGHT32(w) + HWEIGHT32(w >> 32))
+
 /**
  * rol32 - rotate a 32-bit value left
  * @word: value to rotate

commit 952043ac12a117d8e94bddd9088338d7ad20ca7d
Author: Steven Whitehouse <swhiteho@redhat.com>
Date:   Thu Apr 23 08:48:15 2009 +0100

    bitops: Add __ffs64 bitop
    
    Finds the first set bit in a 64 bit word. This is required in order
    to fix a bug in GFS2, but I think it should be a generic function
    in case of future users.
    
    Signed-off-by: Steven Whitehouse <swhiteho@redhat.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: Willy Tarreau <w@1wt.eu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 61829139795a..c05a29cb9bb2 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -112,6 +112,25 @@ static inline unsigned fls_long(unsigned long l)
 	return fls64(l);
 }
 
+/**
+ * __ffs64 - find first set bit in a 64 bit word
+ * @word: The 64 bit word
+ *
+ * On 64 bit arches this is a synomyn for __ffs
+ * The result is not defined if no bits are set, so check that @word
+ * is non-zero before calling this.
+ */
+static inline unsigned long __ffs64(u64 word)
+{
+#if BITS_PER_LONG == 32
+	if (((u32)word) == 0UL)
+		return __ffs((u32)(word >> 32)) + 32;
+#elif BITS_PER_LONG != 64
+#error BITS_PER_LONG not 32 or 64
+#endif
+	return __ffs((unsigned long)word);
+}
+
 #ifdef __KERNEL__
 #ifdef CONFIG_GENERIC_FIND_FIRST_BIT
 

commit ab53d472e785e51fdfc08fc1d66252c1153e6c0f
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:19 2009 +1030

    bitmap: find_last_bit()
    
    Impact: New API
    
    As the name suggests.  For the moment everyone uses the generic one.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 024f2b027244..61829139795a 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -134,9 +134,20 @@ extern unsigned long find_first_bit(const unsigned long *addr,
  */
 extern unsigned long find_first_zero_bit(const unsigned long *addr,
 					 unsigned long size);
-
 #endif /* CONFIG_GENERIC_FIND_FIRST_BIT */
 
+#ifdef CONFIG_GENERIC_FIND_LAST_BIT
+/**
+ * find_last_bit - find the last set bit in a memory region
+ * @addr: The address to start the search at
+ * @size: The maximum size to search
+ *
+ * Returns the bit number of the first set bit, or size.
+ */
+extern unsigned long find_last_bit(const unsigned long *addr,
+				   unsigned long size);
+#endif /* CONFIG_GENERIC_FIND_LAST_BIT */
+
 #ifdef CONFIG_GENERIC_FIND_NEXT_BIT
 
 /**

commit fee4b19fb3f28d17c0b9f9ea0668db5275697178
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 29 12:01:02 2008 +0200

    bitops: remove "optimizations"
    
    The mapsize optimizations which were moved from x86 to the generic
    code in commit 64970b68d2b3ed32b964b0b30b1b98518fde388e increased the
    binary size on non x86 architectures.
    
    Looking into the real effects of the "optimizations" it turned out
    that they are not used in find_next_bit() and find_next_zero_bit().
    
    The ones in find_first_bit() and find_first_zero_bit() are used in a
    couple of places but none of them is a real hot path.
    
    Remove the "optimizations" all together and call the library functions
    unconditionally.
    
    Boot-tested on x86 and compile tested on every cross compiler I have.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 8340a3aba49a..024f2b027244 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -114,8 +114,6 @@ static inline unsigned fls_long(unsigned long l)
 
 #ifdef __KERNEL__
 #ifdef CONFIG_GENERIC_FIND_FIRST_BIT
-extern unsigned long __find_first_bit(const unsigned long *addr,
-		unsigned long size);
 
 /**
  * find_first_bit - find the first set bit in a memory region
@@ -124,28 +122,8 @@ extern unsigned long __find_first_bit(const unsigned long *addr,
  *
  * Returns the bit number of the first set bit.
  */
-static __always_inline unsigned long
-find_first_bit(const unsigned long *addr, unsigned long size)
-{
-	/* Avoid a function call if the bitmap size is a constant */
-	/* and not bigger than BITS_PER_LONG. */
-
-	/* insert a sentinel so that __ffs returns size if there */
-	/* are no set bits in the bitmap */
-	if (__builtin_constant_p(size) && (size < BITS_PER_LONG))
-		return __ffs((*addr) | (1ul << size));
-
-	/* the result of __ffs(0) is undefined, so it needs to be */
-	/* handled separately */
-	if (__builtin_constant_p(size) && (size == BITS_PER_LONG))
-		return ((*addr) == 0) ? BITS_PER_LONG : __ffs(*addr);
-
-	/* size is not constant or too big */
-	return __find_first_bit(addr, size);
-}
-
-extern unsigned long __find_first_zero_bit(const unsigned long *addr,
-		unsigned long size);
+extern unsigned long find_first_bit(const unsigned long *addr,
+				    unsigned long size);
 
 /**
  * find_first_zero_bit - find the first cleared bit in a memory region
@@ -154,31 +132,12 @@ extern unsigned long __find_first_zero_bit(const unsigned long *addr,
  *
  * Returns the bit number of the first cleared bit.
  */
-static __always_inline unsigned long
-find_first_zero_bit(const unsigned long *addr, unsigned long size)
-{
-	/* Avoid a function call if the bitmap size is a constant */
-	/* and not bigger than BITS_PER_LONG. */
-
-	/* insert a sentinel so that __ffs returns size if there */
-	/* are no set bits in the bitmap */
-	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
-		return __ffs(~(*addr) | (1ul << size));
-	}
-
-	/* the result of __ffs(0) is undefined, so it needs to be */
-	/* handled separately */
-	if (__builtin_constant_p(size) && (size == BITS_PER_LONG))
-		return (~(*addr) == 0) ? BITS_PER_LONG : __ffs(~(*addr));
-
-	/* size is not constant or too big */
-	return __find_first_zero_bit(addr, size);
-}
+extern unsigned long find_first_zero_bit(const unsigned long *addr,
+					 unsigned long size);
+
 #endif /* CONFIG_GENERIC_FIND_FIRST_BIT */
 
 #ifdef CONFIG_GENERIC_FIND_NEXT_BIT
-extern unsigned long __find_next_bit(const unsigned long *addr,
-		unsigned long size, unsigned long offset);
 
 /**
  * find_next_bit - find the next set bit in a memory region
@@ -186,36 +145,8 @@ extern unsigned long __find_next_bit(const unsigned long *addr,
  * @offset: The bitnumber to start searching at
  * @size: The bitmap size in bits
  */
-static __always_inline unsigned long
-find_next_bit(const unsigned long *addr, unsigned long size,
-		unsigned long offset)
-{
-	unsigned long value;
-
-	/* Avoid a function call if the bitmap size is a constant */
-	/* and not bigger than BITS_PER_LONG. */
-
-	/* insert a sentinel so that __ffs returns size if there */
-	/* are no set bits in the bitmap */
-	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
-		value = (*addr) & ((~0ul) << offset);
-		value |= (1ul << size);
-		return __ffs(value);
-	}
-
-	/* the result of __ffs(0) is undefined, so it needs to be */
-	/* handled separately */
-	if (__builtin_constant_p(size) && (size == BITS_PER_LONG)) {
-		value = (*addr) & ((~0ul) << offset);
-		return (value == 0) ? BITS_PER_LONG : __ffs(value);
-	}
-
-	/* size is not constant or too big */
-	return __find_next_bit(addr, size, offset);
-}
-
-extern unsigned long __find_next_zero_bit(const unsigned long *addr,
-		unsigned long size, unsigned long offset);
+extern unsigned long find_next_bit(const unsigned long *addr,
+				   unsigned long size, unsigned long offset);
 
 /**
  * find_next_zero_bit - find the next cleared bit in a memory region
@@ -223,33 +154,11 @@ extern unsigned long __find_next_zero_bit(const unsigned long *addr,
  * @offset: The bitnumber to start searching at
  * @size: The bitmap size in bits
  */
-static __always_inline unsigned long
-find_next_zero_bit(const unsigned long *addr, unsigned long size,
-		unsigned long offset)
-{
-	unsigned long value;
-
-	/* Avoid a function call if the bitmap size is a constant */
-	/* and not bigger than BITS_PER_LONG. */
-
-	/* insert a sentinel so that __ffs returns size if there */
-	/* are no set bits in the bitmap */
-	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
-		value = (~(*addr)) & ((~0ul) << offset);
-		value |= (1ul << size);
-		return __ffs(value);
-	}
-
-	/* the result of __ffs(0) is undefined, so it needs to be */
-	/* handled separately */
-	if (__builtin_constant_p(size) && (size == BITS_PER_LONG)) {
-		value = (~(*addr)) & ((~0ul) << offset);
-		return (value == 0) ? BITS_PER_LONG : __ffs(value);
-	}
-
-	/* size is not constant or too big */
-	return __find_next_zero_bit(addr, size, offset);
-}
+
+extern unsigned long find_next_zero_bit(const unsigned long *addr,
+					unsigned long size,
+					unsigned long offset);
+
 #endif /* CONFIG_GENERIC_FIND_NEXT_BIT */
 #endif /* __KERNEL__ */
 #endif

commit ede9c697bc7513f210103fa77a9031e89726ae40
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Apr 29 00:58:35 2008 -0700

    Avoid divides in BITS_TO_LONGS
    
    BITS_PER_LONG is a signed value (32 or 64)
    
    DIV_ROUND_UP(nr, BITS_PER_LONG) performs signed arithmetic if "nr" is signed too.
    
    Converting BITS_TO_LONGS(nr) to DIV_ROUND_UP(nr, BITS_PER_BYTE *
    sizeof(long)) makes sure compiler can perform a right shift, even if "nr"
    is a signed value, instead of an expensive integer divide.
    
    Applying this patch saves 141 bytes on x86 when CONFIG_CC_OPTIMIZE_FOR_SIZE=y
    and speedup bitmap operations.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 48bde600a2db..8340a3aba49a 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -6,8 +6,8 @@
 #define BIT(nr)			(1UL << (nr))
 #define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
 #define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
-#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_LONG)
 #define BITS_PER_BYTE		8
+#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
 #endif
 
 /*

commit 3a48305028aa38afba93fc05066c71a6ee668ad8
Author: Alexander van Heukelum <heukelum@mailshack.com>
Date:   Tue Apr 1 17:42:21 2008 +0200

    x86: optimize find_first_bit for small bitmaps
    
    Avoid a call to find_first_bit if the bitmap size is know at
    compile time and small enough to fit in a single long integer.
    Modeled after an optimization in the original x86_64-specific
    code.
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 355d67ba3bdc..48bde600a2db 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -127,6 +127,20 @@ extern unsigned long __find_first_bit(const unsigned long *addr,
 static __always_inline unsigned long
 find_first_bit(const unsigned long *addr, unsigned long size)
 {
+	/* Avoid a function call if the bitmap size is a constant */
+	/* and not bigger than BITS_PER_LONG. */
+
+	/* insert a sentinel so that __ffs returns size if there */
+	/* are no set bits in the bitmap */
+	if (__builtin_constant_p(size) && (size < BITS_PER_LONG))
+		return __ffs((*addr) | (1ul << size));
+
+	/* the result of __ffs(0) is undefined, so it needs to be */
+	/* handled separately */
+	if (__builtin_constant_p(size) && (size == BITS_PER_LONG))
+		return ((*addr) == 0) ? BITS_PER_LONG : __ffs(*addr);
+
+	/* size is not constant or too big */
 	return __find_first_bit(addr, size);
 }
 
@@ -143,6 +157,21 @@ extern unsigned long __find_first_zero_bit(const unsigned long *addr,
 static __always_inline unsigned long
 find_first_zero_bit(const unsigned long *addr, unsigned long size)
 {
+	/* Avoid a function call if the bitmap size is a constant */
+	/* and not bigger than BITS_PER_LONG. */
+
+	/* insert a sentinel so that __ffs returns size if there */
+	/* are no set bits in the bitmap */
+	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
+		return __ffs(~(*addr) | (1ul << size));
+	}
+
+	/* the result of __ffs(0) is undefined, so it needs to be */
+	/* handled separately */
+	if (__builtin_constant_p(size) && (size == BITS_PER_LONG))
+		return (~(*addr) == 0) ? BITS_PER_LONG : __ffs(~(*addr));
+
+	/* size is not constant or too big */
 	return __find_first_zero_bit(addr, size);
 }
 #endif /* CONFIG_GENERIC_FIND_FIRST_BIT */

commit 77b9bd9c49442407804c37bcc82021a35277f83c
Author: Alexander van Heukelum <heukelum@mailshack.com>
Date:   Tue Apr 1 11:46:19 2008 +0200

    x86: generic versions of find_first_(zero_)bit, convert i386
    
    Generic versions of __find_first_bit and __find_first_zero_bit
    are introduced as simplified versions of __find_next_bit and
    __find_next_zero_bit. Their compilation and use are guarded by
    a new config variable GENERIC_FIND_FIRST_BIT.
    
    The generic versions of find_first_bit and find_first_zero_bit
    are implemented in terms of the newly introduced __find_first_bit
    and __find_first_zero_bit.
    
    This patch does not remove the i386-specific implementation,
    but it does switch i386 to use the generic functions by setting
    GENERIC_FIND_FIRST_BIT=y for X86_32.
    
    Signed-off-by: Alexander van Heukelum <heukelum@fastmail.fm>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 3865f2c93bd8..355d67ba3bdc 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -113,6 +113,40 @@ static inline unsigned fls_long(unsigned long l)
 }
 
 #ifdef __KERNEL__
+#ifdef CONFIG_GENERIC_FIND_FIRST_BIT
+extern unsigned long __find_first_bit(const unsigned long *addr,
+		unsigned long size);
+
+/**
+ * find_first_bit - find the first set bit in a memory region
+ * @addr: The address to start the search at
+ * @size: The maximum size to search
+ *
+ * Returns the bit number of the first set bit.
+ */
+static __always_inline unsigned long
+find_first_bit(const unsigned long *addr, unsigned long size)
+{
+	return __find_first_bit(addr, size);
+}
+
+extern unsigned long __find_first_zero_bit(const unsigned long *addr,
+		unsigned long size);
+
+/**
+ * find_first_zero_bit - find the first cleared bit in a memory region
+ * @addr: The address to start the search at
+ * @size: The maximum size to search
+ *
+ * Returns the bit number of the first cleared bit.
+ */
+static __always_inline unsigned long
+find_first_zero_bit(const unsigned long *addr, unsigned long size)
+{
+	return __find_first_zero_bit(addr, size);
+}
+#endif /* CONFIG_GENERIC_FIND_FIRST_BIT */
+
 #ifdef CONFIG_GENERIC_FIND_NEXT_BIT
 extern unsigned long __find_next_bit(const unsigned long *addr,
 		unsigned long size, unsigned long offset);

commit 64970b68d2b3ed32b964b0b30b1b98518fde388e
Author: Alexander van Heukelum <heukelum@mailshack.com>
Date:   Tue Mar 11 16:17:19 2008 +0100

    x86, generic: optimize find_next_(zero_)bit for small constant-size bitmaps
    
    This moves an optimization for searching constant-sized small
    bitmaps form x86_64-specific to generic code.
    
    On an i386 defconfig (the x86#testing one), the size of vmlinux hardly
    changes with this applied. I have observed only four places where this
    optimization avoids a call into find_next_bit:
    
    In the functions return_unused_surplus_pages, alloc_fresh_huge_page,
    and adjust_pool_surplus, this patch avoids a call for a 1-bit bitmap.
    In __next_cpu a call is avoided for a 32-bit bitmap. That's it.
    
    On x86_64, 52 locations are optimized with a minimal increase in
    code size:
    
    Current #testing defconfig:
            146 x bsf, 27 x find_next_*bit
       text    data     bss     dec     hex filename
       5392637  846592  724424 6963653  6a41c5 vmlinux
    
    After removing the x86_64 specific optimization for find_next_*bit:
            94 x bsf, 79 x find_next_*bit
       text    data     bss     dec     hex filename
       5392358  846592  724424 6963374  6a40ae vmlinux
    
    After this patch (making the optimization generic):
            146 x bsf, 27 x find_next_*bit
       text    data     bss     dec     hex filename
       5392396  846592  724424 6963412  6a40d4 vmlinux
    
    [ tglx@linutronix.de: build fixes ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 40d54731de7e..3865f2c93bd8 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -112,4 +112,81 @@ static inline unsigned fls_long(unsigned long l)
 	return fls64(l);
 }
 
+#ifdef __KERNEL__
+#ifdef CONFIG_GENERIC_FIND_NEXT_BIT
+extern unsigned long __find_next_bit(const unsigned long *addr,
+		unsigned long size, unsigned long offset);
+
+/**
+ * find_next_bit - find the next set bit in a memory region
+ * @addr: The address to base the search on
+ * @offset: The bitnumber to start searching at
+ * @size: The bitmap size in bits
+ */
+static __always_inline unsigned long
+find_next_bit(const unsigned long *addr, unsigned long size,
+		unsigned long offset)
+{
+	unsigned long value;
+
+	/* Avoid a function call if the bitmap size is a constant */
+	/* and not bigger than BITS_PER_LONG. */
+
+	/* insert a sentinel so that __ffs returns size if there */
+	/* are no set bits in the bitmap */
+	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
+		value = (*addr) & ((~0ul) << offset);
+		value |= (1ul << size);
+		return __ffs(value);
+	}
+
+	/* the result of __ffs(0) is undefined, so it needs to be */
+	/* handled separately */
+	if (__builtin_constant_p(size) && (size == BITS_PER_LONG)) {
+		value = (*addr) & ((~0ul) << offset);
+		return (value == 0) ? BITS_PER_LONG : __ffs(value);
+	}
+
+	/* size is not constant or too big */
+	return __find_next_bit(addr, size, offset);
+}
+
+extern unsigned long __find_next_zero_bit(const unsigned long *addr,
+		unsigned long size, unsigned long offset);
+
+/**
+ * find_next_zero_bit - find the next cleared bit in a memory region
+ * @addr: The address to base the search on
+ * @offset: The bitnumber to start searching at
+ * @size: The bitmap size in bits
+ */
+static __always_inline unsigned long
+find_next_zero_bit(const unsigned long *addr, unsigned long size,
+		unsigned long offset)
+{
+	unsigned long value;
+
+	/* Avoid a function call if the bitmap size is a constant */
+	/* and not bigger than BITS_PER_LONG. */
+
+	/* insert a sentinel so that __ffs returns size if there */
+	/* are no set bits in the bitmap */
+	if (__builtin_constant_p(size) && (size < BITS_PER_LONG)) {
+		value = (~(*addr)) & ((~0ul) << offset);
+		value |= (1ul << size);
+		return __ffs(value);
+	}
+
+	/* the result of __ffs(0) is undefined, so it needs to be */
+	/* handled separately */
+	if (__builtin_constant_p(size) && (size == BITS_PER_LONG)) {
+		value = (~(*addr)) & ((~0ul) << offset);
+		return (value == 0) ? BITS_PER_LONG : __ffs(value);
+	}
+
+	/* size is not constant or too big */
+	return __find_next_zero_bit(addr, size, offset);
+}
+#endif /* CONFIG_GENERIC_FIND_NEXT_BIT */
+#endif /* __KERNEL__ */
 #endif

commit 3afe3925987adc3fc052abe404e44520c2072fc8
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Mar 28 14:16:01 2008 -0700

    kernel: add bit rotation helpers for 16 and 8 bit
    
    Will replace open-coded variants elsewhere.  Done in the same
    style as the 32-bit versions.
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: John W. Linville <linville@tuxdriver.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Jiri Benc <jbenc@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 69c1edb9fe54..40d54731de7e 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -65,6 +65,46 @@ static inline __u32 ror32(__u32 word, unsigned int shift)
 	return (word >> shift) | (word << (32 - shift));
 }
 
+/**
+ * rol16 - rotate a 16-bit value left
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u16 rol16(__u16 word, unsigned int shift)
+{
+	return (word << shift) | (word >> (16 - shift));
+}
+
+/**
+ * ror16 - rotate a 16-bit value right
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u16 ror16(__u16 word, unsigned int shift)
+{
+	return (word >> shift) | (word << (16 - shift));
+}
+
+/**
+ * rol8 - rotate an 8-bit value left
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u8 rol8(__u8 word, unsigned int shift)
+{
+	return (word << shift) | (word >> (8 - shift));
+}
+
+/**
+ * ror8 - rotate an 8-bit value right
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u8 ror8(__u8 word, unsigned int shift)
+{
+	return (word >> shift) | (word << (8 - shift));
+}
+
 static inline unsigned fls_long(unsigned long l)
 {
 	if (sizeof(l) == 4)

commit 14ed9d23aa9acd79210a92ac561a728b42a8e281
Author: Jiri Slaby <jirislaby@gmail.com>
Date:   Thu Oct 18 23:40:37 2007 -0700

    remove BITS_TO_TYPE macro
    
    remove BITS_TO_TYPE macro
    
    I realized, that it is actually the same as DIV_ROUND_UP, use it instead.
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 7fc90d7cd0c9..69c1edb9fe54 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -6,8 +6,7 @@
 #define BIT(nr)			(1UL << (nr))
 #define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
 #define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
-#define BITS_TO_TYPE(nr, t)	(((nr)+(t)-1)/(t))
-#define BITS_TO_LONGS(nr)	BITS_TO_TYPE(nr, BITS_PER_LONG)
+#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_LONG)
 #define BITS_PER_BYTE		8
 #endif
 

commit 93043ece030af58529e3e1367502461d265ab4e2
Author: Jiri Slaby <jirislaby@gmail.com>
Date:   Thu Oct 18 23:40:35 2007 -0700

    define global BIT macro
    
    define global BIT macro
    
    move all local BIT defines to the new globally define macro.
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Kumar Gala <galak@gate.crashing.org>
    Cc: Dmitry Torokhov <dtor@mail.ru>
    Cc: Jeff Garzik <jeff@garzik.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: "Antonino A. Daplas" <adaplas@pol.net>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: "John W. Linville" <linville@tuxdriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 939e80bdbef7..7fc90d7cd0c9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -3,6 +3,7 @@
 #include <asm/types.h>
 
 #ifdef	__KERNEL__
+#define BIT(nr)			(1UL << (nr))
 #define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
 #define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
 #define BITS_TO_TYPE(nr, t)	(((nr)+(t)-1)/(t))

commit d05be13bcc6ec615fb2e9556a9b85d52800669b6
Author: Jiri Slaby <jirislaby@gmail.com>
Date:   Thu Oct 18 23:40:31 2007 -0700

    define first set of BIT* macros
    
    define first set of BIT* macros
    
    - move BITOP_MASK and BITOP_WORD from asm-generic/bitops/atomic.h to
      include/linux/bitops.h and rename it to BIT_MASK and BIT_WORD
    - move BITS_TO_LONGS and BITS_PER_BYTE to bitops.h too and allow easily
      define another BITS_TO_something (e.g. in event.c) by BITS_TO_TYPE macro
    Remaining (and common) BIT macro will be defined after all occurences and
    conflicts will be sorted out in the patches.
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index b9fb8ee3308b..939e80bdbef7 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -2,6 +2,14 @@
 #define _LINUX_BITOPS_H
 #include <asm/types.h>
 
+#ifdef	__KERNEL__
+#define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
+#define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
+#define BITS_TO_TYPE(nr, t)	(((nr)+(t)-1)/(t))
+#define BITS_TO_LONGS(nr)	BITS_TO_TYPE(nr, BITS_PER_LONG)
+#define BITS_PER_BYTE		8
+#endif
+
 /*
  * Include this here because some architectures need generic_ffs/fls in
  * scope

commit 3e037454bcfa4b187e8293d2121bd8c0f5a5c31c
Author: Shannon Nelson <shannon.nelson@intel.com>
Date:   Tue Oct 16 01:27:40 2007 -0700

    I/OAT: Add support for MSI and MSI-X
    
    Add support for MSI and MSI-X interrupt handling, including the ability
    to choose the desired interrupt method.
    
    Signed-off-by: Shannon Nelson <shannon.nelson@intel.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    [bunk@kernel.org: drivers/dma/ioat_dma.c: make 3 functions static]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 638165f571da..b9fb8ee3308b 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -8,6 +8,12 @@
  */
 #include <asm/bitops.h>
 
+#define for_each_bit(bit, addr, size) \
+	for ((bit) = find_first_bit((addr), (size)); \
+	     (bit) < (size); \
+	     (bit) = find_next_bit((addr), (size), (bit) + 1))
+
+
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit 45f8bde0d0d6deb168b45998c72b4fbeb2f57efb
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Fri Jan 26 00:57:09 2007 -0800

    [PATCH] fix various kernel-doc in header files
    
    Fix a number of kernel-doc entries for header files in include/linux by
    making sure they begin with the appropriate '/**' notation and use @var
    notation.
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 5d1eabcde5d5..638165f571da 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -31,9 +31,8 @@ static inline unsigned long hweight_long(unsigned long w)
 	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
-/*
+/**
  * rol32 - rotate a 32-bit value left
- *
  * @word: value to rotate
  * @shift: bits to roll
  */
@@ -42,9 +41,8 @@ static inline __u32 rol32(__u32 word, unsigned int shift)
 	return (word << shift) | (word >> (32 - shift));
 }
 
-/*
+/**
  * ror32 - rotate a 32-bit value right
- *
  * @word: value to rotate
  * @shift: bits to roll
  */

commit e9bebd6f3acee68fa07d44726895b40733cb1dc0
Author: Akinobu Mita <mita@miraclelinux.com>
Date:   Sun Mar 26 01:39:55 2006 -0800

    [PATCH] bitops: remove unused generic bitops in include/linux/bitops.h
    
    generic_{ffs,fls,fls64,hweight{64,32,16,8}}() were moved into
    include/asm-generic/bitops.h.  So all architectures don't use them.
    
    Signed-off-by: Akinobu Mita <mita@miraclelinux.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index f17525a963d1..5d1eabcde5d5 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -2,89 +2,12 @@
 #define _LINUX_BITOPS_H
 #include <asm/types.h>
 
-/*
- * ffs: find first bit set. This is defined the same way as
- * the libc and compiler builtin ffs routines, therefore
- * differs in spirit from the above ffz (man ffs).
- */
-
-static inline int generic_ffs(int x)
-{
-	int r = 1;
-
-	if (!x)
-		return 0;
-	if (!(x & 0xffff)) {
-		x >>= 16;
-		r += 16;
-	}
-	if (!(x & 0xff)) {
-		x >>= 8;
-		r += 8;
-	}
-	if (!(x & 0xf)) {
-		x >>= 4;
-		r += 4;
-	}
-	if (!(x & 3)) {
-		x >>= 2;
-		r += 2;
-	}
-	if (!(x & 1)) {
-		x >>= 1;
-		r += 1;
-	}
-	return r;
-}
-
-/*
- * fls: find last bit set.
- */
-
-static __inline__ int generic_fls(int x)
-{
-	int r = 32;
-
-	if (!x)
-		return 0;
-	if (!(x & 0xffff0000u)) {
-		x <<= 16;
-		r -= 16;
-	}
-	if (!(x & 0xff000000u)) {
-		x <<= 8;
-		r -= 8;
-	}
-	if (!(x & 0xf0000000u)) {
-		x <<= 4;
-		r -= 4;
-	}
-	if (!(x & 0xc0000000u)) {
-		x <<= 2;
-		r -= 2;
-	}
-	if (!(x & 0x80000000u)) {
-		x <<= 1;
-		r -= 1;
-	}
-	return r;
-}
-
 /*
  * Include this here because some architectures need generic_ffs/fls in
  * scope
  */
 #include <asm/bitops.h>
 
-
-static inline int generic_fls64(__u64 x)
-{
-	__u32 h = x >> 32;
-	if (h)
-		return fls(h) + 32;
-	return fls(x);
-}
-
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;
@@ -103,54 +26,9 @@ static __inline__ int get_count_order(unsigned int count)
 	return order;
 }
 
-/*
- * hweightN: returns the hamming weight (i.e. the number
- * of bits set) of a N-bit word
- */
-
-static inline unsigned int generic_hweight32(unsigned int w)
-{
-        unsigned int res = (w & 0x55555555) + ((w >> 1) & 0x55555555);
-        res = (res & 0x33333333) + ((res >> 2) & 0x33333333);
-        res = (res & 0x0F0F0F0F) + ((res >> 4) & 0x0F0F0F0F);
-        res = (res & 0x00FF00FF) + ((res >> 8) & 0x00FF00FF);
-        return (res & 0x0000FFFF) + ((res >> 16) & 0x0000FFFF);
-}
-
-static inline unsigned int generic_hweight16(unsigned int w)
-{
-        unsigned int res = (w & 0x5555) + ((w >> 1) & 0x5555);
-        res = (res & 0x3333) + ((res >> 2) & 0x3333);
-        res = (res & 0x0F0F) + ((res >> 4) & 0x0F0F);
-        return (res & 0x00FF) + ((res >> 8) & 0x00FF);
-}
-
-static inline unsigned int generic_hweight8(unsigned int w)
-{
-        unsigned int res = (w & 0x55) + ((w >> 1) & 0x55);
-        res = (res & 0x33) + ((res >> 2) & 0x33);
-        return (res & 0x0F) + ((res >> 4) & 0x0F);
-}
-
-static inline unsigned long generic_hweight64(__u64 w)
-{
-#if BITS_PER_LONG < 64
-	return generic_hweight32((unsigned int)(w >> 32)) +
-				generic_hweight32((unsigned int)w);
-#else
-	u64 res;
-	res = (w & 0x5555555555555555ul) + ((w >> 1) & 0x5555555555555555ul);
-	res = (res & 0x3333333333333333ul) + ((res >> 2) & 0x3333333333333333ul);
-	res = (res & 0x0F0F0F0F0F0F0F0Ful) + ((res >> 4) & 0x0F0F0F0F0F0F0F0Ful);
-	res = (res & 0x00FF00FF00FF00FFul) + ((res >> 8) & 0x00FF00FF00FF00FFul);
-	res = (res & 0x0000FFFF0000FFFFul) + ((res >> 16) & 0x0000FFFF0000FFFFul);
-	return (res & 0x00000000FFFFFFFFul) + ((res >> 32) & 0x00000000FFFFFFFFul);
-#endif
-}
-
 static inline unsigned long hweight_long(unsigned long w)
 {
-	return sizeof(w) == 4 ? generic_hweight32(w) : generic_hweight64(w);
+	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
 }
 
 /*

commit 962749af67b145c57917bfbff3c303ebd7d5988c
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Mar 25 03:08:01 2006 -0800

    [PATCH] roundup_pow_of_two() 64-bit fix
    
    fls() takes an integer, so roundup_pow_of_two() is busted for ulongs larger
    than 2^32-1.
    
    Fix this by implementing and using fls_long().
    
    (Why does roundup_pow_of_two() return a long?)
    
    (Why is roundup_pow_of_two() __attribute_const__ whereas long_log2() is
    __attribute_pure__?)
    
    (Why does long_log2() suck so much?  Because we were missing fls_long()?)
    
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Cc: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 208650b1ad3a..f17525a963d1 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -175,4 +175,11 @@ static inline __u32 ror32(__u32 word, unsigned int shift)
 	return (word >> shift) | (word << (32 - shift));
 }
 
+static inline unsigned fls_long(unsigned long l)
+{
+	if (sizeof(l) == 4)
+		return fls(l);
+	return fls64(l);
+}
+
 #endif

commit f434baf4c6ae4a392b7c34843825af0894c89db2
Author: Akinobu Mita <mita@miraclelinux.com>
Date:   Fri Feb 3 03:03:46 2006 -0800

    [PATCH] fix generic_fls64()
    
    Noticed by Rune Torgersen.
    
    Fix generic_fls64().  tcp_cubic is using fls64().
    
    Signed-off-by: Akinobu Mita <mita@miraclelinux.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 6a2a19f14bb2..208650b1ad3a 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -81,7 +81,7 @@ static inline int generic_fls64(__u64 x)
 {
 	__u32 h = x >> 32;
 	if (h)
-		return fls(x) + 32;
+		return fls(h) + 32;
 	return fls(x);
 }
 

commit 3821af2fe13700cab6fd67367128fa180e43f8b8
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Wed Dec 21 19:30:53 2005 -0800

    [FLS64]: generic version
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index 38c2fb7ebe09..6a2a19f14bb2 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -76,6 +76,15 @@ static __inline__ int generic_fls(int x)
  */
 #include <asm/bitops.h>
 
+
+static inline int generic_fls64(__u64 x)
+{
+	__u32 h = x >> 32;
+	if (h)
+		return fls(x) + 32;
+	return fls(x);
+}
+
 static __inline__ int get_bitmask_order(unsigned int count)
 {
 	int order;

commit 94605eff572b727aaad9b4b29bc358b919096503
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Sat Nov 5 17:25:54 2005 +0100

    [PATCH] x86-64/i386: Intel HT, Multi core detection fixes
    
    Fields obtained through cpuid vector 0x1(ebx[16:23]) and
    vector 0x4(eax[14:25], eax[26:31]) indicate the maximum values and might not
    always be the same as what is available and what OS sees.  So make sure
    "siblings" and "cpu cores" values in /proc/cpuinfo reflect the values as seen
    by OS instead of what cpuid instruction says. This will also fix the buggy BIOS
    cases (for example where cpuid on a single core cpu says there are "2" siblings,
    even when HT is disabled in the BIOS.
    http://bugzilla.kernel.org/show_bug.cgi?id=4359)
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index cb3c3ef50f50..38c2fb7ebe09 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -84,6 +84,16 @@ static __inline__ int get_bitmask_order(unsigned int count)
 	return order;	/* We could be slightly more clever with -1 here... */
 }
 
+static __inline__ int get_count_order(unsigned int count)
+{
+	int order;
+	
+	order = fls(count) - 1;
+	if (count & (count - 1))
+		order++;
+	return order;
+}
+
 /*
  * hweightN: returns the hamming weight (i.e. the number
  * of bits set) of a N-bit word

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/bitops.h b/include/linux/bitops.h
new file mode 100644
index 000000000000..cb3c3ef50f50
--- /dev/null
+++ b/include/linux/bitops.h
@@ -0,0 +1,159 @@
+#ifndef _LINUX_BITOPS_H
+#define _LINUX_BITOPS_H
+#include <asm/types.h>
+
+/*
+ * ffs: find first bit set. This is defined the same way as
+ * the libc and compiler builtin ffs routines, therefore
+ * differs in spirit from the above ffz (man ffs).
+ */
+
+static inline int generic_ffs(int x)
+{
+	int r = 1;
+
+	if (!x)
+		return 0;
+	if (!(x & 0xffff)) {
+		x >>= 16;
+		r += 16;
+	}
+	if (!(x & 0xff)) {
+		x >>= 8;
+		r += 8;
+	}
+	if (!(x & 0xf)) {
+		x >>= 4;
+		r += 4;
+	}
+	if (!(x & 3)) {
+		x >>= 2;
+		r += 2;
+	}
+	if (!(x & 1)) {
+		x >>= 1;
+		r += 1;
+	}
+	return r;
+}
+
+/*
+ * fls: find last bit set.
+ */
+
+static __inline__ int generic_fls(int x)
+{
+	int r = 32;
+
+	if (!x)
+		return 0;
+	if (!(x & 0xffff0000u)) {
+		x <<= 16;
+		r -= 16;
+	}
+	if (!(x & 0xff000000u)) {
+		x <<= 8;
+		r -= 8;
+	}
+	if (!(x & 0xf0000000u)) {
+		x <<= 4;
+		r -= 4;
+	}
+	if (!(x & 0xc0000000u)) {
+		x <<= 2;
+		r -= 2;
+	}
+	if (!(x & 0x80000000u)) {
+		x <<= 1;
+		r -= 1;
+	}
+	return r;
+}
+
+/*
+ * Include this here because some architectures need generic_ffs/fls in
+ * scope
+ */
+#include <asm/bitops.h>
+
+static __inline__ int get_bitmask_order(unsigned int count)
+{
+	int order;
+	
+	order = fls(count);
+	return order;	/* We could be slightly more clever with -1 here... */
+}
+
+/*
+ * hweightN: returns the hamming weight (i.e. the number
+ * of bits set) of a N-bit word
+ */
+
+static inline unsigned int generic_hweight32(unsigned int w)
+{
+        unsigned int res = (w & 0x55555555) + ((w >> 1) & 0x55555555);
+        res = (res & 0x33333333) + ((res >> 2) & 0x33333333);
+        res = (res & 0x0F0F0F0F) + ((res >> 4) & 0x0F0F0F0F);
+        res = (res & 0x00FF00FF) + ((res >> 8) & 0x00FF00FF);
+        return (res & 0x0000FFFF) + ((res >> 16) & 0x0000FFFF);
+}
+
+static inline unsigned int generic_hweight16(unsigned int w)
+{
+        unsigned int res = (w & 0x5555) + ((w >> 1) & 0x5555);
+        res = (res & 0x3333) + ((res >> 2) & 0x3333);
+        res = (res & 0x0F0F) + ((res >> 4) & 0x0F0F);
+        return (res & 0x00FF) + ((res >> 8) & 0x00FF);
+}
+
+static inline unsigned int generic_hweight8(unsigned int w)
+{
+        unsigned int res = (w & 0x55) + ((w >> 1) & 0x55);
+        res = (res & 0x33) + ((res >> 2) & 0x33);
+        return (res & 0x0F) + ((res >> 4) & 0x0F);
+}
+
+static inline unsigned long generic_hweight64(__u64 w)
+{
+#if BITS_PER_LONG < 64
+	return generic_hweight32((unsigned int)(w >> 32)) +
+				generic_hweight32((unsigned int)w);
+#else
+	u64 res;
+	res = (w & 0x5555555555555555ul) + ((w >> 1) & 0x5555555555555555ul);
+	res = (res & 0x3333333333333333ul) + ((res >> 2) & 0x3333333333333333ul);
+	res = (res & 0x0F0F0F0F0F0F0F0Ful) + ((res >> 4) & 0x0F0F0F0F0F0F0F0Ful);
+	res = (res & 0x00FF00FF00FF00FFul) + ((res >> 8) & 0x00FF00FF00FF00FFul);
+	res = (res & 0x0000FFFF0000FFFFul) + ((res >> 16) & 0x0000FFFF0000FFFFul);
+	return (res & 0x00000000FFFFFFFFul) + ((res >> 32) & 0x00000000FFFFFFFFul);
+#endif
+}
+
+static inline unsigned long hweight_long(unsigned long w)
+{
+	return sizeof(w) == 4 ? generic_hweight32(w) : generic_hweight64(w);
+}
+
+/*
+ * rol32 - rotate a 32-bit value left
+ *
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u32 rol32(__u32 word, unsigned int shift)
+{
+	return (word << shift) | (word >> (32 - shift));
+}
+
+/*
+ * ror32 - rotate a 32-bit value right
+ *
+ * @word: value to rotate
+ * @shift: bits to roll
+ */
+static inline __u32 ror32(__u32 word, unsigned int shift)
+{
+	return (word >> shift) | (word << (32 - shift));
+}
+
+#endif
