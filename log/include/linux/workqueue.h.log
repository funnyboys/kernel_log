commit c39ba6b3a8d47be07c180f857564a25a0356d336
Author: Lai Jiangshan <laijs@linux.alibaba.com>
Date:   Mon Jun 1 08:44:39 2020 +0000

    workqueue: fix a piece of comment about reserved bits for work flags
    
    8a2e8e5dec7e("workqueue: fix cwq->nr_active underflow")
    allocated one more bit from the work flags, and it updated
    partial of the comments (128 bytes -> 256 bytes), but it
    failed to update the info about the number of reserved bits.
    
    Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 8b505d22fc0e..26de0cae2a0a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -62,7 +62,7 @@ enum {
 	WORK_CPU_UNBOUND	= NR_CPUS,
 
 	/*
-	 * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.
+	 * Reserve 8 bits off of pwq pointer w/ debugobjects turned off.
 	 * This makes pwqs aligned to 256 bytes and allows 15 workqueue
 	 * flush colors.
 	 */

commit 0adb8bc0391f1fa7820529c0200fb0c4912fe365
Merge: d88360052364 00d5d15b0641
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 3 12:27:36 2020 -0700

    Merge branch 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "Nothing too interesting. Just two trivial patches"
    
    * 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: Mark up unlocked access to wq->first_flusher
      workqueue: Make workqueue_init*() return void

commit 2333e829952fb437db915bbb17f4d8c43127d438
Author: Yu Chen <chen.yu@easystack.cn>
Date:   Sun Feb 23 15:28:52 2020 +0800

    workqueue: Make workqueue_init*() return void
    
    The return values of workqueue_init() and workqueue_early_int() are
    always 0, and there is no usage of their return value.  So just make
    them return void.
    
    Signed-off-by: Yu Chen <chen.yu@easystack.cn>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4261d1c6e87b..c86a7691e13c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -649,7 +649,7 @@ int workqueue_online_cpu(unsigned int cpu);
 int workqueue_offline_cpu(unsigned int cpu);
 #endif
 
-int __init workqueue_init_early(void);
-int __init workqueue_init(void);
+void __init workqueue_init_early(void);
+void __init workqueue_init(void);
 
 #endif

commit dbb92f88648d6206bf22fcb764fb9fe2939d401a
Author: Andrea Parri <parri.andrea@gmail.com>
Date:   Wed Jan 22 19:39:52 2020 +0100

    workqueue: Document (some) memory-ordering properties of {queue,schedule}_work()
    
    It's desirable to be able to rely on the following property:  All stores
    preceding (in program order) a call to a successful queue_work() will be
    visible from the CPU which will execute the queued work by the time such
    work executes, e.g.,
    
      { x is initially 0 }
    
        CPU0                              CPU1
    
        WRITE_ONCE(x, 1);                 [ "work" is being executed ]
        r0 = queue_work(wq, work);          r1 = READ_ONCE(x);
    
      Forbids: r0 == true && r1 == 0
    
    The current implementation of queue_work() provides such memory-ordering
    property:
    
      - In __queue_work(), the ->lock spinlock is acquired.
    
      - On the other side, in worker_thread(), this same ->lock is held
        when dequeueing work.
    
    So the locking ordering makes things work out.
    
    Add this property to the DocBook headers of {queue,schedule}_work().
    
    Suggested-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Andrea Parri <parri.andrea@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4261d1c6e87b..e48554e6526c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -487,6 +487,19 @@ extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);
  *
  * We queue the work to the CPU on which it was submitted, but if the CPU dies
  * it can be processed by another CPU.
+ *
+ * Memory-ordering properties:  If it returns %true, guarantees that all stores
+ * preceding the call to queue_work() in the program order will be visible from
+ * the CPU which will execute @work by the time such work executes, e.g.,
+ *
+ * { x is initially 0 }
+ *
+ *   CPU0				CPU1
+ *
+ *   WRITE_ONCE(x, 1);			[ @work is being executed ]
+ *   r0 = queue_work(wq, work);		  r1 = READ_ONCE(x);
+ *
+ * Forbids: r0 == true && r1 == 0
  */
 static inline bool queue_work(struct workqueue_struct *wq,
 			      struct work_struct *work)
@@ -546,6 +559,9 @@ static inline bool schedule_work_on(int cpu, struct work_struct *work)
  * This puts a job in the kernel-global workqueue if it was not already
  * queued and leaves it in the same position on the kernel-global
  * workqueue otherwise.
+ *
+ * Shares the same memory-ordering properties of queue_work(), cf. the
+ * DocBook header of queue_work().
  */
 static inline bool schedule_work(struct work_struct *work)
 {

commit 513c98d08682957cc9eba20e7e4bb349970711f3
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Sep 5 21:40:22 2019 -0400

    workqueue: unconfine alloc/apply/free_workqueue_attrs()
    
    padata will use these these interfaces in a later patch, so unconfine them.
    
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b7c585b5ec1c..4261d1c6e87b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -435,6 +435,10 @@ struct workqueue_struct *alloc_workqueue(const char *fmt,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
+struct workqueue_attrs *alloc_workqueue_attrs(void);
+void free_workqueue_attrs(struct workqueue_attrs *attrs);
+int apply_workqueue_attrs(struct workqueue_struct *wq,
+			  const struct workqueue_attrs *attrs);
 int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,

commit 2c9858ecbeb1e68224290043445990e29337d4c0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 26 16:52:37 2019 +0200

    workqueue: Make alloc/apply/free_workqueue_attrs() static
    
    None of those functions have any users outside of workqueue.c. Confine
    them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d59525fca4d3..b7c585b5ec1c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -435,10 +435,6 @@ struct workqueue_struct *alloc_workqueue(const char *fmt,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
-struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);
-void free_workqueue_attrs(struct workqueue_attrs *attrs);
-int apply_workqueue_attrs(struct workqueue_struct *wq,
-			  const struct workqueue_attrs *attrs);
 int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,

commit e431f2d74e1b91e00e71e97cadcadffc4cda8a9b
Merge: 45763bf4bc1e 36cf3b1363f4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 14:52:48 2019 -0800

    Merge tag 'driver-core-5.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull driver core updates from Greg KH:
     "Here is the big driver core patchset for 5.1-rc1
    
      More patches than "normal" here this merge window, due to some work in
      the driver core by Alexander Duyck to rework the async probe
      functionality to work better for a number of devices, and independant
      work from Rafael for the device link functionality to make it work
      "correctly".
    
      Also in here is:
    
       - lots of BUS_ATTR() removals, the macro is about to go away
    
       - firmware test fixups
    
       - ihex fixups and simplification
    
       - component additions (also includes i915 patches)
    
       - lots of minor coding style fixups and cleanups.
    
      All of these have been in linux-next for a while with no reported
      issues"
    
    * tag 'driver-core-5.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core: (65 commits)
      driver core: platform: remove misleading err_alloc label
      platform: set of_node in platform_device_register_full()
      firmware: hardcode the debug message for -ENOENT
      driver core: Add missing description of new struct device_link field
      driver core: Fix PM-runtime for links added during consumer probe
      drivers/component: kerneldoc polish
      async: Add cmdline option to specify drivers to be async probed
      driver core: Fix possible supplier PM-usage counter imbalance
      PM-runtime: Fix __pm_runtime_set_status() race with runtime resume
      driver: platform: Support parsing GpioInt 0 in platform_get_irq()
      selftests: firmware: fix verify_reqs() return value
      Revert "selftests: firmware: remove use of non-standard diff -Z option"
      Revert "selftests: firmware: add CONFIG_FW_LOADER_USER_HELPER_FALLBACK to config"
      device: Fix comment for driver_data in struct device
      kernfs: Allocating memory for kernfs_iattrs with kmem_cache.
      sysfs: remove unused include of kernfs-internal.h
      driver core: Postpone DMA tear-down until after devres release
      driver core: Document limitation related to DL_FLAG_RPM_ACTIVE
      PM-runtime: Take suppliers into account in __pm_runtime_set_status()
      device.h: Add __cold to dev_<level> logging functions
      ...

commit 669de8bda87b92ab9a2fc663b3f5743c2ad1ae9f
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:54 2019 -0800

    kernel/workqueue: Use dynamic lockdep keys for workqueues
    
    The following commit:
    
      87915adc3f0a ("workqueue: re-add lockdep dependencies for flushing")
    
    improved deadlock checking in the workqueue implementation. Unfortunately
    that patch also introduced a few false positive lockdep complaints.
    
    This patch suppresses these false positives by allocating the workqueue mutex
    lockdep key dynamically.
    
    An example of a false positive lockdep complaint suppressed by this patch
    can be found below. The root cause of the lockdep complaint shown below
    is that the direct I/O code can call alloc_workqueue() from inside a work
    item created by another alloc_workqueue() call and that both workqueues
    share the same lockdep key. This patch avoids that that lockdep complaint
    is triggered by allocating the work queue lockdep keys dynamically.
    
    In other words, this patch guarantees that a unique lockdep key is
    associated with each work queue mutex.
    
      ======================================================
      WARNING: possible circular locking dependency detected
      4.19.0-dbg+ #1 Not tainted
      fio/4129 is trying to acquire lock:
      00000000a01cfe1a ((wq_completion)"dio/%s"sb->s_id){+.+.}, at: flush_workqueue+0xd0/0x970
    
      but task is already holding lock:
      00000000a0acecf9 (&sb->s_type->i_mutex_key#14){+.+.}, at: ext4_file_write_iter+0x154/0x710
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #2 (&sb->s_type->i_mutex_key#14){+.+.}:
             down_write+0x3d/0x80
             __generic_file_fsync+0x77/0xf0
             ext4_sync_file+0x3c9/0x780
             vfs_fsync_range+0x66/0x100
             dio_complete+0x2f5/0x360
             dio_aio_complete_work+0x1c/0x20
             process_one_work+0x481/0x9f0
             worker_thread+0x63/0x5a0
             kthread+0x1cf/0x1f0
             ret_from_fork+0x24/0x30
    
      -> #1 ((work_completion)(&dio->complete_work)){+.+.}:
             process_one_work+0x447/0x9f0
             worker_thread+0x63/0x5a0
             kthread+0x1cf/0x1f0
             ret_from_fork+0x24/0x30
    
      -> #0 ((wq_completion)"dio/%s"sb->s_id){+.+.}:
             lock_acquire+0xc5/0x200
             flush_workqueue+0xf3/0x970
             drain_workqueue+0xec/0x220
             destroy_workqueue+0x23/0x350
             sb_init_dio_done_wq+0x6a/0x80
             do_blockdev_direct_IO+0x1f33/0x4be0
             __blockdev_direct_IO+0x79/0x86
             ext4_direct_IO+0x5df/0xbb0
             generic_file_direct_write+0x119/0x220
             __generic_file_write_iter+0x131/0x2d0
             ext4_file_write_iter+0x3fa/0x710
             aio_write+0x235/0x330
             io_submit_one+0x510/0xeb0
             __x64_sys_io_submit+0x122/0x340
             do_syscall_64+0x71/0x220
             entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
      other info that might help us debug this:
    
      Chain exists of:
        (wq_completion)"dio/%s"sb->s_id --> (work_completion)(&dio->complete_work) --> &sb->s_type->i_mutex_key#14
    
       Possible unsafe locking scenario:
    
             CPU0                    CPU1
             ----                    ----
        lock(&sb->s_type->i_mutex_key#14);
                                     lock((work_completion)(&dio->complete_work));
                                     lock(&sb->s_type->i_mutex_key#14);
        lock((wq_completion)"dio/%s"sb->s_id);
    
       *** DEADLOCK ***
    
      1 lock held by fio/4129:
       #0: 00000000a0acecf9 (&sb->s_type->i_mutex_key#14){+.+.}, at: ext4_file_write_iter+0x154/0x710
    
      stack backtrace:
      CPU: 3 PID: 4129 Comm: fio Not tainted 4.19.0-dbg+ #1
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1 04/01/2014
      Call Trace:
       dump_stack+0x86/0xc5
       print_circular_bug.isra.32+0x20a/0x218
       __lock_acquire+0x1c68/0x1cf0
       lock_acquire+0xc5/0x200
       flush_workqueue+0xf3/0x970
       drain_workqueue+0xec/0x220
       destroy_workqueue+0x23/0x350
       sb_init_dio_done_wq+0x6a/0x80
       do_blockdev_direct_IO+0x1f33/0x4be0
       __blockdev_direct_IO+0x79/0x86
       ext4_direct_IO+0x5df/0xbb0
       generic_file_direct_write+0x119/0x220
       __generic_file_write_iter+0x131/0x2d0
       ext4_file_write_iter+0x3fa/0x710
       aio_write+0x235/0x330
       io_submit_one+0x510/0xeb0
       __x64_sys_io_submit+0x122/0x340
       do_syscall_64+0x71/0x220
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes.berg@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://lkml.kernel.org/r/20190214230058.196511-20-bvanassche@acm.org
    [ Reworked the changelog a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 60d673e15632..d9a1a480e920 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -390,43 +390,23 @@ extern struct workqueue_struct *system_freezable_wq;
 extern struct workqueue_struct *system_power_efficient_wq;
 extern struct workqueue_struct *system_freezable_power_efficient_wq;
 
-extern struct workqueue_struct *
-__alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
-	struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);
-
 /**
  * alloc_workqueue - allocate a workqueue
  * @fmt: printf format for the name of the workqueue
  * @flags: WQ_* flags
  * @max_active: max in-flight work items, 0 for default
- * @args...: args for @fmt
+ * remaining args: args for @fmt
  *
  * Allocate a workqueue with the specified parameters.  For detailed
  * information on WQ_* flags, please refer to
  * Documentation/core-api/workqueue.rst.
  *
- * The __lock_name macro dance is to guarantee that single lock_class_key
- * doesn't end up with different namesm, which isn't allowed by lockdep.
- *
  * RETURNS:
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
-#ifdef CONFIG_LOCKDEP
-#define alloc_workqueue(fmt, flags, max_active, args...)		\
-({									\
-	static struct lock_class_key __key;				\
-	const char *__lock_name;					\
-									\
-	__lock_name = "(wq_completion)"#fmt#args;			\
-									\
-	__alloc_workqueue_key((fmt), (flags), (max_active),		\
-			      &__key, __lock_name, ##args);		\
-})
-#else
-#define alloc_workqueue(fmt, flags, max_active, args...)		\
-	__alloc_workqueue_key((fmt), (flags), (max_active),		\
-			      NULL, NULL, ##args)
-#endif
+struct workqueue_struct *alloc_workqueue(const char *fmt,
+					 unsigned int flags,
+					 int max_active, ...);
 
 /**
  * alloc_ordered_workqueue - allocate an ordered workqueue

commit 8204e0c1113d6b7f599bcd7ebfbfde72e76c102f
Author: Alexander Duyck <alexander.h.duyck@linux.intel.com>
Date:   Tue Jan 22 10:39:26 2019 -0800

    workqueue: Provide queue_work_node to queue work near a given NUMA node
    
    Provide a new function, queue_work_node, which is meant to schedule work on
    a "random" CPU of the requested NUMA node. The main motivation for this is
    to help assist asynchronous init to better improve boot times for devices
    that are local to a specific node.
    
    For now we just default to the first CPU that is in the intersection of the
    cpumask of the node and the online cpumask. The only exception is if the
    CPU is local to the node we will just use the current CPU. This should work
    for our purposes as we are currently only using this for unbound work so
    the CPU will be translated to a node anyway instead of being directly used.
    
    As we are only using the first CPU to represent the NUMA node for now I am
    limiting the scope of the function so that it can only be used with unbound
    workqueues.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 60d673e15632..1f50c1e586e7 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -463,6 +463,8 @@ int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
+extern bool queue_work_node(int node, struct workqueue_struct *wq,
+			    struct work_struct *work);
 extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,

commit 6b59808bfe482642287ddf3fe9d4cccb10756652
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 18 08:47:13 2018 -0700

    workqueue: Show the latest workqueue name in /proc/PID/{comm,stat,status}
    
    There can be a lot of workqueue workers and they all show up with the
    cryptic kworker/* names making it difficult to understand which is
    doing what and how they came to be.
    
      # ps -ef | grep kworker
      root           4       2  0 Feb25 ?        00:00:00 [kworker/0:0H]
      root           6       2  0 Feb25 ?        00:00:00 [kworker/u112:0]
      root          19       2  0 Feb25 ?        00:00:00 [kworker/1:0H]
      root          25       2  0 Feb25 ?        00:00:00 [kworker/2:0H]
      root          31       2  0 Feb25 ?        00:00:00 [kworker/3:0H]
      ...
    
    This patch makes workqueue workers report the latest workqueue it was
    executing for through /proc/PID/{comm,stat,status}.  The extra
    information is appended to the kthread name with intervening '+' if
    currently executing, otherwise '-'.
    
      # cat /proc/25/comm
      kworker/2:0-events_power_efficient
      # cat /proc/25/stat
      25 (kworker/2:0-events_power_efficient) I 2 0 0 0 -1 69238880 0 0...
      # grep Name /proc/25/status
      Name:   kworker/2:0-events_power_efficient
    
    Unfortunately, ps(1) truncates comm to 15 characters,
    
      # ps 25
        PID TTY      STAT   TIME COMMAND
         25 ?        I      0:00 [kworker/2:0-eve]
    
    making it a lot less useful; however, this should be an easy fix from
    ps(1) side.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Craig Small <csmall@enc.com.au>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 39a0e215022a..60d673e15632 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -494,6 +494,7 @@ extern unsigned int work_busy(struct work_struct *work);
 extern __printf(1, 2) void set_worker_desc(const char *fmt, ...);
 extern void print_worker_info(const char *log_lvl, struct task_struct *task);
 extern void show_workqueue_state(void);
+extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);
 
 /**
  * queue_work - queue work on a workqueue

commit d92cd810e64aa7cf22b05f0ea1c7d3e8dbae75fe
Merge: a23867f1d2de f75da8a8a918
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 3 18:00:13 2018 -0700

    Merge branch 'for-4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "rcu_work addition and a couple trivial changes"
    
    * 'for-4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: remove the comment about the old manager_arb mutex
      workqueue: fix the comments of nr_idle
      fs/aio: Use rcu_work instead of explicit rcu and work item
      cgroup: Use rcu_work instead of explicit rcu and work item
      RCU, workqueue: Implement rcu_work

commit 05f0fe6b74dbd7690a4cbd61810948b7d575576a
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 14 12:45:13 2018 -0700

    RCU, workqueue: Implement rcu_work
    
    There are cases where RCU callback needs to be bounced to a sleepable
    context.  This is currently done by the RCU callback queueing a work
    item, which can be cumbersome to write and confusing to read.
    
    This patch introduces rcu_work, a workqueue work variant which gets
    executed after a RCU grace period, and converts the open coded
    bouncing in fs/aio and kernel/cgroup.
    
    v3: Dropped queue_rcu_work_on().  Documented rcu grace period behavior
        after queue_rcu_work().
    
    v2: Use rcu_barrier() instead of synchronize_rcu() to wait for
        completion of previously queued rcu callback as per Paul.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index bc0cda180c8b..d026f8f818cc 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -13,6 +13,7 @@
 #include <linux/threads.h>
 #include <linux/atomic.h>
 #include <linux/cpumask.h>
+#include <linux/rcupdate.h>
 
 struct workqueue_struct;
 
@@ -120,6 +121,14 @@ struct delayed_work {
 	int cpu;
 };
 
+struct rcu_work {
+	struct work_struct work;
+	struct rcu_head rcu;
+
+	/* target workqueue ->rcu uses to queue ->work */
+	struct workqueue_struct *wq;
+};
+
 /**
  * struct workqueue_attrs - A struct for workqueue attributes.
  *
@@ -151,6 +160,11 @@ static inline struct delayed_work *to_delayed_work(struct work_struct *work)
 	return container_of(work, struct delayed_work, work);
 }
 
+static inline struct rcu_work *to_rcu_work(struct work_struct *work)
+{
+	return container_of(work, struct rcu_work, work);
+}
+
 struct execute_work {
 	struct work_struct work;
 };
@@ -266,6 +280,12 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define INIT_DEFERRABLE_WORK_ONSTACK(_work, _func)			\
 	__INIT_DELAYED_WORK_ONSTACK(_work, _func, TIMER_DEFERRABLE)
 
+#define INIT_RCU_WORK(_work, _func)					\
+	INIT_WORK(&(_work)->work, (_func))
+
+#define INIT_RCU_WORK_ONSTACK(_work, _func)				\
+	INIT_WORK_ONSTACK(&(_work)->work, (_func))
+
 /**
  * work_pending - Find out whether a work item is currently pending
  * @work: The work item in question
@@ -447,6 +467,7 @@ extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *dwork, unsigned long delay);
+extern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
@@ -463,6 +484,8 @@ extern bool flush_delayed_work(struct delayed_work *dwork);
 extern bool cancel_delayed_work(struct delayed_work *dwork);
 extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
+extern bool flush_rcu_work(struct rcu_work *rwork);
+
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
 extern struct work_struct *current_work(void);

commit 6417250d3f894e66a68ba1cd93676143f2376a6f
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Tue Mar 6 19:34:42 2018 -0800

    workqueue: remove unused cancel_work()
    
    Found this by accident.
    There are no usages of bare cancel_work() in current kernel source.
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index bc0cda180c8b..0c3301421c57 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -456,7 +456,6 @@ extern int schedule_on_each_cpu(work_func_t func);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern bool flush_work(struct work_struct *work);
-extern bool cancel_work(struct work_struct *work);
 extern bool cancel_work_sync(struct work_struct *work);
 
 extern bool flush_delayed_work(struct delayed_work *dwork);

commit 27d4ee03078aba88c5e07dcc4917e8d01d046f38
Author: Lukas Wunner <lukas@wunner.de>
Date:   Sun Feb 11 10:38:28 2018 +0100

    workqueue: Allow retrieval of current task's work struct
    
    Introduce a helper to retrieve the current task's work struct if it is
    a workqueue worker.
    
    This allows us to fix a long-standing deadlock in several DRM drivers
    wherein the ->runtime_suspend callback waits for a specific worker to
    finish and that worker in turn calls a function which waits for runtime
    suspend to finish.  That function is invoked from multiple call sites
    and waiting for runtime suspend to finish is the correct thing to do
    except if it's executing in the context of the worker.
    
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: Ben Skeggs <bskeggs@redhat.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lyude Paul <lyude@redhat.com>
    Signed-off-by: Lukas Wunner <lukas@wunner.de>
    Link: https://patchwork.freedesktop.org/patch/msgid/2d8f603074131eb87e588d2b803a71765bd3a2fd.1518338788.git.lukas@wunner.de

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4a54ef96aff5..bc0cda180c8b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -465,6 +465,7 @@ extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
+extern struct work_struct *current_work(void);
 extern bool current_is_workqueue_rescuer(void);
 extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);

commit 841b86f3289dbe858daeceec36423d4ea286fac2
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Oct 23 09:40:42 2017 +0200

    treewide: Remove TIMER_FUNC_TYPE and TIMER_DATA_TYPE casts
    
    With all callbacks converted, and the timer callback prototype
    switched over, the TIMER_FUNC_TYPE cast is no longer needed,
    so remove it. Conversion was done with the following scripts:
    
        perl -pi -e 's|\(TIMER_FUNC_TYPE\)||g' \
            $(git grep TIMER_FUNC_TYPE | cut -d: -f1 | sort -u)
    
        perl -pi -e 's|\(TIMER_DATA_TYPE\)||g' \
            $(git grep TIMER_DATA_TYPE | cut -d: -f1 | sort -u)
    
    The now unused macros are also dropped from include/linux/timer.h.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index bff39faba793..4a54ef96aff5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -176,7 +176,7 @@ struct execute_work {
 
 #define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
-	.timer = __TIMER_INITIALIZER((TIMER_FUNC_TYPE)delayed_work_timer_fn,\
+	.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,\
 				     (tflags) | TIMER_IRQSAFE),		\
 	}
 

commit 919b250f8570618e84af544c3e18dad5210eb9b6
Author: Kees Cook <keescook@chromium.org>
Date:   Sun Oct 22 18:48:43 2017 -0700

    timer: Remove redundant __setup_timer*() macros
    
    With __init_timer*() now matching __setup_timer*(), remove the redundant
    internal interface, clean up the resulting definitions and add more
    documentation.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Shaohua Li <shli@fb.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 8d11580237f5..bff39faba793 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -241,17 +241,17 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define __INIT_DELAYED_WORK(_work, _func, _tflags)			\
 	do {								\
 		INIT_WORK(&(_work)->work, (_func));			\
-		__setup_timer(&(_work)->timer,				\
-			      (TIMER_FUNC_TYPE)delayed_work_timer_fn,	\
-			      (_tflags) | TIMER_IRQSAFE);		\
+		__init_timer(&(_work)->timer,				\
+			     delayed_work_timer_fn,			\
+			     (_tflags) | TIMER_IRQSAFE);		\
 	} while (0)
 
 #define __INIT_DELAYED_WORK_ONSTACK(_work, _func, _tflags)		\
 	do {								\
 		INIT_WORK_ONSTACK(&(_work)->work, (_func));		\
-		__setup_timer_on_stack(&(_work)->timer,			\
-				       (TIMER_FUNC_TYPE)delayed_work_timer_fn,\
-				       (_tflags) | TIMER_IRQSAFE);	\
+		__init_timer_on_stack(&(_work)->timer,			\
+				      delayed_work_timer_fn,		\
+				      (_tflags) | TIMER_IRQSAFE);	\
 	} while (0)
 
 #define INIT_DELAYED_WORK(_work, _func)					\

commit 1fe66ba572b455270dc35a2c099dd7328cec9e4c
Author: Kees Cook <keescook@chromium.org>
Date:   Sun Oct 22 18:22:50 2017 -0700

    timer: Remove unused data arguments from macros
    
    With the .data field removed, the ignored data arguments in timer macros
    can be removed.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Shaohua Li <shli@fb.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 01a050fc6650..8d11580237f5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -177,7 +177,6 @@ struct execute_work {
 #define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
 	.timer = __TIMER_INITIALIZER((TIMER_FUNC_TYPE)delayed_work_timer_fn,\
-				     (TIMER_DATA_TYPE)&(n.timer),	\
 				     (tflags) | TIMER_IRQSAFE),		\
 	}
 
@@ -244,7 +243,6 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		INIT_WORK(&(_work)->work, (_func));			\
 		__setup_timer(&(_work)->timer,				\
 			      (TIMER_FUNC_TYPE)delayed_work_timer_fn,	\
-			      (TIMER_DATA_TYPE)&(_work)->timer,		\
 			      (_tflags) | TIMER_IRQSAFE);		\
 	} while (0)
 
@@ -253,7 +251,6 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		INIT_WORK_ONSTACK(&(_work)->work, (_func));		\
 		__setup_timer_on_stack(&(_work)->timer,			\
 				       (TIMER_FUNC_TYPE)delayed_work_timer_fn,\
-				       (TIMER_DATA_TYPE)&(_work)->timer,\
 				       (_tflags) | TIMER_IRQSAFE);	\
 	} while (0)
 

commit 2bcc673101268dc50e52b83226c5bbf38391e16d
Merge: 670310dfbae0 b24591e2fcf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 17:56:58 2017 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "Yet another big pile of changes:
    
       - More year 2038 work from Arnd slowly reaching the point where we
         need to think about the syscalls themself.
    
       - A new timer function which allows to conditionally (re)arm a timer
         only when it's either not running or the new expiry time is sooner
         than the armed expiry time. This allows to use a single timer for
         multiple timeout requirements w/o caring about the first expiry
         time at the call site.
    
       - A new NMI safe accessor to clock real time for the printk timestamp
         work. Can be used by tracing, perf as well if required.
    
       - A large number of timer setup conversions from Kees which got
         collected here because either maintainers requested so or they
         simply got ignored. As Kees pointed out already there are a few
         trivial merge conflicts and some redundant commits which was
         unavoidable due to the size of this conversion effort.
    
       - Avoid a redundant iteration in the timer wheel softirq processing.
    
       - Provide a mechanism to treat RTC implementations depending on their
         hardware properties, i.e. don't inflict the write at the 0.5
         seconds boundary which originates from the PC CMOS RTC to all RTCs.
         No functional change as drivers need to be updated separately.
    
       - The usual small updates to core code clocksource drivers. Nothing
         really exciting"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (111 commits)
      timers: Add a function to start/reduce a timer
      pstore: Use ktime_get_real_fast_ns() instead of __getnstimeofday()
      timer: Prepare to change all DEFINE_TIMER() callbacks
      netfilter: ipvs: Convert timers to use timer_setup()
      scsi: qla2xxx: Convert timers to use timer_setup()
      block/aoe: discover_timer: Convert timers to use timer_setup()
      ide: Convert timers to use timer_setup()
      drbd: Convert timers to use timer_setup()
      mailbox: Convert timers to use timer_setup()
      crypto: Convert timers to use timer_setup()
      drivers/pcmcia: omap1: Fix error in automated timer conversion
      ARM: footbridge: Fix typo in timer conversion
      drivers/sgi-xp: Convert timers to use timer_setup()
      drivers/pcmcia: Convert timers to use timer_setup()
      drivers/memstick: Convert timers to use timer_setup()
      drivers/macintosh: Convert timers to use timer_setup()
      hwrng/xgene-rng: Convert timers to use timer_setup()
      auxdisplay: Convert timers to use timer_setup()
      sparc/led: Convert timers to use timer_setup()
      mips: ip22/32: Convert timers to use timer_setup()
      ...

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1c49431f3121..0eae11fc7a23 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * workqueue.h --- work queue handling for Linux.
  */

commit fd1a5b04dfb899f84ddeb8acdaea6b98283df1e5
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Wed Oct 25 17:56:04 2017 +0900

    workqueue: Remove now redundant lock acquisitions wrt. workqueue flushes
    
    The workqueue code added manual lock acquisition annotations to catch
    deadlocks.
    
    After lockdepcrossrelease was introduced, some of those became redundant,
    since wait_for_completion() already does the acquisition and tracking.
    
    Remove the duplicate annotations.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: amir73il@gmail.com
    Cc: axboe@kernel.dk
    Cc: darrick.wong@oracle.com
    Cc: david@fromorbit.com
    Cc: hch@infradead.org
    Cc: idryomov@gmail.com
    Cc: johan@kernel.org
    Cc: johannes.berg@intel.com
    Cc: kernel-team@lge.com
    Cc: linux-block@vger.kernel.org
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-xfs@vger.kernel.org
    Cc: oleg@redhat.com
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/1508921765-15396-9-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1c49431f3121..c8a572cb49be 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -218,7 +218,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 									\
 		__init_work((_work), _onstack);				\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
-		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0); \
+		lockdep_init_map(&(_work)->lockdep_map, "(work_completion)"#_work, &__key, 0); \
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		(_work)->func = (_func);				\
 	} while (0)
@@ -398,7 +398,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	static struct lock_class_key __key;				\
 	const char *__lock_name;					\
 									\
-	__lock_name = #fmt#args;					\
+	__lock_name = "(wq_completion)"#fmt#args;			\
 									\
 	__alloc_workqueue_key((fmt), (flags), (max_active),		\
 			      &__key, __lock_name, ##args);		\

commit 8c20feb60604d91a29cd7fef8ac758bd92d9fd2c
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 4 16:27:07 2017 -0700

    workqueue: Convert callback to use from_timer()
    
    In preparation for unconditionally passing the struct timer_list pointer
    to all timer callbacks, switch workqueue to use from_timer() and pass the
    timer pointer explicitly.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Sebastian Reichel <sre@kernel.org>
    Cc: Kalle Valo <kvalo@qca.qualcomm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: linux1394-devel@lists.sourceforge.net
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: linux-s390@vger.kernel.org
    Cc: linux-wireless@vger.kernel.org
    Cc: "James E.J. Bottomley" <jejb@linux.vnet.ibm.com>
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Harish Patil <harish.patil@cavium.com>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Manish Chopra <manish.chopra@cavium.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Julian Wiedmann <jwi@linux.vnet.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Mark Gross <mark.gross@intel.com>
    Cc: linux-watchdog@vger.kernel.org
    Cc: linux-scsi@vger.kernel.org
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Stefan Richter <stefanr@s5r6.in-berlin.de>
    Cc: Michael Reed <mdr@sgi.com>
    Cc: netdev@vger.kernel.org
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Link: https://lkml.kernel.org/r/1507159627-127660-14-git-send-email-keescook@chromium.org

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f4960260feaf..f3c47a05fd06 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -17,7 +17,7 @@ struct workqueue_struct;
 
 struct work_struct;
 typedef void (*work_func_t)(struct work_struct *work);
-void delayed_work_timer_fn(unsigned long __data);
+void delayed_work_timer_fn(struct timer_list *t);
 
 /*
  * The first word is the work queue pointer and the flags rolled into
@@ -175,8 +175,8 @@ struct execute_work {
 
 #define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
-	.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,		\
-				     (unsigned long)&(n),		\
+	.timer = __TIMER_INITIALIZER((TIMER_FUNC_TYPE)delayed_work_timer_fn,\
+				     (TIMER_DATA_TYPE)&(n.timer),	\
 				     (tflags) | TIMER_IRQSAFE),		\
 	}
 
@@ -241,8 +241,9 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define __INIT_DELAYED_WORK(_work, _func, _tflags)			\
 	do {								\
 		INIT_WORK(&(_work)->work, (_func));			\
-		__setup_timer(&(_work)->timer, delayed_work_timer_fn,	\
-			      (unsigned long)(_work),			\
+		__setup_timer(&(_work)->timer,				\
+			      (TIMER_FUNC_TYPE)delayed_work_timer_fn,	\
+			      (TIMER_DATA_TYPE)&(_work)->timer,		\
 			      (_tflags) | TIMER_IRQSAFE);		\
 	} while (0)
 
@@ -250,8 +251,8 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 	do {								\
 		INIT_WORK_ONSTACK(&(_work)->work, (_func));		\
 		__setup_timer_on_stack(&(_work)->timer,			\
-				       delayed_work_timer_fn,		\
-				       (unsigned long)(_work),		\
+				       (TIMER_FUNC_TYPE)delayed_work_timer_fn,\
+				       (TIMER_DATA_TYPE)&(_work)->timer,\
 				       (_tflags) | TIMER_IRQSAFE);	\
 	} while (0)
 

commit 8ede369b2cccfa585e2969bbed18edc0e2a18c50
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 4 16:27:05 2017 -0700

    timer: Remove expires argument from __TIMER_INITIALIZER()
    
    The expires field is normally initialized during the first mod_timer()
    call. It was unused by all callers, so remove it from the macro.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: linux-mips@linux-mips.org
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Sebastian Reichel <sre@kernel.org>
    Cc: Kalle Valo <kvalo@qca.qualcomm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: linux1394-devel@lists.sourceforge.net
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: linux-s390@vger.kernel.org
    Cc: linux-wireless@vger.kernel.org
    Cc: "James E.J. Bottomley" <jejb@linux.vnet.ibm.com>
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Harish Patil <harish.patil@cavium.com>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Michael Reed <mdr@sgi.com>
    Cc: Manish Chopra <manish.chopra@cavium.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Julian Wiedmann <jwi@linux.vnet.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Mark Gross <mark.gross@intel.com>
    Cc: linux-watchdog@vger.kernel.org
    Cc: linux-scsi@vger.kernel.org
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Stefan Richter <stefanr@s5r6.in-berlin.de>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: netdev@vger.kernel.org
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Link: https://lkml.kernel.org/r/1507159627-127660-12-git-send-email-keescook@chromium.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1c49431f3121..f4960260feaf 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -176,7 +176,7 @@ struct execute_work {
 #define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
 	.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,		\
-				     0, (unsigned long)&(n),		\
+				     (unsigned long)&(n),		\
 				     (tflags) | TIMER_IRQSAFE),		\
 	}
 

commit fbf1c41fc0f4d3574ac2377245efd666c1fa3075
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sun Sep 3 01:18:41 2017 +0100

    workqueue: Fix flag collision
    
    Commit 0a94efb5acbb ("workqueue: implicit ordered attribute should be
    overridable") introduced a __WQ_ORDERED_EXPLICIT flag but gave it the
    same value as __WQ_LEGACY.  I don't believe these were intended to
    mean the same thing, so renumber __WQ_ORDERED_EXPLICIT.
    
    Fixes: 0a94efb5acbb ("workqueue: implicit ordered attribute should be ...")
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Cc: stable@vger.kernel.org # v4.13
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index db6dc9dc0482..1c49431f3121 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -323,8 +323,8 @@ enum {
 
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
-	__WQ_ORDERED_EXPLICIT	= 1 << 18, /* internal: alloc_ordered_workqueue() */
 	__WQ_LEGACY		= 1 << 18, /* internal: create*_workqueue() */
+	__WQ_ORDERED_EXPLICIT	= 1 << 19, /* internal: alloc_ordered_workqueue() */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */

commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index c102ef65cb64..db6dc9dc0482 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -323,6 +323,7 @@ enum {
 
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
+	__WQ_ORDERED_EXPLICIT	= 1 << 18, /* internal: alloc_ordered_workqueue() */
 	__WQ_LEGACY		= 1 << 18, /* internal: create*_workqueue() */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
@@ -422,7 +423,8 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
 #define alloc_ordered_workqueue(fmt, flags, args...)			\
-	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)
+	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED |		\
+			__WQ_ORDERED_EXPLICIT | (flags), 1, ##args)
 
 #define create_workqueue(name)						\
 	alloc_workqueue("%s", __WQ_LEGACY | WQ_MEM_RECLAIM, 1, (name))

commit 0e8d6a9336b487a1dd6f1991ff376e669d4c87c6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:28 2017 +0200

    workqueue: Provide work_on_cpu_safe()
    
    work_on_cpu() is not protected against CPU hotplug. For code which requires
    to be either executed on an online CPU or to fail if the CPU is not
    available the callsite would have to protect against CPU hotplug.
    
    Provide a function which does get/put_online_cpus() around the call to
    work_on_cpu() and fails the call with -ENODEV if the target CPU is not
    online.
    
    Preparatory patch to convert several racy task affinity manipulations.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.262610721@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index bde063cefd04..c102ef65cb64 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -608,8 +608,13 @@ static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
 {
 	return fn(arg);
 }
+static inline long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg)
+{
+	return fn(arg);
+}
 #else
 long work_on_cpu(int cpu, long (*fn)(void *), void *arg);
+long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg);
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_FREEZER

commit a45463cbf3f9dcdae683033c256f50bded513d6a
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 1 18:01:17 2017 +0100

    workqueue: avoid clang warning
    
    Building with clang shows lots of warning like:
    
    drivers/amba/bus.c:447:8: warning: implicit conversion from 'long long' to 'int' changes value from 4294967248 to -48
          [-Wconstant-conversion]
    static DECLARE_DELAYED_WORK(deferred_retry_work, amba_deferred_retry_func);
           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    include/linux/workqueue.h:187:26: note: expanded from macro 'DECLARE_DELAYED_WORK'
            struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, 0)
                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    include/linux/workqueue.h:177:10: note: expanded from macro '__DELAYED_WORK_INITIALIZER'
            .work = __WORK_INITIALIZER((n).work, (f)),                      \
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    include/linux/workqueue.h:170:10: note: expanded from macro '__WORK_INITIALIZER'
            .data = WORK_DATA_STATIC_INIT(),                                \
                    ^~~~~~~~~~~~~~~~~~~~~~~
    include/linux/workqueue.h:111:39: note: expanded from macro 'WORK_DATA_STATIC_INIT'
            ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
    include/asm-generic/atomic-long.h:32:41: note: expanded from macro 'ATOMIC_LONG_INIT'
     #define ATOMIC_LONG_INIT(i)     ATOMIC_INIT(i)
                                    ~~~~~~~~~~~~^~
    arch/arm/include/asm/atomic.h:21:27: note: expanded from macro 'ATOMIC_INIT'
     #define ATOMIC_INIT(i)  { (i) }
                            ~  ^
    
    This makes the type cast explicit, which shuts up the warning.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a26cc437293c..bde063cefd04 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -106,9 +106,9 @@ struct work_struct {
 #endif
 };
 
-#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL)
+#define WORK_DATA_INIT()	ATOMIC_LONG_INIT((unsigned long)WORK_STRUCT_NO_POOL)
 #define WORK_DATA_STATIC_INIT()	\
-	ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC)
+	ATOMIC_LONG_INIT((unsigned long)(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC))
 
 struct delayed_work {
 	struct work_struct work;

commit c11a6cfb0103d5d831e20bd9b75d10d13519fec5
Merge: e6efef7260ac 8bc4a0445596
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 12:59:57 2016 -0800

    Merge branch 'for-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "Mostly patches to initialize workqueue subsystem earlier and get rid
      of keventd_up().
    
      The patches were headed for the last merge cycle but got delayed due
      to a bug found late minute, which is fixed now.
    
      Also, to help debugging, destroy_workqueue() is more chatty now on a
      sanity check failure."
    
    * 'for-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: move wq_numa_init() to workqueue_init()
      workqueue: remove keventd_up()
      debugobj, workqueue: remove keventd_up() usage
      slab, workqueue: remove keventd_up() usage
      power, workqueue: remove keventd_up() usage
      tty, workqueue: remove keventd_up() usage
      mce, workqueue: remove keventd_up() usage
      workqueue: make workqueue available early during boot
      workqueue: dump workqueue state on sanity check failures in destroy_workqueue()

commit 42412c3aae5d8ea57a46b8ff86bb67bc1a270d9c
Author: Silvio Fricke <silvio.fricke@gmail.com>
Date:   Fri Oct 28 10:14:09 2016 +0200

    workqueue: kerneldocify workqueue_attrs
    
    Only formating changes.
    
    Signed-off-by: Silvio Fricke <silvio.fricke@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index fc6e22186405..d4f16cf6281c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -119,18 +119,30 @@ struct delayed_work {
 	int cpu;
 };
 
-/*
- * A struct for workqueue attributes.  This can be used to change
- * attributes of an unbound workqueue.
+/**
+ * struct workqueue_attrs - A struct for workqueue attributes.
  *
- * Unlike other fields, ->no_numa isn't a property of a worker_pool.  It
- * only modifies how apply_workqueue_attrs() select pools and thus doesn't
- * participate in pool hash calculations or equality comparisons.
+ * This can be used to change attributes of an unbound workqueue.
  */
 struct workqueue_attrs {
-	int			nice;		/* nice level */
-	cpumask_var_t		cpumask;	/* allowed CPUs */
-	bool			no_numa;	/* disable NUMA affinity */
+	/**
+	 * @nice: nice level
+	 */
+	int nice;
+
+	/**
+	 * @cpumask: allowed CPUs
+	 */
+	cpumask_var_t cpumask;
+
+	/**
+	 * @no_numa: disable NUMA affinity
+	 *
+	 * Unlike other fields, ``no_numa`` isn't a property of a worker_pool. It
+	 * only modifies how :c:func:`apply_workqueue_attrs` select pools and thus
+	 * doesn't participate in pool hash calculations or equality comparisons.
+	 */
+	bool no_numa;
 };
 
 static inline struct delayed_work *to_delayed_work(struct work_struct *work)
@@ -272,7 +284,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 
 /*
  * Workqueue flags and constants.  For details, please refer to
- * Documentation/workqueue.txt.
+ * Documentation/core-api/workqueue.rst.
  */
 enum {
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
@@ -370,7 +382,8 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * @args...: args for @fmt
  *
  * Allocate a workqueue with the specified parameters.  For detailed
- * information on WQ_* flags, please refer to Documentation/workqueue.txt.
+ * information on WQ_* flags, please refer to
+ * Documentation/core-api/workqueue.rst.
  *
  * The __lock_name macro dance is to guarantee that single lock_class_key
  * doesn't end up with different namesm, which isn't allowed by lockdep.

commit 8bc4a04455969c36bf54a942ad9d28d80969ed51
Merge: 1001354ca341 2186d9f940b6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Oct 19 12:12:40 2016 -0400

    Merge branch 'for-4.9' into for-4.10

commit 863b710b664bdcb90c0c682ee24adb368f497a5b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 16 15:49:34 2016 -0400

    workqueue: remove keventd_up()
    
    keventd_up() no longer has in-kernel users.  Remove it and make
    wq_online static.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 91d416f9c0a7..56417133c672 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -358,8 +358,6 @@ extern struct workqueue_struct *system_freezable_wq;
 extern struct workqueue_struct *system_power_efficient_wq;
 extern struct workqueue_struct *system_freezable_power_efficient_wq;
 
-extern bool wq_online;
-
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);
@@ -591,14 +589,6 @@ static inline bool schedule_delayed_work(struct delayed_work *dwork,
 	return queue_delayed_work(system_wq, dwork, delay);
 }
 
-/**
- * keventd_up - is workqueue initialized yet?
- */
-static inline bool keventd_up(void)
-{
-	return wq_online;
-}
-
 #ifndef CONFIG_SMP
 static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
 {

commit 3347fa0928210d96aaa2bd6cd5a8391d5e630873
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 16 15:49:32 2016 -0400

    workqueue: make workqueue available early during boot
    
    Workqueue is currently initialized in an early init call; however,
    there are cases where early boot code has to be split and reordered to
    come after workqueue initialization or the same code path which makes
    use of workqueues is used both before workqueue initailization and
    after.  The latter cases have to gate workqueue usages with
    keventd_up() tests, which is nasty and easy to get wrong.
    
    Workqueue usages have become widespread and it'd be a lot more
    convenient if it can be used very early from boot.  This patch splits
    workqueue initialization into two steps.  workqueue_init_early() which
    sets up the basic data structures so that workqueues can be created
    and work items queued, and workqueue_init() which actually brings up
    workqueues online and starts executing queued work items.  The former
    step can be done very early during boot once memory allocation,
    cpumasks and idr are initialized.  The latter right after kthreads
    become available.
    
    This allows work item queueing and canceling from very early boot
    which is what most of these use cases want.
    
    * As systemd_wq being initialized doesn't indicate that workqueue is
      fully online anymore, update keventd_up() to test wq_online instead.
      The follow-up patches will get rid of all its usages and the
      function itself.
    
    * Flushing doesn't make sense before workqueue is fully initialized.
      The flush functions trigger WARN and return immediately before fully
      online.
    
    * Work items are never in-flight before fully online.  Canceling can
      always succeed by skipping the flush step.
    
    * Some code paths can no longer assume to be called with irq enabled
      as irq is disabled during early boot.  Use irqsave/restore
      operations instead.
    
    v2: Watchdog init, which requires timer to be running, moved from
        workqueue_init_early() to workqueue_init().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/CA+55aFx0vPuMuxn00rBSM192n-Du5uxy+4AvKa0SBSOVJeuCGg@mail.gmail.com

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 26cc1df280d6..91d416f9c0a7 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -358,6 +358,8 @@ extern struct workqueue_struct *system_freezable_wq;
 extern struct workqueue_struct *system_power_efficient_wq;
 extern struct workqueue_struct *system_freezable_power_efficient_wq;
 
+extern bool wq_online;
+
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);
@@ -594,7 +596,7 @@ static inline bool schedule_delayed_work(struct delayed_work *dwork,
  */
 static inline bool keventd_up(void)
 {
-	return system_wq != NULL;
+	return wq_online;
 }
 
 #ifndef CONFIG_SMP
@@ -631,4 +633,7 @@ int workqueue_online_cpu(unsigned int cpu);
 int workqueue_offline_cpu(unsigned int cpu);
 #endif
 
+int __init workqueue_init_early(void);
+int __init workqueue_init(void);
+
 #endif

commit f72b8792d180948b4b3898374998f5ac8c02e539
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Aug 24 15:51:50 2016 -0600

    workqueue: add cancel_work()
    
    Like cancel_delayed_work(), but for regular work.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Mehed-by: Tejun Heo <tj@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 26cc1df280d6..fc6e22186405 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -442,6 +442,7 @@ extern int schedule_on_each_cpu(work_func_t func);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern bool flush_work(struct work_struct *work);
+extern bool cancel_work(struct work_struct *work);
 extern bool cancel_work_sync(struct work_struct *work);
 
 extern bool flush_delayed_work(struct delayed_work *dwork);

commit 7ee681b25284782ecf380bf5ccf55f13c52fd0ce
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:16:29 2016 +0000

    workqueue: Convert to state machine callbacks
    
    Get rid of the prio ordering of the separate notifiers and use a proper state
    callback pair.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153335.197083890@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ca73c503b92a..26cc1df280d6 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -625,4 +625,10 @@ void wq_watchdog_touch(int cpu);
 static inline void wq_watchdog_touch(int cpu) { }
 #endif	/* CONFIG_WQ_WATCHDOG */
 
+#ifdef CONFIG_SMP
+int workqueue_prepare_cpu(unsigned int cpu);
+int workqueue_online_cpu(unsigned int cpu);
+int workqueue_offline_cpu(unsigned int cpu);
+#endif
+
 #endif

commit 23d11a58a9a60dcb52c8fc6494efce908b24c295
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 29 05:59:46 2016 -0500

    workqueue: skip flush dependency checks for legacy workqueues
    
    fca839c00a12 ("workqueue: warn if memory reclaim tries to flush
    !WQ_MEM_RECLAIM workqueue") implemented flush dependency warning which
    triggers if a PF_MEMALLOC task or WQ_MEM_RECLAIM workqueue tries to
    flush a !WQ_MEM_RECLAIM workquee.
    
    This assumes that workqueues marked with WQ_MEM_RECLAIM sit in memory
    reclaim path and making it depend on something which may need more
    memory to make forward progress can lead to deadlocks.  Unfortunately,
    workqueues created with the legacy create*_workqueue() interface
    always have WQ_MEM_RECLAIM regardless of whether they are depended
    upon memory reclaim or not.  These spurious WQ_MEM_RECLAIM markings
    cause spurious triggering of the flush dependency checks.
    
      WARNING: CPU: 0 PID: 6 at kernel/workqueue.c:2361 check_flush_dependency+0x138/0x144()
      workqueue: WQ_MEM_RECLAIM deferwq:deferred_probe_work_func is flushing !WQ_MEM_RECLAIM events:lru_add_drain_per_cpu
      ...
      Workqueue: deferwq deferred_probe_work_func
      [<c0017acc>] (unwind_backtrace) from [<c0013134>] (show_stack+0x10/0x14)
      [<c0013134>] (show_stack) from [<c0245f18>] (dump_stack+0x94/0xd4)
      [<c0245f18>] (dump_stack) from [<c0026f9c>] (warn_slowpath_common+0x80/0xb0)
      [<c0026f9c>] (warn_slowpath_common) from [<c0026ffc>] (warn_slowpath_fmt+0x30/0x40)
      [<c0026ffc>] (warn_slowpath_fmt) from [<c00390b8>] (check_flush_dependency+0x138/0x144)
      [<c00390b8>] (check_flush_dependency) from [<c0039ca0>] (flush_work+0x50/0x15c)
      [<c0039ca0>] (flush_work) from [<c00c51b0>] (lru_add_drain_all+0x130/0x180)
      [<c00c51b0>] (lru_add_drain_all) from [<c00f728c>] (migrate_prep+0x8/0x10)
      [<c00f728c>] (migrate_prep) from [<c00bfbc4>] (alloc_contig_range+0xd8/0x338)
      [<c00bfbc4>] (alloc_contig_range) from [<c00f8f18>] (cma_alloc+0xe0/0x1ac)
      [<c00f8f18>] (cma_alloc) from [<c001cac4>] (__alloc_from_contiguous+0x38/0xd8)
      [<c001cac4>] (__alloc_from_contiguous) from [<c001ceb4>] (__dma_alloc+0x240/0x278)
      [<c001ceb4>] (__dma_alloc) from [<c001cf78>] (arm_dma_alloc+0x54/0x5c)
      [<c001cf78>] (arm_dma_alloc) from [<c0355ea4>] (dmam_alloc_coherent+0xc0/0xec)
      [<c0355ea4>] (dmam_alloc_coherent) from [<c039cc4c>] (ahci_port_start+0x150/0x1dc)
      [<c039cc4c>] (ahci_port_start) from [<c0384734>] (ata_host_start.part.3+0xc8/0x1c8)
      [<c0384734>] (ata_host_start.part.3) from [<c03898dc>] (ata_host_activate+0x50/0x148)
      [<c03898dc>] (ata_host_activate) from [<c039d558>] (ahci_host_activate+0x44/0x114)
      [<c039d558>] (ahci_host_activate) from [<c039f05c>] (ahci_platform_init_host+0x1d8/0x3c8)
      [<c039f05c>] (ahci_platform_init_host) from [<c039e6bc>] (tegra_ahci_probe+0x448/0x4e8)
      [<c039e6bc>] (tegra_ahci_probe) from [<c0347058>] (platform_drv_probe+0x50/0xac)
      [<c0347058>] (platform_drv_probe) from [<c03458cc>] (driver_probe_device+0x214/0x2c0)
      [<c03458cc>] (driver_probe_device) from [<c0343cc0>] (bus_for_each_drv+0x60/0x94)
      [<c0343cc0>] (bus_for_each_drv) from [<c03455d8>] (__device_attach+0xb0/0x114)
      [<c03455d8>] (__device_attach) from [<c0344ab8>] (bus_probe_device+0x84/0x8c)
      [<c0344ab8>] (bus_probe_device) from [<c0344f48>] (deferred_probe_work_func+0x68/0x98)
      [<c0344f48>] (deferred_probe_work_func) from [<c003b738>] (process_one_work+0x120/0x3f8)
      [<c003b738>] (process_one_work) from [<c003ba48>] (worker_thread+0x38/0x55c)
      [<c003ba48>] (worker_thread) from [<c0040f14>] (kthread+0xdc/0xf4)
      [<c0040f14>] (kthread) from [<c000f778>] (ret_from_fork+0x14/0x3c)
    
    Fix it by marking workqueues created via create*_workqueue() with
    __WQ_LEGACY and disabling flush dependency checks on them.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Thierry Reding <thierry.reding@gmail.com>
    Link: http://lkml.kernel.org/g/20160126173843.GA11115@ulmo.nvidia.com
    Fixes: fca839c00a12 ("workqueue: warn if memory reclaim tries to flush !WQ_MEM_RECLAIM workqueue")

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0e32bc71245e..ca73c503b92a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -311,6 +311,7 @@ enum {
 
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
+	__WQ_LEGACY		= 1 << 18, /* internal: create*_workqueue() */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
@@ -411,12 +412,12 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)
 
 #define create_workqueue(name)						\
-	alloc_workqueue("%s", WQ_MEM_RECLAIM, 1, (name))
+	alloc_workqueue("%s", __WQ_LEGACY | WQ_MEM_RECLAIM, 1, (name))
 #define create_freezable_workqueue(name)				\
-	alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, \
-			1, (name))
+	alloc_workqueue("%s", __WQ_LEGACY | WQ_FREEZABLE | WQ_UNBOUND |	\
+			WQ_MEM_RECLAIM, 1, (name))
 #define create_singlethread_workqueue(name)				\
-	alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)
+	alloc_ordered_workqueue("%s", __WQ_LEGACY | WQ_MEM_RECLAIM, name)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 82607adcf9cdf40fb7b5331269780c8f70ec6e35
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 8 11:28:04 2015 -0500

    workqueue: implement lockup detector
    
    Workqueue stalls can happen from a variety of usage bugs such as
    missing WQ_MEM_RECLAIM flag or concurrency managed work item
    indefinitely staying RUNNING.  These stalls can be extremely difficult
    to hunt down because the usual warning mechanisms can't detect
    workqueue stalls and the internal state is pretty opaque.
    
    To alleviate the situation, this patch implements workqueue lockup
    detector.  It periodically monitors all worker_pools periodically and,
    if any pool failed to make forward progress longer than the threshold
    duration, triggers warning and dumps workqueue state as follows.
    
     BUG: workqueue lockup - pool cpus=0 node=0 flags=0x0 nice=0 stuck for 31s!
     Showing busy workqueues and worker pools:
     workqueue events: flags=0x0
       pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=17/256
         pending: monkey_wrench_fn, e1000_watchdog, cache_reap, vmstat_shepherd, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, release_one_tty, cgroup_release_agent
     workqueue events_power_efficient: flags=0x80
       pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=2/256
         pending: check_lifetime, neigh_periodic_work
     workqueue cgroup_pidlist_destroy: flags=0x0
       pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=1/1
         pending: cgroup_pidlist_destroy_work_fn
     ...
    
    The detection mechanism is controller through kernel parameter
    workqueue.watchdog_thresh and can be updated at runtime through the
    sysfs module parameter file.
    
    v2: Decoupled from softlockup control knobs.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0197358f1e81..0e32bc71245e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -618,4 +618,10 @@ static inline int workqueue_sysfs_register(struct workqueue_struct *wq)
 { return 0; }
 #endif	/* CONFIG_SYSFS */
 
+#ifdef CONFIG_WQ_WATCHDOG
+void wq_watchdog_touch(int cpu);
+#else	/* CONFIG_WQ_WATCHDOG */
+static inline void wq_watchdog_touch(int cpu) { }
+#endif	/* CONFIG_WQ_WATCHDOG */
+
 #endif

commit 355c06633e233a57155b827ebe99b91c35bc1f5c
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Thu Aug 13 17:52:02 2015 -0600

    workqueue: fix some docbook warnings
    
    There are some errors in the docbook comments in workqueue.h that cause
    warnings when the docs are built; this only recently came to light because
    these comments were not used until now.  Fix the comments to make the
    warnings go away.
    
    The "args..." "fix" is a hack.  kerneldoc doesn't deal properly with named
    variadic arguments in macros, so all I've really achieved here is to make
    it shut up.  Fixing kerneldoc will have to wait for more time.
    
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 738b30b39b68..0197358f1e81 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -265,7 +265,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 /**
  * delayed_work_pending - Find out whether a delayable work item is currently
  * pending
- * @work: The work item in question
+ * @w: The work item in question
  */
 #define delayed_work_pending(w) \
 	work_pending(&(w)->work)
@@ -366,7 +366,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * @fmt: printf format for the name of the workqueue
  * @flags: WQ_* flags
  * @max_active: max in-flight work items, 0 for default
- * @args: args for @fmt
+ * @args...: args for @fmt
  *
  * Allocate a workqueue with the specified parameters.  For detailed
  * information on WQ_* flags, please refer to Documentation/workqueue.txt.
@@ -398,7 +398,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * alloc_ordered_workqueue - allocate an ordered workqueue
  * @fmt: printf format for the name of the workqueue
  * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
- * @args: args for @fmt
+ * @args...: args for @fmt
  *
  * Allocate an ordered workqueue.  An ordered workqueue executes at
  * most one work item at any given time in the queued order.  They are

commit 37b1ef31a568fc02e53587620226e5f3c66454c8
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed May 20 14:41:19 2015 +0800

    workqueue: move flush_scheduled_work() to workqueue.h
    
    flush_scheduled_work() is just a simple call to flush_work().
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4618dd672d1b..738b30b39b68 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -435,7 +435,6 @@ extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
-extern void flush_scheduled_work(void);
 
 extern int schedule_on_each_cpu(work_func_t func);
 
@@ -531,6 +530,35 @@ static inline bool schedule_work(struct work_struct *work)
 	return queue_work(system_wq, work);
 }
 
+/**
+ * flush_scheduled_work - ensure that any scheduled work has run to completion.
+ *
+ * Forces execution of the kernel-global workqueue and blocks until its
+ * completion.
+ *
+ * Think twice before calling this function!  It's very easy to get into
+ * trouble if you don't take great care.  Either of the following situations
+ * will lead to deadlock:
+ *
+ *	One of the work items currently on the workqueue needs to acquire
+ *	a lock held by your code or its caller.
+ *
+ *	Your code is running in the context of a work routine.
+ *
+ * They will be detected by lockdep when they occur, but the first might not
+ * occur very often.  It depends on what work items are on the workqueue and
+ * what locks they need, which you have no control over.
+ *
+ * In most situations flushing the entire workqueue is overkill; you merely
+ * need to know that a particular work item isn't queued and isn't running.
+ * In such cases you should use cancel_delayed_work_sync() or
+ * cancel_work_sync() instead.
+ */
+static inline void flush_scheduled_work(void)
+{
+	flush_workqueue(system_wq);
+}
+
 /**
  * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
  * @cpu: cpu to use

commit 042f7df15a4fff8eec42873f755aea848dcdedd1
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Apr 30 17:16:12 2015 +0800

    workqueue: Allow modifying low level unbound workqueue cpumask
    
    Allow to modify the low-level unbound workqueues cpumask through
    sysfs. This is performed by traversing the entire workqueue list
    and calling apply_wqattrs_prepare() on the unbound workqueues
    with the new low level mask. Only after all the preparation are done,
    we commit them all together.
    
    Ordered workqueues are ignored from the low level unbound workqueue
    cpumask, it will be handled in near future.
    
    All the (default & per-node) pwqs are mandatorily controlled by
    the low level cpumask. If the user configured cpumask doesn't overlap
    with the low level cpumask, the low level cpumask will be used for the
    wq instead.
    
    The comment of wq_calc_node_cpumask() is updated and explicitly
    requires that its first argument should be the attrs of the default
    pwq.
    
    The default wq_unbound_cpumask is cpu_possible_mask.  The workqueue
    subsystem doesn't know its best default value, let the system manager
    or the other subsystem set it when needed.
    
    Changed from V8:
      merge the calculating code for the attrs of the default pwq together.
      minor change the code&comments for saving the user configured attrs.
      remove unnecessary list_del().
      minor update the comment of wq_calc_node_cpumask().
      update the comment of workqueue_set_unbound_cpumask();
    
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Original-patch-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index deee212af8e0..4618dd672d1b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -424,6 +424,7 @@ struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);
 void free_workqueue_attrs(struct workqueue_attrs *attrs);
 int apply_workqueue_attrs(struct workqueue_struct *wq,
 			  const struct workqueue_attrs *attrs);
+int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);

commit 3494fc30846dceb808de4cc02930ef347fabd21a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 9 09:22:28 2015 -0400

    workqueue: dump workqueues on sysrq-t
    
    Workqueues are used extensively throughout the kernel but sometimes
    it's difficult to debug stalls involving work items because visibility
    into its inner workings is fairly limited.  Although sysrq-t task dump
    annotates each active worker task with the information on the work
    item being executed, it is challenging to find out which work items
    are pending or delayed on which queues and how pools are being
    managed.
    
    This patch implements show_workqueue_state() which dumps all busy
    workqueues and pools and is called from the sysrq-t handler.  At the
    end of sysrq-t dump, something like the following is printed.
    
     Showing busy workqueues and worker pools:
     ...
     workqueue filler_wq: flags=0x0
       pwq 2: cpus=1 node=0 flags=0x0 nice=0 active=2/256
         in-flight: 491:filler_workfn, 507:filler_workfn
       pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=2/256
         in-flight: 501:filler_workfn
         pending: filler_workfn
     ...
     workqueue test_wq: flags=0x8
       pwq 2: cpus=1 node=0 flags=0x0 nice=0 active=1/1
         in-flight: 510(RESCUER):test_workfn BAR(69) BAR(500)
         delayed: test_workfn1 BAR(492), test_workfn2
     ...
     pool 0: cpus=0 node=0 flags=0x0 nice=0 workers=2 manager: 137
     pool 2: cpus=1 node=0 flags=0x0 nice=0 workers=3 manager: 469
     pool 3: cpus=1 node=0 flags=0x0 nice=-20 workers=2 idle: 16
     pool 8: cpus=0-3 flags=0x4 nice=0 workers=2 manager: 62
    
    The above shows that test_wq is executing test_workfn() on pid 510
    which is the rescuer and also that there are two tasks 69 and 500
    waiting for the work item to finish in flush_work().  As test_wq has
    max_active of 1, there are two work items for test_workfn1() and
    test_workfn2() which are delayed till the current work item is
    finished.  In addition, pid 492 is flushing test_workfn1().
    
    The work item for test_workfn() is being executed on pwq of pool 2
    which is the normal priority per-cpu pool for CPU 1.  The pool has
    three workers, two of which are executing filler_workfn() for
    filler_wq and the last one is assuming the manager role trying to
    create more workers.
    
    This extra workqueue state dump will hopefully help chasing down hangs
    involving workqueues.
    
    v3: cpulist_pr_cont() replaced with "%*pbl" printf formatting.
    
    v2: As suggested by Andrew, minor formatting change in pr_cont_work(),
        printk()'s replaced with pr_info()'s, and cpumask printing now
        uses cpulist_pr_cont().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f597846ff605..deee212af8e0 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -454,6 +454,7 @@ extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);
 extern __printf(1, 2) void set_worker_desc(const char *fmt, ...);
 extern void print_worker_info(const char *log_lvl, struct task_struct *task);
+extern void show_workqueue_state(void);
 
 /**
  * queue_work - queue work on a workqueue

commit 8603e1b30027f943cc9c1eef2b291d42c3347af1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 5 08:04:13 2015 -0500

    workqueue: fix hang involving racing cancel[_delayed]_work_sync()'s for PREEMPT_NONE
    
    cancel[_delayed]_work_sync() are implemented using
    __cancel_work_timer() which grabs the PENDING bit using
    try_to_grab_pending() and then flushes the work item with PENDING set
    to prevent the on-going execution of the work item from requeueing
    itself.
    
    try_to_grab_pending() can always grab PENDING bit without blocking
    except when someone else is doing the above flushing during
    cancelation.  In that case, try_to_grab_pending() returns -ENOENT.  In
    this case, __cancel_work_timer() currently invokes flush_work().  The
    assumption is that the completion of the work item is what the other
    canceling task would be waiting for too and thus waiting for the same
    condition and retrying should allow forward progress without excessive
    busy looping
    
    Unfortunately, this doesn't work if preemption is disabled or the
    latter task has real time priority.  Let's say task A just got woken
    up from flush_work() by the completion of the target work item.  If,
    before task A starts executing, task B gets scheduled and invokes
    __cancel_work_timer() on the same work item, its try_to_grab_pending()
    will return -ENOENT as the work item is still being canceled by task A
    and flush_work() will also immediately return false as the work item
    is no longer executing.  This puts task B in a busy loop possibly
    preventing task A from executing and clearing the canceling state on
    the work item leading to a hang.
    
    task A                  task B                  worker
    
                                                    executing work
    __cancel_work_timer()
      try_to_grab_pending()
      set work CANCELING
      flush_work()
        block for work completion
                                                    completion, wakes up A
                            __cancel_work_timer()
                            while (forever) {
                              try_to_grab_pending()
                                -ENOENT as work is being canceled
                              flush_work()
                                false as work is no longer executing
                            }
    
    This patch removes the possible hang by updating __cancel_work_timer()
    to explicitly wait for clearing of CANCELING rather than invoking
    flush_work() after try_to_grab_pending() fails with -ENOENT.
    
    Link: http://lkml.kernel.org/g/20150206171156.GA8942@axis.com
    
    v3: bit_waitqueue() can't be used for work items defined in vmalloc
        area.  Switched to custom wake function which matches the target
        work item and exclusive wait and wakeup.
    
    v2: v1 used wake_up() on bit_waitqueue() which leads to NULL deref if
        the target bit waitqueue has wait_bit_queue's on it.  Use
        DEFINE_WAIT_BIT() and __wake_up_bit() instead.  Reported by Tomeu
        Vizoso.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Rabin Vincent <rabin.vincent@axis.com>
    Cc: Tomeu Vizoso <tomeu.vizoso@gmail.com>
    Cc: stable@vger.kernel.org
    Tested-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Tested-by: Rabin Vincent <rabin.vincent@axis.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 74db135f9957..f597846ff605 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -70,7 +70,8 @@ enum {
 	/* data contains off-queue information when !WORK_STRUCT_PWQ */
 	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_COLOR_SHIFT,
 
-	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),
+	__WORK_OFFQ_CANCELING	= WORK_OFFQ_FLAG_BASE,
+	WORK_OFFQ_CANCELING	= (1 << __WORK_OFFQ_CANCELING),
 
 	/*
 	 * When a work item is off queue, its high bits point to the last

commit 9da7dae94fb8adab5cc5f395640e30736a66e910
Author: Valentin Rothberg <valentinrothberg@gmail.com>
Date:   Tue Jan 6 17:29:29 2015 +0100

    workqueue.h: remove loops of single statement macros
    
    checkpatch.pl complained about two single statement macros in
    do while (0) loops.  The loops and the trailing semicolons are
    now removed, which makes checkpatch happy and the two macros
    consistent with the rest of the file.
    
    Signed-off-by: Valentin Rothberg <valentinrothberg@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b996e6cde6bb..74db135f9957 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -220,14 +220,10 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #endif
 
 #define INIT_WORK(_work, _func)						\
-	do {								\
-		__INIT_WORK((_work), (_func), 0);			\
-	} while (0)
+	__INIT_WORK((_work), (_func), 0)
 
 #define INIT_WORK_ONSTACK(_work, _func)					\
-	do {								\
-		__INIT_WORK((_work), (_func), 1);			\
-	} while (0)
+	__INIT_WORK((_work), (_func), 1)
 
 #define __INIT_DELAYED_WORK(_work, _func, _tflags)			\
 	do {								\

commit e09c2c295468476a239d13324ce9042ec4de05eb
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Sep 13 04:14:30 2014 +0900

    workqueue: apply __WQ_ORDERED to create_singlethread_workqueue()
    
    create_singlethread_workqueue() is a compat interface for single
    threaded workqueue which maps to ordered workqueue w/ rescuer in the
    current implementation.  create_singlethread_workqueue() currently
    implemented by invoking alloc_workqueue() w/ appropriate parameters.
    
    8719dceae2f9 ("workqueue: reject adjusting max_active or applying
    attrs to ordered workqueues") introduced __WQ_ORDERED to protect
    ordered workqueues against dynamic attribute changes which can break
    ordering guarantees but forgot to apply it to
    create_singlethread_workqueue().  This in itself is okay as nobody
    currently uses dynamic attribute change on workqueues created with
    create_singlethread_workqueue().
    
    However, 4c16bd327c ("workqueue: implement NUMA affinity for unbound
    workqueues") broke singlethreaded guarantee for ordered workqueues
    through allocating a separate pool_workqueue on each NUMA node by
    default.  A later change 8a2b75384444 ("workqueue: fix ordered
    workqueues in NUMA setups") fixed it by allocating only one global
    pool_workqueue if __WQ_ORDERED is set.
    
    Combined, the __WQ_ORDERED omission in create_singlethread_workqueue()
    became critical breaking its single threadedness and ordering
    guarantee.
    
    Let's make create_singlethread_workqueue() wrap
    alloc_ordered_workqueue() instead so that it inherits __WQ_ORDERED and
    can implicitly track future ordered_workqueue changes.
    
    v2: I missed that __WQ_ORDERED now protects against pwq splitting
        across NUMA nodes and incorrectly described the patch as a
        nice-to-have fix to protect against future dynamic attribute
        usages.  Oleg pointed out that this is actually a critical
        breakage due to 8a2b75384444 ("workqueue: fix ordered workqueues
        in NUMA setups").
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Mike Anderson <mike.anderson@us.ibm.com>
    Cc: Oleg Nesterov <onestero@redhat.com>
    Cc: Gustavo Luiz Duarte <gduarte@redhat.com>
    Cc: Tomas Henzl <thenzl@redhat.com>
    Cc: stable@vger.kernel.org
    Fixes: 4c16bd327c ("workqueue: implement NUMA affinity for unbound workqueues")

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a0cc2e95ed1b..b996e6cde6bb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -419,7 +419,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, \
 			1, (name))
 #define create_singlethread_workqueue(name)				\
-	alloc_workqueue("%s", WQ_UNBOUND | WQ_MEM_RECLAIM, 1, (name))
+	alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit cafebac153ae54fd0aba5d4ad28af995532c5375
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu May 22 16:43:56 2014 +0800

    workqueue: remove unused work_clear_pending()
    
    In 8930caba3dbd ("workqueue: disable irq while manipulating PENDING"),
    setting last CPU and clearing PENDING got merged into a single
    operation (set_work_cpu_and_clear_pending()), which resulted that the
    internal routine work_clear_pending() is not used any more.
    
    tj: Minor description tweak.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b8aee9453f22..a0cc2e95ed1b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -273,13 +273,6 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define delayed_work_pending(w) \
 	work_pending(&(w)->work)
 
-/**
- * work_clear_pending - for internal use only, mark a work item as not pending
- * @work: The work item in question
- */
-#define work_clear_pending(work) \
-	clear_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))
-
 /*
  * Workqueue flags and constants.  For details, please refer to
  * Documentation/workqueue.txt.

commit 79bc251f0e0aea67bc230c530f7fa57f66f9cdf3
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu May 22 16:43:44 2014 +0800

    workqueue: remove unused WORK_CPU_END
    
    WORK_CPU_END is totally unused since 4e8b22bd1a37 ("workqueue: fix
    pool ID allocation leakage and remove BUILD_BUG_ON() in
    init_workqueues"). It should be removed.
    
    After it is removed, the comment "special cpu IDs" is not precise due to
    there is only one special CPU ID (WORK_CPU_UNBOUND) left, so we also
    change this comment to the description for WORK_CPU_UNBOUND.
    
    tj: Minor description and comment tweaks.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b263b29bd98b..b8aee9453f22 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -56,9 +56,8 @@ enum {
 	WORK_NR_COLORS		= (1 << WORK_STRUCT_COLOR_BITS) - 1,
 	WORK_NO_COLOR		= WORK_NR_COLORS,
 
-	/* special cpu IDs */
+	/* not bound to any CPU, prefer the local CPU */
 	WORK_CPU_UNBOUND	= NR_CPUS,
-	WORK_CPU_END		= NR_CPUS + 1,
 
 	/*
 	 * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.

commit 73e4354444eef5251e5cdfd388ab02ef9f2e727e
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu May 22 16:42:41 2014 +0800

    workqueue: declare system_highpri_wq
    
    system_highpri_wq is exported to modules via EXPORT_SYMBOL_GPL(),
    but it was forgotten to be declared in workqueue.h. So we add the declaration
    and a short description for it.
    
    tj: Minor comment tweak.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d93d28b2ec73..b263b29bd98b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -340,6 +340,9 @@ enum {
  * short queue flush time.  Don't queue works which can run for too
  * long.
  *
+ * system_highpri_wq is similar to system_wq but for work items which
+ * require WQ_HIGHPRI.
+ *
  * system_long_wq is similar to system_wq but may host long running
  * works.  Queue flushing might take relatively long.
  *
@@ -358,6 +361,7 @@ enum {
  * 'wq_power_efficient' is disabled.  See WQ_POWER_EFFICIENT for more info.
  */
 extern struct workqueue_struct *system_wq;
+extern struct workqueue_struct *system_highpri_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;

commit cf416171e7e1d966111f53bdae82f51af05e7bf8
Author: Jingoo Han <jg1.han@samsung.com>
Date:   Wed May 14 13:58:06 2014 +0900

    workqueue: Remove deprecated system_nrt[_freezable]_wq
    
    system_nrt[_freezable]_wq were deprecated by 3b07e9c ("workqueue:
    deprecate system_nrt[_freezable]_wq") and have been deprecated
    for a long time. In addition, these are not used anymore. So,
    let's remove these functions.
    
    Signed-off-by: Jingoo Han <jg1.han@samsung.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index aa92d0295e28..d93d28b2ec73 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -364,20 +364,6 @@ extern struct workqueue_struct *system_freezable_wq;
 extern struct workqueue_struct *system_power_efficient_wq;
 extern struct workqueue_struct *system_freezable_power_efficient_wq;
 
-static inline struct workqueue_struct * __deprecated __system_nrt_wq(void)
-{
-	return system_wq;
-}
-
-static inline struct workqueue_struct * __deprecated __system_nrt_freezable_wq(void)
-{
-	return system_freezable_wq;
-}
-
-/* equivlalent to system_wq and system_freezable_wq, deprecated */
-#define system_nrt_wq			__system_nrt_wq()
-#define system_nrt_freezable_wq		__system_nrt_freezable_wq()
-
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);

commit 1a56f2aa4752293e5a9c0c3a2331620aa1fdb808
Author: Jingoo Han <jg1.han@samsung.com>
Date:   Wed May 14 13:43:37 2014 +0900

    workqueue: Remove deprecated flush[_delayed]_work_sync()
    
    flush[_delayed]_work_sync() were deprecated by 4382973 ("workqueue:
    deprecate flush[_delayed]_work_sync()") and have been deprecated
    for a long time. In addition, these are not used anymore. So,
    let's remove these functions.
    
    Signed-off-by: Jingoo Han <jg1.han@samsung.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1b22c42e9c2d..aa92d0295e28 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -587,18 +587,6 @@ static inline bool keventd_up(void)
 	return system_wq != NULL;
 }
 
-/* used to be different but now identical to flush_work(), deprecated */
-static inline bool __deprecated flush_work_sync(struct work_struct *work)
-{
-	return flush_work(work);
-}
-
-/* used to be different but now identical to flush_delayed_work(), deprecated */
-static inline bool __deprecated flush_delayed_work_sync(struct delayed_work *dwork)
-{
-	return flush_delayed_work(dwork);
-}
-
 #ifndef CONFIG_SMP
 static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
 {

commit 1ead65812486cda65093683a99b8907a7242fa93
Merge: b6d739e95812 b97f0291a250
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 11:00:07 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes from Thomas Gleixner:
     "This assorted collection provides:
    
       - A new timer based timer broadcast feature for systems which do not
         provide a global accessible timer device.  That allows those
         systems to put CPUs into deep idle states where the per cpu timer
         device stops.
    
       - A few NOHZ_FULL related improvements to the timer wheel
    
       - The usual updates to timer devices found in ARM SoCs
    
       - Small improvements and updates all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      tick: Remove code duplication in tick_handle_periodic()
      tick: Fix spelling mistake in tick_handle_periodic()
      x86: hpet: Use proper destructor for delayed work
      workqueue: Provide destroy_delayed_work_on_stack()
      clocksource: CMT, MTU2, TMU and STI should depend on GENERIC_CLOCKEVENTS
      timer: Remove code redundancy while calling get_nohz_timer_target()
      hrtimer: Rearrange comments in the order struct members are declared
      timer: Use variable head instead of &work_list in __run_timers()
      clocksource: exynos_mct: silence a static checker warning
      arm: zynq: Add support for cpufreq
      arm: zynq: Don't use arm_global_timer with cpufreq
      clocksource/cadence_ttc: Overhaul clocksource frequency adjustment
      clocksource/cadence_ttc: Call clockevents_update_freq() with IRQs enabled
      clocksource: Add Kconfig entries for CMT, MTU2, TMU and STI
      sh: Remove Kconfig entries for TMU, CMT and MTU2
      ARM: shmobile: Remove CMT, TMU and STI Kconfig entries
      clocksource: armada-370-xp: Use atomic access for shared registers
      clocksource: orion: Use atomic access for shared registers
      clocksource: timer-keystone: Delete unnecessary variable
      clocksource: timer-keystone: introduce clocksource driver for Keystone
      ...

commit 59ff3eb6d6f75c6c1c3ea8b46ac2cc64eb216547
Author: ZhangZhen <zhenzhang.zhang@huawei.com>
Date:   Thu Mar 27 09:41:47 2014 +0800

    workqueue: remove deprecated WQ_NON_REENTRANT
    
    Tejun Heo has made WQ_NON_REENTRANT useless in the dbf2576e37
    ("workqueue: make all workqueues non-reentrant"). So remove its
    usages and definition.
    
    This patch doesn't introduce any behavior changes.
    
    tj: minor description updates.
    
    Signed-off-by: ZhangZhen <zhenzhang.zhang@huawei.com>
    Sigend-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: James Chapman <jchapman@katalix.com>
    Acked-by: Ulf Hansson <ulf.hansson@linaro.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0523eab05f63..532994651684 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -284,12 +284,6 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
  * Documentation/workqueue.txt.
  */
 enum {
-	/*
-	 * All wqs are now non-reentrant making the following flag
-	 * meaningless.  Will be removed.
-	 */
-	WQ_NON_REENTRANT	= 1 << 0, /* DEPRECATED */
-
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
 	WQ_FREEZABLE		= 1 << 2, /* freeze during suspend */
 	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */

commit ea2e64f280d2a34a8ed9ae3d783cd770d14b70ec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Mar 23 14:20:44 2014 +0000

    workqueue: Provide destroy_delayed_work_on_stack()
    
    If a delayed or deferrable work is on stack we need to tell debug
    objects that we are destroying the timer and the work. Otherwise we
    leak the tracking object.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Acked-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/20140323141939.911487677@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 594521ba0d43..abdbe5af119d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -191,6 +191,7 @@ struct execute_work {
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 extern void __init_work(struct work_struct *work, int onstack);
 extern void destroy_work_on_stack(struct work_struct *work);
+extern void destroy_delayed_work_on_stack(struct delayed_work *work);
 static inline unsigned int work_static(struct work_struct *work)
 {
 	return *work_data_bits(work) & WORK_STRUCT_STATIC;
@@ -198,6 +199,7 @@ static inline unsigned int work_static(struct work_struct *work)
 #else
 static inline void __init_work(struct work_struct *work, int onstack) { }
 static inline void destroy_work_on_stack(struct work_struct *work) { }
+static inline void destroy_delayed_work_on_stack(struct delayed_work *work) { }
 static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #endif
 

commit 41f50094b2ab4e37673e41a084ea61b907447159
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Mon Mar 24 21:37:02 2014 +0100

    workqueue: Spelling s/instensive/intensive/
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 29da9e77c3bb..0523eab05f63 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -294,7 +294,7 @@ enum {
 	WQ_FREEZABLE		= 1 << 2, /* freeze during suspend */
 	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
-	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
+	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu intensive workqueue */
 	WQ_SYSFS		= 1 << 6, /* visible in sysfs, see wq_sysfs_register() */
 
 	/*

commit f073f9229ff1137d3be20558bec3bfb77e3af2a4
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Mar 7 10:24:50 2014 -0500

    workqueue: remove PREPARE_[DELAYED_]WORK()
    
    Peter Hurley noticed that since a2c1c57be8d9 ("workqueue: consider
    work function when searching for busy work items"), a work item which
    gets assigned a different work function would break out of the
    non-reentrancy guarantee as workqueue would consider it a different
    work item.
    
    This is fragile and extremely subtle.  PREPARE_[DELAYED_]WORK() have
    never been used widely and its semantics has always been somewhat
    iffy.  If the work item is known not to be on queue when
    PREPARE_WORK() is called, there's no difference from using
    INIT_WORK().  If the work item may be queued at the time of
    PREPARE_WORK(), we can't really tell whether the old or new function
    will be executed the next time.
    
    We really don't want this level of subtlety in workqueue interface for
    such marginal use cases.  The previous patches converted all existing
    users away from PREPARE_[DELAYED_]WORK().  Let's remove them.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Link: http://lkml.kernel.org/g/1392493119-9277-1-git-send-email-peter@hurleysoftware.com

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 8059334a6b02..29da9e77c3bb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -177,17 +177,6 @@ struct execute_work {
 #define DECLARE_DEFERRABLE_WORK(n, f)					\
 	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, TIMER_DEFERRABLE)
 
-/*
- * initialize a work item's function pointer
- */
-#define PREPARE_WORK(_work, _func)					\
-	do {								\
-		(_work)->func = (_func);				\
-	} while (0)
-
-#define PREPARE_DELAYED_WORK(_work, _func)				\
-	PREPARE_WORK(&(_work)->work, (_func))
-
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 extern void __init_work(struct work_struct *work, int onstack);
 extern void destroy_work_on_stack(struct work_struct *work);
@@ -217,7 +206,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0); \
 		INIT_LIST_HEAD(&(_work)->entry);			\
-		PREPARE_WORK((_work), (_func));				\
+		(_work)->func = (_func);				\
 	} while (0)
 #else
 #define __INIT_WORK(_work, _func, _onstack)				\
@@ -225,7 +214,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		__init_work((_work), _onstack);				\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		INIT_LIST_HEAD(&(_work)->entry);			\
-		PREPARE_WORK((_work), (_func));				\
+		(_work)->func = (_func);				\
 	} while (0)
 #endif
 

commit 7104ce9b349e0b0c9a71ee25efeb007057029677
Merge: 90d88bd75424 70044d71d31d
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Mar 7 10:20:20 2014 -0500

    Merge branch 'wq/for-3.14-fixes' into wq/for-3.15
    
    To receive 70044d71d31d ("firewire: don't use PREPARE_DELAYED_WORK").
    There will be further related updates in for-3.15 branch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 90d88bd75424dff51e2072fd2f8fa85ee893aa17
Author: Tan Xiaojun <tanxiaojun@huawei.com>
Date:   Sat Feb 15 13:19:51 2014 +0800

    workqueue: Remove deprecated __cancel_delayed_work()
    
    __cancel_delayed_work() was deprecated by 136b5721d75a ("workqueue:
    deprecate __cancel_delayed_work()") as cancel_delayed_work() was
    updated so that it could be used from all contexts.  Enough time has
    passed since the deprecation.  Let's remove it.
    
    tj: description update
    
    Signed-off-by: Tan Xiaojun <tanxiaojun@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 594521ba0d43..edc941049d79 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -605,21 +605,6 @@ static inline bool keventd_up(void)
 	return system_wq != NULL;
 }
 
-/*
- * Like above, but uses del_timer() instead of del_timer_sync(). This means,
- * if it returns 0 the timer function may be running and the queueing is in
- * progress.
- */
-static inline bool __deprecated __cancel_delayed_work(struct delayed_work *work)
-{
-	bool ret;
-
-	ret = del_timer(&work->timer);
-	if (ret)
-		work_clear_pending(&work->work);
-	return ret;
-}
-
 /* used to be different but now identical to flush_work(), deprecated */
 static inline bool __deprecated flush_work_sync(struct work_struct *work)
 {

commit fada94ee64e6e18793b1db60fb8278d2eddbf922
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Fri Feb 14 10:52:57 2014 +0800

    workqueue: add args to workqueue lockdep name
    
    Tommi noticed a 'funny' lock class name: "%s#5" from a lock acquired in
    process_one_work().
    
    Maybe #fmt plus #args could be used as the lock_name to give some more
    information for some fmt string like the above.
    
    __builtin_constant_p() check is removed (as there seems no good way to
    check all the variables in args list). However, by removing the check,
    it only adds two additional "s for those constants.
    
    Some lockdep name examples printed out after the change:
    
    lockdep name                    wq->name
    
    "events_long"                   events_long
    "%s"("khelper")                 khelper
    "xfs-data/%s"mp->m_fsname       xfs-data/dm-3
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 594521ba0d43..704f4f652d0a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -419,10 +419,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	static struct lock_class_key __key;				\
 	const char *__lock_name;					\
 									\
-	if (__builtin_constant_p(fmt))					\
-		__lock_name = (fmt);					\
-	else								\
-		__lock_name = #fmt;					\
+	__lock_name = #fmt#args;					\
 									\
 	__alloc_workqueue_key((fmt), (flags), (max_active),		\
 			      &__key, __lock_name, ##args);		\

commit 1207637304990374231fe4e9aeb527904f4ec1e6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 30 08:30:16 2013 -0400

    workqueue: mark WQ_NON_REENTRANT deprecated
    
    dbf2576e37 ("workqueue: make all workqueues non-reentrant") made
    WQ_NON_REENTRANT no-op but the following patches didn't remove the
    flag or update the documentation.  Let's mark the flag deprecated and
    update the documentation accordingly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a0ed78ab54d7..594521ba0d43 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -295,7 +295,12 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
  * Documentation/workqueue.txt.
  */
 enum {
-	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
+	/*
+	 * All wqs are now non-reentrant making the following flag
+	 * meaningless.  Will be removed.
+	 */
+	WQ_NON_REENTRANT	= 1 << 0, /* DEPRECATED */
+
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
 	WQ_FREEZABLE		= 1 << 2, /* freeze during suspend */
 	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */

commit d8537548c924db3c44afde7646b6e220c7beb79d
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Jul 3 15:04:57 2013 -0700

    drivers: avoid format strings in names passed to alloc_workqueue()
    
    For the workqueue creation interfaces that do not expect format strings,
    make sure they cannot accidently be parsed that way.  Additionally, clean
    up calls made with a single parameter that would be handled as a format
    string.  Many callers are passing potentially dynamic string content, so
    use "%s" in those cases to avoid any potential accidents.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a9f4119c7e2e..a0ed78ab54d7 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -445,11 +445,12 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)
 
 #define create_workqueue(name)						\
-	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)
+	alloc_workqueue("%s", WQ_MEM_RECLAIM, 1, (name))
 #define create_freezable_workqueue(name)				\
-	alloc_workqueue((name), WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
+	alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, \
+			1, (name))
 #define create_singlethread_workqueue(name)				\
-	alloc_workqueue((name), WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
+	alloc_workqueue("%s", WQ_UNBOUND | WQ_MEM_RECLAIM, 1, (name))
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 0668106ca3865ba945e155097fb042bf66d364d3
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Apr 24 17:12:54 2013 +0530

    workqueue: Add system wide power_efficient workqueues
    
    This patch adds system wide workqueues aligned towards power saving. This is
    done by allocating them with WQ_UNBOUND flag if 'wq_power_efficient' is set to
    'true'.
    
    tj: updated comments a bit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index fc0136b604f2..a9f4119c7e2e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -360,11 +360,19 @@ enum {
  *
  * system_freezable_wq is equivalent to system_wq except that it's
  * freezable.
+ *
+ * *_power_efficient_wq are inclined towards saving power and converted
+ * into WQ_UNBOUND variants if 'wq_power_efficient' is enabled; otherwise,
+ * they are same as their non-power-efficient counterparts - e.g.
+ * system_power_efficient_wq is identical to system_wq if
+ * 'wq_power_efficient' is disabled.  See WQ_POWER_EFFICIENT for more info.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;
+extern struct workqueue_struct *system_power_efficient_wq;
+extern struct workqueue_struct *system_freezable_power_efficient_wq;
 
 static inline struct workqueue_struct * __deprecated __system_nrt_wq(void)
 {

commit cee22a15052faa817e3ec8985a28154d3fabc7aa
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Apr 8 16:45:40 2013 +0530

    workqueues: Introduce new flag WQ_POWER_EFFICIENT for power oriented workqueues
    
    Workqueues can be performance or power-oriented. Currently, most workqueues are
    bound to the CPU they were created on. This gives good performance (due to cache
    effects) at the cost of potentially waking up otherwise idle cores (Idle from
    scheduler's perspective. Which may or may not be physically idle) just to
    process some work. To save power, we can allow the work to be rescheduled on a
    core that is already awake.
    
    Workqueues created with the WQ_UNBOUND flag will allow some power savings.
    However, we don't change the default behaviour of the system.  To enable
    power-saving behaviour, a new config option CONFIG_WQ_POWER_EFFICIENT needs to
    be turned on. This option can also be overridden by the
    workqueue.power_efficient boot parameter.
    
    tj: Updated config description and comments.  Renamed
        CONFIG_WQ_POWER_EFFICIENT to CONFIG_WQ_POWER_EFFICIENT_DEFAULT.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 623488fdc1f5..fc0136b604f2 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -303,6 +303,33 @@ enum {
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 	WQ_SYSFS		= 1 << 6, /* visible in sysfs, see wq_sysfs_register() */
 
+	/*
+	 * Per-cpu workqueues are generally preferred because they tend to
+	 * show better performance thanks to cache locality.  Per-cpu
+	 * workqueues exclude the scheduler from choosing the CPU to
+	 * execute the worker threads, which has an unfortunate side effect
+	 * of increasing power consumption.
+	 *
+	 * The scheduler considers a CPU idle if it doesn't have any task
+	 * to execute and tries to keep idle cores idle to conserve power;
+	 * however, for example, a per-cpu work item scheduled from an
+	 * interrupt handler on an idle CPU will force the scheduler to
+	 * excute the work item on that CPU breaking the idleness, which in
+	 * turn may lead to more scheduling choices which are sub-optimal
+	 * in terms of power consumption.
+	 *
+	 * Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default
+	 * but become unbound if workqueue.power_efficient kernel param is
+	 * specified.  Per-cpu workqueues which are identified to
+	 * contribute significantly to power-consumption are identified and
+	 * marked with this flag and enabling the power_efficient mode
+	 * leads to noticeable power saving at the cost of small
+	 * performance disadvantage.
+	 *
+	 * http://thread.gmane.org/gmane.linux.kernel/1480396
+	 */
+	WQ_POWER_EFFICIENT	= 1 << 7,
+
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
 

commit 3d1cb2059d9374e58da481b783332cf191cb6620
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:22 2013 -0700

    workqueue: include workqueue info when printing debug dump of a worker task
    
    One of the problems that arise when converting dedicated custom
    threadpool to workqueue is that the shared worker pool used by workqueue
    anonimizes each worker making it more difficult to identify what the
    worker was doing on which target from the output of sysrq-t or debug
    dump from oops, BUG() and friends.
    
    This patch implements set_worker_desc() which can be called from any
    workqueue work function to set its description.  When the worker task is
    dumped for whatever reason - sysrq-t, WARN, BUG, oops, lockdep assertion
    and so on - the description will be printed out together with the
    workqueue name and the worker function pointer.
    
    The printing side is implemented by print_worker_info() which is called
    from functions in task dump paths - sched_show_task() and
    dump_stack_print_info().  print_worker_info() can be safely called on
    any task in any state as long as the task struct itself is accessible.
    It uses probe_*() functions to access worker fields.  It may print
    garbage if something went very wrong, but it wouldn't cause (another)
    oops.
    
    The description is currently limited to 24bytes including the
    terminating \0.  worker->desc_valid and workder->desc[] are added and
    the 64 bytes marker which was already incorrect before adding the new
    fields is moved to the correct position.
    
    Here's an example dump with writeback updated to set the bdi name as
    worker desc.
    
     Hardware name: Bochs
     Modules linked in:
     Pid: 7, comm: kworker/u9:0 Not tainted 3.9.0-rc1-work+ #1
     Workqueue: writeback bdi_writeback_workfn (flush-8:0)
      ffffffff820a3ab0 ffff88000f6e9cb8 ffffffff81c61845 ffff88000f6e9cf8
      ffffffff8108f50f 0000000000000000 0000000000000000 ffff88000cde16b0
      ffff88000cde1aa8 ffff88001ee19240 ffff88000f6e9fd8 ffff88000f6e9d08
     Call Trace:
      [<ffffffff81c61845>] dump_stack+0x19/0x1b
      [<ffffffff8108f50f>] warn_slowpath_common+0x7f/0xc0
      [<ffffffff8108f56a>] warn_slowpath_null+0x1a/0x20
      [<ffffffff81200150>] bdi_writeback_workfn+0x2a0/0x3b0
     ...
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 717975639378..623488fdc1f5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -92,6 +92,9 @@ enum {
 	/* bit mask for work_busy() return values */
 	WORK_BUSY_PENDING	= 1 << 0,
 	WORK_BUSY_RUNNING	= 1 << 1,
+
+	/* maximum string length for set_worker_desc() */
+	WORKER_DESC_LEN		= 24,
 };
 
 struct work_struct {
@@ -447,6 +450,8 @@ extern void workqueue_set_max_active(struct workqueue_struct *wq,
 extern bool current_is_workqueue_rescuer(void);
 extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);
+extern __printf(1, 2) void set_worker_desc(const char *fmt, ...);
+extern void print_worker_info(const char *log_lvl, struct task_struct *task);
 
 /**
  * queue_work - queue work on a workqueue

commit d55262c4d164759a8debe772da6c9b16059dec47
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Apr 1 11:23:38 2013 -0700

    workqueue: update sysfs interface to reflect NUMA awareness and a kernel param to disable NUMA affinity
    
    Unbound workqueues are now NUMA aware.  Let's add some control knobs
    and update sysfs interface accordingly.
    
    * Add kernel param workqueue.numa_disable which disables NUMA affinity
      globally.
    
    * Replace sysfs file "pool_id" with "pool_ids" which contain
      node:pool_id pairs.  This change is userland-visible but "pool_id"
      hasn't seen a release yet, so this is okay.
    
    * Add a new sysf files "numa" which can toggle NUMA affinity on
      individual workqueues.  This is implemented as attrs->no_numa whichn
      is special in that it isn't part of a pool's attributes.  It only
      affects how apply_workqueue_attrs() picks which pools to use.
    
    After "pool_ids" change, first_pwq() doesn't have any user left.
    Removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 835d12b76960..717975639378 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -119,10 +119,15 @@ struct delayed_work {
 /*
  * A struct for workqueue attributes.  This can be used to change
  * attributes of an unbound workqueue.
+ *
+ * Unlike other fields, ->no_numa isn't a property of a worker_pool.  It
+ * only modifies how apply_workqueue_attrs() select pools and thus doesn't
+ * participate in pool hash calculations or equality comparisons.
  */
 struct workqueue_attrs {
 	int			nice;		/* nice level */
 	cpumask_var_t		cpumask;	/* allowed CPUs */
+	bool			no_numa;	/* disable NUMA affinity */
 };
 
 static inline struct delayed_work *to_delayed_work(struct work_struct *work)

commit 8425e3d5bdbe8e741d2c73cf3189ed59b4038b84
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 13 16:51:36 2013 -0700

    workqueue: inline trivial wrappers
    
    There's no reason to make these trivial wrappers full (exported)
    functions.  Inline the followings.
    
     queue_work()
     queue_delayed_work()
     mod_delayed_work()
     schedule_work_on()
     schedule_work()
     schedule_delayed_work_on()
     schedule_delayed_work()
     keventd_up()
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index df30763c8682..835d12b76960 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -417,28 +417,16 @@ int apply_workqueue_attrs(struct workqueue_struct *wq,
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
-extern bool queue_work(struct workqueue_struct *wq, struct work_struct *work);
 extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
-extern bool queue_delayed_work(struct workqueue_struct *wq,
-			struct delayed_work *work, unsigned long delay);
 extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *dwork, unsigned long delay);
-extern bool mod_delayed_work(struct workqueue_struct *wq,
-			struct delayed_work *dwork, unsigned long delay);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
-extern bool schedule_work_on(int cpu, struct work_struct *work);
-extern bool schedule_work(struct work_struct *work);
-extern bool schedule_delayed_work_on(int cpu, struct delayed_work *work,
-				     unsigned long delay);
-extern bool schedule_delayed_work(struct delayed_work *work,
-				  unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);
-extern int keventd_up(void);
 
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
@@ -455,6 +443,117 @@ extern bool current_is_workqueue_rescuer(void);
 extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);
 
+/**
+ * queue_work - queue work on a workqueue
+ * @wq: workqueue to use
+ * @work: work to queue
+ *
+ * Returns %false if @work was already on a queue, %true otherwise.
+ *
+ * We queue the work to the CPU on which it was submitted, but if the CPU dies
+ * it can be processed by another CPU.
+ */
+static inline bool queue_work(struct workqueue_struct *wq,
+			      struct work_struct *work)
+{
+	return queue_work_on(WORK_CPU_UNBOUND, wq, work);
+}
+
+/**
+ * queue_delayed_work - queue work on a workqueue after delay
+ * @wq: workqueue to use
+ * @dwork: delayable work to queue
+ * @delay: number of jiffies to wait before queueing
+ *
+ * Equivalent to queue_delayed_work_on() but tries to use the local CPU.
+ */
+static inline bool queue_delayed_work(struct workqueue_struct *wq,
+				      struct delayed_work *dwork,
+				      unsigned long delay)
+{
+	return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
+}
+
+/**
+ * mod_delayed_work - modify delay of or queue a delayed work
+ * @wq: workqueue to use
+ * @dwork: work to queue
+ * @delay: number of jiffies to wait before queueing
+ *
+ * mod_delayed_work_on() on local CPU.
+ */
+static inline bool mod_delayed_work(struct workqueue_struct *wq,
+				    struct delayed_work *dwork,
+				    unsigned long delay)
+{
+	return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
+}
+
+/**
+ * schedule_work_on - put work task on a specific cpu
+ * @cpu: cpu to put the work task on
+ * @work: job to be done
+ *
+ * This puts a job on a specific cpu
+ */
+static inline bool schedule_work_on(int cpu, struct work_struct *work)
+{
+	return queue_work_on(cpu, system_wq, work);
+}
+
+/**
+ * schedule_work - put work task in global workqueue
+ * @work: job to be done
+ *
+ * Returns %false if @work was already on the kernel-global workqueue and
+ * %true otherwise.
+ *
+ * This puts a job in the kernel-global workqueue if it was not already
+ * queued and leaves it in the same position on the kernel-global
+ * workqueue otherwise.
+ */
+static inline bool schedule_work(struct work_struct *work)
+{
+	return queue_work(system_wq, work);
+}
+
+/**
+ * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
+ * @cpu: cpu to use
+ * @dwork: job to be done
+ * @delay: number of jiffies to wait
+ *
+ * After waiting for a given time this puts a job in the kernel-global
+ * workqueue on the specified CPU.
+ */
+static inline bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
+					    unsigned long delay)
+{
+	return queue_delayed_work_on(cpu, system_wq, dwork, delay);
+}
+
+/**
+ * schedule_delayed_work - put work task in global workqueue after delay
+ * @dwork: job to be done
+ * @delay: number of jiffies to wait or 0 for immediate execution
+ *
+ * After waiting for a given time this puts a job in the kernel-global
+ * workqueue.
+ */
+static inline bool schedule_delayed_work(struct delayed_work *dwork,
+					 unsigned long delay)
+{
+	return queue_delayed_work(system_wq, dwork, delay);
+}
+
+/**
+ * keventd_up - is workqueue initialized yet?
+ */
+static inline bool keventd_up(void)
+{
+	return system_wq != NULL;
+}
+
 /*
  * Like above, but uses del_timer() instead of del_timer_sync(). This means,
  * if it returns 0 the timer function may be running and the queueing is in

commit e62676169118bc2d42e5008b3f8872646313f077
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 17:41:37 2013 -0700

    workqueue: implement current_is_workqueue_rescuer()
    
    Implement a function which queries whether it currently is running off
    a workqueue rescuer.  This will be used to convert writeback to
    workqueue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7f6d29a417c0..df30763c8682 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -451,6 +451,7 @@ extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
+extern bool current_is_workqueue_rescuer(void);
 extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);
 

commit 226223ab3c4118ddd10688cc2c131135848371ab
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:05 2013 -0700

    workqueue: implement sysfs interface for workqueues
    
    There are cases where workqueue users want to expose control knobs to
    userland.  e.g. Unbound workqueues with custom attributes are
    scheduled to be used for writeback workers and depending on
    configuration it can be useful to allow admins to tinker with the
    priority or allowed CPUs.
    
    This patch implements workqueue_sysfs_register(), which makes the
    workqueue visible under /sys/bus/workqueue/devices/WQ_NAME.  There
    currently are two attributes common to both per-cpu and unbound pools
    and extra attributes for unbound pools including nice level and
    cpumask.
    
    If alloc_workqueue*() is called with WQ_SYSFS,
    workqueue_sysfs_register() is called automatically as part of
    workqueue creation.  This is the preferred method unless the workqueue
    user wants to apply workqueue_attrs before making the workqueue
    visible to userland.
    
    v2: Disallow exposing ordered workqueues as ordered workqueues can't
        be tuned in any way.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5668ab249af5..7f6d29a417c0 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -293,6 +293,7 @@ enum {
 	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
+	WQ_SYSFS		= 1 << 6, /* visible in sysfs, see wq_sysfs_register() */
 
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
@@ -495,4 +496,11 @@ extern bool freeze_workqueues_busy(void);
 extern void thaw_workqueues(void);
 #endif /* CONFIG_FREEZER */
 
+#ifdef CONFIG_SYSFS
+int workqueue_sysfs_register(struct workqueue_struct *wq);
+#else	/* CONFIG_SYSFS */
+static inline int workqueue_sysfs_register(struct workqueue_struct *wq)
+{ return 0; }
+#endif	/* CONFIG_SYSFS */
+
 #endif

commit 8719dceae2f98a578507c0f6b49c93f320bd729c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:04 2013 -0700

    workqueue: reject adjusting max_active or applying attrs to ordered workqueues
    
    Adjusting max_active of or applying new workqueue_attrs to an ordered
    workqueue breaks its ordering guarantee.  The former is obvious.  The
    latter is because applying attrs creates a new pwq (pool_workqueue)
    and there is no ordering constraint between the old and new pwqs.
    
    Make apply_workqueue_attrs() and workqueue_set_max_active() trigger
    WARN_ON() if those operations are requested on an ordered workqueue
    and fail / ignore respectively.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1751ec4c47c9..5668ab249af5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -295,6 +295,7 @@ enum {
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
 	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
+	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
@@ -397,7 +398,7 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
 #define alloc_ordered_workqueue(fmt, flags, args...)			\
-	alloc_workqueue(fmt, WQ_UNBOUND | (flags), 1, ##args)
+	alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)
 
 #define create_workqueue(name)						\
 	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)

commit 618b01eb426dd2d73a4b5e5ebc6379e4eee3b123
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:04 2013 -0700

    workqueue: make it clear that WQ_DRAINING is an internal flag
    
    We're gonna add another internal WQ flag.  Let's make the distinction
    clear.  Prefix WQ_DRAINING with __ and move it to bit 16.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index e152394fa7eb..1751ec4c47c9 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -294,7 +294,7 @@ enum {
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
-	WQ_DRAINING		= 1 << 6, /* internal: workqueue is draining */
+	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */

commit 9e8cd2f5898ab6710ad81f4583fada08bf8049a4
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:04 2013 -0700

    workqueue: implement apply_workqueue_attrs()
    
    Implement apply_workqueue_attrs() which applies workqueue_attrs to the
    specified unbound workqueue by creating a new pwq (pool_workqueue)
    linked to worker_pool with the specified attributes.
    
    A new pwq is linked at the head of wq->pwqs instead of tail and
    __queue_work() verifies that the first unbound pwq has positive refcnt
    before choosing it for the actual queueing.  This is to cover the case
    where creation of a new pwq races with queueing.  As base ref on a pwq
    won't be dropped without making another pwq the first one,
    __queue_work() is guaranteed to make progress and not add work item to
    a dead pwq.
    
    init_and_link_pwq() is updated to return the last first pwq the new
    pwq replaced, which is put by apply_workqueue_attrs().
    
    Note that apply_workqueue_attrs() is almost identical to unbound pwq
    part of alloc_and_link_pwqs().  The only difference is that there is
    no previous first pwq.  apply_workqueue_attrs() is implemented to
    handle such cases and replaces unbound pwq handling in
    alloc_and_link_pwqs().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index c270b4eedf16..e152394fa7eb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -410,6 +410,8 @@ extern void destroy_workqueue(struct workqueue_struct *wq);
 
 struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);
 void free_workqueue_attrs(struct workqueue_attrs *attrs);
+int apply_workqueue_attrs(struct workqueue_struct *wq,
+			  const struct workqueue_attrs *attrs);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);

commit 493008a8e475771a2126e0ce95a73e35b371d277
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:03 2013 -0700

    workqueue: drop WQ_RESCUER and test workqueue->rescuer for NULL instead
    
    WQ_RESCUER is superflous.  WQ_MEM_RECLAIM indicates that the user
    wants a rescuer and testing wq->rescuer for NULL can answer whether a
    given workqueue has a rescuer or not.  Drop WQ_RESCUER and test
    wq->rescuer directly.
    
    This will help simplifying __alloc_workqueue_key() failure path by
    allowing it to use destroy_workqueue() on a partially constructed
    workqueue, which in turn will help implementing dynamic management of
    pool_workqueues.
    
    While at it, clear wq->rescuer after freeing it in
    destroy_workqueue().  This is a precaution as scheduled changes will
    make destruction more complex.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 00c1b9ba8252..c270b4eedf16 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -295,7 +295,6 @@ enum {
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
 	WQ_DRAINING		= 1 << 6, /* internal: workqueue is draining */
-	WQ_RESCUER		= 1 << 7, /* internal: workqueue has rescuer */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */

commit 7a4e344c5675eefbde93ed9a98ef45e0e4957bc2
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:30:00 2013 -0700

    workqueue: introduce workqueue_attrs
    
    Introduce struct workqueue_attrs which carries worker attributes -
    currently the nice level and allowed cpumask along with helper
    routines alloc_workqueue_attrs() and free_workqueue_attrs().
    
    Each worker_pool now carries ->attrs describing the attributes of its
    workers.  All functions dealing with cpumask and nice level of workers
    are updated to follow worker_pool->attrs instead of determining them
    from other characteristics of the worker_pool, and init_workqueues()
    is updated to set worker_pool->attrs appropriately for all standard
    pools.
    
    Note that create_worker() is updated to always perform set_user_nice()
    and use set_cpus_allowed_ptr() combined with manual assertion of
    PF_THREAD_BOUND instead of kthread_bind().  This simplifies handling
    random attributes without affecting the outcome.
    
    This patch doesn't introduce any behavior changes.
    
    v2: Missing cpumask_var_t definition caused build failure on some
        archs.  linux/cpumask.h included.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 899be6636d20..00c1b9ba8252 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -11,6 +11,7 @@
 #include <linux/lockdep.h>
 #include <linux/threads.h>
 #include <linux/atomic.h>
+#include <linux/cpumask.h>
 
 struct workqueue_struct;
 
@@ -115,6 +116,15 @@ struct delayed_work {
 	int cpu;
 };
 
+/*
+ * A struct for workqueue attributes.  This can be used to change
+ * attributes of an unbound workqueue.
+ */
+struct workqueue_attrs {
+	int			nice;		/* nice level */
+	cpumask_var_t		cpumask;	/* allowed CPUs */
+};
+
 static inline struct delayed_work *to_delayed_work(struct work_struct *work)
 {
 	return container_of(work, struct delayed_work, work);
@@ -399,6 +409,9 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
+struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);
+void free_workqueue_attrs(struct workqueue_attrs *attrs);
+
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
 extern bool queue_work(struct workqueue_struct *wq, struct work_struct *work);

commit d84ff0512f1bfc0d8c864efadb4523fce68919cc
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 12 11:29:59 2013 -0700

    workqueue: consistently use int for @cpu variables
    
    Workqueue is mixing unsigned int and int for @cpu variables.  There's
    no point in using unsigned int for cpus - many of cpu related APIs
    take int anyway.  Consistently use int for @cpu variables so that we
    can use negative values to mark special ones.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5bd030f630a9..899be6636d20 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -435,7 +435,7 @@ extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
-extern bool workqueue_congested(unsigned int cpu, struct workqueue_struct *wq);
+extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
 extern unsigned int work_busy(struct work_struct *work);
 
 /*
@@ -466,12 +466,12 @@ static inline bool __deprecated flush_delayed_work_sync(struct delayed_work *dwo
 }
 
 #ifndef CONFIG_SMP
-static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
+static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
 {
 	return fn(arg);
 }
 #else
-long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
+long work_on_cpu(int cpu, long (*fn)(void *), void *arg);
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_FREEZER

commit 45d9550a0e7e9230606ca3c4c6f4dc6297848b2f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Feb 19 12:17:01 2013 -0800

    workqueue: allow more off-queue flag space
    
    When a work item is off-queue, its work->data contains WORK_STRUCT_*
    and WORK_OFFQ_* flags.  As WORK_OFFQ_* flags are used only while a
    work item is off-queue, it can occupy bits of work->data which aren't
    used while off-queue.  WORK_OFFQ_* currently only use bits used by
    on-queue CWQ pointer.  As color bits aren't used while off-queue,
    there's no reason to not use them.
    
    Lower WORK_OFFQ_FLAG_BASE from WORK_STRUCT_FLAG_BITS to
    WORK_STRUCT_COLOR_SHIFT thus giving 4 more bits to off-queue flag
    space which is also used to record worker_pool ID while off-queue.
    
    This doesn't introduce any visible behavior difference.
    
    tj: Rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 8afab27cdbc2..5bd030f630a9 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -68,7 +68,7 @@ enum {
 				  WORK_STRUCT_COLOR_BITS,
 
 	/* data contains off-queue information when !WORK_STRUCT_PWQ */
-	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_FLAG_BITS,
+	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_COLOR_SHIFT,
 
 	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),
 

commit 112202d9098aae2c36436e5178c0cf3ced423c7b
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 13 19:29:12 2013 -0800

    workqueue: rename cpu_workqueue to pool_workqueue
    
    workqueue has moved away from global_cwqs to worker_pools and with the
    scheduled custom worker pools, wforkqueues will be associated with
    pools which don't have anything to do with CPUs.  The workqueue code
    went through significant amount of changes recently and mass renaming
    isn't likely to hurt much additionally.  Let's replace 'cpu' with
    'pool' so that it reflects the current design.
    
    * s/struct cpu_workqueue_struct/struct pool_workqueue/
    * s/cpu_wq/pool_wq/
    * s/cwq/pwq/
    
    This patch is purely cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a3d7556510c3..8afab27cdbc2 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -27,7 +27,7 @@ void delayed_work_timer_fn(unsigned long __data);
 enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
 	WORK_STRUCT_DELAYED_BIT	= 1,	/* work item is delayed */
-	WORK_STRUCT_CWQ_BIT	= 2,	/* data points to cwq */
+	WORK_STRUCT_PWQ_BIT	= 2,	/* data points to pwq */
 	WORK_STRUCT_LINKED_BIT	= 3,	/* next work is linked to this one */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC_BIT	= 4,	/* static initializer (debugobjects) */
@@ -40,7 +40,7 @@ enum {
 
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
 	WORK_STRUCT_DELAYED	= 1 << WORK_STRUCT_DELAYED_BIT,
-	WORK_STRUCT_CWQ		= 1 << WORK_STRUCT_CWQ_BIT,
+	WORK_STRUCT_PWQ		= 1 << WORK_STRUCT_PWQ_BIT,
 	WORK_STRUCT_LINKED	= 1 << WORK_STRUCT_LINKED_BIT,
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
@@ -60,14 +60,14 @@ enum {
 	WORK_CPU_END		= NR_CPUS + 1,
 
 	/*
-	 * Reserve 7 bits off of cwq pointer w/ debugobjects turned
-	 * off.  This makes cwqs aligned to 256 bytes and allows 15
-	 * workqueue flush colors.
+	 * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.
+	 * This makes pwqs aligned to 256 bytes and allows 15 workqueue
+	 * flush colors.
 	 */
 	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +
 				  WORK_STRUCT_COLOR_BITS,
 
-	/* data contains off-queue information when !WORK_STRUCT_CWQ */
+	/* data contains off-queue information when !WORK_STRUCT_PWQ */
 	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_FLAG_BITS,
 
 	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),

commit 60c057bca22285efefbba033624763a778f243bf
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: add delayed_work->wq to simplify reentrancy handling
    
    To avoid executing the same work item from multiple CPUs concurrently,
    a work_struct records the last pool it was on in its ->data so that,
    on the next queueing, the pool can be queried to determine whether the
    work item is still executing or not.
    
    A delayed_work goes through timer before actually being queued on the
    target workqueue and the timer needs to know the target workqueue and
    CPU.  This is currently achieved by modifying delayed_work->work.data
    such that it points to the cwq which points to the target workqueue
    and the last CPU the work item was on.  __queue_delayed_work()
    extracts the last CPU from delayed_work->work.data and then combines
    it with the target workqueue to create new work.data.
    
    The only thing this rather ugly hack achieves is encoding the target
    workqueue into delayed_work->work.data without using a separate field,
    which could be a trade off one can make; unfortunately, this entangles
    work->data management between regular workqueue and delayed_work code
    by setting cwq pointer before the work item is actually queued and
    becomes a hindrance for further improvements of work->data handling.
    
    This can be easily made sane by adding a target workqueue field to
    delayed_work.  While delayed_work is used widely in the kernel and
    this does make it a bit larger (<5%), I think this is the right
    trade-off especially given the prospect of much saner handling of
    work->data which currently involves quite tricky memory barrier
    dancing, and don't expect to see any measureable effect.
    
    Add delayed_work->wq and drop the delayed_work->work.data overloading.
    
    tj: Rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 426c39c2aaa4..a3d7556510c3 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -109,6 +109,9 @@ struct work_struct {
 struct delayed_work {
 	struct work_struct work;
 	struct timer_list timer;
+
+	/* target workqueue and CPU ->timer uses to queue ->work */
+	struct workqueue_struct *wq;
 	int cpu;
 };
 

commit 6be195886ac26abe0194ed1bc7a9224f8a97c310
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 6 18:04:53 2013 -0800

    workqueue: replace WORK_CPU_NONE/LAST with WORK_CPU_END
    
    Now that workqueue has moved away from gcwqs, workqueue no longer has
    the need to have a CPU identifier indicating "no cpu associated" - we
    now use WORK_OFFQ_POOL_NONE instead - and most uses of WORK_CPU_NONE
    are gone.
    
    The only left usage is as the end marker for for_each_*wq*()
    iterators, where the name WORK_CPU_NONE is confusing w/o actual
    WORK_CPU_NONE usages.  Similarly, WORK_CPU_LAST which equals
    WORK_CPU_NONE no longer makes sense.
    
    Replace both WORK_CPU_NONE and LAST with WORK_CPU_END.  This patch
    doesn't introduce any functional difference.
    
    tj: s/WORK_CPU_LAST/WORK_CPU_END/ and rewrote the description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a94e4e84e7b1..426c39c2aaa4 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -57,8 +57,7 @@ enum {
 
 	/* special cpu IDs */
 	WORK_CPU_UNBOUND	= NR_CPUS,
-	WORK_CPU_NONE		= NR_CPUS + 1,
-	WORK_CPU_LAST		= WORK_CPU_NONE,
+	WORK_CPU_END		= NR_CPUS + 1,
 
 	/*
 	 * Reserve 7 bits off of cwq pointer w/ debugobjects turned

commit 7c3eed5cd60d0f736516e6ade77d90c6255860bd
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: record pool ID instead of CPU in work->data when off-queue
    
    Currently, when a work item is off-queue, work->data records the CPU
    it was last on, which is used to locate the last executing instance
    for non-reentrance, flushing, etc.
    
    We're in the process of removing global_cwq and making worker_pool the
    top level abstraction.  This patch makes work->data point to the pool
    it was last associated with instead of CPU.
    
    After the previous WORK_OFFQ_POOL_CPU and worker_poo->id additions,
    the conversion is fairly straight-forward.  WORK_OFFQ constants and
    functions are modified to record and read back pool ID instead.
    worker_pool_by_id() is added to allow looking up pool from ID.
    get_work_pool() replaces get_work_gcwq(), which is reimplemented using
    get_work_pool().  get_work_pool_id() replaces work_cpu().
    
    This patch shouldn't introduce any observable behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f8b35763e55f..a94e4e84e7b1 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -75,19 +75,19 @@ enum {
 
 	/*
 	 * When a work item is off queue, its high bits point to the last
-	 * cpu it was on.  Cap at 31 bits and use the highest number to
-	 * indicate that no cpu is associated.
+	 * pool it was on.  Cap at 31 bits and use the highest number to
+	 * indicate that no pool is associated.
 	 */
 	WORK_OFFQ_FLAG_BITS	= 1,
-	WORK_OFFQ_CPU_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
-	WORK_OFFQ_LEFT		= BITS_PER_LONG - WORK_OFFQ_CPU_SHIFT,
-	WORK_OFFQ_CPU_BITS	= WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
-	WORK_OFFQ_CPU_NONE	= (1LU << WORK_OFFQ_CPU_BITS) - 1,
+	WORK_OFFQ_POOL_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
+	WORK_OFFQ_LEFT		= BITS_PER_LONG - WORK_OFFQ_POOL_SHIFT,
+	WORK_OFFQ_POOL_BITS	= WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
+	WORK_OFFQ_POOL_NONE	= (1LU << WORK_OFFQ_POOL_BITS) - 1,
 
 	/* convenience constants */
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
-	WORK_STRUCT_NO_CPU	= (unsigned long)WORK_OFFQ_CPU_NONE << WORK_OFFQ_CPU_SHIFT,
+	WORK_STRUCT_NO_POOL	= (unsigned long)WORK_OFFQ_POOL_NONE << WORK_OFFQ_POOL_SHIFT,
 
 	/* bit mask for work_busy() return values */
 	WORK_BUSY_PENDING	= 1 << 0,
@@ -103,9 +103,9 @@ struct work_struct {
 #endif
 };
 
-#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_NO_CPU)
+#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL)
 #define WORK_DATA_STATIC_INIT()	\
-	ATOMIC_LONG_INIT(WORK_STRUCT_NO_CPU | WORK_STRUCT_STATIC)
+	ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC)
 
 struct delayed_work {
 	struct work_struct work;

commit 715b06b864c99a18cb8368dfb187da4f569788cd
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:33 2013 -0800

    workqueue: introduce WORK_OFFQ_CPU_NONE
    
    Currently, when a work item is off queue, high bits of its data
    encodes the last CPU it was on.  This is scheduled to be changed to
    pool ID, which will make it impossible to use WORK_CPU_NONE to
    indicate no association.
    
    This patch limits the number of bits which are used for off-queue cpu
    number to 31 (so that the max fits in an int) and uses the highest
    possible value - WORK_OFFQ_CPU_NONE - to indicate no association.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ff68b1d95b41..f8b35763e55f 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -73,13 +73,21 @@ enum {
 
 	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),
 
+	/*
+	 * When a work item is off queue, its high bits point to the last
+	 * cpu it was on.  Cap at 31 bits and use the highest number to
+	 * indicate that no cpu is associated.
+	 */
 	WORK_OFFQ_FLAG_BITS	= 1,
 	WORK_OFFQ_CPU_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
+	WORK_OFFQ_LEFT		= BITS_PER_LONG - WORK_OFFQ_CPU_SHIFT,
+	WORK_OFFQ_CPU_BITS	= WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
+	WORK_OFFQ_CPU_NONE	= (1LU << WORK_OFFQ_CPU_BITS) - 1,
 
 	/* convenience constants */
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
-	WORK_STRUCT_NO_CPU	= (unsigned long)WORK_CPU_NONE << WORK_OFFQ_CPU_SHIFT,
+	WORK_STRUCT_NO_CPU	= (unsigned long)WORK_OFFQ_CPU_NONE << WORK_OFFQ_CPU_SHIFT,
 
 	/* bit mask for work_busy() return values */
 	WORK_BUSY_PENDING	= 1 << 0,

commit e2905b29122173b72b612c962b138e3fa07476b8
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 24 11:01:32 2013 -0800

    workqueue: unexport work_cpu()
    
    This function no longer has any external users.  Unexport it.  It will
    be removed later on.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 2b58905d3504..ff68b1d95b41 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -426,7 +426,6 @@ extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
 extern bool workqueue_congested(unsigned int cpu, struct workqueue_struct *wq);
-extern unsigned int work_cpu(struct work_struct *work);
 extern unsigned int work_busy(struct work_struct *work);
 
 /*

commit 136b5721d75a62a8f02c601c89122e32c1a85a84
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:24 2012 -0700

    workqueue: deprecate __cancel_delayed_work()
    
    Now that cancel_delayed_work() can be safely called from IRQ handlers,
    there's no reason to use __cancel_delayed_work().  Use
    cancel_delayed_work() instead of __cancel_delayed_work() and mark the
    latter deprecated.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4898289564ab..2b58905d3504 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -434,7 +434,7 @@ extern unsigned int work_busy(struct work_struct *work);
  * if it returns 0 the timer function may be running and the queueing is in
  * progress.
  */
-static inline bool __cancel_delayed_work(struct delayed_work *work)
+static inline bool __deprecated __cancel_delayed_work(struct delayed_work *work)
 {
 	bool ret;
 

commit 57b30ae77bf00d2318df711ef9a4d2a9be0a3a2a
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:24 2012 -0700

    workqueue: reimplement cancel_delayed_work() using try_to_grab_pending()
    
    cancel_delayed_work() can't be called from IRQ handlers due to its use
    of del_timer_sync() and can't cancel work items which are already
    transferred from timer to worklist.
    
    Also, unlike other flush and cancel functions, a canceled delayed_work
    would still point to the last associated cpu_workqueue.  If the
    workqueue is destroyed afterwards and the work item is re-used on a
    different workqueue, the queueing code can oops trying to dereference
    already freed cpu_workqueue.
    
    This patch reimplements cancel_delayed_work() using
    try_to_grab_pending() and set_work_cpu_and_clear_pending().  This
    allows the function to be called from IRQ handlers and makes its
    behavior consistent with other flush / cancel functions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d86b320319e0..4898289564ab 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -420,6 +420,7 @@ extern bool flush_work(struct work_struct *work);
 extern bool cancel_work_sync(struct work_struct *work);
 
 extern bool flush_delayed_work(struct delayed_work *dwork);
+extern bool cancel_delayed_work(struct delayed_work *dwork);
 extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
@@ -428,22 +429,6 @@ extern bool workqueue_congested(unsigned int cpu, struct workqueue_struct *wq);
 extern unsigned int work_cpu(struct work_struct *work);
 extern unsigned int work_busy(struct work_struct *work);
 
-/*
- * Kill off a pending schedule_delayed_work().  Note that the work callback
- * function may still be running on return from cancel_delayed_work(), unless
- * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
- * cancel_work_sync() to wait on it.
- */
-static inline bool cancel_delayed_work(struct delayed_work *work)
-{
-	bool ret;
-
-	ret = del_timer_sync(&work->timer);
-	if (ret)
-		work_clear_pending(&work->work);
-	return ret;
-}
-
 /*
  * Like above, but uses del_timer() instead of del_timer_sync(). This means,
  * if it returns 0 the timer function may be running and the queueing is in

commit e0aecdd874d78b7129a64b056c20e529e2c916df
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:24 2012 -0700

    workqueue: use irqsafe timer for delayed_work
    
    Up to now, for delayed_works, try_to_grab_pending() couldn't be used
    from IRQ handlers because IRQs may happen while
    delayed_work_timer_fn() is in progress leading to indefinite -EAGAIN.
    
    This patch makes delayed_work use the new TIMER_IRQSAFE flag for
    delayed_work->timer.  This makes try_to_grab_pending() and thus
    mod_delayed_work_on() safe to call from IRQ handlers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index e84ebb69607d..d86b320319e0 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -136,7 +136,8 @@ struct execute_work {
 #define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
 	.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,		\
-			0, (unsigned long)&(n), (tflags)),		\
+				     0, (unsigned long)&(n),		\
+				     (tflags) | TIMER_IRQSAFE),		\
 	}
 
 #define DECLARE_WORK(n, f)						\
@@ -214,7 +215,8 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 	do {								\
 		INIT_WORK(&(_work)->work, (_func));			\
 		__setup_timer(&(_work)->timer, delayed_work_timer_fn,	\
-			      (unsigned long)(_work), (_tflags));	\
+			      (unsigned long)(_work),			\
+			      (_tflags) | TIMER_IRQSAFE);		\
 	} while (0)
 
 #define __INIT_DELAYED_WORK_ONSTACK(_work, _func, _tflags)		\
@@ -223,7 +225,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		__setup_timer_on_stack(&(_work)->timer,			\
 				       delayed_work_timer_fn,		\
 				       (unsigned long)(_work),		\
-				       (_tflags));			\
+				       (_tflags) | TIMER_IRQSAFE);	\
 	} while (0)
 
 #define INIT_DELAYED_WORK(_work, _func)					\

commit f991b318cc6627a493b0d317a565bb7c3271f36b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: clean up delayed_work initializers and add missing one
    
    Reimplement delayed_work initializers using new timer initializers
    which take timer flags.  This reduces code duplications and will ease
    further initializer changes.  This patch also adds a missing
    initializer - INIT_DEFERRABLE_WORK_ONSTACK().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 49a9c51f9ee3..e84ebb69607d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -133,26 +133,20 @@ struct execute_work {
 	__WORK_INIT_LOCKDEP_MAP(#n, &(n))				\
 	}
 
-#define __DELAYED_WORK_INITIALIZER(n, f) {				\
+#define __DELAYED_WORK_INITIALIZER(n, f, tflags) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
-	.timer = TIMER_INITIALIZER(delayed_work_timer_fn,		\
-				0, (unsigned long)&(n)),		\
-	}
-
-#define __DEFERRABLE_WORK_INITIALIZER(n, f) {				\
-	.work = __WORK_INITIALIZER((n).work, (f)),			\
-	.timer = TIMER_DEFERRED_INITIALIZER(delayed_work_timer_fn,	\
-				0, (unsigned long)&(n)),		\
+	.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,		\
+			0, (unsigned long)&(n), (tflags)),		\
 	}
 
 #define DECLARE_WORK(n, f)						\
 	struct work_struct n = __WORK_INITIALIZER(n, f)
 
 #define DECLARE_DELAYED_WORK(n, f)					\
-	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
+	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, 0)
 
 #define DECLARE_DEFERRABLE_WORK(n, f)					\
-	struct delayed_work n = __DEFERRABLE_WORK_INITIALIZER(n, f)
+	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, TIMER_DEFERRABLE)
 
 /*
  * initialize a work item's function pointer
@@ -216,29 +210,33 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		__INIT_WORK((_work), (_func), 1);			\
 	} while (0)
 
-#define INIT_DELAYED_WORK(_work, _func)					\
+#define __INIT_DELAYED_WORK(_work, _func, _tflags)			\
 	do {								\
 		INIT_WORK(&(_work)->work, (_func));			\
-		init_timer(&(_work)->timer);				\
-		(_work)->timer.function = delayed_work_timer_fn;	\
-		(_work)->timer.data = (unsigned long)(_work);		\
+		__setup_timer(&(_work)->timer, delayed_work_timer_fn,	\
+			      (unsigned long)(_work), (_tflags));	\
 	} while (0)
 
-#define INIT_DELAYED_WORK_ONSTACK(_work, _func)				\
+#define __INIT_DELAYED_WORK_ONSTACK(_work, _func, _tflags)		\
 	do {								\
 		INIT_WORK_ONSTACK(&(_work)->work, (_func));		\
-		init_timer_on_stack(&(_work)->timer);			\
-		(_work)->timer.function = delayed_work_timer_fn;	\
-		(_work)->timer.data = (unsigned long)(_work);		\
+		__setup_timer_on_stack(&(_work)->timer,			\
+				       delayed_work_timer_fn,		\
+				       (unsigned long)(_work),		\
+				       (_tflags));			\
 	} while (0)
 
+#define INIT_DELAYED_WORK(_work, _func)					\
+	__INIT_DELAYED_WORK(_work, _func, 0)
+
+#define INIT_DELAYED_WORK_ONSTACK(_work, _func)				\
+	__INIT_DELAYED_WORK_ONSTACK(_work, _func, 0)
+
 #define INIT_DEFERRABLE_WORK(_work, _func)				\
-	do {								\
-		INIT_WORK(&(_work)->work, (_func));			\
-		init_timer_deferrable(&(_work)->timer);			\
-		(_work)->timer.function = delayed_work_timer_fn;	\
-		(_work)->timer.data = (unsigned long)(_work);		\
-	} while (0)
+	__INIT_DELAYED_WORK(_work, _func, TIMER_DEFERRABLE)
+
+#define INIT_DEFERRABLE_WORK_ONSTACK(_work, _func)			\
+	__INIT_DELAYED_WORK_ONSTACK(_work, _func, TIMER_DEFERRABLE)
 
 /**
  * work_pending - Find out whether a work item is currently pending

commit 203b42f7317494ae5e5efc7be6fb7f29c927f102
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: make deferrable delayed_work initializer names consistent
    
    Initalizers for deferrable delayed_work are confused.
    
    * __DEFERRED_WORK_INITIALIZER()
    * DECLARE_DEFERRED_WORK()
    * INIT_DELAYED_WORK_DEFERRABLE()
    
    Rename them to
    
    * __DEFERRABLE_WORK_INITIALIZER()
    * DECLARE_DEFERRABLE_WORK()
    * INIT_DEFERRABLE_WORK()
    
    This patch doesn't cause any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 26c5b4c63861..49a9c51f9ee3 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -139,7 +139,7 @@ struct execute_work {
 				0, (unsigned long)&(n)),		\
 	}
 
-#define __DEFERRED_WORK_INITIALIZER(n, f) {				\
+#define __DEFERRABLE_WORK_INITIALIZER(n, f) {				\
 	.work = __WORK_INITIALIZER((n).work, (f)),			\
 	.timer = TIMER_DEFERRED_INITIALIZER(delayed_work_timer_fn,	\
 				0, (unsigned long)&(n)),		\
@@ -151,8 +151,8 @@ struct execute_work {
 #define DECLARE_DELAYED_WORK(n, f)					\
 	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
 
-#define DECLARE_DEFERRED_WORK(n, f)					\
-	struct delayed_work n = __DEFERRED_WORK_INITIALIZER(n, f)
+#define DECLARE_DEFERRABLE_WORK(n, f)					\
+	struct delayed_work n = __DEFERRABLE_WORK_INITIALIZER(n, f)
 
 /*
  * initialize a work item's function pointer
@@ -232,7 +232,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		(_work)->timer.data = (unsigned long)(_work);		\
 	} while (0)
 
-#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
+#define INIT_DEFERRABLE_WORK(_work, _func)				\
 	do {								\
 		INIT_WORK(&(_work)->work, (_func));			\
 		init_timer_deferrable(&(_work)->timer);			\

commit ee64e7f697ad7e5575e6ac8900cfb71975484421
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:23 2012 -0700

    workqueue: cosmetic whitespace updates for macro definitions
    
    Consistently use the last tab position for '\' line continuation in
    complex macro definitions.  This is to help the following patches.
    
    This patch is cosmetic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1ce3fb08308d..26c5b4c63861 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -126,43 +126,43 @@ struct execute_work {
 #define __WORK_INIT_LOCKDEP_MAP(n, k)
 #endif
 
-#define __WORK_INITIALIZER(n, f) {				\
-	.data = WORK_DATA_STATIC_INIT(),			\
-	.entry	= { &(n).entry, &(n).entry },			\
-	.func = (f),						\
-	__WORK_INIT_LOCKDEP_MAP(#n, &(n))			\
+#define __WORK_INITIALIZER(n, f) {					\
+	.data = WORK_DATA_STATIC_INIT(),				\
+	.entry	= { &(n).entry, &(n).entry },				\
+	.func = (f),							\
+	__WORK_INIT_LOCKDEP_MAP(#n, &(n))				\
 	}
 
-#define __DELAYED_WORK_INITIALIZER(n, f) {			\
-	.work = __WORK_INITIALIZER((n).work, (f)),		\
-	.timer = TIMER_INITIALIZER(delayed_work_timer_fn,	\
-				0, (unsigned long)&(n)),	\
+#define __DELAYED_WORK_INITIALIZER(n, f) {				\
+	.work = __WORK_INITIALIZER((n).work, (f)),			\
+	.timer = TIMER_INITIALIZER(delayed_work_timer_fn,		\
+				0, (unsigned long)&(n)),		\
 	}
 
-#define __DEFERRED_WORK_INITIALIZER(n, f) {			\
-	.work = __WORK_INITIALIZER((n).work, (f)),		\
-	.timer = TIMER_DEFERRED_INITIALIZER(delayed_work_timer_fn, \
-				0, (unsigned long)&(n)),	\
+#define __DEFERRED_WORK_INITIALIZER(n, f) {				\
+	.work = __WORK_INITIALIZER((n).work, (f)),			\
+	.timer = TIMER_DEFERRED_INITIALIZER(delayed_work_timer_fn,	\
+				0, (unsigned long)&(n)),		\
 	}
 
-#define DECLARE_WORK(n, f)					\
+#define DECLARE_WORK(n, f)						\
 	struct work_struct n = __WORK_INITIALIZER(n, f)
 
-#define DECLARE_DELAYED_WORK(n, f)				\
+#define DECLARE_DELAYED_WORK(n, f)					\
 	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
 
-#define DECLARE_DEFERRED_WORK(n, f)				\
+#define DECLARE_DEFERRED_WORK(n, f)					\
 	struct delayed_work n = __DEFERRED_WORK_INITIALIZER(n, f)
 
 /*
  * initialize a work item's function pointer
  */
-#define PREPARE_WORK(_work, _func)				\
-	do {							\
-		(_work)->func = (_func);			\
+#define PREPARE_WORK(_work, _func)					\
+	do {								\
+		(_work)->func = (_func);				\
 	} while (0)
 
-#define PREPARE_DELAYED_WORK(_work, _func)			\
+#define PREPARE_DELAYED_WORK(_work, _func)				\
 	PREPARE_WORK(&(_work)->work, (_func))
 
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
@@ -192,7 +192,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 									\
 		__init_work((_work), _onstack);				\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
-		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0);\
+		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0); \
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
@@ -206,38 +206,38 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 	} while (0)
 #endif
 
-#define INIT_WORK(_work, _func)					\
-	do {							\
-		__INIT_WORK((_work), (_func), 0);		\
+#define INIT_WORK(_work, _func)						\
+	do {								\
+		__INIT_WORK((_work), (_func), 0);			\
 	} while (0)
 
-#define INIT_WORK_ONSTACK(_work, _func)				\
-	do {							\
-		__INIT_WORK((_work), (_func), 1);		\
+#define INIT_WORK_ONSTACK(_work, _func)					\
+	do {								\
+		__INIT_WORK((_work), (_func), 1);			\
 	} while (0)
 
-#define INIT_DELAYED_WORK(_work, _func)				\
-	do {							\
-		INIT_WORK(&(_work)->work, (_func));		\
-		init_timer(&(_work)->timer);			\
-		(_work)->timer.function = delayed_work_timer_fn;\
-		(_work)->timer.data = (unsigned long)(_work);	\
+#define INIT_DELAYED_WORK(_work, _func)					\
+	do {								\
+		INIT_WORK(&(_work)->work, (_func));			\
+		init_timer(&(_work)->timer);				\
+		(_work)->timer.function = delayed_work_timer_fn;	\
+		(_work)->timer.data = (unsigned long)(_work);		\
 	} while (0)
 
-#define INIT_DELAYED_WORK_ONSTACK(_work, _func)			\
-	do {							\
-		INIT_WORK_ONSTACK(&(_work)->work, (_func));	\
-		init_timer_on_stack(&(_work)->timer);		\
-		(_work)->timer.function = delayed_work_timer_fn;\
-		(_work)->timer.data = (unsigned long)(_work);	\
+#define INIT_DELAYED_WORK_ONSTACK(_work, _func)				\
+	do {								\
+		INIT_WORK_ONSTACK(&(_work)->work, (_func));		\
+		init_timer_on_stack(&(_work)->timer);			\
+		(_work)->timer.function = delayed_work_timer_fn;	\
+		(_work)->timer.data = (unsigned long)(_work);		\
 	} while (0)
 
-#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)		\
-	do {							\
-		INIT_WORK(&(_work)->work, (_func));		\
-		init_timer_deferrable(&(_work)->timer);		\
-		(_work)->timer.function = delayed_work_timer_fn;\
-		(_work)->timer.data = (unsigned long)(_work);	\
+#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
+	do {								\
+		INIT_WORK(&(_work)->work, (_func));			\
+		init_timer_deferrable(&(_work)->timer);			\
+		(_work)->timer.function = delayed_work_timer_fn;	\
+		(_work)->timer.data = (unsigned long)(_work);		\
 	} while (0)
 
 /**
@@ -345,22 +345,22 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
 #ifdef CONFIG_LOCKDEP
-#define alloc_workqueue(fmt, flags, max_active, args...)	\
-({								\
-	static struct lock_class_key __key;			\
-	const char *__lock_name;				\
-								\
-	if (__builtin_constant_p(fmt))				\
-		__lock_name = (fmt);				\
-	else							\
-		__lock_name = #fmt;				\
-								\
-	__alloc_workqueue_key((fmt), (flags), (max_active),	\
-			      &__key, __lock_name, ##args);	\
+#define alloc_workqueue(fmt, flags, max_active, args...)		\
+({									\
+	static struct lock_class_key __key;				\
+	const char *__lock_name;					\
+									\
+	if (__builtin_constant_p(fmt))					\
+		__lock_name = (fmt);					\
+	else								\
+		__lock_name = #fmt;					\
+									\
+	__alloc_workqueue_key((fmt), (flags), (max_active),		\
+			      &__key, __lock_name, ##args);		\
 })
 #else
-#define alloc_workqueue(fmt, flags, max_active, args...)	\
-	__alloc_workqueue_key((fmt), (flags), (max_active),	\
+#define alloc_workqueue(fmt, flags, max_active, args...)		\
+	__alloc_workqueue_key((fmt), (flags), (max_active),		\
 			      NULL, NULL, ##args)
 #endif
 
@@ -377,14 +377,14 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
  * RETURNS:
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
-#define alloc_ordered_workqueue(fmt, flags, args...)		\
+#define alloc_ordered_workqueue(fmt, flags, args...)			\
 	alloc_workqueue(fmt, WQ_UNBOUND | (flags), 1, ##args)
 
-#define create_workqueue(name)					\
+#define create_workqueue(name)						\
 	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)
-#define create_freezable_workqueue(name)			\
+#define create_freezable_workqueue(name)				\
 	alloc_workqueue((name), WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
-#define create_singlethread_workqueue(name)			\
+#define create_singlethread_workqueue(name)				\
 	alloc_workqueue((name), WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);

commit 3b07e9ca26866697616097044f25fbe53dbab693
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 20 14:51:24 2012 -0700

    workqueue: deprecate system_nrt[_freezable]_wq
    
    system_nrt[_freezable]_wq are now spurious.  Mark them deprecated and
    convert all users to system[_freezable]_wq.
    
    If you're cc'd and wondering what's going on: Now all workqueues are
    non-reentrant, so there's no reason to use system_nrt[_freezable]_wq.
    Please use system[_freezable]_wq instead.
    
    This patch doesn't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-By: Lai Jiangshan <laijs@cn.fujitsu.com>
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index a351be7c3e91..1ce3fb08308d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -310,12 +310,12 @@ extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;
 
-static inline struct workqueue_struct *__system_nrt_wq(void)
+static inline struct workqueue_struct * __deprecated __system_nrt_wq(void)
 {
 	return system_wq;
 }
 
-static inline struct workqueue_struct *__system_nrt_freezable_wq(void)
+static inline struct workqueue_struct * __deprecated __system_nrt_freezable_wq(void)
 {
 	return system_freezable_wq;
 }

commit 43829731dd372d04d6706c51052b9dabab9ca356
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 20 14:51:24 2012 -0700

    workqueue: deprecate flush[_delayed]_work_sync()
    
    flush[_delayed]_work_sync() are now spurious.  Mark them deprecated
    and convert all users to flush[_delayed]_work().
    
    If you're cc'd and wondering what's going on: Now all workqueues are
    non-reentrant and the regular flushes guarantee that the work item is
    not pending or running on any CPU on return, so there's no reason to
    use the sync flushes at all and they're going away.
    
    This patch doesn't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Mattia Dongili <malattia@linux.it>
    Cc: Kent Yoder <key@linux.vnet.ibm.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Karsten Keil <isdn@linux-pingi.de>
    Cc: Bryan Wu <bryan.wu@canonical.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mauro Carvalho Chehab <mchehab@infradead.org>
    Cc: Florian Tobias Schandinat <FlorianSchandinat@gmx.de>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: linux-wireless@vger.kernel.org
    Cc: Anton Vorontsov <cbou@mail.ru>
    Cc: Sangbeom Kim <sbkim73@samsung.com>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Eric Van Hensbergen <ericvh@gmail.com>
    Cc: Takashi Iwai <tiwai@suse.de>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Petr Vandrovec <petr@vandrovec.name>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 855fcdaa2d72..a351be7c3e91 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -460,13 +460,13 @@ static inline bool __cancel_delayed_work(struct delayed_work *work)
 }
 
 /* used to be different but now identical to flush_work(), deprecated */
-static inline bool flush_work_sync(struct work_struct *work)
+static inline bool __deprecated flush_work_sync(struct work_struct *work)
 {
 	return flush_work(work);
 }
 
 /* used to be different but now identical to flush_delayed_work(), deprecated */
-static inline bool flush_delayed_work_sync(struct delayed_work *dwork)
+static inline bool __deprecated flush_delayed_work_sync(struct delayed_work *dwork)
 {
 	return flush_delayed_work(dwork);
 }

commit ae930e0f4e66fd540c6fbad9f1e2a7743d8b9afe
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 20 14:51:23 2012 -0700

    workqueue: gut system_nrt[_freezable]_wq()
    
    Now that all workqueues are non-reentrant, system[_freezable]_wq() are
    equivalent to system_nrt[_freezable]_wq().  Replace the latter with
    wrappers around system[_freezable]_wq().  The wrapping goes through
    inline functions so that __deprecated can be added easily.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4f9d3bc161a2..855fcdaa2d72 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -297,10 +297,6 @@ enum {
  * system_long_wq is similar to system_wq but may host long running
  * works.  Queue flushing might take relatively long.
  *
- * system_nrt_wq is non-reentrant and guarantees that any given work
- * item is never executed in parallel by multiple CPUs.  Queue
- * flushing might take relatively long.
- *
  * system_unbound_wq is unbound workqueue.  Workers are not bound to
  * any specific CPU, not concurrency managed, and all queued works are
  * executed immediately as long as max_active limit is not reached and
@@ -308,16 +304,25 @@ enum {
  *
  * system_freezable_wq is equivalent to system_wq except that it's
  * freezable.
- *
- * system_nrt_freezable_wq is equivalent to system_nrt_wq except that
- * it's freezable.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
-extern struct workqueue_struct *system_nrt_wq;
 extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;
-extern struct workqueue_struct *system_nrt_freezable_wq;
+
+static inline struct workqueue_struct *__system_nrt_wq(void)
+{
+	return system_wq;
+}
+
+static inline struct workqueue_struct *__system_nrt_freezable_wq(void)
+{
+	return system_freezable_wq;
+}
+
+/* equivlalent to system_wq and system_freezable_wq, deprecated */
+#define system_nrt_wq			__system_nrt_wq()
+#define system_nrt_freezable_wq		__system_nrt_freezable_wq()
 
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,

commit 606a5020b9bdceb20b4f43e11db0054afa349028
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 20 14:51:23 2012 -0700

    workqueue: gut flush[_delayed]_work_sync()
    
    Now that all workqueues are non-reentrant, flush[_delayed]_work_sync()
    are equivalent to flush[_delayed]_work().  Drop the separate
    implementation and make them thin wrappers around
    flush[_delayed]_work().
    
    * start_flush_work() no longer takes @wait_executing as the only left
      user - flush_work() - always sets it to %true.
    
    * __cancel_work_timer() uses flush_work() instead of wait_on_work().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b14d5d59af7c..4f9d3bc161a2 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -412,11 +412,9 @@ extern int keventd_up(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern bool flush_work(struct work_struct *work);
-extern bool flush_work_sync(struct work_struct *work);
 extern bool cancel_work_sync(struct work_struct *work);
 
 extern bool flush_delayed_work(struct delayed_work *dwork);
-extern bool flush_delayed_work_sync(struct delayed_work *work);
 extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
@@ -456,6 +454,18 @@ static inline bool __cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
+/* used to be different but now identical to flush_work(), deprecated */
+static inline bool flush_work_sync(struct work_struct *work)
+{
+	return flush_work(work);
+}
+
+/* used to be different but now identical to flush_delayed_work(), deprecated */
+static inline bool flush_delayed_work_sync(struct delayed_work *dwork)
+{
+	return flush_delayed_work(dwork);
+}
+
 #ifndef CONFIG_SMP
 static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
 {

commit 1265057fa02c7bed3b6d9ddc8a2048065a370364
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Aug 8 09:38:42 2012 -0700

    workqueue: fix CPU binding of flush_delayed_work[_sync]()
    
    delayed_work encodes the workqueue to use and the last CPU in
    delayed_work->work.data while it's on timer.  The target CPU is
    implicitly recorded as the CPU the timer is queued on and
    delayed_work_timer_fn() queues delayed_work->work to the CPU it is
    running on.
    
    Unfortunately, this leaves flush_delayed_work[_sync]() no way to find
    out which CPU the delayed_work was queued for when they try to
    re-queue after killing the timer.  Currently, it chooses the local CPU
    flush is running on.  This can unexpectedly move a delayed_work queued
    on a specific CPU to another CPU and lead to subtle errors.
    
    There isn't much point in trying to save several bytes in struct
    delayed_work, which is already close to a hundred bytes on 64bit with
    all debug options turned off.  This patch adds delayed_work->cpu to
    remember the CPU it's queued for.
    
    Note that if the timer is migrated during CPU down, the work item
    could be queued to the downed global_cwq after this change.  As a
    detached global_cwq behaves like an unbound one, this doesn't change
    much for the delayed_work.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 20000305a8a6..b14d5d59af7c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -102,6 +102,7 @@ struct work_struct {
 struct delayed_work {
 	struct work_struct work;
 	struct timer_list timer;
+	int cpu;
 };
 
 static inline struct delayed_work *to_delayed_work(struct work_struct *work)

commit 8376fe22c7e79c7e90857d39f82aeae6cad6c4b8
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:47 2012 -0700

    workqueue: implement mod_delayed_work[_on]()
    
    Workqueue was lacking a mechanism to modify the timeout of an already
    pending delayed_work.  delayed_work users have been working around
    this using several methods - using an explicit timer + work item,
    messing directly with delayed_work->timer, and canceling before
    re-queueing, all of which are error-prone and/or ugly.
    
    This patch implements mod_delayed_work[_on]() which behaves similarly
    to mod_timer() - if the delayed_work is idle, it's queued with the
    given delay; otherwise, its timeout is modified to the new value.
    Zero @delay guarantees immediate execution.
    
    v2: Updated to reflect try_to_grab_pending() changes.  Now safe to be
        called from bh context.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5f4aeaa9f3e6..20000305a8a6 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -390,6 +390,10 @@ extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 extern bool queue_delayed_work(struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
+extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
+			struct delayed_work *dwork, unsigned long delay);
+extern bool mod_delayed_work(struct workqueue_struct *wq,
+			struct delayed_work *dwork, unsigned long delay);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);

commit bbb68dfaba73e8338fe0f1dc711cc1d261daec87
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:46 2012 -0700

    workqueue: mark a work item being canceled as such
    
    There can be two reasons try_to_grab_pending() can fail with -EAGAIN.
    One is when someone else is queueing or deqeueing the work item.  With
    the previous patches, it is guaranteed that PENDING and queued state
    will soon agree making it safe to busy-retry in this case.
    
    The other is if multiple __cancel_work_timer() invocations are racing
    one another.  __cancel_work_timer() grabs PENDING and then waits for
    running instances of the target work item on all CPUs while holding
    PENDING and !queued.  try_to_grab_pending() invoked from another task
    will keep returning -EAGAIN while the current owner is waiting.
    
    Not distinguishing the two cases is okay because __cancel_work_timer()
    is the only user of try_to_grab_pending() and it invokes
    wait_on_work() whenever grabbing fails.  For the first case, busy
    looping should be fine but wait_on_work() doesn't cause any critical
    problem.  For the latter case, the new contender usually waits for the
    same condition as the current owner, so no unnecessarily extended
    busy-looping happens.  Combined, these make __cancel_work_timer()
    technically correct even without irq protection while grabbing PENDING
    or distinguishing the two different cases.
    
    While the current code is technically correct, not distinguishing the
    two cases makes it difficult to use try_to_grab_pending() for other
    purposes than canceling because it's impossible to tell whether it's
    safe to busy-retry grabbing.
    
    This patch adds a mechanism to mark a work item being canceled.
    try_to_grab_pending() now disables irq on success and returns -EAGAIN
    to indicate that grabbing failed but PENDING and queued states are
    gonna agree soon and it's safe to busy-loop.  It returns -ENOENT if
    the work item is being canceled and it may stay PENDING && !queued for
    arbitrary amount of time.
    
    __cancel_work_timer() is modified to mark the work canceling with
    WORK_OFFQ_CANCELING after grabbing PENDING, thus making
    try_to_grab_pending() fail with -ENOENT instead of -EAGAIN.  Also, it
    invokes wait_on_work() iff grabbing failed with -ENOENT.  This isn't
    necessary for correctness but makes it consistent with other future
    users of try_to_grab_pending().
    
    v2: try_to_grab_pending() was testing preempt_count() to ensure that
        the caller has disabled preemption.  This triggers spuriously if
        !CONFIG_PREEMPT_COUNT.  Use preemptible() instead.  Reported by
        Fengguang Wu.
    
    v3: Updated so that try_to_grab_pending() disables irq on success
        rather than requiring preemption disabled by the caller.  This
        makes busy-looping easier and will allow try_to_grap_pending() to
        be used from bh/irq contexts.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f562674db404..5f4aeaa9f3e6 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -70,7 +70,10 @@ enum {
 
 	/* data contains off-queue information when !WORK_STRUCT_CWQ */
 	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_FLAG_BITS,
-	WORK_OFFQ_FLAG_BITS	= 0,
+
+	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),
+
+	WORK_OFFQ_FLAG_BITS	= 1,
 	WORK_OFFQ_CPU_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
 
 	/* convenience constants */

commit b5490077274482efde57a50b060b99bc839acd45
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:46 2012 -0700

    workqueue: introduce WORK_OFFQ_FLAG_*
    
    Low WORK_STRUCT_FLAG_BITS bits of work_struct->data contain
    WORK_STRUCT_FLAG_* and flush color.  If the work item is queued, the
    rest point to the cpu_workqueue with WORK_STRUCT_CWQ set; otherwise,
    WORK_STRUCT_CWQ is clear and the bits contain the last CPU number -
    either a real CPU number or one of WORK_CPU_*.
    
    Scheduled addition of mod_delayed_work[_on]() requires an additional
    flag, which is used only while a work item is off queue.  There are
    more than enough bits to represent off-queue CPU number on both 32 and
    64bits.  This patch introduces WORK_OFFQ_FLAG_* which occupy the lower
    part of the @work->data high bits while off queue.  This patch doesn't
    define any actual OFFQ flag yet.
    
    Off-queue CPU number is now shifted by WORK_OFFQ_CPU_SHIFT, which adds
    the number of bits used by OFFQ flags to WORK_STRUCT_FLAG_SHIFT, to
    make room for OFFQ flags.
    
    To avoid shift width warning with large WORK_OFFQ_FLAG_BITS, ulong
    cast is added to WORK_STRUCT_NO_CPU and, just in case, BUILD_BUG_ON()
    to check that there are enough bits to accomodate off-queue CPU number
    is added.
    
    This patch doesn't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ab95fef38d56..f562674db404 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -68,9 +68,15 @@ enum {
 	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +
 				  WORK_STRUCT_COLOR_BITS,
 
+	/* data contains off-queue information when !WORK_STRUCT_CWQ */
+	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_FLAG_BITS,
+	WORK_OFFQ_FLAG_BITS	= 0,
+	WORK_OFFQ_CPU_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
+
+	/* convenience constants */
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
-	WORK_STRUCT_NO_CPU	= WORK_CPU_NONE << WORK_STRUCT_FLAG_BITS,
+	WORK_STRUCT_NO_CPU	= (unsigned long)WORK_CPU_NONE << WORK_OFFQ_CPU_SHIFT,
 
 	/* bit mask for work_busy() return values */
 	WORK_BUSY_PENDING	= 1 << 0,

commit d8e794dfd51c368ed3f686b7f4172830b60ae47b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:45 2012 -0700

    workqueue: set delayed_work->timer function on initialization
    
    delayed_work->timer.function is currently initialized during
    queue_delayed_work_on().  Export delayed_work_timer_fn() and set
    delayed_work timer function during delayed_work initialization
    together with other fields.
    
    This ensures the timer function is always valid on an initialized
    delayed_work.  This is to help mod_delayed_work() implementation.
    
    To detect delayed_work users which diddle with the internal timer,
    trigger WARN if timer function doesn't match on queue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 278dc5ddb73f..ab95fef38d56 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -16,6 +16,7 @@ struct workqueue_struct;
 
 struct work_struct;
 typedef void (*work_func_t)(struct work_struct *work);
+void delayed_work_timer_fn(unsigned long __data);
 
 /*
  * The first word is the work queue pointer and the flags rolled into
@@ -124,12 +125,14 @@ struct execute_work {
 
 #define __DELAYED_WORK_INITIALIZER(n, f) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),		\
-	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
+	.timer = TIMER_INITIALIZER(delayed_work_timer_fn,	\
+				0, (unsigned long)&(n)),	\
 	}
 
 #define __DEFERRED_WORK_INITIALIZER(n, f) {			\
 	.work = __WORK_INITIALIZER((n).work, (f)),		\
-	.timer = TIMER_DEFERRED_INITIALIZER(NULL, 0, 0),	\
+	.timer = TIMER_DEFERRED_INITIALIZER(delayed_work_timer_fn, \
+				0, (unsigned long)&(n)),	\
 	}
 
 #define DECLARE_WORK(n, f)					\
@@ -207,18 +210,24 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\
 		init_timer(&(_work)->timer);			\
+		(_work)->timer.function = delayed_work_timer_fn;\
+		(_work)->timer.data = (unsigned long)(_work);	\
 	} while (0)
 
 #define INIT_DELAYED_WORK_ONSTACK(_work, _func)			\
 	do {							\
 		INIT_WORK_ONSTACK(&(_work)->work, (_func));	\
 		init_timer_on_stack(&(_work)->timer);		\
+		(_work)->timer.function = delayed_work_timer_fn;\
+		(_work)->timer.data = (unsigned long)(_work);	\
 	} while (0)
 
 #define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)		\
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\
 		init_timer_deferrable(&(_work)->timer);		\
+		(_work)->timer.function = delayed_work_timer_fn;\
+		(_work)->timer.data = (unsigned long)(_work);	\
 	} while (0)
 
 /**

commit d4283e9378619c14dc3826a6b0527eb5d967ffde
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:44 2012 -0700

    workqueue: make queueing functions return bool
    
    All queueing functions return 1 on success, 0 if the work item was
    already pending.  Update them to return bool instead.  This signifies
    better that they don't return 0 / -errno.
    
    This is cleanup and doesn't cause any functional difference.
    
    While at it, fix comment opening for schedule_work_on().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 597034276611..278dc5ddb73f 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -365,24 +365,24 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
-extern int queue_work_on(int cpu, struct workqueue_struct *wq,
+extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
-extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
-extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
+extern bool queue_work(struct workqueue_struct *wq, struct work_struct *work);
+extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
-extern int queue_delayed_work(struct workqueue_struct *wq,
+extern bool queue_delayed_work(struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
-extern int schedule_work_on(int cpu, struct work_struct *work);
-extern int schedule_work(struct work_struct *work);
-extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
-				    unsigned long delay);
-extern int schedule_delayed_work(struct delayed_work *work,
-				 unsigned long delay);
+extern bool schedule_work_on(int cpu, struct work_struct *work);
+extern bool schedule_work(struct work_struct *work);
+extern bool schedule_delayed_work_on(int cpu, struct delayed_work *work,
+				     unsigned long delay);
+extern bool schedule_delayed_work(struct delayed_work *work,
+				  unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);
 extern int keventd_up(void);
 

commit 0a13c00e9d4502b8e3fd9260ce781758ff2c3970
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 3 10:30:44 2012 -0700

    workqueue: reorder queueing functions so that _on() variants are on top
    
    Currently, queue/schedule[_delayed]_work_on() are located below the
    counterpart without the _on postifx even though the latter is usually
    implemented using the former.  Swap them.
    
    This is cleanup and doesn't cause any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index af155450cabb..597034276611 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -365,23 +365,24 @@ __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
-extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
 extern int queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
-extern int queue_delayed_work(struct workqueue_struct *wq,
-			struct delayed_work *work, unsigned long delay);
+extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
+extern int queue_delayed_work(struct workqueue_struct *wq,
+			struct delayed_work *work, unsigned long delay);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
-extern int schedule_work(struct work_struct *work);
 extern int schedule_work_on(int cpu, struct work_struct *work);
-extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
+extern int schedule_work(struct work_struct *work);
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
-					unsigned long delay);
+				    unsigned long delay);
+extern int schedule_delayed_work(struct delayed_work *work,
+				 unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);
 extern int keventd_up(void);
 

commit 62d3c5439c534b0e6c653fc63e6d8c67be3a57b1
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Fri Mar 2 10:51:00 2012 +0100

    Block: use a freezable workqueue for disk-event polling
    
    This patch (as1519) fixes a bug in the block layer's disk-events
    polling.  The polling is done by a work routine queued on the
    system_nrt_wq workqueue.  Since that workqueue isn't freezable, the
    polling continues even in the middle of a system sleep transition.
    
    Obviously, polling a suspended drive for media changes and such isn't
    a good thing to do; in the case of USB mass-storage devices it can
    lead to real problems requiring device resets and even re-enumeration.
    
    The patch fixes things by creating a new system-wide, non-reentrant,
    freezable workqueue and using it for disk-events polling.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    CC: <stable@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index eb8b9f15f2e0..af155450cabb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -289,12 +289,16 @@ enum {
  *
  * system_freezable_wq is equivalent to system_wq except that it's
  * freezable.
+ *
+ * system_nrt_freezable_wq is equivalent to system_nrt_wq except that
+ * it's freezable.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_nrt_wq;
 extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;
+extern struct workqueue_struct *system_nrt_freezable_wq;
 
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,

commit b196be89cdc14a88cc637cdad845a75c5886c82d
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 10 15:11:35 2012 -0800

    workqueue: make alloc_workqueue() take printf fmt and args for name
    
    alloc_workqueue() currently expects the passed in @name pointer to remain
    accessible.  This is inconvenient and a bit silly given that the whole wq
    is being dynamically allocated.  This patch updates alloc_workqueue() and
    friends to take printf format string instead of opaque string and matching
    varargs at the end.  The name is allocated together with the wq and
    formatted.
    
    alloc_ordered_workqueue() is converted to a macro to unify varargs
    handling with alloc_workqueue(), and, while at it, add comment to
    alloc_workqueue().
    
    None of the current in-kernel users pass in string with '%' as constant
    name and this change shouldn't cause any problem.
    
    [akpm@linux-foundation.org: use __printf]
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0d556deb497b..eb8b9f15f2e0 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -297,32 +297,50 @@ extern struct workqueue_struct *system_unbound_wq;
 extern struct workqueue_struct *system_freezable_wq;
 
 extern struct workqueue_struct *
-__alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
-		      struct lock_class_key *key, const char *lock_name);
+__alloc_workqueue_key(const char *fmt, unsigned int flags, int max_active,
+	struct lock_class_key *key, const char *lock_name, ...) __printf(1, 6);
 
+/**
+ * alloc_workqueue - allocate a workqueue
+ * @fmt: printf format for the name of the workqueue
+ * @flags: WQ_* flags
+ * @max_active: max in-flight work items, 0 for default
+ * @args: args for @fmt
+ *
+ * Allocate a workqueue with the specified parameters.  For detailed
+ * information on WQ_* flags, please refer to Documentation/workqueue.txt.
+ *
+ * The __lock_name macro dance is to guarantee that single lock_class_key
+ * doesn't end up with different namesm, which isn't allowed by lockdep.
+ *
+ * RETURNS:
+ * Pointer to the allocated workqueue on success, %NULL on failure.
+ */
 #ifdef CONFIG_LOCKDEP
-#define alloc_workqueue(name, flags, max_active)		\
+#define alloc_workqueue(fmt, flags, max_active, args...)	\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
 								\
-	if (__builtin_constant_p(name))				\
-		__lock_name = (name);				\
+	if (__builtin_constant_p(fmt))				\
+		__lock_name = (fmt);				\
 	else							\
-		__lock_name = #name;				\
+		__lock_name = #fmt;				\
 								\
-	__alloc_workqueue_key((name), (flags), (max_active),	\
-			      &__key, __lock_name);		\
+	__alloc_workqueue_key((fmt), (flags), (max_active),	\
+			      &__key, __lock_name, ##args);	\
 })
 #else
-#define alloc_workqueue(name, flags, max_active)		\
-	__alloc_workqueue_key((name), (flags), (max_active), NULL, NULL)
+#define alloc_workqueue(fmt, flags, max_active, args...)	\
+	__alloc_workqueue_key((fmt), (flags), (max_active),	\
+			      NULL, NULL, ##args)
 #endif
 
 /**
  * alloc_ordered_workqueue - allocate an ordered workqueue
- * @name: name of the workqueue
+ * @fmt: printf format for the name of the workqueue
  * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
+ * @args: args for @fmt
  *
  * Allocate an ordered workqueue.  An ordered workqueue executes at
  * most one work item at any given time in the queued order.  They are
@@ -331,11 +349,8 @@ __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
  * RETURNS:
  * Pointer to the allocated workqueue on success, %NULL on failure.
  */
-static inline struct workqueue_struct *
-alloc_ordered_workqueue(const char *name, unsigned int flags)
-{
-	return alloc_workqueue(name, WQ_UNBOUND | flags, 1);
-}
+#define alloc_ordered_workqueue(fmt, flags, args...)		\
+	alloc_workqueue(fmt, WQ_UNBOUND | (flags), 1, ##args)
 
 #define create_workqueue(name)					\
 	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 2be2887c6958..0d556deb497b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -10,7 +10,7 @@
 #include <linux/bitops.h>
 #include <linux/lockdep.h>
 #include <linux/threads.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 
 struct workqueue_struct;
 

commit 9c5a2ba70251ecaab18c7a83e38b3c620223476c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 5 18:01:44 2011 +0200

    workqueue: separate out drain_workqueue() from destroy_workqueue()
    
    There are users which want to drain workqueues without destroying it.
    Separate out drain functionality from destroy_workqueue() into
    drain_workqueue() and make it accessible to workqueue users.
    
    To guarantee forward-progress, only chain queueing is allowed while
    drain is in progress.  If a new work item which isn't chained from the
    running or pending work items is queued while draining is in progress,
    WARN_ON_ONCE() is triggered.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: James Bottomley <James.Bottomley@hansenpartnership.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 57b31b3d83bd..2be2887c6958 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -255,7 +255,7 @@ enum {
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
-	WQ_DYING		= 1 << 6, /* internal: workqueue is dying */
+	WQ_DRAINING		= 1 << 6, /* internal: workqueue is draining */
 	WQ_RESCUER		= 1 << 7, /* internal: workqueue has rescuer */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
@@ -355,6 +355,7 @@ extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 
 extern void flush_workqueue(struct workqueue_struct *wq);
+extern void drain_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
 extern int schedule_work(struct work_struct *work);

commit 2543a87108d2af7d48a43b3d6685c2b1ea279e36
Author: Amerigo Wang <amwang@redhat.com>
Date:   Wed Apr 6 10:43:11 2011 +0200

    workqueue: remove cancel_rearming_delayed_work[queue]()
    
    cancel_rearming_delayed_work() and cancel_rearming_delayed_workqueue()
    can be removed now.
    
    Signed-off-by: WANG Cong <amwang@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f584aba78ca9..57b31b3d83bd 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -412,21 +412,6 @@ static inline bool __cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
-/* Obsolete. use cancel_delayed_work_sync() */
-static inline __deprecated
-void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
-					struct delayed_work *work)
-{
-	cancel_delayed_work_sync(work);
-}
-
-/* Obsolete. use cancel_delayed_work_sync() */
-static inline __deprecated
-void cancel_rearming_delayed_work(struct delayed_work *work)
-{
-	cancel_delayed_work_sync(work);
-}
-
 #ifndef CONFIG_SMP
 static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
 {

commit 24d51add7438f9696a7205927bf9de3c5c787a58
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 21 09:52:50 2011 +0100

    workqueue: fix build failure introduced by s/freezeable/freezable/
    
    wq:fixes-2.6.38 does s/WQ_FREEZEABLE/WQ_FREEZABLE and wq:for-2.6.39
    adds new usage of the flag.  The combination of the two creates a
    build failure after merge.  Fix it by renaming all freezeables to
    freezables.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d110cc4f9fed..f584aba78ca9 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -287,14 +287,14 @@ enum {
  * executed immediately as long as max_active limit is not reached and
  * resources are available.
  *
- * system_freezeable_wq is equivalent to system_wq except that it's
- * freezeable.
+ * system_freezable_wq is equivalent to system_wq except that it's
+ * freezable.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_nrt_wq;
 extern struct workqueue_struct *system_unbound_wq;
-extern struct workqueue_struct *system_freezeable_wq;
+extern struct workqueue_struct *system_freezable_wq;
 
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,

commit 43d133c18b44e7d82d82ef0dcc2bddd55d5dfe81
Merge: 4149efb22da6 6f576d57f1fa
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 21 09:43:56 2011 +0100

    Merge branch 'master' into for-2.6.39

commit 58a69cb47ec6991bf006a3e5d202e8571b0327a4
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 16 09:25:31 2011 +0100

    workqueue, freezer: unify spelling of 'freeze' + 'able' to 'freezable'
    
    There are two spellings in use for 'freeze' + 'able' - 'freezable' and
    'freezeable'.  The former is the more prominent one.  The latter is
    mostly used by workqueue and in a few other odd places.  Unify the
    spelling to 'freezable'.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Dmitry Torokhov <dtor@mail.ru>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Alex Dubov <oakad@yahoo.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Steven Whitehouse <swhiteho@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1ac11586a2f5..f7998a3bf020 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -250,7 +250,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 enum {
 	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
-	WQ_FREEZEABLE		= 1 << 2, /* freeze during suspend */
+	WQ_FREEZABLE		= 1 << 2, /* freeze during suspend */
 	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
@@ -318,7 +318,7 @@ __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
 /**
  * alloc_ordered_workqueue - allocate an ordered workqueue
  * @name: name of the workqueue
- * @flags: WQ_* flags (only WQ_FREEZEABLE and WQ_MEM_RECLAIM are meaningful)
+ * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
  *
  * Allocate an ordered workqueue.  An ordered workqueue executes at
  * most one work item at any given time in the queued order.  They are
@@ -335,8 +335,8 @@ alloc_ordered_workqueue(const char *name, unsigned int flags)
 
 #define create_workqueue(name)					\
 	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)
-#define create_freezeable_workqueue(name)			\
-	alloc_workqueue((name), WQ_FREEZEABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
+#define create_freezable_workqueue(name)			\
+	alloc_workqueue((name), WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
 #define create_singlethread_workqueue(name)			\
 	alloc_workqueue((name), WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
 

commit 4149efb22da66e326fc48baf80d628834509f7f0
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 8 10:39:03 2011 +0100

    workqueue: add system_freezeable_wq
    
    Add system wide freezeable workqueue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 1ac11586a2f5..de6a755befac 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -286,11 +286,15 @@ enum {
  * any specific CPU, not concurrency managed, and all queued works are
  * executed immediately as long as max_active limit is not reached and
  * resources are available.
+ *
+ * system_freezeable_wq is equivalent to system_wq except that it's
+ * freezeable.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_nrt_wq;
 extern struct workqueue_struct *system_unbound_wq;
+extern struct workqueue_struct *system_freezeable_wq;
 
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,

commit 23d69b09b78c4876e134f104a3814c30747c53f1
Merge: e744070fd4ff 569ff2de2e1c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 16:58:04 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (33 commits)
      usb: don't use flush_scheduled_work()
      speedtch: don't abuse struct delayed_work
      media/video: don't use flush_scheduled_work()
      media/video: explicitly flush request_module work
      ioc4: use static work_struct for ioc4_load_modules()
      init: don't call flush_scheduled_work() from do_initcalls()
      s390: don't use flush_scheduled_work()
      rtc: don't use flush_scheduled_work()
      mmc: update workqueue usages
      mfd: update workqueue usages
      dvb: don't use flush_scheduled_work()
      leds-wm8350: don't use flush_scheduled_work()
      mISDN: don't use flush_scheduled_work()
      macintosh/ams: don't use flush_scheduled_work()
      vmwgfx: don't use flush_scheduled_work()
      tpm: don't use flush_scheduled_work()
      sonypi: don't use flush_scheduled_work()
      hvsi: don't use flush_scheduled_work()
      xen: don't use flush_scheduled_work()
      gdrom: don't use flush_scheduled_work()
      ...
    
    Fixed up trivial conflict in drivers/media/video/bt8xx/bttv-input.c
    as per Tejun.

commit dda5f0a372873bca5f0b1d1866d7784dffd8b675
Merge: 65b2074f84be 88606e80da0e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 6 10:42:43 2011 -0800

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      MAINTAINERS: Update timer related entries
      timers: Use this_cpu_read
      timerqueue: Make timerqueue_getnext() static inline
      hrtimer: fix timerqueue conversion flub
      hrtimers: Convert hrtimers to use timerlist infrastructure
      timers: Fixup allmodconfig build issue
      timers: Rename timerlist infrastructure to timerqueue
      timers: Introduce timerlist infrastructure.
      hrtimer: Remove stale comment on curr_timer
      timer: Warn when del_timer_sync() is called in hardirq context
      timer: Del_timer_sync() can be used in softirq context
      timer: Make try_to_del_timer_sync() the same on SMP and UP
      posix-timers: Annotate lock_timer()
      timer: Permit statically-declared work with deferrable timers
      time: Use ARRAY_SIZE macro in timecompare.c
      timer: Initialize the field slack of timer_list
      timer_list: Remove alignment padding on 64 bit when CONFIG_TIMER_STATS
      time: Compensate for rounding on odd-frequency clocksources
    
    Fix up trivial conflict in MAINTAINERS

commit ed41390fa57a21d06e6e3a3c4bc238bab8957fbb
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 14 16:23:10 2010 +0100

    workqueue: deprecate cancel_rearming_delayed_work[queue]()
    
    There's no in-kernel user left for these two obsolete functions.  Mark
    them deprecated and schedule for removal during 2.6.39 cycle.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0c0771f06bfa..6b5193d70268 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -401,7 +401,7 @@ static inline bool __cancel_delayed_work(struct delayed_work *work)
 }
 
 /* Obsolete. use cancel_delayed_work_sync() */
-static inline
+static inline __deprecated
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
 					struct delayed_work *work)
 {
@@ -409,7 +409,7 @@ void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
 }
 
 /* Obsolete. use cancel_delayed_work_sync() */
-static inline
+static inline __deprecated
 void cancel_rearming_delayed_work(struct delayed_work *work)
 {
 	cancel_delayed_work_sync(work);

commit ca1cab37d91cbe8a8333732540d43cabb54cfa85
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Oct 26 14:22:34 2010 -0700

    workqueues: s/ON_STACK/ONSTACK/
    
    Silly though it is, completions and wait_queue_heads use foo_ONSTACK
    (COMPLETION_INITIALIZER_ONSTACK, DECLARE_COMPLETION_ONSTACK,
    __WAIT_QUEUE_HEAD_INIT_ONSTACK and DECLARE_WAIT_QUEUE_HEAD_ONSTACK) so I
    guess workqueues should do the same thing.
    
    s/INIT_WORK_ON_STACK/INIT_WORK_ONSTACK/
    s/INIT_DELAYED_WORK_ON_STACK/INIT_DELAYED_WORK_ONSTACK/
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 070bb7a88936..0c0771f06bfa 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -190,7 +190,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		__INIT_WORK((_work), (_func), 0);		\
 	} while (0)
 
-#define INIT_WORK_ON_STACK(_work, _func)			\
+#define INIT_WORK_ONSTACK(_work, _func)				\
 	do {							\
 		__INIT_WORK((_work), (_func), 1);		\
 	} while (0)
@@ -201,9 +201,9 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
-#define INIT_DELAYED_WORK_ON_STACK(_work, _func)		\
+#define INIT_DELAYED_WORK_ONSTACK(_work, _func)			\
 	do {							\
-		INIT_WORK_ON_STACK(&(_work)->work, (_func));	\
+		INIT_WORK_ONSTACK(&(_work)->work, (_func));	\
 		init_timer_on_stack(&(_work)->timer);		\
 	} while (0)
 

commit dd6414b50fa2b1cd247a8aa8f8bd42414b7453e1
Author: Phil Carmody <ext-phil.2.carmody@nokia.com>
Date:   Wed Oct 20 15:57:33 2010 -0700

    timer: Permit statically-declared work with deferrable timers
    
    Currently, you have to just define a delayed_work uninitialised, and then
    initialise it before first use.  That's a tad clumsy.  At risk of playing
    mind-games with the compiler, fooling it into doing pointer arithmetic
    with compile-time-constants, this lets clients properly initialise delayed
    work with deferrable timers statically.
    
    This patch was inspired by the issues which lead Artem Bityutskiy to
    commit 8eab945c5616fc984 ("sunrpc: make the cache cleaner workqueue
    deferrable").
    
    Signed-off-by: Phil Carmody <ext-phil.2.carmody@nokia.com>
    Acked-by: Artem Bityutskiy <Artem.Bityutskiy@nokia.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f11100f96482..88238c15ec3e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -127,12 +127,20 @@ struct execute_work {
 	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
 	}
 
+#define __DEFERRED_WORK_INITIALIZER(n, f) {			\
+	.work = __WORK_INITIALIZER((n).work, (f)),		\
+	.timer = TIMER_DEFERRED_INITIALIZER(NULL, 0, 0),	\
+	}
+
 #define DECLARE_WORK(n, f)					\
 	struct work_struct n = __WORK_INITIALIZER(n, f)
 
 #define DECLARE_DELAYED_WORK(n, f)				\
 	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
 
+#define DECLARE_DEFERRED_WORK(n, f)				\
+	struct delayed_work n = __DEFERRED_WORK_INITIALIZER(n, f)
+
 /*
  * initialize a work item's function pointer
  */

commit daaae6b010ac0f60c9c35e481589966f9f1fcc22
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Oct 19 11:28:15 2010 +0200

    workqueue: remove in_workqueue_context()
    
    Commit a25909a4 (lockdep: Add an in_workqueue_context() lockdep-based
    test function) added in_workqueue_context() but there hasn't been any
    in-kernel user and the lockdep annotation in workqueue is scheduled to
    change.  Remove the unused function.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 03bbe903e5ce..070bb7a88936 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -430,8 +430,4 @@ extern bool freeze_workqueues_busy(void);
 extern void thaw_workqueues(void);
 #endif /* CONFIG_FREEZER */
 
-#ifdef CONFIG_LOCKDEP
-int in_workqueue_context(struct workqueue_struct *wq);
-#endif
-
 #endif

commit 6370a6ad3b53df90b4700977f7718118a2cd524a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Oct 11 15:12:27 2010 +0200

    workqueue: add and use WQ_MEM_RECLAIM flag
    
    Add WQ_MEM_RECLAIM flag which currently maps to WQ_RESCUER, mark
    WQ_RESCUER as internal and replace all external WQ_RESCUER usages to
    WQ_MEM_RECLAIM.
    
    This makes the API users express the intent of the workqueue instead
    of indicating the internal mechanism used to guarantee forward
    progress.  This is also to make it cleaner to add more semantics to
    WQ_MEM_RECLAIM.  For example, if deemed necessary, memory reclaim
    workqueues can be made highpri.
    
    This patch doesn't introduce any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jeff Garzik <jgarzik@pobox.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index e33ff4a91703..03bbe903e5ce 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -243,11 +243,12 @@ enum {
 	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
 	WQ_FREEZEABLE		= 1 << 2, /* freeze during suspend */
-	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
+	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
 	WQ_DYING		= 1 << 6, /* internal: workqueue is dying */
+	WQ_RESCUER		= 1 << 7, /* internal: workqueue has rescuer */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
@@ -309,7 +310,7 @@ __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
 /**
  * alloc_ordered_workqueue - allocate an ordered workqueue
  * @name: name of the workqueue
- * @flags: WQ_* flags (only WQ_FREEZEABLE and WQ_RESCUER are meaningful)
+ * @flags: WQ_* flags (only WQ_FREEZEABLE and WQ_MEM_RECLAIM are meaningful)
  *
  * Allocate an ordered workqueue.  An ordered workqueue executes at
  * most one work item at any given time in the queued order.  They are
@@ -325,11 +326,11 @@ alloc_ordered_workqueue(const char *name, unsigned int flags)
 }
 
 #define create_workqueue(name)					\
-	alloc_workqueue((name), WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_MEM_RECLAIM, 1)
 #define create_freezeable_workqueue(name)			\
-	alloc_workqueue((name), WQ_FREEZEABLE | WQ_UNBOUND | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_FREEZEABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
 #define create_singlethread_workqueue(name)			\
-	alloc_workqueue((name), WQ_UNBOUND | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_UNBOUND | WQ_MEM_RECLAIM, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 09383498c5d35262e643bfdbae84826177a3c624
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Sep 16 10:48:29 2010 +0200

    workqueue: implement flush[_delayed]_work_sync()
    
    Implement flush[_delayed]_work_sync().  These are flush functions
    which also make sure no CPU is still executing the target work from
    earlier queueing instances.  These are similar to
    cancel[_delayed]_work_sync() except that the target work item is
    flushed instead of cancelled.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index bb9b683ea6fa..e33ff4a91703 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -355,9 +355,11 @@ extern int keventd_up(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern bool flush_work(struct work_struct *work);
+extern bool flush_work_sync(struct work_struct *work);
 extern bool cancel_work_sync(struct work_struct *work);
 
 extern bool flush_delayed_work(struct delayed_work *dwork);
+extern bool flush_delayed_work_sync(struct delayed_work *work);
 extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,

commit 401a8d048eadfbe1b1c1bf53d3b614fcc894c61a
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Sep 16 10:36:00 2010 +0200

    workqueue: cleanup flush/cancel functions
    
    Make the following cleanup changes.
    
    * Relocate flush/cancel function prototypes and definitions.
    
    * Relocate wait_on_cpu_work() and wait_on_work() before
      try_to_grab_pending().  These will be used to implement
      flush_work_sync().
    
    * Make all flush/cancel functions return bool instead of int.
    
    * Update wait_on_cpu_work() and wait_on_work() to return %true if they
      actually waited.
    
    * Add / update comments.
    
    This patch doesn't cause any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 07c48925a8fc..bb9b683ea6fa 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -343,7 +343,6 @@ extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
-extern void flush_delayed_work(struct delayed_work *work);
 
 extern int schedule_work(struct work_struct *work);
 extern int schedule_work_on(int cpu, struct work_struct *work);
@@ -355,8 +354,11 @@ extern int keventd_up(void);
 
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
-extern int flush_work(struct work_struct *work);
-extern int cancel_work_sync(struct work_struct *work);
+extern bool flush_work(struct work_struct *work);
+extern bool cancel_work_sync(struct work_struct *work);
+
+extern bool flush_delayed_work(struct delayed_work *dwork);
+extern bool cancel_delayed_work_sync(struct delayed_work *dwork);
 
 extern void workqueue_set_max_active(struct workqueue_struct *wq,
 				     int max_active);
@@ -370,9 +372,9 @@ extern unsigned int work_busy(struct work_struct *work);
  * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
  * cancel_work_sync() to wait on it.
  */
-static inline int cancel_delayed_work(struct delayed_work *work)
+static inline bool cancel_delayed_work(struct delayed_work *work)
 {
-	int ret;
+	bool ret;
 
 	ret = del_timer_sync(&work->timer);
 	if (ret)
@@ -385,9 +387,9 @@ static inline int cancel_delayed_work(struct delayed_work *work)
  * if it returns 0 the timer function may be running and the queueing is in
  * progress.
  */
-static inline int __cancel_delayed_work(struct delayed_work *work)
+static inline bool __cancel_delayed_work(struct delayed_work *work)
 {
-	int ret;
+	bool ret;
 
 	ret = del_timer(&work->timer);
 	if (ret)
@@ -395,8 +397,6 @@ static inline int __cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
-extern int cancel_delayed_work_sync(struct delayed_work *work);
-
 /* Obsolete. use cancel_delayed_work_sync() */
 static inline
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,

commit 81dcaf6516d8bbd75b894862c8ae7bba04380cfe
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Sep 16 10:17:35 2010 +0200

    workqueue: implement alloc_ordered_workqueue()
    
    alloc_ordered_workqueue() creates a workqueue which processes each
    work itemp one by one in the queued order.  This will be used to
    replace create_freezeable_workqueue() and
    create_singlethread_workqueue().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 25e02c941bac..07c48925a8fc 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -306,6 +306,24 @@ __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
 	__alloc_workqueue_key((name), (flags), (max_active), NULL, NULL)
 #endif
 
+/**
+ * alloc_ordered_workqueue - allocate an ordered workqueue
+ * @name: name of the workqueue
+ * @flags: WQ_* flags (only WQ_FREEZEABLE and WQ_RESCUER are meaningful)
+ *
+ * Allocate an ordered workqueue.  An ordered workqueue executes at
+ * most one work item at any given time in the queued order.  They are
+ * implemented as unbound workqueues with @max_active of one.
+ *
+ * RETURNS:
+ * Pointer to the allocated workqueue on success, %NULL on failure.
+ */
+static inline struct workqueue_struct *
+alloc_ordered_workqueue(const char *name, unsigned int flags)
+{
+	return alloc_workqueue(name, WQ_UNBOUND | flags, 1);
+}
+
 #define create_workqueue(name)					\
 	alloc_workqueue((name), WQ_RESCUER, 1)
 #define create_freezeable_workqueue(name)			\

commit c54fce6eff197d9c57c97afbf6c9722ce434fc8f
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 10 16:51:36 2010 +0200

    workqueue: add documentation
    
    Update copyright notice and add Documentation/workqueue.txt.
    
    Randy Dunlap, Dave Chinner: misc fixes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-By: Florian Mickler <florian@mickler.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f11100f96482..25e02c941bac 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -235,6 +235,10 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define work_clear_pending(work) \
 	clear_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))
 
+/*
+ * Workqueue flags and constants.  For details, please refer to
+ * Documentation/workqueue.txt.
+ */
 enum {
 	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
 	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */

commit 8a2e8e5dec7e29c56a46ba176c664ab6a3d04118
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Aug 25 10:33:56 2010 +0200

    workqueue: fix cwq->nr_active underflow
    
    cwq->nr_active is used to keep track of how many work items are active
    for the cpu workqueue, where 'active' is defined as either pending on
    global worklist or executing.  This is used to implement the
    max_active limit and workqueue freezing.  If a work item is queued
    after nr_active has already reached max_active, the work item doesn't
    increment nr_active and is put on the delayed queue and gets activated
    later as previous active work items retire.
    
    try_to_grab_pending() which is used in the cancellation path
    unconditionally decremented nr_active whether the work item being
    cancelled is currently active or delayed, so cancelling a delayed work
    item makes nr_active underflow.  This breaks max_active enforcement
    and triggers BUG_ON() in destroy_workqueue() later on.
    
    This patch fixes this bug by adding a flag WORK_STRUCT_DELAYED, which
    is set while a work item in on the delayed list and making
    try_to_grab_pending() decrement nr_active iff the work item is
    currently active.
    
    The addition of the flag enlarges cwq alignment to 256 bytes which is
    getting a bit too large.  It's scheduled to be reduced back to 128
    bytes by merging WORK_STRUCT_PENDING and WORK_STRUCT_CWQ in the next
    devel cycle.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Johannes Berg <johannes@sipsolutions.net>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index c959666eafca..f11100f96482 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -25,18 +25,20 @@ typedef void (*work_func_t)(struct work_struct *work);
 
 enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
-	WORK_STRUCT_CWQ_BIT	= 1,	/* data points to cwq */
-	WORK_STRUCT_LINKED_BIT	= 2,	/* next work is linked to this one */
+	WORK_STRUCT_DELAYED_BIT	= 1,	/* work item is delayed */
+	WORK_STRUCT_CWQ_BIT	= 2,	/* data points to cwq */
+	WORK_STRUCT_LINKED_BIT	= 3,	/* next work is linked to this one */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
-	WORK_STRUCT_STATIC_BIT	= 3,	/* static initializer (debugobjects) */
-	WORK_STRUCT_COLOR_SHIFT	= 4,	/* color for workqueue flushing */
+	WORK_STRUCT_STATIC_BIT	= 4,	/* static initializer (debugobjects) */
+	WORK_STRUCT_COLOR_SHIFT	= 5,	/* color for workqueue flushing */
 #else
-	WORK_STRUCT_COLOR_SHIFT	= 3,	/* color for workqueue flushing */
+	WORK_STRUCT_COLOR_SHIFT	= 4,	/* color for workqueue flushing */
 #endif
 
 	WORK_STRUCT_COLOR_BITS	= 4,
 
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
+	WORK_STRUCT_DELAYED	= 1 << WORK_STRUCT_DELAYED_BIT,
 	WORK_STRUCT_CWQ		= 1 << WORK_STRUCT_CWQ_BIT,
 	WORK_STRUCT_LINKED	= 1 << WORK_STRUCT_LINKED_BIT,
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
@@ -59,8 +61,8 @@ enum {
 
 	/*
 	 * Reserve 7 bits off of cwq pointer w/ debugobjects turned
-	 * off.  This makes cwqs aligned to 128 bytes which isn't too
-	 * excessive while allowing 15 workqueue flush colors.
+	 * off.  This makes cwqs aligned to 256 bytes and allows 15
+	 * workqueue flush colors.
 	 */
 	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +
 				  WORK_STRUCT_COLOR_BITS,

commit e41e704bc4f49057fc68b643108366e6e6781aa3
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 24 14:22:47 2010 +0200

    workqueue: improve destroy_workqueue() debuggability
    
    Now that the worklist is global, having works pending after wq
    destruction can easily lead to oops and destroy_workqueue() have
    several BUG_ON()s to catch these cases.  Unfortunately, BUG_ON()
    doesn't tell much about how the work became pending after the final
    flush_workqueue().
    
    This patch adds WQ_DYING which is set before the final flush begins.
    If a work is requested to be queued on a dying workqueue,
    WARN_ON_ONCE() is triggered and the request is ignored.  This clearly
    indicates which caller is trying to queue a work on a dying workqueue
    and keeps the system working in most cases.
    
    Locking rule comment is updated such that the 'I' rule includes
    modifying the field from destruction path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4f9d277bcd9a..c959666eafca 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -241,6 +241,8 @@ enum {
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
+	WQ_DYING		= 1 << 6, /* internal: workqueue is dying */
+
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
 	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,

commit 3b7433b8a8a83c87972065b1852b7dcae691e464
Merge: 4a386c3e177c 6ee0578b4daa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 7 12:42:58 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (55 commits)
      workqueue: mark init_workqueues() as early_initcall()
      workqueue: explain for_each_*cwq_cpu() iterators
      fscache: fix build on !CONFIG_SYSCTL
      slow-work: kill it
      gfs2: use workqueue instead of slow-work
      drm: use workqueue instead of slow-work
      cifs: use workqueue instead of slow-work
      fscache: drop references to slow-work
      fscache: convert operation to use workqueue instead of slow-work
      fscache: convert object to use workqueue instead of slow-work
      workqueue: fix how cpu number is stored in work->data
      workqueue: fix mayday_mask handling on UP
      workqueue: fix build problem on !CONFIG_SMP
      workqueue: fix locking in retry path of maybe_create_worker()
      async: use workqueue for worker pool
      workqueue: remove WQ_SINGLE_CPU and use WQ_UNBOUND instead
      workqueue: implement unbound workqueue
      workqueue: prepare for WQ_UNBOUND implementation
      libata: take advantage of cmwq and remove concurrency limitations
      workqueue: fix worker management invocation without pending works
      ...
    
    Fixed up conflicts in fs/cifs/* as per Tejun. Other trivial conflicts in
    include/linux/workqueue.h, kernel/trace/Kconfig and kernel/workqueue.c

commit 6ee0578b4daaea01c96b172c6aacca43fd9807a6
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jul 30 14:57:37 2010 -0700

    workqueue: mark init_workqueues() as early_initcall()
    
    Mark init_workqueues() as early_initcall() and thus it will be initialized
    before smp bringup. init_workqueues() registers for the hotcpu notifier
    and thus it should cope with the processors that are brought online after
    the workqueues are initialized.
    
    x86 smp bringup code uses workqueues and uses a workaround for the
    cold boot process (as the workqueues are initialized post smp_init()).
    Marking init_workqueues() as early_initcall() will pave the way for
    cleaning up this code.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5f76001c4e6d..51dc9a727e5e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -327,7 +327,6 @@ extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
 extern int schedule_on_each_cpu(work_func_t func);
 extern int keventd_up(void);
 
-extern void init_workqueues(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern int flush_work(struct work_struct *work);

commit e120153ddf8620fd0a194d301e9c5a8b28483bb5
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 22 14:14:25 2010 +0200

    workqueue: fix how cpu number is stored in work->data
    
    Once a work starts execution, its data contains the cpu number it was
    on instead of pointing to cwq.  This is added by commit 7a22ad75
    (workqueue: carry cpu number in work data once execution starts) to
    reliably determine the work was last on even if the workqueue itself
    was destroyed inbetween.
    
    Whether data points to a cwq or contains a cpu number was
    distinguished by comparing the value against PAGE_OFFSET.  The
    assumption was that a cpu number should be below PAGE_OFFSET while a
    pointer to cwq should be above it.  However, on architectures which
    use separate address spaces for user and kernel spaces, this doesn't
    hold as PAGE_OFFSET is zero.
    
    Fix it by using an explicit flag, WORK_STRUCT_CWQ, to mark what the
    data field contains.  If the flag is set, it's pointing to a cwq;
    otherwise, it contains a cpu number.
    
    Reported on s390 and microblaze during linux-next testing.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Sachin Sant <sachinp@in.ibm.com>
    Reported-by: Michal Simek <michal.simek@petalogix.com>
    Reported-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Tested-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Tested-by: Michal Simek <monstr@monstr.eu>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d74a529ed13e..5f76001c4e6d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -25,17 +25,19 @@ typedef void (*work_func_t)(struct work_struct *work);
 
 enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
-	WORK_STRUCT_LINKED_BIT	= 1,	/* next work is linked to this one */
+	WORK_STRUCT_CWQ_BIT	= 1,	/* data points to cwq */
+	WORK_STRUCT_LINKED_BIT	= 2,	/* next work is linked to this one */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
-	WORK_STRUCT_STATIC_BIT	= 2,	/* static initializer (debugobjects) */
-	WORK_STRUCT_COLOR_SHIFT	= 3,	/* color for workqueue flushing */
+	WORK_STRUCT_STATIC_BIT	= 3,	/* static initializer (debugobjects) */
+	WORK_STRUCT_COLOR_SHIFT	= 4,	/* color for workqueue flushing */
 #else
-	WORK_STRUCT_COLOR_SHIFT	= 2,	/* color for workqueue flushing */
+	WORK_STRUCT_COLOR_SHIFT	= 3,	/* color for workqueue flushing */
 #endif
 
 	WORK_STRUCT_COLOR_BITS	= 4,
 
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
+	WORK_STRUCT_CWQ		= 1 << WORK_STRUCT_CWQ_BIT,
 	WORK_STRUCT_LINKED	= 1 << WORK_STRUCT_LINKED_BIT,
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
@@ -56,8 +58,8 @@ enum {
 	WORK_CPU_LAST		= WORK_CPU_NONE,
 
 	/*
-	 * Reserve 6 bits off of cwq pointer w/ debugobjects turned
-	 * off.  This makes cwqs aligned to 64 bytes which isn't too
+	 * Reserve 7 bits off of cwq pointer w/ debugobjects turned
+	 * off.  This makes cwqs aligned to 128 bytes which isn't too
 	 * excessive while allowing 15 workqueue flush colors.
 	 */
 	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +

commit c7fc77f78f16d138ca997ce096a62f46e2e9420a
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 2 10:03:51 2010 +0200

    workqueue: remove WQ_SINGLE_CPU and use WQ_UNBOUND instead
    
    WQ_SINGLE_CPU combined with @max_active of 1 is used to achieve full
    ordering among works queued to a workqueue.  The same can be achieved
    using WQ_UNBOUND as unbound workqueues always use the gcwq for
    WORK_CPU_UNBOUND.  As @max_active is always one and benefits from cpu
    locality isn't accessible anyway, serving them with unbound workqueues
    should be fine.
    
    Drop WQ_SINGLE_CPU support and use WQ_UNBOUND instead.  Note that most
    single thread workqueue users will be converted to use multithread or
    non-reentrant instead and only the ones which require strict ordering
    will keep using WQ_UNBOUND + @max_active of 1.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 67ce734747f6..d74a529ed13e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -233,12 +233,11 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 
 enum {
 	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
-	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
+	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
 	WQ_FREEZEABLE		= 1 << 2, /* freeze during suspend */
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
-	WQ_UNBOUND		= 1 << 6, /* not bound to any cpu */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
@@ -300,9 +299,9 @@ __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
 #define create_workqueue(name)					\
 	alloc_workqueue((name), WQ_RESCUER, 1)
 #define create_freezeable_workqueue(name)			\
-	alloc_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_CPU | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_FREEZEABLE | WQ_UNBOUND | WQ_RESCUER, 1)
 #define create_singlethread_workqueue(name)			\
-	alloc_workqueue((name), WQ_SINGLE_CPU | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_UNBOUND | WQ_RESCUER, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit f34217977d717385a3e9fd7018ac39fade3964c0
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 2 10:03:51 2010 +0200

    workqueue: implement unbound workqueue
    
    This patch implements unbound workqueue which can be specified with
    WQ_UNBOUND flag on creation.  An unbound workqueue has the following
    properties.
    
    * It uses a dedicated gcwq with a pseudo CPU number WORK_CPU_UNBOUND.
      This gcwq is always online and disassociated.
    
    * Workers are not bound to any CPU and not concurrency managed.  Works
      are dispatched to workers as soon as possible and the only applied
      limitation is @max_active.  IOW, all unbound workqeueues are
      implicitly high priority.
    
    Unbound workqueues can be used as simple execution context provider.
    Contexts unbound to any cpu are served as soon as possible.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 139069a6286c..67ce734747f6 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -51,7 +51,8 @@ enum {
 	WORK_NO_COLOR		= WORK_NR_COLORS,
 
 	/* special cpu IDs */
-	WORK_CPU_NONE		= NR_CPUS,
+	WORK_CPU_UNBOUND	= NR_CPUS,
+	WORK_CPU_NONE		= NR_CPUS + 1,
 	WORK_CPU_LAST		= WORK_CPU_NONE,
 
 	/*
@@ -237,11 +238,17 @@ enum {
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
+	WQ_UNBOUND		= 1 << 6, /* not bound to any cpu */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
+	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
 	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,
 };
 
+/* unbound wq's aren't per-cpu, scale max_active according to #cpus */
+#define WQ_UNBOUND_MAX_ACTIVE	\
+	max_t(int, WQ_MAX_ACTIVE, num_possible_cpus() * WQ_MAX_UNBOUND_PER_CPU)
+
 /*
  * System-wide workqueues which are always present.
  *
@@ -256,10 +263,16 @@ enum {
  * system_nrt_wq is non-reentrant and guarantees that any given work
  * item is never executed in parallel by multiple CPUs.  Queue
  * flushing might take relatively long.
+ *
+ * system_unbound_wq is unbound workqueue.  Workers are not bound to
+ * any specific CPU, not concurrency managed, and all queued works are
+ * executed immediately as long as max_active limit is not reached and
+ * resources are available.
  */
 extern struct workqueue_struct *system_wq;
 extern struct workqueue_struct *system_long_wq;
 extern struct workqueue_struct *system_nrt_wq;
+extern struct workqueue_struct *system_unbound_wq;
 
 extern struct workqueue_struct *
 __alloc_workqueue_key(const char *name, unsigned int flags, int max_active,

commit bdbc5dd7de5d07d6c9d3536e598956165a031d4c
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 2 10:03:51 2010 +0200

    workqueue: prepare for WQ_UNBOUND implementation
    
    In preparation of WQ_UNBOUND addition, make the following changes.
    
    * Add WORK_CPU_* constants for pseudo cpu id numbers used (currently
      only WORK_CPU_NONE) and use them instead of NR_CPUS.  This is to
      allow another pseudo cpu id for unbound cpu.
    
    * Reorder WQ_* flags.
    
    * Make workqueue_struct->cpu_wq a union which contains a percpu
      pointer, regular pointer and an unsigned long value and use
      kzalloc/kfree() in UP allocation path.  This will be used to
      implement unbound workqueues which will use only one cwq on SMPs.
    
    * Move alloc_cwqs() allocation after initialization of wq fields, so
      that alloc_cwqs() has access to wq->flags.
    
    * Trivial relocation of wq local variables in freeze functions.
    
    These changes don't cause any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 3f36d37ac5ba..139069a6286c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -50,6 +50,10 @@ enum {
 	WORK_NR_COLORS		= (1 << WORK_STRUCT_COLOR_BITS) - 1,
 	WORK_NO_COLOR		= WORK_NR_COLORS,
 
+	/* special cpu IDs */
+	WORK_CPU_NONE		= NR_CPUS,
+	WORK_CPU_LAST		= WORK_CPU_NONE,
+
 	/*
 	 * Reserve 6 bits off of cwq pointer w/ debugobjects turned
 	 * off.  This makes cwqs aligned to 64 bytes which isn't too
@@ -60,7 +64,7 @@ enum {
 
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
-	WORK_STRUCT_NO_CPU	= NR_CPUS << WORK_STRUCT_FLAG_BITS,
+	WORK_STRUCT_NO_CPU	= WORK_CPU_NONE << WORK_STRUCT_FLAG_BITS,
 
 	/* bit mask for work_busy() return values */
 	WORK_BUSY_PENDING	= 1 << 0,
@@ -227,9 +231,9 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 	clear_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))
 
 enum {
-	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */
+	WQ_NON_REENTRANT	= 1 << 0, /* guarantee non-reentrance */
 	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
-	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
+	WQ_FREEZEABLE		= 1 << 2, /* freeze during suspend */
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
 	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */

commit fb0e7beb5c1b6fb4da786ba709d7138373d5fb22
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:15 2010 +0200

    workqueue: implement cpu intensive workqueue
    
    This patch implements cpu intensive workqueue which can be specified
    with WQ_CPU_INTENSIVE flag on creation.  Works queued to a cpu
    intensive workqueue don't participate in concurrency management.  IOW,
    it doesn't contribute to gcwq->nr_running and thus doesn't delay
    excution of other works.
    
    Note that although cpu intensive works won't delay other works, they
    can be delayed by other works.  Combine with WQ_HIGHPRI to avoid being
    delayed by other works too.
    
    As the name suggests this is useful when using workqueue for cpu
    intensive works.  Workers executing cpu intensive works are not
    considered for workqueue concurrency management and left for the
    scheduler to manage.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 006dcf7e808a..3f36d37ac5ba 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -232,6 +232,7 @@ enum {
 	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
 	WQ_HIGHPRI		= 1 << 4, /* high priority */
+	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu instensive workqueue */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,

commit 649027d73a6309ac34dc2886362e662bd73456dc
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: implement high priority workqueue
    
    This patch implements high priority workqueue which can be specified
    with WQ_HIGHPRI flag on creation.  A high priority workqueue has the
    following properties.
    
    * A work queued to it is queued at the head of the worklist of the
      respective gcwq after other highpri works, while normal works are
      always appended at the end.
    
    * As long as there are highpri works on gcwq->worklist,
      [__]need_more_worker() remains %true and process_one_work() wakes up
      another worker before it start executing a work.
    
    The above two properties guarantee that works queued to high priority
    workqueues are dispatched to workers and start execution as soon as
    possible regardless of the state of other works.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0a7f79729380..006dcf7e808a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -231,6 +231,7 @@ enum {
 	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
 	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
+	WQ_HIGHPRI		= 1 << 4, /* high priority */
 
 	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
 	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,

commit dcd989cb73ab0f7b722d64ab6516f101d9f43f88
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: implement several utility APIs
    
    Implement the following utility APIs.
    
     workqueue_set_max_active()     : adjust max_active of a wq
     workqueue_congested()          : test whether a wq is contested
     work_cpu()                     : determine the last / current cpu of a work
     work_busy()                    : query whether a work is busy
    
    * Anton Blanchard fixed missing ret initialization in work_busy().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Anton Blanchard <anton@samba.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 48b7422f25ae..0a7f79729380 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -61,6 +61,10 @@ enum {
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
 	WORK_STRUCT_NO_CPU	= NR_CPUS << WORK_STRUCT_FLAG_BITS,
+
+	/* bit mask for work_busy() return values */
+	WORK_BUSY_PENDING	= 1 << 0,
+	WORK_BUSY_RUNNING	= 1 << 1,
 };
 
 struct work_struct {
@@ -307,9 +311,14 @@ extern void init_workqueues(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 extern int flush_work(struct work_struct *work);
-
 extern int cancel_work_sync(struct work_struct *work);
 
+extern void workqueue_set_max_active(struct workqueue_struct *wq,
+				     int max_active);
+extern bool workqueue_congested(unsigned int cpu, struct workqueue_struct *wq);
+extern unsigned int work_cpu(struct work_struct *work);
+extern unsigned int work_busy(struct work_struct *work);
+
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback
  * function may still be running on return from cancel_delayed_work(), unless

commit d320c03830b17af64e4547075003b1eeb274bc6c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: s/__create_workqueue()/alloc_workqueue()/, and add system workqueues
    
    This patch makes changes to make new workqueue features available to
    its users.
    
    * Now that workqueue is more featureful, there should be a public
      workqueue creation function which takes paramters to control them.
      Rename __create_workqueue() to alloc_workqueue() and make 0
      max_active mean WQ_DFL_ACTIVE.  In the long run, all
      create_workqueue_*() will be converted over to alloc_workqueue().
    
    * To further unify access interface, rename keventd_wq to system_wq
      and export it.
    
    * Add system_long_wq and system_nrt_wq.  The former is to host long
      running works separately (so that flush_scheduled_work() dosen't
      take so long) and the latter guarantees any queued work item is
      never executed in parallel by multiple CPUs.  These will be used by
      future patches to update workqueue users.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 33e24e734d50..48b7422f25ae 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -232,12 +232,31 @@ enum {
 	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,
 };
 
+/*
+ * System-wide workqueues which are always present.
+ *
+ * system_wq is the one used by schedule[_delayed]_work[_on]().
+ * Multi-CPU multi-threaded.  There are users which expect relatively
+ * short queue flush time.  Don't queue works which can run for too
+ * long.
+ *
+ * system_long_wq is similar to system_wq but may host long running
+ * works.  Queue flushing might take relatively long.
+ *
+ * system_nrt_wq is non-reentrant and guarantees that any given work
+ * item is never executed in parallel by multiple CPUs.  Queue
+ * flushing might take relatively long.
+ */
+extern struct workqueue_struct *system_wq;
+extern struct workqueue_struct *system_long_wq;
+extern struct workqueue_struct *system_nrt_wq;
+
 extern struct workqueue_struct *
-__create_workqueue_key(const char *name, unsigned int flags, int max_active,
-		       struct lock_class_key *key, const char *lock_name);
+__alloc_workqueue_key(const char *name, unsigned int flags, int max_active,
+		      struct lock_class_key *key, const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
-#define __create_workqueue(name, flags, max_active)		\
+#define alloc_workqueue(name, flags, max_active)		\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
@@ -247,21 +266,20 @@ __create_workqueue_key(const char *name, unsigned int flags, int max_active,
 	else							\
 		__lock_name = #name;				\
 								\
-	__create_workqueue_key((name), (flags), (max_active),	\
-				&__key, __lock_name);		\
+	__alloc_workqueue_key((name), (flags), (max_active),	\
+			      &__key, __lock_name);		\
 })
 #else
-#define __create_workqueue(name, flags, max_active)		\
-	__create_workqueue_key((name), (flags), (max_active), NULL, NULL)
+#define alloc_workqueue(name, flags, max_active)		\
+	__alloc_workqueue_key((name), (flags), (max_active), NULL, NULL)
 #endif
 
 #define create_workqueue(name)					\
-	__create_workqueue((name), WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_RESCUER, 1)
 #define create_freezeable_workqueue(name)			\
-	__create_workqueue((name),				\
-			   WQ_FREEZEABLE | WQ_SINGLE_CPU | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_CPU | WQ_RESCUER, 1)
 #define create_singlethread_workqueue(name)			\
-	__create_workqueue((name), WQ_SINGLE_CPU | WQ_RESCUER, 1)
+	alloc_workqueue((name), WQ_SINGLE_CPU | WQ_RESCUER, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit b71ab8c2025caef8db719aa41af0ed735dc543cd
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: increase max_active of keventd and kill current_is_keventd()
    
    Define WQ_MAX_ACTIVE and create keventd with max_active set to half of
    it which means that keventd now can process upto WQ_MAX_ACTIVE / 2 - 1
    works concurrently.  Unless some combination can result in dependency
    loop longer than max_active, deadlock won't happen and thus it's
    unnecessary to check whether current_is_keventd() before trying to
    schedule a work.  Kill current_is_keventd().
    
    (Lockdep annotations are broken.  We need lock_map_acquire_read_norecurse())
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b8f4ec45c40a..33e24e734d50 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -227,6 +227,9 @@ enum {
 	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
 	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
 	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
+
+	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
+	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,
 };
 
 extern struct workqueue_struct *
@@ -280,7 +283,6 @@ extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay)
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
 					unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);
-extern int current_is_keventd(void);
 extern int keventd_up(void);
 
 extern void init_workqueues(void);

commit e22bee782b3b00bd4534ae9b1c5fb2e8e6573c5c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:14 2010 +0200

    workqueue: implement concurrency managed dynamic worker pool
    
    Instead of creating a worker for each cwq and putting it into the
    shared pool, manage per-cpu workers dynamically.
    
    Works aren't supposed to be cpu cycle hogs and maintaining just enough
    concurrency to prevent work processing from stalling due to lack of
    processing context is optimal.  gcwq keeps the number of concurrent
    active workers to minimum but no less.  As long as there's one or more
    running workers on the cpu, no new worker is scheduled so that works
    can be processed in batch as much as possible but when the last
    running worker blocks, gcwq immediately schedules new worker so that
    the cpu doesn't sit idle while there are works to be processed.
    
    gcwq always keeps at least single idle worker around.  When a new
    worker is necessary and the worker is the last idle one, the worker
    assumes the role of "manager" and manages the worker pool -
    ie. creates another worker.  Forward-progress is guaranteed by having
    dedicated rescue workers for workqueues which may be necessary while
    creating a new worker.  When the manager is having problem creating a
    new worker, mayday timer activates and rescue workers are summoned to
    the cpu and execute works which might be necessary to create new
    workers.
    
    Trustee is expanded to serve the role of manager while a CPU is being
    taken down and stays down.  As no new works are supposed to be queued
    on a dead cpu, it just needs to drain all the existing ones.  Trustee
    continues to try to create new workers and summon rescuers as long as
    there are pending works.  If the CPU is brought back up while the
    trustee is still trying to drain the gcwq from the previous offlining,
    the trustee will kill all idles ones and tell workers which are still
    busy to rebind to the cpu, and pass control over to gcwq which assumes
    the manager role as necessary.
    
    Concurrency managed worker pool reduces the number of workers
    drastically.  Only workers which are necessary to keep the processing
    going are created and kept.  Also, it reduces cache footprint by
    avoiding unnecessarily switching contexts between different workers.
    
    Please note that this patch does not increase max_active of any
    workqueue.  All workqueues can still only process one work per cpu.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 07cf5e5f91cb..b8f4ec45c40a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -226,6 +226,7 @@ enum {
 	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */
 	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
 	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
+	WQ_RESCUER		= 1 << 3, /* has an rescue worker */
 };
 
 extern struct workqueue_struct *
@@ -252,11 +253,12 @@ __create_workqueue_key(const char *name, unsigned int flags, int max_active,
 #endif
 
 #define create_workqueue(name)					\
-	__create_workqueue((name), 0, 1)
+	__create_workqueue((name), WQ_RESCUER, 1)
 #define create_freezeable_workqueue(name)			\
-	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_CPU, 1)
+	__create_workqueue((name),				\
+			   WQ_FREEZEABLE | WQ_SINGLE_CPU | WQ_RESCUER, 1)
 #define create_singlethread_workqueue(name)			\
-	__create_workqueue((name), WQ_SINGLE_CPU, 1)
+	__create_workqueue((name), WQ_SINGLE_CPU | WQ_RESCUER, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 18aa9effad4adb2c1efe123af4eb24fec9f59b30
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:13 2010 +0200

    workqueue: implement WQ_NON_REENTRANT
    
    With gcwq managing all the workers and work->data pointing to the last
    gcwq it was on, non-reentrance can be easily implemented by checking
    whether the work is still running on the previous gcwq on queueing.
    Implement it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0a7814131e66..07cf5e5f91cb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -225,6 +225,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 enum {
 	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */
 	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
+	WQ_NON_REENTRANT	= 1 << 2, /* guarantee non-reentrance */
 };
 
 extern struct workqueue_struct *

commit 7a22ad757ec75186ad43a5b4670fa7423ee8f480
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:13 2010 +0200

    workqueue: carry cpu number in work data once execution starts
    
    To implement non-reentrant workqueue, the last gcwq a work was
    executed on must be reliably obtainable as long as the work structure
    is valid even if the previous workqueue has been destroyed.
    
    To achieve this, work->data will be overloaded to carry the last cpu
    number once execution starts so that the previous gcwq can be located
    reliably.  This means that cwq can't be obtained from work after
    execution starts but only gcwq.
    
    Implement set_work_{cwq|cpu}(), get_work_[g]cwq() and
    clear_work_data() to set work data to the cpu number when starting
    execution, access the overloaded work data and clear it after
    cancellation.
    
    queue_delayed_work_on() is updated to preserve the last cpu while
    in-flight in timer and other callers which depended on getting cwq
    from work after execution starts are converted to depend on gcwq
    instead.
    
    * Anton Blanchard fixed compile error on powerpc due to missing
      linux/threads.h include.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Anton Blanchard <anton@samba.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 10611f7fc809..0a7814131e66 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -9,6 +9,7 @@
 #include <linux/linkage.h>
 #include <linux/bitops.h>
 #include <linux/lockdep.h>
+#include <linux/threads.h>
 #include <asm/atomic.h>
 
 struct workqueue_struct;
@@ -59,6 +60,7 @@ enum {
 
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
+	WORK_STRUCT_NO_CPU	= NR_CPUS << WORK_STRUCT_FLAG_BITS,
 };
 
 struct work_struct {
@@ -70,8 +72,9 @@ struct work_struct {
 #endif
 };
 
-#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(0)
-#define WORK_DATA_STATIC_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_STATIC)
+#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_NO_CPU)
+#define WORK_DATA_STATIC_INIT()	\
+	ATOMIC_LONG_INIT(WORK_STRUCT_NO_CPU | WORK_STRUCT_STATIC)
 
 struct delayed_work {
 	struct work_struct work;

commit 502ca9d819792e7d79b6e002afe9094c641fe410
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:13 2010 +0200

    workqueue: make single thread workqueue shared worker pool friendly
    
    Reimplement st (single thread) workqueue so that it's friendly to
    shared worker pool.  It was originally implemented by confining st
    workqueues to use cwq of a fixed cpu and always having a worker for
    the cpu.  This implementation isn't very friendly to shared worker
    pool and suboptimal in that it ends up crossing cpu boundaries often.
    
    Reimplement st workqueue using dynamic single cpu binding and
    cwq->limit.  WQ_SINGLE_THREAD is replaced with WQ_SINGLE_CPU.  In a
    single cpu workqueue, at most single cwq is bound to the wq at any
    given time.  Arbitration is done using atomic accesses to
    wq->single_cpu when queueing a work.  Once bound, the binding stays
    till the workqueue is drained.
    
    Note that the binding is never broken while a workqueue is frozen.
    This is because idle cwqs may have works waiting in delayed_works
    queue while frozen.  On thaw, the cwq is restarted if there are any
    delayed works or unbound otherwise.
    
    When combined with max_active limit of 1, single cpu workqueue has
    exactly the same execution properties as the original single thread
    workqueue while allowing sharing of per-cpu workers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ab0b7fb99bc2..10611f7fc809 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -221,7 +221,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 
 enum {
 	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */
-	WQ_SINGLE_THREAD	= 1 << 1, /* no per-cpu worker */
+	WQ_SINGLE_CPU		= 1 << 1, /* only single cpu at a time */
 };
 
 extern struct workqueue_struct *
@@ -250,9 +250,9 @@ __create_workqueue_key(const char *name, unsigned int flags, int max_active,
 #define create_workqueue(name)					\
 	__create_workqueue((name), 0, 1)
 #define create_freezeable_workqueue(name)			\
-	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_THREAD, 1)
+	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_CPU, 1)
 #define create_singlethread_workqueue(name)			\
-	__create_workqueue((name), WQ_SINGLE_THREAD, 1)
+	__create_workqueue((name), WQ_SINGLE_CPU, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit a0a1a5fd4fb15ec61117c759fe9f5c16c53d9e9c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:12 2010 +0200

    workqueue: reimplement workqueue freeze using max_active
    
    Currently, workqueue freezing is implemented by marking the worker
    freezeable and calling try_to_freeze() from dispatch loop.
    Reimplement it using cwq->limit so that the workqueue is frozen
    instead of the worker.
    
    * workqueue_struct->saved_max_active is added which stores the
      specified max_active on initialization.
    
    * On freeze, all cwq->max_active's are quenched to zero.  Freezing is
      complete when nr_active on all cwqs reach zero.
    
    * On thaw, all cwq->max_active's are restored to wq->saved_max_active
      and the worklist is repopulated.
    
    This new implementation allows having single shared pool of workers
    per cpu.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index eb753b7790e5..ab0b7fb99bc2 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -340,4 +340,11 @@ static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
 #else
 long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
 #endif /* CONFIG_SMP */
+
+#ifdef CONFIG_FREEZER
+extern void freeze_workqueues_begin(void);
+extern bool freeze_workqueues_busy(void);
+extern void thaw_workqueues(void);
+#endif /* CONFIG_FREEZER */
+
 #endif

commit 1e19ffc63dbbaea7a7d1c63d99c38d3e5a4c7edf
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:12 2010 +0200

    workqueue: implement per-cwq active work limit
    
    Add cwq->nr_active, cwq->max_active and cwq->delayed_work.  nr_active
    counts the number of active works per cwq.  A work is active if it's
    flushable (colored) and is on cwq's worklist.  If nr_active reaches
    max_active, new works are queued on cwq->delayed_work and activated
    later as works on the cwq complete and decrement nr_active.
    
    cwq->max_active can be specified via the new @max_active parameter to
    __create_workqueue() and is set to 1 for all workqueues for now.  As
    each cwq has only single worker now, this double queueing doesn't
    cause any behavior difference visible to its users.
    
    This will be used to reimplement freeze/thaw and implement shared
    worker pool.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4f4fdba722c3..eb753b7790e5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -225,11 +225,11 @@ enum {
 };
 
 extern struct workqueue_struct *
-__create_workqueue_key(const char *name, unsigned int flags,
+__create_workqueue_key(const char *name, unsigned int flags, int max_active,
 		       struct lock_class_key *key, const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
-#define __create_workqueue(name, flags)				\
+#define __create_workqueue(name, flags, max_active)		\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
@@ -239,20 +239,20 @@ __create_workqueue_key(const char *name, unsigned int flags,
 	else							\
 		__lock_name = #name;				\
 								\
-	__create_workqueue_key((name), (flags), &__key,		\
-			       __lock_name);			\
+	__create_workqueue_key((name), (flags), (max_active),	\
+				&__key, __lock_name);		\
 })
 #else
-#define __create_workqueue(name, flags)				\
-	__create_workqueue_key((name), (flags), NULL, NULL)
+#define __create_workqueue(name, flags, max_active)		\
+	__create_workqueue_key((name), (flags), (max_active), NULL, NULL)
 #endif
 
 #define create_workqueue(name)					\
-	__create_workqueue((name), 0)
+	__create_workqueue((name), 0, 1)
 #define create_freezeable_workqueue(name)			\
-	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_THREAD)
+	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_THREAD, 1)
 #define create_singlethread_workqueue(name)			\
-	__create_workqueue((name), WQ_SINGLE_THREAD)
+	__create_workqueue((name), WQ_SINGLE_THREAD, 1)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit affee4b294a0fc97d67c8a77dc080c4dd262a79e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:12 2010 +0200

    workqueue: reimplement work flushing using linked works
    
    A work is linked to the next one by having WORK_STRUCT_LINKED bit set
    and these links can be chained.  When a linked work is dispatched to a
    worker, all linked works are dispatched to the worker's newly added
    ->scheduled queue and processed back-to-back.
    
    Currently, as there's only single worker per cwq, having linked works
    doesn't make any visible behavior difference.  This change is to
    prepare for multiple shared workers per cpu.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 8762f62103d8..4f4fdba722c3 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -24,8 +24,9 @@ typedef void (*work_func_t)(struct work_struct *work);
 
 enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
+	WORK_STRUCT_LINKED_BIT	= 1,	/* next work is linked to this one */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
-	WORK_STRUCT_STATIC_BIT	= 1,	/* static initializer (debugobjects) */
+	WORK_STRUCT_STATIC_BIT	= 2,	/* static initializer (debugobjects) */
 	WORK_STRUCT_COLOR_SHIFT	= 3,	/* color for workqueue flushing */
 #else
 	WORK_STRUCT_COLOR_SHIFT	= 2,	/* color for workqueue flushing */
@@ -34,6 +35,7 @@ enum {
 	WORK_STRUCT_COLOR_BITS	= 4,
 
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
+	WORK_STRUCT_LINKED	= 1 << WORK_STRUCT_LINKED_BIT,
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
 #else

commit 73f53c4aa732eced5fcb1844d3d452c30905f20f
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:11 2010 +0200

    workqueue: reimplement workqueue flushing using color coded works
    
    Reimplement workqueue flushing using color coded works.  wq has the
    current work color which is painted on the works being issued via
    cwqs.  Flushing a workqueue is achieved by advancing the current work
    colors of cwqs and waiting for all the works which have any of the
    previous colors to drain.
    
    Currently there are 16 possible colors, one is reserved for no color
    and 15 colors are useable allowing 14 concurrent flushes.  When color
    space gets full, flush attempts are batched up and processed together
    when color frees up, so even with many concurrent flushers, the new
    implementation won't build up huge queue of flushers which has to be
    processed one after another.
    
    Only works which are queued via __queue_work() are colored.  Works
    which are directly put on queue using insert_work() use NO_COLOR and
    don't participate in workqueue flushing.  Currently only works used
    for work-specific flush fall in this category.
    
    This new implementation leaves only cleanup_workqueue_thread() as the
    user of flush_cpu_workqueue().  Just make its users use
    flush_workqueue() and kthread_stop() directly and kill
    cleanup_workqueue_thread().  As workqueue flushing doesn't use barrier
    request anymore, the comment describing the complex synchronization
    around it in cleanup_workqueue_thread() is removed together with the
    function.
    
    This new implementation is to allow having and sharing multiple
    workers per cpu.
    
    Please note that one more bit is reserved for a future work flag by
    this patch.  This is to avoid shifting bits and updating comments
    later.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b90958a037dc..8762f62103d8 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -26,11 +26,13 @@ enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC_BIT	= 1,	/* static initializer (debugobjects) */
-	WORK_STRUCT_FLAG_BITS	= 2,
+	WORK_STRUCT_COLOR_SHIFT	= 3,	/* color for workqueue flushing */
 #else
-	WORK_STRUCT_FLAG_BITS	= 1,
+	WORK_STRUCT_COLOR_SHIFT	= 2,	/* color for workqueue flushing */
 #endif
 
+	WORK_STRUCT_COLOR_BITS	= 4,
+
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
@@ -38,6 +40,21 @@ enum {
 	WORK_STRUCT_STATIC	= 0,
 #endif
 
+	/*
+	 * The last color is no color used for works which don't
+	 * participate in workqueue flushing.
+	 */
+	WORK_NR_COLORS		= (1 << WORK_STRUCT_COLOR_BITS) - 1,
+	WORK_NO_COLOR		= WORK_NR_COLORS,
+
+	/*
+	 * Reserve 6 bits off of cwq pointer w/ debugobjects turned
+	 * off.  This makes cwqs aligned to 64 bytes which isn't too
+	 * excessive while allowing 15 workqueue flush colors.
+	 */
+	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +
+				  WORK_STRUCT_COLOR_BITS,
+
 	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
 };

commit 0f900049cbe2767d47c2a62b54f0e822e1d66840
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:11 2010 +0200

    workqueue: update cwq alignement
    
    work->data field is used for two purposes.  It points to cwq it's
    queued on and the lower bits are used for flags.  Currently, two bits
    are reserved which is always safe as 4 byte alignment is guaranteed on
    every architecture.  However, future changes will need more flag bits.
    
    On SMP, the percpu allocator is capable of honoring larger alignment
    (there are other users which depend on it) and larger alignment works
    just fine.  On UP, percpu allocator is a thin wrapper around
    kzalloc/kfree() and don't honor alignment request.
    
    This patch introduces WORK_STRUCT_FLAG_BITS and implements
    alloc/free_cwqs() which guarantees max(1 << WORK_STRUCT_FLAG_BITS,
    __alignof__(unsigned long long) alignment both on SMP and UP.  On SMP,
    simply wrapping percpu allocator is enough.  On UP, extra space is
    allocated so that cwq can be aligned and the original pointer can be
    stored after it which is used in the free path.
    
    * Alignment problem on UP is reported by Michal Simek.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Reported-by: Michal Simek <michal.simek@petalogix.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d60c5701ab45..b90958a037dc 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -26,6 +26,9 @@ enum {
 	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 	WORK_STRUCT_STATIC_BIT	= 1,	/* static initializer (debugobjects) */
+	WORK_STRUCT_FLAG_BITS	= 2,
+#else
+	WORK_STRUCT_FLAG_BITS	= 1,
 #endif
 
 	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
@@ -35,7 +38,7 @@ enum {
 	WORK_STRUCT_STATIC	= 0,
 #endif
 
-	WORK_STRUCT_FLAG_MASK	= 3UL,
+	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
 };
 

commit 22df02bb3fab24af97bff4c69cc6fd8529fc66fe
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:10 2010 +0200

    workqueue: define masks for work flags and conditionalize STATIC flags
    
    Work flags are about to see more traditional mask handling.  Define
    WORK_STRUCT_*_BIT as the bit position constant and redefine
    WORK_STRUCT_* as bit masks.  Also, make WORK_STRUCT_STATIC_* flags
    conditional
    
    While at it, re-define these constants as enums and use
    WORK_STRUCT_STATIC instead of hard-coding 2 in
    WORK_DATA_STATIC_INIT().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d89cfc143b1a..d60c5701ab45 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -22,12 +22,25 @@ typedef void (*work_func_t)(struct work_struct *work);
  */
 #define work_data_bits(work) ((unsigned long *)(&(work)->data))
 
+enum {
+	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
+#ifdef CONFIG_DEBUG_OBJECTS_WORK
+	WORK_STRUCT_STATIC_BIT	= 1,	/* static initializer (debugobjects) */
+#endif
+
+	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
+#ifdef CONFIG_DEBUG_OBJECTS_WORK
+	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
+#else
+	WORK_STRUCT_STATIC	= 0,
+#endif
+
+	WORK_STRUCT_FLAG_MASK	= 3UL,
+	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
+};
+
 struct work_struct {
 	atomic_long_t data;
-#define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
-#define WORK_STRUCT_STATIC  1		/* static initializer (debugobjects) */
-#define WORK_STRUCT_FLAG_MASK (3UL)
-#define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
 	work_func_t func;
 #ifdef CONFIG_LOCKDEP
@@ -36,7 +49,7 @@ struct work_struct {
 };
 
 #define WORK_DATA_INIT()	ATOMIC_LONG_INIT(0)
-#define WORK_DATA_STATIC_INIT()	ATOMIC_LONG_INIT(2)
+#define WORK_DATA_STATIC_INIT()	ATOMIC_LONG_INIT(WORK_STRUCT_STATIC)
 
 struct delayed_work {
 	struct work_struct work;
@@ -98,7 +111,7 @@ extern void __init_work(struct work_struct *work, int onstack);
 extern void destroy_work_on_stack(struct work_struct *work);
 static inline unsigned int work_static(struct work_struct *work)
 {
-	return *work_data_bits(work) & (1 << WORK_STRUCT_STATIC);
+	return *work_data_bits(work) & WORK_STRUCT_STATIC;
 }
 #else
 static inline void __init_work(struct work_struct *work, int onstack) { }
@@ -167,7 +180,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
  * @work: The work item in question
  */
 #define work_pending(work) \
-	test_bit(WORK_STRUCT_PENDING, work_data_bits(work))
+	test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))
 
 /**
  * delayed_work_pending - Find out whether a delayable work item is currently
@@ -182,7 +195,7 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
  * @work: The work item in question
  */
 #define work_clear_pending(work) \
-	clear_bit(WORK_STRUCT_PENDING, work_data_bits(work))
+	clear_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))
 
 enum {
 	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */

commit 97e37d7b9e65a6ac939f796f91081135b7a08acc
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:10 2010 +0200

    workqueue: merge feature parameters into flags
    
    Currently, __create_workqueue_key() takes @singlethread and
    @freezeable paramters and store them separately in workqueue_struct.
    Merge them into a single flags parameter and field and use
    WQ_FREEZEABLE and WQ_SINGLE_THREAD.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index e724dafc9e6d..d89cfc143b1a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -184,13 +184,17 @@ static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #define work_clear_pending(work) \
 	clear_bit(WORK_STRUCT_PENDING, work_data_bits(work))
 
+enum {
+	WQ_FREEZEABLE		= 1 << 0, /* freeze during suspend */
+	WQ_SINGLE_THREAD	= 1 << 1, /* no per-cpu worker */
+};
 
 extern struct workqueue_struct *
-__create_workqueue_key(const char *name, int singlethread, int freezeable,
+__create_workqueue_key(const char *name, unsigned int flags,
 		       struct lock_class_key *key, const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
-#define __create_workqueue(name, singlethread, freezeable)	\
+#define __create_workqueue(name, flags)				\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
@@ -200,19 +204,20 @@ __create_workqueue_key(const char *name, int singlethread, int freezeable,
 	else							\
 		__lock_name = #name;				\
 								\
-	__create_workqueue_key((name), (singlethread),		\
-			       (freezeable), &__key,		\
+	__create_workqueue_key((name), (flags), &__key,		\
 			       __lock_name);			\
 })
 #else
-#define __create_workqueue(name, singlethread, freezeable)	\
-	__create_workqueue_key((name), (singlethread), (freezeable), \
-			       NULL, NULL)
+#define __create_workqueue(name, flags)				\
+	__create_workqueue_key((name), (flags), NULL, NULL)
 #endif
 
-#define create_workqueue(name) __create_workqueue((name), 0, 0)
-#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1)
-#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)
+#define create_workqueue(name)					\
+	__create_workqueue((name), 0)
+#define create_freezeable_workqueue(name)			\
+	__create_workqueue((name), WQ_FREEZEABLE | WQ_SINGLE_THREAD)
+#define create_singlethread_workqueue(name)			\
+	__create_workqueue((name), WQ_SINGLE_THREAD)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 4690c4ab56c71919893ca25252f2dd65b58188c7
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:10 2010 +0200

    workqueue: misc/cosmetic updates
    
    Make the following updates in preparation of concurrency managed
    workqueue.  None of these changes causes any visible behavior
    difference.
    
    * Add comments and adjust indentations to data structures and several
      functions.
    
    * Rename wq_per_cpu() to get_cwq() and swap the position of two
      parameters for consistency.  Convert a direct per_cpu_ptr() access
      to wq->cpu_wq to get_cwq().
    
    * Add work_static() and Update set_wq_data() such that it sets the
      flags part to WORK_STRUCT_PENDING | WORK_STRUCT_STATIC if static |
      @extra_flags.
    
    * Move santiy check on work->entry emptiness from queue_work_on() to
      __queue_work() which all queueing paths share.
    
    * Make __queue_work() take @cpu and @wq instead of @cwq.
    
    * Restructure flush_work() and __create_workqueue_key() to make them
      easier to modify.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 0697946c66a1..e724dafc9e6d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -96,9 +96,14 @@ struct execute_work {
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 extern void __init_work(struct work_struct *work, int onstack);
 extern void destroy_work_on_stack(struct work_struct *work);
+static inline unsigned int work_static(struct work_struct *work)
+{
+	return *work_data_bits(work) & (1 << WORK_STRUCT_STATIC);
+}
 #else
 static inline void __init_work(struct work_struct *work, int onstack) { }
 static inline void destroy_work_on_stack(struct work_struct *work) { }
+static inline unsigned int work_static(struct work_struct *work) { return 0; }
 #endif
 
 /*

commit c790bce0481857412c964c5e9d46d56e41c4b051
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:09 2010 +0200

    workqueue: kill RT workqueue
    
    With stop_machine() converted to use cpu_stop, RT workqueue doesn't
    have any user left.  Kill RT workqueue support.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 9466e860d8c2..0697946c66a1 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -181,12 +181,11 @@ static inline void destroy_work_on_stack(struct work_struct *work) { }
 
 
 extern struct workqueue_struct *
-__create_workqueue_key(const char *name, int singlethread,
-		       int freezeable, int rt, struct lock_class_key *key,
-		       const char *lock_name);
+__create_workqueue_key(const char *name, int singlethread, int freezeable,
+		       struct lock_class_key *key, const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
-#define __create_workqueue(name, singlethread, freezeable, rt)	\
+#define __create_workqueue(name, singlethread, freezeable)	\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
@@ -197,19 +196,18 @@ __create_workqueue_key(const char *name, int singlethread,
 		__lock_name = #name;				\
 								\
 	__create_workqueue_key((name), (singlethread),		\
-			       (freezeable), (rt), &__key,	\
+			       (freezeable), &__key,		\
 			       __lock_name);			\
 })
 #else
-#define __create_workqueue(name, singlethread, freezeable, rt)	\
-	__create_workqueue_key((name), (singlethread), (freezeable), (rt), \
+#define __create_workqueue(name, singlethread, freezeable)	\
+	__create_workqueue_key((name), (singlethread), (freezeable), \
 			       NULL, NULL)
 #endif
 
-#define create_workqueue(name) __create_workqueue((name), 0, 0, 0)
-#define create_rt_workqueue(name) __create_workqueue((name), 0, 0, 1)
-#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1, 0)
-#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0, 0)
+#define create_workqueue(name) __create_workqueue((name), 0, 0)
+#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1)
+#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit a25909a4d4a29e272f953e12595bf2f04a292dbd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 13 12:32:28 2010 -0700

    lockdep: Add an in_workqueue_context() lockdep-based test function
    
    Some recent uses of RCU make use of workqueues.  In these uses, execution
    within the context of a specific workqueue takes the place of the usual
    RCU read-side primitives such as rcu_read_lock(), and flushing of workqueues
    takes the place of the usual RCU grace-period primitives.  Checking for
    correct use of rcu_dereference() in such cases requires a test of whether
    the code is executing in the context of a particular workqueue.  This
    commit adds an in_workqueue_context() function that provides this test.
    This new function is only defined when lockdep is enabled, which allows
    it to be used as the second argument of rcu_dereference_check().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 9466e860d8c2..d0f7c8178498 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -297,4 +297,8 @@ static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
 #else
 long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
 #endif /* CONFIG_SMP */
+
+#ifdef CONFIG_LOCKDEP
+int in_workqueue_context(struct workqueue_struct *wq);
+#endif
 #endif

commit dc186ad741c12ae9ecac8b89e317ef706fdaf8f6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 16 01:09:48 2009 +0900

    workqueue: Add debugobjects support
    
    Add debugobject support to track the life time of work_structs.
    
    While at it, remove duplicate definition of
    INIT_DELAYED_WORK_ON_STACK().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index cf24c20de9e4..9466e860d8c2 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -25,6 +25,7 @@ typedef void (*work_func_t)(struct work_struct *work);
 struct work_struct {
 	atomic_long_t data;
 #define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
+#define WORK_STRUCT_STATIC  1		/* static initializer (debugobjects) */
 #define WORK_STRUCT_FLAG_MASK (3UL)
 #define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
@@ -35,6 +36,7 @@ struct work_struct {
 };
 
 #define WORK_DATA_INIT()	ATOMIC_LONG_INIT(0)
+#define WORK_DATA_STATIC_INIT()	ATOMIC_LONG_INIT(2)
 
 struct delayed_work {
 	struct work_struct work;
@@ -63,7 +65,7 @@ struct execute_work {
 #endif
 
 #define __WORK_INITIALIZER(n, f) {				\
-	.data = WORK_DATA_INIT(),				\
+	.data = WORK_DATA_STATIC_INIT(),			\
 	.entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
 	__WORK_INIT_LOCKDEP_MAP(#n, &(n))			\
@@ -91,6 +93,14 @@ struct execute_work {
 #define PREPARE_DELAYED_WORK(_work, _func)			\
 	PREPARE_WORK(&(_work)->work, (_func))
 
+#ifdef CONFIG_DEBUG_OBJECTS_WORK
+extern void __init_work(struct work_struct *work, int onstack);
+extern void destroy_work_on_stack(struct work_struct *work);
+#else
+static inline void __init_work(struct work_struct *work, int onstack) { }
+static inline void destroy_work_on_stack(struct work_struct *work) { }
+#endif
+
 /*
  * initialize all of a work item in one go
  *
@@ -99,24 +109,36 @@ struct execute_work {
  * to generate better code.
  */
 #ifdef CONFIG_LOCKDEP
-#define INIT_WORK(_work, _func)						\
+#define __INIT_WORK(_work, _func, _onstack)				\
 	do {								\
 		static struct lock_class_key __key;			\
 									\
+		__init_work((_work), _onstack);				\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0);\
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
 #else
-#define INIT_WORK(_work, _func)						\
+#define __INIT_WORK(_work, _func, _onstack)				\
 	do {								\
+		__init_work((_work), _onstack);				\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
 #endif
 
+#define INIT_WORK(_work, _func)					\
+	do {							\
+		__INIT_WORK((_work), (_func), 0);		\
+	} while (0)
+
+#define INIT_WORK_ON_STACK(_work, _func)			\
+	do {							\
+		__INIT_WORK((_work), (_func), 1);		\
+	} while (0)
+
 #define INIT_DELAYED_WORK(_work, _func)				\
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\
@@ -125,22 +147,16 @@ struct execute_work {
 
 #define INIT_DELAYED_WORK_ON_STACK(_work, _func)		\
 	do {							\
-		INIT_WORK(&(_work)->work, (_func));		\
+		INIT_WORK_ON_STACK(&(_work)->work, (_func));	\
 		init_timer_on_stack(&(_work)->timer);		\
 	} while (0)
 
-#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
+#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)		\
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\
 		init_timer_deferrable(&(_work)->timer);		\
 	} while (0)
 
-#define INIT_DELAYED_WORK_ON_STACK(_work, _func)		\
-	do {							\
-		INIT_WORK(&(_work)->work, (_func));		\
-		init_timer_on_stack(&(_work)->timer);		\
-	} while (0)
-
 /**
  * work_pending - Find out whether a work item is currently pending
  * @work: The work item in question

commit 43046b606673c9c991919ff75b980b72541e9ede
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 14 09:16:42 2009 -0700

    workqueue: add 'flush_delayed_work()' to run and wait for delayed work
    
    It basically turns a delayed work into an immediate work, and then waits
    for it to finish.

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7ef0c7b94f31..cf24c20de9e4 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -207,6 +207,7 @@ extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 
 extern void flush_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
+extern void flush_delayed_work(struct delayed_work *work);
 
 extern int schedule_work(struct work_struct *work);
 extern int schedule_work_on(int cpu, struct work_struct *work);

commit b9049df5a0e7f35456c06b949b08b898b9c2e7bc
Author: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
Date:   Tue Jun 23 12:09:29 2009 +0200

    Change "useing" -> "using".
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 6273fa97b527..7ef0c7b94f31 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -94,7 +94,7 @@ struct execute_work {
 /*
  * initialize all of a work item in one go
  *
- * NOTE! No point in using "atomic_long_set()": useing a direct
+ * NOTE! No point in using "atomic_long_set()": using a direct
  * assignment of the work data initializer allows the compiler
  * to generate better code.
  */

commit 4e49627b9bc29a14b393c480e8c979e3bc922ef7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sat Sep 5 11:17:06 2009 -0700

    workqueues: introduce __cancel_delayed_work()
    
    cancel_delayed_work() has to use del_timer_sync() to guarantee the timer
    function is not running after return.  But most users doesn't actually
    need this, and del_timer_sync() has problems: it is not useable from
    interrupt, and it depends on every lock which could be taken from irq.
    
    Introduce __cancel_delayed_work() which calls del_timer() instead.
    
    The immediate reason for this patch is
    http://bugzilla.kernel.org/show_bug.cgi?id=13757
    but hopefully this helper makes sense anyway.
    
    As for 13757 bug, actually we need requeue_delayed_work(), but its
    semantics are not yet clear.
    
    Merge this patch early to resolves cross-tree interdependencies between
    input and infiniband.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: Stefan Richter <stefanr@s5r6.in-berlin.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 13e1adf55c4c..6273fa97b527 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -240,6 +240,21 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
+/*
+ * Like above, but uses del_timer() instead of del_timer_sync(). This means,
+ * if it returns 0 the timer function may be running and the queueing is in
+ * progress.
+ */
+static inline int __cancel_delayed_work(struct delayed_work *work)
+{
+	int ret;
+
+	ret = del_timer(&work->timer);
+	if (ret)
+		work_clear_pending(&work->work);
+	return ret;
+}
+
 extern int cancel_delayed_work_sync(struct delayed_work *work);
 
 /* Obsolete. use cancel_delayed_work_sync() */

commit bf6aede712334d7338d5c47a5ee5ba3883c82a61
Author: Jean Delvare <khali@linux-fr.org>
Date:   Thu Apr 2 16:56:54 2009 -0700

    workqueue: add to_delayed_work() helper function
    
    It is a fairly common operation to have a pointer to a work and to need a
    pointer to the delayed work it is contained in.  In particular, all
    delayed works which want to rearm themselves will have to do that.  So it
    would seem fair to offer a helper function for this operation.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jean Delvare <khali@linux-fr.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Greg KH <greg@kroah.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 3cd51e579ab1..13e1adf55c4c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -41,6 +41,11 @@ struct delayed_work {
 	struct timer_list timer;
 };
 
+static inline struct delayed_work *to_delayed_work(struct work_struct *work)
+{
+	return container_of(work, struct delayed_work, work);
+}
+
 struct execute_work {
 	struct work_struct work;
 };

commit 3386c05bdbd3e60ca7158253442f0a00133db28e
Merge: 1e70c7f7a9d4 6552ebae25ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 26 09:47:56 2009 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      debugobjects: add and use INIT_WORK_ON_STACK
      rcu: remove duplicate CONFIG_RCU_CPU_STALL_DETECTOR
      relay: fix lock imbalance in relay_late_setup_files
      oprofile: fix uninitialized use of struct op_entry
      rcu: move Kconfig menu
      softlock: fix false panic which can occur if softlockup_thresh is reduced
      rcu: add __cpuinit to rcu_init_percpu_data()

commit 336f6c322d87806ef93afad6308ac65083a865e5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 22 09:50:44 2009 +0100

    debugobjects: add and use INIT_WORK_ON_STACK
    
    Impact: Fix debugobjects warning
    
    debugobject enabled kernels spit out a warning in hpet code due to a
    workqueue which is initialized on stack.
    
    Add INIT_WORK_ON_STACK() which calls init_timer_on_stack() and use it
    in hpet.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b36291130f22..20b59eb1facd 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -124,6 +124,12 @@ struct execute_work {
 		init_timer_deferrable(&(_work)->timer);		\
 	} while (0)
 
+#define INIT_DELAYED_WORK_ON_STACK(_work, _func)		\
+	do {							\
+		INIT_WORK(&(_work)->work, (_func));		\
+		init_timer_on_stack(&(_work)->timer);		\
+	} while (0)
+
 /**
  * work_pending - Find out whether a work item is currently pending
  * @work: The work item in question

commit 6d612b0f943289856c6e8186c564cda922cd040e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 12 12:52:23 2009 +0100

    locking, hpet: annotate false positive warning
    
    Alexander Beregalov reported that this warning is caused by the HPET code:
    
    > hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0
    > hpet0: 3 comparators, 64-bit 14.318180 MHz counter
    > ODEBUG: object is on stack, but not annotated
    > ------------[ cut here ]------------
    > WARNING: at lib/debugobjects.c:251 __debug_object_init+0x2a4/0x352()
    
    > Bisected down to 26afe5f2fbf06ea0765aaa316640c4dd472310c0
    > (x86: HPET_MSI Initialise per-cpu HPET timers)
    
    The commit is fine - but the on-stack workqueue entry needs annotation.
    
    Reported-and-bisected-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b36291130f22..47151c8495aa 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -118,6 +118,12 @@ struct execute_work {
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
+#define INIT_DELAYED_WORK_ON_STACK(_work, _func)		\
+	do {							\
+		INIT_WORK(&(_work)->work, (_func));		\
+		init_timer_on_stack(&(_work)->timer);		\
+	} while (0)
+
 #define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\

commit 2d3854a37e8b767a51aba38ed6d22817b0631e33
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Wed Nov 5 13:39:10 2008 +1100

    cpumask: introduce new API, without changing anything
    
    Impact: introduce new APIs
    
    We want to deprecate cpumasks on the stack, as we are headed for
    gynormous numbers of CPUs.  Eventually, we want to head towards an
    undefined 'struct cpumask' so they can never be declared on stack.
    
    1) New cpumask functions which take pointers instead of copies.
       (cpus_* -> cpumask_*)
    
    2) Several new helpers to reduce requirements for temporary cpumasks
       (cpumask_first_and, cpumask_next_and, cpumask_any_and)
    
    3) Helpers for declaring cpumasks on or offstack for large NR_CPUS
       (cpumask_var_t, alloc_cpumask_var and free_cpumask_var)
    
    4) 'struct cpumask' for explicitness and to mark new-style code.
    
    5) Make iterator functions stop at nr_cpu_ids (a runtime constant),
       not NR_CPUS for time efficiency and for smaller dynamic allocations
       in future.
    
    6) cpumask_copy() so we can allocate less than a full cpumask eventually
       (for alloc_cpumask_var), and so we can eliminate the 'struct cpumask'
       definition eventually.
    
    7) work_on_cpu() helper for doing task on a CPU, rather than saving old
       cpumask for current thread and manipulating it.
    
    8) smp_call_function_many() which is smp_call_function_mask() except
       taking a cpumask pointer.
    
    Note that this patch simply introduces the new functions and leaves
    the obsolescent ones in place.  This is to simplify the transition
    patches.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 89a5a1231ffb..b36291130f22 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -240,4 +240,12 @@ void cancel_rearming_delayed_work(struct delayed_work *work)
 	cancel_delayed_work_sync(work);
 }
 
+#ifndef CONFIG_SMP
+static inline long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg)
+{
+	return fn(arg);
+}
+#else
+long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
+#endif /* CONFIG_SMP */
 #endif

commit 0d557dc97f4bb501f086a03d0f00b99a7855d794
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Oct 13 23:50:09 2008 +0200

    workqueue: introduce create_rt_workqueue
    
    create_rt_workqueue will create a real time prioritized workqueue.
    This is needed for the conversion of stop_machine to a workqueue based
    implementation.
    This patch adds yet another parameter to __create_workqueue_key to tell
    it that we want an rt workqueue.
    However it looks like we rather should have something like "int type"
    instead of singlethread, freezable and rt.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5c158c477ac7..89a5a1231ffb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -149,11 +149,11 @@ struct execute_work {
 
 extern struct workqueue_struct *
 __create_workqueue_key(const char *name, int singlethread,
-		       int freezeable, struct lock_class_key *key,
+		       int freezeable, int rt, struct lock_class_key *key,
 		       const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
-#define __create_workqueue(name, singlethread, freezeable)	\
+#define __create_workqueue(name, singlethread, freezeable, rt)	\
 ({								\
 	static struct lock_class_key __key;			\
 	const char *__lock_name;				\
@@ -164,17 +164,19 @@ __create_workqueue_key(const char *name, int singlethread,
 		__lock_name = #name;				\
 								\
 	__create_workqueue_key((name), (singlethread),		\
-			       (freezeable), &__key,		\
+			       (freezeable), (rt), &__key,	\
 			       __lock_name);			\
 })
 #else
-#define __create_workqueue(name, singlethread, freezeable)	\
-	__create_workqueue_key((name), (singlethread), (freezeable), NULL, NULL)
+#define __create_workqueue(name, singlethread, freezeable, rt)	\
+	__create_workqueue_key((name), (singlethread), (freezeable), (rt), \
+			       NULL, NULL)
 #endif
 
-#define create_workqueue(name) __create_workqueue((name), 0, 0)
-#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1)
-#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)
+#define create_workqueue(name) __create_workqueue((name), 0, 0, 0)
+#define create_rt_workqueue(name) __create_workqueue((name), 0, 0, 1)
+#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1, 0)
+#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0, 0)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit db700897224b5ebdf852f2d38920ce428940d059
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:49 2008 -0700

    workqueues: implement flush_work()
    
    Most of users of flush_workqueue() can be changed to use cancel_work_sync(),
    but sometimes we really need to wait for the completion and cancelling is not
    an option. schedule_on_each_cpu() is good example.
    
    Add the new helper, flush_work(work), which waits for the completion of the
    specific work_struct. More precisely, it "flushes" the result of of the last
    queue_work() which is visible to the caller.
    
    For example, this code
    
            queue_work(wq, work);
            /* WINDOW */
            queue_work(wq, work);
    
            flush_work(work);
    
    doesn't necessary work "as expected". What can happen in the WINDOW above is
    
            - wq starts the execution of work->func()
    
            - the caller migrates to another CPU
    
    now, after the 2nd queue_work() this work is active on the previous CPU, and
    at the same time it is queued on another. In this case flush_work(work) may
    return before the first work->func() completes.
    
    It is trivial to add another helper
    
            int flush_work_sync(struct work_struct *work)
            {
                    return flush_work(work) || wait_on_work(work);
            }
    
    which works "more correctly", but it has to iterate over all CPUs and thus
    it much slower than flush_work().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Max Krasnyansky <maxk@qualcomm.com>
    Acked-by: Jarek Poplawski <jarkao2@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 14d47120682b..5c158c477ac7 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -201,6 +201,8 @@ extern int keventd_up(void);
 extern void init_workqueues(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
+extern int flush_work(struct work_struct *work);
+
 extern int cancel_work_sync(struct work_struct *work);
 
 /*

commit c1a220e7acf8ad2c03504891f4a70cd9c32c904b
Author: Zhang Rui <rui.zhang@intel.com>
Date:   Wed Jul 23 21:28:39 2008 -0700

    pm: introduce new interfaces schedule_work_on() and queue_work_on()
    
    This interface allows adding a job on a specific cpu.
    
    Although a work struct on a cpu will be scheduled to other cpu if the cpu
    dies, there is a recursion if a work task tries to offline the cpu it's
    running on.  we need to schedule the task to a specific cpu in this case.
    http://bugzilla.kernel.org/show_bug.cgi?id=10897
    
    [oleg@tv-sign.ru: cleanups]
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>
    Tested-by: Rus <harbour@sfinx.od.ua>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 542526c6e8ef..14d47120682b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -179,6 +179,8 @@ __create_workqueue_key(const char *name, int singlethread,
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
 extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
+extern int queue_work_on(int cpu, struct workqueue_struct *wq,
+			struct work_struct *work);
 extern int queue_delayed_work(struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
@@ -188,6 +190,7 @@ extern void flush_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
 extern int schedule_work(struct work_struct *work);
+extern int schedule_work_on(int cpu, struct work_struct *work);
 extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
 					unsigned long delay);

commit b3c97528689619fc66569b30bf83d09d9929521a
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Wed Feb 13 15:03:15 2008 -0800

    include/linux: Remove all users of FASTCALL() macro
    
    FASTCALL() is always expanded to empty, remove it.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7f28c32d9aca..542526c6e8ef 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -178,18 +178,17 @@ __create_workqueue_key(const char *name, int singlethread,
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
-extern int FASTCALL(queue_work(struct workqueue_struct *wq, struct work_struct *work));
-extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq,
-			struct delayed_work *work, unsigned long delay));
+extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
+extern int queue_delayed_work(struct workqueue_struct *wq,
+			struct delayed_work *work, unsigned long delay);
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *work, unsigned long delay);
 
-extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
+extern void flush_workqueue(struct workqueue_struct *wq);
 extern void flush_scheduled_work(void);
 
-extern int FASTCALL(schedule_work(struct work_struct *work));
-extern int FASTCALL(schedule_delayed_work(struct delayed_work *work,
-					unsigned long delay));
+extern int schedule_work(struct work_struct *work);
+extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
 					unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);

commit eb13ba873881abd5e15af784756a61af635e665e
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Wed Jan 16 09:51:58 2008 +0100

    lockdep: fix workqueue creation API lockdep interaction
    Dave Young reported warnings from lockdep that the workqueue API
    can sometimes try to register lockdep classes with the same key
    but different names. This is not permitted in lockdep.
    
    Unfortunately, I was unaware of that restriction when I wrote
    the code to debug workqueue problems with lockdep and used the
    workqueue name as the lockdep class name. This can obviously
    lead to the problem if the workqueue name is dynamic.
    
    This patch solves the problem by always using a constant name
    for the workqueue's lockdep class, namely either the constant
    name that was passed in or a string consisting of the variable
    name.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7daafdc2514b..7f28c32d9aca 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -149,19 +149,27 @@ struct execute_work {
 
 extern struct workqueue_struct *
 __create_workqueue_key(const char *name, int singlethread,
-		       int freezeable, struct lock_class_key *key);
+		       int freezeable, struct lock_class_key *key,
+		       const char *lock_name);
 
 #ifdef CONFIG_LOCKDEP
 #define __create_workqueue(name, singlethread, freezeable)	\
 ({								\
 	static struct lock_class_key __key;			\
+	const char *__lock_name;				\
+								\
+	if (__builtin_constant_p(name))				\
+		__lock_name = (name);				\
+	else							\
+		__lock_name = #name;				\
 								\
 	__create_workqueue_key((name), (singlethread),		\
-			       (freezeable), &__key);		\
+			       (freezeable), &__key,		\
+			       __lock_name);			\
 })
 #else
 #define __create_workqueue(name, singlethread, freezeable)	\
-	__create_workqueue_key((name), (singlethread), (freezeable), NULL)
+	__create_workqueue_key((name), (singlethread), (freezeable), NULL, NULL)
 #endif
 
 #define create_workqueue(name) __create_workqueue((name), 0, 0)

commit 4e6045f134784f4b158b3c0f7a282b04bd816887
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Thu Oct 18 23:39:55 2007 -0700

    workqueue: debug flushing deadlocks with lockdep
    
    In the following scenario:
    
    code path 1:
      my_function() -> lock(L1); ...; flush_workqueue(); ...
    
    code path 2:
      run_workqueue() -> my_work() -> ...; lock(L1); ...
    
    you can get a deadlock when my_work() is queued or running
    but my_function() has acquired L1 already.
    
    This patch adds a pseudo-lock to each workqueue to make lockdep
    warn about this scenario.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ce6badc98f6d..7daafdc2514b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -8,6 +8,7 @@
 #include <linux/timer.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
+#include <linux/lockdep.h>
 #include <asm/atomic.h>
 
 struct workqueue_struct;
@@ -28,6 +29,9 @@ struct work_struct {
 #define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
 	work_func_t func;
+#ifdef CONFIG_LOCKDEP
+	struct lockdep_map lockdep_map;
+#endif
 };
 
 #define WORK_DATA_INIT()	ATOMIC_LONG_INIT(0)
@@ -41,10 +45,23 @@ struct execute_work {
 	struct work_struct work;
 };
 
+#ifdef CONFIG_LOCKDEP
+/*
+ * NB: because we have to copy the lockdep_map, setting _key
+ * here is required, otherwise it could get initialised to the
+ * copy of the lockdep_map!
+ */
+#define __WORK_INIT_LOCKDEP_MAP(n, k) \
+	.lockdep_map = STATIC_LOCKDEP_MAP_INIT(n, k),
+#else
+#define __WORK_INIT_LOCKDEP_MAP(n, k)
+#endif
+
 #define __WORK_INITIALIZER(n, f) {				\
 	.data = WORK_DATA_INIT(),				\
 	.entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
+	__WORK_INIT_LOCKDEP_MAP(#n, &(n))			\
 	}
 
 #define __DELAYED_WORK_INITIALIZER(n, f) {			\
@@ -76,12 +93,24 @@ struct execute_work {
  * assignment of the work data initializer allows the compiler
  * to generate better code.
  */
+#ifdef CONFIG_LOCKDEP
+#define INIT_WORK(_work, _func)						\
+	do {								\
+		static struct lock_class_key __key;			\
+									\
+		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
+		lockdep_init_map(&(_work)->lockdep_map, #_work, &__key, 0);\
+		INIT_LIST_HEAD(&(_work)->entry);			\
+		PREPARE_WORK((_work), (_func));				\
+	} while (0)
+#else
 #define INIT_WORK(_work, _func)						\
 	do {								\
 		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
+#endif
 
 #define INIT_DELAYED_WORK(_work, _func)				\
 	do {							\
@@ -118,9 +147,23 @@ struct execute_work {
 	clear_bit(WORK_STRUCT_PENDING, work_data_bits(work))
 
 
-extern struct workqueue_struct *__create_workqueue(const char *name,
-						    int singlethread,
-						    int freezeable);
+extern struct workqueue_struct *
+__create_workqueue_key(const char *name, int singlethread,
+		       int freezeable, struct lock_class_key *key);
+
+#ifdef CONFIG_LOCKDEP
+#define __create_workqueue(name, singlethread, freezeable)	\
+({								\
+	static struct lock_class_key __key;			\
+								\
+	__create_workqueue_key((name), (singlethread),		\
+			       (freezeable), &__key);		\
+})
+#else
+#define __create_workqueue(name, singlethread, freezeable)	\
+	__create_workqueue_key((name), (singlethread), (freezeable), NULL)
+#endif
+
 #define create_workqueue(name) __create_workqueue((name), 0, 0)
 #define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1)
 #define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)

commit 1f1f642e2f092e37eb9038060eb0100c44f55a11
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Jul 15 23:41:44 2007 -0700

    make cancel_xxx_work_sync() return a boolean
    
    Change cancel_work_sync() and cancel_delayed_work_sync() to return a boolean
    indicating whether the work was actually cancelled.  A zero return value means
    that the work was not pending/queued.
    
    Without that kind of change it is not possible to avoid flush_workqueue()
    sometimes, see the next patch as an example.
    
    Also, this patch unifies both functions and kills the (unlikely) busy-wait
    loop.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Jarek Poplawski <jarkao2@o2.pl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5c89ac6e7f55..ce6badc98f6d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -148,7 +148,7 @@ extern int keventd_up(void);
 extern void init_workqueues(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
-extern void cancel_work_sync(struct work_struct *work);
+extern int cancel_work_sync(struct work_struct *work);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback
@@ -166,7 +166,7 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
-extern void cancel_delayed_work_sync(struct delayed_work *work);
+extern int cancel_delayed_work_sync(struct delayed_work *work);
 
 /* Obsolete. use cancel_delayed_work_sync() */
 static inline

commit f5a421a4509a7e2dff11da0f01b0548f4f84d503
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Jul 15 23:41:44 2007 -0700

    rename cancel_rearming_delayed_work() to cancel_delayed_work_sync()
    
    Imho, the current naming of cancel_xxx workqueue functions is very confusing.
    
            cancel_delayed_work()
            cancel_rearming_delayed_work()
            cancel_rearming_delayed_workqueue()     // obsolete
    
            cancel_work_sync()
    
    This looks as if the first 2 functions differ in "type" of their argument
    which is not true any longer, nowadays the difference is the behaviour.
    
    The semantics of cancel_rearming_delayed_work(dwork) was changed
    significantly, it doesn't require that dwork rearms itself, and cancels dwork
    synchronously.
    
    Rename it to cancel_delayed_work_sync().  This matches cancel_delayed_work()
    and cancel_work_sync().  Re-create cancel_rearming_delayed_work() as a simple
    inline obsolete wrapper, like cancel_rearming_delayed_workqueue().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Jarek Poplawski <jarkao2@o2.pl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ce0719a2cfeb..5c89ac6e7f55 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -166,14 +166,21 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
-extern void cancel_rearming_delayed_work(struct delayed_work *work);
+extern void cancel_delayed_work_sync(struct delayed_work *work);
 
-/* Obsolete. use cancel_rearming_delayed_work() */
+/* Obsolete. use cancel_delayed_work_sync() */
 static inline
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
 					struct delayed_work *work)
 {
-	cancel_rearming_delayed_work(work);
+	cancel_delayed_work_sync(work);
+}
+
+/* Obsolete. use cancel_delayed_work_sync() */
+static inline
+void cancel_rearming_delayed_work(struct delayed_work *work)
+{
+	cancel_delayed_work_sync(work);
 }
 
 #endif

commit 223a10a98135da38d3668973d72cdffb1ced4b7d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri May 18 00:36:42 2007 -0700

    revert "cancel_delayed_work: use del_timer() instead of del_timer_sync()"
    
    As pointed out by Jarek Poplawski, the patch
    
            [WORKQUEUE]: cancel_delayed_work: use del_timer() instead of del_timer_sync()
            commit: 071b638689464c6b39407025eedd810d5b5e6f5d
    
    was wrong, it was merged by mistake after that.
    
    From the changelog:
    
            after this patch:
                    ...
                    delayed_work_timer_fn->__queue_work() in progress.
    
                    The latter doesn't differ from the caller's POV,
    
    it does make a difference if the caller calls flush_workqueue() after
    cancel_delayed_work(), in that case flush_workqueue() can miss this
    work_struct.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Jarek Poplawski <jarkao2@o2.pl>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7eae8665ff59..ce0719a2cfeb 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -160,7 +160,7 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 {
 	int ret;
 
-	ret = del_timer(&work->timer);
+	ret = del_timer_sync(&work->timer);
 	if (ret)
 		work_clear_pending(&work->work);
 	return ret;

commit e3dfd2964ea86ae65f511b10d62ea54d46db3708
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 16 22:11:11 2007 -0700

    make freezeable workqueues singlethread
    
    It is a known fact that freezeable multithreaded workqueues doesn't like
    CPU_DEAD. We keep them only for the incoming CPU-hotplug rework.
    
    Sadly, we can't just kill create_freezeable_workqueue() right now, make
    them singlethread.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index d555f31c0746..7eae8665ff59 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -122,7 +122,7 @@ extern struct workqueue_struct *__create_workqueue(const char *name,
 						    int singlethread,
 						    int freezeable);
 #define create_workqueue(name) __create_workqueue((name), 0, 0)
-#define create_freezeable_workqueue(name) __create_workqueue((name), 0, 1)
+#define create_freezeable_workqueue(name) __create_workqueue((name), 1, 1)
 #define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);

commit 28e53bddf814485699a4142bc056fd37d4e11dd4
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:22 2007 -0700

    unify flush_work/flush_work_keventd and rename it to cancel_work_sync
    
    flush_work(wq, work) doesn't need the first parameter, we can use cwq->wq
    (this was possible from the very beginnig, I missed this).  So we can unify
    flush_work_keventd and flush_work.
    
    Also, rename flush_work() to cancel_work_sync() and fix all callers.
    Perhaps this is not the best name, but "flush_work" is really bad.
    
    (akpm: this is why the earlier patches bypassed maintainers)
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Jeff Garzik <jeff@garzik.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Auke Kok <auke-jan.h.kok@intel.com>,
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index e1581dce5890..d555f31c0746 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -128,30 +128,33 @@ extern struct workqueue_struct *__create_workqueue(const char *name,
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
 extern int FASTCALL(queue_work(struct workqueue_struct *wq, struct work_struct *work));
-extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct delayed_work *work, unsigned long delay));
+extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq,
+			struct delayed_work *work, unsigned long delay));
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
-	struct delayed_work *work, unsigned long delay);
+			struct delayed_work *work, unsigned long delay);
+
 extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
-extern void flush_work(struct workqueue_struct *wq, struct work_struct *work);
-extern void flush_work_keventd(struct work_struct *work);
+extern void flush_scheduled_work(void);
 
 extern int FASTCALL(schedule_work(struct work_struct *work));
-extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
-
-extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);
+extern int FASTCALL(schedule_delayed_work(struct delayed_work *work,
+					unsigned long delay));
+extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
+					unsigned long delay);
 extern int schedule_on_each_cpu(work_func_t func);
-extern void flush_scheduled_work(void);
 extern int current_is_keventd(void);
 extern int keventd_up(void);
 
 extern void init_workqueues(void);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
+extern void cancel_work_sync(struct work_struct *work);
+
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback
  * function may still be running on return from cancel_delayed_work(), unless
  * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
- * flush_work() or cancel_work_sync() to wait on it.
+ * cancel_work_sync() to wait on it.
  */
 static inline int cancel_delayed_work(struct delayed_work *work)
 {

commit 23b2e5991afde5af91a1a661d7f47ee56120759e
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:19 2007 -0700

    workqueue: kill NOAUTOREL works
    
    We don't have any users, and it is not so trivial to use NOAUTOREL works
    correctly.  It is better to simplify API.
    
    Delete NOAUTOREL support and rename work_release to work_clear_pending to
    avoid a confusion.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 27110c04f21e..e1581dce5890 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -24,15 +24,13 @@ typedef void (*work_func_t)(struct work_struct *work);
 struct work_struct {
 	atomic_long_t data;
 #define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
-#define WORK_STRUCT_NOAUTOREL 1		/* F if work item automatically released on exec */
 #define WORK_STRUCT_FLAG_MASK (3UL)
 #define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
 	work_func_t func;
 };
 
-#define WORK_DATA_INIT(autorelease) \
-	ATOMIC_LONG_INIT((autorelease) << WORK_STRUCT_NOAUTOREL)
+#define WORK_DATA_INIT()	ATOMIC_LONG_INIT(0)
 
 struct delayed_work {
 	struct work_struct work;
@@ -44,14 +42,8 @@ struct execute_work {
 };
 
 #define __WORK_INITIALIZER(n, f) {				\
-	.data = WORK_DATA_INIT(0),				\
-        .entry	= { &(n).entry, &(n).entry },			\
-	.func = (f),						\
-	}
-
-#define __WORK_INITIALIZER_NAR(n, f) {				\
-	.data = WORK_DATA_INIT(1),				\
-        .entry	= { &(n).entry, &(n).entry },			\
+	.data = WORK_DATA_INIT(),				\
+	.entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
 	}
 
@@ -60,23 +52,12 @@ struct execute_work {
 	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
 	}
 
-#define __DELAYED_WORK_INITIALIZER_NAR(n, f) {			\
-	.work = __WORK_INITIALIZER_NAR((n).work, (f)),		\
-	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
-	}
-
 #define DECLARE_WORK(n, f)					\
 	struct work_struct n = __WORK_INITIALIZER(n, f)
 
-#define DECLARE_WORK_NAR(n, f)					\
-	struct work_struct n = __WORK_INITIALIZER_NAR(n, f)
-
 #define DECLARE_DELAYED_WORK(n, f)				\
 	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
 
-#define DECLARE_DELAYED_WORK_NAR(n, f)			\
-	struct dwork_struct n = __DELAYED_WORK_INITIALIZER_NAR(n, f)
-
 /*
  * initialize a work item's function pointer
  */
@@ -95,16 +76,9 @@ struct execute_work {
  * assignment of the work data initializer allows the compiler
  * to generate better code.
  */
-#define INIT_WORK(_work, _func)					\
-	do {							\
-		(_work)->data = (atomic_long_t) WORK_DATA_INIT(0);	\
-		INIT_LIST_HEAD(&(_work)->entry);		\
-		PREPARE_WORK((_work), (_func));			\
-	} while (0)
-
-#define INIT_WORK_NAR(_work, _func)					\
+#define INIT_WORK(_work, _func)						\
 	do {								\
-		(_work)->data = (atomic_long_t) WORK_DATA_INIT(1);	\
+		(_work)->data = (atomic_long_t) WORK_DATA_INIT();	\
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
@@ -115,12 +89,6 @@ struct execute_work {
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
-#define INIT_DELAYED_WORK_NAR(_work, _func)			\
-	do {							\
-		INIT_WORK_NAR(&(_work)->work, (_func));		\
-		init_timer(&(_work)->timer);			\
-	} while (0)
-
 #define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
 	do {							\
 		INIT_WORK(&(_work)->work, (_func));		\
@@ -143,24 +111,10 @@ struct execute_work {
 	work_pending(&(w)->work)
 
 /**
- * work_release - Release a work item under execution
- * @work: The work item to release
- *
- * This is used to release a work item that has been initialised with automatic
- * release mode disabled (WORK_STRUCT_NOAUTOREL is set).  This gives the work
- * function the opportunity to grab auxiliary data from the container of the
- * work_struct before clearing the pending bit as the work_struct may be
- * subject to deallocation the moment the pending bit is cleared.
- *
- * In such a case, this should be called in the work function after it has
- * fetched any data it may require from the containter of the work_struct.
- * After this function has been called, the work_struct may be scheduled for
- * further execution or it may be deallocated unless other precautions are
- * taken.
- *
- * This should also be used to release a delayed work item.
+ * work_clear_pending - for internal use only, mark a work item as not pending
+ * @work: The work item in question
  */
-#define work_release(work) \
+#define work_clear_pending(work) \
 	clear_bit(WORK_STRUCT_PENDING, work_data_bits(work))
 
 
@@ -205,7 +159,7 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 
 	ret = del_timer(&work->timer);
 	if (ret)
-		work_release(&work->work);
+		work_clear_pending(&work->work);
 	return ret;
 }
 

commit 1634c48f8b85dcb05101f1eb2eab9af40b5976da
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:18 2007 -0700

    make cancel_rearming_delayed_work() work on any workqueue, not just keventd_wq
    
    cancel_rearming_delayed_workqueue(wq, dwork) doesn't need the first
    parameter.  We don't hang on un-queued dwork any longer, and work->data
    doesn't change its type.  This means we can always figure out "wq" from
    dwork when it is needed.
    
    Remove this parameter, and rename the function to
    cancel_rearming_delayed_work().  Re-create an inline "obsolete"
    cancel_rearming_delayed_workqueue(wq) which just calls
    cancel_rearming_delayed_work().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 2a58f16e1961..27110c04f21e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -191,9 +191,6 @@ extern int current_is_keventd(void);
 extern int keventd_up(void);
 
 extern void init_workqueues(void);
-void cancel_rearming_delayed_work(struct delayed_work *work);
-void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
-				       struct delayed_work *);
 int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 /*
@@ -212,4 +209,14 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 	return ret;
 }
 
+extern void cancel_rearming_delayed_work(struct delayed_work *work);
+
+/* Obsolete. use cancel_rearming_delayed_work() */
+static inline
+void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
+					struct delayed_work *work)
+{
+	cancel_rearming_delayed_work(work);
+}
+
 #endif

commit 7097a87afe937a5879528d52880c2d95f089e96c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:10 2007 -0700

    workqueue: kill run_scheduled_work()
    
    Because it has no callers.
    
    Actually, I think the whole idea of run_scheduled_work() was not right, not
    good to mix "unqueue this work and execute its ->func()" in one function.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 26a70992dec8..2a58f16e1961 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -182,7 +182,6 @@ extern void flush_work(struct workqueue_struct *wq, struct work_struct *work);
 extern void flush_work_keventd(struct work_struct *work);
 
 extern int FASTCALL(schedule_work(struct work_struct *work));
-extern int FASTCALL(run_scheduled_work(struct work_struct *work));
 extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
 
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);

commit b89deed32ccc96098bd6bc953c64bba6b847774f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:33:52 2007 -0700

    implement flush_work()
    
    A basic problem with flush_scheduled_work() is that it blocks behind _all_
    presently-queued works, rather than just the work whcih the caller wants to
    flush.  If the caller holds some lock, and if one of the queued work happens
    to want that lock as well then accidental deadlocks can occur.
    
    One example of this is the phy layer: it wants to flush work while holding
    rtnl_lock().  But if a linkwatch event happens to be queued, the phy code will
    deadlock because the linkwatch callback function takes rtnl_lock.
    
    So we implement a new function which will flush a *single* work - just the one
    which the caller wants to free up.  Thus we avoid the accidental deadlocks
    which can arise from unrelated subsystems' callbacks taking shared locks.
    
    flush_work() non-blockingly dequeues the work_struct which we want to kill,
    then it waits for its handler to complete on all CPUs.
    
    Add ->current_work to the "struct cpu_workqueue_struct", it points to
    currently running "struct work_struct". When flush_work(work) detects
    ->current_work == work, it inserts a barrier at the _head_ of ->worklist
    (and thus right _after_ that work) and waits for completition. This means
    that the next work fired on that CPU will be this barrier, or another
    barrier queued by concurrent flush_work(), so the caller of flush_work()
    will be woken before any "regular" work has a chance to run.
    
    When wait_on_work() unlocks workqueue_mutex (or whatever we choose to protect
    against CPU hotplug), CPU may go away. But in that case take_over_work() will
    move a barrier we queued to another CPU, it will be fired sometime, and
    wait_on_work() will be woken.
    
    Actually, we are doing cleanup_workqueue_thread()->kthread_stop() before
    take_over_work(), so cwq->thread should complete its ->worklist (and thus
    the barrier), because currently we don't check kthread_should_stop() in
    run_workqueue(). But even if we did, everything should be ok.
    
    [akpm@osdl.org: cleanup]
    [akpm@osdl.org: add flush_work_keventd() wrapper]
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f16ba1e0687d..26a70992dec8 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -178,6 +178,8 @@ extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct delay
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 	struct delayed_work *work, unsigned long delay);
 extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
+extern void flush_work(struct workqueue_struct *wq, struct work_struct *work);
+extern void flush_work_keventd(struct work_struct *work);
 
 extern int FASTCALL(schedule_work(struct work_struct *work));
 extern int FASTCALL(run_scheduled_work(struct work_struct *work));
@@ -199,7 +201,7 @@ int execute_in_process_context(work_func_t fn, struct execute_work *);
  * Kill off a pending schedule_delayed_work().  Note that the work callback
  * function may still be running on return from cancel_delayed_work(), unless
  * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
- * cancel_work_sync() to wait on it.
+ * flush_work() or cancel_work_sync() to wait on it.
  */
 static inline int cancel_delayed_work(struct delayed_work *work)
 {

commit 28287033e12463c8ff89f1ea8038783d0360391c
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Tue May 8 00:27:47 2007 -0700

    Add a new deferrable delayed work init
    
    Add a new deferrable delayed work init.  This can be used to schedule work
    that are 'unimportant' when CPU is idle and can be called later, when CPU
    eventually comes out of idle.
    
    Use this init in cpufreq ondemand governor.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Dave Jones <davej@codemonkey.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index b8abfc74d038..f16ba1e0687d 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -121,6 +121,12 @@ struct execute_work {
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
+#define INIT_DELAYED_WORK_DEFERRABLE(_work, _func)			\
+	do {							\
+		INIT_WORK(&(_work)->work, (_func));		\
+		init_timer_deferrable(&(_work)->timer);		\
+	} while (0)
+
 /**
  * work_pending - Find out whether a work item is currently pending
  * @work: The work item in question

commit 071b638689464c6b39407025eedd810d5b5e6f5d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Apr 26 15:45:32 2007 -0700

    [WORKQUEUE]: cancel_delayed_work: use del_timer() instead of del_timer_sync()
    
    del_timer_sync() buys nothing for cancel_delayed_work(), but it is less
    efficient since it locks the timer unconditionally, and may wait for the
    completion of the delayed_work_timer_fn().
    
    cancel_delayed_work() == 0 means:
    
            before this patch:
                    work->func may still be running or queued
    
            after this patch:
                    work->func may still be running or queued, or
                    delayed_work_timer_fn->__queue_work() in progress.
    
                    The latter doesn't differ from the caller's POV,
                    delayed_work_timer_fn() is called with _PENDING
                    bit set.
    
    cancel_delayed_work() == 1 with this patch adds a new possibility:
    
            delayed_work->work was cancelled, but delayed_work_timer_fn
            is still running (this is only possible for the re-arming
            works on single-threaded workqueue).
    
            In this case the timer was re-started by work->func(), nobody
            else can do this. This in turn means that delayed_work_timer_fn
            has already passed __queue_work() (and wont't touch delayed_work)
            because nobody else can queue delayed_work->work.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 2a7b38d87018..b8abfc74d038 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -191,14 +191,15 @@ int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback
- * function may still be running on return from cancel_delayed_work().  Run
- * flush_scheduled_work() to wait on it.
+ * function may still be running on return from cancel_delayed_work(), unless
+ * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
+ * cancel_work_sync() to wait on it.
  */
 static inline int cancel_delayed_work(struct delayed_work *work)
 {
 	int ret;
 
-	ret = del_timer_sync(&work->timer);
+	ret = del_timer(&work->timer);
 	if (ret)
 		work_release(&work->work);
 	return ret;

commit a08727bae727fc2ca3a6ee9506d77786b71070b3
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Sat Dec 16 09:53:50 2006 -0800

    Make workqueue bit operations work on "atomic_long_t"
    
    On architectures where the atomicity of the bit operations is handled by
    external means (ie a separate spinlock to protect concurrent accesses),
    just doing a direct assignment on the workqueue data field (as done by
    commit 4594bf159f1962cec3b727954b7c598b07e2e737) can cause the
    assignment to be lost due to lack of serialization with the bitops on
    the same word.
    
    So we need to serialize the assignment with the locks on those
    architectures (notably older ARM chips, PA-RISC and sparc32).
    
    So rather than using an "unsigned long", let's use "atomic_long_t",
    which already has a safe assignment operation (atomic_long_set()) on
    such architectures.
    
    This requires that the atomic operations use the same atomicity locks as
    the bit operations do, but that is largely the case anyway.  Sparc32
    will probably need fixing.
    
    Architectures (including modern ARM with LL/SC) that implement sane
    atomic operations for SMP won't see any of this matter.
    
    Cc: Russell King <rmk+lkml@arm.linux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David Miller <davem@davemloft.com>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Linux Arch Maintainers <linux-arch@vger.kernel.org>
    Cc: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 5b13dcf02714..2a7b38d87018 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -8,16 +8,21 @@
 #include <linux/timer.h>
 #include <linux/linkage.h>
 #include <linux/bitops.h>
+#include <asm/atomic.h>
 
 struct workqueue_struct;
 
 struct work_struct;
 typedef void (*work_func_t)(struct work_struct *work);
 
+/*
+ * The first word is the work queue pointer and the flags rolled into
+ * one
+ */
+#define work_data_bits(work) ((unsigned long *)(&(work)->data))
+
 struct work_struct {
-	/* the first word is the work queue pointer and the flags rolled into
-	 * one */
-	unsigned long management;
+	atomic_long_t data;
 #define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
 #define WORK_STRUCT_NOAUTOREL 1		/* F if work item automatically released on exec */
 #define WORK_STRUCT_FLAG_MASK (3UL)
@@ -26,6 +31,9 @@ struct work_struct {
 	work_func_t func;
 };
 
+#define WORK_DATA_INIT(autorelease) \
+	ATOMIC_LONG_INIT((autorelease) << WORK_STRUCT_NOAUTOREL)
+
 struct delayed_work {
 	struct work_struct work;
 	struct timer_list timer;
@@ -36,13 +44,13 @@ struct execute_work {
 };
 
 #define __WORK_INITIALIZER(n, f) {				\
-	.management = 0,					\
+	.data = WORK_DATA_INIT(0),				\
         .entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
 	}
 
 #define __WORK_INITIALIZER_NAR(n, f) {				\
-	.management = (1 << WORK_STRUCT_NOAUTOREL),		\
+	.data = WORK_DATA_INIT(1),				\
         .entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
 	}
@@ -82,17 +90,21 @@ struct execute_work {
 
 /*
  * initialize all of a work item in one go
+ *
+ * NOTE! No point in using "atomic_long_set()": useing a direct
+ * assignment of the work data initializer allows the compiler
+ * to generate better code.
  */
 #define INIT_WORK(_work, _func)					\
 	do {							\
-		(_work)->management = 0;			\
+		(_work)->data = (atomic_long_t) WORK_DATA_INIT(0);	\
 		INIT_LIST_HEAD(&(_work)->entry);		\
 		PREPARE_WORK((_work), (_func));			\
 	} while (0)
 
 #define INIT_WORK_NAR(_work, _func)					\
 	do {								\
-		(_work)->management = (1 << WORK_STRUCT_NOAUTOREL);	\
+		(_work)->data = (atomic_long_t) WORK_DATA_INIT(1);	\
 		INIT_LIST_HEAD(&(_work)->entry);			\
 		PREPARE_WORK((_work), (_func));				\
 	} while (0)
@@ -114,7 +126,7 @@ struct execute_work {
  * @work: The work item in question
  */
 #define work_pending(work) \
-	test_bit(WORK_STRUCT_PENDING, &(work)->management)
+	test_bit(WORK_STRUCT_PENDING, work_data_bits(work))
 
 /**
  * delayed_work_pending - Find out whether a delayable work item is currently
@@ -143,7 +155,7 @@ struct execute_work {
  * This should also be used to release a delayed work item.
  */
 #define work_release(work) \
-	clear_bit(WORK_STRUCT_PENDING, &(work)->management)
+	clear_bit(WORK_STRUCT_PENDING, work_data_bits(work))
 
 
 extern struct workqueue_struct *__create_workqueue(const char *name,
@@ -188,7 +200,7 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 
 	ret = del_timer_sync(&work->timer);
 	if (ret)
-		clear_bit(WORK_STRUCT_PENDING, &work->work.management);
+		work_release(&work->work);
 	return ret;
 }
 

commit 0221872a3b0aa2fa2f3fa60affcbaebd662c4a90
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Fri Dec 15 14:13:51 2006 -0800

    Fix "delayed_work_pending()" macro expansion
    
    Nobody uses it, but it was still wrong.  Using the macro argument name
    'work' meant that when we used 'work' as a member name, that would also
    get replaced by the macro argument.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index edef8d50b26b..5b13dcf02714 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -121,8 +121,8 @@ struct execute_work {
  * pending
  * @work: The work item in question
  */
-#define delayed_work_pending(work) \
-	test_bit(WORK_STRUCT_PENDING, &(work)->work.management)
+#define delayed_work_pending(w) \
+	work_pending(&(w)->work)
 
 /**
  * work_release - Release a work item under execution

commit 68380b581383c028830f79ec2670f4a193854aa6
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Thu Dec 7 09:28:19 2006 -0800

    Add "run_scheduled_work()" workqueue function
    
    This allows workqueue users to run just their own pending work, rather
    than wait for the whole workqueue to finish running.  This solves the
    deadlock with networking libphy that was due to other workqueue entries
    possibly needing a lock that was held by the routine that wanted to
    flush its own work.
    
    It's not wonderful: if you absolutely need to synchronize with the work
    function having been executed, any user strictly speaking should have
    its own completion tracking logic, since when we run things explicitly
    by hand, the generic workqueue layer can no longer help us synchronize.
    
    Also, this is strictly only usable for work that has been scheduled
    without any delayed timers.  You can not mix the new interface with
    schedule_delayed_work().
    
    But it's better than what we had currently.
    
    Acked-by: Maciej W. Rozycki <macro@linux-mips.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index f0cb1df7b475..edef8d50b26b 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -162,6 +162,7 @@ extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
 
 extern int FASTCALL(schedule_work(struct work_struct *work));
+extern int FASTCALL(run_scheduled_work(struct work_struct *work));
 extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
 
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);

commit 341a595850dac1b0503df34260257d71b4fdf72c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Dec 6 20:34:49 2006 -0800

    [PATCH] Support for freezeable workqueues
    
    Make it possible to create a workqueue the worker thread of which will be
    frozen during suspend, along with other kernel threads.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Cc: Nigel Cunningham <nigel@suspend2.net>
    Cc: David Chinner <dgc@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 4a3ea83c6d16..f0cb1df7b475 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -147,9 +147,11 @@ struct execute_work {
 
 
 extern struct workqueue_struct *__create_workqueue(const char *name,
-						    int singlethread);
-#define create_workqueue(name) __create_workqueue((name), 0)
-#define create_singlethread_workqueue(name) __create_workqueue((name), 1)
+						    int singlethread,
+						    int freezeable);
+#define create_workqueue(name) __create_workqueue((name), 0, 0)
+#define create_freezeable_workqueue(name) __create_workqueue((name), 0, 1)
+#define create_singlethread_workqueue(name) __create_workqueue((name), 1, 0)
 
 extern void destroy_workqueue(struct workqueue_struct *wq);
 

commit 65f27f38446e1976cc98fd3004b110fedcddd189
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:55:48 2006 +0000

    WorkStruct: Pass the work_struct pointer instead of context data
    
    Pass the work_struct pointer to the work function rather than context data.
    The work function can use container_of() to work out the data.
    
    For the cases where the container of the work_struct may go away the moment the
    pending bit is cleared, it is made possible to defer the release of the
    structure by deferring the clearing of the pending bit.
    
    To make this work, an extra flag is introduced into the management side of the
    work_struct.  This governs auto-release of the structure upon execution.
    
    Ordinarily, the work queue executor would release the work_struct for further
    scheduling or deallocation by clearing the pending bit prior to jumping to the
    work function.  This means that, unless the driver makes some guarantee itself
    that the work_struct won't go away, the work function may not access anything
    else in the work_struct or its container lest they be deallocated..  This is a
    problem if the auxiliary data is taken away (as done by the last patch).
    
    However, if the pending bit is *not* cleared before jumping to the work
    function, then the work function *may* access the work_struct and its container
    with no problems.  But then the work function must itself release the
    work_struct by calling work_release().
    
    In most cases, automatic release is fine, so this is the default.  Special
    initiators exist for the non-auto-release case (ending in _NAR).
    
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ecc017d24cf3..4a3ea83c6d16 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -11,18 +11,19 @@
 
 struct workqueue_struct;
 
-typedef void (*work_func_t)(void *data);
+struct work_struct;
+typedef void (*work_func_t)(struct work_struct *work);
 
 struct work_struct {
-	/* the first word is the work queue pointer and the pending flag
-	 * rolled into one */
+	/* the first word is the work queue pointer and the flags rolled into
+	 * one */
 	unsigned long management;
 #define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
+#define WORK_STRUCT_NOAUTOREL 1		/* F if work item automatically released on exec */
 #define WORK_STRUCT_FLAG_MASK (3UL)
 #define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
 	work_func_t func;
-	void *data;
 };
 
 struct delayed_work {
@@ -34,48 +35,77 @@ struct execute_work {
 	struct work_struct work;
 };
 
-#define __WORK_INITIALIZER(n, f, d) {				\
+#define __WORK_INITIALIZER(n, f) {				\
+	.management = 0,					\
         .entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
-	.data = (d),						\
 	}
 
-#define __DELAYED_WORK_INITIALIZER(n, f, d) {			\
-	.work = __WORK_INITIALIZER((n).work, (f), (d)),		\
+#define __WORK_INITIALIZER_NAR(n, f) {				\
+	.management = (1 << WORK_STRUCT_NOAUTOREL),		\
+        .entry	= { &(n).entry, &(n).entry },			\
+	.func = (f),						\
+	}
+
+#define __DELAYED_WORK_INITIALIZER(n, f) {			\
+	.work = __WORK_INITIALIZER((n).work, (f)),		\
+	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
+	}
+
+#define __DELAYED_WORK_INITIALIZER_NAR(n, f) {			\
+	.work = __WORK_INITIALIZER_NAR((n).work, (f)),		\
 	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
 	}
 
-#define DECLARE_WORK(n, f, d)					\
-	struct work_struct n = __WORK_INITIALIZER(n, f, d)
+#define DECLARE_WORK(n, f)					\
+	struct work_struct n = __WORK_INITIALIZER(n, f)
+
+#define DECLARE_WORK_NAR(n, f)					\
+	struct work_struct n = __WORK_INITIALIZER_NAR(n, f)
 
-#define DECLARE_DELAYED_WORK(n, f, d)				\
-	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, d)
+#define DECLARE_DELAYED_WORK(n, f)				\
+	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f)
+
+#define DECLARE_DELAYED_WORK_NAR(n, f)			\
+	struct dwork_struct n = __DELAYED_WORK_INITIALIZER_NAR(n, f)
 
 /*
- * initialize a work item's function and data pointers
+ * initialize a work item's function pointer
  */
-#define PREPARE_WORK(_work, _func, _data)			\
+#define PREPARE_WORK(_work, _func)				\
 	do {							\
 		(_work)->func = (_func);			\
-		(_work)->data = (_data);			\
 	} while (0)
 
-#define PREPARE_DELAYED_WORK(_work, _func, _data)		\
-	PREPARE_WORK(&(_work)->work, (_func), (_data))
+#define PREPARE_DELAYED_WORK(_work, _func)			\
+	PREPARE_WORK(&(_work)->work, (_func))
 
 /*
  * initialize all of a work item in one go
  */
-#define INIT_WORK(_work, _func, _data)				\
+#define INIT_WORK(_work, _func)					\
 	do {							\
-		INIT_LIST_HEAD(&(_work)->entry);		\
 		(_work)->management = 0;			\
-		PREPARE_WORK((_work), (_func), (_data));	\
+		INIT_LIST_HEAD(&(_work)->entry);		\
+		PREPARE_WORK((_work), (_func));			\
+	} while (0)
+
+#define INIT_WORK_NAR(_work, _func)					\
+	do {								\
+		(_work)->management = (1 << WORK_STRUCT_NOAUTOREL);	\
+		INIT_LIST_HEAD(&(_work)->entry);			\
+		PREPARE_WORK((_work), (_func));				\
+	} while (0)
+
+#define INIT_DELAYED_WORK(_work, _func)				\
+	do {							\
+		INIT_WORK(&(_work)->work, (_func));		\
+		init_timer(&(_work)->timer);			\
 	} while (0)
 
-#define INIT_DELAYED_WORK(_work, _func, _data)		\
+#define INIT_DELAYED_WORK_NAR(_work, _func)			\
 	do {							\
-		INIT_WORK(&(_work)->work, (_func), (_data));	\
+		INIT_WORK_NAR(&(_work)->work, (_func));		\
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
@@ -94,6 +124,27 @@ struct execute_work {
 #define delayed_work_pending(work) \
 	test_bit(WORK_STRUCT_PENDING, &(work)->work.management)
 
+/**
+ * work_release - Release a work item under execution
+ * @work: The work item to release
+ *
+ * This is used to release a work item that has been initialised with automatic
+ * release mode disabled (WORK_STRUCT_NOAUTOREL is set).  This gives the work
+ * function the opportunity to grab auxiliary data from the container of the
+ * work_struct before clearing the pending bit as the work_struct may be
+ * subject to deallocation the moment the pending bit is cleared.
+ *
+ * In such a case, this should be called in the work function after it has
+ * fetched any data it may require from the containter of the work_struct.
+ * After this function has been called, the work_struct may be scheduled for
+ * further execution or it may be deallocated unless other precautions are
+ * taken.
+ *
+ * This should also be used to release a delayed work item.
+ */
+#define work_release(work) \
+	clear_bit(WORK_STRUCT_PENDING, &(work)->management)
+
 
 extern struct workqueue_struct *__create_workqueue(const char *name,
 						    int singlethread);
@@ -112,7 +163,7 @@ extern int FASTCALL(schedule_work(struct work_struct *work));
 extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
 
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);
-extern int schedule_on_each_cpu(work_func_t func, void *info);
+extern int schedule_on_each_cpu(work_func_t func);
 extern void flush_scheduled_work(void);
 extern int current_is_keventd(void);
 extern int keventd_up(void);
@@ -121,7 +172,7 @@ extern void init_workqueues(void);
 void cancel_rearming_delayed_work(struct delayed_work *work);
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
 				       struct delayed_work *);
-int execute_in_process_context(work_func_t fn, void *, struct execute_work *);
+int execute_in_process_context(work_func_t fn, struct execute_work *);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback

commit 365970a1ea76d81cb1ad2f652acb605f06dae256
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:54:49 2006 +0000

    WorkStruct: Merge the pending bit into the wq_data pointer
    
    Reclaim a word from the size of the work_struct by folding the pending bit and
    the wq_data pointer together.  This shouldn't cause misalignment problems as
    all pointers should be at least 4-byte aligned.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index cef40b22ff9a..ecc017d24cf3 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -14,11 +14,15 @@ struct workqueue_struct;
 typedef void (*work_func_t)(void *data);
 
 struct work_struct {
-	unsigned long pending;
+	/* the first word is the work queue pointer and the pending flag
+	 * rolled into one */
+	unsigned long management;
+#define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
+#define WORK_STRUCT_FLAG_MASK (3UL)
+#define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
 	struct list_head entry;
 	work_func_t func;
 	void *data;
-	void *wq_data;
 };
 
 struct delayed_work {
@@ -65,7 +69,7 @@ struct execute_work {
 #define INIT_WORK(_work, _func, _data)				\
 	do {							\
 		INIT_LIST_HEAD(&(_work)->entry);		\
-		(_work)->pending = 0;				\
+		(_work)->management = 0;			\
 		PREPARE_WORK((_work), (_func), (_data));	\
 	} while (0)
 
@@ -75,6 +79,21 @@ struct execute_work {
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
+/**
+ * work_pending - Find out whether a work item is currently pending
+ * @work: The work item in question
+ */
+#define work_pending(work) \
+	test_bit(WORK_STRUCT_PENDING, &(work)->management)
+
+/**
+ * delayed_work_pending - Find out whether a delayable work item is currently
+ * pending
+ * @work: The work item in question
+ */
+#define delayed_work_pending(work) \
+	test_bit(WORK_STRUCT_PENDING, &(work)->work.management)
+
 
 extern struct workqueue_struct *__create_workqueue(const char *name,
 						    int singlethread);
@@ -115,7 +134,7 @@ static inline int cancel_delayed_work(struct delayed_work *work)
 
 	ret = del_timer_sync(&work->timer);
 	if (ret)
-		clear_bit(0, &work->work.pending);
+		clear_bit(WORK_STRUCT_PENDING, &work->work.management);
 	return ret;
 }
 

commit 6bb49e5965c1fc399b4d3cd2b5cf2da535b330c0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:54:45 2006 +0000

    WorkStruct: Typedef the work function prototype
    
    Define a type for the work function prototype.  It's not only kept in the
    work_struct struct, it's also passed as an argument to several functions.
    
    This makes it easier to change it.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 9faaccae570e..cef40b22ff9a 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -11,10 +11,12 @@
 
 struct workqueue_struct;
 
+typedef void (*work_func_t)(void *data);
+
 struct work_struct {
 	unsigned long pending;
 	struct list_head entry;
-	void (*func)(void *);
+	work_func_t func;
 	void *data;
 	void *wq_data;
 };
@@ -91,7 +93,7 @@ extern int FASTCALL(schedule_work(struct work_struct *work));
 extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
 
 extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);
-extern int schedule_on_each_cpu(void (*func)(void *info), void *info);
+extern int schedule_on_each_cpu(work_func_t func, void *info);
 extern void flush_scheduled_work(void);
 extern int current_is_keventd(void);
 extern int keventd_up(void);
@@ -100,8 +102,7 @@ extern void init_workqueues(void);
 void cancel_rearming_delayed_work(struct delayed_work *work);
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
 				       struct delayed_work *);
-int execute_in_process_context(void (*fn)(void *), void *,
-			       struct execute_work *);
+int execute_in_process_context(work_func_t fn, void *, struct execute_work *);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback

commit 52bad64d95bd89e08c49ec5a071fa6dcbe5a1a9c
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:54:01 2006 +0000

    WorkStruct: Separate delayable and non-delayable events.
    
    Separate delayable work items from non-delayable work items be splitting them
    into a separate structure (delayed_work), which incorporates a work_struct and
    the timer_list removed from work_struct.
    
    The work_struct struct is huge, and this limits it's usefulness.  On a 64-bit
    architecture it's nearly 100 bytes in size.  This reduces that by half for the
    non-delayable type of event.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 9bca3539a1e5..9faaccae570e 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -17,6 +17,10 @@ struct work_struct {
 	void (*func)(void *);
 	void *data;
 	void *wq_data;
+};
+
+struct delayed_work {
+	struct work_struct work;
 	struct timer_list timer;
 };
 
@@ -28,32 +32,48 @@ struct execute_work {
         .entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
 	.data = (d),						\
+	}
+
+#define __DELAYED_WORK_INITIALIZER(n, f, d) {			\
+	.work = __WORK_INITIALIZER((n).work, (f), (d)),		\
 	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
 	}
 
 #define DECLARE_WORK(n, f, d)					\
 	struct work_struct n = __WORK_INITIALIZER(n, f, d)
 
+#define DECLARE_DELAYED_WORK(n, f, d)				\
+	struct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, d)
+
 /*
- * initialize a work-struct's func and data pointers:
+ * initialize a work item's function and data pointers
  */
 #define PREPARE_WORK(_work, _func, _data)			\
 	do {							\
-		(_work)->func = _func;				\
-		(_work)->data = _data;				\
+		(_work)->func = (_func);			\
+		(_work)->data = (_data);			\
 	} while (0)
 
+#define PREPARE_DELAYED_WORK(_work, _func, _data)		\
+	PREPARE_WORK(&(_work)->work, (_func), (_data))
+
 /*
- * initialize all of a work-struct:
+ * initialize all of a work item in one go
  */
 #define INIT_WORK(_work, _func, _data)				\
 	do {							\
 		INIT_LIST_HEAD(&(_work)->entry);		\
 		(_work)->pending = 0;				\
 		PREPARE_WORK((_work), (_func), (_data));	\
+	} while (0)
+
+#define INIT_DELAYED_WORK(_work, _func, _data)		\
+	do {							\
+		INIT_WORK(&(_work)->work, (_func), (_data));	\
 		init_timer(&(_work)->timer);			\
 	} while (0)
 
+
 extern struct workqueue_struct *__create_workqueue(const char *name,
 						    int singlethread);
 #define create_workqueue(name) __create_workqueue((name), 0)
@@ -62,24 +82,24 @@ extern struct workqueue_struct *__create_workqueue(const char *name,
 extern void destroy_workqueue(struct workqueue_struct *wq);
 
 extern int FASTCALL(queue_work(struct workqueue_struct *wq, struct work_struct *work));
-extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, unsigned long delay));
+extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct delayed_work *work, unsigned long delay));
 extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
-	struct work_struct *work, unsigned long delay);
+	struct delayed_work *work, unsigned long delay);
 extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
 
 extern int FASTCALL(schedule_work(struct work_struct *work));
-extern int FASTCALL(schedule_delayed_work(struct work_struct *work, unsigned long delay));
+extern int FASTCALL(schedule_delayed_work(struct delayed_work *work, unsigned long delay));
 
-extern int schedule_delayed_work_on(int cpu, struct work_struct *work, unsigned long delay);
+extern int schedule_delayed_work_on(int cpu, struct delayed_work *work, unsigned long delay);
 extern int schedule_on_each_cpu(void (*func)(void *info), void *info);
 extern void flush_scheduled_work(void);
 extern int current_is_keventd(void);
 extern int keventd_up(void);
 
 extern void init_workqueues(void);
-void cancel_rearming_delayed_work(struct work_struct *work);
+void cancel_rearming_delayed_work(struct delayed_work *work);
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
-				       struct work_struct *);
+				       struct delayed_work *);
 int execute_in_process_context(void (*fn)(void *), void *,
 			       struct execute_work *);
 
@@ -88,13 +108,13 @@ int execute_in_process_context(void (*fn)(void *), void *,
  * function may still be running on return from cancel_delayed_work().  Run
  * flush_scheduled_work() to wait on it.
  */
-static inline int cancel_delayed_work(struct work_struct *work)
+static inline int cancel_delayed_work(struct delayed_work *work)
 {
 	int ret;
 
 	ret = del_timer_sync(&work->timer);
 	if (ret)
-		clear_bit(0, &work->pending);
+		clear_bit(0, &work->work.pending);
 	return ret;
 }
 

commit 7a6bc1cdd506cf81f856f0fef4e56a2ba0c5a26d
Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Wed Jun 28 13:50:33 2006 -0700

    [CPUFREQ] Add queue_delayed_work_on() interface for workqueues.
    
    Add queue_delayed_work_on() interface for workqueues.
    
    Signed-off-by: Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 957c21c16d62..9bca3539a1e5 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -63,6 +63,8 @@ extern void destroy_workqueue(struct workqueue_struct *wq);
 
 extern int FASTCALL(queue_work(struct workqueue_struct *wq, struct work_struct *work));
 extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, unsigned long delay));
+extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
+	struct work_struct *work, unsigned long delay);
 extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
 
 extern int FASTCALL(schedule_work(struct work_struct *work));

commit 1fa44ecad2b86475e038aed81b0bf333fa484f8b
Author: James Bottomley <James.Bottomley@steeleye.com>
Date:   Thu Feb 23 12:43:43 2006 -0600

    [SCSI] add execute_in_process_context() API
    
    We have several points in the SCSI stack (primarily for our device
    functions) where we need to guarantee process context, but (given the
    place where the last reference was released) we cannot guarantee this.
    
    This API gets around the issue by executing the function directly if
    the caller has process context, but scheduling a workqueue to execute
    in process context if the caller doesn't have it.
    
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 86b111300231..957c21c16d62 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -20,6 +20,10 @@ struct work_struct {
 	struct timer_list timer;
 };
 
+struct execute_work {
+	struct work_struct work;
+};
+
 #define __WORK_INITIALIZER(n, f, d) {				\
         .entry	= { &(n).entry, &(n).entry },			\
 	.func = (f),						\
@@ -74,6 +78,8 @@ extern void init_workqueues(void);
 void cancel_rearming_delayed_work(struct work_struct *work);
 void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
 				       struct work_struct *);
+int execute_in_process_context(void (*fn)(void *), void *,
+			       struct execute_work *);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback

commit 15316ba81aee6775d6079fb46c66c801989e7d10
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Sun Jan 8 01:00:43 2006 -0800

    [PATCH] add schedule_on_each_cpu()
    
    swap migration's isolate_lru_page() currently uses an IPI to notify other
    processors that the lru caches need to be drained if the page cannot be
    found on the LRU.  The IPI interrupt may interrupt a processor that is just
    processing lru requests and cause a race condition.
    
    This patch introduces a new function run_on_each_cpu() that uses the
    keventd() to run the LRU draining on each processor.  Processors disable
    preemption when dealing the LRU caches (these are per processor) and thus
    executing LRU draining from another process is safe.
    
    Thanks to Lee Schermerhorn <lee.schermerhorn@hp.com> for finding this race
    condition.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ac39d04d027c..86b111300231 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -65,6 +65,7 @@ extern int FASTCALL(schedule_work(struct work_struct *work));
 extern int FASTCALL(schedule_delayed_work(struct work_struct *work, unsigned long delay));
 
 extern int schedule_delayed_work_on(int cpu, struct work_struct *work, unsigned long delay);
+extern int schedule_on_each_cpu(void (*func)(void *info), void *info);
 extern void flush_scheduled_work(void);
 extern int current_is_keventd(void);
 extern int keventd_up(void);

commit 81ddef77bb774e771db8588b937665cd38f40cee
Author: James Bottomley <James.Bottomley@SteelEye.com>
Date:   Sat Apr 16 15:23:59 2005 -0700

    [PATCH] re-export cancel_rearming_delayed_workqueue
    
    This was unexported by Arjan because we have no current users.
    
    However, during a conversion from tasklets to workqueues of the parisc led
    functions, we ran across a case where this was needed.  In particular, the
    open coded equivalent of cancel_rearming_delayed_workqueue was implemented
    incorrectly, which is, I think, all the evidence necessary that this is a
    useful API.
    
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index ff46f537ba9b..ac39d04d027c 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -71,6 +71,8 @@ extern int keventd_up(void);
 
 extern void init_workqueues(void);
 void cancel_rearming_delayed_work(struct work_struct *work);
+void cancel_rearming_delayed_workqueue(struct workqueue_struct *,
+				       struct work_struct *);
 
 /*
  * Kill off a pending schedule_delayed_work().  Note that the work callback

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
new file mode 100644
index 000000000000..ff46f537ba9b
--- /dev/null
+++ b/include/linux/workqueue.h
@@ -0,0 +1,90 @@
+/*
+ * workqueue.h --- work queue handling for Linux.
+ */
+
+#ifndef _LINUX_WORKQUEUE_H
+#define _LINUX_WORKQUEUE_H
+
+#include <linux/timer.h>
+#include <linux/linkage.h>
+#include <linux/bitops.h>
+
+struct workqueue_struct;
+
+struct work_struct {
+	unsigned long pending;
+	struct list_head entry;
+	void (*func)(void *);
+	void *data;
+	void *wq_data;
+	struct timer_list timer;
+};
+
+#define __WORK_INITIALIZER(n, f, d) {				\
+        .entry	= { &(n).entry, &(n).entry },			\
+	.func = (f),						\
+	.data = (d),						\
+	.timer = TIMER_INITIALIZER(NULL, 0, 0),			\
+	}
+
+#define DECLARE_WORK(n, f, d)					\
+	struct work_struct n = __WORK_INITIALIZER(n, f, d)
+
+/*
+ * initialize a work-struct's func and data pointers:
+ */
+#define PREPARE_WORK(_work, _func, _data)			\
+	do {							\
+		(_work)->func = _func;				\
+		(_work)->data = _data;				\
+	} while (0)
+
+/*
+ * initialize all of a work-struct:
+ */
+#define INIT_WORK(_work, _func, _data)				\
+	do {							\
+		INIT_LIST_HEAD(&(_work)->entry);		\
+		(_work)->pending = 0;				\
+		PREPARE_WORK((_work), (_func), (_data));	\
+		init_timer(&(_work)->timer);			\
+	} while (0)
+
+extern struct workqueue_struct *__create_workqueue(const char *name,
+						    int singlethread);
+#define create_workqueue(name) __create_workqueue((name), 0)
+#define create_singlethread_workqueue(name) __create_workqueue((name), 1)
+
+extern void destroy_workqueue(struct workqueue_struct *wq);
+
+extern int FASTCALL(queue_work(struct workqueue_struct *wq, struct work_struct *work));
+extern int FASTCALL(queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, unsigned long delay));
+extern void FASTCALL(flush_workqueue(struct workqueue_struct *wq));
+
+extern int FASTCALL(schedule_work(struct work_struct *work));
+extern int FASTCALL(schedule_delayed_work(struct work_struct *work, unsigned long delay));
+
+extern int schedule_delayed_work_on(int cpu, struct work_struct *work, unsigned long delay);
+extern void flush_scheduled_work(void);
+extern int current_is_keventd(void);
+extern int keventd_up(void);
+
+extern void init_workqueues(void);
+void cancel_rearming_delayed_work(struct work_struct *work);
+
+/*
+ * Kill off a pending schedule_delayed_work().  Note that the work callback
+ * function may still be running on return from cancel_delayed_work().  Run
+ * flush_scheduled_work() to wait on it.
+ */
+static inline int cancel_delayed_work(struct work_struct *work)
+{
+	int ret;
+
+	ret = del_timer_sync(&work->timer);
+	if (ret)
+		clear_bit(0, &work->pending);
+	return ret;
+}
+
+#endif
