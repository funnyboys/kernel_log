commit e616cb8daadf637175af4fe53138a94c190c4816
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Feb 24 22:14:51 2020 +0100

    lockdep: Always inline lockdep_{off,on}()
    
    These functions are called {early,late} in nmi_{enter,exit} and should
    not be traced or probed. They are also puny, so 'inline' them.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134101.048523500@linutronix.de

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 206774ac6946..8fce5c98a4b0 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -308,8 +308,27 @@ extern void lockdep_set_selftest_task(struct task_struct *task);
 
 extern void lockdep_init_task(struct task_struct *task);
 
-extern void lockdep_off(void);
-extern void lockdep_on(void);
+/*
+ * Split the recrursion counter in two to readily detect 'off' vs recursion.
+ */
+#define LOCKDEP_RECURSION_BITS	16
+#define LOCKDEP_OFF		(1U << LOCKDEP_RECURSION_BITS)
+#define LOCKDEP_RECURSION_MASK	(LOCKDEP_OFF - 1)
+
+/*
+ * lockdep_{off,on}() are macros to avoid tracing and kprobes; not inlines due
+ * to header dependencies.
+ */
+
+#define lockdep_off()					\
+do {							\
+	current->lockdep_recursion += LOCKDEP_OFF;	\
+} while (0)
+
+#define lockdep_on()					\
+do {							\
+	current->lockdep_recursion -= LOCKDEP_OFF;	\
+} while (0)
 
 extern void lockdep_register_key(struct lock_class_key *key);
 extern void lockdep_unregister_key(struct lock_class_key *key);

commit 8bf6c677ddb9c922423ea3bf494fe7c508bfbb8c
Author: Sebastian Siewior <bigeasy@linutronix.de>
Date:   Mon Mar 23 16:20:19 2020 +0100

    completion: Use lockdep_assert_RT_in_threaded_ctx() in complete_all()
    
    The warning was intended to spot complete_all() users from hardirq
    context on PREEMPT_RT. The warning as-is will also trigger in interrupt
    handlers, which are threaded on PREEMPT_RT, which was not intended.
    
    Use lockdep_assert_RT_in_threaded_ctx() which triggers in non-preemptive
    context on PREEMPT_RT.
    
    Fixes: a5c6234e1028 ("completion: Use simple wait queues")
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200323152019.4qjwluldohuh3by5@linutronix.de

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 425b4ceb7cd0..206774ac6946 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -711,6 +711,21 @@ do {									\
 # define lockdep_assert_in_irq() do { } while (0)
 #endif
 
+#ifdef CONFIG_PROVE_RAW_LOCK_NESTING
+
+# define lockdep_assert_RT_in_threaded_ctx() do {			\
+		WARN_ONCE(debug_locks && !current->lockdep_recursion &&	\
+			  current->hardirq_context &&			\
+			  !(current->hardirq_threaded || current->irq_config),	\
+			  "Not in threaded context on PREEMPT_RT as expected\n");	\
+} while (0)
+
+#else
+
+# define lockdep_assert_RT_in_threaded_ctx() do { } while (0)
+
+#endif
+
 #ifdef CONFIG_LOCKDEP
 void lockdep_rcu_suspicious(const char *file, const int line, const char *s);
 #else

commit de8f5e4f2dc1f032b46afda0a78cab5456974f89
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Mar 21 12:26:01 2020 +0100

    lockdep: Introduce wait-type checks
    
    Extend lockdep to validate lock wait-type context.
    
    The current wait-types are:
    
            LD_WAIT_FREE,           /* wait free, rcu etc.. */
            LD_WAIT_SPIN,           /* spin loops, raw_spinlock_t etc.. */
            LD_WAIT_CONFIG,         /* CONFIG_PREEMPT_LOCK, spinlock_t etc.. */
            LD_WAIT_SLEEP,          /* sleeping locks, mutex_t etc.. */
    
    Where lockdep validates that the current lock (the one being acquired)
    fits in the current wait-context (as generated by the held stack).
    
    This ensures that there is no attempt to acquire mutexes while holding
    spinlocks, to acquire spinlocks while holding raw_spinlocks and so on. In
    other words, its a more fancy might_sleep().
    
    Obviously RCU made the entire ordeal more complex than a simple single
    value test because RCU can be acquired in (pretty much) any context and
    while it presents a context to nested locks it is not the same as it
    got acquired in.
    
    Therefore its necessary to split the wait_type into two values, one
    representing the acquire (outer) and one representing the nested context
    (inner). For most 'normal' locks these two are the same.
    
    [ To make static initialization easier we have the rule that:
      .outer == INV means .outer == .inner; because INV == 0. ]
    
    It further means that its required to find the minimal .inner of the held
    stack to compare against the outer of the new lock; because while 'normal'
    RCU presents a CONFIG type to nested locks, if it is taken while already
    holding a SPIN type it obviously doesn't relax the rules.
    
    Below is an example output generated by the trivial test code:
    
      raw_spin_lock(&foo);
      spin_lock(&bar);
      spin_unlock(&bar);
      raw_spin_unlock(&foo);
    
     [ BUG: Invalid wait context ]
     -----------------------------
     swapper/0/1 is trying to lock:
     ffffc90000013f20 (&bar){....}-{3:3}, at: kernel_init+0xdb/0x187
     other info that might help us debug this:
     1 lock held by swapper/0/1:
      #0: ffffc90000013ee0 (&foo){+.+.}-{2:2}, at: kernel_init+0xd1/0x187
    
    The way to read it is to look at the new -{n,m} part in the lock
    description; -{3:3} for the attempted lock, and try and match that up to
    the held locks, which in this case is the one: -{2,2}.
    
    This tells that the acquiring lock requires a more relaxed environment than
    presented by the lock stack.
    
    Currently only the normal locks and RCU are converted, the rest of the
    lockdep users defaults to .inner = INV which is ignored. More conversions
    can be done when desired.
    
    The check for spinlock_t nesting is not enabled by default. It's a separate
    config option for now as there are known problems which are currently
    addressed. The config option allows to identify these problems and to
    verify that the solutions found are indeed solving them.
    
    The config switch will be removed and the checks will permanently enabled
    once the vast majority of issues has been addressed.
    
    [ bigeasy: Move LD_WAIT_FREE,â€¦ out of CONFIG_LOCKDEP to avoid compile
               failure with CONFIG_DEBUG_SPINLOCK + !CONFIG_LOCKDEP]
    [ tglx: Add the config option ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.427089655@linutronix.de

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 664f52c6dd4c..425b4ceb7cd0 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -21,6 +21,22 @@ extern int lock_stat;
 
 #include <linux/types.h>
 
+enum lockdep_wait_type {
+	LD_WAIT_INV = 0,	/* not checked, catch all */
+
+	LD_WAIT_FREE,		/* wait free, rcu etc.. */
+	LD_WAIT_SPIN,		/* spin loops, raw_spinlock_t etc.. */
+
+#ifdef CONFIG_PROVE_RAW_LOCK_NESTING
+	LD_WAIT_CONFIG,		/* CONFIG_PREEMPT_LOCK, spinlock_t etc.. */
+#else
+	LD_WAIT_CONFIG = LD_WAIT_SPIN,
+#endif
+	LD_WAIT_SLEEP,		/* sleeping locks, mutex_t etc.. */
+
+	LD_WAIT_MAX,		/* must be last */
+};
+
 #ifdef CONFIG_LOCKDEP
 
 #include <linux/linkage.h>
@@ -111,6 +127,9 @@ struct lock_class {
 	int				name_version;
 	const char			*name;
 
+	short				wait_type_inner;
+	short				wait_type_outer;
+
 #ifdef CONFIG_LOCK_STAT
 	unsigned long			contention_point[LOCKSTAT_POINTS];
 	unsigned long			contending_point[LOCKSTAT_POINTS];
@@ -158,6 +177,8 @@ struct lockdep_map {
 	struct lock_class_key		*key;
 	struct lock_class		*class_cache[NR_LOCKDEP_CACHING_CLASSES];
 	const char			*name;
+	short				wait_type_outer; /* can be taken in this context */
+	short				wait_type_inner; /* presents this context */
 #ifdef CONFIG_LOCK_STAT
 	int				cpu;
 	unsigned long			ip;
@@ -299,8 +320,21 @@ extern void lockdep_unregister_key(struct lock_class_key *key);
  * to lockdep:
  */
 
-extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
-			     struct lock_class_key *key, int subclass);
+extern void lockdep_init_map_waits(struct lockdep_map *lock, const char *name,
+	struct lock_class_key *key, int subclass, short inner, short outer);
+
+static inline void
+lockdep_init_map_wait(struct lockdep_map *lock, const char *name,
+		      struct lock_class_key *key, int subclass, short inner)
+{
+	lockdep_init_map_waits(lock, name, key, subclass, inner, LD_WAIT_INV);
+}
+
+static inline void lockdep_init_map(struct lockdep_map *lock, const char *name,
+			     struct lock_class_key *key, int subclass)
+{
+	lockdep_init_map_wait(lock, name, key, subclass, LD_WAIT_INV);
+}
 
 /*
  * Reinitialize a lock key - for cases where there is special locking or
@@ -308,18 +342,29 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
  * of dependencies wrong: they are either too broad (they need a class-split)
  * or they are too narrow (they suffer from a false class-split):
  */
-#define lockdep_set_class(lock, key) \
-		lockdep_init_map(&(lock)->dep_map, #key, key, 0)
-#define lockdep_set_class_and_name(lock, key, name) \
-		lockdep_init_map(&(lock)->dep_map, name, key, 0)
-#define lockdep_set_class_and_subclass(lock, key, sub) \
-		lockdep_init_map(&(lock)->dep_map, #key, key, sub)
-#define lockdep_set_subclass(lock, sub)	\
-		lockdep_init_map(&(lock)->dep_map, #lock, \
-				 (lock)->dep_map.key, sub)
+#define lockdep_set_class(lock, key)				\
+	lockdep_init_map_waits(&(lock)->dep_map, #key, key, 0,	\
+			       (lock)->dep_map.wait_type_inner,	\
+			       (lock)->dep_map.wait_type_outer)
+
+#define lockdep_set_class_and_name(lock, key, name)		\
+	lockdep_init_map_waits(&(lock)->dep_map, name, key, 0,	\
+			       (lock)->dep_map.wait_type_inner,	\
+			       (lock)->dep_map.wait_type_outer)
+
+#define lockdep_set_class_and_subclass(lock, key, sub)		\
+	lockdep_init_map_waits(&(lock)->dep_map, #key, key, sub,\
+			       (lock)->dep_map.wait_type_inner,	\
+			       (lock)->dep_map.wait_type_outer)
+
+#define lockdep_set_subclass(lock, sub)					\
+	lockdep_init_map_waits(&(lock)->dep_map, #lock, (lock)->dep_map.key, sub,\
+			       (lock)->dep_map.wait_type_inner,		\
+			       (lock)->dep_map.wait_type_outer)
 
 #define lockdep_set_novalidate_class(lock) \
 	lockdep_set_class_and_name(lock, &__lockdep_no_validate__, #lock)
+
 /*
  * Compare locking classes
  */
@@ -432,6 +477,10 @@ static inline void lockdep_set_selftest_task(struct task_struct *task)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
+# define lockdep_init_map_waits(lock, name, key, sub, inner, outer) \
+		do { (void)(name); (void)(key); } while (0)
+# define lockdep_init_map_wait(lock, name, key, sub, inner) \
+		do { (void)(name); (void)(key); } while (0)
 # define lockdep_init_map(lock, name, key, sub) \
 		do { (void)(name); (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)

commit 023265ed75d8792ca1d555430a8985511d3f8788
Merge: 2b68392e638d e42617b825f8
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Wed Dec 11 10:35:37 2019 +0200

    Merge drm/drm-next into drm-intel-next-queued
    
    Sync up with v5.5-rc1 to get the updated lock_release() API among other
    things. Fix the conflict reported by Stephen Rothwell [1].
    
    [1] http://lore.kernel.org/r/20191210093957.5120f717@canb.auug.org.au
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit e692b4021a2e48745d4bdac2b2775bdc8f03b433
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Mon Nov 4 18:37:19 2019 +0100

    lockdep: add might_lock_nested()
    
    Necessary to annotate functions where we might acquire a
    mutex_lock_nested() or similar. Needed by i915.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191104173720.2696-2-daniel.vetter@ffwll.ch

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b8a835fd611b..827722c21797 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -628,6 +628,13 @@ do {									\
 	lock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);	\
 	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
 } while (0)
+# define might_lock_nested(lock, subclass) 				\
+do {									\
+	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
+	lock_acquire(&(lock)->dep_map, subclass, 0, 1, 1, NULL,		\
+		     _THIS_IP_);					\
+	lock_release(&(lock)->dep_map, 0, _THIS_IP_);		\
+} while (0)
 
 #define lockdep_assert_irqs_enabled()	do {				\
 		WARN_ONCE(debug_locks && !current->lockdep_recursion &&	\
@@ -650,6 +657,7 @@ do {									\
 #else
 # define might_lock(lock) do { } while (0)
 # define might_lock_read(lock) do { } while (0)
+# define might_lock_nested(lock, subclass) do { } while (0)
 # define lockdep_assert_irqs_enabled() do { } while (0)
 # define lockdep_assert_irqs_disabled() do { } while (0)
 # define lockdep_assert_in_irq() do { } while (0)

commit 5facae4f3549b5cf7c0e10ec312a65ffd43b5726
Author: Qian Cai <cai@lca.pw>
Date:   Thu Sep 19 12:09:40 2019 -0400

    locking/lockdep: Remove unused @nested argument from lock_release()
    
    Since the following commit:
    
      b4adfe8e05f1 ("locking/lockdep: Remove unused argument in __lock_release")
    
    @nested is no longer used in lock_release(), so remove it from all
    lock_release() calls and friends.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: alexander.levin@microsoft.com
    Cc: daniel@iogearbox.net
    Cc: davem@davemloft.net
    Cc: dri-devel@lists.freedesktop.org
    Cc: duyuyang@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: hannes@cmpxchg.org
    Cc: intel-gfx@lists.freedesktop.org
    Cc: jack@suse.com
    Cc: jlbec@evilplan.or
    Cc: joonas.lahtinen@linux.intel.com
    Cc: joseph.qi@linux.alibaba.com
    Cc: jslaby@suse.com
    Cc: juri.lelli@redhat.com
    Cc: maarten.lankhorst@linux.intel.com
    Cc: mark@fasheh.com
    Cc: mhocko@kernel.org
    Cc: mripard@kernel.org
    Cc: ocfs2-devel@oss.oracle.com
    Cc: rodrigo.vivi@intel.com
    Cc: sean@poorly.run
    Cc: st@kernel.org
    Cc: tj@kernel.org
    Cc: tytso@mit.edu
    Cc: vdavydov.dev@gmail.com
    Cc: vincent.guittot@linaro.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1568909380-32199-1-git-send-email-cai@lca.pw
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b8a835fd611b..c50d01ef1414 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -349,8 +349,7 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 			 int trylock, int read, int check,
 			 struct lockdep_map *nest_lock, unsigned long ip);
 
-extern void lock_release(struct lockdep_map *lock, int nested,
-			 unsigned long ip);
+extern void lock_release(struct lockdep_map *lock, unsigned long ip);
 
 /*
  * Same "read" as for lock_acquire(), except -1 means any.
@@ -428,7 +427,7 @@ static inline void lockdep_set_selftest_task(struct task_struct *task)
 }
 
 # define lock_acquire(l, s, t, r, c, n, i)	do { } while (0)
-# define lock_release(l, n, i)			do { } while (0)
+# define lock_release(l, i)			do { } while (0)
 # define lock_downgrade(l, i)			do { } while (0)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
@@ -591,42 +590,42 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #define spin_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define spin_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
-#define spin_release(l, n, i)			lock_release(l, n, i)
+#define spin_release(l, i)			lock_release(l, i)
 
 #define rwlock_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_recursive(l, s, t, NULL, i)
-#define rwlock_release(l, n, i)			lock_release(l, n, i)
+#define rwlock_release(l, i)			lock_release(l, i)
 
 #define seqcount_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define seqcount_acquire_read(l, s, t, i)	lock_acquire_shared_recursive(l, s, t, NULL, i)
-#define seqcount_release(l, n, i)		lock_release(l, n, i)
+#define seqcount_release(l, i)			lock_release(l, i)
 
 #define mutex_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define mutex_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
-#define mutex_release(l, n, i)			lock_release(l, n, i)
+#define mutex_release(l, i)			lock_release(l, i)
 
 #define rwsem_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
 #define rwsem_acquire_read(l, s, t, i)		lock_acquire_shared(l, s, t, NULL, i)
-#define rwsem_release(l, n, i)			lock_release(l, n, i)
+#define rwsem_release(l, i)			lock_release(l, i)
 
 #define lock_map_acquire(l)			lock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)
 #define lock_map_acquire_read(l)		lock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)
 #define lock_map_acquire_tryread(l)		lock_acquire_shared_recursive(l, 0, 1, NULL, _THIS_IP_)
-#define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
+#define lock_map_release(l)			lock_release(l, _THIS_IP_)
 
 #ifdef CONFIG_PROVE_LOCKING
 # define might_lock(lock) 						\
 do {									\
 	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
 	lock_acquire(&(lock)->dep_map, 0, 0, 0, 1, NULL, _THIS_IP_);	\
-	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
+	lock_release(&(lock)->dep_map, _THIS_IP_);			\
 } while (0)
 # define might_lock_read(lock) 						\
 do {									\
 	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
 	lock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);	\
-	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
+	lock_release(&(lock)->dep_map, _THIS_IP_);			\
 } while (0)
 
 #define lockdep_assert_irqs_enabled()	do {				\

commit 12593b7467f9130b64a6d4b6a26ed4ec217b6784
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Jul 22 11:24:42 2019 -0700

    locking/lockdep: Reduce space occupied by stack traces
    
    Although commit 669de8bda87b ("kernel/workqueue: Use dynamic lockdep keys
    for workqueues") unregisters dynamic lockdep keys when a workqueue is
    destroyed, a side effect of that commit is that all stack traces
    associated with the lockdep key are leaked when a workqueue is destroyed.
    Fix this by storing each unique stack trace once. Other changes in this
    patch are:
    
    - Use NULL instead of { .nr_entries = 0 } to represent 'no trace'.
    - Store a pointer to a stack trace in struct lock_class and struct
      lock_list instead of storing 'nr_entries' and 'offset'.
    
    This patch avoids that the following program triggers the "BUG:
    MAX_STACK_TRACE_ENTRIES too low!" complaint:
    
            #include <fcntl.h>
            #include <unistd.h>
    
            int main()
            {
                    for (;;) {
                            int fd = open("/dev/infiniband/rdma_cm", O_RDWR);
                            close(fd);
                    }
            }
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Reported-by: Eric Biggers <ebiggers@kernel.org>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yuyang Du <duyuyang@gmail.com>
    Link: https://lkml.kernel.org/r/20190722182443.216015-4-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index cdb3c2f06092..b8a835fd611b 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -66,10 +66,7 @@ struct lock_class_key {
 
 extern struct lock_class_key __lockdep_no_validate__;
 
-struct lock_trace {
-	unsigned int		nr_entries;
-	unsigned int		offset;
-};
+struct lock_trace;
 
 #define LOCKSTAT_POINTS		4
 
@@ -105,7 +102,7 @@ struct lock_class {
 	 * IRQ/softirq usage tracking bits:
 	 */
 	unsigned long			usage_mask;
-	struct lock_trace		usage_traces[XXX_LOCK_USAGE_STATES];
+	const struct lock_trace		*usage_traces[XXX_LOCK_USAGE_STATES];
 
 	/*
 	 * Generation counter, when doing certain classes of graph walking,
@@ -193,7 +190,7 @@ struct lock_list {
 	struct list_head		entry;
 	struct lock_class		*class;
 	struct lock_class		*links_to;
-	struct lock_trace		trace;
+	const struct lock_trace		*trace;
 	int				distance;
 
 	/*

commit 364f6afc4f5537b79cf454eb35cae92920676075
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Jul 22 11:24:40 2019 -0700

    locking/lockdep: Make it clear that what lock_class::key points at is not modified
    
    This patch does not change the behavior of the lockdep code.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://lkml.kernel.org/r/20190722182443.216015-2-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 0b0d7259276d..cdb3c2f06092 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -97,7 +97,7 @@ struct lock_class {
 	 */
 	struct list_head		locks_after, locks_before;
 
-	struct lockdep_subclass_key	*key;
+	const struct lockdep_subclass_key *key;
 	unsigned int			subclass;
 	unsigned int			dep_gen_id;
 

commit 387b14684f94483cbbb72843db406ec9a8d0d6d2
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Wed Apr 10 08:32:41 2019 -0300

    docs: locking: convert docs to ReST and rename to *.rst
    
    Convert the locking documents to ReST and add them to the
    kernel development book where it belongs.
    
    Most of the stuff here is just to make Sphinx to properly
    parse the text file, as they're already in good shape,
    not requiring massive changes in order to be parsed.
    
    The conversion is actually:
      - add blank lines and identation in order to identify paragraphs;
      - fix tables markups;
      - add some lists markups;
      - mark literal blocks;
      - adjust title markups.
    
    At its new index.rst, let's add a :orphan: while this is not linked to
    the main index.rst file, in order to avoid build warnings.
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Federico Vaga <federico.vaga@vaga.pv.it>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 57baa27f238c..0b0d7259276d 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -5,7 +5,7 @@
  *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
  *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
  *
- * see Documentation/locking/lockdep-design.txt for more details.
+ * see Documentation/locking/lockdep-design.rst for more details.
  */
 #ifndef __LINUX_LOCKDEP_H
 #define __LINUX_LOCKDEP_H

commit e1928328699a582a540b105e5f4c160832a7fdcb
Merge: 46f1ec23a469 9156e545765e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:12:03 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - rwsem scalability improvements, phase #2, by Waiman Long, which are
         rather impressive:
    
           "On a 2-socket 40-core 80-thread Skylake system with 40 reader
            and writer locking threads, the min/mean/max locking operations
            done in a 5-second testing window before the patchset were:
    
             40 readers, Iterations Min/Mean/Max = 1,807/1,808/1,810
             40 writers, Iterations Min/Mean/Max = 1,807/50,344/151,255
    
            After the patchset, they became:
    
             40 readers, Iterations Min/Mean/Max = 30,057/31,359/32,741
             40 writers, Iterations Min/Mean/Max = 94,466/95,845/97,098"
    
         There's a lot of changes to the locking implementation that makes
         it similar to qrwlock, including owner handoff for more fair
         locking.
    
         Another microbenchmark shows how across the spectrum the
         improvements are:
    
           "With a locking microbenchmark running on 5.1 based kernel, the
            total locking rates (in kops/s) on a 2-socket Skylake system
            with equal numbers of readers and writers (mixed) before and
            after this patchset were:
    
            # of Threads   Before Patch      After Patch
            ------------   ------------      -----------
                 2            2,618             4,193
                 4            1,202             3,726
                 8              802             3,622
                16              729             3,359
                32              319             2,826
                64              102             2,744"
    
         The changes are extensive and the patch-set has been through
         several iterations addressing various locking workloads. There
         might be more regressions, but unless they are pathological I
         believe we want to use this new implementation as the baseline
         going forward.
    
       - jump-label optimizations by Daniel Bristot de Oliveira: the primary
         motivation was to remove IPI disturbance of isolated RT-workload
         CPUs, which resulted in the implementation of batched jump-label
         updates. Beyond the improvement of the real-time characteristics
         kernel, in one test this patchset improved static key update
         overhead from 57 msecs to just 1.4 msecs - which is a nice speedup
         as well.
    
       - atomic64_t cross-arch type cleanups by Mark Rutland: over the last
         ~10 years of atomic64_t existence the various types used by the
         APIs only had to be self-consistent within each architecture -
         which means they became wildly inconsistent across architectures.
         Mark puts and end to this by reworking all the atomic64
         implementations to use 's64' as the base type for atomic64_t, and
         to ensure that this type is consistently used for parameters and
         return values in the API, avoiding further problems in this area.
    
       - A large set of small improvements to lockdep by Yuyang Du: type
         cleanups, output cleanups, function return type and othr cleanups
         all around the place.
    
       - A set of percpu ops cleanups and fixes by Peter Zijlstra.
    
       - Misc other changes - please see the Git log for more details"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (82 commits)
      locking/lockdep: increase size of counters for lockdep statistics
      locking/atomics: Use sed(1) instead of non-standard head(1) option
      locking/lockdep: Move mark_lock() inside CONFIG_TRACE_IRQFLAGS && CONFIG_PROVE_LOCKING
      x86/jump_label: Make tp_vec_nr static
      x86/percpu: Optimize raw_cpu_xchg()
      x86/percpu, sched/fair: Avoid local_clock()
      x86/percpu, x86/irq: Relax {set,get}_irq_regs()
      x86/percpu: Relax smp_processor_id()
      x86/percpu: Differentiate this_cpu_{}() and __this_cpu_{}()
      locking/rwsem: Guard against making count negative
      locking/rwsem: Adaptive disabling of reader optimistic spinning
      locking/rwsem: Enable time-based spinning on reader-owned rwsem
      locking/rwsem: Make rwsem->owner an atomic_long_t
      locking/rwsem: Enable readers spinning on writer
      locking/rwsem: Clarify usage of owner's nonspinaable bit
      locking/rwsem: Wake up almost all readers in wait queue
      locking/rwsem: More optimal RT task handling of null owner
      locking/rwsem: Always release wait_lock before waking up tasks
      locking/rwsem: Implement lock handoff to prevent lock starvation
      locking/rwsem: Make rwsem_spin_on_owner() return owner state
      ...

commit 9ffbe8ac05dbb4ab4a4836a55a47fc6be945a38f
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri May 31 13:06:51 2019 +0300

    locking/lockdep: Rename lockdep_assert_held_exclusive() -> lockdep_assert_held_write()
    
    All callers of lockdep_assert_held_exclusive() use it to verify the
    correct locking state of either a semaphore (ldisc_sem in tty,
    mmap_sem for perf events, i_rwsem of inode for dax) or rwlock by
    apparmor. Thus it makes sense to rename _exclusive to _write since
    that's the semantics callers care. Additionally there is already
    lockdep_assert_held_read(), which this new naming is more consistent with.
    
    No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190531100651.3969-1-nborisov@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 30a0f81aa130..151d55711082 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -394,7 +394,7 @@ extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 		WARN_ON(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
 
-#define lockdep_assert_held_exclusive(l)	do {			\
+#define lockdep_assert_held_write(l)	do {			\
 		WARN_ON(debug_locks && !lockdep_is_held_type(l, 0));	\
 	} while (0)
 
@@ -479,7 +479,7 @@ struct lockdep_map { };
 #define lockdep_is_held_type(l, r)		(1)
 
 #define lockdep_assert_held(l)			do { (void)(l); } while (0)
-#define lockdep_assert_held_exclusive(l)	do { (void)(l); } while (0)
+#define lockdep_assert_held_write(l)	do { (void)(l); } while (0)
 #define lockdep_assert_held_read(l)		do { (void)(l); } while (0)
 #define lockdep_assert_held_once(l)		do { (void)(l); } while (0)
 

commit 01bb6f0af992a1e6b7797d92fd31a7864872e347
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:25 2019 +0800

    locking/lockdep: Change the range of class_idx in held_lock struct
    
    held_lock->class_idx is used to point to the class of the held lock. The
    index is shifted by 1 to make index 0 mean no class, which results in class
    index shifting back and forth but is not worth doing so.
    
    The reason is: (1) there will be no "no-class" held_lock to begin with, and
    (2) index 0 seems to be used for error checking, but if something wrong
    indeed happened, the index can't be counted on to distinguish it as that
    something won't set the class_idx to 0 on purpose to tell us it is wrong.
    
    Therefore, change the index to start from 0. This saves a lot of
    back-and-forth shifts and a class slot back to lock_classes.
    
    Since index 0 is now used for lock class, we change the initial chain key to
    -1 to avoid key collision, which is due to the fact that __jhash_mix(0, 0, 0) = 0.
    Actually, the initial chain key can be any arbitrary value other than 0.
    
    In addition, a bitmap is maintained to keep track of the used lock classes,
    and we check the validity of the held lock against that bitmap.
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-10-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index d4e69595dbd4..30a0f81aa130 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -223,13 +223,8 @@ struct lock_chain {
 };
 
 #define MAX_LOCKDEP_KEYS_BITS		13
-/*
- * Subtract one because we offset hlock->class_idx by 1 in order
- * to make 0 mean no class. This avoids overflowing the class_idx
- * bitfield and hitting the BUG in hlock_class().
- */
-#define MAX_LOCKDEP_KEYS		((1UL << MAX_LOCKDEP_KEYS_BITS) - 1)
-#define INITIAL_CHAIN_KEY		0
+#define MAX_LOCKDEP_KEYS		(1UL << MAX_LOCKDEP_KEYS_BITS)
+#define INITIAL_CHAIN_KEY		-1
 
 struct held_lock {
 	/*
@@ -254,6 +249,11 @@ struct held_lock {
 	u64 				waittime_stamp;
 	u64				holdtime_stamp;
 #endif
+	/*
+	 * class_idx is zero-indexed; it points to the element in
+	 * lock_classes this held lock instance belongs to. class_idx is in
+	 * the range from 0 to (MAX_LOCKDEP_KEYS-1) inclusive.
+	 */
 	unsigned int			class_idx:MAX_LOCKDEP_KEYS_BITS;
 	/*
 	 * The lock-stack is unified in that the lock chains of interrupt

commit f6ec8829ac9d59b637366c13038f15d6f6156fe1
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:24 2019 +0800

    locking/lockdep: Define INITIAL_CHAIN_KEY for chain keys to start with
    
    Chain keys are computed using Jenkins hash function, which needs an initial
    hash to start with. Dedicate a macro to make this clear and configurable. A
    later patch changes this initial chain key.
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-9-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 5d05b8149f19..d4e69595dbd4 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -229,6 +229,7 @@ struct lock_chain {
  * bitfield and hitting the BUG in hlock_class().
  */
 #define MAX_LOCKDEP_KEYS		((1UL << MAX_LOCKDEP_KEYS_BITS) - 1)
+#define INITIAL_CHAIN_KEY		0
 
 struct held_lock {
 	/*

commit e196e479a3b844da6e6e71e0d2a8694040cb4e52
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:23 2019 +0800

    locking/lockdep: Use lockdep_init_task for task initiation consistently
    
    Despite that there is a lockdep_init_task() which does nothing, lockdep
    initiates tasks by assigning lockdep fields and does so inconsistently. Fix
    this by using lockdep_init_task().
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-8-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 851d44fa5457..5d05b8149f19 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -287,6 +287,8 @@ extern void lockdep_free_key_range(void *start, unsigned long size);
 extern asmlinkage void lockdep_sys_exit(void);
 extern void lockdep_set_selftest_task(struct task_struct *task);
 
+extern void lockdep_init_task(struct task_struct *task);
+
 extern void lockdep_off(void);
 extern void lockdep_on(void);
 
@@ -411,6 +413,10 @@ extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 
 #else /* !CONFIG_LOCKDEP */
 
+static inline void lockdep_init_task(struct task_struct *task)
+{
+}
+
 static inline void lockdep_off(void)
 {
 }
@@ -503,7 +509,6 @@ enum xhlock_context_t {
 	{ .name = (_name), .key = (void *)(_key), }
 
 static inline void lockdep_invariant_state(bool force) {}
-static inline void lockdep_init_task(struct task_struct *task) {}
 static inline void lockdep_free_task(struct task_struct *task) {}
 
 #ifdef CONFIG_LOCK_STAT

commit d16dbd1b8a29bb9f8aca2c2f3bd1a0d2b7621126
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:22 2019 +0800

    locking/lockdep: Update obsolete struct field description
    
    The lock_chain struct definition has outdated comment, update it and add
    struct member description.
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-7-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 6e2377e6c1d6..851d44fa5457 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -203,11 +203,17 @@ struct lock_list {
 	struct lock_list		*parent;
 };
 
-/*
- * We record lock dependency chains, so that we can cache them:
+/**
+ * struct lock_chain - lock dependency chain record
+ *
+ * @irq_context: the same as irq_context in held_lock below
+ * @depth:       the number of held locks in this chain
+ * @base:        the index in chain_hlocks for this chain
+ * @entry:       the collided lock chains in lock_chain hash list
+ * @chain_key:   the hash key of this lock_chain
  */
 struct lock_chain {
-	/* see BUILD_BUG_ON()s in lookup_chain_cache() */
+	/* see BUILD_BUG_ON()s in add_chain_cache() */
 	unsigned int			irq_context :  2,
 					depth       :  6,
 					base	    : 24;

commit 71d8d1531e0904e08adf1540e191bd707dfd73da
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Tue Mar 26 15:24:08 2019 -0400

    lockdep: Add assertion to check if in an interrupt
    
    In rcu_rrupt_from_idle, we want to check if it is called from within an
    interrupt, but want to do such checking only for debug builds. lockdep
    already tracks when we enter an interrupt. Let us expose it as an
    assertion macro so it can be used to assert this.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: kernel-team@android.com
    Cc: rcu@vger.kernel.org
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 6e2377e6c1d6..e8eef38b2213 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -632,11 +632,18 @@ do {									\
 			  "IRQs not disabled as expected\n");		\
 	} while (0)
 
+#define lockdep_assert_in_irq() do {					\
+		WARN_ONCE(debug_locks && !current->lockdep_recursion &&	\
+			  !current->hardirq_context,			\
+			  "Not in hardirq as expected\n");		\
+	} while (0)
+
 #else
 # define might_lock(lock) do { } while (0)
 # define might_lock_read(lock) do { } while (0)
 # define lockdep_assert_irqs_enabled() do { } while (0)
 # define lockdep_assert_irqs_disabled() do { } while (0)
+# define lockdep_assert_in_irq() do { } while (0)
 #endif
 
 #ifdef CONFIG_LOCKDEP

commit 007dc78fea62610bf06829e38f1d8c69b6ea5af6
Merge: 2f1835dffa94 d671002be6bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 13:50:15 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "Here are the locking changes in this cycle:
    
       - rwsem unification and simpler micro-optimizations to prepare for
         more intrusive (and more lucrative) scalability improvements in
         v5.3 (Waiman Long)
    
       - Lockdep irq state tracking flag usage cleanups (Frederic
         Weisbecker)
    
       - static key improvements (Jakub Kicinski, Peter Zijlstra)
    
       - misc updates, cleanups and smaller fixes"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      locking/lockdep: Remove unnecessary unlikely()
      locking/static_key: Don't take sleeping locks in __static_key_slow_dec_deferred()
      locking/static_key: Factor out the fast path of static_key_slow_dec()
      locking/static_key: Add support for deferred static branches
      locking/lockdep: Test all incompatible scenarios at once in check_irq_usage()
      locking/lockdep: Avoid bogus Clang warning
      locking/lockdep: Generate LOCKF_ bit composites
      locking/lockdep: Use expanded masks on find_usage_*() functions
      locking/lockdep: Map remaining magic numbers to lock usage mask names
      locking/lockdep: Move valid_state() inside CONFIG_TRACE_IRQFLAGS && CONFIG_PROVE_LOCKING
      locking/rwsem: Prevent unneeded warning during locking selftest
      locking/rwsem: Optimize rwsem structure for uncontended lock acquisition
      locking/rwsem: Enable lock event counting
      locking/lock_events: Don't show pvqspinlock events on bare metal
      locking/lock_events: Make lock_events available for all archs & other locks
      locking/qspinlock_stat: Introduce generic lockevent_*() counting APIs
      locking/rwsem: Enhance DEBUG_RWSEMS_WARN_ON() macro
      locking/rwsem: Add debug check for __down_read*()
      locking/rwsem: Micro-optimize rwsem_try_read_lock_unqueued()
      locking/rwsem: Move rwsem internal function declarations to rwsem-xadd.h
      ...

commit c120bce78065cbea460a58b1572c215db9c148ba
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:12 2019 +0200

    lockdep: Simplify stack trace handling
    
    Replace the indirection through struct stack_trace by using the storage
    array based interfaces and storing the information is a small lockdep
    specific data structure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094802.891724020@linutronix.de

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 79c3873d58ac..6f165d625320 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -66,6 +66,11 @@ struct lock_class_key {
 
 extern struct lock_class_key __lockdep_no_validate__;
 
+struct lock_trace {
+	unsigned int		nr_entries;
+	unsigned int		offset;
+};
+
 #define LOCKSTAT_POINTS		4
 
 /*
@@ -100,7 +105,7 @@ struct lock_class {
 	 * IRQ/softirq usage tracking bits:
 	 */
 	unsigned long			usage_mask;
-	struct stack_trace		usage_traces[XXX_LOCK_USAGE_STATES];
+	struct lock_trace		usage_traces[XXX_LOCK_USAGE_STATES];
 
 	/*
 	 * Generation counter, when doing certain classes of graph walking,
@@ -188,7 +193,7 @@ struct lock_list {
 	struct list_head		entry;
 	struct lock_class		*class;
 	struct lock_class		*links_to;
-	struct stack_trace		trace;
+	struct lock_trace		trace;
 	int				distance;
 
 	/*

commit 3771b0fe9dfc3801eac0142d1af6ba94dee83c6c
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Mar 25 13:57:57 2019 +0100

    locking/lockdep: Avoid bogus Clang warning
    
    When lockdep is enabled, and -Wuninitialized warnings are enabled,
    Clang produces a silly warning for every file we compile:
    
     In file included from  kernel/sched/fair.c:23:
      kernel/sched/sched.h:1094:15: error: variable 'cookie' is uninitialized when used here [-Werror,-Wuninitialized]
             rf->cookie = lockdep_pin_lock(&rq->lock);
                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~
      include/linux/lockdep.h:474:60: note: expanded from macro 'lockdep_pin_lock'
      #define lockdep_pin_lock(l)                     ({ struct pin_cookie cookie; cookie; })
                                                                                 ^~~~~~
      kernel/sched/sched.h:1094:15: note: variable 'cookie' is declared here
      include/linux/lockdep.h:474:34: note: expanded from macro 'lockdep_pin_lock'
      #define lockdep_pin_lock(l)                     ({ struct pin_cookie cookie; cookie; })
                                                        ^
    
    As the 'struct pin_cookie' structure is empty in this configuration,
    there is no need to initialize it for correctness, but it also
    does not hurt to set it to an empty structure, so do that to
    avoid the warning.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nathan Chancellor <natechancellor@gmail.com>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: clang-built-linux@googlegroups.com
    Link: http://lkml.kernel.org/r/20190325125807.1437049-1-arnd@arndb.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 79c3873d58ac..21725a91442b 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -471,7 +471,7 @@ struct pin_cookie { };
 
 #define NIL_COOKIE (struct pin_cookie){ }
 
-#define lockdep_pin_lock(l)			({ struct pin_cookie cookie; cookie; })
+#define lockdep_pin_lock(l)			({ struct pin_cookie cookie = { }; cookie; })
 #define lockdep_repin_lock(l, c)		do { (void)(l); (void)(c); } while (0)
 #define lockdep_unpin_lock(l, c)		do { (void)(l); (void)(c); } while (0)
 

commit 28d49e282665e2a51cc91b716937fccfa24d80e1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Feb 26 18:19:09 2019 +0100

    locking/lockdep: Shrink struct lock_class_key
    
    Shrink struct lock_class_key; we never store anything in subkeys[], we
    only use the addresses.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 43fb35bd7baf..79c3873d58ac 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -58,8 +58,10 @@ struct lockdep_subclass_key {
 
 /* hash_entry is used to keep track of dynamically allocated keys. */
 struct lock_class_key {
-	struct hlist_node		hash_entry;
-	struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
+	union {
+		struct hlist_node		hash_entry;
+		struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
+	};
 };
 
 extern struct lock_class_key __lockdep_no_validate__;

commit 108c14858b9ea224686e476c8f5ec345a0df9e27
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:53 2019 -0800

    locking/lockdep: Add support for dynamic keys
    
    A shortcoming of the current lockdep implementation is that it requires
    lock keys to be allocated statically. That forces all instances of lock
    objects that occur in a given data structure to share a lock key. Since
    lock dependency analysis groups lock objects per key sharing lock keys
    can cause false positive lockdep reports. Make it possible to avoid
    such false positive reports by allowing lock keys to be allocated
    dynamically. Require that dynamically allocated lock keys are
    registered before use by calling lockdep_register_key(). Complain about
    attempts to register the same lock key pointer twice without calling
    lockdep_unregister_key() between successive registration calls.
    
    The purpose of the new lock_keys_hash[] data structure that keeps
    track of all dynamic keys is twofold:
    
      - Verify whether the lockdep_register_key() and lockdep_unregister_key()
        functions are used correctly.
    
      - Avoid that lockdep_init_map() complains when encountering a dynamically
        allocated key.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: johannes.berg@intel.com
    Cc: tj@kernel.org
    Link: https://lkml.kernel.org/r/20190214230058.196511-19-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 619ec3f26cdc..43fb35bd7baf 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -46,15 +46,19 @@ extern int lock_stat;
 #define NR_LOCKDEP_CACHING_CLASSES	2
 
 /*
- * Lock-classes are keyed via unique addresses, by embedding the
- * lockclass-key into the kernel (or module) .data section. (For
- * static locks we use the lock address itself as the key.)
+ * A lockdep key is associated with each lock object. For static locks we use
+ * the lock address itself as the key. Dynamically allocated lock objects can
+ * have a statically or dynamically allocated key. Dynamically allocated lock
+ * keys must be registered before being used and must be unregistered before
+ * the key memory is freed.
  */
 struct lockdep_subclass_key {
 	char __one_byte;
 } __attribute__ ((__packed__));
 
+/* hash_entry is used to keep track of dynamically allocated keys. */
 struct lock_class_key {
+	struct hlist_node		hash_entry;
 	struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
 };
 
@@ -273,6 +277,9 @@ extern void lockdep_set_selftest_task(struct task_struct *task);
 extern void lockdep_off(void);
 extern void lockdep_on(void);
 
+extern void lockdep_register_key(struct lock_class_key *key);
+extern void lockdep_unregister_key(struct lock_class_key *key);
+
 /*
  * These methods are used by specific locking variants (spinlocks,
  * rwlocks, mutexes and rwsems) to pass init/acquire/release events
@@ -434,6 +441,14 @@ static inline void lockdep_set_selftest_task(struct task_struct *task)
  */
 struct lock_class_key { };
 
+static inline void lockdep_register_key(struct lock_class_key *key)
+{
+}
+
+static inline void lockdep_unregister_key(struct lock_class_key *key)
+{
+}
+
 /*
  * The lockdep_map takes no space if lockdep is disabled:
  */

commit a0b0fd53e1e67639b303b15939b9c653dbe7a8c4
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:46 2019 -0800

    locking/lockdep: Free lock classes that are no longer in use
    
    Instead of leaving lock classes that are no longer in use in the
    lock_classes array, reuse entries from that array that are no longer in
    use. Maintain a linked list of free lock classes with list head
    'free_lock_class'. Only add freed lock classes to the free_lock_classes
    list after a grace period to avoid that a lock_classes[] element would
    be reused while an RCU reader is accessing it. Since the lockdep
    selftests run in a context where sleeping is not allowed and since the
    selftests require that lock resetting/zapping works with debug_locks
    off, make the behavior of lockdep_free_key_range() and
    lockdep_reset_lock() depend on whether or not these are called from
    the context of the lockdep selftests.
    
    Thanks to Peter for having shown how to modify get_pending_free()
    such that that function does not have to sleep.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: johannes.berg@intel.com
    Cc: tj@kernel.org
    Link: https://lkml.kernel.org/r/20190214230058.196511-12-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 66eee1ba0f2a..619ec3f26cdc 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -63,7 +63,8 @@ extern struct lock_class_key __lockdep_no_validate__;
 #define LOCKSTAT_POINTS		4
 
 /*
- * The lock-class itself:
+ * The lock-class itself. The order of the structure members matters.
+ * reinit_class() zeroes the key member and all subsequent members.
  */
 struct lock_class {
 	/*
@@ -72,7 +73,9 @@ struct lock_class {
 	struct hlist_node		hash_entry;
 
 	/*
-	 * global list of all lock-classes:
+	 * Entry in all_lock_classes when in use. Entry in free_lock_classes
+	 * when not in use. Instances that are being freed are on one of the
+	 * zapped_classes lists.
 	 */
 	struct list_head		lock_entry;
 
@@ -104,7 +107,7 @@ struct lock_class {
 	unsigned long			contention_point[LOCKSTAT_POINTS];
 	unsigned long			contending_point[LOCKSTAT_POINTS];
 #endif
-};
+} __no_randomize_layout;
 
 #ifdef CONFIG_LOCK_STAT
 struct lock_time {

commit cdc84d794947b5431c0a6916c303aee7114819d2
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:44 2019 -0800

    locking/lockdep: Make it easy to detect whether or not inside a selftest
    
    The patch that frees unused lock classes will modify the behavior of
    lockdep_free_key_range() and lockdep_reset_lock() depending on whether
    or not these functions are called from the context of the lockdep
    selftests. Hence make it easy to detect whether or not lockdep code
    is called from the context of a lockdep selftest.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: johannes.berg@intel.com
    Cc: tj@kernel.org
    Link: https://lkml.kernel.org/r/20190214230058.196511-10-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b5e6bfe0ae4a..66eee1ba0f2a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -265,6 +265,7 @@ extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
 extern void lockdep_free_key_range(void *start, unsigned long size);
 extern asmlinkage void lockdep_sys_exit(void);
+extern void lockdep_set_selftest_task(struct task_struct *task);
 
 extern void lockdep_off(void);
 extern void lockdep_on(void);
@@ -395,6 +396,10 @@ static inline void lockdep_on(void)
 {
 }
 
+static inline void lockdep_set_selftest_task(struct task_struct *task)
+{
+}
+
 # define lock_acquire(l, s, t, r, c, n, i)	do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
 # define lock_downgrade(l, i)			do { } while (0)

commit 86cffb80a525f7b8f969c8c79669d383e02f17d1
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:41 2019 -0800

    locking/lockdep: Make zap_class() remove all matching lock order entries
    
    Make sure that all lock order entries that refer to a class are removed
    from the list_entries[] array when a kernel module is unloaded.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: johannes.berg@intel.com
    Cc: tj@kernel.org
    Link: https://lkml.kernel.org/r/20190214230058.196511-7-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 0c38bade84b7..b5e6bfe0ae4a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -178,6 +178,7 @@ static inline void lockdep_copy_map(struct lockdep_map *to,
 struct lock_list {
 	struct list_head		entry;
 	struct lock_class		*class;
+	struct lock_class		*links_to;
 	struct stack_trace		trace;
 	int				distance;
 

commit 09329d1c2024522308ca4de977fc6bba753bab1a
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Feb 14 15:00:40 2019 -0800

    locking/lockdep: Reorder struct lock_class members
    
    This patch does not change any functionality but makes the patch that
    frees lock classes that are no longer in use easier to read.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: johannes.berg@intel.com
    Cc: tj@kernel.org
    Link: https://lkml.kernel.org/r/20190214230058.196511-6-bvanassche@acm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c5335df2372f..0c38bade84b7 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -76,6 +76,13 @@ struct lock_class {
 	 */
 	struct list_head		lock_entry;
 
+	/*
+	 * These fields represent a directed graph of lock dependencies,
+	 * to every node we attach a list of "forward" and a list of
+	 * "backward" graph nodes.
+	 */
+	struct list_head		locks_after, locks_before;
+
 	struct lockdep_subclass_key	*key;
 	unsigned int			subclass;
 	unsigned int			dep_gen_id;
@@ -86,13 +93,6 @@ struct lock_class {
 	unsigned long			usage_mask;
 	struct stack_trace		usage_traces[XXX_LOCK_USAGE_STATES];
 
-	/*
-	 * These fields represent a directed graph of lock dependencies,
-	 * to every node we attach a list of "forward" and a list of
-	 * "backward" graph nodes.
-	 */
-	struct list_head		locks_after, locks_before;
-
 	/*
 	 * Generation counter, when doing certain classes of graph walking,
 	 * to ensure that we check one node only once:

commit 2421b7f3573babfe1673a5ffee1677a5013e6df1
Author: Waiman Long <longman@redhat.com>
Date:   Mon Nov 19 13:55:10 2018 -0500

    locking/lockdep: Remove ::version from lock_class structure
    
    It turns out the version field in the lock_class structure isn't used
    anywhere. Just remove it.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: iommu@lists.linux-foundation.org
    Cc: kasan-dev@googlegroups.com
    Link: https://lkml.kernel.org/r/1542653726-5655-2-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 1fd82ff99c65..c5335df2372f 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -97,8 +97,6 @@ struct lock_class {
 	 * Generation counter, when doing certain classes of graph walking,
 	 * to ensure that we check one node only once:
 	 */
-	unsigned int			version;
-
 	int				name_version;
 	const char			*name;
 

commit 8ca2b56cd7da98fc8f8d787bb706b9d6c8674a3b
Author: Waiman Long <longman@redhat.com>
Date:   Wed Oct 3 13:07:18 2018 -0400

    locking/lockdep: Make class->ops a percpu counter and move it under CONFIG_DEBUG_LOCKDEP=y
    
    A sizable portion of the CPU cycles spent on the __lock_acquire() is used
    up by the atomic increment of the class->ops stat counter. By taking it out
    from the lock_class structure and changing it to a per-cpu per-lock-class
    counter, we can reduce the amount of cacheline contention on the class
    structure when multiple CPUs are trying to acquire locks of the same
    class simultaneously.
    
    To limit the increase in memory consumption because of the percpu nature
    of that counter, it is now put back under the CONFIG_DEBUG_LOCKDEP
    config option. So the memory consumption increase will only occur if
    CONFIG_DEBUG_LOCKDEP is defined. The lock_class structure, however,
    is reduced in size by 16 bytes on 64-bit archs after ops removal and
    a minor restructuring of the fields.
    
    This patch also fixes a bug in the increment code as the counter is of
    the 'unsigned long' type, but atomic_inc() was used to increment it.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/d66681f3-8781-9793-1dcf-2436a284550b@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b0d0b51c4d85..1fd82ff99c65 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -99,13 +99,8 @@ struct lock_class {
 	 */
 	unsigned int			version;
 
-	/*
-	 * Statistics counter:
-	 */
-	unsigned long			ops;
-
-	const char			*name;
 	int				name_version;
+	const char			*name;
 
 #ifdef CONFIG_LOCK_STAT
 	unsigned long			contention_point[LOCKSTAT_POINTS];

commit bff1b208a5d1dbb2355822ef859edcb9be0379e4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Aug 6 15:50:58 2018 -0400

    tracing: Partial revert of "tracing: Centralize preemptirq tracepoints and unify their usage"
    
    Joel Fernandes created a nice patch that cleaned up the duplicate hooks used
    by lockdep and irqsoff latency tracer. It made both use tracepoints. But it
    caused lockdep to trigger several false positives. We have not figured out
    why yet, but removing lockdep from using the trace event hooks and just call
    its helper functions directly (like it use to), makes the problem go away.
    
    This is a partial revert of the clean up patch c3bc8fd637a9 ("tracing:
    Centralize preemptirq tracepoints and unify their usage") that adds direct
    calls for lockdep, but also keeps most of the clean up done to get rid of
    the horrible preprocessor if statements.
    
    Link: http://lkml.kernel.org/r/20180806155058.5ee875f4@gandalf.local.home
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Fixes: c3bc8fd637a9 ("tracing: Centralize preemptirq tracepoints and unify their usage")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index a8113357ceeb..b0d0b51c4d85 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -267,7 +267,6 @@ struct held_lock {
  * Initialization, self-test and debugging-output methods:
  */
 extern void lockdep_init(void);
-extern void lockdep_init_early(void);
 extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
 extern void lockdep_free_key_range(void *start, unsigned long size);
@@ -408,7 +407,6 @@ static inline void lockdep_on(void)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
-# define lockdep_init_early()			do { } while (0)
 # define lockdep_init_map(lock, name, key, sub) \
 		do { (void)(name); (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)

commit c3bc8fd637a9623f5c507bd18f9677effbddf584
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Mon Jul 30 15:24:23 2018 -0700

    tracing: Centralize preemptirq tracepoints and unify their usage
    
    This patch detaches the preemptirq tracepoints from the tracers and
    keeps it separate.
    
    Advantages:
    * Lockdep and irqsoff event can now run in parallel since they no longer
    have their own calls.
    
    * This unifies the usecase of adding hooks to an irqsoff and irqson
    event, and a preemptoff and preempton event.
      3 users of the events exist:
      - Lockdep
      - irqsoff and preemptoff tracers
      - irqs and preempt trace events
    
    The unification cleans up several ifdefs and makes the code in preempt
    tracer and irqsoff tracers simpler. It gets rid of all the horrific
    ifdeferry around PROVE_LOCKING and makes configuration of the different
    users of the tracepoints more easy and understandable. It also gets rid
    of the time_* function calls from the lockdep hooks used to call into
    the preemptirq tracer which is not needed anymore. The negative delta in
    lines of code in this patch is quite large too.
    
    In the patch we introduce a new CONFIG option PREEMPTIRQ_TRACEPOINTS
    as a single point for registering probes onto the tracepoints. With
    this,
    the web of config options for preempt/irq toggle tracepoints and its
    users becomes:
    
     PREEMPT_TRACER   PREEMPTIRQ_EVENTS  IRQSOFF_TRACER PROVE_LOCKING
           |                 |     \         |           |
           \    (selects)    /      \        \ (selects) /
          TRACE_PREEMPT_TOGGLE       ----> TRACE_IRQFLAGS
                          \                  /
                           \ (depends on)   /
                         PREEMPTIRQ_TRACEPOINTS
    
    Other than the performance tests mentioned in the previous patch, I also
    ran the locking API test suite. I verified that all tests cases are
    passing.
    
    I also injected issues by not registering lockdep probes onto the
    tracepoints and I see failures to confirm that the probes are indeed
    working.
    
    This series + lockdep probes not registered (just to inject errors):
    [    0.000000]      hard-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]      soft-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]          hard-safe-A + irqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]          soft-safe-A + irqs-on/12:FAILED|FAILED|  ok  |
    [    0.000000]          hard-safe-A + irqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]          soft-safe-A + irqs-on/21:FAILED|FAILED|  ok  |
    [    0.000000]     hard-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    [    0.000000]     soft-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    
    With this series + lockdep probes registered, all locking tests pass:
    
    [    0.000000]      hard-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]      soft-irqs-on + irq-safe-A/21:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]        sirq-safe-A => hirqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]          hard-safe-A + irqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]          soft-safe-A + irqs-on/12:  ok  |  ok  |  ok  |
    [    0.000000]          hard-safe-A + irqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]          soft-safe-A + irqs-on/21:  ok  |  ok  |  ok  |
    [    0.000000]     hard-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    [    0.000000]     soft-safe-A + unsafe-B #1/123:  ok  |  ok  |  ok  |
    
    Link: http://lkml.kernel.org/r/20180730222423.196630-4-joel@joelfernandes.org
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 6fc77d4dbdcd..a8113357ceeb 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -266,7 +266,8 @@ struct held_lock {
 /*
  * Initialization, self-test and debugging-output methods:
  */
-extern void lockdep_info(void);
+extern void lockdep_init(void);
+extern void lockdep_init_early(void);
 extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
 extern void lockdep_free_key_range(void *start, unsigned long size);
@@ -406,7 +407,8 @@ static inline void lockdep_on(void)
 # define lock_downgrade(l, i)			do { } while (0)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
-# define lockdep_info()				do { } while (0)
+# define lockdep_init()				do { } while (0)
+# define lockdep_init_early()			do { } while (0)
 # define lockdep_init_map(lock, name, key, sub) \
 		do { (void)(name); (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)
@@ -532,7 +534,7 @@ do {								\
 
 #endif /* CONFIG_LOCKDEP */
 
-#ifdef CONFIG_TRACE_IRQFLAGS
+#ifdef CONFIG_PROVE_LOCKING
 extern void print_irqtrace_events(struct task_struct *curr);
 #else
 static inline void print_irqtrace_events(struct task_struct *curr)

commit 5e7481a25e90b661d1dbbba18be3fd3dfe12ec6f
Merge: b8dbf7308608 05b93801a23c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 10:44:56 2018 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes relate to making lock_is_held() et al (and external
      wrappers of them) work on const data types - this requires const
      propagation through the depths of lockdep.
    
      This removes a number of ugly type hacks the external helpers used"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      lockdep: Convert some users to const
      lockdep: Make lockdep checking constant
      lockdep: Assign lock keys on registration

commit 49f9c3552ccc30f4f98c45d94d7f9b335596913f
Merge: d8a5b80568a9 e1e871aff3de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 09:08:34 2018 -0800

    Merge tag 'init_task-20180117' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull init_task initializer cleanups from David Howells:
     "It doesn't seem useful to have the init_task in a header file rather
      than in a normal source file. We could consolidate init_task handling
      instead and expand out various macros.
    
      Here's a series of patches that consolidate init_task handling:
    
       (1) Make THREAD_SIZE available to vmlinux.lds for cris, hexagon and
           openrisc.
    
       (2) Alter the INIT_TASK_DATA linker script macro to set
           init_thread_union and init_stack rather than defining these in C.
    
           Insert init_task and init_thread_into into the init_stack area in
           the linker script as appropriate to the configuration, with
           different section markers so that they end up correctly ordered.
    
           We can then get merge ia64's init_task.c into the main one.
    
           We then have a bunch of single-use INIT_*() macros that seem only
           to be macros because they used to be used per-arch. We can then
           expand these in place of the user and get rid of a few lines and
           a lot of backslashes.
    
       (3) Expand INIT_TASK() in place.
    
       (4) Expand in place various small INIT_*() macros that are defined
           conditionally. Expand them and surround them by #if[n]def/#endif
           in the .c file as it takes fewer lines.
    
       (5) Expand INIT_SIGNALS() and INIT_SIGHAND() in place.
    
       (6) Expand INIT_STRUCT_PID in place.
    
      These macros can then be discarded"
    
    * tag 'init_task-20180117' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      Expand INIT_STRUCT_PID and remove
      Expand the INIT_SIGNALS and INIT_SIGHAND macros and remove
      Expand various INIT_* macros and remove
      Expand INIT_TASK() in init/init_task.c and remove
      Construct init thread stack in the linker script rather than by union
      openrisc: Make THREAD_SIZE available to vmlinux.lds
      hexagon: Make THREAD_SIZE available to vmlinux.lds
      cris: Make THREAD_SIZE available to vmlinux.lds

commit 08f36ff642342fb058212099757cb5d40f158c2a
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Wed Jan 17 07:14:13 2018 -0800

    lockdep: Make lockdep checking constant
    
    There are several places in the kernel which would like to pass a const
    pointer to lockdep_is_held().  Constify the entire path so nobody has to
    trick the compiler.
    
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: https://lkml.kernel.org/r/20180117151414.23686-3-willy@infradead.org

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 3251d9c0d313..864d6fc60fa6 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -337,9 +337,9 @@ extern void lock_release(struct lockdep_map *lock, int nested,
 /*
  * Same "read" as for lock_acquire(), except -1 means any.
  */
-extern int lock_is_held_type(struct lockdep_map *lock, int read);
+extern int lock_is_held_type(const struct lockdep_map *lock, int read);
 
-static inline int lock_is_held(struct lockdep_map *lock)
+static inline int lock_is_held(const struct lockdep_map *lock)
 {
 	return lock_is_held_type(lock, -1);
 }

commit 4e7e3adbba5224604b34b0d42003ff6dbdc8ddd9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand various INIT_* macros and remove
    
    Expand various INIT_* macros into the single places they're used in
    init/init_task.c and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 2e75dc34bff5..26f2ccc60669 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -367,8 +367,6 @@ extern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);
 extern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);
 extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 
-# define INIT_LOCKDEP				.lockdep_recursion = 0,
-
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
 #define lockdep_assert_held(l)	do {				\
@@ -426,7 +424,6 @@ static inline void lockdep_on(void)
  * #ifdef the call himself.
  */
 
-# define INIT_LOCKDEP
 # define lockdep_reset()		do { debug_locks = 1; } while (0)
 # define lockdep_free_key_range(start, size)	do { } while (0)
 # define lockdep_sys_exit() 			do { } while (0)

commit 527187d28569e39c5d489d6306d3b79605cf85a6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 8 17:27:19 2018 +0100

    locking/lockdep: Remove cross-release leftovers
    
    There's two cross-release leftover facilities:
    
     - the crossrelease_hist_*() irq-tracing callbacks (NOPs currently)
     - the complete_release_commit() callback (NOP as well)
    
    Remove them.
    
    Cc: David Sterba <dsterba@suse.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 2e75dc34bff5..3251d9c0d313 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -475,8 +475,6 @@ enum xhlock_context_t {
 #define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .name = (_name), .key = (void *)(_key), }
 
-static inline void crossrelease_hist_start(enum xhlock_context_t c) {}
-static inline void crossrelease_hist_end(enum xhlock_context_t c) {}
 static inline void lockdep_invariant_state(bool force) {}
 static inline void lockdep_init_task(struct task_struct *task) {}
 static inline void lockdep_free_task(struct task_struct *task) {}

commit e966eaeeb623f09975ef362c2866fae6f86844f9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 12 12:31:16 2017 +0100

    locking/lockdep: Remove the cross-release locking checks
    
    This code (CONFIG_LOCKDEP_CROSSRELEASE=y and CONFIG_LOCKDEP_COMPLETIONS=y),
    while it found a number of old bugs initially, was also causing too many
    false positives that caused people to disable lockdep - which is arguably
    a worse overall outcome.
    
    If we disable cross-release by default but keep the code upstream then
    in practice the most likely outcome is that we'll allow the situation
    to degrade gradually, by allowing entropy to introduce more and more
    false positives, until it overwhelms maintenance capacity.
    
    Another bad side effect was that people were trying to work around
    the false positives by uglifying/complicating unrelated code. There's
    a marked difference between annotating locking operations and
    uglifying good code just due to bad lock debugging code ...
    
    This gradual decrease in quality happened to a number of debugging
    facilities in the kernel, and lockdep is pretty complex already,
    so we cannot risk this outcome.
    
    Either cross-release checking can be done right with no false positives,
    or it should not be included in the upstream kernel.
    
    ( Note that it might make sense to maintain it out of tree and go through
      the false positives every now and then and see whether new bugs were
      introduced. )
    
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index a842551fe044..2e75dc34bff5 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -158,12 +158,6 @@ struct lockdep_map {
 	int				cpu;
 	unsigned long			ip;
 #endif
-#ifdef CONFIG_LOCKDEP_CROSSRELEASE
-	/*
-	 * Whether it's a crosslock.
-	 */
-	int				cross;
-#endif
 };
 
 static inline void lockdep_copy_map(struct lockdep_map *to,
@@ -267,95 +261,8 @@ struct held_lock {
 	unsigned int hardirqs_off:1;
 	unsigned int references:12;					/* 32 bits */
 	unsigned int pin_count;
-#ifdef CONFIG_LOCKDEP_CROSSRELEASE
-	/*
-	 * Generation id.
-	 *
-	 * A value of cross_gen_id will be stored when holding this,
-	 * which is globally increased whenever each crosslock is held.
-	 */
-	unsigned int gen_id;
-#endif
-};
-
-#ifdef CONFIG_LOCKDEP_CROSSRELEASE
-#define MAX_XHLOCK_TRACE_ENTRIES 5
-
-/*
- * This is for keeping locks waiting for commit so that true dependencies
- * can be added at commit step.
- */
-struct hist_lock {
-	/*
-	 * Id for each entry in the ring buffer. This is used to
-	 * decide whether the ring buffer was overwritten or not.
-	 *
-	 * For example,
-	 *
-	 *           |<----------- hist_lock ring buffer size ------->|
-	 *           pppppppppppppppppppppiiiiiiiiiiiiiiiiiiiiiiiiiiiii
-	 * wrapped > iiiiiiiiiiiiiiiiiiiiiiiiiii.......................
-	 *
-	 *           where 'p' represents an acquisition in process
-	 *           context, 'i' represents an acquisition in irq
-	 *           context.
-	 *
-	 * In this example, the ring buffer was overwritten by
-	 * acquisitions in irq context, that should be detected on
-	 * rollback or commit.
-	 */
-	unsigned int hist_id;
-
-	/*
-	 * Seperate stack_trace data. This will be used at commit step.
-	 */
-	struct stack_trace	trace;
-	unsigned long		trace_entries[MAX_XHLOCK_TRACE_ENTRIES];
-
-	/*
-	 * Seperate hlock instance. This will be used at commit step.
-	 *
-	 * TODO: Use a smaller data structure containing only necessary
-	 * data. However, we should make lockdep code able to handle the
-	 * smaller one first.
-	 */
-	struct held_lock	hlock;
 };
 
-/*
- * To initialize a lock as crosslock, lockdep_init_map_crosslock() should
- * be called instead of lockdep_init_map().
- */
-struct cross_lock {
-	/*
-	 * When more than one acquisition of crosslocks are overlapped,
-	 * we have to perform commit for them based on cross_gen_id of
-	 * the first acquisition, which allows us to add more true
-	 * dependencies.
-	 *
-	 * Moreover, when no acquisition of a crosslock is in progress,
-	 * we should not perform commit because the lock might not exist
-	 * any more, which might cause incorrect memory access. So we
-	 * have to track the number of acquisitions of a crosslock.
-	 */
-	int nr_acquire;
-
-	/*
-	 * Seperate hlock instance. This will be used at commit step.
-	 *
-	 * TODO: Use a smaller data structure containing only necessary
-	 * data. However, we should make lockdep code able to handle the
-	 * smaller one first.
-	 */
-	struct held_lock	hlock;
-};
-
-struct lockdep_map_cross {
-	struct lockdep_map map;
-	struct cross_lock xlock;
-};
-#endif
-
 /*
  * Initialization, self-test and debugging-output methods:
  */
@@ -560,37 +467,6 @@ enum xhlock_context_t {
 	XHLOCK_CTX_NR,
 };
 
-#ifdef CONFIG_LOCKDEP_CROSSRELEASE
-extern void lockdep_init_map_crosslock(struct lockdep_map *lock,
-				       const char *name,
-				       struct lock_class_key *key,
-				       int subclass);
-extern void lock_commit_crosslock(struct lockdep_map *lock);
-
-/*
- * What we essencially have to initialize is 'nr_acquire'. Other members
- * will be initialized in add_xlock().
- */
-#define STATIC_CROSS_LOCK_INIT() \
-	{ .nr_acquire = 0,}
-
-#define STATIC_CROSS_LOCKDEP_MAP_INIT(_name, _key) \
-	{ .map.name = (_name), .map.key = (void *)(_key), \
-	  .map.cross = 1, .xlock = STATIC_CROSS_LOCK_INIT(), }
-
-/*
- * To initialize a lockdep_map statically use this macro.
- * Note that _name must not be NULL.
- */
-#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
-	{ .name = (_name), .key = (void *)(_key), .cross = 0, }
-
-extern void crossrelease_hist_start(enum xhlock_context_t c);
-extern void crossrelease_hist_end(enum xhlock_context_t c);
-extern void lockdep_invariant_state(bool force);
-extern void lockdep_init_task(struct task_struct *task);
-extern void lockdep_free_task(struct task_struct *task);
-#else /* !CROSSRELEASE */
 #define lockdep_init_map_crosslock(m, n, k, s) do {} while (0)
 /*
  * To initialize a lockdep_map statically use this macro.
@@ -604,7 +480,6 @@ static inline void crossrelease_hist_end(enum xhlock_context_t c) {}
 static inline void lockdep_invariant_state(bool force) {}
 static inline void lockdep_init_task(struct task_struct *task) {}
 static inline void lockdep_free_task(struct task_struct *task) {}
-#endif /* CROSSRELEASE */
 
 #ifdef CONFIG_LOCK_STAT
 

commit f54bb2ec02c839f6bfe3e8d438cd93d30b4809dd
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Mon Nov 6 16:01:17 2017 +0100

    locking/lockdep: Add IRQs disabled/enabled assertion APIs: lockdep_assert_irqs_enabled()/disabled()
    
    Checking whether IRQs are enabled or disabled is a very common sanity
    check, however not free of overhead especially on fastpath where such
    assertion is very common.
    
    Lockdep is a good host for such concurrency correctness check and it
    even already tracks down IRQs disablement state. Just reuse its
    machinery. This will allow us to get rid of the flags pop and check
    overhead from fast path when kernel is built for production.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David S . Miller <davem@davemloft.net>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1509980490-4285-2-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 02720769c159..a842551fe044 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -725,9 +725,24 @@ do {									\
 	lock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);	\
 	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
 } while (0)
+
+#define lockdep_assert_irqs_enabled()	do {				\
+		WARN_ONCE(debug_locks && !current->lockdep_recursion &&	\
+			  !current->hardirqs_enabled,			\
+			  "IRQs not enabled as expected\n");		\
+	} while (0)
+
+#define lockdep_assert_irqs_disabled()	do {				\
+		WARN_ONCE(debug_locks && !current->lockdep_recursion &&	\
+			  current->hardirqs_enabled,			\
+			  "IRQs not disabled as expected\n");		\
+	} while (0)
+
 #else
 # define might_lock(lock) do { } while (0)
 # define might_lock_read(lock) do { } while (0)
+# define lockdep_assert_irqs_enabled() do { } while (0)
+# define lockdep_assert_irqs_disabled() do { } while (0)
 #endif
 
 #ifdef CONFIG_LOCKDEP

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index bfa8e0b0d6f1..f301d31b473c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Runtime locking correctness validator
  *

commit 6f0397d7e100f3b3978d6ebb6b2dea29ee7c4a95
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Wed Oct 25 17:55:58 2017 +0900

    locking/lockdep: Provide empty lockdep_map structure for !CONFIG_LOCKDEP
    
    After this patch the lockdep_map structure takes no space if lockdep is
    disabled, reducing the number of #ifdefs in unrelated kernel code.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: amir73il@gmail.com
    Cc: axboe@kernel.dk
    Cc: darrick.wong@oracle.com
    Cc: david@fromorbit.com
    Cc: hch@infradead.org
    Cc: idryomov@gmail.com
    Cc: johan@kernel.org
    Cc: johannes.berg@intel.com
    Cc: kernel-team@lge.com
    Cc: linux-block@vger.kernel.org
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-xfs@vger.kernel.org
    Cc: oleg@redhat.com
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/1508921765-15396-3-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index bfa8e0b0d6f1..b6662d05bcda 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -527,6 +527,11 @@ static inline void lockdep_on(void)
  */
 struct lock_class_key { };
 
+/*
+ * The lockdep_map takes no space if lockdep is disabled:
+ */
+struct lockdep_map { };
+
 #define lockdep_depth(tsk)	(0)
 
 #define lockdep_is_held_type(l, r)		(1)

commit f52be5708076b75a045ac52c6fef3fffb8300525
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 29 10:59:39 2017 +0200

    locking/lockdep: Untangle xhlock history save/restore from task independence
    
    Where XHLOCK_{SOFT,HARD} are save/restore points in the xhlocks[] to
    ensure the temporal IRQ events don't interact with task state, the
    XHLOCK_PROC is a fundament different beast that just happens to share
    the interface.
    
    The purpose of XHLOCK_PROC is to annotate independent execution inside
    one task. For example workqueues, each work should appear to run in its
    own 'pristine' 'task'.
    
    Remove XHLOCK_PROC in favour of its own interface to avoid confusion.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: david@fromorbit.com
    Cc: johannes@sipsolutions.net
    Cc: kernel-team@lge.com
    Cc: oleg@redhat.com
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/20170829085939.ggmb6xiohw67micb@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 78bb7133abed..bfa8e0b0d6f1 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -551,7 +551,6 @@ struct pin_cookie { };
 enum xhlock_context_t {
 	XHLOCK_HARD,
 	XHLOCK_SOFT,
-	XHLOCK_PROC,
 	XHLOCK_CTX_NR,
 };
 
@@ -580,8 +579,9 @@ extern void lock_commit_crosslock(struct lockdep_map *lock);
 #define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .name = (_name), .key = (void *)(_key), .cross = 0, }
 
-extern void crossrelease_hist_start(enum xhlock_context_t c, bool force);
+extern void crossrelease_hist_start(enum xhlock_context_t c);
 extern void crossrelease_hist_end(enum xhlock_context_t c);
+extern void lockdep_invariant_state(bool force);
 extern void lockdep_init_task(struct task_struct *task);
 extern void lockdep_free_task(struct task_struct *task);
 #else /* !CROSSRELEASE */
@@ -593,8 +593,9 @@ extern void lockdep_free_task(struct task_struct *task);
 #define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .name = (_name), .key = (void *)(_key), }
 
-static inline void crossrelease_hist_start(enum xhlock_context_t c, bool force) {}
+static inline void crossrelease_hist_start(enum xhlock_context_t c) {}
 static inline void crossrelease_hist_end(enum xhlock_context_t c) {}
+static inline void lockdep_invariant_state(bool force) {}
 static inline void lockdep_init_task(struct task_struct *task) {}
 static inline void lockdep_free_task(struct task_struct *task) {}
 #endif /* CROSSRELEASE */

commit e6f3faa734a00c606b7b06c6b9f15e5627d3245b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 23 13:23:30 2017 +0200

    locking/lockdep: Fix workqueue crossrelease annotation
    
    The new completion/crossrelease annotations interact unfavourable with
    the extant flush_work()/flush_workqueue() annotations.
    
    The problem is that when a single work class does:
    
      wait_for_completion(&C)
    
    and
    
      complete(&C)
    
    in different executions, we'll build dependencies like:
    
      lock_map_acquire(W)
      complete_acquire(C)
    
    and
    
      lock_map_acquire(W)
      complete_release(C)
    
    which results in the dependency chain: W->C->W, which lockdep thinks
    spells deadlock, even though there is no deadlock potential since
    works are ran concurrently.
    
    One possibility would be to change the work 'lock' to recursive-read,
    but that would mean hitting a lockdep limitation on recursive locks.
    Also, unconditinoally switching to recursive-read here would fail to
    detect the actual deadlock on single-threaded workqueues, which do
    have a problem with this.
    
    For now, forcefully disregard these locks for crossrelease.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: byungchul.park@lge.com
    Cc: david@fromorbit.com
    Cc: johannes@sipsolutions.net
    Cc: oleg@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index fc827cab6d6e..78bb7133abed 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -18,6 +18,8 @@ extern int lock_stat;
 
 #define MAX_LOCKDEP_SUBCLASSES		8UL
 
+#include <linux/types.h>
+
 #ifdef CONFIG_LOCKDEP
 
 #include <linux/linkage.h>
@@ -578,11 +580,11 @@ extern void lock_commit_crosslock(struct lockdep_map *lock);
 #define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .name = (_name), .key = (void *)(_key), .cross = 0, }
 
-extern void crossrelease_hist_start(enum xhlock_context_t c);
+extern void crossrelease_hist_start(enum xhlock_context_t c, bool force);
 extern void crossrelease_hist_end(enum xhlock_context_t c);
 extern void lockdep_init_task(struct task_struct *task);
 extern void lockdep_free_task(struct task_struct *task);
-#else
+#else /* !CROSSRELEASE */
 #define lockdep_init_map_crosslock(m, n, k, s) do {} while (0)
 /*
  * To initialize a lockdep_map statically use this macro.
@@ -591,11 +593,11 @@ extern void lockdep_free_task(struct task_struct *task);
 #define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .name = (_name), .key = (void *)(_key), }
 
-static inline void crossrelease_hist_start(enum xhlock_context_t c) {}
+static inline void crossrelease_hist_start(enum xhlock_context_t c, bool force) {}
 static inline void crossrelease_hist_end(enum xhlock_context_t c) {}
 static inline void lockdep_init_task(struct task_struct *task) {}
 static inline void lockdep_free_task(struct task_struct *task) {}
-#endif
+#endif /* CROSSRELEASE */
 
 #ifdef CONFIG_LOCK_STAT
 

commit 52fa5bc5cbba089f09bc2c372e3432f3f3e48051
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Aug 17 17:46:12 2017 +0800

    locking/lockdep: Explicitly initialize wq_barrier::done::map
    
    With the new lockdep crossrelease feature, which checks completions usage,
    a false positive is reported in the workqueue code:
    
    > Worker A : acquired of wfc.work -> wait for cpu_hotplug_lock to be released
    > Task   B : acquired of cpu_hotplug_lock -> wait for lock#3 to be released
    > Task   C : acquired of lock#3 -> wait for completion of barr->done
    > (Task C is in lru_add_drain_all_cpuslocked())
    > Worker D : wait for wfc.work to be released -> will complete barr->done
    
    Such a dead lock can not happen because Task C's barr->done and Worker D's
    barr->done can not be the same instance.
    
    The reason of this false positive is we initialize all wq_barrier::done
    at insert_wq_barrier() via init_completion(), which makes them belong to
    the same lock class, therefore, impossible circles are reported.
    
    To fix this, explicitly initialize the lockdep map for wq_barrier::done
    in insert_wq_barrier(), so that the lock class key of wq_barrier::done
    is a subkey of the corresponding work_struct, as a result we won't build
    a dependency between a wq_barrier with a unrelated work, and we can
    differ wq barriers based on the related works, so the false positive
    above is avoided.
    
    Also define the empty lockdep_init_map_crosslock() for !CROSSRELEASE
    to make the code simple and away from unnecessary #ifdefs.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170817094622.12915-1-boqun.feng@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 651cc61af041..fc827cab6d6e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -583,6 +583,7 @@ extern void crossrelease_hist_end(enum xhlock_context_t c);
 extern void lockdep_init_task(struct task_struct *task);
 extern void lockdep_free_task(struct task_struct *task);
 #else
+#define lockdep_init_map_crosslock(m, n, k, s) do {} while (0)
 /*
  * To initialize a lockdep_map statically use this macro.
  * Note that _name must not be NULL.

commit 28a903f63ec0811ead70ad0f8665e838d207a25e
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:54 2017 +0900

    locking/lockdep: Handle non(or multi)-acquisition of a crosslock
    
    No acquisition might be in progress on commit of a crosslock. Completion
    operations enabling crossrelease are the case like:
    
       CONTEXT X                         CONTEXT Y
       ---------                         ---------
       trigger completion context
                                         complete AX
                                            commit AX
       wait_for_complete AX
          acquire AX
          wait
    
       where AX is a crosslock.
    
    When no acquisition is in progress, we should not perform commit because
    the lock does not exist, which might cause incorrect memory access. So
    we have to track the number of acquisitions of a crosslock to handle it.
    
    Moreover, in case that more than one acquisition of a crosslock are
    overlapped like:
    
       CONTEXT W        CONTEXT X        CONTEXT Y        CONTEXT Z
       ---------        ---------        ---------        ---------
       acquire AX (gen_id: 1)
                                         acquire A
                        acquire AX (gen_id: 10)
                                         acquire B
                                         commit AX
                                                          acquire C
                                                          commit AX
    
       where A, B and C are typical locks and AX is a crosslock.
    
    Current crossrelease code performs commits in Y and Z with gen_id = 10.
    However, we can use gen_id = 1 to do it, since not only 'acquire AX in X'
    but 'acquire AX in W' also depends on each acquisition in Y and Z until
    their commits. So make it use gen_id = 1 instead of 10 on their commits,
    which adds an additional dependency 'AX -> A' in the example above.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-8-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c75eedd55af5..651cc61af041 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -324,6 +324,19 @@ struct hist_lock {
  * be called instead of lockdep_init_map().
  */
 struct cross_lock {
+	/*
+	 * When more than one acquisition of crosslocks are overlapped,
+	 * we have to perform commit for them based on cross_gen_id of
+	 * the first acquisition, which allows us to add more true
+	 * dependencies.
+	 *
+	 * Moreover, when no acquisition of a crosslock is in progress,
+	 * we should not perform commit because the lock might not exist
+	 * any more, which might cause incorrect memory access. So we
+	 * have to track the number of acquisitions of a crosslock.
+	 */
+	int nr_acquire;
+
 	/*
 	 * Seperate hlock instance. This will be used at commit step.
 	 *
@@ -547,9 +560,16 @@ extern void lockdep_init_map_crosslock(struct lockdep_map *lock,
 				       int subclass);
 extern void lock_commit_crosslock(struct lockdep_map *lock);
 
+/*
+ * What we essencially have to initialize is 'nr_acquire'. Other members
+ * will be initialized in add_xlock().
+ */
+#define STATIC_CROSS_LOCK_INIT() \
+	{ .nr_acquire = 0,}
+
 #define STATIC_CROSS_LOCKDEP_MAP_INIT(_name, _key) \
 	{ .map.name = (_name), .map.key = (void *)(_key), \
-	  .map.cross = 1, }
+	  .map.cross = 1, .xlock = STATIC_CROSS_LOCK_INIT(), }
 
 /*
  * To initialize a lockdep_map statically use this macro.

commit 23f873d8f9526ed7e49a1a02a45f8afb9ae5fb84
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:53 2017 +0900

    locking/lockdep: Detect and handle hist_lock ring buffer overwrite
    
    The ring buffer can be overwritten by hardirq/softirq/work contexts.
    That cases must be considered on rollback or commit. For example,
    
              |<------ hist_lock ring buffer size ----->|
              ppppppppppppiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
    wrapped > iiiiiiiiiiiiiiiiiiiiiii....................
    
              where 'p' represents an acquisition in process context,
              'i' represents an acquisition in irq context.
    
    On irq exit, crossrelease tries to rollback idx to original position,
    but it should not because the entry already has been invalid by
    overwriting 'i'. Avoid rollback or commit for entries overwritten.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-7-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index e1e0fcd99613..c75eedd55af5 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -283,6 +283,26 @@ struct held_lock {
  * can be added at commit step.
  */
 struct hist_lock {
+	/*
+	 * Id for each entry in the ring buffer. This is used to
+	 * decide whether the ring buffer was overwritten or not.
+	 *
+	 * For example,
+	 *
+	 *           |<----------- hist_lock ring buffer size ------->|
+	 *           pppppppppppppppppppppiiiiiiiiiiiiiiiiiiiiiiiiiiiii
+	 * wrapped > iiiiiiiiiiiiiiiiiiiiiiiiiii.......................
+	 *
+	 *           where 'p' represents an acquisition in process
+	 *           context, 'i' represents an acquisition in irq
+	 *           context.
+	 *
+	 * In this example, the ring buffer was overwritten by
+	 * acquisitions in irq context, that should be detected on
+	 * rollback or commit.
+	 */
+	unsigned int hist_id;
+
 	/*
 	 * Seperate stack_trace data. This will be used at commit step.
 	 */

commit b09be676e0ff25bd6d2e7637e26d349f9109ad75
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:52 2017 +0900

    locking/lockdep: Implement the 'crossrelease' feature
    
    Lockdep is a runtime locking correctness validator that detects and
    reports a deadlock or its possibility by checking dependencies between
    locks. It's useful since it does not report just an actual deadlock but
    also the possibility of a deadlock that has not actually happened yet.
    That enables problems to be fixed before they affect real systems.
    
    However, this facility is only applicable to typical locks, such as
    spinlocks and mutexes, which are normally released within the context in
    which they were acquired. However, synchronization primitives like page
    locks or completions, which are allowed to be released in any context,
    also create dependencies and can cause a deadlock.
    
    So lockdep should track these locks to do a better job. The 'crossrelease'
    implementation makes these primitives also be tracked.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-6-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 0a4c02c2d7a2..e1e0fcd99613 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -155,6 +155,12 @@ struct lockdep_map {
 	int				cpu;
 	unsigned long			ip;
 #endif
+#ifdef CONFIG_LOCKDEP_CROSSRELEASE
+	/*
+	 * Whether it's a crosslock.
+	 */
+	int				cross;
+#endif
 };
 
 static inline void lockdep_copy_map(struct lockdep_map *to,
@@ -258,8 +264,62 @@ struct held_lock {
 	unsigned int hardirqs_off:1;
 	unsigned int references:12;					/* 32 bits */
 	unsigned int pin_count;
+#ifdef CONFIG_LOCKDEP_CROSSRELEASE
+	/*
+	 * Generation id.
+	 *
+	 * A value of cross_gen_id will be stored when holding this,
+	 * which is globally increased whenever each crosslock is held.
+	 */
+	unsigned int gen_id;
+#endif
+};
+
+#ifdef CONFIG_LOCKDEP_CROSSRELEASE
+#define MAX_XHLOCK_TRACE_ENTRIES 5
+
+/*
+ * This is for keeping locks waiting for commit so that true dependencies
+ * can be added at commit step.
+ */
+struct hist_lock {
+	/*
+	 * Seperate stack_trace data. This will be used at commit step.
+	 */
+	struct stack_trace	trace;
+	unsigned long		trace_entries[MAX_XHLOCK_TRACE_ENTRIES];
+
+	/*
+	 * Seperate hlock instance. This will be used at commit step.
+	 *
+	 * TODO: Use a smaller data structure containing only necessary
+	 * data. However, we should make lockdep code able to handle the
+	 * smaller one first.
+	 */
+	struct held_lock	hlock;
+};
+
+/*
+ * To initialize a lock as crosslock, lockdep_init_map_crosslock() should
+ * be called instead of lockdep_init_map().
+ */
+struct cross_lock {
+	/*
+	 * Seperate hlock instance. This will be used at commit step.
+	 *
+	 * TODO: Use a smaller data structure containing only necessary
+	 * data. However, we should make lockdep code able to handle the
+	 * smaller one first.
+	 */
+	struct held_lock	hlock;
 };
 
+struct lockdep_map_cross {
+	struct lockdep_map map;
+	struct cross_lock xlock;
+};
+#endif
+
 /*
  * Initialization, self-test and debugging-output methods:
  */
@@ -281,13 +341,6 @@ extern void lockdep_on(void);
 extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 			     struct lock_class_key *key, int subclass);
 
-/*
- * To initialize a lockdep_map statically use this macro.
- * Note that _name must not be NULL.
- */
-#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
-	{ .name = (_name), .key = (void *)(_key), }
-
 /*
  * Reinitialize a lock key - for cases where there is special locking or
  * special initialization of locks so that the validator gets the scope
@@ -460,6 +513,49 @@ struct pin_cookie { };
 
 #endif /* !LOCKDEP */
 
+enum xhlock_context_t {
+	XHLOCK_HARD,
+	XHLOCK_SOFT,
+	XHLOCK_PROC,
+	XHLOCK_CTX_NR,
+};
+
+#ifdef CONFIG_LOCKDEP_CROSSRELEASE
+extern void lockdep_init_map_crosslock(struct lockdep_map *lock,
+				       const char *name,
+				       struct lock_class_key *key,
+				       int subclass);
+extern void lock_commit_crosslock(struct lockdep_map *lock);
+
+#define STATIC_CROSS_LOCKDEP_MAP_INIT(_name, _key) \
+	{ .map.name = (_name), .map.key = (void *)(_key), \
+	  .map.cross = 1, }
+
+/*
+ * To initialize a lockdep_map statically use this macro.
+ * Note that _name must not be NULL.
+ */
+#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
+	{ .name = (_name), .key = (void *)(_key), .cross = 0, }
+
+extern void crossrelease_hist_start(enum xhlock_context_t c);
+extern void crossrelease_hist_end(enum xhlock_context_t c);
+extern void lockdep_init_task(struct task_struct *task);
+extern void lockdep_free_task(struct task_struct *task);
+#else
+/*
+ * To initialize a lockdep_map statically use this macro.
+ * Note that _name must not be NULL.
+ */
+#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
+	{ .name = (_name), .key = (void *)(_key), }
+
+static inline void crossrelease_hist_start(enum xhlock_context_t c) {}
+static inline void crossrelease_hist_end(enum xhlock_context_t c) {}
+static inline void lockdep_init_task(struct task_struct *task) {}
+static inline void lockdep_free_task(struct task_struct *task) {}
+#endif
+
 #ifdef CONFIG_LOCK_STAT
 
 extern void lock_contended(struct lockdep_map *lock, unsigned long ip);

commit d92a8cfcb37ecd1315269dab741f073b63b3a8b6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 3 10:13:38 2017 +0100

    locking/lockdep: Rework FS_RECLAIM annotation
    
    A while ago someone, and I cannot find the email just now, asked if we
    could not implement the RECLAIM_FS inversion stuff with a 'fake' lock
    like we use for other things like workqueues etc. I think this should
    be possible which allows reducing the 'irq' states and will reduce the
    amount of __bfs() lookups we do.
    
    Removing the 1 IRQ state results in 4 less __bfs() walks per
    dependency, improving lockdep performance. And by moving this
    annotation out of the lockdep code it becomes easier for the mm people
    to extend.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: iamjoonsoo.kim@lge.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index fffe49f188e6..0a4c02c2d7a2 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -29,7 +29,7 @@ extern int lock_stat;
  * We'd rather not expose kernel/lockdep_states.h this wide, but we do need
  * the total number of states... :-(
  */
-#define XXX_LOCK_USAGE_STATES		(1+3*4)
+#define XXX_LOCK_USAGE_STATES		(1+2*4)
 
 /*
  * NR_LOCKDEP_CACHING_CLASSES ... Number of classes
@@ -363,10 +363,6 @@ static inline void lock_set_subclass(struct lockdep_map *lock,
 
 extern void lock_downgrade(struct lockdep_map *lock, unsigned long ip);
 
-extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
-extern void lockdep_clear_current_reclaim_state(void);
-extern void lockdep_trace_alloc(gfp_t mask);
-
 struct pin_cookie { unsigned int val; };
 
 #define NIL_COOKIE (struct pin_cookie){ .val = 0U, }
@@ -375,7 +371,7 @@ extern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);
 extern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);
 extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 
-# define INIT_LOCKDEP				.lockdep_recursion = 0, .lockdep_reclaim_gfp = 0,
+# define INIT_LOCKDEP				.lockdep_recursion = 0,
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
@@ -416,9 +412,6 @@ static inline void lockdep_on(void)
 # define lock_downgrade(l, i)			do { } while (0)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
-# define lockdep_set_current_reclaim_state(g)	do { } while (0)
-# define lockdep_clear_current_reclaim_state()	do { } while (0)
-# define lockdep_trace_alloc(g)			do { } while (0)
 # define lockdep_info()				do { } while (0)
 # define lockdep_init_map(lock, name, key, sub) \
 		do { (void)(name); (void)(key); } while (0)

commit 6419c4af777a773a45a1b1af735de0fcd9a7dcc7
Author: J. R. Okajima <hooanon05g@gmail.com>
Date:   Fri Feb 3 01:38:17 2017 +0900

    locking/lockdep: Add new check to lock_downgrade()
    
    Commit:
    
      f8319483f57f ("locking/lockdep: Provide a type check for lock_is_held")
    
    didn't fully cover rwsems as downgrade_write() was left out.
    
    Introduce lock_downgrade() and use it to add new checks.
    
    See-also: http://marc.info/?l=linux-kernel&m=148581164003149&w=2
    Originally-written-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: J. R. Okajima <hooanon05g@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1486053497-9948-3-git-send-email-hooanon05g@gmail.com
    [ Rewrote the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 1e327bb80838..fffe49f188e6 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -361,6 +361,8 @@ static inline void lock_set_subclass(struct lockdep_map *lock,
 	lock_set_class(lock, lock->name, lock->key, subclass, ip);
 }
 
+extern void lock_downgrade(struct lockdep_map *lock, unsigned long ip);
+
 extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
 extern void lockdep_clear_current_reclaim_state(void);
 extern void lockdep_trace_alloc(gfp_t mask);
@@ -411,6 +413,7 @@ static inline void lockdep_on(void)
 
 # define lock_acquire(l, s, t, r, c, n, i)	do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
+# define lock_downgrade(l, i)			do { } while (0)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_set_current_reclaim_state(g)	do { } while (0)

commit f8319483f57f1ca22370f4150bb990aca7728a67
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 30 14:32:25 2016 +1100

    locking/lockdep: Provide a type check for lock_is_held
    
    Christoph requested lockdep_assert_held() variants that distinguish
    between held-for-read or held-for-write.
    
    Provide:
    
      int lock_is_held_type(struct lockdep_map *lock, int read)
    
    which takes the same argument as lock_acquire(.read) and matches it to
    the held_lock instance.
    
    Use of this function should be gated by the debug_locks variable. When
    that is 0 the return value of the lock_is_held_type() function is
    undefined. This is done to allow both negative and positive tests for
    holding locks.
    
    By default we provide (positive) lockdep_assert_held{,_exclusive,_read}()
    macros.
    
    Requested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Dave Chinner <david@fromorbit.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c1458fede1f9..1e327bb80838 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -338,9 +338,18 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
 
-#define lockdep_is_held(lock)	lock_is_held(&(lock)->dep_map)
+/*
+ * Same "read" as for lock_acquire(), except -1 means any.
+ */
+extern int lock_is_held_type(struct lockdep_map *lock, int read);
+
+static inline int lock_is_held(struct lockdep_map *lock)
+{
+	return lock_is_held_type(lock, -1);
+}
 
-extern int lock_is_held(struct lockdep_map *lock);
+#define lockdep_is_held(lock)		lock_is_held(&(lock)->dep_map)
+#define lockdep_is_held_type(lock, r)	lock_is_held_type(&(lock)->dep_map, (r))
 
 extern void lock_set_class(struct lockdep_map *lock, const char *name,
 			   struct lock_class_key *key, unsigned int subclass,
@@ -372,6 +381,14 @@ extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 		WARN_ON(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
 
+#define lockdep_assert_held_exclusive(l)	do {			\
+		WARN_ON(debug_locks && !lockdep_is_held_type(l, 0));	\
+	} while (0)
+
+#define lockdep_assert_held_read(l)	do {				\
+		WARN_ON(debug_locks && !lockdep_is_held_type(l, 1));	\
+	} while (0)
+
 #define lockdep_assert_held_once(l)	do {				\
 		WARN_ON_ONCE(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
@@ -428,7 +445,11 @@ struct lock_class_key { };
 
 #define lockdep_depth(tsk)	(0)
 
+#define lockdep_is_held_type(l, r)		(1)
+
 #define lockdep_assert_held(l)			do { (void)(l); } while (0)
+#define lockdep_assert_held_exclusive(l)	do { (void)(l); } while (0)
+#define lockdep_assert_held_read(l)		do { (void)(l); } while (0)
 #define lockdep_assert_held_once(l)		do { (void)(l); } while (0)
 
 #define lockdep_recursing(tsk)			(0)

commit a5ecddfe0bf82c92cc2e47385e287b21361a8d70
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Fri Sep 16 18:02:43 2016 +0200

    lockdep: make MAX_LOCKDEP_SUBCLASSES unconditionally visible
    
    This define is needed by i2c_adapter_depth() to detect if we don't
    exceed the maximum number of lock subclasses. Make it visible even
    if lockdep is disabled.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Acked-by: Peter Rosin <peda@axentia.se>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Wolfram Sang <wsa@the-dreams.de>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index eabe0138eb06..c1458fede1f9 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -16,6 +16,8 @@ struct lockdep_map;
 extern int prove_locking;
 extern int lock_stat;
 
+#define MAX_LOCKDEP_SUBCLASSES		8UL
+
 #ifdef CONFIG_LOCKDEP
 
 #include <linux/linkage.h>
@@ -29,8 +31,6 @@ extern int lock_stat;
  */
 #define XXX_LOCK_USAGE_STATES		(1+3*4)
 
-#define MAX_LOCKDEP_SUBCLASSES		8UL
-
 /*
  * NR_LOCKDEP_CACHING_CLASSES ... Number of classes
  * cached in the instance of lockdep_map

commit 825a3b2605c3aa193e0075d0f9c72e33c17ab16a
Merge: cf6ed9a6682d ef0491ea17f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 14:47:16 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - massive CPU hotplug rework (Thomas Gleixner)
    
     - improve migration fairness (Peter Zijlstra)
    
     - CPU load calculation updates/cleanups (Yuyang Du)
    
     - cpufreq updates (Steve Muckle)
    
     - nohz optimizations (Frederic Weisbecker)
    
     - switch_mm() micro-optimization on x86 (Andy Lutomirski)
    
     - ... lots of other enhancements, fixes and cleanups.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (66 commits)
      ARM: Hide finish_arch_post_lock_switch() from modules
      sched/core: Provide a tsk_nr_cpus_allowed() helper
      sched/core: Use tsk_cpus_allowed() instead of accessing ->cpus_allowed
      sched/loadavg: Fix loadavg artifacts on fully idle and on fully loaded systems
      sched/fair: Correct unit of load_above_capacity
      sched/fair: Clean up scale confusion
      sched/nohz: Fix affine unpinned timers mess
      sched/fair: Fix fairness issue on migration
      sched/core: Kill sched_class::task_waking to clean up the migration logic
      sched/fair: Prepare to fix fairness problems on migration
      sched/fair: Move record_wakee()
      sched/core: Fix comment typo in wake_q_add()
      sched/core: Remove unused variable
      sched: Make hrtick_notifier an explicit call
      sched/fair: Make ilb_notifier an explicit call
      sched/hotplug: Make activate() the last hotplug step
      sched/hotplug: Move migration CPU_DYING to sched_cpu_dying()
      sched/migration: Move CPU_ONLINE into scheduler state
      sched/migration: Move calc_load_migrate() into CPU_DYING
      sched/migration: Move prepare transition to SCHED_STARTING state
      ...

commit 3469d261eac65912927dca13ee8f77c744ad7aa2
Merge: 1c19b68a279c 4544ba8c6b17
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 13:41:02 2016 -0700

    Merge branch 'locking-rwsem-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull support for killable rwsems from Ingo Molnar:
     "This, by Michal Hocko, implements down_write_killable().
    
      The main usecase will be to update mm_sem usage sites to use this new
      API, to allow the mm-reaper introduced in commit aac453635549 ("mm,
      oom: introduce oom reaper") to tear down oom victim address spaces
      asynchronously with minimum latencies and without deadlock worries"
    
    [ The vfs will want it too as the inode lock is changed from a mutex to
      a rwsem due to the parallel lookup and readdir updates ]
    
    * 'locking-rwsem-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/rwsem: Fix comment on register clobbering
      locking/rwsem: Fix down_write_killable()
      locking/rwsem, x86: Add frame annotation for call_rwsem_down_write_failed_killable()
      locking/rwsem: Provide down_write_killable()
      locking/rwsem, x86: Provide __down_write_killable()
      locking/rwsem, s390: Provide __down_write_killable()
      locking/rwsem, ia64: Provide __down_write_killable()
      locking/rwsem, alpha: Provide __down_write_killable()
      locking/rwsem: Introduce basis for down_write_killable()
      locking/rwsem, sparc: Drop superfluous arch specific implementation
      locking/rwsem, sh: Drop superfluous arch specific implementation
      locking/rwsem, xtensa: Drop superfluous arch specific implementation
      locking/rwsem: Drop explicit memory barriers
      locking/rwsem: Get rid of __down_write_nested()

commit eb60b3e5e8dfdd590e586a6fc22daf2f63a7b7e6
Merge: 58fe9c4621b7 53d3bc773eaa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 12 09:18:13 2016 +0200

    Merge branch 'sched/urgent' into sched/core to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e7904a28f5331c21d17af638cb477c83662e3cb6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Aug 1 19:25:08 2015 +0200

    locking/lockdep, sched/core: Implement a better lock pinning scheme
    
    The problem with the existing lock pinning is that each pin is of
    value 1; this mean you can simply unpin if you know its pinned,
    without having any extra information.
    
    This scheme generates a random (16 bit) cookie for each pin and
    requires this same cookie to unpin. This means you have to keep the
    cookie in context.
    
    No objsize difference for !LOCKDEP kernels.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index d026b190c530..cf9bf9612702 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -354,8 +354,13 @@ extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
 extern void lockdep_clear_current_reclaim_state(void);
 extern void lockdep_trace_alloc(gfp_t mask);
 
-extern void lock_pin_lock(struct lockdep_map *lock);
-extern void lock_unpin_lock(struct lockdep_map *lock);
+struct pin_cookie { unsigned int val; };
+
+#define NIL_COOKIE (struct pin_cookie){ .val = 0U, }
+
+extern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);
+extern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);
+extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 
 # define INIT_LOCKDEP				.lockdep_recursion = 0, .lockdep_reclaim_gfp = 0,
 
@@ -371,8 +376,9 @@ extern void lock_unpin_lock(struct lockdep_map *lock);
 
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 
-#define lockdep_pin_lock(l)		lock_pin_lock(&(l)->dep_map)
-#define lockdep_unpin_lock(l)	lock_unpin_lock(&(l)->dep_map)
+#define lockdep_pin_lock(l)	lock_pin_lock(&(l)->dep_map)
+#define lockdep_repin_lock(l,c)	lock_repin_lock(&(l)->dep_map, (c))
+#define lockdep_unpin_lock(l,c)	lock_unpin_lock(&(l)->dep_map, (c))
 
 #else /* !CONFIG_LOCKDEP */
 
@@ -425,8 +431,13 @@ struct lock_class_key { };
 
 #define lockdep_recursing(tsk)			(0)
 
-#define lockdep_pin_lock(l)				do { (void)(l); } while (0)
-#define lockdep_unpin_lock(l)			do { (void)(l); } while (0)
+struct pin_cookie { };
+
+#define NIL_COOKIE (struct pin_cookie){ }
+
+#define lockdep_pin_lock(l)			({ struct pin_cookie cookie; cookie; })
+#define lockdep_repin_lock(l, c)		do { (void)(l); (void)(c); } while (0)
+#define lockdep_unpin_lock(l, c)		do { (void)(l); (void)(c); } while (0)
 
 #endif /* !LOCKDEP */
 

commit 75dd602a5198a6e5f75534db52b6e6fbaabb33d1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 30 11:36:59 2016 +0200

    lockdep: Fix lock_chain::base size
    
    lock_chain::base is used to store an index into the chain_hlocks[]
    array, however that array contains more elements than can be indexed
    using the u16.
    
    Change the lock_chain structure to use a bitfield to encode the data
    it needs and add BUILD_BUG_ON() assertions to check the fields are
    wide enough.
    
    Also, for DEBUG_LOCKDEP, assert that we don't run out of elements of
    that array; as that would wreck the collision detectoring.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alfredo Alvarez Fernandez <alfredoalvarezfernandez@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160330093659.GS3408@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index d026b190c530..d10ef06971b5 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -196,9 +196,11 @@ struct lock_list {
  * We record lock dependency chains, so that we can cache them:
  */
 struct lock_chain {
-	u8				irq_context;
-	u8				depth;
-	u16				base;
+	/* see BUILD_BUG_ON()s in lookup_chain_cache() */
+	unsigned int			irq_context :  2,
+					depth       :  6,
+					base	    : 24;
+	/* 4 byte hole */
 	struct hlist_node		entry;
 	u64				chain_key;
 };

commit 916633a403702549d37ea353e63a68e5b0dc27ad
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Apr 7 17:12:31 2016 +0200

    locking/rwsem: Provide down_write_killable()
    
    Now that all the architectures implement the necessary glue code
    we can introduce down_write_killable(). The only difference wrt. regular
    down_write() is that the slow path waits in TASK_KILLABLE state and the
    interruption by the fatal signal is reported as -EINTR to the caller.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Signed-off-by: Jason Low <jason.low2@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1460041951-22347-12-git-send-email-mhocko@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index d026b190c530..accfe56d8c51 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -444,6 +444,18 @@ do {								\
 	lock_acquired(&(_lock)->dep_map, _RET_IP_);			\
 } while (0)
 
+#define LOCK_CONTENDED_RETURN(_lock, try, lock)			\
+({								\
+	int ____err = 0;					\
+	if (!try(_lock)) {					\
+		lock_contended(&(_lock)->dep_map, _RET_IP_);	\
+		____err = lock(_lock);				\
+	}							\
+	if (!____err)						\
+		lock_acquired(&(_lock)->dep_map, _RET_IP_);	\
+	____err;						\
+})
+
 #else /* CONFIG_LOCK_STAT */
 
 #define lock_contended(lockdep_map, ip) do {} while (0)
@@ -452,6 +464,9 @@ do {								\
 #define LOCK_CONTENDED(_lock, try, lock) \
 	lock(_lock)
 
+#define LOCK_CONTENDED_RETURN(_lock, try, lock) \
+	lock(_lock)
+
 #endif /* CONFIG_LOCK_STAT */
 
 #ifdef CONFIG_LOCKDEP

commit 06bea3dbfe6a4c333c4333362c46bdf4d9e43504
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu Feb 4 11:29:36 2016 -0800

    locking/lockdep: Eliminate lockdep_init()
    
    Lockdep is initialized at compile time now.  Get rid of lockdep_init().
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Krinkin <krinkin.m.u@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: mm-commits@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 4dca42fd32f5..d026b190c530 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -261,7 +261,6 @@ struct held_lock {
 /*
  * Initialization, self-test and debugging-output methods:
  */
-extern void lockdep_init(void);
 extern void lockdep_info(void);
 extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
@@ -392,7 +391,6 @@ static inline void lockdep_on(void)
 # define lockdep_set_current_reclaim_state(g)	do { } while (0)
 # define lockdep_clear_current_reclaim_state()	do { } while (0)
 # define lockdep_trace_alloc(g)			do { } while (0)
-# define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
 # define lockdep_init_map(lock, name, key, sub) \
 		do { (void)(name); (void)(key); } while (0)

commit a63f38cc4ccfa076f87fc3d0c276ee62e710f953
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Feb 3 13:44:12 2016 -0800

    locking/lockdep: Convert hash tables to hlists
    
    Mike said:
    
    : CONFIG_UBSAN_ALIGNMENT breaks x86-64 kernel with lockdep enabled, i.e.
    : kernel with CONFIG_UBSAN_ALIGNMENT=y fails to load without even any error
    : message.
    :
    : The problem is that ubsan callbacks use spinlocks and might be called
    : before lockdep is initialized.  Particularly this line in the
    : reserve_ebda_region function causes problem:
    :
    : lowmem = *(unsigned short *)__va(BIOS_LOWMEM_KILOBYTES);
    :
    : If i put lockdep_init() before reserve_ebda_region call in
    : x86_64_start_reservations kernel loads well.
    
    Fix this ordering issue permanently: change lockdep so that it uses hlists
    for the hash tables.  Unlike a list_head, an hlist_head is in its
    initialized state when it is all-zeroes, so lockdep is ready for operation
    immediately upon boot - lockdep_init() need not have run.
    
    The patch will also save some memory.
    
    Probably lockdep_init() and lockdep_initialized can be done away with now.
    
    Suggested-by: Mike Krinkin <krinkin.m.u@gmail.com>
    Reported-by: Mike Krinkin <krinkin.m.u@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: mm-commits@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c57e424d914b..4dca42fd32f5 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -66,7 +66,7 @@ struct lock_class {
 	/*
 	 * class-hash:
 	 */
-	struct list_head		hash_entry;
+	struct hlist_node		hash_entry;
 
 	/*
 	 * global list of all lock-classes:
@@ -199,7 +199,7 @@ struct lock_chain {
 	u8				irq_context;
 	u8				depth;
 	u16				base;
-	struct list_head		entry;
+	struct hlist_node		entry;
 	u64				chain_key;
 };
 

commit 90eec103b96e30401c0b846045bf8a1c7159b6da
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 16 11:08:45 2015 +0100

    treewide: Remove old email address
    
    There were still a number of references to my old Red Hat email
    address in the kernel source. Remove these while keeping the
    Red Hat copyright notices intact.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 70400dc7660f..c57e424d914b 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -2,7 +2,7 @@
  * Runtime locking correctness validator
  *
  *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
- *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
+ *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
  *
  * see Documentation/locking/lockdep-design.txt for more details.
  */

commit 98ec21a01896751b673b6c731ca8881daa8b2c6d
Merge: a262948335bc cbce1a686700
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 15:09:40 2015 -0700

    Merge branch 'sched-hrtimers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Thomas Gleixner:
     "This series of scheduler updates depends on sched/core and timers/core
      branches, which are already in your tree:
    
       - Scheduler balancing overhaul to plug a hard to trigger race which
         causes an oops in the balancer (Peter Zijlstra)
    
       - Lockdep updates which are related to the balancing updates (Peter
         Zijlstra)"
    
    * 'sched-hrtimers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched,lockdep: Employ lock pinning
      lockdep: Implement lock pinning
      lockdep: Simplify lock_release()
      sched: Streamline the task migration locking a little
      sched: Move code around
      sched,dl: Fix sched class hopping CBS hole
      sched, dl: Convert switched_{from, to}_dl() / prio_changed_dl() to balance callbacks
      sched,dl: Remove return value from pull_dl_task()
      sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks
      sched,rt: Remove return value from pull_rt_task()
      sched: Allow balance callbacks for check_class_changed()
      sched: Use replace normalize_task() with __sched_setscheduler()
      sched: Replace post_schedule with a balance callback list

commit 68722101ec3a0e179408a13708dd020e04f54aab
Author: George Beshers <gbeshers@sgi.com>
Date:   Thu Jun 18 10:25:13 2015 -0500

    locking/lockdep: Remove hard coded array size dependency
    
    An apparent oversight left a hardcoded '4' in place when
    LOCKSTAT_POINTS was introduced.
    
    The contention_point[] and contending_point[] arrays in the
    structs lock_class and lock_class_stats need to be the same
    size for the loops in lock_stats() to be correct.
    
    This patch allows LOCKSTAT_POINTS to be changed without
    affecting the correctness of the code.
    
    Signed-off-by: George Beshers <gbeshers@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 066ba4157541..2722111591a3 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -130,8 +130,8 @@ enum bounce_type {
 };
 
 struct lock_class_stats {
-	unsigned long			contention_point[4];
-	unsigned long			contending_point[4];
+	unsigned long			contention_point[LOCKSTAT_POINTS];
+	unsigned long			contending_point[LOCKSTAT_POINTS];
 	struct lock_time		read_waittime;
 	struct lock_time		write_waittime;
 	struct lock_time		read_holdtime;

commit a24fc60d63da2b0b31bf7c876d12a51ed4b778bd
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 11 14:46:53 2015 +0200

    lockdep: Implement lock pinning
    
    Add a lockdep annotation that WARNs if you 'accidentially' unlock a
    lock.
    
    This is especially helpful for code with callbacks, where the upper
    layer assumes a lock remains taken but a lower layer thinks it maybe
    can drop and reacquire the lock.
    
    By unwittingly breaking up the lock, races can be introduced.
    
    Lock pinning is a lockdep annotation that helps with this, when you
    lockdep_pin_lock() a held lock, any unlock without a
    lockdep_unpin_lock() will produce a WARN. Think of this as a relative
    of lockdep_assert_held(), except you don't only assert its held now,
    but ensure it stays held until you release your assertion.
    
    RFC: a possible alternative API would be something like:
    
      int cookie = lockdep_pin_lock(&foo);
      ...
      lockdep_unpin_lock(&foo, cookie);
    
    Where we pick a random number for the pin_count; this makes it
    impossible to sneak a lock break in without also passing the right
    cookie along.
    
    I've not done this because it ends up generating code for !LOCKDEP,
    esp. if you need to pass the cookie around for some reason.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: ktkhai@parallels.com
    Cc: rostedt@goodmis.org
    Cc: juri.lelli@gmail.com
    Cc: pang.xunlei@linaro.org
    Cc: oleg@redhat.com
    Cc: wanpeng.li@linux.intel.com
    Cc: umgwanakikbuti@gmail.com
    Link: http://lkml.kernel.org/r/20150611124743.906731065@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 066ba4157541..c5b6b5830acf 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -255,6 +255,7 @@ struct held_lock {
 	unsigned int check:1;       /* see lock_acquire() comment */
 	unsigned int hardirqs_off:1;
 	unsigned int references:12;					/* 32 bits */
+	unsigned int pin_count;
 };
 
 /*
@@ -354,6 +355,9 @@ extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
 extern void lockdep_clear_current_reclaim_state(void);
 extern void lockdep_trace_alloc(gfp_t mask);
 
+extern void lock_pin_lock(struct lockdep_map *lock);
+extern void lock_unpin_lock(struct lockdep_map *lock);
+
 # define INIT_LOCKDEP				.lockdep_recursion = 0, .lockdep_reclaim_gfp = 0,
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
@@ -368,6 +372,9 @@ extern void lockdep_trace_alloc(gfp_t mask);
 
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 
+#define lockdep_pin_lock(l)		lock_pin_lock(&(l)->dep_map)
+#define lockdep_unpin_lock(l)	lock_unpin_lock(&(l)->dep_map)
+
 #else /* !CONFIG_LOCKDEP */
 
 static inline void lockdep_off(void)
@@ -420,6 +427,9 @@ struct lock_class_key { };
 
 #define lockdep_recursing(tsk)			(0)
 
+#define lockdep_pin_lock(l)				do { (void)(l); } while (0)
+#define lockdep_unpin_lock(l)			do { (void)(l); } while (0)
+
 #endif /* !LOCKDEP */
 
 #ifdef CONFIG_LOCK_STAT

commit d24209bb689e2c7f7418faec9b4a948e922d24da
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 21 15:26:03 2015 -0800

    rcu: Improve diagnostics for blocked critical sections in irq
    
    If an RCU read-side critical section occurs within an interrupt handler
    or a softirq handler, it cannot have been preempted.  Therefore, there is
    a check in rcu_read_unlock_special() checking for this error.  However,
    when this check triggers, it lacks diagnostic information.  This commit
    therefore moves rcu_read_unlock()'s lockdep annotation to follow the
    call to __rcu_read_unlock() and changes rcu_read_unlock_special()'s
    WARN_ON_ONCE() to an lockdep_rcu_suspicious() in order to locate where
    the offending RCU read-side critical section began.  In addition, the
    value of the ->rcu_read_unlock_special field is printed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 74ab23176e9b..066ba4157541 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -531,8 +531,13 @@ do {									\
 # define might_lock_read(lock) do { } while (0)
 #endif
 
-#ifdef CONFIG_PROVE_RCU
+#ifdef CONFIG_LOCKDEP
 void lockdep_rcu_suspicious(const char *file, const int line, const char *s);
+#else
+static inline void
+lockdep_rcu_suspicious(const char *file, const int line, const char *s)
+{
+}
 #endif
 
 #endif /* __LINUX_LOCKDEP_H */

commit 6d5f0ebfc0be9cbfeaafdd9258d5fa24b7975a36
Merge: dbb885fecc1b 8acd91e86208
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 15:51:40 2014 +0200

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core locking updates from Ingo Molnar:
     "The main updates in this cycle were:
    
       - mutex MCS refactoring finishing touches: improve comments, refactor
         and clean up code, reduce debug data structure footprint, etc.
    
       - qrwlock finishing touches: remove old code, self-test updates.
    
       - small rwsem optimization
    
       - various smaller fixes/cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/lockdep: Revert qrwlock recusive stuff
      locking/rwsem: Avoid double checking before try acquiring write lock
      locking/rwsem: Move EXPORT_SYMBOL() lines to follow function definition
      locking/rwlock, x86: Delete unused asm/rwlock.h and rwlock.S
      locking/rwlock, x86: Clean up asm/spinlock*.h to remove old rwlock code
      locking/semaphore: Resolve some shadow warnings
      locking/selftest: Support queued rwlock
      locking/lockdep: Restrict the use of recursive read_lock() with qrwlock
      locking/spinlocks: Always evaluate the second argument of spin_lock_nested()
      locking/Documentation: Update locking/mutex-design.txt disadvantages
      locking/Documentation: Move locking related docs into Documentation/locking/
      locking/mutexes: Use MUTEX_SPIN_ON_OWNER when appropriate
      locking/mutexes: Refactor optimistic spinning code
      locking/mcs: Remove obsolete comment
      locking/mutexes: Document quick lock release when unlocking
      locking/mutexes: Standardize arguments in lock/unlock slowpaths
      locking: Remove deprecated smp_mb__() barriers

commit d6dd50e07c5bec00db2005969b1a01f8ca3d25ef
Merge: 5ff0b9e1a1da fd19bda49120
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 15:44:12 2014 +0200

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes related to No-CBs CPUs and NO_HZ_FULL
    
       - RCU-tasks implementation
    
       - torture-test updates
    
       - miscellaneous fixes
    
       - locktorture updates
    
       - RCU documentation updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (81 commits)
      workqueue: Use cond_resched_rcu_qs macro
      workqueue: Add quiescent state between work items
      locktorture: Cleanup header usage
      locktorture: Cannot hold read and write lock
      locktorture: Fix __acquire annotation for spinlock irq
      locktorture: Support rwlocks
      rcu: Eliminate deadlock between CPU hotplug and expedited grace periods
      locktorture: Document boot/module parameters
      rcutorture: Rename rcutorture_runnable parameter
      locktorture: Add test scenario for rwsem_lock
      locktorture: Add test scenario for mutex_lock
      locktorture: Make torture scripting account for new _runnable name
      locktorture: Introduce torture context
      locktorture: Support rwsems
      locktorture: Add infrastructure for torturing read locks
      torture: Address race in module cleanup
      locktorture: Make statistics generic
      locktorture: Teach about lock debugging
      locktorture: Support mutexes
      locktorture: Add documentation
      ...

commit 8acd91e8620836a56ff62028ed28ba629f2881a0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 30 15:26:00 2014 +0200

    locking/lockdep: Revert qrwlock recusive stuff
    
    Commit f0bab73cb539 ("locking/lockdep: Restrict the use of recursive
    read_lock() with qrwlock") changed lockdep to try and conform to the
    qrwlock semantics which differ from the traditional rwlock semantics.
    
    In particular qrwlock is fair outside of interrupt context, but in
    interrupt context readers will ignore all fairness.
    
    The problem modeling this is that read and write side have different
    lock state (interrupts) semantics but we only have a single
    representation of these. Therefore lockdep will get confused, thinking
    the lock can cause interrupt lock inversions.
    
    So revert it for now; the old rwlock semantics were already imperfectly
    modeled and the qrwlock extra won't fit either.
    
    If we want to properly fix this, I think we need to resurrect the work
    by Gautham did a few years ago that split the read and write state of
    locks:
    
       http://lwn.net/Articles/332801/
    
    FWIW the locking selftest that would've failed (and was reported by
    Borislav earlier) is something like:
    
      RL(X1);       /* IRQ-ON */
      LOCK(A);
      UNLOCK(A);
      RU(X1);
    
      IRQ_ENTER();
      RL(X1);       /* IN-IRQ */
      RU(X1);
      IRQ_EXIT();
    
    At which point it would report that because A is an IRQ-unsafe lock we
    can suffer the following inversion:
    
            CPU0            CPU1
    
            lock(A)
                            lock(X1)
                            lock(A)
            <IRQ>
             lock(X1)
    
    And this is 'wrong' because X1 can recurse (assuming the above lock are
    in fact read-lock) but lockdep doesn't know about this.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: ego@linux.vnet.ibm.com
    Cc: bp@alien8.de
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20140930132600.GA7444@worktop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b5a84b62fb84..f388481201cd 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -478,24 +478,16 @@ static inline void print_irqtrace_events(struct task_struct *curr)
  * on the per lock-class debug mode:
  */
 
-/*
- * Read states in the 2-bit held_lock:read field:
- *  0: Exclusive lock
- *  1: Shareable lock, cannot be recursively called
- *  2: Shareable lock, can be recursively called
- *  3: Shareable lock, cannot be recursively called except in interrupt context
- */
 #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 1, n, i)
 #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 1, n, i)
 #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 1, n, i)
-#define lock_acquire_shared_irecursive(l, s, t, n, i)	lock_acquire(l, s, t, 3, 1, n, i)
 
 #define spin_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define spin_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
 #define spin_release(l, n, i)			lock_release(l, n, i)
 
 #define rwlock_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
-#define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_irecursive(l, s, t, NULL, i)
+#define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_recursive(l, s, t, NULL, i)
 #define rwlock_release(l, n, i)			lock_release(l, n, i)
 
 #define seqcount_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)

commit 9a37110d20c95d1ebf6c04881177fe8f62831db2
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Sep 10 14:31:39 2014 -0400

    locking: Add WARN_ON_ONCE lock assertion
    
    An interface may need to assert a lock invariant and not flood the
    system logs; add a lockdep helper macro equivalent to
    lockdep_assert_held() which only WARNs once.
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 008388f920d7..64c7425afbce 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -362,6 +362,10 @@ extern void lockdep_trace_alloc(gfp_t mask);
 		WARN_ON(debug_locks && !lockdep_is_held(l));	\
 	} while (0)
 
+#define lockdep_assert_held_once(l)	do {				\
+		WARN_ON_ONCE(debug_locks && !lockdep_is_held(l));	\
+	} while (0)
+
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 
 #else /* !CONFIG_LOCKDEP */
@@ -412,6 +416,7 @@ struct lock_class_key { };
 #define lockdep_depth(tsk)	(0)
 
 #define lockdep_assert_held(l)			do { (void)(l); } while (0)
+#define lockdep_assert_held_once(l)		do { (void)(l); } while (0)
 
 #define lockdep_recursing(tsk)			(0)
 

commit dd56af42bd829c6e770ed69812bd65a04eaeb1e4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 25 20:25:06 2014 -0700

    rcu: Eliminate deadlock between CPU hotplug and expedited grace periods
    
    Currently, the expedited grace-period primitives do get_online_cpus().
    This greatly simplifies their implementation, but means that calls
    to them holding locks that are acquired by CPU-hotplug notifiers (to
    say nothing of calls to these primitives from CPU-hotplug notifiers)
    can deadlock.  But this is starting to become inconvenient, as can be
    seen here: https://lkml.org/lkml/2014/8/5/754.  The problem in this
    case is that some developers need to acquire a mutex from a CPU-hotplug
    notifier, but also need to hold it across a synchronize_rcu_expedited().
    As noted above, this currently results in deadlock.
    
    This commit avoids the deadlock and retains the simplicity by creating
    a try_get_online_cpus(), which returns false if the get_online_cpus()
    reference count could not immediately be incremented.  If a call to
    try_get_online_cpus() returns true, the expedited primitives operate as
    before.  If a call returns false, the expedited primitives fall back to
    normal grace-period operations.  This falling back of course results in
    increased grace-period latency, but only during times when CPU hotplug
    operations are actually in flight.  The effect should therefore be
    negligible during normal operation.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Tested-by: Lan Tianyu <tianyu.lan@intel.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 008388f920d7..4f86465cc317 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -505,6 +505,7 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #define lock_map_acquire(l)			lock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)
 #define lock_map_acquire_read(l)		lock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)
+#define lock_map_acquire_tryread(l)		lock_acquire_shared_recursive(l, 0, 1, NULL, _THIS_IP_)
 #define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
 
 #ifdef CONFIG_PROVE_LOCKING

commit f0bab73cb539fb803c4d419951e8d28aa4964f8f
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Wed Aug 6 13:22:01 2014 -0400

    locking/lockdep: Restrict the use of recursive read_lock() with qrwlock
    
    Unlike the original unfair rwlock implementation, queued rwlock
    will grant lock according to the chronological sequence of the lock
    requests except when the lock requester is in the interrupt context.
    Consequently, recursive read_lock calls will now hang the process if
    there is a write_lock call somewhere in between the read_lock calls.
    
    This patch updates the lockdep implementation to look for recursive
    read_lock calls. A new read state (3) is used to mark those read_lock
    call that cannot be recursively called except in the interrupt
    context. The new read state does exhaust the 2 bits available in
    held_lock:read bit field. The addition of any new read state in the
    future may require a redesign of how all those bits are squeezed
    together in the held_lock structure.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Maarten Lankhorst <maarten.lankhorst@canonical.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1407345722-61615-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index f388481201cd..b5a84b62fb84 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -478,16 +478,24 @@ static inline void print_irqtrace_events(struct task_struct *curr)
  * on the per lock-class debug mode:
  */
 
+/*
+ * Read states in the 2-bit held_lock:read field:
+ *  0: Exclusive lock
+ *  1: Shareable lock, cannot be recursively called
+ *  2: Shareable lock, can be recursively called
+ *  3: Shareable lock, cannot be recursively called except in interrupt context
+ */
 #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 1, n, i)
 #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 1, n, i)
 #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 1, n, i)
+#define lock_acquire_shared_irecursive(l, s, t, n, i)	lock_acquire(l, s, t, 3, 1, n, i)
 
 #define spin_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define spin_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
 #define spin_release(l, n, i)			lock_release(l, n, i)
 
 #define rwlock_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
-#define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_recursive(l, s, t, NULL, i)
+#define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_irecursive(l, s, t, NULL, i)
 #define rwlock_release(l, n, i)			lock_release(l, n, i)
 
 #define seqcount_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)

commit 214e0aed639ef40987bf6159fad303171a6de31e
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Wed Jul 30 13:41:55 2014 -0700

    locking/Documentation: Move locking related docs into Documentation/locking/
    
    Specifically:
      Documentation/locking/lockdep-design.txt
      Documentation/locking/lockstat.txt
      Documentation/locking/mutex-design.txt
      Documentation/locking/rt-mutex-design.txt
      Documentation/locking/rt-mutex.txt
      Documentation/locking/spinlocks.txt
      Documentation/locking/ww-mutex-design.txt
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: jason.low2@hp.com
    Cc: aswin@hp.com
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lubomir Rintel <lkundrak@v3.sk>
    Cc: Masanari Iida <standby24x7@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: fengguang.wu@intel.com
    Link: http://lkml.kernel.org/r/1406752916-3341-6-git-send-email-davidlohr@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 008388f920d7..f388481201cd 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -4,7 +4,7 @@
  *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
  *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
  *
- * see Documentation/lockdep-design.txt for more details.
+ * see Documentation/locking/lockdep-design.txt for more details.
  */
 #ifndef __LINUX_LOCKDEP_H
 #define __LINUX_LOCKDEP_H

commit 176ab02d4916f09d5d8cb63372d142df4378cdea
Merge: e06df6a7eae1 ef178f9238b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 31 14:13:25 2014 -0700

    Merge branch 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 LTO changes from Peter Anvin:
     "More infrastructure work in preparation for link-time optimization
      (LTO).  Most of these changes is to make sure symbols accessed from
      assembly code are properly marked as visible so the linker doesn't
      remove them.
    
      My understanding is that the changes to support LTO are still not
      upstream in binutils, but are on the way there.  This patchset should
      conclude the x86-specific changes, and remaining patches to actually
      enable LTO will be fed through the Kbuild tree (other than keeping up
      with changes to the x86 code base, of course), although not
      necessarily in this merge window"
    
    * 'x86-asmlinkage-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (25 commits)
      Kbuild, lto: Handle basic LTO in modpost
      Kbuild, lto: Disable LTO for asm-offsets.c
      Kbuild, lto: Add a gcc-ld script to let run gcc as ld
      Kbuild, lto: add ld-version and ld-ifversion macros
      Kbuild, lto: Drop .number postfixes in modpost
      Kbuild, lto, workaround: Don't warn for initcall_reference in modpost
      lto: Disable LTO for sys_ni
      lto: Handle LTO common symbols in module loader
      lto, workaround: Add workaround for initcall reordering
      lto: Make asmlinkage __visible
      x86, lto: Disable LTO for the x86 VDSO
      initconst, x86: Fix initconst mistake in ts5500 code
      initconst: Fix initconst mistake in dcdbas
      asmlinkage: Make trace_hardirqs_on/off_caller visible
      asmlinkage, x86: Fix 32bit memcpy for LTO
      asmlinkage Make __stack_chk_failed and memcmp visible
      asmlinkage: Mark rwsem functions that can be called from assembler asmlinkage
      asmlinkage: Make main_extable_sort_needed visible
      asmlinkage, mutex: Mark __visible
      asmlinkage: Make trace_hardirq visible
      ...

commit 63f9a7fde715352e0769302527670542a664b981
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sat Feb 8 08:52:01 2014 +0100

    asmlinkage: Make lockdep_sys_exit asmlinkage
    
    lockdep_sys_exit can be called from assembler code, so make it
    asmlinkage.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1391845930-28580-5-git-send-email-ak@linux.intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 92b1bfc5da60..7df9aa6902c0 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -265,7 +265,7 @@ extern void lockdep_info(void);
 extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
 extern void lockdep_free_key_range(void *start, unsigned long size);
-extern void lockdep_sys_exit(void);
+extern asmlinkage void lockdep_sys_exit(void);
 
 extern void lockdep_off(void);
 extern void lockdep_on(void);

commit 47be1c1a0e188232b5e5962917b21750053cd3f8
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 20 19:20:16 2014 +0100

    lockdep: Change lockdep_set_novalidate_class() to use _and_name
    
    Cosmetic. This doesn't really matter because a) device->mutex is
    the only user of __lockdep_no_validate__ and b) this class should
    be never reported as the source of problem, but if something goes
    wrong "&dev->mutex" looks better than "&__lockdep_no_validate__"
    as the name of the lock.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140120182016.GA26512@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 1626047c1f26..060e5137fd80 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -303,7 +303,7 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 				 (lock)->dep_map.key, sub)
 
 #define lockdep_set_novalidate_class(lock) \
-	lockdep_set_class(lock, &__lockdep_no_validate__)
+	lockdep_set_class_and_name(lock, &__lockdep_no_validate__, #lock)
 /*
  * Compare locking classes
  */

commit fb9edbe98493fcd9df66de926ae9157cbe0e4dcd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 20 19:20:06 2014 +0100

    lockdep: Make held_lock->check and "int check" argument bool
    
    The "int check" argument of lock_acquire() and held_lock->check are
    misleading. This is actually a boolean: 2 means "true", everything
    else is "false".
    
    And there is no need to pass 1 or 0 to lock_acquire() depending on
    CONFIG_PROVE_LOCKING, __lock_acquire() checks prove_locking at the
    start and clears "check" if !CONFIG_PROVE_LOCKING.
    
    Note: probably we can simply kill this member/arg. The only explicit
    user of check => 0 is rcu_lock_acquire(), perhaps we can change it to
    use lock_acquire(trylock =>, read => 2). __lockdep_no_validate means
    check => 0 implicitly, but we can change validate_chain() to check
    hlock->instance->key instead. Not to mention it would be nice to get
    rid of lockdep_set_novalidate_class().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140120182006.GA26495@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 92b1bfc5da60..1626047c1f26 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -252,9 +252,9 @@ struct held_lock {
 	unsigned int trylock:1;						/* 16 bits */
 
 	unsigned int read:2;        /* see lock_acquire() comment */
-	unsigned int check:2;       /* see lock_acquire() comment */
+	unsigned int check:1;       /* see lock_acquire() comment */
 	unsigned int hardirqs_off:1;
-	unsigned int references:11;					/* 32 bits */
+	unsigned int references:12;					/* 32 bits */
 };
 
 /*
@@ -326,9 +326,8 @@ static inline int lockdep_match_key(struct lockdep_map *lock,
  *
  * Values for check:
  *
- *   0: disabled
- *   1: simple checks (freeing, held-at-exit-time, etc.)
- *   2: full validation
+ *   0: simple checks (freeing, held-at-exit-time, etc.)
+ *   1: full validation
  */
 extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 			 int trylock, int read, int check,
@@ -479,15 +478,9 @@ static inline void print_irqtrace_events(struct task_struct *curr)
  * on the per lock-class debug mode:
  */
 
-#ifdef CONFIG_PROVE_LOCKING
- #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 2, n, i)
- #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 2, n, i)
- #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 2, n, i)
-#else
- #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 1, n, i)
- #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 1, n, i)
- #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 1, n, i)
-#endif
+#define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 1, n, i)
+#define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 1, n, i)
+#define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 1, n, i)
 
 #define spin_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define spin_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
@@ -518,13 +511,13 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 # define might_lock(lock) 						\
 do {									\
 	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
-	lock_acquire(&(lock)->dep_map, 0, 0, 0, 2, NULL, _THIS_IP_);	\
+	lock_acquire(&(lock)->dep_map, 0, 0, 0, 1, NULL, _THIS_IP_);	\
 	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
 } while (0)
 # define might_lock_read(lock) 						\
 do {									\
 	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
-	lock_acquire(&(lock)->dep_map, 0, 0, 1, 2, NULL, _THIS_IP_);	\
+	lock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);	\
 	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
 } while (0)
 #else

commit 1ca7d67cf5d5a2aef26a8d9afd789006fa098347
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Oct 7 15:51:59 2013 -0700

    seqcount: Add lockdep functionality to seqcount/seqlock structures
    
    Currently seqlocks and seqcounts don't support lockdep.
    
    After running across a seqcount related deadlock in the timekeeping
    code, I used a less-refined and more focused variant of this patch
    to narrow down the cause of the issue.
    
    This is a first-pass attempt to properly enable lockdep functionality
    on seqlocks and seqcounts.
    
    Since seqcounts are used in the vdso gettimeofday code, I've provided
    non-lockdep accessors for those needs.
    
    I've also handled one case where there were nested seqlock writers
    and there may be more edge cases.
    
    Comments and feedback would be appreciated!
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/1381186321-4906-3-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index cfc2f119779a..92b1bfc5da60 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -497,6 +497,10 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_recursive(l, s, t, NULL, i)
 #define rwlock_release(l, n, i)			lock_release(l, n, i)
 
+#define seqcount_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
+#define seqcount_acquire_read(l, s, t, i)	lock_acquire_shared_recursive(l, s, t, NULL, i)
+#define seqcount_release(l, n, i)		lock_release(l, n, i)
+
 #define mutex_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define mutex_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
 #define mutex_release(l, n, i)			lock_release(l, n, i)
@@ -504,11 +508,11 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #define rwsem_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
 #define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
 #define rwsem_acquire_read(l, s, t, i)		lock_acquire_shared(l, s, t, NULL, i)
-# define rwsem_release(l, n, i)			lock_release(l, n, i)
+#define rwsem_release(l, n, i)			lock_release(l, n, i)
 
 #define lock_map_acquire(l)			lock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)
 #define lock_map_acquire_read(l)		lock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)
-# define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
+#define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
 
 #ifdef CONFIG_PROVE_LOCKING
 # define might_lock(lock) 						\

commit a51805efae5dda0da66f79268ffcf0715f9dbea4
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jul 8 14:23:49 2013 -0700

    lockdep: Introduce lock_acquire_exclusive()/shared() helper macros
    
    In lockdep.h, the spinlock/mutex/rwsem/rwlock/lock_map acquire macros have
    different definitions based on the value of CONFIG_PROVE_LOCKING.  We have
    separate ifdefs for each of these definitions, which seems redundant.
    
    Introduce lock_acquire_{exclusive,shared,shared_recursive} helpers which
    will have different definitions based on CONFIG_PROVE_LOCKING.  Then all
    other helper macros can be defined based on the above ones, which reduces
    the amount of ifdefined code.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: "Paul E. McKenney" <paulmck@us.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130708212350.6DD1931C15E@corp2gmr1-1.hot.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index f1e877b79ed8..cfc2f119779a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -365,7 +365,7 @@ extern void lockdep_trace_alloc(gfp_t mask);
 
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 
-#else /* !LOCKDEP */
+#else /* !CONFIG_LOCKDEP */
 
 static inline void lockdep_off(void)
 {
@@ -479,82 +479,36 @@ static inline void print_irqtrace_events(struct task_struct *curr)
  * on the per lock-class debug mode:
  */
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# ifdef CONFIG_PROVE_LOCKING
-#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
-#  define spin_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
-# else
-#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
-#  define spin_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, NULL, i)
-# endif
-# define spin_release(l, n, i)			lock_release(l, n, i)
+#ifdef CONFIG_PROVE_LOCKING
+ #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 2, n, i)
+ #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 2, n, i)
+ #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 2, n, i)
 #else
-# define spin_acquire(l, s, t, i)		do { } while (0)
-# define spin_release(l, n, i)			do { } while (0)
+ #define lock_acquire_exclusive(l, s, t, n, i)		lock_acquire(l, s, t, 0, 1, n, i)
+ #define lock_acquire_shared(l, s, t, n, i)		lock_acquire(l, s, t, 1, 1, n, i)
+ #define lock_acquire_shared_recursive(l, s, t, n, i)	lock_acquire(l, s, t, 2, 1, n, i)
 #endif
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# ifdef CONFIG_PROVE_LOCKING
-#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
-#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, NULL, i)
-# else
-#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
-#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, NULL, i)
-# endif
-# define rwlock_release(l, n, i)		lock_release(l, n, i)
-#else
-# define rwlock_acquire(l, s, t, i)		do { } while (0)
-# define rwlock_acquire_read(l, s, t, i)	do { } while (0)
-# define rwlock_release(l, n, i)		do { } while (0)
-#endif
+#define spin_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
+#define spin_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
+#define spin_release(l, n, i)			lock_release(l, n, i)
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# ifdef CONFIG_PROVE_LOCKING
-#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
-#  define mutex_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
-# else
-#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
-#  define mutex_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, n, i)
-# endif
-# define mutex_release(l, n, i)			lock_release(l, n, i)
-#else
-# define mutex_acquire(l, s, t, i)		do { } while (0)
-# define mutex_acquire_nest(l, s, t, n, i)	do { } while (0)
-# define mutex_release(l, n, i)			do { } while (0)
-#endif
+#define rwlock_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
+#define rwlock_acquire_read(l, s, t, i)		lock_acquire_shared_recursive(l, s, t, NULL, i)
+#define rwlock_release(l, n, i)			lock_release(l, n, i)
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# ifdef CONFIG_PROVE_LOCKING
-#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
-#  define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
-#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 2, NULL, i)
-# else
-#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
-#  define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, n, i)
-#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 1, NULL, i)
-# endif
+#define mutex_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
+#define mutex_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
+#define mutex_release(l, n, i)			lock_release(l, n, i)
+
+#define rwsem_acquire(l, s, t, i)		lock_acquire_exclusive(l, s, t, NULL, i)
+#define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire_exclusive(l, s, t, n, i)
+#define rwsem_acquire_read(l, s, t, i)		lock_acquire_shared(l, s, t, NULL, i)
 # define rwsem_release(l, n, i)			lock_release(l, n, i)
-#else
-# define rwsem_acquire(l, s, t, i)		do { } while (0)
-# define rwsem_acquire_nest(l, s, t, n, i)	do { } while (0)
-# define rwsem_acquire_read(l, s, t, i)		do { } while (0)
-# define rwsem_release(l, n, i)			do { } while (0)
-#endif
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# ifdef CONFIG_PROVE_LOCKING
-#  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, NULL, _THIS_IP_)
-#  define lock_map_acquire_read(l)	lock_acquire(l, 0, 0, 2, 2, NULL, _THIS_IP_)
-# else
-#  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, NULL, _THIS_IP_)
-#  define lock_map_acquire_read(l)	lock_acquire(l, 0, 0, 2, 1, NULL, _THIS_IP_)
-# endif
+#define lock_map_acquire(l)			lock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)
+#define lock_map_acquire_read(l)		lock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)
 # define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
-#else
-# define lock_map_acquire(l)			do { } while (0)
-# define lock_map_acquire_read(l)		do { } while (0)
-# define lock_map_release(l)			do { } while (0)
-#endif
 
 #ifdef CONFIG_PROVE_LOCKING
 # define might_lock(lock) 						\

commit 3b5d8510b94a95e493e8c4951ffc3d1cf6a6792d
Merge: c47f39e3b75e 41ef8f826692
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 22 19:25:09 2013 -0800

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core locking changes from Ingo Molnar:
     "The biggest change is the rwsem lock-steal improvements, both to the
      assembly optimized and the spinlock based variants.
    
      The other notable change is the clean up of the seqlock implementation
      to be based on the seqcount infrastructure.
    
      The rest is assorted smaller debuggability, cleanup and continued -rt
      locking changes."
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      rwsem-spinlock: Implement writer lock-stealing for better scalability
      futex: Revert "futex: Mark get_robust_list as deprecated"
      generic: Use raw local irq variant for generic cmpxchg
      lockdep: Selftest: convert spinlock to raw spinlock
      seqlock: Use seqcount infrastructure
      seqlock: Remove unused functions
      ntp: Make ntp_lock raw
      intel_idle: Convert i7300_idle_lock to raw_spinlock
      locking: Various static lock initializer fixes
      lockdep: Print more info when MAX_LOCK_DEPTH is exceeded
      rwsem: Implement writer lock-stealing for better scalability
      lockdep: Silence warning if CONFIG_LOCKDEP isn't set
      watchdog: Use local_clock for get_timestamp()
      lockdep: Rename print_unlock_inbalance_bug() to print_unlock_imbalance_bug()
      locking/stat: Fix a typo

commit b1ae345d971664f70cfdc293029c40ccfb093591
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Thu Feb 21 16:42:47 2013 -0800

    lockdep: make lockdep_assert_held() not have a return value
    
    I recently made the mistake of writing:
    
      foo = lockdep_dereference_protected(..., lockdep_assert_held(...));
    
    which is clearly bogus.  If lockdep is disabled in the config this would
    cause a compile failure, if it is enabled then it compiles and causes a
    puzzling warning about dereferencing without the correct protection.
    
    Wrap the macro in "do { ...  } while (0)" to also fail compile for this
    when lockdep is enabled.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 2bca44b0893c..bfe88c4aa251 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -359,7 +359,9 @@ extern void lockdep_trace_alloc(gfp_t mask);
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
-#define lockdep_assert_held(l)	WARN_ON(debug_locks && !lockdep_is_held(l))
+#define lockdep_assert_held(l)	do {				\
+		WARN_ON(debug_locks && !lockdep_is_held(l));	\
+	} while (0)
 
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 

commit 5cd3f5affad2109fd1458aab3f6216f2181e26ea
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Thu Jan 24 21:53:17 2013 +0100

    lockdep: Silence warning if CONFIG_LOCKDEP isn't set
    
    Since commit c9a4962881929df7f1ef6e63e1b9da304faca4dd ("nfsd:
    make client_lock per net") compiling nfs4state.o without
    CONFIG_LOCKDEP set, triggers this GCC warning:
    
        fs/nfsd/nfs4state.c: In function â€˜free_clientâ€™:
        fs/nfsd/nfs4state.c:1051:19: warning: unused variable â€˜nnâ€™ [-Wunused-variable]
    
    The cause of that warning is that lockdep_assert_held() compiles
    away if CONFIG_LOCKDEP is not set. Silence this warning by using
    the argument to lockdep_assert_held() as a nop if CONFIG_LOCKDEP
    is not set.
    
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislav Kinsbursky <skinsbursky@parallels.com>
    Cc: J. Bruce Fields <bfields@redhat.com>
    Link: http://lkml.kernel.org/r/1359060797.1325.33.camel@x61.thuisdomein
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    --
     include/linux/lockdep.h |    2 +-
     1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 2bca44b0893c..f05631effc73 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -410,7 +410,7 @@ struct lock_class_key { };
 
 #define lockdep_depth(tsk)	(0)
 
-#define lockdep_assert_held(l)			do { } while (0)
+#define lockdep_assert_held(l)			do { (void)(l); } while (0)
 
 #define lockdep_recursing(tsk)			(0)
 

commit 1b963c81b14509e330e0fe3218b645ece2738dc5
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Jan 11 14:31:56 2013 -0800

    lockdep, rwsem: provide down_write_nest_lock()
    
    down_write_nest_lock() provides a means to annotate locking scenario
    where an outer lock is guaranteed to serialize the order nested locks
    are being acquired.
    
    This is analogoue to already existing mutex_lock_nest_lock() and
    spin_lock_nest_lock().
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 00e46376e28f..2bca44b0893c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -524,14 +524,17 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
 #  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
 #  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 2, NULL, i)
 # else
 #  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define rwsem_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, n, i)
 #  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 1, NULL, i)
 # endif
 # define rwsem_release(l, n, i)			lock_release(l, n, i)
 #else
 # define rwsem_acquire(l, s, t, i)		do { } while (0)
+# define rwsem_acquire_nest(l, s, t, n, i)	do { } while (0)
 # define rwsem_acquire_read(l, s, t, i)		do { } while (0)
 # define rwsem_release(l, n, i)			do { } while (0)
 #endif

commit 4d82a1debbffec129cc387aafa8f40b7bbab3297
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 15 08:06:19 2012 -0700

    lockdep: fix oops in processing workqueue
    
    Under memory load, on x86_64, with lockdep enabled, the workqueue's
    process_one_work() has been seen to oops in __lock_acquire(), barfing
    on a 0xffffffff00000000 pointer in the lockdep_map's class_cache[].
    
    Because it's permissible to free a work_struct from its callout function,
    the map used is an onstack copy of the map given in the work_struct: and
    that copy is made without any locking.
    
    Surprisingly, gcc (4.5.1 in Hugh's case) uses "rep movsl" rather than
    "rep movsq" for that structure copy: which might race with a workqueue
    user's wait_on_work() doing lock_map_acquire() on the source of the
    copy, putting a pointer into the class_cache[], but only in time for
    the top half of that pointer to be copied to the destination map.
    
    Boom when process_one_work() subsequently does lock_map_acquire()
    on its onstack copy of the lockdep_map.
    
    Fix this, and a similar instance in call_timer_fn(), with a
    lockdep_copy_map() function which additionally NULLs the class_cache[].
    
    Note: this oops was actually seen on 3.4-next, where flush_work() newly
    does the racing lock_map_acquire(); but Tejun points out that 3.4 and
    earlier are already vulnerable to the same through wait_on_work().
    
    * Patch orginally from Peter.  Hugh modified it a bit and wrote the
      description.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Reported-by: Hugh Dickins <hughd@google.com>
    LKML-Reference: <alpine.LSU.2.00.1205070951170.1544@eggly.anvils>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index d36619ead3ba..00e46376e28f 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -157,6 +157,24 @@ struct lockdep_map {
 #endif
 };
 
+static inline void lockdep_copy_map(struct lockdep_map *to,
+				    struct lockdep_map *from)
+{
+	int i;
+
+	*to = *from;
+	/*
+	 * Since the class cache can be modified concurrently we could observe
+	 * half pointers (64bit arch using 32bit copy insns). Therefore clear
+	 * the caches and take the performance hit.
+	 *
+	 * XXX it doesn't work well with lockdep_set_class_and_subclass(), since
+	 *     that relies on cache abuse.
+	 */
+	for (i = 0; i < NR_LOCKDEP_CACHING_CLASSES; i++)
+		to->class_cache[i] = NULL;
+}
+
 /*
  * Every lock has a list of other locks that were taken after it.
  * We only grow the list, never remove from it:

commit 94d24fc47219219b5aa23b45956cc37ee5aa5b01
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jun 7 11:17:30 2011 +0200

    printk, lockdep: Disable lock debugging on zap_locks()
    
    zap_locks() is used by printk() in a last ditch effort to get data
    out, clearly we cannot trust lock state after this so make it disable
    lock debugging.
    
    Also don't treat printk recursion through lockdep as a normal
    recursion bug but try hard to get the lockdep splat out.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-kqxwmo4xz37e1s8w0xopvr0q@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b6a56e37284c..d36619ead3ba 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -343,6 +343,8 @@ extern void lockdep_trace_alloc(gfp_t mask);
 
 #define lockdep_assert_held(l)	WARN_ON(debug_locks && !lockdep_is_held(l))
 
+#define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
+
 #else /* !LOCKDEP */
 
 static inline void lockdep_off(void)
@@ -392,6 +394,8 @@ struct lock_class_key { };
 
 #define lockdep_assert_held(l)			do { } while (0)
 
+#define lockdep_recursing(tsk)			(0)
+
 #endif /* !LOCKDEP */
 
 #ifdef CONFIG_LOCK_STAT

commit b3fbab0571eb09746cc0283648165ec00efc8eb2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 24 08:31:09 2011 -0700

    rcu: Restore checks for blocking in RCU read-side critical sections
    
    Long ago, using TREE_RCU with PREEMPT would result in "scheduling
    while atomic" diagnostics if you blocked in an RCU read-side critical
    section.  However, PREEMPT now implies TREE_PREEMPT_RCU, which defeats
    this diagnostic.  This commit therefore adds a replacement diagnostic
    based on PROVE_RCU.
    
    Because rcu_lockdep_assert() and lockdep_rcu_dereference() are now being
    used for things that have nothing to do with rcu_dereference(), rename
    lockdep_rcu_dereference() to lockdep_rcu_suspicious() and add a third
    argument that is a string indicating what is suspicious.  This third
    argument is passed in from a new third argument to rcu_lockdep_assert().
    Update all calls to rcu_lockdep_assert() to add an informative third
    argument.
    
    Also, add a pair of rcu_lockdep_assert() calls from within
    rcu_note_context_switch(), one complaining if a context switch occurs
    in an RCU-bh read-side critical section and another complaining if a
    context switch occurs in an RCU-sched read-side critical section.
    These are present only if the PROVE_RCU kernel parameter is enabled.
    
    Finally, fix some checkpatch whitespace complaints in lockdep.c.
    
    Again, you must enable PROVE_RCU to see these new diagnostics.  But you
    are enabling PROVE_RCU to check out new RCU uses in any case, aren't you?
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index ef820a3c378b..b6a56e37284c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -548,7 +548,7 @@ do {									\
 #endif
 
 #ifdef CONFIG_PROVE_RCU
-extern void lockdep_rcu_dereference(const char *file, const int line);
+void lockdep_rcu_suspicious(const char *file, const int line, const char *s);
 #endif
 
 #endif /* __LINUX_LOCKDEP_H */

commit e4c70a6629f9c74c4b0de258a3951890e9047c82
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:12:03 2011 -0700

    lockdep, mutex: provide mutex_lock_nest_lock
    
    In order to convert i_mmap_lock to a mutex we need a mutex equivalent to
    spin_lock_nest_lock(), thus provide the mutex_lock_nest_lock() annotation.
    
    As with spin_lock_nest_lock(), mutex_lock_nest_lock() allows annotation of
    the locking pattern where an outer lock serializes the acquisition order
    of nested locks.  That is, if every time you lock multiple locks A, say A1
    and A2 you first acquire N, the order of acquiring A1 and A2 is
    irrelevant.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 4aef1dda6406..ef820a3c378b 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -487,12 +487,15 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
 #  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define mutex_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
 # else
 #  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define mutex_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, n, i)
 # endif
 # define mutex_release(l, n, i)			lock_release(l, n, i)
 #else
 # define mutex_acquire(l, s, t, i)		do { } while (0)
+# define mutex_acquire_nest(l, s, t, n, i)	do { } while (0)
 # define mutex_release(l, n, i)			do { } while (0)
 #endif
 

commit 5bf7a6503ff416214b9a53569677dbf07657e6fd
Merge: 0f5c2ac58f22 42c025f3de90
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 21 13:38:57 2011 -0800

    Merge branch 'fixes-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    * 'fixes-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: note the nested NOT_RUNNING test in worker_clr_flags() isn't a noop
      workqueue: relax lockdep annotation on flush_work()

commit 2ce802f62ba32a7d95748ac92bf351f76affb6ff
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 20 12:06:35 2011 +0100

    lockdep: Move early boot local IRQ enable/disable status to init/main.c
    
    During early boot, local IRQ is disabled until IRQ subsystem is
    properly initialized.  During this time, no one should enable
    local IRQ and some operations which usually are not allowed with
    IRQ disabled, e.g. operations which might sleep or require
    communications with other processors, are allowed.
    
    lockdep tracked this with early_boot_irqs_off/on() callbacks.
    As other subsystems need this information too, move it to
    init/main.c and make it generally available.  While at it,
    toggle the boolean to early_boot_irqs_disabled instead of
    enabled so that it can be initialized with %false and %true
    indicates the exceptional condition.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <20110120110635.GB6036@htj.dyndns.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 71c09b26c759..f638fd78d106 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -436,16 +436,8 @@ do {								\
 #endif /* CONFIG_LOCKDEP */
 
 #ifdef CONFIG_TRACE_IRQFLAGS
-extern void early_boot_irqs_off(void);
-extern void early_boot_irqs_on(void);
 extern void print_irqtrace_events(struct task_struct *curr);
 #else
-static inline void early_boot_irqs_off(void)
-{
-}
-static inline void early_boot_irqs_on(void)
-{
-}
 static inline void print_irqtrace_events(struct task_struct *curr)
 {
 }

commit e159489baa717dbae70f9903770a6a4990865887
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jan 9 23:32:15 2011 +0100

    workqueue: relax lockdep annotation on flush_work()
    
    Currently, the lockdep annotation in flush_work() requires exclusive
    access on the workqueue the target work is queued on and triggers
    warning if a work is trying to flush another work on the same
    workqueue; however, this is no longer true as workqueues can now
    execute multiple works concurrently.
    
    This patch adds lock_map_acquire_read() and make process_one_work()
    hold read access to the workqueue while executing a work and
    start_flush_work() check for write access if concurrnecy level is one
    or the workqueue has a rescuer (as only one execution resource - the
    rescuer - is guaranteed to be available under memory pressure), and
    read access if higher.
    
    This better represents what's going on and removes spurious lockdep
    warnings which are triggered by fake dependency chain created through
    flush_work().
    
    * Peter pointed out that flushing another work from a WQ_MEM_RECLAIM
      wq breaks forward progress guarantee under memory pressure.
      Condition check accordingly updated.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Tested-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@kernel.org

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 71c09b26c759..9f19430c7d07 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -522,12 +522,15 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
 #  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, NULL, _THIS_IP_)
+#  define lock_map_acquire_read(l)	lock_acquire(l, 0, 0, 2, 2, NULL, _THIS_IP_)
 # else
 #  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, NULL, _THIS_IP_)
+#  define lock_map_acquire_read(l)	lock_acquire(l, 0, 0, 2, 1, NULL, _THIS_IP_)
 # endif
 # define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
 #else
 # define lock_map_acquire(l)			do { } while (0)
+# define lock_map_acquire_read(l)		do { } while (0)
 # define lock_map_release(l)			do { } while (0)
 #endif
 

commit 4a60cfa9457749f7987fd4f3c956dbba5a281129
Merge: 62bea97f54d8 27afdf2008da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 14:11:46 2010 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (96 commits)
      apic, x86: Use BIOS settings for IBS and MCE threshold interrupt LVT offsets
      apic, x86: Check if EILVT APIC registers are available (AMD only)
      x86: ioapic: Call free_irte only if interrupt remapping enabled
      arm: Use ARCH_IRQ_INIT_FLAGS
      genirq, ARM: Fix boot on ARM platforms
      genirq: Fix CONFIG_GENIRQ_NO_DEPRECATED=y build
      x86: Switch sparse_irq allocations to GFP_KERNEL
      genirq: Switch sparse_irq allocator to GFP_KERNEL
      genirq: Make sparse_lock a mutex
      x86: lguest: Use new irq allocator
      genirq: Remove the now unused sparse irq leftovers
      genirq: Sanitize dynamic irq handling
      genirq: Remove arch_init_chip_data()
      x86: xen: Sanitise sparse_irq handling
      x86: Use sane enumeration
      x86: uv: Clean up the direct access to irq_desc
      x86: Make io_apic.c local functions static
      genirq: Remove irq_2_iommu
      x86: Speed up the irq_remapped check in hot pathes
      intr_remap: Simplify the code further
      ...
    
    Fix up trivial conflicts in arch/x86/Kconfig

commit 620162505e5d46bc4494b1761743e4b0b3bf8e16
Author: Hitoshi Mitake <mitake@dcl.info.waseda.ac.jp>
Date:   Tue Oct 5 18:01:51 2010 +0900

    lockdep: Add improved subclass caching
    
    Current lockdep_map only caches one class with subclass == 0,
    and looks up hash table of classes when subclass != 0.
    
    It seems that this has no problem because the case of
    subclass != 0 is rare. But locks of struct rq are
    acquired with subclass == 1 when task migration is executed.
    Task migration is high frequent event, so I modified lockdep
    to cache subclasses.
    
    I measured the score of perf bench sched messaging.
    This patch has slightly but certain (order of milli seconds
    or 10 milli seconds) effect when lots of tasks are running.
    I'll show the result in the tail of this description.
    
    NR_LOCKDEP_CACHING_CLASSES specifies how many classes can be
    cached in the instances of lockdep_map.
    I discussed with Peter Zijlstra in LinuxCon Japan about
    this approach and he taught me that caching every subclasses(8)
    is cleary waste of memory. So number of cached classes
    should be configurable.
    
    === Score comparison of benchmarks ===
    # "min" means best score, and "max" means worst score
    
    for i in `seq 1 10`; do ./perf bench -f simple sched messaging; done
    
    before: min: 0.565000, max: 0.583000, avg: 0.572500
    after:  min: 0.559000, max: 0.568000, avg: 0.563300
    
    # with more processes
    for i in `seq 1 10`; do ./perf bench -f simple sched messaging -g 40; done
    
    before: min: 2.274000, max: 2.298000, avg: 2.286300
    after:  min: 2.242000, max: 2.270000, avg: 2.259700
    
    Signed-off-by: Hitoshi Mitake <mitake@dcl.info.waseda.ac.jp>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286269311-28336-2-git-send-email-mitake@dcl.info.waseda.ac.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 06aed8305bf3..2186a64ee4b5 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -31,6 +31,17 @@ extern int lock_stat;
 
 #define MAX_LOCKDEP_SUBCLASSES		8UL
 
+/*
+ * NR_LOCKDEP_CACHING_CLASSES ... Number of classes
+ * cached in the instance of lockdep_map
+ *
+ * Currently main class (subclass == 0) and signle depth subclass
+ * are cached in lockdep_map. This optimization is mainly targeting
+ * on rq->lock. double_rq_lock() acquires this highly competitive with
+ * single depth.
+ */
+#define NR_LOCKDEP_CACHING_CLASSES	2
+
 /*
  * Lock-classes are keyed via unique addresses, by embedding the
  * lockclass-key into the kernel (or module) .data section. (For
@@ -138,7 +149,7 @@ void clear_lock_stats(struct lock_class *class);
  */
 struct lockdep_map {
 	struct lock_class_key		*key;
-	struct lock_class		*class_cache;
+	struct lock_class		*class_cache[NR_LOCKDEP_CACHING_CLASSES];
 	const char			*name;
 #ifdef CONFIG_LOCK_STAT
 	int				cpu;

commit 154cd387cdf0e5566ce523cbddf92dd2a062dfd6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 22 15:58:45 2010 +0200

    genirq: Remove early_init_irq_lock_class()
    
    early_init_irq_lock_class() is called way before anything touches the
    irq descriptors. In case of SPARSE_IRQ=y this is a NOP operation
    because the radix tree is empty at this point. For the SPARSE_IRQ=n
    case it's sufficient to set the lock class in early_init_irq().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 06aed8305bf3..17d050ce7ab8 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -424,14 +424,6 @@ do {								\
 
 #endif /* CONFIG_LOCKDEP */
 
-#ifdef CONFIG_GENERIC_HARDIRQS
-extern void early_init_irq_lock_class(void);
-#else
-static inline void early_init_irq_lock_class(void)
-{
-}
-#endif
-
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void early_boot_irqs_off(void);
 extern void early_boot_irqs_on(void);

commit 1704f47b50b5d9e1b825e43e1baaf2c5897baf03
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 19 01:37:42 2010 +0100

    lockdep: Add novalidate class for dev->mutex conversion
    
    The conversion of device->sem to device->mutex resulted in lockdep
    warnings. Create a novalidate class for now until the driver folks
    come up with separate classes. That way we have at least the basic
    mutex debugging coverage.
    
    Add a checkpatch error so the usage is reserved for device->mutex.
    
    [ tglx: checkpatch and compile fix for LOCKDEP=n ]
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index a03977a96d7e..06aed8305bf3 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -44,6 +44,8 @@ struct lock_class_key {
 	struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
 };
 
+extern struct lock_class_key __lockdep_no_validate__;
+
 #define LOCKSTAT_POINTS		4
 
 /*
@@ -270,6 +272,9 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 #define lockdep_set_subclass(lock, sub)	\
 		lockdep_init_map(&(lock)->dep_map, #lock, \
 				 (lock)->dep_map.key, sub)
+
+#define lockdep_set_novalidate_class(lock) \
+	lockdep_set_class(lock, &__lockdep_no_validate__)
 /*
  * Compare locking classes
  */
@@ -354,6 +359,9 @@ static inline void lockdep_on(void)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
 		do { (void)(key); } while (0)
 #define lockdep_set_subclass(lock, sub)		do { } while (0)
+
+#define lockdep_set_novalidate_class(lock) do { } while (0)
+
 /*
  * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
  * case since the result is not well defined and the caller should rather

commit 2edf5e49800846a2b2b6461d99cdae18067c440f
Author: Dave Young <hidave.darkstar@gmail.com>
Date:   Wed Mar 10 15:24:10 2010 -0800

    sysctl extern cleanup: lockdep
    
    Extern declarations in sysctl.c should be moved to their own header file,
    and then include them in relavant .c files.
    
    Move lockdep extern declarations to linux/lockdep.h
    
    Signed-off-by: Dave Young <hidave.darkstar@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 10206a87da19..a03977a96d7e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -12,6 +12,10 @@
 struct task_struct;
 struct lockdep_map;
 
+/* for sysctl */
+extern int prove_locking;
+extern int lock_stat;
+
 #ifdef CONFIG_LOCKDEP
 
 #include <linux/linkage.h>

commit 0632eb3d7563d6a76d49a3860b6352d800c92854
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 22 17:04:47 2010 -0800

    rcu: Integrate rcu_dereference_check() message into lockdep
    
    Make rcu_dereference_check() print the list of held locks in
    addition to the stack dump to ease debugging.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1266887105-1528-3-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 9ccf0e286b2a..10206a87da19 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -534,4 +534,8 @@ do {									\
 # define might_lock_read(lock) do { } while (0)
 #endif
 
+#ifdef CONFIG_PROVE_RCU
+extern void lockdep_rcu_dereference(const char *file, const int line);
+#endif
+
 #endif /* __LINUX_LOCKDEP_H */

commit e351b660fddd4df76cc4635f896d311ed0ff3752
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Wed Jul 22 22:48:09 2009 +0800

    lockdep: Reintroduce generation count to make BFS faster
    
    We still can apply DaveM's generation count optimization to
    BFS, based on the following idea:
    
     - before doing each BFS, increase the global generation id
       by 1
    
     - if one node in the graph has been visited, mark it as
       visited by storing the current global generation id into
       the node's dep_gen_id field
    
     - so we can decide if one node has been visited already, by
       comparing the node's dep_gen_id with the global generation id.
    
    By applying DaveM's generation count optimization to current
    implementation of BFS, we gain the following advantages:
    
     - we save MAX_LOCKDEP_ENTRIES/8 bytes memory;
    
     - we remove the bitmap_zero(bfs_accessed, MAX_LOCKDEP_ENTRIES);
       in each BFS, which is very time-consuming since
       MAX_LOCKDEP_ENTRIES may be very large.(16384UL)
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "David S. Miller" <davem@davemloft.net>
    LKML-Reference: <1248274089-6358-1-git-send-email-tom.leiming@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 47d42eff6124..9ccf0e286b2a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -58,6 +58,7 @@ struct lock_class {
 
 	struct lockdep_subclass_key	*key;
 	unsigned int			subclass;
+	unsigned int			dep_gen_id;
 
 	/*
 	 * IRQ/softirq usage tracking bits:

commit bb97a91e2549a7f2df9c21d32542582f549ab3ec
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jul 20 19:15:35 2009 +0200

    lockdep: Deal with many similar locks
    
    spin_lock_nest_lock() allows to take many instances of the same
    class, this can easily lead to overflow of MAX_LOCK_DEPTH.
    
    To avoid this overflow, we'll stop accounting instances but
    start reference counting the class in the held_lock structure.
    
    [ We could maintain a list of instances, if we'd move the hlock
      stuff into __lock_acquired(), but that would require
      significant modifications to the current code. ]
    
    We restrict this mode to spin_lock_nest_lock() only, because it
    degrades the lockdep quality due to lost of instance.
    
    For lockstat this means we don't track lock statistics for any
    but the first lock in the series.
    
    Currently nesting is limited to 11 bits because that was the
    spare space available in held_lock. This yields a 2048
    instances maximium.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index a6d5e5e4d084..47d42eff6124 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -213,10 +213,12 @@ struct held_lock {
 	 * interrupt context:
 	 */
 	unsigned int irq_context:2; /* bit 0 - soft, bit 1 - hard */
-	unsigned int trylock:1;
+	unsigned int trylock:1;						/* 16 bits */
+
 	unsigned int read:2;        /* see lock_acquire() comment */
 	unsigned int check:2;       /* see lock_acquire() comment */
 	unsigned int hardirqs_off:1;
+	unsigned int references:11;					/* 32 bits */
 };
 
 /*

commit f607c6685774811b8112e124f10a053d77015485
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jul 20 19:16:29 2009 +0200

    lockdep: Introduce lockdep_assert_held()
    
    Add a lockdep helper to validate that we indeed are the owner
    of a lock.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 12aabfcb45f6..a6d5e5e4d084 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -296,6 +296,10 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
 
+#define lockdep_is_held(lock)	lock_is_held(&(lock)->dep_map)
+
+extern int lock_is_held(struct lockdep_map *lock);
+
 extern void lock_set_class(struct lockdep_map *lock, const char *name,
 			   struct lock_class_key *key, unsigned int subclass,
 			   unsigned long ip);
@@ -314,6 +318,8 @@ extern void lockdep_trace_alloc(gfp_t mask);
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
+#define lockdep_assert_held(l)	WARN_ON(debug_locks && !lockdep_is_held(l))
+
 #else /* !LOCKDEP */
 
 static inline void lockdep_off(void)
@@ -358,6 +364,8 @@ struct lock_class_key { };
 
 #define lockdep_depth(tsk)	(0)
 
+#define lockdep_assert_held(l)			do { } while (0)
+
 #endif /* !LOCKDEP */
 
 #ifdef CONFIG_LOCK_STAT

commit af012961450949ea297b209e091bd1a3805b8a0a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 16 15:44:29 2009 +0200

    lockdep: BFS cleanup
    
    Some cleanups of the lockdep code after the BFS series:
    
     - Remove the last traces of the generation id
     - Fixup comment style
     - Move the bfs routines into lockdep.c
     - Cleanup the bfs routines
    
    [ tom.leiming@gmail.com: Fix crash ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1246201486-7308-11-git-send-email-tom.leiming@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 9ec026f8d09e..12aabfcb45f6 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -58,7 +58,6 @@ struct lock_class {
 
 	struct lockdep_subclass_key	*key;
 	unsigned int			subclass;
-	unsigned int			dep_gen_id;
 
 	/*
 	 * IRQ/softirq usage tracking bits:
@@ -150,9 +149,9 @@ struct lock_list {
 	struct stack_trace		trace;
 	int				distance;
 
-	/*The parent field is used to implement breadth-first search,and
-	 *the bit 0 is reused to indicate if the lock has been accessed
-	 *in BFS.
+	/*
+	 * The parent field is used to implement breadth-first search, and the
+	 * bit 0 is reused to indicate if the lock has been accessed in BFS.
 	 */
 	struct lock_list		*parent;
 };

commit c94aa5ca3088018d2a7a9bd3258aefffe29df265
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Thu Jul 16 15:44:29 2009 +0200

    lockdep: Print the shortest dependency chain if finding a circle
    
    Currently lockdep will print the 1st circle detected if it
    exists when acquiring a new (next) lock.
    
    This patch prints the shortest path from the next lock to be
    acquired to the previous held lock if a circle is found.
    
    The patch still uses the current method to check circle, and
    once the circle is found, breadth-first search algorithem is
    used to compute the shortest path from the next lock to the
    previous lock in the forward lock dependency graph.
    
    Printing the shortest path will shorten the dependency chain,
    and make troubleshooting for possible circular locking easier.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1246201486-7308-2-git-send-email-tom.leiming@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b25d1b53df0d..9ec026f8d09e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -149,6 +149,12 @@ struct lock_list {
 	struct lock_class		*class;
 	struct stack_trace		trace;
 	int				distance;
+
+	/*The parent field is used to implement breadth-first search,and
+	 *the bit 0 is reused to indicate if the lock has been accessed
+	 *in BFS.
+	 */
+	struct lock_list		*parent;
 };
 
 /*

commit 9a7aa12f3911853a3574d47d567b81a2a5df7208
Author: Jan Kara <jack@suse.cz>
Date:   Thu Jun 4 15:26:49 2009 +0200

    vfs: Set special lockdep map for dirs only if not set by fs
    
    Some filesystems need to set lockdep map for i_mutex differently for
    different directories. For example OCFS2 has system directories (for
    orphan inode tracking and for gathering all system files like journal
    or quota files into a single place) which have different locking
    locking rules than standard directories. For a filesystem setting
    lockdep map is naturaly done when the inode is read but we have to
    modify unlock_new_inode() not to overwrite the lockdep map the filesystem
    has set.
    
    Acked-by: peterz@infradead.org
    CC: mingo@redhat.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Joel Becker <joel.becker@oracle.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index da5a5a1f4cd2..b25d1b53df0d 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -258,6 +258,16 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 #define lockdep_set_subclass(lock, sub)	\
 		lockdep_init_map(&(lock)->dep_map, #lock, \
 				 (lock)->dep_map.key, sub)
+/*
+ * Compare locking classes
+ */
+#define lockdep_match_class(lock, key) lockdep_match_key(&(lock)->dep_map, key)
+
+static inline int lockdep_match_key(struct lockdep_map *lock,
+				    struct lock_class_key *key)
+{
+	return lock->key == key;
+}
 
 /*
  * Acquire a lock.
@@ -326,6 +336,11 @@ static inline void lockdep_on(void)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
 		do { (void)(key); } while (0)
 #define lockdep_set_subclass(lock, sub)		do { } while (0)
+/*
+ * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
+ * case since the result is not well defined and the caller should rather
+ * #ifdef the call himself.
+ */
 
 # define INIT_LOCKDEP
 # define lockdep_reset()		do { debug_locks = 1; } while (0)

commit e8c158bb313c1df421eab7dc4299cd39cbbf5895
Author: Robin Holt <holt@sgi.com>
Date:   Thu Apr 2 16:59:45 2009 -0700

    Factor out #ifdefs from kernel/spinlock.c to LOCK_CONTENDED_FLAGS
    
    SGI has observed that on large systems, interrupts are not serviced for a
    long period of time when waiting for a rwlock.  The following patch series
    re-enables irqs while waiting for the lock, resembling the code which is
    already there for spinlocks.
    
    I only made the ia64 version, because the patch adds some overhead to the
    fast path.  I assume there is currently no demand to have this for other
    architectures, because the systems are not so large.  Of course, the
    possibility to implement raw_{read|write}_lock_flags for any architecture
    is still there.
    
    This patch:
    
    The new macro LOCK_CONTENDED_FLAGS expands to the correct implementation
    depending on the config options, so that IRQ's are re-enabled when
    possible, but they remain disabled if CONFIG_LOCKDEP is set.
    
    Signed-off-by: Petr Tesarik <ptesarik@suse.cz>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 5a58ea3e91e9..da5a5a1f4cd2 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -364,6 +364,23 @@ do {								\
 
 #endif /* CONFIG_LOCK_STAT */
 
+#ifdef CONFIG_LOCKDEP
+
+/*
+ * On lockdep we dont want the hand-coded irq-enable of
+ * _raw_*_lock_flags() code, because lockdep assumes
+ * that interrupts are not re-enabled during lock-acquire:
+ */
+#define LOCK_CONTENDED_FLAGS(_lock, try, lock, lockfl, flags) \
+	LOCK_CONTENDED((_lock), (try), (lock))
+
+#else /* CONFIG_LOCKDEP */
+
+#define LOCK_CONTENDED_FLAGS(_lock, try, lock, lockfl, flags) \
+	lockfl((_lock), (flags))
+
+#endif /* CONFIG_LOCKDEP */
+
 #ifdef CONFIG_GENERIC_HARDIRQS
 extern void early_init_irq_lock_class(void);
 #else

commit 9851673bc32bc9fcafbbaeffc858ead434bd6d58
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 22 14:18:40 2009 +0100

    lockdep: move state bit definitions around
    
    For convenience later.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 6d729c9d1d27..5a58ea3e91e9 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -20,51 +20,10 @@ struct lockdep_map;
 #include <linux/stacktrace.h>
 
 /*
- * Lock-class usage-state bits:
+ * We'd rather not expose kernel/lockdep_states.h this wide, but we do need
+ * the total number of states... :-(
  */
-enum lock_usage_bit
-{
-	LOCK_USED = 0,
-	LOCK_USED_IN_HARDIRQ,
-	LOCK_USED_IN_SOFTIRQ,
-	LOCK_USED_IN_RECLAIM_FS,
-	LOCK_ENABLED_SOFTIRQ,
-	LOCK_ENABLED_HARDIRQ,
-	LOCK_ENABLED_RECLAIM_FS,
-	LOCK_USED_IN_HARDIRQ_READ,
-	LOCK_USED_IN_SOFTIRQ_READ,
-	LOCK_USED_IN_RECLAIM_FS_READ,
-	LOCK_ENABLED_SOFTIRQ_READ,
-	LOCK_ENABLED_HARDIRQ_READ,
-	LOCK_ENABLED_RECLAIM_FS_READ,
-	LOCK_USAGE_STATES
-};
-
-/*
- * Usage-state bitmasks:
- */
-#define LOCKF_USED			(1 << LOCK_USED)
-#define LOCKF_USED_IN_HARDIRQ		(1 << LOCK_USED_IN_HARDIRQ)
-#define LOCKF_USED_IN_SOFTIRQ		(1 << LOCK_USED_IN_SOFTIRQ)
-#define LOCKF_USED_IN_RECLAIM_FS	(1 << LOCK_USED_IN_RECLAIM_FS)
-#define LOCKF_ENABLED_HARDIRQ		(1 << LOCK_ENABLED_HARDIRQ)
-#define LOCKF_ENABLED_SOFTIRQ		(1 << LOCK_ENABLED_SOFTIRQ)
-#define LOCKF_ENABLED_RECLAIM_FS	(1 << LOCK_ENABLED_RECLAIM_FS)
-
-#define LOCKF_ENABLED_IRQ (LOCKF_ENABLED_HARDIRQ | LOCKF_ENABLED_SOFTIRQ)
-#define LOCKF_USED_IN_IRQ (LOCKF_USED_IN_HARDIRQ | LOCKF_USED_IN_SOFTIRQ)
-
-#define LOCKF_USED_IN_HARDIRQ_READ	(1 << LOCK_USED_IN_HARDIRQ_READ)
-#define LOCKF_USED_IN_SOFTIRQ_READ	(1 << LOCK_USED_IN_SOFTIRQ_READ)
-#define LOCKF_USED_IN_RECLAIM_FS_READ	(1 << LOCK_USED_IN_RECLAIM_FS_READ)
-#define LOCKF_ENABLED_HARDIRQ_READ	(1 << LOCK_ENABLED_HARDIRQ_READ)
-#define LOCKF_ENABLED_SOFTIRQ_READ	(1 << LOCK_ENABLED_SOFTIRQ_READ)
-#define LOCKF_ENABLED_RECLAIM_FS_READ	(1 << LOCK_ENABLED_RECLAIM_FS_READ)
-
-#define LOCKF_ENABLED_IRQ_READ \
-		(LOCKF_ENABLED_HARDIRQ_READ | LOCKF_ENABLED_SOFTIRQ_READ)
-#define LOCKF_USED_IN_IRQ_READ \
-		(LOCKF_USED_IN_HARDIRQ_READ | LOCKF_USED_IN_SOFTIRQ_READ)
+#define XXX_LOCK_USAGE_STATES		(1+3*4)
 
 #define MAX_LOCKDEP_SUBCLASSES		8UL
 
@@ -105,7 +64,7 @@ struct lock_class {
 	 * IRQ/softirq usage tracking bits:
 	 */
 	unsigned long			usage_mask;
-	struct stack_trace		usage_traces[LOCK_USAGE_STATES];
+	struct stack_trace		usage_traces[XXX_LOCK_USAGE_STATES];
 
 	/*
 	 * These fields represent a directed graph of lock dependencies,

commit a652d7081bc96b3094e85ca30e47f50185d2f717
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 22 13:13:11 2009 +0100

    lockdep: sanitize reclaim bit names
    
    s/HELD_OVER/ENABLED/g
    
    so that its similar to the hard and soft-irq names.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index da2e2b25b3b2..6d729c9d1d27 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -30,13 +30,13 @@ enum lock_usage_bit
 	LOCK_USED_IN_RECLAIM_FS,
 	LOCK_ENABLED_SOFTIRQ,
 	LOCK_ENABLED_HARDIRQ,
-	LOCK_HELD_OVER_RECLAIM_FS,
+	LOCK_ENABLED_RECLAIM_FS,
 	LOCK_USED_IN_HARDIRQ_READ,
 	LOCK_USED_IN_SOFTIRQ_READ,
 	LOCK_USED_IN_RECLAIM_FS_READ,
 	LOCK_ENABLED_SOFTIRQ_READ,
 	LOCK_ENABLED_HARDIRQ_READ,
-	LOCK_HELD_OVER_RECLAIM_FS_READ,
+	LOCK_ENABLED_RECLAIM_FS_READ,
 	LOCK_USAGE_STATES
 };
 
@@ -49,7 +49,7 @@ enum lock_usage_bit
 #define LOCKF_USED_IN_RECLAIM_FS	(1 << LOCK_USED_IN_RECLAIM_FS)
 #define LOCKF_ENABLED_HARDIRQ		(1 << LOCK_ENABLED_HARDIRQ)
 #define LOCKF_ENABLED_SOFTIRQ		(1 << LOCK_ENABLED_SOFTIRQ)
-#define LOCKF_HELD_OVER_RECLAIM_FS	(1 << LOCK_HELD_OVER_RECLAIM_FS)
+#define LOCKF_ENABLED_RECLAIM_FS	(1 << LOCK_ENABLED_RECLAIM_FS)
 
 #define LOCKF_ENABLED_IRQ (LOCKF_ENABLED_HARDIRQ | LOCKF_ENABLED_SOFTIRQ)
 #define LOCKF_USED_IN_IRQ (LOCKF_USED_IN_HARDIRQ | LOCKF_USED_IN_SOFTIRQ)
@@ -59,7 +59,7 @@ enum lock_usage_bit
 #define LOCKF_USED_IN_RECLAIM_FS_READ	(1 << LOCK_USED_IN_RECLAIM_FS_READ)
 #define LOCKF_ENABLED_HARDIRQ_READ	(1 << LOCK_ENABLED_HARDIRQ_READ)
 #define LOCKF_ENABLED_SOFTIRQ_READ	(1 << LOCK_ENABLED_SOFTIRQ_READ)
-#define LOCKF_HELD_OVER_RECLAIM_FS_READ	(1 << LOCK_HELD_OVER_RECLAIM_FS_READ)
+#define LOCKF_ENABLED_RECLAIM_FS_READ	(1 << LOCK_ENABLED_RECLAIM_FS_READ)
 
 #define LOCKF_ENABLED_IRQ_READ \
 		(LOCKF_ENABLED_HARDIRQ_READ | LOCKF_ENABLED_SOFTIRQ_READ)

commit 4fc95e867f1e75351b89db3c68212dfcce7ea563
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 22 13:10:52 2009 +0100

    lockdep: sanitize bit names
    
    s/\(LOCKF\?_ENABLED_[^ ]*\)S\(_READ\)\?\>/\1\2/g
    
    So that the USED_IN and ENABLED have the same names.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index cc97bdbc7969..da2e2b25b3b2 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -28,14 +28,14 @@ enum lock_usage_bit
 	LOCK_USED_IN_HARDIRQ,
 	LOCK_USED_IN_SOFTIRQ,
 	LOCK_USED_IN_RECLAIM_FS,
-	LOCK_ENABLED_SOFTIRQS,
-	LOCK_ENABLED_HARDIRQS,
+	LOCK_ENABLED_SOFTIRQ,
+	LOCK_ENABLED_HARDIRQ,
 	LOCK_HELD_OVER_RECLAIM_FS,
 	LOCK_USED_IN_HARDIRQ_READ,
 	LOCK_USED_IN_SOFTIRQ_READ,
 	LOCK_USED_IN_RECLAIM_FS_READ,
-	LOCK_ENABLED_SOFTIRQS_READ,
-	LOCK_ENABLED_HARDIRQS_READ,
+	LOCK_ENABLED_SOFTIRQ_READ,
+	LOCK_ENABLED_HARDIRQ_READ,
 	LOCK_HELD_OVER_RECLAIM_FS_READ,
 	LOCK_USAGE_STATES
 };
@@ -47,22 +47,22 @@ enum lock_usage_bit
 #define LOCKF_USED_IN_HARDIRQ		(1 << LOCK_USED_IN_HARDIRQ)
 #define LOCKF_USED_IN_SOFTIRQ		(1 << LOCK_USED_IN_SOFTIRQ)
 #define LOCKF_USED_IN_RECLAIM_FS	(1 << LOCK_USED_IN_RECLAIM_FS)
-#define LOCKF_ENABLED_HARDIRQS		(1 << LOCK_ENABLED_HARDIRQS)
-#define LOCKF_ENABLED_SOFTIRQS		(1 << LOCK_ENABLED_SOFTIRQS)
+#define LOCKF_ENABLED_HARDIRQ		(1 << LOCK_ENABLED_HARDIRQ)
+#define LOCKF_ENABLED_SOFTIRQ		(1 << LOCK_ENABLED_SOFTIRQ)
 #define LOCKF_HELD_OVER_RECLAIM_FS	(1 << LOCK_HELD_OVER_RECLAIM_FS)
 
-#define LOCKF_ENABLED_IRQS (LOCKF_ENABLED_HARDIRQS | LOCKF_ENABLED_SOFTIRQS)
+#define LOCKF_ENABLED_IRQ (LOCKF_ENABLED_HARDIRQ | LOCKF_ENABLED_SOFTIRQ)
 #define LOCKF_USED_IN_IRQ (LOCKF_USED_IN_HARDIRQ | LOCKF_USED_IN_SOFTIRQ)
 
 #define LOCKF_USED_IN_HARDIRQ_READ	(1 << LOCK_USED_IN_HARDIRQ_READ)
 #define LOCKF_USED_IN_SOFTIRQ_READ	(1 << LOCK_USED_IN_SOFTIRQ_READ)
 #define LOCKF_USED_IN_RECLAIM_FS_READ	(1 << LOCK_USED_IN_RECLAIM_FS_READ)
-#define LOCKF_ENABLED_HARDIRQS_READ	(1 << LOCK_ENABLED_HARDIRQS_READ)
-#define LOCKF_ENABLED_SOFTIRQS_READ	(1 << LOCK_ENABLED_SOFTIRQS_READ)
+#define LOCKF_ENABLED_HARDIRQ_READ	(1 << LOCK_ENABLED_HARDIRQ_READ)
+#define LOCKF_ENABLED_SOFTIRQ_READ	(1 << LOCK_ENABLED_SOFTIRQ_READ)
 #define LOCKF_HELD_OVER_RECLAIM_FS_READ	(1 << LOCK_HELD_OVER_RECLAIM_FS_READ)
 
-#define LOCKF_ENABLED_IRQS_READ \
-		(LOCKF_ENABLED_HARDIRQS_READ | LOCKF_ENABLED_SOFTIRQS_READ)
+#define LOCKF_ENABLED_IRQ_READ \
+		(LOCKF_ENABLED_HARDIRQ_READ | LOCKF_ENABLED_SOFTIRQ_READ)
 #define LOCKF_USED_IN_IRQ_READ \
 		(LOCKF_USED_IN_HARDIRQ_READ | LOCKF_USED_IN_SOFTIRQ_READ)
 

commit cf40bd16fdad42c053040bcd3988f5fdedbb6c57
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Jan 21 08:12:39 2009 +0100

    lockdep: annotate reclaim context (__GFP_NOFS)
    
    Here is another version, with the incremental patch rolled up, and
    added reclaim context annotation to kswapd, and allocation tracing
    to slab allocators (which may only ever reach the page allocator
    in rare cases, so it is good to put annotations here too).
    
    Haven't tested this version as such, but it should be getting closer
    to merge worthy ;)
    
    --
    After noticing some code in mm/filemap.c accidentally perform a __GFP_FS
    allocation when it should not have been, I thought it might be a good idea to
    try to catch this kind of thing with lockdep.
    
    I coded up a little idea that seems to work. Unfortunately the system has to
    actually be in __GFP_FS page reclaim, then take the lock, before it will mark
    it. But at least that might still be some orders of magnitude more common
    (and more debuggable) than an actual deadlock condition, so we have some
    improvement I hope (the concept is no less complete than discovery of a lock's
    interrupt contexts).
    
    I guess we could even do the same thing with __GFP_IO (normal reclaim), and
    even GFP_NOIO locks too... but filesystems will have the most locks and fiddly
    code paths, so let's start there and see how it goes.
    
    It *seems* to work. I did a quick test.
    
    =================================
    [ INFO: inconsistent lock state ]
    2.6.28-rc6-00007-ged31348-dirty #26
    ---------------------------------
    inconsistent {in-reclaim-W} -> {ov-reclaim-W} usage.
    modprobe/8526 [HC0[0]:SC0[0]:HE1:SE1] takes:
     (testlock){--..}, at: [<ffffffffa0020055>] brd_init+0x55/0x216 [brd]
    {in-reclaim-W} state was registered at:
      [<ffffffff80267bdb>] __lock_acquire+0x75b/0x1a60
      [<ffffffff80268f71>] lock_acquire+0x91/0xc0
      [<ffffffff8070f0e1>] mutex_lock_nested+0xb1/0x310
      [<ffffffffa002002b>] brd_init+0x2b/0x216 [brd]
      [<ffffffff8020903b>] _stext+0x3b/0x170
      [<ffffffff80272ebf>] sys_init_module+0xaf/0x1e0
      [<ffffffff8020c3fb>] system_call_fastpath+0x16/0x1b
      [<ffffffffffffffff>] 0xffffffffffffffff
    irq event stamp: 3929
    hardirqs last  enabled at (3929): [<ffffffff8070f2b5>] mutex_lock_nested+0x285/0x310
    hardirqs last disabled at (3928): [<ffffffff8070f089>] mutex_lock_nested+0x59/0x310
    softirqs last  enabled at (3732): [<ffffffff8061f623>] sk_filter+0x83/0xe0
    softirqs last disabled at (3730): [<ffffffff8061f5b6>] sk_filter+0x16/0xe0
    
    other info that might help us debug this:
    1 lock held by modprobe/8526:
     #0:  (testlock){--..}, at: [<ffffffffa0020055>] brd_init+0x55/0x216 [brd]
    
    stack backtrace:
    Pid: 8526, comm: modprobe Not tainted 2.6.28-rc6-00007-ged31348-dirty #26
    Call Trace:
     [<ffffffff80265483>] print_usage_bug+0x193/0x1d0
     [<ffffffff80266530>] mark_lock+0xaf0/0xca0
     [<ffffffff80266735>] mark_held_locks+0x55/0xc0
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffff802667ca>] trace_reclaim_fs+0x2a/0x60
     [<ffffffff80285005>] __alloc_pages_internal+0x475/0x580
     [<ffffffff8070f29e>] ? mutex_lock_nested+0x26e/0x310
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffffa002006a>] brd_init+0x6a/0x216 [brd]
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffff8020903b>] _stext+0x3b/0x170
     [<ffffffff8070f8b9>] ? mutex_unlock+0x9/0x10
     [<ffffffff8070f83d>] ? __mutex_unlock_slowpath+0x10d/0x180
     [<ffffffff802669ec>] ? trace_hardirqs_on_caller+0x12c/0x190
     [<ffffffff80272ebf>] sys_init_module+0xaf/0x1e0
     [<ffffffff8020c3fb>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 23bf02fb124f..cc97bdbc7969 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -27,12 +27,16 @@ enum lock_usage_bit
 	LOCK_USED = 0,
 	LOCK_USED_IN_HARDIRQ,
 	LOCK_USED_IN_SOFTIRQ,
+	LOCK_USED_IN_RECLAIM_FS,
 	LOCK_ENABLED_SOFTIRQS,
 	LOCK_ENABLED_HARDIRQS,
+	LOCK_HELD_OVER_RECLAIM_FS,
 	LOCK_USED_IN_HARDIRQ_READ,
 	LOCK_USED_IN_SOFTIRQ_READ,
+	LOCK_USED_IN_RECLAIM_FS_READ,
 	LOCK_ENABLED_SOFTIRQS_READ,
 	LOCK_ENABLED_HARDIRQS_READ,
+	LOCK_HELD_OVER_RECLAIM_FS_READ,
 	LOCK_USAGE_STATES
 };
 
@@ -42,16 +46,20 @@ enum lock_usage_bit
 #define LOCKF_USED			(1 << LOCK_USED)
 #define LOCKF_USED_IN_HARDIRQ		(1 << LOCK_USED_IN_HARDIRQ)
 #define LOCKF_USED_IN_SOFTIRQ		(1 << LOCK_USED_IN_SOFTIRQ)
+#define LOCKF_USED_IN_RECLAIM_FS	(1 << LOCK_USED_IN_RECLAIM_FS)
 #define LOCKF_ENABLED_HARDIRQS		(1 << LOCK_ENABLED_HARDIRQS)
 #define LOCKF_ENABLED_SOFTIRQS		(1 << LOCK_ENABLED_SOFTIRQS)
+#define LOCKF_HELD_OVER_RECLAIM_FS	(1 << LOCK_HELD_OVER_RECLAIM_FS)
 
 #define LOCKF_ENABLED_IRQS (LOCKF_ENABLED_HARDIRQS | LOCKF_ENABLED_SOFTIRQS)
 #define LOCKF_USED_IN_IRQ (LOCKF_USED_IN_HARDIRQ | LOCKF_USED_IN_SOFTIRQ)
 
 #define LOCKF_USED_IN_HARDIRQ_READ	(1 << LOCK_USED_IN_HARDIRQ_READ)
 #define LOCKF_USED_IN_SOFTIRQ_READ	(1 << LOCK_USED_IN_SOFTIRQ_READ)
+#define LOCKF_USED_IN_RECLAIM_FS_READ	(1 << LOCK_USED_IN_RECLAIM_FS_READ)
 #define LOCKF_ENABLED_HARDIRQS_READ	(1 << LOCK_ENABLED_HARDIRQS_READ)
 #define LOCKF_ENABLED_SOFTIRQS_READ	(1 << LOCK_ENABLED_SOFTIRQS_READ)
+#define LOCKF_HELD_OVER_RECLAIM_FS_READ	(1 << LOCK_HELD_OVER_RECLAIM_FS_READ)
 
 #define LOCKF_ENABLED_IRQS_READ \
 		(LOCKF_ENABLED_HARDIRQS_READ | LOCKF_ENABLED_SOFTIRQS_READ)
@@ -324,7 +332,11 @@ static inline void lock_set_subclass(struct lockdep_map *lock,
 	lock_set_class(lock, lock->name, lock->key, subclass, ip);
 }
 
-# define INIT_LOCKDEP				.lockdep_recursion = 0,
+extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
+extern void lockdep_clear_current_reclaim_state(void);
+extern void lockdep_trace_alloc(gfp_t mask);
+
+# define INIT_LOCKDEP				.lockdep_recursion = 0, .lockdep_reclaim_gfp = 0,
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
@@ -342,6 +354,9 @@ static inline void lockdep_on(void)
 # define lock_release(l, n, i)			do { } while (0)
 # define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
+# define lockdep_set_current_reclaim_state(g)	do { } while (0)
+# define lockdep_clear_current_reclaim_state()	do { } while (0)
+# define lockdep_trace_alloc(g)			do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
 # define lockdep_init_map(lock, name, key, sub) \

commit 179475a3b46f86e2d06f83e2312218ac3f0cf3a7
Merge: bb758e9637e5 860cf8894b32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 30 16:20:19 2008 -0800

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, sparseirq: clean up Kconfig entry
      x86: turn CONFIG_SPARSE_IRQ off by default
      sparseirq: fix numa_migrate_irq_desc dependency and comments
      sparseirq: add kernel-doc notation for new member in irq_desc, -v2
      locking, irq: enclose irq_desc_lock_class in CONFIG_LOCKDEP
      sparseirq, xen: make sure irq_desc is allocated for interrupts
      sparseirq: fix !SMP building, #2
      x86, sparseirq: move irq_desc according to smp_affinity, v7
      proc: enclose desc variable of show_stat() in CONFIG_SPARSE_IRQ
      sparse irqs: add irqnr.h to the user headers list
      sparse irqs: handle !GENIRQ platforms
      sparseirq: fix !SMP && !PCI_MSI && !HT_IRQ build
      sparseirq: fix Alpha build failure
      sparseirq: fix typo in !CONFIG_IO_APIC case
      x86, MSI: pass irq_cfg and irq_desc
      x86: MSI start irq numbering from nr_irqs_gsi
      x86: use NR_IRQS_LEGACY
      sparse irq_desc[] array: core kernel and x86 changes
      genirq: record IRQ_LEVEL in irq_desc[]
      irq.h: remove padding from irq_desc on 64bits

commit 74c8a6130486bed224e960790f4aa72dd09c061e
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Dec 17 19:40:33 2008 +0900

    locking, irq: enclose irq_desc_lock_class in CONFIG_LOCKDEP
    
    Impact: simplify code
    
    commit "08678b0: generic: sparse irqs: use irq_desc() [...]" introduced
    the irq_desc_lock_class variable.
    
    But it is used only if CONFIG_SPARSE_IRQ=Y or CONFIG_TRACE_IRQFLAGS=Y.
    Otherwise, following warnings happen:
    
            CC      kernel/irq/handle.o
            kernel/irq/handle.c:26: warning: 'irq_desc_lock_class' defined but not used
    
    Actually, current early_init_irq_lock_class has a bit strange and messy ifdef.
    In addition, it is not valueable.
    
    1. this function is protected by !CONFIG_SPARSE_IRQ, but that is not necessary.
       if CONFIG_SPARSE_IRQ=Y, desc of all irq number are initialized by NULL
       at first - then this function calling is safe.
    
    2. this function protected by CONFIG_TRACE_IRQFLAGS too. but it is not
       necessary either, because lockdep_set_class() doesn't have bad side
       effect even if CONFIG_TRACE_IRQFLAGS=n.
    
    This patch bloat kernel size a bit on CONFIG_TRACE_IRQFLAGS=n and
    CONFIG_SPARSE_IRQ=Y - but that's ok. early_init_irq_lock_class() is not
    a fastpatch at all.
    
    To avoid messy ifdefs is more important than a few bytes diet.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 29aec6e10020..9dba554c802c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -377,7 +377,7 @@ do {								\
 
 #endif /* CONFIG_LOCK_STAT */
 
-#if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_GENERIC_HARDIRQS)
+#ifdef CONFIG_GENERIC_HARDIRQS
 extern void early_init_irq_lock_class(void);
 #else
 static inline void early_init_irq_lock_class(void)

commit 00ef9f7348dfd2fc223ec42aceb30836e86b367f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Dec 4 09:00:17 2008 +0100

    lockdep: change a held lock's class
    
    Impact: introduce new lockdep API
    
    Allow to change a held lock's class. Basically the same as the existing
    code to change a subclass therefore reuse all that.
    
    The XFS code will be able to use this to annotate their inode locking.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 8956daf64abd..37a0361f4685 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -314,8 +314,15 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
 
-extern void lock_set_subclass(struct lockdep_map *lock, unsigned int subclass,
-			      unsigned long ip);
+extern void lock_set_class(struct lockdep_map *lock, const char *name,
+			   struct lock_class_key *key, unsigned int subclass,
+			   unsigned long ip);
+
+static inline void lock_set_subclass(struct lockdep_map *lock,
+		unsigned int subclass, unsigned long ip)
+{
+	lock_set_class(lock, lock->name, lock->key, subclass, ip);
+}
 
 # define INIT_LOCKDEP				.lockdep_recursion = 0,
 
@@ -333,6 +340,7 @@ static inline void lockdep_on(void)
 
 # define lock_acquire(l, s, t, r, c, n, i)	do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
+# define lock_set_class(l, n, k, s, i)		do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)

commit e8f6fbf62de37cbc2e179176ac7010d5f4396b67
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 12 01:38:36 2008 +0000

    lockdep: include/linux/lockdep.h - fix warning in net/bluetooth/af_bluetooth.c
    
    fix this warning:
    
      net/bluetooth/af_bluetooth.c:60: warning: â€˜bt_key_stringsâ€™ defined but not used
      net/bluetooth/af_bluetooth.c:71: warning: â€˜bt_slock_key_stringsâ€™ defined but not used
    
    this is a lockdep macro problem in the !LOCKDEP case.
    
    We cannot convert it to an inline because the macro works on multiple types,
    but we can mark the parameter used.
    
    [ also clean up a misaligned tab in sock_lock_init_class_and_name() ]
    
    [ also remove #ifdefs from around af_family_clock_key strings - which
      were certainly added to get rid of the ugly build warnings. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 331e5f1c2d8e..29aec6e10020 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -331,10 +331,11 @@ static inline void lockdep_on(void)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
-# define lockdep_init_map(lock, name, key, sub)	do { (void)(key); } while (0)
+# define lockdep_init_map(lock, name, key, sub) \
+		do { (void)(name); (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)
 # define lockdep_set_class_and_name(lock, key, name) \
-		do { (void)(key); } while (0)
+		do { (void)(key); (void)(name); } while (0)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
 		do { (void)(key); } while (0)
 #define lockdep_set_subclass(lock, sub)		do { } while (0)

commit e25cf3db560e803292946ef23a30c69e341ce56f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 17 15:55:07 2008 +0200

    lockdep: include/linux/lockdep.h - fix warning in net/bluetooth/af_bluetooth.c
    
    fix this warning:
    
      net/bluetooth/af_bluetooth.c:60: warning: â€˜bt_key_stringsâ€™ defined but not used
      net/bluetooth/af_bluetooth.c:71: warning: â€˜bt_slock_key_stringsâ€™ defined but not used
    
    this is a lockdep macro problem in the !LOCKDEP case.
    
    We cannot convert it to an inline because the macro works on multiple types,
    but we can mark the parameter used.
    
    [ also clean up a misaligned tab in sock_lock_init_class_and_name() ]
    
    [ also remove #ifdefs from around af_family_clock_key strings - which
      were certainly added to get rid of the ugly build warnings. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index fc9f8e88123b..8956daf64abd 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -336,10 +336,11 @@ static inline void lockdep_on(void)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
-# define lockdep_init_map(lock, name, key, sub)	do { (void)(key); } while (0)
+# define lockdep_init_map(lock, name, key, sub) \
+		do { (void)(name); (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)
 # define lockdep_set_class_and_name(lock, key, name) \
-		do { (void)(key); } while (0)
+		do { (void)(key); (void)(name); } while (0)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
 		do { (void)(key); } while (0)
 #define lockdep_set_subclass(lock, sub)		do { } while (0)

commit c7e78cff6b7518212247fb20b1dc6411540dc9af
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 16 23:17:09 2008 +0200

    lockstat: contend with points
    
    We currently only provide points that have to wait on contention, also
    lists the points we have to wait for.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 0aa657aa8a1e..fc9f8e88123b 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -73,6 +73,8 @@ struct lock_class_key {
 	struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
 };
 
+#define LOCKSTAT_POINTS		4
+
 /*
  * The lock-class itself:
  */
@@ -119,7 +121,8 @@ struct lock_class {
 	int				name_version;
 
 #ifdef CONFIG_LOCK_STAT
-	unsigned long			contention_point[4];
+	unsigned long			contention_point[LOCKSTAT_POINTS];
+	unsigned long			contending_point[LOCKSTAT_POINTS];
 #endif
 };
 
@@ -144,6 +147,7 @@ enum bounce_type {
 
 struct lock_class_stats {
 	unsigned long			contention_point[4];
+	unsigned long			contending_point[4];
 	struct lock_time		read_waittime;
 	struct lock_time		write_waittime;
 	struct lock_time		read_holdtime;
@@ -165,6 +169,7 @@ struct lockdep_map {
 	const char			*name;
 #ifdef CONFIG_LOCK_STAT
 	int				cpu;
+	unsigned long			ip;
 #endif
 };
 
@@ -355,7 +360,7 @@ struct lock_class_key { };
 #ifdef CONFIG_LOCK_STAT
 
 extern void lock_contended(struct lockdep_map *lock, unsigned long ip);
-extern void lock_acquired(struct lockdep_map *lock);
+extern void lock_acquired(struct lockdep_map *lock, unsigned long ip);
 
 #define LOCK_CONTENDED(_lock, try, lock)			\
 do {								\
@@ -363,13 +368,13 @@ do {								\
 		lock_contended(&(_lock)->dep_map, _RET_IP_);	\
 		lock(_lock);					\
 	}							\
-	lock_acquired(&(_lock)->dep_map);			\
+	lock_acquired(&(_lock)->dep_map, _RET_IP_);			\
 } while (0)
 
 #else /* CONFIG_LOCK_STAT */
 
 #define lock_contended(lockdep_map, ip) do {} while (0)
-#define lock_acquired(lockdep_map) do {} while (0)
+#define lock_acquired(lockdep_map, ip) do {} while (0)
 
 #define LOCK_CONTENDED(_lock, try, lock) \
 	lock(_lock)

commit 76b189e91845eab3a9d52bb97f971d312d25652d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 10 09:57:35 2008 +0200

    lockdep: add might_lock() / might_lock_read()
    
    useful to establish a lock dependency in case the actual dependency is
    rare or hard to trigger.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 331e5f1c2d8e..0aa657aa8a1e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -480,4 +480,22 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 # define lock_map_release(l)			do { } while (0)
 #endif
 
+#ifdef CONFIG_PROVE_LOCKING
+# define might_lock(lock) 						\
+do {									\
+	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
+	lock_acquire(&(lock)->dep_map, 0, 0, 0, 2, NULL, _THIS_IP_);	\
+	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
+} while (0)
+# define might_lock_read(lock) 						\
+do {									\
+	typecheck(struct lockdep_map *, &(lock)->dep_map);		\
+	lock_acquire(&(lock)->dep_map, 0, 0, 1, 2, NULL, _THIS_IP_);	\
+	lock_release(&(lock)->dep_map, 0, _THIS_IP_);			\
+} while (0)
+#else
+# define might_lock(lock) do { } while (0)
+# define might_lock_read(lock) do { } while (0)
+#endif
+
 #endif /* __LINUX_LOCKDEP_H */

commit e5f363e358cf16e4ad13a6826e15088c5495efe9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 11 12:37:27 2008 +0200

    lockdep: increase MAX_LOCKDEP_KEYS
    
    certain configs produce:
    
     [   70.076229] BUG: MAX_LOCKDEP_KEYS too low!
     [   70.080230] turning off the locking correctness validator.
    
    tune them up.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c88aa3d8e87f..331e5f1c2d8e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -190,7 +190,7 @@ struct lock_chain {
 	u64				chain_key;
 };
 
-#define MAX_LOCKDEP_KEYS_BITS		11
+#define MAX_LOCKDEP_KEYS_BITS		13
 /*
  * Subtract one because we offset hlock->class_idx by 1 in order
  * to make 0 mean no class. This avoids overflowing the class_idx

commit b42e737e576339c795d9ac77a1fce6057f6bc0cf
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 12:34:42 2008 +0200

    lockdep: fix overflow in the hlock shrinkage code
    
    There is a overflow by 1 case in the new shrunken hlock code.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 67f42b300c65..c88aa3d8e87f 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -191,7 +191,12 @@ struct lock_chain {
 };
 
 #define MAX_LOCKDEP_KEYS_BITS		11
-#define MAX_LOCKDEP_KEYS		(1UL << MAX_LOCKDEP_KEYS_BITS)
+/*
+ * Subtract one because we offset hlock->class_idx by 1 in order
+ * to make 0 mean no class. This avoids overflowing the class_idx
+ * bitfield and hitting the BUG in hlock_class().
+ */
+#define MAX_LOCKDEP_KEYS		((1UL << MAX_LOCKDEP_KEYS_BITS) - 1)
 
 struct held_lock {
 	/*

commit 3295f0ef9ff048a4619ede597ad9ec9cab725654
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 11 10:30:30 2008 +0200

    lockdep: rename map_[acquire|release]() => lock_map_[acquire|release]()
    
    the names were too generic:
    
     drivers/uio/uio.c:87: error: expected identifier or '(' before 'do'
     drivers/uio/uio.c:87: error: expected identifier or '(' before 'while'
     drivers/uio/uio.c:113: error: 'map_release' undeclared here (not in a function)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 4452c04a7f6e..67f42b300c65 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -465,14 +465,14 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, NULL, _THIS_IP_)
+#  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, NULL, _THIS_IP_)
 # else
-#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, NULL, _THIS_IP_)
+#  define lock_map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, NULL, _THIS_IP_)
 # endif
-# define map_release(l)			lock_release(l, 1, _THIS_IP_)
+# define lock_map_release(l)			lock_release(l, 1, _THIS_IP_)
 #else
-# define map_acquire(l)			do { } while (0)
-# define map_release(l)			do { } while (0)
+# define lock_map_acquire(l)			do { } while (0)
+# define lock_map_release(l)			do { } while (0)
 #endif
 
 #endif /* __LINUX_LOCKDEP_H */

commit b7d39aff91454f2534db2275f55908656ec0470c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 09:30:24 2008 +0200

    lockdep: spin_lock_nest_lock()
    
    Expose the new lock protection lock.
    
    This can be used to annotate places where we take multiple locks of the
    same class and avoid deadlocks by always taking another (top-level) lock
    first.
    
    NOTE: we're still bound to the MAX_LOCK_DEPTH (48) limit.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 93a8cc02a033..4452c04a7f6e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -410,8 +410,10 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
 #  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define spin_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 2, n, i)
 # else
 #  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define spin_acquire_nest(l, s, t, n, i)	lock_acquire(l, s, t, 0, 1, NULL, i)
 # endif
 # define spin_release(l, n, i)			lock_release(l, n, i)
 #else

commit 7531e2f34d1d551b096143f19111139f0dd84c8b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 09:30:24 2008 +0200

    lockdep: lock protection locks
    
    On Fri, 2008-08-01 at 16:26 -0700, Linus Torvalds wrote:
    
    > On Fri, 1 Aug 2008, David Miller wrote:
    > >
    > > Taking more than a few locks of the same class at once is bad
    > > news and it's better to find an alternative method.
    >
    > It's not always wrong.
    >
    > If you can guarantee that anybody that takes more than one lock of a
    > particular class will always take a single top-level lock _first_, then
    > that's all good. You can obviously screw up and take the same lock _twice_
    > (which will deadlock), but at least you cannot get into ABBA situations.
    >
    > So maybe the right thing to do is to just teach lockdep about "lock
    > protection locks". That would have solved the multi-queue issues for
    > networking too - all the actual network drivers would still have taken
    > just their single queue lock, but the one case that needs to take all of
    > them would have taken a separate top-level lock first.
    >
    > Never mind that the multi-queue locks were always taken in the same order:
    > it's never wrong to just have some top-level serialization, and anybody
    > who needs to take <n> locks might as well do <n+1>, because they sure as
    > hell aren't going to be on _any_ fastpaths.
    >
    > So the simplest solution really sounds like just teaching lockdep about
    > that one special case. It's not "nesting" exactly, although it's obviously
    > related to it.
    
    Do as Linus suggested. The lock protection lock is called nest_lock.
    
    Note that we still have the MAX_LOCK_DEPTH (48) limit to consider, so anything
    that spills that it still up shit creek.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index e431d1d6eaf3..93a8cc02a033 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -211,6 +211,7 @@ struct held_lock {
 	u64				prev_chain_key;
 	unsigned long			acquire_ip;
 	struct lockdep_map		*instance;
+	struct lockdep_map		*nest_lock;
 #ifdef CONFIG_LOCK_STAT
 	u64 				waittime_stamp;
 	u64				holdtime_stamp;
@@ -297,7 +298,8 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
  *   2: full validation
  */
 extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
-			 int trylock, int read, int check, unsigned long ip);
+			 int trylock, int read, int check,
+			 struct lockdep_map *nest_lock, unsigned long ip);
 
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
@@ -319,7 +321,7 @@ static inline void lockdep_on(void)
 {
 }
 
-# define lock_acquire(l, s, t, r, c, i)		do { } while (0)
+# define lock_acquire(l, s, t, r, c, n, i)	do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
 # define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
@@ -407,9 +409,9 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
 # else
-#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
 # endif
 # define spin_release(l, n, i)			lock_release(l, n, i)
 #else
@@ -419,11 +421,11 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
-#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, i)
+#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, NULL, i)
 # else
-#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
-#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, i)
+#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, NULL, i)
 # endif
 # define rwlock_release(l, n, i)		lock_release(l, n, i)
 #else
@@ -434,9 +436,9 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
 # else
-#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
 # endif
 # define mutex_release(l, n, i)			lock_release(l, n, i)
 #else
@@ -446,11 +448,11 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
-#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 2, i)
+#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 2, NULL, i)
 # else
-#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
-#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 1, i)
+#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 1, NULL, i)
 # endif
 # define rwsem_release(l, n, i)			lock_release(l, n, i)
 #else
@@ -461,9 +463,9 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # ifdef CONFIG_PROVE_LOCKING
-#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, _THIS_IP_)
+#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, NULL, _THIS_IP_)
 # else
-#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, _THIS_IP_)
+#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, NULL, _THIS_IP_)
 # endif
 # define map_release(l)			lock_release(l, 1, _THIS_IP_)
 #else

commit 4f3e7524b2e703d9f8b02ac338153a53dd7ede66
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 09:30:23 2008 +0200

    lockdep: map_acquire
    
    Most the free-standing lock_acquire() usages look remarkably similar, sweep
    them into a new helper.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b49bfa8e4a5c..e431d1d6eaf3 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -459,4 +459,16 @@ static inline void print_irqtrace_events(struct task_struct *curr)
 # define rwsem_release(l, n, i)			do { } while (0)
 #endif
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 2, _THIS_IP_)
+# else
+#  define map_acquire(l)		lock_acquire(l, 0, 0, 0, 1, _THIS_IP_)
+# endif
+# define map_release(l)			lock_release(l, 1, _THIS_IP_)
+#else
+# define map_acquire(l)			do { } while (0)
+# define map_release(l)			do { } while (0)
+#endif
+
 #endif /* __LINUX_LOCKDEP_H */

commit f82b217e3513fe3af342c0f3ee1494e86250c21c
Author: Dave Jones <davej@redhat.com>
Date:   Mon Aug 11 09:30:23 2008 +0200

    lockdep: shrink held_lock structure
    
    struct held_lock {
            u64                        prev_chain_key;       /*     0     8 */
            struct lock_class *        class;                /*     8     8 */
            long unsigned int          acquire_ip;           /*    16     8 */
            struct lockdep_map *       instance;             /*    24     8 */
            int                        irq_context;          /*    32     4 */
            int                        trylock;              /*    36     4 */
            int                        read;                 /*    40     4 */
            int                        check;                /*    44     4 */
            int                        hardirqs_off;         /*    48     4 */
    
            /* size: 56, cachelines: 1 */
            /* padding: 4 */
            /* last cacheline: 56 bytes */
    };
    
    struct held_lock {
            u64                        prev_chain_key;       /*     0     8 */
            long unsigned int          acquire_ip;           /*     8     8 */
            struct lockdep_map *       instance;             /*    16     8 */
            unsigned int               class_idx:11;         /*    24:21  4 */
            unsigned int               irq_context:2;        /*    24:19  4 */
            unsigned int               trylock:1;            /*    24:18  4 */
            unsigned int               read:2;               /*    24:16  4 */
            unsigned int               check:2;              /*    24:14  4 */
            unsigned int               hardirqs_off:1;       /*    24:13  4 */
    
            /* size: 32, cachelines: 1 */
            /* padding: 4 */
            /* bit_padding: 13 bits */
            /* last cacheline: 32 bytes */
    };
    
    [mingo@elte.hu: shrunk hlock->class too]
    [peterz@infradead.org: fixup bit sizes]
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index f270ce1582ff..b49bfa8e4a5c 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -190,6 +190,9 @@ struct lock_chain {
 	u64				chain_key;
 };
 
+#define MAX_LOCKDEP_KEYS_BITS		11
+#define MAX_LOCKDEP_KEYS		(1UL << MAX_LOCKDEP_KEYS_BITS)
+
 struct held_lock {
 	/*
 	 * One-way hash of the dependency chain up to this point. We
@@ -206,14 +209,13 @@ struct held_lock {
 	 * with zero), here we store the previous hash value:
 	 */
 	u64				prev_chain_key;
-	struct lock_class		*class;
 	unsigned long			acquire_ip;
 	struct lockdep_map		*instance;
-
 #ifdef CONFIG_LOCK_STAT
 	u64 				waittime_stamp;
 	u64				holdtime_stamp;
 #endif
+	unsigned int			class_idx:MAX_LOCKDEP_KEYS_BITS;
 	/*
 	 * The lock-stack is unified in that the lock chains of interrupt
 	 * contexts nest ontop of process context chains, but we 'separate'
@@ -227,11 +229,11 @@ struct held_lock {
 	 * The following field is used to detect when we cross into an
 	 * interrupt context:
 	 */
-	int				irq_context;
-	int				trylock;
-	int				read;
-	int				check;
-	int				hardirqs_off;
+	unsigned int irq_context:2; /* bit 0 - soft, bit 1 - hard */
+	unsigned int trylock:1;
+	unsigned int read:2;        /* see lock_acquire() comment */
+	unsigned int check:2;       /* see lock_acquire() comment */
+	unsigned int hardirqs_off:1;
 };
 
 /*

commit 64aa348edc617dea17bbd01ddee4e47886d5ec8c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 09:30:21 2008 +0200

    lockdep: lock_set_subclass - reset a held lock's subclass
    
    this can be used to reset a held lock's subclass, for arbitrary-depth
    iterated data structures such as trees or lists which have per-node
    locks.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 1bfdc30bb0af..f270ce1582ff 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -300,6 +300,9 @@ extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 extern void lock_release(struct lockdep_map *lock, int nested,
 			 unsigned long ip);
 
+extern void lock_set_subclass(struct lockdep_map *lock, unsigned int subclass,
+			      unsigned long ip);
+
 # define INIT_LOCKDEP				.lockdep_recursion = 0,
 
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
@@ -316,6 +319,7 @@ static inline void lockdep_on(void)
 
 # define lock_acquire(l, s, t, r, c, i)		do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
+# define lock_set_subclass(l, s, i)		do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
 # define lockdep_init_map(lock, name, key, sub)	do { (void)(key); } while (0)

commit 419ca3f13532793b81aff09f80c60af3eacbb43d
Author: David Miller <davem@davemloft.net>
Date:   Tue Jul 29 21:45:03 2008 -0700

    lockdep: fix combinatorial explosion in lock subgraph traversal
    
    When we traverse the graph, either forwards or backwards, we
    are interested in whether a certain property exists somewhere
    in a node reachable in the graph.
    
    Therefore it is never necessary to traverse through a node more
    than once to get a correct answer to the given query.
    
    Take advantage of this property using a global ID counter so that we
    need not clear all the markers in all the lock_class entries before
    doing a traversal.  A new ID is choosen when we start to traverse, and
    we continue through a lock_class only if it's ID hasn't been marked
    with the new value yet.
    
    This short-circuiting is essential especially for high CPU count
    systems.  The scheduler has a runqueue per cpu, and needs to take
    two runqueue locks at a time, which leads to long chains of
    backwards and forwards subgraphs from these runqueue lock nodes.
    Without the short-circuit implemented here, a graph traversal on
    a runqueue lock can take up to (1 << (N - 1)) checks on a system
    with N cpus.
    
    For anything more than 16 cpus or so, lockdep will eventually bring
    the machine to a complete standstill.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 2486eb4edbf1..1bfdc30bb0af 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -89,6 +89,7 @@ struct lock_class {
 
 	struct lockdep_subclass_key	*key;
 	unsigned int			subclass;
+	unsigned int			dep_gen_id;
 
 	/*
 	 * IRQ/softirq usage tracking bits:

commit a033c332e047397904ed74816946b2edd9b0d5cd
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Jun 23 10:52:42 2008 +0800

    lockdep: remove duplicate definition of STATIC_LOCKDEP_MAP_INIT
    
    STATIC_LOCKDEP_MAP_INIT is defined twice in lockdep.h. I guess
    it's a copy & paste.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b26fbc715a50..2486eb4edbf1 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -278,14 +278,6 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 		lockdep_init_map(&(lock)->dep_map, #lock, \
 				 (lock)->dep_map.key, sub)
 
-/*
- * To initialize a lockdep_map statically use this macro.
- * Note that _name must not be NULL.
- */
-#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
-	{ .name = (_name), .key = (void *)(_key), }
-
-
 /*
  * Acquire a lock.
  *

commit 443cd507ce7f78c6f8742b72736585c031d5a921
Author: Huang, Ying <ying.huang@intel.com>
Date:   Fri Jun 20 16:39:21 2008 +0800

    lockdep: add lock_class information to lock_chain and output it
    
    This patch records array of lock_class into lock_chain, and export
    lock_chain information via /proc/lockdep_chains.
    
    It is based on x86/master branch of git-x86 tree, and has been tested
    on x86_64 platform.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 4c4d236ded18..b26fbc715a50 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -182,6 +182,9 @@ struct lock_list {
  * We record lock dependency chains, so that we can cache them:
  */
 struct lock_chain {
+	u8				irq_context;
+	u8				depth;
+	u16				base;
 	struct list_head		entry;
 	u64				chain_key;
 };

commit 4e6045f134784f4b158b3c0f7a282b04bd816887
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Thu Oct 18 23:39:55 2007 -0700

    workqueue: debug flushing deadlocks with lockdep
    
    In the following scenario:
    
    code path 1:
      my_function() -> lock(L1); ...; flush_workqueue(); ...
    
    code path 2:
      run_workqueue() -> my_work() -> ...; lock(L1); ...
    
    you can get a deadlock when my_work() is queued or running
    but my_function() has acquired L1 already.
    
    This patch adds a pseudo-lock to each workqueue to make lockdep
    warn about this scenario.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index f6279f68a827..4c4d236ded18 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -275,6 +275,14 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 		lockdep_init_map(&(lock)->dep_map, #lock, \
 				 (lock)->dep_map.key, sub)
 
+/*
+ * To initialize a lockdep_map statically use this macro.
+ * Note that _name must not be NULL.
+ */
+#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
+	{ .name = (_name), .key = (void *)(_key), }
+
+
 /*
  * Acquire a lock.
  *

commit 851a67b825540a8e00c0be3ee25e4627ba8b133b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 11 22:11:12 2007 +0200

    lockdep: annotate rcu_read_{,un}lock{,_bh}
    
    lockdep annotate rcu_read_{,un}lock{,_bh} in order to catch imbalanced
    usage.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 9dc46db2985e..f6279f68a827 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -252,6 +252,13 @@ extern void lockdep_on(void);
 extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
 			     struct lock_class_key *key, int subclass);
 
+/*
+ * To initialize a lockdep_map statically use this macro.
+ * Note that _name must not be NULL.
+ */
+#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \
+	{ .name = (_name), .key = (void *)(_key), }
+
 /*
  * Reinitialize a lock key - for cases where there is special locking or
  * special initialization of locks so that the validator gets the scope

commit b351d164e860d1ffffdc501c32f55dd1446c385b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 11 22:11:12 2007 +0200

    lockdep: syscall exit check
    
    Provide a check to validate that we do not hold any locks when switching
    back to user-space.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 0e843bf65877..9dc46db2985e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -238,6 +238,7 @@ extern void lockdep_info(void);
 extern void lockdep_reset(void);
 extern void lockdep_reset_lock(struct lockdep_map *lock);
 extern void lockdep_free_key_range(void *start, unsigned long size);
+extern void lockdep_sys_exit(void);
 
 extern void lockdep_off(void);
 extern void lockdep_on(void);
@@ -317,6 +318,7 @@ static inline void lockdep_on(void)
 # define INIT_LOCKDEP
 # define lockdep_reset()		do { debug_locks = 1; } while (0)
 # define lockdep_free_key_range(start, size)	do { } while (0)
+# define lockdep_sys_exit() 			do { } while (0)
 /*
  * The class key takes no space if lockdep is disabled:
  */

commit 96645678cd726e87ce42a0664de71e047e32bca4
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 19 01:49:00 2007 -0700

    lockstat: measure lock bouncing
    
        __acquire
            |
           lock _____
            |        \
            |    __contended
            |         |
            |        wait
            | _______/
            |/
            |
       __acquired
            |
       __release
            |
         unlock
    
    We measure acquisition and contention bouncing.
    
    This is done by recording a cpu stamp in each lock instance.
    
    Contention bouncing requires the cpu stamp to be set on acquisition. Hence we
    move __acquired into the generic path.
    
    __acquired is then used to measure acquisition bouncing by comparing the
    current cpu with the old stamp before replacing it.
    
    __contended is used to measure contention bouncing (only useful for preemptable
    locks)
    
    [akpm@linux-foundation.org: cleanups]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 3d3386b88b6a..0e843bf65877 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -130,12 +130,24 @@ struct lock_time {
 	unsigned long			nr;
 };
 
+enum bounce_type {
+	bounce_acquired_write,
+	bounce_acquired_read,
+	bounce_contended_write,
+	bounce_contended_read,
+	nr_bounce_types,
+
+	bounce_acquired = bounce_acquired_write,
+	bounce_contended = bounce_contended_write,
+};
+
 struct lock_class_stats {
 	unsigned long			contention_point[4];
 	struct lock_time		read_waittime;
 	struct lock_time		write_waittime;
 	struct lock_time		read_holdtime;
 	struct lock_time		write_holdtime;
+	unsigned long			bounces[nr_bounce_types];
 };
 
 struct lock_class_stats lock_stats(struct lock_class *class);
@@ -150,6 +162,9 @@ struct lockdep_map {
 	struct lock_class_key		*key;
 	struct lock_class		*class_cache;
 	const char			*name;
+#ifdef CONFIG_LOCK_STAT
+	int				cpu;
+#endif
 };
 
 /*
@@ -321,8 +336,8 @@ do {								\
 	if (!try(_lock)) {					\
 		lock_contended(&(_lock)->dep_map, _RET_IP_);	\
 		lock(_lock);					\
-		lock_acquired(&(_lock)->dep_map);		\
 	}							\
+	lock_acquired(&(_lock)->dep_map);			\
 } while (0)
 
 #else /* CONFIG_LOCK_STAT */

commit 4b32d0a4e9ec07808a5c406a416c6576c986b047
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 19 01:48:59 2007 -0700

    lockdep: various fixes
    
     - update the copyright notices
     - use the default hash function
     - fix a thinko in a BUILD_BUG_ON
     - add a WARN_ON to spot inconsitent naming
     - fix a termination issue in /proc/lock_stat
    
    [akpm@linux-foundation.org: cleanups]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 8f946f614f8e..3d3386b88b6a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -1,7 +1,8 @@
 /*
  * Runtime locking correctness validator
  *
- *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
  *
  * see Documentation/lockdep-design.txt for more details.
  */

commit f20786ff4da51e56b1956acf30be2552be266746
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 19 01:48:56 2007 -0700

    lockstat: core infrastructure
    
    Introduce the core lock statistics code.
    
    Lock statistics provides lock wait-time and hold-time (as well as the count
    of corresponding contention and acquisitions events). Also, the first few
    call-sites that encounter contention are tracked.
    
    Lock wait-time is the time spent waiting on the lock. This provides insight
    into the locking scheme, that is, a heavily contended lock is indicative of
    a too coarse locking scheme.
    
    Lock hold-time is the duration the lock was held, this provides a reference for
    the wait-time numbers, so they can be put into perspective.
    
      1)
        lock
      2)
        ... do stuff ..
        unlock
      3)
    
    The time between 1 and 2 is the wait-time. The time between 2 and 3 is the
    hold-time.
    
    The lockdep held-lock tracking code is reused, because it already collects locks
    into meaningful groups (classes), and because it is an existing infrastructure
    for lock instrumentation.
    
    Currently lockdep tracks lock acquisition with two hooks:
    
      lock()
        lock_acquire()
        _lock()
    
     ... code protected by lock ...
    
      unlock()
        lock_release()
        _unlock()
    
    We need to extend this with two more hooks, in order to measure contention.
    
      lock_contended() - used to measure contention events
      lock_acquired()  - completion of the contention
    
    These are then placed the following way:
    
      lock()
        lock_acquire()
        if (!_try_lock())
          lock_contended()
          _lock()
          lock_acquired()
    
     ... do locked stuff ...
    
      unlock()
        lock_release()
        _unlock()
    
    (Note: the try_lock() 'trick' is used to avoid instrumenting all platform
           dependent lock primitive implementations.)
    
    It is also possible to toggle the two lockdep features at runtime using:
    
      /proc/sys/kernel/prove_locking
      /proc/sys/kernel/lock_stat
    
    (esp. turning off the O(n^2) prove_locking functionaliy can help)
    
    [akpm@linux-foundation.org: build fixes]
    [akpm@linux-foundation.org: nuke unneeded ifdefs]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 14c937d345cb..8f946f614f8e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -9,6 +9,7 @@
 #define __LINUX_LOCKDEP_H
 
 struct task_struct;
+struct lockdep_map;
 
 #ifdef CONFIG_LOCKDEP
 
@@ -114,8 +115,32 @@ struct lock_class {
 
 	const char			*name;
 	int				name_version;
+
+#ifdef CONFIG_LOCK_STAT
+	unsigned long			contention_point[4];
+#endif
+};
+
+#ifdef CONFIG_LOCK_STAT
+struct lock_time {
+	s64				min;
+	s64				max;
+	s64				total;
+	unsigned long			nr;
 };
 
+struct lock_class_stats {
+	unsigned long			contention_point[4];
+	struct lock_time		read_waittime;
+	struct lock_time		write_waittime;
+	struct lock_time		read_holdtime;
+	struct lock_time		write_holdtime;
+};
+
+struct lock_class_stats lock_stats(struct lock_class *class);
+void clear_lock_stats(struct lock_class *class);
+#endif
+
 /*
  * Map the lock object (the lock instance) to the lock-class object.
  * This is embedded into specific lock instances:
@@ -165,6 +190,10 @@ struct held_lock {
 	unsigned long			acquire_ip;
 	struct lockdep_map		*instance;
 
+#ifdef CONFIG_LOCK_STAT
+	u64 				waittime_stamp;
+	u64				holdtime_stamp;
+#endif
 	/*
 	 * The lock-stack is unified in that the lock chains of interrupt
 	 * contexts nest ontop of process context chains, but we 'separate'
@@ -281,6 +310,30 @@ struct lock_class_key { };
 
 #endif /* !LOCKDEP */
 
+#ifdef CONFIG_LOCK_STAT
+
+extern void lock_contended(struct lockdep_map *lock, unsigned long ip);
+extern void lock_acquired(struct lockdep_map *lock);
+
+#define LOCK_CONTENDED(_lock, try, lock)			\
+do {								\
+	if (!try(_lock)) {					\
+		lock_contended(&(_lock)->dep_map, _RET_IP_);	\
+		lock(_lock);					\
+		lock_acquired(&(_lock)->dep_map);		\
+	}							\
+} while (0)
+
+#else /* CONFIG_LOCK_STAT */
+
+#define lock_contended(lockdep_map, ip) do {} while (0)
+#define lock_acquired(lockdep_map) do {} while (0)
+
+#define LOCK_CONTENDED(_lock, try, lock) \
+	lock(_lock)
+
+#endif /* CONFIG_LOCK_STAT */
+
 #if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_GENERIC_HARDIRQS)
 extern void early_init_irq_lock_class(void);
 #else

commit e3a55fd18deab758a2970e0dfcd60a677a920426
Author: Jarek Poplawski <jarkao2@o2.pl>
Date:   Thu Mar 22 00:11:26 2007 -0800

    [PATCH] lockdep: lockdep_depth vs. debug_locks
    
    lockdep found a bug during a run of workqueue function - this could be also
    caused by a bug from other code running simultaneously.
    
    lockdep really shouldn't be used when debug_locks == 0!
    
    Reported-by: Folkert van Heusden <folkert@vanheusden.com>
    Inspired-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Jarek Poplawski <jarkao2@o2.pl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 06fe93a3e916..14c937d345cb 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -245,7 +245,7 @@ extern void lock_release(struct lockdep_map *lock, int nested,
 
 # define INIT_LOCKDEP				.lockdep_recursion = 0,
 
-#define lockdep_depth(tsk)	((tsk)->lockdep_depth)
+#define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
 #else /* !LOCKDEP */
 

commit a1e96b0310d70b72012b5ecde5e97b8262785aae
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Feb 12 00:52:20 2007 -0800

    [PATCH] lockdep: forward declare struct task_struct
    
    3117df0453828bd045c16244e6f50e5714667a8a causes this:
    
    In file included from arch/s390/kernel/early.c:13:
    include/linux/lockdep.h:300: warning:
                    "struct task_struct" declared inside parameter list
    include/linux/lockdep.h:300:
                    warning: its scope is only this definition or
                    declaration, which is probably not what you want
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 7e1160dde5e7..06fe93a3e916 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -8,6 +8,8 @@
 #ifndef __LINUX_LOCKDEP_H
 #define __LINUX_LOCKDEP_H
 
+struct task_struct;
+
 #ifdef CONFIG_LOCKDEP
 
 #include <linux/linkage.h>

commit 068135e63518314d4efd711142f674ad0841599e
Author: Jason Baron <jbaron@redhat.com>
Date:   Sat Feb 10 01:44:59 2007 -0800

    [PATCH] lockdep: add graph depth information to /proc/lockdep
    
    Generate locking graph information into /proc/lockdep, for lock hierarchy
    documentation and visualization purposes.
    
    sample output:
    
     c089fd5c OPS:     138 FD:   14 BD:    1 --..: &tty->termios_mutex
      -> [c07a3430] tty_ldisc_lock
      -> [c07a37f0] &port_lock_key
      -> [c07afdc0] &rq->rq_lock_key#2
    
    The lock classes listed are all the first-hop lock dependencies that
    lockdep has seen so far.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index ea097dddc44f..7e1160dde5e7 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -132,6 +132,7 @@ struct lock_list {
 	struct list_head		entry;
 	struct lock_class		*class;
 	struct stack_trace		trace;
+	int				distance;
 };
 
 /*

commit 3117df0453828bd045c16244e6f50e5714667a8a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 13 00:34:43 2006 -0800

    [PATCH] lockdep: print irq-trace info on asserts
    
    When we print an assert due to scheduling-in-atomic bugs, and if lockdep
    is enabled, then the IRQ tracing information of lockdep can be printed
    to pinpoint the code location that disabled interrupts. This saved me
    quite a bit of debugging time in cases where the backtrace did not
    identify the irq-disabling site well enough.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 498bfbd3b4e1..ea097dddc44f 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -281,15 +281,25 @@ struct lock_class_key { };
 #if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_GENERIC_HARDIRQS)
 extern void early_init_irq_lock_class(void);
 #else
-# define early_init_irq_lock_class()		do { } while (0)
+static inline void early_init_irq_lock_class(void)
+{
+}
 #endif
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void early_boot_irqs_off(void);
 extern void early_boot_irqs_on(void);
+extern void print_irqtrace_events(struct task_struct *curr);
 #else
-# define early_boot_irqs_off()			do { } while (0)
-# define early_boot_irqs_on()			do { } while (0)
+static inline void early_boot_irqs_off(void)
+{
+}
+static inline void early_boot_irqs_on(void)
+{
+}
+static inline void print_irqtrace_events(struct task_struct *curr)
+{
+}
 #endif
 
 /*

commit ebe7e5fe4b41deeb2731c5b52d8c8e6ac08b1f74
Author: Adrian Bunk <bunk@stusta.de>
Date:   Wed Dec 6 20:38:21 2006 -0800

    [PATCH] remove kernel/lockdep.c:lockdep_internal
    
    Remove the no longer used lockdep_internal().
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index da19aeb0dbe8..498bfbd3b4e1 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -193,7 +193,6 @@ extern void lockdep_free_key_range(void *start, unsigned long size);
 
 extern void lockdep_off(void);
 extern void lockdep_on(void);
-extern int lockdep_internal(void);
 
 /*
  * These methods are used by specific locking variants (spinlocks,
@@ -255,11 +254,6 @@ static inline void lockdep_on(void)
 {
 }
 
-static inline int lockdep_internal(void)
-{
-	return 0;
-}
-
 # define lock_acquire(l, s, t, r, c, i)		do { } while (0)
 # define lock_release(l, n, i)			do { } while (0)
 # define lockdep_init()				do { } while (0)

commit d5abe669172f20a4129a711de0f250a4e07db298
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 6 20:37:26 2006 -0800

    [PATCH] debug: workqueue locking sanity
    
    Workqueue functions should not leak locks, assert so, printing the
    last function ran.
    
    Use macros in lockdep.h to avoid include dependency pains.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 819f08f1310d..da19aeb0dbe8 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -243,6 +243,8 @@ extern void lock_release(struct lockdep_map *lock, int nested,
 
 # define INIT_LOCKDEP				.lockdep_recursion = 0,
 
+#define lockdep_depth(tsk)	((tsk)->lockdep_depth)
+
 #else /* !LOCKDEP */
 
 static inline void lockdep_off(void)
@@ -277,6 +279,9 @@ static inline int lockdep_internal(void)
  * The class key takes no space if lockdep is disabled:
  */
 struct lock_class_key { };
+
+#define lockdep_depth(tsk)	(0)
+
 #endif /* !LOCKDEP */
 
 #if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_GENERIC_HARDIRQS)

commit 07646e217f473a3e6213f8228336a9046833d6aa
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Oct 11 23:45:23 2006 -0400

    Lockdep: fix compile error in drivers/input/serio/serio.c
    
    lockdep_set_subclass() was missing in !LOCKDEP case
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Dmitry Torokhov <dtor@mail.ru>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 14fec2a23b2e..819f08f1310d 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -268,6 +268,8 @@ static inline int lockdep_internal(void)
 		do { (void)(key); } while (0)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
 		do { (void)(key); } while (0)
+#define lockdep_set_subclass(lock, sub)		do { } while (0)
+
 # define INIT_LOCKDEP
 # define lockdep_reset()		do { debug_locks = 1; } while (0)
 # define lockdep_free_key_range(start, size)	do { } while (0)

commit 4dfbb9d8c6cbfc32faa5c71145bd2a43e1f8237c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Oct 11 01:45:14 2006 -0400

    Lockdep: add lockdep_set_class_and_subclass() and lockdep_set_subclass()
    
    This annotation makes it possible to assign a subclass on lock init. This
    annotation is meant to reduce the _nested() annotations by assigning a
    default subclass.
    
    One could do without this annotation and rely on lockdep_set_class()
    exclusively, but that would require a manual stack of struct lock_class_key
    objects.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Dmitry Torokhov <dtor@mail.ru>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 1314ca0f29be..14fec2a23b2e 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -202,7 +202,7 @@ extern int lockdep_internal(void);
  */
 
 extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
-			     struct lock_class_key *key);
+			     struct lock_class_key *key, int subclass);
 
 /*
  * Reinitialize a lock key - for cases where there is special locking or
@@ -211,9 +211,14 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
  * or they are too narrow (they suffer from a false class-split):
  */
 #define lockdep_set_class(lock, key) \
-		lockdep_init_map(&(lock)->dep_map, #key, key)
+		lockdep_init_map(&(lock)->dep_map, #key, key, 0)
 #define lockdep_set_class_and_name(lock, key, name) \
-		lockdep_init_map(&(lock)->dep_map, name, key)
+		lockdep_init_map(&(lock)->dep_map, name, key, 0)
+#define lockdep_set_class_and_subclass(lock, key, sub) \
+		lockdep_init_map(&(lock)->dep_map, #key, key, sub)
+#define lockdep_set_subclass(lock, sub)	\
+		lockdep_init_map(&(lock)->dep_map, #lock, \
+				 (lock)->dep_map.key, sub)
 
 /*
  * Acquire a lock.
@@ -257,10 +262,12 @@ static inline int lockdep_internal(void)
 # define lock_release(l, n, i)			do { } while (0)
 # define lockdep_init()				do { } while (0)
 # define lockdep_info()				do { } while (0)
-# define lockdep_init_map(lock, name, key)	do { (void)(key); } while (0)
+# define lockdep_init_map(lock, name, key, sub)	do { (void)(key); } while (0)
 # define lockdep_set_class(lock, key)		do { (void)(key); } while (0)
 # define lockdep_set_class_and_name(lock, key, name) \
 		do { (void)(key); } while (0)
+#define lockdep_set_class_and_subclass(lock, key, sub) \
+		do { (void)(key); } while (0)
 # define INIT_LOCKDEP
 # define lockdep_reset()		do { debug_locks = 1; } while (0)
 # define lockdep_free_key_range(start, size)	do { } while (0)

commit db0b0ead60815155c791e8f479ee4777e7946369
Author: Michael S. Tsirkin <mst@mellanox.co.il>
Date:   Fri Sep 29 01:59:28 2006 -0700

    [PATCH] lockdep: don't pull in includes when lockdep disabled
    
    Do not pull in various includes through lockdep.h if lockdep is disabled.
    
    Signed-off-by: Michael S. Tsirkin <mst@mellanox.co.il>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index c040a8c969aa..1314ca0f29be 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -8,13 +8,13 @@
 #ifndef __LINUX_LOCKDEP_H
 #define __LINUX_LOCKDEP_H
 
+#ifdef CONFIG_LOCKDEP
+
 #include <linux/linkage.h>
 #include <linux/list.h>
 #include <linux/debug_locks.h>
 #include <linux/stacktrace.h>
 
-#ifdef CONFIG_LOCKDEP
-
 /*
  * Lock-class usage-state bits:
  */

commit d6d897cec29252b8d0785198cfa6ca16d30c739d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 10 04:44:04 2006 -0700

    [PATCH] lockdep: core, reduce per-lock class-cache size
    
    lockdep_map is embedded into every lock, which blows up data structure
    sizes all around the kernel.  Reduce the class-cache to be for the default
    class only - that is used in 99.9% of the cases and even if we dont have a
    class cached, the lookup in the class-hash is lockless.
    
    This change reduces the per-lock dep_map overhead by 56 bytes on 64-bit
    platforms and by 28 bytes on 32-bit platforms.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 316e0fb8d7b1..c040a8c969aa 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -120,7 +120,7 @@ struct lock_class {
  */
 struct lockdep_map {
 	struct lock_class_key		*key;
-	struct lock_class		*class[MAX_LOCKDEP_SUBCLASSES];
+	struct lock_class		*class_cache;
 	const char			*name;
 };
 

commit 243c7621aac4ed1aa79524c9a1cecf7c05a28124
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:06 2006 -0700

    [PATCH] lockdep: annotate genirq
    
    Teach special (recursive) locking code to the lock validator.  Has no effect
    on non-lockdep kernels.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 80ec7a4dbc98..316e0fb8d7b1 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -270,6 +270,12 @@ static inline int lockdep_internal(void)
 struct lock_class_key { };
 #endif /* !LOCKDEP */
 
+#if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_GENERIC_HARDIRQS)
+extern void early_init_irq_lock_class(void);
+#else
+# define early_init_irq_lock_class()		do { } while (0)
+#endif
+
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void early_boot_irqs_off(void);
 extern void early_boot_irqs_on(void);

commit fbb9ce9530fd9b66096d5187fa6a115d16d9746c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:50 2006 -0700

    [PATCH] lockdep: core
    
    Do 'make oldconfig' and accept all the defaults for new config options -
    reboot into the kernel and if everything goes well it should boot up fine and
    you should have /proc/lockdep and /proc/lockdep_stats files.
    
    Typically if the lock validator finds some problem it will print out
    voluminous debug output that begins with "BUG: ..." and which syslog output
    can be used by kernel developers to figure out the precise locking scenario.
    
    What does the lock validator do?  It "observes" and maps all locking rules as
    they occur dynamically (as triggered by the kernel's natural use of spinlocks,
    rwlocks, mutexes and rwsems).  Whenever the lock validator subsystem detects a
    new locking scenario, it validates this new rule against the existing set of
    rules.  If this new rule is consistent with the existing set of rules then the
    new rule is added transparently and the kernel continues as normal.  If the
    new rule could create a deadlock scenario then this condition is printed out.
    
    When determining validity of locking, all possible "deadlock scenarios" are
    considered: assuming arbitrary number of CPUs, arbitrary irq context and task
    context constellations, running arbitrary combinations of all the existing
    locking scenarios.  In a typical system this means millions of separate
    scenarios.  This is why we call it a "locking correctness" validator - for all
    rules that are observed the lock validator proves it with mathematical
    certainty that a deadlock could not occur (assuming that the lock validator
    implementation itself is correct and its internal data structures are not
    corrupted by some other kernel subsystem).  [see more details and conditionals
    of this statement in include/linux/lockdep.h and
    Documentation/lockdep-design.txt]
    
    Furthermore, this "all possible scenarios" property of the validator also
    enables the finding of complex, highly unlikely multi-CPU multi-context races
    via single single-context rules, increasing the likelyhood of finding bugs
    drastically.  In practical terms: the lock validator already found a bug in
    the upstream kernel that could only occur on systems with 3 or more CPUs, and
    which needed 3 very unlikely code sequences to occur at once on the 3 CPUs.
    That bug was found and reported on a single-CPU system (!).  So in essence a
    race will be found "piecemail-wise", triggering all the necessary components
    for the race, without having to reproduce the race scenario itself!  In its
    short existence the lock validator found and reported many bugs before they
    actually caused a real deadlock.
    
    To further increase the efficiency of the validator, the mapping is not per
    "lock instance", but per "lock-class".  For example, all struct inode objects
    in the kernel have inode->inotify_mutex.  If there are 10,000 inodes cached,
    then there are 10,000 lock objects.  But ->inotify_mutex is a single "lock
    type", and all locking activities that occur against ->inotify_mutex are
    "unified" into this single lock-class.  The advantage of the lock-class
    approach is that all historical ->inotify_mutex uses are mapped into a single
    (and as narrow as possible) set of locking rules - regardless of how many
    different tasks or inode structures it took to build this set of rules.  The
    set of rules persist during the lifetime of the kernel.
    
    To see the rough magnitude of checking that the lock validator does, here's a
    portion of /proc/lockdep_stats, fresh after bootup:
    
     lock-classes:                            694 [max: 2048]
     direct dependencies:                  1598 [max: 8192]
     indirect dependencies:               17896
     all direct dependencies:             16206
     dependency chains:                    1910 [max: 8192]
     in-hardirq chains:                      17
     in-softirq chains:                     105
     in-process chains:                    1065
     stack-trace entries:                 38761 [max: 131072]
     combined max dependencies:         2033928
     hardirq-safe locks:                     24
     hardirq-unsafe locks:                  176
     softirq-safe locks:                     53
     softirq-unsafe locks:                  137
     irq-safe locks:                         59
     irq-unsafe locks:                      176
    
    The lock validator has observed 1598 actual single-thread locking patterns,
    and has validated all possible 2033928 distinct locking scenarios.
    
    More details about the design of the lock validator can be found in
    Documentation/lockdep-design.txt, which can also found at:
    
       http://redhat.com/~mingo/lockdep-patches/lockdep-design.txt
    
    [bunk@stusta.de: cleanups]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
new file mode 100644
index 000000000000..80ec7a4dbc98
--- /dev/null
+++ b/include/linux/lockdep.h
@@ -0,0 +1,347 @@
+/*
+ * Runtime locking correctness validator
+ *
+ *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *
+ * see Documentation/lockdep-design.txt for more details.
+ */
+#ifndef __LINUX_LOCKDEP_H
+#define __LINUX_LOCKDEP_H
+
+#include <linux/linkage.h>
+#include <linux/list.h>
+#include <linux/debug_locks.h>
+#include <linux/stacktrace.h>
+
+#ifdef CONFIG_LOCKDEP
+
+/*
+ * Lock-class usage-state bits:
+ */
+enum lock_usage_bit
+{
+	LOCK_USED = 0,
+	LOCK_USED_IN_HARDIRQ,
+	LOCK_USED_IN_SOFTIRQ,
+	LOCK_ENABLED_SOFTIRQS,
+	LOCK_ENABLED_HARDIRQS,
+	LOCK_USED_IN_HARDIRQ_READ,
+	LOCK_USED_IN_SOFTIRQ_READ,
+	LOCK_ENABLED_SOFTIRQS_READ,
+	LOCK_ENABLED_HARDIRQS_READ,
+	LOCK_USAGE_STATES
+};
+
+/*
+ * Usage-state bitmasks:
+ */
+#define LOCKF_USED			(1 << LOCK_USED)
+#define LOCKF_USED_IN_HARDIRQ		(1 << LOCK_USED_IN_HARDIRQ)
+#define LOCKF_USED_IN_SOFTIRQ		(1 << LOCK_USED_IN_SOFTIRQ)
+#define LOCKF_ENABLED_HARDIRQS		(1 << LOCK_ENABLED_HARDIRQS)
+#define LOCKF_ENABLED_SOFTIRQS		(1 << LOCK_ENABLED_SOFTIRQS)
+
+#define LOCKF_ENABLED_IRQS (LOCKF_ENABLED_HARDIRQS | LOCKF_ENABLED_SOFTIRQS)
+#define LOCKF_USED_IN_IRQ (LOCKF_USED_IN_HARDIRQ | LOCKF_USED_IN_SOFTIRQ)
+
+#define LOCKF_USED_IN_HARDIRQ_READ	(1 << LOCK_USED_IN_HARDIRQ_READ)
+#define LOCKF_USED_IN_SOFTIRQ_READ	(1 << LOCK_USED_IN_SOFTIRQ_READ)
+#define LOCKF_ENABLED_HARDIRQS_READ	(1 << LOCK_ENABLED_HARDIRQS_READ)
+#define LOCKF_ENABLED_SOFTIRQS_READ	(1 << LOCK_ENABLED_SOFTIRQS_READ)
+
+#define LOCKF_ENABLED_IRQS_READ \
+		(LOCKF_ENABLED_HARDIRQS_READ | LOCKF_ENABLED_SOFTIRQS_READ)
+#define LOCKF_USED_IN_IRQ_READ \
+		(LOCKF_USED_IN_HARDIRQ_READ | LOCKF_USED_IN_SOFTIRQ_READ)
+
+#define MAX_LOCKDEP_SUBCLASSES		8UL
+
+/*
+ * Lock-classes are keyed via unique addresses, by embedding the
+ * lockclass-key into the kernel (or module) .data section. (For
+ * static locks we use the lock address itself as the key.)
+ */
+struct lockdep_subclass_key {
+	char __one_byte;
+} __attribute__ ((__packed__));
+
+struct lock_class_key {
+	struct lockdep_subclass_key	subkeys[MAX_LOCKDEP_SUBCLASSES];
+};
+
+/*
+ * The lock-class itself:
+ */
+struct lock_class {
+	/*
+	 * class-hash:
+	 */
+	struct list_head		hash_entry;
+
+	/*
+	 * global list of all lock-classes:
+	 */
+	struct list_head		lock_entry;
+
+	struct lockdep_subclass_key	*key;
+	unsigned int			subclass;
+
+	/*
+	 * IRQ/softirq usage tracking bits:
+	 */
+	unsigned long			usage_mask;
+	struct stack_trace		usage_traces[LOCK_USAGE_STATES];
+
+	/*
+	 * These fields represent a directed graph of lock dependencies,
+	 * to every node we attach a list of "forward" and a list of
+	 * "backward" graph nodes.
+	 */
+	struct list_head		locks_after, locks_before;
+
+	/*
+	 * Generation counter, when doing certain classes of graph walking,
+	 * to ensure that we check one node only once:
+	 */
+	unsigned int			version;
+
+	/*
+	 * Statistics counter:
+	 */
+	unsigned long			ops;
+
+	const char			*name;
+	int				name_version;
+};
+
+/*
+ * Map the lock object (the lock instance) to the lock-class object.
+ * This is embedded into specific lock instances:
+ */
+struct lockdep_map {
+	struct lock_class_key		*key;
+	struct lock_class		*class[MAX_LOCKDEP_SUBCLASSES];
+	const char			*name;
+};
+
+/*
+ * Every lock has a list of other locks that were taken after it.
+ * We only grow the list, never remove from it:
+ */
+struct lock_list {
+	struct list_head		entry;
+	struct lock_class		*class;
+	struct stack_trace		trace;
+};
+
+/*
+ * We record lock dependency chains, so that we can cache them:
+ */
+struct lock_chain {
+	struct list_head		entry;
+	u64				chain_key;
+};
+
+struct held_lock {
+	/*
+	 * One-way hash of the dependency chain up to this point. We
+	 * hash the hashes step by step as the dependency chain grows.
+	 *
+	 * We use it for dependency-caching and we skip detection
+	 * passes and dependency-updates if there is a cache-hit, so
+	 * it is absolutely critical for 100% coverage of the validator
+	 * to have a unique key value for every unique dependency path
+	 * that can occur in the system, to make a unique hash value
+	 * as likely as possible - hence the 64-bit width.
+	 *
+	 * The task struct holds the current hash value (initialized
+	 * with zero), here we store the previous hash value:
+	 */
+	u64				prev_chain_key;
+	struct lock_class		*class;
+	unsigned long			acquire_ip;
+	struct lockdep_map		*instance;
+
+	/*
+	 * The lock-stack is unified in that the lock chains of interrupt
+	 * contexts nest ontop of process context chains, but we 'separate'
+	 * the hashes by starting with 0 if we cross into an interrupt
+	 * context, and we also keep do not add cross-context lock
+	 * dependencies - the lock usage graph walking covers that area
+	 * anyway, and we'd just unnecessarily increase the number of
+	 * dependencies otherwise. [Note: hardirq and softirq contexts
+	 * are separated from each other too.]
+	 *
+	 * The following field is used to detect when we cross into an
+	 * interrupt context:
+	 */
+	int				irq_context;
+	int				trylock;
+	int				read;
+	int				check;
+	int				hardirqs_off;
+};
+
+/*
+ * Initialization, self-test and debugging-output methods:
+ */
+extern void lockdep_init(void);
+extern void lockdep_info(void);
+extern void lockdep_reset(void);
+extern void lockdep_reset_lock(struct lockdep_map *lock);
+extern void lockdep_free_key_range(void *start, unsigned long size);
+
+extern void lockdep_off(void);
+extern void lockdep_on(void);
+extern int lockdep_internal(void);
+
+/*
+ * These methods are used by specific locking variants (spinlocks,
+ * rwlocks, mutexes and rwsems) to pass init/acquire/release events
+ * to lockdep:
+ */
+
+extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
+			     struct lock_class_key *key);
+
+/*
+ * Reinitialize a lock key - for cases where there is special locking or
+ * special initialization of locks so that the validator gets the scope
+ * of dependencies wrong: they are either too broad (they need a class-split)
+ * or they are too narrow (they suffer from a false class-split):
+ */
+#define lockdep_set_class(lock, key) \
+		lockdep_init_map(&(lock)->dep_map, #key, key)
+#define lockdep_set_class_and_name(lock, key, name) \
+		lockdep_init_map(&(lock)->dep_map, name, key)
+
+/*
+ * Acquire a lock.
+ *
+ * Values for "read":
+ *
+ *   0: exclusive (write) acquire
+ *   1: read-acquire (no recursion allowed)
+ *   2: read-acquire with same-instance recursion allowed
+ *
+ * Values for check:
+ *
+ *   0: disabled
+ *   1: simple checks (freeing, held-at-exit-time, etc.)
+ *   2: full validation
+ */
+extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
+			 int trylock, int read, int check, unsigned long ip);
+
+extern void lock_release(struct lockdep_map *lock, int nested,
+			 unsigned long ip);
+
+# define INIT_LOCKDEP				.lockdep_recursion = 0,
+
+#else /* !LOCKDEP */
+
+static inline void lockdep_off(void)
+{
+}
+
+static inline void lockdep_on(void)
+{
+}
+
+static inline int lockdep_internal(void)
+{
+	return 0;
+}
+
+# define lock_acquire(l, s, t, r, c, i)		do { } while (0)
+# define lock_release(l, n, i)			do { } while (0)
+# define lockdep_init()				do { } while (0)
+# define lockdep_info()				do { } while (0)
+# define lockdep_init_map(lock, name, key)	do { (void)(key); } while (0)
+# define lockdep_set_class(lock, key)		do { (void)(key); } while (0)
+# define lockdep_set_class_and_name(lock, key, name) \
+		do { (void)(key); } while (0)
+# define INIT_LOCKDEP
+# define lockdep_reset()		do { debug_locks = 1; } while (0)
+# define lockdep_free_key_range(start, size)	do { } while (0)
+/*
+ * The class key takes no space if lockdep is disabled:
+ */
+struct lock_class_key { };
+#endif /* !LOCKDEP */
+
+#ifdef CONFIG_TRACE_IRQFLAGS
+extern void early_boot_irqs_off(void);
+extern void early_boot_irqs_on(void);
+#else
+# define early_boot_irqs_off()			do { } while (0)
+# define early_boot_irqs_on()			do { } while (0)
+#endif
+
+/*
+ * For trivial one-depth nesting of a lock-class, the following
+ * global define can be used. (Subsystems with multiple levels
+ * of nesting should define their own lock-nesting subclasses.)
+ */
+#define SINGLE_DEPTH_NESTING			1
+
+/*
+ * Map the dependency ops to NOP or to real lockdep ops, depending
+ * on the per lock-class debug mode:
+ */
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+# else
+#  define spin_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+# endif
+# define spin_release(l, n, i)			lock_release(l, n, i)
+#else
+# define spin_acquire(l, s, t, i)		do { } while (0)
+# define spin_release(l, n, i)			do { } while (0)
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, i)
+# else
+#  define rwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+#  define rwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, i)
+# endif
+# define rwlock_release(l, n, i)		lock_release(l, n, i)
+#else
+# define rwlock_acquire(l, s, t, i)		do { } while (0)
+# define rwlock_acquire_read(l, s, t, i)	do { } while (0)
+# define rwlock_release(l, n, i)		do { } while (0)
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+# else
+#  define mutex_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+# endif
+# define mutex_release(l, n, i)			lock_release(l, n, i)
+#else
+# define mutex_acquire(l, s, t, i)		do { } while (0)
+# define mutex_release(l, n, i)			do { } while (0)
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, i)
+#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 2, i)
+# else
+#  define rwsem_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, i)
+#  define rwsem_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 1, 1, i)
+# endif
+# define rwsem_release(l, n, i)			lock_release(l, n, i)
+#else
+# define rwsem_acquire(l, s, t, i)		do { } while (0)
+# define rwsem_acquire_read(l, s, t, i)		do { } while (0)
+# define rwsem_release(l, n, i)			do { } while (0)
+#endif
+
+#endif /* __LINUX_LOCKDEP_H */
