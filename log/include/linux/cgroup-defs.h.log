commit 14b032b8f8fce03a546dcf365454bec8c4a58d7d
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Thu Jul 9 16:28:44 2020 -0700

    cgroup: Fix sock_cgroup_data on big-endian.
    
    In order for no_refcnt and is_data to be the lowest order two
    bits in the 'val' we have to pad out the bitfield of the u8.
    
    Fixes: ad0f75e5f57c ("cgroup: fix cgroup_sk_alloc() for sk_clone_lock()")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 4f1cd0edc57d..fee0b5547cd0 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -792,6 +792,7 @@ struct sock_cgroup_data {
 		struct {
 			u8	is_data : 1;
 			u8	no_refcnt : 1;
+			u8	unused : 6;
 			u8	padding;
 			u16	prioidx;
 			u32	classid;
@@ -801,6 +802,7 @@ struct sock_cgroup_data {
 			u32	classid;
 			u16	prioidx;
 			u8	padding;
+			u8	unused : 6;
 			u8	no_refcnt : 1;
 			u8	is_data : 1;
 		} __packed;

commit ad0f75e5f57ccbceec13274e1e242f2b5a6397ed
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Thu Jul 2 11:52:56 2020 -0700

    cgroup: fix cgroup_sk_alloc() for sk_clone_lock()
    
    When we clone a socket in sk_clone_lock(), its sk_cgrp_data is
    copied, so the cgroup refcnt must be taken too. And, unlike the
    sk_alloc() path, sock_update_netprioidx() is not called here.
    Therefore, it is safe and necessary to grab the cgroup refcnt
    even when cgroup_sk_alloc is disabled.
    
    sk_clone_lock() is in BH context anyway, the in_interrupt()
    would terminate this function if called there. And for sk_alloc()
    skcd->val is always zero. So it's safe to factor out the code
    to make it more readable.
    
    The global variable 'cgroup_sk_alloc_disabled' is used to determine
    whether to take these reference counts. It is impossible to make
    the reference counting correct unless we save this bit of information
    in skcd->val. So, add a new bit there to record whether the socket
    has already taken the reference counts. This obviously relies on
    kmalloc() to align cgroup pointers to at least 4 bytes,
    ARCH_KMALLOC_MINALIGN is certainly larger than that.
    
    This bug seems to be introduced since the beginning, commit
    d979a39d7242 ("cgroup: duplicate cgroup reference when cloning sockets")
    tried to fix it but not compeletely. It seems not easy to trigger until
    the recent commit 090e28b229af
    ("netprio_cgroup: Fix unlimited memory leak of v2 cgroups") was merged.
    
    Fixes: bd1060a1d671 ("sock, cgroup: add sock->sk_cgroup")
    Reported-by: Cameron Berkenpas <cam@neo-zeon.de>
    Reported-by: Peter Geis <pgwipeout@gmail.com>
    Reported-by: Lu Fengqi <lufq.fnst@cn.fujitsu.com>
    Reported-by: Daniël Sonck <dsonck92@gmail.com>
    Reported-by: Zhang Qiang <qiang.zhang@windriver.com>
    Tested-by: Cameron Berkenpas <cam@neo-zeon.de>
    Tested-by: Peter Geis <pgwipeout@gmail.com>
    Tested-by: Thomas Lamprecht <t.lamprecht@proxmox.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Roman Gushchin <guro@fb.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 52661155f85f..4f1cd0edc57d 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -790,7 +790,8 @@ struct sock_cgroup_data {
 	union {
 #ifdef __LITTLE_ENDIAN
 		struct {
-			u8	is_data;
+			u8	is_data : 1;
+			u8	no_refcnt : 1;
 			u8	padding;
 			u16	prioidx;
 			u32	classid;
@@ -800,7 +801,8 @@ struct sock_cgroup_data {
 			u32	classid;
 			u16	prioidx;
 			u8	padding;
-			u8	is_data;
+			u8	no_refcnt : 1;
+			u8	is_data : 1;
 		} __packed;
 #endif
 		u64		val;

commit d8836005236425cf3cfcc8967abd1d5c21f607f8
Merge: f2c3bec3c90d 0c05b9bdbfe5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 3 11:30:20 2020 -0700

    Merge branch 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - Christian extended clone3 so that processes can be spawned into
       cgroups directly.
    
       This is not only neat in terms of semantics but also avoids grabbing
       the global cgroup_threadgroup_rwsem for migration.
    
     - Daniel added !root xattr support to cgroupfs.
    
       Userland already uses xattrs on cgroupfs for bookkeeping. This will
       allow delegated cgroups to support such usages.
    
     - Prateek tried to make cpuset hotplug handling synchronous but that
       led to possible deadlock scenarios. Reverted.
    
     - Other minor changes including release_agent_path handling cleanup.
    
    * 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      docs: cgroup-v1: Document the cpuset_v2_mode mount option
      Revert "cpuset: Make cpuset hotplug synchronous"
      cgroupfs: Support user xattrs
      kernfs: Add option to enable user xattrs
      kernfs: Add removed_size out param for simple_xattr_set
      kernfs: kvmalloc xattr value instead of kmalloc
      cgroup: Restructure release_agent_path handling
      selftests/cgroup: add tests for cloning into cgroups
      clone3: allow spawning processes into cgroups
      cgroup: add cgroup_may_write() helper
      cgroup: refactor fork helpers
      cgroup: add cgroup_get_from_file() helper
      cgroup: unify attach permission checking
      cpuset: Make cpuset hotplug synchronous
      cgroup.c: Use built-in RCU list checking
      kselftest/cgroup: add cgroup destruction test
      cgroup: Clean up css_set task traversal

commit 8a931f801340c2be10552c7b5622d5f4852f3a36
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Apr 1 21:07:07 2020 -0700

    mm: memcontrol: recursive memory.low protection
    
    Right now, the effective protection of any given cgroup is capped by its
    own explicit memory.low setting, regardless of what the parent says.  The
    reasons for this are mostly historical and ease of implementation: to make
    delegation of memory.low safe, effective protection is the min() of all
    memory.low up the tree.
    
    Unfortunately, this limitation makes it impossible to protect an entire
    subtree from another without forcing the user to make explicit protection
    allocations all the way to the leaf cgroups - something that is highly
    undesirable in real life scenarios.
    
    Consider memory in a data center host.  At the cgroup top level, we have a
    distinction between system management software and the actual workload the
    system is executing.  Both branches are further subdivided into individual
    services, job components etc.
    
    We want to protect the workload as a whole from the system management
    software, but that doesn't mean we want to protect and prioritize
    individual workload wrt each other.  Their memory demand can vary over
    time, and we'd want the VM to simply cache the hottest data within the
    workload subtree.  Yet, the current memory.low limitations force us to
    allocate a fixed amount of protection to each workload component in order
    to get protection from system management software in general.  This
    results in very inefficient resource distribution.
    
    Another concern with mandating downward allocation is that, as the
    complexity of the cgroup tree grows, it gets harder for the lower levels
    to be informed about decisions made at the host-level.  Consider a
    container inside a namespace that in turn creates its own nested tree of
    cgroups to run multiple workloads.  It'd be extremely difficult to
    configure memory.low parameters in those leaf cgroups that on one hand
    balance pressure among siblings as the container desires, while also
    reflecting the host-level protection from e.g.  rpm upgrades, that lie
    beyond one or more delegation and namespacing points in the tree.
    
    It's highly unusual from a cgroup interface POV that nested levels have to
    be aware of and reflect decisions made at higher levels for them to be
    effective.
    
    To enable such use cases and scale configurability for complex trees, this
    patch implements a resource inheritance model for memory that is similar
    to how the CPU and the IO controller implement work-conserving resource
    allocations: a share of a resource allocated to a subree always applies to
    the entire subtree recursively, while allowing, but not mandating,
    children to further specify distribution rules.
    
    That means that if protection is explicitly allocated among siblings,
    those configured shares are being followed during page reclaim just like
    they are now.  However, if the memory.low set at a higher level is not
    fully claimed by the children in that subtree, the "floating" remainder is
    applied to each cgroup in the tree in proportion to its size.  Since
    reclaim pressure is applied in proportion to size as well, each child in
    that tree gets the same boost, and the effect is neutral among siblings -
    with respect to each other, they behave as if no memory control was
    enabled at all, and the VM simply balances the memory demands optimally
    within the subtree.  But collectively those cgroups enjoy a boost over the
    cgroups in neighboring trees.
    
    E.g.  a leaf cgroup with a memory.low setting of 0 no longer means that
    it's not getting a share of the hierarchically assigned resource, just
    that it doesn't claim a fixed amount of it to protect from its siblings.
    
    This allows us to recursively protect one subtree (workload) from another
    (system management), while letting subgroups compete freely among each
    other - without having to assign fixed shares to each leaf, and without
    nested groups having to echo higher-level settings.
    
    The floating protection composes naturally with fixed protection.
    Consider the following example tree:
    
                    A            A: low = 2G
                   / \          A1: low = 1G
                  A1 A2         A2: low = 0G
    
    As outside pressure is applied to this tree, A1 will enjoy a fixed
    protection from A2 of 1G, but the remaining, unclaimed 1G from A is split
    evenly among A1 and A2, coming out to 1.5G and 0.5G.
    
    There is a slight risk of regressing theoretical setups where the
    top-level cgroups don't know about the true budgeting and set bogusly high
    "bypass" values that are meaningfully allocated down the tree.  Such
    setups would rely on unclaimed protection to be discarded, and
    distributing it would change the intended behavior.  Be safe and hide the
    new behavior behind a mount option, 'memory_recursiveprot'.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: Chris Down <chris@chrisdown.name>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Koutný <mkoutny@suse.com>
    Link: http://lkml.kernel.org/r/20200227195606.46212-4-hannes@cmpxchg.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 63097cb243cb..e1fafed22db1 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -94,6 +94,11 @@ enum {
 	 * Enable legacy local memory.events.
 	 */
 	CGRP_ROOT_MEMORY_LOCAL_EVENTS = (1 << 5),
+
+	/*
+	 * Enable recursive subtree protection
+	 */
+	CGRP_ROOT_MEMORY_RECURSIVE_PROT = (1 << 6),
 };
 
 /* cftype->flags */

commit ef2c41cf38a7559bbf91af42d5b6a4429db8fc68
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Wed Feb 5 14:26:22 2020 +0100

    clone3: allow spawning processes into cgroups
    
    This adds support for creating a process in a different cgroup than its
    parent. Callers can limit and account processes and threads right from
    the moment they are spawned:
    - A service manager can directly spawn new services into dedicated
      cgroups.
    - A process can be directly created in a frozen cgroup and will be
      frozen as well.
    - The initial accounting jitter experienced by process supervisors and
      daemons is eliminated with this.
    - Threaded applications or even thread implementations can choose to
      create a specific cgroup layout where each thread is spawned
      directly into a dedicated cgroup.
    
    This feature is limited to the unified hierarchy. Callers need to pass
    a directory file descriptor for the target cgroup. The caller can
    choose to pass an O_PATH file descriptor. All usual migration
    restrictions apply, i.e. there can be no processes in inner nodes. In
    general, creating a process directly in a target cgroup adheres to all
    migration restrictions.
    
    One of the biggest advantages of this feature is that CLONE_INTO_GROUP does
    not need to grab the write side of the cgroup cgroup_threadgroup_rwsem.
    This global lock makes moving tasks/threads around super expensive. With
    clone3() this lock is avoided.
    
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: cgroups@vger.kernel.org
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 63097cb243cb..68c391f451d1 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -628,8 +628,9 @@ struct cgroup_subsys {
 	void (*cancel_attach)(struct cgroup_taskset *tset);
 	void (*attach)(struct cgroup_taskset *tset);
 	void (*post_attach)(void);
-	int (*can_fork)(struct task_struct *task);
-	void (*cancel_fork)(struct task_struct *task);
+	int (*can_fork)(struct task_struct *task,
+			struct css_set *cset);
+	void (*cancel_fork)(struct task_struct *task, struct css_set *cset);
 	void (*fork)(struct task_struct *task);
 	void (*exit)(struct task_struct *task);
 	void (*release)(struct task_struct *task);

commit 743210386c0354a2f8ef3d697353c7d8477fa81d
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 4 15:54:30 2019 -0800

    cgroup: use cgrp->kn->id as the cgroup ID
    
    cgroup ID is currently allocated using a dedicated per-hierarchy idr
    and used internally and exposed through tracepoints and bpf.  This is
    confusing because there are tracepoints and other interfaces which use
    the cgroupfs ino as IDs.
    
    The preceding changes made kn->id exposed as ino as 64bit ino on
    supported archs or ino+gen (low 32bits as ino, high gen).  There's no
    reason for cgroup to use different IDs.  The kernfs IDs are unique and
    userland can easily discover them and map them back to paths using
    standard file operations.
    
    This patch replaces cgroup IDs with kernfs IDs.
    
    * cgroup_id() is added and all cgroup ID users are converted to use it.
    
    * kernfs_node creation is moved to earlier during cgroup init so that
      cgroup_id() is available during init.
    
    * While at it, s/cgroup/cgrp/ in psi helpers for consistency.
    
    * Fallback ID value is changed to 1 to be consistent with root cgroup
      ID.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 4904b1ebd1ff..63097cb243cb 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -354,16 +354,6 @@ struct cgroup {
 
 	unsigned long flags;		/* "unsigned long" so bitops work */
 
-	/*
-	 * idr allocated in-hierarchy ID.
-	 *
-	 * ID 0 is not used, the ID of the root cgroup is always 1, and a
-	 * new cgroup will be assigned with a smallest available ID.
-	 *
-	 * Allocating/Removing ID must be protected by cgroup_mutex.
-	 */
-	int id;
-
 	/*
 	 * The depth this cgroup is at.  The root is at depth zero and each
 	 * step down the hierarchy increments the level.  This along with
@@ -488,7 +478,7 @@ struct cgroup {
 	struct cgroup_freezer_state freezer;
 
 	/* ids of the ancestors at each level including self */
-	int ancestor_ids[];
+	u64 ancestor_ids[];
 };
 
 /*
@@ -509,7 +499,7 @@ struct cgroup_root {
 	struct cgroup cgrp;
 
 	/* for cgrp->ancestor_ids[0] */
-	int cgrp_ancestor_id_storage;
+	u64 cgrp_ancestor_id_storage;
 
 	/* Number of cgroups in the hierarchy, used only for /proc/cgroups */
 	atomic_t nr_cgrps;
@@ -520,9 +510,6 @@ struct cgroup_root {
 	/* Hierarchy-specific flags */
 	unsigned int flags;
 
-	/* IDs for cgroups in this hierarchy */
-	struct idr cgroup_idr;
-
 	/* The path to use for release notifications. */
 	char release_agent_path[PATH_MAX];
 

commit 1bb5ec2eec48dcab1d8ae3707e4a388da6a9c9dc
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Nov 6 12:49:57 2019 -0800

    cgroup: use cgroup->last_bstat instead of cgroup->bstat_pending for consistency
    
    cgroup->bstat_pending is used to determine the base stat delta to
    propagate to the parent.  While correct, this is different from how
    percpu delta is determined for no good reason and the inconsistency
    makes the code more difficult to understand.
    
    This patch makes parent propagation delta calculation use the same
    method as percpu to global propagation.
    
    * cgroup_base_stat_accumulate() is renamed to cgroup_base_stat_add()
      and cgroup_base_stat_sub() is added.
    
    * percpu propagation calculation is updated to use the above helpers.
    
    * cgroup->bstat_pending is replaced with cgroup->last_bstat and
      updated to use the same calculation as percpu propagation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 430e219e3aba..4904b1ebd1ff 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -458,7 +458,7 @@ struct cgroup {
 	struct list_head rstat_css_list;
 
 	/* cgroup basic resource statistics */
-	struct cgroup_base_stat pending_bstat;	/* pending from children */
+	struct cgroup_base_stat last_bstat;
 	struct cgroup_base_stat bstat;
 	struct prev_cputime prev_cputime;	/* for printing out cputime */
 

commit da82c92f1150f66afabf78d2c85ef9ac18dc6d38
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Thu Jun 27 13:08:35 2019 -0300

    docs: cgroup-v1: add it to the admin-guide book
    
    Those files belong to the admin guide, so add them.
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c5311935239d..430e219e3aba 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -624,7 +624,7 @@ struct cftype {
 
 /*
  * Control Group subsystem type.
- * See Documentation/cgroup-v1/cgroups.rst for details
+ * See Documentation/admin-guide/cgroup-v1/cgroups.rst for details
  */
 struct cgroup_subsys {
 	struct cgroup_subsys_state *(*css_alloc)(struct cgroup_subsys_state *parent_css);

commit 92c1d6522135050cb377a18cc6e30d08dfb87efb
Merge: df2a40f549e6 99c8b231ae6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 21:35:12 2019 -0700

    Merge branch 'for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Documentation updates and the addition of cgroup_parse_float() which
      will be used by new controllers including blk-iocost"
    
    * 'for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      docs: cgroup-v1: convert docs to ReST and rename to *.rst
      cgroup: Move cgroup_parse_float() implementation out of CONFIG_SYSFS
      cgroup: add cgroup_parse_float()

commit 0011572c883082a95e02d47f45fc4a42dc0e8634
Merge: 6aa7a22b9790 d477f8c202d1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 14 17:46:14 2019 -1000

    Merge branch 'for-5.2-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup fixes from Tejun Heo:
     "This has an unusually high density of tricky fixes:
    
       - task_get_css() could deadlock when it races against a dying cgroup.
    
       - cgroup.procs didn't list thread group leaders with live threads.
    
         This could mislead readers to think that a cgroup is empty when
         it's not. Fixed by making PROCS iterator include dead tasks. I made
         a couple mistakes making this change and this pull request contains
         a couple follow-up patches.
    
       - When cpusets run out of online cpus, it updates cpusmasks of member
         tasks in bizarre ways. Joel improved the behavior significantly"
    
    * 'for-5.2-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cpuset: restore sanity to cpuset_cpus_allowed_fallback()
      cgroup: Fix css_task_iter_advance_css_set() cset skip condition
      cgroup: css_task_iter_skip()'d iterators must be advanced before accessed
      cgroup: Include dying leaders with live threads in PROCS iterations
      cgroup: Implement css_task_iter_skip()
      cgroup: Call cgroup_release() before __exit_signal()
      docs cgroups: add another example size for hugetlb
      cgroup: Use css_tryget() instead of css_tryget_online() in task_get_css()

commit 99c8b231ae6c6ca4ca2fd1c0b3701071f589661f
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Wed Jun 12 14:52:41 2019 -0300

    docs: cgroup-v1: convert docs to ReST and rename to *.rst
    
    Convert the cgroup-v1 files to ReST format, in order to
    allow a later addition to the admin-guide.
    
    The conversion is actually:
      - add blank lines and identation in order to identify paragraphs;
      - fix tables markups;
      - add some lists markups;
      - mark literal blocks;
      - adjust title markups.
    
    At its new index.rst, let's add a :orphan: while this is not linked to
    the main index.rst file, in order to avoid build warnings.
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 1615b9c17e02..a3699d4d27e0 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -619,7 +619,7 @@ struct cftype {
 
 /*
  * Control Group subsystem type.
- * See Documentation/cgroup-v1/cgroups.txt for details
+ * See Documentation/cgroup-v1/cgroups.rst for details
  */
 struct cgroup_subsys {
 	struct cgroup_subsys_state *(*css_alloc)(struct cgroup_subsys_state *parent_css);

commit cf8929885de318c0bf73438c9e5dde59d6536f7c
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Jun 10 03:35:41 2019 -0600

    cgroup/bfq: revert bfq.weight symlink change
    
    There's some discussion on how to do this the best, and Tejun prefers
    that BFQ just create the file itself instead of having cgroups support
    a symlink feature.
    
    Hence revert commit 54b7b868e826 and 19e9da9e86c4 for 5.2, and this
    can be done properly for 5.3.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index d71b079bb021..11e215d7937e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -106,8 +106,6 @@ enum {
 	CFTYPE_WORLD_WRITABLE	= (1 << 4),	/* (DON'T USE FOR NEW FILES) S_IWUGO */
 	CFTYPE_DEBUG		= (1 << 5),	/* create when cgroup_debug */
 
-	CFTYPE_SYMLINKED	= (1 << 6),	/* pointed to by symlink too */
-
 	/* internal flags, do not use outside cgroup core proper */
 	__CFTYPE_ONLY_ON_DFL	= (1 << 16),	/* only on default hierarchy */
 	__CFTYPE_NOT_ON_DFL	= (1 << 17),	/* not on default hierarchy */
@@ -545,7 +543,6 @@ struct cftype {
 	 * end of cftype array.
 	 */
 	char name[MAX_CFTYPE_NAME];
-	char link_name[MAX_CFTYPE_NAME];
 	unsigned long private;
 
 	/*

commit 54b7b868e826b294687c439b68ec55fe20cafe5b
Author: Angelo Ruocco <angeloruocco90@gmail.com>
Date:   Tue May 21 10:01:54 2019 +0200

    cgroup: let a symlink too be created with a cftype file
    
    This commit enables a cftype to have a symlink (of any name) that
    points to the file associated with the cftype.
    
    Signed-off-by: Angelo Ruocco <angeloruocco90@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 11e215d7937e..d71b079bb021 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -106,6 +106,8 @@ enum {
 	CFTYPE_WORLD_WRITABLE	= (1 << 4),	/* (DON'T USE FOR NEW FILES) S_IWUGO */
 	CFTYPE_DEBUG		= (1 << 5),	/* create when cgroup_debug */
 
+	CFTYPE_SYMLINKED	= (1 << 6),	/* pointed to by symlink too */
+
 	/* internal flags, do not use outside cgroup core proper */
 	__CFTYPE_ONLY_ON_DFL	= (1 << 16),	/* only on default hierarchy */
 	__CFTYPE_NOT_ON_DFL	= (1 << 17),	/* not on default hierarchy */
@@ -543,6 +545,7 @@ struct cftype {
 	 * end of cftype array.
 	 */
 	char name[MAX_CFTYPE_NAME];
+	char link_name[MAX_CFTYPE_NAME];
 	unsigned long private;
 
 	/*

commit 9852ae3fe5293264f01c49f2571ef7688f7823ce
Author: Chris Down <chris@chrisdown.name>
Date:   Fri May 31 22:30:22 2019 -0700

    mm, memcg: consider subtrees in memory.events
    
    memory.stat and other files already consider subtrees in their output, and
    we should too in order to not present an inconsistent interface.
    
    The current situation is fairly confusing, because people interacting with
    cgroups expect hierarchical behaviour in the vein of memory.stat,
    cgroup.events, and other files.  For example, this causes confusion when
    debugging reclaim events under low, as currently these always read "0" at
    non-leaf memcg nodes, which frequently causes people to misdiagnose breach
    behaviour.  The same confusion applies to other counters in this file when
    debugging issues.
    
    Aggregation is done at write time instead of at read-time since these
    counters aren't hot (unlike memory.stat which is per-page, so it does it
    at read time), and it makes sense to bundle this with the file
    notifications.
    
    After this patch, events are propagated up the hierarchy:
    
        [root@ktst ~]# cat /sys/fs/cgroup/system.slice/memory.events
        low 0
        high 0
        max 0
        oom 0
        oom_kill 0
        [root@ktst ~]# systemd-run -p MemoryMax=1 true
        Running as unit: run-r251162a189fb4562b9dabfdc9b0422f5.service
        [root@ktst ~]# cat /sys/fs/cgroup/system.slice/memory.events
        low 0
        high 0
        max 7
        oom 1
        oom_kill 1
    
    As this is a change in behaviour, this can be reverted to the old
    behaviour by mounting with the `memory_localevents' flag set.  However, we
    use the new behaviour by default as there's a lack of evidence that there
    are any current users of memory.events that would find this change
    undesirable.
    
    akpm: this is a behaviour change, so Cc:stable.  THis is so that
    forthcoming distros which use cgroup v2 are more likely to pick up the
    revised behaviour.
    
    Link: http://lkml.kernel.org/r/20190208224419.GA24772@chrisdown.name
    Signed-off-by: Chris Down <chris@chrisdown.name>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 77258d276f93..11e215d7937e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -89,6 +89,11 @@ enum {
 	 * Enable cpuset controller in v1 cgroup to use v2 behavior.
 	 */
 	CGRP_ROOT_CPUSET_V2_MODE = (1 << 4),
+
+	/*
+	 * Enable legacy local memory.events.
+	 */
+	CGRP_ROOT_MEMORY_LOCAL_EVENTS = (1 << 5),
 };
 
 /* cftype->flags */

commit c03cd7738a83b13739f00546166969342c8ff014
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 31 10:38:58 2019 -0700

    cgroup: Include dying leaders with live threads in PROCS iterations
    
    CSS_TASK_ITER_PROCS currently iterates live group leaders; however,
    this means that a process with dying leader and live threads will be
    skipped.  IOW, cgroup.procs might be empty while cgroup.threads isn't,
    which is confusing to say the least.
    
    Fix it by making cset track dying tasks and include dying leaders with
    live threads in PROCS iteration.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Topi Miettinen <toiwoton@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 77258d276f93..1615b9c17e02 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -216,6 +216,7 @@ struct css_set {
 	 */
 	struct list_head tasks;
 	struct list_head mg_tasks;
+	struct list_head dying_tasks;
 
 	/* all css_task_iters currently walking this cset */
 	struct list_head task_iters;

commit 76f969e8948d82e78e1bc4beb6b9465908e74873
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 19 10:03:04 2019 -0700

    cgroup: cgroup v2 freezer
    
    Cgroup v1 implements the freezer controller, which provides an ability
    to stop the workload in a cgroup and temporarily free up some
    resources (cpu, io, network bandwidth and, potentially, memory)
    for some other tasks. Cgroup v2 lacks this functionality.
    
    This patch implements freezer for cgroup v2.
    
    Cgroup v2 freezer tries to put tasks into a state similar to jobctl
    stop. This means that tasks can be killed, ptraced (using
    PTRACE_SEIZE*), and interrupted. It is possible to attach to
    a frozen task, get some information (e.g. read registers) and detach.
    It's also possible to migrate a frozen tasks to another cgroup.
    
    This differs cgroup v2 freezer from cgroup v1 freezer, which mostly
    tried to imitate the system-wide freezer. However uninterruptible
    sleep is fine when all tasks are going to be frozen (hibernation case),
    it's not the acceptable state for some subset of the system.
    
    Cgroup v2 freezer is not supporting freezing kthreads.
    If a non-root cgroup contains kthread, the cgroup still can be frozen,
    but the kthread will remain running, the cgroup will be shown
    as non-frozen, and the notification will not be delivered.
    
    * PTRACE_ATTACH is not working because non-fatal signal delivery
    is blocked in frozen state.
    
    There are some interface differences between cgroup v1 and cgroup v2
    freezer too, which are required to conform the cgroup v2 interface
    design principles:
    1) There is no separate controller, which has to be turned on:
    the functionality is always available and is represented by
    cgroup.freeze and cgroup.events cgroup control files.
    2) The desired state is defined by the cgroup.freeze control file.
    Any hierarchical configuration is allowed.
    3) The interface is asynchronous. The actual state is available
    using cgroup.events control file ("frozen" field). There are no
    dedicated transitional states.
    4) It's allowed to make any changes with the cgroup hierarchy
    (create new cgroups, remove old cgroups, move tasks between cgroups)
    no matter if some cgroups are frozen.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    No-objection-from-me-by: Oleg Nesterov <oleg@redhat.com>
    Cc: kernel-team@fb.com

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 7d57890cec67..77258d276f93 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -65,6 +65,12 @@ enum {
 	 * specified at mount time and thus is implemented here.
 	 */
 	CGRP_CPUSET_CLONE_CHILDREN,
+
+	/* Control group has to be frozen. */
+	CGRP_FREEZE,
+
+	/* Cgroup is frozen. */
+	CGRP_FROZEN,
 };
 
 /* cgroup_root->flags */
@@ -317,6 +323,25 @@ struct cgroup_rstat_cpu {
 	struct cgroup *updated_next;		/* NULL iff not on the list */
 };
 
+struct cgroup_freezer_state {
+	/* Should the cgroup and its descendants be frozen. */
+	bool freeze;
+
+	/* Should the cgroup actually be frozen? */
+	int e_freeze;
+
+	/* Fields below are protected by css_set_lock */
+
+	/* Number of frozen descendant cgroups */
+	int nr_frozen_descendants;
+
+	/*
+	 * Number of tasks, which are counted as frozen:
+	 * frozen, SIGSTOPped, and PTRACEd.
+	 */
+	int nr_frozen_tasks;
+};
+
 struct cgroup {
 	/* self css with NULL ->ss, points back to this cgroup */
 	struct cgroup_subsys_state self;
@@ -453,6 +478,9 @@ struct cgroup {
 	/* If there is block congestion on this cgroup. */
 	atomic_t congestion_count;
 
+	/* Used to store internal freezer state */
+	struct cgroup_freezer_state freezer;
+
 	/* ids of the ancestors at each level including self */
 	int ancestor_ids[];
 };

commit 4dcabece4c3a9f9522127be12cc12cc120399b2f
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 19 10:03:03 2019 -0700

    cgroup: protect cgroup->nr_(dying_)descendants by css_set_lock
    
    The number of descendant cgroups and the number of dying
    descendant cgroups are currently synchronized using the cgroup_mutex.
    
    The number of descendant cgroups will be required by the cgroup v2
    freezer, which will use it to determine if a cgroup is frozen
    (depending on total number of descendants and number of frozen
    descendants). It's not always acceptable to grab the cgroup_mutex,
    especially from quite hot paths (e.g. exit()).
    
    To avoid this, let's additionally synchronize these counters using
    the css_set_lock.
    
    So, it's safe to read these counters with either cgroup_mutex or
    css_set_lock locked, and for changing both locks should be acquired.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: kernel-team@fb.com

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 1c70803e9f77..7d57890cec67 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -349,6 +349,11 @@ struct cgroup {
 	 * Dying cgroups are cgroups which were deleted by a user,
 	 * but are still existing because someone else is holding a reference.
 	 * max_descendants is a maximum allowed number of descent cgroups.
+	 *
+	 * nr_descendants and nr_dying_descendants are protected
+	 * by cgroup_mutex and css_set_lock. It's fine to read them holding
+	 * any of cgroup_mutex and css_set_lock; for writing both locks
+	 * should be held.
 	 */
 	int nr_descendants;
 	int nr_dying_descendants;

commit 1fc1cd8399ab5541a488a7e47b2f21537dd76c2d
Merge: abf7c3d8ddea 6a613d24effc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 10:11:41 2019 -0800

    Merge branch 'for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - Oleg's pids controller accounting update which gets rid of rcu delay
       in pids accounting updates
    
     - rstat (cgroup hierarchical stat collection mechanism) optimization
    
     - Doc updates
    
    * 'for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cpuset: remove unused task_has_mempolicy()
      cgroup, rstat: Don't flush subtree root unless necessary
      cgroup: add documentation for pids.events file
      Documentation: cgroup-v2: eliminate markup warnings
      MAINTAINERS: Update cgroup entry
      cgroup/pids: turn cgroup_subsys->free() into cgroup_subsys->release() to fix the accounting

commit dc50537bdd1a0804fa2cbc990565ee9a944e66fa
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Mar 5 15:45:48 2019 -0800

    kernel: cgroup: add poll file operation
    
    Cgroup has a standardized poll/notification mechanism for waking all
    pollers on all fds when a filesystem node changes.  To allow polling for
    custom events, add a .poll callback that can override the default.
    
    This is in preparation for pollable cgroup pressure files which have
    per-fd trigger configurations.
    
    Link: http://lkml.kernel.org/r/20190124211518.244221-3-surenb@google.com
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8fcbae1b8db0..aad3babef007 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -32,6 +32,7 @@ struct kernfs_node;
 struct kernfs_ops;
 struct kernfs_open_file;
 struct seq_file;
+struct poll_table_struct;
 
 #define MAX_CGROUP_TYPE_NAMELEN 32
 #define MAX_CGROUP_ROOT_NAMELEN 64
@@ -574,6 +575,9 @@ struct cftype {
 	ssize_t (*write)(struct kernfs_open_file *of,
 			 char *buf, size_t nbytes, loff_t off);
 
+	__poll_t (*poll)(struct kernfs_open_file *of,
+			 struct poll_table_struct *pt);
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lock_class_key	lockdep_key;
 #endif

commit 51bee5abeab2058ea5813c5615d6197a23dbf041
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 28 17:00:13 2019 +0100

    cgroup/pids: turn cgroup_subsys->free() into cgroup_subsys->release() to fix the accounting
    
    The only user of cgroup_subsys->free() callback is pids_cgrp_subsys which
    needs pids_free() to uncharge the pid.
    
    However, ->free() is called from __put_task_struct()->cgroup_free() and this
    is too late. Even the trivial program which does
    
            for (;;) {
                    int pid = fork();
                    assert(pid >= 0);
                    if (pid)
                            wait(NULL);
                    else
                            exit(0);
            }
    
    can run out of limits because release_task()->call_rcu(delayed_put_task_struct)
    implies an RCU gp after the task/pid goes away and before the final put().
    
    Test-case:
    
            mkdir -p /tmp/CG
            mount -t cgroup2 none /tmp/CG
            echo '+pids' > /tmp/CG/cgroup.subtree_control
    
            mkdir /tmp/CG/PID
            echo 2 > /tmp/CG/PID/pids.max
    
            perl -e 'while ($p = fork) { wait; } $p // die "fork failed: $!\n"' &
            echo $! > /tmp/CG/PID/cgroup.procs
    
    Without this patch the forking process fails soon after migration.
    
    Rename cgroup_subsys->free() to cgroup_subsys->release() and move the callsite
    into the new helper, cgroup_release(), called by release_task() which actually
    frees the pid(s).
    
    Reported-by: Herton R. Krzesinski <hkrzesin@redhat.com>
    Reported-by: Jan Stancek <jstancek@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8fcbae1b8db0..120d1d40704b 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -602,7 +602,7 @@ struct cgroup_subsys {
 	void (*cancel_fork)(struct task_struct *task);
 	void (*fork)(struct task_struct *task);
 	void (*exit)(struct task_struct *task);
-	void (*free)(struct task_struct *task);
+	void (*release)(struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
 
 	bool early_init:1;

commit 5cf8114d6e90b3822be5eb6a2faedf99d1c08f77
Author: Waiman Long <longman@redhat.com>
Date:   Thu Nov 8 10:08:46 2018 -0500

    cpuset: Expose cpuset.cpus.subpartitions with cgroup_debug
    
    For debugging purpose, it will be useful to expose the content of the
    subparts_cpus as a read-only file to see if the code work correctly.
    However, subparts_cpus will not be used at all in most use cases. So
    adding a new cpuset file that clutters the cgroup directory may not be
    desirable.  This is now being done by using the hidden "cgroup_debug"
    kernel command line option to expose a new "cpuset.cpus.subpartitions"
    file.
    
    That option was originally used by the debug controller to expose
    itself when configured into the kernel. This is now extended to set an
    internal flag used by cgroup_addrm_files(). A new CFTYPE_DEBUG flag
    can now be used to specify that a cgroup file should only be created
    when the "cgroup_debug" option is specified.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 5e1694fe035b..8fcbae1b8db0 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -92,6 +92,7 @@ enum {
 
 	CFTYPE_NO_PREFIX	= (1 << 3),	/* (DON'T USE FOR NEW FILES) no subsys prefix */
 	CFTYPE_WORLD_WRITABLE	= (1 << 4),	/* (DON'T USE FOR NEW FILES) S_IWUGO */
+	CFTYPE_DEBUG		= (1 << 5),	/* create when cgroup_debug */
 
 	/* internal flags, do not use outside cgroup core proper */
 	__CFTYPE_ONLY_ON_DFL	= (1 << 16),	/* only on default hierarchy */

commit 2ce7135adc9ad081aa3c49744144376ac74fea60
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:31 2018 -0700

    psi: cgroup support
    
    On a system that executes multiple cgrouped jobs and independent
    workloads, we don't just care about the health of the overall system, but
    also that of individual jobs, so that we can ensure individual job health,
    fairness between jobs, or prioritize some jobs over others.
    
    This patch implements pressure stall tracking for cgroups.  In kernels
    with CONFIG_PSI=y, cgroup2 groups will have cpu.pressure, memory.pressure,
    and io.pressure files that track aggregate pressure stall times for only
    the tasks inside the cgroup.
    
    Link: http://lkml.kernel.org/r/20180828172258.3185-10-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 22254c1fe1c5..5e1694fe035b 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -20,6 +20,7 @@
 #include <linux/u64_stats_sync.h>
 #include <linux/workqueue.h>
 #include <linux/bpf-cgroup.h>
+#include <linux/psi_types.h>
 
 #ifdef CONFIG_CGROUPS
 
@@ -436,6 +437,9 @@ struct cgroup {
 	/* used to schedule release agent */
 	struct work_struct release_agent_work;
 
+	/* used to track pressure stalls */
+	struct psi_group psi;
+
 	/* used to store eBPF programs */
 	struct cgroup_bpf bpf;
 

commit 479adb89a97b0a33e5a9d702119872cc82ca21aa
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 4 13:28:08 2018 -0700

    cgroup: Fix dom_cgrp propagation when enabling threaded mode
    
    A cgroup which is already a threaded domain may be converted into a
    threaded cgroup if the prerequisite conditions are met.  When this
    happens, all threaded descendant should also have their ->dom_cgrp
    updated to the new threaded domain cgroup.  Unfortunately, this
    propagation was missing leading to the following failure.
    
      # cd /sys/fs/cgroup/unified
      # cat cgroup.subtree_control    # show that no controllers are enabled
    
      # mkdir -p mycgrp/a/b/c
      # echo threaded > mycgrp/a/b/cgroup.type
    
      At this point, the hierarchy looks as follows:
    
          mycgrp [d]
              a [dt]
                  b [t]
                      c [inv]
    
      Now let's make node "a" threaded (and thus "mycgrp" s made "domain threaded"):
    
      # echo threaded > mycgrp/a/cgroup.type
    
      By this point, we now have a hierarchy that looks as follows:
    
          mycgrp [dt]
              a [t]
                  b [t]
                      c [inv]
    
      But, when we try to convert the node "c" from "domain invalid" to
      "threaded", we get ENOTSUP on the write():
    
      # echo threaded > mycgrp/a/b/c/cgroup.type
      sh: echo: write error: Operation not supported
    
    This patch fixes the problem by
    
    * Moving the opencoded ->dom_cgrp save and restoration in
      cgroup_enable_threaded() into cgroup_{save|restore}_control() so
      that mulitple cgroups can be handled.
    
    * Updating all threaded descendants' ->dom_cgrp to point to the new
      dom_cgrp when enabling threaded mode.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: "Michael Kerrisk (man-pages)" <mtk.manpages@gmail.com>
    Reported-by: Amin Jamali <ajamali@pivotal.io>
    Reported-by: Joao De Almeida Pereira <jpereira@pivotal.io>
    Link: https://lore.kernel.org/r/CAKgNAkhHYCMn74TCNiMJ=ccLd7DcmXSbvw3CbZ1YREeG7iJM5g@mail.gmail.com
    Fixes: 454000adaa2a ("cgroup: introduce cgroup->dom_cgrp and threaded css_set handling")
    Cc: stable@vger.kernel.org # v4.14+

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ff20b677fb9f..22254c1fe1c5 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -412,6 +412,7 @@ struct cgroup {
 	 * specific task are charged to the dom_cgrp.
 	 */
 	struct cgroup *dom_cgrp;
+	struct cgroup *old_dom_cgrp;		/* used while enabling threaded */
 
 	/* per-cpu recursive resource statistics */
 	struct cgroup_rstat_cpu __percpu *rstat_cpu;

commit d09d8df3a29403693d9d20cc34ed101f2c558e2b
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Jul 3 11:14:55 2018 -0400

    blkcg: add generic throttling mechanism
    
    Since IO can be issued from literally anywhere it's almost impossible to
    do throttling without having some sort of adverse effect somewhere else
    in the system because of locking or other dependencies.  The best way to
    solve this is to do the throttling when we know we aren't holding any
    other kernel resources.  Do this by tracking throttling in a per-blkg
    basis, and if we require throttling flag the task that it needs to check
    before it returns to user space and possibly sleep there.
    
    This is to address the case where a process is doing work that is
    generating IO that can't be throttled, whether that is directly with a
    lot of REQ_META IO, or indirectly by allocating so much memory that it
    is swamping the disk with REQ_SWAP.  We can't use task_add_work as we
    don't want to induce a memory allocation in the IO path, so simply
    saving the request queue in the task and flagging it to do the
    notify_resume thing achieves the same result without the overhead of a
    memory allocation.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c0e68f903011..ff20b677fb9f 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -438,6 +438,9 @@ struct cgroup {
 	/* used to store eBPF programs */
 	struct cgroup_bpf bpf;
 
+	/* If there is block congestion on this cgroup. */
+	atomic_t congestion_count;
+
 	/* ids of the ancestors at each level including self */
 	int ancestor_ids[];
 };

commit 8f53470bab04229e93ff9e4c20338cc08b42b344
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 26 14:29:05 2018 -0700

    cgroup: Add cgroup_subsys->css_rstat_flush()
    
    This patch adds cgroup_subsys->css_rstat_flush().  If a subsystem has
    this callback, its csses are linked on cgrp->css_rstat_list and rstat
    will call the function whenever the associated cgroup is flushed.
    Flush is also performed when such csses are released so that residual
    counts aren't lost.
    
    Combined with the rstat API previous patches factored out, this allows
    controllers to plug into rstat to manage their statistics in a
    scalable way.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 60d62fe97dc3..c0e68f903011 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -130,6 +130,9 @@ struct cgroup_subsys_state {
 	struct list_head sibling;
 	struct list_head children;
 
+	/* flush target list anchored at cgrp->rstat_css_list */
+	struct list_head rstat_css_node;
+
 	/*
 	 * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
 	 * matching css can be looked up using css_from_id().
@@ -412,6 +415,7 @@ struct cgroup {
 
 	/* per-cpu recursive resource statistics */
 	struct cgroup_rstat_cpu __percpu *rstat_cpu;
+	struct list_head rstat_css_list;
 
 	/* cgroup basic resource statistics */
 	struct cgroup_base_stat pending_bstat;	/* pending from children */
@@ -577,6 +581,7 @@ struct cgroup_subsys {
 	void (*css_released)(struct cgroup_subsys_state *css);
 	void (*css_free)(struct cgroup_subsys_state *css);
 	void (*css_reset)(struct cgroup_subsys_state *css);
+	void (*css_rstat_flush)(struct cgroup_subsys_state *css, int cpu);
 	int (*css_extra_stat_show)(struct seq_file *seq,
 				   struct cgroup_subsys_state *css);
 

commit d4ff749b5e0f1e2d4d69a3e4ea81cdeaeb4904d2
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 26 14:29:04 2018 -0700

    cgroup: Distinguish base resource stat implementation from rstat
    
    Base resource stat accounts universial (not specific to any
    controller) resource consumptions on top of rstat.  Currently, its
    implementation is intermixed with rstat implementation making the code
    confusing to follow.
    
    This patch clarifies the distintion by doing the followings.
    
    * Encapsulate base resource stat counters, currently only cputime, in
      struct cgroup_base_stat.
    
    * Move prev_cputime into struct cgroup and initialize it with cgroup.
    
    * Rename the related functions so that they start with cgroup_base_stat.
    
    * Prefix the related variables and field names with b.
    
    This patch doesn't make any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 04cb42419310..60d62fe97dc3 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -258,6 +258,10 @@ struct css_set {
 	struct rcu_head rcu_head;
 };
 
+struct cgroup_base_stat {
+	struct task_cputime cputime;
+};
+
 /*
  * rstat - cgroup scalable recursive statistics.  Accounting is done
  * per-cpu in cgroup_rstat_cpu which is then lazily propagated up the
@@ -273,20 +277,24 @@ struct css_set {
  * aren't active and stat may be read frequently.  The combination can
  * become very expensive.  By propagating selectively, increasing reading
  * frequency decreases the cost of each read.
+ *
+ * This struct hosts both the fields which implement the above -
+ * updated_children and updated_next - and the fields which track basic
+ * resource statistics on top of it - bsync, bstat and last_bstat.
  */
 struct cgroup_rstat_cpu {
 	/*
-	 * ->sync protects all the current counters.  These are the only
-	 * fields which get updated in the hot path.
+	 * ->bsync protects ->bstat.  These are the only fields which get
+	 * updated in the hot path.
 	 */
-	struct u64_stats_sync sync;
-	struct task_cputime cputime;
+	struct u64_stats_sync bsync;
+	struct cgroup_base_stat bstat;
 
 	/*
 	 * Snapshots at the last reading.  These are used to calculate the
 	 * deltas to propagate to the global counters.
 	 */
-	struct task_cputime last_cputime;
+	struct cgroup_base_stat last_bstat;
 
 	/*
 	 * Child cgroups with stat updates on this cpu since the last read
@@ -303,12 +311,6 @@ struct cgroup_rstat_cpu {
 	struct cgroup *updated_next;		/* NULL iff not on the list */
 };
 
-struct cgroup_stat {
-	/* per-cpu statistics are collected into the folowing global counters */
-	struct task_cputime cputime;
-	struct prev_cputime prev_cputime;
-};
-
 struct cgroup {
 	/* self css with NULL ->ss, points back to this cgroup */
 	struct cgroup_subsys_state self;
@@ -412,8 +414,9 @@ struct cgroup {
 	struct cgroup_rstat_cpu __percpu *rstat_cpu;
 
 	/* cgroup basic resource statistics */
-	struct cgroup_stat pending_stat;	/* pending from children */
-	struct cgroup_stat stat;
+	struct cgroup_base_stat pending_bstat;	/* pending from children */
+	struct cgroup_base_stat bstat;
+	struct prev_cputime prev_cputime;	/* for printing out cputime */
 
 	/*
 	 * list of pidlists, up to two for each namespace (one for procs, one

commit c58632b3631cb222da41d9dc0dd39e106c1eafd0
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 26 14:29:04 2018 -0700

    cgroup: Rename stat to rstat
    
    stat is too generic a name and ends up causing subtle confusions.
    It'll be made generic so that controllers can plug into it, which will
    make the problem worse.  Let's rename it to something more specific -
    cgroup_rstat for cgroup recursive stat.
    
    This patch does the following renames.  No other changes.
    
    * cpu_stat      -> rstat_cpu
    * stat          -> rstat
    * ?cstat        -> ?rstatc
    
    Note that the renames are selective.  The unrenamed are the ones which
    implement basic resource statistics on top of rstat.  This will be
    further cleaned up in the following patches.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 133531fcfb33..04cb42419310 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -259,11 +259,11 @@ struct css_set {
 };
 
 /*
- * cgroup basic resource usage statistics.  Accounting is done per-cpu in
- * cgroup_cpu_stat which is then lazily propagated up the hierarchy on
- * reads.
+ * rstat - cgroup scalable recursive statistics.  Accounting is done
+ * per-cpu in cgroup_rstat_cpu which is then lazily propagated up the
+ * hierarchy on reads.
  *
- * When a stat gets updated, the cgroup_cpu_stat and its ancestors are
+ * When a stat gets updated, the cgroup_rstat_cpu and its ancestors are
  * linked into the updated tree.  On the following read, propagation only
  * considers and consumes the updated tree.  This makes reading O(the
  * number of descendants which have been active since last read) instead of
@@ -274,7 +274,7 @@ struct css_set {
  * become very expensive.  By propagating selectively, increasing reading
  * frequency decreases the cost of each read.
  */
-struct cgroup_cpu_stat {
+struct cgroup_rstat_cpu {
 	/*
 	 * ->sync protects all the current counters.  These are the only
 	 * fields which get updated in the hot path.
@@ -297,7 +297,7 @@ struct cgroup_cpu_stat {
 	 * to the cgroup makes it unnecessary for each per-cpu struct to
 	 * point back to the associated cgroup.
 	 *
-	 * Protected by per-cpu cgroup_cpu_stat_lock.
+	 * Protected by per-cpu cgroup_rstat_cpu_lock.
 	 */
 	struct cgroup *updated_children;	/* terminated by self cgroup */
 	struct cgroup *updated_next;		/* NULL iff not on the list */
@@ -408,8 +408,10 @@ struct cgroup {
 	 */
 	struct cgroup *dom_cgrp;
 
+	/* per-cpu recursive resource statistics */
+	struct cgroup_rstat_cpu __percpu *rstat_cpu;
+
 	/* cgroup basic resource statistics */
-	struct cgroup_cpu_stat __percpu *cpu_stat;
 	struct cgroup_stat pending_stat;	/* pending from children */
 	struct cgroup_stat stat;
 

commit b12e35832805bf120819b83d8fdb6e4d227a5ddb
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 26 14:29:04 2018 -0700

    cgroup: Limit event generation frequency
    
    ".events" files generate file modified event to notify userland of
    possible new events.  Some of the events can be quite bursty
    (e.g. memory high event) and generating notification each time is
    costly and pointless.
    
    This patch implements a event rate limit mechanism.  If a new
    notification is requested before 10ms has passed since the previous
    notification, the new notification is delayed till then.
    
    As this only delays from the second notification on in a given close
    cluster of notifications, userland reactions to notifications
    shouldn't be delayed at all in most cases while avoiding notification
    storms.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index dc5b70449dc6..133531fcfb33 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -105,6 +105,8 @@ enum {
 struct cgroup_file {
 	/* do not access any fields from outside cgroup core */
 	struct kernfs_node *kn;
+	unsigned long notified_at;
+	struct timer_list notify_timer;
 };
 
 /*

commit d92cd810e64aa7cf22b05f0ea1c7d3e8dbae75fe
Merge: a23867f1d2de f75da8a8a918
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 3 18:00:13 2018 -0700

    Merge branch 'for-4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "rcu_work addition and a couple trivial changes"
    
    * 'for-4.17' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: remove the comment about the old manager_arb mutex
      workqueue: fix the comments of nr_idle
      fs/aio: Use rcu_work instead of explicit rcu and work item
      cgroup: Use rcu_work instead of explicit rcu and work item
      RCU, workqueue: Implement rcu_work

commit 8f36aaec9c929f2864196b0799203491f6a67dc6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 14 12:45:14 2018 -0700

    cgroup: Use rcu_work instead of explicit rcu and work item
    
    Workqueue now has rcu_work.  Use it instead of open-coding rcu -> work
    item bouncing.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 9f242b876fde..92d7640632ef 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -151,8 +151,8 @@ struct cgroup_subsys_state {
 	atomic_t online_cnt;
 
 	/* percpu_ref killing and RCU release */
-	struct rcu_head rcu_head;
 	struct work_struct destroy_work;
+	struct rcu_work destroy_rwork;
 
 	/*
 	 * PI: the parent css.	Placed here for cache proximity to following

commit 4dcb31d4649df36297296b819437709f5407059c
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 14 09:04:16 2018 -0700

    net: use skb_to_full_sk() in skb_update_prio()
    
    Andrei Vagin reported a KASAN: slab-out-of-bounds error in
    skb_update_prio()
    
    Since SYNACK might be attached to a request socket, we need to
    get back to the listener socket.
    Since this listener is manipulated without locks, add const
    qualifiers to sock_cgroup_prioidx() so that the const can also
    be used in skb_update_prio()
    
    Also add the const qualifier to sock_cgroup_classid() for consistency.
    
    Fixes: ca6fb0651883 ("tcp: attach SYNACK messages to request sockets instead of listener")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Andrei Vagin <avagin@virtuozzo.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 9f242b876fde..f8e76d01a5ad 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -755,13 +755,13 @@ struct sock_cgroup_data {
  * updaters and return part of the previous pointer as the prioidx or
  * classid.  Such races are short-lived and the result isn't critical.
  */
-static inline u16 sock_cgroup_prioidx(struct sock_cgroup_data *skcd)
+static inline u16 sock_cgroup_prioidx(const struct sock_cgroup_data *skcd)
 {
 	/* fallback to 1 which is always the ID of the root cgroup */
 	return (skcd->is_data & 1) ? skcd->prioidx : 1;
 }
 
-static inline u32 sock_cgroup_classid(struct sock_cgroup_data *skcd)
+static inline u32 sock_cgroup_classid(const struct sock_cgroup_data *skcd)
 {
 	/* fallback to 0 which is the unconfigured default classid */
 	return (skcd->is_data & 1) ? skcd->classid : 0;

commit 392536b731cfe82eea414f4b09c128ef37cd477e
Author: Matt Roper <matthew.d.roper@intel.com>
Date:   Fri Dec 29 12:02:00 2017 -0800

    cgroup: Update documentation reference
    
    The cgroup_subsys structure references a documentation file that has been
    renamed after the v1/v2 split.  Since the v2 documentation doesn't
    currently contain any information on kernel interfaces for controllers,
    point the user to the v1 docs.
    
    Cc: Tejun Heo <tj@kernel.org>
    Cc: linux-doc@vger.kernel.org
    Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8b7fd8eeccee..9f242b876fde 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -561,7 +561,7 @@ struct cftype {
 
 /*
  * Control Group subsystem type.
- * See Documentation/cgroups/cgroups.txt for details
+ * See Documentation/cgroup-v1/cgroups.txt for details
  */
 struct cgroup_subsys {
 	struct cgroup_subsys_state *(*css_alloc)(struct cgroup_subsys_state *parent_css);

commit 22714a2ba4b55737cd7d5299db7aaf1fa8287354
Merge: 766ec76a27aa 5f2e673405b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 14:29:44 2017 -0800

    Merge branch 'for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Cgroup2 cpu controller support is finally merged.
    
       - Basic cpu statistics support to allow monitoring by default without
         the CPU controller enabled.
    
       - cgroup2 cpu controller support.
    
       - /sys/kernel/cgroup files to help dealing with new / optional
         features"
    
    * 'for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: export list of cgroups v2 features using sysfs
      cgroup: export list of delegatable control files using sysfs
      cgroup: mark @cgrp __maybe_unused in cpu_stat_show()
      MAINTAINERS: relocate cpuset.c
      cgroup, sched: Move basic cpu stats from cgroup.stat to cpu.stat
      sched: Implement interface for cgroup unified hierarchy
      sched: Misc preps for cgroup unified hierarchy interface
      sched/cputime: Add dummy cputime_adjust() implementation for CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
      cgroup: statically initialize init_css_set->dfl_cgrp
      cgroup: Implement cgroup2 basic CPU usage accounting
      cpuacct: Introduce cgroup_account_cputime[_field]()
      sched/cputime: Expose cputime_adjust()

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ade4a78a54c2..1dff0a478b45 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * linux/cgroup-defs.h - basic definitions for cgroup
  *

commit d41bf8c9deaed1a90b18d3ffc5639d4c19f0259a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Oct 23 16:18:27 2017 -0700

    cgroup, sched: Move basic cpu stats from cgroup.stat to cpu.stat
    
    The basic cpu stat is currently shown with "cpu." prefix in
    cgroup.stat, and the same information is duplicated in cpu.stat when
    cpu controller is enabled.  This is ugly and not very scalable as we
    want to expand the coverage of stat information which is always
    available.
    
    This patch makes cgroup core always create "cpu.stat" file and show
    the basic cpu stat there and calls the cpu controller to show the
    extra stats when enabled.  This ensures that the same information
    isn't presented in multiple places and makes future expansion of basic
    stats easier.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 3e55bbd31ad1..ada6df7b1f55 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -569,6 +569,8 @@ struct cgroup_subsys {
 	void (*css_released)(struct cgroup_subsys_state *css);
 	void (*css_free)(struct cgroup_subsys_state *css);
 	void (*css_reset)(struct cgroup_subsys_state *css);
+	int (*css_extra_stat_show)(struct seq_file *seq,
+				   struct cgroup_subsys_state *css);
 
 	int (*can_attach)(struct cgroup_taskset *tset);
 	void (*cancel_attach)(struct cgroup_taskset *tset);

commit 041cd640b2f3c5607171c59d8712b503659d21f7
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 25 08:12:05 2017 -0700

    cgroup: Implement cgroup2 basic CPU usage accounting
    
    In cgroup1, while cpuacct isn't actually controlling any resources, it
    is a separate controller due to combination of two factors -
    1. enabling cpu controller has significant side effects, and 2. we
    have to pick one of the hierarchies to account CPU usages on.  cpuacct
    controller is effectively used to designate a hierarchy to track CPU
    usages on.
    
    cgroup2's unified hierarchy removes the second reason and we can
    account basic CPU usages by default.  While we can use cpuacct for
    this purpose, both its interface and implementation leave a lot to be
    desired - it collects and exposes two sources of truth which don't
    agree with each other and some of the exposed statistics don't make
    much sense.  Also, it propagates all the way up the hierarchy on each
    accounting event which is unnecessary.
    
    This patch adds basic resource accounting mechanism to cgroup2's
    unified hierarchy and accounts CPU usages using it.
    
    * All accountings are done per-cpu and don't propagate immediately.
      It just bumps the per-cgroup per-cpu counters and links to the
      parent's updated list if not already on it.
    
    * On a read, the per-cpu counters are collected into the global ones
      and then propagated upwards.  Only the per-cpu counters which have
      changed since the last read are propagated.
    
    * CPU usage stats are collected and shown in "cgroup.stat" with "cpu."
      prefix.  Total usage is collected from scheduling events.  User/sys
      breakdown is sourced from tick sampling and adjusted to the usage
      using cputime_adjust().
    
    This keeps the accounting side hot path O(1) and per-cpu and the read
    side O(nr_updated_since_last_read).
    
    v2: Minor changes and documentation updates as suggested by Waiman and
        Roman.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Roman Gushchin <guro@fb.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ade4a78a54c2..3e55bbd31ad1 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -16,6 +16,7 @@
 #include <linux/refcount.h>
 #include <linux/percpu-refcount.h>
 #include <linux/percpu-rwsem.h>
+#include <linux/u64_stats_sync.h>
 #include <linux/workqueue.h>
 #include <linux/bpf-cgroup.h>
 
@@ -254,6 +255,57 @@ struct css_set {
 	struct rcu_head rcu_head;
 };
 
+/*
+ * cgroup basic resource usage statistics.  Accounting is done per-cpu in
+ * cgroup_cpu_stat which is then lazily propagated up the hierarchy on
+ * reads.
+ *
+ * When a stat gets updated, the cgroup_cpu_stat and its ancestors are
+ * linked into the updated tree.  On the following read, propagation only
+ * considers and consumes the updated tree.  This makes reading O(the
+ * number of descendants which have been active since last read) instead of
+ * O(the total number of descendants).
+ *
+ * This is important because there can be a lot of (draining) cgroups which
+ * aren't active and stat may be read frequently.  The combination can
+ * become very expensive.  By propagating selectively, increasing reading
+ * frequency decreases the cost of each read.
+ */
+struct cgroup_cpu_stat {
+	/*
+	 * ->sync protects all the current counters.  These are the only
+	 * fields which get updated in the hot path.
+	 */
+	struct u64_stats_sync sync;
+	struct task_cputime cputime;
+
+	/*
+	 * Snapshots at the last reading.  These are used to calculate the
+	 * deltas to propagate to the global counters.
+	 */
+	struct task_cputime last_cputime;
+
+	/*
+	 * Child cgroups with stat updates on this cpu since the last read
+	 * are linked on the parent's ->updated_children through
+	 * ->updated_next.
+	 *
+	 * In addition to being more compact, singly-linked list pointing
+	 * to the cgroup makes it unnecessary for each per-cpu struct to
+	 * point back to the associated cgroup.
+	 *
+	 * Protected by per-cpu cgroup_cpu_stat_lock.
+	 */
+	struct cgroup *updated_children;	/* terminated by self cgroup */
+	struct cgroup *updated_next;		/* NULL iff not on the list */
+};
+
+struct cgroup_stat {
+	/* per-cpu statistics are collected into the folowing global counters */
+	struct task_cputime cputime;
+	struct prev_cputime prev_cputime;
+};
+
 struct cgroup {
 	/* self css with NULL ->ss, points back to this cgroup */
 	struct cgroup_subsys_state self;
@@ -353,6 +405,11 @@ struct cgroup {
 	 */
 	struct cgroup *dom_cgrp;
 
+	/* cgroup basic resource statistics */
+	struct cgroup_cpu_stat __percpu *cpu_stat;
+	struct cgroup_stat pending_stat;	/* pending from children */
+	struct cgroup_stat stat;
+
 	/*
 	 * list of pidlists, up to two for each namespace (one for procs, one
 	 * for tasks); created on demand.

commit e1cba4b85daa71b710384d451ff6238d5e4d1ff6
Author: Waiman Long <longman@redhat.com>
Date:   Thu Aug 17 15:33:09 2017 -0400

    cgroup: Add mount flag to enable cpuset to use v2 behavior in v1 cgroup
    
    A new mount option "cpuset_v2_mode" is added to the v1 cgroupfs
    filesystem to enable cpuset controller to use v2 behavior in a v1
    cgroup. This mount option applies only to cpuset controller and have
    no effect on other controllers.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 59e4ad9e7bac..ade4a78a54c2 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -74,6 +74,11 @@ enum {
 	 * aren't writeable from inside the namespace.
 	 */
 	CGRP_ROOT_NS_DELEGATE	= (1 << 3),
+
+	/*
+	 * Enable cpuset controller in v1 cgroup to use v2 behavior.
+	 */
+	CGRP_ROOT_CPUSET_V2_MODE = (1 << 4),
 };
 
 /* cftype->flags */

commit 1a926e0bbab83bae8207d05a533173425e0496d1
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Jul 28 18:28:44 2017 +0100

    cgroup: implement hierarchy limits
    
    Creating cgroup hierearchies of unreasonable size can affect
    overall system performance. A user might want to limit the
    size of cgroup hierarchy. This is especially important if a user
    is delegating some cgroup sub-tree.
    
    To address this issue, introduce an ability to control
    the size of cgroup hierarchy.
    
    The cgroup.max.descendants control file allows to set the maximum
    allowed number of descendant cgroups.
    The cgroup.max.depth file controls the maximum depth of the cgroup
    tree. Both are single value r/w files, with "max" default value.
    
    The control files exist on each hierarchy level (including root).
    When a new cgroup is created, we check the total descendants
    and depth limits on each level, and if none of them are exceeded,
    a new cgroup is created.
    
    Only alive cgroups are counted, removed (dying) cgroups are
    ignored.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Suggested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: kernel-team@fb.com
    Cc: cgroups@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 58b4c425a155..59e4ad9e7bac 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -273,13 +273,18 @@ struct cgroup {
 	 */
 	int level;
 
+	/* Maximum allowed descent tree depth */
+	int max_depth;
+
 	/*
 	 * Keep track of total numbers of visible and dying descent cgroups.
 	 * Dying cgroups are cgroups which were deleted by a user,
 	 * but are still existing because someone else is holding a reference.
+	 * max_descendants is a maximum allowed number of descent cgroups.
 	 */
 	int nr_descendants;
 	int nr_dying_descendants;
+	int max_descendants;
 
 	/*
 	 * Each non-empty css_set associated with this cgroup contributes

commit 0679dee03c6d706d57145ea92c23d08fa10a1999
Author: Roman Gushchin <guro@fb.com>
Date:   Wed Aug 2 17:55:29 2017 +0100

    cgroup: keep track of number of descent cgroups
    
    Keep track of the number of online and dying descent cgroups.
    
    This data will be used later to add an ability to control cgroup
    hierarchy (limit the depth and the number of descent cgroups)
    and display hierarchy stats.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Suggested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: kernel-team@fb.com
    Cc: cgroups@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 9d741959f218..58b4c425a155 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -273,6 +273,14 @@ struct cgroup {
 	 */
 	int level;
 
+	/*
+	 * Keep track of total numbers of visible and dying descent cgroups.
+	 * Dying cgroups are cgroups which were deleted by a user,
+	 * but are still existing because someone else is holding a reference.
+	 */
+	int nr_descendants;
+	int nr_dying_descendants;
+
 	/*
 	 * Each non-empty css_set associated with this cgroup contributes
 	 * one to nr_populated_csets.  The counter is zero iff this cgroup

commit 8cfd8147df67e741d93b8783a3ea8f3c74f93a0e
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 21 11:14:51 2017 -0400

    cgroup: implement cgroup v2 thread support
    
    This patch implements cgroup v2 thread support.  The goal of the
    thread mode is supporting hierarchical accounting and control at
    thread granularity while staying inside the resource domain model
    which allows coordination across different resource controllers and
    handling of anonymous resource consumptions.
    
    A cgroup is always created as a domain and can be made threaded by
    writing to the "cgroup.type" file.  When a cgroup becomes threaded, it
    becomes a member of a threaded subtree which is anchored at the
    closest ancestor which isn't threaded.
    
    The threads of the processes which are in a threaded subtree can be
    placed anywhere without being restricted by process granularity or
    no-internal-process constraint.  Note that the threads aren't allowed
    to escape to a different threaded subtree.  To be used inside a
    threaded subtree, a controller should explicitly support threaded mode
    and be able to handle internal competition in the way which is
    appropriate for the resource.
    
    The root of a threaded subtree, the nearest ancestor which isn't
    threaded, is called the threaded domain and serves as the resource
    domain for the whole subtree.  This is the last cgroup where domain
    controllers are operational and where all the domain-level resource
    consumptions in the subtree are accounted.  This allows threaded
    controllers to operate at thread granularity when requested while
    staying inside the scope of system-level resource distribution.
    
    As the root cgroup is exempt from the no-internal-process constraint,
    it can serve as both a threaded domain and a parent to normal cgroups,
    so, unlike non-root cgroups, the root cgroup can have both domain and
    threaded children.
    
    Internally, in a threaded subtree, each css_set has its ->dom_cset
    pointing to a matching css_set which belongs to the threaded domain.
    This ensures that thread root level cgroup_subsys_state for all
    threaded controllers are readily accessible for domain-level
    operations.
    
    This patch enables threaded mode for the pids and perf_events
    controllers.  Neither has to worry about domain-level resource
    consumptions and it's enough to simply set the flag.
    
    For more details on the interface and behavior of the thread mode,
    please refer to the section 2-2-2 in Documentation/cgroup-v2.txt added
    by this patch.
    
    v5: - Dropped silly no-op ->dom_cgrp init from cgroup_create().
          Spotted by Waiman.
        - Documentation updated as suggested by Waiman.
        - cgroup.type content slightly reformatted.
        - Mark the debug controller threaded.
    
    v4: - Updated to the general idea of marking specific cgroups
          domain/threaded as suggested by PeterZ.
    
    v3: - Dropped "join" and always make mixed children join the parent's
          threaded subtree.
    
    v2: - After discussions with Waiman, support for mixed thread mode is
          added.  This should address the issue that Peter pointed out
          where any nesting should be avoided for thread subtrees while
          coexisting with other domain cgroups.
        - Enabling / disabling thread mode now piggy backs on the existing
          control mask update mechanism.
        - Bug fixes and cleanup.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 651c4363c85e..9d741959f218 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -521,6 +521,18 @@ struct cgroup_subsys {
 	 */
 	bool implicit_on_dfl:1;
 
+	/*
+	 * If %true, the controller, supports threaded mode on the default
+	 * hierarchy.  In a threaded subtree, both process granularity and
+	 * no-internal-process constraint are ignored and a threaded
+	 * controllers should be able to handle that.
+	 *
+	 * Note that as an implicit controller is automatically enabled on
+	 * all cgroups on the default hierarchy, it should also be
+	 * threaded.  implicit && !threaded is not supported.
+	 */
+	bool threaded:1;
+
 	/*
 	 * If %false, this subsystem is properly hierarchical -
 	 * configuration, resource accounting and restriction on a parent

commit 454000adaa2a7420df6e56a42f22726d05872a3f
Author: Tejun Heo <tj@kernel.org>
Date:   Mon May 15 09:34:02 2017 -0400

    cgroup: introduce cgroup->dom_cgrp and threaded css_set handling
    
    cgroup v2 is in the process of growing thread granularity support.  A
    threaded subtree is composed of a thread root and threaded cgroups
    which are proper members of the subtree.
    
    The root cgroup of the subtree serves as the domain cgroup to which
    the processes (as opposed to threads / tasks) of the subtree
    conceptually belong and domain-level resource consumptions not tied to
    any specific task are charged.  Inside the subtree, threads won't be
    subject to process granularity or no-internal-task constraint and can
    be distributed arbitrarily across the subtree.
    
    This patch introduces cgroup->dom_cgrp along with threaded css_set
    handling.
    
    * cgroup->dom_cgrp points to self for normal and thread roots.  For
      proper thread subtree members, points to the dom_cgrp (the thread
      root).
    
    * css_set->dom_cset points to self if for normal and thread roots.  If
      threaded, points to the css_set which belongs to the cgrp->dom_cgrp.
      The dom_cgrp serves as the resource domain and keeps the matching
      csses available.  The dom_cset holds those csses and makes them
      easily accessible.
    
    * All threaded csets are linked on their dom_csets to enable iteration
      of all threaded tasks.
    
    * cgroup->nr_threaded_children keeps track of the number of threaded
      children.
    
    This patch adds the above but doesn't actually use them yet.  The
    following patches will build on top.
    
    v4: ->nr_threaded_children added.
    
    v3: ->proc_cgrp/cset renamed to ->dom_cgrp/cset.  Updated for the new
        enable-threaded-per-cgroup behavior.
    
    v2: Added cgroup_is_threaded() helper.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ae7bc1e70085..651c4363c85e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -172,6 +172,14 @@ struct css_set {
 	/* reference count */
 	refcount_t refcount;
 
+	/*
+	 * For a domain cgroup, the following points to self.  If threaded,
+	 * to the matching cset of the nearest domain ancestor.  The
+	 * dom_cset provides access to the domain cgroup and its csses to
+	 * which domain level resource consumptions should be charged.
+	 */
+	struct css_set *dom_cset;
+
 	/* the default cgroup associated with this css_set */
 	struct cgroup *dfl_cgrp;
 
@@ -200,6 +208,10 @@ struct css_set {
 	 */
 	struct list_head e_cset_node[CGROUP_SUBSYS_COUNT];
 
+	/* all threaded csets whose ->dom_cset points to this cset */
+	struct list_head threaded_csets;
+	struct list_head threaded_csets_node;
+
 	/*
 	 * List running through all cgroup groups in the same hash
 	 * slot. Protected by css_set_lock
@@ -267,12 +279,16 @@ struct cgroup {
 	 * doesn't have any tasks.
 	 *
 	 * All children which have non-zero nr_populated_csets and/or
-	 * nr_populated_children of their own contribute one to
-	 * nr_populated_children.  The counter is zero iff this cgroup's
-	 * subtree proper doesn't have any tasks.
+	 * nr_populated_children of their own contribute one to either
+	 * nr_populated_domain_children or nr_populated_threaded_children
+	 * depending on their type.  Each counter is zero iff all cgroups
+	 * of the type in the subtree proper don't have any tasks.
 	 */
 	int nr_populated_csets;
-	int nr_populated_children;
+	int nr_populated_domain_children;
+	int nr_populated_threaded_children;
+
+	int nr_threaded_children;	/* # of live threaded child cgroups */
 
 	struct kernfs_node *kn;		/* cgroup kernfs entry */
 	struct cgroup_file procs_file;	/* handle for "cgroup.procs" */
@@ -310,6 +326,15 @@ struct cgroup {
 	 */
 	struct list_head e_csets[CGROUP_SUBSYS_COUNT];
 
+	/*
+	 * If !threaded, self.  If threaded, it points to the nearest
+	 * domain ancestor.  Inside a threaded subtree, cgroups are exempt
+	 * from process granularity and no-internal-task constraint.
+	 * Domain level resource consumptions which aren't tied to a
+	 * specific task are charged to the dom_cgrp.
+	 */
+	struct cgroup *dom_cgrp;
+
 	/*
 	 * list of pidlists, up to two for each namespace (one for procs, one
 	 * for tasks); created on demand.

commit 788b950c62e06b02278a0fd380e1a0667996ce3c
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 16 21:43:33 2017 -0400

    cgroup: distinguish local and children populated states
    
    cgrp->populated_cnt counts both local (the cgroup's populated
    css_sets) and subtree proper (populated children) so that it's only
    zero when the whole subtree, including self, is empty.
    
    This patch splits the counter into two so that local and children
    populated states are tracked separately.  It allows finer-grained
    tests on the state of the hierarchy which will be used to replace
    css_set walking local populated test.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 09f4c7df1478..ae7bc1e70085 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -263,11 +263,16 @@ struct cgroup {
 
 	/*
 	 * Each non-empty css_set associated with this cgroup contributes
-	 * one to populated_cnt.  All children with non-zero popuplated_cnt
-	 * of their own contribute one.  The count is zero iff there's no
-	 * task in this cgroup or its subtree.
+	 * one to nr_populated_csets.  The counter is zero iff this cgroup
+	 * doesn't have any tasks.
+	 *
+	 * All children which have non-zero nr_populated_csets and/or
+	 * nr_populated_children of their own contribute one to
+	 * nr_populated_children.  The counter is zero iff this cgroup's
+	 * subtree proper doesn't have any tasks.
 	 */
-	int populated_cnt;
+	int nr_populated_csets;
+	int nr_populated_children;
 
 	struct kernfs_node *kn;		/* cgroup kernfs entry */
 	struct cgroup_file procs_file;	/* handle for "cgroup.procs" */

commit 5136f6365ce3eace5a926e10f16ed2a233db5ba9
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 27 14:30:28 2017 -0400

    cgroup: implement "nsdelegate" mount option
    
    Currently, cgroup only supports delegation to !root users and cgroup
    namespaces don't get any special treatments.  This limits the
    usefulness of cgroup namespaces as they by themselves can't be safe
    delegation boundaries.  A process inside a cgroup can change the
    resource control knobs of the parent in the namespace root and may
    move processes in and out of the namespace if cgroups outside its
    namespace are visible somehow.
    
    This patch adds a new mount option "nsdelegate" which makes cgroup
    namespaces delegation boundaries.  If set, cgroup behaves as if write
    permission based delegation took place at namespace boundaries -
    writes to the resource control knobs from the namespace root are
    denied and migration crossing the namespace boundary aren't allowed
    from inside the namespace.
    
    This allows cgroup namespace to function as a delegation boundary by
    itself.
    
    v2: Silently ignore nsdelegate specified on !init mounts.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Aravind Anbudurai <aru7@fb.com>
    Cc: Serge Hallyn <serge@hallyn.com>
    Cc: Eric Biederman <ebiederm@xmission.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 3bc4196bf217..09f4c7df1478 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -67,12 +67,21 @@ enum {
 enum {
 	CGRP_ROOT_NOPREFIX	= (1 << 1), /* mounted subsystems have no named prefix */
 	CGRP_ROOT_XATTR		= (1 << 2), /* supports extended attributes */
+
+	/*
+	 * Consider namespaces as delegation boundaries.  If this flag is
+	 * set, controller specific interface files in a namespace root
+	 * aren't writeable from inside the namespace.
+	 */
+	CGRP_ROOT_NS_DELEGATE	= (1 << 3),
 };
 
 /* cftype->flags */
 enum {
 	CFTYPE_ONLY_ON_ROOT	= (1 << 0),	/* only create on root cgrp */
 	CFTYPE_NOT_ON_ROOT	= (1 << 1),	/* don't create on root cgrp */
+	CFTYPE_NS_DELEGATABLE	= (1 << 2),	/* writeable beyond delegation boundaries */
+
 	CFTYPE_NO_PREFIX	= (1 << 3),	/* (DON'T USE FOR NEW FILES) no subsys prefix */
 	CFTYPE_WORLD_WRITABLE	= (1 << 4),	/* (DON'T USE FOR NEW FILES) S_IWUGO */
 

commit 73a7242a06ff995d771fbe243e72b516feaa6e3d
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jun 13 17:18:01 2017 -0400

    cgroup: Keep accurate count of tasks in each css_set
    
    The reference count in the css_set data structure was used as a
    proxy of the number of tasks attached to that css_set. However, that
    count is actually not an accurate measure especially with thread mode
    support. So a new variable nr_tasks is added to the css_set to keep
    track of the actual task count. This new variable is protected by
    the css_set_lock. Functions that require the actual task count are
    updated to use the new variable.
    
    tj: s/task_count/nr_tasks/ for consistency with cgroup_root->nr_cgrps.
        Refreshed on top of cgroup/for-v4.13 which dropped on
        css_set_populated() -> nr_tasks conversion.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ec47101cb1bf..3bc4196bf217 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -166,6 +166,9 @@ struct css_set {
 	/* the default cgroup associated with this css_set */
 	struct cgroup *dfl_cgrp;
 
+	/* internal task count, protected by css_set_lock */
+	int nr_tasks;
+
 	/*
 	 * Lists running through all tasks using this cgroup group.
 	 * mg_tasks lists tasks which belong to this cset but are in the

commit 33c35aa4817864e056fd772230b0c6b552e36ea2
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 15 09:34:06 2017 -0400

    cgroup: Prevent kill_css() from being called more than once
    
    The kill_css() function may be called more than once under the condition
    that the css was killed but not physically removed yet followed by the
    removal of the cgroup that is hosting the css. This patch prevents any
    harmm from being done when that happens.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@vger.kernel.org # v4.5+

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 21745946cae1..ec47101cb1bf 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -48,6 +48,7 @@ enum {
 	CSS_ONLINE	= (1 << 1), /* between ->css_online() and ->css_offline() */
 	CSS_RELEASED	= (1 << 2), /* refcnt reached zero, released */
 	CSS_VISIBLE	= (1 << 3), /* css is visible to userland */
+	CSS_DYING	= (1 << 4), /* css is dying */
 };
 
 /* bits in struct cgroup flags field */

commit b8b1a2e5eca6bbf20e3a29c5f9db8331ec52af2d
Author: Todd Poynor <toddpoynor@google.com>
Date:   Thu Apr 6 18:47:57 2017 -0700

    cgroup: move cgroup_subsys_state parent field for cache locality
    
    Various structures embed a struct cgroup_subsys_state, typically at
    the top of the containing structure.  It is common for code that
    accesses the structures to perform operations that iterate over the
    chain of parent css pointers, also accessing data in each containing
    structure.  In particular, struct cpuacct is used by fairly hot code
    paths in the scheduler such as cpuacct_charge().
    
    Move the parent css pointer field to the end of the structure to
    increase the chances of residing in the same cache line as the data
    from the containing structure.
    
    Signed-off-by: Todd Poynor <toddpoynor@google.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c74b78ecd583..21745946cae1 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -107,9 +107,6 @@ struct cgroup_subsys_state {
 	/* reference count - access via css_[try]get() and css_put() */
 	struct percpu_ref refcnt;
 
-	/* PI: the parent css */
-	struct cgroup_subsys_state *parent;
-
 	/* siblings list anchored at the parent's ->children */
 	struct list_head sibling;
 	struct list_head children;
@@ -139,6 +136,12 @@ struct cgroup_subsys_state {
 	/* percpu_ref killing and RCU release */
 	struct rcu_head rcu_head;
 	struct work_struct destroy_work;
+
+	/*
+	 * PI: the parent css.	Placed here for cache proximity to following
+	 * fields of the containing structure.
+	 */
+	struct cgroup_subsys_state *parent;
 };
 
 /*

commit 4b9502e63b5e2b1b5ef491919d3219b9440fe0b3
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Wed Mar 8 10:00:40 2017 +0200

    kernel: convert css_set.refcount from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 6a3f850cabab..c74b78ecd583 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -13,6 +13,7 @@
 #include <linux/wait.h>
 #include <linux/mutex.h>
 #include <linux/rcupdate.h>
+#include <linux/refcount.h>
 #include <linux/percpu-refcount.h>
 #include <linux/percpu-rwsem.h>
 #include <linux/workqueue.h>
@@ -156,7 +157,7 @@ struct css_set {
 	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
 
 	/* reference count */
-	atomic_t refcount;
+	refcount_t refcount;
 
 	/* the default cgroup associated with this css_set */
 	struct cgroup *dfl_cgrp;

commit 780de9dd2720debc14c501dab4dc80d1f75ad50e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:50:56 2017 +0100

    sched/headers, cgroups: Remove the threadgroup_change_*() wrappery
    
    threadgroup_change_begin()/end() is a pointless wrapper around
    cgroup_threadgroup_change_begin()/end(), minus a might_sleep()
    in the !CONFIG_CGROUPS=y case.
    
    Remove the wrappery, move the might_sleep() (the down_read()
    already has a might_sleep() check).
    
    This debloats <linux/sched.h> a bit and simplifies this API.
    
    Update all call sites.
    
    No change in functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 3c02404cfce9..6a3f850cabab 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -531,8 +531,8 @@ extern struct percpu_rw_semaphore cgroup_threadgroup_rwsem;
  * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
  * @tsk: target task
  *
- * Called from threadgroup_change_begin() and allows cgroup operations to
- * synchronize against threadgroup changes using a percpu_rw_semaphore.
+ * Allows cgroup operations to synchronize against threadgroup changes
+ * using a percpu_rw_semaphore.
  */
 static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
 {
@@ -543,8 +543,7 @@ static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
  * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
  * @tsk: target task
  *
- * Called from threadgroup_change_end().  Counterpart of
- * cgroup_threadcgroup_change_begin().
+ * Counterpart of cgroup_threadcgroup_change_begin().
  */
 static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
 {
@@ -555,7 +554,11 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
 
 #define CGROUP_SUBSYS_COUNT 0
 
-static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk) {}
+static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
+{
+	might_sleep();
+}
+
 static inline void cgroup_threadgroup_change_end(struct task_struct *tsk) {}
 
 #endif	/* CONFIG_CGROUPS */

commit 5f617ebbdf10abd49312a89e3b894b927c7367f5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 27 14:49:05 2016 -0500

    cgroup: reorder css_set fields
    
    Reorder css_set fields so that they're roughly in the order of how hot
    they are.  The rough order is
    
    1. the actual csses
    2. reference counter and the default cgroup pointer.
    3. task lists and iterations
    4. fields used during merge including css_set lookup
    5. the rest
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Acked-by: Zefan Li <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8a916dc96e84..3c02404cfce9 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -148,14 +148,18 @@ struct cgroup_subsys_state {
  * set for a task.
  */
 struct css_set {
-	/* Reference count */
-	atomic_t refcount;
-
 	/*
-	 * List running through all cgroup groups in the same hash
-	 * slot. Protected by css_set_lock
+	 * Set of subsystem states, one for each subsystem. This array is
+	 * immutable after creation apart from the init_css_set during
+	 * subsystem registration (at boot time).
 	 */
-	struct hlist_node hlist;
+	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
+
+	/* reference count */
+	atomic_t refcount;
+
+	/* the default cgroup associated with this css_set */
+	struct cgroup *dfl_cgrp;
 
 	/*
 	 * Lists running through all tasks using this cgroup group.
@@ -167,21 +171,29 @@ struct css_set {
 	struct list_head tasks;
 	struct list_head mg_tasks;
 
+	/* all css_task_iters currently walking this cset */
+	struct list_head task_iters;
+
 	/*
-	 * List of cgrp_cset_links pointing at cgroups referenced from this
-	 * css_set.  Protected by css_set_lock.
+	 * On the default hierarhcy, ->subsys[ssid] may point to a css
+	 * attached to an ancestor instead of the cgroup this css_set is
+	 * associated with.  The following node is anchored at
+	 * ->subsys[ssid]->cgroup->e_csets[ssid] and provides a way to
+	 * iterate through all css's attached to a given cgroup.
 	 */
-	struct list_head cgrp_links;
+	struct list_head e_cset_node[CGROUP_SUBSYS_COUNT];
 
-	/* the default cgroup associated with this css_set */
-	struct cgroup *dfl_cgrp;
+	/*
+	 * List running through all cgroup groups in the same hash
+	 * slot. Protected by css_set_lock
+	 */
+	struct hlist_node hlist;
 
 	/*
-	 * Set of subsystem states, one for each subsystem. This array is
-	 * immutable after creation apart from the init_css_set during
-	 * subsystem registration (at boot time).
+	 * List of cgrp_cset_links pointing at cgroups referenced from this
+	 * css_set.  Protected by css_set_lock.
 	 */
-	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
+	struct list_head cgrp_links;
 
 	/*
 	 * List of csets participating in the on-going migration either as
@@ -201,18 +213,6 @@ struct css_set {
 	struct cgroup *mg_dst_cgrp;
 	struct css_set *mg_dst_cset;
 
-	/*
-	 * On the default hierarhcy, ->subsys[ssid] may point to a css
-	 * attached to an ancestor instead of the cgroup this css_set is
-	 * associated with.  The following node is anchored at
-	 * ->subsys[ssid]->cgroup->e_csets[ssid] and provides a way to
-	 * iterate through all css's attached to a given cgroup.
-	 */
-	struct list_head e_cset_node[CGROUP_SUBSYS_COUNT];
-
-	/* all css_task_iters currently walking this cset */
-	struct list_head task_iters;
-
 	/* dead and being drained, ignore for migration */
 	bool dead;
 

commit e90cbebc3fa5caea4c8bfeb0d0157a0cee53efc7
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 27 14:49:03 2016 -0500

    cgroup add cftype->open/release() callbacks
    
    Pipe the newly added kernfs->open/release() callbacks through cftype.
    While at it, as cleanup operations now can be performed from
    ->release() instead of ->seq_stop(), make the latter optional.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Acked-by: Zefan Li <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 861b4677fc5b..8a916dc96e84 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -388,6 +388,9 @@ struct cftype {
 	struct list_head node;		/* anchored at ss->cfts */
 	struct kernfs_ops *kf_ops;
 
+	int (*open)(struct kernfs_open_file *of);
+	void (*release)(struct kernfs_open_file *of);
+
 	/*
 	 * read_u64() is a shortcut for the common case of returning a
 	 * single integer. Use it in place of read()

commit 3007098494bec614fb55dee7bc0410bb7db5ad18
Author: Daniel Mack <daniel@zonque.org>
Date:   Wed Nov 23 16:52:26 2016 +0100

    cgroup: add support for eBPF programs
    
    This patch adds two sets of eBPF program pointers to struct cgroup.
    One for such that are directly pinned to a cgroup, and one for such
    that are effective for it.
    
    To illustrate the logic behind that, assume the following example
    cgroup hierarchy.
    
      A - B - C
            \ D - E
    
    If only B has a program attached, it will be effective for B, C, D
    and E. If D then attaches a program itself, that will be effective for
    both D and E, and the program in B will only affect B and C. Only one
    program of a given type is effective for a cgroup.
    
    Attaching and detaching programs will be done through the bpf(2)
    syscall. For now, ingress and egress inet socket filtering are the
    only supported use-cases.
    
    Signed-off-by: Daniel Mack <daniel@zonque.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 5b17de62c962..861b4677fc5b 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -16,6 +16,7 @@
 #include <linux/percpu-refcount.h>
 #include <linux/percpu-rwsem.h>
 #include <linux/workqueue.h>
+#include <linux/bpf-cgroup.h>
 
 #ifdef CONFIG_CGROUPS
 
@@ -300,6 +301,9 @@ struct cgroup {
 	/* used to schedule release agent */
 	struct work_struct release_agent_work;
 
+	/* used to store eBPF programs */
+	struct cgroup_bpf bpf;
+
 	/* ids of the ancestors at each level including self */
 	int ancestor_ids[];
 };

commit 5cf1cacb49aee39c3e02ae87068fc3c6430659b0
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 21 19:06:48 2016 -0400

    cgroup, cpuset: replace cpuset_post_attach_flush() with cgroup_subsys->post_attach callback
    
    Since e93ad19d0564 ("cpuset: make mm migration asynchronous"), cpuset
    kicks off asynchronous NUMA node migration if necessary during task
    migration and flushes it from cpuset_post_attach_flush() which is
    called at the end of __cgroup_procs_write().  This is to avoid
    performing migration with cgroup_threadgroup_rwsem write-locked which
    can lead to deadlock through dependency on kworker creation.
    
    memcg has a similar issue with charge moving, so let's convert it to
    an official callback rather than the current one-off cpuset specific
    function.  This patch adds cgroup_subsys->post_attach callback and
    makes cpuset register cpuset_post_attach_flush() as its ->post_attach.
    
    The conversion is mostly one-to-one except that the new callback is
    called under cgroup_mutex.  This is to guarantee that no other
    migration operations are started before ->post_attach callbacks are
    finished.  cgroup_mutex is one of the outermost mutex in the system
    and has never been and shouldn't be a problem.  We can add specialized
    synchronization around __cgroup_procs_write() but I don't think
    there's any noticeable benefit.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: <stable@vger.kernel.org> # 4.4+ prerequisite for the next patch

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 3e39ae5bc799..5b17de62c962 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -444,6 +444,7 @@ struct cgroup_subsys {
 	int (*can_attach)(struct cgroup_taskset *tset);
 	void (*cancel_attach)(struct cgroup_taskset *tset);
 	void (*attach)(struct cgroup_taskset *tset);
+	void (*post_attach)(void);
 	int (*can_fork)(struct task_struct *task);
 	void (*cancel_fork)(struct task_struct *task);
 	void (*fork)(struct task_struct *task);

commit 2b021cbf3cb6208f0d40fd2f1869f237934340ed
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 15 20:43:04 2016 -0400

    cgroup: ignore css_sets associated with dead cgroups during migration
    
    Before 2e91fa7f6d45 ("cgroup: keep zombies associated with their
    original cgroups"), all dead tasks were associated with init_css_set.
    If a zombie task is requested for migration, while migration prep
    operations would still be performed on init_css_set, the actual
    migration would ignore zombie tasks.  As init_css_set is always valid,
    this worked fine.
    
    However, after 2e91fa7f6d45, zombie tasks stay with the css_set it was
    associated with at the time of death.  Let's say a task T associated
    with cgroup A on hierarchy H-1 and cgroup B on hiearchy H-2.  After T
    becomes a zombie, it would still remain associated with A and B.  If A
    only contains zombie tasks, it can be removed.  On removal, A gets
    marked offline but stays pinned until all zombies are drained.  At
    this point, if migration is initiated on T to a cgroup C on hierarchy
    H-2, migration path would try to prepare T's css_set for migration and
    trigger the following.
    
     WARNING: CPU: 0 PID: 1576 at kernel/cgroup.c:474 cgroup_get+0x121/0x160()
     CPU: 0 PID: 1576 Comm: bash Not tainted 4.4.0-work+ #289
     ...
     Call Trace:
      [<ffffffff8127e63c>] dump_stack+0x4e/0x82
      [<ffffffff810445e8>] warn_slowpath_common+0x78/0xb0
      [<ffffffff810446d5>] warn_slowpath_null+0x15/0x20
      [<ffffffff810c33e1>] cgroup_get+0x121/0x160
      [<ffffffff810c349b>] link_css_set+0x7b/0x90
      [<ffffffff810c4fbc>] find_css_set+0x3bc/0x5e0
      [<ffffffff810c5269>] cgroup_migrate_prepare_dst+0x89/0x1f0
      [<ffffffff810c7547>] cgroup_attach_task+0x157/0x230
      [<ffffffff810c7a17>] __cgroup_procs_write+0x2b7/0x470
      [<ffffffff810c7bdc>] cgroup_tasks_write+0xc/0x10
      [<ffffffff810c4790>] cgroup_file_write+0x30/0x1b0
      [<ffffffff811c68fc>] kernfs_fop_write+0x13c/0x180
      [<ffffffff81151673>] __vfs_write+0x23/0xe0
      [<ffffffff81152494>] vfs_write+0xa4/0x1a0
      [<ffffffff811532d4>] SyS_write+0x44/0xa0
      [<ffffffff814af2d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    
    It doesn't make sense to prepare migration for css_sets pointing to
    dead cgroups as they are guaranteed to contain only zombies which are
    ignored later during migration.  This patch makes cgroup destruction
    path mark all affected css_sets as dead and updates the migration path
    to ignore them during preparation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 2e91fa7f6d45 ("cgroup: keep zombies associated with their original cgroups")
    Cc: stable@vger.kernel.org # v4.4+

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 34b42f03fcd8..3e39ae5bc799 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -212,6 +212,9 @@ struct css_set {
 	/* all css_task_iters currently walking this cset */
 	struct list_head task_iters;
 
+	/* dead and being drained, ignore for migration */
+	bool dead;
+
 	/* For RCU-protected deletion */
 	struct rcu_head rcu_head;
 };

commit f6d635ad341d5cc0b9c7ab46adfbf3bf5886cee4
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 8 11:51:26 2016 -0500

    cgroup: implement cgroup_subsys->implicit_on_dfl
    
    Some controllers, perf_event for now and possibly freezer in the
    future, don't really make sense to control explicitly through
    "cgroup.subtree_control".  For example, the primary role of perf_event
    is identifying the cgroups of tasks; however, because the controller
    also keeps a small amount of state per cgroup, it can't be replaced
    with simple cgroup membership tests.
    
    This patch implements cgroup_subsys->implicit_on_dfl flag.  When set,
    the controller is implicitly enabled on all cgroups on the v2
    hierarchy so that utility type controllers such as perf_event can be
    enabled and function transparently.
    
    An implicit controller doesn't show up in "cgroup.controllers" or
    "cgroup.subtree_control", is exempt from no internal process rule and
    can be stolen from the default hierarchy even if there are non-root
    csses.
    
    v2: Reimplemented on top of the recent updates to css handling and
        subsystem rebinding.  Rebinding implicit subsystems is now a
        simple matter of exempting it from the busy subsystem check.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 590291d56049..34b42f03fcd8 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -450,6 +450,19 @@ struct cgroup_subsys {
 
 	bool early_init:1;
 
+	/*
+	 * If %true, the controller, on the default hierarchy, doesn't show
+	 * up in "cgroup.controllers" or "cgroup.subtree_control", is
+	 * implicitly enabled on all cgroups on the default hierarchy, and
+	 * bypasses the "no internal process" constraint.  This is for
+	 * utility type controllers which is transparent to userland.
+	 *
+	 * An implicit controller can be stolen from the default hierarchy
+	 * anytime and thus must be okay with offline csses from previous
+	 * hierarchies coexisting with csses for the current one.
+	 */
+	bool implicit_on_dfl:1;
+
 	/*
 	 * If %false, this subsystem is properly hierarchical -
 	 * configuration, resource accounting and restriction on a parent

commit e4857982f49d21c05a84351b56724bf353022355
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 8 11:51:26 2016 -0500

    cgroup: use css_set->mg_dst_cgrp for the migration target cgroup
    
    Migration can be multi-target on the default hierarchy when a
    controller is enabled - processes belonging to each child cgroup have
    to be moved to the child cgroup itself to refresh css association.
    
    This isn't a problem for cgroup_migrate_add_src() as each source
    css_set still maps to single source and target cgroups; however,
    cgroup_migrate_prepare_dst() is called once after all source css_sets
    are added and thus might not have a single destination cgroup.  This
    is currently worked around by specifying NULL for @dst_cgrp and using
    the source's default cgroup as destination as the only multi-target
    migration in use is self-targetting.  While this works, it's subtle
    and clunky.
    
    As all taget cgroups are already specified while preparing the source
    css_sets, this clunkiness can easily be removed by recording the
    target cgroup in each source css_set.  This patch adds
    css_set->mg_dst_cgrp which is recorded on cgroup_migrate_src() and
    used by cgroup_migrate_prepare_dst().  This also makes migration code
    ready for arbitrary multi-target migration.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index aae8c94de4b3..590291d56049 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -191,12 +191,13 @@ struct css_set {
 
 	/*
 	 * If this cset is acting as the source of migration the following
-	 * two fields are set.  mg_src_cgrp is the source cgroup of the
-	 * on-going migration and mg_dst_cset is the destination cset the
-	 * target tasks on this cset should be migrated to.  Protected by
-	 * cgroup_mutex.
+	 * two fields are set.  mg_src_cgrp and mg_dst_cgrp are
+	 * respectively the source and destination cgroups of the on-going
+	 * migration.  mg_dst_cset is the destination cset the target tasks
+	 * on this cset should be migrated to.  Protected by cgroup_mutex.
 	 */
 	struct cgroup *mg_src_cgrp;
+	struct cgroup *mg_dst_cgrp;
 	struct css_set *mg_dst_cset;
 
 	/*

commit 15a27c362d54378f17ec078579b2f6af88495a3f
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 3 09:57:59 2016 -0500

    cgroup: introduce cgroup_{save|propagate|restore}_control()
    
    While controllers are being enabled and disabled in
    cgroup_subtree_control_write(), the original subsystem masks are
    stashed in local variables so that they can be restored if the
    operation fails in the middle.
    
    This patch adds dedicated fields to struct cgroup to be used instead
    of the local variables and implements functions to stash the current
    values, propagate the changes and restore them recursively.  Combined
    with the previous changes, this makes subsystem management operations
    fully recursive and modularlized.  This will be used to expand cgroup
    core functionalities.
    
    While at it, remove now unused @css_enable and @css_disable from
    cgroup_subtree_control_write().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Zefan Li <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 7593c1a46786..aae8c94de4b3 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -260,6 +260,8 @@ struct cgroup {
 	 */
 	u16 subtree_control;
 	u16 subtree_ss_mask;
+	u16 old_subtree_control;
+	u16 old_subtree_ss_mask;
 
 	/* Private pointers for each registered subsystem */
 	struct cgroup_subsys_state __rcu *subsys[CGROUP_SUBSYS_COUNT];

commit 88cb04b96a1934ecbfd1d324e7cde55890c1a576
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 3 09:57:58 2016 -0500

    cgroup: explicitly track whether a cgroup_subsys_state is visible to userland
    
    Currently, whether a css (cgroup_subsys_state) has its interface files
    created is not tracked and assumed to change together with the owning
    cgroup's lifecycle.  cgroup directory and interface creation is being
    separated out from internal object creation to help refactoring and
    eventually allow cgroups which are not visible through cgroupfs.
    
    This patch adds CSS_VISIBLE to track whether a css has its interface
    files created and perform management operations only when necessary
    which helps decoupling interface file handling from internal object
    lifecycle.  After this patch, all css interface file management
    functions can be called regardless of the current state and will
    achieve the expected result.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Zefan Li <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8fc3f0452289..7593c1a46786 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -45,6 +45,7 @@ enum {
 	CSS_NO_REF	= (1 << 0), /* no reference counting for this css */
 	CSS_ONLINE	= (1 << 1), /* between ->css_online() and ->css_offline() */
 	CSS_RELEASED	= (1 << 2), /* refcnt reached zero, released */
+	CSS_VISIBLE	= (1 << 3), /* css is visible to userland */
 };
 
 /* bits in struct cgroup flags field */

commit b38e42e962dbc2fbc3839ce70750881db7c9277e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 23 10:00:50 2016 -0500

    cgroup: convert cgroup_subsys flag fields to bool bitfields
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 0abf6aa86c81..8fc3f0452289 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -444,7 +444,7 @@ struct cgroup_subsys {
 	void (*free)(struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
 
-	int early_init;
+	bool early_init:1;
 
 	/*
 	 * If %false, this subsystem is properly hierarchical -
@@ -458,8 +458,8 @@ struct cgroup_subsys {
 	 * cases.  Eventually, all subsystems will be made properly
 	 * hierarchical and this will go away.
 	 */
-	bool broken_hierarchy;
-	bool warned_broken_hierarchy;
+	bool broken_hierarchy:1;
+	bool warned_broken_hierarchy:1;
 
 	/* the following two fields are initialized automtically during boot */
 	int id;

commit 6e5c830770f9045a17b1b931c3e11fbd5591e630
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 22 22:25:47 2016 -0500

    cgroup: make cgroup subsystem masks u16
    
    After the recent do_each_subsys_mask() conversion, there's no reason
    to use ulong for subsystem masks.  We'll be adding more subsystem
    masks to persistent data structures, let's reduce its size to u16
    which should be enough for now and the foreseeable future.
    
    This doesn't create any noticeable behavior differences.
    
    v2: Johannes spotted that the initial patch missed cgroup_no_v1_mask.
        Converted.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c68ae7f0fb5f..0abf6aa86c81 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -257,8 +257,8 @@ struct cgroup {
 	 * one which may have more subsystems enabled.  Controller knobs
 	 * are made available iff it's enabled in ->subtree_control.
 	 */
-	unsigned long subtree_control;
-	unsigned long subtree_ss_mask;
+	u16 subtree_control;
+	u16 subtree_ss_mask;
 
 	/* Private pointers for each registered subsystem */
 	struct cgroup_subsys_state __rcu *subsys[CGROUP_SUBSYS_COUNT];

commit 8699b7762a623c46ced891b3cf490058b56cf99c
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 22 22:25:46 2016 -0500

    cgroup: s/child_subsys_mask/subtree_ss_mask/
    
    For consistency with cgroup->subtree_control.
    
    * cgroup->child_subsys_mask -> cgroup->subtree_ss_mask
    * cgroup_calc_child_subsys_mask() -> cgroup_calc_subtree_ss_mask()
    * cgroup_refresh_child_subsys_mask() -> cgroup_refresh_subtree_ss_mask()
    
    No functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 4f3c0dac26b5..c68ae7f0fb5f 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -253,13 +253,12 @@ struct cgroup {
 	/*
 	 * The bitmask of subsystems enabled on the child cgroups.
 	 * ->subtree_control is the one configured through
-	 * "cgroup.subtree_control" while ->child_subsys_mask is the
-	 * effective one which may have more subsystems enabled.
-	 * Controller knobs are made available iff it's enabled in
-	 * ->subtree_control.
+	 * "cgroup.subtree_control" while ->child_ss_mask is the effective
+	 * one which may have more subsystems enabled.  Controller knobs
+	 * are made available iff it's enabled in ->subtree_control.
 	 */
-	unsigned int subtree_control;
-	unsigned int child_subsys_mask;
+	unsigned long subtree_control;
+	unsigned long subtree_ss_mask;
 
 	/* Private pointers for each registered subsystem */
 	struct cgroup_subsys_state __rcu *subsys[CGROUP_SUBSYS_COUNT];

commit 5eb385cc5ae1b31fbcdd727854a00c5a083f6b9b
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Feb 22 22:25:46 2016 -0500

    Revert "cgroup: add cgroup_subsys->css_e_css_changed()"
    
    This reverts commit 56c807ba4e91f0980567b6a69de239677879b17f.
    
    cgroup_subsys->css_e_css_changed() was supposed to be used by cgroup
    writeback support; however, the change to per-inode cgroup association
    made it unnecessary and the callback doesn't have any user.  Remove
    it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 789471dba6fb..4f3c0dac26b5 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -434,7 +434,6 @@ struct cgroup_subsys {
 	void (*css_released)(struct cgroup_subsys_state *css);
 	void (*css_free)(struct cgroup_subsys_state *css);
 	void (*css_reset)(struct cgroup_subsys_state *css);
-	void (*css_e_css_changed)(struct cgroup_subsys_state *css);
 
 	int (*can_attach)(struct cgroup_taskset *tset);
 	void (*cancel_attach)(struct cgroup_taskset *tset);

commit aa226ff4a1ce79f229c6b7a4c0a14e17fececd01
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jan 21 15:31:11 2016 -0500

    cgroup: make sure a parent css isn't offlined before its children
    
    There are three subsystem callbacks in css shutdown path -
    css_offline(), css_released() and css_free().  Except for
    css_released(), cgroup core didn't guarantee the order of invocation.
    css_offline() or css_free() could be called on a parent css before its
    children.  This behavior is unexpected and led to bugs in cpu and
    memory controller.
    
    This patch updates offline path so that a parent css is never offlined
    before its children.  Each css keeps online_cnt which reaches zero iff
    itself and all its children are offline and offline_css() is invoked
    only after online_cnt reaches zero.
    
    This fixes the memory controller bug and allows the fix for cpu
    controller.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reported-by: Brian Christiansen <brian.o.christiansen@gmail.com>
    Link: http://lkml.kernel.org/g/5698A023.9070703@de.ibm.com
    Link: http://lkml.kernel.org/g/CAKB58ikDkzc8REt31WBkD99+hxNzjK4+FBmhkgS+NVrC9vjMSg@mail.gmail.com
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 7f540f7f588d..789471dba6fb 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -127,6 +127,12 @@ struct cgroup_subsys_state {
 	 */
 	u64 serial_nr;
 
+	/*
+	 * Incremented by online self and children.  Used to guarantee that
+	 * parents are not offlined before their children.
+	 */
+	atomic_t online_cnt;
+
 	/* percpu_ref killing and RCU release */
 	struct rcu_head rcu_head;
 	struct work_struct destroy_work;

commit 34a9304a96d6351c2d35dcdc9293258378fc0bd8
Merge: aee3bfa3307c 6255c46fa037
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 19:20:32 2016 -0800

    Merge branch 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - cgroup v2 interface is now official.  It's no longer hidden behind a
       devel flag and can be mounted using the new cgroup2 fs type.
    
       Unfortunately, cpu v2 interface hasn't made it yet due to the
       discussion around in-process hierarchical resource distribution and
       only memory and io controllers can be used on the v2 interface at the
       moment.
    
     - The existing documentation which has always been a bit of mess is
       relocated under Documentation/cgroup-v1/. Documentation/cgroup-v2.txt
       is added as the authoritative documentation for the v2 interface.
    
     - Some features are added through for-4.5-ancestor-test branch to
       enable netfilter xt_cgroup match to use cgroup v2 paths.  The actual
       netfilter changes will be merged through the net tree which pulled in
       the said branch.
    
     - Various cleanups
    
    * 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: rename cgroup documentations
      cgroup: fix a typo.
      cgroup: Remove resource_counter.txt in Documentation/cgroup-legacy/00-INDEX.
      cgroup: demote subsystem init messages to KERN_DEBUG
      cgroup: Fix uninitialized variable warning
      cgroup: put controller Kconfig options in meaningful order
      cgroup: clean up the kernel configuration menu nomenclature
      cgroup_pids: fix a typo.
      Subject: cgroup: Fix incomplete dd command in blkio documentation
      cgroup: kill cgrp_ss_priv[CGROUP_CANFORK_COUNT] and friends
      cpuset: Replace all instances of time_t with time64_t
      cgroup: replace unified-hierarchy.txt with a proper cgroup v2 documentation
      cgroup: rename Documentation/cgroups/ to Documentation/cgroup-legacy/
      cgroup: replace __DEVEL__sane_behavior with cgroup2 fs type

commit b3e0d3d7bab14f2544a3314bec53a23dc7dd2206
Merge: 3268e5cb494d 73796d8bf273
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 17 22:08:28 2015 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/geneve.c
    
    Here we had an overlapping change, where in 'net' the extraneous stats
    bump was being removed whilst in 'net-next' the final argument to
    udp_tunnel6_xmit_skb() was being changed.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ad2c8c73d29702c3193f739390f6661f9a4ecad9
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 9 12:30:46 2015 -0500

    cgroup: fix sock_cgroup_data initialization on earlier compilers
    
    sock_cgroup_data is a struct containing an anonymous union.
    sock_cgroup_set_prioidx() and sock_cgroup_set_classid() were
    initializing a field inside the anonymous union as follows.
    
     struct sock_ccgroup_data skcd_buf = { .val = VAL };
    
    While this is fine on more recent compilers, gcc-4.4.7 triggers the
    following errors.
    
     include/linux/cgroup-defs.h: In function ‘sock_cgroup_set_prioidx’:
     include/linux/cgroup-defs.h:619: error: unknown field ‘val’ specified in initializer
     include/linux/cgroup-defs.h:619: warning: missing braces around initializer
     include/linux/cgroup-defs.h:619: warning: (near initialization for ‘skcd_buf.<anonymous>’)
    
    This is because .val belongs to the anonymous union nested inside the
    struct but the initializer is missing the nesting.  Fix it by adding
    an extra pair of braces.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Alaa Hleihel <alaa@dev.mellanox.co.il>
    Fixes: bd1060a1d671 ("sock, cgroup: add sock->sk_cgroup")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 9dc226345e4e..097901a68671 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -616,7 +616,7 @@ static inline u32 sock_cgroup_classid(struct sock_cgroup_data *skcd)
 static inline void sock_cgroup_set_prioidx(struct sock_cgroup_data *skcd,
 					   u16 prioidx)
 {
-	struct sock_cgroup_data skcd_buf = { .val = READ_ONCE(skcd->val) };
+	struct sock_cgroup_data skcd_buf = {{ .val = READ_ONCE(skcd->val) }};
 
 	if (sock_cgroup_prioidx(&skcd_buf) == prioidx)
 		return;
@@ -633,7 +633,7 @@ static inline void sock_cgroup_set_prioidx(struct sock_cgroup_data *skcd,
 static inline void sock_cgroup_set_classid(struct sock_cgroup_data *skcd,
 					   u32 classid)
 {
-	struct sock_cgroup_data skcd_buf = { .val = READ_ONCE(skcd->val) };
+	struct sock_cgroup_data skcd_buf = {{ .val = READ_ONCE(skcd->val) }};
 
 	if (sock_cgroup_classid(&skcd_buf) == classid)
 		return;

commit bd1060a1d67128bb8fbe2e1384c518912cbe54e7
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 7 17:38:53 2015 -0500

    sock, cgroup: add sock->sk_cgroup
    
    In cgroup v1, dealing with cgroup membership was difficult because the
    number of membership associations was unbound.  As a result, cgroup v1
    grew several controllers whose primary purpose is either tagging
    membership or pull in configuration knobs from other subsystems so
    that cgroup membership test can be avoided.
    
    net_cls and net_prio controllers are examples of the latter.  They
    allow configuring network-specific attributes from cgroup side so that
    network subsystem can avoid testing cgroup membership; unfortunately,
    these are not only cumbersome but also problematic.
    
    Both net_cls and net_prio aren't properly hierarchical.  Both inherit
    configuration from the parent on creation but there's no interaction
    afterwards.  An ancestor doesn't restrict the behavior in its subtree
    in anyway and configuration changes aren't propagated downwards.
    Especially when combined with cgroup delegation, this is problematic
    because delegatees can mess up whatever network configuration
    implemented at the system level.  net_prio would allow the delegatees
    to set whatever priority value regardless of CAP_NET_ADMIN and net_cls
    the same for classid.
    
    While it is possible to solve these issues from controller side by
    implementing hierarchical allowable ranges in both controllers, it
    would involve quite a bit of complexity in the controllers and further
    obfuscate network configuration as it becomes even more difficult to
    tell what's actually being configured looking from the network side.
    While not much can be done for v1 at this point, as membership
    handling is sane on cgroup v2, it'd be better to make cgroup matching
    behave like other network matches and classifiers than introducing
    further complications.
    
    In preparation, this patch updates sock->sk_cgrp_data handling so that
    it points to the v2 cgroup that sock was created in until either
    net_prio or net_cls is used.  Once either of the two is used,
    sock->sk_cgrp_data reverts to its previous role of carrying prioidx
    and classid.  This is to avoid adding yet another cgroup related field
    to struct sock.
    
    As the mode switching can happen at most once per boot, the switching
    mechanism is aimed at lowering hot path overhead.  It may leak a
    finite, likely small, number of cgroup refs and report spurious
    prioidx or classid on switching; however, dynamic updates of prioidx
    and classid have always been racy and lossy - socks between creation
    and fd installation are never updated, config changes don't update
    existing sockets at all, and prioidx may index with dead and recycled
    cgroup IDs.  Non-critical inaccuracies from small race windows won't
    make any noticeable difference.
    
    This patch doesn't make use of the pointer yet.  The following patch
    will implement netfilter match for cgroup2 membership.
    
    v2: Use sock_cgroup_data to avoid inflating struct sock w/ another
        cgroup specific field.
    
    v3: Add comments explaining why sock_data_prioidx() and
        sock_data_classid() use different fallback values.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Daniel Wagner <daniel.wagner@bmw-carit.de>
    CC: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index ed128fed0335..9dc226345e4e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -544,31 +544,107 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk) {}
 
 #ifdef CONFIG_SOCK_CGROUP_DATA
 
+/*
+ * sock_cgroup_data is embedded at sock->sk_cgrp_data and contains
+ * per-socket cgroup information except for memcg association.
+ *
+ * On legacy hierarchies, net_prio and net_cls controllers directly set
+ * attributes on each sock which can then be tested by the network layer.
+ * On the default hierarchy, each sock is associated with the cgroup it was
+ * created in and the networking layer can match the cgroup directly.
+ *
+ * To avoid carrying all three cgroup related fields separately in sock,
+ * sock_cgroup_data overloads (prioidx, classid) and the cgroup pointer.
+ * On boot, sock_cgroup_data records the cgroup that the sock was created
+ * in so that cgroup2 matches can be made; however, once either net_prio or
+ * net_cls starts being used, the area is overriden to carry prioidx and/or
+ * classid.  The two modes are distinguished by whether the lowest bit is
+ * set.  Clear bit indicates cgroup pointer while set bit prioidx and
+ * classid.
+ *
+ * While userland may start using net_prio or net_cls at any time, once
+ * either is used, cgroup2 matching no longer works.  There is no reason to
+ * mix the two and this is in line with how legacy and v2 compatibility is
+ * handled.  On mode switch, cgroup references which are already being
+ * pointed to by socks may be leaked.  While this can be remedied by adding
+ * synchronization around sock_cgroup_data, given that the number of leaked
+ * cgroups is bound and highly unlikely to be high, this seems to be the
+ * better trade-off.
+ */
 struct sock_cgroup_data {
-	u16	prioidx;
-	u32	classid;
+	union {
+#ifdef __LITTLE_ENDIAN
+		struct {
+			u8	is_data;
+			u8	padding;
+			u16	prioidx;
+			u32	classid;
+		} __packed;
+#else
+		struct {
+			u32	classid;
+			u16	prioidx;
+			u8	padding;
+			u8	is_data;
+		} __packed;
+#endif
+		u64		val;
+	};
 };
 
+/*
+ * There's a theoretical window where the following accessors race with
+ * updaters and return part of the previous pointer as the prioidx or
+ * classid.  Such races are short-lived and the result isn't critical.
+ */
 static inline u16 sock_cgroup_prioidx(struct sock_cgroup_data *skcd)
 {
-	return skcd->prioidx;
+	/* fallback to 1 which is always the ID of the root cgroup */
+	return (skcd->is_data & 1) ? skcd->prioidx : 1;
 }
 
 static inline u32 sock_cgroup_classid(struct sock_cgroup_data *skcd)
 {
-	return skcd->classid;
+	/* fallback to 0 which is the unconfigured default classid */
+	return (skcd->is_data & 1) ? skcd->classid : 0;
 }
 
+/*
+ * If invoked concurrently, the updaters may clobber each other.  The
+ * caller is responsible for synchronization.
+ */
 static inline void sock_cgroup_set_prioidx(struct sock_cgroup_data *skcd,
 					   u16 prioidx)
 {
-	skcd->prioidx = prioidx;
+	struct sock_cgroup_data skcd_buf = { .val = READ_ONCE(skcd->val) };
+
+	if (sock_cgroup_prioidx(&skcd_buf) == prioidx)
+		return;
+
+	if (!(skcd_buf.is_data & 1)) {
+		skcd_buf.val = 0;
+		skcd_buf.is_data = 1;
+	}
+
+	skcd_buf.prioidx = prioidx;
+	WRITE_ONCE(skcd->val, skcd_buf.val);	/* see sock_cgroup_ptr() */
 }
 
 static inline void sock_cgroup_set_classid(struct sock_cgroup_data *skcd,
 					   u32 classid)
 {
-	skcd->classid = classid;
+	struct sock_cgroup_data skcd_buf = { .val = READ_ONCE(skcd->val) };
+
+	if (sock_cgroup_classid(&skcd_buf) == classid)
+		return;
+
+	if (!(skcd_buf.is_data & 1)) {
+		skcd_buf.val = 0;
+		skcd_buf.is_data = 1;
+	}
+
+	skcd_buf.classid = classid;
+	WRITE_ONCE(skcd->val, skcd_buf.val);	/* see sock_cgroup_ptr() */
 }
 
 #else	/* CONFIG_SOCK_CGROUP_DATA */

commit 2a56a1fec290bf0bc4676bbf4efdb3744953a3e7
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 7 17:38:52 2015 -0500

    net: wrap sock->sk_cgrp_prioidx and ->sk_classid inside a struct
    
    Introduce sock->sk_cgrp_data which is a struct sock_cgroup_data.
    ->sk_cgroup_prioidx and ->sk_classid are moved into it.  The struct
    and its accessors are defined in cgroup-defs.h.  This is to prepare
    for overloading the fields with a cgroup pointer.
    
    This patch mostly performs equivalent conversions but the followings
    are noteworthy.
    
    * Equality test before updating classid is removed from
      sock_update_classid().  This shouldn't make any noticeable
      difference and a similar test will be implemented on the helper side
      later.
    
    * sock_update_netprioidx() now takes struct sock_cgroup_data and can
      be moved to netprio_cgroup.h without causing include dependency
      loop.  Moved.
    
    * The dummy version of sock_update_netprioidx() converted to a static
      inline function while at it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 504d8591b6d3..ed128fed0335 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -542,4 +542,40 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk) {}
 
 #endif	/* CONFIG_CGROUPS */
 
+#ifdef CONFIG_SOCK_CGROUP_DATA
+
+struct sock_cgroup_data {
+	u16	prioidx;
+	u32	classid;
+};
+
+static inline u16 sock_cgroup_prioidx(struct sock_cgroup_data *skcd)
+{
+	return skcd->prioidx;
+}
+
+static inline u32 sock_cgroup_classid(struct sock_cgroup_data *skcd)
+{
+	return skcd->classid;
+}
+
+static inline void sock_cgroup_set_prioidx(struct sock_cgroup_data *skcd,
+					   u16 prioidx)
+{
+	skcd->prioidx = prioidx;
+}
+
+static inline void sock_cgroup_set_classid(struct sock_cgroup_data *skcd,
+					   u32 classid)
+{
+	skcd->classid = classid;
+}
+
+#else	/* CONFIG_SOCK_CGROUP_DATA */
+
+struct sock_cgroup_data {
+};
+
+#endif	/* CONFIG_SOCK_CGROUP_DATA */
+
 #endif	/* _LINUX_CGROUP_DEFS_H */

commit 177493987c1a15145922a65240f2f8ab6c63770a
Merge: e11362bb25d9 16af43964545
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 7 17:24:10 2015 -0500

    Merge branch 'for-4.5-ancestor-test' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup into for-4.5
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit b53202e6308939d33ba0c78712e850f891b4e76f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Dec 3 10:24:08 2015 -0500

    cgroup: kill cgrp_ss_priv[CGROUP_CANFORK_COUNT] and friends
    
    Now that nobody use the "priv" arg passed to can_fork/cancel_fork/fork we can
    kill CGROUP_CANFORK_COUNT/SUBSYS_TAG/etc and cgrp_ss_priv[] in copy_process().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 678cd5e4e881..8cfbc9dfd650 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -34,17 +34,12 @@ struct seq_file;
 
 /* define the enumeration of all cgroup subsystems */
 #define SUBSYS(_x) _x ## _cgrp_id,
-#define SUBSYS_TAG(_t) CGROUP_ ## _t, \
-	__unused_tag_ ## _t = CGROUP_ ## _t - 1,
 enum cgroup_subsys_id {
 #include <linux/cgroup_subsys.h>
 	CGROUP_SUBSYS_COUNT,
 };
-#undef SUBSYS_TAG
 #undef SUBSYS
 
-#define CGROUP_CANFORK_COUNT (CGROUP_CANFORK_END - CGROUP_CANFORK_START)
-
 /* bits in struct cgroup_subsys_state flags field */
 enum {
 	CSS_NO_REF	= (1 << 0), /* no reference counting for this css */
@@ -424,9 +419,9 @@ struct cgroup_subsys {
 	int (*can_attach)(struct cgroup_taskset *tset);
 	void (*cancel_attach)(struct cgroup_taskset *tset);
 	void (*attach)(struct cgroup_taskset *tset);
-	int (*can_fork)(struct task_struct *task, void **priv_p);
-	void (*cancel_fork)(struct task_struct *task, void *priv);
-	void (*fork)(struct task_struct *task, void *priv);
+	int (*can_fork)(struct task_struct *task);
+	void (*cancel_fork)(struct task_struct *task);
+	void (*fork)(struct task_struct *task);
 	void (*exit)(struct task_struct *task);
 	void (*free)(struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
@@ -512,7 +507,6 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
 
 #else	/* CONFIG_CGROUPS */
 
-#define CGROUP_CANFORK_COUNT 0
 #define CGROUP_SUBSYS_COUNT 0
 
 static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk) {}

commit 8075b542cf9f5d8a6afd92b4a940e29a677a7510
Merge: d2b436580906 67cde9c49389
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Dec 3 10:22:52 2015 -0500

    Merge branch 'for-4.4-fixes' into for-4.5

commit 1f7dd3e5a6e4f093017fff12232572ee1aa4639b
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Dec 3 10:18:21 2015 -0500

    cgroup: fix handling of multi-destination migration from subtree_control enabling
    
    Consider the following v2 hierarchy.
    
      P0 (+memory) --- P1 (-memory) --- A
                                     \- B
    
    P0 has memory enabled in its subtree_control while P1 doesn't.  If
    both A and B contain processes, they would belong to the memory css of
    P1.  Now if memory is enabled on P1's subtree_control, memory csses
    should be created on both A and B and A's processes should be moved to
    the former and B's processes the latter.  IOW, enabling controllers
    can cause atomic migrations into different csses.
    
    The core cgroup migration logic has been updated accordingly but the
    controller migration methods haven't and still assume that all tasks
    migrate to a single target css; furthermore, the methods were fed the
    css in which subtree_control was updated which is the parent of the
    target csses.  pids controller depends on the migration methods to
    move charges and this made the controller attribute charges to the
    wrong csses often triggering the following warning by driving a
    counter negative.
    
     WARNING: CPU: 1 PID: 1 at kernel/cgroup_pids.c:97 pids_cancel.constprop.6+0x31/0x40()
     Modules linked in:
     CPU: 1 PID: 1 Comm: systemd Not tainted 4.4.0-rc1+ #29
     ...
      ffffffff81f65382 ffff88007c043b90 ffffffff81551ffc 0000000000000000
      ffff88007c043bc8 ffffffff810de202 ffff88007a752000 ffff88007a29ab00
      ffff88007c043c80 ffff88007a1d8400 0000000000000001 ffff88007c043bd8
     Call Trace:
      [<ffffffff81551ffc>] dump_stack+0x4e/0x82
      [<ffffffff810de202>] warn_slowpath_common+0x82/0xc0
      [<ffffffff810de2fa>] warn_slowpath_null+0x1a/0x20
      [<ffffffff8118e031>] pids_cancel.constprop.6+0x31/0x40
      [<ffffffff8118e0fd>] pids_can_attach+0x6d/0xf0
      [<ffffffff81188a4c>] cgroup_taskset_migrate+0x6c/0x330
      [<ffffffff81188e05>] cgroup_migrate+0xf5/0x190
      [<ffffffff81189016>] cgroup_attach_task+0x176/0x200
      [<ffffffff8118949d>] __cgroup_procs_write+0x2ad/0x460
      [<ffffffff81189684>] cgroup_procs_write+0x14/0x20
      [<ffffffff811854e5>] cgroup_file_write+0x35/0x1c0
      [<ffffffff812e26f1>] kernfs_fop_write+0x141/0x190
      [<ffffffff81265f88>] __vfs_write+0x28/0xe0
      [<ffffffff812666fc>] vfs_write+0xac/0x1a0
      [<ffffffff81267019>] SyS_write+0x49/0xb0
      [<ffffffff81bcef32>] entry_SYSCALL_64_fastpath+0x12/0x76
    
    This patch fixes the bug by removing @css parameter from the three
    migration methods, ->can_attach, ->cancel_attach() and ->attach() and
    updating cgroup_taskset iteration helpers also return the destination
    css in addition to the task being migrated.  All controllers are
    updated accordingly.
    
    * Controllers which don't care whether there are one or multiple
      target csses can be converted trivially.  cpu, io, freezer, perf,
      netclassid and netprio fall in this category.
    
    * cpuset's current implementation assumes that there's single source
      and destination and thus doesn't support v2 hierarchy already.  The
      only change made by this patchset is how that single destination css
      is obtained.
    
    * memory migration path already doesn't do anything on v2.  How the
      single destination css is obtained is updated and the prep stage of
      mem_cgroup_can_attach() is reordered to accomodate the change.
    
    * pids is the only controller which was affected by this bug.  It now
      correctly handles multi-destination migrations and no longer causes
      counter underflow from incorrect accounting.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Cc: Aleksa Sarai <cyphar@cyphar.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 869fd4a3d28e..06b77f9dd3f2 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -422,12 +422,9 @@ struct cgroup_subsys {
 	void (*css_reset)(struct cgroup_subsys_state *css);
 	void (*css_e_css_changed)(struct cgroup_subsys_state *css);
 
-	int (*can_attach)(struct cgroup_subsys_state *css,
-			  struct cgroup_taskset *tset);
-	void (*cancel_attach)(struct cgroup_subsys_state *css,
-			      struct cgroup_taskset *tset);
-	void (*attach)(struct cgroup_subsys_state *css,
-		       struct cgroup_taskset *tset);
+	int (*can_attach)(struct cgroup_taskset *tset);
+	void (*cancel_attach)(struct cgroup_taskset *tset);
+	void (*attach)(struct cgroup_taskset *tset);
 	int (*can_fork)(struct task_struct *task, void **priv_p);
 	void (*cancel_fork)(struct task_struct *task, void *priv);
 	void (*fork)(struct task_struct *task, void *priv);

commit b11cfb5807e30333b36c02701382b820b7dcf0d5
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Nov 20 15:55:52 2015 -0500

    cgroup: record ancestor IDs and reimplement cgroup_is_descendant() using it
    
    cgroup_is_descendant() currently walks up the hierarchy and compares
    each ancestor to the cgroup in question.  While enough for cgroup core
    usages, this can't be used in hot paths to test cgroup membership.
    This patch adds cgroup->ancestor_ids[] which records the IDs of all
    ancestors including self and cgroup->level for the nesting level.
    
    This allows testing whether a given cgroup is a descendant of another
    in three finite steps - testing whether the two belong to the same
    hierarchy, whether the descendant candidate is at the same or a higher
    level than the ancestor and comparing the recorded ancestor_id at the
    matching level.  cgroup_is_descendant() is accordingly reimplmented
    and made inline.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 60d44b26276d..504d8591b6d3 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -234,6 +234,14 @@ struct cgroup {
 	 */
 	int id;
 
+	/*
+	 * The depth this cgroup is at.  The root is at depth zero and each
+	 * step down the hierarchy increments the level.  This along with
+	 * ancestor_ids[] can determine whether a given cgroup is a
+	 * descendant of another without traversing the hierarchy.
+	 */
+	int level;
+
 	/*
 	 * Each non-empty css_set associated with this cgroup contributes
 	 * one to populated_cnt.  All children with non-zero popuplated_cnt
@@ -289,6 +297,9 @@ struct cgroup {
 
 	/* used to schedule release agent */
 	struct work_struct release_agent_work;
+
+	/* ids of the ancestors at each level including self */
+	int ancestor_ids[];
 };
 
 /*
@@ -308,6 +319,9 @@ struct cgroup_root {
 	/* The root cgroup.  Root is destroyed on its release. */
 	struct cgroup cgrp;
 
+	/* for cgrp->ancestor_ids[0] */
+	int cgrp_ancestor_id_storage;
+
 	/* Number of cgroups in the hierarchy, used only for /proc/cgroups */
 	atomic_t nr_cgrps;
 

commit 67e9c74b8a873408c27ac9a8e4c1d1c8d72c93ff
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 16 11:13:34 2015 -0500

    cgroup: replace __DEVEL__sane_behavior with cgroup2 fs type
    
    With major controllers - cpu, memory and io - shaping up for the
    unified hierarchy, cgroup2 is about ready to be, gradually, released
    into the wild.  Replace __DEVEL__sane_behavior flag which was used to
    select the unified hierarchy with a separate filesystem type "cgroup2"
    so that unified hierarchy can be mounted as follows.
    
      mount -t cgroup2 none $MOUNT_POINT
    
    The cgroup2 fs has its own magic number - 0x63677270 ("cgrp").
    
    v2: Assign a different magic number to cgroup2 fs.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 869fd4a3d28e..80e2ae655208 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -66,7 +66,6 @@ enum {
 
 /* cgroup_root->flags */
 enum {
-	CGRP_ROOT_SANE_BEHAVIOR	= (1 << 0), /* __DEVEL__sane_behavior specified */
 	CGRP_ROOT_NOPREFIX	= (1 << 1), /* mounted subsystems have no named prefix */
 	CGRP_ROOT_XATTR		= (1 << 2), /* supports extended attributes */
 };

commit 34c06254ff82a815fdccdfae7517a06c9b768cee
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Nov 5 00:12:24 2015 -0500

    cgroup: fix cftype->file_offset handling
    
    6f60eade2433 ("cgroup: generalize obtaining the handles of and
    notifying cgroup files") introduced cftype->file_offset so that the
    handles for per-css file instances can be recorded.  These handles
    then can be used, for example, to generate file modified
    notifications.
    
    Unfortunately, it made the wrong assumption that files are created
    once for a given css and removed on its destruction.  Due to the
    dependencies among subsystems, a css may be hidden from userland and
    then later shown again.  This is implemented by removing and
    re-creating the affected files, so the associated kernfs_node for a
    given cgroup file may change over time.  This incorrect assumption led
    to the corruption of css->files lists.
    
    Reimplement cftype->file_offset handling so that cgroup_file->kn is
    protected by a lock and updated as files are created and destroyed.
    This also makes keeping them on per-cgroup list unnecessary.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: James Sedgwick <jsedgwick@fb.com>
    Fixes: 6f60eade2433 ("cgroup: generalize obtaining the handles of and notifying cgroup files")
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Zefan Li <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 60d44b26276d..869fd4a3d28e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -90,7 +90,6 @@ enum {
  */
 struct cgroup_file {
 	/* do not access any fields from outside cgroup core */
-	struct list_head node;			/* anchored at css->files */
 	struct kernfs_node *kn;
 };
 
@@ -134,9 +133,6 @@ struct cgroup_subsys_state {
 	 */
 	u64 serial_nr;
 
-	/* all cgroup_files associated with this css */
-	struct list_head files;
-
 	/* percpu_ref killing and RCU release */
 	struct rcu_head rcu_head;
 	struct work_struct destroy_work;

commit afcf6c8b75444382e0f9996157207ebae34a8848
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 15 16:41:53 2015 -0400

    cgroup: add cgroup_subsys->free() method and use it to fix pids controller
    
    pids controller is completely broken in that it uncharges when a task
    exits allowing zombies to escape resource control.  With the recent
    updates, cgroup core now maintains cgroup association till task free
    and pids controller can be fixed by uncharging on free instead of
    exit.
    
    This patch adds cgroup_subsys->free() method and update pids
    controller to use it instead of ->exit() for uncharging.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Aleksa Sarai <cyphar@cyphar.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 6a1ab64ee5f9..60d44b26276d 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -436,6 +436,7 @@ struct cgroup_subsys {
 	void (*cancel_fork)(struct task_struct *task, void *priv);
 	void (*fork)(struct task_struct *task, void *priv);
 	void (*exit)(struct task_struct *task);
+	void (*free)(struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
 
 	int early_init;

commit 2e91fa7f6d451e3ea9fec999065d2fd199691f9d
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 15 16:41:53 2015 -0400

    cgroup: keep zombies associated with their original cgroups
    
    cgroup_exit() is called when a task exits and disassociates the
    exiting task from its cgroups and half-attach it to the root cgroup.
    This is unnecessary and undesirable.
    
    No controller actually needs an exiting task to be disassociated with
    non-root cgroups.  Both cpu and perf_event controllers update the
    association to the root cgroup from their exit callbacks just to keep
    consistent with the cgroup core behavior.
    
    Also, this disassociation makes it difficult to track resources held
    by zombies or determine where the zombies came from.  Currently, pids
    controller is completely broken as it uncharges on exit and zombies
    always escape the resource restriction.  With cgroup association being
    reset on exit, fixing it is pretty painful.
    
    There's no reason to reset cgroup membership on exit.  The zombie can
    be removed from its css_set so that it doesn't show up on
    "cgroup.procs" and thus can't be migrated or interfere with cgroup
    removal.  It can still pin and point to the css_set so that its cgroup
    membership is maintained.  This patch makes cgroup core keep zombies
    associated with their cgroups at the time of exit.
    
    * Previous patches decoupled populated_cnt tracking from css_set
      lifetime, so a dying task can be simply unlinked from its css_set
      while pinning and pointing to the css_set.  This keeps css_set
      association from task side alive while hiding it from "cgroup.procs"
      and populated_cnt tracking.  The css_set reference is dropped when
      the task_struct is freed.
    
    * ->exit() callback no longer needs the css arguments as the
      associated css never changes once PF_EXITING is set.  Removed.
    
    * cpu and perf_events controllers no longer need ->exit() callbacks.
      There's no reason to explicitly switch away on exit.  The final
      schedule out is enough.  The callbacks are removed.
    
    * On traditional hierarchies, nothing changes.  "/proc/PID/cgroup"
      still reports "/" for all zombies.  On the default hierarchy,
      "/proc/PID/cgroup" keeps reporting the cgroup that the task belonged
      to at the time of exit.  If the cgroup gets removed before the task
      is reaped, " (deleted)" is appended.
    
    v2: Build brekage due to missing dummy cgroup_free() when
        !CONFIG_CGROUP fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 62413c3e2f4b..6a1ab64ee5f9 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -435,9 +435,7 @@ struct cgroup_subsys {
 	int (*can_fork)(struct task_struct *task, void **priv_p);
 	void (*cancel_fork)(struct task_struct *task, void *priv);
 	void (*fork)(struct task_struct *task, void *priv);
-	void (*exit)(struct cgroup_subsys_state *css,
-		     struct cgroup_subsys_state *old_css,
-		     struct task_struct *task);
+	void (*exit)(struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
 
 	int early_init;

commit ed27b9f7a17ddfbc007e16d4d11f33dff4fc2de7
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 15 16:41:52 2015 -0400

    cgroup: don't hold css_set_rwsem across css task iteration
    
    css_sets are synchronized through css_set_rwsem but the locking scheme
    is kinda bizarre.  The hot paths - fork and exit - have to write lock
    the rwsem making the rw part pointless; furthermore, many readers
    already hold cgroup_mutex.
    
    One of the readers is css task iteration.  It read locks the rwsem
    over the entire duration of iteration.  This leads to silly locking
    behavior.  When cpuset tries to migrate processes of a cgroup to a
    different NUMA node, css_set_rwsem is held across the entire migration
    attempt which can take a long time locking out forking, exiting and
    other cgroup operations.
    
    This patch updates css task iteration so that it locks css_set_rwsem
    only while the iterator is being advanced.  css task iteration
    involves two levels - css_set and task iteration.  As css_sets in use
    are practically immutable, simply pinning the current one is enough
    for resuming iteration afterwards.  Task iteration is tricky as tasks
    may leave their css_set while iteration is in progress.  This is
    solved by keeping track of active iterators and advancing them if
    their next task leaves its css_set.
    
    v2: put_task_struct() in css_task_iter_next() moved outside
        css_set_rwsem.  A later patch will add cgroup operations to
        task_struct free path which may grab the same lock and this avoids
        deadlock possibilities.
    
        css_set_move_task() updated to use list_for_each_entry_safe() when
        walking task_iters and advancing them.  This is necessary as
        advancing an iter may remove it from the list.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 17444505c870..62413c3e2f4b 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -211,6 +211,9 @@ struct css_set {
 	 */
 	struct list_head e_cset_node[CGROUP_SUBSYS_COUNT];
 
+	/* all css_task_iters currently walking this cset */
+	struct list_head task_iters;
+
 	/* For RCU-protected deletion */
 	struct rcu_head rcu_head;
 };

commit 0de0942db2b36dd91c088a7950398d2e87f23b23
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 15 16:41:49 2015 -0400

    cgroup: make cgroup->nr_populated count the number of populated css_sets
    
    Currently, cgroup->nr_populated counts whether the cgroup has any
    css_sets linked to it and the number of children which has non-zero
    ->nr_populated.  This works because a css_set's refcnt converges with
    the number of tasks linked to it and thus there's no css_set linked to
    a cgroup if it doesn't have any live tasks.
    
    To help tracking resource usage of zombie tasks, putting the ref of
    css_set will be separated from disassociating the task from the
    css_set which means that a cgroup may have css_sets linked to it even
    when it doesn't have any live tasks.
    
    This patch updates cgroup->nr_populated so that for the cgroup itself
    it counts the number of css_sets which have tasks associated with them
    so that empty css_sets don't skew the populated test.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index df589a097539..17444505c870 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -232,10 +232,10 @@ struct cgroup {
 	int id;
 
 	/*
-	 * If this cgroup contains any tasks, it contributes one to
-	 * populated_cnt.  All children with non-zero popuplated_cnt of
-	 * their own contribute one.  The count is zero iff there's no task
-	 * in this cgroup or its subtree.
+	 * Each non-empty css_set associated with this cgroup contributes
+	 * one to populated_cnt.  All children with non-zero popuplated_cnt
+	 * of their own contribute one.  The count is zero iff there's no
+	 * task in this cgroup or its subtree.
 	 */
 	int populated_cnt;
 

commit 6f60eade2433cb3a38687d5f8a4f44b92c6c51bf
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 18 17:54:23 2015 -0400

    cgroup: generalize obtaining the handles of and notifying cgroup files
    
    cgroup core handles creations and removals of cgroup interface files
    as described by cftypes.  There are cases where the handle for a given
    file instance is necessary, for example, to generate a file modified
    event.  Currently, this is handled by explicitly matching the callback
    method pointer and storing the file handle manually in
    cgroup_add_file().  While this simple approach works for cgroup core
    files, it can't for controller interface files.
    
    This patch generalizes cgroup interface file handle handling.  struct
    cgroup_file is defined and each cftype can optionally tell cgroup core
    to store the file handle by setting ->file_offset.  A file handle
    remains accessible as long as the containing css is accessible.
    
    Both "cgroup.procs" and "cgroup.events" are converted to use the new
    generic mechanism instead of hooking directly into cgroup_add_file().
    Also, cgroup_file_notify() which takes a struct cgroup_file and
    generates a file modified event on it is added and replaces explicit
    kernfs_notify() invocations.
    
    This generalizes cgroup file handle handling and allows controllers to
    generate file modified notifications.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 10d814bcd487..df589a097539 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -83,6 +83,17 @@ enum {
 	__CFTYPE_NOT_ON_DFL	= (1 << 17),	/* not on default hierarchy */
 };
 
+/*
+ * cgroup_file is the handle for a file instance created in a cgroup which
+ * is used, for example, to generate file changed notifications.  This can
+ * be obtained by setting cftype->file_offset.
+ */
+struct cgroup_file {
+	/* do not access any fields from outside cgroup core */
+	struct list_head node;			/* anchored at css->files */
+	struct kernfs_node *kn;
+};
+
 /*
  * Per-subsystem/per-cgroup state maintained by the system.  This is the
  * fundamental structural building block that controllers deal with.
@@ -123,6 +134,9 @@ struct cgroup_subsys_state {
 	 */
 	u64 serial_nr;
 
+	/* all cgroup_files associated with this css */
+	struct list_head files;
+
 	/* percpu_ref killing and RCU release */
 	struct rcu_head rcu_head;
 	struct work_struct destroy_work;
@@ -226,8 +240,8 @@ struct cgroup {
 	int populated_cnt;
 
 	struct kernfs_node *kn;		/* cgroup kernfs entry */
-	struct kernfs_node *procs_kn;	/* kn for "cgroup.procs" */
-	struct kernfs_node *events_kn;	/* kn for "cgroup.events" */
+	struct cgroup_file procs_file;	/* handle for "cgroup.procs" */
+	struct cgroup_file events_file;	/* handle for "cgroup.events" */
 
 	/*
 	 * The bitmask of subsystems enabled on the child cgroups.
@@ -335,6 +349,14 @@ struct cftype {
 	/* CFTYPE_* flags */
 	unsigned int flags;
 
+	/*
+	 * If non-zero, should contain the offset from the start of css to
+	 * a struct cgroup_file field.  cgroup will record the handle of
+	 * the created file into it.  The recorded handle can be used as
+	 * long as the containing css remains accessible.
+	 */
+	unsigned int file_offset;
+
 	/*
 	 * Fields used for internal bookkeeping.  Initialized automatically
 	 * during registration.

commit 7dbdb199d3bf88f043ea17e97113eb28d5b100bc
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 18 17:54:23 2015 -0400

    cgroup: replace cftype->mode with CFTYPE_WORLD_WRITABLE
    
    cftype->mode allows controllers to give arbitrary permissions to
    interface knobs.  Except for "cgroup.event_control", the existing uses
    are spurious.
    
    * Some explicitly specify S_IRUGO | S_IWUSR even though that's the
      default.
    
    * "cpuset.memory_pressure" specifies S_IRUGO while also setting a
      write callback which returns -EACCES.  All it needs to do is simply
      not setting a write callback.
    
    "cgroup.event_control" uses cftype->mode to make the file
    world-writable.  It's a misdesigned interface and we don't want
    controllers to be tweaking interface file permissions in general.
    This patch removes cftype->mode and all its spurious uses and
    implements CFTYPE_WORLD_WRITABLE for "cgroup.event_control" which is
    marked as compatibility-only.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index d95cc88e9dc2..10d814bcd487 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -76,6 +76,7 @@ enum {
 	CFTYPE_ONLY_ON_ROOT	= (1 << 0),	/* only create on root cgrp */
 	CFTYPE_NOT_ON_ROOT	= (1 << 1),	/* don't create on root cgrp */
 	CFTYPE_NO_PREFIX	= (1 << 3),	/* (DON'T USE FOR NEW FILES) no subsys prefix */
+	CFTYPE_WORLD_WRITABLE	= (1 << 4),	/* (DON'T USE FOR NEW FILES) S_IWUGO */
 
 	/* internal flags, do not use outside cgroup core proper */
 	__CFTYPE_ONLY_ON_DFL	= (1 << 16),	/* only on default hierarchy */
@@ -324,11 +325,6 @@ struct cftype {
 	 */
 	char name[MAX_CFTYPE_NAME];
 	unsigned long private;
-	/*
-	 * If not 0, file mode is set to this value, otherwise it will
-	 * be figured out automatically
-	 */
-	umode_t mode;
 
 	/*
 	 * The maximum length of string, excluding trailing nul, that can

commit 4a07c222d3afb00e1113834fee38d23a8e5d71dc
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 18 17:54:22 2015 -0400

    cgroup: replace "cgroup.populated" with "cgroup.events"
    
    memcg already uses "memory.events" for event reporting and other
    controllers may need event reporting too.  Let's standardize on
    "$SUBSYS.events" interface file for reporting events which don't
    happen too frequently and thus can share event notification.
    
    "cgroup.populated" is replaced with "populated" field in
    "cgroup.events" and documentation is updated accordingly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c5d41c3c1f00..d95cc88e9dc2 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -226,7 +226,7 @@ struct cgroup {
 
 	struct kernfs_node *kn;		/* cgroup kernfs entry */
 	struct kernfs_node *procs_kn;	/* kn for "cgroup.procs" */
-	struct kernfs_node *populated_kn; /* kn for "cgroup.subtree_populated" */
+	struct kernfs_node *events_kn;	/* kn for "cgroup.events" */
 
 	/*
 	 * The bitmask of subsystems enabled on the child cgroups.

commit fc5ed1e95410ad73b2ab8f33cd90eb3bcf6c98a1
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 18 11:56:28 2015 -0400

    cgroup: replace cgroup_subsys->disabled tests with cgroup_subsys_enabled()
    
    Replace cgroup_subsys->disabled tests in controllers with
    cgroup_subsys_enabled().  cgroup_subsys_enabled() requires literal
    subsys name as its parameter and thus can't be used for cgroup core
    which iterates through controllers.  For cgroup core, introduce and
    use cgroup_ssid_enabled() which uses slower static_key_enabled() test
    and can be indexed by subsys ID.
    
    This leaves cgroup_subsys->disabled unused.  Removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 4d8fcf2187dc..c5d41c3c1f00 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -419,7 +419,6 @@ struct cgroup_subsys {
 		     struct task_struct *task);
 	void (*bind)(struct cgroup_subsys_state *root_css);
 
-	int disabled;
 	int early_init;
 
 	/*

commit 1ed1328792ff46e4bb86a3d7f7be2971f4549f6c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 12:53:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    Note: This commit was originally committed as d59cfc09c32a but got
          reverted by 0c986253b939 due to the performance regression from
          the percpu_rwsem write down/up operations added to cgroup task
          migration path.  percpu_rwsem changes which alleviate the
          performance issue are pending for v4.4-rc1 merge window.
          Re-apply.
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8492721b39be..4d8fcf2187dc 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -473,8 +473,31 @@ struct cgroup_subsys {
 	unsigned int depends_on;
 };
 
-void cgroup_threadgroup_change_begin(struct task_struct *tsk);
-void cgroup_threadgroup_change_end(struct task_struct *tsk);
+extern struct percpu_rw_semaphore cgroup_threadgroup_rwsem;
+
+/**
+ * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
+ * @tsk: target task
+ *
+ * Called from threadgroup_change_begin() and allows cgroup operations to
+ * synchronize against threadgroup changes using a percpu_rw_semaphore.
+ */
+static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
+{
+	percpu_down_read(&cgroup_threadgroup_rwsem);
+}
+
+/**
+ * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
+ * @tsk: target task
+ *
+ * Called from threadgroup_change_end().  Counterpart of
+ * cgroup_threadcgroup_change_begin().
+ */
+static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
+{
+	percpu_up_read(&cgroup_threadgroup_rwsem);
+}
 
 #else	/* CONFIG_CGROUPS */
 

commit 0c986253b939cc14c69d4adbe2b4121bdf4aa220
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 11:51:12 2015 -0400

    Revert "sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem"
    
    This reverts commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14.
    
    d59cfc09c32a ("sched, cgroup: replace signal_struct->group_rwsem with
    a global percpu_rwsem") and b5ba75b5fc0e ("cgroup: simplify
    threadgroup locking") changed how cgroup synchronizes against task
    fork and exits so that it uses global percpu_rwsem instead of
    per-process rwsem; unfortunately, the write [un]lock paths of
    percpu_rwsem always involve synchronize_rcu_expedited() which turned
    out to be too expensive.
    
    Improvements for percpu_rwsem are scheduled to be merged in the coming
    v4.4-rc1 merge window which alleviates this issue.  For now, revert
    the two commits to restore per-process rwsem.  They will be re-applied
    for the v4.4-rc1 merge window.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: stable@vger.kernel.org # v4.2+

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 4d8fcf2187dc..8492721b39be 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -473,31 +473,8 @@ struct cgroup_subsys {
 	unsigned int depends_on;
 };
 
-extern struct percpu_rw_semaphore cgroup_threadgroup_rwsem;
-
-/**
- * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
- * @tsk: target task
- *
- * Called from threadgroup_change_begin() and allows cgroup operations to
- * synchronize against threadgroup changes using a percpu_rw_semaphore.
- */
-static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
-{
-	percpu_down_read(&cgroup_threadgroup_rwsem);
-}
-
-/**
- * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
- * @tsk: target task
- *
- * Called from threadgroup_change_end().  Counterpart of
- * cgroup_threadcgroup_change_begin().
- */
-static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
-{
-	percpu_up_read(&cgroup_threadgroup_rwsem);
-}
+void cgroup_threadgroup_change_begin(struct task_struct *tsk);
+void cgroup_threadgroup_change_end(struct task_struct *tsk);
 
 #else	/* CONFIG_CGROUPS */
 

commit 20f1f4b5ffb870631bf4a4e7c7ba10e3528ae6a6
Merge: ce52399520e4 3e1d2eed39d8
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 25 14:19:29 2015 -0400

    Merge branch 'for-4.3-unified-base' into for-4.3

commit 3e1d2eed39d804e48282931835c7203fa47fe1d9
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 18 13:58:16 2015 -0700

    cgroup: introduce cgroup_subsys->legacy_name
    
    This allows cgroup subsystems to use a different name on the unified
    hierarchy.  cgroup_subsys->name is used on the unified hierarchy,
    ->legacy_name elsewhere.  If ->legacy_name is not explicitly set, it's
    automatically set to ->name and the userland visible behavior remains
    unchanged.
    
    v2: Make parse_cgroupfs_options() only consider ->legacy_name as mount
        options are used only on legacy hierarchies.  Suggested by Li
        Zefan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: cgroups@vger.kernel.org

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 8f5770a7d85a..7d0bb53e4553 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -434,6 +434,9 @@ struct cgroup_subsys {
 	int id;
 	const char *name;
 
+	/* optional, initialized automatically during boot if not set */
+	const char *legacy_name;
+
 	/* link to parent, protected by cgroup_lock() */
 	struct cgroup_root *root;
 

commit 731a981e1059dd68c97212ccc9c0e1f169c1a77b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 11 13:35:42 2015 -0400

    cgroup: make cftype->private a unsigned long
    
    It's pretty unusual to have an int as a private data field and it
    makes it impossible to carray a pointer value through it.  Let's make
    it an unsigned long.  AFAICS, this shouldn't break anything.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 93755a629299..8f5770a7d85a 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -318,7 +318,7 @@ struct cftype {
 	 * end of cftype array.
 	 */
 	char name[MAX_CFTYPE_NAME];
-	int private;
+	unsigned long private;
 	/*
 	 * If not 0, file mode is set to this value, otherwise it will
 	 * be figured out automatically

commit 7e47682ea555e7c1edef1d8fd96e2aa4c12abe59
Author: Aleksa Sarai <cyphar@cyphar.com>
Date:   Tue Jun 9 21:32:09 2015 +1000

    cgroup: allow a cgroup subsystem to reject a fork
    
    Add a new cgroup subsystem callback can_fork that conditionally
    states whether or not the fork is accepted or rejected by a cgroup
    policy. In addition, add a cancel_fork callback so that if an error
    occurs later in the forking process, any state modified by can_fork can
    be reverted.
    
    Allow for a private opaque pointer to be passed from cgroup_can_fork to
    cgroup_post_fork, allowing for the fork state to be stored by each
    subsystem separately.
    
    Also add a tagging system for cgroup_subsys.h to allow for CGROUP_<TAG>
    enumerations to be be defined and used. In addition, explicitly add a
    CGROUP_CANFORK_COUNT macro to make arrays easier to define.
    
    This is in preparation for implementing the pids cgroup subsystem.
    
    Signed-off-by: Aleksa Sarai <cyphar@cyphar.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 93755a629299..83e37d8c4d80 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -34,12 +34,17 @@ struct seq_file;
 
 /* define the enumeration of all cgroup subsystems */
 #define SUBSYS(_x) _x ## _cgrp_id,
+#define SUBSYS_TAG(_t) CGROUP_ ## _t, \
+	__unused_tag_ ## _t = CGROUP_ ## _t - 1,
 enum cgroup_subsys_id {
 #include <linux/cgroup_subsys.h>
 	CGROUP_SUBSYS_COUNT,
 };
+#undef SUBSYS_TAG
 #undef SUBSYS
 
+#define CGROUP_CANFORK_COUNT (CGROUP_CANFORK_END - CGROUP_CANFORK_START)
+
 /* bits in struct cgroup_subsys_state flags field */
 enum {
 	CSS_NO_REF	= (1 << 0), /* no reference counting for this css */
@@ -406,7 +411,9 @@ struct cgroup_subsys {
 			      struct cgroup_taskset *tset);
 	void (*attach)(struct cgroup_subsys_state *css,
 		       struct cgroup_taskset *tset);
-	void (*fork)(struct task_struct *task);
+	int (*can_fork)(struct task_struct *task, void **priv_p);
+	void (*cancel_fork)(struct task_struct *task, void *priv);
+	void (*fork)(struct task_struct *task, void *priv);
 	void (*exit)(struct cgroup_subsys_state *css,
 		     struct cgroup_subsys_state *old_css,
 		     struct task_struct *task);
@@ -491,6 +498,7 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
 
 #else	/* CONFIG_CGROUPS */
 
+#define CGROUP_CANFORK_COUNT 0
 #define CGROUP_SUBSYS_COUNT 0
 
 static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk) {}

commit 187fe84067bd377047cfcb7f2bbc7c9dc12d290c
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 18 16:54:28 2015 -0400

    cgroup: require write perm on common ancestor when moving processes on the default hierarchy
    
    On traditional hierarchies, if a task has write access to "tasks" or
    "cgroup.procs" file of a cgroup and its euid agrees with the target,
    it can move the target to the cgroup; however, consider the following
    scenario.  The owner of each cgroup is in the parentheses.
    
     R (root) - 0 (root) - 00 (user1) - 000 (user1)
              |                       \ 001 (user1)
              \ 1 (root) - 10 (user1)
    
    The subtrees of 00 and 10 are delegated to user1; however, while both
    subtrees may belong to the same user, it is clear that the two
    subtrees are to be isolated - they're under completely separate
    resource limits imposed by 0 and 1, respectively.  Note that 0 and 1
    aren't strictly necessary but added to ease illustrating the issue.
    
    If user1 is allowed to move processes between the two subtrees, the
    intention of the hierarchy - keeping a given group of processes under
    a subtree with certain resource restrictions while delegating
    management of the subtree - can be circumvented by user1.
    
    This happens because migration permission check doesn't consider the
    hierarchical nature of cgroups.  To fix the issue, this patch adds an
    extra permission requirement when userland tries to migrate a process
    in the default hierarchy - the issuing task must have write access to
    the common ancestor of "cgroup.procs" file of the ancestor in addition
    to the destination's.
    
    Conceptually, the issuer must be able to move the target process from
    the source cgroup to the common ancestor of source and destination
    cgroups and then to the destination.  As long as delegation is done in
    a proper top-down way, this guarantees that a delegatee can't smuggle
    processes across disjoint delegation domains.
    
    The next patch will add documentation on the delegation model on the
    default hierarchy.
    
    v2: Fixed missing !ret test.  Spotted by Li Zefan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Li Zefan <lizefan@huawei.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index c5588c438448..93755a629299 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -220,6 +220,7 @@ struct cgroup {
 	int populated_cnt;
 
 	struct kernfs_node *kn;		/* cgroup kernfs entry */
+	struct kernfs_node *procs_kn;	/* kn for "cgroup.procs" */
 	struct kernfs_node *populated_kn; /* kn for "cgroup.subtree_populated" */
 
 	/*

commit cb4a316752709be4a644f070440a8be470d92b7d
Author: Aleksa Sarai <cyphar@cyphar.com>
Date:   Sat Jun 6 10:02:14 2015 +1000

    cgroup: use bitmask to filter for_each_subsys
    
    Add a new macro for_each_subsys_which that allows all enabled cgroup
    subsystems to be filtered by a bitmask, such that mask & (1 << ssid)
    determines if the subsystem is to be processed in the loop body (where
    ssid is the unique id of the subsystem).
    
    Also replace the need_forkexit_callback with two separate bitmasks for
    each callback to make (ss->{fork,exit}) checks unnecessary.
    
    tj: add a short comment for "if (!CGROUP_SUBSYS_COUNT)".
    
    Signed-off-by: Aleksa Sarai <cyphar@cyphar.com>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 26d1cea7929f..c5588c438448 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -490,6 +490,8 @@ static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
 
 #else	/* CONFIG_CGROUPS */
 
+#define CGROUP_SUBSYS_COUNT 0
+
 static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk) {}
 static inline void cgroup_threadgroup_change_end(struct task_struct *tsk) {}
 

commit c80ef9e0c021ff86771fdd72583c75d8f7b6a720
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri May 29 10:52:59 2015 +0200

    cgroup: add seq_file forward declaration for struct cftype
    
    Recent header file changes for cgroup caused lots of warnings
    about a missing struct seq_file form declaration for every
    inclusion of include/linux/cgroup-defs.h.
    
    As some files are built with -Werror, this leads to build
    failure like:
    
                     from /git/arm-soc/drivers/gpu/drm/tilcdc/tilcdc_crtc.c:18:
    /git/arm-soc/include/linux/cgroup-defs.h:354:25: error: 'struct seq_file' declared inside parameter list [-Werror]
    cc1: all warnings being treated as errors
    make[6]: *** [drivers/gpu/drm/tilcdc/tilcdc_crtc.o] Error 1
    
    This patch adds the declaration, which resolves both the
    warnings and the drm failure.
    
    tj: Moved it where other type declarations are.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Fixes: b4a04ab7a37b ("cgroup: separate out include/linux/cgroup-defs.h")
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 7d83d7f73420..26d1cea7929f 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -26,6 +26,7 @@ struct cgroup_taskset;
 struct kernfs_node;
 struct kernfs_ops;
 struct kernfs_open_file;
+struct seq_file;
 
 #define MAX_CGROUP_TYPE_NAMELEN 32
 #define MAX_CGROUP_ROOT_NAMELEN 64

commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 1b8c93806dbd..7d83d7f73420 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -461,8 +461,31 @@ struct cgroup_subsys {
 	unsigned int depends_on;
 };
 
-void cgroup_threadgroup_change_begin(struct task_struct *tsk);
-void cgroup_threadgroup_change_end(struct task_struct *tsk);
+extern struct percpu_rw_semaphore cgroup_threadgroup_rwsem;
+
+/**
+ * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
+ * @tsk: target task
+ *
+ * Called from threadgroup_change_begin() and allows cgroup operations to
+ * synchronize against threadgroup changes using a percpu_rw_semaphore.
+ */
+static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk)
+{
+	percpu_down_read(&cgroup_threadgroup_rwsem);
+}
+
+/**
+ * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
+ * @tsk: target task
+ *
+ * Called from threadgroup_change_end().  Counterpart of
+ * cgroup_threadcgroup_change_begin().
+ */
+static inline void cgroup_threadgroup_change_end(struct task_struct *tsk)
+{
+	percpu_up_read(&cgroup_threadgroup_rwsem);
+}
 
 #else	/* CONFIG_CGROUPS */
 

commit 7d7efec368d537226142cbe559f45797f18672f9
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:16 2015 -0400

    sched, cgroup: reorganize threadgroup locking
    
    threadgroup_change_begin/end() are used to mark the beginning and end
    of threadgroup modifying operations to allow code paths which require
    a threadgroup to stay stable across blocking operations to synchronize
    against those sections using threadgroup_lock/unlock().
    
    It's currently implemented as a general mechanism in sched.h using
    per-signal_struct rwsem; however, this never grew non-cgroup use cases
    and becomes noop if !CONFIG_CGROUPS.  It turns out that cgroups is
    gonna be better served with a different sycnrhonization scheme and is
    a bit silly to keep cgroups specific details as a general mechanism.
    
    What's general here is identifying the places where threadgroups are
    modified.  This patch restructures threadgroup locking so that
    threadgroup_change_begin/end() become a place where subsystems which
    need to sycnhronize against threadgroup changes can hook into.
    
    cgroup_threadgroup_change_begin/end() which operate on the
    per-signal_struct rwsem are created and threadgroup_lock/unlock() are
    moved to cgroup.c and made static.
    
    This is pure reorganization which doesn't cause any functional
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index 55f3120fb952..1b8c93806dbd 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -14,6 +14,7 @@
 #include <linux/mutex.h>
 #include <linux/rcupdate.h>
 #include <linux/percpu-refcount.h>
+#include <linux/percpu-rwsem.h>
 #include <linux/workqueue.h>
 
 #ifdef CONFIG_CGROUPS
@@ -460,5 +461,14 @@ struct cgroup_subsys {
 	unsigned int depends_on;
 };
 
+void cgroup_threadgroup_change_begin(struct task_struct *tsk);
+void cgroup_threadgroup_change_end(struct task_struct *tsk);
+
+#else	/* CONFIG_CGROUPS */
+
+static inline void cgroup_threadgroup_change_begin(struct task_struct *tsk) {}
+static inline void cgroup_threadgroup_change_end(struct task_struct *tsk) {}
+
 #endif	/* CONFIG_CGROUPS */
+
 #endif	/* _LINUX_CGROUP_DEFS_H */

commit b4a04ab7a37b490cad48e69abfe14288cacb669c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 15:38:40 2015 -0400

    cgroup: separate out include/linux/cgroup-defs.h
    
    From 2d728f74bfc071df06773e2fd7577dd5dab6425d Mon Sep 17 00:00:00 2001
    From: Tejun Heo <tj@kernel.org>
    Date: Wed, 13 May 2015 15:37:01 -0400
    
    This patch separates out cgroup-defs.h from cgroup.h which has grown a
    lot of dependencies.  cgroup-defs.h currently only contains constant
    and type definitions and can be used to break circular include
    dependency.  While moving, definitions are reordered so that
    cgroup-defs.h has consistent logical structure.
    
    This patch is pure reorganization.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
new file mode 100644
index 000000000000..55f3120fb952
--- /dev/null
+++ b/include/linux/cgroup-defs.h
@@ -0,0 +1,464 @@
+/*
+ * linux/cgroup-defs.h - basic definitions for cgroup
+ *
+ * This file provides basic type and interface.  Include this file directly
+ * only if necessary to avoid cyclic dependencies.
+ */
+#ifndef _LINUX_CGROUP_DEFS_H
+#define _LINUX_CGROUP_DEFS_H
+
+#include <linux/limits.h>
+#include <linux/list.h>
+#include <linux/idr.h>
+#include <linux/wait.h>
+#include <linux/mutex.h>
+#include <linux/rcupdate.h>
+#include <linux/percpu-refcount.h>
+#include <linux/workqueue.h>
+
+#ifdef CONFIG_CGROUPS
+
+struct cgroup;
+struct cgroup_root;
+struct cgroup_subsys;
+struct cgroup_taskset;
+struct kernfs_node;
+struct kernfs_ops;
+struct kernfs_open_file;
+
+#define MAX_CGROUP_TYPE_NAMELEN 32
+#define MAX_CGROUP_ROOT_NAMELEN 64
+#define MAX_CFTYPE_NAME		64
+
+/* define the enumeration of all cgroup subsystems */
+#define SUBSYS(_x) _x ## _cgrp_id,
+enum cgroup_subsys_id {
+#include <linux/cgroup_subsys.h>
+	CGROUP_SUBSYS_COUNT,
+};
+#undef SUBSYS
+
+/* bits in struct cgroup_subsys_state flags field */
+enum {
+	CSS_NO_REF	= (1 << 0), /* no reference counting for this css */
+	CSS_ONLINE	= (1 << 1), /* between ->css_online() and ->css_offline() */
+	CSS_RELEASED	= (1 << 2), /* refcnt reached zero, released */
+};
+
+/* bits in struct cgroup flags field */
+enum {
+	/* Control Group requires release notifications to userspace */
+	CGRP_NOTIFY_ON_RELEASE,
+	/*
+	 * Clone the parent's configuration when creating a new child
+	 * cpuset cgroup.  For historical reasons, this option can be
+	 * specified at mount time and thus is implemented here.
+	 */
+	CGRP_CPUSET_CLONE_CHILDREN,
+};
+
+/* cgroup_root->flags */
+enum {
+	CGRP_ROOT_SANE_BEHAVIOR	= (1 << 0), /* __DEVEL__sane_behavior specified */
+	CGRP_ROOT_NOPREFIX	= (1 << 1), /* mounted subsystems have no named prefix */
+	CGRP_ROOT_XATTR		= (1 << 2), /* supports extended attributes */
+};
+
+/* cftype->flags */
+enum {
+	CFTYPE_ONLY_ON_ROOT	= (1 << 0),	/* only create on root cgrp */
+	CFTYPE_NOT_ON_ROOT	= (1 << 1),	/* don't create on root cgrp */
+	CFTYPE_NO_PREFIX	= (1 << 3),	/* (DON'T USE FOR NEW FILES) no subsys prefix */
+
+	/* internal flags, do not use outside cgroup core proper */
+	__CFTYPE_ONLY_ON_DFL	= (1 << 16),	/* only on default hierarchy */
+	__CFTYPE_NOT_ON_DFL	= (1 << 17),	/* not on default hierarchy */
+};
+
+/*
+ * Per-subsystem/per-cgroup state maintained by the system.  This is the
+ * fundamental structural building block that controllers deal with.
+ *
+ * Fields marked with "PI:" are public and immutable and may be accessed
+ * directly without synchronization.
+ */
+struct cgroup_subsys_state {
+	/* PI: the cgroup that this css is attached to */
+	struct cgroup *cgroup;
+
+	/* PI: the cgroup subsystem that this css is attached to */
+	struct cgroup_subsys *ss;
+
+	/* reference count - access via css_[try]get() and css_put() */
+	struct percpu_ref refcnt;
+
+	/* PI: the parent css */
+	struct cgroup_subsys_state *parent;
+
+	/* siblings list anchored at the parent's ->children */
+	struct list_head sibling;
+	struct list_head children;
+
+	/*
+	 * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
+	 * matching css can be looked up using css_from_id().
+	 */
+	int id;
+
+	unsigned int flags;
+
+	/*
+	 * Monotonically increasing unique serial number which defines a
+	 * uniform order among all csses.  It's guaranteed that all
+	 * ->children lists are in the ascending order of ->serial_nr and
+	 * used to allow interrupting and resuming iterations.
+	 */
+	u64 serial_nr;
+
+	/* percpu_ref killing and RCU release */
+	struct rcu_head rcu_head;
+	struct work_struct destroy_work;
+};
+
+/*
+ * A css_set is a structure holding pointers to a set of
+ * cgroup_subsys_state objects. This saves space in the task struct
+ * object and speeds up fork()/exit(), since a single inc/dec and a
+ * list_add()/del() can bump the reference count on the entire cgroup
+ * set for a task.
+ */
+struct css_set {
+	/* Reference count */
+	atomic_t refcount;
+
+	/*
+	 * List running through all cgroup groups in the same hash
+	 * slot. Protected by css_set_lock
+	 */
+	struct hlist_node hlist;
+
+	/*
+	 * Lists running through all tasks using this cgroup group.
+	 * mg_tasks lists tasks which belong to this cset but are in the
+	 * process of being migrated out or in.  Protected by
+	 * css_set_rwsem, but, during migration, once tasks are moved to
+	 * mg_tasks, it can be read safely while holding cgroup_mutex.
+	 */
+	struct list_head tasks;
+	struct list_head mg_tasks;
+
+	/*
+	 * List of cgrp_cset_links pointing at cgroups referenced from this
+	 * css_set.  Protected by css_set_lock.
+	 */
+	struct list_head cgrp_links;
+
+	/* the default cgroup associated with this css_set */
+	struct cgroup *dfl_cgrp;
+
+	/*
+	 * Set of subsystem states, one for each subsystem. This array is
+	 * immutable after creation apart from the init_css_set during
+	 * subsystem registration (at boot time).
+	 */
+	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
+
+	/*
+	 * List of csets participating in the on-going migration either as
+	 * source or destination.  Protected by cgroup_mutex.
+	 */
+	struct list_head mg_preload_node;
+	struct list_head mg_node;
+
+	/*
+	 * If this cset is acting as the source of migration the following
+	 * two fields are set.  mg_src_cgrp is the source cgroup of the
+	 * on-going migration and mg_dst_cset is the destination cset the
+	 * target tasks on this cset should be migrated to.  Protected by
+	 * cgroup_mutex.
+	 */
+	struct cgroup *mg_src_cgrp;
+	struct css_set *mg_dst_cset;
+
+	/*
+	 * On the default hierarhcy, ->subsys[ssid] may point to a css
+	 * attached to an ancestor instead of the cgroup this css_set is
+	 * associated with.  The following node is anchored at
+	 * ->subsys[ssid]->cgroup->e_csets[ssid] and provides a way to
+	 * iterate through all css's attached to a given cgroup.
+	 */
+	struct list_head e_cset_node[CGROUP_SUBSYS_COUNT];
+
+	/* For RCU-protected deletion */
+	struct rcu_head rcu_head;
+};
+
+struct cgroup {
+	/* self css with NULL ->ss, points back to this cgroup */
+	struct cgroup_subsys_state self;
+
+	unsigned long flags;		/* "unsigned long" so bitops work */
+
+	/*
+	 * idr allocated in-hierarchy ID.
+	 *
+	 * ID 0 is not used, the ID of the root cgroup is always 1, and a
+	 * new cgroup will be assigned with a smallest available ID.
+	 *
+	 * Allocating/Removing ID must be protected by cgroup_mutex.
+	 */
+	int id;
+
+	/*
+	 * If this cgroup contains any tasks, it contributes one to
+	 * populated_cnt.  All children with non-zero popuplated_cnt of
+	 * their own contribute one.  The count is zero iff there's no task
+	 * in this cgroup or its subtree.
+	 */
+	int populated_cnt;
+
+	struct kernfs_node *kn;		/* cgroup kernfs entry */
+	struct kernfs_node *populated_kn; /* kn for "cgroup.subtree_populated" */
+
+	/*
+	 * The bitmask of subsystems enabled on the child cgroups.
+	 * ->subtree_control is the one configured through
+	 * "cgroup.subtree_control" while ->child_subsys_mask is the
+	 * effective one which may have more subsystems enabled.
+	 * Controller knobs are made available iff it's enabled in
+	 * ->subtree_control.
+	 */
+	unsigned int subtree_control;
+	unsigned int child_subsys_mask;
+
+	/* Private pointers for each registered subsystem */
+	struct cgroup_subsys_state __rcu *subsys[CGROUP_SUBSYS_COUNT];
+
+	struct cgroup_root *root;
+
+	/*
+	 * List of cgrp_cset_links pointing at css_sets with tasks in this
+	 * cgroup.  Protected by css_set_lock.
+	 */
+	struct list_head cset_links;
+
+	/*
+	 * On the default hierarchy, a css_set for a cgroup with some
+	 * susbsys disabled will point to css's which are associated with
+	 * the closest ancestor which has the subsys enabled.  The
+	 * following lists all css_sets which point to this cgroup's css
+	 * for the given subsystem.
+	 */
+	struct list_head e_csets[CGROUP_SUBSYS_COUNT];
+
+	/*
+	 * list of pidlists, up to two for each namespace (one for procs, one
+	 * for tasks); created on demand.
+	 */
+	struct list_head pidlists;
+	struct mutex pidlist_mutex;
+
+	/* used to wait for offlining of csses */
+	wait_queue_head_t offline_waitq;
+
+	/* used to schedule release agent */
+	struct work_struct release_agent_work;
+};
+
+/*
+ * A cgroup_root represents the root of a cgroup hierarchy, and may be
+ * associated with a kernfs_root to form an active hierarchy.  This is
+ * internal to cgroup core.  Don't access directly from controllers.
+ */
+struct cgroup_root {
+	struct kernfs_root *kf_root;
+
+	/* The bitmask of subsystems attached to this hierarchy */
+	unsigned int subsys_mask;
+
+	/* Unique id for this hierarchy. */
+	int hierarchy_id;
+
+	/* The root cgroup.  Root is destroyed on its release. */
+	struct cgroup cgrp;
+
+	/* Number of cgroups in the hierarchy, used only for /proc/cgroups */
+	atomic_t nr_cgrps;
+
+	/* A list running through the active hierarchies */
+	struct list_head root_list;
+
+	/* Hierarchy-specific flags */
+	unsigned int flags;
+
+	/* IDs for cgroups in this hierarchy */
+	struct idr cgroup_idr;
+
+	/* The path to use for release notifications. */
+	char release_agent_path[PATH_MAX];
+
+	/* The name for this hierarchy - may be empty */
+	char name[MAX_CGROUP_ROOT_NAMELEN];
+};
+
+/*
+ * struct cftype: handler definitions for cgroup control files
+ *
+ * When reading/writing to a file:
+ *	- the cgroup to use is file->f_path.dentry->d_parent->d_fsdata
+ *	- the 'cftype' of the file is file->f_path.dentry->d_fsdata
+ */
+struct cftype {
+	/*
+	 * By convention, the name should begin with the name of the
+	 * subsystem, followed by a period.  Zero length string indicates
+	 * end of cftype array.
+	 */
+	char name[MAX_CFTYPE_NAME];
+	int private;
+	/*
+	 * If not 0, file mode is set to this value, otherwise it will
+	 * be figured out automatically
+	 */
+	umode_t mode;
+
+	/*
+	 * The maximum length of string, excluding trailing nul, that can
+	 * be passed to write.  If < PAGE_SIZE-1, PAGE_SIZE-1 is assumed.
+	 */
+	size_t max_write_len;
+
+	/* CFTYPE_* flags */
+	unsigned int flags;
+
+	/*
+	 * Fields used for internal bookkeeping.  Initialized automatically
+	 * during registration.
+	 */
+	struct cgroup_subsys *ss;	/* NULL for cgroup core files */
+	struct list_head node;		/* anchored at ss->cfts */
+	struct kernfs_ops *kf_ops;
+
+	/*
+	 * read_u64() is a shortcut for the common case of returning a
+	 * single integer. Use it in place of read()
+	 */
+	u64 (*read_u64)(struct cgroup_subsys_state *css, struct cftype *cft);
+	/*
+	 * read_s64() is a signed version of read_u64()
+	 */
+	s64 (*read_s64)(struct cgroup_subsys_state *css, struct cftype *cft);
+
+	/* generic seq_file read interface */
+	int (*seq_show)(struct seq_file *sf, void *v);
+
+	/* optional ops, implement all or none */
+	void *(*seq_start)(struct seq_file *sf, loff_t *ppos);
+	void *(*seq_next)(struct seq_file *sf, void *v, loff_t *ppos);
+	void (*seq_stop)(struct seq_file *sf, void *v);
+
+	/*
+	 * write_u64() is a shortcut for the common case of accepting
+	 * a single integer (as parsed by simple_strtoull) from
+	 * userspace. Use in place of write(); return 0 or error.
+	 */
+	int (*write_u64)(struct cgroup_subsys_state *css, struct cftype *cft,
+			 u64 val);
+	/*
+	 * write_s64() is a signed version of write_u64()
+	 */
+	int (*write_s64)(struct cgroup_subsys_state *css, struct cftype *cft,
+			 s64 val);
+
+	/*
+	 * write() is the generic write callback which maps directly to
+	 * kernfs write operation and overrides all other operations.
+	 * Maximum write size is determined by ->max_write_len.  Use
+	 * of_css/cft() to access the associated css and cft.
+	 */
+	ssize_t (*write)(struct kernfs_open_file *of,
+			 char *buf, size_t nbytes, loff_t off);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lock_class_key	lockdep_key;
+#endif
+};
+
+/*
+ * Control Group subsystem type.
+ * See Documentation/cgroups/cgroups.txt for details
+ */
+struct cgroup_subsys {
+	struct cgroup_subsys_state *(*css_alloc)(struct cgroup_subsys_state *parent_css);
+	int (*css_online)(struct cgroup_subsys_state *css);
+	void (*css_offline)(struct cgroup_subsys_state *css);
+	void (*css_released)(struct cgroup_subsys_state *css);
+	void (*css_free)(struct cgroup_subsys_state *css);
+	void (*css_reset)(struct cgroup_subsys_state *css);
+	void (*css_e_css_changed)(struct cgroup_subsys_state *css);
+
+	int (*can_attach)(struct cgroup_subsys_state *css,
+			  struct cgroup_taskset *tset);
+	void (*cancel_attach)(struct cgroup_subsys_state *css,
+			      struct cgroup_taskset *tset);
+	void (*attach)(struct cgroup_subsys_state *css,
+		       struct cgroup_taskset *tset);
+	void (*fork)(struct task_struct *task);
+	void (*exit)(struct cgroup_subsys_state *css,
+		     struct cgroup_subsys_state *old_css,
+		     struct task_struct *task);
+	void (*bind)(struct cgroup_subsys_state *root_css);
+
+	int disabled;
+	int early_init;
+
+	/*
+	 * If %false, this subsystem is properly hierarchical -
+	 * configuration, resource accounting and restriction on a parent
+	 * cgroup cover those of its children.  If %true, hierarchy support
+	 * is broken in some ways - some subsystems ignore hierarchy
+	 * completely while others are only implemented half-way.
+	 *
+	 * It's now disallowed to create nested cgroups if the subsystem is
+	 * broken and cgroup core will emit a warning message on such
+	 * cases.  Eventually, all subsystems will be made properly
+	 * hierarchical and this will go away.
+	 */
+	bool broken_hierarchy;
+	bool warned_broken_hierarchy;
+
+	/* the following two fields are initialized automtically during boot */
+	int id;
+	const char *name;
+
+	/* link to parent, protected by cgroup_lock() */
+	struct cgroup_root *root;
+
+	/* idr for css->id */
+	struct idr css_idr;
+
+	/*
+	 * List of cftypes.  Each entry is the first entry of an array
+	 * terminated by zero length name.
+	 */
+	struct list_head cfts;
+
+	/*
+	 * Base cftypes which are automatically registered.  The two can
+	 * point to the same array.
+	 */
+	struct cftype *dfl_cftypes;	/* for the default hierarchy */
+	struct cftype *legacy_cftypes;	/* for the legacy hierarchies */
+
+	/*
+	 * A subsystem may depend on other subsystems.  When such subsystem
+	 * is enabled on a cgroup, the depended-upon subsystems are enabled
+	 * together if available.  Subsystems enabled due to dependency are
+	 * not visible to userland until explicitly enabled.  The following
+	 * specifies the mask of subsystems that this one depends on.
+	 */
+	unsigned int depends_on;
+};
+
+#endif	/* CONFIG_CGROUPS */
+#endif	/* _LINUX_CGROUP_DEFS_H */
