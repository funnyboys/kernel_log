commit 667c790169e2b9ebd33188c9285d30ee96844a02
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Apr 1 21:04:30 2020 -0700

    revert "topology: add support for node_to_mem_node() to determine the fallback node"
    
    This reverts commit ad2c8144418c6a81cefe65379fd47bbe8344cef2.
    
    The function node_to_mem_node() was introduced by that commit for use in SLUB
    on systems with memoryless nodes, but it turned out to be unreliable on some
    architectures/configurations and a simpler solution exists than fixing it up.
    
    Thus commit 0715e6c516f1 ("mm, slub: prevent kmalloc_node crashes and
    memory leaks") removed the only user of node_to_mem_node() and we can
    revert the commit that introduced the function.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Bharata B Rao <bharata@linux.ibm.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nathan Lynch <nathanl@linux.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: PUVICHAKRAVARTHY RAMACHANDRAN <puvichakravarthy@in.ibm.com>
    Cc: Sachin Sant <sachinp@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20200320115533.9604-2-vbabka@suse.cz
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index eb2fe6edd73c..608fa4aadf0e 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -130,20 +130,11 @@ static inline int numa_node_id(void)
  * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem().
  */
 DECLARE_PER_CPU(int, _numa_mem_);
-extern int _node_numa_mem_[MAX_NUMNODES];
 
 #ifndef set_numa_mem
 static inline void set_numa_mem(int node)
 {
 	this_cpu_write(_numa_mem_, node);
-	_node_numa_mem_[numa_node_id()] = node;
-}
-#endif
-
-#ifndef node_to_mem_node
-static inline int node_to_mem_node(int node)
-{
-	return _node_numa_mem_[node];
 }
 #endif
 
@@ -166,7 +157,6 @@ static inline int cpu_to_mem(int cpu)
 static inline void set_cpu_numa_mem(int cpu, int node)
 {
 	per_cpu(_numa_mem_, cpu) = node;
-	_node_numa_mem_[cpu_to_node(cpu)] = node;
 }
 #endif
 
@@ -180,13 +170,6 @@ static inline int numa_mem_id(void)
 }
 #endif
 
-#ifndef node_to_mem_node
-static inline int node_to_mem_node(int node)
-{
-	return node;
-}
-#endif
-
 #ifndef cpu_to_mem
 static inline int cpu_to_mem(int cpu)
 {

commit 7e67a859997aad47727aff9c5a32e160da079ce3
Merge: 772c1d06bd40 563c4f85f9f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 17:25:49 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - MAINTAINERS: Add Mark Rutland as perf submaintainer, Juri Lelli and
       Vincent Guittot as scheduler submaintainers. Add Dietmar Eggemann,
       Steven Rostedt, Ben Segall and Mel Gorman as scheduler reviewers.
    
       As perf and the scheduler is getting bigger and more complex,
       document the status quo of current responsibilities and interests,
       and spread the review pain^H^H^H^H fun via an increase in the Cc:
       linecount generated by scripts/get_maintainer.pl. :-)
    
     - Add another series of patches that brings the -rt (PREEMPT_RT) tree
       closer to mainline: split the monolithic CONFIG_PREEMPT dependencies
       into a new CONFIG_PREEMPTION category that will allow the eventual
       introduction of CONFIG_PREEMPT_RT. Still a few more hundred patches
       to go though.
    
     - Extend the CPU cgroup controller with uclamp.min and uclamp.max to
       allow the finer shaping of CPU bandwidth usage.
    
     - Micro-optimize energy-aware wake-ups from O(CPUS^2) to O(CPUS).
    
     - Improve the behavior of high CPU count, high thread count
       applications running under cpu.cfs_quota_us constraints.
    
     - Improve balancing with SCHED_IDLE (SCHED_BATCH) tasks present.
    
     - Improve CPU isolation housekeeping CPU allocation NUMA locality.
    
     - Fix deadline scheduler bandwidth calculations and logic when cpusets
       rebuilds the topology, or when it gets deadline-throttled while it's
       being offlined.
    
     - Convert the cpuset_mutex to percpu_rwsem, to allow it to be used from
       setscheduler() system calls without creating global serialization.
       Add new synchronization between cpuset topology-changing events and
       the deadline acceptance tests in setscheduler(), which were broken
       before.
    
     - Rework the active_mm state machine to be less confusing and more
       optimal.
    
     - Rework (simplify) the pick_next_task() slowpath.
    
     - Improve load-balancing on AMD EPYC systems.
    
     - ... and misc cleanups, smaller fixes and improvements - please see
       the Git log for more details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      sched/psi: Correct overly pessimistic size calculation
      sched/fair: Speed-up energy-aware wake-ups
      sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
      sched/uclamp: Update CPU's refcount on TG's clamp changes
      sched/uclamp: Use TG's clamps to restrict TASK's clamps
      sched/uclamp: Propagate system defaults to the root group
      sched/uclamp: Propagate parent clamps
      sched/uclamp: Extend CPU's cgroup controller
      sched/topology: Improve load balancing on AMD EPYC systems
      arch, ia64: Make NUMA select SMP
      sched, perf: MAINTAINERS update, add submaintainers and reviewers
      sched/fair: Use rq_lock/unlock in online_fair_sched_group
      cpufreq: schedutil: fix equation in comment
      sched: Rework pick_next_task() slow-path
      sched: Allow put_prev_task() to drop rq->lock
      sched/fair: Expose newidle_balance()
      sched: Add task_struct pointer to sched_class::set_curr_task
      sched: Rework CPU hotplug task selection
      sched/{rt,deadline}: Fix set_next_task vs pick_next_task
      sched: Fix kerneldoc comment for ia64_set_curr_task
      ...

commit a55c7454a8c887b226a01d7eed088ccb5374d81e
Author: Matt Fleming <matt@codeblueprint.co.uk>
Date:   Thu Aug 8 20:53:01 2019 +0100

    sched/topology: Improve load balancing on AMD EPYC systems
    
    SD_BALANCE_{FORK,EXEC} and SD_WAKE_AFFINE are stripped in sd_init()
    for any sched domains with a NUMA distance greater than 2 hops
    (RECLAIM_DISTANCE). The idea being that it's expensive to balance
    across domains that far apart.
    
    However, as is rather unfortunately explained in:
    
      commit 32e45ff43eaf ("mm: increase RECLAIM_DISTANCE to 30")
    
    the value for RECLAIM_DISTANCE is based on node distance tables from
    2011-era hardware.
    
    Current AMD EPYC machines have the following NUMA node distances:
    
     node distances:
     node   0   1   2   3   4   5   6   7
       0:  10  16  16  16  32  32  32  32
       1:  16  10  16  16  32  32  32  32
       2:  16  16  10  16  32  32  32  32
       3:  16  16  16  10  32  32  32  32
       4:  32  32  32  32  10  16  16  16
       5:  32  32  32  32  16  10  16  16
       6:  32  32  32  32  16  16  10  16
       7:  32  32  32  32  16  16  16  10
    
    where 2 hops is 32.
    
    The result is that the scheduler fails to load balance properly across
    NUMA nodes on different sockets -- 2 hops apart.
    
    For example, pinning 16 busy threads to NUMA nodes 0 (CPUs 0-7) and 4
    (CPUs 32-39) like so,
    
      $ numactl -C 0-7,32-39 ./spinner 16
    
    causes all threads to fork and remain on node 0 until the active
    balancer kicks in after a few seconds and forcibly moves some threads
    to node 4.
    
    Override node_reclaim_distance for AMD Zen.
    
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Suravee.Suthikulpanit@amd.com
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas.Lendacky@amd.com
    Cc: Tony Luck <tony.luck@intel.com>
    Link: https://lkml.kernel.org/r/20190808195301.13222-3-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 47a3e3c08036..579522ec446c 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -59,6 +59,20 @@ int arch_update_cpu_topology(void);
  */
 #define RECLAIM_DISTANCE 30
 #endif
+
+/*
+ * The following tunable allows platforms to override the default node
+ * reclaim distance (RECLAIM_DISTANCE) if remote memory accesses are
+ * sufficiently fast that the default value actually hurts
+ * performance.
+ *
+ * AMD EPYC machines use this because even though the 2-hop distance
+ * is 32 (3.2x slower than a local memory access) performance actually
+ * *improves* if allowed to reclaim memory and load balance tasks
+ * between NUMA nodes 2-hops apart.
+ */
+extern int __read_mostly node_reclaim_distance;
+
 #ifndef PENALTY_FOR_NODE_WITH_CPUS
 #define PENALTY_FOR_NODE_WITH_CPUS	(1)
 #endif

commit 60c1b220d8bc6baeaf837cd60f94a331b25c26bc
Author: Atish Patra <atish.patra@wdc.com>
Date:   Thu Jun 27 12:52:58 2019 -0700

    cpu-topology: Move cpu topology code to common code.
    
    Both RISC-V & ARM64 are using cpu-map device tree to describe
    their cpu topology. It's better to move the relevant code to
    a common place instead of duplicate code.
    
    To: Will Deacon <will.deacon@arm.com>
    To: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Atish Patra <atish.patra@wdc.com>
    [Tested on QDF2400]
    Tested-by: Jeffrey Hugo <jhugo@codeaurora.org>
    [Tested on Juno and other embedded platforms.]
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 47a3e3c08036..2a19d196af28 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -27,6 +27,7 @@
 #ifndef _LINUX_TOPOLOGY_H
 #define _LINUX_TOPOLOGY_H
 
+#include <linux/arch_topology.h>
 #include <linux/cpumask.h>
 #include <linux/bitops.h>
 #include <linux/mmzone.h>

commit 2e4c54dac7b360c3820399bdf06cde9134a4495b
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:56 2019 -0400

    topology: Create core_cpus and die_cpus sysfs attributes
    
    Create CPU topology sysfs attributes: "core_cpus" and "core_cpus_list"
    
    These attributes represent all of the logical CPUs that share the
    same core.
    
    These attriutes is synonymous with the existing "thread_siblings" and
    "thread_siblings_list" attribute, which will be deprecated.
    
    Create CPU topology sysfs attributes: "die_cpus" and "die_cpus_list".
    These attributes represent all of the logical CPUs that share the
    same die.
    
    Suggested-by: Brice Goglin <Brice.Goglin@inria.fr>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/071c23a298cd27ede6ed0b6460cae190d193364f.1557769318.git.len.brown@intel.com

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 5cc8595dd0e4..47a3e3c08036 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -196,6 +196,9 @@ static inline int cpu_to_mem(int cpu)
 #ifndef topology_core_cpumask
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
 #endif
+#ifndef topology_die_cpumask
+#define topology_die_cpumask(cpu)		cpumask_of(cpu)
+#endif
 
 #ifdef CONFIG_SCHED_SMT
 static inline const struct cpumask *cpu_smt_mask(int cpu)

commit 0e344d8c709fe01d882fc0fb5452bedfe5eba67a
Author: Len Brown <len.brown@intel.com>
Date:   Mon May 13 13:58:47 2019 -0400

    cpu/topology: Export die_id
    
    Export die_id in cpu topology, for the benefit of hardware that has
    multiple-die/package.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-doc@vger.kernel.org
    Link: https://lkml.kernel.org/r/e7d1caaf4fbd24ee40db6d557ab28d7d83298900.1557769318.git.len.brown@intel.com

diff --git a/include/linux/topology.h b/include/linux/topology.h
index cb0775e1ee4b..5cc8595dd0e4 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -184,6 +184,9 @@ static inline int cpu_to_mem(int cpu)
 #ifndef topology_physical_package_id
 #define topology_physical_package_id(cpu)	((void)(cpu), -1)
 #endif
+#ifndef topology_die_id
+#define topology_die_id(cpu)			((void)(cpu), -1)
+#endif
 #ifndef topology_core_id
 #define topology_core_id(cpu)			((void)(cpu), 0)
 #endif

commit a5f5f91da6ad647fb0cc7fce0e17343c0d1c5a9a
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:32 2016 -0700

    mm: convert zone_reclaim to node_reclaim
    
    As reclaim is now per-node based, convert zone_reclaim to be
    node_reclaim.  It is possible that a node will be reclaimed multiple
    times if it has multiple zones but this is unavoidable without caching
    all nodes traversed so far.  The documentation and interface to
    userspace is the same from a configuration perspective and will will be
    similar in behaviour unless the node-local allocation requests were also
    limited to lower zones.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-24-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index afce69296ac0..cb0775e1ee4b 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -54,7 +54,7 @@ int arch_update_cpu_topology(void);
 /*
  * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
  * (in whatever arch specific measurement units returned by node_distance())
- * and zone_reclaim_mode is enabled then the VM will only call zone_reclaim()
+ * and node_reclaim_mode is enabled then the VM will only call node_reclaim()
  * on nodes within this distance.
  */
 #define RECLAIM_DISTANCE 30

commit 00d27c6336b00345724b2510f7c5b8cee3055f02
Author: Chris Metcalf <cmetcalf@ezchip.com>
Date:   Tue Jan 5 11:22:10 2016 -0500

    numa: remove stale node_has_online_mem() define
    
    This isn't used anywhere, so delete it.
    
    Looks like the last usage (in x86-specific code) was removed by Tejun
    in 2011 in commit bd6709a91a59 ("x86, NUMA: Make 32bit use common NUMA
    init path").
    
    Signed-off-by: Chris Metcalf <cmetcalf@ezchip.com>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 73ddad1e0fa3..afce69296ac0 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -34,10 +34,6 @@
 #include <linux/percpu.h>
 #include <asm/topology.h>
 
-#ifndef node_has_online_mem
-#define node_has_online_mem(nid) (1)
-#endif
-
 #ifndef nr_cpus_node
 #define nr_cpus_node(node) cpumask_weight(cpumask_of_node(node))
 #endif

commit 06931e62246844c73fba24d7aeb4a5dc897a2739
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Tue May 26 15:11:28 2015 +0200

    sched/topology: Rename topology_thread_cpumask() to topology_sibling_cpumask()
    
    Rename topology_thread_cpumask() to topology_sibling_cpumask()
    for more consistency with scheduler code.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Benoit Cousson <bcousson@baylibre.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Jean Delvare <jdelvare@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Drokin <oleg.drokin@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: http://lkml.kernel.org/r/1432645896-12588-2-git-send-email-bgolaszewski@baylibre.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 909b6e43b694..73ddad1e0fa3 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -191,8 +191,8 @@ static inline int cpu_to_mem(int cpu)
 #ifndef topology_core_id
 #define topology_core_id(cpu)			((void)(cpu), 0)
 #endif
-#ifndef topology_thread_cpumask
-#define topology_thread_cpumask(cpu)		cpumask_of(cpu)
+#ifndef topology_sibling_cpumask
+#define topology_sibling_cpumask(cpu)		cpumask_of(cpu)
 #endif
 #ifndef topology_core_cpumask
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
@@ -201,7 +201,7 @@ static inline int cpu_to_mem(int cpu)
 #ifdef CONFIG_SCHED_SMT
 static inline const struct cpumask *cpu_smt_mask(int cpu)
 {
-	return topology_thread_cpumask(cpu);
+	return topology_sibling_cpumask(cpu);
 }
 #endif
 

commit ad2c8144418c6a81cefe65379fd47bbe8344cef2
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Oct 9 15:26:13 2014 -0700

    topology: add support for node_to_mem_node() to determine the fallback node
    
    Anton noticed (http://www.spinics.net/lists/linux-mm/msg67489.html) that
    on ppc LPARs with memoryless nodes, a large amount of memory was consumed
    by slabs and was marked unreclaimable.  He tracked it down to slab
    deactivations in the SLUB core when we allocate remotely, leading to poor
    efficiency always when memoryless nodes are present.
    
    After much discussion, Joonsoo provided a few patches that help
    significantly.  They don't resolve the problem altogether:
    
     - memory hotplug still needs testing, that is when a memoryless node
       becomes memory-ful, we want to dtrt
     - there are other reasons for going off-node than memoryless nodes,
       e.g., fully exhausted local nodes
    
    Neither case is resolved with this series, but I don't think that should
    block their acceptance, as they can be explored/resolved with follow-on
    patches.
    
    The series consists of:
    
    [1/3] topology: add support for node_to_mem_node() to determine the
          fallback node
    
    [2/3] slub: fallback to node_to_mem_node() node if allocating on
          memoryless node
    
          - Joonsoo's patches to cache the nearest node with memory for each
            NUMA node
    
    [3/3] Partial revert of 81c98869faa5 (""kthread: ensure locality of
          task_struct allocations")
    
     - At Tejun's request, keep the knowledge of memoryless node fallback
       to the allocator core.
    
    This patch (of 3):
    
    We need to determine the fallback node in slub allocator if the allocation
    target node is memoryless node.  Without it, the SLUB wrongly select the
    node which has no memory and can't use a partial slab, because of node
    mismatch.  Introduced function, node_to_mem_node(X), will return a node Y
    with memory that has the nearest distance.  If X is memoryless node, it
    will return nearest distance node, but, if X is normal node, it will
    return itself.
    
    We will use this function in following patch to determine the fallback
    node.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Han Pingtian <hanpt@linux.vnet.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index dda6ee521e74..909b6e43b694 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -119,11 +119,20 @@ static inline int numa_node_id(void)
  * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem().
  */
 DECLARE_PER_CPU(int, _numa_mem_);
+extern int _node_numa_mem_[MAX_NUMNODES];
 
 #ifndef set_numa_mem
 static inline void set_numa_mem(int node)
 {
 	this_cpu_write(_numa_mem_, node);
+	_node_numa_mem_[numa_node_id()] = node;
+}
+#endif
+
+#ifndef node_to_mem_node
+static inline int node_to_mem_node(int node)
+{
+	return _node_numa_mem_[node];
 }
 #endif
 
@@ -146,6 +155,7 @@ static inline int cpu_to_mem(int cpu)
 static inline void set_cpu_numa_mem(int cpu, int node)
 {
 	per_cpu(_numa_mem_, cpu) = node;
+	_node_numa_mem_[cpu_to_node(cpu)] = node;
 }
 #endif
 
@@ -159,6 +169,13 @@ static inline int numa_mem_id(void)
 }
 #endif
 
+#ifndef node_to_mem_node
+static inline int node_to_mem_node(int node)
+{
+	return node;
+}
+#endif
+
 #ifndef cpu_to_mem
 static inline int cpu_to_mem(int cpu)
 {

commit 4f9b16a64753d0bb607454347036dc997fd03b82
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:07:14 2014 -0700

    mm: disable zone_reclaim_mode by default
    
    When it was introduced, zone_reclaim_mode made sense as NUMA distances
    punished and workloads were generally partitioned to fit into a NUMA
    node.  NUMA machines are now common but few of the workloads are
    NUMA-aware and it's routine to see major performance degradation due to
    zone_reclaim_mode being enabled but relatively few can identify the
    problem.
    
    Those that require zone_reclaim_mode are likely to be able to detect
    when it needs to be enabled and tune appropriately so lets have a
    sensible default for the bulk of users.
    
    This patch (of 2):
    
    zone_reclaim_mode causes processes to prefer reclaiming memory from
    local node instead of spilling over to other nodes.  This made sense
    initially when NUMA machines were almost exclusively HPC and the
    workload was partitioned into nodes.  The NUMA penalties were
    sufficiently high to justify reclaiming the memory.  On current machines
    and workloads it is often the case that zone_reclaim_mode destroys
    performance but not all users know how to detect this.  Favour the
    common case and disable it by default.  Users that are sophisticated
    enough to know they need zone_reclaim_mode will detect it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 973671ff9e7d..dda6ee521e74 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -58,7 +58,8 @@ int arch_update_cpu_topology(void);
 /*
  * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
  * (in whatever arch specific measurement units returned by node_distance())
- * then switch on zone reclaim on boot.
+ * and zone_reclaim_mode is enabled then the VM will only call zone_reclaim()
+ * on nodes within this distance.
  */
 #define RECLAIM_DISTANCE 30
 #endif

commit 143e1e28cb40bed836b0a06567208bd7347c9672
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Apr 11 11:44:37 2014 +0200

    sched: Rework sched_domain topology definition
    
    We replace the old way to configure the scheduler topology with a new method
    which enables a platform to declare additionnal level (if needed).
    
    We still have a default topology table definition that can be used by platform
    that don't want more level than the SMT, MC, CPU and NUMA ones. This table can
    be overwritten by an arch which either wants to add new level where a load
    balance make sense like BOOK or powergating level or wants to change the flags
    configuration of some levels.
    
    For each level, we need a function pointer that returns cpumask for each cpu,
    a function pointer that returns the flags for the level and a name. Only flags
    that describe topology, can be set by an architecture. The current topology
    flags are:
    
     SD_SHARE_CPUPOWER
     SD_SHARE_PKG_RESOURCES
     SD_NUMA
     SD_ASYM_PACKING
    
    Then, each level must be a subset on the next one. The build sequence of the
    sched_domain will take care of removing useless levels like those with 1 CPU
    and those with the same CPU span and no more relevant information for
    load balancing than its children.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux390@de.ibm.com
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Link: http://lkml.kernel.org/r/1397209481-28542-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 7062330a1329..973671ff9e7d 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -66,121 +66,6 @@ int arch_update_cpu_topology(void);
 #define PENALTY_FOR_NODE_WITH_CPUS	(1)
 #endif
 
-/*
- * Below are the 3 major initializers used in building sched_domains:
- * SD_SIBLING_INIT, for SMT domains
- * SD_CPU_INIT, for SMP domains
- *
- * Any architecture that cares to do any tuning to these values should do so
- * by defining their own arch-specific initializer in include/asm/topology.h.
- * A definition there will automagically override these default initializers
- * and allow arch-specific performance tuning of sched_domains.
- * (Only non-zero and non-null fields need be specified.)
- */
-
-#ifdef CONFIG_SCHED_SMT
-/* MCD - Do we really need this?  It is always on if CONFIG_SCHED_SMT is,
- * so can't we drop this in favor of CONFIG_SCHED_SMT?
- */
-#define ARCH_HAS_SCHED_WAKE_IDLE
-/* Common values for SMT siblings */
-#ifndef SD_SIBLING_INIT
-#define SD_SIBLING_INIT (struct sched_domain) {				\
-	.min_interval		= 1,					\
-	.max_interval		= 2,					\
-	.busy_factor		= 64,					\
-	.imbalance_pct		= 110,					\
-									\
-	.flags			= 1*SD_LOAD_BALANCE			\
-				| 1*SD_BALANCE_NEWIDLE			\
-				| 1*SD_BALANCE_EXEC			\
-				| 1*SD_BALANCE_FORK			\
-				| 0*SD_BALANCE_WAKE			\
-				| 1*SD_WAKE_AFFINE			\
-				| 1*SD_SHARE_CPUPOWER			\
-				| 1*SD_SHARE_PKG_RESOURCES		\
-				| 0*SD_SERIALIZE			\
-				| 0*SD_PREFER_SIBLING			\
-				| arch_sd_sibling_asym_packing()	\
-				,					\
-	.last_balance		= jiffies,				\
-	.balance_interval	= 1,					\
-	.smt_gain		= 1178,	/* 15% */			\
-	.max_newidle_lb_cost	= 0,					\
-	.next_decay_max_lb_cost	= jiffies,				\
-}
-#endif
-#endif /* CONFIG_SCHED_SMT */
-
-#ifdef CONFIG_SCHED_MC
-/* Common values for MC siblings. for now mostly derived from SD_CPU_INIT */
-#ifndef SD_MC_INIT
-#define SD_MC_INIT (struct sched_domain) {				\
-	.min_interval		= 1,					\
-	.max_interval		= 4,					\
-	.busy_factor		= 64,					\
-	.imbalance_pct		= 125,					\
-	.cache_nice_tries	= 1,					\
-	.busy_idx		= 2,					\
-	.wake_idx		= 0,					\
-	.forkexec_idx		= 0,					\
-									\
-	.flags			= 1*SD_LOAD_BALANCE			\
-				| 1*SD_BALANCE_NEWIDLE			\
-				| 1*SD_BALANCE_EXEC			\
-				| 1*SD_BALANCE_FORK			\
-				| 0*SD_BALANCE_WAKE			\
-				| 1*SD_WAKE_AFFINE			\
-				| 0*SD_SHARE_CPUPOWER			\
-				| 1*SD_SHARE_PKG_RESOURCES		\
-				| 0*SD_SERIALIZE			\
-				,					\
-	.last_balance		= jiffies,				\
-	.balance_interval	= 1,					\
-	.max_newidle_lb_cost	= 0,					\
-	.next_decay_max_lb_cost	= jiffies,				\
-}
-#endif
-#endif /* CONFIG_SCHED_MC */
-
-/* Common values for CPUs */
-#ifndef SD_CPU_INIT
-#define SD_CPU_INIT (struct sched_domain) {				\
-	.min_interval		= 1,					\
-	.max_interval		= 4,					\
-	.busy_factor		= 64,					\
-	.imbalance_pct		= 125,					\
-	.cache_nice_tries	= 1,					\
-	.busy_idx		= 2,					\
-	.idle_idx		= 1,					\
-	.newidle_idx		= 0,					\
-	.wake_idx		= 0,					\
-	.forkexec_idx		= 0,					\
-									\
-	.flags			= 1*SD_LOAD_BALANCE			\
-				| 1*SD_BALANCE_NEWIDLE			\
-				| 1*SD_BALANCE_EXEC			\
-				| 1*SD_BALANCE_FORK			\
-				| 0*SD_BALANCE_WAKE			\
-				| 1*SD_WAKE_AFFINE			\
-				| 0*SD_SHARE_CPUPOWER			\
-				| 0*SD_SHARE_PKG_RESOURCES		\
-				| 0*SD_SERIALIZE			\
-				| 1*SD_PREFER_SIBLING			\
-				,					\
-	.last_balance		= jiffies,				\
-	.balance_interval	= 1,					\
-	.max_newidle_lb_cost	= 0,					\
-	.next_decay_max_lb_cost	= jiffies,				\
-}
-#endif
-
-#ifdef CONFIG_SCHED_BOOK
-#ifndef SD_BOOK_INIT
-#error Please define an appropriate SD_BOOK_INIT in include/asm/topology.h!!!
-#endif
-#endif /* CONFIG_SCHED_BOOK */
-
 #ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID
 DECLARE_PER_CPU(int, numa_node);
 
@@ -295,4 +180,17 @@ static inline int cpu_to_mem(int cpu)
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
 #endif
 
+#ifdef CONFIG_SCHED_SMT
+static inline const struct cpumask *cpu_smt_mask(int cpu)
+{
+	return topology_thread_cpumask(cpu);
+}
+#endif
+
+static inline const struct cpumask *cpu_cpu_mask(int cpu)
+{
+	return cpumask_of_node(cpu_to_node(cpu));
+}
+
+
 #endif /* _LINUX_TOPOLOGY_H */

commit dc322a99d31fff5d3f8acfa061ad033953efdebe
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Apr 7 15:39:38 2014 -0700

    mm: use raw_cpu ops for determining current NUMA node
    
    With the preempt checking logic for __this_cpu_ops we will get false
    positives from locations in the code that use numa_node_id.
    
    Before the __this_cpu ops where introduced there were no checks for
    preemption present either.  smp_raw_processor_id() was used.  See
    
      http://www.spinics.net/lists/linux-numa/msg00641.html
    
    Therefore we need to use raw_cpu_read here to avoid false postives.
    
    Note that this issue has been discussed in prior years.  If the process
    changes nodes after retrieving the current numa node then that is
    acceptable since most uses of numa_node etc are for optimization and not
    for correctness.
    
    There were suggestions to implement a raw_numa_node_id in order to do
    preempt checks for numa_node_id as well.  But I think we better defer
    that to another patch since that would mean investigating how
    numa_node_id() is used throughout the kernel which would increase the
    scope of this patchset significantly.  After all preemption was never
    checked before when numa_node_id() was used.
    
    Some sample traces:
    
    __this_cpu_read operation in preemptible [00000000] code: login/1456
    caller is __this_cpu_preempt_check+0x2b/0x2d
    CPU: 0 PID: 1456 Comm: login Not tainted 3.12.0-rc4-cl-00062-g2fe80d3-dirty #185
    Call Trace:
      dump_stack+0x4e/0x82
      check_preemption_disabled+0xc5/0xe0
      __this_cpu_preempt_check+0x2b/0x2d
      get_task_policy+0x1d/0x49
      get_vma_policy+0x14/0x76
      alloc_pages_vma+0x35/0xff
      handle_mm_fault+0x290/0x73b
      __do_page_fault+0x3fe/0x44d
      do_page_fault+0x9/0xc
      page_fault+0x22/0x30
      generic_file_aio_read+0x38e/0x624
      do_sync_read+0x54/0x73
      vfs_read+0x9d/0x12a
      SyS_read+0x47/0x7e
      cstar_dispatch+0x7/0x23
    
    caller is __this_cpu_preempt_check+0x2b/0x2d
    CPU: 0 PID: 1456 Comm: login Not tainted 3.12.0-rc4-cl-00062-g2fe80d3-dirty #185
    Call Trace:
      dump_stack+0x4e/0x82
      check_preemption_disabled+0xc5/0xe0
      __this_cpu_preempt_check+0x2b/0x2d
      alloc_pages_current+0x8f/0xbc
      __page_cache_alloc+0xb/0xd
      __do_page_cache_readahead+0xf4/0x219
      ra_submit+0x1c/0x20
      ondemand_readahead+0x28c/0x2b4
      page_cache_sync_readahead+0x38/0x3a
      generic_file_aio_read+0x261/0x624
      do_sync_read+0x54/0x73
      vfs_read+0x9d/0x12a
      SyS_read+0x47/0x7e
      cstar_dispatch+0x7/0x23
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Alex Shi <alex.shi@intel.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 12ae6ce997d6..7062330a1329 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -188,7 +188,7 @@ DECLARE_PER_CPU(int, numa_node);
 /* Returns the number of the current Node. */
 static inline int numa_node_id(void)
 {
-	return __this_cpu_read(numa_node);
+	return raw_cpu_read(numa_node);
 }
 #endif
 
@@ -245,7 +245,7 @@ static inline void set_numa_mem(int node)
 /* Returns the number of the nearest Node with memory */
 static inline int numa_mem_id(void)
 {
-	return __this_cpu_read(_numa_mem_);
+	return raw_cpu_read(_numa_mem_);
 }
 #endif
 

commit f48627e686a69f5215cb0761e731edb3d9859dd9
Author: Jason Low <jason.low2@hp.com>
Date:   Fri Sep 13 11:26:53 2013 -0700

    sched/balancing: Periodically decay max cost of idle balance
    
    This patch builds on patch 2 and periodically decays that max value to
    do idle balancing per sched domain by approximately 1% per second. Also
    decay the rq's max_idle_balance_cost value.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379096813-3032-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e2a2c3da2929..12ae6ce997d6 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -107,6 +107,7 @@ int arch_update_cpu_topology(void);
 	.balance_interval	= 1,					\
 	.smt_gain		= 1178,	/* 15% */			\
 	.max_newidle_lb_cost	= 0,					\
+	.next_decay_max_lb_cost	= jiffies,				\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */
@@ -137,6 +138,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.max_newidle_lb_cost	= 0,					\
+	.next_decay_max_lb_cost	= jiffies,				\
 }
 #endif
 #endif /* CONFIG_SCHED_MC */
@@ -169,6 +171,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.max_newidle_lb_cost	= 0,					\
+	.next_decay_max_lb_cost	= jiffies,				\
 }
 #endif
 

commit 9bd721c55c8a886b938a45198aab0ccb52f1f7fa
Author: Jason Low <jason.low2@hp.com>
Date:   Fri Sep 13 11:26:52 2013 -0700

    sched/balancing: Consider max cost of idle balance per sched domain
    
    In this patch, we keep track of the max cost we spend doing idle load balancing
    for each sched domain. If the avg time the CPU remains idle is less then the
    time we have already spent on idle balancing + the max cost of idle balancing
    in the sched domain, then we don't continue to attempt the balance. We also
    keep a per rq variable, max_idle_balance_cost, which keeps track of the max
    time spent on newidle load balances throughout all its domains so that we can
    determine the avg_idle's max value.
    
    By using the max, we avoid overrunning the average. This further reduces the
    chance we attempt balancing when the CPU is not idle for longer than the cost
    to balance.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379096813-3032-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index d3cf0d6e7712..e2a2c3da2929 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -106,6 +106,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.smt_gain		= 1178,	/* 15% */			\
+	.max_newidle_lb_cost	= 0,					\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */
@@ -135,6 +136,7 @@ int arch_update_cpu_topology(void);
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
+	.max_newidle_lb_cost	= 0,					\
 }
 #endif
 #endif /* CONFIG_SCHED_MC */
@@ -166,6 +168,7 @@ int arch_update_cpu_topology(void);
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
+	.max_newidle_lb_cost	= 0,					\
 }
 #endif
 

commit f03542a7019c600163ac4441d8a826c92c1bd510
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jul 26 08:55:34 2012 +0800

    sched: recover SD_WAKE_AFFINE in select_task_rq_fair and code clean up
    
    Since power saving code was removed from sched now, the implement
    code is out of service in this function, and even pollute other logical.
    like, 'want_sd' never has chance to be set '0', that remove the effect
    of SD_WAKE_AFFINE here.
    
    So, clean up the obsolete code, includes SD_PREFER_LOCAL.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/5028F431.6000306@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index fec12d667211..d3cf0d6e7712 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -129,7 +129,6 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 0*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
@@ -160,7 +159,6 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 0*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\

commit 6956dc568f34107f1d02b24f87efe7250803fc87
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri Jul 20 14:19:50 2012 +0800

    sched/numa: Add SD_PERFER_SIBLING to CPU domain
    
    Commit 8e7fbcbc22c ("sched: Remove stale power aware scheduling remnants
    and dysfunctional knobs") removed SD_PERFER_SIBLING from the CPU domain.
    
    On NUMA machines this causes that load_balance() doesn't perfer LCPU in
     same physical CPU package.
    
    It causes some actual performance regressions on our NUMA machines from
    Core2 to NHM and SNB.
    
    Adding this domain flag again recovers the performance drop.
    
    This change doesn't have any bad impact on any of my benchmarks:
     specjbb, kbuild, fio, hackbench .. etc, on all my machines.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1342765190-21540-1-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e91cd43394df..fec12d667211 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -164,6 +164,7 @@ int arch_update_cpu_topology(void);
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
+				| 1*SD_PREFER_SIBLING			\
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\

commit d79ee93de909dfb252279b9a95978bbda9a814a9
Merge: 2ff2b289a695 1c2927f18576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 18:27:32 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is the cleanup/simplification of the load-balancer:
      instead of the current practice of architectures twiddling scheduler
      internal data structures and providing the scheduler domains in
      colorfully inconsistent ways, we now have generic scheduler code in
      kernel/sched/core.c:sched_init_numa() that looks at the architecture's
      node_distance() parameters and (while not fully trusting it) deducts a
      NUMA topology from it.
    
      This inevitably changes balancing behavior - hopefully for the better.
    
      There are various smaller optimizations, cleanups and fixlets as well"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Taint kernel with TAINT_WARN after sleep-in-atomic bug
      sched: Remove stale power aware scheduling remnants and dysfunctional knobs
      sched/debug: Fix printing large integers on 32-bit platforms
      sched/fair: Improve the ->group_imb logic
      sched/nohz: Fix rq->cpu_load[] calculations
      sched/numa: Don't scale the imbalance
      sched/fair: Revert sched-domain iteration breakage
      sched/x86: Rewrite set_cpu_sibling_map()
      sched/numa: Fix the new NUMA topology bits
      sched/numa: Rewrite the CONFIG_NUMA sched domain support
      sched/fair: Propagate 'struct lb_env' usage into find_busiest_group
      sched/fair: Add some serialization to the sched_domain load-balance walk
      sched/fair: Let minimally loaded cpu balance the group
      sched: Change rq->nr_running to unsigned int
      x86/numa: Check for nonsensical topologies on real hw as well
      x86/numa: Hard partition cpu topology masks on node boundaries
      x86/numa: Allow specifying node_distance() for numa=fake
      x86/sched: Make mwait_usable() heed to "idle=" kernel parameters properly
      sched: Update documentation and comments
      sched_rt: Avoid unnecessary dequeue and enqueue of pushable tasks in set_cpus_allowed_rt()

commit 8e7fbcbc22c12414bcc9dfdd683637f58fb32759
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 9 11:28:35 2012 +0100

    sched: Remove stale power aware scheduling remnants and dysfunctional knobs
    
    It's been broken forever (i.e. it's not scheduling in a power
    aware fashion), as reported by Suresh and others sending
    patches, and nobody cares enough to fix it properly ...
    so remove it to make space free for something better.
    
    There's various problems with the code as it stands today, first
    and foremost the user interface which is bound to topology
    levels and has multiple values per level. This results in a
    state explosion which the administrator or distro needs to
    master and almost nobody does.
    
    Furthermore large configuration state spaces aren't good, it
    means the thing doesn't just work right because it's either
    under so many impossibe to meet constraints, or even if
    there's an achievable state workloads have to be aware of
    it precisely and can never meet it for dynamic workloads.
    
    So pushing this kind of decision to user-space was a bad idea
    even with a single knob - it's exponentially worse with knobs
    on every node of the topology.
    
    There is a proposal to replace the user interface with a single
    3 state knob:
    
     sched_balance_policy := { performance, power, auto }
    
    where 'auto' would be the preferred default which looks at things
    like Battery/AC mode and possible cpufreq state or whatever the hw
    exposes to show us power use expectations - but there's been no
    progress on it in the past many months.
    
    Aside from that, the actual implementation of the various knobs
    is known to be broken. There have been sporadic attempts at
    fixing things but these always stop short of reaching a mergable
    state.
    
    Therefore this wholesale removal with the hopes of spurring
    people who care to come forward once again and work on a
    coherent replacement.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1326104915.2442.53.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 4f59bf36f0af..09558d1daacd 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -98,7 +98,6 @@ int arch_update_cpu_topology(void);
 				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_SHARE_CPUPOWER			\
-				| 0*SD_POWERSAVINGS_BALANCE		\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
 				| 0*SD_PREFER_SIBLING			\
@@ -134,8 +133,6 @@ int arch_update_cpu_topology(void);
 				| 0*SD_SHARE_CPUPOWER			\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
-				| sd_balance_for_mc_power()		\
-				| sd_power_saving_flags()		\
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
@@ -167,8 +164,6 @@ int arch_update_cpu_topology(void);
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
-				| sd_balance_for_package_power()	\
-				| sd_power_saving_flags()		\
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\

commit c6ae41e7d469f00d9c92a2b2887c7235d121c009
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri May 11 15:35:27 2012 +0800

    x86: replace percpu_xxx funcs with this_cpu_xxx
    
    Since percpu_xxx() serial functions are duplicated with this_cpu_xxx().
    Removing percpu_xxx() definition and replacing them by this_cpu_xxx()
    in code. There is no function change in this patch, just preparation for
    later percpu_xxx serial function removing.
    
    On x86 machine the this_cpu_xxx() serial functions are same as
    __this_cpu_xxx() without no unnecessary premmpt enable/disable.
    
    Thanks for Stephen Rothwell, he found and fixed a i386 build error in
    the patch.
    
    Also thanks for Andrew Morton, he kept updating the patchset in Linus'
    tree.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@gentwo.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e26db031303b..9dc427cdb6ff 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -239,7 +239,7 @@ static inline int cpu_to_node(int cpu)
 #ifndef set_numa_node
 static inline void set_numa_node(int node)
 {
-	percpu_write(numa_node, node);
+	this_cpu_write(numa_node, node);
 }
 #endif
 
@@ -274,7 +274,7 @@ DECLARE_PER_CPU(int, _numa_mem_);
 #ifndef set_numa_mem
 static inline void set_numa_mem(int node)
 {
-	percpu_write(_numa_mem_, node);
+	this_cpu_write(_numa_mem_, node);
 }
 #endif
 

commit cb83b629bae0327cf9f44f096adc38d150ceb913
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 17 15:49:36 2012 +0200

    sched/numa: Rewrite the CONFIG_NUMA sched domain support
    
    The current code groups up to 16 nodes in a level and then puts an
    ALLNODES domain spanning the entire tree on top of that. This doesn't
    reflect the numa topology and esp for the smaller not-fully-connected
    machines out there today this might make a difference.
    
    Therefore, build a proper numa topology based on node_distance().
    
    Since there's no fixed numa layers anymore, the static SD_NODE_INIT
    and SD_ALLNODES_INIT aren't usable anymore, the new code tries to
    construct something similar and scales some values either on the
    number of cpus in the domain and/or the node_distance() ratio.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-sh@vger.kernel.org
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: sparclinux@vger.kernel.org
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Greg Pearson <greg.pearson@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: bob.picco@oracle.com
    Cc: chris.mason@oracle.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-r74n3n8hhuc2ynbrnp3vt954@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e26db031303b..4f59bf36f0af 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -70,7 +70,6 @@ int arch_update_cpu_topology(void);
  * Below are the 3 major initializers used in building sched_domains:
  * SD_SIBLING_INIT, for SMT domains
  * SD_CPU_INIT, for SMP domains
- * SD_NODE_INIT, for NUMA domains
  *
  * Any architecture that cares to do any tuning to these values should do so
  * by defining their own arch-specific initializer in include/asm/topology.h.
@@ -176,48 +175,12 @@ int arch_update_cpu_topology(void);
 }
 #endif
 
-/* sched_domains SD_ALLNODES_INIT for NUMA machines */
-#define SD_ALLNODES_INIT (struct sched_domain) {			\
-	.min_interval		= 64,					\
-	.max_interval		= 64*num_online_cpus(),			\
-	.busy_factor		= 128,					\
-	.imbalance_pct		= 133,					\
-	.cache_nice_tries	= 1,					\
-	.busy_idx		= 3,					\
-	.idle_idx		= 3,					\
-	.flags			= 1*SD_LOAD_BALANCE			\
-				| 1*SD_BALANCE_NEWIDLE			\
-				| 0*SD_BALANCE_EXEC			\
-				| 0*SD_BALANCE_FORK			\
-				| 0*SD_BALANCE_WAKE			\
-				| 0*SD_WAKE_AFFINE			\
-				| 0*SD_SHARE_CPUPOWER			\
-				| 0*SD_POWERSAVINGS_BALANCE		\
-				| 0*SD_SHARE_PKG_RESOURCES		\
-				| 1*SD_SERIALIZE			\
-				| 0*SD_PREFER_SIBLING			\
-				,					\
-	.last_balance		= jiffies,				\
-	.balance_interval	= 64,					\
-}
-
-#ifndef SD_NODES_PER_DOMAIN
-#define SD_NODES_PER_DOMAIN 16
-#endif
-
 #ifdef CONFIG_SCHED_BOOK
 #ifndef SD_BOOK_INIT
 #error Please define an appropriate SD_BOOK_INIT in include/asm/topology.h!!!
 #endif
 #endif /* CONFIG_SCHED_BOOK */
 
-#ifdef CONFIG_NUMA
-#ifndef SD_NODE_INIT
-#error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!
-#endif
-
-#endif /* CONFIG_NUMA */
-
 #ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID
 DECLARE_PER_CPU(int, numa_node);
 

commit 590e4d857153c5d4cf86052cdfd42cf9b0779841
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Jul 24 16:33:13 2011 +0000

    sched: Allow SD_NODES_PER_DOMAIN to be overridden
    
    We want to override the default value of SD_NODES_PER_DOMAIN on ppc64,
    so move it into linux/topology.h.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index fc839bfa7935..e26db031303b 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -201,6 +201,10 @@ int arch_update_cpu_topology(void);
 	.balance_interval	= 64,					\
 }
 
+#ifndef SD_NODES_PER_DOMAIN
+#define SD_NODES_PER_DOMAIN 16
+#endif
+
 #ifdef CONFIG_SCHED_BOOK
 #ifndef SD_BOOK_INIT
 #error Please define an appropriate SD_BOOK_INIT in include/asm/topology.h!!!

commit 32e45ff43eaf5c17f5a82c9ad358d515622c2562
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Jun 15 15:08:20 2011 -0700

    mm: increase RECLAIM_DISTANCE to 30
    
    Recently, Robert Mueller reported (http://lkml.org/lkml/2010/9/12/236)
    that zone_reclaim_mode doesn't work properly on his new NUMA server (Dual
    Xeon E5520 + Intel S5520UR MB).  He is using Cyrus IMAPd and it's built on
    a very traditional single-process model.
    
      * a master process which reads config files and manages the other
        process
      * multiple imapd processes, one per connection
      * multiple pop3d processes, one per connection
      * multiple lmtpd processes, one per connection
      * periodical "cleanup" processes.
    
    There are thousands of independent processes.  The problem is, recent
    Intel motherboard turn on zone_reclaim_mode by default and traditional
    prefork model software don't work well on it.  Unfortunatelly, such models
    are still typical even in the 21st century.  We can't ignore them.
    
    This patch raises the zone_reclaim_mode threshold to 30.  30 doesn't have
    any specific meaning.  but 20 means that one-hop QPI/Hypertransport and
    such relatively cheap 2-4 socket machine are often used for traditional
    servers as above.  The intention is that these machines don't use
    zone_reclaim_mode.
    
    Note: ia64 and Power have arch specific RECLAIM_DISTANCE definitions.
    This patch doesn't change such high-end NUMA machine behavior.
    
    Dave Hansen said:
    
    : I know specifically of pieces of x86 hardware that set the information
    : in the BIOS to '21' *specifically* so they'll get the zone_reclaim_mode
    : behavior which that implies.
    :
    : They've done performance testing and run very large and scary benchmarks
    : to make sure that they _want_ this turned on.  What this means for them
    : is that they'll probably be de-optimized, at least on newer versions of
    : the kernel.
    :
    : If you want to do this for particular systems, maybe _that_'s what we
    : should do.  Have a list of specific configurations that need the
    : defaults overridden either because they're buggy, or they have an
    : unusual hardware configuration not really reflected in the distance
    : table.
    
    And later said:
    
    : The original change in the hardware tables was for the benefit of a
    : benchmark.  Said benchmark isn't going to get run on mainline until the
    : next batch of enterprise distros drops, at which point the hardware where
    : this was done will be irrelevant for the benchmark.  I'm sure any new
    : hardware will just set this distance to another yet arbitrary value to
    : make the kernel do what it wants.  :)
    :
    : Also, when the hardware got _set_ to this initially, I complained.  So, I
    : guess I'm getting my way now, with this patch.  I'm cool with it.
    
    Reported-by: Robert Mueller <robm@fastmail.fm>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Acked-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index b91a40e847d2..fc839bfa7935 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -60,7 +60,7 @@ int arch_update_cpu_topology(void);
  * (in whatever arch specific measurement units returned by node_distance())
  * then switch on zone reclaim on boot.
  */
-#define RECLAIM_DISTANCE 20
+#define RECLAIM_DISTANCE 30
 #endif
 #ifndef PENALTY_FOR_NODE_WITH_CPUS
 #define PENALTY_FOR_NODE_WITH_CPUS	(1)

commit 01a08546af311c065f34727787dd0cc8dc0c216f
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Aug 31 10:28:16 2010 +0200

    sched: Add book scheduling domain
    
    On top of the SMT and MC scheduling domains this adds the BOOK scheduling
    domain. This is useful for NUMA like machines which do not have an interface
    which tells which piece of memory is attached to which node or where the
    hardware performs striping.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100831082844.253053798@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 64e084ff5e5c..b91a40e847d2 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -201,6 +201,12 @@ int arch_update_cpu_topology(void);
 	.balance_interval	= 64,					\
 }
 
+#ifdef CONFIG_SCHED_BOOK
+#ifndef SD_BOOK_INIT
+#error Please define an appropriate SD_BOOK_INIT in include/asm/topology.h!!!
+#endif
+#endif /* CONFIG_SCHED_BOOK */
+
 #ifdef CONFIG_NUMA
 #ifndef SD_NODE_INIT
 #error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!

commit 251060006003b79b788f8ce5a827ee5354a42910
Author: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Date:   Mon Aug 9 17:19:00 2010 -0700

    topology: alternate fix for ia64 tiger_defconfig build breakage
    
    Define stubs for the numa_*_id() generic percpu related functions for
    non-NUMA configurations in <asm-generic/topology.h> where the other
    non-numa stubs live.
    
    Fixes ia64 !NUMA build breakage -- e.g., tiger_defconfig
    
    Back out now unneeded '#ifndef CONFIG_NUMA' guards from ia64 smpboot.c
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index b572e432d2f3..64e084ff5e5c 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -292,10 +292,6 @@ static inline void set_cpu_numa_mem(int cpu, int node)
 
 #else	/* !CONFIG_HAVE_MEMORYLESS_NODES */
 
-static inline void set_numa_mem(int node) {}
-
-static inline void set_cpu_numa_mem(int cpu, int node) {}
-
 #ifndef numa_mem_id
 /* Returns the number of the nearest Node with memory */
 static inline int numa_mem_id(void)

commit 2ec57d448b2e8fcfba539a46701b43f14f037f17
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 29 12:02:01 2010 +1000

    sched: Fix spelling of sibling
    
    No logic changes, only spelling.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: linuxppc-dev@ozlabs.org
    Cc: David Howells <dhowells@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <15249.1277776921@neuling.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index cf57f30d0dcb..b572e432d2f3 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -103,7 +103,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
 				| 0*SD_PREFER_SIBLING			\
-				| arch_sd_sibiling_asym_packing()	\
+				| arch_sd_sibling_asym_packing()	\
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\

commit 532cb4c401e225b084c14d6bd6a2f8ee561de2f1
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 8 14:57:02 2010 +1000

    sched: Add asymmetric group packing option for sibling domain
    
    Check to see if the group is packed in a sched doman.
    
    This is primarily intended to used at the sibling level.  Some cores
    like POWER7 prefer to use lower numbered SMT threads.  In the case of
    POWER7, it can move to lower SMT modes only when higher threads are
    idle.  When in lower SMT modes, the threads will perform better since
    they share less core resources.  Hence when we have idle threads, we
    want them to be the higher ones.
    
    This adds a hook into f_b_g() called check_asym_packing() to check the
    packing.  This packing function is run on idle threads.  It checks to
    see if the busiest CPU in this domain (core in the P7 case) has a
    higher CPU number than what where the packing function is being run
    on.  If it is, calculate the imbalance and return the higher busier
    thread as the busiest group to f_b_g().  Here we are assuming a lower
    CPU number will be equivalent to a lower SMT thread number.
    
    It also creates a new SD_ASYM_PACKING flag to enable this feature at
    any scheduler domain level.
    
    It also creates an arch hook to enable this feature at the sibling
    level.  The default function doesn't enable this feature.
    
    Based heavily on patch from Peter Zijlstra.
    Fixes from Srivatsa Vaddagiri.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <20100608045702.2936CCC897@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index c44df50a05ab..cf57f30d0dcb 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -103,6 +103,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
 				| 0*SD_PREFER_SIBLING			\
+				| arch_sd_sibiling_asym_packing()	\
 				,					\
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\

commit 7aac789885512388a66d47280d7e7777ffba1e59
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Wed May 26 14:45:00 2010 -0700

    numa: introduce numa_mem_id()- effective local memory node id
    
    Introduce numa_mem_id(), based on generic percpu variable infrastructure
    to track "nearest node with memory" for archs that support memoryless
    nodes.
    
    Define API in <linux/topology.h> when CONFIG_HAVE_MEMORYLESS_NODES
    defined, else stubs.  Architectures will define HAVE_MEMORYLESS_NODES
    if/when they support them.
    
    Archs can override definitions of:
    
    numa_mem_id() - returns node number of "local memory" node
    set_numa_mem() - initialize [this cpus'] per cpu variable 'numa_mem'
    cpu_to_mem()  - return numa_mem for specified cpu; may be used as lvalue
    
    Generic initialization of 'numa_mem' occurs in __build_all_zonelists().
    This will initialize the boot cpu at boot time, and all cpus on change of
    numa_zonelist_order, or when node or memory hot-plug requires zonelist
    rebuild.  Archs that support memoryless nodes will need to initialize
    'numa_mem' for secondary cpus as they're brought on-line.
    
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 2e5518f46571..c44df50a05ab 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -251,6 +251,67 @@ static inline int numa_node_id(void)
 
 #endif	/* [!]CONFIG_USE_PERCPU_NUMA_NODE_ID */
 
+#ifdef CONFIG_HAVE_MEMORYLESS_NODES
+
+/*
+ * N.B., Do NOT reference the '_numa_mem_' per cpu variable directly.
+ * It will not be defined when CONFIG_HAVE_MEMORYLESS_NODES is not defined.
+ * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem().
+ */
+DECLARE_PER_CPU(int, _numa_mem_);
+
+#ifndef set_numa_mem
+static inline void set_numa_mem(int node)
+{
+	percpu_write(_numa_mem_, node);
+}
+#endif
+
+#ifndef numa_mem_id
+/* Returns the number of the nearest Node with memory */
+static inline int numa_mem_id(void)
+{
+	return __this_cpu_read(_numa_mem_);
+}
+#endif
+
+#ifndef cpu_to_mem
+static inline int cpu_to_mem(int cpu)
+{
+	return per_cpu(_numa_mem_, cpu);
+}
+#endif
+
+#ifndef set_cpu_numa_mem
+static inline void set_cpu_numa_mem(int cpu, int node)
+{
+	per_cpu(_numa_mem_, cpu) = node;
+}
+#endif
+
+#else	/* !CONFIG_HAVE_MEMORYLESS_NODES */
+
+static inline void set_numa_mem(int node) {}
+
+static inline void set_cpu_numa_mem(int cpu, int node) {}
+
+#ifndef numa_mem_id
+/* Returns the number of the nearest Node with memory */
+static inline int numa_mem_id(void)
+{
+	return numa_node_id();
+}
+#endif
+
+#ifndef cpu_to_mem
+static inline int cpu_to_mem(int cpu)
+{
+	return cpu_to_node(cpu);
+}
+#endif
+
+#endif	/* [!]CONFIG_HAVE_MEMORYLESS_NODES */
+
 #ifndef topology_physical_package_id
 #define topology_physical_package_id(cpu)	((void)(cpu), -1)
 #endif

commit 7281201922a0063fa60804ce39c277fc98142a47
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Wed May 26 14:44:56 2010 -0700

    numa: add generic percpu var numa_node_id() implementation
    
    Rework the generic version of the numa_node_id() function to use the new
    generic percpu variable infrastructure.
    
    Guard the new implementation with a new config option:
    
            CONFIG_USE_PERCPU_NUMA_NODE_ID.
    
    Archs which support this new implemention will default this option to 'y'
    when NUMA is configured.  This config option could be removed if/when all
    archs switch over to the generic percpu implementation of numa_node_id().
    Arch support involves:
    
      1) converting any existing per cpu variable implementations to use
         this implementation.  x86_64 is an instance of such an arch.
      2) archs that don't use a per cpu variable for numa_node_id() will
         need to initialize the new per cpu variable "numa_node" as cpus
         are brought on-line.  ia64 is an example.
      3) Defining USE_PERCPU_NUMA_NODE_ID in arch dependent Kconfig--e.g.,
         when NUMA is configured.  This is required because I have
         retained the old implementation by default to allow archs to
         be modified incrementally, as desired.
    
    Subsequent patches will convert x86_64 and ia64 to use this implemenation.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 5b81156780b1..2e5518f46571 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -31,6 +31,7 @@
 #include <linux/bitops.h>
 #include <linux/mmzone.h>
 #include <linux/smp.h>
+#include <linux/percpu.h>
 #include <asm/topology.h>
 
 #ifndef node_has_online_mem
@@ -203,8 +204,53 @@ int arch_update_cpu_topology(void);
 #ifndef SD_NODE_INIT
 #error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!
 #endif
+
 #endif /* CONFIG_NUMA */
 
+#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID
+DECLARE_PER_CPU(int, numa_node);
+
+#ifndef numa_node_id
+/* Returns the number of the current Node. */
+static inline int numa_node_id(void)
+{
+	return __this_cpu_read(numa_node);
+}
+#endif
+
+#ifndef cpu_to_node
+static inline int cpu_to_node(int cpu)
+{
+	return per_cpu(numa_node, cpu);
+}
+#endif
+
+#ifndef set_numa_node
+static inline void set_numa_node(int node)
+{
+	percpu_write(numa_node, node);
+}
+#endif
+
+#ifndef set_cpu_numa_node
+static inline void set_cpu_numa_node(int cpu, int node)
+{
+	per_cpu(numa_node, cpu) = node;
+}
+#endif
+
+#else	/* !CONFIG_USE_PERCPU_NUMA_NODE_ID */
+
+/* Returns the number of the current Node. */
+#ifndef numa_node_id
+static inline int numa_node_id(void)
+{
+	return cpu_to_node(raw_smp_processor_id());
+}
+#endif
+
+#endif	/* [!]CONFIG_USE_PERCPU_NUMA_NODE_ID */
+
 #ifndef topology_physical_package_id
 #define topology_physical_package_id(cpu)	((void)(cpu), -1)
 #endif
@@ -218,9 +264,4 @@ int arch_update_cpu_topology(void);
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
 #endif
 
-/* Returns the number of the current Node. */
-#ifndef numa_node_id
-#define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
-#endif
-
 #endif /* _LINUX_TOPOLOGY_H */

commit 50b926e439620c469565e8be0f28be78f5fca1ce
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Jan 4 14:44:56 2010 +0100

    sched: Fix vmark regression on big machines
    
    SD_PREFER_SIBLING is set at the CPU domain level if power saving isn't
    enabled, leading to many cache misses on large machines as we traverse
    looking for an idle shared cache to wake to.  Change the enabler of
    select_idle_sibling() to SD_SHARE_PKG_RESOURCES, and enable same at the
    sibling domain level.
    
    Reported-by: Lin Ming <ming.m.lin@intel.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1262612696.15495.15.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 57e63579bfdd..5b81156780b1 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -99,7 +99,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_SHARE_CPUPOWER			\
 				| 0*SD_POWERSAVINGS_BALANCE		\
-				| 0*SD_SHARE_PKG_RESOURCES		\
+				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
 				| 0*SD_PREFER_SIBLING			\
 				,					\

commit 799e2205ec65e174f752b558c62a92c4752df313
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Oct 9 12:16:40 2009 +0200

    sched: Disable SD_PREFER_LOCAL for MC/CPU domains
    
    Yanmin reported that both tbench and hackbench were significantly
    hurt by trying to keep tasks local on these domains, esp on small
    cache machines.
    
    So disable it in order to promote spreading outside of the cache
    domains.
    
    Reported-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <1255083400.8802.15.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index fc0bf3edeb67..57e63579bfdd 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -129,7 +129,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 1*SD_PREFER_LOCAL			\
+				| 0*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
@@ -162,7 +162,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 1*SD_PREFER_LOCAL			\
+				| 0*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\

commit 6f401420e2822c24c36e6e1c657f6e7f7f777a93
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:40 2009 -0600

    cpumask: remove obsolete topology_core_siblings and topology_thread_siblings: core
    
    There were replaced by topology_core_cpumask and topology_thread_cpumask.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 809b26c07090..fc0bf3edeb67 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -211,12 +211,6 @@ int arch_update_cpu_topology(void);
 #ifndef topology_core_id
 #define topology_core_id(cpu)			((void)(cpu), 0)
 #endif
-#ifndef topology_thread_siblings
-#define topology_thread_siblings(cpu)		cpumask_of_cpu(cpu)
-#endif
-#ifndef topology_core_siblings
-#define topology_core_siblings(cpu)		cpumask_of_cpu(cpu)
-#endif
 #ifndef topology_thread_cpumask
 #define topology_thread_cpumask(cpu)		cpumask_of(cpu)
 #endif

commit 182a85f8a119c789610a9d464f4129ded9f3c107
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 16 13:24:49 2009 +0200

    sched: Disable wakeup balancing
    
    Sysbench thinks SD_BALANCE_WAKE is too agressive and kbuild doesn't
    really mind too much, SD_BALANCE_NEWIDLE picks up most of the
    slack.
    
    On a dual socket, quad core, dual thread nehalem system:
    
    sysbench (--num_threads=16):
    
     SD_BALANCE_WAKE-: 13982 tx/s
     SD_BALANCE_WAKE+: 15688 tx/s
    
    kbuild (-j16):
    
     SD_BALANCE_WAKE-: 47.648295846  seconds time elapsed   ( +-   0.312% )
     SD_BALANCE_WAKE+: 47.608607360  seconds time elapsed   ( +-   0.026% )
    
    (same within noise)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index a6614b0242a9..809b26c07090 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -95,7 +95,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 1*SD_BALANCE_WAKE			\
+				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_SHARE_CPUPOWER			\
 				| 0*SD_POWERSAVINGS_BALANCE		\
@@ -127,7 +127,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 1*SD_BALANCE_WAKE			\
+				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
@@ -160,7 +160,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 1*SD_BALANCE_WAKE			\
+				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\

commit 59abf02644c45f1591e1374ee7bb45dc757fcb88
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 16 08:28:30 2009 +0200

    sched: Add SD_PREFER_LOCAL
    
    And turn it on for NUMA and MC domains. This improves
    locality in balancing decisions by keeping up to
    capacity amount of tasks local before looking for idle
    CPUs. (and twice the capacity if SD_POWERSAVINGS_BALANCE
    is set.)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 936ab2b37683..a6614b0242a9 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -129,6 +129,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 1*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
+				| 1*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
@@ -161,6 +162,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_FORK			\
 				| 1*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
+				| 1*SD_PREFER_LOCAL			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\

commit b8a543ea5a5896830a9969bacfd047f9d15940b2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 15 15:22:03 2009 +0200

    sched: Reduce forkexec_idx
    
    If we're looking to place a new task, we might as well find the
    idlest position _now_, not 1 tick ago.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 4298745615a5..936ab2b37683 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -121,7 +121,7 @@ int arch_update_cpu_topology(void);
 	.cache_nice_tries	= 1,					\
 	.busy_idx		= 2,					\
 	.wake_idx		= 0,					\
-	.forkexec_idx		= 1,					\
+	.forkexec_idx		= 0,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\
 				| 1*SD_BALANCE_NEWIDLE			\
@@ -153,7 +153,7 @@ int arch_update_cpu_topology(void);
 	.idle_idx		= 1,					\
 	.newidle_idx		= 0,					\
 	.wake_idx		= 0,					\
-	.forkexec_idx		= 1,					\
+	.forkexec_idx		= 0,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\
 				| 1*SD_BALANCE_NEWIDLE			\

commit 0ec9fab3d186d9cbb00c0f694d4a260d07c198d9
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Sep 15 15:07:03 2009 +0200

    sched: Improve latencies and throughput
    
    Make the idle balancer more agressive, to improve a
    x264 encoding workload provided by Jason Garrett-Glaser:
    
     NEXT_BUDDY NO_LB_BIAS
     encoded 600 frames, 252.82 fps, 22096.60 kb/s
     encoded 600 frames, 250.69 fps, 22096.60 kb/s
     encoded 600 frames, 245.76 fps, 22096.60 kb/s
    
     NO_NEXT_BUDDY LB_BIAS
     encoded 600 frames, 344.44 fps, 22096.60 kb/s
     encoded 600 frames, 346.66 fps, 22096.60 kb/s
     encoded 600 frames, 352.59 fps, 22096.60 kb/s
    
     NO_NEXT_BUDDY NO_LB_BIAS
     encoded 600 frames, 425.75 fps, 22096.60 kb/s
     encoded 600 frames, 425.45 fps, 22096.60 kb/s
     encoded 600 frames, 422.49 fps, 22096.60 kb/s
    
    Peter pointed out that this is better done via newidle_idx,
    not via LB_BIAS, newidle balancing should look for where
    there is load _now_, not where there was load 2 ticks ago.
    
    Worst-case latencies are improved as well as no buddies
    means less vruntime spread. (as per prior lkml discussions)
    
    This change improves kbuild-peak parallelism as well.
    
    Reported-by: Jason Garrett-Glaser <darkshikari@gmail.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1253011667.9128.16.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index c87edcd87967..4298745615a5 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -151,7 +151,7 @@ int arch_update_cpu_topology(void);
 	.cache_nice_tries	= 1,					\
 	.busy_idx		= 2,					\
 	.idle_idx		= 1,					\
-	.newidle_idx		= 2,					\
+	.newidle_idx		= 0,					\
 	.wake_idx		= 0,					\
 	.forkexec_idx		= 1,					\
 									\

commit 6bd7821f905a8d6c471f0d6675f5cb7ea448d791
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Sep 11 18:42:15 2009 +0200

    sched: Fix some domain tunings
    
    CPU level should have WAKE_AFFINE, whereas ALLNODES is dubious.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index fef57040a4e2..c87edcd87967 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -160,7 +160,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
 				| 1*SD_BALANCE_WAKE			\
-				| 0*SD_WAKE_AFFINE			\
+				| 1*SD_WAKE_AFFINE			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
@@ -186,7 +186,7 @@ int arch_update_cpu_topology(void);
 				| 0*SD_BALANCE_EXEC			\
 				| 0*SD_BALANCE_FORK			\
 				| 0*SD_BALANCE_WAKE			\
-				| 1*SD_WAKE_AFFINE			\
+				| 0*SD_WAKE_AFFINE			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_POWERSAVINGS_BALANCE		\
 				| 0*SD_SHARE_PKG_RESOURCES		\

commit 78e7ed53c9f42f04f9401ada6f7047db60781676
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 3 13:16:51 2009 +0200

    sched: Tweak wake_idx
    
    When merging select_task_rq_fair() and sched_balance_self() we lost
    the use of wake_idx, restore that and set them to 0 to make wake
    balancing more aggressive.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 6a8cd15555bb..fef57040a4e2 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -120,7 +120,7 @@ int arch_update_cpu_topology(void);
 	.imbalance_pct		= 125,					\
 	.cache_nice_tries	= 1,					\
 	.busy_idx		= 2,					\
-	.wake_idx		= 1,					\
+	.wake_idx		= 0,					\
 	.forkexec_idx		= 1,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\
@@ -152,7 +152,7 @@ int arch_update_cpu_topology(void);
 	.busy_idx		= 2,					\
 	.idle_idx		= 1,					\
 	.newidle_idx		= 2,					\
-	.wake_idx		= 1,					\
+	.wake_idx		= 0,					\
 	.forkexec_idx		= 1,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\

commit c88d5910890ad35af283344417891344604f0438
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 10 13:50:02 2009 +0200

    sched: Merge select_task_rq_fair() and sched_balance_self()
    
    The problem with wake_idle() is that is doesn't respect things like
    cpu_power, which means it doesn't deal well with SMT nor the recent
    RT interaction.
    
    To cure this, it needs to do what sched_balance_self() does, which
    leads to the possibility of merging select_task_rq_fair() and
    sched_balance_self().
    
    Modify sched_balance_self() to:
    
      - update_shares() when walking up the domain tree,
        (it only called it for the top domain, but it should
         have done this anyway), which allows us to remove
        this ugly bit from try_to_wake_up().
    
      - do wake_affine() on the smallest domain that contains
        both this (the waking) and the prev (the wakee) cpu for
        WAKE invocations.
    
    Then use the top-down balance steps it had to replace wake_idle().
    
    This leads to the dissapearance of SD_WAKE_BALANCE and
    SD_WAKE_IDLE_FAR, with SD_WAKE_IDLE replaced with SD_BALANCE_WAKE.
    
    SD_WAKE_AFFINE needs SD_BALANCE_WAKE to be effective.
    
    Touch all topology bits to replace the old with new SD flags --
    platforms might need re-tuning, enabling SD_BALANCE_WAKE
    conditionally on a NUMA distance seems like a good additional
    feature, magny-core and small nehalem systems would want this
    enabled, systems with slow interconnects would not.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 85e8cf7d393c..6a8cd15555bb 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -95,14 +95,12 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 0*SD_WAKE_IDLE			\
+				| 1*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 1*SD_WAKE_BALANCE			\
 				| 1*SD_SHARE_CPUPOWER			\
 				| 0*SD_POWERSAVINGS_BALANCE		\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
-				| 0*SD_WAKE_IDLE_FAR			\
 				| 0*SD_PREFER_SIBLING			\
 				,					\
 	.last_balance		= jiffies,				\
@@ -129,13 +127,11 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 1*SD_WAKE_IDLE			\
+				| 1*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 1*SD_WAKE_BALANCE			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 1*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
-				| 0*SD_WAKE_IDLE_FAR			\
 				| sd_balance_for_mc_power()		\
 				| sd_power_saving_flags()		\
 				,					\
@@ -163,13 +159,11 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 1*SD_WAKE_IDLE			\
+				| 1*SD_BALANCE_WAKE			\
 				| 0*SD_WAKE_AFFINE			\
-				| 1*SD_WAKE_BALANCE			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 0*SD_SERIALIZE			\
-				| 0*SD_WAKE_IDLE_FAR			\
 				| sd_balance_for_package_power()	\
 				| sd_power_saving_flags()		\
 				,					\
@@ -191,14 +185,12 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 0*SD_BALANCE_EXEC			\
 				| 0*SD_BALANCE_FORK			\
-				| 0*SD_WAKE_IDLE			\
+				| 0*SD_BALANCE_WAKE			\
 				| 1*SD_WAKE_AFFINE			\
-				| 0*SD_WAKE_BALANCE			\
 				| 0*SD_SHARE_CPUPOWER			\
 				| 0*SD_POWERSAVINGS_BALANCE		\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 1*SD_SERIALIZE			\
-				| 1*SD_WAKE_IDLE_FAR			\
 				| 0*SD_PREFER_SIBLING			\
 				,					\
 	.last_balance		= jiffies,				\

commit a8fae3ec5f118dc92517dcbed3ecf69ddb641d0f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 7 18:32:32 2009 +0200

    sched: enable SD_WAKE_IDLE
    
    Now that SD_WAKE_IDLE doesn't make pipe-test suck anymore,
    enable it by default for MC, CPU and NUMA domains.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 66774fddec9b..85e8cf7d393c 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -129,7 +129,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 0*SD_WAKE_IDLE			\
+				| 1*SD_WAKE_IDLE			\
 				| 1*SD_WAKE_AFFINE			\
 				| 1*SD_WAKE_BALANCE			\
 				| 0*SD_SHARE_CPUPOWER			\
@@ -163,7 +163,7 @@ int arch_update_cpu_topology(void);
 				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
-				| 0*SD_WAKE_IDLE			\
+				| 1*SD_WAKE_IDLE			\
 				| 0*SD_WAKE_AFFINE			\
 				| 1*SD_WAKE_BALANCE			\
 				| 0*SD_SHARE_CPUPOWER			\
@@ -198,7 +198,7 @@ int arch_update_cpu_topology(void);
 				| 0*SD_POWERSAVINGS_BALANCE		\
 				| 0*SD_SHARE_PKG_RESOURCES		\
 				| 1*SD_SERIALIZE			\
-				| 0*SD_WAKE_IDLE_FAR			\
+				| 1*SD_WAKE_IDLE_FAR			\
 				| 0*SD_PREFER_SIBLING			\
 				,					\
 	.last_balance		= jiffies,				\

commit 840a0653100dbde599ae8ddf83fa214dfa5fd1aa
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 4 11:32:54 2009 +0200

    sched: Turn on SD_BALANCE_NEWIDLE
    
    Start the re-tuning of the balancer by turning on newidle.
    
    It improves hackbench performance and parallelism on a 4x4 box.
    The "perf stat --repeat 10" measurements give us:
    
      domain0             domain1
      .......................................
     -SD_BALANCE_NEWIDLE -SD_BALANCE_NEWIDLE:
       2041.273208  task-clock-msecs         #      9.354 CPUs    ( +-   0.363% )
    
     +SD_BALANCE_NEWIDLE -SD_BALANCE_NEWIDLE:
       2086.326925  task-clock-msecs         #     11.934 CPUs    ( +-   0.301% )
    
     +SD_BALANCE_NEWIDLE +SD_BALANCE_NEWIDLE:
       2115.289791  task-clock-msecs         #     12.158 CPUs    ( +-   0.263% )
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index fe2c0329f82f..66774fddec9b 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -126,7 +126,7 @@ int arch_update_cpu_topology(void);
 	.forkexec_idx		= 1,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\
-				| 0*SD_BALANCE_NEWIDLE			\
+				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_WAKE_IDLE			\
@@ -160,7 +160,7 @@ int arch_update_cpu_topology(void);
 	.forkexec_idx		= 1,					\
 									\
 	.flags			= 1*SD_LOAD_BALANCE			\
-				| 0*SD_BALANCE_NEWIDLE			\
+				| 1*SD_BALANCE_NEWIDLE			\
 				| 1*SD_BALANCE_EXEC			\
 				| 1*SD_BALANCE_FORK			\
 				| 0*SD_WAKE_IDLE			\

commit 47734f89be0614b5acbd6a532390f9c72f019648
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 4 11:21:24 2009 +0200

    sched: Clean up topology.h
    
    Re-organize the flag settings so that it's visible at a glance
    which sched-domains flags are set and which not.
    
    With the new balancer code we'll need to re-tune these details
    anyway, so make it cleaner to make fewer mistakes down the
    road ;-)
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 6203ae5067ce..fe2c0329f82f 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -85,21 +85,29 @@ int arch_update_cpu_topology(void);
 #define ARCH_HAS_SCHED_WAKE_IDLE
 /* Common values for SMT siblings */
 #ifndef SD_SIBLING_INIT
-#define SD_SIBLING_INIT (struct sched_domain) {		\
-	.min_interval		= 1,			\
-	.max_interval		= 2,			\
-	.busy_factor		= 64,			\
-	.imbalance_pct		= 110,			\
-	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_NEWIDLE	\
-				| SD_BALANCE_FORK	\
-				| SD_BALANCE_EXEC	\
-				| SD_WAKE_AFFINE	\
-				| SD_WAKE_BALANCE	\
-				| SD_SHARE_CPUPOWER,	\
-	.last_balance		= jiffies,		\
-	.balance_interval	= 1,			\
-	.smt_gain		= 1178,	/* 15% */	\
+#define SD_SIBLING_INIT (struct sched_domain) {				\
+	.min_interval		= 1,					\
+	.max_interval		= 2,					\
+	.busy_factor		= 64,					\
+	.imbalance_pct		= 110,					\
+									\
+	.flags			= 1*SD_LOAD_BALANCE			\
+				| 1*SD_BALANCE_NEWIDLE			\
+				| 1*SD_BALANCE_EXEC			\
+				| 1*SD_BALANCE_FORK			\
+				| 0*SD_WAKE_IDLE			\
+				| 1*SD_WAKE_AFFINE			\
+				| 1*SD_WAKE_BALANCE			\
+				| 1*SD_SHARE_CPUPOWER			\
+				| 0*SD_POWERSAVINGS_BALANCE		\
+				| 0*SD_SHARE_PKG_RESOURCES		\
+				| 0*SD_SERIALIZE			\
+				| 0*SD_WAKE_IDLE_FAR			\
+				| 0*SD_PREFER_SIBLING			\
+				,					\
+	.last_balance		= jiffies,				\
+	.balance_interval	= 1,					\
+	.smt_gain		= 1178,	/* 15% */			\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */
@@ -107,69 +115,94 @@ int arch_update_cpu_topology(void);
 #ifdef CONFIG_SCHED_MC
 /* Common values for MC siblings. for now mostly derived from SD_CPU_INIT */
 #ifndef SD_MC_INIT
-#define SD_MC_INIT (struct sched_domain) {		\
-	.min_interval		= 1,			\
-	.max_interval		= 4,			\
-	.busy_factor		= 64,			\
-	.imbalance_pct		= 125,			\
-	.cache_nice_tries	= 1,			\
-	.busy_idx		= 2,			\
-	.wake_idx		= 1,			\
-	.forkexec_idx		= 1,			\
-	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_FORK	\
-				| SD_BALANCE_EXEC	\
-				| SD_WAKE_AFFINE	\
-				| SD_WAKE_BALANCE	\
-				| SD_SHARE_PKG_RESOURCES\
-				| sd_balance_for_mc_power()\
-				| sd_power_saving_flags(),\
-	.last_balance		= jiffies,		\
-	.balance_interval	= 1,			\
+#define SD_MC_INIT (struct sched_domain) {				\
+	.min_interval		= 1,					\
+	.max_interval		= 4,					\
+	.busy_factor		= 64,					\
+	.imbalance_pct		= 125,					\
+	.cache_nice_tries	= 1,					\
+	.busy_idx		= 2,					\
+	.wake_idx		= 1,					\
+	.forkexec_idx		= 1,					\
+									\
+	.flags			= 1*SD_LOAD_BALANCE			\
+				| 0*SD_BALANCE_NEWIDLE			\
+				| 1*SD_BALANCE_EXEC			\
+				| 1*SD_BALANCE_FORK			\
+				| 0*SD_WAKE_IDLE			\
+				| 1*SD_WAKE_AFFINE			\
+				| 1*SD_WAKE_BALANCE			\
+				| 0*SD_SHARE_CPUPOWER			\
+				| 1*SD_SHARE_PKG_RESOURCES		\
+				| 0*SD_SERIALIZE			\
+				| 0*SD_WAKE_IDLE_FAR			\
+				| sd_balance_for_mc_power()		\
+				| sd_power_saving_flags()		\
+				,					\
+	.last_balance		= jiffies,				\
+	.balance_interval	= 1,					\
 }
 #endif
 #endif /* CONFIG_SCHED_MC */
 
 /* Common values for CPUs */
 #ifndef SD_CPU_INIT
-#define SD_CPU_INIT (struct sched_domain) {		\
-	.min_interval		= 1,			\
-	.max_interval		= 4,			\
-	.busy_factor		= 64,			\
-	.imbalance_pct		= 125,			\
-	.cache_nice_tries	= 1,			\
-	.busy_idx		= 2,			\
-	.idle_idx		= 1,			\
-	.newidle_idx		= 2,			\
-	.wake_idx		= 1,			\
-	.forkexec_idx		= 1,			\
-	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_EXEC	\
-				| SD_BALANCE_FORK	\
-				| SD_WAKE_AFFINE	\
-				| SD_WAKE_BALANCE	\
-				| sd_balance_for_package_power()\
-				| sd_power_saving_flags(),\
-	.last_balance		= jiffies,		\
-	.balance_interval	= 1,			\
+#define SD_CPU_INIT (struct sched_domain) {				\
+	.min_interval		= 1,					\
+	.max_interval		= 4,					\
+	.busy_factor		= 64,					\
+	.imbalance_pct		= 125,					\
+	.cache_nice_tries	= 1,					\
+	.busy_idx		= 2,					\
+	.idle_idx		= 1,					\
+	.newidle_idx		= 2,					\
+	.wake_idx		= 1,					\
+	.forkexec_idx		= 1,					\
+									\
+	.flags			= 1*SD_LOAD_BALANCE			\
+				| 0*SD_BALANCE_NEWIDLE			\
+				| 1*SD_BALANCE_EXEC			\
+				| 1*SD_BALANCE_FORK			\
+				| 0*SD_WAKE_IDLE			\
+				| 0*SD_WAKE_AFFINE			\
+				| 1*SD_WAKE_BALANCE			\
+				| 0*SD_SHARE_CPUPOWER			\
+				| 0*SD_SHARE_PKG_RESOURCES		\
+				| 0*SD_SERIALIZE			\
+				| 0*SD_WAKE_IDLE_FAR			\
+				| sd_balance_for_package_power()	\
+				| sd_power_saving_flags()		\
+				,					\
+	.last_balance		= jiffies,				\
+	.balance_interval	= 1,					\
 }
 #endif
 
 /* sched_domains SD_ALLNODES_INIT for NUMA machines */
-#define SD_ALLNODES_INIT (struct sched_domain) {	\
-	.min_interval		= 64,			\
-	.max_interval		= 64*num_online_cpus(),	\
-	.busy_factor		= 128,			\
-	.imbalance_pct		= 133,			\
-	.cache_nice_tries	= 1,			\
-	.busy_idx		= 3,			\
-	.idle_idx		= 3,			\
-	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_NEWIDLE	\
-				| SD_WAKE_AFFINE	\
-				| SD_SERIALIZE,		\
-	.last_balance		= jiffies,		\
-	.balance_interval	= 64,			\
+#define SD_ALLNODES_INIT (struct sched_domain) {			\
+	.min_interval		= 64,					\
+	.max_interval		= 64*num_online_cpus(),			\
+	.busy_factor		= 128,					\
+	.imbalance_pct		= 133,					\
+	.cache_nice_tries	= 1,					\
+	.busy_idx		= 3,					\
+	.idle_idx		= 3,					\
+	.flags			= 1*SD_LOAD_BALANCE			\
+				| 1*SD_BALANCE_NEWIDLE			\
+				| 0*SD_BALANCE_EXEC			\
+				| 0*SD_BALANCE_FORK			\
+				| 0*SD_WAKE_IDLE			\
+				| 1*SD_WAKE_AFFINE			\
+				| 0*SD_WAKE_BALANCE			\
+				| 0*SD_SHARE_CPUPOWER			\
+				| 0*SD_POWERSAVINGS_BALANCE		\
+				| 0*SD_SHARE_PKG_RESOURCES		\
+				| 1*SD_SERIALIZE			\
+				| 0*SD_WAKE_IDLE_FAR			\
+				| 0*SD_PREFER_SIBLING			\
+				,					\
+	.last_balance		= jiffies,				\
+	.balance_interval	= 64,					\
 }
 
 #ifdef CONFIG_NUMA

commit a52bfd73589eaf88d9c95ad2c1de0b38a6b27972
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 1 10:34:35 2009 +0200

    sched: Add smt_gain
    
    The idea is that multi-threading a core yields more work
    capacity than a single thread, provide a way to express a
    static gain for threads.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <20090901083826.073345955@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 7402c1a27c4f..6203ae5067ce 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -99,6 +99,7 @@ int arch_update_cpu_topology(void);
 				| SD_SHARE_CPUPOWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
+	.smt_gain		= 1178,	/* 15% */	\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */

commit 082edb7bf443eb8eda15b482d16ad9dd8137ad24
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 23:43:37 2009 +1030

    numa, cpumask: move numa_node_id default implementation to topology.h
    
    Impact: cleanup, potential bugfix
    
    Not sure what changed to expose this, but clearly that numa_node_id()
    doesn't belong in mmzone.h (the inline in gfp.h is probably overkill, too).
    
    In file included from include/linux/topology.h:34,
                     from arch/x86/mm/numa.c:2:
    /home/rusty/patches-cpumask/linux-2.6/arch/x86/include/asm/topology.h:64:1: warning: "numa_node_id" redefined
    In file included from include/linux/topology.h:32,
                     from arch/x86/mm/numa.c:2:
    include/linux/mmzone.h:770:1: warning: this is the location of the previous definition
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903132343.37661.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 16b7d6896ce9..7402c1a27c4f 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -196,4 +196,9 @@ int arch_update_cpu_topology(void);
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
 #endif
 
+/* Returns the number of the current Node. */
+#ifndef numa_node_id
+#define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
+#endif
+
 #endif /* _LINUX_TOPOLOGY_H */

commit a70f730282019f487aa33a84e5ac9a5e89c5abd0
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 14:49:46 2009 +1030

    cpumask: replace node_to_cpumask with cpumask_of_node.
    
    Impact: cleanup
    
    node_to_cpumask (and the blecherous node_to_cpumask_ptr which
    contained a declaration) are replaced now everyone implements
    cpumask_of_node.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index a16b9e06f2e5..16b7d6896ce9 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -38,11 +38,7 @@
 #endif
 
 #ifndef nr_cpus_node
-#define nr_cpus_node(node)				\
-	({						\
-		node_to_cpumask_ptr(__tmp__, node);	\
-		cpus_weight(*__tmp__);			\
-	})
+#define nr_cpus_node(node) cpumask_weight(cpumask_of_node(node))
 #endif
 
 #define for_each_node_with_cpus(node)			\

commit fbd59a8d1f7cf325fdb6828659f1fb76631e87b3
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Jan 10 21:58:08 2009 -0800

    cpumask: Use topology_core_cpumask()/topology_thread_cpumask()
    
    Impact: reduce stack usage, use new cpumask API.
    
    This actually uses topology_core_cpumask() and
    topology_thread_cpumask(), removing the only users of
    topology_core_siblings() and topology_thread_siblings()
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: linux-net-drivers@solarflare.com

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e632d29f0544..a16b9e06f2e5 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -193,5 +193,11 @@ int arch_update_cpu_topology(void);
 #ifndef topology_core_siblings
 #define topology_core_siblings(cpu)		cpumask_of_cpu(cpu)
 #endif
+#ifndef topology_thread_cpumask
+#define topology_thread_cpumask(cpu)		cpumask_of(cpu)
+#endif
+#ifndef topology_core_cpumask
+#define topology_core_cpumask(cpu)		cpumask_of(cpu)
+#endif
 
 #endif /* _LINUX_TOPOLOGY_H */

commit 100fdaee70ebf5f31b9451fbc01300c627091328
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Thu Dec 18 23:26:47 2008 +0530

    sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
    
    Impact: change task balancing to save power more agressively
    
    Add SD_BALANCE_NEWIDLE flag at MC level and CPU level
    if sched_mc is set.  This helps power savings and
    will not affect performance when sched_mc=0
    
    Ingo and Mike Galbraith have optimised the SD flags by
    removing SD_BALANCE_NEWIDLE at MC and CPU level.  This
    helps performance but hurts power savings since this
    slows down task consolidation by reducing the number
    of times load_balance is run.
    
        sched: fine-tune SD_MC_INIT
            commit 14800984706bf6936bbec5187f736e928be5c218
            Author: Mike Galbraith <efault@gmx.de>
            Date:   Fri Nov 7 15:26:50 2008 +0100
    
        sched: re-tune balancing -- revert
            commit 9fcd18c9e63e325dbd2b4c726623f760788d5aa8
            Author: Ingo Molnar <mingo@elte.hu>
            Date:   Wed Nov 5 16:52:08 2008 +0100
    
    This patch selectively enables SD_BALANCE_NEWIDLE flag
    only when sched_mc is set to 1 or 2.  This helps power savings
    by task consolidation and also does not hurt performance at
    sched_mc=0 where all power saving optimisations are turned off.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 0ce7c0dac06c..e632d29f0544 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -125,7 +125,8 @@ int arch_update_cpu_topology(void);
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_BALANCE	\
 				| SD_SHARE_PKG_RESOURCES\
-				| sd_balance_for_mc_power(),\
+				| sd_balance_for_mc_power()\
+				| sd_power_saving_flags(),\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 }
@@ -150,7 +151,8 @@ int arch_update_cpu_topology(void);
 				| SD_BALANCE_FORK	\
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_BALANCE	\
-				| sd_balance_for_package_power(),\
+				| sd_balance_for_package_power()\
+				| sd_power_saving_flags(),\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 }

commit 716707b29906e1d8d190defe3d646610b097a861
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Thu Dec 18 23:26:02 2008 +0530

    sched: convert BALANCE_FOR_xx_POWER to inline functions
    
    Impact: cleanup
    
    BALANCE_FOR_MC_POWER and similar macros defined in sched.h are
    not constants and have various condition checks and significant
    amount of code that is not suitable to be contain in a macro.
    Also there could be side effects on the expressions passed to
    some of them like test_sd_parent().
    
    This patch converts all complex macros related to power savings
    balance to inline functions.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 0c5b5ac36d8e..0ce7c0dac06c 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -125,7 +125,7 @@ int arch_update_cpu_topology(void);
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_BALANCE	\
 				| SD_SHARE_PKG_RESOURCES\
-				| BALANCE_FOR_MC_POWER,	\
+				| sd_balance_for_mc_power(),\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 }
@@ -150,7 +150,7 @@ int arch_update_cpu_topology(void);
 				| SD_BALANCE_FORK	\
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_BALANCE	\
-				| BALANCE_FOR_PKG_POWER,\
+				| sd_balance_for_package_power(),\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 }

commit ee79d1bdb6a10499e53f80b1e8d14110215178ba
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Dec 9 18:49:50 2008 +0100

    sched: let arch_update_cpu_topology indicate if topology changed
    
    Change arch_update_cpu_topology so it returns 1 if the cpu topology changed
    and 0 if it didn't change. This will be useful for the next patch which adds
    a call to this function in partition_sched_domains.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 117f1b7405cf..0c5b5ac36d8e 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -49,7 +49,7 @@
 	for_each_online_node(node)			\
 		if (nr_cpus_node(node))
 
-void arch_update_cpu_topology(void);
+int arch_update_cpu_topology(void);
 
 /* Conform to ACPI 2.0 SLIT distance definitions */
 #define LOCAL_DISTANCE		10

commit 52c642f33b14bfa1b00ef2b68296effb34a573f3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Nov 7 16:09:23 2008 +0100

    sched: fine-tune SD_SIBLING_INIT
    
    fine-tune the HT sched-domains parameters as well.
    
    On a HT capable box, this increases lat_ctx performance from 23.87
    usecs to 1.49 usecs:
    
     # before
    
     $ ./lat_ctx -s 0 2
    
       "size=0k ovr=1.89
        2 23.87
    
     # after
    
     $ ./lat_ctx -s 0 2
    
       "size=0k ovr=1.84
         2 1.49
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index a8d840595b7e..117f1b7405cf 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -99,7 +99,7 @@ void arch_update_cpu_topology(void);
 				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
+				| SD_WAKE_BALANCE	\
 				| SD_SHARE_CPUPOWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\

commit 14800984706bf6936bbec5187f736e928be5c218
Author: Mike Galbraith <efault@gmx.de>
Date:   Fri Nov 7 15:26:50 2008 +0100

    sched: fine-tune SD_MC_INIT
    
    Tune SD_MC_INIT the same way as SD_CPU_INIT:
    unset SD_BALANCE_NEWIDLE, and set SD_WAKE_BALANCE.
    
    This improves vmark by 5%:
    
    vmark         132102 125968 125497 messages/sec    avg 127855.66    .984
    vmark         139404 131719 131272 messages/sec    avg 134131.66   1.033
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
     # *DOCUMENTATION*

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 34a7ee0ebed2..a8d840595b7e 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -120,10 +120,10 @@ void arch_update_cpu_topology(void);
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
+				| SD_WAKE_BALANCE	\
 				| SD_SHARE_PKG_RESOURCES\
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\

commit 9fcd18c9e63e325dbd2b4c726623f760788d5aa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 5 16:52:08 2008 +0100

    sched: re-tune balancing
    
    Impact: improve wakeup affinity on NUMA systems, tweak SMP systems
    
    Given the fixes+tweaks to the wakeup-buddy code, re-tweak the domain
    balancing defaults on NUMA and SMP systems.
    
    Turn on SD_WAKE_AFFINE which was off on x86 NUMA - there's no reason
    why we would not want to have wakeup affinity across nodes as well.
    (we already do this in the standard NUMA template.)
    
    lat_ctx on a NUMA box is particularly happy about this change:
    
    before:
    
     |   phoenix:~/l> ./lat_ctx -s 0 2
     |   "size=0k ovr=2.60
     |   2 5.70
    
    after:
    
     |   phoenix:~/l> ./lat_ctx -s 0 2
     |   "size=0k ovr=2.65
     |   2 2.07
    
    a 2.75x speedup.
    
    pipe-test is similarly happy about it too:
    
     |  phoenix:~/sched-tests> ./pipe-test
     |   18.26 usecs/loop.
     |   14.70 usecs/loop.
     |   14.38 usecs/loop.
     |   10.55 usecs/loop.              # +WAKE_AFFINE on domain0+domain1
     |   8.63 usecs/loop.
     |   8.59 usecs/loop.
     |   9.03 usecs/loop.
     |   8.94 usecs/loop.
     |   8.96 usecs/loop.
     |   8.63 usecs/loop.
    
    Also:
    
     - disable SD_BALANCE_NEWIDLE on NUMA and SMP domains (keep it for siblings)
     - enable SD_WAKE_BALANCE on SMP domains
    
    Sysbench+postgresql improves all around the board, quite significantly:
    
               .28-rc3-11474e2c  .28-rc3-11474e2c-tune
    -------------------------------------------------
        1:             571              688    +17.08%
        2:            1236             1206    -2.55%
        4:            2381             2642    +9.89%
        8:            4958             5164    +3.99%
       16:            9580             9574    -0.07%
       32:            7128             8118    +12.20%
       64:            7342             8266    +11.18%
      128:            7342             8064    +8.95%
      256:            7519             7884    +4.62%
      512:            7350             7731    +4.93%
    -------------------------------------------------
      SUM:           55412            59341    +6.62%
    
    So it's a win both for the runup portion, the peak area and the tail.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 2158fc0d5a56..34a7ee0ebed2 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -146,10 +146,10 @@ void arch_update_cpu_topology(void);
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
-				| SD_BALANCE_NEWIDLE	\
-				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
+				| SD_BALANCE_FORK	\
 				| SD_WAKE_AFFINE	\
+				| SD_WAKE_BALANCE	\
 				| BALANCE_FOR_PKG_POWER,\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\

commit c50cbb05a05cf1f9ca3592272eff053c847727d8
Author: Ben Hutchings <bhutchings@solarflare.com>
Date:   Wed Jun 4 21:47:29 2008 -0700

    cpu topology: always define CPU topology information
    
    This can result in an empty topology directory in sysfs, and requires
    in-kernel users to protect all uses with #ifdef - see
    <http://marc.info/?l=linux-netdev&m=120639033904472&w=2>.
    
    The documentation of CPU topology specifies what the defaults should be if
    only partial information is available from the hardware.  So we can
    provide these defaults as a fallback.
    
    This patch:
    
    - Adds default definitions of the 4 topology macros to <linux/topology.h>
    - Changes drivers/base/topology.c to use the topology macros unconditionally
      and to cope with definitions that aren't lvalues
    - Updates documentation accordingly
    
    [ From: Andrew Morton <akpm@linux-foundation.org>
      - fold now-duplicated code
      - fix layout
    ]
    
    Signed-off-by: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Chandra Seetharaman <sekharan@us.ibm.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Mike Travis <travis@sgi.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: John Hawkes <hawkes@sgi.com>
    Cc: Zhang, Yanmin <yanmin.zhang@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 24f3d2282e11..2158fc0d5a56 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -179,4 +179,17 @@ void arch_update_cpu_topology(void);
 #endif
 #endif /* CONFIG_NUMA */
 
+#ifndef topology_physical_package_id
+#define topology_physical_package_id(cpu)	((void)(cpu), -1)
+#endif
+#ifndef topology_core_id
+#define topology_core_id(cpu)			((void)(cpu), 0)
+#endif
+#ifndef topology_thread_siblings
+#define topology_thread_siblings(cpu)		cpumask_of_cpu(cpu)
+#endif
+#ifndef topology_core_siblings
+#define topology_core_siblings(cpu)		cpumask_of_cpu(cpu)
+#endif
+
 #endif /* _LINUX_TOPOLOGY_H */

commit ea3f01f8afd3bc5daff915cc4ea5cc5ea9e7d427
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 29 14:32:23 2008 +0200

    sched: re-tune NUMA topologies
    
    improve the sysbench ramp-up phase and its peak throughput on
    a 16way NUMA box, by turning on WAKE_AFFINE:
    
                 tip/sched   tip/sched+wake-affine
    -------------------------------------------------
        1:             700              830    +15.65%
        2:            1465             1391    -5.28%
        4:            3017             3105    +2.81%
        8:            5100             6021    +15.30%
       16:           10725            10745    +0.19%
       32:           10135            10150    +0.16%
       64:            9338             9240    -1.06%
      128:            8599             8252    -4.21%
      256:            8475             8144    -4.07%
    -------------------------------------------------
      SUM:           57558            57882    +0.56%
    
    this change also improves lat_ctx from 6.69 usecs to 1.11 usec:
    
      $ ./lat_ctx -s 0 2
      "size=0k ovr=1.19
      2 1.11
    
      $ ./lat_ctx -s 0 2
      "size=0k ovr=1.22
      2 6.69
    
    in sysbench it's an overall win with some weakness at the lots-of-clients
    side. That happens because we now under-balance this workload
    a bit. To counter that effect, turn on NEWIDLE:
    
                  wake-idle          wake-idle+newidle
     -------------------------------------------------
         1:             830              834    +0.43%
         2:            1391             1401    +0.65%
         4:            3105             3091    -0.43%
         8:            6021             6046    +0.42%
        16:           10745            10736    -0.08%
        32:           10150            10206    +0.55%
        64:            9240             9533    +3.08%
       128:            8252             8355    +1.24%
       256:            8144             8384    +2.87%
     -------------------------------------------------
       SUM:           57882            58591    +1.21%
    
    as a bonus this not only improves the many-clients case but
    also improves the (more important) rampup phase.
    
    sysbench is a workload that quickly breaks down if the
    scheduler over-balances, so since it showed an improvement
    under NEWIDLE this change is definitely good.

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 4bb7074a2c3a..24f3d2282e11 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -166,7 +166,9 @@ void arch_update_cpu_topology(void);
 	.busy_idx		= 3,			\
 	.idle_idx		= 3,			\
 	.flags			= SD_LOAD_BALANCE	\
-				| SD_SERIALIZE,	\
+				| SD_BALANCE_NEWIDLE	\
+				| SD_WAKE_AFFINE	\
+				| SD_SERIALIZE,		\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 64,			\
 }

commit 7c16ec585c558960a508ccf9a08fcb9ed49b3754
Author: Mike Travis <travis@sgi.com>
Date:   Fri Apr 4 18:11:11 2008 -0700

    cpumask: reduce stack usage in SD_x_INIT initializers
    
      * Remove empty cpumask_t (and all non-zero/non-null) variables
        in SD_*_INIT macros.  Use memset(0) to clear.  Also, don't
        inline the initializer functions to save on stack space in
        build_sched_domains().
    
      * Merge change to include/linux/topology.h that uses the new
        node_to_cpumask_ptr function in the nr_cpus_node macro into
        this patch.
    
    Depends on:
            [mm-patch]: asm-generic-add-node_to_cpumask_ptr-macro.patch
            [sched-devel]: sched: add new set_cpus_allowed_ptr function
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index bd14f8b30f09..4bb7074a2c3a 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -38,16 +38,15 @@
 #endif
 
 #ifndef nr_cpus_node
-#define nr_cpus_node(node)							\
-	({									\
-		cpumask_t __tmp__;						\
-		__tmp__ = node_to_cpumask(node);				\
-		cpus_weight(__tmp__);						\
+#define nr_cpus_node(node)				\
+	({						\
+		node_to_cpumask_ptr(__tmp__, node);	\
+		cpus_weight(*__tmp__);			\
 	})
 #endif
 
-#define for_each_node_with_cpus(node)						\
-	for_each_online_node(node)						\
+#define for_each_node_with_cpus(node)			\
+	for_each_online_node(node)			\
 		if (nr_cpus_node(node))
 
 void arch_update_cpu_topology(void);
@@ -80,7 +79,9 @@ void arch_update_cpu_topology(void);
  * by defining their own arch-specific initializer in include/asm/topology.h.
  * A definition there will automagically override these default initializers
  * and allow arch-specific performance tuning of sched_domains.
+ * (Only non-zero and non-null fields need be specified.)
  */
+
 #ifdef CONFIG_SCHED_SMT
 /* MCD - Do we really need this?  It is always on if CONFIG_SCHED_SMT is,
  * so can't we drop this in favor of CONFIG_SCHED_SMT?
@@ -89,20 +90,10 @@ void arch_update_cpu_topology(void);
 /* Common values for SMT siblings */
 #ifndef SD_SIBLING_INIT
 #define SD_SIBLING_INIT (struct sched_domain) {		\
-	.span			= CPU_MASK_NONE,	\
-	.parent			= NULL,			\
-	.child			= NULL,			\
-	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 2,			\
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 110,			\
-	.cache_nice_tries	= 0,			\
-	.busy_idx		= 0,			\
-	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
-	.wake_idx		= 0,			\
-	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_FORK	\
@@ -112,7 +103,6 @@ void arch_update_cpu_topology(void);
 				| SD_SHARE_CPUPOWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
-	.nr_balance_failed	= 0,			\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */
@@ -121,18 +111,12 @@ void arch_update_cpu_topology(void);
 /* Common values for MC siblings. for now mostly derived from SD_CPU_INIT */
 #ifndef SD_MC_INIT
 #define SD_MC_INIT (struct sched_domain) {		\
-	.span			= CPU_MASK_NONE,	\
-	.parent			= NULL,			\
-	.child			= NULL,			\
-	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 4,			\
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
@@ -144,7 +128,6 @@ void arch_update_cpu_topology(void);
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
-	.nr_balance_failed	= 0,			\
 }
 #endif
 #endif /* CONFIG_SCHED_MC */
@@ -152,10 +135,6 @@ void arch_update_cpu_topology(void);
 /* Common values for CPUs */
 #ifndef SD_CPU_INIT
 #define SD_CPU_INIT (struct sched_domain) {		\
-	.span			= CPU_MASK_NONE,	\
-	.parent			= NULL,			\
-	.child			= NULL,			\
-	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 4,			\
 	.busy_factor		= 64,			\
@@ -174,16 +153,11 @@ void arch_update_cpu_topology(void);
 				| BALANCE_FOR_PKG_POWER,\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
-	.nr_balance_failed	= 0,			\
 }
 #endif
 
 /* sched_domains SD_ALLNODES_INIT for NUMA machines */
 #define SD_ALLNODES_INIT (struct sched_domain) {	\
-	.span			= CPU_MASK_NONE,	\
-	.parent			= NULL,			\
-	.child			= NULL,			\
-	.groups			= NULL,			\
 	.min_interval		= 64,			\
 	.max_interval		= 64*num_online_cpus(),	\
 	.busy_factor		= 128,			\
@@ -191,14 +165,10 @@ void arch_update_cpu_topology(void);
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 3,			\
 	.idle_idx		= 3,			\
-	.newidle_idx		= 0, /* unused */	\
-	.wake_idx		= 0, /* unused */	\
-	.forkexec_idx		= 0, /* unused */	\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_SERIALIZE,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 64,			\
-	.nr_balance_failed	= 0,			\
 }
 
 #ifdef CONFIG_NUMA

commit 22e52b072dd87faa9b2559fe89d4e8f2370f81ca
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Mar 12 18:31:59 2008 +0100

    sched: add arch_update_cpu_topology hook.
    
    Will be called each time the scheduling domains are rebuild.
    Needed for architectures that don't have a static cpu topology.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 2d8dac8799cf..bd14f8b30f09 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -50,6 +50,8 @@
 	for_each_online_node(node)						\
 		if (nr_cpus_node(node))
 
+void arch_update_cpu_topology(void);
+
 /* Conform to ACPI 2.0 SLIT distance definitions */
 #define LOCAL_DISTANCE		10
 #define REMOTE_DISTANCE		20

commit 33b0c4217dcd67b788318c3192a2912b530e4eef
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Mar 16 11:14:30 2008 +0100

    sched: tune multi-core idle balancing
    
    WAKE_IDLE is too agressive on multi-core CPUs with the new
    wake-affine code, keep it on for SMT/HT balancing alone
    (where there's no cache affinity at all between logical CPUs).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 2352f46160d3..2d8dac8799cf 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -138,7 +138,6 @@
 				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
 				| SD_SHARE_PKG_RESOURCES\
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\

commit 32525d022ad52a5c14e80e130260431e16e294b6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:20 2008 +0100

    sched: whitespace cleanups in topology.h
    
    whitespace cleanups in topology.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index b6c073d8b709..2352f46160d3 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -5,7 +5,7 @@
  *
  * Copyright (C) 2002, IBM Corp.
  *
- * All rights reserved.          
+ * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by

commit 52d853431e8d9dc17ba94792123a3fe2bc039831
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:20 2008 +0100

    sched: reactivate fork balancing
    
    reactivate fork balancing.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 47729f18bfdf..b6c073d8b709 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -103,6 +103,7 @@
 	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_IDLE		\
@@ -134,6 +135,7 @@
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_IDLE		\
@@ -165,6 +167,7 @@
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_FORK	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
 				| BALANCE_FOR_PKG_POWER,\

commit 7a6c6bcee029a978f866511d6e41dbc7301fde4c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: enable wake-idle on CONFIG_SCHED_MC=y
    
    most multicore CPUs today have shared L2 caches, so tune things so
    that the spreading amongst cores is more aggressive.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 865a63e65578..47729f18bfdf 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -98,7 +98,7 @@
 	.cache_nice_tries	= 0,			\
 	.busy_idx		= 0,			\
 	.idle_idx		= 0,			\
-	.newidle_idx		= 1,			\
+	.newidle_idx		= 0,			\
 	.wake_idx		= 0,			\
 	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
@@ -128,14 +128,15 @@
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 1,			\
-	.newidle_idx		= 2,			\
+	.idle_idx		= 0,			\
+	.newidle_idx		= 0,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
 				| SD_SHARE_PKG_RESOURCES\
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\

commit 95dbb421d12fdd9796ed153853daf3679809274f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: reintroduce topology.h tunings
    
    reintroduce the 2.6.22 topology.h tunings again - they result in
    slightly better balancing.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 525d437b1253..865a63e65578 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -98,7 +98,7 @@
 	.cache_nice_tries	= 0,			\
 	.busy_idx		= 0,			\
 	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
+	.newidle_idx		= 1,			\
 	.wake_idx		= 0,			\
 	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
@@ -128,15 +128,14 @@
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
+	.idle_idx		= 1,			\
+	.newidle_idx		= 2,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
 				| SD_SHARE_PKG_RESOURCES\
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\
@@ -159,15 +158,14 @@
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
+	.idle_idx		= 1,			\
+	.newidle_idx		= 2,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
 				| BALANCE_FOR_PKG_POWER,\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\

commit 362a7016637648c6aefc98b706298baedfaa1543
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 2 17:41:40 2007 +0200

    [PATCH] sched: remove cache_hot_time
    
    remove the last unused remains of cache_hot_time.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index d0890a7e5bab..525d437b1253 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -185,7 +185,6 @@
 	.max_interval		= 64*num_online_cpus(),	\
 	.busy_factor		= 128,			\
 	.imbalance_pct		= 133,			\
-	.cache_hot_time		= (10*1000000),		\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 3,			\
 	.idle_idx		= 3,			\

commit f787a50306680c187cf2896a8017937c1bf6dc7e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 11 21:21:47 2007 +0200

    [PATCH] sched: small topology.h cleanup
    
    trivial cleanup: LOCAL_DISTANCE and REMOTE_DISTANCE are only used in
    topology.h and inside an #ifndef section - limit their existence to
    that #ifndef.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index da6c39b2d051..d0890a7e5bab 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -50,10 +50,10 @@
 	for_each_online_node(node)						\
 		if (nr_cpus_node(node))
 
-#ifndef node_distance
 /* Conform to ACPI 2.0 SLIT distance definitions */
 #define LOCAL_DISTANCE		10
 #define REMOTE_DISTANCE		20
+#ifndef node_distance
 #define node_distance(from,to)	((from) == (to) ? LOCAL_DISTANCE : REMOTE_DISTANCE)
 #endif
 #ifndef RECLAIM_DISTANCE

commit 9c4801cebc2add1fe514bc8eb201b16372eee11a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:01 2007 +0200

    sched: more agressive idle balancing
    
    the Linux scheduler is starving a number of workloads. So default
    to more agressive idle-balancing. This hurts lmbench context-switching
    numbers (which was the main reason we sucked at idle-balancing for
    such a long time) but the lmbench numbers are fine once the system is
    minimally utilized.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index a9d1f049cc15..da6c39b2d051 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -98,7 +98,7 @@
 	.cache_nice_tries	= 0,			\
 	.busy_idx		= 0,			\
 	.idle_idx		= 0,			\
-	.newidle_idx		= 1,			\
+	.newidle_idx		= 0,			\
 	.wake_idx		= 0,			\
 	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
@@ -128,14 +128,15 @@
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 1,			\
-	.newidle_idx		= 2,			\
+	.idle_idx		= 0,			\
+	.newidle_idx		= 0,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
 				| SD_SHARE_PKG_RESOURCES\
 				| BALANCE_FOR_MC_POWER,	\
 	.last_balance		= jiffies,		\
@@ -158,14 +159,15 @@
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 1,			\
-	.newidle_idx		= 2,			\
+	.idle_idx		= 0,			\
+	.newidle_idx		= 0,			\
 	.wake_idx		= 1,			\
 	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
 				| BALANCE_FOR_PKG_POWER,\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\

commit 69f7c0a1be84b10a81b6edcce2dbee0cdec26eba
Author: Con Kolivas <kernel@kolivas.org>
Date:   Mon Mar 5 00:30:29 2007 -0800

    [PATCH] sched: remove SMT nice
    
    Remove the SMT-nice feature which idles sibling cpus on SMT cpus to
    facilitiate nice working properly where cpu power is shared.  The idling of
    cpus in the presence of runnable tasks is considered too fragile, easy to
    break with outside code, and the complexity of managing this system if an
    architecture comes along with many logical cores sharing cpu power will be
    unworkable.
    
    Remove the associated per_cpu_gain variable in sched_domains used only by
    this code.
    
    Also:
    
      The reason is that with dynticks enabled, this code breaks without yet
      further tweaks so dynticks brought on the rapid demise of this code.  So
      either we tweak this code or kill it off entirely.  It was Ingo's preference
      to kill it off.  Either way this needs to happen for 2.6.21 since dynticks
      has gone in.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 6c5a6e6e813b..a9d1f049cc15 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -96,7 +96,6 @@
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 110,			\
 	.cache_nice_tries	= 0,			\
-	.per_cpu_gain		= 25,			\
 	.busy_idx		= 0,			\
 	.idle_idx		= 0,			\
 	.newidle_idx		= 1,			\
@@ -128,7 +127,6 @@
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
-	.per_cpu_gain		= 100,			\
 	.busy_idx		= 2,			\
 	.idle_idx		= 1,			\
 	.newidle_idx		= 2,			\
@@ -159,7 +157,6 @@
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 125,			\
 	.cache_nice_tries	= 1,			\
-	.per_cpu_gain		= 100,			\
 	.busy_idx		= 2,			\
 	.idle_idx		= 1,			\
 	.newidle_idx		= 2,			\
@@ -193,7 +190,6 @@
 	.newidle_idx		= 0, /* unused */	\
 	.wake_idx		= 0, /* unused */	\
 	.forkexec_idx		= 0, /* unused */	\
-	.per_cpu_gain		= 100,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_SERIALIZE,	\
 	.last_balance		= jiffies,		\

commit 08c183f31bdbb709f177f6d3110d5f288ea33933
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Dec 10 02:20:29 2006 -0800

    [PATCH] sched: add option to serialize load balancing
    
    Large sched domains can be very expensive to scan.  Add an option SD_SERIALIZE
    to the sched domain flags.  If that flag is set then we make sure that no
    other such domain is being balanced.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Peter Williams <pwil3058@bigpond.net.au>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index b93bb6cc6cc2..6c5a6e6e813b 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -194,7 +194,8 @@
 	.wake_idx		= 0, /* unused */	\
 	.forkexec_idx		= 0, /* unused */	\
 	.per_cpu_gain		= 100,			\
-	.flags			= SD_LOAD_BALANCE,	\
+	.flags			= SD_LOAD_BALANCE	\
+				| SD_SERIALIZE,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 64,			\
 	.nr_balance_failed	= 0,			\

commit ac7d550499e225efb51a53d0b00667f26b93bdff
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Sun Dec 10 02:20:12 2006 -0800

    [PATCH] sched domain: increase the SMT busy rebalance interval
    
    With SMT, if the logical processor is busy, load balance happens for every
    8msec(min)-16msec(max).  There is no need to do this often, as this is just
    for fairness(to maintain uniform runqueue lengths) and default time slice
    anyhow is 100msec.
    
    Appended patch increases this interval to 64msec(min)-128msec(max) when the
    logical processor is busy.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index da508d1998e4..b93bb6cc6cc2 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -93,7 +93,7 @@
 	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 2,			\
-	.busy_factor		= 8,			\
+	.busy_factor		= 64,			\
 	.imbalance_pct		= 110,			\
 	.cache_nice_tries	= 0,			\
 	.per_cpu_gain		= 25,			\

commit 89c4710ee9bbbefe6a4d469d9f36266a92c275c5
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Oct 3 01:14:09 2006 -0700

    [PATCH] sched: cleanup sched_group cpu_power setup
    
    Up to now sched group's cpu_power for each sched domain is initialized
    independently.  This made the setup code ugly as the new sched domains are
    getting added.
    
    Make the sched group cpu_power setup code generic, by using domain child
    field and new domain flag in sched_domain.  For most of the sched
    domains(except NUMA), sched group's cpu_power is now computed generically
    using the domain properties of itself and of the child domain.
    
    sched groups in NUMA domains are setup little differently and hence they
    don't use this generic mechanism.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 486bec23f986..da508d1998e4 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -115,6 +115,38 @@
 #endif
 #endif /* CONFIG_SCHED_SMT */
 
+#ifdef CONFIG_SCHED_MC
+/* Common values for MC siblings. for now mostly derived from SD_CPU_INIT */
+#ifndef SD_MC_INIT
+#define SD_MC_INIT (struct sched_domain) {		\
+	.span			= CPU_MASK_NONE,	\
+	.parent			= NULL,			\
+	.child			= NULL,			\
+	.groups			= NULL,			\
+	.min_interval		= 1,			\
+	.max_interval		= 4,			\
+	.busy_factor		= 64,			\
+	.imbalance_pct		= 125,			\
+	.cache_nice_tries	= 1,			\
+	.per_cpu_gain		= 100,			\
+	.busy_idx		= 2,			\
+	.idle_idx		= 1,			\
+	.newidle_idx		= 2,			\
+	.wake_idx		= 1,			\
+	.forkexec_idx		= 1,			\
+	.flags			= SD_LOAD_BALANCE	\
+				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_EXEC	\
+				| SD_WAKE_AFFINE	\
+				| SD_SHARE_PKG_RESOURCES\
+				| BALANCE_FOR_MC_POWER,	\
+	.last_balance		= jiffies,		\
+	.balance_interval	= 1,			\
+	.nr_balance_failed	= 0,			\
+}
+#endif
+#endif /* CONFIG_SCHED_MC */
+
 /* Common values for CPUs */
 #ifndef SD_CPU_INIT
 #define SD_CPU_INIT (struct sched_domain) {		\
@@ -137,7 +169,7 @@
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| BALANCE_FOR_POWER,	\
+				| BALANCE_FOR_PKG_POWER,\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\
@@ -168,15 +200,6 @@
 	.nr_balance_failed	= 0,			\
 }
 
-#ifdef CONFIG_SCHED_MC
-#ifndef SD_MC_INIT
-/* for now its same as SD_CPU_INIT.
- * TBD: Tune Domain parameters!
- */
-#define SD_MC_INIT   SD_CPU_INIT
-#endif
-#endif
-
 #ifdef CONFIG_NUMA
 #ifndef SD_NODE_INIT
 #error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!

commit 1a84887080dc15f048db7c3a643e98f1435790d6
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Oct 3 01:14:08 2006 -0700

    [PATCH] sched: introduce child field in sched_domain
    
    Introduce the child field in sched_domain struct and use it in
    sched_balance_self().
    
    We will also use this field in cleaning up the sched group cpu_power
    setup(done in a different patch) code.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index ec1eca85290a..486bec23f986 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -89,6 +89,7 @@
 #define SD_SIBLING_INIT (struct sched_domain) {		\
 	.span			= CPU_MASK_NONE,	\
 	.parent			= NULL,			\
+	.child			= NULL,			\
 	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 2,			\
@@ -119,6 +120,7 @@
 #define SD_CPU_INIT (struct sched_domain) {		\
 	.span			= CPU_MASK_NONE,	\
 	.parent			= NULL,			\
+	.child			= NULL,			\
 	.groups			= NULL,			\
 	.min_interval		= 1,			\
 	.max_interval		= 4,			\
@@ -146,6 +148,7 @@
 #define SD_ALLNODES_INIT (struct sched_domain) {	\
 	.span			= CPU_MASK_NONE,	\
 	.parent			= NULL,			\
+	.child			= NULL,			\
 	.groups			= NULL,			\
 	.min_interval		= 64,			\
 	.max_interval		= 64*num_online_cpus(),	\

commit 5c45bf279d378d436ce45825c0f136696c7b6109
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Jun 27 02:54:42 2006 -0700

    [PATCH] sched: mc/smt power savings sched policy
    
    sysfs entries 'sched_mc_power_savings' and 'sched_smt_power_savings' in
    /sys/devices/system/cpu/ control the MC/SMT power savings policy for the
    scheduler.
    
    Based on the values (1-enable, 0-disable) for these controls, sched groups
    cpu power will be determined for different domains.  When power savings
    policy is enabled and under light load conditions, scheduler will minimize
    the physical packages/cpu cores carrying the load and thus conserving
    power(with a perf impact based on the workload characteristics...  see OLS
    2005 CMP kernel scheduler paper for more details..)
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Con Kolivas <kernel@kolivas.org>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index a305ae2e44b6..ec1eca85290a 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -134,7 +134,8 @@
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
-				| SD_WAKE_AFFINE,	\
+				| SD_WAKE_AFFINE	\
+				| BALANCE_FOR_POWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\

commit 1e9f28fa1eb9773bf65bae08288c6a0a38eef4a7
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Mon Mar 27 01:15:22 2006 -0800

    [PATCH] sched: new sched domain for representing multi-core
    
    Add a new sched domain for representing multi-core with shared caches
    between cores.  Consider a dual package system, each package containing two
    cores and with last level cache shared between cores with in a package.  If
    there are two runnable processes, with this appended patch those two
    processes will be scheduled on different packages.
    
    On such systems, with this patch we have observed 8% perf improvement with
    specJBB(2 warehouse) benchmark and 35% improvement with CFP2000 rate(with 2
    users).
    
    This new domain will come into play only on multi-core systems with shared
    caches.  On other systems, this sched domain will be removed by domain
    degeneration code.  This new domain can be also used for implementing power
    savings policy (see OLS 2005 CMP kernel scheduler paper for more details..
    I will post another patch for power savings policy soon)
    
    Most of the arch/* file changes are for cpu_coregroup_map() implementation.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index e8eb0040ce3a..a305ae2e44b6 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -164,6 +164,15 @@
 	.nr_balance_failed	= 0,			\
 }
 
+#ifdef CONFIG_SCHED_MC
+#ifndef SD_MC_INIT
+/* for now its same as SD_CPU_INIT.
+ * TBD: Tune Domain parameters!
+ */
+#define SD_MC_INIT   SD_CPU_INIT
+#endif
+#endif
+
 #ifdef CONFIG_NUMA
 #ifndef SD_NODE_INIT
 #error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!

commit 9eeff2395e3cfd05c9b2e6074ff943a34b0c5c21
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Jan 18 17:42:31 2006 -0800

    [PATCH] Zone reclaim: Reclaim logic
    
    Some bits for zone reclaim exists in 2.6.15 but they are not usable.  This
    patch fixes them up, removes unused code and makes zone reclaim usable.
    
    Zone reclaim allows the reclaiming of pages from a zone if the number of
    free pages falls below the watermarks even if other zones still have enough
    pages available.  Zone reclaim is of particular importance for NUMA
    machines.  It can be more beneficial to reclaim a page than taking the
    performance penalties that come with allocating a page on a remote zone.
    
    Zone reclaim is enabled if the maximum distance to another node is higher
    than RECLAIM_DISTANCE, which may be defined by an arch.  By default
    RECLAIM_DISTANCE is 20.  20 is the distance to another node in the same
    component (enclosure or motherboard) on IA64.  The meaning of the NUMA
    distance information seems to vary by arch.
    
    If zone reclaim is not successful then no further reclaim attempts will
    occur for a certain time period (ZONE_RECLAIM_INTERVAL).
    
    This patch was discussed before. See
    
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113519961504207&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113408418232531&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113389027420032&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113380938612205&w=2
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 315a5163d6a0..e8eb0040ce3a 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -56,6 +56,14 @@
 #define REMOTE_DISTANCE		20
 #define node_distance(from,to)	((from) == (to) ? LOCAL_DISTANCE : REMOTE_DISTANCE)
 #endif
+#ifndef RECLAIM_DISTANCE
+/*
+ * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
+ * (in whatever arch specific measurement units returned by node_distance())
+ * then switch on zone reclaim on boot.
+ */
+#define RECLAIM_DISTANCE 20
+#endif
 #ifndef PENALTY_FOR_NODE_WITH_CPUS
 #define PENALTY_FOR_NODE_WITH_CPUS	(1)
 #endif

commit 198e2f181163233b379dc7ce8a6d7516b84042e7
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Thu Jan 12 01:05:30 2006 -0800

    [PATCH] scheduler cache-hot-autodetect
    
    )
    
    From: Ingo Molnar <mingo@elte.hu>
    
    This is the latest version of the scheduler cache-hot-auto-tune patch.
    
    The first problem was that detection time scaled with O(N^2), which is
    unacceptable on larger SMP and NUMA systems. To solve this:
    
    - I've added a 'domain distance' function, which is used to cache
      measurement results. Each distance is only measured once. This means
      that e.g. on NUMA distances of 0, 1 and 2 might be measured, on HT
      distances 0 and 1, and on SMP distance 0 is measured. The code walks
      the domain tree to determine the distance, so it automatically follows
      whatever hierarchy an architecture sets up. This cuts down on the boot
      time significantly and removes the O(N^2) limit. The only assumption
      is that migration costs can be expressed as a function of domain
      distance - this covers the overwhelming majority of existing systems,
      and is a good guess even for more assymetric systems.
    
      [ People hacking systems that have assymetries that break this
        assumption (e.g. different CPU speeds) should experiment a bit with
        the cpu_distance() function. Adding a ->migration_distance factor to
        the domain structure would be one possible solution - but lets first
        see the problem systems, if they exist at all. Lets not overdesign. ]
    
    Another problem was that only a single cache-size was used for measuring
    the cost of migration, and most architectures didnt set that variable
    up. Furthermore, a single cache-size does not fit NUMA hierarchies with
    L3 caches and does not fit HT setups, where different CPUs will often
    have different 'effective cache sizes'. To solve this problem:
    
    - Instead of relying on a single cache-size provided by the platform and
      sticking to it, the code now auto-detects the 'effective migration
      cost' between two measured CPUs, via iterating through a wide range of
      cachesizes. The code searches for the maximum migration cost, which
      occurs when the working set of the test-workload falls just below the
      'effective cache size'. I.e. real-life optimized search is done for
      the maximum migration cost, between two real CPUs.
    
      This, amongst other things, has the positive effect hat if e.g. two
      CPUs share a L2/L3 cache, a different (and accurate) migration cost
      will be found than between two CPUs on the same system that dont share
      any caches.
    
    (The reliable measurement of migration costs is tricky - see the source
    for details.)
    
    Furthermore i've added various boot-time options to override/tune
    migration behavior.
    
    Firstly, there's a blanket override for autodetection:
    
            migration_cost=1000,2000,3000
    
    will override the depth 0/1/2 values with 1msec/2msec/3msec values.
    
    Secondly, there's a global factor that can be used to increase (or
    decrease) the autodetected values:
    
            migration_factor=120
    
    will increase the autodetected values by 20%. This option is useful to
    tune things in a workload-dependent way - e.g. if a workload is
    cache-insensitive then CPU utilization can be maximized by specifying
    migration_factor=0.
    
    I've tested the autodetection code quite extensively on x86, on 3
    P3/Xeon/2MB, and the autodetected values look pretty good:
    
    Dual Celeron (128K L2 cache):
    
     ---------------------
     migration cost matrix (max_cache_size: 131072, cpu: 467 MHz):
     ---------------------
               [00]    [01]
     [00]:     -     1.7(1)
     [01]:   1.7(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (0) 1.7 (1784008)
     ---------------------
    
    Here the slow memory subsystem dominates system performance, and even
    though caches are small, the migration cost is 1.7 msecs.
    
    Dual HT P4 (512K L2 cache):
    
     ---------------------
     migration cost matrix (max_cache_size: 524288, cpu: 2379 MHz):
     ---------------------
               [00]    [01]    [02]    [03]
     [00]:     -     0.4(1)  0.0(0)  0.4(1)
     [01]:   0.4(1)    -     0.4(1)  0.0(0)
     [02]:   0.0(0)  0.4(1)    -     0.4(1)
     [03]:   0.4(1)  0.0(0)  0.4(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (33900) 0.4 (448514)
     ---------------------
    
    Here it can be seen that there is no migration cost between two HT
    siblings (CPU#0/2 and CPU#1/3 are separate physical CPUs). A fast memory
    system makes inter-physical-CPU migration pretty cheap: 0.4 msecs.
    
    8-way P3/Xeon [2MB L2 cache]:
    
     ---------------------
     migration cost matrix (max_cache_size: 2097152, cpu: 700 MHz):
     ---------------------
               [00]    [01]    [02]    [03]    [04]    [05]    [06]    [07]
     [00]:     -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [01]:  19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [02]:  19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [03]:  19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [04]:  19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1)
     [05]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1)
     [06]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1)
     [07]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (0) 19.2 (19281756)
     ---------------------
    
    This one has huge caches and a relatively slow memory subsystem - so the
    migration cost is 19 msecs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Cc: <wilder@us.ibm.com>
    Signed-off-by: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 3df1d474e5c5..315a5163d6a0 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -86,7 +86,6 @@
 	.max_interval		= 2,			\
 	.busy_factor		= 8,			\
 	.imbalance_pct		= 110,			\
-	.cache_hot_time		= 0,			\
 	.cache_nice_tries	= 0,			\
 	.per_cpu_gain		= 25,			\
 	.busy_idx		= 0,			\
@@ -117,7 +116,6 @@
 	.max_interval		= 4,			\
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 125,			\
-	.cache_hot_time		= (5*1000000/2),	\
 	.cache_nice_tries	= 1,			\
 	.per_cpu_gain		= 100,			\
 	.busy_idx		= 2,			\

commit 9c1cfda20a508b181bdda8c0045f7c0c333880a5
Author: John Hawkes <hawkes@sgi.com>
Date:   Tue Sep 6 15:18:14 2005 -0700

    [PATCH] cpusets: Move the ia64 domain setup code to the generic code
    
    Signed-off-by: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 0320225e96da..3df1d474e5c5 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -135,6 +135,29 @@
 }
 #endif
 
+/* sched_domains SD_ALLNODES_INIT for NUMA machines */
+#define SD_ALLNODES_INIT (struct sched_domain) {	\
+	.span			= CPU_MASK_NONE,	\
+	.parent			= NULL,			\
+	.groups			= NULL,			\
+	.min_interval		= 64,			\
+	.max_interval		= 64*num_online_cpus(),	\
+	.busy_factor		= 128,			\
+	.imbalance_pct		= 133,			\
+	.cache_hot_time		= (10*1000000),		\
+	.cache_nice_tries	= 1,			\
+	.busy_idx		= 3,			\
+	.idle_idx		= 3,			\
+	.newidle_idx		= 0, /* unused */	\
+	.wake_idx		= 0, /* unused */	\
+	.forkexec_idx		= 0, /* unused */	\
+	.per_cpu_gain		= 100,			\
+	.flags			= SD_LOAD_BALANCE,	\
+	.last_balance		= jiffies,		\
+	.balance_interval	= 64,			\
+	.nr_balance_failed	= 0,			\
+}
+
 #ifdef CONFIG_NUMA
 #ifndef SD_NODE_INIT
 #error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!

commit 687f1661d302bc70ce906594a6d3f615ef075a50
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:21 2005 -0700

    [PATCH] sched: sched tuning
    
    Do some basic initial tuning.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index 665597207def..0320225e96da 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -91,7 +91,7 @@
 	.per_cpu_gain		= 25,			\
 	.busy_idx		= 0,			\
 	.idle_idx		= 0,			\
-	.newidle_idx		= 0,			\
+	.newidle_idx		= 1,			\
 	.wake_idx		= 0,			\
 	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
@@ -121,15 +121,14 @@
 	.cache_nice_tries	= 1,			\
 	.per_cpu_gain		= 100,			\
 	.busy_idx		= 2,			\
-	.idle_idx		= 0,			\
-	.newidle_idx		= 1,			\
+	.idle_idx		= 1,			\
+	.newidle_idx		= 2,			\
 	.wake_idx		= 1,			\
-	.forkexec_idx		= 0,			\
+	.forkexec_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
-				| SD_WAKE_AFFINE	\
-				| SD_WAKE_BALANCE,	\
+				| SD_WAKE_AFFINE,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\

commit 147cbb4bbe991452698f0772d8292f22825710ba
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:19 2005 -0700

    [PATCH] sched: balance on fork
    
    Reimplement the balance on exec balancing to be sched-domains aware.  Use this
    to also do balance on fork balancing.  Make x86_64 do balance on fork over the
    NUMA domain.
    
    The problem that the non sched domains aware blancing became apparent on dual
    core, multi socket opterons.  What we want is for the new tasks to be sent to
    a different socket, but more often than not, we would first load up our
    sibling core, or fill two cores of a single remote socket before selecting a
    new one.
    
    This gives large improvements to STREAM on such systems.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index b23ec64df7f1..665597207def 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -93,6 +93,7 @@
 	.idle_idx		= 0,			\
 	.newidle_idx		= 0,			\
 	.wake_idx		= 0,			\
+	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
@@ -123,6 +124,7 @@
 	.idle_idx		= 0,			\
 	.newidle_idx		= 1,			\
 	.wake_idx		= 1,			\
+	.forkexec_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\

commit cafb20c1f9976a70d633bb1e1c8c24eab00e4e80
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:17 2005 -0700

    [PATCH] sched: no aggressive idle balancing
    
    Remove the very aggressive idle stuff that has recently gone into 2.6 - it is
    going against the direction we are trying to go.  Hopefully we can regain
    performance through other methods.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index ae9c2216dfa6..b23ec64df7f1 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -127,7 +127,6 @@
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
 				| SD_WAKE_BALANCE,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\

commit 7897986bad8f6cd50d6149345aca7f6480f49464
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:13 2005 -0700

    [PATCH] sched: balance timers
    
    Do CPU load averaging over a number of different intervals.  Allow each
    interval to be chosen by sending a parameter to source_load and target_load.
    0 is instantaneous, idx > 0 returns a decaying average with the most recent
    sample weighted at 2^(idx-1).  To a maximum of 3 (could be easily increased).
    
    So generally a higher number will result in more conservative balancing.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/topology.h b/include/linux/topology.h
index d70e8972c67f..ae9c2216dfa6 100644
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -89,6 +89,10 @@
 	.cache_hot_time		= 0,			\
 	.cache_nice_tries	= 0,			\
 	.per_cpu_gain		= 25,			\
+	.busy_idx		= 0,			\
+	.idle_idx		= 0,			\
+	.newidle_idx		= 0,			\
+	.wake_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
@@ -115,6 +119,10 @@
 	.cache_hot_time		= (5*1000000/2),	\
 	.cache_nice_tries	= 1,			\
 	.per_cpu_gain		= 100,			\
+	.busy_idx		= 2,			\
+	.idle_idx		= 0,			\
+	.newidle_idx		= 1,			\
+	.wake_idx		= 1,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/topology.h b/include/linux/topology.h
new file mode 100644
index 000000000000..d70e8972c67f
--- /dev/null
+++ b/include/linux/topology.h
@@ -0,0 +1,136 @@
+/*
+ * include/linux/topology.h
+ *
+ * Written by: Matthew Dobson, IBM Corporation
+ *
+ * Copyright (C) 2002, IBM Corp.
+ *
+ * All rights reserved.          
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Send feedback to <colpatch@us.ibm.com>
+ */
+#ifndef _LINUX_TOPOLOGY_H
+#define _LINUX_TOPOLOGY_H
+
+#include <linux/cpumask.h>
+#include <linux/bitops.h>
+#include <linux/mmzone.h>
+#include <linux/smp.h>
+#include <asm/topology.h>
+
+#ifndef node_has_online_mem
+#define node_has_online_mem(nid) (1)
+#endif
+
+#ifndef nr_cpus_node
+#define nr_cpus_node(node)							\
+	({									\
+		cpumask_t __tmp__;						\
+		__tmp__ = node_to_cpumask(node);				\
+		cpus_weight(__tmp__);						\
+	})
+#endif
+
+#define for_each_node_with_cpus(node)						\
+	for_each_online_node(node)						\
+		if (nr_cpus_node(node))
+
+#ifndef node_distance
+/* Conform to ACPI 2.0 SLIT distance definitions */
+#define LOCAL_DISTANCE		10
+#define REMOTE_DISTANCE		20
+#define node_distance(from,to)	((from) == (to) ? LOCAL_DISTANCE : REMOTE_DISTANCE)
+#endif
+#ifndef PENALTY_FOR_NODE_WITH_CPUS
+#define PENALTY_FOR_NODE_WITH_CPUS	(1)
+#endif
+
+/*
+ * Below are the 3 major initializers used in building sched_domains:
+ * SD_SIBLING_INIT, for SMT domains
+ * SD_CPU_INIT, for SMP domains
+ * SD_NODE_INIT, for NUMA domains
+ *
+ * Any architecture that cares to do any tuning to these values should do so
+ * by defining their own arch-specific initializer in include/asm/topology.h.
+ * A definition there will automagically override these default initializers
+ * and allow arch-specific performance tuning of sched_domains.
+ */
+#ifdef CONFIG_SCHED_SMT
+/* MCD - Do we really need this?  It is always on if CONFIG_SCHED_SMT is,
+ * so can't we drop this in favor of CONFIG_SCHED_SMT?
+ */
+#define ARCH_HAS_SCHED_WAKE_IDLE
+/* Common values for SMT siblings */
+#ifndef SD_SIBLING_INIT
+#define SD_SIBLING_INIT (struct sched_domain) {		\
+	.span			= CPU_MASK_NONE,	\
+	.parent			= NULL,			\
+	.groups			= NULL,			\
+	.min_interval		= 1,			\
+	.max_interval		= 2,			\
+	.busy_factor		= 8,			\
+	.imbalance_pct		= 110,			\
+	.cache_hot_time		= 0,			\
+	.cache_nice_tries	= 0,			\
+	.per_cpu_gain		= 25,			\
+	.flags			= SD_LOAD_BALANCE	\
+				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_EXEC	\
+				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
+				| SD_SHARE_CPUPOWER,	\
+	.last_balance		= jiffies,		\
+	.balance_interval	= 1,			\
+	.nr_balance_failed	= 0,			\
+}
+#endif
+#endif /* CONFIG_SCHED_SMT */
+
+/* Common values for CPUs */
+#ifndef SD_CPU_INIT
+#define SD_CPU_INIT (struct sched_domain) {		\
+	.span			= CPU_MASK_NONE,	\
+	.parent			= NULL,			\
+	.groups			= NULL,			\
+	.min_interval		= 1,			\
+	.max_interval		= 4,			\
+	.busy_factor		= 64,			\
+	.imbalance_pct		= 125,			\
+	.cache_hot_time		= (5*1000000/2),	\
+	.cache_nice_tries	= 1,			\
+	.per_cpu_gain		= 100,			\
+	.flags			= SD_LOAD_BALANCE	\
+				| SD_BALANCE_NEWIDLE	\
+				| SD_BALANCE_EXEC	\
+				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
+				| SD_WAKE_BALANCE,	\
+	.last_balance		= jiffies,		\
+	.balance_interval	= 1,			\
+	.nr_balance_failed	= 0,			\
+}
+#endif
+
+#ifdef CONFIG_NUMA
+#ifndef SD_NODE_INIT
+#error Please define an appropriate SD_NODE_INIT in include/asm/topology.h!!!
+#endif
+#endif /* CONFIG_NUMA */
+
+#endif /* _LINUX_TOPOLOGY_H */
