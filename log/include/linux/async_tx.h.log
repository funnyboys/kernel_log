commit a61127c2130236168321cc76c5a58e15c00ad154
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 16:57:49 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 335
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms and conditions of the gnu general public license
      version 2 as published by the free software foundation this program
      is distributed in the hope it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation inc 51 franklin st fifth floor boston ma 02110
      1301 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 111 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190530000436.567572064@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 28e3cf1465ab..75e582b8d2d9 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -1,19 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright Â© 2006, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc.,
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
- *
  */
 #ifndef _ASYNC_TX_H_
 #define _ASYNC_TX_H_

commit b802c8410ca915e7772e738e68420115f1a3c811
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Feb 15 18:42:09 2017 -0800

    async_tx: deprecate broken support for channel switching
    
    Back in 2011, Russell pointed out that the "async_tx channel switch"
    capability was violating expectations of the dma mapping api [1]. At the
    time the existing uses were reviewed as still usable, but that longer
    term we needed a rework of the raid offload implementation. While some
    of the framework for a fixed implementation was introduced in 2012 [2],
    the wider rewrite never materialized.
    
    There continues to be interest in raid offload with new dma/raid engine
    drivers being submitted. Those drivers must not build on top of the
    broken channel switching capability.
    
    Prevent async_tx from using an offload engine if the channel switching
    capability is enabled. This still allows the engine to be used for other
    purposes, but the broken way async_tx uses these engines for raid will
    be disabled. For configurations where this causes a performance
    regression the only solution is to start the work of eliminating the
    async_tx api and moving channel management into the raid code directly
    where it can manage marshalling an operation stream between multiple dma
    channels.
    
    [1]: http://lists.infradead.org/pipermail/linux-arm-kernel/2011-January/036753.html
    [2]: https://lkml.org/lkml/2012/12/6/71
    
    Cc: Anatolij Gustschin <agust@denx.de>
    Cc: Anup Patel <anup.patel@broadcom.com>
    Cc: Rameshwar Prasad Sahu <rsahu@apm.com>
    Cc: Saeed Bishara <saeed.bishara@gmail.com>
    Cc: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reported-by: Russell King <linux@armlinux.org.uk>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 388574ea38ed..28e3cf1465ab 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -87,7 +87,7 @@ struct async_submit_ctl {
 	void *scribble;
 };
 
-#ifdef CONFIG_DMA_ENGINE
+#if defined(CONFIG_DMA_ENGINE) && !defined(CONFIG_ASYNC_TX_CHANNEL_SWITCH)
 #define async_tx_issue_pending_all dma_issue_pending_all
 
 /**

commit 584acdd49cd2472ca0f5a06adbe979db82d0b4af
Author: Markus Stockhausen <stockhausen@collogia.de>
Date:   Mon Dec 15 12:57:05 2014 +1100

    md/raid5: activate raid6 rmw feature
    
    Glue it altogehter. The raid6 rmw path should work the same as the
    already existing raid5 logic. So emulate the prexor handling/flags
    and split functions as needed.
    
    1) Enable xor_syndrome() in the async layer.
    
    2) Split ops_run_prexor() into RAID4/5 and RAID6 logic. Xor the syndrome
    at the start of a rmw run as we did it before for the single parity.
    
    3) Take care of rmw run in ops_run_reconstruct6(). Again process only
    the changed pages to get syndrome back into sync.
    
    4) Enhance set_syndrome_sources() to fill NULL pages if we are in a rmw
    run. The lower layers will calculate start & end pages from that and
    call the xor_syndrome() correspondingly.
    
    5) Adapt the several places where we ignored Q handling up to now.
    
    Performance numbers for a single E5630 system with a mix of 10 7200k
    desktop/server disks. 300 seconds random write with 8 threads onto a
    3,2TB (10*400GB) RAID6 64K chunk without spare (group_thread_cnt=4)
    
    bsize   rmw_level=1   rmw_level=0   rmw_level=1   rmw_level=0
            skip_copy=1   skip_copy=1   skip_copy=0   skip_copy=0
       4K      115 KB/s      141 KB/s      165 KB/s      140 KB/s
       8K      225 KB/s      275 KB/s      324 KB/s      274 KB/s
      16K      434 KB/s      536 KB/s      640 KB/s      534 KB/s
      32K      751 KB/s    1,051 KB/s    1,234 KB/s    1,045 KB/s
      64K    1,339 KB/s    1,958 KB/s    2,282 KB/s    1,962 KB/s
     128K    2,673 KB/s    3,862 KB/s    4,113 KB/s    3,898 KB/s
     256K    7,685 KB/s    7,539 KB/s    7,557 KB/s    7,638 KB/s
     512K   19,556 KB/s   19,558 KB/s   19,652 KB/s   19,688 Kb/s
    
    Signed-off-by: Markus Stockhausen <stockhausen@collogia.de>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 179b38ffd351..388574ea38ed 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -60,12 +60,15 @@ struct dma_chan_ref {
  * dependency chain
  * @ASYNC_TX_FENCE: specify that the next operation in the dependency
  * chain uses this operation's result as an input
+ * @ASYNC_TX_PQ_XOR_DST: do not overwrite the syndrome but XOR it with the
+ * input data. Required for rmw case.
  */
 enum async_tx_flags {
 	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
 	ASYNC_TX_XOR_DROP_DST	 = (1 << 1),
 	ASYNC_TX_ACK		 = (1 << 2),
 	ASYNC_TX_FENCE		 = (1 << 3),
+	ASYNC_TX_PQ_XOR_DST	 = (1 << 4),
 };
 
 /**

commit 48a9db462d99494583dad829969616ac90a8df4e
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Wed Jul 3 15:05:06 2013 -0700

    drivers/dma: remove unused support for MEMSET operations
    
    There have never been any real users of MEMSET operations since they
    have been introduced in January 2007 by commit 7405f74badf4 ("dmaengine:
    refactor dmaengine around dma_async_tx_descriptor").  Therefore remove
    support for them for now, it can be always brought back when needed.
    
    [sebastian.hesselbarth@gmail.com: fix drivers/dma/mv_xor]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Sebastian Hesselbarth <sebastian.hesselbarth@gmail.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index a1c486a88e88..179b38ffd351 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -182,10 +182,6 @@ async_memcpy(struct page *dest, struct page *src, unsigned int dest_offset,
 	     unsigned int src_offset, size_t len,
 	     struct async_submit_ctl *submit);
 
-struct dma_async_tx_descriptor *
-async_memset(struct page *dest, int val, unsigned int offset,
-	     size_t len, struct async_submit_ctl *submit);
-
 struct dma_async_tx_descriptor *async_trigger_callback(struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *

commit 0403e3827788d878163f9ef0541b748b0f88ca5d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:50 2009 -0700

    dmaengine: add fence support
    
    Some engines optimize operation by reading ahead in the descriptor chain
    such that descriptor2 may start execution before descriptor1 completes.
    If descriptor2 depends on the result from descriptor1 then a fence is
    required (on descriptor2) to disable this optimization.  The async_tx
    api could implicitly identify dependencies via the 'depend_tx'
    parameter, but that would constrain cases where the dependency chain
    only specifies a completion order rather than a data dependency.  So,
    provide an ASYNC_TX_FENCE to explicitly identify data dependencies.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 866e61c4e2e0..a1c486a88e88 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -58,11 +58,14 @@ struct dma_chan_ref {
  * array.
  * @ASYNC_TX_ACK: immediately ack the descriptor, precludes setting up a
  * dependency chain
+ * @ASYNC_TX_FENCE: specify that the next operation in the dependency
+ * chain uses this operation's result as an input
  */
 enum async_tx_flags {
 	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
 	ASYNC_TX_XOR_DROP_DST	 = (1 << 1),
 	ASYNC_TX_ACK		 = (1 << 2),
+	ASYNC_TX_FENCE		 = (1 << 3),
 };
 
 /**

commit 0a82a6239beecc95db6e05fe43ee62d16b381d38
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 12:20:37 2009 -0700

    async_tx: add support for asynchronous RAID6 recovery operations
    
     async_raid6_2data_recov() recovers two data disk failures
    
     async_raid6_datap_recov() recovers a data disk and the P disk
    
    These routines are a port of the synchronous versions found in
    drivers/md/raid6recov.c.  The primary difference is breaking out the xor
    operations into separate calls to async_xor.  Two helper routines are
    introduced to perform scalar multiplication where needed.
    async_sum_product() multiplies two sources by scalar coefficients and
    then sums (xor) the result.  async_mult() simply multiplies a single
    source by a scalar.
    
    This implemention also includes, in contrast to the original
    synchronous-only code, special case handling for the 4-disk and 5-disk
    array cases.  In these situations the default N-disk algorithm will
    present 0-source or 1-source operations to dma devices.  To cover for
    dma devices where the minimum source count is 2 we implement 4-disk and
    5-disk handling in the recovery code.
    
    [ Impact: asynchronous raid6 recovery routines for 2data and datap cases ]
    
    Cc: Yuri Tikhonov <yur@emcraft.com>
    Cc: Ilya Yanok <yanok@emcraft.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: David Woodhouse <David.Woodhouse@intel.com>
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index e6ce5f004f98..866e61c4e2e0 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -194,5 +194,13 @@ async_syndrome_val(struct page **blocks, unsigned int offset, int src_cnt,
 		   size_t len, enum sum_check_flags *pqres, struct page *spare,
 		   struct async_submit_ctl *submit);
 
+struct dma_async_tx_descriptor *
+async_raid6_2data_recov(int src_num, size_t bytes, int faila, int failb,
+			struct page **ptrs, struct async_submit_ctl *submit);
+
+struct dma_async_tx_descriptor *
+async_raid6_datap_recov(int src_num, size_t bytes, int faila,
+			struct page **ptrs, struct async_submit_ctl *submit);
+
 void async_tx_quiesce(struct dma_async_tx_descriptor **tx);
 #endif /* _ASYNC_TX_H_ */

commit b2f46fd8ef3dff2ab30f31126833f78b7480283a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 12:20:36 2009 -0700

    async_tx: add support for asynchronous GF multiplication
    
    [ Based on an original patch by Yuri Tikhonov ]
    
    This adds support for doing asynchronous GF multiplication by adding
    two additional functions to the async_tx API:
    
     async_gen_syndrome() does simultaneous XOR and Galois field
        multiplication of sources.
    
     async_syndrome_val() validates the given source buffers against known P
        and Q values.
    
    When a request is made to run async_pq against more than the hardware
    maximum number of supported sources we need to reuse the previous
    generated P and Q values as sources into the next operation.  Care must
    be taken to remove Q from P' and P from Q'.  For example to perform a 5
    source pq op with hardware that only supports 4 sources at a time the
    following approach is taken:
    
    p, q = PQ(src0, src1, src2, src3, COEF({01}, {02}, {04}, {08}))
    p', q' = PQ(p, q, q, src4, COEF({00}, {01}, {00}, {10}))
    
    p' = p + q + q + src4 = p + src4
    q' = {00}*p + {01}*q + {00}*q + {10}*src4 = q + {10}*src4
    
    Note: 4 is the minimum acceptable maxpq otherwise we punt to
    synchronous-software path.
    
    The DMA_PREP_CONTINUE flag indicates to the driver to reuse p and q as
    sources (in the above manner) and fill the remaining slots up to maxpq
    with the new sources/coefficients.
    
    Note1: Some devices have native support for P+Q continuation and can skip
    this extra work.  Devices with this capability can advertise it with
    dma_set_maxpq.  It is up to each driver how to handle the
    DMA_PREP_CONTINUE flag.
    
    Note2: The api supports disabling the generation of P when generating Q,
    this is ignored by the synchronous path but is implemented by some dma
    devices to save unnecessary writes.  In this case the continuation
    algorithm is simplified to only reuse Q as a source.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 12a2efcbd565..e6ce5f004f98 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -185,5 +185,14 @@ async_memset(struct page *dest, int val, unsigned int offset,
 
 struct dma_async_tx_descriptor *async_trigger_callback(struct async_submit_ctl *submit);
 
+struct dma_async_tx_descriptor *
+async_gen_syndrome(struct page **blocks, unsigned int offset, int src_cnt,
+		   size_t len, struct async_submit_ctl *submit);
+
+struct dma_async_tx_descriptor *
+async_syndrome_val(struct page **blocks, unsigned int offset, int src_cnt,
+		   size_t len, enum sum_check_flags *pqres, struct page *spare,
+		   struct async_submit_ctl *submit);
+
 void async_tx_quiesce(struct dma_async_tx_descriptor **tx);
 #endif /* _ASYNC_TX_H_ */

commit 95475e57113c66aac7583925736ed2e2d58c990d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 12:19:02 2009 -0700

    async_tx: remove walk of tx->parent chain in dma_wait_for_async_tx
    
    We currently walk the parent chain when waiting for a given tx to
    complete however this walk may race with the driver cleanup routine.
    The routines in async_raid6_recov.c may fall back to the synchronous
    path at any point so we need to be prepared to call async_tx_quiesce()
    (which calls  dma_wait_for_async_tx).  To remove the ->parent walk we
    guarantee that every time a dependency is attached ->issue_pending() is
    invoked, then we can simply poll the initial descriptor until
    completion.
    
    This also allows for a lighter weight 'issue pending' implementation as
    there is no longer a requirement to iterate through all the channels'
    ->issue_pending() routines as long as operations have been submitted in
    an ordered chain.  async_tx_issue_pending() is added for this case.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 3d21a2517518..12a2efcbd565 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -83,6 +83,24 @@ struct async_submit_ctl {
 
 #ifdef CONFIG_DMA_ENGINE
 #define async_tx_issue_pending_all dma_issue_pending_all
+
+/**
+ * async_tx_issue_pending - send pending descriptor to the hardware channel
+ * @tx: descriptor handle to retrieve hardware context
+ *
+ * Note: any dependent operations will have already been issued by
+ * async_tx_channel_switch, or (in the case of no channel switch) will
+ * be already pending on this channel.
+ */
+static inline void async_tx_issue_pending(struct dma_async_tx_descriptor *tx)
+{
+	if (likely(tx)) {
+		struct dma_chan *chan = tx->chan;
+		struct dma_device *dma = chan->device;
+
+		dma->device_issue_pending(chan);
+	}
+}
 #ifdef CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL
 #include <asm/async_tx.h>
 #else
@@ -98,6 +116,11 @@ static inline void async_tx_issue_pending_all(void)
 	do { } while (0);
 }
 
+static inline void async_tx_issue_pending(struct dma_async_tx_descriptor *tx)
+{
+	do { } while (0);
+}
+
 static inline struct dma_chan *
 async_tx_find_channel(struct async_submit_ctl *submit,
 		      enum dma_transaction_type tx_type, struct page **dst,

commit ad283ea4a3ce82cda2efe33163748a397b31b1eb
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Aug 29 19:09:26 2009 -0700

    async_tx: add sum check flags
    
    Replace the flat zero_sum_result with a collection of flags to contain
    the P (xor) zero-sum result, and the soon to be utilized Q (raid6 reed
    solomon syndrome) zero-sum result.  Use the SUM_CHECK_ namespace instead
    of DMA_ since these flags will be used on non-dma-zero-sum enabled
    platforms.
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 00cfb637ddf2..3d21a2517518 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -148,7 +148,7 @@ async_xor(struct page *dest, struct page **src_list, unsigned int offset,
 
 struct dma_async_tx_descriptor *
 async_xor_val(struct page *dest, struct page **src_list, unsigned int offset,
-	      int src_cnt, size_t len, u32 *result,
+	      int src_cnt, size_t len, enum sum_check_flags *result,
 	      struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *

commit a08abd8ca890a377521d65d493d174bebcaf694b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Jun 3 11:43:59 2009 -0700

    async_tx: structify submission arguments, add scribble
    
    Prepare the api for the arrival of a new parameter, 'scribble'.  This
    will allow callers to identify scratchpad memory for dma address or page
    address conversions.  As this adds yet another parameter, take this
    opportunity to convert the common submission parameters (flags,
    dependency, callback, and callback argument) into an object that is
    passed by reference.
    
    Also, take this opportunity to fix up the kerneldoc and add notes about
    the relevant ASYNC_TX_* flags for each routine.
    
    [ Impact: moves api pass-by-value parameters to a pass-by-reference struct ]
    
    Signed-off-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 9f14cd540cd2..00cfb637ddf2 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -65,6 +65,22 @@ enum async_tx_flags {
 	ASYNC_TX_ACK		 = (1 << 2),
 };
 
+/**
+ * struct async_submit_ctl - async_tx submission/completion modifiers
+ * @flags: submission modifiers
+ * @depend_tx: parent dependency of the current operation being submitted
+ * @cb_fn: callback routine to run at operation completion
+ * @cb_param: parameter for the callback routine
+ * @scribble: caller provided space for dma/page address conversions
+ */
+struct async_submit_ctl {
+	enum async_tx_flags flags;
+	struct dma_async_tx_descriptor *depend_tx;
+	dma_async_tx_callback cb_fn;
+	void *cb_param;
+	void *scribble;
+};
+
 #ifdef CONFIG_DMA_ENGINE
 #define async_tx_issue_pending_all dma_issue_pending_all
 #ifdef CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL
@@ -73,8 +89,8 @@ enum async_tx_flags {
 #define async_tx_find_channel(dep, type, dst, dst_count, src, src_count, len) \
 	 __async_tx_find_channel(dep, type)
 struct dma_chan *
-__async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
-	enum dma_transaction_type tx_type);
+__async_tx_find_channel(struct async_submit_ctl *submit,
+			enum dma_transaction_type tx_type);
 #endif /* CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL */
 #else
 static inline void async_tx_issue_pending_all(void)
@@ -83,9 +99,10 @@ static inline void async_tx_issue_pending_all(void)
 }
 
 static inline struct dma_chan *
-async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
-	enum dma_transaction_type tx_type, struct page **dst, int dst_count,
-	struct page **src, int src_count, size_t len)
+async_tx_find_channel(struct async_submit_ctl *submit,
+		      enum dma_transaction_type tx_type, struct page **dst,
+		      int dst_count, struct page **src, int src_count,
+		      size_t len)
 {
 	return NULL;
 }
@@ -97,46 +114,53 @@ async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
  * @cb_fn_param: parameter to pass to the callback routine
  */
 static inline void
-async_tx_sync_epilog(dma_async_tx_callback cb_fn, void *cb_fn_param)
+async_tx_sync_epilog(struct async_submit_ctl *submit)
+{
+	if (submit->cb_fn)
+		submit->cb_fn(submit->cb_param);
+}
+
+typedef union {
+	unsigned long addr;
+	struct page *page;
+	dma_addr_t dma;
+} addr_conv_t;
+
+static inline void
+init_async_submit(struct async_submit_ctl *args, enum async_tx_flags flags,
+		  struct dma_async_tx_descriptor *tx,
+		  dma_async_tx_callback cb_fn, void *cb_param,
+		  addr_conv_t *scribble)
 {
-	if (cb_fn)
-		cb_fn(cb_fn_param);
+	args->flags = flags;
+	args->depend_tx = tx;
+	args->cb_fn = cb_fn;
+	args->cb_param = cb_param;
+	args->scribble = scribble;
 }
 
-void
-async_tx_submit(struct dma_chan *chan, struct dma_async_tx_descriptor *tx,
-	enum async_tx_flags flags, struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+void async_tx_submit(struct dma_chan *chan, struct dma_async_tx_descriptor *tx,
+		     struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *
 async_xor(struct page *dest, struct page **src_list, unsigned int offset,
-	int src_cnt, size_t len, enum async_tx_flags flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+	  int src_cnt, size_t len, struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *
-async_xor_val(struct page *dest, struct page **src_list,
-	unsigned int offset, int src_cnt, size_t len,
-	u32 *result, enum async_tx_flags flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+async_xor_val(struct page *dest, struct page **src_list, unsigned int offset,
+	      int src_cnt, size_t len, u32 *result,
+	      struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *
 async_memcpy(struct page *dest, struct page *src, unsigned int dest_offset,
-	unsigned int src_offset, size_t len, enum async_tx_flags flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+	     unsigned int src_offset, size_t len,
+	     struct async_submit_ctl *submit);
 
 struct dma_async_tx_descriptor *
 async_memset(struct page *dest, int val, unsigned int offset,
-	size_t len, enum async_tx_flags flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+	     size_t len, struct async_submit_ctl *submit);
 
-struct dma_async_tx_descriptor *
-async_trigger_callback(enum async_tx_flags flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param);
+struct dma_async_tx_descriptor *async_trigger_callback(struct async_submit_ctl *submit);
 
 void async_tx_quiesce(struct dma_async_tx_descriptor **tx);
 #endif /* _ASYNC_TX_H_ */

commit 88ba2aa586c874681c072101287e15d40de7e6e2
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Apr 9 16:16:18 2009 -0700

    async_tx: kill ASYNC_TX_DEP_ACK flag
    
    In support of inter-channel chaining async_tx utilizes an ack flag to
    gate whether a dependent operation can be chained to another.  While the
    flag is not set the chain can be considered open for appending.  Setting
    the ack flag closes the chain and flags the descriptor for garbage
    collection.  The ASYNC_TX_DEP_ACK flag essentially means "close the
    chain after adding this dependency".  Since each operation can only have
    one child the api now implicitly sets the ack flag at dependency
    submission time.  This removes an unnecessary management burden from
    clients of the api.
    
    [ Impact: clean up and enforce one dependency per operation ]
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 513150d8c25b..9f14cd540cd2 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -58,13 +58,11 @@ struct dma_chan_ref {
  * array.
  * @ASYNC_TX_ACK: immediately ack the descriptor, precludes setting up a
  * dependency chain
- * @ASYNC_TX_DEP_ACK: ack the dependency descriptor.  Useful for chaining.
  */
 enum async_tx_flags {
 	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
 	ASYNC_TX_XOR_DROP_DST	 = (1 << 1),
-	ASYNC_TX_ACK		 = (1 << 3),
-	ASYNC_TX_DEP_ACK	 = (1 << 4),
+	ASYNC_TX_ACK		 = (1 << 2),
 };
 
 #ifdef CONFIG_DMA_ENGINE

commit 099f53cb50e45ef617a9f1d63ceec799e489418b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Apr 8 14:28:37 2009 -0700

    async_tx: rename zero_sum to val
    
    'zero_sum' does not properly describe the operation of generating parity
    and checking that it validates against an existing buffer.  Change the
    name of the operation to 'val' (for 'validate').  This is in
    anticipation of the p+q case where it is a requirement to identify the
    target parity buffers separately from the source buffers, because the
    target parity buffers will not have corresponding pq coefficients.
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 5fc2ef8d97fa..513150d8c25b 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -117,7 +117,7 @@ async_xor(struct page *dest, struct page **src_list, unsigned int offset,
 	dma_async_tx_callback cb_fn, void *cb_fn_param);
 
 struct dma_async_tx_descriptor *
-async_xor_zero_sum(struct page *dest, struct page **src_list,
+async_xor_val(struct page *dest, struct page **src_list,
 	unsigned int offset, int src_cnt, size_t len,
 	u32 *result, enum async_tx_flags flags,
 	struct dma_async_tx_descriptor *depend_tx,

commit 06164f3194e01ea4c76941ac60f541d656c8975f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Mar 25 09:13:25 2009 -0700

    async_tx: provide __async_inline for HAS_DMA=n archs
    
    To allow an async_tx routine to be compiled away on HAS_DMA=n arch it
    needs to be declared __always_inline otherwise the compiler may emit
    code and cause a link error.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 45f6297821bd..5fc2ef8d97fa 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -21,6 +21,15 @@
 #include <linux/spinlock.h>
 #include <linux/interrupt.h>
 
+/* on architectures without dma-mapping capabilities we need to ensure
+ * that the asynchronous path compiles away
+ */
+#ifdef CONFIG_HAS_DMA
+#define __async_inline
+#else
+#define __async_inline __always_inline
+#endif
+
 /**
  * dma_chan_ref - object used to manage dma channels received from the
  *   dmaengine core.

commit 2ba05622b8b143b0c95968ba59bddfbd6d2f2559
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 6 11:38:14 2009 -0700

    dmaengine: provide a common 'issue_pending_all' implementation
    
    async_tx and net_dma each have open-coded versions of issue_pending_all,
    so provide a common routine in dmaengine.
    
    The implementation needs to walk the global device list, so implement
    rcu to allow dma_issue_pending_all to run lockless.  Clients protect
    themselves from channel removal events by holding a dmaengine reference.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 1c816775f135..45f6297821bd 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -59,7 +59,7 @@ enum async_tx_flags {
 };
 
 #ifdef CONFIG_DMA_ENGINE
-void async_tx_issue_pending_all(void);
+#define async_tx_issue_pending_all dma_issue_pending_all
 #ifdef CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL
 #include <asm/async_tx.h>
 #else

commit 07f2211e4fbce6990722d78c4f04225da9c0e9cf
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Jan 5 17:14:31 2009 -0700

    dmaengine: remove dependency on async_tx
    
    async_tx.ko is a consumer of dma channels.  A circular dependency arises
    if modules in drivers/dma rely on common code in async_tx.ko.  It
    prevents either module from being unloaded.
    
    Move dma_wait_for_async_tx and async_tx_run_dependencies to dmaeninge.o
    where they should have been from the beginning.
    
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 0f50d4cc4360..1c816775f135 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -60,8 +60,6 @@ enum async_tx_flags {
 
 #ifdef CONFIG_DMA_ENGINE
 void async_tx_issue_pending_all(void);
-enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
-void async_tx_run_dependencies(struct dma_async_tx_descriptor *tx);
 #ifdef CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL
 #include <asm/async_tx.h>
 #else
@@ -77,19 +75,6 @@ static inline void async_tx_issue_pending_all(void)
 	do { } while (0);
 }
 
-static inline enum dma_status
-dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
-{
-	return DMA_SUCCESS;
-}
-
-static inline void
-async_tx_run_dependencies(struct dma_async_tx_descriptor *tx,
-	struct dma_chan *host_chan)
-{
-	do { } while (0);
-}
-
 static inline struct dma_chan *
 async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
 	enum dma_transaction_type tx_type, struct page **dst, int dst_count,

commit 3dce01713723bbcc92562bd4488e8b840a4f786c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 17 17:59:55 2008 -0700

    async_tx: remove depend_tx from async_tx_sync_epilog
    
    All callers of async_tx_sync_epilog have called async_tx_quiesce on the
    depend_tx, so async_tx_sync_epilog need only call the callback to
    complete the operation.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 9f0e7bd5bdc9..0f50d4cc4360 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -101,21 +101,14 @@ async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
 
 /**
  * async_tx_sync_epilog - actions to take if an operation is run synchronously
- * @flags: async_tx flags
- * @depend_tx: transaction depends on depend_tx
  * @cb_fn: function to call when the transaction completes
  * @cb_fn_param: parameter to pass to the callback routine
  */
 static inline void
-async_tx_sync_epilog(unsigned long flags,
-	struct dma_async_tx_descriptor *depend_tx,
-	dma_async_tx_callback cb_fn, void *cb_fn_param)
+async_tx_sync_epilog(dma_async_tx_callback cb_fn, void *cb_fn_param)
 {
 	if (cb_fn)
 		cb_fn(cb_fn_param);
-
-	if (depend_tx && (flags & ASYNC_TX_DEP_ACK))
-		async_tx_ack(depend_tx);
 }
 
 void

commit d2c52b7983b95bb3fc2a784e479f832f142d4523
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 17 17:59:55 2008 -0700

    async_tx: export async_tx_quiesce
    
    Replace open coded "wait and acknowledge" instances with async_tx_quiesce.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index eb640f0acfac..9f0e7bd5bdc9 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -152,4 +152,6 @@ struct dma_async_tx_descriptor *
 async_trigger_callback(enum async_tx_flags flags,
 	struct dma_async_tx_descriptor *depend_tx,
 	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+void async_tx_quiesce(struct dma_async_tx_descriptor **tx);
 #endif /* _ASYNC_TX_H_ */

commit 47437b2c9a64315efeb3d84e97ffefd6c3c67ef1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Feb 2 19:49:59 2008 -0700

    async_tx: allow architecture specific async_tx_find_channel implementations
    
    The source and destination addresses are included to allow channel
    selection based on address alignment.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Haavard Skinnemoen <hskinnemoen@atmel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index 3d59d371dd32..eb640f0acfac 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -62,9 +62,15 @@ enum async_tx_flags {
 void async_tx_issue_pending_all(void);
 enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
 void async_tx_run_dependencies(struct dma_async_tx_descriptor *tx);
+#ifdef CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL
+#include <asm/async_tx.h>
+#else
+#define async_tx_find_channel(dep, type, dst, dst_count, src, src_count, len) \
+	 __async_tx_find_channel(dep, type)
 struct dma_chan *
-async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
+__async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
 	enum dma_transaction_type tx_type);
+#endif /* CONFIG_ARCH_HAS_ASYNC_TX_FIND_CHANNEL */
 #else
 static inline void async_tx_issue_pending_all(void)
 {
@@ -86,7 +92,8 @@ async_tx_run_dependencies(struct dma_async_tx_descriptor *tx,
 
 static inline struct dma_chan *
 async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
-	enum dma_transaction_type tx_type)
+	enum dma_transaction_type tx_type, struct page **dst, int dst_count,
+	struct page **src, int src_count, size_t len)
 {
 	return NULL;
 }

commit d909b347591a23c5a2c324fbccd4c9c966f31c67
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Feb 2 19:30:14 2008 -0700

    async_tx: kill ASYNC_TX_ASSUME_COHERENT
    
    Remove the unused ASYNC_TX_ASSUME_COHERENT flag.  Async_tx is
    meant to hide the difference between asynchronous hardware and synchronous
    software operations, this flag requires clients to understand cache
    coherency consequences of the async path.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Haavard Skinnemoen <hskinnemoen@atmel.com>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index bdca3f1b3213..3d59d371dd32 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -47,7 +47,6 @@ struct dma_chan_ref {
  * address is an implied source, whereas the asynchronous case it must be listed
  * as a source.  The destination address must be the first address in the source
  * array.
- * @ASYNC_TX_ASSUME_COHERENT: skip cache maintenance operations
  * @ASYNC_TX_ACK: immediately ack the descriptor, precludes setting up a
  * dependency chain
  * @ASYNC_TX_DEP_ACK: ack the dependency descriptor.  Useful for chaining.
@@ -55,7 +54,6 @@ struct dma_chan_ref {
 enum async_tx_flags {
 	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
 	ASYNC_TX_XOR_DROP_DST	 = (1 << 1),
-	ASYNC_TX_ASSUME_COHERENT = (1 << 2),
 	ASYNC_TX_ACK		 = (1 << 3),
 	ASYNC_TX_DEP_ACK	 = (1 << 4),
 };

commit eb0645a8b1f14da300f40bb9f424640cd1181fbf
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jul 20 00:31:46 2007 -0700

    async_tx: fix kmap_atomic usage in async_memcpy
    
    Andrew Morton:
            [async_memcpy] is very wrong if both ASYNC_TX_KMAP_DST and
            ASYNC_TX_KMAP_SRC can ever be set.  We'll end up using the same kmap
            slot for both src add dest and we get either corrupted data or a BUG.
    
    Evgeniy Polyakov:
            Btw, shouldn't it always be kmap_atomic() even if flag is not set.
            That pages are usual one returned by alloc_page().
    
    So fix the usage of kmap_atomic and kill the ASYNC_TX_KMAP_DST and
    ASYNC_TX_KMAP_SRC flags.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Evgeniy Polyakov <johnpol@2ka.mipt.ru>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
index ff1255079fa1..bdca3f1b3213 100644
--- a/include/linux/async_tx.h
+++ b/include/linux/async_tx.h
@@ -51,10 +51,6 @@ struct dma_chan_ref {
  * @ASYNC_TX_ACK: immediately ack the descriptor, precludes setting up a
  * dependency chain
  * @ASYNC_TX_DEP_ACK: ack the dependency descriptor.  Useful for chaining.
- * @ASYNC_TX_KMAP_SRC: if the transaction is to be performed synchronously
- * take an atomic mapping (KM_USER0) on the source page(s)
- * @ASYNC_TX_KMAP_DST: if the transaction is to be performed synchronously
- * take an atomic mapping (KM_USER0) on the dest page(s)
  */
 enum async_tx_flags {
 	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
@@ -62,8 +58,6 @@ enum async_tx_flags {
 	ASYNC_TX_ASSUME_COHERENT = (1 << 2),
 	ASYNC_TX_ACK		 = (1 << 3),
 	ASYNC_TX_DEP_ACK	 = (1 << 4),
-	ASYNC_TX_KMAP_SRC	 = (1 << 5),
-	ASYNC_TX_KMAP_DST	 = (1 << 6),
 };
 
 #ifdef CONFIG_DMA_ENGINE

commit 9bc89cd82d6f88fb0ca39b30445c329a430fd66b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jan 2 11:10:44 2007 -0700

    async_tx: add the async_tx api
    
    The async_tx api provides methods for describing a chain of asynchronous
    bulk memory transfers/transforms with support for inter-transactional
    dependencies.  It is implemented as a dmaengine client that smooths over
    the details of different hardware offload engine implementations.  Code
    that is written to the api can optimize for asynchronous operation and the
    api will fit the chain of operations to the available offload resources.
    
            I imagine that any piece of ADMA hardware would register with the
            'async_*' subsystem, and a call to async_X would be routed as
            appropriate, or be run in-line. - Neil Brown
    
    async_tx exploits the capabilities of struct dma_async_tx_descriptor to
    provide an api of the following general format:
    
    struct dma_async_tx_descriptor *
    async_<operation>(..., struct dma_async_tx_descriptor *depend_tx,
                            dma_async_tx_callback cb_fn, void *cb_param)
    {
            struct dma_chan *chan = async_tx_find_channel(depend_tx, <operation>);
            struct dma_device *device = chan ? chan->device : NULL;
            int int_en = cb_fn ? 1 : 0;
            struct dma_async_tx_descriptor *tx = device ?
                    device->device_prep_dma_<operation>(chan, len, int_en) : NULL;
    
            if (tx) { /* run <operation> asynchronously */
                    ...
                    tx->tx_set_dest(addr, tx, index);
                    ...
                    tx->tx_set_src(addr, tx, index);
                    ...
                    async_tx_submit(chan, tx, flags, depend_tx, cb_fn, cb_param);
            } else { /* run <operation> synchronously */
                    ...
                    <operation>
                    ...
                    async_tx_sync_epilog(flags, depend_tx, cb_fn, cb_param);
            }
    
            return tx;
    }
    
    async_tx_find_channel() returns a capable channel from its pool.  The
    channel pool is organized as a per-cpu array of channel pointers.  The
    async_tx_rebalance() routine is tasked with managing these arrays.  In the
    uniprocessor case async_tx_rebalance() tries to spread responsibility
    evenly over channels of similar capabilities.  For example if there are two
    copy+xor channels, one will handle copy operations and the other will
    handle xor.  In the SMP case async_tx_rebalance() attempts to spread the
    operations evenly over the cpus, e.g. cpu0 gets copy channel0 and xor
    channel0 while cpu1 gets copy channel 1 and xor channel 1.  When a
    dependency is specified async_tx_find_channel defaults to keeping the
    operation on the same channel.  A xor->copy->xor chain will stay on one
    channel if it supports both operation types, otherwise the transaction will
    transition between a copy and a xor resource.
    
    Currently the raid5 implementation in the MD raid456 driver has been
    converted to the async_tx api.  A driver for the offload engines on the
    Intel Xscale series of I/O processors, iop-adma, is provided in a later
    commit.  With the iop-adma driver and async_tx, raid456 is able to offload
    copy, xor, and xor-zero-sum operations to hardware engines.
    
    On iop342 tiobench showed higher throughput for sequential writes (20 - 30%
    improvement) and sequential reads to a degraded array (40 - 55%
    improvement).  For the other cases performance was roughly equal, +/- a few
    percentage points.  On a x86-smp platform the performance of the async_tx
    implementation (in synchronous mode) was also +/- a few percentage points
    of the original implementation.  According to 'top' on iop342 CPU
    utilization drops from ~50% to ~15% during a 'resync' while the speed
    according to /proc/mdstat doubles from ~25 MB/s to ~50 MB/s.
    
    The tiobench command line used for testing was: tiobench --size 2048
    --block 4096 --block 131072 --dir /mnt/raid --numruns 5
    * iop342 had 1GB of memory available
    
    Details:
    * if CONFIG_DMA_ENGINE=n the asynchronous path is compiled away by making
      async_tx_find_channel a static inline routine that always returns NULL
    * when a callback is specified for a given transaction an interrupt will
      fire at operation completion time and the callback will occur in a
      tasklet.  if the the channel does not support interrupts then a live
      polling wait will be performed
    * the api is written as a dmaengine client that requests all available
      channels
    * In support of dependencies the api implicitly schedules channel-switch
      interrupts.  The interrupt triggers the cleanup tasklet which causes
      pending operations to be scheduled on the next channel
    * Xor engines treat an xor destination address differently than a software
      xor routine.  To the software routine the destination address is an implied
      source, whereas engines treat it as a write-only destination.  This patch
      modifies the xor_blocks routine to take a an explicit destination address
      to mirror the hardware.
    
    Changelog:
    * fixed a leftover debug print
    * don't allow callbacks in async_interrupt_cond
    * fixed xor_block changes
    * fixed usage of ASYNC_TX_XOR_DROP_DEST
    * drop dma mapping methods, suggested by Chris Leech
    * printk warning fixups from Andrew Morton
    * don't use inline in C files, Adrian Bunk
    * select the API when MD is enabled
    * BUG_ON xor source counts <= 1
    * implicitly handle hardware concerns like channel switching and
      interrupts, Neil Brown
    * remove the per operation type list, and distribute operation capabilities
      evenly amongst the available channels
    * simplify async_tx_find_channel to optimize the fast path
    * introduce the channel_table_initialized flag to prevent early calls to
      the api
    * reorganize the code to mimic crypto
    * include mm.h as not all archs include it in dma-mapping.h
    * make the Kconfig options non-user visible, Adrian Bunk
    * move async_tx under crypto since it is meant as 'core' functionality, and
      the two may share algorithms in the future
    * move large inline functions into c files
    * checkpatch.pl fixes
    * gpl v2 only correction
    
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Acked-By: NeilBrown <neilb@suse.de>

diff --git a/include/linux/async_tx.h b/include/linux/async_tx.h
new file mode 100644
index 000000000000..ff1255079fa1
--- /dev/null
+++ b/include/linux/async_tx.h
@@ -0,0 +1,156 @@
+/*
+ * Copyright Â© 2006, Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ */
+#ifndef _ASYNC_TX_H_
+#define _ASYNC_TX_H_
+#include <linux/dmaengine.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+
+/**
+ * dma_chan_ref - object used to manage dma channels received from the
+ *   dmaengine core.
+ * @chan - the channel being tracked
+ * @node - node for the channel to be placed on async_tx_master_list
+ * @rcu - for list_del_rcu
+ * @count - number of times this channel is listed in the pool
+ *	(for channels with multiple capabiities)
+ */
+struct dma_chan_ref {
+	struct dma_chan *chan;
+	struct list_head node;
+	struct rcu_head rcu;
+	atomic_t count;
+};
+
+/**
+ * async_tx_flags - modifiers for the async_* calls
+ * @ASYNC_TX_XOR_ZERO_DST: this flag must be used for xor operations where the
+ * the destination address is not a source.  The asynchronous case handles this
+ * implicitly, the synchronous case needs to zero the destination block.
+ * @ASYNC_TX_XOR_DROP_DST: this flag must be used if the destination address is
+ * also one of the source addresses.  In the synchronous case the destination
+ * address is an implied source, whereas the asynchronous case it must be listed
+ * as a source.  The destination address must be the first address in the source
+ * array.
+ * @ASYNC_TX_ASSUME_COHERENT: skip cache maintenance operations
+ * @ASYNC_TX_ACK: immediately ack the descriptor, precludes setting up a
+ * dependency chain
+ * @ASYNC_TX_DEP_ACK: ack the dependency descriptor.  Useful for chaining.
+ * @ASYNC_TX_KMAP_SRC: if the transaction is to be performed synchronously
+ * take an atomic mapping (KM_USER0) on the source page(s)
+ * @ASYNC_TX_KMAP_DST: if the transaction is to be performed synchronously
+ * take an atomic mapping (KM_USER0) on the dest page(s)
+ */
+enum async_tx_flags {
+	ASYNC_TX_XOR_ZERO_DST	 = (1 << 0),
+	ASYNC_TX_XOR_DROP_DST	 = (1 << 1),
+	ASYNC_TX_ASSUME_COHERENT = (1 << 2),
+	ASYNC_TX_ACK		 = (1 << 3),
+	ASYNC_TX_DEP_ACK	 = (1 << 4),
+	ASYNC_TX_KMAP_SRC	 = (1 << 5),
+	ASYNC_TX_KMAP_DST	 = (1 << 6),
+};
+
+#ifdef CONFIG_DMA_ENGINE
+void async_tx_issue_pending_all(void);
+enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);
+void async_tx_run_dependencies(struct dma_async_tx_descriptor *tx);
+struct dma_chan *
+async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
+	enum dma_transaction_type tx_type);
+#else
+static inline void async_tx_issue_pending_all(void)
+{
+	do { } while (0);
+}
+
+static inline enum dma_status
+dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)
+{
+	return DMA_SUCCESS;
+}
+
+static inline void
+async_tx_run_dependencies(struct dma_async_tx_descriptor *tx,
+	struct dma_chan *host_chan)
+{
+	do { } while (0);
+}
+
+static inline struct dma_chan *
+async_tx_find_channel(struct dma_async_tx_descriptor *depend_tx,
+	enum dma_transaction_type tx_type)
+{
+	return NULL;
+}
+#endif
+
+/**
+ * async_tx_sync_epilog - actions to take if an operation is run synchronously
+ * @flags: async_tx flags
+ * @depend_tx: transaction depends on depend_tx
+ * @cb_fn: function to call when the transaction completes
+ * @cb_fn_param: parameter to pass to the callback routine
+ */
+static inline void
+async_tx_sync_epilog(unsigned long flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param)
+{
+	if (cb_fn)
+		cb_fn(cb_fn_param);
+
+	if (depend_tx && (flags & ASYNC_TX_DEP_ACK))
+		async_tx_ack(depend_tx);
+}
+
+void
+async_tx_submit(struct dma_chan *chan, struct dma_async_tx_descriptor *tx,
+	enum async_tx_flags flags, struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+struct dma_async_tx_descriptor *
+async_xor(struct page *dest, struct page **src_list, unsigned int offset,
+	int src_cnt, size_t len, enum async_tx_flags flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+struct dma_async_tx_descriptor *
+async_xor_zero_sum(struct page *dest, struct page **src_list,
+	unsigned int offset, int src_cnt, size_t len,
+	u32 *result, enum async_tx_flags flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+struct dma_async_tx_descriptor *
+async_memcpy(struct page *dest, struct page *src, unsigned int dest_offset,
+	unsigned int src_offset, size_t len, enum async_tx_flags flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+struct dma_async_tx_descriptor *
+async_memset(struct page *dest, int val, unsigned int offset,
+	size_t len, enum async_tx_flags flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+
+struct dma_async_tx_descriptor *
+async_trigger_callback(enum async_tx_flags flags,
+	struct dma_async_tx_descriptor *depend_tx,
+	dma_async_tx_callback cb_fn, void *cb_fn_param);
+#endif /* _ASYNC_TX_H_ */
