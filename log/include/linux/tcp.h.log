commit 480aeb9639d6a077c611b303a22f9b1e5937d081
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:25 2020 +0200

    tcp: add tcp_sock_set_keepcnt
    
    Add a helper to directly set the TCP_KEEPCNT sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1f9bada00faa..9aac824c523c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -498,6 +498,7 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 		  int shiftlen);
 
 void tcp_sock_set_cork(struct sock *sk, bool on);
+int tcp_sock_set_keepcnt(struct sock *sk, int val);
 int tcp_sock_set_keepidle(struct sock *sk, int val);
 int tcp_sock_set_keepintvl(struct sock *sk, int val);
 void tcp_sock_set_nodelay(struct sock *sk);

commit d41ecaac903c9f4658a71d4e7a708673cfb5abba
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:24 2020 +0200

    tcp: add tcp_sock_set_keepintvl
    
    Add a helper to directly set the TCP_KEEPINTVL sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 5724dd84a85e..1f9bada00faa 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -499,6 +499,7 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 
 void tcp_sock_set_cork(struct sock *sk, bool on);
 int tcp_sock_set_keepidle(struct sock *sk, int val);
+int tcp_sock_set_keepintvl(struct sock *sk, int val);
 void tcp_sock_set_nodelay(struct sock *sk);
 void tcp_sock_set_quickack(struct sock *sk, int val);
 int tcp_sock_set_syncnt(struct sock *sk, int val);

commit 71c48eb81c9ecb6fed49dc33e7c9b621fdcb7bf8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:23 2020 +0200

    tcp: add tcp_sock_set_keepidle
    
    Add a helper to directly set the TCP_KEEP_IDLE sockopt from kernel
    space without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index de682143efe4..5724dd84a85e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -498,6 +498,7 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 		  int shiftlen);
 
 void tcp_sock_set_cork(struct sock *sk, bool on);
+int tcp_sock_set_keepidle(struct sock *sk, int val);
 void tcp_sock_set_nodelay(struct sock *sk);
 void tcp_sock_set_quickack(struct sock *sk, int val);
 int tcp_sock_set_syncnt(struct sock *sk, int val);

commit c488aeadcbd002a992593e6090d54e8ac27c4310
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:22 2020 +0200

    tcp: add tcp_sock_set_user_timeout
    
    Add a helper to directly set the TCP_USER_TIMEOUT sockopt from kernel
    space without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 6aa4ae5ebf3d..de682143efe4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -501,5 +501,6 @@ void tcp_sock_set_cork(struct sock *sk, bool on);
 void tcp_sock_set_nodelay(struct sock *sk);
 void tcp_sock_set_quickack(struct sock *sk, int val);
 int tcp_sock_set_syncnt(struct sock *sk, int val);
+void tcp_sock_set_user_timeout(struct sock *sk, u32 val);
 
 #endif	/* _LINUX_TCP_H */

commit 557eadfcc5ee8f8fa98a795e05ed21db58a65db5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:21 2020 +0200

    tcp: add tcp_sock_set_syncnt
    
    Add a helper to directly set the TCP_SYNCNT sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2eaf8320b9db..6aa4ae5ebf3d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -500,5 +500,6 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 void tcp_sock_set_cork(struct sock *sk, bool on);
 void tcp_sock_set_nodelay(struct sock *sk);
 void tcp_sock_set_quickack(struct sock *sk, int val);
+int tcp_sock_set_syncnt(struct sock *sk, int val);
 
 #endif	/* _LINUX_TCP_H */

commit ddd061b8daed3ce0c01109a69c9a2a9f9669f01a
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:20 2020 +0200

    tcp: add tcp_sock_set_quickack
    
    Add a helper to directly set the TCP_QUICKACK sockopt from kernel space
    without going through a fake uaccess.  Cleanup the callers to avoid
    pointless wrappers now that this is a simple function call.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9e42c7fe50a8..2eaf8320b9db 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -499,5 +499,6 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 
 void tcp_sock_set_cork(struct sock *sk, bool on);
 void tcp_sock_set_nodelay(struct sock *sk);
+void tcp_sock_set_quickack(struct sock *sk, int val);
 
 #endif	/* _LINUX_TCP_H */

commit 12abc5ee7873a085cc280240822b8ac53c86fecd
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:19 2020 +0200

    tcp: add tcp_sock_set_nodelay
    
    Add a helper to directly set the TCP_NODELAY sockopt from kernel space
    without going through a fake uaccess.  Cleanup the callers to avoid
    pointless wrappers now that this is a simple function call.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Acked-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 889eeb2256c2..9e42c7fe50a8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -498,5 +498,6 @@ int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 		  int shiftlen);
 
 void tcp_sock_set_cork(struct sock *sk, bool on);
+void tcp_sock_set_nodelay(struct sock *sk);
 
 #endif	/* _LINUX_TCP_H */

commit db10538a4b997a77a1fd561adaaa58afc7dcfa2f
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:18 2020 +0200

    tcp: add tcp_sock_set_cork
    
    Add a helper to directly set the TCP_CORK sockopt from kernel space
    without going through a fake uaccess.  Cleanup the callers to avoid
    pointless wrappers now that this is a simple function call.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index bf44e85d709d..889eeb2256c2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -497,4 +497,6 @@ static inline u16 tcp_mss_clamp(const struct tcp_sock *tp, u16 mss)
 int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
 		  int shiftlen);
 
+void tcp_sock_set_cork(struct sock *sk, bool on);
+
 #endif	/* _LINUX_TCP_H */

commit 90bf45134d55d626ae2713cac50cda10c6c8b0c2
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Fri May 15 19:22:15 2020 +0200

    mptcp: add new sock flag to deal with join subflows
    
    MP_JOIN subflows must not land into the accept queue.
    Currently tcp_check_req() calls an mptcp specific helper
    to detect such scenario.
    
    Such helper leverages the subflow context to check for
    MP_JOIN subflows. We need to deal also with MP JOIN
    failures, even when the subflow context is not available
    due allocation failure.
    
    A possible solution would be changing the syn_recv_sock()
    signature to allow returning a more descriptive action/
    error code and deal with that in tcp_check_req().
    
    Since the above need is MPTCP specific, this patch instead
    uses a TCP request socket hole to add a MPTCP specific flag.
    Such flag is used by the MPTCP syn_recv_sock() to tell
    tcp_check_req() how to deal with the request socket.
    
    This change is a no-op for !MPTCP build, and makes the
    MPTCP code simpler. It allows also the next patch to deal
    correctly with MP JOIN failure.
    
    v1 -> v2:
     - be more conservative on drop_req initialization (Mat)
    
    RFC -> v1:
     - move the drop_req bit inside tcp_request_sock (Eric)
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Reviewed-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e60db06ec28d..bf44e85d709d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -120,6 +120,9 @@ struct tcp_request_sock {
 	u64				snt_synack; /* first SYNACK sent time */
 	bool				tfo_listener;
 	bool				is_mptcp;
+#if IS_ENABLED(CONFIG_MPTCP)
+	bool				drop_req;
+#endif
 	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;

commit 3793faad7b5b730941b2efbc252d14374b60843a
Merge: ae1804de93f6 a811c1fa0a02
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 6 22:10:13 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Conflicts were all overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2b195850128f5bafde177b12489d9fa27962cc1e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Apr 30 10:35:41 2020 -0700

    tcp: add tp->dup_ack_counter
    
    In commit 86de5921a3d5 ("tcp: defer SACK compression after DupThresh")
    I added a TCP_FASTRETRANS_THRESH bias to tp->compressed_ack in order
    to enable sack compression only after 3 dupacks.
    
    Since we plan to relax this rule for flows that involve
    stacks not requiring this old rule, this patch adds
    a distinct tp->dup_ack_counter.
    
    This means the TCP_FASTRETRANS_THRESH value is now used
    in a single location that a future patch can adjust:
    
            if (tp->dup_ack_counter < TCP_FASTRETRANS_THRESH) {
                    tp->dup_ack_counter++;
                    goto send_now;
            }
    
    This patch also introduces tcp_sack_compress_send_ack()
    helper to ease following patch comprehension.
    
    This patch refines LINUX_MIB_TCPACKCOMPRESSED to not
    count the acks that we had to send if the timer expires
    or tcp_sack_compress_send_ack() is sending an ack.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 421c99c12291..2c6f87e9f0cf 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -268,6 +268,7 @@ struct tcp_sock {
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
 	u8	compressed_ack;
+	u8	dup_ack_counter;
 	u32	chrono_start;	/* Start time in jiffies of a TCP chrono */
 	u32	chrono_stat[3];	/* Time in jiffies for chrono_stat stats */
 	u8	chrono_type:2,	/* current chronograph type */

commit cfde141ea3faa30e362bbdb5c28001bbbdb0b8e0
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Thu Apr 30 15:01:52 2020 +0200

    mptcp: move option parsing into mptcp_incoming_options()
    
    The mptcp_options_received structure carries several per
    packet flags (mp_capable, mp_join, etc.). Such fields must
    be cleared on each packet, even on dropped ones or packet
    not carrying any MPTCP options, but the current mptcp
    code clears them only on TCP option reset.
    
    On several races/corner cases we end-up with stray bits in
    incoming options, leading to WARN_ON splats. e.g.:
    
    [  171.164906] Bad mapping: ssn=32714 map_seq=1 map_data_len=32713
    [  171.165006] WARNING: CPU: 1 PID: 5026 at net/mptcp/subflow.c:533 warn_bad_map (linux-mptcp/net/mptcp/subflow.c:533 linux-mptcp/net/mptcp/subflow.c:531)
    [  171.167632] Modules linked in: ip6_vti ip_vti ip_gre ipip sit tunnel4 ip_tunnel geneve ip6_udp_tunnel udp_tunnel macsec macvtap tap ipvlan macvlan 8021q garp mrp xfrm_interface veth netdevsim nlmon dummy team bonding vcan bridge stp llc ip6_gre gre ip6_tunnel tunnel6 tun binfmt_misc intel_rapl_msr intel_rapl_common rfkill kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel joydev virtio_balloon pcspkr i2c_piix4 sunrpc ip_tables xfs libcrc32c crc32c_intel serio_raw virtio_console ata_generic virtio_blk virtio_net net_failover failover ata_piix libata
    [  171.199464] CPU: 1 PID: 5026 Comm: repro Not tainted 5.7.0-rc1.mptcp_f227fdf5d388+ #95
    [  171.200886] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-2.fc30 04/01/2014
    [  171.202546] RIP: 0010:warn_bad_map (linux-mptcp/net/mptcp/subflow.c:533 linux-mptcp/net/mptcp/subflow.c:531)
    [  171.206537] Code: c1 ea 03 0f b6 14 02 48 89 f8 83 e0 07 83 c0 03 38 d0 7c 04 84 d2 75 1d 8b 55 3c 44 89 e6 48 c7 c7 20 51 13 95 e8 37 8b 22 fe <0f> 0b 48 83 c4 08 5b 5d 41 5c c3 89 4c 24 04 e8 db d6 94 fe 8b 4c
    [  171.220473] RSP: 0018:ffffc90000150560 EFLAGS: 00010282
    [  171.221639] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000
    [  171.223108] RDX: 0000000000000000 RSI: 0000000000000008 RDI: fffff5200002a09e
    [  171.224388] RBP: ffff8880aa6e3c00 R08: 0000000000000001 R09: fffffbfff2ec9955
    [  171.225706] R10: ffffffff9764caa7 R11: fffffbfff2ec9954 R12: 0000000000007fca
    [  171.227211] R13: ffff8881066f4a7f R14: ffff8880aa6e3c00 R15: 0000000000000020
    [  171.228460] FS:  00007f8623719740(0000) GS:ffff88810be00000(0000) knlGS:0000000000000000
    [  171.230065] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  171.231303] CR2: 00007ffdab190a50 CR3: 00000001038ea006 CR4: 0000000000160ee0
    [  171.232586] Call Trace:
    [  171.233109]  <IRQ>
    [  171.233531] get_mapping_status (linux-mptcp/net/mptcp/subflow.c:691)
    [  171.234371] mptcp_subflow_data_available (linux-mptcp/net/mptcp/subflow.c:736 linux-mptcp/net/mptcp/subflow.c:832)
    [  171.238181] subflow_state_change (linux-mptcp/net/mptcp/subflow.c:1085 (discriminator 1))
    [  171.239066] tcp_fin (linux-mptcp/net/ipv4/tcp_input.c:4217)
    [  171.240123] tcp_data_queue (linux-mptcp/./include/linux/compiler.h:199 linux-mptcp/net/ipv4/tcp_input.c:4822)
    [  171.245083] tcp_rcv_established (linux-mptcp/./include/linux/skbuff.h:1785 linux-mptcp/./include/net/tcp.h:1774 linux-mptcp/./include/net/tcp.h:1847 linux-mptcp/net/ipv4/tcp_input.c:5238 linux-mptcp/net/ipv4/tcp_input.c:5730)
    [  171.254089] tcp_v4_rcv (linux-mptcp/./include/linux/spinlock.h:393 linux-mptcp/net/ipv4/tcp_ipv4.c:2009)
    [  171.258969] ip_protocol_deliver_rcu (linux-mptcp/net/ipv4/ip_input.c:204 (discriminator 1))
    [  171.260214] ip_local_deliver_finish (linux-mptcp/./include/linux/rcupdate.h:651 linux-mptcp/net/ipv4/ip_input.c:232)
    [  171.261389] ip_local_deliver (linux-mptcp/./include/linux/netfilter.h:307 linux-mptcp/./include/linux/netfilter.h:301 linux-mptcp/net/ipv4/ip_input.c:252)
    [  171.265884] ip_rcv (linux-mptcp/./include/linux/netfilter.h:307 linux-mptcp/./include/linux/netfilter.h:301 linux-mptcp/net/ipv4/ip_input.c:539)
    [  171.273666] process_backlog (linux-mptcp/./include/linux/rcupdate.h:651 linux-mptcp/net/core/dev.c:6135)
    [  171.275328] net_rx_action (linux-mptcp/net/core/dev.c:6572 linux-mptcp/net/core/dev.c:6640)
    [  171.280472] __do_softirq (linux-mptcp/./arch/x86/include/asm/jump_label.h:25 linux-mptcp/./include/linux/jump_label.h:200 linux-mptcp/./include/trace/events/irq.h:142 linux-mptcp/kernel/softirq.c:293)
    [  171.281379] do_softirq_own_stack (linux-mptcp/arch/x86/entry/entry_64.S:1083)
    [  171.282358]  </IRQ>
    
    We could address the issue clearing explicitly the relevant fields
    in several places - tcp_parse_option, tcp_fast_parse_options,
    possibly others.
    
    Instead we move the MPTCP option parsing into the already existing
    mptcp ingress hook, so that we need to clear the fields in a single
    place.
    
    This allows us dropping an MPTCP hook from the TCP code and
    removing the quite large mptcp_options_received from the tcp_sock
    struct. On the flip side, the MPTCP sockets will traverse the
    option space twice (in tcp_parse_option() and in
    mptcp_incoming_options(). That looks acceptable: we already
    do that for syn and 3rd ack packets, plain TCP socket will
    benefit from it, and even MPTCP sockets will experience better
    code locality, reducing the jumps between TCP and MPTCP code.
    
    v1 -> v2:
     - rebased on current '-net' tree
    
    Fixes: 648ef4b88673 ("mptcp: Implement MPTCP receive path")
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 421c99c12291..4f8159e90ce1 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -78,47 +78,6 @@ struct tcp_sack_block {
 #define TCP_SACK_SEEN     (1 << 0)   /*1 = peer is SACK capable, */
 #define TCP_DSACK_SEEN    (1 << 2)   /*1 = DSACK was received from peer*/
 
-#if IS_ENABLED(CONFIG_MPTCP)
-struct mptcp_options_received {
-	u64	sndr_key;
-	u64	rcvr_key;
-	u64	data_ack;
-	u64	data_seq;
-	u32	subflow_seq;
-	u16	data_len;
-	u16	mp_capable : 1,
-		mp_join : 1,
-		dss : 1,
-		add_addr : 1,
-		rm_addr : 1,
-		family : 4,
-		echo : 1,
-		backup : 1;
-	u32	token;
-	u32	nonce;
-	u64	thmac;
-	u8	hmac[20];
-	u8	join_id;
-	u8	use_map:1,
-		dsn64:1,
-		data_fin:1,
-		use_ack:1,
-		ack64:1,
-		mpc_map:1,
-		__unused:2;
-	u8	addr_id;
-	u8	rm_id;
-	union {
-		struct in_addr	addr;
-#if IS_ENABLED(CONFIG_MPTCP_IPV6)
-		struct in6_addr	addr6;
-#endif
-	};
-	u64	ahmac;
-	u16	port;
-};
-#endif
-
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
 	int	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
@@ -136,9 +95,6 @@ struct tcp_options_received {
 	u8	num_sacks;	/* Number of SACK blocks		*/
 	u16	user_mss;	/* mss requested by user in ioctl	*/
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
-#if IS_ENABLED(CONFIG_MPTCP)
-	struct mptcp_options_received	mptcp;
-#endif
 };
 
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
@@ -148,13 +104,6 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 #if IS_ENABLED(CONFIG_SMC)
 	rx_opt->smc_ok = 0;
 #endif
-#if IS_ENABLED(CONFIG_MPTCP)
-	rx_opt->mptcp.mp_capable = 0;
-	rx_opt->mptcp.mp_join = 0;
-	rx_opt->mptcp.add_addr = 0;
-	rx_opt->mptcp.rm_addr = 0;
-	rx_opt->mptcp.dss = 0;
-#endif
 }
 
 /* This is the max number of SACKS that we'll generate and process. It's safe

commit f296234c98a8fcec94eec80304a873f635d350ea
Author: Peter Krystad <peter.krystad@linux.intel.com>
Date:   Fri Mar 27 14:48:39 2020 -0700

    mptcp: Add handling of incoming MP_JOIN requests
    
    Process the MP_JOIN option in a SYN packet with the same flow
    as MP_CAPABLE but when the third ACK is received add the
    subflow to the MPTCP socket subflow list instead of adding it to
    the TCP socket accept queue.
    
    The subflow is added at the end of the subflow list so it will not
    interfere with the existing subflows operation and no data is
    expected to be transmitted on it.
    
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1225db308957..421c99c12291 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -92,7 +92,13 @@ struct mptcp_options_received {
 		add_addr : 1,
 		rm_addr : 1,
 		family : 4,
-		echo : 1;
+		echo : 1,
+		backup : 1;
+	u32	token;
+	u32	nonce;
+	u64	thmac;
+	u8	hmac[20];
+	u8	join_id;
 	u8	use_map:1,
 		dsn64:1,
 		data_fin:1,

commit 3df523ab582c52f745f9a73b9ebf9368ede555ac
Author: Peter Krystad <peter.krystad@linux.intel.com>
Date:   Fri Mar 27 14:48:37 2020 -0700

    mptcp: Add ADD_ADDR handling
    
    Add handling for sending and receiving the ADD_ADDR, ADD_ADDR6,
    and RM_ADDR suboptions.
    
    Co-developed-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3dc964010fef..1225db308957 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -86,9 +86,13 @@ struct mptcp_options_received {
 	u64	data_seq;
 	u32	subflow_seq;
 	u16	data_len;
-	u8	mp_capable : 1,
+	u16	mp_capable : 1,
 		mp_join : 1,
-		dss : 1;
+		dss : 1,
+		add_addr : 1,
+		rm_addr : 1,
+		family : 4,
+		echo : 1;
 	u8	use_map:1,
 		dsn64:1,
 		data_fin:1,
@@ -96,6 +100,16 @@ struct mptcp_options_received {
 		ack64:1,
 		mpc_map:1,
 		__unused:2;
+	u8	addr_id;
+	u8	rm_id;
+	union {
+		struct in_addr	addr;
+#if IS_ENABLED(CONFIG_MPTCP_IPV6)
+		struct in6_addr	addr6;
+#endif
+	};
+	u64	ahmac;
+	u16	port;
 };
 #endif
 
@@ -131,6 +145,8 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 #if IS_ENABLED(CONFIG_MPTCP)
 	rx_opt->mptcp.mp_capable = 0;
 	rx_opt->mptcp.mp_join = 0;
+	rx_opt->mptcp.add_addr = 0;
+	rx_opt->mptcp.rm_addr = 0;
 	rx_opt->mptcp.dss = 0;
 #endif
 }

commit ae2dd7164943e03644293af92802550d052632e6
Author: Florian Westphal <fw@strlen.de>
Date:   Wed Jan 29 15:54:46 2020 +0100

    mptcp: handle tcp fallback when using syn cookies
    
    We can't deal with syncookie mode yet, the syncookie rx path will create
    tcp reqsk, i.e. we get OOB access because we treat tcp reqsk as mptcp reqsk one:
    
    TCP: SYN flooding on port 20002. Sending cookies.
    BUG: KASAN: slab-out-of-bounds in subflow_syn_recv_sock+0x451/0x4d0 net/mptcp/subflow.c:191
    Read of size 1 at addr ffff8881167bc148 by task syz-executor099/2120
     subflow_syn_recv_sock+0x451/0x4d0 net/mptcp/subflow.c:191
     tcp_get_cookie_sock+0xcf/0x520 net/ipv4/syncookies.c:209
     cookie_v6_check+0x15a5/0x1e90 net/ipv6/syncookies.c:252
     tcp_v6_cookie_check net/ipv6/tcp_ipv6.c:1123 [inline]
     [..]
    
    Bug can be reproduced via "sysctl net.ipv4.tcp_syncookies=2".
    
    Note that MPTCP should work with syncookies (4th ack would carry needed
    state), but it appears better to sort that out in -next so do tcp
    fallback for now.
    
    I removed the MPTCP ifdef for tcp_rsk "is_mptcp" member because
    if (IS_ENABLED()) is easier to read than "#ifdef IS_ENABLED()/#endif" pair.
    
    Cc: Eric Dumazet <edumazet@google.com>
    Fixes: cec37a6e41aae7bf ("mptcp: Handle MP_CAPABLE options for outgoing connections")
    Reported-by: Christoph Paasch <cpaasch@apple.com>
    Tested-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1cf73e6f85ca..3dc964010fef 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -148,9 +148,7 @@ struct tcp_request_sock {
 	const struct tcp_request_sock_ops *af_specific;
 	u64				snt_synack; /* first SYNACK sent time */
 	bool				tfo_listener;
-#if IS_ENABLED(CONFIG_MPTCP)
 	bool				is_mptcp;
-#endif
 	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;

commit 32efcc06d2a15fa87585614d12d6c2308cc2d3f3
Author: Abdul Kabbani <akabbani@google.com>
Date:   Fri Jan 24 16:34:02 2020 -0500

    tcp: export count for rehash attempts
    
    Using IPv6 flow-label to swiftly route around avoid congested or
    disconnected network path can greatly improve TCP reliability.
    
    This patch adds SNMP counters and a OPT_STATS counter to track both
    host-level and connection-level statistics. Network administrators
    can use these counters to evaluate the impact of this new ability better.
    
    Export count for rehash attempts to
    1) two SNMP counters: TcpTimeoutRehash (rehash due to timeouts),
       and TcpDuplicateDataRehash (rehash due to receiving duplicate
       packets)
    2) Timestamping API SOF_TIMESTAMPING_OPT_STATS.
    
    Signed-off-by: Abdul Kabbani <akabbani@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Kevin(Yudong) Yang <yyd@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4e2124607d32..1cf73e6f85ca 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -386,6 +386,8 @@ struct tcp_sock {
 #define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) 0
 #endif
 
+	u16 timeout_rehash;	/* Timeout-triggered rehash attempts */
+
 	u32 rcv_ooopack; /* Received out-of-order packets, for tcpinfo */
 
 /* Receiver side RTT estimation */

commit cc7972ea1932335e0a0ee00ac8a24b3e8304630d
Author: Christoph Paasch <cpaasch@apple.com>
Date:   Tue Jan 21 16:56:31 2020 -0800

    mptcp: parse and emit MP_CAPABLE option according to v1 spec
    
    This implements MP_CAPABLE options parsing and writing according
    to RFC 6824 bis / RFC 8684: MPTCP v1.
    
    Local key is sent on syn/ack, and both keys are sent on 3rd ack.
    MP_CAPABLE messages len are updated accordingly. We need the skbuff to
    correctly emit the above, so we push the skbuff struct as an argument
    all the way from tcp code to the relevant mptcp callbacks.
    
    When processing incoming MP_CAPABLE + data, build a full blown DSS-like
    map info, to simplify later processing.  On child socket creation, we
    need to record the remote key, if available.
    
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0d00dad4b85d..4e2124607d32 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -94,7 +94,8 @@ struct mptcp_options_received {
 		data_fin:1,
 		use_ack:1,
 		ack64:1,
-		__unused:3;
+		mpc_map:1,
+		__unused:2;
 };
 #endif
 

commit 648ef4b88673dadb8463bf0d4b10fbf33d55def8
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Tue Jan 21 16:56:24 2020 -0800

    mptcp: Implement MPTCP receive path
    
    Parses incoming DSS options and populates outgoing MPTCP ACK
    fields. MPTCP fields are parsed from the TCP option header and placed in
    an skb extension, allowing the upper MPTCP layer to access MPTCP
    options after the skb has gone through the TCP stack.
    
    The subflow implements its own data_ready() ops, which ensures that the
    pending data is in sequence - according to MPTCP seq number - dropping
    out-of-seq skbs. The DATA_READY bit flag is set if this is the case.
    This allows the MPTCP socket layer to determine if more data is
    available without having to consult the individual subflows.
    
    It additionally validates the current mapping and propagates EoF events
    to the connection socket.
    
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Co-developed-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Co-developed-by: Davide Caratti <dcaratti@redhat.com>
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Co-developed-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e9ee06d887fa..0d00dad4b85d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -82,9 +82,19 @@ struct tcp_sack_block {
 struct mptcp_options_received {
 	u64	sndr_key;
 	u64	rcvr_key;
+	u64	data_ack;
+	u64	data_seq;
+	u32	subflow_seq;
+	u16	data_len;
 	u8	mp_capable : 1,
 		mp_join : 1,
 		dss : 1;
+	u8	use_map:1,
+		dsn64:1,
+		data_fin:1,
+		use_ack:1,
+		ack64:1,
+		__unused:3;
 };
 #endif
 

commit cec37a6e41aae7bf3df9a3da783380a4d9325fd8
Author: Peter Krystad <peter.krystad@linux.intel.com>
Date:   Tue Jan 21 16:56:18 2020 -0800

    mptcp: Handle MP_CAPABLE options for outgoing connections
    
    Add hooks to tcp_output.c to add MP_CAPABLE to an outgoing SYN request,
    to capture the MP_CAPABLE in the received SYN-ACK, to add MP_CAPABLE to
    the final ACK of the three-way handshake.
    
    Use the .sk_rx_dst_set() handler in the subflow proto to capture when the
    responding SYN-ACK is received and notify the MPTCP connection layer.
    
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 877947475814..e9ee06d887fa 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -137,6 +137,9 @@ struct tcp_request_sock {
 	const struct tcp_request_sock_ops *af_specific;
 	u64				snt_synack; /* first SYNACK sent time */
 	bool				tfo_listener;
+#if IS_ENABLED(CONFIG_MPTCP)
+	bool				is_mptcp;
+#endif
 	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;

commit 2303f994b3e187091fd08148066688b08f837efc
Author: Peter Krystad <peter.krystad@linux.intel.com>
Date:   Tue Jan 21 16:56:17 2020 -0800

    mptcp: Associate MPTCP context with TCP socket
    
    Use ULP to associate a subflow_context structure with each TCP subflow
    socket. Creating these sockets requires new bind and connect functions
    to make sure ULP is set up immediately when the subflow sockets are
    created.
    
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Co-developed-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Co-developed-by: Davide Caratti <dcaratti@redhat.com>
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 52798ab00394..877947475814 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -397,6 +397,9 @@ struct tcp_sock {
 	u32	mtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG
 			   * while socket was owned by user.
 			   */
+#if IS_ENABLED(CONFIG_MPTCP)
+	bool	is_mptcp;
+#endif
 
 #ifdef CONFIG_TCP_MD5SIG
 /* TCP AF-Specific parts; only used by MD5 Signature support so far */

commit eda7acddf8080bb2d022a8d4b8b2345eb80c63ec
Author: Peter Krystad <peter.krystad@linux.intel.com>
Date:   Tue Jan 21 16:56:16 2020 -0800

    mptcp: Handle MPTCP TCP options
    
    Add hooks to parse and format the MP_CAPABLE option.
    
    This option is handled according to MPTCP version 0 (RFC6824).
    MPTCP version 1 MP_CAPABLE (RFC6824bis/RFC8684) will be added later in
    coordination with related code changes.
    
    Co-developed-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Co-developed-by: Davide Caratti <dcaratti@redhat.com>
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ca6f01531e64..52798ab00394 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -78,6 +78,16 @@ struct tcp_sack_block {
 #define TCP_SACK_SEEN     (1 << 0)   /*1 = peer is SACK capable, */
 #define TCP_DSACK_SEEN    (1 << 2)   /*1 = DSACK was received from peer*/
 
+#if IS_ENABLED(CONFIG_MPTCP)
+struct mptcp_options_received {
+	u64	sndr_key;
+	u64	rcvr_key;
+	u8	mp_capable : 1,
+		mp_join : 1,
+		dss : 1;
+};
+#endif
+
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
 	int	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
@@ -95,6 +105,9 @@ struct tcp_options_received {
 	u8	num_sacks;	/* Number of SACK blocks		*/
 	u16	user_mss;	/* mss requested by user in ioctl	*/
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
+#if IS_ENABLED(CONFIG_MPTCP)
+	struct mptcp_options_received	mptcp;
+#endif
 };
 
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
@@ -104,6 +117,11 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 #if IS_ENABLED(CONFIG_SMC)
 	rx_opt->smc_ok = 0;
 #endif
+#if IS_ENABLED(CONFIG_MPTCP)
+	rx_opt->mptcp.mp_capable = 0;
+	rx_opt->mptcp.mp_join = 0;
+	rx_opt->mptcp.dss = 0;
+#endif
 }
 
 /* This is the max number of SACKS that we'll generate and process. It's safe

commit 480274787d7e3458bc5a7cfbbbe07033984ad711
Author: Jason Baron <jbaron@akamai.com>
Date:   Wed Oct 23 11:09:26 2019 -0400

    tcp: add TCP_INFO status for failed client TFO
    
    The TCPI_OPT_SYN_DATA bit as part of tcpi_options currently reports whether
    or not data-in-SYN was ack'd on both the client and server side. We'd like
    to gather more information on the client-side in the failure case in order
    to indicate the reason for the failure. This can be useful for not only
    debugging TFO, but also for creating TFO socket policies. For example, if
    a middle box removes the TFO option or drops a data-in-SYN, we can
    can detect this case, and turn off TFO for these connections saving the
    extra retransmits.
    
    The newly added tcpi_fastopen_client_fail status is 2 bits and has the
    following 4 states:
    
    1) TFO_STATUS_UNSPEC
    
    Catch-all state which includes when TFO is disabled via black hole
    detection, which is indicated via LINUX_MIB_TCPFASTOPENBLACKHOLE.
    
    2) TFO_COOKIE_UNAVAILABLE
    
    If TFO_CLIENT_NO_COOKIE mode is off, this state indicates that no cookie
    is available in the cache.
    
    3) TFO_DATA_NOT_ACKED
    
    Data was sent with SYN, we received a SYN/ACK but it did not cover the data
    portion. Cookie is not accepted by server because the cookie may be invalid
    or the server may be overloaded.
    
    4) TFO_SYN_RETRANSMITTED
    
    Data was sent with SYN, we received a SYN/ACK which did not cover the data
    after at least 1 additional SYN was sent (without data). It may be the case
    that a middle-box is dropping data-in-SYN packets. Thus, it would be more
    efficient to not use TFO on this connection to avoid extra retransmits
    during connection establishment.
    
    These new fields do not cover all the cases where TFO may fail, but other
    failures, such as SYN/ACK + data being dropped, will result in the
    connection not becoming established. And a connection blackhole after
    session establishment shows up as a stalled connection.
    
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Christoph Paasch <cpaasch@apple.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 668e25a76d69..ca6f01531e64 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -223,7 +223,7 @@ struct tcp_sock {
 		fastopen_connect:1, /* FASTOPEN_CONNECT sockopt */
 		fastopen_no_cookie:1, /* Allow send/recv SYN+data without a cookie */
 		is_sack_reneg:1,    /* in recovery from loss with SACK reneg? */
-		unused:2;
+		fastopen_client_fail:2; /* reason why fastopen failed */
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		recvmsg_inq : 1,/* Indicate # of bytes in queue upon recvmsg */

commit d983ea6f16b835dcde2ee9a58a1e764ce68bfccc
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:38 2019 -0700

    tcp: add rcu protection around tp->fastopen_rsk
    
    Both tcp_v4_err() and tcp_v6_err() do the following operations
    while they do not own the socket lock :
    
            fastopen = tp->fastopen_rsk;
            snd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;
    
    The problem is that without appropriate barrier, the compiler
    might reload tp->fastopen_rsk and trigger a NULL deref.
    
    request sockets are protected by RCU, we can simply add
    the missing annotations and barriers to solve the issue.
    
    Fixes: 168a8f58059a ("tcp: TCP Fast Open Server - main code path")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 99617e528ea2..668e25a76d69 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -393,7 +393,7 @@ struct tcp_sock {
 	/* fastopen_rsk points to request_sock that resulted in this big
 	 * socket. Used to retransmit SYNACKs etc.
 	 */
-	struct request_sock *fastopen_rsk;
+	struct request_sock __rcu *fastopen_rsk;
 	u32	*saved_syn;
 };
 
@@ -447,8 +447,8 @@ static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)
 
 static inline bool tcp_passive_fastopen(const struct sock *sk)
 {
-	return (sk->sk_state == TCP_SYN_RECV &&
-		tcp_sk(sk)->fastopen_rsk != NULL);
+	return sk->sk_state == TCP_SYN_RECV &&
+	       rcu_access_pointer(tcp_sk(sk)->fastopen_rsk) != NULL;
 }
 
 static inline void fastopen_queue_tune(struct sock *sk, int backlog)

commit f9af2dbbfe01def62765a58af7fbc488351893c3
Author: Thomas Higdon <tph@fb.com>
Date:   Fri Sep 13 23:23:34 2019 +0000

    tcp: Add TCP_INFO counter for packets received out-of-order
    
    For receive-heavy cases on the server-side, we want to track the
    connection quality for individual client IPs. This counter, similar to
    the existing system-wide TCPOFOQueue counter in /proc/net/netstat,
    tracks out-of-order packet reception. By providing this counter in
    TCP_INFO, it will allow understanding to what degree receive-heavy
    sockets are experiencing out-of-order delivery and packet drops
    indicating congestion.
    
    Please note that this is similar to the counter in NetBSD TCP_INFO, and
    has the same name.
    
    Also note that we avoid increasing the size of the tcp_sock struct by
    taking advantage of a hole.
    
    Signed-off-by: Thomas Higdon <tph@fb.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f3a85a7fb4b1..99617e528ea2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -354,6 +354,8 @@ struct tcp_sock {
 #define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) 0
 #endif
 
+	u32 rcv_ooopack; /* Received out-of-order packets, for tcpinfo */
+
 /* Receiver side RTT estimation */
 	u32 rcv_rtt_last_tsecr;
 	struct {

commit 438ac88009bcb10f9ced07fbb4b32d5377ee936b
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Jun 19 23:46:28 2019 +0200

    net: fastopen: robustness and endianness fixes for SipHash
    
    Some changes to the TCP fastopen code to make it more robust
    against future changes in the choice of key/cookie size, etc.
    
    - Instead of keeping the SipHash key in an untyped u8[] buffer
      and casting it to the right type upon use, use the correct
      type directly. This ensures that the key will appear at the
      correct alignment if we ever change the way these data
      structures are allocated. (Currently, they are only allocated
      via kmalloc so they always appear at the correct alignment)
    
    - Use DIV_ROUND_UP when sizing the u64[] array to hold the
      cookie, so it is always of sufficient size, even if
      TCP_FASTOPEN_COOKIE_MAX is no longer a multiple of 8.
    
    - Drop the 'len' parameter from the tcp_fastopen_reset_cipher()
      function, which is no longer used.
    
    - Add endian swabbing when setting the keys and calculating the hash,
      to ensure that cookie values are the same for a given key and
      source/destination address pair regardless of the endianness of
      the server.
    
    Note that none of these are functional changes wrt the current
    state of the code, with the exception of the swabbing, which only
    affects big endian systems.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2689b0b0b68a..f3a85a7fb4b1 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -58,7 +58,7 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 
 /* TCP Fast Open Cookie as stored in memory */
 struct tcp_fastopen_cookie {
-	u64	val[TCP_FASTOPEN_COOKIE_MAX / sizeof(u64)];
+	__le64	val[DIV_ROUND_UP(TCP_FASTOPEN_COOKIE_MAX, sizeof(u64))];
 	s8	len;
 	bool	exp;	/* In RFC6994 experimental option format */
 };

commit 13091aa30535b719e269f20a7bc34002bf5afae5
Merge: f97252a8c33f 29f785ff76b6
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 17 19:48:13 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Honestly all the conflicts were simple overlapping changes,
    nothing really interesting to report.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c681edae33e86ff27be2d6cc717663d91df20b0e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 17 10:09:33 2019 +0200

    net: ipv4: move tcp_fastopen server side code to SipHash library
    
    Using a bare block cipher in non-crypto code is almost always a bad idea,
    not only for security reasons (and we've seen some examples of this in
    the kernel in the past), but also for performance reasons.
    
    In the TCP fastopen case, we call into the bare AES block cipher one or
    two times (depending on whether the connection is IPv4 or IPv6). On most
    systems, this results in a call chain such as
    
      crypto_cipher_encrypt_one(ctx, dst, src)
        crypto_cipher_crt(tfm)->cit_encrypt_one(crypto_cipher_tfm(tfm), ...);
          aesni_encrypt
            kernel_fpu_begin();
            aesni_enc(ctx, dst, src); // asm routine
            kernel_fpu_end();
    
    It is highly unlikely that the use of special AES instructions has a
    benefit in this case, especially since we are doing the above twice
    for IPv6 connections, instead of using a transform which can process
    the entire input in one go.
    
    We could switch to the cbcmac(aes) shash, which would at least get
    rid of the duplicated overhead in *some* cases (i.e., today, only
    arm64 has an accelerated implementation of cbcmac(aes), while x86 will
    end up using the generic cbcmac template wrapping the AES-NI cipher,
    which basically ends up doing exactly the above). However, in the given
    context, it makes more sense to use a light-weight MAC algorithm that
    is more suitable for the purpose at hand, such as SipHash.
    
    Since the output size of SipHash already matches our chosen value for
    TCP_FASTOPEN_COOKIE_SIZE, and given that it accepts arbitrary input
    sizes, this greatly simplifies the code as well.
    
    NOTE: Server farms backing a single server IP for load balancing purposes
          and sharing a single fastopen key will be adversely affected by
          this change unless all systems in the pool receive their kernel
          upgrades at the same time.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c23019a3b264..9ea0e71f5c6a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -58,12 +58,7 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 
 /* TCP Fast Open Cookie as stored in memory */
 struct tcp_fastopen_cookie {
-	union {
-		u8	val[TCP_FASTOPEN_COOKIE_MAX];
-#if IS_ENABLED(CONFIG_IPV6)
-		struct in6_addr addr;
-#endif
-	};
+	u64	val[TCP_FASTOPEN_COOKIE_MAX / sizeof(u64)];
 	s8	len;
 	bool	exp;	/* In RFC6994 experimental option format */
 };

commit 3b4929f65b0d8249f19a50245cd88ed1a2f78cff
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 17 17:17:22 2019 -0700

    tcp: limit payload size of sacked skbs
    
    Jonathan Looney reported that TCP can trigger the following crash
    in tcp_shifted_skb() :
    
            BUG_ON(tcp_skb_pcount(skb) < pcount);
    
    This can happen if the remote peer has advertized the smallest
    MSS that linux TCP accepts : 48
    
    An skb can hold 17 fragments, and each fragment can hold 32KB
    on x86, or 64KB on PowerPC.
    
    This means that the 16bit witdh of TCP_SKB_CB(skb)->tcp_gso_segs
    can overflow.
    
    Note that tcp_sendmsg() builds skbs with less than 64KB
    of payload, so this problem needs SACK to be enabled.
    SACK blocks allow TCP to coalesce multiple skbs in the retransmit
    queue, thus filling the 17 fragments to maximal capacity.
    
    CVE-2019-11477 -- u16 overflow of TCP_SKB_CB(skb)->tcp_gso_segs
    
    Fixes: 832d11c5cd07 ("tcp: Try to restore large SKBs while SACK processing")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Jonathan Looney <jtl@netflix.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Tyler Hicks <tyhicks@canonical.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Bruce Curtis <brucec@netflix.com>
    Cc: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 711361af9ce0..9a478a0cd3a2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -484,4 +484,8 @@ static inline u16 tcp_mss_clamp(const struct tcp_sock *tp, u16 mss)
 
 	return (user_mss && user_mss < mss) ? user_mss : mss;
 }
+
+int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
+		  int shiftlen);
+
 #endif	/* _LINUX_TCP_H */

commit a842fe1425cb20f457abd3f8ef98b468f83ca98b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 12 11:57:25 2019 -0700

    tcp: add optional per socket transmit delay
    
    Adding delays to TCP flows is crucial for studying behavior
    of TCP stacks, including congestion control modules.
    
    Linux offers netem module, but it has unpractical constraints :
    - Need root access to change qdisc
    - Hard to setup on egress if combined with non trivial qdisc like FQ
    - Single delay for all flows.
    
    EDT (Earliest Departure Time) adoption in TCP stack allows us
    to enable a per socket delay at a very small cost.
    
    Networking tools can now establish thousands of flows, each of them
    with a different delay, simulating real world conditions.
    
    This requires FQ packet scheduler or a EDT-enabled NIC.
    
    This patchs adds TCP_TX_DELAY socket option, to set a delay in
    usec units.
    
      unsigned int tx_delay = 10000; /* 10 msec */
    
      setsockopt(fd, SOL_TCP, TCP_TX_DELAY, &tx_delay, sizeof(tx_delay));
    
    Note that FQ packet scheduler limits might need some tweaking :
    
    man tc-fq
    
    PARAMETERS
       limit
           Hard  limit  on  the  real  queue  size. When this limit is
           reached, new packets are dropped. If the value is  lowered,
           packets  are  dropped so that the new limit is met. Default
           is 10000 packets.
    
       flow_limit
           Hard limit on the maximum  number  of  packets  queued  per
           flow.  Default value is 100.
    
    Use of TCP_TX_DELAY option will increase number of skbs in FQ qdisc,
    so packets would be dropped if any of the previous limit is hit.
    
    Use of a jump label makes this support runtime-free, for hosts
    never using the option.
    
    Also note that TSQ (TCP Small Queues) limits are slightly changed
    with this patch : we need to account that skbs artificially delayed
    wont stop us providind more skbs to feed the pipe (netem uses
    skb_orphan_partial() for this purpose, but FQ can not use this trick)
    
    Because of that, using big delays might very well trigger
    old bugs in TSO auto defer logic and/or sndbuf limited detection.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 711361af9ce0..c23019a3b264 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -245,6 +245,7 @@ struct tcp_sock {
 		syn_smc:1;	/* SYN includes SMC */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
+	u32	tcp_tx_delay;	/* delay (in usec) added to TX packets */
 	u64	tcp_wstamp_ns;	/* departure time for next sent data packet */
 	u64	tcp_clock_cache; /* cache last tcp_clock_ns() (see tcp_mstamp_refresh()) */
 
@@ -436,6 +437,7 @@ struct tcp_timewait_sock {
 	u32			  tw_last_oow_ack_time;
 
 	int			  tw_ts_recent_stamp;
+	u32			  tw_tx_delay;
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	  *tw_md5_key;
 #endif

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index a9b0280687d5..711361af9ce0 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * INET		An implementation of the TCP/IP protocol suite for the LINUX
  *		operating system.  INET is implemented using the  BSD Socket
@@ -8,11 +9,6 @@
  * Version:	@(#)tcp.h	1.0.2	04/28/93
  *
  * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
- *
- *		This program is free software; you can redistribute it and/or
- *		modify it under the terms of the GNU General Public License
- *		as published by the Free Software Foundation; either version
- *		2 of the License, or (at your option) any later version.
  */
 #ifndef _LINUX_TCP_H
 #define _LINUX_TCP_H

commit 86de5921a3d5dd246df661e09bdd0a6131b39ae3
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 20 05:53:59 2018 -0800

    tcp: defer SACK compression after DupThresh
    
    Jean-Louis reported a TCP regression and bisected to recent SACK
    compression.
    
    After a loss episode (receiver not able to keep up and dropping
    packets because its backlog is full), linux TCP stack is sending
    a single SACK (DUPACK).
    
    Sender waits a full RTO timer before recovering losses.
    
    While RFC 6675 says in section 5, "Algorithm Details",
    
       (2) If DupAcks < DupThresh but IsLost (HighACK + 1) returns true --
           indicating at least three segments have arrived above the current
           cumulative acknowledgment point, which is taken to indicate loss
           -- go to step (4).
    ...
       (4) Invoke fast retransmit and enter loss recovery as follows:
    
    there are old TCP stacks not implementing this strategy, and
    still counting the dupacks before starting fast retransmit.
    
    While these stacks probably perform poorly when receivers implement
    LRO/GRO, we should be a little more gentle to them.
    
    This patch makes sure we do not enable SACK compression unless
    3 dupacks have been sent since last rcv_nxt update.
    
    Ideally we should even rearm the timer to send one or two
    more DUPACK if no more packets are coming, but that will
    be work aiming for linux-4.21.
    
    Many thanks to Jean-Louis for bisecting the issue, providing
    packet captures and testing this patch.
    
    Fixes: 5d9f4262b7ea ("tcp: add SACK compression")
    Reported-by: Jean-Louis Dupond <jean-louis@dupond.be>
    Tested-by: Jean-Louis Dupond <jean-louis@dupond.be>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8ed77bb4ed86..a9b0280687d5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -196,6 +196,7 @@ struct tcp_sock {
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
 	u32	last_oow_ack_time;  /* timestamp of last out-of-window ACK */
+	u32	compressed_ack_rcv_nxt;
 
 	u32	tsoffset;	/* timestamp offset */
 

commit 5f6188a8003d080e3753b8f14f4a5a2325ae1ff6
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 15 09:37:52 2018 -0700

    tcp: do not change tcp_wstamp_ns in tcp_mstamp_refresh
    
    In EDT design, I made the mistake of using tcp_wstamp_ns
    to store the last tcp_clock_ns() sample and to store the
    pacing virtual timer.
    
    This causes major regressions at high speed flows.
    
    Introduce tcp_clock_cache to store last tcp_clock_ns().
    This is needed because some arches have slow high-resolution
    kernel time service.
    
    tcp_wstamp_ns is only updated when a packet is sent.
    
    Note that we can remove tcp_mstamp in the future since
    tcp_mstamp is essentially tcp_clock_cache/1000, so the
    apparent socket size increase is temporary.
    
    Fixes: 9799ccb0e984 ("tcp: add tcp_wstamp_ns socket field")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 848f5b25e178..8ed77bb4ed86 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -249,6 +249,7 @@ struct tcp_sock {
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 	u64	tcp_wstamp_ns;	/* departure time for next sent data packet */
+	u64	tcp_clock_cache; /* cache last tcp_clock_ns() (see tcp_mstamp_refresh()) */
 
 /* RTT measurement */
 	u64	tcp_mstamp;	/* most recent packet received/sent */

commit 9799ccb0e984a5c1311b22a212e7ff96e8b736de
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 21 08:51:49 2018 -0700

    tcp: add tcp_wstamp_ns socket field
    
    TCP will soon provide earliest departure time on TX skbs.
    It needs to track this in a new variable.
    
    tcp_mstamp_refresh() needs to update this variable, and
    became too big to stay an inline.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 263e37271afd..848f5b25e178 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -248,6 +248,8 @@ struct tcp_sock {
 		syn_smc:1;	/* SYN includes SMC */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
+	u64	tcp_wstamp_ns;	/* departure time for next sent data packet */
+
 /* RTT measurement */
 	u64	tcp_mstamp;	/* most recent packet received/sent */
 	u32	srtt_us;	/* smoothed round trip time << 3 in usecs */

commit 7ec65372ca534217b53fd208500cf7aac223a383
Author: Wei Wang <weiwan@google.com>
Date:   Tue Jul 31 17:46:24 2018 -0700

    tcp: add stat of data packet reordering events
    
    Introduce a new TCP stats to record the number of reordering events seen
    and expose it in both tcp_info (TCP_INFO) and opt_stats
    (SOF_TIMESTAMPING_OPT_STATS).
    Application can use this stats to track the frequency of the reordering
    events in addition to the existing reordering stats which tracks the
    magnitude of the latest reordering event.
    
    Note: this new stats tracks reordering events triggered by ACKs, which
    could often be fewer than the actual number of packets being delivered
    out-of-order.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index da6281c549a5..263e37271afd 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -220,8 +220,7 @@ struct tcp_sock {
 #define TCP_RACK_RECOVERY_THRESH 16
 		u8 reo_wnd_persist:5, /* No. of recovery since last adj */
 		   dsack_seen:1, /* Whether DSACK seen after last adj */
-		   advanced:1,	 /* mstamp advanced since last lost marking */
-		   reord:1;	 /* reordering detected */
+		   advanced:1;	 /* mstamp advanced since last lost marking */
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
 	u8	compressed_ack;
@@ -267,6 +266,7 @@ struct tcp_sock {
 	u8	ecn_flags;	/* ECN status bits.			*/
 	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 	u32	reordering;	/* Packet reordering metric.		*/
+	u32	reord_seen;	/* number of data packet reordering events */
 	u32	snd_up;		/* Urgent pointer		*/
 
 /*

commit 7e10b6554ff2ce7f86d5d3eec3af5db8db482caa
Author: Wei Wang <weiwan@google.com>
Date:   Tue Jul 31 17:46:23 2018 -0700

    tcp: add dsack blocks received stats
    
    Introduce a new TCP stat to record the number of DSACK blocks received
    (RFC4989 tcpEStatsStackDSACKDups) and expose it in both tcp_info
    (TCP_INFO) and opt_stats (SOF_TIMESTAMPING_OPT_STATS).
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fb67f9a51b95..da6281c549a5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -188,6 +188,9 @@ struct tcp_sock {
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.
 				 */
+	u32	dsack_dups;	/* RFC4898 tcpEStatsStackDSACKDups
+				 * total number of DSACK blocks received
+				 */
  	u32	snd_una;	/* First byte we want an ack for	*/
  	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */

commit fb31c9b9f6c85b1bad569ecedbde78d9e37cd87b
Author: Wei Wang <weiwan@google.com>
Date:   Tue Jul 31 17:46:22 2018 -0700

    tcp: add data bytes retransmitted stats
    
    Introduce a new TCP stat to record the number of bytes retransmitted
    (RFC4898 tcpEStatsPerfOctetsRetrans) and expose it in both tcp_info
    (TCP_INFO) and opt_stats (SOF_TIMESTAMPING_OPT_STATS).
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d0798dcd2cab..fb67f9a51b95 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -333,6 +333,9 @@ struct tcp_sock {
 				 * the first SYN. */
 	u32	undo_marker;	/* snd_una upon a new recovery episode. */
 	int	undo_retrans;	/* number of undoable retransmissions. */
+	u64	bytes_retrans;	/* RFC4898 tcpEStatsPerfOctetsRetrans
+				 * Total data bytes retransmitted
+				 */
 	u32	total_retrans;	/* Total retransmits for entire connection */
 
 	u32	urg_seq;	/* Seq of received urgent pointer */

commit ba113c3aa79a7f941ac162d05a3620bdc985c58d
Author: Wei Wang <weiwan@google.com>
Date:   Tue Jul 31 17:46:21 2018 -0700

    tcp: add data bytes sent stats
    
    Introduce a new TCP stat to record the number of bytes sent
    (RFC4898 tcpEStatsPerfHCDataOctetsOut) and expose it in both tcp_info
    (TCP_INFO) and opt_stats (SOF_TIMESTAMPING_OPT_STATS).
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 58a8d7d71354..d0798dcd2cab 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -181,6 +181,9 @@ struct tcp_sock {
 	u32	data_segs_out;	/* RFC4898 tcpEStatsPerfDataSegsOut
 				 * total number of data segments sent.
 				 */
+	u64	bytes_sent;	/* RFC4898 tcpEStatsPerfHCDataOctetsOut
+				 * total number of data bytes sent.
+				 */
 	u64	bytes_acked;	/* RFC4898 tcpEStatsAppHCThruOctetsAcked
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.

commit cca9bab1b72cd2296097c75f59ef11ef80461279
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jul 11 12:16:12 2018 +0200

    tcp: use monotonic timestamps for PAWS
    
    Using get_seconds() for timestamps is deprecated since it can lead
    to overflows on 32-bit systems. While the interface generally doesn't
    overflow until year 2106, the specific implementation of the TCP PAWS
    algorithm breaks in 2038 when the intermediate signed 32-bit timestamps
    overflow.
    
    A related problem is that the local timestamps in CLOCK_REALTIME form
    lead to unexpected behavior when settimeofday is called to set the system
    clock backwards or forwards by more than 24 days.
    
    While the first problem could be solved by using an overflow-safe method
    of comparing the timestamps, a nicer solution is to use a monotonic
    clocksource with ktime_get_seconds() that simply doesn't overflow (at
    least not until 136 years after boot) and that doesn't change during
    settimeofday().
    
    To make 32-bit and 64-bit architectures behave the same way here, and
    also save a few bytes in the tcp_options_received structure, I'm changing
    the type to a 32-bit integer, which is now safe on all architectures.
    
    Finally, the ts_recent_stamp field also (confusingly) gets used to store
    a jiffies value in tcp_synq_overflow()/tcp_synq_no_recent_overflow().
    This is currently safe, but changing the type to 32-bit requires
    some small changes there to keep it working.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3dbea6610304..58a8d7d71354 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -89,7 +89,7 @@ struct tcp_sack_block {
 
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
-	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
+	int	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
 	u32	ts_recent;	/* Time stamp to echo next		*/
 	u32	rcv_tsval;	/* Time stamp value             	*/
 	u32	rcv_tsecr;	/* Time stamp echo reply        	*/
@@ -426,7 +426,7 @@ struct tcp_timewait_sock {
 	/* The time we sent the last out-of-window ACK: */
 	u32			  tw_last_oow_ack_time;
 
-	long			  tw_ts_recent_stamp;
+	int			  tw_ts_recent_stamp;
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	  *tw_md5_key;
 #endif

commit 3f6c65d6255a872846c44182c82c78d3dc6239f5
Author: Wei Wang <weiwan@google.com>
Date:   Tue Jun 19 21:42:50 2018 -0700

    tcp: ignore rcv_rtt sample with old ts ecr value
    
    When receiving multiple packets with the same ts ecr value, only try
    to compute rcv_rtt sample with the earliest received packet.
    This is because the rcv_rtt calculated by later received packets
    could possibly include long idle time or other types of delay.
    For example:
    (1) server sends last packet of reply with TS val V1
    (2) client ACKs last packet of reply with TS ecr V1
    (3) long idle time passes
    (4) client sends next request data packet with TS ecr V1 (again!)
    At this time, the rcv_rtt computed on server with TS ecr V1 will be
    inflated with the idle time and should get ignored.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 72705eaf4b84..3dbea6610304 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -350,6 +350,7 @@ struct tcp_sock {
 #endif
 
 /* Receiver side RTT estimation */
+	u32 rcv_rtt_last_tsecr;
 	struct {
 		u32	rtt_us;
 		u32	seq;

commit 5d9f4262b7ea41ca9981cc790e37cca6e37c789e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 17 14:47:26 2018 -0700

    tcp: add SACK compression
    
    When TCP receives an out-of-order packet, it immediately sends
    a SACK packet, generating network load but also forcing the
    receiver to send 1-MSS pathological packets, increasing its
    RTX queue length/depth, and thus processing time.
    
    Wifi networks suffer from this aggressive behavior, but generally
    speaking, all these SACK packets add fuel to the fire when networks
    are under congestion.
    
    This patch adds a high resolution timer and tp->compressed_ack counter.
    
    Instead of sending a SACK, we program this timer with a small delay,
    based on RTT and capped to 1 ms :
    
            delay = min ( 5 % of RTT, 1 ms)
    
    If subsequent SACKs need to be sent while the timer has not yet
    expired, we simply increment tp->compressed_ack.
    
    When timer expires, a SACK is sent with the latest information.
    Whenever an ACK is sent (if data is sent, or if in-order
    data is received) timer is canceled.
    
    Note that tcp_sack_new_ofo_skb() is able to force a SACK to be sent
    if the sack blocks need to be shuffled, even if the timer has not
    expired.
    
    A new SNMP counter is added in the following patch.
    
    Two other patches add sysctls to allow changing the 1,000,000 and 44
    values that this commit hard-coded.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Toke Høiland-Jørgensen <toke@toke.dk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 807776928cb8..72705eaf4b84 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -218,6 +218,7 @@ struct tcp_sock {
 		   reord:1;	 /* reordering detected */
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
+	u8	compressed_ack;
 	u32	chrono_start;	/* Start time in jiffies of a TCP chrono */
 	u32	chrono_stat[3];	/* Time in jiffies for chrono_stat stats */
 	u8	chrono_type:2,	/* current chronograph type */
@@ -297,6 +298,7 @@ struct tcp_sock {
 	u32	sacked_out;	/* SACK'd packets			*/
 
 	struct hrtimer	pacing_timer;
+	struct hrtimer	compressed_ack_timer;
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit b75eba76d3d72e2374fac999926dafef2997edd2
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Tue May 1 15:39:15 2018 -0400

    tcp: send in-queue bytes in cmsg upon read
    
    Applications with many concurrent connections, high variance
    in receive queue length and tight memory bounds cannot
    allocate worst-case buffer size to drain sockets. Knowing
    the size of receive queue length, applications can optimize
    how they allocate buffers to read from the socket.
    
    The number of bytes pending on the socket is directly
    available through ioctl(FIONREAD/SIOCINQ) and can be
    approximated using getsockopt(MEMINFO) (rmem_alloc includes
    skb overheads in addition to application data). But, both of
    these options add an extra syscall per recvmsg. Moreover,
    ioctl(FIONREAD/SIOCINQ) takes the socket lock.
    
    Add the TCP_INQ socket option to TCP. When this socket
    option is set, recvmsg() relays the number of bytes available
    on the socket for reading to the application via the
    TCP_CM_INQ control message.
    
    Calculate the number of bytes after releasing the socket lock
    to include the processed backlog, if any. To avoid an extra
    branch in the hot path of recvmsg() for this new control
    message, move all cmsg processing inside an existing branch for
    processing receive timestamps. Since the socket lock is not held
    when calculating the size of receive queue, TCP_INQ is a hint.
    For example, it can overestimate the queue size by one byte,
    if FIN is received.
    
    With this method, applications can start reading from the socket
    using a small buffer, and then use larger buffers based on the
    remaining data when needed.
    
    V3 change-log:
            As suggested by David Miller, added loads with barrier
            to check whether we have multiple threads calling recvmsg
            in parallel. When that happens we lock the socket to
            calculate inq.
    V4 change-log:
            Removed inline from a static function.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Suggested-by: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 20585d5c4e1c..807776928cb8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -228,7 +228,7 @@ struct tcp_sock {
 		unused:2;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
-		unused1	    : 1,
+		recvmsg_inq : 1,/* Indicate # of bytes in queue upon recvmsg */
 		repair      : 1,
 		frto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 	u8	repair_queue;

commit e21db6f69a95b846ff04e31fe0a86004cbd000d7
Author: Yuchung Cheng <ycheng@google.com>
Date:   Tue Apr 17 23:18:48 2018 -0700

    tcp: track total bytes delivered with ECN CE marks
    
    Introduce a new delivered_ce stat in tcp socket to estimate
    number of packets being marked with CE bits. The estimation is
    done via ACKs with ECE bit. Depending on the actual receiver
    behavior, the estimation could have biases.
    
    Since the TCP sender can't really see the CE bit in the data path,
    so the sender is technically counting packets marked delivered with
    the "ECE / ECN-Echo" flag set.
    
    With RFC3168 ECN, because the ECE bit is sticky, this count can
    drastically overestimate the nummber of CE-marked data packets
    
    With DCTCP-style ECN this should be reasonably precise unless there
    is loss in the ACK path, in which case it's not precise.
    
    With AccECN proposal this can be made still more precise, even in
    the case some degree of ACK loss.
    
    However this is sender's best estimate of CE information.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8f4c54986f97..20585d5c4e1c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -281,6 +281,7 @@ struct tcp_sock {
 				 * receiver in Recovery. */
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */
 	u32	delivered;	/* Total data packets delivered incl. rexmits */
+	u32	delivered_ce;	/* Like the above but only ECE marked packets */
 	u32	lost;		/* Total data packets lost incl. rexmits */
 	u32	app_limited;	/* limited until "delivered" reaches this val */
 	u64	first_tx_mstamp;  /* start of window send phase */

commit b13d880721729384757f235166068c315326f4a1
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Thu Jan 25 16:14:10 2018 -0800

    bpf: Adds field bpf_sock_ops_cb_flags to tcp_sock
    
    Adds field bpf_sock_ops_cb_flags to tcp_sock and bpf_sock_ops. Its primary
    use is to determine if there should be calls to sock_ops bpf program at
    various points in the TCP code. The field is initialized to zero,
    disabling the calls. A sock_ops BPF program can set it, per connection and
    as necessary, when the connection is established.
    
    It also adds support for reading and writting the field within a
    sock_ops BPF program. Reading is done by accessing the field directly.
    However, writing is done through the helper function
    bpf_sock_ops_cb_flags_set, in order to return an error if a BPF program
    is trying to set a callback that is not supported in the current kernel
    (i.e. running an older kernel). The helper function returns 0 if it was
    able to set all of the bits set in the argument, a positive number
    containing the bits that could not be set, or -EINVAL if the socket is
    not a full TCP socket.
    
    Examples of where one could call the bpf program:
    
    1) When RTO fires
    2) When a packet is retransmitted
    3) When the connection terminates
    4) When a packet is sent
    5) When a packet is received
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4f93f0953c41..8f4c54986f97 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -335,6 +335,17 @@ struct tcp_sock {
 
 	int			linger2;
 
+
+/* Sock_ops bpf program related variables */
+#ifdef CONFIG_BPF
+	u8	bpf_sock_ops_cb_flags;  /* Control calling BPF programs
+					 * values defined in uapi/linux/tcp.h
+					 */
+#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) (TP->bpf_sock_ops_cb_flags & ARG)
+#else
+#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) 0
+#endif
+
 /* Receiver side RTT estimation */
 	struct {
 		u32	rtt_us;

commit 607065bad9931e72207b0cac365d7d4abc06bd99
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Dec 10 17:55:03 2017 -0800

    tcp: avoid integer overflows in tcp_rcv_space_adjust()
    
    When using large tcp_rmem[2] values (I did tests with 500 MB),
    I noticed overflows while computing rcvwin.
    
    Lets fix this before the following patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Wei Wang <weiwan@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ca4a6361389b..4f93f0953c41 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -344,7 +344,7 @@ struct tcp_sock {
 
 /* Receiver queue space */
 	struct {
-		int	space;
+		u32	space;
 		u32	seq;
 		u64	time;
 	} rcvq_space;

commit d4761754b4fb2ef8d9a1e9d121c4bec84e1fe292
Author: Yousuk Seung <ysseung@google.com>
Date:   Thu Dec 7 13:41:34 2017 -0800

    tcp: invalidate rate samples during SACK reneging
    
    Mark tcp_sock during a SACK reneging event and invalidate rate samples
    while marked. Such rate samples may overestimate bw by including packets
    that were SACKed before reneging.
    
    < ack 6001 win 10000 sack 7001:38001
    < ack 7001 win 0 sack 8001:38001 // Reneg detected
    > seq 7001:8001 // RTO, SACK cleared.
    < ack 38001 win 10000
    
    In above example the rate sample taken after the last ack will count
    7001-38001 as delivered while the actual delivery rate likely could
    be much lower i.e. 7001-8001.
    
    This patch adds a new field tcp_sock.sack_reneg and marks it when we
    declare SACK reneging and entering TCP_CA_Loss, and unmarks it after
    the last rate sample was taken before moving back to TCP_CA_Open. This
    patch also invalidates rate samples taken while tcp_sock.is_sack_reneg
    is set.
    
    Fixes: b9f64820fb22 ("tcp: track data delivery rate for a TCP connection")
    Signed-off-by: Yousuk Seung <ysseung@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index df5d97a85e1a..ca4a6361389b 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -224,7 +224,8 @@ struct tcp_sock {
 		rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
 		fastopen_connect:1, /* FASTOPEN_CONNECT sockopt */
 		fastopen_no_cookie:1, /* Allow send/recv SYN+data without a cookie */
-		unused:3;
+		is_sack_reneg:1,    /* in recovery from loss with SACK reneg? */
+		unused:2;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		unused1	    : 1,

commit 737ff314563ca27f044f9a3a041e9d42491ef7ce
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Nov 8 13:01:27 2017 -0800

    tcp: use sequence distance to detect reordering
    
    Replace the reordering distance measurement in packet unit with
    sequence based approach. Previously it trackes the number of "packets"
    toward the forward ACK (i.e.  highest sacked sequence)in a state
    variable "fackets_out".
    
    Precisely measuring reordering degree on packet distance has not much
    benefit, as the degree constantly changes by factors like path, load,
    and congestion window. It is also complicated and prone to arcane bugs.
    This patch replaces with sequence-based approach that's much simpler.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9574936fe041..df5d97a85e1a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -293,7 +293,6 @@ struct tcp_sock {
 	u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
 	u32	lost_out;	/* Lost packets			*/
 	u32	sacked_out;	/* SACK'd packets			*/
-	u32	fackets_out;	/* FACK'd packets			*/
 
 	struct hrtimer	pacing_timer;
 

commit 713bafea92920103cd3d361657406cf04d0e22dd
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Nov 8 13:01:26 2017 -0800

    tcp: retire FACK loss detection
    
    FACK loss detection has been disabled by default and the
    successor RACK subsumed FACK and can handle reordering better.
    This patch removes FACK to simplify TCP loss recovery.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 22f40c96a15b..9574936fe041 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -85,7 +85,6 @@ struct tcp_sack_block {
 
 /*These are used to set the sack_ok field in struct tcp_options_received */
 #define TCP_SACK_SEEN     (1 << 0)   /*1 = peer is SACK capable, */
-#define TCP_FACK_ENABLED  (1 << 1)   /*1 = FACK is enabled locally*/
 #define TCP_DSACK_SEEN    (1 << 2)   /*1 = DSACK was received from peer*/
 
 struct tcp_options_received {

commit 1f2556916d974cfb62b6af51660186b5f58bd869
Author: Priyaranjan Jha <priyarjha@google.com>
Date:   Fri Nov 3 16:38:48 2017 -0700

    tcp: higher throughput under reordering with adaptive RACK reordering wnd
    
    Currently TCP RACK loss detection does not work well if packets are
    being reordered beyond its static reordering window (min_rtt/4).Under
    such reordering it may falsely trigger loss recoveries and reduce TCP
    throughput significantly.
    
    This patch improves that by increasing and reducing the reordering
    window based on DSACK, which is now supported in major TCP implementations.
    It makes RACK's reo_wnd adaptive based on DSACK and no. of recoveries.
    
    - If DSACK is received, increment reo_wnd by min_rtt/4 (upper bounded
      by srtt), since there is possibility that spurious retransmission was
      due to reordering delay longer than reo_wnd.
    
    - Persist the current reo_wnd value for TCP_RACK_RECOVERY_THRESH (16)
      no. of successful recoveries (accounts for full DSACK-based loss
      recovery undo). After that, reset it to default (min_rtt/4).
    
    - At max, reo_wnd is incremented only once per rtt. So that the new
      DSACK on which we are reacting, is due to the spurious retx (approx)
      after the reo_wnd has been updated last time.
    
    - reo_wnd is tracked in terms of steps (of min_rtt/4), rather than
      absolute value to account for change in rtt.
    
    In our internal testing, we observed significant increase in throughput,
    in scenarios where reordering exceeds min_rtt/4 (previous static value).
    
    Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8c431385b272..22f40c96a15b 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -210,8 +210,13 @@ struct tcp_sock {
 		u64 mstamp; /* (Re)sent time of the skb */
 		u32 rtt_us;  /* Associated RTT */
 		u32 end_seq; /* Ending TCP sequence of the skb */
-		u8 advanced; /* mstamp advanced since last lost marking */
-		u8 reord;    /* reordering detected */
+		u32 last_delivered; /* tp->delivered at last reo_wnd adj */
+		u8 reo_wnd_steps;   /* Allowed reordering window */
+#define TCP_RACK_RECOVERY_THRESH 16
+		u8 reo_wnd_persist:5, /* No. of recovery since last adj */
+		   dsack_seen:1, /* Whether DSACK seen after last adj */
+		   advanced:1,	 /* mstamp advanced since last lost marking */
+		   reord:1;	 /* reordering detected */
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
 	u32	chrono_start;	/* Start time in jiffies of a TCP chrono */

commit 60e2a7780793bae0debc275a9ccd57f7da0cf195
Author: Ursula Braun <ubraun@linux.vnet.ibm.com>
Date:   Wed Oct 25 11:01:45 2017 +0200

    tcp: TCP experimental option for SMC
    
    The SMC protocol [1] relies on the use of a new TCP experimental
    option [2, 3]. With this option, SMC capabilities are exchanged
    between peers during the TCP three way handshake. This patch adds
    support for this experimental option to TCP.
    
    References:
    [1] SMC-R Informational RFC: http://www.rfc-editor.org/info/rfc7609
    [2] Shared Use of TCP Experimental Options RFC 6994:
        https://tools.ietf.org/rfc/rfc6994.txt
    [3] IANA ExID SMCR:
    http://www.iana.org/assignments/tcp-parameters/tcp-parameters.xhtml#tcp-exids
    
    Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 173a7c2f9636..8c431385b272 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -98,7 +98,8 @@ struct tcp_options_received {
 		tstamp_ok : 1,	/* TIMESTAMP seen on SYN packet		*/
 		dsack : 1,	/* D-SACK is scheduled			*/
 		wscale_ok : 1,	/* Wscale seen on SYN packet		*/
-		sack_ok : 4,	/* SACK seen on SYN packet		*/
+		sack_ok : 3,	/* SACK seen on SYN packet		*/
+		smc_ok : 1,	/* SMC seen on SYN packet		*/
 		snd_wscale : 4,	/* Window scaling received from sender	*/
 		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
 	u8	num_sacks;	/* Number of SACK blocks		*/
@@ -110,6 +111,9 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 {
 	rx_opt->tstamp_ok = rx_opt->sack_ok = 0;
 	rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+#if IS_ENABLED(CONFIG_SMC)
+	rx_opt->smc_ok = 0;
+#endif
 }
 
 /* This is the max number of SACKS that we'll generate and process. It's safe
@@ -229,7 +233,8 @@ struct tcp_sock {
 		syn_fastopen_ch:1, /* Active TFO re-enabling probe */
 		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
 		save_syn:1,	/* Save headers of SYN packet */
-		is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
+		is_cwnd_limited:1,/* forward progress limited by snd_cwnd? */
+		syn_smc:1;	/* SYN includes SMC */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */

commit 71c02379c762cb616c00fd5c4ed253fbf6bbe11b
Author: Christoph Paasch <cpaasch@apple.com>
Date:   Mon Oct 23 13:22:23 2017 -0700

    tcp: Configure TFO without cookie per socket and/or per route
    
    We already allow to enable TFO without a cookie by using the
    fastopen-sysctl and setting it to TFO_SERVER_COOKIE_NOT_REQD (or
    TFO_CLIENT_NO_COOKIE).
    This is safe to do in certain environments where we know that there
    isn't a malicous host (aka., data-centers) or when the
    application-protocol already provides an authentication mechanism in the
    first flight of data.
    
    A server however might be providing multiple services or talking to both
    sides (public Internet and data-center). So, this server would want to
    enable cookie-less TFO for certain services and/or for connections that
    go to the data-center.
    
    This patch exposes a socket-option and a per-route attribute to enable such
    fine-grained configurations.
    
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Reviewed-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1d2c44e09e31..173a7c2f9636 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -215,7 +215,8 @@ struct tcp_sock {
 	u8	chrono_type:2,	/* current chronograph type */
 		rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
 		fastopen_connect:1, /* FASTOPEN_CONNECT sockopt */
-		unused:4;
+		fastopen_no_cookie:1, /* Allow send/recv SYN+data without a cookie */
+		unused:3;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		unused1	    : 1,

commit e2080072ed2d98a55ae69d95dea60ff7a17cddd5
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 4 12:59:58 2017 -0700

    tcp: new list for sent but unacked skbs for RACK recovery
    
    This patch adds a new queue (list) that tracks the sent but not yet
    acked or SACKed skbs for a TCP connection. The list is chronologically
    ordered by skb->skb_mstamp (the head is the oldest sent skb).
    
    This list will be used to optimize TCP Rack recovery, which checks
    an skb's timestamp to judge if it has been lost and needs to be
    retransmitted. Since TCP write queue is ordered by sequence instead
    of sent time, RACK has to scan over the write queue to catch all
    eligible packets to detect lost retransmission, and iterates through
    SACKed skbs repeatedly.
    
    Special cares for rare events:
    1. TCP repair fakes skb transmission so the send queue needs adjusted
    2. SACK reneging would require re-inserting SACKed skbs into the
       send queue. For now I believe it's not worth the complexity to
       make RACK work perfectly on SACK reneging, so we do nothing here.
    3. Fast Open: currently for non-TFO, send-queue correctly queues
       the pure SYN packet. For TFO which queues a pure SYN and
       then a data packet, send-queue only queues the data packet but
       not the pure SYN due to the structure of TFO code. This is okay
       because the SYN receiver would never respond with a SACK on a
       missing SYN (i.e. SYN is never fast-retransmitted by SACK/RACK).
    
    In order to not grow sk_buff, we use an union for the new list and
    _skb_refdst/destructor fields. This is a bit complicated because
    we need to make sure _skb_refdst and destructor are properly zeroed
    before skb is cloned/copied at transmit, and before being freed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4aa40ef02d32..1d2c44e09e31 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -191,6 +191,7 @@ struct tcp_sock {
 	u32	tsoffset;	/* timestamp offset */
 
 	struct list_head tsq_node; /* anchor in tsq_tasklet.head list */
+	struct list_head tsorted_sent_queue; /* time-sorted sent but un-SACKed skbs */
 
 	u32	snd_wl1;	/* Sequence for window update		*/
 	u32	snd_wnd;	/* The window we expect to receive	*/

commit 31770e34e43d6c8dee129bfee77e56c34e61f0e5
Author: Florian Westphal <fw@strlen.de>
Date:   Wed Aug 30 19:24:58 2017 +0200

    tcp: Revert "tcp: remove header prediction"
    
    This reverts commit 45f119bf936b1f9f546a0b139c5b56f9bb2bdc78.
    
    Eric Dumazet says:
      We found at Google a significant regression caused by
      45f119bf936b1f9f546a0b139c5b56f9bb2bdc78 tcp: remove header prediction
    
      In typical RPC  (TCP_RR), when a TCP socket receives data, we now call
      tcp_ack() while we used to not call it.
    
      This touches enough cache lines to cause a slowdown.
    
    so problem does not seem to be HP removal itself but the tcp_ack()
    call.  Therefore, it might be possible to remove HP after all, provided
    one finds a way to elide tcp_ack for most cases.
    
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 267164a1d559..4aa40ef02d32 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -147,6 +147,12 @@ struct tcp_sock {
 	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
 	u16	gso_segs;	/* Max number of segs per GSO packet	*/
 
+/*
+ *	Header prediction flags
+ *	0x5?10 << 16 + snd_wnd in net byte order
+ */
+	__be32	pred_flags;
+
 /*
  *	RFC793 variables by their proper names. This means you can
  *	read the code and the spec side by side (and laugh ...)

commit 4faf783998b8cb88294e9df89032f473f8771b78
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Aug 3 20:38:51 2017 -0700

    tcp: fix cwnd undo in Reno and HTCP congestion controls
    
    Using ssthresh to revert cwnd is less reliable when ssthresh is
    bounded to 2 packets. This patch uses an existing variable in TCP
    "prior_cwnd" that snapshots the cwnd right before entering fast
    recovery and RTO recovery in Reno.  This fixes the issue discussed
    in netdev thread: "A buggy behavior for Linux TCP Reno and HTCP"
    https://www.spinics.net/lists/netdev/msg444955.html
    
    Suggested-by: Neal Cardwell <ncardwell@google.com>
    Reported-by: Wei Sun <unlcsewsun@gmail.com>
    Signed-off-by: Yuchung Cheng <ncardwell@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d7389ea36e10..267164a1d559 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -258,7 +258,7 @@ struct tcp_sock {
 	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
-	u32	prior_cwnd;	/* Congestion window at start of Recovery. */
+	u32	prior_cwnd;	/* cwnd right before starting loss recovery */
 	u32	prr_delivered;	/* Number of newly delivered packets to
 				 * receiver in Recovery. */
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */

commit 45f119bf936b1f9f546a0b139c5b56f9bb2bdc78
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:21 2017 +0200

    tcp: remove header prediction
    
    Like prequeue, I am not sure this is overly useful nowadays.
    
    If we receive a train of packets, GRO will aggregate them if the
    headers are the same (HP predates GRO by several years) so we don't
    get a per-packet benefit, only a per-aggregated-packet one.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 32fb37cfb0d1..d7389ea36e10 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -147,12 +147,6 @@ struct tcp_sock {
 	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
 	u16	gso_segs;	/* Max number of segs per GSO packet	*/
 
-/*
- *	Header prediction flags
- *	0x5?10 << 16 + snd_wnd in net byte order
- */
-	__be32	pred_flags;
-
 /*
  *	RFC793 variables by their proper names. This means you can
  *	read the code and the spec side by side (and laugh ...)

commit e7942d0633c47c791ece6afa038be9cf977226de
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:18 2017 +0200

    tcp: remove prequeue support
    
    prequeue is a tcp receive optimization that moves part of rx processing
    from bh to process context.
    
    This only works if the socket being processed belongs to a process that
    is blocked in recv on that socket.
    
    In practice, this doesn't happen anymore that often because nowadays
    servers tend to use an event driven (epoll) model.
    
    Even normal client applications (web browsers) commonly use many tcp
    connections in parallel.
    
    This has measureable impact only in netperf (which uses plain recv and
    thus allows prequeue use) from host to locally running vm (~4%), however,
    there were no changes when using netperf between two physical hosts with
    ixgbe interfaces.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 542ca1ae02c4..32fb37cfb0d1 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -192,15 +192,6 @@ struct tcp_sock {
 
 	struct list_head tsq_node; /* anchor in tsq_tasklet.head list */
 
-	/* Data for direct copy to user */
-	struct {
-		struct sk_buff_head	prequeue;
-		struct task_struct	*task;
-		struct msghdr		*msg;
-		int			memory;
-		int			len;
-	} ucopy;
-
 	u32	snd_wl1;	/* Sequence for window update		*/
 	u32	snd_wnd;	/* The window we expect to receive	*/
 	u32	max_window;	/* Maximal window ever seen from peer	*/

commit 9a568de4818dea9a05af141046bd3e589245ab83
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 14:00:14 2017 -0700

    tcp: switch TCP TS option (RFC 7323) to 1ms clock
    
    TCP Timestamps option is defined in RFC 7323
    
    Traditionally on linux, it has been tied to the internal
    'jiffies' variable, because it had been a cheap and good enough
    generator.
    
    For TCP flows on the Internet, 1 ms resolution would be much better
    than 4ms or 10ms (HZ=250 or HZ=100 respectively)
    
    For TCP flows in the DC, Google has used usec resolution for more
    than two years with great success [1]
    
    Receive size autotuning (DRS) is indeed more precise and converges
    faster to optimal window size.
    
    This patch converts tp->tcp_mstamp to a plain u64 value storing
    a 1 usec TCP clock.
    
    This choice will allow us to upstream the 1 usec TS option as
    discussed in IETF 97.
    
    [1] https://www.ietf.org/proceedings/97/slides/slides-97-tcpm-tcp-options-for-low-latency-00.pdf
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 22854f028434..542ca1ae02c4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -123,7 +123,7 @@ struct tcp_request_sock_ops;
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
 	const struct tcp_request_sock_ops *af_specific;
-	struct skb_mstamp		snt_synack; /* first SYNACK sent time */
+	u64				snt_synack; /* first SYNACK sent time */
 	bool				tfo_listener;
 	u32				txhash;
 	u32				rcv_isn;
@@ -211,7 +211,7 @@ struct tcp_sock {
 
 	/* Information of the most recently (s)acked skb */
 	struct tcp_rack {
-		struct skb_mstamp mstamp; /* (Re)sent time of the skb */
+		u64 mstamp; /* (Re)sent time of the skb */
 		u32 rtt_us;  /* Associated RTT */
 		u32 end_seq; /* Ending TCP sequence of the skb */
 		u8 advanced; /* mstamp advanced since last lost marking */
@@ -240,7 +240,7 @@ struct tcp_sock {
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */
-	struct skb_mstamp tcp_mstamp; /* most recent packet received/sent */
+	u64	tcp_mstamp;	/* most recent packet received/sent */
 	u32	srtt_us;	/* smoothed round trip time << 3 in usecs */
 	u32	mdev_us;	/* medium deviation			*/
 	u32	mdev_max_us;	/* maximal mdev for the last rtt period	*/
@@ -280,8 +280,8 @@ struct tcp_sock {
 	u32	delivered;	/* Total data packets delivered incl. rexmits */
 	u32	lost;		/* Total data packets lost incl. rexmits */
 	u32	app_limited;	/* limited until "delivered" reaches this val */
-	struct skb_mstamp first_tx_mstamp;  /* start of window send phase */
-	struct skb_mstamp delivered_mstamp; /* time we reached "delivered" */
+	u64	first_tx_mstamp;  /* start of window send phase */
+	u64	delivered_mstamp; /* time we reached "delivered" */
 	u32	rate_delivered;    /* saved rate sample: packets delivered */
 	u32	rate_interval_us;  /* saved rate sample: time elapsed */
 
@@ -335,16 +335,16 @@ struct tcp_sock {
 
 /* Receiver side RTT estimation */
 	struct {
-		u32		rtt_us;
-		u32		seq;
-		struct skb_mstamp time;
+		u32	rtt_us;
+		u32	seq;
+		u64	time;
 	} rcv_rtt_est;
 
 /* Receiver queue space */
 	struct {
-		int		space;
-		u32		seq;
-		struct skb_mstamp time;
+		int	space;
+		u32	seq;
+		u64	time;
 	} rcvq_space;
 
 /* TCP-specific MTU probe information. */

commit 218af599fa635b107cfe10acf3249c4dfe5e4123
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 04:24:36 2017 -0700

    tcp: internal implementation for pacing
    
    BBR congestion control depends on pacing, and pacing is
    currently handled by sch_fq packet scheduler for performance reasons,
    and also because implemening pacing with FQ was convenient to truly
    avoid bursts.
    
    However there are many cases where this packet scheduler constraint
    is not practical.
    - Many linux hosts are not focusing on handling thousands of TCP
      flows in the most efficient way.
    - Some routers use fq_codel or other AQM, but still would like
      to use BBR for the few TCP flows they initiate/terminate.
    
    This patch implements an automatic fallback to internal pacing.
    
    Pacing is requested either by BBR or use of SO_MAX_PACING_RATE option.
    
    If sch_fq happens to be in the egress path, pacing is delegated to
    the qdisc, otherwise pacing is done by TCP itself.
    
    One advantage of pacing from TCP stack is to get more precise rtt
    estimations, and less work done from TX completion, since TCP Small
    queue limits are not generally hit. Setups with single TX queue but
    many cpus might even benefit from this.
    
    Note that unlike sch_fq, we do not take into account header sizes.
    Taking care of these headers would add additional complexity for
    no practical differences in behavior.
    
    Some performance numbers using 800 TCP_STREAM flows rate limited to
    ~48 Mbit per second on 40Gbit NIC.
    
    If MQ+pfifo_fast is used on the NIC :
    
    $ sar -n DEV 1 5 | grep eth
    14:48:44         eth0 725743.00 2932134.00  46776.76 4335184.68      0.00      0.00      1.00
    14:48:45         eth0 725349.00 2932112.00  46751.86 4335158.90      0.00      0.00      0.00
    14:48:46         eth0 725101.00 2931153.00  46735.07 4333748.63      0.00      0.00      0.00
    14:48:47         eth0 725099.00 2931161.00  46735.11 4333760.44      0.00      0.00      1.00
    14:48:48         eth0 725160.00 2931731.00  46738.88 4334606.07      0.00      0.00      0.00
    Average:         eth0 725290.40 2931658.20  46747.54 4334491.74      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     4  0      0 259825920  45644 2708324    0    0    21     2  247   98  0  0 100  0  0
     4  0      0 259823744  45644 2708356    0    0     0     0 2400825 159843  0 19 81  0  0
     0  0      0 259824208  45644 2708072    0    0     0     0 2407351 159929  0 19 81  0  0
     1  0      0 259824592  45644 2708128    0    0     0     0 2405183 160386  0 19 80  0  0
     1  0      0 259824272  45644 2707868    0    0     0    32 2396361 158037  0 19 81  0  0
    
    Now use MQ+FQ :
    
    lpaa23:~# echo fq >/proc/sys/net/core/default_qdisc
    lpaa23:~# tc qdisc replace dev eth0 root mq
    
    $ sar -n DEV 1 5 | grep eth
    14:49:57         eth0 678614.00 2727930.00  43739.13 4033279.14      0.00      0.00      0.00
    14:49:58         eth0 677620.00 2723971.00  43674.69 4027429.62      0.00      0.00      1.00
    14:49:59         eth0 676396.00 2719050.00  43596.83 4020125.02      0.00      0.00      0.00
    14:50:00         eth0 675197.00 2714173.00  43518.62 4012938.90      0.00      0.00      1.00
    14:50:01         eth0 676388.00 2719063.00  43595.47 4020171.64      0.00      0.00      0.00
    Average:         eth0 676843.00 2720837.40  43624.95 4022788.86      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     2  0      0 259832240  46008 2710912    0    0    21     2  223  192  0  1 99  0  0
     1  0      0 259832896  46008 2710744    0    0     0     0 1702206 198078  0 17 82  0  0
     0  0      0 259830272  46008 2710596    0    0     0     0 1696340 197756  1 17 83  0  0
     4  0      0 259829168  46024 2710584    0    0    16     0 1688472 197158  1 17 82  0  0
     3  0      0 259830224  46024 2710408    0    0     0     0 1692450 197212  0 18 82  0  0
    
    As expected, number of interrupts per second is very different.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Cc: Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b6d5adcee8fc..22854f028434 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -293,6 +293,8 @@ struct tcp_sock {
 	u32	sacked_out;	/* SACK'd packets			*/
 	u32	fackets_out;	/* FACK'd packets			*/
 
+	struct hrtimer	pacing_timer;
+
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;
 	struct sk_buff *retransmit_skb_hint;

commit 645f4c6f2ebd040688cc2a5f626ffc909e66ccf2
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 25 10:15:41 2017 -0700

    tcp: switch rcv_rtt_est and rcvq_space to high resolution timestamps
    
    Some devices or distributions use HZ=100 or HZ=250
    
    TCP receive buffer autotuning has poor behavior caused by this choice.
    Since autotuning happens after 4 ms or 10 ms, short distance flows
    get their receive buffer tuned to a very high value, but after an initial
    period where it was frozen to (too small) initial value.
    
    With tp->tcp_mstamp introduction, we can switch to high resolution
    timestamps almost for free (at the expense of 8 additional bytes per
    TCP structure)
    
    Note that some TCP stacks use usec TCP timestamps where this
    patch makes even more sense : Many TCP flows have < 500 usec RTT.
    Hopefully this finer TS option can be standardized soon.
    
    Tested:
     HZ=100 kernel
     ./netperf -H lpaa24 -t TCP_RR -l 1000 -- -r 10000,10000 &
    
     Peer without patch :
     lpaa24:~# ss -tmi dst lpaa23
     ...
     skmem:(r0,rb8388608,...)
     rcv_rtt:10 rcv_space:3210000 minrtt:0.017
    
     Peer with the patch :
     lpaa23:~# ss -tmi dst lpaa24
     ...
     skmem:(r0,rb428800,...)
     rcv_rtt:0.069 rcv_space:30000 minrtt:0.017
    
    We can see saner RCVBUF, and more precise rcv_rtt information.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 99a22f44c32e..b6d5adcee8fc 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -333,16 +333,16 @@ struct tcp_sock {
 
 /* Receiver side RTT estimation */
 	struct {
-		u32	rtt;
-		u32	seq;
-		u32	time;
+		u32		rtt_us;
+		u32		seq;
+		struct skb_mstamp time;
 	} rcv_rtt_est;
 
 /* Receiver queue space */
 	struct {
-		int	space;
-		u32	seq;
-		u32	time;
+		int		space;
+		u32		seq;
+		struct skb_mstamp time;
 	} rcvq_space;
 
 /* TCP-specific MTU probe information. */

commit 69e996c58a35db9ca79b3f021a15bcd22202e1c0
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 25 10:15:32 2017 -0700

    tcp: add tp->tcp_mstamp field
    
    We want to use precise timestamps in TCP stack, but we do not
    want to call possibly expensive kernel time services too often.
    
    tp->tcp_mstamp is guaranteed to be updated once per incoming packet.
    
    We will use it in the following patches, removing specific
    skb_mstamp_get() calls, and removing ack_time from
    struct tcp_sacktag_state.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index cbe5b602a2d3..99a22f44c32e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -240,6 +240,7 @@ struct tcp_sock {
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */
+	struct skb_mstamp tcp_mstamp; /* most recent packet received/sent */
 	u32	srtt_us;	/* smoothed round trip time << 3 in usecs */
 	u32	mdev_us;	/* medium deviation			*/
 	u32	mdev_max_us;	/* maximal mdev for the last rtt period	*/

commit cf1ef3f0719b4dcb74810ed507e2a2540f9811b4
Author: Wei Wang <weiwan@google.com>
Date:   Thu Apr 20 14:45:46 2017 -0700

    net/tcp_fastopen: Disable active side TFO in certain scenarios
    
    Middlebox firewall issues can potentially cause server's data being
    blackholed after a successful 3WHS using TFO. Following are the related
    reports from Apple:
    https://www.nanog.org/sites/default/files/Paasch_Network_Support.pdf
    Slide 31 identifies an issue where the client ACK to the server's data
    sent during a TFO'd handshake is dropped.
    C ---> syn-data ---> S
    C <--- syn/ack ----- S
    C (accept & write)
    C <---- data ------- S
    C ----- ACK -> X     S
                    [retry and timeout]
    
    https://www.ietf.org/proceedings/94/slides/slides-94-tcpm-13.pdf
    Slide 5 shows a similar situation that the server's data gets dropped
    after 3WHS.
    C ---- syn-data ---> S
    C <--- syn/ack ----- S
    C ---- ack --------> S
    S (accept & write)
    C?  X <- data ------ S
                    [retry and timeout]
    
    This is the worst failure b/c the client can not detect such behavior to
    mitigate the situation (such as disabling TFO). Failing to proceed, the
    application (e.g., SSL library) may simply timeout and retry with TFO
    again, and the process repeats indefinitely.
    
    The proposed solution is to disable active TFO globally under the
    following circumstances:
    1. client side TFO socket detects out of order FIN
    2. client side TFO socket receives out of order RST
    
    We disable active side TFO globally for 1hr at first. Then if it
    happens again, we disable it for 2h, then 4h, 8h, ...
    And we reset the timeout to 1hr if a client side TFO sockets not opened
    on loopback has successfully received data segs from server.
    And we examine this condition during close().
    
    The rational behind it is that when such firewall issue happens,
    application running on the client should eventually close the socket as
    it is not able to get the data it is expecting. Or application running
    on the server should close the socket as it is not able to receive any
    response from client.
    In both cases, out of order FIN or RST will get received on the client
    given that the firewall will not block them as no data are in those
    frames.
    And we want to disable active TFO globally as it helps if the middle box
    is very close to the client and most of the connections are likely to
    fail.
    
    Also, add a debug sysctl:
      tcp_fastopen_blackhole_detect_timeout_sec:
        the initial timeout to use when firewall blackhole issue happens.
        This can be set and read.
        When setting it to 0, it means to disable the active disable logic.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index cfc2d9506ce8..cbe5b602a2d3 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -233,6 +233,7 @@ struct tcp_sock {
 	u8	syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
 		syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
+		syn_fastopen_ch:1, /* Active TFO re-enabling probe */
 		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
 		save_syn:1,	/* Save headers of SYN packet */
 		is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */

commit 3541f9e8bdebce02458882b66b638d7302c1f616
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Feb 2 08:04:56 2017 -0800

    tcp: add tcp_mss_clamp() helper
    
    Small cleanup factorizing code doing the TCP_MAXSEG clamping.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f88f4649ba6f..cfc2d9506ce8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -445,4 +445,13 @@ static inline void tcp_saved_syn_free(struct tcp_sock *tp)
 
 struct sk_buff *tcp_get_timestamping_opt_stats(const struct sock *sk);
 
+static inline u16 tcp_mss_clamp(const struct tcp_sock *tp, u16 mss)
+{
+	/* We use READ_ONCE() here because socket might not be locked.
+	 * This happens for listeners.
+	 */
+	u16 user_mss = READ_ONCE(tp->rx_opt.user_mss);
+
+	return (user_mss && user_mss < mss) ? user_mss : mss;
+}
 #endif	/* _LINUX_TCP_H */

commit 19f6d3f3c8422d65b5e3d2162e30ef07c6e21ea2
Author: Wei Wang <weiwan@google.com>
Date:   Mon Jan 23 10:59:22 2017 -0800

    net/tcp-fastopen: Add new API support
    
    This patch adds a new socket option, TCP_FASTOPEN_CONNECT, as an
    alternative way to perform Fast Open on the active side (client). Prior
    to this patch, a client needs to replace the connect() call with
    sendto(MSG_FASTOPEN). This can be cumbersome for applications who want
    to use Fast Open: these socket operations are often done in lower layer
    libraries used by many other applications. Changing these libraries
    and/or the socket call sequences are not trivial. A more convenient
    approach is to perform Fast Open by simply enabling a socket option when
    the socket is created w/o changing other socket calls sequence:
      s = socket()
        create a new socket
      setsockopt(s, IPPROTO_TCP, TCP_FASTOPEN_CONNECT …);
        newly introduced sockopt
        If set, new functionality described below will be used.
        Return ENOTSUPP if TFO is not supported or not enabled in the
        kernel.
    
      connect()
        With cookie present, return 0 immediately.
        With no cookie, initiate 3WHS with TFO cookie-request option and
        return -1 with errno = EINPROGRESS.
    
      write()/sendmsg()
        With cookie present, send out SYN with data and return the number of
        bytes buffered.
        With no cookie, and 3WHS not yet completed, return -1 with errno =
        EINPROGRESS.
        No MSG_FASTOPEN flag is needed.
    
      read()
        Return -1 with errno = EWOULDBLOCK/EAGAIN if connect() is called but
        write() is not called yet.
        Return -1 with errno = EWOULDBLOCK/EAGAIN if connection is
        established but no msg is received yet.
        Return number of bytes read if socket is established and there is
        msg received.
    
    The new API simplifies life for applications that always perform a write()
    immediately after a successful connect(). Such applications can now take
    advantage of Fast Open by merely making one new setsockopt() call at the time
    of creating the socket. Nothing else about the application's socket call
    sequence needs to change.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 5371b3d70cfe..f88f4649ba6f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -222,7 +222,8 @@ struct tcp_sock {
 	u32	chrono_stat[3];	/* Time in jiffies for chrono_stat stats */
 	u8	chrono_type:2,	/* current chronograph type */
 		rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
-		unused:5;
+		fastopen_connect:1, /* FASTOPEN_CONNECT sockopt */
+		unused:4;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		unused1	    : 1,

commit 580bdf5650fff8f66468ce491f8308f1117b7074
Merge: e60a42635b76 a249708bc2aa
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jan 17 15:19:37 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 4a7f6009441144783e5925551c72e3f2e1b0839b
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:41 2017 -0800

    tcp: remove thin_dupack feature
    
    Thin stream DUPACK is to start fast recovery on only one DUPACK
    provided the connection is a thin stream (i.e., low inflight).  But
    this older feature is now subsumed with RACK. If a connection
    receives only a single DUPACK, RACK would arm a reordering timer
    and soon starts fast recovery instead of timeout if no further
    ACKs are received.
    
    The socket option (THIN_DUPACK) is kept as a nop for compatibility.
    Note that this patch does not change another thin-stream feature
    which enables linear RTO. Although it might be good to generalize
    that in the future (i.e., linear RTO for the first say 3 retries).
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4733368f953a..6c22332afb75 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -220,7 +220,7 @@ struct tcp_sock {
 		unused:5;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
-		thin_dupack : 1,/* Fast retransmit on first dupack      */
+		unused1	    : 1,
 		repair      : 1,
 		frto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 	u8	repair_queue;

commit bec41a11dd3dc8c54f766b4f494140ca92ba7c10
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:39 2017 -0800

    tcp: remove early retransmit
    
    This patch removes the support of RFC5827 early retransmit (i.e.,
    fast recovery on small inflight with <3 dupacks) because it is
    subsumed by the new RACK loss detection. More specifically when
    RACK receives DUPACKs, it'll arm a reordering timer to start fast
    recovery after a quarter of (min)RTT, hence it covers the early
    retransmit except RACK does not limit itself to specific inflight
    or dupack numbers.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8e5f4c15d0e5..4733368f953a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -224,8 +224,7 @@ struct tcp_sock {
 		repair      : 1,
 		frto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 	u8	repair_queue;
-	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
-		syn_data:1,	/* SYN includes data */
+	u8	syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
 		syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
 		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */

commit 840a3cbe89694fad75578856976f180e852e69aa
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:38 2017 -0800

    tcp: remove forward retransmit feature
    
    Forward retransmit is an esoteric feature in RFC3517 (condition(3)
    in the NextSeg()). Basically if a packet is not considered lost by
    the current criteria (# of dupacks etc), but the congestion window
    has room for more packets, then retransmit this packet.
    
    However it actually conflicts with the rest of recovery design. For
    example, when reordering is detected we want to be conservative
    in retransmitting packets but forward-retransmit feature would
    break that to force more retransmission. Also the implementation is
    fairly complicated inside the retransmission logic inducing extra
    iterations in the write queue. With RACK losses are being detected
    timely and this heuristic is no longer necessary. There this patch
    removes the feature.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 970d5f00589f..8e5f4c15d0e5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -307,7 +307,6 @@ struct tcp_sock {
 					 */
 
 	int     lost_cnt_hint;
-	u32     retransmit_high;	/* L-bits may be on up to this seqno */
 
 	u32	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	u32	high_seq;	/* snd_nxt at onset of congestion	*/

commit 1d0833df594390876647c54c2c88069d29059665
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:34 2017 -0800

    tcp: use sequence to break TS ties for RACK loss detection
    
    The packets inside a jumbo skb (e.g., TSO) share the same skb
    timestamp, even though they are sent sequentially on the wire. Since
    RACK is based on time, it can not detect some packets inside the
    same skb are lost.  However, we can leverage the packet sequence
    numbers as extended timestamps to detect losses. Therefore, when
    RACK timestamp is identical to skb's timestamp (i.e., one of the
    packets of the skb is acked or sacked), we use the sequence numbers
    of the acked and unacked packets to break ties.
    
    We can use the same sequence logic to advance RACK xmit time as
    well to detect more losses and avoid timeout.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1255c592719c..970d5f00589f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -208,6 +208,7 @@ struct tcp_sock {
 	struct tcp_rack {
 		struct skb_mstamp mstamp; /* (Re)sent time of the skb */
 		u32 rtt_us;  /* Associated RTT */
+		u32 end_seq; /* Ending TCP sequence of the skb */
 		u8 advanced; /* mstamp advanced since last lost marking */
 		u8 reord;    /* reordering detected */
 	} rack;

commit deed7be78f512d003c6290da0a781479b31b3d74
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:32 2017 -0800

    tcp: record most recent RTT in RACK loss detection
    
    Record the most recent RTT in RACK. It is often identical to the
    "ca_rtt_us" values in tcp_clean_rtx_queue. But when the packet has
    been retransmitted, RACK choses to believe the ACK is for the
    (latest) retransmitted packet if the RTT is over minimum RTT.
    
    This requires passing the arrival time of the most recent ACK to
    RACK routines. The timestamp is now recorded in the "ack_time"
    in tcp_sacktag_state during the ACK processing.
    
    This patch does not change the RACK algorithm itself. It only adds
    the RTT variable to prepare the next main patch.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fc5848dad7a4..1255c592719c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -207,6 +207,7 @@ struct tcp_sock {
 	/* Information of the most recently (s)acked skb */
 	struct tcp_rack {
 		struct skb_mstamp mstamp; /* (Re)sent time of the skb */
+		u32 rtt_us;  /* Associated RTT */
 		u8 advanced; /* mstamp advanced since last lost marking */
 		u8 reord;    /* reordering detected */
 	} rack;

commit 003c941057eaa868ca6fedd29a274c863167230d
Author: Shannon Nelson <shannon.nelson@oracle.com>
Date:   Thu Jan 12 14:24:58 2017 -0800

    tcp: fix tcp_fastopen unaligned access complaints on sparc
    
    Fix up a data alignment issue on sparc by swapping the order
    of the cookie byte array field with the length field in
    struct tcp_fastopen_cookie, and making it a proper union
    to clean up the typecasting.
    
    This addresses log complaints like these:
        log_unaligned: 113 callbacks suppressed
        Kernel unaligned access at TPC[976490] tcp_try_fastopen+0x2d0/0x360
        Kernel unaligned access at TPC[9764ac] tcp_try_fastopen+0x2ec/0x360
        Kernel unaligned access at TPC[9764c8] tcp_try_fastopen+0x308/0x360
        Kernel unaligned access at TPC[9764e4] tcp_try_fastopen+0x324/0x360
        Kernel unaligned access at TPC[976490] tcp_try_fastopen+0x2d0/0x360
    
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Shannon Nelson <shannon.nelson@oracle.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fc5848dad7a4..c93f4b3a59cb 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -62,8 +62,13 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 
 /* TCP Fast Open Cookie as stored in memory */
 struct tcp_fastopen_cookie {
+	union {
+		u8	val[TCP_FASTOPEN_COOKIE_MAX];
+#if IS_ENABLED(CONFIG_IPV6)
+		struct in6_addr addr;
+#endif
+	};
 	s8	len;
-	u8	val[TCP_FASTOPEN_COOKIE_MAX];
 	bool	exp;	/* In RFC6994 experimental option format */
 };
 

commit 7aa5470c2c09265902b5e4289afa82e4e7c2987e
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Dec 3 11:14:57 2016 -0800

    tcp: tsq: move tsq_flags close to sk_wmem_alloc
    
    tsq_flags being in the same cache line than sk_wmem_alloc
    makes a lot of sense. Both fields are changed from tcp_wfree()
    and more generally by various TSQ related functions.
    
    Prior patch made room in struct sock and added sk_tsq_flags,
    this patch deletes tsq_flags from struct tcp_sock.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d8be083ab0b0..fc5848dad7a4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -186,7 +186,6 @@ struct tcp_sock {
 	u32	tsoffset;	/* timestamp offset */
 
 	struct list_head tsq_node; /* anchor in tsq_tasklet.head list */
-	unsigned long	tsq_flags;
 
 	/* Data for direct copy to user */
 	struct {

commit 40fc3423b983b864bf70b03199191260ae9b2ea6
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Dec 3 11:14:50 2016 -0800

    tcp: tsq: add tsq_flags / tsq_enum
    
    This is a cleanup, to ease code review of following patches.
    
    Old 'enum tsq_flags' is renamed, and a new enumeration is added
    with the flags used in cmpxchg() operations as opposed to
    single bit operations.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 734bab4c3bef..d8be083ab0b0 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -364,7 +364,7 @@ struct tcp_sock {
 	u32	*saved_syn;
 };
 
-enum tsq_flags {
+enum tsq_enum {
 	TSQ_THROTTLED,
 	TSQ_QUEUED,
 	TCP_TSQ_DEFERRED,	   /* tcp_tasklet_func() found socket was owned */
@@ -375,6 +375,15 @@ enum tsq_flags {
 				    */
 };
 
+enum tsq_flags {
+	TSQF_THROTTLED			= (1UL << TSQ_THROTTLED),
+	TSQF_QUEUED			= (1UL << TSQ_QUEUED),
+	TCPF_TSQ_DEFERRED		= (1UL << TCP_TSQ_DEFERRED),
+	TCPF_WRITE_TIMER_DEFERRED	= (1UL << TCP_WRITE_TIMER_DEFERRED),
+	TCPF_DELACK_TIMER_DEFERRED	= (1UL << TCP_DELACK_TIMER_DEFERRED),
+	TCPF_MTU_REDUCED_DEFERRED	= (1UL << TCP_MTU_REDUCED_DEFERRED),
+};
+
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 {
 	return (struct tcp_sock *)sk;

commit 95a22caee396cef0bb2ca8fafdd82966a49367bb
Author: Florian Westphal <fw@strlen.de>
Date:   Thu Dec 1 11:32:06 2016 +0100

    tcp: randomize tcp timestamp offsets for each connection
    
    jiffies based timestamps allow for easy inference of number of devices
    behind NAT translators and also makes tracking of hosts simpler.
    
    commit ceaa1fef65a7c2e ("tcp: adding a per-socket timestamp offset")
    added the main infrastructure that is needed for per-connection ts
    randomization, in particular writing/reading the on-wire tcp header
    format takes the offset into account so rest of stack can use normal
    tcp_time_stamp (jiffies).
    
    So only two items are left:
     - add a tsoffset for request sockets
     - extend the tcp isn generator to also return another 32bit number
       in addition to the ISN.
    
    Re-use of ISN generator also means timestamps are still monotonically
    increasing for same connection quadruple, i.e. PAWS will still work.
    
    Includes fixes from Eric Dumazet.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 00e0ee8f001f..734bab4c3bef 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -123,6 +123,7 @@ struct tcp_request_sock {
 	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;
+	u32				ts_off;
 	u32				last_oow_ack_time; /* last SYNACK */
 	u32				rcv_nxt; /* the ack # by SYNACK. For
 						  * FastOpen it's the seq#

commit 1c885808e45601b2b6f68b30ac1d999e10b6f606
Author: Francis Yan <francisyyan@gmail.com>
Date:   Sun Nov 27 23:07:18 2016 -0800

    tcp: SOF_TIMESTAMPING_OPT_STATS option for SO_TIMESTAMPING
    
    This patch exports the sender chronograph stats via the socket
    SO_TIMESTAMPING channel. Currently we can instrument how long a
    particular application unit of data was queued in TCP by tracking
    SOF_TIMESTAMPING_TX_SOFTWARE and SOF_TIMESTAMPING_TX_SCHED. Having
    these sender chronograph stats exported simultaneously along with
    these timestamps allow further breaking down the various sender
    limitation.  For example, a video server can tell if a particular
    chunk of video on a connection takes a long time to deliver because
    TCP was experiencing small receive window. It is not possible to
    tell before this patch without packet traces.
    
    To prepare these stats, the user needs to set
    SOF_TIMESTAMPING_OPT_STATS and SOF_TIMESTAMPING_OPT_TSONLY flags
    while requesting other SOF_TIMESTAMPING TX timestamps. When the
    timestamps are available in the error queue, the stats are returned
    in a separate control message of type SCM_TIMESTAMPING_OPT_STATS,
    in a list of TLVs (struct nlattr) of types: TCP_NLA_BUSY_TIME,
    TCP_NLA_RWND_LIMITED, TCP_NLA_SNDBUF_LIMITED. Unit is microsecond.
    
    Signed-off-by: Francis Yan <francisyyan@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d5d3bd814338..00e0ee8f001f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -428,4 +428,6 @@ static inline void tcp_saved_syn_free(struct tcp_sock *tp)
 	tp->saved_syn = NULL;
 }
 
+struct sk_buff *tcp_get_timestamping_opt_stats(const struct sock *sk);
+
 #endif	/* _LINUX_TCP_H */

commit 05b055e89121394058c75dc354e9a46e1e765579
Author: Francis Yan <francisyyan@gmail.com>
Date:   Sun Nov 27 23:07:13 2016 -0800

    tcp: instrument tcp sender limits chronographs
    
    This patch implements the skeleton of the TCP chronograph
    instrumentation on sender side limits:
    
            1) idle (unspec)
            2) busy sending data other than 3-4 below
            3) rwnd-limited
            4) sndbuf-limited
    
    The limits are enumerated 'tcp_chrono'. Since a connection in
    theory can idle forever, we do not track the actual length of this
    uninteresting idle period. For the rest we track how long the sender
    spends in each limit. At any point during the life time of a
    connection, the sender must be in one of the four states.
    
    If there are multiple conditions worthy of tracking in a chronograph
    then the highest priority enum takes precedence over
    the other conditions. So that if something "more interesting"
    starts happening, stop the previous chrono and start a new one.
    
    The time unit is jiffy(u32) in order to save space in tcp_sock.
    This implies application must sample the stats no longer than every
    49 days of 1ms jiffy.
    
    Signed-off-by: Francis Yan <francisyyan@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 32a7c7e35b71..d5d3bd814338 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -211,8 +211,11 @@ struct tcp_sock {
 		u8 reord;    /* reordering detected */
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
-	u8	rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
-		unused:7;
+	u32	chrono_start;	/* Start time in jiffies of a TCP chrono */
+	u32	chrono_stat[3];	/* Time in jiffies for chrono_stat stats */
+	u8	chrono_type:2,	/* current chronograph type */
+		rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
+		unused:5;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		thin_dupack : 1,/* Fast retransmit on first dupack      */

commit 67db3e4bfbc90657c7be840aad5585be46240d6f
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Nov 4 11:54:32 2016 -0700

    tcp: no longer hold ehash lock while calling tcp_get_info()
    
    We had various problems in the past in tcp_get_info() and used
    specific synchronization to avoid deadlocks.
    
    We would like to add more instrumentation points for TCP, and
    avoiding grabing socket lock in tcp_getinfo() was too costly.
    
    Being able to lock the socket allows to provide consistent set
    of fields.
    
    inet_diag_dump_icsk() can make sure ehash locks are not
    held any more when tcp_get_info() is called.
    
    We can remove syncp added in commit d654976cbf85
    ("tcp: fix a potential deadlock in tcp_get_info()"), but we need
    to use lock_sock_fast() instead of spin_lock_bh() since TCP input
    path can now be run from process context.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index a17ae7b85218..32a7c7e35b71 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -176,8 +176,6 @@ struct tcp_sock {
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.
 				 */
-	struct u64_stats_sync syncp; /* protects 64bit vars (cf tcp_get_info()) */
-
  	u32	snd_una;	/* First byte we want an ack for	*/
  	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */

commit eb8329e0a04db0061f714f033b4454326ba147f4
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 19 23:39:16 2016 -0400

    tcp: export data delivery rate
    
    This commit export two new fields in struct tcp_info:
    
      tcpi_delivery_rate: The most recent goodput, as measured by
        tcp_rate_gen(). If the socket is limited by the sending
        application (e.g., no data to send), it reports the highest
        measurement instead of the most recent. The unit is bytes per
        second (like other rate fields in tcp_info).
    
      tcpi_delivery_rate_app_limited: A boolean indicating if the goodput
        was measured when the socket's throughput was limited by the
        sending application.
    
    This delivery rate information can be useful for applications that
    want to know the current throughput the TCP connection is seeing,
    e.g. adaptive bitrate video streaming. It can also be very useful for
    debugging or troubleshooting.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fdcd00ffcb66..a17ae7b85218 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -213,7 +213,8 @@ struct tcp_sock {
 		u8 reord;    /* reordering detected */
 	} rack;
 	u16	advmss;		/* Advertised MSS			*/
-	u8	unused;
+	u8	rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
+		unused:7;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		thin_dupack : 1,/* Fast retransmit on first dupack      */
@@ -271,6 +272,8 @@ struct tcp_sock {
 	u32	app_limited;	/* limited until "delivered" reaches this val */
 	struct skb_mstamp first_tx_mstamp;  /* start of window send phase */
 	struct skb_mstamp delivered_mstamp; /* time we reached "delivered" */
+	u32	rate_delivered;    /* saved rate sample: packets delivered */
+	u32	rate_interval_us;  /* saved rate sample: time elapsed */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit d7722e8570fc0f1e003cee7cf37694041828918b
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Mon Sep 19 23:39:15 2016 -0400

    tcp: track application-limited rate samples
    
    This commit adds code to track whether the delivery rate represented
    by each rate_sample was limited by the application.
    
    Upon each transmit, we store in the is_app_limited field in the skb a
    boolean bit indicating whether there is a known "bubble in the pipe":
    a point in the rate sample interval where the sender was
    application-limited, and did not transmit even though the cwnd and
    pacing rate allowed it.
    
    This logic marks the flow app-limited on a write if *all* of the
    following are true:
    
      1) There is less than 1 MSS of unsent data in the write queue
         available to transmit.
    
      2) There is no packet in the sender's queues (e.g. in fq or the NIC
         tx queue).
    
      3) The connection is not limited by cwnd.
    
      4) There are no lost packets to retransmit.
    
    The tcp_rate_check_app_limited() code in tcp_rate.c determines whether
    the connection is application-limited at the moment. If the flow is
    application-limited, it sets the tp->app_limited field. If the flow is
    application-limited then that means there is effectively a "bubble" of
    silence in the pipe now, and this silence will be reflected in a lower
    bandwidth sample for any rate samples from now until we get an ACK
    indicating this bubble has exited the pipe: specifically, until we get
    an ACK for the next packet we transmit.
    
    When we send every skb we record in scb->tx.is_app_limited whether the
    resulting rate sample will be application-limited.
    
    The code in tcp_rate_gen() checks to see when it is safe to mark all
    known application-limited bubbles of silence as having exited the
    pipe. It does this by checking to see when the delivered count moves
    past the tp->app_limited marker. At this point it zeroes the
    tp->app_limited marker, as all known bubbles are out of the pipe.
    
    We make room for the tx.is_app_limited bit in the skb by borrowing a
    bit from the in_flight field used by NV to record the number of bytes
    in flight. The receive window in the TCP header is 16 bits, and the
    max receive window scaling shift factor is 14 (RFC 1323). So the max
    receive window offered by the TCP protocol is 2^(16+14) = 2^30. So we
    only need 30 bits for the tx.in_flight used by NV.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c50e6aec005a..fdcd00ffcb66 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -268,6 +268,7 @@ struct tcp_sock {
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */
 	u32	delivered;	/* Total data packets delivered incl. rexmits */
 	u32	lost;		/* Total data packets lost incl. rexmits */
+	u32	app_limited;	/* limited until "delivered" reaches this val */
 	struct skb_mstamp first_tx_mstamp;  /* start of window send phase */
 	struct skb_mstamp delivered_mstamp; /* time we reached "delivered" */
 

commit b9f64820fb226a4e8ab10591f46cecd91ca56b30
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 19 23:39:14 2016 -0400

    tcp: track data delivery rate for a TCP connection
    
    This patch generates data delivery rate (throughput) samples on a
    per-ACK basis. These rate samples can be used by congestion control
    modules, and specifically will be used by TCP BBR in later patches in
    this series.
    
    Key state:
    
    tp->delivered: Tracks the total number of data packets (original or not)
                   delivered so far. This is an already-existing field.
    
    tp->delivered_mstamp: the last time tp->delivered was updated.
    
    Algorithm:
    
    A rate sample is calculated as (d1 - d0)/(t1 - t0) on a per-ACK basis:
    
      d1: the current tp->delivered after processing the ACK
      t1: the current time after processing the ACK
    
      d0: the prior tp->delivered when the acked skb was transmitted
      t0: the prior tp->delivered_mstamp when the acked skb was transmitted
    
    When an skb is transmitted, we snapshot d0 and t0 in its control
    block in tcp_rate_skb_sent().
    
    When an ACK arrives, it may SACK and ACK some skbs. For each SACKed
    or ACKed skb, tcp_rate_skb_delivered() updates the rate_sample struct
    to reflect the latest (d0, t0).
    
    Finally, tcp_rate_gen() generates a rate sample by storing
    (d1 - d0) in rs->delivered and (t1 - t0) in rs->interval_us.
    
    One caveat: if an skb was sent with no packets in flight, then
    tp->delivered_mstamp may be either invalid (if the connection is
    starting) or outdated (if the connection was idle). In that case,
    we'll re-stamp tp->delivered_mstamp.
    
    At first glance it seems t0 should always be the time when an skb was
    transmitted, but actually this could over-estimate the rate due to
    phase mismatch between transmit and ACK events. To track the delivery
    rate, we ensure that if packets are in flight then t0 and and t1 are
    times at which packets were marked delivered.
    
    If the initial and final RTTs are different then one may be corrupted
    by some sort of noise. The noise we see most often is sending gaps
    caused by delayed, compressed, or stretched acks. This either affects
    both RTTs equally or artificially reduces the final RTT. We approach
    this by recording the info we need to compute the initial RTT
    (duration of the "send phase" of the window) when we recorded the
    associated inflight. Then, for a filter to avoid bandwidth
    overestimates, we generalize the per-sample bandwidth computation
    from:
    
        bw = delivered / ack_phase_rtt
    
    to the following:
    
        bw = delivered / max(send_phase_rtt, ack_phase_rtt)
    
    In large-scale experiments, this filtering approach incorporating
    send_phase_rtt is effective at avoiding bandwidth overestimates due to
    ACK compression or stretched ACKs.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 38590fbc0ac5..c50e6aec005a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -268,6 +268,8 @@ struct tcp_sock {
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */
 	u32	delivered;	/* Total data packets delivered incl. rexmits */
 	u32	lost;		/* Total data packets lost incl. rexmits */
+	struct skb_mstamp first_tx_mstamp;  /* start of window send phase */
+	struct skb_mstamp delivered_mstamp; /* time we reached "delivered" */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit 0682e6902a52aca7caf6ad42551b16ea0f87bc31
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Sep 19 23:39:13 2016 -0400

    tcp: count packets marked lost for a TCP connection
    
    Count the number of packets that a TCP connection marks lost.
    
    Congestion control modules can use this loss rate information for more
    intelligent decisions about how fast to send.
    
    Specifically, this is used in TCP BBR policer detection. BBR uses a
    high packet loss rate as one signal in its policer detection and
    policer bandwidth estimation algorithm.
    
    The BBR policer detection algorithm cannot simply track retransmits,
    because a retransmit can be (and often is) an indicator of packets
    lost long, long ago. This is particularly true in a long CA_Loss
    period that repairs the initial massive losses when a policer kicks
    in.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 6433cc8b4667..38590fbc0ac5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -267,6 +267,7 @@ struct tcp_sock {
 				 * receiver in Recovery. */
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */
 	u32	delivered;	/* Total data packets delivered incl. rexmits */
+	u32	lost;		/* Total data packets lost incl. rexmits */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit 6403389211e1f4d40ed963fe47a96fce1a3ba7a9
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Sep 19 23:39:10 2016 -0400

    tcp: use windowed min filter library for TCP min_rtt estimation
    
    Refactor the TCP min_rtt code to reuse the new win_minmax library in
    lib/win_minmax.c to simplify the TCP code.
    
    This is a pure refactor: the functionality is exactly the same. We
    just moved the windowed min code to make TCP easier to read and
    maintain, and to allow other parts of the kernel to use the windowed
    min/max filter code.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c723a465125d..6433cc8b4667 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -19,6 +19,7 @@
 
 
 #include <linux/skbuff.h>
+#include <linux/win_minmax.h>
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -234,9 +235,7 @@ struct tcp_sock {
 	u32	mdev_max_us;	/* maximal mdev for the last rtt period	*/
 	u32	rttvar_us;	/* smoothed mdev_max			*/
 	u32	rtt_seq;	/* sequence number to update rttvar	*/
-	struct rtt_meas {
-		u32 rtt, ts;	/* RTT in usec and sampling time in jiffies. */
-	} rtt_min[3];
+	struct  minmax rtt_min;
 
 	u32	packets_out;	/* Packets which are "in flight"	*/
 	u32	retrans_out;	/* Retransmitted packets out		*/

commit 9f5afeae51526b3ad7b7cb21ee8b145ce6ea7a7a
Author: Yaogong Wang <wygivan@google.com>
Date:   Wed Sep 7 14:49:28 2016 -0700

    tcp: use an RB tree for ooo receive queue
    
    Over the years, TCP BDP has increased by several orders of magnitude,
    and some people are considering to reach the 2 Gbytes limit.
    
    Even with current window scale limit of 14, ~1 Gbytes maps to ~740,000
    MSS.
    
    In presence of packet losses (or reorders), TCP stores incoming packets
    into an out of order queue, and number of skbs sitting there waiting for
    the missing packets to be received can be in the 10^5 range.
    
    Most packets are appended to the tail of this queue, and when
    packets can finally be transferred to receive queue, we scan the queue
    from its head.
    
    However, in presence of heavy losses, we might have to find an arbitrary
    point in this queue, involving a linear scan for every incoming packet,
    throwing away cpu caches.
    
    This patch converts it to a RB tree, to get bounded latencies.
    
    Yaogong wrote a preliminary patch about 2 years ago.
    Eric did the rebase, added ofo_last_skb cache, polishing and tests.
    
    Tested with network dropping between 1 and 10 % packets, with good
    success (about 30 % increase of throughput in stress tests)
    
    Next step would be to also use an RB tree for the write queue at sender
    side ;)
    
    Signed-off-by: Yaogong Wang <wygivan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Acked-By: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 7be9b1242354..c723a465125d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -281,10 +281,9 @@ struct tcp_sock {
 	struct sk_buff* lost_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
 
-	/* OOO segments go in this list. Note that socket lock must be held,
-	 * as we do not use sk_buff_head lock.
-	 */
-	struct sk_buff_head	out_of_order_queue;
+	/* OOO segments go in this rbtree. Socket lock must be held. */
+	struct rb_root	out_of_order_queue;
+	struct sk_buff	*ooo_last_skb; /* cache rb_last(out_of_order_queue) */
 
 	/* SACKs data, these 2 need to be together (see tcp_options_write) */
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */

commit a44d6eacdaf56f74fad699af7f4925a5f5ac0e7f
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Mar 14 10:52:15 2016 -0700

    tcp: Add RFC4898 tcpEStatsPerfDataSegsOut/In
    
    Per RFC4898, they count segments sent/received
    containing a positive length data segment (that includes
    retransmission segments carrying data).  Unlike
    tcpi_segs_out/in, tcpi_data_segs_out/in excludes segments
    carrying no data (e.g. pure ack).
    
    The patch also updates the segs_in in tcp_fastopen_add_skb()
    so that segs_in >= data_segs_in property is kept.
    
    Together with retransmission data, tcpi_data_segs_out
    gives a better signal on the rxmit rate.
    
    v6: Rebase on the latest net-next
    
    v5: Eric pointed out that checking skb->len is still needed in
    tcp_fastopen_add_skb() because skb can carry a FIN without data.
    Hence, instead of open coding segs_in and data_segs_in, tcp_segs_in()
    helper is used.  Comment is added to the fastopen case to explain why
    segs_in has to be reset and tcp_segs_in() has to be called before
    __skb_pull().
    
    v4: Add comment to the changes in tcp_fastopen_add_skb()
    and also add remark on this case in the commit message.
    
    v3: Add const modifier to the skb parameter in tcp_segs_in()
    
    v2: Rework based on recent fix by Eric:
    commit a9d99ce28ed3 ("tcp: fix tcpi_segs_in after connection establishment")
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Chris Rapier <rapier@psc.edu>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Marcelo Ricardo Leitner <mleitner@redhat.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index bcbf51da4e1e..7be9b1242354 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -158,6 +158,9 @@ struct tcp_sock {
 	u32	segs_in;	/* RFC4898 tcpEStatsPerfSegsIn
 				 * total number of segments in.
 				 */
+	u32	data_segs_in;	/* RFC4898 tcpEStatsPerfDataSegsIn
+				 * total number of data segments in.
+				 */
  	u32	rcv_nxt;	/* What we want to receive next 	*/
 	u32	copied_seq;	/* Head of yet unread data		*/
 	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
@@ -165,6 +168,9 @@ struct tcp_sock {
 	u32	segs_out;	/* RFC4898 tcpEStatsPerfSegsOut
 				 * The total number of segments sent.
 				 */
+	u32	data_segs_out;	/* RFC4898 tcpEStatsPerfDataSegsOut
+				 * total number of data segments sent.
+				 */
 	u64	bytes_acked;	/* RFC4898 tcpEStatsAppHCThruOctetsAcked
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.

commit d9b3fca27385eafe61c3ca6feab6cb1e7dc77482
Author: Craig Gallek <kraig@google.com>
Date:   Wed Feb 10 11:50:37 2016 -0500

    tcp: __tcp_hdrlen() helper
    
    tcp_hdrlen is wasteful if you already have a pointer to struct tcphdr.
    This splits the size calculation into a helper function that can be
    used if a struct tcphdr is already available.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d909feeeaea2..bcbf51da4e1e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -29,9 +29,14 @@ static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
 	return (struct tcphdr *)skb_transport_header(skb);
 }
 
+static inline unsigned int __tcp_hdrlen(const struct tcphdr *th)
+{
+	return th->doff * 4;
+}
+
 static inline unsigned int tcp_hdrlen(const struct sk_buff *skb)
 {
-	return tcp_hdr(skb)->doff * 4;
+	return __tcp_hdrlen(tcp_hdr(skb));
 }
 
 static inline struct tcphdr *inner_tcp_hdr(const struct sk_buff *skb)

commit ddf1af6fa00e772fdb67a7d22cb83fac2b8968a8
Author: Yuchung Cheng <ycheng@google.com>
Date:   Tue Feb 2 10:33:06 2016 -0800

    tcp: new delivery accounting
    
    This patch changes the accounting of how many packets are
    newly acked or sacked when the sender receives an ACK.
    
    The current approach basically computes
    
       newly_acked_sacked = (prior_packets - prior_sacked) -
                            (tp->packets_out - tp->sacked_out)
    
       where prior_packets and prior_sacked out are snapshot
       at the beginning of the ACK processing.
    
    The new approach tracks the delivery information via a new
    TCP state variable "delivered" which monotically increases
    as new packets are delivered in order or out-of-order.
    
    The reason for this change is that the current approach is
    brittle that produces negative or inaccurate estimate.
    
       1) For non-SACK connections, an ACK that advances the SND.UNA
       could reset the DUPACK counters (tp->sacked_out) in
       tcp_process_loss() or tcp_fastretrans_alert(). This inflates
       the inflight suddenly and causes under-estimate or even
       negative estimate. Here is a real example:
    
                       before   after (processing ACK)
       packets_out     75       73
       sacked_out      23        0
       ca state        Loss     Open
    
       The old approach computes (75-23) - (73 - 0) = -21 delivered
       while the new approach computes 1 delivered since it
       considers the 2nd-24th packets are delivered OOO.
    
       2) MSS change would re-count packets_out and sacked_out so
       the estimate is in-accurate and can even become negative.
       E.g., the inflight is doubled when MSS is halved.
    
       3) Spurious retransmission signaled by DSACK is not accounted
    
    The new approach is simpler and more robust. For SACK connections,
    tp->delivered increments as packets are being acked or sacked in
    SACK and ACK processing.
    
    For non-sack connections, it's done in tcp_remove_reno_sacks() and
    tcp_add_reno_sack(). When an ACK advances the SND.UNA, tp->delivered
    is incremented by the number of packets ACKed (less the current
    number of DUPACKs received plus one packet hole).  Upon receiving
    a DUPACK, tp->delivered is incremented assuming one out-of-order
    packet is delivered.
    
    Upon receiving a DSACK, tp->delivered is incremtened assuming one
    retransmission is delivered in tcp_sacktag_write_queue().
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b386361ba3e8..d909feeeaea2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -256,6 +256,7 @@ struct tcp_sock {
 	u32	prr_delivered;	/* Number of newly delivered packets to
 				 * receiver in Recovery. */
 	u32	prr_out;	/* Total number of pkts sent during Recovery. */
+	u32	delivered;	/* Total data packets delivered incl. rexmits */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit 805c4bc05705fb2b71ec970960b456eee9900953
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Nov 5 11:07:13 2015 -0800

    tcp: fix req->saved_syn race
    
    For the reasons explained in commit ce1050089c96 ("tcp/dccp: fix
    ireq->pktopts race"), we need to make sure we do not access
    req->saved_syn unless we own the request sock.
    
    This fixes races for listeners using TCP_SAVE_SYN option.
    
    Fixes: e994b2f0fb92 ("tcp: do not lock listener to process SYN packets")
    Fixes: 079096f103fa ("tcp/dccp: install syn_recv requests into ehash table")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Ying Cai <ycai@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c906f4534581..b386361ba3e8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -397,6 +397,13 @@ static inline void fastopen_queue_tune(struct sock *sk, int backlog)
 	queue->fastopenq.max_qlen = min_t(unsigned int, backlog, somaxconn);
 }
 
+static inline void tcp_move_syn(struct tcp_sock *tp,
+				struct request_sock *req)
+{
+	tp->saved_syn = req->saved_syn;
+	req->saved_syn = NULL;
+}
+
 static inline void tcp_saved_syn_free(struct tcp_sock *tp)
 {
 	kfree(tp->saved_syn);

commit dbf650b67bb4db1b95807d2aafe2d7cfafd458da
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Oct 20 13:17:40 2015 -0700

    tcp: fastopen: limit max_qlen
    
    Allowing an application to set whatever limit for
    the list of recently RST fastopen sessions [1] is not wise,
    as it open ways to deplete kernel memory.
    
    Cap the user provided limit by somaxconn sysctl,
    like listen() backlog.
    
    [1] https://tools.ietf.org/html/rfc7413#section-5.1
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 5dce9705fe84..c906f4534581 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -392,8 +392,9 @@ static inline bool tcp_passive_fastopen(const struct sock *sk)
 static inline void fastopen_queue_tune(struct sock *sk, int backlog)
 {
 	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
+	int somaxconn = READ_ONCE(sock_net(sk)->core.sysctl_somaxconn);
 
-	queue->fastopenq.max_qlen = backlog;
+	queue->fastopenq.max_qlen = min_t(unsigned int, backlog, somaxconn);
 }
 
 static inline void tcp_saved_syn_free(struct tcp_sock *tp)

commit 659a8ad56f490279f0efee43a62ffa1ac914a4e0
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:46 2015 -0700

    tcp: track the packet timings in RACK
    
    This patch is the first half of the RACK loss recovery.
    
    RACK loss recovery uses the notion of time instead
    of packet sequence (FACK) or counts (dupthresh). It's inspired by the
    previous FACK heuristic in tcp_mark_lost_retrans(): when a limited
    transmit (new data packet) is sacked, then current retransmitted
    sequence below the newly sacked sequence must been lost,
    since at least one round trip time has elapsed.
    
    But it has several limitations:
    1) can't detect tail drops since it depends on limited transmit
    2) is disabled upon reordering (assumes no reordering)
    3) only enabled in fast recovery ut not timeout recovery
    
    RACK (Recently ACK) addresses these limitations with the notion
    of time instead: a packet P1 is lost if a later packet P2 is s/acked,
    as at least one round trip has passed.
    
    Since RACK cares about the time sequence instead of the data sequence
    of packets, it can detect tail drops when later retransmission is
    s/acked while FACK or dupthresh can't. For reordering RACK uses a
    dynamically adjusted reordering window ("reo_wnd") to reduce false
    positives on ever (small) degree of reordering.
    
    This patch implements tcp_advanced_rack() which tracks the
    most recent transmission time among the packets that have been
    delivered (ACKed or SACKed) in tp->rack.mstamp. This timestamp
    is the key to determine which packet has been lost.
    
    Consider an example that the sender sends six packets:
    T1: P1 (lost)
    T2: P2
    T3: P3
    T4: P4
    T100: sack of P2. rack.mstamp = T2
    T101: retransmit P1
    T102: sack of P2,P3,P4. rack.mstamp = T4
    T205: ACK of P4 since the hole is repaired. rack.mstamp = T101
    
    We need to be careful about spurious retransmission because it may
    falsely advance tp->rack.mstamp by an RTT or an RTO, causing RACK
    to falsely mark all packets lost, just like a spurious timeout.
    
    We identify spurious retransmission by the ACK's TS echo value.
    If TS option is not applicable but the retransmission is acknowledged
    less than min-RTT ago, it is likely to be spurious. We refrain from
    using the transmission time of these spurious retransmissions.
    
    The second half is implemented in the next patch that marks packet
    lost using RACK timestamp.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8c54863dfc38..5dce9705fe84 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -194,6 +194,12 @@ struct tcp_sock {
 	u32	window_clamp;	/* Maximal window to advertise		*/
 	u32	rcv_ssthresh;	/* Current window clamp			*/
 
+	/* Information of the most recently (s)acked skb */
+	struct tcp_rack {
+		struct skb_mstamp mstamp; /* (Re)sent time of the skb */
+		u8 advanced; /* mstamp advanced since last lost marking */
+		u8 reord;    /* reordering detected */
+	} rack;
 	u16	advmss;		/* Advertised MSS			*/
 	u8	unused;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */

commit af82f4e84866ecd360a53f770d6217637116e6c1
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:43 2015 -0700

    tcp: remove tcp_mark_lost_retrans()
    
    Remove the existing lost retransmit detection because RACK subsumes
    it completely. This also stops the overloading the ack_seq field of
    the skb control block.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 90edef5508f9..8c54863dfc38 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -283,8 +283,6 @@ struct tcp_sock {
 	int     lost_cnt_hint;
 	u32     retransmit_high;	/* L-bits may be on up to this seqno */
 
-	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
-
 	u32	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	u32	high_seq;	/* snd_nxt at onset of congestion	*/
 

commit f672258391b42a5c7cc2732c9c063e56a85c8dbe
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:42 2015 -0700

    tcp: track min RTT using windowed min-filter
    
    Kathleen Nichols' algorithm for tracking the minimum RTT of a
    data stream over some measurement window. It uses constant space
    and constant time per update. Yet it almost always delivers
    the same minimum as an implementation that has to keep all
    the data in the window. The measurement window is tunable via
    sysctl.net.ipv4.tcp_min_rtt_wlen with a default value of 5 minutes.
    
    The algorithm keeps track of the best, 2nd best & 3rd best min
    values, maintaining an invariant that the measurement time of
    the n'th best >= n-1'th best. It also makes sure that the three
    values are widely separated in the time window since that bounds
    the worse case error when that data is monotonically increasing
    over the window.
    
    Upon getting a new min, we can forget everything earlier because
    it has no value - the new min is less than everything else in the
    window by definition and it's the most recent. So we restart fresh
    on every new min and overwrites the 2nd & 3rd choices. The same
    property holds for the 2nd & 3rd best.
    
    Therefore we have to maintain two invariants to maximize the
    information in the samples, one on values (1st.v <= 2nd.v <=
    3rd.v) and the other on times (now-win <=1st.t <= 2nd.t <= 3rd.t <=
    now). These invariants determine the structure of the code
    
    The RTT input to the windowed filter is the minimum RTT measured
    from ACK or SACK, or as the last resort from TCP timestamps.
    
    The accessor tcp_min_rtt() returns the minimum RTT seen in the
    window. ~0U indicates it is not available. The minimum is 1usec
    even if the true RTT is below that.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 86a7edaa6797..90edef5508f9 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -217,6 +217,9 @@ struct tcp_sock {
 	u32	mdev_max_us;	/* maximal mdev for the last rtt period	*/
 	u32	rttvar_us;	/* smoothed mdev_max			*/
 	u32	rtt_seq;	/* sequence number to update rttvar	*/
+	struct rtt_meas {
+		u32 rtt, ts;	/* RTT in usec and sampling time in jiffies. */
+	} rtt_min[3];
 
 	u32	packets_out;	/* Packets which are "in flight"	*/
 	u32	retrans_out;	/* Retransmitted packets out		*/

commit d475f090bf1c0dc2999e98bbf2e7cb2243358849
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:24 2015 -0700

    tcp: shrink tcp_timewait_sock by 8 bytes
    
    Reducing tcp_timewait_sock from 280 bytes to 272 bytes
    allows SLAB to pack 15 objects per page instead of 14 (on x86)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e442e6e9a365..86a7edaa6797 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -356,8 +356,8 @@ static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 
 struct tcp_timewait_sock {
 	struct inet_timewait_sock tw_sk;
-	u32			  tw_rcv_nxt;
-	u32			  tw_snd_nxt;
+#define tw_rcv_nxt tw_sk.__tw_common.skc_tw_rcv_nxt
+#define tw_snd_nxt tw_sk.__tw_common.skc_tw_snd_nxt
 	u32			  tw_rcv_wnd;
 	u32			  tw_ts_offset;
 	u32			  tw_ts_recent;

commit 0536fcc039a8926ec12ec587f41a83f7acafeb82
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:52 2015 -0700

    tcp: prepare fastopen code for upcoming listener changes
    
    While auditing TCP stack for upcoming 'lockless' listener changes,
    I found I had to change fastopen_init_queue() to properly init the object
    before publishing it.
    
    Otherwise an other cpu could try to lock the spinlock before it gets
    properly initialized.
    
    Instead of adding appropriate barriers, just remove dynamic memory
    allocations :
    - Structure is 28 bytes on 64bit arches. Using additional 8 bytes
      for holding a pointer seems overkill.
    - Two listeners can share same cache line and performance would suffer.
    
    If we really want to save few bytes, we would instead dynamically allocate
    whole struct request_sock_queue in the future.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fcb573be75d9..e442e6e9a365 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -382,25 +382,11 @@ static inline bool tcp_passive_fastopen(const struct sock *sk)
 		tcp_sk(sk)->fastopen_rsk != NULL);
 }
 
-extern void tcp_sock_destruct(struct sock *sk);
-
-static inline int fastopen_init_queue(struct sock *sk, int backlog)
+static inline void fastopen_queue_tune(struct sock *sk, int backlog)
 {
-	struct request_sock_queue *queue =
-	    &inet_csk(sk)->icsk_accept_queue;
-
-	if (queue->fastopenq == NULL) {
-		queue->fastopenq = kzalloc(
-		    sizeof(struct fastopen_queue),
-		    sk->sk_allocation);
-		if (queue->fastopenq == NULL)
-			return -ENOMEM;
-
-		sk->sk_destruct = tcp_sock_destruct;
-		spin_lock_init(&queue->fastopenq->lock);
-	}
-	queue->fastopenq->max_qlen = backlog;
-	return 0;
+	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
+
+	queue->fastopenq.max_qlen = backlog;
 }
 
 static inline void tcp_saved_syn_free(struct tcp_sock *tp)

commit 0f1c28ae74bb1a34d36fca2db5161611d58b3148
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Sep 18 11:36:14 2015 -0700

    tcp: usec resolution SYN/ACK RTT
    
    Currently SYN/ACK RTT is measured in jiffies. For LAN the SYN/ACK
    RTT is often measured as 0ms or sometimes 1ms, which would affect
    RTT estimation and min RTT samping used by some congestion control.
    
    This patch improves SYN/ACK RTT to be usec resolution if platform
    supports it. While the timestamping of SYN/ACK is done in request
    sock, the RTT measurement is carefully arranged to avoid storing
    another u64 timestamp in tcp_sock.
    
    For regular handshake w/o SYNACK retransmission, the RTT is sampled
    right after the child socket is created and right before the request
    sock is released (tcp_check_req() in tcp_minisocks.c)
    
    For Fast Open the child socket is already created when SYN/ACK was
    sent, the RTT is sampled in tcp_rcv_state_process() after processing
    the final ACK an right before the request socket is released.
    
    If the SYN/ACK was retransmistted or SYN-cookie was used, we rely
    on TCP timestamps to measure the RTT. The sample is taken at the
    same place in tcp_rcv_state_process() after the timestamp values
    are validated in tcp_validate_incoming(). Note that we do not store
    TS echo value in request_sock for SYN-cookies, because the value
    is already stored in tp->rx_opt used by tcp_ack_update_rtt().
    
    One side benefit is that the RTT measurement now happens before
    initializing congestion control (of the passive side). Therefore
    the congestion control can use the SYN/ACK RTT.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 937b97893d5f..fcb573be75d9 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -112,11 +112,11 @@ struct tcp_request_sock_ops;
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
 	const struct tcp_request_sock_ops *af_specific;
+	struct skb_mstamp		snt_synack; /* first SYNACK sent time */
 	bool				tfo_listener;
 	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;
-	u32				snt_synack; /* synack sent time */
 	u32				last_oow_ack_time; /* last SYNACK */
 	u32				rcv_nxt; /* the ack # by SYNACK. For
 						  * FastOpen it's the seq#

commit 58d607d3e52f2b15902f58a1161da9fb3b0f6d47
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 15 15:24:20 2015 -0700

    tcp: provide skb->hash to synack packets
    
    In commit b73c3d0e4f0e ("net: Save TX flow hash in sock and set in skbuf
    on xmit"), Tom provided a l4 hash to most outgoing TCP packets.
    
    We'd like to provide one as well for SYNACK packets, so that all packets
    of a given flow share same txhash, to later enable bonding driver to
    also use skb->hash to perform slave selection.
    
    Note that a SYNACK retransmit shuffles the tx hash, as Tom did
    in commit 265f94ff54d62 ("net: Recompute sk_txhash on negative routing
    advice") for established sockets.
    
    This has nice effect making TCP flows resilient to some kind of black
    holes, even at connection establish phase.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <tom@herbertland.com>
    Cc: Mahesh Bandewar <maheshb@google.com>
    Acked-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 48c3696e8645..937b97893d5f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -113,6 +113,7 @@ struct tcp_request_sock {
 	struct inet_request_sock 	req;
 	const struct tcp_request_sock_ops *af_specific;
 	bool				tfo_listener;
+	u32				txhash;
 	u32				rcv_isn;
 	u32				snt_isn;
 	u32				snt_synack; /* synack sent time */

commit 36583eb54d46c36a447afd6c379839f292397429
Merge: fa7912be9671 cf539cbd8a81
Author: David S. Miller <davem@davemloft.net>
Date:   Sat May 23 01:22:35 2015 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/cadence/macb.c
            drivers/net/phy/phy.c
            include/linux/skbuff.h
            net/ipv4/tcp.c
            net/switchdev/switchdev.c
    
    Switchdev was a case of RTNH_H_{EXTERNAL --> OFFLOAD}
    renaming overlapping with net-next changes of various
    sorts.
    
    phy.c was a case of two changes, one adding a local
    variable to a function whilst the second was removing
    one.
    
    tcp.c overlapped a deadlock fix with the addition of new tcp_info
    statistic values.
    
    macb.c involved the addition of two zyncq device entries.
    
    skbuff.h involved adding back ipv4_daddr to nf_bridge_info
    whilst net-next changes put two other existing members of
    that struct into a union.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d654976cbf852ee20612ee10dbe57cdacda9f452
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 21 21:51:19 2015 -0700

    tcp: fix a potential deadlock in tcp_get_info()
    
    Taking socket spinlock in tcp_get_info() can deadlock, as
    inet_diag_dump_icsk() holds the &hashinfo->ehash_locks[i],
    while packet processing can use the reverse locking order.
    
    We could avoid this locking for TCP_LISTEN states, but lockdep would
    certainly get confused as all TCP sockets share same lockdep classes.
    
    [  523.722504] ======================================================
    [  523.728706] [ INFO: possible circular locking dependency detected ]
    [  523.734990] 4.1.0-dbg-DEV #1676 Not tainted
    [  523.739202] -------------------------------------------------------
    [  523.745474] ss/18032 is trying to acquire lock:
    [  523.750002]  (slock-AF_INET){+.-...}, at: [<ffffffff81669d44>] tcp_get_info+0x2c4/0x360
    [  523.758129]
    [  523.758129] but task is already holding lock:
    [  523.763968]  (&(&hashinfo->ehash_locks[i])->rlock){+.-...}, at: [<ffffffff816bcb75>] inet_diag_dump_icsk+0x1d5/0x6c0
    [  523.774661]
    [  523.774661] which lock already depends on the new lock.
    [  523.774661]
    [  523.782850]
    [  523.782850] the existing dependency chain (in reverse order) is:
    [  523.790326]
    -> #1 (&(&hashinfo->ehash_locks[i])->rlock){+.-...}:
    [  523.796599]        [<ffffffff811126bb>] lock_acquire+0xbb/0x270
    [  523.802565]        [<ffffffff816f5868>] _raw_spin_lock+0x38/0x50
    [  523.808628]        [<ffffffff81665af8>] __inet_hash_nolisten+0x78/0x110
    [  523.815273]        [<ffffffff816819db>] tcp_v4_syn_recv_sock+0x24b/0x350
    [  523.822067]        [<ffffffff81684d41>] tcp_check_req+0x3c1/0x500
    [  523.828199]        [<ffffffff81682d09>] tcp_v4_do_rcv+0x239/0x3d0
    [  523.834331]        [<ffffffff816842fe>] tcp_v4_rcv+0xa8e/0xc10
    [  523.840202]        [<ffffffff81658fa3>] ip_local_deliver_finish+0x133/0x3e0
    [  523.847214]        [<ffffffff81659a9a>] ip_local_deliver+0xaa/0xc0
    [  523.853440]        [<ffffffff816593b8>] ip_rcv_finish+0x168/0x5c0
    [  523.859624]        [<ffffffff81659db7>] ip_rcv+0x307/0x420
    
    Lets use u64_sync infrastructure instead. As a bonus, 64bit
    arches get optimized, as these are nop for them.
    
    Fixes: 0df48c26d841 ("tcp: add tcpi_bytes_acked to tcp_info")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3b2911502a8c..e8bbf403618f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -158,6 +158,8 @@ struct tcp_sock {
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.
 				 */
+	struct u64_stats_sync syncp; /* protects 64bit vars (cf tcp_get_info()) */
+
  	u32	snd_una;	/* First byte we want an ack for	*/
  	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */

commit 2efd055c53c06b7e89c167c98069bab9afce7e59
Author: Marcelo Ricardo Leitner <mleitner@redhat.com>
Date:   Wed May 20 16:35:41 2015 -0700

    tcp: add tcpi_segs_in and tcpi_segs_out to tcp_info
    
    This patch tracks the total number of inbound and outbound segments on a
    TCP socket. One may use this number to have an idea on connection
    quality when compared against the retransmissions.
    
    RFC4898 named these : tcpEStatsPerfSegsIn and tcpEStatsPerfSegsOut
    
    These are a 32bit field each and can be fetched both from TCP_INFO
    getsockopt() if one has a handle on a TCP socket, or from inet_diag
    netlink facility (iproute2/ss patch will follow)
    
    Note that tp->segs_out was placed near tp->snd_nxt for good data
    locality and minimal performance impact, while tp->segs_in was placed
    near tp->bytes_received for the same reason.
    
    Join work with Eric Dumazet.
    
    Note that received SYN are accounted on the listener, but sent SYNACK
    are not accounted.
    
    Signed-off-by: Marcelo Ricardo Leitner <mleitner@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e6fb5df22db1..f0212026c77f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -149,11 +149,16 @@ struct tcp_sock {
 				 * sum(delta(rcv_nxt)), or how many bytes
 				 * were acked.
 				 */
+	u32	segs_in;	/* RFC4898 tcpEStatsPerfSegsIn
+				 * total number of segments in.
+				 */
  	u32	rcv_nxt;	/* What we want to receive next 	*/
 	u32	copied_seq;	/* Head of yet unread data		*/
 	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
  	u32	snd_nxt;	/* Next sequence we send		*/
-
+	u32	segs_out;	/* RFC4898 tcpEStatsPerfSegsOut
+				 * The total number of segments sent.
+				 */
 	u64	bytes_acked;	/* RFC4898 tcpEStatsAppHCThruOctetsAcked
 				 * sum(delta(snd_una)), or how many bytes
 				 * were acked.

commit cd8ae85299d54155702a56811b2e035e63064d3d
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun May 3 21:34:46 2015 -0700

    tcp: provide SYN headers for passive connections
    
    This patch allows a server application to get the TCP SYN headers for
    its passive connections.  This is useful if the server is doing
    fingerprinting of clients based on SYN packet contents.
    
    Two socket options are added: TCP_SAVE_SYN and TCP_SAVED_SYN.
    
    The first is used on a socket to enable saving the SYN headers
    for child connections. This can be set before or after the listen()
    call.
    
    The latter is used to retrieve the SYN headers for passive connections,
    if the parent listener has enabled TCP_SAVE_SYN.
    
    TCP_SAVED_SYN is read once, it frees the saved SYN headers.
    
    The data returned in TCP_SAVED_SYN are network (IPv4/IPv6) and TCP
    headers.
    
    Original patch was written by Tom Herbert, I changed it to not hold
    a full skb (and associated dst and conntracking reference).
    
    We have used such patch for about 3 years at Google.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Tested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3b2911502a8c..e6fb5df22db1 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -199,6 +199,7 @@ struct tcp_sock {
 		syn_fastopen:1,	/* SYN includes Fast Open option */
 		syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
 		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
+		save_syn:1,	/* Save headers of SYN packet */
 		is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
@@ -326,6 +327,7 @@ struct tcp_sock {
 	 * socket. Used to retransmit SYNACKs etc.
 	 */
 	struct request_sock *fastopen_rsk;
+	u32	*saved_syn;
 };
 
 enum tsq_flags {
@@ -393,4 +395,10 @@ static inline int fastopen_init_queue(struct sock *sk, int backlog)
 	return 0;
 }
 
+static inline void tcp_saved_syn_free(struct tcp_sock *tp)
+{
+	kfree(tp->saved_syn);
+	tp->saved_syn = NULL;
+}
+
 #endif	/* _LINUX_TCP_H */

commit bdd1f9edacb5f5835d1e6276571bbbe5b88ded48
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 28 15:28:18 2015 -0700

    tcp: add tcpi_bytes_received to tcp_info
    
    This patch tracks total number of payload bytes received on a TCP socket.
    This is the sum of all changes done to tp->rcv_nxt
    
    RFC4898 named this : tcpEStatsAppHCThruOctetsReceived
    
    This is a 64bit field, and can be fetched both from TCP_INFO
    getsockopt() if one has a handle on a TCP socket, or from inet_diag
    netlink facility (iproute2/ss patch will follow)
    
    Note that tp->bytes_received was placed near tp->rcv_nxt for
    best data locality and minimal performance impact.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Eric Salo <salo@google.com>
    Cc: Martin Lau <kafai@fb.com>
    Cc: Chris Rapier <rapier@psc.edu>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0f73b43171da..3b2911502a8c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -145,6 +145,10 @@ struct tcp_sock {
  *	read the code and the spec side by side (and laugh ...)
  *	See RFC793 and RFC1122. The RFC writes these in capitals.
  */
+	u64	bytes_received;	/* RFC4898 tcpEStatsAppHCThruOctetsReceived
+				 * sum(delta(rcv_nxt)), or how many bytes
+				 * were acked.
+				 */
  	u32	rcv_nxt;	/* What we want to receive next 	*/
 	u32	copied_seq;	/* Head of yet unread data		*/
 	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/

commit 0df48c26d8418c5c9fba63fac15b660d70ca2f1c
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 28 15:28:17 2015 -0700

    tcp: add tcpi_bytes_acked to tcp_info
    
    This patch tracks total number of bytes acked for a TCP socket.
    This is the sum of all changes done to tp->snd_una, and allows
    for precise tracking of delivered data.
    
    RFC4898 named this : tcpEStatsAppHCThruOctetsAcked
    
    This is a 64bit field, and can be fetched both from TCP_INFO
    getsockopt() if one has a handle on a TCP socket, or from inet_diag
    netlink facility (iproute2/ss patch will follow)
    
    Note that tp->bytes_acked was placed near tp->snd_una for
    best data locality and minimal performance impact.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Eric Salo <salo@google.com>
    Cc: Martin Lau <kafai@fb.com>
    Cc: Chris Rapier <rapier@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0caa3a2d4106..0f73b43171da 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -150,6 +150,10 @@ struct tcp_sock {
 	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
  	u32	snd_nxt;	/* Next sequence we send		*/
 
+	u64	bytes_acked;	/* RFC4898 tcpEStatsAppHCThruOctetsAcked
+				 * sum(delta(snd_una)), or how many bytes
+				 * were acked.
+				 */
  	u32	snd_una;	/* First byte we want an ack for	*/
  	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */

commit 2646c831c00c5d22aa72b79d24069c1b412cda7c
Author: Daniel Lee <Longinus00@gmail.com>
Date:   Mon Apr 6 14:37:27 2015 -0700

    tcp: RFC7413 option support for Fast Open client
    
    Fast Open has been using an experimental option with a magic number
    (RFC6994). This patch makes the client by default use the RFC7413
    option (34) to get and send Fast Open cookies.  This patch makes
    the client solicit cookies from a given server first with the
    RFC7413 option. If that fails to elicit a cookie, then it tries
    the RFC6994 experimental option. If that also fails, it uses the
    RFC7413 option on all subsequent connect attempts.  If the server
    returns a Fast Open cookie then the client caches the form of the
    option that successfully elicited a cookie, and uses that form on
    later connects when it presents that cookie.
    
    The idea is to gradually obsolete the use of experimental options as
    the servers and clients upgrade, while keeping the interoperability
    meanwhile.
    
    Signed-off-by: Daniel Lee <Longinus00@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index a48d00318683..0caa3a2d4106 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -189,6 +189,7 @@ struct tcp_sock {
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
+		syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
 		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
 		is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */

commit 7f9b838b71eb78a27de27a12ca5de8542fac3115
Author: Daniel Lee <Longinus00@gmail.com>
Date:   Mon Apr 6 14:37:26 2015 -0700

    tcp: RFC7413 option support for Fast Open server
    
    Fast Open has been using the experimental option with a magic number
    (RFC6994) to request and grant Fast Open cookies. This patch enables
    the server to support the official IANA option 34 in RFC7413 in
    addition.
    
    The change has passed all existing Fast Open tests with both
    old and new options at Google.
    
    Signed-off-by: Daniel Lee <Longinus00@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f869ae8afbaf..a48d00318683 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -58,6 +58,7 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 struct tcp_fastopen_cookie {
 	s8	len;
 	u8	val[TCP_FASTOPEN_COOKIE_MAX];
+	bool	exp;	/* In RFC6994 experimental option format */
 };
 
 /* This defines a selective acknowledgement block. */

commit 9439ce00f208d95703a6725e4ea986dd90e37ffd
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 17 18:32:29 2015 -0700

    tcp: rename struct tcp_request_sock listener
    
    The listener field in struct tcp_request_sock is a pointer
    back to the listener. We now have req->rsk_listener, so TCP
    only needs one boolean and not a full pointer.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 97dbf16f7d9d..f869ae8afbaf 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -111,7 +111,7 @@ struct tcp_request_sock_ops;
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
 	const struct tcp_request_sock_ops *af_specific;
-	struct sock			*listener; /* needed for TFO */
+	bool				tfo_listener;
 	u32				rcv_isn;
 	u32				snt_isn;
 	u32				snt_synack; /* synack sent time */

commit 5f852eb536ad651b8734559dcf4353514cb0bea3
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Feb 26 14:10:18 2015 -0800

    tcp: tso: remove tp->tso_deferred
    
    TSO relies on ability to defer sending a small amount of packets.
    Heuristic is to wait for future ACKS in hope to send more packets at once.
    Current algorithm uses a per socket tso_deferred field as a pseudo timer.
    
    This pseudo timer relies on future ACK, but there is no guarantee
    we receive them in time.
    
    Fix would be to use a real timer, but cost of such timer is probably too
    expensive for typical cases.
    
    This patch changes the logic to test the time of last transmit,
    because we should not add bursts of more than 1ms for any given flow.
    
    We've used this patch for about two years at Google, before FQ/pacing
    as it would reduce a fair amount of bursts.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1a7adb411647..97dbf16f7d9d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -236,7 +236,6 @@ struct tcp_sock {
 	u32	lost_out;	/* Lost packets			*/
 	u32	sacked_out;	/* SACK'd packets			*/
 	u32	fackets_out;	/* FACK'd packets			*/
-	u32	tso_deferred;
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit 4fb17a6091674f469e8ac85dc770fbf9a9ba7cc8
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Feb 6 16:04:41 2015 -0500

    tcp: mitigate ACK loops for connections as tcp_timewait_sock
    
    Ensure that in state FIN_WAIT2 or TIME_WAIT, where the connection is
    represented by a tcp_timewait_sock, we rate limit dupacks in response
    to incoming packets (a) with TCP timestamps that fail PAWS checks, or
    (b) with sequence numbers that are out of the acceptable window.
    
    We do not send a dupack in response to out-of-window packets if it has
    been less than sysctl_tcp_invalid_ratelimit (default 500ms) since we
    last sent a dupack in response to an out-of-window packet.
    
    Reported-by: Avery Fay <avery@mixpanel.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 66d85a80a1ec..1a7adb411647 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -342,6 +342,10 @@ struct tcp_timewait_sock {
 	u32			  tw_rcv_wnd;
 	u32			  tw_ts_offset;
 	u32			  tw_ts_recent;
+
+	/* The time we sent the last out-of-window ACK: */
+	u32			  tw_last_oow_ack_time;
+
 	long			  tw_ts_recent_stamp;
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	  *tw_md5_key;

commit f2b2c582e82429270d5818fbabe653f4359d7024
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Feb 6 16:04:40 2015 -0500

    tcp: mitigate ACK loops for connections as tcp_sock
    
    Ensure that in state ESTABLISHED, where the connection is represented
    by a tcp_sock, we rate limit dupacks in response to incoming packets
    (a) with TCP timestamps that fail PAWS checks, or (b) with sequence
    numbers or ACK numbers that are out of the acceptable window.
    
    We do not send a dupack in response to out-of-window packets if it has
    been less than sysctl_tcp_invalid_ratelimit (default 500ms) since we
    last sent a dupack in response to an out-of-window packet.
    
    There is already a similar (although global) rate-limiting mechanism
    for "challenge ACKs". When deciding whether to send a challence ACK,
    we first consult the new per-connection rate limit, and then the
    global rate limit.
    
    Reported-by: Avery Fay <avery@mixpanel.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index bcc828d3b9b9..66d85a80a1ec 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -153,6 +153,7 @@ struct tcp_sock {
  	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
+	u32	last_oow_ack_time;  /* timestamp of last out-of-window ACK */
 
 	u32	tsoffset;	/* timestamp offset */
 

commit a9b2c06dbef48ed31cff1764c5ce824829106f4f
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Feb 6 16:04:39 2015 -0500

    tcp: mitigate ACK loops for connections as tcp_request_sock
    
    In the SYN_RECV state, where the TCP connection is represented by
    tcp_request_sock, we now rate-limit SYNACKs in response to a client's
    retransmitted SYNs: we do not send a SYNACK in response to client SYN
    if it has been less than sysctl_tcp_invalid_ratelimit (default 500ms)
    since we last sent a SYNACK in response to a client's retransmitted
    SYN.
    
    This allows the vast majority of legitimate client connections to
    proceed unimpeded, even for the most aggressive platforms, iOS and
    MacOS, which actually retransmit SYNs 1-second intervals for several
    times in a row. They use SYN RTO timeouts following the progression:
    1,1,1,1,1,2,4,8,16,32.
    
    Reported-by: Avery Fay <avery@mixpanel.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 67309ece0772..bcc828d3b9b9 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -115,6 +115,7 @@ struct tcp_request_sock {
 	u32				rcv_isn;
 	u32				snt_isn;
 	u32				snt_synack; /* synack sent time */
+	u32				last_oow_ack_time; /* last SYNACK */
 	u32				rcv_nxt; /* the ack # by SYNACK. For
 						  * FastOpen it's the seq#
 						  * after data-in-SYN.

commit 6e5f59aacbf9527dfe425541c78cb8c56623e7eb
Merge: 6c702fab6263 218321e7a083
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Dec 10 13:17:23 2014 -0500

    Merge branch 'for-davem-2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    More iov_iter work for the networking from Al Viro.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 605ad7f184b60cfaacbc038aa6c55ee68dee3c89
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Dec 7 12:22:18 2014 -0800

    tcp: refine TSO autosizing
    
    Commit 95bd09eb2750 ("tcp: TSO packets automatic sizing") tried to
    control TSO size, but did this at the wrong place (sendmsg() time)
    
    At sendmsg() time, we might have a pessimistic view of flow rate,
    and we end up building very small skbs (with 2 MSS per skb).
    
    This is bad because :
    
     - It sends small TSO packets even in Slow Start where rate quickly
       increases.
     - It tends to make socket write queue very big, increasing tcp_ack()
       processing time, but also increasing memory needs, not necessarily
       accounted for, as fast clones overhead is currently ignored.
     - Lower GRO efficiency and more ACK packets.
    
    Servers with a lot of small lived connections suffer from this.
    
    Lets instead fill skbs as much as possible (64KB of payload), but split
    them at xmit time, when we have a precise idea of the flow rate.
    skb split is actually quite efficient.
    
    Patch looks bigger than necessary, because TCP Small Queue decision now
    has to take place after the eventual split.
    
    As Neal suggested, introduce a new tcp_tso_autosize() helper, so that
    tcp_tso_should_defer() can be synchronized on same goal.
    
    Rename tp->xmit_size_goal_segs to tp->gso_segs, as this variable
    contains number of mss that we can put in GSO packet, and is not
    related to the autosizing goal anymore.
    
    Tested:
    
    40 ms rtt link
    
    nstat >/dev/null
    netperf -H remote -l -2000000 -- -s 1000000
    nstat | egrep "IpInReceives|IpOutRequests|TcpOutSegs|IpExtOutOctets"
    
    Before patch :
    
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/s
    
     87380 2000000 2000000    0.36         44.22
    IpInReceives                    600                0.0
    IpOutRequests                   599                0.0
    TcpOutSegs                      1397               0.0
    IpExtOutOctets                  2033249            0.0
    
    After patch :
    
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/sec
    
     87380 2000000 2000000    0.36       44.27
    IpInReceives                    221                0.0
    IpOutRequests                   232                0.0
    TcpOutSegs                      1397               0.0
    IpExtOutOctets                  2013953            0.0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f566b8567892..3fa0a9669a3a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -130,7 +130,7 @@ struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
 	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
-	u16	xmit_size_goal_segs; /* Goal for segmenting output packets */
+	u16	gso_segs;	/* Max number of segs per GSO packet	*/
 
 /*
  *	Header prediction flags

commit f4362a2c9524678f0459cf410403f8595e5cfce5
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Nov 24 13:26:06 2014 -0500

    switch tcp_sock->ucopy from iovec (ucopy.iov) to msghdr (ucopy.msg)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f566b8567892..5d9cc9cd2855 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -162,7 +162,7 @@ struct tcp_sock {
 	struct {
 		struct sk_buff_head	prequeue;
 		struct task_struct	*task;
-		struct iovec		*iov;
+		struct msghdr		*msg;
 		int			memory;
 		int			len;
 	} ucopy;

commit dca145ffaa8d39ea1904491ac81b92b7049372c0
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 27 21:45:24 2014 -0700

    tcp: allow for bigger reordering level
    
    While testing upcoming Yaogong patch (converting out of order queue
    into an RB tree), I hit the max reordering level of linux TCP stack.
    
    Reordering level was limited to 127 for no good reason, and some
    network setups [1] can easily reach this limit and get limited
    throughput.
    
    Allow a new max limit of 300, and add a sysctl to allow admins to even
    allow bigger (or lower) values if needed.
    
    [1] Aggregation of links, per packet load balancing, fabrics not doing
     deep packet inspections, alternative TCP congestion modules...
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yaogong Wang <wygivan@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c2dee7deefa8..f566b8567892 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -204,10 +204,10 @@ struct tcp_sock {
 
 	u16	urg_data;	/* Saved octet of OOB data and control flags */
 	u8	ecn_flags;	/* ECN status bits.			*/
-	u8	reordering;	/* Packet reordering metric.		*/
+	u8	keepalive_probes; /* num of allowed keep alive probes	*/
+	u32	reordering;	/* Packet reordering metric.		*/
 	u32	snd_up;		/* Urgent pointer		*/
 
-	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */

commit 35a9ad8af0bb0fa3525e6d0d20e32551d226f38e
Merge: d5935b07da53 64b1f00a0830
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 21:40:54 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Most notable changes in here:
    
       1) By far the biggest accomplishment, thanks to a large range of
          contributors, is the addition of multi-send for transmit.  This is
          the result of discussions back in Chicago, and the hard work of
          several individuals.
    
          Now, when the ->ndo_start_xmit() method of a driver sees
          skb->xmit_more as true, it can choose to defer the doorbell
          telling the driver to start processing the new TX queue entires.
    
          skb->xmit_more means that the generic networking is guaranteed to
          call the driver immediately with another SKB to send.
    
          There is logic added to the qdisc layer to dequeue multiple
          packets at a time, and the handling mis-predicted offloads in
          software is now done with no locks held.
    
          Finally, pktgen is extended to have a "burst" parameter that can
          be used to test a multi-send implementation.
    
          Several drivers have xmit_more support: i40e, igb, ixgbe, mlx4,
          virtio_net
    
          Adding support is almost trivial, so export more drivers to
          support this optimization soon.
    
          I want to thank, in no particular or implied order, Jesper
          Dangaard Brouer, Eric Dumazet, Alexander Duyck, Tom Herbert, Jamal
          Hadi Salim, John Fastabend, Florian Westphal, Daniel Borkmann,
          David Tat, Hannes Frederic Sowa, and Rusty Russell.
    
       2) PTP and timestamping support in bnx2x, from Michal Kalderon.
    
       3) Allow adjusting the rx_copybreak threshold for a driver via
          ethtool, and add rx_copybreak support to enic driver.  From
          Govindarajulu Varadarajan.
    
       4) Significant enhancements to the generic PHY layer and the bcm7xxx
          driver in particular (EEE support, auto power down, etc.) from
          Florian Fainelli.
    
       5) Allow raw buffers to be used for flow dissection, allowing drivers
          to determine the optimal "linear pull" size for devices that DMA
          into pools of pages.  The objective is to get exactly the
          necessary amount of headers into the linear SKB area pre-pulled,
          but no more.  The new interface drivers use is eth_get_headlen().
          From WANG Cong, with driver conversions (several had their own
          by-hand duplicated implementations) by Alexander Duyck and Eric
          Dumazet.
    
       6) Support checksumming more smoothly and efficiently for
          encapsulations, and add "foo over UDP" facility.  From Tom
          Herbert.
    
       7) Add Broadcom SF2 switch driver to DSA layer, from Florian
          Fainelli.
    
       8) eBPF now can load programs via a system call and has an extensive
          testsuite.  Alexei Starovoitov and Daniel Borkmann.
    
       9) Major overhaul of the packet scheduler to use RCU in several major
          areas such as the classifiers and rate estimators.  From John
          Fastabend.
    
      10) Add driver for Intel FM10000 Ethernet Switch, from Alexander
          Duyck.
    
      11) Rearrange TCP_SKB_CB() to reduce cache line misses, from Eric
          Dumazet.
    
      12) Add Datacenter TCP congestion control algorithm support, From
          Florian Westphal.
    
      13) Reorganize sk_buff so that __copy_skb_header() is significantly
          faster.  From Eric Dumazet"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1558 commits)
      netlabel: directly return netlbl_unlabel_genl_init()
      net: add netdev_txq_bql_{enqueue, complete}_prefetchw() helpers
      net: description of dma_cookie cause make xmldocs warning
      cxgb4: clean up a type issue
      cxgb4: potential shift wrapping bug
      i40e: skb->xmit_more support
      net: fs_enet: Add NAPI TX
      net: fs_enet: Remove non NAPI RX
      r8169:add support for RTL8168EP
      net_sched: copy exts->type in tcf_exts_change()
      wimax: convert printk to pr_foo()
      af_unix: remove 0 assignment on static
      ipv6: Do not warn for informational ICMP messages, regardless of type.
      Update Intel Ethernet Driver maintainers list
      bridge: Save frag_max_size between PRE_ROUTING and POST_ROUTING
      tipc: fix bug in multicast congestion handling
      net: better IFF_XMIT_DST_RELEASE support
      net/mlx4_en: remove NETDEV_TX_BUSY
      3c59x: fix bad split of cpu_to_le32(pci_map_single())
      net: bcmgenet: fix Tx ring priority programming
      ...

commit d0cd84817c745655428dbfdb1e3f754230b46bef
Merge: bdf428feb225 3f3340785672
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 7 20:39:25 2014 -0400

    Merge tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine updates from Dan Williams:
     "Even though this has fixes marked for -stable, given the size and the
      needed conflict resolutions this is 3.18-rc1/merge-window material.
    
      These patches have been languishing in my tree for a long while.  The
      fact that I do not have the time to do proper/prompt maintenance of
      this tree is a primary factor in the decision to step down as
      dmaengine maintainer.  That and the fact that the bulk of drivers/dma/
      activity is going through Vinod these days.
    
      The net_dma removal has not been in -next.  It has developed simple
      conflicts against mainline and net-next (for-3.18).
    
      Continuing thanks to Vinod for staying on top of drivers/dma/.
    
      Summary:
    
       1/ Step down as dmaengine maintainer see commit 08223d80df38
          "dmaengine maintainer update"
    
       2/ Removal of net_dma, as it has been marked 'broken' since 3.13
          (commit 77873803363c "net_dma: mark broken"), without reports of
          performance regression.
    
       3/ Miscellaneous fixes"
    
    * tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      net: make tcp_cleanup_rbuf private
      net_dma: revert 'copied_early'
      net_dma: simple removal
      dmaengine maintainer update
      dmatest: prevent memory leakage on error path in thread
      ioat: Use time_before_jiffies()
      dmaengine: fix xor sources continuation
      dma: mv_xor: Rename __mv_xor_slot_cleanup() to mv_xor_slot_cleanup()
      dma: mv_xor: Remove all callers of mv_xor_slot_cleanup()
      dma: mv_xor: Remove unneeded mv_xor_clean_completed_slots() call
      ioat: Use pci_enable_msix_exact() instead of pci_enable_msix()
      drivers: dma: Include appropriate header file in dca.c
      drivers: dma: Mark functions as static in dma_v3.c
      dma: mv_xor: Add DMA API error checks
      ioat/dca: Use dev_is_pci() to check whether it is pci device

commit 7bced397510ab569d31de4c70b39e13355046387
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 12:37:29 2013 -0800

    net_dma: simple removal
    
    Per commit "77873803363c net_dma: mark broken" net_dma is no longer used
    and there is no plan to fix it.
    
    This is the mechanical removal of bits in CONFIG_NET_DMA ifdef guards.
    Reverting the remainder of the net_dma induced changes is deferred to
    subsequent patches.
    
    Marked for stable due to Roman's report of a memory leak in
    dma_pin_iovec_pages():
    
        https://lkml.org/lkml/2014/9/3/177
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: David Whipple <whipple@securedatainnovations.ch>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4ad0706d40eb..90895b8dc7f2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -19,7 +19,6 @@
 
 
 #include <linux/skbuff.h>
-#include <linux/dmaengine.h>
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -169,13 +168,6 @@ struct tcp_sock {
 		struct iovec		*iov;
 		int			memory;
 		int			len;
-#ifdef CONFIG_NET_DMA
-		/* members for async copy */
-		struct dma_chan		*dma_chan;
-		int			wakeup;
-		struct dma_pinned_list	*pinned_list;
-		dma_cookie_t		dma_cookie;
-#endif
 	} ucopy;
 
 	u32	snd_wl1;	/* Sequence for window update		*/

commit 989e04c5bc3ff77d65e1f0d87bf7904dfa30d41c
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Aug 22 14:15:22 2014 -0700

    tcp: improve undo on timeout
    
    Upon timeout, undo (via both timestamps/Eifel and DSACKs) was
    disabled if any retransmits were still in flight.  The concern was
    perhaps that spurious retransmission sent in a previous recovery
    episode may trigger DSACKs to falsely undo the current recovery.
    
    However, this inadvertently misses undo opportunities (using either
    TCP timestamps or DSACKs) when timeout occurs during a loss episode,
    i.e.  recurring timeouts or timeout during fast recovery. In these
    cases some retransmissions will be in flight but we should allow
    undo. Furthermore, we should only reset undo_marker and undo_retrans
    upon timeout if we are starting a new recovery episode. Finally,
    when we do reset our undo state, we now do so in a manner similar
    to tcp_enter_recovery(), so that we require a DSACK for each of
    the outstsanding retransmissions. This will achieve the original
    goal by requiring that we receive the same number of DSACKs as
    retransmissions.
    
    This patch increases the undo events by 50% on Google servers.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fa5258f322e7..e567f0dbf282 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -276,7 +276,7 @@ struct tcp_sock {
 	u32	retrans_stamp;	/* Timestamp of the last retransmit,
 				 * also used in SYN-SENT to remember stamp of
 				 * the first SYN. */
-	u32	undo_marker;	/* tracking retrans started here. */
+	u32	undo_marker;	/* snd_una upon a new recovery episode. */
 	int	undo_retrans;	/* number of undoable retransmissions. */
 	u32	total_retrans;	/* Total retransmits for entire connection */
 

commit 16bea70aa7302b6f3bf3502d5a0efb4ea2ce4712
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:53 2014 +0300

    tcp: add init_req method to tcp_request_sock_ops
    
    Move the specific IPv4/IPv6 intializations to a new method in
    tcp_request_sock_ops in preparation for unifying tcp_v4_conn_request
    and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index a0513210798f..fa5258f322e7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -111,10 +111,7 @@ struct tcp_request_sock_ops;
 
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
-#ifdef CONFIG_TCP_MD5SIG
-	/* Only used by TCP MD5 Signature so far. */
 	const struct tcp_request_sock_ops *af_specific;
-#endif
 	struct sock			*listener; /* needed for TFO */
 	u32				rcv_isn;
 	u32				snt_isn;

commit ca8a22634381537c92b5a10308652e1c38fd9edf
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu May 22 10:41:08 2014 -0400

    tcp: make cwnd-limited checks measurement-based, and gentler
    
    Experience with the recent e114a710aa50 ("tcp: fix cwnd limited
    checking to improve congestion control") has shown that there are
    common cases where that commit can cause cwnd to be much larger than
    necessary. This leads to TSO autosizing cooking skbs that are too
    large, among other things.
    
    The main problems seemed to be:
    
    (1) That commit attempted to predict the future behavior of the
    connection by looking at the write queue (if TSO or TSQ limit
    sending). That prediction sometimes overestimated future outstanding
    packets.
    
    (2) That commit always allowed cwnd to grow to twice the number of
    outstanding packets (even in congestion avoidance, where this is not
    needed).
    
    This commit improves both of these, by:
    
    (1) Switching to a measurement-based approach where we explicitly
    track the largest number of packets in flight during the past window
    ("max_packets_out"), and remember whether we were cwnd-limited at the
    moment we finished sending that flight.
    
    (2) Only allowing cwnd to grow to twice the number of outstanding
    packets ("max_packets_out") in slow start. In congestion avoidance
    mode we now only allow cwnd to grow if it was fully utilized.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index bc35e4709e8e..a0513210798f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -197,7 +197,8 @@ struct tcp_sock {
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
-		syn_data_acked:1;/* data in SYN is acked by SYN-ACK */
+		syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
+		is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */
@@ -209,6 +210,8 @@ struct tcp_sock {
 
 	u32	packets_out;	/* Packets which are "in flight"	*/
 	u32	retrans_out;	/* Retransmitted packets out		*/
+	u32	max_packets_out;  /* max packets_out in last window */
+	u32	max_packets_seq;  /* right edge of max_packets_out flight */
 
 	u16	urg_data;	/* Saved octet of OOB data and control flags */
 	u8	ecn_flags;	/* ECN status bits.			*/
@@ -230,7 +233,6 @@ struct tcp_sock {
 	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
-	u32	lsnd_pending;	/* packets inflight or unsent since last xmit */
 	u32	prior_cwnd;	/* Congestion window at start of Recovery. */
 	u32	prr_delivered;	/* Number of newly delivered packets to
 				 * receiver in Recovery. */

commit 89278c9dc922272df921042aafa18311f3398c6c
Author: Yuchung Cheng <ycheng@google.com>
Date:   Sun May 11 20:22:10 2014 -0700

    tcp: simplify fast open cookie processing
    
    Consolidate various cookie checking and generation code to simplify
    the fast open processing. The main goal is to reduce code duplication
    in tcp_v4_conn_request() for IPv6 support.
    
    Removes two experimental sysctl flags TFO_SERVER_ALWAYS and
    TFO_SERVER_COOKIE_NOT_CHKD used primarily for developmental debugging
    purposes.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Daniel Lee <longinus00@gmail.com>
    Signed-off-by: Jerry Chu <hkchu@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4e37c71ecd74..bc35e4709e8e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -366,11 +366,6 @@ static inline bool tcp_passive_fastopen(const struct sock *sk)
 		tcp_sk(sk)->fastopen_rsk != NULL);
 }
 
-static inline bool fastopen_cookie_present(struct tcp_fastopen_cookie *foc)
-{
-	return foc->len != -1;
-}
-
 extern void tcp_sock_destruct(struct sock *sk);
 
 static inline int fastopen_init_queue(struct sock *sk, int backlog)

commit e114a710aa5058c0ba4aa1dfb105132aefeb5e04
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 30 11:58:13 2014 -0700

    tcp: fix cwnd limited checking to improve congestion control
    
    Yuchung discovered tcp_is_cwnd_limited() was returning false in
    slow start phase even if the application filled the socket write queue.
    
    All congestion modules take into account tcp_is_cwnd_limited()
    before increasing cwnd, so this behavior limits slow start from
    probing the bandwidth at full speed.
    
    The problem is that even if write queue is full (aka we are _not_
    application limited), cwnd can be under utilized if TSO should auto
    defer or TCP Small queues decided to hold packets.
    
    So the in_flight can be kept to smaller value, and we can get to the
    point tcp_is_cwnd_limited() returns false.
    
    With TCP Small Queues and FQ/pacing, this issue is more visible.
    
    We fix this by having tcp_cwnd_validate(), which is supposed to track
    such things, take into account unsent_segs, the number of segs that we
    are not sending at the moment due to TSO or TSQ, but intend to send
    real soon. Then when we are cwnd-limited, remember this fact while we
    are processing the window of ACKs that comes back.
    
    For example, suppose we have a brand new connection with cwnd=10; we
    are in slow start, and we send a flight of 9 packets. By the time we
    have received ACKs for all 9 packets we want our cwnd to be 18.
    We implement this by setting tp->lsnd_pending to 9, and
    considering ourselves to be cwnd-limited while cwnd is less than
    twice tp->lsnd_pending (2*9 -> 18).
    
    This makes tcp_is_cwnd_limited() more understandable, by removing
    the GSO/TSO kludge, that tried to work around the issue.
    
    Note the in_flight parameter can be removed in a followup cleanup
    patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 239946868142..4e37c71ecd74 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -230,6 +230,7 @@ struct tcp_sock {
 	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
+	u32	lsnd_pending;	/* packets inflight or unsent since last xmit */
 	u32	prior_cwnd;	/* Congestion window at start of Recovery. */
 	u32	prr_delivered;	/* Number of newly delivered packets to
 				 * receiver in Recovery. */

commit 740b0f1841f6e39085b711d41db9ffb07198682b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Feb 26 14:02:48 2014 -0800

    tcp: switch rtt estimations to usec resolution
    
    Upcoming congestion controls for TCP require usec resolution for RTT
    estimations. Millisecond resolution is simply not enough these days.
    
    FQ/pacing in DC environments also require this change for finer control
    and removal of bimodal behavior due to the current hack in
    tcp_update_pacing_rate() for 'small rtt'
    
    TCP_CONG_RTT_STAMP is no longer needed.
    
    As Julian Anastasov pointed out, we need to keep user compatibility :
    tcp_metrics used to export RTT and RTTVAR in msec resolution,
    so we added RTT_US and RTTVAR_US. An iproute2 patch is needed
    to use the new attributes if provided by the kernel.
    
    In this example ss command displays a srtt of 32 usecs (10Gbit link)
    
    lpk51:~# ./ss -i dst lpk52
    Netid  State      Recv-Q Send-Q   Local Address:Port       Peer
    Address:Port
    tcp    ESTAB      0      1         10.246.11.51:42959
    10.246.11.52:64614
             cubic wscale:6,6 rto:201 rtt:0.032/0.001 ato:40 mss:1448
    cwnd:10 send
    3620.0Mbps pacing_rate 7240.0Mbps unacked:1 rcv_rtt:993 rcv_space:29559
    
    Updated iproute2 ip command displays :
    
    lpk51:~# ./ip tcp_metrics | grep 10.246.11.52
    10.246.11.52 age 561.914sec cwnd 10 rtt 274us rttvar 213us source
    10.246.11.51
    
    Old binary displays :
    
    lpk51:~# ip tcp_metrics | grep 10.246.11.52
    10.246.11.52 age 561.914sec cwnd 10 rtt 250us rttvar 125us source
    10.246.11.51
    
    With help from Julian Anastasov, Stephen Hemminger and Yuchung Cheng
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Cc: Stephen Hemminger <stephen@networkplumber.org>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Larry Brakmo <brakmo@google.com>
    Cc: Julian Anastasov <ja@ssi.bg>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4ad0706d40eb..239946868142 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -201,10 +201,10 @@ struct tcp_sock {
 	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */
-	u32	srtt;		/* smoothed round trip time << 3	*/
-	u32	mdev;		/* medium deviation			*/
-	u32	mdev_max;	/* maximal mdev for the last rtt period	*/
-	u32	rttvar;		/* smoothed mdev_max			*/
+	u32	srtt_us;	/* smoothed round trip time << 3 in usecs */
+	u32	mdev_us;	/* medium deviation			*/
+	u32	mdev_max_us;	/* maximal mdev for the last rtt period	*/
+	u32	rttvar_us;	/* smoothed mdev_max			*/
 	u32	rtt_seq;	/* sequence number to update rttvar	*/
 
 	u32	packets_out;	/* Packets which are "in flight"	*/

commit 996b175e39ed42ec2aa0c63b4a03cc500aa6269f
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jan 6 09:36:12 2014 -0800

    tcp: out_of_order_queue do not use its lock
    
    TCP out_of_order_queue lock is not used, as queue manipulation
    happens with socket lock held and we therefore use the lockless
    skb queue routines (as __skb_queue_head())
    
    We can use __skb_queue_head_init() instead of skb_queue_head_init()
    to make this more consistent.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d68633452d9b..4ad0706d40eb 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -248,7 +248,10 @@ struct tcp_sock {
 	struct sk_buff* lost_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
 
-	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
+	/* OOO segments go in this list. Note that socket lock must be held,
+	 * as we do not use sk_buff_head lock.
+	 */
+	struct sk_buff_head	out_of_order_queue;
 
 	/* SACKs data, these 2 need to be together (see tcp_options_write) */
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */

commit c0155b2da4cd6583b1b729451249ca346e1c05a2
Author: Dmitry Popov <dp@highloadlab.com>
Date:   Wed Jul 31 13:39:45 2013 +0400

    tcp: Remove unused tcpct declarations and comments
    
    Remove declaration, 4 defines and confusing comment that are no longer used
    since 1a2c6181c4 ("tcp: Remove TCPCT").
    
    Signed-off-by: Dmitry Popov <dp@highloadlab.com>
    Acked-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9640803a17a7..d68633452d9b 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -107,7 +107,6 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
  * only four options will fit in a standard TCP header */
 #define TCP_NUM_SACKS 4
 
-struct tcp_cookie_values;
 struct tcp_request_sock_ops;
 
 struct tcp_request_sock {

commit c9bee3b7fdecb0c1d070c7b54113b3bdfb9a3d36
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 22 20:27:07 2013 -0700

    tcp: TCP_NOTSENT_LOWAT socket option
    
    Idea of this patch is to add optional limitation of number of
    unsent bytes in TCP sockets, to reduce usage of kernel memory.
    
    TCP receiver might announce a big window, and TCP sender autotuning
    might allow a large amount of bytes in write queue, but this has little
    performance impact if a large part of this buffering is wasted :
    
    Write queue needs to be large only to deal with large BDP, not
    necessarily to cope with scheduling delays (incoming ACKS make room
    for the application to queue more bytes)
    
    For most workloads, using a value of 128 KB or less is OK to give
    applications enough time to react to POLLOUT events in time
    (or being awaken in a blocking sendmsg())
    
    This patch adds two ways to set the limit :
    
    1) Per socket option TCP_NOTSENT_LOWAT
    
    2) A sysctl (/proc/sys/net/ipv4/tcp_notsent_lowat) for sockets
    not using TCP_NOTSENT_LOWAT socket option (or setting a zero value)
    Default value being UINT_MAX (0xFFFFFFFF), meaning this has no effect.
    
    This changes poll()/select()/epoll() to report POLLOUT
    only if number of unsent bytes is below tp->nosent_lowat
    
    Note this might increase number of sendmsg()/sendfile() calls
    when using non blocking sockets,
    and increase number of context switches for blocking sockets.
    
    Note this is not related to SO_SNDLOWAT (as SO_SNDLOWAT is
    defined as :
     Specify the minimum number of bytes in the buffer until
     the socket layer will pass the data to the protocol)
    
    Tested:
    
    netperf sessions, and watching /proc/net/protocols "memory" column for TCP
    
    With 200 concurrent netperf -t TCP_STREAM sessions, amount of kernel memory
    used by TCP buffers shrinks by ~55 % (20567 pages instead of 45458)
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   45458   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   45458   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   20567   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   20567   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    Using 128KB has no bad effect on the throughput or cpu usage
    of a single flow, although there is an increase of context switches.
    
    A bonus is that we hold socket lock for a shorter amount
    of time and should improve latencies of ACK processing.
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1651584     6291456     16384  20.00   17447.90   10^6bits/s  3.13  S      -1.00  U      0.353   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
               412,514 context-switches
    
         200.034645535 seconds time elapsed
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1593240     6291456     16384  20.00   17321.16   10^6bits/s  3.35  S      -1.00  U      0.381   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
             2,675,818 context-switches
    
         200.029651391 seconds time elapsed
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-By: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 472120b4fac5..9640803a17a7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -238,6 +238,7 @@ struct tcp_sock {
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
+	u32	notsent_lowat;	/* TCP_NOTSENT_LOWAT */
 	u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
 	u32	lost_out;	/* Lost packets			*/
 	u32	sacked_out;	/* SACK'd packets			*/

commit 3e59cb0ddfd2c59991f38e89352ad8a3c71b2374
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri May 17 13:45:05 2013 +0000

    tcp: remove bad timeout logic in fast recovery
    
    tcp_timeout_skb() was intended to trigger fast recovery on timeout,
    unfortunately in reality it often causes spurious retransmission
    storms during fast recovery. The particular sign is a fast retransmit
    over the highest sacked sequence (SND.FACK).
    
    Currently the RTO timer re-arming (as in RFC6298) offers a nice cushion
    to avoid spurious timeout: when SND.UNA advances the sender re-arms
    RTO and extends the timeout by icsk_rto. The sender does not offset
    the time elapsed since the packet at SND.UNA was sent.
    
    But if the next (DUP)ACK arrives later than ~RTTVAR and triggers
    tcp_fastretrans_alert(), then tcp_timeout_skb() will mark any packet
    sent before the icsk_rto interval lost, including one that's above the
    highest sacked sequence. Most likely a large part of scorebard will be
    marked.
    
    If most packets are not lost then the subsequent DUPACKs with new SACK
    blocks will cause the sender to continue to retransmit packets beyond
    SND.FACK spuriously. Even if only one packet is lost the sender may
    falsely retransmit almost the entire window.
    
    The situation becomes common in the world of bufferbloat: the RTT
    continues to grow as the queue builds up but RTTVAR remains small and
    close to the minimum 200ms. If a data packet is lost and the DUPACK
    triggered by the next data packet is slightly delayed, then a spurious
    retransmission storm forms.
    
    As the original comment on tcp_timeout_skb() suggests: the usefulness
    of this feature is questionable. It also wastes cycles walking the
    sack scoreboard and is actually harmful because of false recovery.
    
    It's time to remove this.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 5adbc33d1ab3..472120b4fac5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -246,7 +246,6 @@ struct tcp_sock {
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;
-	struct sk_buff *scoreboard_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
 
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */

commit e33099f96d99c391b3325caa9c44258de04aae86
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Mar 20 13:33:00 2013 +0000

    tcp: implement RFC5682 F-RTO
    
    This patch implements F-RTO (foward RTO recovery):
    
    When the first retransmission after timeout is acknowledged, F-RTO
    sends new data instead of old data. If the next ACK acknowledges
    some never-retransmitted data, then the timeout was spurious and the
    congestion state is reverted.  Otherwise if the next ACK selectively
    acknowledges the new data, then the timeout was genuine and the
    loss recovery continues. This idea applies to recurring timeouts
    as well. While F-RTO sends different data during timeout recovery,
    it does not (and should not) change the congestion control.
    
    The implementaion follows the three steps of SACK enhanced algorithm
    (section 3) in RFC5682. Step 1 is in tcp_enter_loss(). Step 2 and
    3 are in tcp_process_loss().  The basic version is not supported
    because SACK enhanced version also works for non-SACK connections.
    
    The new implementation is functionally in parity with the old F-RTO
    implementation except the one case where it increases undo events:
    In addition to the RFC algorithm, a spurious timeout may be detected
    without sending data in step 2, as long as the SACK confirms not
    all the original data are dropped. When this happens, the sender
    will undo the cwnd and perhaps enter fast recovery instead. This
    additional check increases the F-RTO undo events by 5x compared
    to the prior implementation on Google Web servers, since the sender
    often does not have new data to send for HTTP.
    
    Note F-RTO may detect spurious timeout before Eifel with timestamps
    does so.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f5f203b36379..5adbc33d1ab3 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -192,7 +192,8 @@ struct tcp_sock {
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		thin_dupack : 1,/* Fast retransmit on first dupack      */
-		repair      : 1;
+		repair      : 1,
+		frto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 	u8	repair_queue;
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		syn_data:1,	/* SYN includes data */

commit 9b44190dc114c1720b34975b5bfc65aece112ced
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Mar 20 13:32:58 2013 +0000

    tcp: refactor F-RTO
    
    The patch series refactor the F-RTO feature (RFC4138/5682).
    
    This is to simplify the loss recovery processing. Existing F-RTO
    was developed during the experimental stage (RFC4138) and has
    many experimental features.  It takes a separate code path from
    the traditional timeout processing by overloading CA_Disorder
    instead of using CA_Loss state. This complicates CA_Disorder state
    handling because it's also used for handling dubious ACKs and undos.
    While the algorithm in the RFC does not change the congestion control,
    the implementation intercepts congestion control in various places
    (e.g., frto_cwnd in tcp_ack()).
    
    The new code implements newer F-RTO RFC5682 using CA_Loss processing
    path.  F-RTO becomes a small extension in the timeout processing
    and interfaces with congestion control and Eifel undo modules.
    It lets congestion control (module) determines how many to send
    independently.  F-RTO only chooses what to send in order to detect
    spurious retranmission. If timeout is found spurious it invokes
    existing Eifel undo algorithms like DSACK or TCP timestamp based
    detection.
    
    The first patch removes all F-RTO code except the sysctl_tcp_frto is
    left for the new implementation.  Since CA_EVENT_FRTO is removed, TCP
    westwood now computes ssthresh on regular timeout CA_EVENT_LOSS event.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ed6a7456eecd..f5f203b36379 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -187,14 +187,12 @@ struct tcp_sock {
 	u32	window_clamp;	/* Maximal window to advertise		*/
 	u32	rcv_ssthresh;	/* Current window clamp			*/
 
-	u32	frto_highmark;	/* snd_nxt when RTO occurred */
 	u16	advmss;		/* Advertised MSS			*/
-	u8	frto_counter;	/* Number of new acks after RTO */
+	u8	unused;
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		thin_dupack : 1,/* Fast retransmit on first dupack      */
-		repair      : 1,
-		unused      : 1;
+		repair      : 1;
 	u8	repair_queue;
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		syn_data:1,	/* SYN includes data */

commit 1a2c6181c4a1922021b4d7df373bba612c3e5f04
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Sun Mar 17 08:23:34 2013 +0000

    tcp: Remove TCPCT
    
    TCPCT uses option-number 253, reserved for experimental use and should
    not be used in production environments.
    Further, TCPCT does not fully implement RFC 6013.
    
    As a nice side-effect, removing TCPCT increases TCP's performance for
    very short flows:
    
    Doing an apache-benchmark with -c 100 -n 100000, sending HTTP-requests
    for files of 1KB size.
    
    before this patch:
            average (among 7 runs) of 20845.5 Requests/Second
    after:
            average (among 7 runs) of 21403.6 Requests/Second
    
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 763c108ee03d..ed6a7456eecd 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -90,9 +90,6 @@ struct tcp_options_received {
 		sack_ok : 4,	/* SACK seen on SYN packet		*/
 		snd_wscale : 4,	/* Window scaling received from sender	*/
 		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
-	u8	cookie_plus:6,	/* bytes in authenticator/cookie option	*/
-		cookie_out_never:1,
-		cookie_in_always:1;
 	u8	num_sacks;	/* Number of SACK blocks		*/
 	u16	user_mss;	/* mss requested by user in ioctl	*/
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
@@ -102,7 +99,6 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 {
 	rx_opt->tstamp_ok = rx_opt->sack_ok = 0;
 	rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
-	rx_opt->cookie_plus = 0;
 }
 
 /* This is the max number of SACKS that we'll generate and process. It's safe
@@ -320,12 +316,6 @@ struct tcp_sock {
 	struct tcp_md5sig_info	__rcu *md5sig_info;
 #endif
 
-	/* When the cookie options are generated and exchanged, then this
-	 * object holds a reference to them (cookie_values->kref).  Also
-	 * contains related tcp_cookie_transactions fields.
-	 */
-	struct tcp_cookie_values  *cookie_values;
-
 /* TCP fastopen related information */
 	struct tcp_fastopen_request *fastopen_req;
 	/* fastopen_rsk points to request_sock that resulted in this big

commit 9b717a8d245075ffb8e95a2dfb4ee97ce4747457
Author: Nandita Dukkipati <nanditad@google.com>
Date:   Mon Mar 11 10:00:44 2013 +0000

    tcp: TLP loss detection.
    
    This is the second of the TLP patch series; it augments the basic TLP
    algorithm with a loss detection scheme.
    
    This patch implements a mechanism for loss detection when a Tail
    loss probe retransmission plugs a hole thereby masking packet loss
    from the sender. The loss detection algorithm relies on counting
    TLP dupacks as outlined in Sec. 3 of:
    http://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01
    
    The basic idea is: Sender keeps track of TLP "episode" upon
    retransmission of a TLP packet. An episode ends when the sender receives
    an ACK above the SND.NXT (tracked by tlp_high_seq) at the time of the
    episode. We want to make sure that before the episode ends the sender
    receives a "TLP dupack", indicating that the TLP retransmission was
    unnecessary, so there was no loss/hole that needed plugging. If the
    sender gets no TLP dupack before the end of the episode, then it reduces
    ssthresh and the congestion window, because the TLP packet arriving at
    the receiver probably plugged a hole.
    
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 01860d74555c..763c108ee03d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -204,6 +204,7 @@ struct tcp_sock {
 		syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
 		syn_data_acked:1;/* data in SYN is acked by SYN-ACK */
+	u32	tlp_high_seq;	/* snd_nxt at the time of TLP retransmit. */
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit 6ba8a3b19e764b6a65e4030ab0999be50c291e6c
Author: Nandita Dukkipati <nanditad@google.com>
Date:   Mon Mar 11 10:00:43 2013 +0000

    tcp: Tail loss probe (TLP)
    
    This patch series implement the Tail loss probe (TLP) algorithm described
    in http://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01. The
    first patch implements the basic algorithm.
    
    TLP's goal is to reduce tail latency of short transactions. It achieves
    this by converting retransmission timeouts (RTOs) occuring due
    to tail losses (losses at end of transactions) into fast recovery.
    TLP transmits one packet in two round-trips when a connection is in
    Open state and isn't receiving any ACKs. The transmitted packet, aka
    loss probe, can be either new or a retransmission. When there is tail
    loss, the ACK from a loss probe triggers FACK/early-retransmit based
    fast recovery, thus avoiding a costly RTO. In the absence of loss,
    there is no change in the connection state.
    
    PTO stands for probe timeout. It is a timer event indicating
    that an ACK is overdue and triggers a loss probe packet. The PTO value
    is set to max(2*SRTT, 10ms) and is adjusted to account for delayed
    ACK timer when there is only one oustanding packet.
    
    TLP Algorithm
    
    On transmission of new data in Open state:
      -> packets_out > 1: schedule PTO in max(2*SRTT, 10ms).
      -> packets_out == 1: schedule PTO in max(2*RTT, 1.5*RTT + 200ms)
      -> PTO = min(PTO, RTO)
    
    Conditions for scheduling PTO:
      -> Connection is in Open state.
      -> Connection is either cwnd limited or no new data to send.
      -> Number of probes per tail loss episode is limited to one.
      -> Connection is SACK enabled.
    
    When PTO fires:
      new_segment_exists:
        -> transmit new segment.
        -> packets_out++. cwnd remains same.
    
      no_new_packet:
        -> retransmit the last segment.
           Its ACK triggers FACK or early retransmit based recovery.
    
    ACK path:
      -> rearm RTO at start of ACK processing.
      -> reschedule PTO if need be.
    
    In addition, the patch includes a small variation to the Early Retransmit
    (ER) algorithm, such that ER and TLP together can in principle recover any
    N-degree of tail loss through fast recovery. TLP is controlled by the same
    sysctl as ER, tcp_early_retrans sysctl.
    tcp_early_retrans==0; disables TLP and ER.
                     ==1; enables RFC5827 ER.
                     ==2; delayed ER.
                     ==3; TLP and delayed ER. [DEFAULT]
                     ==4; TLP only.
    
    The TLP patch series have been extensively tested on Google Web servers.
    It is most effective for short Web trasactions, where it reduced RTOs by 15%
    and improved HTTP response time (average by 6%, 99th percentile by 10%).
    The transmitted probes account for <0.5% of the overall transmissions.
    
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 515c3746b675..01860d74555c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -201,7 +201,6 @@ struct tcp_sock {
 		unused      : 1;
 	u8	repair_queue;
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
-		early_retrans_delayed:1, /* Delayed ER timer installed */
 		syn_data:1,	/* SYN includes data */
 		syn_fastopen:1,	/* SYN includes Fast Open option */
 		syn_data_acked:1;/* data in SYN is acked by SYN-ACK */

commit e61667af2f77d481411f2ccd307fed2247d785a8
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Sun Mar 10 05:18:39 2013 +0000

    tcp: Remove unused tw_cookie_values from tcp_timewait_sock
    
    tw_cookie_values is never used in the TCP-stack.
    
    It was added by 435cf559f (TCPCT part 1d: define TCP cookie option,
    extend existing struct's), but already at that time it was not used at
    all, nor mentioned in the commit-message.
    
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f28408c07dc2..515c3746b675 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -361,10 +361,6 @@ struct tcp_timewait_sock {
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	  *tw_md5_key;
 #endif
-	/* Few sockets in timewait have cookies; in that case, then this
-	 * object holds a reference to them (tw_cookie_values->kref).
-	 */
-	struct tcp_cookie_values  *tw_cookie_values;
 };
 
 static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)

commit ceaa1fef65a7c2e017b260b879b310dd24888083
Author: Andrey Vagin <avagin@openvz.org>
Date:   Mon Feb 11 05:50:17 2013 +0000

    tcp: adding a per-socket timestamp offset
    
    This functionality is used for restoring tcp sockets. A tcp timestamp
    depends on how long a system has been running, so it's differ for each
    host. The solution is to set a per-socket offset.
    
    A per-socket offset for a TIME_WAIT socket is inherited from a proper
    tcp socket.
    
    tcp_request_sock doesn't have a timestamp offset, because the repair
    mode for them are not implemented.
    
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Cc: James Morris <jmorris@namei.org>
    Cc: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Andrey Vagin <avagin@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 6d0d46138ae8..f28408c07dc2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -162,6 +162,8 @@ struct tcp_sock {
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
 
+	u32	tsoffset;	/* timestamp offset */
+
 	struct list_head tsq_node; /* anchor in tsq_tasklet.head list */
 	unsigned long	tsq_flags;
 
@@ -353,6 +355,7 @@ struct tcp_timewait_sock {
 	u32			  tw_rcv_nxt;
 	u32			  tw_snd_nxt;
 	u32			  tw_rcv_wnd;
+	u32			  tw_ts_offset;
 	u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
 #ifdef CONFIG_TCP_MD5SIG

commit ca2eb5679f8ddffff60156af42595df44a315ef0
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Tue Feb 5 07:25:17 2013 +0000

    tcp: remove Appropriate Byte Count support
    
    TCP Appropriate Byte Count was added by me, but later disabled.
    There is no point in maintaining it since it is a potential source
    of bugs and Linux already implements other better window protection
    heuristics.
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4e1d2283e3cc..6d0d46138ae8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -246,7 +246,6 @@ struct tcp_sock {
 	u32	sacked_out;	/* SACK'd packets			*/
 	u32	fackets_out;	/* FACK'd packets			*/
 	u32	tso_deferred;
-	u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit 6a674e9c75b17e7a88ff15b3c2e269eed54f7cfb
Author: Joseph Gasparakis <joseph.gasparakis@intel.com>
Date:   Fri Dec 7 14:14:14 2012 +0000

    net: Add support for hardware-offloaded encapsulation
    
    This patch adds support in the kernel for offloading in the NIC Tx and Rx
    checksumming for encapsulated packets (such as VXLAN and IP GRE).
    
    For Tx encapsulation offload, the driver will need to set the right bits
    in netdev->hw_enc_features. The protocol driver will have to set the
    skb->encapsulation bit and populate the inner headers, so the NIC driver will
    use those inner headers to calculate the csum in hardware.
    
    For Rx encapsulation offload, the driver will need to set again the
    skb->encapsulation flag and the skb->ip_csum to CHECKSUM_UNNECESSARY.
    In that case the protocol driver should push the decapsulated packet up
    to the stack, again with CHECKSUM_UNNECESSARY. In ether case, the protocol
    driver should set the skb->encapsulation flag back to zero. Finally the
    protocol driver should have NETIF_F_RXCSUM flag set in its features.
    
    Signed-off-by: Joseph Gasparakis <joseph.gasparakis@intel.com>
    Signed-off-by: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 60b7aac15e0e..4e1d2283e3cc 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -35,6 +35,16 @@ static inline unsigned int tcp_hdrlen(const struct sk_buff *skb)
 	return tcp_hdr(skb)->doff * 4;
 }
 
+static inline struct tcphdr *inner_tcp_hdr(const struct sk_buff *skb)
+{
+	return (struct tcphdr *)skb_inner_transport_header(skb);
+}
+
+static inline unsigned int inner_tcp_hdrlen(const struct sk_buff *skb)
+{
+	return inner_tcp_hdr(skb)->doff * 4;
+}
+
 static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 {
 	return (tcp_hdr(skb)->doff - 5) * 4;

commit 6f73601efb35c7003f5c58c2bc6fd08f3652169c
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 19 15:14:44 2012 +0000

    tcp: add SYN/data info to TCP_INFO
    
    Add a bit TCPI_OPT_SYN_DATA (32) to the socket option TCP_INFO:tcpi_options.
    It's set if the data in SYN (sent or received) is acked by SYN-ACK. Server or
    client application can use this information to check Fast Open success rate.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8a7fc4be2d75..60b7aac15e0e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -191,7 +191,8 @@ struct tcp_sock {
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		early_retrans_delayed:1, /* Delayed ER timer installed */
 		syn_data:1,	/* SYN includes data */
-		syn_fastopen:1;	/* SYN includes Fast Open option */
+		syn_fastopen:1,	/* SYN includes Fast Open option */
+		syn_data_acked:1;/* data in SYN is acked by SYN-ACK */
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 67c789ae719c..8a7fc4be2d75 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -17,217 +17,13 @@
 #ifndef _LINUX_TCP_H
 #define _LINUX_TCP_H
 
-#include <linux/types.h>
-#include <asm/byteorder.h>
-#include <linux/socket.h>
-
-struct tcphdr {
-	__be16	source;
-	__be16	dest;
-	__be32	seq;
-	__be32	ack_seq;
-#if defined(__LITTLE_ENDIAN_BITFIELD)
-	__u16	res1:4,
-		doff:4,
-		fin:1,
-		syn:1,
-		rst:1,
-		psh:1,
-		ack:1,
-		urg:1,
-		ece:1,
-		cwr:1;
-#elif defined(__BIG_ENDIAN_BITFIELD)
-	__u16	doff:4,
-		res1:4,
-		cwr:1,
-		ece:1,
-		urg:1,
-		ack:1,
-		psh:1,
-		rst:1,
-		syn:1,
-		fin:1;
-#else
-#error	"Adjust your <asm/byteorder.h> defines"
-#endif	
-	__be16	window;
-	__sum16	check;
-	__be16	urg_ptr;
-};
-
-/*
- *	The union cast uses a gcc extension to avoid aliasing problems
- *  (union is compatible to any of its members)
- *  This means this part of the code is -fstrict-aliasing safe now.
- */
-union tcp_word_hdr { 
-	struct tcphdr hdr;
-	__be32 		  words[5];
-}; 
-
-#define tcp_flag_word(tp) ( ((union tcp_word_hdr *)(tp))->words [3]) 
-
-enum { 
-	TCP_FLAG_CWR = __constant_cpu_to_be32(0x00800000),
-	TCP_FLAG_ECE = __constant_cpu_to_be32(0x00400000),
-	TCP_FLAG_URG = __constant_cpu_to_be32(0x00200000),
-	TCP_FLAG_ACK = __constant_cpu_to_be32(0x00100000),
-	TCP_FLAG_PSH = __constant_cpu_to_be32(0x00080000),
-	TCP_FLAG_RST = __constant_cpu_to_be32(0x00040000),
-	TCP_FLAG_SYN = __constant_cpu_to_be32(0x00020000),
-	TCP_FLAG_FIN = __constant_cpu_to_be32(0x00010000),
-	TCP_RESERVED_BITS = __constant_cpu_to_be32(0x0F000000),
-	TCP_DATA_OFFSET = __constant_cpu_to_be32(0xF0000000)
-}; 
-
-/*
- * TCP general constants
- */
-#define TCP_MSS_DEFAULT		 536U	/* IPv4 (RFC1122, RFC2581) */
-#define TCP_MSS_DESIRED		1220U	/* IPv6 (tunneled), EDNS0 (RFC3226) */
-
-/* TCP socket options */
-#define TCP_NODELAY		1	/* Turn off Nagle's algorithm. */
-#define TCP_MAXSEG		2	/* Limit MSS */
-#define TCP_CORK		3	/* Never send partially complete segments */
-#define TCP_KEEPIDLE		4	/* Start keeplives after this period */
-#define TCP_KEEPINTVL		5	/* Interval between keepalives */
-#define TCP_KEEPCNT		6	/* Number of keepalives before death */
-#define TCP_SYNCNT		7	/* Number of SYN retransmits */
-#define TCP_LINGER2		8	/* Life time of orphaned FIN-WAIT-2 state */
-#define TCP_DEFER_ACCEPT	9	/* Wake up listener only when data arrive */
-#define TCP_WINDOW_CLAMP	10	/* Bound advertised window */
-#define TCP_INFO		11	/* Information about this connection. */
-#define TCP_QUICKACK		12	/* Block/reenable quick acks */
-#define TCP_CONGESTION		13	/* Congestion control algorithm */
-#define TCP_MD5SIG		14	/* TCP MD5 Signature (RFC2385) */
-#define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
-#define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
-#define TCP_THIN_DUPACK         17      /* Fast retrans. after 1 dupack */
-#define TCP_USER_TIMEOUT	18	/* How long for loss retry before timeout */
-#define TCP_REPAIR		19	/* TCP sock is under repair right now */
-#define TCP_REPAIR_QUEUE	20
-#define TCP_QUEUE_SEQ		21
-#define TCP_REPAIR_OPTIONS	22
-#define TCP_FASTOPEN		23	/* Enable FastOpen on listeners */
-
-struct tcp_repair_opt {
-	__u32	opt_code;
-	__u32	opt_val;
-};
-
-enum {
-	TCP_NO_QUEUE,
-	TCP_RECV_QUEUE,
-	TCP_SEND_QUEUE,
-	TCP_QUEUES_NR,
-};
-
-/* for TCP_INFO socket option */
-#define TCPI_OPT_TIMESTAMPS	1
-#define TCPI_OPT_SACK		2
-#define TCPI_OPT_WSCALE		4
-#define TCPI_OPT_ECN		8 /* ECN was negociated at TCP session init */
-#define TCPI_OPT_ECN_SEEN	16 /* we received at least one packet with ECT */
-
-enum tcp_ca_state {
-	TCP_CA_Open = 0,
-#define TCPF_CA_Open	(1<<TCP_CA_Open)
-	TCP_CA_Disorder = 1,
-#define TCPF_CA_Disorder (1<<TCP_CA_Disorder)
-	TCP_CA_CWR = 2,
-#define TCPF_CA_CWR	(1<<TCP_CA_CWR)
-	TCP_CA_Recovery = 3,
-#define TCPF_CA_Recovery (1<<TCP_CA_Recovery)
-	TCP_CA_Loss = 4
-#define TCPF_CA_Loss	(1<<TCP_CA_Loss)
-};
-
-struct tcp_info {
-	__u8	tcpi_state;
-	__u8	tcpi_ca_state;
-	__u8	tcpi_retransmits;
-	__u8	tcpi_probes;
-	__u8	tcpi_backoff;
-	__u8	tcpi_options;
-	__u8	tcpi_snd_wscale : 4, tcpi_rcv_wscale : 4;
-
-	__u32	tcpi_rto;
-	__u32	tcpi_ato;
-	__u32	tcpi_snd_mss;
-	__u32	tcpi_rcv_mss;
-
-	__u32	tcpi_unacked;
-	__u32	tcpi_sacked;
-	__u32	tcpi_lost;
-	__u32	tcpi_retrans;
-	__u32	tcpi_fackets;
-
-	/* Times. */
-	__u32	tcpi_last_data_sent;
-	__u32	tcpi_last_ack_sent;     /* Not remembered, sorry. */
-	__u32	tcpi_last_data_recv;
-	__u32	tcpi_last_ack_recv;
-
-	/* Metrics. */
-	__u32	tcpi_pmtu;
-	__u32	tcpi_rcv_ssthresh;
-	__u32	tcpi_rtt;
-	__u32	tcpi_rttvar;
-	__u32	tcpi_snd_ssthresh;
-	__u32	tcpi_snd_cwnd;
-	__u32	tcpi_advmss;
-	__u32	tcpi_reordering;
-
-	__u32	tcpi_rcv_rtt;
-	__u32	tcpi_rcv_space;
-
-	__u32	tcpi_total_retrans;
-};
-
-/* for TCP_MD5SIG socket option */
-#define TCP_MD5SIG_MAXKEYLEN	80
-
-struct tcp_md5sig {
-	struct __kernel_sockaddr_storage tcpm_addr;	/* address associated */
-	__u16	__tcpm_pad1;				/* zero */
-	__u16	tcpm_keylen;				/* key length */
-	__u32	__tcpm_pad2;				/* zero */
-	__u8	tcpm_key[TCP_MD5SIG_MAXKEYLEN];		/* key (binary) */
-};
-
-/* for TCP_COOKIE_TRANSACTIONS (TCPCT) socket option */
-#define TCP_COOKIE_MIN		 8		/*  64-bits */
-#define TCP_COOKIE_MAX		16		/* 128-bits */
-#define TCP_COOKIE_PAIR_SIZE	(2*TCP_COOKIE_MAX)
-
-/* Flags for both getsockopt and setsockopt */
-#define TCP_COOKIE_IN_ALWAYS	(1 << 0)	/* Discard SYN without cookie */
-#define TCP_COOKIE_OUT_NEVER	(1 << 1)	/* Prohibit outgoing cookies,
-						 * supercedes everything. */
-
-/* Flags for getsockopt */
-#define TCP_S_DATA_IN		(1 << 2)	/* Was data received? */
-#define TCP_S_DATA_OUT		(1 << 3)	/* Was data sent? */
-
-/* TCP_COOKIE_TRANSACTIONS data */
-struct tcp_cookie_transactions {
-	__u16	tcpct_flags;			/* see above */
-	__u8	__tcpct_pad1;			/* zero */
-	__u8	tcpct_cookie_desired;		/* bytes */
-	__u16	tcpct_s_data_desired;		/* bytes of variable data */
-	__u16	tcpct_used;			/* bytes in value */
-	__u8	tcpct_value[TCP_MSS_DEFAULT];
-};
-
-#ifdef __KERNEL__
 
 #include <linux/skbuff.h>
 #include <linux/dmaengine.h>
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
+#include <uapi/linux/tcp.h>
 
 static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
 {
@@ -595,6 +391,4 @@ static inline int fastopen_init_queue(struct sock *sk, int backlog)
 	return 0;
 }
 
-#endif	/* __KERNEL__ */
-
 #endif	/* _LINUX_TCP_H */

commit bb68b64724a4fd6b93d83b39aeffa4aadb2562fc
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Tue Sep 18 14:19:23 2012 +0000

    ipv4: Don't add TCP-code in inet_sock_destruct
    
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Acked-by: H.K. Jerry Chu <hkchu@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ae46df590629..67c789ae719c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -574,6 +574,8 @@ static inline bool fastopen_cookie_present(struct tcp_fastopen_cookie *foc)
 	return foc->len != -1;
 }
 
+extern void tcp_sock_destruct(struct sock *sk);
+
 static inline int fastopen_init_queue(struct sock *sk, int backlog)
 {
 	struct request_sock_queue *queue =
@@ -585,6 +587,8 @@ static inline int fastopen_init_queue(struct sock *sk, int backlog)
 		    sk->sk_allocation);
 		if (queue->fastopenq == NULL)
 			return -ENOMEM;
+
+		sk->sk_destruct = tcp_sock_destruct;
 		spin_lock_init(&queue->fastopenq->lock);
 	}
 	queue->fastopenq->max_qlen = backlog;

commit 1046716368979dee857a2b8a91c4a8833f21b9cb
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 31 12:29:11 2012 +0000

    tcp: TCP Fast Open Server - header & support functions
    
    This patch adds all the necessary data structure and support
    functions to implement TFO server side. It also documents a number
    of flags for the sysctl_tcp_fastopen knob, and adds a few Linux
    extension MIBs.
    
    In addition, it includes the following:
    
    1. a new TCP_FASTOPEN socket option an application must call to
    supply a max backlog allowed in order to enable TFO on its listener.
    
    2. A number of key data structures:
    "fastopen_rsk" in tcp_sock - for a big socket to access its
    request_sock for retransmission and ack processing purpose. It is
    non-NULL iff 3WHS not completed.
    
    "fastopenq" in request_sock_queue - points to a per Fast Open
    listener data structure "fastopen_queue" to keep track of qlen (# of
    outstanding Fast Open requests) and max_qlen, among other things.
    
    "listener" in tcp_request_sock - to point to the original listener
    for book-keeping purpose, i.e., to maintain qlen against max_qlen
    as part of defense against IP spoofing attack.
    
    3. various data structure and functions, many in tcp_fastopen.c, to
    support server side Fast Open cookie operations, including
    /proc/sys/net/ipv4/tcp_fastopen_key to allow manual rekeying.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index eb125a4c30b3..ae46df590629 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -110,6 +110,7 @@ enum {
 #define TCP_REPAIR_QUEUE	20
 #define TCP_QUEUE_SEQ		21
 #define TCP_REPAIR_OPTIONS	22
+#define TCP_FASTOPEN		23	/* Enable FastOpen on listeners */
 
 struct tcp_repair_opt {
 	__u32	opt_code;
@@ -246,6 +247,7 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 /* TCP Fast Open */
 #define TCP_FASTOPEN_COOKIE_MIN	4	/* Min Fast Open Cookie size in bytes */
 #define TCP_FASTOPEN_COOKIE_MAX	16	/* Max Fast Open Cookie size in bytes */
+#define TCP_FASTOPEN_COOKIE_SIZE 8	/* the size employed by this impl. */
 
 /* TCP Fast Open Cookie as stored in memory */
 struct tcp_fastopen_cookie {
@@ -312,9 +314,14 @@ struct tcp_request_sock {
 	/* Only used by TCP MD5 Signature so far. */
 	const struct tcp_request_sock_ops *af_specific;
 #endif
+	struct sock			*listener; /* needed for TFO */
 	u32				rcv_isn;
 	u32				snt_isn;
 	u32				snt_synack; /* synack sent time */
+	u32				rcv_nxt; /* the ack # by SYNACK. For
+						  * FastOpen it's the seq#
+						  * after data-in-SYN.
+						  */
 };
 
 static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
@@ -505,14 +512,18 @@ struct tcp_sock {
 	struct tcp_md5sig_info	__rcu *md5sig_info;
 #endif
 
-/* TCP fastopen related information */
-	struct tcp_fastopen_request *fastopen_req;
-
 	/* When the cookie options are generated and exchanged, then this
 	 * object holds a reference to them (cookie_values->kref).  Also
 	 * contains related tcp_cookie_transactions fields.
 	 */
 	struct tcp_cookie_values  *cookie_values;
+
+/* TCP fastopen related information */
+	struct tcp_fastopen_request *fastopen_req;
+	/* fastopen_rsk points to request_sock that resulted in this big
+	 * socket. Used to retransmit SYNACKs etc.
+	 */
+	struct request_sock *fastopen_rsk;
 };
 
 enum tsq_flags {
@@ -552,6 +563,34 @@ static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)
 	return (struct tcp_timewait_sock *)sk;
 }
 
+static inline bool tcp_passive_fastopen(const struct sock *sk)
+{
+	return (sk->sk_state == TCP_SYN_RECV &&
+		tcp_sk(sk)->fastopen_rsk != NULL);
+}
+
+static inline bool fastopen_cookie_present(struct tcp_fastopen_cookie *foc)
+{
+	return foc->len != -1;
+}
+
+static inline int fastopen_init_queue(struct sock *sk, int backlog)
+{
+	struct request_sock_queue *queue =
+	    &inet_csk(sk)->icsk_accept_queue;
+
+	if (queue->fastopenq == NULL) {
+		queue->fastopenq = kzalloc(
+		    sizeof(struct fastopen_queue),
+		    sk->sk_allocation);
+		if (queue->fastopenq == NULL)
+			return -ENOMEM;
+		spin_lock_init(&queue->fastopenq->lock);
+	}
+	queue->fastopenq->max_qlen = backlog;
+	return 0;
+}
+
 #endif	/* __KERNEL__ */
 
 #endif	/* _LINUX_TCP_H */

commit 563d34d05786263893ba4a1042eb9b9374127cf5
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 23 09:48:52 2012 +0200

    tcp: dont drop MTU reduction indications
    
    ICMP messages generated in output path if frame length is bigger than
    mtu are actually lost because socket is owned by user (doing the xmit)
    
    One example is the ipgre_tunnel_xmit() calling
    icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
    
    We had a similar case fixed in commit a34a101e1e6 (ipv6: disable GSO on
    sockets hitting dst_allfrag).
    
    Problem of such fix is that it relied on retransmit timers, so short tcp
    sessions paid a too big latency increase price.
    
    This patch uses the tcp_release_cb() infrastructure so that MTU
    reduction messages (ICMP messages) are not lost, and no extra delay
    is added in TCP transmits.
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Diagnosed-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Tore Anderson <tore@fud.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2761856987b2..eb125a4c30b3 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -493,6 +493,9 @@ struct tcp_sock {
 		u32		  probe_seq_start;
 		u32		  probe_seq_end;
 	} mtu_probe;
+	u32	mtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG
+			   * while socket was owned by user.
+			   */
 
 #ifdef CONFIG_TCP_MD5SIG
 /* TCP AF-Specific parts; only used by MD5 Signature support so far */
@@ -518,6 +521,9 @@ enum tsq_flags {
 	TCP_TSQ_DEFERRED,	   /* tcp_tasklet_func() found socket was owned */
 	TCP_WRITE_TIMER_DEFERRED,  /* tcp_write_timer() found socket was owned */
 	TCP_DELACK_TIMER_DEFERRED, /* tcp_delack_timer() found socket was owned */
+	TCP_MTU_REDUCED_DEFERRED,  /* tcp_v{4|6}_err() could not call
+				    * tcp_v{4|6}_mtu_reduced()
+				    */
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)

commit 6f458dfb409272082c9bfa412f77ff2fc21c626f
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jul 20 05:45:50 2012 +0000

    tcp: improve latencies of timer triggered events
    
    Modern TCP stack highly depends on tcp_write_timer() having a small
    latency, but current implementation doesn't exactly meet the
    expectations.
    
    When a timer fires but finds the socket is owned by the user, it rearms
    itself for an additional delay hoping next run will be more
    successful.
    
    tcp_write_timer() for example uses a 50ms delay for next try, and it
    defeats many attempts to get predictable TCP behavior in term of
    latencies.
    
    Use the recently introduced tcp_release_cb(), so that the user owning
    the socket will call various handlers right before socket release.
    
    This will permit us to post a followup patch to address the
    tcp_tso_should_defer() syndrome (some deferred packets have to wait
    RTO timer to be transmitted, while cwnd should allow us to send them
    sooner)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: H.K. Jerry Chu <hkchu@google.com>
    Cc: John Heffner <johnwheffner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9febfb685c33..2761856987b2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -515,7 +515,9 @@ struct tcp_sock {
 enum tsq_flags {
 	TSQ_THROTTLED,
 	TSQ_QUEUED,
-	TSQ_OWNED, /* tcp_tasklet_func() found socket was locked */
+	TCP_TSQ_DEFERRED,	   /* tcp_tasklet_func() found socket was owned */
+	TCP_WRITE_TIMER_DEFERRED,  /* tcp_write_timer() found socket was owned */
+	TCP_DELACK_TIMER_DEFERRED, /* tcp_delack_timer() found socket was owned */
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)

commit 67da22d23fa6f3324e03bcd0580b914b2e4afbf3
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:11 2012 +0000

    net-tcp: Fast Open client - cookie-less mode
    
    In trusted networks, e.g., intranet, data-center, the client does not
    need to use Fast Open cookie to mitigate DoS attacks. In cookie-less
    mode, sendmsg() with MSG_FASTOPEN flag will send SYN-data regardless
    of cookie availability.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1edf96afab44..9febfb685c33 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -387,6 +387,7 @@ struct tcp_sock {
 	u8	repair_queue;
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
 		early_retrans_delayed:1, /* Delayed ER timer installed */
+		syn_data:1,	/* SYN includes data */
 		syn_fastopen:1;	/* SYN includes Fast Open option */
 
 /* RTT measurement */

commit 783237e8daf13481ee234997cbbbb823872ac388
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:07 2012 +0000

    net-tcp: Fast Open client - sending SYN-data
    
    This patch implements sending SYN-data in tcp_connect(). The data is
    from tcp_sendmsg() with flag MSG_FASTOPEN (implemented in a later patch).
    
    The length of the cookie in tcp_fastopen_req, init'd to 0, controls the
    type of the SYN. If the cookie is not cached (len==0), the host sends
    data-less SYN with Fast Open cookie request option to solicit a cookie
    from the remote. If cookie is not available (len > 0), the host sends
    a SYN-data with Fast Open cookie option. If cookie length is negative,
      the SYN will not include any Fast Open option (for fall back operations).
    
    To deal with middleboxes that may drop SYN with data or experimental TCP
    option, the SYN-data is only sent once. SYN retransmits do not include
    data or Fast Open options. The connection will fall back to regular TCP
    handshake.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 12948f543839..1edf96afab44 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -386,7 +386,8 @@ struct tcp_sock {
 		unused      : 1;
 	u8	repair_queue;
 	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
-		early_retrans_delayed:1; /* Delayed ER timer installed */
+		early_retrans_delayed:1, /* Delayed ER timer installed */
+		syn_fastopen:1;	/* SYN includes Fast Open option */
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/
@@ -500,6 +501,9 @@ struct tcp_sock {
 	struct tcp_md5sig_info	__rcu *md5sig_info;
 #endif
 
+/* TCP fastopen related information */
+	struct tcp_fastopen_request *fastopen_req;
+
 	/* When the cookie options are generated and exchanged, then this
 	 * object holds a reference to them (cookie_values->kref).  Also
 	 * contains related tcp_cookie_transactions fields.

commit 2100c8d2d9db23c0a09901a782bb4e3b21bee298
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:05 2012 +0000

    net-tcp: Fast Open base
    
    This patch impelements the common code for both the client and server.
    
    1. TCP Fast Open option processing. Since Fast Open does not have an
       option number assigned by IANA yet, it shares the experiment option
       code 254 by implementing draft-ietf-tcpm-experimental-options
       with a 16 bits magic number 0xF989. This enables global experiments
       without clashing the scarce(2) experimental options available for TCP.
    
       When the draft status becomes standard (maybe), the client should
       switch to the new option number assigned while the server supports
       both numbers for transistion.
    
    2. The new sysctl tcp_fastopen
    
    3. A place holder init function
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1888169e07c7..12948f543839 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -243,6 +243,16 @@ static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 	return (tcp_hdr(skb)->doff - 5) * 4;
 }
 
+/* TCP Fast Open */
+#define TCP_FASTOPEN_COOKIE_MIN	4	/* Min Fast Open Cookie size in bytes */
+#define TCP_FASTOPEN_COOKIE_MAX	16	/* Max Fast Open Cookie size in bytes */
+
+/* TCP Fast Open Cookie as stored in memory */
+struct tcp_fastopen_cookie {
+	s8	len;
+	u8	val[TCP_FASTOPEN_COOKIE_MAX];
+};
+
 /* This defines a selective acknowledgement block. */
 struct tcp_sack_block_wire {
 	__be32	start_seq;

commit 46d3ceabd8d98ed0ad10f20c595ca784e34786c5
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Jul 11 05:50:31 2012 +0000

    tcp: TCP Small Queues
    
    This introduce TSQ (TCP Small Queues)
    
    TSQ goal is to reduce number of TCP packets in xmit queues (qdisc &
    device queues), to reduce RTT and cwnd bias, part of the bufferbloat
    problem.
    
    sk->sk_wmem_alloc not allowed to grow above a given limit,
    allowing no more than ~128KB [1] per tcp socket in qdisc/dev layers at a
    given time.
    
    TSO packets are sized/capped to half the limit, so that we have two
    TSO packets in flight, allowing better bandwidth use.
    
    As a side effect, setting the limit to 40000 automatically reduces the
    standard gso max limit (65536) to 40000/2 : It can help to reduce
    latencies of high prio packets, having smaller TSO packets.
    
    This means we divert sock_wfree() to a tcp_wfree() handler, to
    queue/send following frames when skb_orphan() [2] is called for the
    already queued skbs.
    
    Results on my dev machines (tg3/ixgbe nics) are really impressive,
    using standard pfifo_fast, and with or without TSO/GSO.
    
    Without reduction of nominal bandwidth, we have reduction of buffering
    per bulk sender :
    < 1ms on Gbit (instead of 50ms with TSO)
    < 8ms on 100Mbit (instead of 132 ms)
    
    I no longer have 4 MBytes backlogged in qdisc by a single netperf
    session, and both side socket autotuning no longer use 4 Mbytes.
    
    As skb destructor cannot restart xmit itself ( as qdisc lock might be
    taken at this point ), we delegate the work to a tasklet. We use one
    tasklest per cpu for performance reasons.
    
    If tasklet finds a socket owned by the user, it sets TSQ_OWNED flag.
    This flag is tested in a new protocol method called from release_sock(),
    to eventually send new segments.
    
    [1] New /proc/sys/net/ipv4/tcp_limit_output_bytes tunable
    [2] skb_orphan() is usually called at TX completion time,
      but some drivers call it in their start_xmit() handler.
      These drivers should at least use BQL, or else a single TCP
      session can still fill the whole NIC TX ring, since TSQ will
      have no effect.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Dave Taht <dave.taht@bufferbloat.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2de9cf46f9fc..1888169e07c7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -339,6 +339,9 @@ struct tcp_sock {
 	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
 
+	struct list_head tsq_node; /* anchor in tsq_tasklet.head list */
+	unsigned long	tsq_flags;
+
 	/* Data for direct copy to user */
 	struct {
 		struct sk_buff_head	prequeue;
@@ -494,6 +497,12 @@ struct tcp_sock {
 	struct tcp_cookie_values  *cookie_values;
 };
 
+enum tsq_flags {
+	TSQ_THROTTLED,
+	TSQ_QUEUED,
+	TSQ_OWNED, /* tcp_tasklet_func() found socket was locked */
+};
+
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 {
 	return (struct tcp_sock *)sk;

commit b6242b9b45e84ef71c59002cd128c3197938cb2f
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 10 03:27:56 2012 -0700

    tcp: Remove tw->tw_peer
    
    No longer used.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 7d3bcedc062a..2de9cf46f9fc 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -506,7 +506,6 @@ struct tcp_timewait_sock {
 	u32			  tw_rcv_wnd;
 	u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
-	struct inet_peer	  *tw_peer;
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	  *tw_md5_key;
 #endif

commit 43b03f1f6d6832d744918947d185a7aee89d1e0f
Merge: 2da45db2bdd4 5ee31c6898ea
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 12 21:59:18 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            MAINTAINERS
            drivers/net/wireless/iwlwifi/pcie/trans.c
    
    The iwlwifi conflict was resolved by keeping the code added
    in 'net' that turns off the buggy chip feature.
    
    The MAINTAINERS conflict was merely overlapping changes, one
    change updated all the wireless web site URLs and the other
    changed some GIT trees to be Johannes's instead of John's.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8876d6b5f81f4e242a6660da22bbd92f17a8d058
Author: Paul Pluzhnikov <ppluzhnikov@google.com>
Date:   Sat Jun 9 07:53:03 2012 -0700

    net: Make linux/tcp.h C++ friendly (trivial)
    
    I originally sent this patch to <trivial@kernel.org>, but Jiri Kosina did
    not feel that this is fully appropriate for the trivial tree.
    
    Using linux/tcp.h from C++ results in:
    
    cat t.cc
    #include <linux/tcp.h>
    int main() { }
    
    g++ -c t.cc
    
    In file included from t.cc:1:
    /usr/include/linux/tcp.h:72: error: '__u32 __fswab32(__u32)' cannot appear in a constant-expression
    /usr/include/linux/tcp.h:72: error: a function call cannot appear in a constant-expression
    ...
    
    Attached trivial patch fixes this problem.
    
    Tested:
    - the t.cc above compiles with g++ and
    - the following program generates the same output before/after
      the patch:
    
    #include <linux/tcp.h>
    #include <stdio.h>
    
    int main ()
    {
    #define P(a) printf("%s: %08x\n", #a, (int)a)
     P(TCP_FLAG_CWR);
     P(TCP_FLAG_ECE);
     P(TCP_FLAG_URG);
     P(TCP_FLAG_ACK);
     P(TCP_FLAG_PSH);
     P(TCP_FLAG_RST);
     P(TCP_FLAG_SYN);
     P(TCP_FLAG_FIN);
     P(TCP_RESERVED_BITS);
     P(TCP_DATA_OFFSET);
    #undef P
     return 0;
    }
    
    Signed-off-by: Paul Pluzhnikov <ppluzhnikov@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4c5b63283377..5f359dbfcdce 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -69,16 +69,16 @@ union tcp_word_hdr {
 #define tcp_flag_word(tp) ( ((union tcp_word_hdr *)(tp))->words [3]) 
 
 enum { 
-	TCP_FLAG_CWR = __cpu_to_be32(0x00800000),
-	TCP_FLAG_ECE = __cpu_to_be32(0x00400000),
-	TCP_FLAG_URG = __cpu_to_be32(0x00200000),
-	TCP_FLAG_ACK = __cpu_to_be32(0x00100000),
-	TCP_FLAG_PSH = __cpu_to_be32(0x00080000),
-	TCP_FLAG_RST = __cpu_to_be32(0x00040000),
-	TCP_FLAG_SYN = __cpu_to_be32(0x00020000),
-	TCP_FLAG_FIN = __cpu_to_be32(0x00010000),
-	TCP_RESERVED_BITS = __cpu_to_be32(0x0F000000),
-	TCP_DATA_OFFSET = __cpu_to_be32(0xF0000000)
+	TCP_FLAG_CWR = __constant_cpu_to_be32(0x00800000),
+	TCP_FLAG_ECE = __constant_cpu_to_be32(0x00400000),
+	TCP_FLAG_URG = __constant_cpu_to_be32(0x00200000),
+	TCP_FLAG_ACK = __constant_cpu_to_be32(0x00100000),
+	TCP_FLAG_PSH = __constant_cpu_to_be32(0x00080000),
+	TCP_FLAG_RST = __constant_cpu_to_be32(0x00040000),
+	TCP_FLAG_SYN = __constant_cpu_to_be32(0x00020000),
+	TCP_FLAG_FIN = __constant_cpu_to_be32(0x00010000),
+	TCP_RESERVED_BITS = __constant_cpu_to_be32(0x0F000000),
+	TCP_DATA_OFFSET = __constant_cpu_to_be32(0xF0000000)
 }; 
 
 /*

commit 2397849baa7c44c242e5d5142d5d16d1e7ed53d0
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jun 9 14:56:12 2012 -0700

    [PATCH] tcp: Cache inetpeer in timewait socket, and only when necessary.
    
    Since it's guarenteed that we will access the inetpeer if we're trying
    to do timewait recycling and TCP options were enabled on the
    connection, just cache the peer in the timewait socket.
    
    In the future, inetpeer lookups will be context dependent (per routing
    realm), and this helps facilitate that as well.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4c5b63283377..23e8234f75a5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -506,8 +506,9 @@ struct tcp_timewait_sock {
 	u32			  tw_rcv_wnd;
 	u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
+	struct inet_peer	  *tw_peer;
 #ifdef CONFIG_TCP_MD5SIG
-	struct tcp_md5sig_key	*tw_md5_key;
+	struct tcp_md5sig_key	  *tw_md5_key;
 #endif
 	/* Few sockets in timewait have cookies; in that case, then this
 	 * object holds a reference to them (tw_cookie_values->kref).

commit e8650a08232e75274304b812ff04cfce9af9671c
Merge: 3c2c4b73aa79 f70d4a95edc7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 19:22:50 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial updates from Jiri Kosina:
     "As usual, it's mostly typo fixes, redundant code elimination and some
      documentation updates."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (57 commits)
      edac, mips: don't change code that has been removed in edac/mips tree
      xtensa: Change mail addresses of Hannes Weiner and Oskar Schirmer
      lib: Change mail address of Oskar Schirmer
      net: Change mail address of Oskar Schirmer
      arm/m68k: Change mail address of Sebastian Hess
      i2c: Change mail address of Oskar Schirmer
      net: Fix tcp_build_and_update_options comment in struct tcp_sock
      atomic64_32.h: fix parameter naming mismatch
      Kconfig: replace "--- help ---" with "---help---"
      c2port: fix bogus Kconfig "default no"
      edac: Fix spelling errors.
      qla1280: Remove redundant NULL check before release_firmware() call
      remoteproc: remove redundant NULL check before release_firmware()
      qla2xxx: Remove redundant NULL check before release_firmware() call.
      aic94xx: Get rid of redundant NULL check before release_firmware() call
      tehuti: delete redundant NULL check before release_firmware()
      qlogic: get rid of a redundant test for NULL before call to release_firmware()
      bna: remove redundant NULL test before release_firmware()
      tg3: remove redundant NULL test before release_firmware() call
      typhoon: get rid of redundant conditional before all to release_firmware()
      ...

commit c0a788c451e1f88b3fb5a85113b87dbc98c7abe4
Author: Kyle McMartin <kyle@redhat.com>
Date:   Mon May 7 15:33:04 2012 -0400

    net: Fix tcp_build_and_update_options comment in struct tcp_sock
    
    Noticed this comment didn't get updated when
    tcp_build_and_update_options was refactored.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b6c62d294380..288d201c3b59 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -406,7 +406,7 @@ struct tcp_sock {
 
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 
-	/* SACKs data, these 2 need to be together (see tcp_build_and_update_options) */
+	/* SACKs data, these 2 need to be together (see tcp_options_write) */
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 

commit 750ea2bafa55aaed208b2583470ecd7122225634
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 2 13:30:04 2012 +0000

    tcp: early retransmit: delayed fast retransmit
    
    Implementing the advanced early retransmit (sysctl_tcp_early_retrans==2).
    Delays the fast retransmit by an interval of RTT/4. We borrow the
    RTO timer to implement the delay. If we receive another ACK or send
    a new packet, the timer is cancelled and restored to original RTO
    value offset by time elapsed.  When the delayed-ER timer fires,
    we enter fast recovery and perform fast retransmit.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 7859b416d46e..d9b42c5be088 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -372,7 +372,8 @@ struct tcp_sock {
 		repair      : 1,
 		unused      : 1;
 	u8	repair_queue;
-	u8	do_early_retrans:1;/* Enable RFC5827 early-retransmit  */
+	u8	do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
+		early_retrans_delayed:1; /* Delayed ER timer installed */
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit eed530b6c67624db3f2cf477bac7c4d005d8f7ba
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 2 13:30:03 2012 +0000

    tcp: early retransmit
    
    This patch implements RFC 5827 early retransmit (ER) for TCP.
    It reduces DUPACK threshold (dupthresh) if outstanding packets are
    less than 4 to recover losses by fast recovery instead of timeout.
    
    While the algorithm is simple, small but frequent network reordering
    makes this feature dangerous: the connection repeatedly enter
    false recovery and degrade performance. Therefore we implement
    a mitigation suggested in the appendix of the RFC that delays
    entering fast recovery by a small interval, i.e., RTT/4. Currently
    ER is conservative and is disabled for the rest of the connection
    after the first reordering event. A large scale web server
    experiment on the performance impact of ER is summarized in
    section 6 of the paper "Proportional Rate Reduction for TCP”,
    IMC 2011. http://conferences.sigcomm.org/imc/2011/docs/p155.pdf
    
    Note that Linux has a similar feature called THIN_DUPACK. The
    differences are THIN_DUPACK do not mitigate reorderings and is only
    used after slow start. Currently ER is disabled if THIN_DUPACK is
    enabled. I would be happy to merge THIN_DUPACK feature with ER if
    people think it's a good idea.
    
    ER is enabled by sysctl_tcp_early_retrans:
      0: Disables ER
    
      1: Reduce dupthresh to packets_out - 1 when outstanding packets < 4.
    
      2: (Default) reduce dupthresh like mode 1. In addition, delay
         entering fast recovery by RTT/4.
    
    Note: mode 2 is implemented in the third part of this patch series.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 278af9ea42d4..7859b416d46e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -372,6 +372,7 @@ struct tcp_sock {
 		repair      : 1,
 		unused      : 1;
 	u8	repair_queue;
+	u8	do_early_retrans:1;/* Enable RFC5827 early-retransmit  */
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit de248a75c35e0208294cf304b112916254b69184
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Wed Apr 25 23:43:04 2012 +0000

    tcp repair: Fix unaligned access when repairing options (v2)
    
    Don't pick __u8/__u16 values directly from raw pointers, but instead use
    an array of structures of code:value pairs. This is OK, since the buffer
    we take options from is not an skb memory, but a user-to-kernel one.
    
    For those options which don't require any value now, require this to be
    zero (for potential future extension of this API).
    
    v2: Changed tcp_repair_opt to use two __u32-s as spotted by David Laight.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 986593685566..278af9ea42d4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -111,6 +111,11 @@ enum {
 #define TCP_QUEUE_SEQ		21
 #define TCP_REPAIR_OPTIONS	22
 
+struct tcp_repair_opt {
+	__u32	opt_code;
+	__u32	opt_val;
+};
+
 enum {
 	TCP_NO_QUEUE,
 	TCP_RECV_QUEUE,

commit b139ba4e90dccbf4cd4efb112af96a5c9e0b098c
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Apr 19 03:41:57 2012 +0000

    tcp: Repair connection-time negotiated parameters
    
    There are options, which are set up on a socket while performing
    TCP handshake. Need to resurrect them on a socket while repairing.
    A new sockoption accepts a buffer and parses it. The buffer should
    be CODE:VALUE sequence of bytes, where CODE is standard option
    code and VALUE is the respective value.
    
    Only 4 options should be handled on repaired socket.
    
    To read 3 out of 4 of these options the TCP_INFO sockoption can be
    used. An ability to get the last one (the mss_clamp) was added by
    the previous patch.
    
    Now the restore. Three of these options -- timestamp_ok, mss_clamp
    and snd_wscale -- are just restored on a coket.
    
    The sack_ok flags has 2 issues. First, whether or not to do sacks
    at all. This flag is just read and set back. No other sack  info is
    saved or restored, since according to the standart and the code
    dropping all sack-ed segments is OK, the sender will resubmit them
    again, so after the repair we will probably experience a pause in
    connection. Next, the fack bit. It's just set back on a socket if
    the respective sysctl is set. No collected stats about packets flow
    is preserved. As far as I see (plz, correct me if I'm wrong) the
    fack-based congestion algorithm survives dropping all of the stats
    and repairs itself eventually, probably losing the performance for
    that period.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4e90e6ae79df..986593685566 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -109,6 +109,7 @@ enum {
 #define TCP_REPAIR		19	/* TCP sock is under repair right now */
 #define TCP_REPAIR_QUEUE	20
 #define TCP_QUEUE_SEQ		21
+#define TCP_REPAIR_OPTIONS	22
 
 enum {
 	TCP_NO_QUEUE,

commit ee9952831cfd0bbe834f4a26489d7dce74582e37
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Apr 19 03:40:39 2012 +0000

    tcp: Initial repair mode
    
    This includes (according the the previous description):
    
    * TCP_REPAIR sockoption
    
    This one just puts the socket in/out of the repair mode.
    Allowed for CAP_NET_ADMIN and for closed/establised sockets only.
    When repair mode is turned off and the socket happens to be in
    the established state the window probe is sent to the peer to
    'unlock' the connection.
    
    * TCP_REPAIR_QUEUE sockoption
    
    This one sets the queue which we're about to repair. The
    'no-queue' is set by default.
    
    * TCP_QUEUE_SEQ socoption
    
    Sets the write_seq/rcv_nxt of a selected repaired queue.
    Allowed for TCP_CLOSE-d sockets only. When the socket changes
    its state the other seq-s are changed by the kernel according
    to the protocol rules (most of the existing code is actually
    reused).
    
    * Ability to forcibly bind a socket to a port
    
    The sk->sk_reuse is set to SK_FORCE_REUSE.
    
    * Immediate connect modification
    
    The connect syscall initializes the connection, then directly jumps
    to the code which finalizes it.
    
    * Silent close modification
    
    The close just aborts the connection (similar to SO_LINGER with 0
    time) but without sending any FIN/RST-s to peer.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b6c62d294380..4e90e6ae79df 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -106,6 +106,16 @@ enum {
 #define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
 #define TCP_THIN_DUPACK         17      /* Fast retrans. after 1 dupack */
 #define TCP_USER_TIMEOUT	18	/* How long for loss retry before timeout */
+#define TCP_REPAIR		19	/* TCP sock is under repair right now */
+#define TCP_REPAIR_QUEUE	20
+#define TCP_QUEUE_SEQ		21
+
+enum {
+	TCP_NO_QUEUE,
+	TCP_RECV_QUEUE,
+	TCP_SEND_QUEUE,
+	TCP_QUEUES_NR,
+};
 
 /* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1
@@ -353,7 +363,9 @@ struct tcp_sock {
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		thin_dupack : 1,/* Fast retransmit on first dupack      */
-		unused      : 2;
+		repair      : 1,
+		unused      : 1;
+	u8	repair_queue;
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit b4017c5368f992fb8fb3a2545a0977082c6664e4
Merge: 97767a87f3be 413708bbaf5c
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Mar 1 17:57:40 2012 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/broadcom/tg3.c
    
    Conflicts in the statistics regression bug fix from 'net',
    but happily Matt Carlson originally posted the fix against
    'net-next' so I used that to resolve this.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ecb971923614775a118bc05ad16b2bde450cac7d
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Feb 27 17:52:52 2012 -0500

    tcp: fix comment for tp->highest_sack
    
    There was an off-by-one error in the comments describing the
    highest_sack field in struct tcp_sock. The comments previously claimed
    that it was the "start sequence of the highest skb with SACKed
    bit". This commit fixes the comments to note that it is the "start
    sequence of the skb just *after* the highest skb with SACKed bit".
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 46a85c9e1f25..3c7ffdb40dc6 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -412,7 +412,8 @@ struct tcp_sock {
 
 	struct tcp_sack_block recv_sack_cache[4];
 
-	struct sk_buff *highest_sack;   /* highest skb with SACK received
+	struct sk_buff *highest_sack;   /* skb just after the highest
+					 * skb with SACKed bit set
 					 * (validity guaranteed only if
 					 * sacked_out > 0)
 					 */

commit a8afca032998850ec63e83d555cdcf0eb5680cd6
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 31 18:45:40 2012 +0000

    tcp: md5: protects md5sig_info with RCU
    
    This patch makes sure we use appropriate memory barriers before
    publishing tp->md5sig_info, allowing tcp_md5_do_lookup() being used from
    tcp_v4_send_reset() without holding socket lock (upcoming patch from
    Shawn Lu)
    
    Note we also need to respect rcu grace period before its freeing, since
    we can free socket without this grace period thanks to
    SLAB_DESTROY_BY_RCU
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Shawn Lu <shawn.lu@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c2025f159641..115389e9b945 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -463,7 +463,7 @@ struct tcp_sock {
 	const struct tcp_sock_af_ops	*af_specific;
 
 /* TCP MD5 Signature Option information */
-	struct tcp_md5sig_info	*md5sig_info;
+	struct tcp_md5sig_info	__rcu *md5sig_info;
 #endif
 
 	/* When the cookie options are generated and exchanged, then this

commit a915da9b69273815527ccb3789421cb7027b545b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 31 05:18:33 2012 +0000

    tcp: md5: rcu conversion
    
    In order to be able to support proper RST messages for TCP MD5 flows, we
    need to allow access to MD5 keys without locking listener socket.
    
    This conversion is a nice cleanup, and shrinks size of timewait sockets
    by 80 bytes.
    
    IPv6 code reuses generic code found in IPv4 instead of duplicating it.
    
    Control path uses GFP_KERNEL allocations instead of GFP_ATOMIC.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Shawn Lu <shawn.lu@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 46a85c9e1f25..c2025f159641 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -486,8 +486,7 @@ struct tcp_timewait_sock {
 	u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
 #ifdef CONFIG_TCP_MD5SIG
-	u16			  tw_md5_keylen;
-	u8			  tw_md5_key[TCP_MD5SIG_MAXKEYLEN];
+	struct tcp_md5sig_key	*tw_md5_key;
 #endif
 	/* Few sockets in timewait have cookies; in that case, then this
 	 * object holds a reference to them (tw_cookie_values->kref).

commit ab56222a32b9dbaae19c1d37f07b0ac4fc3c27ec
Author: Vijay Subramanian <subramanian.vijay@gmail.com>
Date:   Tue Dec 20 13:23:24 2011 +0000

    tcp: Replace constants with #define macros
    
    to record the state of SACK/FACK and DSACK for better readability and maintenance.
    
    Signed-off-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 7f59ee946983..46a85c9e1f25 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -238,6 +238,11 @@ struct tcp_sack_block {
 	u32	end_seq;
 };
 
+/*These are used to set the sack_ok field in struct tcp_options_received */
+#define TCP_SACK_SEEN     (1 << 0)   /*1 = peer is SACK capable, */
+#define TCP_FACK_ENABLED  (1 << 1)   /*1 = FACK is enabled locally*/
+#define TCP_DSACK_SEEN    (1 << 2)   /*1 = DSACK was received from peer*/
+
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
 	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */

commit b5c5693bb723a019deac3cd1345f3e7233c8a67e
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Oct 3 14:01:21 2011 -0400

    tcp: report ECN_SEEN in tcp_info
    
    Allows ss command (iproute2) to display "ecnseen" if at least one packet
    with ECT(0) or ECT(1) or ECN was received by this socket.
    
    "ecn" means ECN was negotiated at session establishment (TCP level)
    
    "ecnseen" means we received at least one packet with ECT fields set (IP
    level)
    
    ss -i
    ...
    ESTAB      0      0   192.168.20.110:22  192.168.20.144:38016
    ino:5950 sk:f178e400
             mem:(r0,w0,f0,t0) ts sack ecn ecnseen bic wscale:7,8 rto:210
    rtt:12.5/7.5 cwnd:10 send 9.3Mbps rcv_space:14480
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 6b63b310af36..7f59ee946983 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -111,7 +111,8 @@ enum {
 #define TCPI_OPT_TIMESTAMPS	1
 #define TCPI_OPT_SACK		2
 #define TCPI_OPT_WSCALE		4
-#define TCPI_OPT_ECN		8
+#define TCPI_OPT_ECN		8 /* ECN was negociated at TCP session init */
+#define TCPI_OPT_ECN_SEEN	16 /* we received at least one packet with ECT */
 
 enum tcp_ca_state {
 	TCP_CA_Open = 0,

commit a262f0cdf1f2916ea918dc329492abb5323d9a6c
Author: Nandita Dukkipati <nanditad@google.com>
Date:   Sun Aug 21 20:21:57 2011 +0000

    Proportional Rate Reduction for TCP.
    
    This patch implements Proportional Rate Reduction (PRR) for TCP.
    PRR is an algorithm that determines TCP's sending rate in fast
    recovery. PRR avoids excessive window reductions and aims for
    the actual congestion window size at the end of recovery to be as
    close as possible to the window determined by the congestion control
    algorithm. PRR also improves accuracy of the amount of data sent
    during loss recovery.
    
    The patch implements the recommended flavor of PRR called PRR-SSRB
    (Proportional rate reduction with slow start reduction bound) and
    replaces the existing rate halving algorithm. PRR improves upon the
    existing Linux fast recovery under a number of conditions including:
      1) burst losses where the losses implicitly reduce the amount of
    outstanding data (pipe) below the ssthresh value selected by the
    congestion control algorithm and,
      2) losses near the end of short flows where application runs out of
    data to send.
    
    As an example, with the existing rate halving implementation a single
    loss event can cause a connection carrying short Web transactions to
    go into the slow start mode after the recovery. This is because during
    recovery Linux pulls the congestion window down to packets_in_flight+1
    on every ACK. A short Web response often runs out of new data to send
    and its pipe reduces to zero by the end of recovery when all its packets
    are drained from the network. Subsequent HTTP responses using the same
    connection will have to slow start to raise cwnd to ssthresh. PRR on
    the other hand aims for the cwnd to be as close as possible to ssthresh
    by the end of recovery.
    
    A description of PRR and a discussion of its performance can be found at
    the following links:
    - IETF Draft:
        http://tools.ietf.org/html/draft-mathis-tcpm-proportional-rate-reduction-01
    - IETF Slides:
        http://www.ietf.org/proceedings/80/slides/tcpm-6.pdf
        http://tools.ietf.org/agenda/81/slides/tcpm-2.pdf
    - Paper to appear in Internet Measurements Conference (IMC) 2011:
        Improving TCP Loss Recovery
        Nandita Dukkipati, Matt Mathis, Yuchung Cheng
    
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 531ede8006d9..6b63b310af36 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -379,6 +379,10 @@ struct tcp_sock {
 	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
+	u32	prior_cwnd;	/* Congestion window at start of Recovery. */
+	u32	prr_delivered;	/* Number of newly delivered packets to
+				 * receiver in Recovery. */
+	u32	prr_out;	/* Total number of pkts sent during Recovery. */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit 9ad7c049f0f79c418e293b1b68cf10d68f54fcdb
Author: Jerry Chu <hkchu@google.com>
Date:   Wed Jun 8 11:08:38 2011 +0000

    tcp: RFC2988bis + taking RTT sample from 3WHS for the passive open side
    
    This patch lowers the default initRTO from 3secs to 1sec per
    RFC2988bis. It falls back to 3secs if the SYN or SYN-ACK packet
    has been retransmitted, AND the TCP timestamp option is not on.
    
    It also adds support to take RTT sample during 3WHS on the passive
    open side, just like its active open counterpart, and uses it, if
    valid, to seed the initRTO for the data transmission phase.
    
    The patch also resets ssthresh to its initial default at the
    beginning of the data transmission phase, and reduces cwnd to 1 if
    there has been MORE THAN ONE retransmission during 3WHS per RFC5681.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e64f4c67d0ef..531ede8006d9 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -282,6 +282,7 @@ struct tcp_request_sock {
 #endif
 	u32				rcv_isn;
 	u32				snt_isn;
+	u32				snt_synack; /* synack sent time */
 };
 
 static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)

commit dca43c75e7e545694a9dd6288553f55c53e2a3a3
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 27 19:13:28 2010 +0000

    tcp: Add TCP_USER_TIMEOUT socket option.
    
    This patch provides a "user timeout" support as described in RFC793. The
    socket option is also needed for the the local half of RFC5482 "TCP User
    Timeout Option".
    
    TCP_USER_TIMEOUT is a TCP level socket option that takes an unsigned int,
    when > 0, to specify the maximum amount of time in ms that transmitted
    data may remain unacknowledged before TCP will forcefully close the
    corresponding connection and return ETIMEDOUT to the application. If
    0 is given, TCP will continue to use the system default.
    
    Increasing the user timeouts allows a TCP connection to survive extended
    periods without end-to-end connectivity. Decreasing the user timeouts
    allows applications to "fail fast" if so desired. Otherwise it may take
    upto 20 minutes with the current system defaults in a normal WAN
    environment.
    
    The socket option can be made during any state of a TCP connection, but
    is only effective during the synchronized states of a connection
    (ESTABLISHED, FIN-WAIT-1, FIN-WAIT-2, CLOSE-WAIT, CLOSING, or LAST-ACK).
    Moreover, when used with the TCP keepalive (SO_KEEPALIVE) option,
    TCP_USER_TIMEOUT will overtake keepalive to determine when to close a
    connection due to keepalive failure.
    
    The option does not change in anyway when TCP retransmits a packet, nor
    when a keepalive probe will be sent.
    
    This option, like many others, will be inherited by an acceptor from its
    listener.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index a778ee024590..e64f4c67d0ef 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -105,6 +105,7 @@ enum {
 #define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
 #define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
 #define TCP_THIN_DUPACK         17      /* Fast retrans. after 1 dupack */
+#define TCP_USER_TIMEOUT	18	/* How long for loss retry before timeout */
 
 /* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1

commit 7e38017557bc0b87434d184f8804cadb102bb903
Author: Andreas Petlund <apetlund@simula.no>
Date:   Thu Feb 18 04:48:19 2010 +0000

    net: TCP thin dupack
    
    This patch enables fast retransmissions after one dupACK for
    TCP if the stream is identified as thin. This will reduce
    latencies for thin streams that are not able to trigger fast
    retransmissions due to high packet interarrival time. This
    mechanism is only active if enabled by iocontrol or syscontrol
    and the stream is identified as thin.
    
    Signed-off-by: Andreas Petlund <apetlund@simula.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3ba8b074612f..a778ee024590 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -104,6 +104,7 @@ enum {
 #define TCP_MD5SIG		14	/* TCP MD5 Signature (RFC2385) */
 #define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
 #define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
+#define TCP_THIN_DUPACK         17      /* Fast retrans. after 1 dupack */
 
 /* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1
@@ -343,7 +344,8 @@ struct tcp_sock {
 	u8	frto_counter;	/* Number of new acks after RTO */
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
-		unused      : 3;
+		thin_dupack : 1,/* Fast retransmit on first dupack      */
+		unused      : 2;
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit 36e31b0af58728071e8023cf8e20c5166b700717
Author: Andreas Petlund <apetlund@simula.no>
Date:   Thu Feb 18 02:47:01 2010 +0000

    net: TCP thin linear timeouts
    
    This patch will make TCP use only linear timeouts if the
    stream is thin. This will help to avoid the very high latencies
    that thin stream suffer because of exponential backoff. This
    mechanism is only active if enabled by iocontrol or syscontrol
    and the stream is identified as thin. A maximum of 6 linear
    timeouts is tried before exponential backoff is resumed.
    
    Signed-off-by: Andreas Petlund <apetlund@simula.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 7fee8a4df931..3ba8b074612f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -103,6 +103,7 @@ enum {
 #define TCP_CONGESTION		13	/* Congestion control algorithm */
 #define TCP_MD5SIG		14	/* TCP MD5 Signature (RFC2385) */
 #define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
+#define TCP_THIN_LINEAR_TIMEOUTS 16      /* Use linear timeouts for thin streams*/
 
 /* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1
@@ -340,7 +341,9 @@ struct tcp_sock {
 	u32	frto_highmark;	/* snd_nxt when RTO occurred */
 	u16	advmss;		/* Advertised MSS			*/
 	u8	frto_counter;	/* Number of new acks after RTO */
-	u8	nonagle;	/* Disable Nagle algorithm?             */
+	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
+		thin_lto    : 1,/* Use linear timeouts for thin streams */
+		unused      : 3;
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/

commit 435cf559f02ea3a3159eb316f97dc88bdebe9432
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:17:05 2009 +0000

    TCPCT part 1d: define TCP cookie option, extend existing struct's
    
    Data structures are carefully composed to require minimal additions.
    For example, the struct tcp_options_received cookie_plus variable fits
    between existing 16-bit and 8-bit variables, requiring no additional
    space (taking alignment into consideration).  There are no additions to
    tcp_request_sock, and only 1 pointer in tcp_sock.
    
    This is a significantly revised implementation of an earlier (year-old)
    patch that no longer applies cleanly, with permission of the original
    author (Adam Langley):
    
        http://thread.gmane.org/gmane.linux.network/102586
    
    The principle difference is using a TCP option to carry the cookie nonce,
    instead of a user configured offset in the data.  This is more flexible and
    less subject to user configuration error.  Such a cookie option has been
    suggested for many years, and is also useful without SYN data, allowing
    several related concepts to use the same extension option.
    
        "Re: SYN floods (was: does history repeat itself?)", September 9, 1996.
        http://www.merit.net/mail.archives/nanog/1996-09/msg00235.html
    
        "Re: what a new TCP header might look like", May 12, 1998.
        ftp://ftp.isi.edu/end2end/end2end-interest-1998.mail
    
    These functions will also be used in subsequent patches that implement
    additional features.
    
    Requires:
       TCPCT part 1a: add request_values parameter for sending SYNACK
       TCPCT part 1b: generate Responder Cookie secret
       TCPCT part 1c: sysctl_tcp_cookie_size, socket option TCP_COOKIE_TRANSACTIONS
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index eaa3113b3786..7fee8a4df931 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -247,31 +247,38 @@ struct tcp_options_received {
 		sack_ok : 4,	/* SACK seen on SYN packet		*/
 		snd_wscale : 4,	/* Window scaling received from sender	*/
 		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
-/*	SACKs data	*/
+	u8	cookie_plus:6,	/* bytes in authenticator/cookie option	*/
+		cookie_out_never:1,
+		cookie_in_always:1;
 	u8	num_sacks;	/* Number of SACK blocks		*/
-	u16	user_mss;  	/* mss requested by user in ioctl */
+	u16	user_mss;	/* mss requested by user in ioctl	*/
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 {
-	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+	rx_opt->tstamp_ok = rx_opt->sack_ok = 0;
+	rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+	rx_opt->cookie_plus = 0;
 }
 
 /* This is the max number of SACKS that we'll generate and process. It's safe
- * to increse this, although since:
+ * to increase this, although since:
  *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
  * only four options will fit in a standard TCP header */
 #define TCP_NUM_SACKS 4
 
+struct tcp_cookie_values;
+struct tcp_request_sock_ops;
+
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
 #ifdef CONFIG_TCP_MD5SIG
 	/* Only used by TCP MD5 Signature so far. */
 	const struct tcp_request_sock_ops *af_specific;
 #endif
-	u32			 	rcv_isn;
-	u32			 	snt_isn;
+	u32				rcv_isn;
+	u32				snt_isn;
 };
 
 static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
@@ -441,6 +448,12 @@ struct tcp_sock {
 /* TCP MD5 Signature Option information */
 	struct tcp_md5sig_info	*md5sig_info;
 #endif
+
+	/* When the cookie options are generated and exchanged, then this
+	 * object holds a reference to them (cookie_values->kref).  Also
+	 * contains related tcp_cookie_transactions fields.
+	 */
+	struct tcp_cookie_values  *cookie_values;
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
@@ -459,6 +472,10 @@ struct tcp_timewait_sock {
 	u16			  tw_md5_keylen;
 	u8			  tw_md5_key[TCP_MD5SIG_MAXKEYLEN];
 #endif
+	/* Few sockets in timewait have cookies; in that case, then this
+	 * object holds a reference to them (tw_cookie_values->kref).
+	 */
+	struct tcp_cookie_values  *tw_cookie_values;
 };
 
 static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)

commit 519855c508b9a17878c0977a3cdefc09b59b30df
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:14:19 2009 +0000

    TCPCT part 1c: sysctl_tcp_cookie_size, socket option TCP_COOKIE_TRANSACTIONS
    
    Define sysctl (tcp_cookie_size) to turn on and off the cookie option
    default globally, instead of a compiled configuration option.
    
    Define per socket option (TCP_COOKIE_TRANSACTIONS) for setting constant
    data values, retrieving variable cookie values, and other facilities.
    
    Move inline tcp_clear_options() unchanged from net/tcp.h to linux/tcp.h,
    near its corresponding struct tcp_options_received (prior to changes).
    
    This is a straightforward re-implementation of an earlier (year-old)
    patch that no longer applies cleanly, with permission of the original
    author (Adam Langley):
    
        http://thread.gmane.org/gmane.linux.network/102586
    
    These functions will also be used in subsequent patches that implement
    additional features.
    
    Requires:
       net: TCP_MSS_DEFAULT, TCP_MSS_DESIRED
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 32d7d77b4a01..eaa3113b3786 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -102,7 +102,9 @@ enum {
 #define TCP_QUICKACK		12	/* Block/reenable quick acks */
 #define TCP_CONGESTION		13	/* Congestion control algorithm */
 #define TCP_MD5SIG		14	/* TCP MD5 Signature (RFC2385) */
+#define TCP_COOKIE_TRANSACTIONS	15	/* TCP Cookie Transactions */
 
+/* for TCP_INFO socket option */
 #define TCPI_OPT_TIMESTAMPS	1
 #define TCPI_OPT_SACK		2
 #define TCPI_OPT_WSCALE		4
@@ -174,6 +176,30 @@ struct tcp_md5sig {
 	__u8	tcpm_key[TCP_MD5SIG_MAXKEYLEN];		/* key (binary) */
 };
 
+/* for TCP_COOKIE_TRANSACTIONS (TCPCT) socket option */
+#define TCP_COOKIE_MIN		 8		/*  64-bits */
+#define TCP_COOKIE_MAX		16		/* 128-bits */
+#define TCP_COOKIE_PAIR_SIZE	(2*TCP_COOKIE_MAX)
+
+/* Flags for both getsockopt and setsockopt */
+#define TCP_COOKIE_IN_ALWAYS	(1 << 0)	/* Discard SYN without cookie */
+#define TCP_COOKIE_OUT_NEVER	(1 << 1)	/* Prohibit outgoing cookies,
+						 * supercedes everything. */
+
+/* Flags for getsockopt */
+#define TCP_S_DATA_IN		(1 << 2)	/* Was data received? */
+#define TCP_S_DATA_OUT		(1 << 3)	/* Was data sent? */
+
+/* TCP_COOKIE_TRANSACTIONS data */
+struct tcp_cookie_transactions {
+	__u16	tcpct_flags;			/* see above */
+	__u8	__tcpct_pad1;			/* zero */
+	__u8	tcpct_cookie_desired;		/* bytes */
+	__u16	tcpct_s_data_desired;		/* bytes of variable data */
+	__u16	tcpct_used;			/* bytes in value */
+	__u8	tcpct_value[TCP_MSS_DEFAULT];
+};
+
 #ifdef __KERNEL__
 
 #include <linux/skbuff.h>
@@ -227,6 +253,11 @@ struct tcp_options_received {
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
+static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
+{
+	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+}
+
 /* This is the max number of SACKS that we'll generate and process. It's safe
  * to increse this, although since:
  *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
@@ -435,6 +466,6 @@ static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)
 	return (struct tcp_timewait_sock *)sk;
 }
 
-#endif
+#endif	/* __KERNEL__ */
 
 #endif	/* _LINUX_TCP_H */

commit bee7ca9ec03a26676ea2b1c28dc4039348eff3e1
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Tue Nov 10 09:51:18 2009 +0000

    net: TCP_MSS_DEFAULT, TCP_MSS_DESIRED
    
    Define two symbols needed in both kernel and user space.
    
    Remove old (somewhat incorrect) kernel variant that wasn't used in
    most cases.  Default should apply to both RMSS and SMSS (RFC2581).
    
    Replace numeric constants with defined symbols.
    
    Stand-alone patch, originally developed for TCPCT.
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index eeecb8547a2a..32d7d77b4a01 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -81,6 +81,12 @@ enum {
 	TCP_DATA_OFFSET = __cpu_to_be32(0xF0000000)
 }; 
 
+/*
+ * TCP general constants
+ */
+#define TCP_MSS_DEFAULT		 536U	/* IPv4 (RFC1122, RFC2581) */
+#define TCP_MSS_DESIRED		1220U	/* IPv6 (tunneled), EDNS0 (RFC3226) */
+
 /* TCP socket options */
 #define TCP_NODELAY		1	/* Turn off Nagle's algorithm. */
 #define TCP_MAXSEG		2	/* Limit MSS */

commit d94d9fee9fa4e66a0b91640a694b8b10177075b3
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Nov 4 09:50:58 2009 -0800

    net: cleanup include/linux
    
    This cleanup patch puts struct/union/enum opening braces,
    in first line to ease grep games.
    
    struct something
    {
    
    becomes :
    
    struct something {
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 61723a7c21fe..eeecb8547a2a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -102,8 +102,7 @@ enum {
 #define TCPI_OPT_WSCALE		4
 #define TCPI_OPT_ECN		8
 
-enum tcp_ca_state
-{
+enum tcp_ca_state {
 	TCP_CA_Open = 0,
 #define TCPF_CA_Open	(1<<TCP_CA_Open)
 	TCP_CA_Disorder = 1,
@@ -116,8 +115,7 @@ enum tcp_ca_state
 #define TCPF_CA_Loss	(1<<TCP_CA_Loss)
 };
 
-struct tcp_info
-{
+struct tcp_info {
 	__u8	tcpi_state;
 	__u8	tcpi_ca_state;
 	__u8	tcpi_retransmits;

commit b2e4b3debc327a5b53d9622e0b1785eea2ea2aad
Author: Stephen Hemminger <shemminger@vyatta.com>
Date:   Tue Sep 1 19:25:03 2009 +0000

    tcp: MD5 operations should be const
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8afac76cd748..61723a7c21fe 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -233,7 +233,7 @@ struct tcp_request_sock {
 	struct inet_request_sock 	req;
 #ifdef CONFIG_TCP_MD5SIG
 	/* Only used by TCP MD5 Signature so far. */
-	struct tcp_request_sock_ops	*af_specific;
+	const struct tcp_request_sock_ops *af_specific;
 #endif
 	u32			 	rcv_isn;
 	u32			 	snt_isn;
@@ -401,9 +401,9 @@ struct tcp_sock {
 
 #ifdef CONFIG_TCP_MD5SIG
 /* TCP AF-Specific parts; only used by MD5 Signature support so far */
-	struct tcp_sock_af_ops	*af_specific;
+	const struct tcp_sock_af_ops	*af_specific;
 
-/* TCP MD5 Signagure Option information */
+/* TCP MD5 Signature Option information */
 	struct tcp_md5sig_info	*md5sig_info;
 #endif
 };

commit a0f82f64e26929776c58a5c93c2ecb38e3d82815
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Apr 19 09:43:48 2009 +0000

    syncookies: remove last_synq_overflow from struct tcp_sock
    
    last_synq_overflow eats 4 or 8 bytes in struct tcp_sock, even
    though it is only used when a listening sockets syn queue
    is full.
    
    We can (ab)use rx_opt.ts_recent_stamp to store the same information;
    it is not used otherwise as long as a socket is in listen state.
    
    Move linger2 around to avoid splitting struct mtu_probe
    across cacheline boundary on 32 bit arches.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9d5078bd23a3..8afac76cd748 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -377,7 +377,7 @@ struct tcp_sock {
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
 
-	unsigned long last_synq_overflow; 
+	int			linger2;
 
 /* Receiver side RTT estimation */
 	struct {
@@ -406,8 +406,6 @@ struct tcp_sock {
 /* TCP MD5 Signagure Option information */
 	struct tcp_md5sig_info	*md5sig_info;
 #endif
-
-	int			linger2;
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)

commit 2a3a041c4e2c1685e668b280c121a5a40a029a03
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Mar 14 22:45:16 2009 +0000

    tcp: cache result of earlier divides when mss-aligning things
    
    The results is very unlikely change every so often so we
    hardly need to divide again after doing that once for a
    connection. Yet, if divide still becomes necessary we
    detect that and do the right thing and again settle for
    non-divide state. Takes the u16 space which was previously
    taken by the plain xmit_size_goal.
    
    This should take care part of the tso vs non-tso difference
    we found earlier.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ad2021ccc55a..9d5078bd23a3 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -248,6 +248,7 @@ struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
 	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
+	u16	xmit_size_goal_segs; /* Goal for segmenting output packets */
 
 /*
  *	Header prediction flags

commit 0c54b85f2828128274f319a1eb3ce7f604fe2a53
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Mar 14 14:23:05 2009 +0000

    tcp: simplify tcp_current_mss
    
    There's very little need for most of the callsites to get
    tp->xmit_goal_size updated. That will cost us divide as is,
    so slice the function in two. Also, the only users of the
    tp->xmit_goal_size are directly behind tcp_current_mss(),
    so there's no need to store that variable into tcp_sock
    at all! The drop of xmit_goal_size currently leaves 16-bit
    hole and some reorganization would again be necessary to
    change that (but I'm aiming to fill that hole with u16
    xmit_goal_size_segs to cache the results of the remaining
    divide to get that tso on regression).
    
    Bring xmit_goal_size parts into tcp.c
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Cc: Evgeniy Polyakov <zbr@ioremap.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4b86ad71e054..ad2021ccc55a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -248,7 +248,6 @@ struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
 	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
-	u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
 
 /*
  *	Header prediction flags

commit cabeccbd172cc305f4383f5a4808ae254745275f
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Feb 28 04:44:38 2009 +0000

    tcp: kill eff_sacks "cache", the sole user can calculate itself
    
    Also fixes insignificant bug that would cause sending of stale
    SACK block (would occur in some corner cases).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0cd99e6baca5..4b86ad71e054 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -218,7 +218,6 @@ struct tcp_options_received {
 		snd_wscale : 4,	/* Window scaling received from sender	*/
 		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
 /*	SACKs data	*/
-	u8	eff_sacks;	/* Size of SACK array to send with next packet */
 	u8	num_sacks;	/* Number of SACK blocks		*/
 	u16	user_mss;  	/* mss requested by user in ioctl */
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */

commit f3a7c66b5ce0b75a9774a50b5dcce93e5ba28370
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Sat Feb 14 22:58:35 2009 -0800

    net: replace __constant_{endian} uses in net headers
    
    Base versions handle constant folding now.  For headers exposed to
    userspace, we must only expose the __ prefixed versions.
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fe77e1499ab7..0cd99e6baca5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -69,16 +69,16 @@ union tcp_word_hdr {
 #define tcp_flag_word(tp) ( ((union tcp_word_hdr *)(tp))->words [3]) 
 
 enum { 
-	TCP_FLAG_CWR = __constant_htonl(0x00800000), 
-	TCP_FLAG_ECE = __constant_htonl(0x00400000), 
-	TCP_FLAG_URG = __constant_htonl(0x00200000), 
-	TCP_FLAG_ACK = __constant_htonl(0x00100000), 
-	TCP_FLAG_PSH = __constant_htonl(0x00080000), 
-	TCP_FLAG_RST = __constant_htonl(0x00040000), 
-	TCP_FLAG_SYN = __constant_htonl(0x00020000), 
-	TCP_FLAG_FIN = __constant_htonl(0x00010000),
-	TCP_RESERVED_BITS = __constant_htonl(0x0F000000),
-	TCP_DATA_OFFSET = __constant_htonl(0xF0000000)
+	TCP_FLAG_CWR = __cpu_to_be32(0x00800000),
+	TCP_FLAG_ECE = __cpu_to_be32(0x00400000),
+	TCP_FLAG_URG = __cpu_to_be32(0x00200000),
+	TCP_FLAG_ACK = __cpu_to_be32(0x00100000),
+	TCP_FLAG_PSH = __cpu_to_be32(0x00080000),
+	TCP_FLAG_RST = __cpu_to_be32(0x00040000),
+	TCP_FLAG_SYN = __cpu_to_be32(0x00020000),
+	TCP_FLAG_FIN = __cpu_to_be32(0x00010000),
+	TCP_RESERVED_BITS = __cpu_to_be32(0x0F000000),
+	TCP_DATA_OFFSET = __cpu_to_be32(0xF0000000)
 }; 
 
 /* TCP socket options */

commit 33f5f57eeb0c6386fdd85f9c690dc8d700ba7928
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Tue Oct 7 14:43:06 2008 -0700

    tcp: kill pointless urg_mode
    
    It all started from me noticing that this urgent check in
    tcp_clean_rtx_queue is unnecessarily inside the loop. Then
    I took a longer look to it and found out that the users of
    urg_mode can trivially do without, well almost, there was
    one gotcha.
    
    Bonus: those funny people who use urg with >= 2^31 write_seq -
    snd_una could now rejoice too (that's the only purpose for the
    between being there, otherwise a simple compare would have done
    the thing). Not that I assume that the rest of the tcp code
    happily lives with such mind-boggling numbers :-). Alas, it
    turned out to be impossible to set wmem to such numbers anyway,
    yes I really tried a big sendfile after setting some wmem but
    nothing happened :-). ...Tcp_wmem is int and so is sk_sndbuf...
    So I hacked a bit variable to long and found out that it seems
    to work... :-)
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 767290628292..fe77e1499ab7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -312,8 +312,11 @@ struct tcp_sock {
 	u32	retrans_out;	/* Retransmitted packets out		*/
 
 	u16	urg_data;	/* Saved octet of OOB data and control flags */
-	u8	urg_mode;	/* In urgent mode		*/
 	u8	ecn_flags;	/* ECN status bits.			*/
+	u8	reordering;	/* Packet reordering metric.		*/
+	u32	snd_up;		/* Urgent pointer		*/
+
+	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */
@@ -361,8 +364,6 @@ struct tcp_sock {
 
 	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
 
-	u8	reordering;	/* Packet reordering metric.		*/
-	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 	u32	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	u32	high_seq;	/* snd_nxt at onset of congestion	*/
 
@@ -374,8 +375,6 @@ struct tcp_sock {
 	u32	total_retrans;	/* Total retransmits for entire connection */
 
 	u32	urg_seq;	/* Seq of received urgent pointer */
-	u32	snd_up;		/* Urgent pointer		*/
-
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
 

commit 0e1c54c2a405494281e0639aacc90db03b50ae77
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:24:21 2008 -0700

    tcp: reorganize retransmit code loops
    
    Both loops are quite similar, so they can be combined
    with little effort. As a result, forward_skb_hint becomes
    obsolete as well.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d7637c4b2840..767290628292 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -342,7 +342,6 @@ struct tcp_sock {
 	struct sk_buff* lost_skb_hint;
 	struct sk_buff *scoreboard_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
-	struct sk_buff *forward_skb_hint;
 
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 

commit 006f582c73f4eda35e06fd323193c3df43fb3459
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:20:20 2008 -0700

    tcp: convert retransmit_cnt_hint to seqno
    
    Main benefit in this is that we can then freely point
    the retransmit_skb_hint to anywhere we want to because
    there's no longer need to know what would be the count
    changes involve, and since this is really used only as a
    terminator, unnecessary work is one time walk at most,
    and if some retransmissions are necessary after that
    point later on, the walk is not full waste of time
    anyway.
    
    Since retransmit_high must be kept valid, all lost
    markers must ensure that.
    
    Now I also have learned how those "holes" in the
    rexmittable skbs can appear, mtu probe does them. So
    I removed the misleading comment as well.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2e2557388e36..d7637c4b2840 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -358,7 +358,7 @@ struct tcp_sock {
 					 */
 
 	int     lost_cnt_hint;
-	int     retransmit_cnt_hint;
+	u32     retransmit_high;	/* L-bits may be on up to this seqno */
 
 	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
 

commit 4389dded7767d24290463f2a8302ba3253ebdd56
Author: Adam Langley <agl@imperialviolet.org>
Date:   Sat Jul 19 00:07:02 2008 -0700

    tcp: Remove redundant checks when setting eff_sacks
    
    Remove redundant checks when setting eff_sacks and make the number of SACKs a
    compile time constant. Now that the options code knows how many SACK blocks can
    fit in the header, we don't need to have the SACK code guessing at it.
    
    Signed-off-by: Adam Langley <agl@imperialviolet.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 07e79bdb9cdf..2e2557388e36 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -224,6 +224,12 @@ struct tcp_options_received {
 	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
+/* This is the max number of SACKS that we'll generate and process. It's safe
+ * to increse this, although since:
+ *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
+ * only four options will fit in a standard TCP header */
+#define TCP_NUM_SACKS 4
+
 struct tcp_request_sock {
 	struct inet_request_sock 	req;
 #ifdef CONFIG_TCP_MD5SIG

commit 4ae127d1b6c71f9240dd4245f240e6dd8fc98014
Merge: 875ec4333b99 7775c9753b94
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 13 20:52:39 2008 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
    
            drivers/net/smc911x.c

commit ec0a196626bd12e0ba108d7daa6d95a4fb25c2c5
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 12 16:31:35 2008 -0700

    tcp: Revert 'process defer accept as established' changes.
    
    This reverts two changesets, ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
    ("[TCP]: TCP_DEFER_ACCEPT updates - process as established") and
    the follow-on bug fix 9ae27e0adbf471c7a6b80102e38e1d5a346b3b38
    ("tcp: Fix slab corruption with ipv6 and tcp6fuzz").
    
    This change causes several problems, first reported by Ingo Molnar
    as a distcc-over-loopback regression where connections were getting
    stuck.
    
    Ilpo Järvinen first spotted the locking problems.  The new function
    added by this code, tcp_defer_accept_check(), only has the
    child socket locked, yet it is modifying state of the parent
    listening socket.
    
    Fixing that is non-trivial at best, because we can't simply just grab
    the parent listening socket lock at this point, because it would
    create an ABBA deadlock.  The normal ordering is parent listening
    socket --> child socket, but this code path would require the
    reverse lock ordering.
    
    Next is a problem noticed by Vitaliy Gusev, he noted:
    
    ----------------------------------------
    >--- a/net/ipv4/tcp_timer.c
    >+++ b/net/ipv4/tcp_timer.c
    >@@ -481,6 +481,11 @@ static void tcp_keepalive_timer (unsigned long data)
    >               goto death;
    >       }
    >
    >+      if (tp->defer_tcp_accept.request && sk->sk_state == TCP_ESTABLISHED) {
    >+              tcp_send_active_reset(sk, GFP_ATOMIC);
    >+              goto death;
    
    Here socket sk is not attached to listening socket's request queue. tcp_done()
    will not call inet_csk_destroy_sock() (and tcp_v4_destroy_sock() which should
    release this sk) as socket is not DEAD. Therefore socket sk will be lost for
    freeing.
    ----------------------------------------
    
    Finally, Alexey Kuznetsov argues that there might not even be any
    real value or advantage to these new semantics even if we fix all
    of the bugs:
    
    ----------------------------------------
    Hiding from accept() sockets with only out-of-order data only
    is the only thing which is impossible with old approach. Is this really
    so valuable? My opinion: no, this is nothing but a new loophole
    to consume memory without control.
    ----------------------------------------
    
    So revert this thing for now.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 18e62e3d406f..b31b6b74aa28 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -239,11 +239,6 @@ static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 	return (struct tcp_request_sock *)req;
 }
 
-struct tcp_deferred_accept_info {
-	struct sock *listen_sk;
-	struct request_sock *request;
-};
-
 struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
@@ -379,8 +374,6 @@ struct tcp_sock {
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
 	int			linger2;
 
-	struct tcp_deferred_accept_info defer_tcp_accept;
-
 	unsigned long last_synq_overflow; 
 
 	u32	tso_deferred;

commit b79eeeb9e48457579cb742cd02e162fcd673c4a3
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu May 29 03:25:23 2008 -0700

    tcp: Reorganize tcp_sock to fill 64-bit holes & improve locality
    
    I tried to group recovery related fields nearby (non-CA_Open related
    variables, to be more accurate) so that one to three cachelines would
    not be necessary in CA_Open. These are now contiguously deployed:
    
      struct sk_buff_head        out_of_order_queue;   /*  1968    80 */
      /* --- cacheline 32 boundary (2048 bytes) --- */
      struct tcp_sack_block      duplicate_sack[1];    /*  2048     8 */
      struct tcp_sack_block      selective_acks[4];    /*  2056    32 */
      struct tcp_sack_block      recv_sack_cache[4];   /*  2088    32 */
      /* --- cacheline 33 boundary (2112 bytes) was 8 bytes ago --- */
      struct sk_buff *           highest_sack;         /*  2120     8 */
      int                        lost_cnt_hint;        /*  2128     4 */
      int                        retransmit_cnt_hint;  /*  2132     4 */
      u32                        lost_retrans_low;     /*  2136     4 */
      u8                         reordering;           /*  2140     1 */
      u8                         keepalive_probes;     /*  2141     1 */
    
      /* XXX 2 bytes hole, try to pack */
    
      u32                        prior_ssthresh;       /*  2144     4 */
      u32                        high_seq;             /*  2148     4 */
      u32                        retrans_stamp;        /*  2152     4 */
      u32                        undo_marker;          /*  2156     4 */
      int                        undo_retrans;         /*  2160     4 */
      u32                        total_retrans;        /*  2164     4 */
    
    ...and they're then followed by URG slowpath & keepalive related
    variables.
    
    Head of the out_of_order_queue always needed for empty checks, if
    that's empty (and TCP is in CA_Open), following ~200 bytes (in 64-bit)
    shouldn't be necessary for anything. If only OFO queue exists but TCP
    is in CA_Open, selective_acks (and possibly duplicate_sack) are
    necessary besides the out_of_order_queue but the rest of the block
    again shouldn't be (ie., the other direction had losses).
    
    As the cacheline boundaries depend on many factors in the preceeding
    stuff, trying to align considering them doesn't make too much sense.
    
    Commented one ordering hazard.
    
    There are number of low utilized u8/16s that could be combined get 2
    bytes less in total so that the hole could be made to vanish (includes
    at least ecn_flags, urg_data, urg_mode, frto_counter, nonagle).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 18e62e3d406f..9881295f3857 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -296,10 +296,9 @@ struct tcp_sock {
 	u32	rcv_ssthresh;	/* Current window clamp			*/
 
 	u32	frto_highmark;	/* snd_nxt when RTO occurred */
-	u8	reordering;	/* Packet reordering metric.		*/
+	u16	advmss;		/* Advertised MSS			*/
 	u8	frto_counter;	/* Number of new acks after RTO */
 	u8	nonagle;	/* Disable Nagle algorithm?             */
-	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 
 /* RTT measurement */
 	u32	srtt;		/* smoothed round trip time << 3	*/
@@ -310,6 +309,10 @@ struct tcp_sock {
 
 	u32	packets_out;	/* Packets which are "in flight"	*/
 	u32	retrans_out;	/* Retransmitted packets out		*/
+
+	u16	urg_data;	/* Saved octet of OOB data and control flags */
+	u8	urg_mode;	/* In urgent mode		*/
+	u8	ecn_flags;	/* ECN status bits.			*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */
@@ -325,13 +328,24 @@ struct tcp_sock {
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
 
-	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
-
  	u32	rcv_wnd;	/* Current receiver window		*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
 	u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
+	u32	lost_out;	/* Lost packets			*/
+	u32	sacked_out;	/* SACK'd packets			*/
+	u32	fackets_out;	/* FACK'd packets			*/
+	u32	tso_deferred;
+	u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
 
-/*	SACKs data	*/
+	/* from STCP, retrans queue hinting */
+	struct sk_buff* lost_skb_hint;
+	struct sk_buff *scoreboard_skb_hint;
+	struct sk_buff *retransmit_skb_hint;
+	struct sk_buff *forward_skb_hint;
+
+	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
+
+	/* SACKs data, these 2 need to be together (see tcp_build_and_update_options) */
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
@@ -342,23 +356,14 @@ struct tcp_sock {
 					 * sacked_out > 0)
 					 */
 
-	/* from STCP, retrans queue hinting */
-	struct sk_buff* lost_skb_hint;
-
-	struct sk_buff *scoreboard_skb_hint;
-	struct sk_buff *retransmit_skb_hint;
-	struct sk_buff *forward_skb_hint;
-
 	int     lost_cnt_hint;
 	int     retransmit_cnt_hint;
 
 	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
 
-	u16	advmss;		/* Advertised MSS			*/
+	u8	reordering;	/* Packet reordering metric.		*/
+	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 	u32	prior_ssthresh; /* ssthresh saved at recovery start	*/
-	u32	lost_out;	/* Lost packets			*/
-	u32	sacked_out;	/* SACK'd packets			*/
-	u32	fackets_out;	/* FACK'd packets			*/
 	u32	high_seq;	/* snd_nxt at onset of congestion	*/
 
 	u32	retrans_stamp;	/* Timestamp of the last retransmit,
@@ -366,25 +371,18 @@ struct tcp_sock {
 				 * the first SYN. */
 	u32	undo_marker;	/* tracking retrans started here. */
 	int	undo_retrans;	/* number of undoable retransmissions. */
+	u32	total_retrans;	/* Total retransmits for entire connection */
+
 	u32	urg_seq;	/* Seq of received urgent pointer */
-	u16	urg_data;	/* Saved octet of OOB data and control flags */
-	u8	urg_mode;	/* In urgent mode		*/
-	u8	ecn_flags;	/* ECN status bits.			*/
 	u32	snd_up;		/* Urgent pointer		*/
 
-	u32	total_retrans;	/* Total retransmits for entire connection */
-	u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
-
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
-	int			linger2;
 
 	struct tcp_deferred_accept_info defer_tcp_accept;
 
 	unsigned long last_synq_overflow; 
 
-	u32	tso_deferred;
-
 /* Receiver side RTT estimation */
 	struct {
 		u32	rtt;
@@ -412,6 +410,8 @@ struct tcp_sock {
 /* TCP MD5 Signagure Option information */
 	struct tcp_md5sig_info	*md5sig_info;
 #endif
+
+	int			linger2;
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)

commit 4b749440445ebcb6fad402fc762bc35af871f689
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Wed May 21 17:40:05 2008 -0700

    tcp: Make prior_ssthresh a u32
    
    If previous window was above representable values of u16,
    strange things will happen if undo with the truncated value
    is called for. Alternatively, this could be fixed by some
    max trickery but that would limit undoing high-speed undos.
    
    Adds 16-bit hole but there isn't anything to fill it with.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d96d9b122304..18e62e3d406f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -355,7 +355,7 @@ struct tcp_sock {
 	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
 
 	u16	advmss;		/* Advertised MSS			*/
-	u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
+	u32	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	u32	lost_out;	/* Lost packets			*/
 	u32	sacked_out;	/* SACK'd packets			*/
 	u32	fackets_out;	/* FACK'd packets			*/

commit ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
Author: Patrick McManus <mcmanus@ducksong.com>
Date:   Fri Mar 21 16:33:01 2008 -0700

    [TCP]: TCP_DEFER_ACCEPT updates - process as established
    
    Change TCP_DEFER_ACCEPT implementation so that it transitions a
    connection to ESTABLISHED after handshake is complete instead of
    leaving it in SYN-RECV until some data arrvies. Place connection in
    accept queue when first data packet arrives from slow path.
    
    Benefits:
      - established connection is now reset if it never makes it
       to the accept queue
    
     - diagnostic state of established matches with the packet traces
       showing completed handshake
    
     - TCP_DEFER_ACCEPT timeouts are expressed in seconds and can now be
       enforced with reasonable accuracy instead of rounding up to next
       exponential back-off of syn-ack retry.
    
    Signed-off-by: Patrick McManus <mcmanus@ducksong.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 08027f1d7f31..d96d9b122304 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -239,6 +239,11 @@ static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 	return (struct tcp_request_sock *)req;
 }
 
+struct tcp_deferred_accept_info {
+	struct sock *listen_sk;
+	struct request_sock *request;
+};
+
 struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
@@ -374,6 +379,8 @@ struct tcp_sock {
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
 	int			linger2;
 
+	struct tcp_deferred_accept_info defer_tcp_accept;
+
 	unsigned long last_synq_overflow; 
 
 	u32	tso_deferred;

commit 68f8353b480e5f2e136c38a511abdbb88eaa8ce2
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Nov 15 19:50:37 2007 -0800

    [TCP]: Rewrite SACK block processing & sack_recv_cache use
    
    Key points of this patch are:
    
      - In case new SACK information is advance only type, no skb
        processing below previously discovered highest point is done
      - Optimize cases below highest point too since there's no need
        to always go up to highest point (which is very likely still
        present in that SACK), this is not entirely true though
        because I'm dropping the fastpath_skb_hint which could
        previously optimize those cases even better. Whether that's
        significant, I'm not too sure.
    
    Currently it will provide skipping by walking. Combined with
    RB-tree, all skipping would become fast too regardless of window
    size (can be done incrementally later).
    
    Previously a number of cases in TCP SACK processing fails to
    take advantage of costly stored information in sack_recv_cache,
    most importantly, expected events such as cumulative ACK and new
    hole ACKs. Processing on such ACKs result in rather long walks
    building up latencies (which easily gets nasty when window is
    huge). Those latencies are often completely unnecessary
    compared with the amount of _new_ information received, usually
    for cumulative ACK there's no new information at all, yet TCP
    walks whole queue unnecessary potentially taking a number of
    costly cache misses on the way, etc.!
    
    Since the inclusion of highest_sack, there's a lot information
    that is very likely redundant (SACK fastpath hint stuff,
    fackets_out, highest_sack), though there's no ultimate guarantee
    that they'll remain the same whole the time (in all unearthly
    scenarios). Take advantage of this knowledge here and drop
    fastpath hint and use direct access to highest SACKed skb as
    a replacement.
    
    Effectively "special cased" fastpath is dropped. This change
    adds some complexity to introduce better coveraged "fastpath",
    though the added complexity should make TCP behave more cache
    friendly.
    
    The current ACK's SACK blocks are compared against each cached
    block individially and only ranges that are new are then scanned
    by the high constant walk. For other parts of write queue, even
    when in previously known part of the SACK blocks, a faster skip
    function is used (if necessary at all). In addition, whenever
    possible, TCP fast-forwards to highest_sack skb that was made
    available by an earlier patch. In typical case, no other things
    but this fast-forward and mandatory markings after that occur
    making the access pattern quite similar to the former fastpath
    "special case".
    
    DSACKs are special case that must always be walked.
    
    The local to recv_sack_cache copying could be more intelligent
    w.r.t DSACKs which are likely to be there only once but that
    is left to a separate patch.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 794497c7d755..08027f1d7f31 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -343,10 +343,7 @@ struct tcp_sock {
 	struct sk_buff *scoreboard_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
 	struct sk_buff *forward_skb_hint;
-	struct sk_buff *fastpath_skb_hint;
 
-	int     fastpath_cnt_hint;	/* Lags behind by current skb's pcount
-					 * compared to respective fackets_out */
 	int     lost_cnt_hint;
 	int     retransmit_cnt_hint;
 

commit fd6dad616d4fe2f08d690f25ca76b0102158fb3a
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Nov 15 19:49:47 2007 -0800

    [TCP]: Earlier SACK block verification & simplify access to them
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 34acee662230..794497c7d755 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -330,7 +330,7 @@ struct tcp_sock {
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
-	struct tcp_sack_block_wire recv_sack_cache[4];
+	struct tcp_sack_block recv_sack_cache[4];
 
 	struct sk_buff *highest_sack;   /* highest skb with SACK received
 					 * (validity guaranteed only if

commit a47e5a988a575e64c8c9bae65a1dfe3dca7cba32
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Nov 15 19:41:46 2007 -0800

    [TCP]: Convert highest_sack to sk_buff to allow direct access
    
    It is going to replace the sack fastpath hint quite soon... :-)
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index bac17c59b24e..34acee662230 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -332,8 +332,10 @@ struct tcp_sock {
 
 	struct tcp_sack_block_wire recv_sack_cache[4];
 
-	u32	highest_sack;	/* Start seq of globally highest revd SACK
-				 * (validity guaranteed only if sacked_out > 0) */
+	struct sk_buff *highest_sack;   /* highest skb with SACK received
+					 * (validity guaranteed only if
+					 * sacked_out > 0)
+					 */
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit f78a1b389288d8327db5a0f4526935b0da1d0967
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Oct 15 12:59:43 2007 -0700

    [TCP]: Make snd_cwnd_cnt 32-bit
    
    Very little point of having 32-bit snd_cnwd if this is not
    32-bit as well, as a number of snd_cwnd incrementation formulas
    assume that snd_cwnd_cnt can be at least as large as snd_cwnd.
    
    Whether 32-bit is useful was discussed when e0ef57cc56c3c96
    was made:
      http://marc.info/?l=linux-netdev&m=117218144409825&w=2
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c5b94c1a5ee2..bac17c59b24e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -315,7 +315,7 @@ struct tcp_sock {
  */
  	u32	snd_ssthresh;	/* Slow start size threshold		*/
  	u32	snd_cwnd;	/* Sending congestion window		*/
- 	u16	snd_cwnd_cnt;	/* Linear increase counter		*/
+	u32	snd_cwnd_cnt;	/* Linear increase counter		*/
 	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;

commit b08d6cb22c777c8c91c16d8e3b8aafc93c98cbd9
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Oct 11 17:36:13 2007 -0700

    [TCP]: Limit processing lost_retrans loop to work-to-do cases
    
    This addition of lost_retrans_low to tcp_sock might be
    unnecessary, it's not clear how often lost_retrans worker is
    executed when there wasn't work to do.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9ff456e8d6c7..c5b94c1a5ee2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -348,6 +348,8 @@ struct tcp_sock {
 	int     lost_cnt_hint;
 	int     retransmit_cnt_hint;
 
+	u32	lost_retrans_low;	/* Sent seq after any rxmit (lowest) */
+
 	u16	advmss;		/* Advertised MSS			*/
 	u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	u32	lost_out;	/* Lost packets			*/

commit c79e3357166a2ca39fd7613b0eb7f493c1ac5e11
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun Oct 7 23:37:25 2007 -0700

    [TCP]: Comment fastpath_cnt_hint off-by-one trap
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f8cf090e8f49..9ff456e8d6c7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -343,7 +343,8 @@ struct tcp_sock {
 	struct sk_buff *forward_skb_hint;
 	struct sk_buff *fastpath_skb_hint;
 
-	int     fastpath_cnt_hint;
+	int     fastpath_cnt_hint;	/* Lags behind by current skb's pcount
+					 * compared to respective fackets_out */
 	int     lost_cnt_hint;
 	int     retransmit_cnt_hint;
 

commit 13dae426318aae073028a4b3bd493104a991e800
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Aug 10 14:31:21 2007 -0700

    [TCP]: Update comment about highest_sack validity
    
    This stale info came from the original idea, which proved to be
    unnecessarily complex, sacked_out > 0 is easy to do and that when
    it's going to be needed anyway (it _can_ be valid also when
    sacked_out == 0 but there's not going to be a guarantee about it
    for now).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 1f12fa0b67d7..f8cf090e8f49 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -332,7 +332,8 @@ struct tcp_sock {
 
 	struct tcp_sack_block_wire recv_sack_cache[4];
 
-	u32	highest_sack;	/* Start seq of globally highest revd SACK (valid only in slowpath) */
+	u32	highest_sack;	/* Start seq of globally highest revd SACK
+				 * (validity guaranteed only if sacked_out > 0) */
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit b5860bbac7be1381626f3dc8a0cb970a60fcefb4
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 14:33:18 2007 +0300

    [TCP]: Tighten tcp_sock's belt, drop left_out
    
    It is easily calculable when needed and user are not that many
    after all.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index d64734389fb6..1f12fa0b67d7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -304,7 +304,6 @@ struct tcp_sock {
 	u32	rtt_seq;	/* sequence number to update rttvar	*/
 
 	u32	packets_out;	/* Packets which are "in flight"	*/
-	u32	left_out;	/* Packets which leaved network	*/
 	u32	retrans_out;	/* Retransmitted packets out		*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).

commit 539d243fdd7900fa5a544c7c154dc3ddf627e840
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun May 27 02:03:20 2007 -0700

    [TCP]: Access to highest_sack obsoletes forward_cnt_hint
    
    In addition, added a reference about the purpose of the loop.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c072f88afb97..d64734389fb6 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -346,7 +346,6 @@ struct tcp_sock {
 	int     fastpath_cnt_hint;
 	int     lost_cnt_hint;
 	int     retransmit_cnt_hint;
-	int     forward_cnt_hint;
 
 	u16	advmss;		/* Advertised MSS			*/
 	u16	prior_ssthresh; /* ssthresh saved at recovery start	*/

commit d738cd8fca948e45d53120247cb7a5f5be3ca09e
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Mar 24 21:03:23 2007 -0700

    [TCP]: Add highest_sack seqno, points to globally highest SACK
    
    It is guaranteed to be valid only when !tp->sacked_out. In most
    cases this seqno is available in the last ACK but there is no
    guarantee for that. The new fast recovery loss marking algorithm
    needs this as entry point.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index c6b9f92e8289..c072f88afb97 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -333,6 +333,8 @@ struct tcp_sock {
 
 	struct tcp_sack_block_wire recv_sack_cache[4];
 
+	u32	highest_sack;	/* Start seq of globally highest revd SACK (valid only in slowpath) */
+
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;
 

commit 9c70220b73908f64792422a2c39c593c4792f2c5
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Apr 25 18:04:18 2007 -0700

    [SK_BUFF]: Introduce skb_transport_header(skb)
    
    For the places where we need a pointer to the transport header, it is
    still legal to touch skb->h.raw directly if just adding to,
    subtracting from or setting it to another layer header.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 911d937fb4c1..c6b9f92e8289 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -180,7 +180,7 @@ struct tcp_md5sig {
 
 static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
 {
-	return (struct tcphdr *)skb->h.raw;
+	return (struct tcphdr *)skb_transport_header(skb);
 }
 
 static inline unsigned int tcp_hdrlen(const struct sk_buff *skb)

commit aa8223c7bb0b05183e1737881ed21827aa5b9e73
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Apr 10 21:04:22 2007 -0700

    [SK_BUFF]: Introduce tcp_hdr(), remove skb->h.th
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 244ae0dacf4a..911d937fb4c1 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -178,14 +178,19 @@ struct tcp_md5sig {
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
 
+static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
+{
+	return (struct tcphdr *)skb->h.raw;
+}
+
 static inline unsigned int tcp_hdrlen(const struct sk_buff *skb)
 {
-	return skb->h.th->doff * 4;
+	return tcp_hdr(skb)->doff * 4;
 }
 
 static inline unsigned int tcp_optlen(const struct sk_buff *skb)
 {
-	return (skb->h.th->doff - 5) * 4;
+	return (tcp_hdr(skb)->doff - 5) * 4;
 }
 
 /* This defines a selective acknowledgement block. */

commit ab6a5bb6b28a970104a34f0f6959b73cf61bdc72
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Mar 18 17:43:48 2007 -0700

    [TCP]: Introduce tcp_hdrlen() and tcp_optlen()
    
    The ip_hdrlen() buddy, created to reduce the number of skb->h.th-> uses and to
    avoid the longer, open coded equivalent.
    
    Ditched a no-op in bnx2 in the process.
    
    I wonder if we should have a BUG_ON(skb->h.th->doff < 5) in tcp_optlen()...
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 18a468dd5055..244ae0dacf4a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -178,6 +178,16 @@ struct tcp_md5sig {
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
 
+static inline unsigned int tcp_hdrlen(const struct sk_buff *skb)
+{
+	return skb->h.th->doff * 4;
+}
+
+static inline unsigned int tcp_optlen(const struct sk_buff *skb)
+{
+	return (skb->h.th->doff - 5) * 4;
+}
+
 /* This defines a selective acknowledgement block. */
 struct tcp_sack_block_wire {
 	__be32	start_seq;

commit e0ef57cc56c3c96493f9b0d6c77bb9608eeaa173
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Thu Feb 22 22:52:59 2007 -0800

    [TCP]: Make snd_cwnd_clamp a u32.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 415193e171e4..18a468dd5055 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -302,7 +302,7 @@ struct tcp_sock {
  	u32	snd_ssthresh;	/* Slow start size threshold		*/
  	u32	snd_cwnd;	/* Sending congestion window		*/
  	u16	snd_cwnd_cnt;	/* Linear increase counter		*/
-	u16	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
+	u32	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 	u32	snd_cwnd_used;
 	u32	snd_cwnd_stamp;
 

commit 54287cc178cf85dbae0decec8b4dc190bff757ad
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Feb 22 03:20:44 2007 -0800

    [TCP]: Keep copied_seq, rcv_wup and rcv_next together.
    
    I noticed in oprofile study a cache miss in tcp_rcv_established() to read
    copied_seq.
    
    ffffffff80400a80 <tcp_rcv_established>: /* tcp_rcv_established total: 4034293  
    2.0400 */
    
     55493  0.0281 :ffffffff80400bc9:   mov    0x4c8(%r12),%eax copied_seq
    543103  0.2746 :ffffffff80400bd1:   cmp    0x3e0(%r12),%eax   rcv_nxt    
    
    if (tp->copied_seq == tp->rcv_nxt &&
            len - tcp_header_len <= tp->ucopy.len) {
    
    In this function, the cache line 0x4c0 -> 0x500 is used only for this
    reading 'copied_seq' field.
    
    rcv_wup and copied_seq should be next to rcv_nxt field, to lower number of
    active cache lines in hot paths. (tcp_rcv_established(), tcp_poll(), ...)
    
    As you suggested, I changed tcp_create_openreq_child() so that these fields
    are changed together, to avoid adding a new store buffer stall.
    
    Patch is 64bit friendly (no new hole because of alignment constraints)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 29d3089038ab..415193e171e4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -242,6 +242,8 @@ struct tcp_sock {
  *	See RFC793 and RFC1122. The RFC writes these in capitals.
  */
  	u32	rcv_nxt;	/* What we want to receive next 	*/
+	u32	copied_seq;	/* Head of yet unread data		*/
+	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
  	u32	snd_nxt;	/* Next sequence we send		*/
 
  	u32	snd_una;	/* First byte we want an ack for	*/
@@ -307,10 +309,8 @@ struct tcp_sock {
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 
  	u32	rcv_wnd;	/* Current receiver window		*/
-	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
 	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
 	u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
-	u32	copied_seq;	/* Head of yet unread data		*/
 
 /*	SACKs data	*/
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */

commit 6f74651ae626ec672028587bc700538076dfbefb
Author: Baruch Even <baruch@ev-en.org>
Date:   Sun Feb 4 23:36:42 2007 -0800

    [TCP]: Seperate DSACK from SACK fast path
    
    Move DSACK code outside the SACK fast-path checking code. If the DSACK
    determined that the information was too old we stayed with a partial cache
    copied. Most likely this matters very little since the next packet will not be
    DSACK and we will find it in the cache. but it's still not good form and there
    is little reason to couple the two checks.
    
    Since the SACK receive cache doesn't need the data to be in host order we also
    remove the ntohl in the checking loop.
    
    Signed-off-by: Baruch Even <baruch@ev-en.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3cc70d1a3504..29d3089038ab 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -316,7 +316,7 @@ struct tcp_sock {
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
-	struct tcp_sack_block recv_sack_cache[4];
+	struct tcp_sack_block_wire recv_sack_cache[4];
 
 	/* from STCP, retrans queue hinting */
 	struct sk_buff* lost_skb_hint;

commit 3a137d2065571864be0301b9ebd72ddb01060997
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Nov 28 01:12:38 2006 -0200

    [TCP]: Renove the __ prefix on the struct tcp_sock members
    
    As this struct is not userland visible at all.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b42ff0efc2df..3cc70d1a3504 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -185,17 +185,17 @@ struct tcp_sack_block_wire {
 };
 
 struct tcp_sack_block {
-	__u32	start_seq;
-	__u32	end_seq;
+	u32	start_seq;
+	u32	end_seq;
 };
 
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
 	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
-	__u32	ts_recent;	/* Time stamp to echo next		*/
-	__u32	rcv_tsval;	/* Time stamp value             	*/
-	__u32	rcv_tsecr;	/* Time stamp echo reply        	*/
-	__u16 	saw_tstamp : 1,	/* Saw TIMESTAMP on last packet		*/
+	u32	ts_recent;	/* Time stamp to echo next		*/
+	u32	rcv_tsval;	/* Time stamp value             	*/
+	u32	rcv_tsecr;	/* Time stamp echo reply        	*/
+	u16 	saw_tstamp : 1,	/* Saw TIMESTAMP on last packet		*/
 		tstamp_ok : 1,	/* TIMESTAMP seen on SYN packet		*/
 		dsack : 1,	/* D-SACK is scheduled			*/
 		wscale_ok : 1,	/* Wscale seen on SYN packet		*/
@@ -203,10 +203,10 @@ struct tcp_options_received {
 		snd_wscale : 4,	/* Window scaling received from sender	*/
 		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
 /*	SACKs data	*/
-	__u8	eff_sacks;	/* Size of SACK array to send with next packet */
-	__u8	num_sacks;	/* Number of SACK blocks		*/
-	__u16	user_mss;  	/* mss requested by user in ioctl */
-	__u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
+	u8	eff_sacks;	/* Size of SACK array to send with next packet */
+	u8	num_sacks;	/* Number of SACK blocks		*/
+	u16	user_mss;  	/* mss requested by user in ioctl */
+	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
 struct tcp_request_sock {
@@ -215,8 +215,8 @@ struct tcp_request_sock {
 	/* Only used by TCP MD5 Signature so far. */
 	struct tcp_request_sock_ops	*af_specific;
 #endif
-	__u32			 	rcv_isn;
-	__u32			 	snt_isn;
+	u32			 	rcv_isn;
+	u32			 	snt_isn;
 };
 
 static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
@@ -241,13 +241,13 @@ struct tcp_sock {
  *	read the code and the spec side by side (and laugh ...)
  *	See RFC793 and RFC1122. The RFC writes these in capitals.
  */
- 	__u32	rcv_nxt;	/* What we want to receive next 	*/
- 	__u32	snd_nxt;	/* Next sequence we send		*/
+ 	u32	rcv_nxt;	/* What we want to receive next 	*/
+ 	u32	snd_nxt;	/* Next sequence we send		*/
 
- 	__u32	snd_una;	/* First byte we want an ack for	*/
- 	__u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
-	__u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
-	__u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
+ 	u32	snd_una;	/* First byte we want an ack for	*/
+ 	u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
+	u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
+	u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
 
 	/* Data for direct copy to user */
 	struct {
@@ -265,30 +265,30 @@ struct tcp_sock {
 #endif
 	} ucopy;
 
-	__u32	snd_wl1;	/* Sequence for window update		*/
-	__u32	snd_wnd;	/* The window we expect to receive	*/
-	__u32	max_window;	/* Maximal window ever seen from peer	*/
-	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
+	u32	snd_wl1;	/* Sequence for window update		*/
+	u32	snd_wnd;	/* The window we expect to receive	*/
+	u32	max_window;	/* Maximal window ever seen from peer	*/
+	u32	mss_cache;	/* Cached effective mss, not including SACKS */
 
-	__u32	window_clamp;	/* Maximal window to advertise		*/
-	__u32	rcv_ssthresh;	/* Current window clamp			*/
+	u32	window_clamp;	/* Maximal window to advertise		*/
+	u32	rcv_ssthresh;	/* Current window clamp			*/
 
-	__u32	frto_highmark;	/* snd_nxt when RTO occurred */
-	__u8	reordering;	/* Packet reordering metric.		*/
-	__u8	frto_counter;	/* Number of new acks after RTO */
-	__u8	nonagle;	/* Disable Nagle algorithm?             */
-	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
+	u32	frto_highmark;	/* snd_nxt when RTO occurred */
+	u8	reordering;	/* Packet reordering metric.		*/
+	u8	frto_counter;	/* Number of new acks after RTO */
+	u8	nonagle;	/* Disable Nagle algorithm?             */
+	u8	keepalive_probes; /* num of allowed keep alive probes	*/
 
 /* RTT measurement */
-	__u32	srtt;		/* smoothed round trip time << 3	*/
-	__u32	mdev;		/* medium deviation			*/
-	__u32	mdev_max;	/* maximal mdev for the last rtt period	*/
-	__u32	rttvar;		/* smoothed mdev_max			*/
-	__u32	rtt_seq;	/* sequence number to update rttvar	*/
-
-	__u32	packets_out;	/* Packets which are "in flight"	*/
-	__u32	left_out;	/* Packets which leaved network	*/
-	__u32	retrans_out;	/* Retransmitted packets out		*/
+	u32	srtt;		/* smoothed round trip time << 3	*/
+	u32	mdev;		/* medium deviation			*/
+	u32	mdev_max;	/* maximal mdev for the last rtt period	*/
+	u32	rttvar;		/* smoothed mdev_max			*/
+	u32	rtt_seq;	/* sequence number to update rttvar	*/
+
+	u32	packets_out;	/* Packets which are "in flight"	*/
+	u32	left_out;	/* Packets which leaved network	*/
+	u32	retrans_out;	/* Retransmitted packets out		*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */
@@ -297,20 +297,20 @@ struct tcp_sock {
 /*
  *	Slow start and congestion control (see also Nagle, and Karn & Partridge)
  */
- 	__u32	snd_ssthresh;	/* Slow start size threshold		*/
- 	__u32	snd_cwnd;	/* Sending congestion window		*/
- 	__u16	snd_cwnd_cnt;	/* Linear increase counter		*/
-	__u16	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
-	__u32	snd_cwnd_used;
-	__u32	snd_cwnd_stamp;
+ 	u32	snd_ssthresh;	/* Slow start size threshold		*/
+ 	u32	snd_cwnd;	/* Sending congestion window		*/
+ 	u16	snd_cwnd_cnt;	/* Linear increase counter		*/
+	u16	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
+	u32	snd_cwnd_used;
+	u32	snd_cwnd_stamp;
 
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 
- 	__u32	rcv_wnd;	/* Current receiver window		*/
-	__u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
-	__u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
-	__u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
-	__u32	copied_seq;	/* Head of yet unread data		*/
+ 	u32	rcv_wnd;	/* Current receiver window		*/
+	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
+	u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
+	u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
+	u32	copied_seq;	/* Head of yet unread data		*/
 
 /*	SACKs data	*/
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
@@ -331,26 +331,26 @@ struct tcp_sock {
 	int     retransmit_cnt_hint;
 	int     forward_cnt_hint;
 
-	__u16	advmss;		/* Advertised MSS			*/
-	__u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
-	__u32	lost_out;	/* Lost packets			*/
-	__u32	sacked_out;	/* SACK'd packets			*/
-	__u32	fackets_out;	/* FACK'd packets			*/
-	__u32	high_seq;	/* snd_nxt at onset of congestion	*/
+	u16	advmss;		/* Advertised MSS			*/
+	u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
+	u32	lost_out;	/* Lost packets			*/
+	u32	sacked_out;	/* SACK'd packets			*/
+	u32	fackets_out;	/* FACK'd packets			*/
+	u32	high_seq;	/* snd_nxt at onset of congestion	*/
 
-	__u32	retrans_stamp;	/* Timestamp of the last retransmit,
+	u32	retrans_stamp;	/* Timestamp of the last retransmit,
 				 * also used in SYN-SENT to remember stamp of
 				 * the first SYN. */
-	__u32	undo_marker;	/* tracking retrans started here. */
+	u32	undo_marker;	/* tracking retrans started here. */
 	int	undo_retrans;	/* number of undoable retransmissions. */
-	__u32	urg_seq;	/* Seq of received urgent pointer */
-	__u16	urg_data;	/* Saved octet of OOB data and control flags */
-	__u8	urg_mode;	/* In urgent mode		*/
-	__u8	ecn_flags;	/* ECN status bits.			*/
-	__u32	snd_up;		/* Urgent pointer		*/
+	u32	urg_seq;	/* Seq of received urgent pointer */
+	u16	urg_data;	/* Saved octet of OOB data and control flags */
+	u8	urg_mode;	/* In urgent mode		*/
+	u8	ecn_flags;	/* ECN status bits.			*/
+	u32	snd_up;		/* Urgent pointer		*/
 
-	__u32	total_retrans;	/* Total retransmits for entire connection */
-	__u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
+	u32	total_retrans;	/* Total retransmits for entire connection */
+	u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
 
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
@@ -358,26 +358,26 @@ struct tcp_sock {
 
 	unsigned long last_synq_overflow; 
 
-	__u32	tso_deferred;
+	u32	tso_deferred;
 
 /* Receiver side RTT estimation */
 	struct {
-		__u32	rtt;
-		__u32	seq;
-		__u32	time;
+		u32	rtt;
+		u32	seq;
+		u32	time;
 	} rcv_rtt_est;
 
 /* Receiver queue space */
 	struct {
 		int	space;
-		__u32	seq;
-		__u32	time;
+		u32	seq;
+		u32	time;
 	} rcvq_space;
 
 /* TCP-specific MTU probe information. */
 	struct {
-		__u32		  probe_seq_start;
-		__u32		  probe_seq_end;
+		u32		  probe_seq_start;
+		u32		  probe_seq_end;
 	} mtu_probe;
 
 #ifdef CONFIG_TCP_MD5SIG
@@ -396,14 +396,14 @@ static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 
 struct tcp_timewait_sock {
 	struct inet_timewait_sock tw_sk;
-	__u32			  tw_rcv_nxt;
-	__u32			  tw_snd_nxt;
-	__u32			  tw_rcv_wnd;
-	__u32			  tw_ts_recent;
+	u32			  tw_rcv_nxt;
+	u32			  tw_snd_nxt;
+	u32			  tw_rcv_wnd;
+	u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
 #ifdef CONFIG_TCP_MD5SIG
-	__u16			  tw_md5_keylen;
-	__u8			  tw_md5_key[TCP_MD5SIG_MAXKEYLEN];
+	u16			  tw_md5_keylen;
+	u8			  tw_md5_key[TCP_MD5SIG_MAXKEYLEN];
 #endif
 };
 

commit 2ff52f282cf287d60e9eda1f3b5ec83e00a86130
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Nov 28 00:48:32 2006 -0200

    [TCP]: Change tcp_header_len member in tcp_sock to u16
    
    With this we eliminate the last hole in struct tcp_sock.
    
    End result:
    
    [acme@newtoy net-2.6.20]$ codiff -sV /tmp/tcp.o.before net/ipv4/tcp.o
    /pub/scm/linux/kernel/git/acme/net-2.6.20/net/ipv4/tcp.c:
      struct tcp_sock |   -4
        tcp_header_len;
         from: int                   /*  1000(0)     4(0) */
         to:   u16                   /*  1000(0)     2(0) */
     1 struct changed
    [acme@newtoy net-2.6.20]$
    
    Now sizeof(tcp_sock) is just...
    
    [acme@newtoy net-2.6.20]$ pahole --sizes ../OUTPUT/qemu/net-2.6.20/net/ipv4/tcp.o | grep -w tcp_sock
    struct tcp_sock: 1500 0
    
    1500 bytes ;-)
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index dd61b172ac68..b42ff0efc2df 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -227,7 +227,8 @@ static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 struct tcp_sock {
 	/* inet_connection_sock has to be the first member of tcp_sock */
 	struct inet_connection_sock	inet_conn;
-	int	tcp_header_len;	/* Bytes of tcp header to send		*/
+	u16	tcp_header_len;	/* Bytes of tcp header to send		*/
+	u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
 
 /*
  *	Header prediction flags
@@ -268,8 +269,6 @@ struct tcp_sock {
 	__u32	snd_wnd;	/* The window we expect to receive	*/
 	__u32	max_window;	/* Maximal window ever seen from peer	*/
 	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
-	__u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
-	/* XXX Two bytes hole, try to pack */
 
 	__u32	window_clamp;	/* Maximal window to advertise		*/
 	__u32	rcv_ssthresh;	/* Current window clamp			*/

commit 9981a0e36a572e9fcf84bfab915fdc93bed0e3c9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 14 21:24:30 2006 -0800

    [NET]: Annotate checksums in on-the-wire packets.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0aecfc955591..dd61b172ac68 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -52,7 +52,7 @@ struct tcphdr {
 #error	"Adjust your <asm/byteorder.h> defines"
 #endif	
 	__be16	window;
-	__be16	check;
+	__sum16	check;
 	__be16	urg_ptr;
 };
 

commit cfb6eeb4c860592edd123fdea908d23c6ad1c7dc
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Tue Nov 14 19:07:45 2006 -0800

    [TCP]: MD5 Signature Option (RFC2385) support.
    
    Based on implementation by Rick Payne.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 2d36f6db3706..0aecfc955591 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -19,6 +19,7 @@
 
 #include <linux/types.h>
 #include <asm/byteorder.h>
+#include <linux/socket.h>
 
 struct tcphdr {
 	__be16	source;
@@ -94,6 +95,7 @@ enum {
 #define TCP_INFO		11	/* Information about this connection. */
 #define TCP_QUICKACK		12	/* Block/reenable quick acks */
 #define TCP_CONGESTION		13	/* Congestion control algorithm */
+#define TCP_MD5SIG		14	/* TCP MD5 Signature (RFC2385) */
 
 #define TCPI_OPT_TIMESTAMPS	1
 #define TCPI_OPT_SACK		2
@@ -157,6 +159,17 @@ struct tcp_info
 	__u32	tcpi_total_retrans;
 };
 
+/* for TCP_MD5SIG socket option */
+#define TCP_MD5SIG_MAXKEYLEN	80
+
+struct tcp_md5sig {
+	struct __kernel_sockaddr_storage tcpm_addr;	/* address associated */
+	__u16	__tcpm_pad1;				/* zero */
+	__u16	tcpm_keylen;				/* key length */
+	__u32	__tcpm_pad2;				/* zero */
+	__u8	tcpm_key[TCP_MD5SIG_MAXKEYLEN];		/* key (binary) */
+};
+
 #ifdef __KERNEL__
 
 #include <linux/skbuff.h>
@@ -197,9 +210,13 @@ struct tcp_options_received {
 };
 
 struct tcp_request_sock {
-	struct inet_request_sock req;
-	__u32			 rcv_isn;
-	__u32			 snt_isn;
+	struct inet_request_sock 	req;
+#ifdef CONFIG_TCP_MD5SIG
+	/* Only used by TCP MD5 Signature so far. */
+	struct tcp_request_sock_ops	*af_specific;
+#endif
+	__u32			 	rcv_isn;
+	__u32			 	snt_isn;
 };
 
 static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
@@ -363,6 +380,14 @@ struct tcp_sock {
 		__u32		  probe_seq_start;
 		__u32		  probe_seq_end;
 	} mtu_probe;
+
+#ifdef CONFIG_TCP_MD5SIG
+/* TCP AF-Specific parts; only used by MD5 Signature support so far */
+	struct tcp_sock_af_ops	*af_specific;
+
+/* TCP MD5 Signagure Option information */
+	struct tcp_md5sig_info	*md5sig_info;
+#endif
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
@@ -377,6 +402,10 @@ struct tcp_timewait_sock {
 	__u32			  tw_rcv_wnd;
 	__u32			  tw_ts_recent;
 	long			  tw_ts_recent_stamp;
+#ifdef CONFIG_TCP_MD5SIG
+	__u16			  tw_md5_keylen;
+	__u8			  tw_md5_key[TCP_MD5SIG_MAXKEYLEN];
+#endif
 };
 
 static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)

commit ae8064ac32d07f609114d73928cdef803be87134
Author: John Heffner <jheffner@psc.edu>
Date:   Wed Oct 18 20:36:48 2006 -0700

    [TCP]: Bound TSO defer time
    
    This patch limits the amount of time you will defer sending a TSO segment
    to less than two clock ticks, or the time between two acks, whichever is
    longer.
    
    On slow links, deferring causes significant bursts.  See attached plots,
    which show RTT through a 1 Mbps link with a 100 ms RTT and ~100 ms queue
    for (a) non-TSO, (b) currnet TSO, and (c) patched TSO.  This burstiness
    causes significant jitter, tends to overflow queues early (bad for short
    queues), and makes delay-based congestion control more difficult.
    
    Deferring by a couple clock ticks I believe will have a relatively small
    impact on performance.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0e058a2d1c6d..2d36f6db3706 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -342,6 +342,8 @@ struct tcp_sock {
 
 	unsigned long last_synq_overflow; 
 
+	__u32	tso_deferred;
+
 /* Receiver side RTT estimation */
 	struct {
 		__u32	rtt;

commit dddc93c05d7dba60b44866486502c155e96ab915
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Sep 27 18:32:46 2006 -0700

    [TCP]: struct tcp_sock .pred_flags is net-endian
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 9632aa866de4..0e058a2d1c6d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -216,7 +216,7 @@ struct tcp_sock {
  *	Header prediction flags
  *	0x5?10 << 16 + snd_wnd in net byte order
  */
-	__u32	pred_flags;
+	__be32	pred_flags;
 
 /*
  *	RFC793 variables by their proper names. This means you can

commit 269bd27e66037a7932cee6d6aa7ef7defd0bfe38
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Sep 27 18:32:28 2006 -0700

    [TCP]: struct tcp_sack_block annotations
    
    Some of the instances of tcp_sack_block are host-endian, some - net-endian.
    Define struct tcp_sack_block_wire identical to struct tcp_sack_block
    with u32 replaced with __be32; annotate uses of tcp_sack_block replacing
    net-endian ones with tcp_sack_block_wire.  Change is obviously safe since
    for cc(1) __be32 is typedefed to u32.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 543f06371840..9632aa866de4 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -166,6 +166,11 @@ struct tcp_info
 #include <net/inet_timewait_sock.h>
 
 /* This defines a selective acknowledgement block. */
+struct tcp_sack_block_wire {
+	__be32	start_seq;
+	__be32	end_seq;
+};
+
 struct tcp_sack_block {
 	__u32	start_seq;
 	__u32	end_seq;

commit 46a97324a5ebdc1e343a0223d993e79551adab0f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Sep 27 18:31:51 2006 -0700

    [IPV4]: TCP headers annotated
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 8ebf497907f8..543f06371840 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -21,10 +21,10 @@
 #include <asm/byteorder.h>
 
 struct tcphdr {
-	__u16	source;
-	__u16	dest;
-	__u32	seq;
-	__u32	ack_seq;
+	__be16	source;
+	__be16	dest;
+	__be32	seq;
+	__be32	ack_seq;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u16	res1:4,
 		doff:4,
@@ -50,9 +50,9 @@ struct tcphdr {
 #else
 #error	"Adjust your <asm/byteorder.h> defines"
 #endif	
-	__u16	window;
-	__u16	check;
-	__u16	urg_ptr;
+	__be16	window;
+	__be16	check;
+	__be16	urg_ptr;
 };
 
 /*
@@ -62,7 +62,7 @@ struct tcphdr {
  */
 union tcp_word_hdr { 
 	struct tcphdr hdr;
-	__u32 		  words[5];
+	__be32 		  words[5];
 }; 
 
 #define tcp_flag_word(tp) ( ((union tcp_word_hdr *)(tp))->words [3]) 

commit c8a553ad7f0bf943047943a758cf07017819cb3c
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Thu Jun 22 14:28:09 2006 -0700

    [TCP]: Move inclusion of <linux/dmaengine.h> to correct place in <linux/tcp.h>
    
    The new <linux/dmaengine.h> header shouldn't be included from
    the !__KERNEL__ portion of tcp.h
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 420a689c3fb4..8ebf497907f8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -18,7 +18,6 @@
 #define _LINUX_TCP_H
 
 #include <linux/types.h>
-#include <linux/dmaengine.h>
 #include <asm/byteorder.h>
 
 struct tcphdr {
@@ -161,6 +160,7 @@ struct tcp_info
 #ifdef __KERNEL__
 
 #include <linux/skbuff.h>
+#include <linux/dmaengine.h>
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>

commit cee4cca740d209bcb4b9857baa2253d5ba4e3fbe
Merge: 2edc322d420a 9348f0de2d2b
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:10:08 2006 -0700

    Merge git://git.infradead.org/hdrcleanup-2.6
    
    * git://git.infradead.org/hdrcleanup-2.6: (63 commits)
      [S390] __FD_foo definitions.
      Switch to __s32 types in joystick.h instead of C99 types for consistency.
      Add <sys/types.h> to headers included for userspace in <linux/input.h>
      Move inclusion of <linux/compat.h> out of user scope in asm-x86_64/mtrr.h
      Remove struct fddi_statistics from user view in <linux/if_fddi.h>
      Move user-visible parts of drivers/s390/crypto/z90crypt.h to include/asm-s390
      Revert include/media changes: Mauro says those ioctls are only used in-kernel(!)
      Include <linux/types.h> and use __uXX types in <linux/cramfs_fs.h>
      Use __uXX types in <linux/i2o_dev.h>, include <linux/ioctl.h> too
      Remove private struct dx_hash_info from public view in <linux/ext3_fs.h>
      Include <linux/types.h> and use __uXX types in <linux/affs_hardblocks.h>
      Use __uXX types in <linux/divert.h> for struct divert_blk et al.
      Use __u32 for elf_addr_t in <asm-powerpc/elf.h>, not u32. It's user-visible.
      Remove PPP_FCS from user view in <linux/ppp_defs.h>, remove __P mess entirely
      Use __uXX types in user-visible structures in <linux/nbd.h>
      Don't use 'u32' in user-visible struct ip_conntrack_old_tuple.
      Use __uXX types for S390 DASD volume label definitions which are user-visible
      S390 BIODASDREADCMB ioctl should use __u64 not u64 type.
      Remove unneeded inclusion of <linux/time.h> from <linux/ufs_fs.h>
      Fix private integer types used in V4L2 ioctls.
      ...
    
    Manually resolve conflict in include/linux/mtd/physmap.h

commit 97fc2f0848c928c63c2ae619deee61a0b1107b69
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 17:55:33 2006 -0700

    [I/OAT]: Structure changes for TCP recv offload to I/OAT
    
    Adds an async_wait_queue and some additional fields to tcp_sock, and a
    dma_cookie_t to sk_buff.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 542d39596bd8..c90daa5da6c3 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -18,6 +18,7 @@
 #define _LINUX_TCP_H
 
 #include <linux/types.h>
+#include <linux/dmaengine.h>
 #include <asm/byteorder.h>
 
 struct tcphdr {
@@ -233,6 +234,13 @@ struct tcp_sock {
 		struct iovec		*iov;
 		int			memory;
 		int			len;
+#ifdef CONFIG_NET_DMA
+		/* members for async copy */
+		struct dma_chan		*dma_chan;
+		int			wakeup;
+		struct dma_pinned_list	*pinned_list;
+		dma_cookie_t		dma_cookie;
+#endif
 	} ucopy;
 
 	__u32	snd_wl1;	/* Sequence for window update		*/

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 542d39596bd8..a8b24eff5b5f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -159,7 +159,6 @@ struct tcp_info
 
 #ifdef __KERNEL__
 
-#include <linux/config.h>
 #include <linux/skbuff.h>
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>

commit 0e7b13685f9a06949ea3070c97c0f0085a08cd37
Author: John Heffner <jheffner@psc.edu>
Date:   Mon Mar 20 21:32:58 2006 -0800

    [TCP] mtu probing: move tcp-specific data out of inet_connection_sock
    
    This moves some TCP-specific MTU probing state out of
    inet_connection_sock back to tcp_sock.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f2bb2396853f..542d39596bd8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -343,6 +343,12 @@ struct tcp_sock {
 		__u32	seq;
 		__u32	time;
 	} rcvq_space;
+
+/* TCP-specific MTU probe information. */
+	struct {
+		__u32		  probe_seq_start;
+		__u32		  probe_seq_end;
+	} mtu_probe;
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)

commit d83d8461f902c672bc1bd8fbc6a94e19f092da97
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:26:10 2005 -0800

    [IP_SOCKGLUE]: Remove most of the tcp specific calls
    
    As DCCP needs to be called in the same spots.
    
    Now we have a member in inet_sock (is_icsk), set at sock creation time from
    struct inet_protosw->flags (if INET_PROTOSW_ICSK is set, like for TCP and
    DCCP) to see if a struct sock instance is a inet_connection_sock for places
    like the ones in ip_sockglue.c (v4 and v6) where we previously were looking if
    sk_type was SOCK_STREAM, that is insufficient because we now use the same code
    for DCCP, that has sk_type SOCK_DCCP.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index da38eea1994b..f2bb2396853f 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -238,10 +238,9 @@ struct tcp_sock {
 	__u32	snd_wl1;	/* Sequence for window update		*/
 	__u32	snd_wnd;	/* The window we expect to receive	*/
 	__u32	max_window;	/* Maximal window ever seen from peer	*/
-	__u32	pmtu_cookie;	/* Last pmtu seen by socket		*/
 	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
 	__u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
-	__u16	ext_header_len;	/* Network protocol overhead (IP/IPv6 options) */
+	/* XXX Two bytes hole, try to pack */
 
 	__u32	window_clamp;	/* Maximal window to advertise		*/
 	__u32	rcv_ssthresh;	/* Current window clamp			*/

commit 22712813620fa8e682dbfb253a60ca0131da1e07
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:25:56 2005 -0800

    [TCP]: Move the TCPF_ enum to tcp_states.h
    
    Upcoming patches will make, for instance, ip_sockglue.c need just this enum
    and not all of tcp.h.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4e1434007f44..da38eea1994b 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -55,22 +55,6 @@ struct tcphdr {
 	__u16	urg_ptr;
 };
 
-#define TCP_ACTION_FIN	(1 << 7)
-
-enum {
-  TCPF_ESTABLISHED = (1 << 1),
-  TCPF_SYN_SENT  = (1 << 2),
-  TCPF_SYN_RECV  = (1 << 3),
-  TCPF_FIN_WAIT1 = (1 << 4),
-  TCPF_FIN_WAIT2 = (1 << 5),
-  TCPF_TIME_WAIT = (1 << 6),
-  TCPF_CLOSE     = (1 << 7),
-  TCPF_CLOSE_WAIT = (1 << 8),
-  TCPF_LAST_ACK  = (1 << 9),
-  TCPF_LISTEN    = (1 << 10),
-  TCPF_CLOSING   = (1 << 11) 
-};
-
 /*
  *	The union cast uses a gcc extension to avoid aliasing problems
  *  (union is compatible to any of its members)

commit 8292a17a399ffb7c5c8b083db4ad994e090055f7
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:15:52 2005 -0800

    [ICSK]: Rename struct tcp_func to struct inet_connection_sock_af_ops
    
    And move it to struct inet_connection_sock. DCCP will use it in the
    upcoming changesets.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0e1da6602e05..4e1434007f44 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -295,8 +295,6 @@ struct tcp_sock {
 
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 
-	struct tcp_func		*af_specific;	/* Operations which are AF_INET{4,6} specific	*/
-
  	__u32	rcv_wnd;	/* Current receiver window		*/
 	__u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
 	__u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */

commit 6a438bbe68c7013a42d9c5aee5a40d7dafdbe6ec
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:14:59 2005 -0800

    [TCP]: speed up SACK processing
    
    Use "hints" to speed up the SACK processing. Various forms
    of this have been used by TCP developers (Web100, STCP, BIC)
    to avoid the 2x linear search of outstanding segments.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 737b32e52956..0e1da6602e05 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -307,6 +307,21 @@ struct tcp_sock {
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
+	struct tcp_sack_block recv_sack_cache[4];
+
+	/* from STCP, retrans queue hinting */
+	struct sk_buff* lost_skb_hint;
+
+	struct sk_buff *scoreboard_skb_hint;
+	struct sk_buff *retransmit_skb_hint;
+	struct sk_buff *forward_skb_hint;
+	struct sk_buff *fastpath_skb_hint;
+
+	int     fastpath_cnt_hint;
+	int     lost_cnt_hint;
+	int     retransmit_cnt_hint;
+	int     forward_cnt_hint;
+
 	__u16	advmss;		/* Advertised MSS			*/
 	__u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	__u32	lost_out;	/* Lost packets			*/

commit 9772efb970780aeed488c19d8b4afd46c3b484af
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:09:53 2005 -0800

    [TCP]: Appropriate Byte Count support
    
    This is an updated version of the RFC3465 ABC patch originally
    for Linux 2.6.11-rc4 by Yee-Ting Li. ABC is a way of counting
    bytes ack'd rather than packets when updating congestion control.
    
    The orignal ABC described in the RFC applied to a Reno style
    algorithm. For advanced congestion control there is little
    change after leaving slow start.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ac4ca44c75ca..737b32e52956 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -326,6 +326,7 @@ struct tcp_sock {
 	__u32	snd_up;		/* Urgent pointer		*/
 
 	__u32	total_retrans;	/* Total retransmits for entire connection */
+	__u32	bytes_acked;	/* Appropriate Byte Counting - RFC3465 */
 
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */

commit 6687e988d9aeaccad6774e6a8304f681f3ec0a03
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Wed Aug 10 04:03:31 2005 -0300

    [ICSK]: Move TCP congestion avoidance members to icsk
    
    This changeset basically moves tcp_sk()->{ca_ops,ca_state,etc} to inet_csk(),
    minimal renaming/moving done in this changeset to ease review.
    
    Most of it is just changes of struct tcp_sock * to struct sock * parameters.
    
    With this we move to a state closer to two interesting goals:
    
    1. Generalisation of net/ipv4/tcp_diag.c, becoming inet_diag.c, being used
       for any INET transport protocol that has struct inet_hashinfo and are
       derived from struct inet_connection_sock. Keeps the userspace API, that will
       just not display DCCP sockets, while newer versions of tools can support
       DCCP.
    
    2. INET generic transport pluggable Congestion Avoidance infrastructure, using
       the current TCP CA infrastructure with DCCP.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 620096840744..ac4ca44c75ca 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -258,19 +258,15 @@ struct tcp_sock {
 	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
 	__u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
 	__u16	ext_header_len;	/* Network protocol overhead (IP/IPv6 options) */
-	__u8	ca_state;	/* State of fast-retransmit machine 	*/
 
-	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
-	__u16	advmss;		/* Advertised MSS			*/
 	__u32	window_clamp;	/* Maximal window to advertise		*/
 	__u32	rcv_ssthresh;	/* Current window clamp			*/
 
 	__u32	frto_highmark;	/* snd_nxt when RTO occurred */
 	__u8	reordering;	/* Packet reordering metric.		*/
 	__u8	frto_counter;	/* Number of new acks after RTO */
-
 	__u8	nonagle;	/* Disable Nagle algorithm?             */
-	/* ONE BYTE HOLE, TRY TO PACK */
+	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
 
 /* RTT measurement */
 	__u32	srtt;		/* smoothed round trip time << 3	*/
@@ -311,8 +307,7 @@ struct tcp_sock {
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
-	__u8	probes_out;	/* unanswered 0 window probes		*/
-	__u8	ecn_flags;	/* ECN status bits.			*/
+	__u16	advmss;		/* Advertised MSS			*/
 	__u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	__u32	lost_out;	/* Lost packets			*/
 	__u32	sacked_out;	/* SACK'd packets			*/
@@ -327,7 +322,7 @@ struct tcp_sock {
 	__u32	urg_seq;	/* Seq of received urgent pointer */
 	__u16	urg_data;	/* Saved octet of OOB data and control flags */
 	__u8	urg_mode;	/* In urgent mode		*/
-	/* ONE BYTE HOLE, TRY TO PACK! */
+	__u8	ecn_flags;	/* ECN status bits.			*/
 	__u32	snd_up;		/* Urgent pointer		*/
 
 	__u32	total_retrans;	/* Total retransmits for entire connection */
@@ -351,11 +346,6 @@ struct tcp_sock {
 		__u32	seq;
 		__u32	time;
 	} rcvq_space;
-
-	/* Pluggable TCP congestion control hook */
-	struct tcp_congestion_ops *ca_ops;
-	u32	ca_priv[16];
-#define TCP_CA_PRIV_SIZE	(16*sizeof(u32))
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
@@ -377,11 +367,6 @@ static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)
 	return (struct tcp_timewait_sock *)sk;
 }
 
-static inline void *tcp_ca(const struct tcp_sock *tp)
-{
-	return (void *) tp->ca_priv;
-}
-
 #endif
 
 #endif	/* _LINUX_TCP_H */

commit 295f7324ff8d9ea58b4d3ec93b1aaa1d80e048a9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:56 2005 -0700

    [ICSK]: Introduce reqsk_queue_prune from code in tcp_synack_timer
    
    With this we're very close to getting all of the current TCP
    refactorings in my dccp-2.6 tree merged, next changeset will export
    some functions needed by the current DCCP code and then dccp-2.6.git
    will be born!
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 800930fac388..620096840744 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -270,7 +270,7 @@ struct tcp_sock {
 	__u8	frto_counter;	/* Number of new acks after RTO */
 
 	__u8	nonagle;	/* Disable Nagle algorithm?             */
-	__u8	defer_accept;	/* User waits for some data after accept() */
+	/* ONE BYTE HOLE, TRY TO PACK */
 
 /* RTT measurement */
 	__u32	srtt;		/* smoothed round trip time << 3	*/

commit 463c84b97f24010a67cd871746d6a7e4c925a5f9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:10:42 2005 -0700

    [NET]: Introduce inet_connection_sock
    
    This creates struct inet_connection_sock, moving members out of struct
    tcp_sock that are shareable with other INET connection oriented
    protocols, such as DCCP, that in my private tree already uses most of
    these members.
    
    The functions that operate on these members were renamed, using a
    inet_csk_ prefix while not being moved yet to a new file, so as to
    ease the review of these changes.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 5d295b1b3de7..800930fac388 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -177,8 +177,8 @@ struct tcp_info
 
 #include <linux/config.h>
 #include <linux/skbuff.h>
-#include <linux/ip.h>
 #include <net/sock.h>
+#include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
 
 /* This defines a selective acknowledgement block. */
@@ -219,8 +219,8 @@ static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 }
 
 struct tcp_sock {
-	/* inet_sock has to be the first member of tcp_sock */
-	struct inet_sock	inet;
+	/* inet_connection_sock has to be the first member of tcp_sock */
+	struct inet_connection_sock	inet_conn;
 	int	tcp_header_len;	/* Bytes of tcp header to send		*/
 
 /*
@@ -241,18 +241,6 @@ struct tcp_sock {
  	__u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	__u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	__u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
-	/* Delayed ACK control data */
-	struct {
-		__u8	pending;	/* ACK is pending */
-		__u8	quick;		/* Scheduled number of quick acks	*/
-		__u8	pingpong;	/* The session is interactive		*/
-		__u8	blocked;	/* Delayed ACK was blocked by socket lock*/
-		__u32	ato;		/* Predicted tick of soft clock		*/
-		unsigned long timeout;	/* Currently scheduled timeout		*/
-		__u32	lrcvtime;	/* timestamp of last received data packet*/
-		__u16	last_seg_size;	/* Size of last incoming segment	*/
-		__u16	rcv_mss;	/* MSS used for delayed ACK decisions	*/ 
-	} ack;
 
 	/* Data for direct copy to user */
 	struct {
@@ -271,8 +259,8 @@ struct tcp_sock {
 	__u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
 	__u16	ext_header_len;	/* Network protocol overhead (IP/IPv6 options) */
 	__u8	ca_state;	/* State of fast-retransmit machine 	*/
-	__u8	retransmits;	/* Number of unrecovered RTO timeouts.	*/
 
+	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
 	__u16	advmss;		/* Advertised MSS			*/
 	__u32	window_clamp;	/* Maximal window to advertise		*/
 	__u32	rcv_ssthresh;	/* Current window clamp			*/
@@ -281,7 +269,7 @@ struct tcp_sock {
 	__u8	reordering;	/* Packet reordering metric.		*/
 	__u8	frto_counter;	/* Number of new acks after RTO */
 
-	__u8	unused;
+	__u8	nonagle;	/* Disable Nagle algorithm?             */
 	__u8	defer_accept;	/* User waits for some data after accept() */
 
 /* RTT measurement */
@@ -290,19 +278,13 @@ struct tcp_sock {
 	__u32	mdev_max;	/* maximal mdev for the last rtt period	*/
 	__u32	rttvar;		/* smoothed mdev_max			*/
 	__u32	rtt_seq;	/* sequence number to update rttvar	*/
-	__u32	rto;		/* retransmit timeout			*/
 
 	__u32	packets_out;	/* Packets which are "in flight"	*/
 	__u32	left_out;	/* Packets which leaved network	*/
 	__u32	retrans_out;	/* Retransmitted packets out		*/
-	__u8	backoff;	/* backoff				*/
 /*
  *      Options received (usually on last packet, some only on SYN packets).
  */
-	__u8	nonagle;	/* Disable Nagle algorithm?             */
-	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
-
-	__u8	probes_out;	/* unanswered 0 window probes		*/
 	struct tcp_options_received rx_opt;
 
 /*
@@ -315,11 +297,6 @@ struct tcp_sock {
 	__u32	snd_cwnd_used;
 	__u32	snd_cwnd_stamp;
 
-	/* Two commonly used timers in both sender and receiver paths. */
-	unsigned long		timeout;
- 	struct timer_list	retransmit_timer;	/* Resend (no ack)	*/
- 	struct timer_list	delack_timer;		/* Ack delay 		*/
-
 	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
 
 	struct tcp_func		*af_specific;	/* Operations which are AF_INET{4,6} specific	*/
@@ -334,7 +311,7 @@ struct tcp_sock {
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
 	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
 
-	__u8	syn_retries;	/* num of allowed syn retries */
+	__u8	probes_out;	/* unanswered 0 window probes		*/
 	__u8	ecn_flags;	/* ECN status bits.			*/
 	__u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
 	__u32	lost_out;	/* Lost packets			*/
@@ -349,14 +326,12 @@ struct tcp_sock {
 	int	undo_retrans;	/* number of undoable retransmissions. */
 	__u32	urg_seq;	/* Seq of received urgent pointer */
 	__u16	urg_data;	/* Saved octet of OOB data and control flags */
-	__u8	pending;	/* Scheduled timer event	*/
 	__u8	urg_mode;	/* In urgent mode		*/
+	/* ONE BYTE HOLE, TRY TO PACK! */
 	__u32	snd_up;		/* Urgent pointer		*/
 
 	__u32	total_retrans;	/* Total retransmits for entire connection */
 
-	struct request_sock_queue accept_queue; /* FIFO of established children */
-
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
 	int			linger2;

commit 8feaf0c0a5488b3d898a9c207eb6678f44ba3f26
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:09:30 2005 -0700

    [INET]: Generalise tcp_tw_bucket, aka TIME_WAIT sockets
    
    This paves the way to generalise the rest of the sock ID lookup
    routines and saves some bytes in TCPv4 TIME_WAIT sockets on distro
    kernels (where IPv6 is always built as a module):
    
    [root@qemu ~]# grep tw_sock /proc/slabinfo
    tw_sock_TCPv6  0  0  128  31  1
    tw_sock_TCP    0  0   96  41  1
    [root@qemu ~]#
    
    Now if a protocol wants to use the TIME_WAIT generic infrastructure it
    only has to set the sk_prot->twsk_obj_size field with the size of its
    inet_timewait_sock derived sock and proto_register will create
    sk_prot->twsk_slab, for now its only for INET sockets, but we can
    introduce timewait_sock later if some non INET transport protocolo
    wants to use this stuff.
    
    Next changesets will take advantage of this new infrastructure to
    generalise even more TCP code.
    
    [acme@toy net-2.6.14]$ grep built-in /tmp/before.size /tmp/after.size
    /tmp/before.size: 188646   11764    5068  205478   322a6 net/ipv4/built-in.o
    /tmp/after.size:  188144   11764    5068  204976   320b0 net/ipv4/built-in.o
    [acme@toy net-2.6.14]$
    
    Tested with both IPv4 & IPv6 (::1 (localhost) & ::ffff:172.20.0.1
    (qemu host)).
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index b88fe05fdcbf..5d295b1b3de7 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -179,6 +179,7 @@ struct tcp_info
 #include <linux/skbuff.h>
 #include <linux/ip.h>
 #include <net/sock.h>
+#include <net/inet_timewait_sock.h>
 
 /* This defines a selective acknowledgement block. */
 struct tcp_sack_block {
@@ -387,6 +388,20 @@ static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 	return (struct tcp_sock *)sk;
 }
 
+struct tcp_timewait_sock {
+	struct inet_timewait_sock tw_sk;
+	__u32			  tw_rcv_nxt;
+	__u32			  tw_snd_nxt;
+	__u32			  tw_rcv_wnd;
+	__u32			  tw_ts_recent;
+	long			  tw_ts_recent_stamp;
+};
+
+static inline struct tcp_timewait_sock *tcp_twsk(const struct sock *sk)
+{
+	return (struct tcp_timewait_sock *)sk;
+}
+
 static inline void *tcp_ca(const struct tcp_sock *tp)
 {
 	return (void *) tp->ca_priv;

commit c752f0739f09b803aed191c4765a3b6650a08653
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:08:28 2005 -0700

    [TCP]: Move the tcp sock states to net/tcp_states.h
    
    Lots of places just needs the states, not even linux/tcp.h, where this
    enum was, needs it.
    
    This speeds up development of the refactorings as less sources are
    rebuilt when things get moved from net/tcp.h.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e70ab19652db..b88fe05fdcbf 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -55,24 +55,6 @@ struct tcphdr {
 	__u16	urg_ptr;
 };
 
-
-enum {
-  TCP_ESTABLISHED = 1,
-  TCP_SYN_SENT,
-  TCP_SYN_RECV,
-  TCP_FIN_WAIT1,
-  TCP_FIN_WAIT2,
-  TCP_TIME_WAIT,
-  TCP_CLOSE,
-  TCP_CLOSE_WAIT,
-  TCP_LAST_ACK,
-  TCP_LISTEN,
-  TCP_CLOSING,	 /* now a valid state */
-
-  TCP_MAX_STATES /* Leave at the end! */
-};
-
-#define TCP_STATE_MASK	0xF
 #define TCP_ACTION_FIN	(1 << 7)
 
 enum {

commit a55ebcc4c4532107ad9eee1c9bb698ab5f12c00f
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:01:14 2005 -0700

    [INET]: Move bind_hash from tcp_sk to inet_sk
    
    This should really be in a inet_connection_sock, but I'm leaving it
    for a later optimization, when some more fields common to INET
    transport protocols now in tcp_sk or inet_sk will be chunked out into
    inet_connection_sock, for now its better to concentrate on getting the
    changes in the core merged to leave the DCCP tree with only DCCP
    specific code.
    
    Next changesets will take advantage of this move to generalise things
    like tcp_bind_hash, tcp_put_port, tcp_inherit_port, making the later
    receive a inet_hashinfo parameter, and even __tcp_tw_hashdance, etc in
    the future, when tcp_tw_bucket gets transformed into the struct
    timewait_sock hierarchy.
    
    tcp_destroy_sock also is eligible as soon as tcp_orphan_count gets
    moved to sk_prot.
    
    A cascade of incremental changes will ultimately make the tcp_lookup
    functions be fully generic.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index ec580a560e8c..e70ab19652db 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -258,7 +258,6 @@ struct tcp_sock {
  	__u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	__u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	__u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
-	struct inet_bind_bucket *bind_hash;
 	/* Delayed ACK control data */
 	struct {
 		__u8	pending;	/* ACK is pending */

commit 0f7ff9274e72fd254fbd1ab117bbc1db6e7cdb34
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:59:44 2005 -0700

    [INET]: Just rename the TCP hashtable functions/structs to inet_
    
    This is to break down the complexity of the series of patches,
    making it very clear that this one just does:
    
    1. renames tcp_ prefixed hashtable functions and data structures that
       were already mostly generic to inet_ to share it with DCCP and
       other INET transport protocols.
    
    2. Removes not used functions (__tb_head & tb_head)
    
    3. Removes some leftover prototypes in the headers (tcp_bucket_unlock &
       tcp_v4_build_header)
    
    Next changesets will move tcp_sk(sk)->bind_hash to inet_sock so that we can
    make functions such as tcp_inherit_port, __tcp_inherit_port, tcp_v4_get_port,
    __tcp_put_port,  generic and get others like tcp_destroy_sock closer to generic
    (tcp_orphan_count will go to sk->sk_prot to allow this).
    
    Eventually most of these functions will be used passing the transport protocol
    inet_hashinfo structure.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index e4fd82e42104..ec580a560e8c 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -258,7 +258,7 @@ struct tcp_sock {
  	__u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
 	__u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
 	__u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
-	struct tcp_bind_bucket *bind_hash;
+	struct inet_bind_bucket *bind_hash;
 	/* Delayed ACK control data */
 	struct {
 		__u8	pending;	/* ACK is pending */

commit c1b4a7e69576d65efc31a8cea0714173c2841244
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:24:38 2005 -0700

    [TCP]: Move to new TSO segmenting scheme.
    
    Make TSO segment transmit size decisions at send time not earlier.
    
    The basic scheme is that we try to build as large a TSO frame as
    possible when pulling in the user data, but the size of the TSO frame
    output to the card is determined at transmit time.
    
    This is guided by tp->xmit_size_goal.  It is always set to a multiple
    of MSS and tells sendmsg/sendpage how large an SKB to try and build.
    
    Later, tcp_write_xmit() and tcp_push_one() chop up the packet if
    necessary and conditions warrant.  These routines can also decide to
    "defer" in order to wait for more ACKs to arrive and thus allow larger
    TSO frames to be emitted.
    
    A general observation is that TSO elongates the pipe, thus requiring a
    larger congestion window and larger buffering especially at the sender
    side.  Therefore, it is important that applications 1) get a large
    enough socket send buffer (this is accomplished by our dynamic send
    buffer expansion code) 2) do large enough writes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index dfd93d03f5d2..e4fd82e42104 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -286,7 +286,7 @@ struct tcp_sock {
 	__u32	max_window;	/* Maximal window ever seen from peer	*/
 	__u32	pmtu_cookie;	/* Last pmtu seen by socket		*/
 	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
-	__u16	mss_cache_std;	/* Like mss_cache, but without TSO */
+	__u16	xmit_size_goal;	/* Goal for segmenting output packets	*/
 	__u16	ext_header_len;	/* Network protocol overhead (IP/IPv6 options) */
 	__u8	ca_state;	/* State of fast-retransmit machine 	*/
 	__u8	retransmits;	/* Number of unrecovered RTO timeouts.	*/

commit 5f8ef48d240963093451bcf83df89f1a1364f51d
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Jun 23 20:37:36 2005 -0700

    [TCP]: Allow choosing TCP congestion control via sockopt.
    
    Allow using setsockopt to set TCP congestion control to use on a per
    socket basis.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 3ea75dd6640a..dfd93d03f5d2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -127,6 +127,7 @@ enum {
 #define TCP_WINDOW_CLAMP	10	/* Bound advertised window */
 #define TCP_INFO		11	/* Information about this connection. */
 #define TCP_QUICKACK		12	/* Block/reenable quick acks */
+#define TCP_CONGESTION		13	/* Congestion control algorithm */
 
 #define TCPI_OPT_TIMESTAMPS	1
 #define TCPI_OPT_SACK		2

commit 317a76f9a44b437d6301718f4e5d08bd93f98da7
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Jun 23 12:19:55 2005 -0700

    [TCP]: Add pluggable congestion control algorithm infrastructure.
    
    Allow TCP to have multiple pluggable congestion control algorithms.
    Algorithms are defined by a set of operations and can be built in
    or modules.  The legacy "new RENO" algorithm is used as a starting
    point and fallback.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 97a7c9e03df5..3ea75dd6640a 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -203,13 +203,6 @@ struct tcp_sack_block {
 	__u32	end_seq;
 };
 
-enum tcp_congestion_algo {
-	TCP_RENO=0,
-	TCP_VEGAS,
-	TCP_WESTWOOD,
-	TCP_BIC,
-};
-
 struct tcp_options_received {
 /*	PAWS/RTTM data	*/
 	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
@@ -305,7 +298,7 @@ struct tcp_sock {
 	__u8	reordering;	/* Packet reordering metric.		*/
 	__u8	frto_counter;	/* Number of new acks after RTO */
 
-	__u8	adv_cong;	/* Using Vegas, Westwood, or BIC */
+	__u8	unused;
 	__u8	defer_accept;	/* User waits for some data after accept() */
 
 /* RTT measurement */
@@ -401,37 +394,10 @@ struct tcp_sock {
 		__u32	time;
 	} rcvq_space;
 
-/* TCP Westwood structure */
-        struct {
-                __u32    bw_ns_est;        /* first bandwidth estimation..not too smoothed 8) */
-                __u32    bw_est;           /* bandwidth estimate */
-                __u32    rtt_win_sx;       /* here starts a new evaluation... */
-                __u32    bk;
-                __u32    snd_una;          /* used for evaluating the number of acked bytes */
-                __u32    cumul_ack;
-                __u32    accounted;
-                __u32    rtt;
-                __u32    rtt_min;          /* minimum observed RTT */
-        } westwood;
-
-/* Vegas variables */
-	struct {
-		__u32	beg_snd_nxt;	/* right edge during last RTT */
-		__u32	beg_snd_una;	/* left edge  during last RTT */
-		__u32	beg_snd_cwnd;	/* saves the size of the cwnd */
-		__u8	doing_vegas_now;/* if true, do vegas for this RTT */
-		__u16	cntRTT;		/* # of RTTs measured within last RTT */
-		__u32	minRTT;		/* min of RTTs measured within last RTT (in usec) */
-		__u32	baseRTT;	/* the min of all Vegas RTT measurements seen (in usec) */
-	} vegas;
-
-	/* BI TCP Parameters */
-	struct {
-		__u32	cnt;		/* increase cwnd by 1 after this number of ACKs */
-		__u32 	last_max_cwnd;	/* last maximium snd_cwnd */
-		__u32	last_cwnd;	/* the last snd_cwnd */
-		__u32   last_stamp;     /* time when updated last_cwnd */
-	} bictcp;
+	/* Pluggable TCP congestion control hook */
+	struct tcp_congestion_ops *ca_ops;
+	u32	ca_priv[16];
+#define TCP_CA_PRIV_SIZE	(16*sizeof(u32))
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
@@ -439,6 +405,11 @@ static inline struct tcp_sock *tcp_sk(const struct sock *sk)
 	return (struct tcp_sock *)sk;
 }
 
+static inline void *tcp_ca(const struct tcp_sock *tp)
+{
+	return (void *) tp->ca_priv;
+}
+
 #endif
 
 #endif	/* _LINUX_TCP_H */

commit 0e87506fcc734647c7b2497eee4eb81e785c857a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:59 2005 -0700

    [NET] Generalise tcp_listen_opt
    
    This chunks out the accept_queue and tcp_listen_opt code and moves
    them to net/core/request_sock.c and include/net/request_sock.h, to
    make it useful for other transport protocols, DCCP being the first one
    to use it.
    
    Next patches will rename tcp_listen_opt to accept_sock and remove the
    inline tcp functions that just call a reqsk_queue_ function.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fb54292a15aa..97a7c9e03df5 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -379,22 +379,7 @@ struct tcp_sock {
 
 	__u32	total_retrans;	/* Total retransmits for entire connection */
 
-	/* The syn_wait_lock is necessary only to avoid proc interface having
-	 * to grab the main lock sock while browsing the listening hash
-	 * (otherwise it's deadlock prone).
-	 * This lock is acquired in read mode only from listening_get_next()
-	 * and it's acquired in write mode _only_ from code that is actively
-	 * changing the syn_wait_queue. All readers that are holding
-	 * the master sock lock don't need to grab this lock in read mode
-	 * too as the syn_wait_queue writes are always protected from
-	 * the main sock lock.
-	 */
-	rwlock_t		syn_wait_lock;
-	struct tcp_listen_opt	*listen_opt;
-
-	/* FIFO of established children */
-	struct request_sock	*accept_queue;
-	struct request_sock	*accept_queue_tail;
+	struct request_sock_queue accept_queue; /* FIFO of established children */
 
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */

commit 60236fdd08b2169045a3bbfc5ffe1576e6c3c17b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:21 2005 -0700

    [NET] Rename open_request to request_sock
    
    Ok, this one just renames some stuff to have a better namespace and to
    dissassociate it from TCP:
    
    struct open_request  -> struct request_sock
    tcp_openreq_alloc    -> reqsk_alloc
    tcp_openreq_free     -> reqsk_free
    tcp_openreq_fastfree -> __reqsk_free
    
    With this most of the infrastructure closely resembles a struct
    sock methods subset.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 86771b37b80d..fb54292a15aa 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -236,7 +236,7 @@ struct tcp_request_sock {
 	__u32			 snt_isn;
 };
 
-static inline struct tcp_request_sock *tcp_rsk(const struct open_request *req)
+static inline struct tcp_request_sock *tcp_rsk(const struct request_sock *req)
 {
 	return (struct tcp_request_sock *)req;
 }
@@ -393,8 +393,8 @@ struct tcp_sock {
 	struct tcp_listen_opt	*listen_opt;
 
 	/* FIFO of established children */
-	struct open_request	*accept_queue;
-	struct open_request	*accept_queue_tail;
+	struct request_sock	*accept_queue;
+	struct request_sock	*accept_queue_tail;
 
 	unsigned int		keepalive_time;	  /* time before keep alive takes place */
 	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */

commit 2e6599cb899ba4b133f42cbf9d2b1883d2dc583a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:46:52 2005 -0700

    [NET] Generalise TCP's struct open_request minisock infrastructure
    
    Kept this first changeset minimal, without changing existing names to
    ease peer review.
    
    Basicaly tcp_openreq_alloc now receives the or_calltable, that in turn
    has two new members:
    
    ->slab, that replaces tcp_openreq_cachep
    ->obj_size, to inform the size of the openreq descendant for
      a specific protocol
    
    The protocol specific fields in struct open_request were moved to a
    class hierarchy, with the things that are common to all connection
    oriented PF_INET protocols in struct inet_request_sock, the TCP ones
    in tcp_request_sock, that is an inet_request_sock, that is an
    open_request.
    
    I.e. this uses the same approach used for the struct sock class
    hierarchy, with sk_prot indicating if the protocol wants to use the
    open_request infrastructure by filling in sk_prot->rsk_prot with an
    or_calltable.
    
    Results? Performance is improved and TCP v4 now uses only 64 bytes per
    open request minisock, down from 96 without this patch :-)
    
    Next changeset will rename some of the structs, fields and functions
    mentioned above, struct or_calltable is way unclear, better name it
    struct request_sock_ops, s/struct open_request/struct request_sock/g,
    etc.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 14a55e3e3a50..86771b37b80d 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -230,6 +230,17 @@ struct tcp_options_received {
 	__u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
 };
 
+struct tcp_request_sock {
+	struct inet_request_sock req;
+	__u32			 rcv_isn;
+	__u32			 snt_isn;
+};
+
+static inline struct tcp_request_sock *tcp_rsk(const struct open_request *req)
+{
+	return (struct tcp_request_sock *)req;
+}
+
 struct tcp_sock {
 	/* inet_sock has to be the first member of tcp_sock */
 	struct inet_sock	inet;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
new file mode 100644
index 000000000000..14a55e3e3a50
--- /dev/null
+++ b/include/linux/tcp.h
@@ -0,0 +1,448 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the TCP protocol.
+ *
+ * Version:	@(#)tcp.h	1.0.2	04/28/93
+ *
+ * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _LINUX_TCP_H
+#define _LINUX_TCP_H
+
+#include <linux/types.h>
+#include <asm/byteorder.h>
+
+struct tcphdr {
+	__u16	source;
+	__u16	dest;
+	__u32	seq;
+	__u32	ack_seq;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u16	res1:4,
+		doff:4,
+		fin:1,
+		syn:1,
+		rst:1,
+		psh:1,
+		ack:1,
+		urg:1,
+		ece:1,
+		cwr:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u16	doff:4,
+		res1:4,
+		cwr:1,
+		ece:1,
+		urg:1,
+		ack:1,
+		psh:1,
+		rst:1,
+		syn:1,
+		fin:1;
+#else
+#error	"Adjust your <asm/byteorder.h> defines"
+#endif	
+	__u16	window;
+	__u16	check;
+	__u16	urg_ptr;
+};
+
+
+enum {
+  TCP_ESTABLISHED = 1,
+  TCP_SYN_SENT,
+  TCP_SYN_RECV,
+  TCP_FIN_WAIT1,
+  TCP_FIN_WAIT2,
+  TCP_TIME_WAIT,
+  TCP_CLOSE,
+  TCP_CLOSE_WAIT,
+  TCP_LAST_ACK,
+  TCP_LISTEN,
+  TCP_CLOSING,	 /* now a valid state */
+
+  TCP_MAX_STATES /* Leave at the end! */
+};
+
+#define TCP_STATE_MASK	0xF
+#define TCP_ACTION_FIN	(1 << 7)
+
+enum {
+  TCPF_ESTABLISHED = (1 << 1),
+  TCPF_SYN_SENT  = (1 << 2),
+  TCPF_SYN_RECV  = (1 << 3),
+  TCPF_FIN_WAIT1 = (1 << 4),
+  TCPF_FIN_WAIT2 = (1 << 5),
+  TCPF_TIME_WAIT = (1 << 6),
+  TCPF_CLOSE     = (1 << 7),
+  TCPF_CLOSE_WAIT = (1 << 8),
+  TCPF_LAST_ACK  = (1 << 9),
+  TCPF_LISTEN    = (1 << 10),
+  TCPF_CLOSING   = (1 << 11) 
+};
+
+/*
+ *	The union cast uses a gcc extension to avoid aliasing problems
+ *  (union is compatible to any of its members)
+ *  This means this part of the code is -fstrict-aliasing safe now.
+ */
+union tcp_word_hdr { 
+	struct tcphdr hdr;
+	__u32 		  words[5];
+}; 
+
+#define tcp_flag_word(tp) ( ((union tcp_word_hdr *)(tp))->words [3]) 
+
+enum { 
+	TCP_FLAG_CWR = __constant_htonl(0x00800000), 
+	TCP_FLAG_ECE = __constant_htonl(0x00400000), 
+	TCP_FLAG_URG = __constant_htonl(0x00200000), 
+	TCP_FLAG_ACK = __constant_htonl(0x00100000), 
+	TCP_FLAG_PSH = __constant_htonl(0x00080000), 
+	TCP_FLAG_RST = __constant_htonl(0x00040000), 
+	TCP_FLAG_SYN = __constant_htonl(0x00020000), 
+	TCP_FLAG_FIN = __constant_htonl(0x00010000),
+	TCP_RESERVED_BITS = __constant_htonl(0x0F000000),
+	TCP_DATA_OFFSET = __constant_htonl(0xF0000000)
+}; 
+
+/* TCP socket options */
+#define TCP_NODELAY		1	/* Turn off Nagle's algorithm. */
+#define TCP_MAXSEG		2	/* Limit MSS */
+#define TCP_CORK		3	/* Never send partially complete segments */
+#define TCP_KEEPIDLE		4	/* Start keeplives after this period */
+#define TCP_KEEPINTVL		5	/* Interval between keepalives */
+#define TCP_KEEPCNT		6	/* Number of keepalives before death */
+#define TCP_SYNCNT		7	/* Number of SYN retransmits */
+#define TCP_LINGER2		8	/* Life time of orphaned FIN-WAIT-2 state */
+#define TCP_DEFER_ACCEPT	9	/* Wake up listener only when data arrive */
+#define TCP_WINDOW_CLAMP	10	/* Bound advertised window */
+#define TCP_INFO		11	/* Information about this connection. */
+#define TCP_QUICKACK		12	/* Block/reenable quick acks */
+
+#define TCPI_OPT_TIMESTAMPS	1
+#define TCPI_OPT_SACK		2
+#define TCPI_OPT_WSCALE		4
+#define TCPI_OPT_ECN		8
+
+enum tcp_ca_state
+{
+	TCP_CA_Open = 0,
+#define TCPF_CA_Open	(1<<TCP_CA_Open)
+	TCP_CA_Disorder = 1,
+#define TCPF_CA_Disorder (1<<TCP_CA_Disorder)
+	TCP_CA_CWR = 2,
+#define TCPF_CA_CWR	(1<<TCP_CA_CWR)
+	TCP_CA_Recovery = 3,
+#define TCPF_CA_Recovery (1<<TCP_CA_Recovery)
+	TCP_CA_Loss = 4
+#define TCPF_CA_Loss	(1<<TCP_CA_Loss)
+};
+
+struct tcp_info
+{
+	__u8	tcpi_state;
+	__u8	tcpi_ca_state;
+	__u8	tcpi_retransmits;
+	__u8	tcpi_probes;
+	__u8	tcpi_backoff;
+	__u8	tcpi_options;
+	__u8	tcpi_snd_wscale : 4, tcpi_rcv_wscale : 4;
+
+	__u32	tcpi_rto;
+	__u32	tcpi_ato;
+	__u32	tcpi_snd_mss;
+	__u32	tcpi_rcv_mss;
+
+	__u32	tcpi_unacked;
+	__u32	tcpi_sacked;
+	__u32	tcpi_lost;
+	__u32	tcpi_retrans;
+	__u32	tcpi_fackets;
+
+	/* Times. */
+	__u32	tcpi_last_data_sent;
+	__u32	tcpi_last_ack_sent;     /* Not remembered, sorry. */
+	__u32	tcpi_last_data_recv;
+	__u32	tcpi_last_ack_recv;
+
+	/* Metrics. */
+	__u32	tcpi_pmtu;
+	__u32	tcpi_rcv_ssthresh;
+	__u32	tcpi_rtt;
+	__u32	tcpi_rttvar;
+	__u32	tcpi_snd_ssthresh;
+	__u32	tcpi_snd_cwnd;
+	__u32	tcpi_advmss;
+	__u32	tcpi_reordering;
+
+	__u32	tcpi_rcv_rtt;
+	__u32	tcpi_rcv_space;
+
+	__u32	tcpi_total_retrans;
+};
+
+#ifdef __KERNEL__
+
+#include <linux/config.h>
+#include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <net/sock.h>
+
+/* This defines a selective acknowledgement block. */
+struct tcp_sack_block {
+	__u32	start_seq;
+	__u32	end_seq;
+};
+
+enum tcp_congestion_algo {
+	TCP_RENO=0,
+	TCP_VEGAS,
+	TCP_WESTWOOD,
+	TCP_BIC,
+};
+
+struct tcp_options_received {
+/*	PAWS/RTTM data	*/
+	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
+	__u32	ts_recent;	/* Time stamp to echo next		*/
+	__u32	rcv_tsval;	/* Time stamp value             	*/
+	__u32	rcv_tsecr;	/* Time stamp echo reply        	*/
+	__u16 	saw_tstamp : 1,	/* Saw TIMESTAMP on last packet		*/
+		tstamp_ok : 1,	/* TIMESTAMP seen on SYN packet		*/
+		dsack : 1,	/* D-SACK is scheduled			*/
+		wscale_ok : 1,	/* Wscale seen on SYN packet		*/
+		sack_ok : 4,	/* SACK seen on SYN packet		*/
+		snd_wscale : 4,	/* Window scaling received from sender	*/
+		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
+/*	SACKs data	*/
+	__u8	eff_sacks;	/* Size of SACK array to send with next packet */
+	__u8	num_sacks;	/* Number of SACK blocks		*/
+	__u16	user_mss;  	/* mss requested by user in ioctl */
+	__u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
+};
+
+struct tcp_sock {
+	/* inet_sock has to be the first member of tcp_sock */
+	struct inet_sock	inet;
+	int	tcp_header_len;	/* Bytes of tcp header to send		*/
+
+/*
+ *	Header prediction flags
+ *	0x5?10 << 16 + snd_wnd in net byte order
+ */
+	__u32	pred_flags;
+
+/*
+ *	RFC793 variables by their proper names. This means you can
+ *	read the code and the spec side by side (and laugh ...)
+ *	See RFC793 and RFC1122. The RFC writes these in capitals.
+ */
+ 	__u32	rcv_nxt;	/* What we want to receive next 	*/
+ 	__u32	snd_nxt;	/* Next sequence we send		*/
+
+ 	__u32	snd_una;	/* First byte we want an ack for	*/
+ 	__u32	snd_sml;	/* Last byte of the most recently transmitted small packet */
+	__u32	rcv_tstamp;	/* timestamp of last received ACK (for keepalives) */
+	__u32	lsndtime;	/* timestamp of last sent data packet (for restart window) */
+	struct tcp_bind_bucket *bind_hash;
+	/* Delayed ACK control data */
+	struct {
+		__u8	pending;	/* ACK is pending */
+		__u8	quick;		/* Scheduled number of quick acks	*/
+		__u8	pingpong;	/* The session is interactive		*/
+		__u8	blocked;	/* Delayed ACK was blocked by socket lock*/
+		__u32	ato;		/* Predicted tick of soft clock		*/
+		unsigned long timeout;	/* Currently scheduled timeout		*/
+		__u32	lrcvtime;	/* timestamp of last received data packet*/
+		__u16	last_seg_size;	/* Size of last incoming segment	*/
+		__u16	rcv_mss;	/* MSS used for delayed ACK decisions	*/ 
+	} ack;
+
+	/* Data for direct copy to user */
+	struct {
+		struct sk_buff_head	prequeue;
+		struct task_struct	*task;
+		struct iovec		*iov;
+		int			memory;
+		int			len;
+	} ucopy;
+
+	__u32	snd_wl1;	/* Sequence for window update		*/
+	__u32	snd_wnd;	/* The window we expect to receive	*/
+	__u32	max_window;	/* Maximal window ever seen from peer	*/
+	__u32	pmtu_cookie;	/* Last pmtu seen by socket		*/
+	__u32	mss_cache;	/* Cached effective mss, not including SACKS */
+	__u16	mss_cache_std;	/* Like mss_cache, but without TSO */
+	__u16	ext_header_len;	/* Network protocol overhead (IP/IPv6 options) */
+	__u8	ca_state;	/* State of fast-retransmit machine 	*/
+	__u8	retransmits;	/* Number of unrecovered RTO timeouts.	*/
+
+	__u16	advmss;		/* Advertised MSS			*/
+	__u32	window_clamp;	/* Maximal window to advertise		*/
+	__u32	rcv_ssthresh;	/* Current window clamp			*/
+
+	__u32	frto_highmark;	/* snd_nxt when RTO occurred */
+	__u8	reordering;	/* Packet reordering metric.		*/
+	__u8	frto_counter;	/* Number of new acks after RTO */
+
+	__u8	adv_cong;	/* Using Vegas, Westwood, or BIC */
+	__u8	defer_accept;	/* User waits for some data after accept() */
+
+/* RTT measurement */
+	__u32	srtt;		/* smoothed round trip time << 3	*/
+	__u32	mdev;		/* medium deviation			*/
+	__u32	mdev_max;	/* maximal mdev for the last rtt period	*/
+	__u32	rttvar;		/* smoothed mdev_max			*/
+	__u32	rtt_seq;	/* sequence number to update rttvar	*/
+	__u32	rto;		/* retransmit timeout			*/
+
+	__u32	packets_out;	/* Packets which are "in flight"	*/
+	__u32	left_out;	/* Packets which leaved network	*/
+	__u32	retrans_out;	/* Retransmitted packets out		*/
+	__u8	backoff;	/* backoff				*/
+/*
+ *      Options received (usually on last packet, some only on SYN packets).
+ */
+	__u8	nonagle;	/* Disable Nagle algorithm?             */
+	__u8	keepalive_probes; /* num of allowed keep alive probes	*/
+
+	__u8	probes_out;	/* unanswered 0 window probes		*/
+	struct tcp_options_received rx_opt;
+
+/*
+ *	Slow start and congestion control (see also Nagle, and Karn & Partridge)
+ */
+ 	__u32	snd_ssthresh;	/* Slow start size threshold		*/
+ 	__u32	snd_cwnd;	/* Sending congestion window		*/
+ 	__u16	snd_cwnd_cnt;	/* Linear increase counter		*/
+	__u16	snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
+	__u32	snd_cwnd_used;
+	__u32	snd_cwnd_stamp;
+
+	/* Two commonly used timers in both sender and receiver paths. */
+	unsigned long		timeout;
+ 	struct timer_list	retransmit_timer;	/* Resend (no ack)	*/
+ 	struct timer_list	delack_timer;		/* Ack delay 		*/
+
+	struct sk_buff_head	out_of_order_queue; /* Out of order segments go here */
+
+	struct tcp_func		*af_specific;	/* Operations which are AF_INET{4,6} specific	*/
+
+ 	__u32	rcv_wnd;	/* Current receiver window		*/
+	__u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
+	__u32	write_seq;	/* Tail(+1) of data held in tcp send buffer */
+	__u32	pushed_seq;	/* Last pushed seq, required to talk to windows */
+	__u32	copied_seq;	/* Head of yet unread data		*/
+
+/*	SACKs data	*/
+	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
+	struct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/
+
+	__u8	syn_retries;	/* num of allowed syn retries */
+	__u8	ecn_flags;	/* ECN status bits.			*/
+	__u16	prior_ssthresh; /* ssthresh saved at recovery start	*/
+	__u32	lost_out;	/* Lost packets			*/
+	__u32	sacked_out;	/* SACK'd packets			*/
+	__u32	fackets_out;	/* FACK'd packets			*/
+	__u32	high_seq;	/* snd_nxt at onset of congestion	*/
+
+	__u32	retrans_stamp;	/* Timestamp of the last retransmit,
+				 * also used in SYN-SENT to remember stamp of
+				 * the first SYN. */
+	__u32	undo_marker;	/* tracking retrans started here. */
+	int	undo_retrans;	/* number of undoable retransmissions. */
+	__u32	urg_seq;	/* Seq of received urgent pointer */
+	__u16	urg_data;	/* Saved octet of OOB data and control flags */
+	__u8	pending;	/* Scheduled timer event	*/
+	__u8	urg_mode;	/* In urgent mode		*/
+	__u32	snd_up;		/* Urgent pointer		*/
+
+	__u32	total_retrans;	/* Total retransmits for entire connection */
+
+	/* The syn_wait_lock is necessary only to avoid proc interface having
+	 * to grab the main lock sock while browsing the listening hash
+	 * (otherwise it's deadlock prone).
+	 * This lock is acquired in read mode only from listening_get_next()
+	 * and it's acquired in write mode _only_ from code that is actively
+	 * changing the syn_wait_queue. All readers that are holding
+	 * the master sock lock don't need to grab this lock in read mode
+	 * too as the syn_wait_queue writes are always protected from
+	 * the main sock lock.
+	 */
+	rwlock_t		syn_wait_lock;
+	struct tcp_listen_opt	*listen_opt;
+
+	/* FIFO of established children */
+	struct open_request	*accept_queue;
+	struct open_request	*accept_queue_tail;
+
+	unsigned int		keepalive_time;	  /* time before keep alive takes place */
+	unsigned int		keepalive_intvl;  /* time interval between keep alive probes */
+	int			linger2;
+
+	unsigned long last_synq_overflow; 
+
+/* Receiver side RTT estimation */
+	struct {
+		__u32	rtt;
+		__u32	seq;
+		__u32	time;
+	} rcv_rtt_est;
+
+/* Receiver queue space */
+	struct {
+		int	space;
+		__u32	seq;
+		__u32	time;
+	} rcvq_space;
+
+/* TCP Westwood structure */
+        struct {
+                __u32    bw_ns_est;        /* first bandwidth estimation..not too smoothed 8) */
+                __u32    bw_est;           /* bandwidth estimate */
+                __u32    rtt_win_sx;       /* here starts a new evaluation... */
+                __u32    bk;
+                __u32    snd_una;          /* used for evaluating the number of acked bytes */
+                __u32    cumul_ack;
+                __u32    accounted;
+                __u32    rtt;
+                __u32    rtt_min;          /* minimum observed RTT */
+        } westwood;
+
+/* Vegas variables */
+	struct {
+		__u32	beg_snd_nxt;	/* right edge during last RTT */
+		__u32	beg_snd_una;	/* left edge  during last RTT */
+		__u32	beg_snd_cwnd;	/* saves the size of the cwnd */
+		__u8	doing_vegas_now;/* if true, do vegas for this RTT */
+		__u16	cntRTT;		/* # of RTTs measured within last RTT */
+		__u32	minRTT;		/* min of RTTs measured within last RTT (in usec) */
+		__u32	baseRTT;	/* the min of all Vegas RTT measurements seen (in usec) */
+	} vegas;
+
+	/* BI TCP Parameters */
+	struct {
+		__u32	cnt;		/* increase cwnd by 1 after this number of ACKs */
+		__u32 	last_max_cwnd;	/* last maximium snd_cwnd */
+		__u32	last_cwnd;	/* the last snd_cwnd */
+		__u32   last_stamp;     /* time when updated last_cwnd */
+	} bictcp;
+};
+
+static inline struct tcp_sock *tcp_sk(const struct sock *sk)
+{
+	return (struct tcp_sock *)sk;
+}
+
+#endif
+
+#endif	/* _LINUX_TCP_H */
