commit de810f490db7ed4c1db2bbfa458b2e27681d2ccb
Author: Tobin C. Harding <tobin@kernel.org>
Date:   Tue Mar 5 15:42:07 2019 -0800

    include/linux/slub_def.h: comment fixes
    
    Capitialize comment string, use C89 comment style, correct
    grammar/punctuation in comments.
    
    Link: http://lkml.kernel.org/r/20190204005713.9463-2-tobin@kernel.org
    Link: http://lkml.kernel.org/r/20190204005713.9463-3-tobin@kernel.org
    Link: http://lkml.kernel.org/r/20190204005713.9463-4-tobin@kernel.org
    Signed-off-by: Tobin C. Harding <tobin@kernel.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 3a1a1dbc6f49..d2153789bd9f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -81,12 +81,12 @@ struct kmem_cache_order_objects {
  */
 struct kmem_cache {
 	struct kmem_cache_cpu __percpu *cpu_slab;
-	/* Used for retriving partial slabs etc */
+	/* Used for retrieving partial slabs, etc. */
 	slab_flags_t flags;
 	unsigned long min_partial;
-	unsigned int size;	/* The size of an object including meta data */
-	unsigned int object_size;/* The size of an object without meta data */
-	unsigned int offset;	/* Free pointer offset. */
+	unsigned int size;	/* The size of an object including metadata */
+	unsigned int object_size;/* The size of an object without metadata */
+	unsigned int offset;	/* Free pointer offset */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
 	/* Number of per cpu partial objects to keep around */
 	unsigned int cpu_partial;
@@ -110,7 +110,7 @@ struct kmem_cache {
 #endif
 #ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;
-	/* for propagation, maximum size of a stored attr */
+	/* For propagation, maximum size of a stored attr */
 	unsigned int max_attr_size;
 #ifdef CONFIG_SYSFS
 	struct kset *memcg_kset;
@@ -151,7 +151,7 @@ struct kmem_cache {
 #else
 #define slub_cpu_partial(s)		(0)
 #define slub_set_cpu_partial(s, n)
-#endif // CONFIG_SLUB_CPU_PARTIAL
+#endif /* CONFIG_SLUB_CPU_PARTIAL */
 
 #ifdef CONFIG_SYSFS
 #define SLAB_SUPPORTS_SYSFS

commit d50d82faa0c964e31f7a946ba8aba7c715ca7ab0
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Wed Jun 27 23:26:09 2018 -0700

    slub: fix failure when we delete and create a slab cache
    
    In kernel 4.17 I removed some code from dm-bufio that did slab cache
    merging (commit 21bb13276768: "dm bufio: remove code that merges slab
    caches") - both slab and slub support merging caches with identical
    attributes, so dm-bufio now just calls kmem_cache_create and relies on
    implicit merging.
    
    This uncovered a bug in the slub subsystem - if we delete a cache and
    immediatelly create another cache with the same attributes, it fails
    because of duplicate filename in /sys/kernel/slab/.  The slub subsystem
    offloads freeing the cache to a workqueue - and if we create the new
    cache before the workqueue runs, it complains because of duplicate
    filename in sysfs.
    
    This patch fixes the bug by moving the call of kobject_del from
    sysfs_slab_remove_workfn to shutdown_cache.  kobject_del must be called
    while we hold slab_mutex - so that the sysfs entry is deleted before a
    cache with the same attributes could be created.
    
    Running device-mapper-test-suite with:
    
      dmtest run --suite thin-provisioning -n /commit_failure_causes_fallback/
    
    triggered:
    
      Buffer I/O error on dev dm-0, logical block 1572848, async page read
      device-mapper: thin: 253:1: metadata operation 'dm_pool_alloc_data_block' failed: error = -5
      device-mapper: thin: 253:1: aborting current metadata transaction
      sysfs: cannot create duplicate filename '/kernel/slab/:a-0000144'
      CPU: 2 PID: 1037 Comm: kworker/u48:1 Not tainted 4.17.0.snitm+ #25
      Hardware name: Supermicro SYS-1029P-WTR/X11DDW-L, BIOS 2.0a 12/06/2017
      Workqueue: dm-thin do_worker [dm_thin_pool]
      Call Trace:
       dump_stack+0x5a/0x73
       sysfs_warn_dup+0x58/0x70
       sysfs_create_dir_ns+0x77/0x80
       kobject_add_internal+0xba/0x2e0
       kobject_init_and_add+0x70/0xb0
       sysfs_slab_add+0xb1/0x250
       __kmem_cache_create+0x116/0x150
       create_cache+0xd9/0x1f0
       kmem_cache_create_usercopy+0x1c1/0x250
       kmem_cache_create+0x18/0x20
       dm_bufio_client_create+0x1ae/0x410 [dm_bufio]
       dm_block_manager_create+0x5e/0x90 [dm_persistent_data]
       __create_persistent_data_objects+0x38/0x940 [dm_thin_pool]
       dm_pool_abort_metadata+0x64/0x90 [dm_thin_pool]
       metadata_operation_failed+0x59/0x100 [dm_thin_pool]
       alloc_data_block.isra.53+0x86/0x180 [dm_thin_pool]
       process_cell+0x2a3/0x550 [dm_thin_pool]
       do_worker+0x28d/0x8f0 [dm_thin_pool]
       process_one_work+0x171/0x370
       worker_thread+0x49/0x3f0
       kthread+0xf8/0x130
       ret_from_fork+0x35/0x40
      kobject_add_internal failed for :a-0000144 with -EEXIST, don't try to register things with the same name in the same directory.
      kmem_cache_create(dm_bufio_buffer-16) failed with error -17
    
    Link: http://lkml.kernel.org/r/alpine.LRH.2.02.1806151817130.6333@file01.intranet.prod.int.rdu2.redhat.com
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Reported-by: Mike Snitzer <snitzer@redhat.com>
    Tested-by: Mike Snitzer <snitzer@redhat.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 09fa2c6f0e68..3a1a1dbc6f49 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -155,8 +155,12 @@ struct kmem_cache {
 
 #ifdef CONFIG_SYSFS
 #define SLAB_SUPPORTS_SYSFS
+void sysfs_slab_unlink(struct kmem_cache *);
 void sysfs_slab_release(struct kmem_cache *);
 #else
+static inline void sysfs_slab_unlink(struct kmem_cache *s)
+{
+}
 static inline void sysfs_slab_release(struct kmem_cache *s)
 {
 }

commit 9736d2a95e36ac3f60b063f498961103f3d4f165
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Thu Jun 7 17:09:10 2018 -0700

    slub: remove kmem_cache->reserved
    
    The reserved field was only used for embedding an rcu_head in the data
    structure.  With the previous commit, we no longer need it.  That lets us
    remove the 'reserved' argument to a lot of functions.
    
    Link: http://lkml.kernel.org/r/20180518194519.3820-16-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jérôme Glisse <jglisse@redhat.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 3773e26c08c1..09fa2c6f0e68 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -101,7 +101,6 @@ struct kmem_cache {
 	void (*ctor)(void *);
 	unsigned int inuse;		/* Offset to metadata */
 	unsigned int align;		/* Alignment */
-	unsigned int reserved;		/* Reserved bytes at the end of slabs */
 	unsigned int red_left_pad;	/* Left redzone padding size */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */

commit 19af27aff901e401a5b79e5c974e881e4701162c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:39 2018 -0700

    slub: make struct kmem_cache_order_objects::x unsigned int
    
    struct kmem_cache_order_objects is for mixing order and number of
    objects, and orders aren't big enough to warrant 64-bit width.
    
    Propagate unsignedness down so that everything fits.
    
    !!! Patch assumes that "PAGE_SIZE << order" doesn't overflow. !!!
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-23-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 623d6ba92036..3773e26c08c1 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -73,7 +73,7 @@ struct kmem_cache_cpu {
  * given order would contain.
  */
 struct kmem_cache_order_objects {
-	unsigned long x;
+	unsigned int x;
 };
 
 /*

commit 7bbdb81ee3de73f2381ceec1bbee831f4c913b5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:31 2018 -0700

    slab: make usercopy region 32-bit
    
    If kmem case sizes are 32-bit, then usecopy region should be too.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-21-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index bc02fd3a8ccf..623d6ba92036 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -137,8 +137,8 @@ struct kmem_cache {
 	struct kasan_cache kasan_info;
 #endif
 
-	size_t useroffset;		/* Usercopy region offset */
-	size_t usersize;		/* Usercopy region size */
+	unsigned int useroffset;	/* Usercopy region offset */
+	unsigned int usersize;		/* Usercopy region size */
 
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };

commit 44065b2e2975ff5987164b98d29cc78e207f9a5a
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:20 2018 -0700

    slub: make ->size unsigned int
    
    Linux doesn't support negative length objects (including meta data).
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-18-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 7d74f121ef4e..bc02fd3a8ccf 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -84,7 +84,7 @@ struct kmem_cache {
 	/* Used for retriving partial slabs etc */
 	slab_flags_t flags;
 	unsigned long min_partial;
-	int size;		/* The size of an object including meta data */
+	unsigned int size;	/* The size of an object including meta data */
 	unsigned int object_size;/* The size of an object without meta data */
 	unsigned int offset;	/* Free pointer offset. */
 #ifdef CONFIG_SLUB_CPU_PARTIAL

commit 1b473f29d5dd766903ac2372ac04b07600f233d0
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:17 2018 -0700

    slub: make ->object_size unsigned int
    
    Linux doesn't support negative length objects.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-17-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index db00dbd7e89f..7d74f121ef4e 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -85,7 +85,7 @@ struct kmem_cache {
 	slab_flags_t flags;
 	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
-	int object_size;	/* The size of an object without meta data */
+	unsigned int object_size;/* The size of an object without meta data */
 	unsigned int offset;	/* Free pointer offset. */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
 	/* Number of per cpu partial objects to keep around */

commit a5035de2c4472d6c58c60a7f8eaad8ed0084b8b2
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:13 2018 -0700

    slub: make ->offset unsigned int
    
    ->offset is free pointer offset from the start of the object, can't be
    negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-16-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index d2cc1391f17a..db00dbd7e89f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -86,7 +86,7 @@ struct kmem_cache {
 	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
 	int object_size;	/* The size of an object without meta data */
-	int offset;		/* Free pointer offset. */
+	unsigned int offset;	/* Free pointer offset. */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
 	/* Number of per cpu partial objects to keep around */
 	unsigned int cpu_partial;

commit e5d9998f3e09359b372a037a6ac55ba235d95d57
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:10 2018 -0700

    slub: make ->cpu_partial unsigned int
    
            /*
             * cpu_partial determined the maximum number of objects
             * kept in the per cpu partial lists of a processor.
             */
    
    Can't be negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-15-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2287b800474f..d2cc1391f17a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -88,7 +88,8 @@ struct kmem_cache {
 	int object_size;	/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
 #ifdef CONFIG_SLUB_CPU_PARTIAL
-	int cpu_partial;	/* Number of per cpu partial objects to keep around */
+	/* Number of per cpu partial objects to keep around */
+	unsigned int cpu_partial;
 #endif
 	struct kmem_cache_order_objects oo;
 

commit 52ee6d74aa23a3c5d4472edf167f2bb47776a733
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:06 2018 -0700

    slub: make ->inuse unsigned int
    
    ->inuse is "the number of bytes in actual use by the object",
    can't be negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-14-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2a0eabeff78f..2287b800474f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -98,7 +98,7 @@ struct kmem_cache {
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(void *);
-	int inuse;		/* Offset to metadata */
+	unsigned int inuse;		/* Offset to metadata */
 	unsigned int align;		/* Alignment */
 	unsigned int reserved;		/* Reserved bytes at the end of slabs */
 	unsigned int red_left_pad;	/* Left redzone padding size */

commit 3a3791ec2ecd5db8d903b66faa340b0dfa72e64b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:02 2018 -0700

    slub: make ->align unsigned int
    
    Kmem cache alignment can't be negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-13-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2b4417aa15d8..2a0eabeff78f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -99,7 +99,7 @@ struct kmem_cache {
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
-	int align;		/* Alignment */
+	unsigned int align;		/* Alignment */
 	unsigned int reserved;		/* Reserved bytes at the end of slabs */
 	unsigned int red_left_pad;	/* Left redzone padding size */
 	const char *name;	/* Name (only for display!) */

commit d66e52d1e82b1adfab541f1aad09526ebf67842d
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:58 2018 -0700

    slub: make ->reserved unsigned int
    
    ->reserved is either 0 or sizeof(struct rcu_head), can't be negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-12-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 9f59fc16444b..2b4417aa15d8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -100,7 +100,7 @@ struct kmem_cache {
 	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
-	int reserved;		/* Reserved bytes at the end of slabs */
+	unsigned int reserved;		/* Reserved bytes at the end of slabs */
 	unsigned int red_left_pad;	/* Left redzone padding size */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */

commit 2ca6d39b31022bb9e1dda77109e292517f701261
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:55 2018 -0700

    slub: make ->red_left_pad unsigned int
    
    Padding length can't be negative.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-11-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 9bb761324a9c..9f59fc16444b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -101,7 +101,7 @@ struct kmem_cache {
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	int reserved;		/* Reserved bytes at the end of slabs */
-	int red_left_pad;	/* Left redzone padding size */
+	unsigned int red_left_pad;	/* Left redzone padding size */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
 #ifdef CONFIG_SYSFS

commit 56d8ceebd39b4db3248291e6d1e3e696fc73b077
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:51 2018 -0700

    slub: make ->max_attr_size unsigned int
    
    ->max_attr_size is maximum length of every SLAB memcg attribute
    ever written. VFS limits those to INT_MAX.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-10-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f6548083fe0f..9bb761324a9c 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -110,7 +110,8 @@ struct kmem_cache {
 #endif
 #ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;
-	int max_attr_size; /* for propagation, maximum size of a stored attr */
+	/* for propagation, maximum size of a stored attr */
+	unsigned int max_attr_size;
 #ifdef CONFIG_SYSFS
 	struct kset *memcg_kset;
 #endif

commit eb7235eb842043ca302e992286ca6af63a8127fe
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:20:48 2018 -0700

    slub: make ->remote_node_defrag_ratio unsigned int
    
    ->remote_node_defrag_ratio is in range 0..1000.
    
    This also adds a check and modifies the behavior to return an error
    code.  Before this patch invalid values were ignored.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-9-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 8ad99c47b19c..f6548083fe0f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -124,7 +124,7 @@ struct kmem_cache {
 	/*
 	 * Defragmentation by allocating from a remote node.
 	 */
-	int remote_node_defrag_ratio;
+	unsigned int remote_node_defrag_ratio;
 #endif
 
 #ifdef CONFIG_SLAB_FREELIST_RANDOM

commit 8eb8284b412906181357c2b0110d879d5af95e52
Author: David Windsor <dave@nullcore.net>
Date:   Sat Jun 10 22:50:28 2017 -0400

    usercopy: Prepare for usercopy whitelisting
    
    This patch prepares the slab allocator to handle caches having annotations
    (useroffset and usersize) defining usercopy regions.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on
    my understanding of the code. Changes or omissions from the original
    code are mine and don't reflect the original grsecurity/PaX code.
    
    Currently, hardened usercopy performs dynamic bounds checking on slab
    cache objects. This is good, but still leaves a lot of kernel memory
    available to be copied to/from userspace in the face of bugs. To further
    restrict what memory is available for copying, this creates a way to
    whitelist specific areas of a given slab cache object for copying to/from
    userspace, allowing much finer granularity of access control. Slab caches
    that are never exposed to userspace can declare no whitelist for their
    objects, thereby keeping them unavailable to userspace via dynamic copy
    operations. (Note, an implicit form of whitelisting is the use of constant
    sizes in usercopy operations and get_user()/put_user(); these bypass
    hardened usercopy checks since these sizes cannot change at runtime.)
    
    To support this whitelist annotation, usercopy region offset and size
    members are added to struct kmem_cache. The slab allocator receives a
    new function, kmem_cache_create_usercopy(), that creates a new cache
    with a usercopy region defined, suitable for declaring spans of fields
    within the objects that get copied to/from userspace.
    
    In this patch, the default kmem_cache_create() marks the entire allocation
    as whitelisted, leaving it semantically unchanged. Once all fine-grained
    whitelists have been added (in subsequent patches), this will be changed
    to a usersize of 0, making caches created with kmem_cache_create() not
    copyable to/from userspace.
    
    After the entire usercopy whitelist series is applied, less than 15%
    of the slab cache memory remains exposed to potential usercopy bugs
    after a fresh boot:
    
    Total Slab Memory:           48074720
    Usercopyable Memory:          6367532  13.2%
             task_struct                    0.2%         4480/1630720
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%       269760/8740224
             dentry                        11.1%       585984/5273856
             mm_struct                     29.1%         54912/188448
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          81920/81920
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        167936/167936
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        455616/455616
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        812032/812032
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1310720/1310720
    
    After some kernel build workloads, the percentage (mainly driven by
    dentry and inode caches expanding) drops under 10%:
    
    Total Slab Memory:           95516184
    Usercopyable Memory:          8497452   8.8%
             task_struct                    0.2%         4000/1456000
             RAW                            0.3%            300/96000
             RAWv6                          2.1%           1408/64768
             ext4_inode_cache               3.0%     1217280/39439872
             dentry                        11.1%     1623200/14608800
             mm_struct                     29.1%         73216/251264
             kmalloc-8                    100.0%          24576/24576
             kmalloc-16                   100.0%          28672/28672
             kmalloc-32                   100.0%          94208/94208
             kmalloc-192                  100.0%          96768/96768
             kmalloc-128                  100.0%        143360/143360
             names_cache                  100.0%        163840/163840
             kmalloc-64                   100.0%        245760/245760
             kmalloc-256                  100.0%        339968/339968
             kmalloc-512                  100.0%        350720/350720
             kmalloc-96                   100.0%        563520/563520
             kmalloc-8192                 100.0%        655360/655360
             kmalloc-1024                 100.0%        794624/794624
             kmalloc-4096                 100.0%        819200/819200
             kmalloc-2048                 100.0%      1257472/1257472
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split out a few extra kmalloc hunks]
    [kees: add field names to function declarations]
    [kees: convert BUGs to WARNs and fail closed]
    [kees: add attack surface reduction analysis to commit log]
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: linux-xfs@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Christoph Lameter <cl@linux.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 0adae162dc8f..8ad99c47b19c 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -135,6 +135,9 @@ struct kmem_cache {
 	struct kasan_cache kasan_info;
 #endif
 
+	size_t useroffset;		/* Usercopy region offset */
+	size_t usersize;		/* Usercopy region size */
+
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 

commit d50112edde1d0c621520e53747044009f11c656b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:18 2017 -0800

    slab, slub, slob: add slab_flags_t
    
    Add sparse-checked slab_flags_t for struct kmem_cache::flags (SLAB_POISON,
    etc).
    
    SLAB is bloated temporarily by switching to "unsigned long", but only
    temporarily.
    
    Link: http://lkml.kernel.org/r/20171021100225.GA22428@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 39fa09bcde23..0adae162dc8f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -82,7 +82,7 @@ struct kmem_cache_order_objects {
 struct kmem_cache {
 	struct kmem_cache_cpu __percpu *cpu_slab;
 	/* Used for retriving partial slabs etc */
-	unsigned long flags;
+	slab_flags_t flags;
 	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
 	int object_size;	/* The size of an object without meta data */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 0783b622311e..39fa09bcde23 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_SLUB_DEF_H
 #define _LINUX_SLUB_DEF_H
 

commit 2482ddec670fb83717d129012bc558777cb159f7
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Sep 6 16:19:18 2017 -0700

    mm: add SLUB free list pointer obfuscation
    
    This SLUB free list pointer obfuscation code is modified from Brad
    Spengler/PaX Team's code in the last public patch of grsecurity/PaX
    based on my understanding of the code.  Changes or omissions from the
    original code are mine and don't reflect the original grsecurity/PaX
    code.
    
    This adds a per-cache random value to SLUB caches that is XORed with
    their freelist pointer address and value.  This adds nearly zero
    overhead and frustrates the very common heap overflow exploitation
    method of overwriting freelist pointers.
    
    A recent example of the attack is written up here:
    
      http://cyseclabs.com/blog/cve-2016-6187-heap-off-by-one-exploit
    
    and there is a section dedicated to the technique the book "A Guide to
    Kernel Exploitation: Attacking the Core".
    
    This is based on patches by Daniel Micay, and refactored to minimize the
    use of #ifdef.
    
    With 200-count cycles of "hackbench -g 20 -l 1000" I saw the following
    run times:
    
     before:
            mean 10.11882499999999999995
            variance .03320378329145728642
            stdev .18221905304181911048
    
      after:
            mean 10.12654000000000000014
            variance .04700556623115577889
            stdev .21680767106160192064
    
    The difference gets lost in the noise, but if the above is to be taken
    literally, using CONFIG_FREELIST_HARDENED is 0.07% slower.
    
    Link: http://lkml.kernel.org/r/20170802180609.GA66807@beast
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Suggested-by: Daniel Micay <danielmicay@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tycho Andersen <tycho@docker.com>
    Cc: Alexander Popov <alex.popov@linux.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index cc0faf3a90be..0783b622311e 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -115,6 +115,10 @@ struct kmem_cache {
 #endif
 #endif
 
+#ifdef CONFIG_SLAB_FREELIST_HARDENED
+	unsigned long random;
+#endif
+
 #ifdef CONFIG_NUMA
 	/*
 	 * Defragmentation by allocating from a remote node.

commit e6d0e1dcf5f07fb04704b87ffab749589d29cb02
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Thu Jul 6 15:36:34 2017 -0700

    mm/slub.c: wrap kmem_cache->cpu_partial in config CONFIG_SLUB_CPU_PARTIAL
    
    kmem_cache->cpu_partial is just used when CONFIG_SLUB_CPU_PARTIAL is
    set, so wrap it with config CONFIG_SLUB_CPU_PARTIAL will save some space
    on 32bit arch.
    
    This patch wraps kmem_cache->cpu_partial in config CONFIG_SLUB_CPU_PARTIAL
    and wraps its sysfs too.
    
    Link: http://lkml.kernel.org/r/20170502144533.10729-4-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a3e9492fed02..cc0faf3a90be 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -86,7 +86,9 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int object_size;	/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
+#ifdef CONFIG_SLUB_CPU_PARTIAL
 	int cpu_partial;	/* Number of per cpu partial objects to keep around */
+#endif
 	struct kmem_cache_order_objects oo;
 
 	/* Allocation and freeing of slabs */
@@ -131,6 +133,17 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
+#ifdef CONFIG_SLUB_CPU_PARTIAL
+#define slub_cpu_partial(s)		((s)->cpu_partial)
+#define slub_set_cpu_partial(s, n)		\
+({						\
+	slub_cpu_partial(s) = (n);		\
+})
+#else
+#define slub_cpu_partial(s)		(0)
+#define slub_set_cpu_partial(s, n)
+#endif // CONFIG_SLUB_CPU_PARTIAL
+
 #ifdef CONFIG_SYSFS
 #define SLAB_SUPPORTS_SYSFS
 void sysfs_slab_release(struct kmem_cache *);

commit a93cf07bc3fb4e7bc924d33c387dabc85086ea38
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Thu Jul 6 15:36:31 2017 -0700

    mm/slub.c: wrap cpu_slab->partial in CONFIG_SLUB_CPU_PARTIAL
    
    cpu_slab's field partial is used when CONFIG_SLUB_CPU_PARTIAL is set,
    which means we can save a pointer's space on each cpu for every slub
    item.
    
    This patch wraps cpu_slab->partial in CONFIG_SLUB_CPU_PARTIAL and wraps
    its sysfs use too.
    
    [akpm@linux-foundation.org: avoid strange 80-col tricks]
    Link: http://lkml.kernel.org/r/20170502144533.10729-3-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 070ff84240e7..a3e9492fed02 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -41,12 +41,31 @@ struct kmem_cache_cpu {
 	void **freelist;	/* Pointer to next available object */
 	unsigned long tid;	/* Globally unique transaction id */
 	struct page *page;	/* The slab from which we are allocating */
+#ifdef CONFIG_SLUB_CPU_PARTIAL
 	struct page *partial;	/* Partially allocated frozen slabs */
+#endif
 #ifdef CONFIG_SLUB_STATS
 	unsigned stat[NR_SLUB_STAT_ITEMS];
 #endif
 };
 
+#ifdef CONFIG_SLUB_CPU_PARTIAL
+#define slub_percpu_partial(c)		((c)->partial)
+
+#define slub_set_percpu_partial(c, p)		\
+({						\
+	slub_percpu_partial(c) = (p)->next;	\
+})
+
+#define slub_percpu_partial_read_once(c)     READ_ONCE(slub_percpu_partial(c))
+#else
+#define slub_percpu_partial(c)			NULL
+
+#define slub_set_percpu_partial(c, p)
+
+#define slub_percpu_partial_read_once(c)	NULL
+#endif // CONFIG_SLUB_CPU_PARTIAL
+
 /*
  * Word size structure that can be atomically updated or read and that
  * contains both the order and the number of objects that a slab of the

commit d3111e6cce6001e71ddc4737d0d412c2300043a2
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Thu Jul 6 15:36:28 2017 -0700

    mm/slub.c: pack red_left_pad with another int to save a word
    
    Patch series "try to save some memory for kmem_cache in some cases", v2.
    
    kmem_cache is a frequently used data in kernel.  During the code
    reading, I found maybe we could save some space in some cases.
    
    1. On 64bit arch, type int will occupy a word if it doesn't sit well.
    
    2. cpu_slab->partial is just used when CONFIG_SLUB_CPU_PARTIAL is set
    
    3. cpu_partial is just used when CONFIG_SLUB_CPU_PARTIAL is set, while
       just save some space on 32bit arch.
    
    This patch (of 3):
    
    On 64bit arch, struct is 8-bytes aligned, so int will occupy a word if
    it doesn't sit well.
    
    This patch pack red_left_pad with reserved to save 8 bytes for struct
    kmem_cache on a 64bit arch.
    
    Link: http://lkml.kernel.org/r/20170502144533.10729-2-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 93315d6b21a8..070ff84240e7 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -79,9 +79,9 @@ struct kmem_cache {
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	int reserved;		/* Reserved bytes at the end of slabs */
+	int red_left_pad;	/* Left redzone padding size */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
-	int red_left_pad;	/* Left redzone padding size */
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
 	struct work_struct kobj_remove_work;

commit 3b7b314053d021601940c50b07f5f1423ae67e21
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jun 23 15:08:52 2017 -0700

    slub: make sysfs file removal asynchronous
    
    Commit bf5eb3de3847 ("slub: separate out sysfs_slab_release() from
    sysfs_slab_remove()") made slub sysfs file removals synchronous to
    kmem_cache shutdown.
    
    Unfortunately, this created a possible ABBA deadlock between slab_mutex
    and sysfs draining mechanism triggering the following lockdep warning.
    
      ======================================================
      [ INFO: possible circular locking dependency detected ]
      4.10.0-test+ #48 Not tainted
      -------------------------------------------------------
      rmmod/1211 is trying to acquire lock:
       (s_active#120){++++.+}, at: [<ffffffff81308073>] kernfs_remove+0x23/0x40
    
      but task is already holding lock:
       (slab_mutex){+.+.+.}, at: [<ffffffff8120f691>] kmem_cache_destroy+0x41/0x2d0
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (slab_mutex){+.+.+.}:
             lock_acquire+0xf6/0x1f0
             __mutex_lock+0x75/0x950
             mutex_lock_nested+0x1b/0x20
             slab_attr_store+0x75/0xd0
             sysfs_kf_write+0x45/0x60
             kernfs_fop_write+0x13c/0x1c0
             __vfs_write+0x28/0x120
             vfs_write+0xc8/0x1e0
             SyS_write+0x49/0xa0
             entry_SYSCALL_64_fastpath+0x1f/0xc2
    
      -> #0 (s_active#120){++++.+}:
             __lock_acquire+0x10ed/0x1260
             lock_acquire+0xf6/0x1f0
             __kernfs_remove+0x254/0x320
             kernfs_remove+0x23/0x40
             sysfs_remove_dir+0x51/0x80
             kobject_del+0x18/0x50
             __kmem_cache_shutdown+0x3e6/0x460
             kmem_cache_destroy+0x1fb/0x2d0
             kvm_exit+0x2d/0x80 [kvm]
             vmx_exit+0x19/0xa1b [kvm_intel]
             SyS_delete_module+0x198/0x1f0
             entry_SYSCALL_64_fastpath+0x1f/0xc2
    
      other info that might help us debug this:
    
       Possible unsafe locking scenario:
    
             CPU0                    CPU1
             ----                    ----
        lock(slab_mutex);
                                     lock(s_active#120);
                                     lock(slab_mutex);
        lock(s_active#120);
    
       *** DEADLOCK ***
    
      2 locks held by rmmod/1211:
       #0:  (cpu_hotplug.dep_map){++++++}, at: [<ffffffff810a7877>] get_online_cpus+0x37/0x80
       #1:  (slab_mutex){+.+.+.}, at: [<ffffffff8120f691>] kmem_cache_destroy+0x41/0x2d0
    
      stack backtrace:
      CPU: 3 PID: 1211 Comm: rmmod Not tainted 4.10.0-test+ #48
      Hardware name: Hewlett-Packard HP Compaq Pro 6300 SFF/339A, BIOS K01 v02.05 05/07/2012
      Call Trace:
       print_circular_bug+0x1be/0x210
       __lock_acquire+0x10ed/0x1260
       lock_acquire+0xf6/0x1f0
       __kernfs_remove+0x254/0x320
       kernfs_remove+0x23/0x40
       sysfs_remove_dir+0x51/0x80
       kobject_del+0x18/0x50
       __kmem_cache_shutdown+0x3e6/0x460
       kmem_cache_destroy+0x1fb/0x2d0
       kvm_exit+0x2d/0x80 [kvm]
       vmx_exit+0x19/0xa1b [kvm_intel]
       SyS_delete_module+0x198/0x1f0
       ? SyS_delete_module+0x5/0x1f0
       entry_SYSCALL_64_fastpath+0x1f/0xc2
    
    It'd be the cleanest to deal with the issue by removing sysfs files
    without holding slab_mutex before the rest of shutdown; however, given
    the current code structure, it is pretty difficult to do so.
    
    This patch punts sysfs file removal to a work item.  Before commit
    bf5eb3de3847, the removal was punted to a RCU delayed work item which is
    executed after release.  Now, we're punting to a different work item on
    shutdown which still maintains the goal removing the sysfs files earlier
    when destroying kmem_caches.
    
    Link: http://lkml.kernel.org/r/20170620204512.GI21326@htj.duckdns.org
    Fixes: bf5eb3de3847 ("slub: separate out sysfs_slab_release() from sysfs_slab_remove()")
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 07ef550c6627..93315d6b21a8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -84,6 +84,7 @@ struct kmem_cache {
 	int red_left_pad;	/* Left redzone padding size */
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
+	struct work_struct kobj_remove_work;
 #endif
 #ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;

commit bf5eb3de3847ebcfd1fea7bc14072ef9f21d4e8d
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 22 15:41:11 2017 -0800

    slub: separate out sysfs_slab_release() from sysfs_slab_remove()
    
    Separate out slub sysfs removal and release, and call the former earlier
    from __kmem_cache_shutdown().  There's no reason to defer sysfs removal
    through RCU and this will later allow us to remove sysfs files way
    earlier during memory cgroup offline instead of release.
    
    Link: http://lkml.kernel.org/r/20170117235411.9408-3-tj@kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 75f56c2ef2d4..07ef550c6627 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -113,9 +113,9 @@ struct kmem_cache {
 
 #ifdef CONFIG_SYSFS
 #define SLAB_SUPPORTS_SYSFS
-void sysfs_slab_remove(struct kmem_cache *);
+void sysfs_slab_release(struct kmem_cache *);
 #else
-static inline void sysfs_slab_remove(struct kmem_cache *s)
+static inline void sysfs_slab_release(struct kmem_cache *s)
 {
 }
 #endif

commit 80a9201a5965f4715d5c09790862e0df84ce0614
Author: Alexander Potapenko <glider@google.com>
Date:   Thu Jul 28 15:49:07 2016 -0700

    mm, kasan: switch SLUB to stackdepot, enable memory quarantine for SLUB
    
    For KASAN builds:
     - switch SLUB allocator to using stackdepot instead of storing the
       allocation/deallocation stacks in the objects;
     - change the freelist hook so that parts of the freelist can be put
       into the quarantine.
    
    [aryabinin@virtuozzo.com: fixes]
      Link: http://lkml.kernel.org/r/1468601423-28676-1-git-send-email-aryabinin@virtuozzo.com
    Link: http://lkml.kernel.org/r/1468347165-41906-3-git-send-email-glider@google.com
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Kuthonuzo Luruo <kuthonuzo.luruo@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index cf501cf8e6db..75f56c2ef2d4 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -104,6 +104,10 @@ struct kmem_cache {
 	unsigned int *random_seq;
 #endif
 
+#ifdef CONFIG_KASAN
+	struct kasan_cache kasan_info;
+#endif
+
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 

commit c146a2b98eb5898eb0fab15a332257a4102ecae9
Author: Alexander Potapenko <glider@google.com>
Date:   Thu Jul 28 15:49:04 2016 -0700

    mm, kasan: account for object redzone in SLUB's nearest_obj()
    
    When looking up the nearest SLUB object for a given address, correctly
    calculate its offset if SLAB_RED_ZONE is enabled for that cache.
    
    Previously, when KASAN had detected an error on an object from a cache
    with SLAB_RED_ZONE set, the actual start address of the object was
    miscalculated, which led to random stacks having been reported.
    
    When looking up the nearest SLUB object for a given address, correctly
    calculate its offset if SLAB_RED_ZONE is enabled for that cache.
    
    Fixes: 7ed2f9e663854db ("mm, kasan: SLAB support")
    Link: http://lkml.kernel.org/r/1468347165-41906-2-git-send-email-glider@google.com
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Kuthonuzo Luruo <kuthonuzo.luruo@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5624c1f3eb0a..cf501cf8e6db 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -119,15 +119,17 @@ static inline void sysfs_slab_remove(struct kmem_cache *s)
 void object_err(struct kmem_cache *s, struct page *page,
 		u8 *object, char *reason);
 
+void *fixup_red_left(struct kmem_cache *s, void *p);
+
 static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
 				void *x) {
 	void *object = x - (x - page_address(page)) % cache->size;
 	void *last_object = page_address(page) +
 		(page->objects - 1) * cache->size;
-	if (unlikely(object > last_object))
-		return last_object;
-	else
-		return object;
+	void *result = (unlikely(object > last_object)) ? last_object : object;
+
+	result = fixup_red_left(cache, result);
+	return result;
 }
 
 #endif /* _LINUX_SLUB_DEF_H */

commit 210e7a43fa905bccafa9bb5966fba1d71f33eb8b
Author: Thomas Garnier <thgarnie@google.com>
Date:   Tue Jul 26 15:21:59 2016 -0700

    mm: SLUB freelist randomization
    
    Implements freelist randomization for the SLUB allocator.  It was
    previous implemented for the SLAB allocator.  Both use the same
    configuration option (CONFIG_SLAB_FREELIST_RANDOM).
    
    The list is randomized during initialization of a new set of pages.  The
    order on different freelist sizes is pre-computed at boot for
    performance.  Each kmem_cache has its own randomized freelist.
    
    This security feature reduces the predictability of the kernel SLUB
    allocator against heap overflows rendering attacks much less stable.
    
    For example these attacks exploit the predictability of the heap:
     - Linux Kernel CAN SLUB overflow (https://goo.gl/oMNWkU)
     - Exploiting Linux Kernel Heap corruptions (http://goo.gl/EXLn95)
    
    Performance results:
    
    slab_test impact is between 3% to 4% on average for 100000 attempts
    without smp.  It is a very focused testing, kernbench show the overall
    impact on the system is way lower.
    
    Before:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      100000 times kmalloc(8) -> 49 cycles kfree -> 77 cycles
      100000 times kmalloc(16) -> 51 cycles kfree -> 79 cycles
      100000 times kmalloc(32) -> 53 cycles kfree -> 83 cycles
      100000 times kmalloc(64) -> 62 cycles kfree -> 90 cycles
      100000 times kmalloc(128) -> 81 cycles kfree -> 97 cycles
      100000 times kmalloc(256) -> 98 cycles kfree -> 121 cycles
      100000 times kmalloc(512) -> 95 cycles kfree -> 122 cycles
      100000 times kmalloc(1024) -> 96 cycles kfree -> 126 cycles
      100000 times kmalloc(2048) -> 115 cycles kfree -> 140 cycles
      100000 times kmalloc(4096) -> 149 cycles kfree -> 171 cycles
      2. Kmalloc: alloc/free test
      100000 times kmalloc(8)/kfree -> 70 cycles
      100000 times kmalloc(16)/kfree -> 70 cycles
      100000 times kmalloc(32)/kfree -> 70 cycles
      100000 times kmalloc(64)/kfree -> 70 cycles
      100000 times kmalloc(128)/kfree -> 70 cycles
      100000 times kmalloc(256)/kfree -> 69 cycles
      100000 times kmalloc(512)/kfree -> 70 cycles
      100000 times kmalloc(1024)/kfree -> 73 cycles
      100000 times kmalloc(2048)/kfree -> 72 cycles
      100000 times kmalloc(4096)/kfree -> 71 cycles
    
    After:
    
      Single thread testing
      =====================
      1. Kmalloc: Repeatedly allocate then free test
      100000 times kmalloc(8) -> 57 cycles kfree -> 78 cycles
      100000 times kmalloc(16) -> 61 cycles kfree -> 81 cycles
      100000 times kmalloc(32) -> 76 cycles kfree -> 93 cycles
      100000 times kmalloc(64) -> 83 cycles kfree -> 94 cycles
      100000 times kmalloc(128) -> 106 cycles kfree -> 107 cycles
      100000 times kmalloc(256) -> 118 cycles kfree -> 117 cycles
      100000 times kmalloc(512) -> 114 cycles kfree -> 116 cycles
      100000 times kmalloc(1024) -> 115 cycles kfree -> 118 cycles
      100000 times kmalloc(2048) -> 147 cycles kfree -> 131 cycles
      100000 times kmalloc(4096) -> 214 cycles kfree -> 161 cycles
      2. Kmalloc: alloc/free test
      100000 times kmalloc(8)/kfree -> 66 cycles
      100000 times kmalloc(16)/kfree -> 66 cycles
      100000 times kmalloc(32)/kfree -> 66 cycles
      100000 times kmalloc(64)/kfree -> 66 cycles
      100000 times kmalloc(128)/kfree -> 65 cycles
      100000 times kmalloc(256)/kfree -> 67 cycles
      100000 times kmalloc(512)/kfree -> 67 cycles
      100000 times kmalloc(1024)/kfree -> 64 cycles
      100000 times kmalloc(2048)/kfree -> 67 cycles
      100000 times kmalloc(4096)/kfree -> 67 cycles
    
    Kernbench, before:
    
      Average Optimal load -j 12 Run (std deviation):
      Elapsed Time 101.873 (1.16069)
      User Time 1045.22 (1.60447)
      System Time 88.969 (0.559195)
      Percent CPU 1112.9 (13.8279)
      Context Switches 189140 (2282.15)
      Sleeps 99008.6 (768.091)
    
    After:
    
      Average Optimal load -j 12 Run (std deviation):
      Elapsed Time 102.47 (0.562732)
      User Time 1045.3 (1.34263)
      System Time 88.311 (0.342554)
      Percent CPU 1105.8 (6.49444)
      Context Switches 189081 (2355.78)
      Sleeps 99231.5 (800.358)
    
    Link: http://lkml.kernel.org/r/1464295031-26375-3-git-send-email-thgarnie@google.com
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index d1faa019c02a..5624c1f3eb0a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -99,6 +99,11 @@ struct kmem_cache {
 	 */
 	int remote_node_defrag_ratio;
 #endif
+
+#ifdef CONFIG_SLAB_FREELIST_RANDOM
+	unsigned int *random_seq;
+#endif
+
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 

commit d96c84f8d27ce57ff08f12b9654d9f505a8cce6e
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu May 26 15:16:14 2016 -0700

    mm: slub: remove unused virt_to_obj()
    
    It's unused since commit 7ed2f9e66385 ("mm, kasan: SLAB support")
    
    Link: http://lkml.kernel.org/r/1464020961-2242-1-git-send-email-aryabinin@virtuozzo.com
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 665cd0cd18b8..d1faa019c02a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -111,22 +111,6 @@ static inline void sysfs_slab_remove(struct kmem_cache *s)
 }
 #endif
 
-
-/**
- * virt_to_obj - returns address of the beginning of object.
- * @s: object's kmem_cache
- * @slab_page: address of slab page
- * @x: address within object memory range
- *
- * Returns address of the beginning of object
- */
-static inline void *virt_to_obj(struct kmem_cache *s,
-				const void *slab_page,
-				const void *x)
-{
-	return (void *)x - ((x - slab_page) % s->size);
-}
-
 void object_err(struct kmem_cache *s, struct page *page,
 		u8 *object, char *reason);
 

commit 7ed2f9e663854db313f177a511145630e398b402
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:21:59 2016 -0700

    mm, kasan: SLAB support
    
    Add KASAN hooks to SLAB allocator.
    
    This patch is based on the "mm: kasan: unified support for SLUB and SLAB
    allocators" patch originally prepared by Dmitry Chernenkov.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index ac5143f95ee6..665cd0cd18b8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -130,4 +130,15 @@ static inline void *virt_to_obj(struct kmem_cache *s,
 void object_err(struct kmem_cache *s, struct page *page,
 		u8 *object, char *reason);
 
+static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
+				void *x) {
+	void *object = x - (x - page_address(page)) % cache->size;
+	void *last_object = page_address(page) +
+		(page->objects - 1) * cache->size;
+	if (unlikely(object > last_object))
+		return last_object;
+	else
+		return object;
+}
+
 #endif /* _LINUX_SLUB_DEF_H */

commit d86bd1bece6fc41d59253002db5441fe960a37f6
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Mar 15 14:55:12 2016 -0700

    mm/slub: support left redzone
    
    SLUB already has a redzone debugging feature.  But it is only positioned
    at the end of object (aka right redzone) so it cannot catch left oob.
    Although current object's right redzone acts as left redzone of next
    object, first object in a slab cannot take advantage of this effect.
    This patch explicitly adds a left red zone to each object to detect left
    oob more precisely.
    
    Background:
    
    Someone complained to me that left OOB doesn't catch even if KASAN is
    enabled which does page allocation debugging.  That page is out of our
    control so it would be allocated when left OOB happens and, in this
    case, we can't find OOB.  Moreover, SLUB debugging feature can be
    enabled without page allocator debugging and, in this case, we will miss
    that OOB.
    
    Before trying to implement, I expected that changes would be too
    complex, but, it doesn't look that complex to me now.  Almost changes
    are applied to debug specific functions so I feel okay.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index b7e57927f521..ac5143f95ee6 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -81,6 +81,7 @@ struct kmem_cache {
 	int reserved;		/* Reserved bytes at the end of slabs */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
+	int red_left_pad;	/* Left redzone padding size */
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
 #endif

commit 127424c86bb6cb87f0b563d9fdcfbbaf3c86ecec
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 20 15:02:32 2016 -0800

    mm: memcontrol: move kmem accounting code to CONFIG_MEMCG
    
    The cgroup2 memory controller will account important in-kernel memory
    consumers per default.  Move all necessary components to CONFIG_MEMCG.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 33885118523c..b7e57927f521 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -84,7 +84,7 @@ struct kmem_cache {
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
 #endif
-#ifdef CONFIG_MEMCG_KMEM
+#ifdef CONFIG_MEMCG
 	struct memcg_cache_params memcg_params;
 	int max_attr_size; /* for propagation, maximum size of a stored attr */
 #ifdef CONFIG_SYSFS

commit 75c66def8d815201aa0386ecc7c66a5c8dbca1ee
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:39:35 2015 -0800

    mm: slub: share object_err function
    
    Remove static and add function declarations to linux/slub_def.h so it
    could be used by kernel address sanitizer.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index db7d5de00c5f..33885118523c 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -126,4 +126,7 @@ static inline void *virt_to_obj(struct kmem_cache *s,
 	return (void *)x - ((x - slab_page) % s->size);
 }
 
+void object_err(struct kmem_cache *s, struct page *page,
+		u8 *object, char *reason);
+
 #endif /* _LINUX_SLUB_DEF_H */

commit 912f5fbf1d3060f25d6994aed0265c55b974b2e9
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:39:31 2015 -0800

    mm: slub: introduce virt_to_obj function
    
    virt_to_obj takes kmem_cache address, address of slab page, address x
    pointing somewhere inside slab object, and returns address of the
    beginning of object.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 9abf04ed0999..db7d5de00c5f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -110,4 +110,20 @@ static inline void sysfs_slab_remove(struct kmem_cache *s)
 }
 #endif
 
+
+/**
+ * virt_to_obj - returns address of the beginning of object.
+ * @s: object's kmem_cache
+ * @slab_page: address of slab page
+ * @x: address within object memory range
+ *
+ * Returns address of the beginning of object
+ */
+static inline void *virt_to_obj(struct kmem_cache *s,
+				const void *slab_page,
+				const void *x)
+{
+	return (void *)x - ((x - slab_page) % s->size);
+}
+
 #endif /* _LINUX_SLUB_DEF_H */

commit f7ce3190c4a35bf887adb7a1aa1ba899b679872d
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:20 2015 -0800

    slab: embed memcg_cache_params to kmem_cache
    
    Currently, kmem_cache stores a pointer to struct memcg_cache_params
    instead of embedding it.  The rationale is to save memory when kmem
    accounting is disabled.  However, the memcg_cache_params has shrivelled
    drastically since it was first introduced:
    
    * Initially:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct kmem_cache *memcg_caches[0];
                    struct {
                            struct mem_cgroup *memcg;
                            struct list_head list;
                            struct kmem_cache *root_cache;
                            bool dead;
                            atomic_t nr_pages;
                            struct work_struct destroy;
                    };
            };
    };
    
    * Now:
    
    struct memcg_cache_params {
            bool is_root_cache;
            union {
                    struct {
                            struct rcu_head rcu_head;
                            struct kmem_cache *memcg_caches[0];
                    };
                    struct {
                            struct mem_cgroup *memcg;
                            struct kmem_cache *root_cache;
                    };
            };
    };
    
    So the memory saving does not seem to be a clear win anymore.
    
    OTOH, keeping a pointer to memcg_cache_params struct instead of embedding
    it results in touching one more cache line on kmem alloc/free hot paths.
    Besides, it makes linking kmem caches in a list chained by a field of
    struct memcg_cache_params really painful due to a level of indirection,
    while I want to make them linked in the following patch.  That said, let
    us embed it.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index d82abd40a3c0..9abf04ed0999 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -85,7 +85,7 @@ struct kmem_cache {
 	struct kobject kobj;	/* For sysfs */
 #endif
 #ifdef CONFIG_MEMCG_KMEM
-	struct memcg_cache_params *memcg_params;
+	struct memcg_cache_params memcg_params;
 	int max_attr_size; /* for propagation, maximum size of a stored attr */
 #ifdef CONFIG_SYSFS
 	struct kset *memcg_kset;

commit 41a212859a4dd583d3aa032cdd3efa564c4f189f
Author: Christoph Lameter <cl@linux.com>
Date:   Tue May 6 12:50:08 2014 -0700

    slub: use sysfs'es release mechanism for kmem_cache
    
    debugobjects warning during netfilter exit:
    
        ------------[ cut here ]------------
        WARNING: CPU: 6 PID: 4178 at lib/debugobjects.c:260 debug_print_object+0x8d/0xb0()
        ODEBUG: free active (active state 0) object type: timer_list hint: delayed_work_timer_fn+0x0/0x20
        Modules linked in:
        CPU: 6 PID: 4178 Comm: kworker/u16:2 Tainted: G        W 3.11.0-next-20130906-sasha #3984
        Workqueue: netns cleanup_net
        Call Trace:
          dump_stack+0x52/0x87
          warn_slowpath_common+0x8c/0xc0
          warn_slowpath_fmt+0x46/0x50
          debug_print_object+0x8d/0xb0
          __debug_check_no_obj_freed+0xa5/0x220
          debug_check_no_obj_freed+0x15/0x20
          kmem_cache_free+0x197/0x340
          kmem_cache_destroy+0x86/0xe0
          nf_conntrack_cleanup_net_list+0x131/0x170
          nf_conntrack_pernet_exit+0x5d/0x70
          ops_exit_list+0x5e/0x70
          cleanup_net+0xfb/0x1c0
          process_one_work+0x338/0x550
          worker_thread+0x215/0x350
          kthread+0xe7/0xf0
          ret_from_fork+0x7c/0xb0
    
    Also during dcookie cleanup:
    
        WARNING: CPU: 12 PID: 9725 at lib/debugobjects.c:260 debug_print_object+0x8c/0xb0()
        ODEBUG: free active (active state 0) object type: timer_list hint: delayed_work_timer_fn+0x0/0x20
        Modules linked in:
        CPU: 12 PID: 9725 Comm: trinity-c141 Not tainted 3.15.0-rc2-next-20140423-sasha-00018-gc4ff6c4 #408
        Call Trace:
          dump_stack (lib/dump_stack.c:52)
          warn_slowpath_common (kernel/panic.c:430)
          warn_slowpath_fmt (kernel/panic.c:445)
          debug_print_object (lib/debugobjects.c:262)
          __debug_check_no_obj_freed (lib/debugobjects.c:697)
          debug_check_no_obj_freed (lib/debugobjects.c:726)
          kmem_cache_free (mm/slub.c:2689 mm/slub.c:2717)
          kmem_cache_destroy (mm/slab_common.c:363)
          dcookie_unregister (fs/dcookies.c:302 fs/dcookies.c:343)
          event_buffer_release (arch/x86/oprofile/../../../drivers/oprofile/event_buffer.c:153)
          __fput (fs/file_table.c:217)
          ____fput (fs/file_table.c:253)
          task_work_run (kernel/task_work.c:125 (discriminator 1))
          do_notify_resume (include/linux/tracehook.h:196 arch/x86/kernel/signal.c:751)
          int_signal (arch/x86/kernel/entry_64.S:807)
    
    Sysfs has a release mechanism.  Use that to release the kmem_cache
    structure if CONFIG_SYSFS is enabled.
    
    Only slub is changed - slab currently only supports /proc/slabinfo and
    not /sys/kernel/slab/*.  We talked about adding that and someone was
    working on it.
    
    [akpm@linux-foundation.org: fix CONFIG_SYSFS=n build]
    [akpm@linux-foundation.org: fix CONFIG_SYSFS=n build even more]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Acked-by: Greg KH <greg@kroah.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f2f7398848cf..d82abd40a3c0 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -101,4 +101,13 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
+#ifdef CONFIG_SYSFS
+#define SLAB_SUPPORTS_SYSFS
+void sysfs_slab_remove(struct kmem_cache *);
+#else
+static inline void sysfs_slab_remove(struct kmem_cache *s)
+{
+}
+#endif
+
 #endif /* _LINUX_SLUB_DEF_H */

commit 9a41707bd3a0811919000daf094e9d50ea65f7da
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Mon Apr 7 15:39:31 2014 -0700

    slub: rework sysfs layout for memcg caches
    
    Currently, we try to arrange sysfs entries for memcg caches in the same
    manner as for global caches.  Apart from turning /sys/kernel/slab into a
    mess when there are a lot of kmem-active memcgs created, it actually
    does not work properly - we won't create more than one link to a memcg
    cache in case its parent is merged with another cache.  For instance, if
    A is a root cache merged with another root cache B, we will have the
    following sysfs setup:
    
      X
      A -> X
      B -> X
    
    where X is some unique id (see create_unique_id()).  Now if memcgs M and
    N start to allocate from cache A (or B, which is the same), we will get:
    
      X
      X:M
      X:N
      A -> X
      B -> X
      A:M -> X:M
      A:N -> X:N
    
    Since B is an alias for A, we won't get entries B:M and B:N, which is
    confusing.
    
    It is more logical to have entries for memcg caches under the
    corresponding root cache's sysfs directory.  This would allow us to keep
    sysfs layout clean, and avoid such inconsistencies like one described
    above.
    
    This patch does the trick.  It creates a "cgroup" kset in each root
    cache kobject to keep its children caches there.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Glauber Costa <glommer@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f56bfa9e4526..f2f7398848cf 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -87,6 +87,9 @@ struct kmem_cache {
 #ifdef CONFIG_MEMCG_KMEM
 	struct memcg_cache_params *memcg_params;
 	int max_attr_size; /* for propagation, maximum size of a stored attr */
+#ifdef CONFIG_SYSFS
+	struct kset *memcg_kset;
+#endif
 #endif
 
 #ifdef CONFIG_NUMA

commit 24f971abbda045c24d5d6f2438a7785567d2fde9
Merge: 3bab0bf045e1 721ae22ae1a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 22 08:10:34 2013 -0800

    Merge branch 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull SLAB changes from Pekka Enberg:
     "The patches from Joonsoo Kim switch mm/slab.c to use 'struct page' for
      slab internals similar to mm/slub.c.  This reduces memory usage and
      improves performance:
    
        https://lkml.org/lkml/2013/10/16/155
    
      Rest of the changes are bug fixes from various people"
    
    * 'slab/next' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux: (21 commits)
      mm, slub: fix the typo in mm/slub.c
      mm, slub: fix the typo in include/linux/slub_def.h
      slub: Handle NULL parameter in kmem_cache_flags
      slab: replace non-existing 'struct freelist *' with 'void *'
      slab: fix to calm down kmemleak warning
      slub: proper kmemleak tracking if CONFIG_SLUB_DEBUG disabled
      slab: rename slab_bufctl to slab_freelist
      slab: remove useless statement for checking pfmemalloc
      slab: use struct page for slab management
      slab: replace free and inuse in struct slab with newly introduced active
      slab: remove SLAB_LIMIT
      slab: remove kmem_bufctl_t
      slab: change the management method of free objects of the slab
      slab: use __GFP_COMP flag for allocating slab pages
      slab: use well-defined macro, virt_to_slab()
      slab: overloading the RCU head over the LRU for RCU free
      slab: remove cachep in struct slab_rcu
      slab: remove nodeid in struct slab
      slab: remove colouroff in struct slab
      slab: change return type of kmem_getpages() to struct page
      ...

commit a941f8360f200d6849b292f9dc50250bca531c0e
Author: Zhi Yong Wu <wuzhy@linux.vnet.ibm.com>
Date:   Fri Nov 8 20:47:36 2013 +0800

    mm, slub: fix the typo in include/linux/slub_def.h
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Zhi Yong Wu <wuzhy@linux.vnet.ibm.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 027276fa8713..2dc4e78fc234 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -17,7 +17,7 @@
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
 	ALLOC_SLOWPATH,		/* Allocation by getting a new cpu slab */
-	FREE_FASTPATH,		/* Free to cpu slub */
+	FREE_FASTPATH,		/* Free to cpu slab */
 	FREE_SLOWPATH,		/* Freeing not to cpu slab */
 	FREE_FROZEN,		/* Freeing to frozen slab */
 	FREE_ADD_PARTIAL,	/* Freeing moves slab to partial list */

commit 76b6f3d255a327383c89cb8c8384872dd4a0a054
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 4 16:35:35 2013 +0000

    slub: remove verify_mem_not_deleted()
    
    I do not see any user for this code in the tree.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 901fb6eb7467..cc0b67eada42 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -98,17 +98,4 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
-/**
- * Calling this on allocated memory will check that the memory
- * is expected to be in use, and print warnings if not.
- */
-#ifdef CONFIG_SLUB_DEBUG
-extern bool verify_mem_not_deleted(const void *x);
-#else
-static inline bool verify_mem_not_deleted(const void *x)
-{
-	return true;
-}
-#endif
-
 #endif /* _LINUX_SLUB_DEF_H */

commit f1b6eb6e6be149b40ebb013f5bfe2ac86b6f1c1b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 4 16:35:34 2013 +0000

    mm/sl[aou]b: Move kmallocXXX functions to common code
    
    The kmalloc* functions of all slab allcoators are similar now so
    lets move them into slab.h. This requires some function naming changes
    in slob.
    
    As a results of this patch there is a common set of functions for
    all allocators. Also means that kmalloc_large() is now available
    in general to perform large order allocations that go directly
    via the page allocator. kmalloc_large() can be substituted if
    kmalloc() throws warnings because of too large allocations.
    
    kmalloc_large() has exactly the same semantics as kmalloc but
    can only used for allocations > PAGE_SIZE.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 027276fa8713..901fb6eb7467 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -6,14 +6,8 @@
  *
  * (C) 2007 SGI, Christoph Lameter
  */
-#include <linux/types.h>
-#include <linux/gfp.h>
-#include <linux/bug.h>
-#include <linux/workqueue.h>
 #include <linux/kobject.h>
 
-#include <linux/kmemleak.h>
-
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
 	ALLOC_SLOWPATH,		/* Allocation by getting a new cpu slab */
@@ -104,20 +98,6 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
-void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
-void *__kmalloc(size_t size, gfp_t flags);
-
-static __always_inline void *
-kmalloc_order(size_t size, gfp_t flags, unsigned int order)
-{
-	void *ret;
-
-	flags |= (__GFP_COMP | __GFP_KMEMCG);
-	ret = (void *) __get_free_pages(flags, order);
-	kmemleak_alloc(ret, size, 1, flags);
-	return ret;
-}
-
 /**
  * Calling this on allocated memory will check that the memory
  * is expected to be in use, and print warnings if not.
@@ -131,81 +111,4 @@ static inline bool verify_mem_not_deleted(const void *x)
 }
 #endif
 
-#ifdef CONFIG_TRACING
-extern void *
-kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size);
-extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order);
-#else
-static __always_inline void *
-kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
-{
-	return kmem_cache_alloc(s, gfpflags);
-}
-
-static __always_inline void *
-kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
-{
-	return kmalloc_order(size, flags, order);
-}
-#endif
-
-static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
-{
-	unsigned int order = get_order(size);
-	return kmalloc_order_trace(size, flags, order);
-}
-
-static __always_inline void *kmalloc(size_t size, gfp_t flags)
-{
-	if (__builtin_constant_p(size)) {
-		if (size > KMALLOC_MAX_CACHE_SIZE)
-			return kmalloc_large(size, flags);
-
-		if (!(flags & GFP_DMA)) {
-			int index = kmalloc_index(size);
-
-			if (!index)
-				return ZERO_SIZE_PTR;
-
-			return kmem_cache_alloc_trace(kmalloc_caches[index],
-					flags, size);
-		}
-	}
-	return __kmalloc(size, flags);
-}
-
-#ifdef CONFIG_NUMA
-void *__kmalloc_node(size_t size, gfp_t flags, int node);
-void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
-
-#ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
-					   gfp_t gfpflags,
-					   int node, size_t size);
-#else
-static __always_inline void *
-kmem_cache_alloc_node_trace(struct kmem_cache *s,
-			      gfp_t gfpflags,
-			      int node, size_t size)
-{
-	return kmem_cache_alloc_node(s, gfpflags, node);
-}
-#endif
-
-static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
-{
-	if (__builtin_constant_p(size) &&
-		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
-		int index = kmalloc_index(size);
-
-		if (!index)
-			return ZERO_SIZE_PTR;
-
-		return kmem_cache_alloc_node_trace(kmalloc_caches[index],
-			       flags, node, size);
-	}
-	return __kmalloc_node(size, flags, node);
-}
-#endif
-
 #endif /* _LINUX_SLUB_DEF_H */

commit ca34956b804b7554fc4e88826773380d9d5122a8
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common definition for kmem_cache_node
    
    Put the definitions for the kmem_cache_node structures together so that
    we have one structure. That will allow us to create more common fields in
    the future which could yield more opportunities to share code.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 16341e5316de..027276fa8713 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -53,17 +53,6 @@ struct kmem_cache_cpu {
 #endif
 };
 
-struct kmem_cache_node {
-	spinlock_t list_lock;	/* Protect partial list and nr_partial */
-	unsigned long nr_partial;
-	struct list_head partial;
-#ifdef CONFIG_SLUB_DEBUG
-	atomic_long_t nr_slabs;
-	atomic_long_t total_objects;
-	struct list_head full;
-#endif
-};
-
 /*
  * Word size structure that can be atomically updated or read and that
  * contains both the order and the number of objects that a slab of the

commit 2c59dd6544212faa5ce761920d2251f4152f408d
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common Kmalloc cache determination
    
    Extract the optimized lookup functions from slub and put them into
    slab_common.c. Then make slab use these functions as well.
    
    Joonsoo notes that this fixes some issues with constant folding which
    also reduces the code size for slub.
    
    https://lkml.org/lkml/2012/10/20/82
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 3701896f7f8a..16341e5316de 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -115,29 +115,6 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
-#ifdef CONFIG_ZONE_DMA
-#define SLUB_DMA __GFP_DMA
-#else
-/* Disable DMA functionality */
-#define SLUB_DMA (__force gfp_t)0
-#endif
-
-/*
- * Find the slab cache for a given combination of allocation flags and size.
- *
- * This ought to end up with a global pointer to the right cache
- * in kmalloc_caches.
- */
-static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
-{
-	int index = kmalloc_index(size);
-
-	if (index == 0)
-		return NULL;
-
-	return kmalloc_caches[index];
-}
-
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
@@ -195,13 +172,14 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 		if (size > KMALLOC_MAX_CACHE_SIZE)
 			return kmalloc_large(size, flags);
 
-		if (!(flags & SLUB_DMA)) {
-			struct kmem_cache *s = kmalloc_slab(size);
+		if (!(flags & GFP_DMA)) {
+			int index = kmalloc_index(size);
 
-			if (!s)
+			if (!index)
 				return ZERO_SIZE_PTR;
 
-			return kmem_cache_alloc_trace(s, flags, size);
+			return kmem_cache_alloc_trace(kmalloc_caches[index],
+					flags, size);
 		}
 	}
 	return __kmalloc(size, flags);
@@ -228,13 +206,14 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) &&
-		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & SLUB_DMA)) {
-			struct kmem_cache *s = kmalloc_slab(size);
+		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
+		int index = kmalloc_index(size);
 
-		if (!s)
+		if (!index)
 			return ZERO_SIZE_PTR;
 
-		return kmem_cache_alloc_node_trace(s, flags, node, size);
+		return kmem_cache_alloc_node_trace(kmalloc_caches[index],
+			       flags, node, size);
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit 9425c58e5445277699ff3c2a87bac1cfebc1b48d
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:12:17 2013 +0000

    slab: Common definition for the array of kmalloc caches
    
    Have a common definition fo the kmalloc cache arrays in
    SLAB and SLUB
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 032028ef9a34..3701896f7f8a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -122,12 +122,6 @@ struct kmem_cache {
 #define SLUB_DMA (__force gfp_t)0
 #endif
 
-/*
- * We keep the general caches in an array of slab caches that are used for
- * 2^x bytes of allocations.
- */
-extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
-
 /*
  * Find the slab cache for a given combination of allocation flags and size.
  *

commit 95a05b428cc675694321c8f762591984f3fd2b1e
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common constants for kmalloc boundaries
    
    Standardize the constants that describe the smallest and largest
    object kept in the kmalloc arrays for SLAB and SLUB.
    
    Differentiate between the maximum size for which a slab cache is used
    (KMALLOC_MAX_CACHE_SIZE) and the maximum allocatable size
    (KMALLOC_MAX_SIZE, KMALLOC_MAX_ORDER).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 99c3e05ff1f0..032028ef9a34 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -115,19 +115,6 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
-/*
- * Maximum kmalloc object size handled by SLUB. Larger object allocations
- * are passed through to the page allocator. The page allocator "fastpath"
- * is relatively slow so we need this value sufficiently high so that
- * performance critical objects are allocated through the SLUB fastpath.
- *
- * This should be dropped to PAGE_SIZE / 2 once the page allocator
- * "fastpath" becomes competitive with the slab allocator fastpaths.
- */
-#define SLUB_MAX_SIZE (2 * PAGE_SIZE)
-
-#define SLUB_PAGE_SHIFT (PAGE_SHIFT + 2)
-
 #ifdef CONFIG_ZONE_DMA
 #define SLUB_DMA __GFP_DMA
 #else
@@ -139,7 +126,7 @@ struct kmem_cache {
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache *kmalloc_caches[SLUB_PAGE_SHIFT];
+extern struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
 
 /*
  * Find the slab cache for a given combination of allocation flags and size.
@@ -211,7 +198,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
-		if (size > SLUB_MAX_SIZE)
+		if (size > KMALLOC_MAX_CACHE_SIZE)
 			return kmalloc_large(size, flags);
 
 		if (!(flags & SLUB_DMA)) {
@@ -247,7 +234,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) &&
-		size <= SLUB_MAX_SIZE && !(flags & SLUB_DMA)) {
+		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)

commit ce6a50263d4ddeba1f0d08f16716a82770c03690
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 10 19:14:19 2013 +0000

    slab: Common kmalloc slab index determination
    
    Extract the function to determine the index of the slab within
    the array of kmalloc caches as well as a function to determine
    maximum object size from the nr of the kmalloc slab.
    
    This is used here only to simplify slub bootstrap but will
    be used later also for SLAB.
    
    Acked-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 9db4825cd393..99c3e05ff1f0 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -115,17 +115,6 @@ struct kmem_cache {
 	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
-/*
- * Kmalloc subsystem.
- */
-#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
-#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
-#else
-#define KMALLOC_MIN_SIZE 8
-#endif
-
-#define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
-
 /*
  * Maximum kmalloc object size handled by SLUB. Larger object allocations
  * are passed through to the page allocator. The page allocator "fastpath"
@@ -152,58 +141,6 @@ struct kmem_cache {
  */
 extern struct kmem_cache *kmalloc_caches[SLUB_PAGE_SHIFT];
 
-/*
- * Sorry that the following has to be that ugly but some versions of GCC
- * have trouble with constant propagation and loops.
- */
-static __always_inline int kmalloc_index(size_t size)
-{
-	if (!size)
-		return 0;
-
-	if (size <= KMALLOC_MIN_SIZE)
-		return KMALLOC_SHIFT_LOW;
-
-	if (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)
-		return 1;
-	if (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)
-		return 2;
-	if (size <=          8) return 3;
-	if (size <=         16) return 4;
-	if (size <=         32) return 5;
-	if (size <=         64) return 6;
-	if (size <=        128) return 7;
-	if (size <=        256) return 8;
-	if (size <=        512) return 9;
-	if (size <=       1024) return 10;
-	if (size <=   2 * 1024) return 11;
-	if (size <=   4 * 1024) return 12;
-/*
- * The following is only needed to support architectures with a larger page
- * size than 4k. We need to support 2 * PAGE_SIZE here. So for a 64k page
- * size we would have to go up to 128k.
- */
-	if (size <=   8 * 1024) return 13;
-	if (size <=  16 * 1024) return 14;
-	if (size <=  32 * 1024) return 15;
-	if (size <=  64 * 1024) return 16;
-	if (size <= 128 * 1024) return 17;
-	if (size <= 256 * 1024) return 18;
-	if (size <= 512 * 1024) return 19;
-	if (size <= 1024 * 1024) return 20;
-	if (size <=  2 * 1024 * 1024) return 21;
-	BUG();
-	return -1; /* Will never be reached */
-
-/*
- * What we really wanted to do and cannot do because of compiler issues is:
- *	int i;
- *	for (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++)
- *		if (size <= (1 << i))
- *			return i;
- */
-}
-
 /*
  * Find the slab cache for a given combination of allocation flags and size.
  *

commit 107dab5c92d5f9c3afe962036e47c207363255c7
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:23:05 2012 -0800

    slub: slub-specific propagation changes
    
    SLUB allows us to tune a particular cache behavior with sysfs-based
    tunables.  When creating a new memcg cache copy, we'd like to preserve any
    tunables the parent cache already had.
    
    This can be done by tapping into the store attribute function provided by
    the allocator.  We of course don't need to mess with read-only fields.
    Since the attributes can have multiple types and are stored internally by
    sysfs, the best strategy is to issue a ->show() in the root cache, and
    then ->store() in the memcg cache.
    
    The drawback of that, is that sysfs can allocate up to a page in buffering
    for show(), that we are likely not to need, but also can't guarantee.  To
    avoid always allocating a page for that, we can update the caches at store
    time with the maximum attribute size ever stored to the root cache.  We
    will then get a buffer big enough to hold it.  The corolary to this, is
    that if no stores happened, nothing will be propagated.
    
    It can also happen that a root cache has its tunables updated during
    normal system operation.  In this case, we will propagate the change to
    all caches that are already active.
    
    [akpm@linux-foundation.org: tweak code to avoid __maybe_unused]
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 364ba6c9fe21..9db4825cd393 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -103,6 +103,7 @@ struct kmem_cache {
 #endif
 #ifdef CONFIG_MEMCG_KMEM
 	struct memcg_cache_params *memcg_params;
+	int max_attr_size; /* for propagation, maximum size of a stored attr */
 #endif
 
 #ifdef CONFIG_NUMA

commit d79923fad95b0cdf7770e024677180c734cb7148
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:48 2012 -0800

    sl[au]b: allocate objects from memcg cache
    
    We are able to match a cache allocation to a particular memcg.  If the
    task doesn't change groups during the allocation itself - a rare event,
    this will give us a good picture about who is the first group to touch a
    cache page.
    
    This patch uses the now available infrastructure by calling
    memcg_kmem_get_cache() before all the cache allocations.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 961e72eab907..364ba6c9fe21 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -225,7 +225,10 @@ void *__kmalloc(size_t size, gfp_t flags);
 static __always_inline void *
 kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 {
-	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
+	void *ret;
+
+	flags |= (__GFP_COMP | __GFP_KMEMCG);
+	ret = (void *) __get_free_pages(flags, order);
 	kmemleak_alloc(ret, size, 1, flags);
 	return ret;
 }

commit ba6c496ed834a37a26fc6fc87fc9aecb0fa0014d
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:27 2012 -0800

    slab/slub: struct memcg_params
    
    For the kmem slab controller, we need to record some extra information in
    the kmem_cache structure.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Suleiman Souhlal <suleiman@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index df448adb7283..961e72eab907 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -101,6 +101,9 @@ struct kmem_cache {
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
 #endif
+#ifdef CONFIG_MEMCG_KMEM
+	struct memcg_cache_params *memcg_params;
+#endif
 
 #ifdef CONFIG_NUMA
 	/*

commit 3b0efdfa1e719303536c04d9abca43abeb40f80a
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 13 10:24:57 2012 -0500

    mm, sl[aou]b: Extract common fields from struct kmem_cache
    
    Define a struct that describes common fields used in all slab allocators.
    A slab allocator either uses the common definition (like SLOB) or is
    required to provide members of kmem_cache with the definition given.
    
    After that it will be possible to share code that
    only operates on those fields of kmem_cache.
    
    The patch basically takes the slob definition of kmem cache and
    uses the field namees for the other allocators.
    
    It also standardizes the names used for basic object lengths in
    allocators:
    
    object_size     Struct size specified at kmem_cache_create. Basically
                    the payload expected to be used by the subsystem.
    
    size            The size of memory allocator for each object. This size
                    is larger than object_size and includes padding, alignment
                    and extra metadata for each object (f.e. for debugging
                    and rcu).
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index ebdcf4ba42ee..df448adb7283 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -82,7 +82,7 @@ struct kmem_cache {
 	unsigned long flags;
 	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
-	int objsize;		/* The size of an object without meta data */
+	int object_size;	/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
 	int cpu_partial;	/* Number of per cpu partial objects to keep around */
 	struct kmem_cache_order_objects oo;

commit ec3ab083a7a004282ee374bdaeb0aa603521b8eb
Author: Christoph Lameter <cl@linux.com>
Date:   Wed May 9 10:09:56 2012 -0500

    slub: Get rid of the node field
    
    The node field is always page_to_nid(c->page). So its rather easy to
    replace. Note that there maybe slightly more overhead in various hot paths
    due to the need to shift the bits from page->flags. However, that is mostly
    compensated for by a smaller footprint of the kmem_cache_cpu structure (this
    patch reduces that to 3 words per cache) which allows better caching.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index c2f8c8bc56ed..ebdcf4ba42ee 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -48,7 +48,6 @@ struct kmem_cache_cpu {
 	unsigned long tid;	/* Globally unique transaction id */
 	struct page *page;	/* The slab from which we are allocating */
 	struct page *partial;	/* Partially allocated frozen slabs */
-	int node;		/* The node of the page (or -1 for debug) */
 #ifdef CONFIG_SLUB_STATS
 	unsigned stat[NR_SLUB_STAT_ITEMS];
 #endif

commit 0c9aac08261512d70d7d4817bd222abca8b6bdd6
Merge: ed0bb8ea0597 8bdec192b40c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:04:26 2012 -0700

    Merge branch 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux
    
    Pull SLAB changes from Pekka Enberg:
     "There's the new kmalloc_array() API, minor fixes and performance
      improvements, but quite honestly, nothing terribly exciting."
    
    * 'slab/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/linux:
      mm: SLAB Out-of-memory diagnostics
      slab: introduce kmalloc_array()
      slub: per cpu partial statistics change
      slub: include include for prefetch
      slub: Do not hold slub_lock when calling sysfs_slab_add()
      slub: prefetch next freelist pointer in slab_alloc()
      slab, cleanup: remove unneeded return

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a32bcfdc7834..ca122b36aec1 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -8,6 +8,7 @@
  */
 #include <linux/types.h>
 #include <linux/gfp.h>
+#include <linux/bug.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
 

commit 8028dcea8abbbd51b5156e40ea214c20b559cd01
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri Feb 3 23:34:56 2012 +0800

    slub: per cpu partial statistics change
    
    This patch split the cpu_partial_free into 2 parts: cpu_partial_node, PCP refilling
    times from node partial; and same name cpu_partial_free, PCP refilling times in
    slab_free slow path. A new statistic 'cpu_partial_drain' is added to get PCP
    drain to node partial times. These info are useful when do PCP tunning.
    
    The slabinfo.c code is unchanged, since cpu_partial_node is not on slow path.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a32bcfdc7834..6388a6681af1 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -21,7 +21,7 @@ enum stat_item {
 	FREE_FROZEN,		/* Freeing to frozen slab */
 	FREE_ADD_PARTIAL,	/* Freeing moves slab to partial list */
 	FREE_REMOVE_PARTIAL,	/* Freeing removes last object */
-	ALLOC_FROM_PARTIAL,	/* Cpu slab acquired from partial list */
+	ALLOC_FROM_PARTIAL,	/* Cpu slab acquired from node partial list */
 	ALLOC_SLAB,		/* Cpu slab acquired from page allocator */
 	ALLOC_REFILL,		/* Refill cpu slab from slab freelist */
 	ALLOC_NODE_MISMATCH,	/* Switching cpu slab */
@@ -37,7 +37,9 @@ enum stat_item {
 	CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
 	CMPXCHG_DOUBLE_FAIL,	/* Number of times that cmpxchg double did not match */
 	CPU_PARTIAL_ALLOC,	/* Used cpu partial on alloc */
-	CPU_PARTIAL_FREE,	/* USed cpu partial on free */
+	CPU_PARTIAL_FREE,	/* Refill cpu partial on free */
+	CPU_PARTIAL_NODE,	/* Refill cpu partial from node partial */
+	CPU_PARTIAL_DRAIN,	/* Drain cpu partial to node partial */
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {

commit 9f26490412cf15b04ac8f44a512ba0b09e774576
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Sep 1 11:32:18 2011 +0800

    slub: correct comments error for per cpu partial
    
    Correct comment errors, that mistake cpu partial objects number as pages
    number, may make reader misunderstand.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 4890ef79d752..a32bcfdc7834 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -82,7 +82,7 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
-	int cpu_partial;	/* Number of per cpu partial pages to keep around */
+	int cpu_partial;	/* Number of per cpu partial objects to keep around */
 	struct kmem_cache_order_objects oo;
 
 	/* Allocation and freeing of slabs */

commit 49e2258586b423684f03c278149ab46d8f8b6700
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Aug 9 16:12:27 2011 -0500

    slub: per cpu cache for partial pages
    
    Allow filling out the rest of the kmem_cache_cpu cacheline with pointers to
    partial pages. The partial page list is used in slab_free() to avoid
    per node lock taking.
    
    In __slab_alloc() we can then take multiple partial pages off the per
    node partial list in one go reducing node lock pressure.
    
    We can also use the per cpu partial list in slab_alloc() to avoid scanning
    partial lists for pages with free objects.
    
    The main effect of a per cpu partial list is that the per node list_lock
    is taken for batches of partial pages instead of individual ones.
    
    Potential future enhancements:
    
    1. The pickup from the partial list could be perhaps be done without disabling
       interrupts with some work. The free path already puts the page into the
       per cpu partial list without disabling interrupts.
    
    2. __slab_free() may have some code paths that could use optimization.
    
    Performance:
    
                                    Before          After
    ./hackbench 100 process 200000
                                    Time: 1953.047  1564.614
    ./hackbench 100 process 20000
                                    Time: 207.176   156.940
    ./hackbench 100 process 20000
                                    Time: 204.468   156.940
    ./hackbench 100 process 20000
                                    Time: 204.879   158.772
    ./hackbench 10 process 20000
                                    Time: 20.153    15.853
    ./hackbench 10 process 20000
                                    Time: 20.153    15.986
    ./hackbench 10 process 20000
                                    Time: 19.363    16.111
    ./hackbench 1 process 20000
                                    Time: 2.518     2.307
    ./hackbench 1 process 20000
                                    Time: 2.258     2.339
    ./hackbench 1 process 20000
                                    Time: 2.864     2.163
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f58d6413d230..4890ef79d752 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -36,12 +36,15 @@ enum stat_item {
 	ORDER_FALLBACK,		/* Number of times fallback was necessary */
 	CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
 	CMPXCHG_DOUBLE_FAIL,	/* Number of times that cmpxchg double did not match */
+	CPU_PARTIAL_ALLOC,	/* Used cpu partial on alloc */
+	CPU_PARTIAL_FREE,	/* USed cpu partial on free */
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {
 	void **freelist;	/* Pointer to next available object */
 	unsigned long tid;	/* Globally unique transaction id */
 	struct page *page;	/* The slab from which we are allocating */
+	struct page *partial;	/* Partially allocated frozen slabs */
 	int node;		/* The node of the page (or -1 for debug) */
 #ifdef CONFIG_SLUB_STATS
 	unsigned stat[NR_SLUB_STAT_ITEMS];
@@ -79,6 +82,7 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
+	int cpu_partial;	/* Number of per cpu partial pages to keep around */
 	struct kmem_cache_order_objects oo;
 
 	/* Allocation and freeing of slabs */

commit c11abbbaa3252875c5740a6880b9a1a6f1e2a870
Merge: 1d3fe4a75b69 9e577e8b46ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 30 08:21:48 2011 -1000

    Merge branch 'slub/lockless' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6
    
    * 'slub/lockless' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6: (21 commits)
      slub: When allocating a new slab also prep the first object
      slub: disable interrupts in cmpxchg_double_slab when falling back to pagelock
      Avoid duplicate _count variables in page_struct
      Revert "SLUB: Fix build breakage in linux/mm_types.h"
      SLUB: Fix build breakage in linux/mm_types.h
      slub: slabinfo update for cmpxchg handling
      slub: Not necessary to check for empty slab on load_freelist
      slub: fast release on full slab
      slub: Add statistics for the case that the current slab does not match the node
      slub: Get rid of the another_slab label
      slub: Avoid disabling interrupts in free slowpath
      slub: Disable interrupts in free_debug processing
      slub: Invert locking and avoid slab lock
      slub: Rework allocator fastpaths
      slub: Pass kmem_cache struct to lock and freeze slab
      slub: explicit list_lock taking
      slub: Add cmpxchg_double_slab()
      mm: Rearrange struct page
      slub: Move page->frozen handling near where the page->freelist handling occurs
      slub: Do not use frozen page flag but a bit in the page counters
      ...

commit d18a90dd85f8243ed20cdadb6d8a37d595df456d
Author: Ben Greear <greearb@candelatech.com>
Date:   Thu Jul 7 11:36:37 2011 -0700

    slub: Add method to verify memory is not freed
    
    This is for tracking down suspect memory usage.
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Ben Greear <greearb@candelatech.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index fd4fdc72bc8c..4b35c06dfbc5 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -218,6 +218,19 @@ kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 	return ret;
 }
 
+/**
+ * Calling this on allocated memory will check that the memory
+ * is expected to be in use, and print warnings if not.
+ */
+#ifdef CONFIG_SLUB_DEBUG
+extern bool verify_mem_not_deleted(const void *x);
+#else
+static inline bool verify_mem_not_deleted(const void *x)
+{
+	return true;
+}
+#endif
+
 #ifdef CONFIG_TRACING
 extern void *
 kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size);

commit 03e404af26dc2ea0d278d7a342de0aab394793ce
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 1 12:25:58 2011 -0500

    slub: fast release on full slab
    
    Make deactivation occur implicitly while checking out the current freelist.
    
    This avoids one cmpxchg operation on a slab that is now fully in use.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5b228b785377..71441f89729b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -32,6 +32,7 @@ enum stat_item {
 	DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
 	DEACTIVATE_TO_TAIL,	/* Cpu slab was moved to the tail of partials */
 	DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
+	DEACTIVATE_BYPASS,	/* Implicit deactivation */
 	ORDER_FALLBACK,		/* Number of times fallback was necessary */
 	CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
 	CMPXCHG_DOUBLE_FAIL,	/* Number of times that cmpxchg double did not match */

commit e36a2652d7d1ad97f7636a39bdd8654d296cc36b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 1 12:25:57 2011 -0500

    slub: Add statistics for the case that the current slab does not match the node
    
    Slub reloads the per cpu slab if the page does not satisfy the NUMA condition. Track
    those reloads since doing so has a performance impact.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index b42715294147..5b228b785377 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -24,6 +24,7 @@ enum stat_item {
 	ALLOC_FROM_PARTIAL,	/* Cpu slab acquired from partial list */
 	ALLOC_SLAB,		/* Cpu slab acquired from page allocator */
 	ALLOC_REFILL,		/* Refill cpu slab from slab freelist */
+	ALLOC_NODE_MISMATCH,	/* Switching cpu slab */
 	FREE_SLAB,		/* Slab freed to the page allocator */
 	CPUSLAB_FLUSH,		/* Abandoning of the cpu slab */
 	DEACTIVATE_FULL,	/* Cpu slab was full when deactivated */

commit b789ef518b2a7231b0668c813f677cee528a9d3f
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jun 1 12:25:49 2011 -0500

    slub: Add cmpxchg_double_slab()
    
    Add a function that operates on the second doubleword in the page struct
    and manipulates the object counters, the freelist and the frozen attribute.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index c8668d161dd8..b42715294147 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -33,6 +33,7 @@ enum stat_item {
 	DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
 	ORDER_FALLBACK,		/* Number of times fallback was necessary */
 	CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
+	CMPXCHG_DOUBLE_FAIL,	/* Number of times that cmpxchg double did not match */
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {

commit 3192b920bf7d0c528ab54e7d3689f44055316a37
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Jun 14 16:16:36 2011 -0500

    slab, slub, slob: Unify alignment definition
    
    Every slab has its on alignment definition in include/linux/sl?b_def.h. Extract those
    and define a common set in include/linux/slab.h.
    
    SLOB: As notes sometimes we need double word alignment on 32 bit. This gives all
    structures allocated by SLOB a unsigned long long alignment like the others do.
    
    SLAB: If ARCH_SLAB_MINALIGN is not set SLAB would set ARCH_SLAB_MINALIGN to
    zero meaning no alignment at all. Give it the default unsigned long long alignment.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index c8668d161dd8..fd4fdc72bc8c 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -113,16 +113,6 @@ struct kmem_cache {
 
 #define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
-#ifdef ARCH_DMA_MINALIGN
-#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
-#else
-#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
-#endif
-
-#ifndef ARCH_SLAB_MINALIGN
-#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
-#endif
-
 /*
  * Maximum kmalloc object size handled by SLUB. Larger object allocations
  * are passed through to the page allocator. The page allocator "fastpath"

commit 3e0c2ab67e48f77c2da0a5c826aac397792a214e
Author: Christoph Lameter <cl@linux.com>
Date:   Fri May 20 09:42:48 2011 -0500

    slub: Deal with hyperthetical case of PAGE_SIZE > 2M
    
    kmalloc_index() currently returns -1 if the PAGE_SIZE is larger than 2M
    which seems to cause some concern since the callers do not check for -1.
    
    Insert a BUG() and add a comment to the -1 explaining that the code
    cannot be reached.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index ca0c076b2374..c8668d161dd8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -177,7 +177,8 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <=   4 * 1024) return 12;
 /*
  * The following is only needed to support architectures with a larger page
- * size than 4k.
+ * size than 4k. We need to support 2 * PAGE_SIZE here. So for a 64k page
+ * size we would have to go up to 128k.
  */
 	if (size <=   8 * 1024) return 13;
 	if (size <=  16 * 1024) return 14;
@@ -188,7 +189,8 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <= 512 * 1024) return 19;
 	if (size <= 1024 * 1024) return 20;
 	if (size <=  2 * 1024 * 1024) return 21;
-	return -1;
+	BUG();
+	return -1; /* Will never be reached */
 
 /*
  * What we really wanted to do and cannot do because of compiler issues is:

commit 1759415e630e5db0dd2390df9f94892cbfb9a8a2
Author: Christoph Lameter <cl@linux.com>
Date:   Thu May 5 15:23:54 2011 -0500

    slub: Remove CONFIG_CMPXCHG_LOCAL ifdeffery
    
    Remove the #ifdefs. This means that the irqsafe_cpu_cmpxchg_double() is used
    everywhere.
    
    There may be performance implications since:
    
    A. We now have to manage a transaction ID for all arches
    
    B. The interrupt holdoff for arches not supporting CONFIG_CMPXCHG_LOCAL is reduced
    to a very short irqoff section.
    
    There are no multiple irqoff/irqon sequences as a result of this change. Even in the fallback
    case we only have to do one disable and enable like before.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 45ca123e8002..ca0c076b2374 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -37,9 +37,7 @@ enum stat_item {
 
 struct kmem_cache_cpu {
 	void **freelist;	/* Pointer to next available object */
-#ifdef CONFIG_CMPXCHG_LOCAL
 	unsigned long tid;	/* Globally unique transaction id */
-#endif
 	struct page *page;	/* The slab from which we are allocating */
 	int node;		/* The node of the page (or -1 for debug) */
 #ifdef CONFIG_SLUB_STATS

commit 4fdccdfbb4652a7bbac8adbce7449eb093775118
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Mar 22 13:35:00 2011 -0500

    slub: Add statistics for this_cmpxchg_double failures
    
    Add some statistics for debugging.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 90fbb6d87e11..45ca123e8002 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -32,6 +32,7 @@ enum stat_item {
 	DEACTIVATE_TO_TAIL,	/* Cpu slab was moved to the tail of partials */
 	DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
 	ORDER_FALLBACK,		/* Number of times fallback was necessary */
+	CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {

commit e8c500c2b64b6e237e67ecba7249e72363c47047
Merge: c53badd08017 a24c5a0ea902
Author: Pekka Enberg <penberg@kernel.org>
Date:   Sun Mar 20 18:13:26 2011 +0200

    Merge branch 'slub/lockless' into for-linus
    
    Conflicts:
            include/linux/slub_def.h

commit ab9a0f196f2f4f080df54402493ea3dc31b5243e
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Mar 10 15:21:48 2011 +0800

    slub: automatically reserve bytes at the end of slab
    
    There is no "struct" for slub's slab, it shares with struct page.
    But struct page is very small, it is insufficient when we need
    to add some metadata for slab.
    
    So we add a field "reserved" to struct kmem_cache, when a slab
    is allocated, kmem_cache->reserved bytes are automatically reserved
    at the end of the slab for slab's metadata.
    
    Changed from v1:
            Export the reserved field via sysfs
    
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 8b6e8ae5d5ca..ae0093cc5189 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -83,6 +83,7 @@ struct kmem_cache {
 	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
+	int reserved;		/* Reserved bytes at the end of slabs */
 	unsigned long min_partial;
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */

commit 8a5ec0ba42c4919e2d8f4c3138cc8b987fdb0b79
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Feb 25 11:38:54 2011 -0600

    Lockless (and preemptless) fastpaths for slub
    
    Use the this_cpu_cmpxchg_double functionality to implement a lockless
    allocation algorithm on arches that support fast this_cpu_ops.
    
    Each of the per cpu pointers is paired with a transaction id that ensures
    that updates of the per cpu information can only occur in sequence on
    a certain cpu.
    
    A transaction id is a "long" integer that is comprised of an event number
    and the cpu number. The event number is incremented for every change to the
    per cpu state. This means that the cmpxchg instruction can verify for an
    update that nothing interfered and that we are updating the percpu structure
    for the processor where we picked up the information and that we are also
    currently on that processor when we update the information.
    
    This results in a significant decrease of the overhead in the fastpaths. It
    also makes it easy to adopt the fast path for realtime kernels since this
    is lockless and does not require the use of the current per cpu area
    over the critical section. It is only important that the per cpu area is
    current at the beginning of the critical section and at the end.
    
    So there is no need even to disable preemption.
    
    Test results show that the fastpath cycle count is reduced by up to ~ 40%
    (alloc/free test goes from ~140 cycles down to ~80). The slowpath for kfree
    adds a few cycles.
    
    Sadly this does nothing for the slowpath which is where the main issues with
    performance in slub are but the best case performance rises significantly.
    (For that see the more complex slub patches that require cmpxchg_double)
    
    Kmalloc: alloc/free test
    
    Before:
    
    10000 times kmalloc(8)/kfree -> 134 cycles
    10000 times kmalloc(16)/kfree -> 152 cycles
    10000 times kmalloc(32)/kfree -> 144 cycles
    10000 times kmalloc(64)/kfree -> 142 cycles
    10000 times kmalloc(128)/kfree -> 142 cycles
    10000 times kmalloc(256)/kfree -> 132 cycles
    10000 times kmalloc(512)/kfree -> 132 cycles
    10000 times kmalloc(1024)/kfree -> 135 cycles
    10000 times kmalloc(2048)/kfree -> 135 cycles
    10000 times kmalloc(4096)/kfree -> 135 cycles
    10000 times kmalloc(8192)/kfree -> 144 cycles
    10000 times kmalloc(16384)/kfree -> 754 cycles
    
    After:
    
    10000 times kmalloc(8)/kfree -> 78 cycles
    10000 times kmalloc(16)/kfree -> 78 cycles
    10000 times kmalloc(32)/kfree -> 82 cycles
    10000 times kmalloc(64)/kfree -> 88 cycles
    10000 times kmalloc(128)/kfree -> 79 cycles
    10000 times kmalloc(256)/kfree -> 79 cycles
    10000 times kmalloc(512)/kfree -> 85 cycles
    10000 times kmalloc(1024)/kfree -> 82 cycles
    10000 times kmalloc(2048)/kfree -> 82 cycles
    10000 times kmalloc(4096)/kfree -> 85 cycles
    10000 times kmalloc(8192)/kfree -> 82 cycles
    10000 times kmalloc(16384)/kfree -> 706 cycles
    
    Kmalloc: Repeatedly allocate then free test
    
    Before:
    
    10000 times kmalloc(8) -> 211 cycles kfree -> 113 cycles
    10000 times kmalloc(16) -> 174 cycles kfree -> 115 cycles
    10000 times kmalloc(32) -> 235 cycles kfree -> 129 cycles
    10000 times kmalloc(64) -> 222 cycles kfree -> 120 cycles
    10000 times kmalloc(128) -> 343 cycles kfree -> 139 cycles
    10000 times kmalloc(256) -> 827 cycles kfree -> 147 cycles
    10000 times kmalloc(512) -> 1048 cycles kfree -> 272 cycles
    10000 times kmalloc(1024) -> 2043 cycles kfree -> 528 cycles
    10000 times kmalloc(2048) -> 4002 cycles kfree -> 571 cycles
    10000 times kmalloc(4096) -> 7740 cycles kfree -> 628 cycles
    10000 times kmalloc(8192) -> 8062 cycles kfree -> 850 cycles
    10000 times kmalloc(16384) -> 8895 cycles kfree -> 1249 cycles
    
    After:
    
    10000 times kmalloc(8) -> 190 cycles kfree -> 129 cycles
    10000 times kmalloc(16) -> 76 cycles kfree -> 123 cycles
    10000 times kmalloc(32) -> 126 cycles kfree -> 124 cycles
    10000 times kmalloc(64) -> 181 cycles kfree -> 128 cycles
    10000 times kmalloc(128) -> 310 cycles kfree -> 140 cycles
    10000 times kmalloc(256) -> 809 cycles kfree -> 165 cycles
    10000 times kmalloc(512) -> 1005 cycles kfree -> 269 cycles
    10000 times kmalloc(1024) -> 1999 cycles kfree -> 527 cycles
    10000 times kmalloc(2048) -> 3967 cycles kfree -> 570 cycles
    10000 times kmalloc(4096) -> 7658 cycles kfree -> 637 cycles
    10000 times kmalloc(8192) -> 8111 cycles kfree -> 859 cycles
    10000 times kmalloc(16384) -> 8791 cycles kfree -> 1173 cycles
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 875df55ab36d..009b0020079d 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -35,7 +35,10 @@ enum stat_item {
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {
-	void **freelist;	/* Pointer to first free per cpu object */
+	void **freelist;	/* Pointer to next available object */
+#ifdef CONFIG_CMPXCHG_LOCAL
+	unsigned long tid;	/* Globally unique transaction id */
+#endif
 	struct page *page;	/* The slab from which we are allocating */
 	int node;		/* The node of the page (or -1 for debug) */
 #ifdef CONFIG_SLUB_STATS

commit 1a757fe5d4234293d6a3acccd7196f1386443956
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Feb 25 11:38:51 2011 -0600

    slub: min_partial needs to be in first cacheline
    
    It is used in unfreeze_slab() which is a performance critical
    function.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 8b6e8ae5d5ca..875df55ab36d 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -70,6 +70,7 @@ struct kmem_cache {
 	struct kmem_cache_cpu __percpu *cpu_slab;
 	/* Used for retriving partial slabs etc */
 	unsigned long flags;
+	unsigned long min_partial;
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
@@ -83,7 +84,6 @@ struct kmem_cache {
 	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
-	unsigned long min_partial;
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
 #ifdef CONFIG_SYSFS

commit 4a92379bdfb48680a5e6775dd53a586df7b6b0b1
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Thu Oct 21 10:29:19 2010 +0100

    slub tracing: move trace calls out of always inlined functions to reduce kernel code size
    
    Having the trace calls defined in the always inlined kmalloc functions
    in include/linux/slub_def.h causes a lot of code duplication as the
    trace functions get instantiated for each kamalloc call site. This can
    simply be removed by pushing the trace calls down into the functions in
    slub.c.
    
    On my x86_64 built this patch shrinks the code size of the kernel by
    approx 36K and also shrinks the code size of many modules -- too many to
    list here ;)
    
    size vmlinux (2.6.36) reports
           text        data     bss     dec     hex filename
        5410611      743172  828928 6982711  6a8c37 vmlinux
        5373738      744244  828928 6946910  6a005e vmlinux + patch
    
    The resulting kernel has had some testing & kmalloc trace still seems to
    work.
    
    This patch
    - moves trace_kmalloc out of the inlined kmalloc() and pushes it down
    into kmem_cache_alloc_trace() so this it only get instantiated once.
    
    - rename kmem_cache_alloc_notrace()  to kmem_cache_alloc_trace() to
    indicate that now is does have tracing. (maybe this would better being
    called something like kmalloc_kmem_cache ?)
    
    - adds a new function kmalloc_order() to handle allocation and tracing
    of large allocations of page order.
    
    - removes tracing from the inlined kmalloc_large() replacing them with a
    call to kmalloc_order();
    
    - move tracing out of inlined kmalloc_node() and pushing it down into
    kmem_cache_alloc_node_trace
    
    - rename kmem_cache_alloc_node_notrace() to
    kmem_cache_alloc_node_trace()
    
    - removes the include of trace/events/kmem.h from slub_def.h.
    
    v2
    - keep kmalloc_order_trace inline when !CONFIG_TRACE
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index e4f5ed180b9b..8b6e8ae5d5ca 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -10,9 +10,8 @@
 #include <linux/gfp.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
-#include <linux/kmemleak.h>
 
-#include <trace/events/kmem.h>
+#include <linux/kmemleak.h>
 
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
@@ -216,31 +215,40 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
+static __always_inline void *
+kmalloc_order(size_t size, gfp_t flags, unsigned int order)
+{
+	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
+	kmemleak_alloc(ret, size, 1, flags);
+	return ret;
+}
+
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_notrace(struct kmem_cache *s, gfp_t gfpflags);
+extern void *
+kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size);
+extern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order);
 #else
 static __always_inline void *
-kmem_cache_alloc_notrace(struct kmem_cache *s, gfp_t gfpflags)
+kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	return kmem_cache_alloc(s, gfpflags);
 }
+
+static __always_inline void *
+kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
+{
+	return kmalloc_order(size, flags, order);
+}
 #endif
 
 static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 {
 	unsigned int order = get_order(size);
-	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
-
-	kmemleak_alloc(ret, size, 1, flags);
-	trace_kmalloc(_THIS_IP_, ret, size, PAGE_SIZE << order, flags);
-
-	return ret;
+	return kmalloc_order_trace(size, flags, order);
 }
 
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
-	void *ret;
-
 	if (__builtin_constant_p(size)) {
 		if (size > SLUB_MAX_SIZE)
 			return kmalloc_large(size, flags);
@@ -251,11 +259,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 			if (!s)
 				return ZERO_SIZE_PTR;
 
-			ret = kmem_cache_alloc_notrace(s, flags);
-
-			trace_kmalloc(_THIS_IP_, ret, size, s->size, flags);
-
-			return ret;
+			return kmem_cache_alloc_trace(s, flags, size);
 		}
 	}
 	return __kmalloc(size, flags);
@@ -266,14 +270,14 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 #ifdef CONFIG_TRACING
-extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *s,
+extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 					   gfp_t gfpflags,
-					   int node);
+					   int node, size_t size);
 #else
 static __always_inline void *
-kmem_cache_alloc_node_notrace(struct kmem_cache *s,
+kmem_cache_alloc_node_trace(struct kmem_cache *s,
 			      gfp_t gfpflags,
-			      int node)
+			      int node, size_t size)
 {
 	return kmem_cache_alloc_node(s, gfpflags, node);
 }
@@ -281,8 +285,6 @@ kmem_cache_alloc_node_notrace(struct kmem_cache *s,
 
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
-	void *ret;
-
 	if (__builtin_constant_p(size) &&
 		size <= SLUB_MAX_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
@@ -290,12 +292,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 		if (!s)
 			return ZERO_SIZE_PTR;
 
-		ret = kmem_cache_alloc_node_notrace(s, flags, node);
-
-		trace_kmalloc_node(_THIS_IP_, ret,
-				   size, s->size, flags, node);
-
-		return ret;
+		return kmem_cache_alloc_node_trace(s, flags, node, size);
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit ab4d5ed5eeda4f57c50d14131ce1b1da75d0c938
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 5 13:57:26 2010 -0500

    slub: Enable sysfs support for !CONFIG_SLUB_DEBUG
    
    Currently disabling CONFIG_SLUB_DEBUG also disabled SYSFS support meaning
    that the slabs cannot be tuned without DEBUG.
    
    Make SYSFS support independent of CONFIG_SLUB_DEBUG
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index b33c0f2e61dc..e4f5ed180b9b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -87,7 +87,7 @@ struct kmem_cache {
 	unsigned long min_partial;
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
-#ifdef CONFIG_SLUB_DEBUG
+#ifdef CONFIG_SYSFS
 	struct kobject kobj;	/* For sysfs */
 #endif
 

commit 7340cc84141d5236c5dd003359ee921513cd9b84
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 28 08:10:26 2010 -0500

    slub: reduce differences between SMP and NUMA
    
    Reduce the #ifdefs and simplify bootstrap by making SMP and NUMA as much alike
    as possible. This means that there will be an additional indirection to get to
    the kmem_cache_node field under SMP.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a6c43ec6a4a5..b33c0f2e61dc 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -96,11 +96,8 @@ struct kmem_cache {
 	 * Defragmentation by allocating from a remote node.
 	 */
 	int remote_node_defrag_ratio;
-	struct kmem_cache_node *node[MAX_NUMNODES];
-#else
-	/* Avoid an extra cache line for UP */
-	struct kmem_cache_node local_node;
 #endif
+	struct kmem_cache_node *node[MAX_NUMNODES];
 };
 
 /*

commit 51df1142816e469173889fb6d6dc810be9b9e022
Author: Christoph Lameter <cl@linux.com>
Date:   Fri Aug 20 12:37:15 2010 -0500

    slub: Dynamically size kmalloc cache allocations
    
    kmalloc caches are statically defined and may take up a lot of space just
    because the sizes of the node array has to be dimensioned for the largest
    node count supported.
    
    This patch makes the size of the kmem_cache structure dynamic throughout by
    creating a kmem_cache slab cache for the kmem_cache objects. The bootstrap
    occurs by allocating the initial one or two kmem_cache objects from the
    page allocator.
    
    C2->C3
            - Fix various issues indicated by David
            - Make create kmalloc_cache return a kmem_cache * pointer.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 9f63538928c0..a6c43ec6a4a5 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -139,19 +139,16 @@ struct kmem_cache {
 
 #ifdef CONFIG_ZONE_DMA
 #define SLUB_DMA __GFP_DMA
-/* Reserve extra caches for potential DMA use */
-#define KMALLOC_CACHES (2 * SLUB_PAGE_SHIFT)
 #else
 /* Disable DMA functionality */
 #define SLUB_DMA (__force gfp_t)0
-#define KMALLOC_CACHES SLUB_PAGE_SHIFT
 #endif
 
 /*
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[KMALLOC_CACHES];
+extern struct kmem_cache *kmalloc_caches[SLUB_PAGE_SHIFT];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -216,7 +213,7 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 	if (index == 0)
 		return NULL;
 
-	return &kmalloc_caches[index];
+	return kmalloc_caches[index];
 }
 
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);

commit bc584c5107bfd97e2aa41c798e3b213bcdd4eae7
Merge: a28e0852d45e 1ab335d8f857
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 22 10:08:52 2010 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6:
      slab: fix object alignment
      slub: add missing __percpu markup in mm/slub_def.h

commit a6eb9fe105d5de0053b261148cee56c94b4720ca
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:22 2010 -0700

    dma-mapping: rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN
    
    Now each architecture has the own dma_get_cache_alignment implementation.
    
    dma_get_cache_alignment returns the minimum DMA alignment.  Architectures
    define it as ARCH_KMALLOC_MINALIGN (it's used to make sure that malloc'ed
    buffer is DMA-safe; the buffer doesn't share a cache with the others).  So
    we can unify dma_get_cache_alignment implementations.
    
    This patch:
    
    dma_get_cache_alignment() needs to know if an architecture defines
    ARCH_KMALLOC_MINALIGN or not (needs to know if architecture has DMA
    alignment restriction).  However, slab.h define ARCH_KMALLOC_MINALIGN if
    architectures doesn't define it.
    
    Let's rename ARCH_KMALLOC_MINALIGN to ARCH_DMA_MINALIGN.
    ARCH_KMALLOC_MINALIGN is used only in the internals of slab/slob/slub
    (except for crypto).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 6447a723ecb1..6d14409c4d9a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -106,15 +106,17 @@ struct kmem_cache {
 /*
  * Kmalloc subsystem.
  */
-#if defined(ARCH_KMALLOC_MINALIGN) && ARCH_KMALLOC_MINALIGN > 8
-#define KMALLOC_MIN_SIZE ARCH_KMALLOC_MINALIGN
+#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8
+#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN
 #else
 #define KMALLOC_MIN_SIZE 8
 #endif
 
 #define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
-#ifndef ARCH_KMALLOC_MINALIGN
+#ifdef ARCH_DMA_MINALIGN
+#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN
+#else
 #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
 #endif
 

commit 1b5ad24878b7e5a543b98c5d2f8c0d8c0dd3088f
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Sat Aug 7 14:29:22 2010 +0200

    slub: add missing __percpu markup in mm/slub_def.h
    
    kmem_cache->cpu_slab is a percpu pointer but was missing __percpu
    markup.  Add it.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 6447a723ecb1..5ec4bc0e45aa 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -68,7 +68,7 @@ struct kmem_cache_order_objects {
  * Slab cache management.
  */
 struct kmem_cache {
-	struct kmem_cache_cpu *cpu_slab;
+	struct kmem_cache_cpu __percpu *cpu_slab;
 	/* Used for retriving partial slabs etc */
 	unsigned long flags;
 	int size;		/* The size of an object including meta data */

commit c726b61c6a5acc54c55ed7a0e7638cc4c5a100a8
Merge: 7be7923633a1 018378c55b03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 9 18:55:20 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into perf/core

commit 039ca4e74a1cf60bd7487324a564ecf5c981f254
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 26 17:22:17 2010 +0800

    tracing: Remove kmemtrace ftrace plugin
    
    We have been resisting new ftrace plugins and removing existing
    ones, and kmemtrace has been superseded by kmem trace events
    and perf-kmem, so we remove it.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    [ remove kmemtrace from the makefile, handle slob too ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 55695c8d2f8a..2345d3a033e6 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -10,9 +10,10 @@
 #include <linux/gfp.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
-#include <linux/kmemtrace.h>
 #include <linux/kmemleak.h>
 
+#include <trace/events/kmem.h>
+
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
 	ALLOC_SLOWPATH,		/* Allocation by getting a new cpu slab */

commit 0f1f694260e0d35b5ce7d471f6e679c3dd4d7d94
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Thu May 27 09:17:17 2010 -0500

    SLUB: Allow full duplication of kmalloc array for 390
    
    Commit 756dee75872a2a764b478e18076360b8a4ec9045 ("SLUB: Get rid of dynamic DMA
    kmalloc cache allocation") makes S390 run out of kmalloc caches.  Increase the
    number of kmalloc caches to a safe size.
    
    Cc: <stable@kernel.org> [ .33 and .34 ]
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 6ac37664e8fe..4ba59cfc1f75 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -137,7 +137,7 @@ struct kmem_cache {
 #ifdef CONFIG_ZONE_DMA
 #define SLUB_DMA __GFP_DMA
 /* Reserve extra caches for potential DMA use */
-#define KMALLOC_CACHES (2 * SLUB_PAGE_SHIFT - 6)
+#define KMALLOC_CACHES (2 * SLUB_PAGE_SHIFT)
 #else
 /* Disable DMA functionality */
 #define SLUB_DMA (__force gfp_t)0

commit 73367bd8eef4f4eb311005886aaa916013073265
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Fri May 21 14:41:35 2010 -0700

    slub: move kmem_cache_node into it's own cacheline
    
    This patch is meant to improve the performance of SLUB by moving the local
    kmem_cache_node lock into it's own cacheline separate from kmem_cache.
    This is accomplished by simply removing the local_node when NUMA is enabled.
    
    On my system with 2 nodes I saw around a 5% performance increase w/
    hackbench times dropping from 6.2 seconds to 5.9 seconds on average.  I
    suspect the performance gain would increase as the number of nodes
    increases, but I do not have the data to currently back that up.
    
    Bugzilla-Reference: http://bugzilla.kernel.org/show_bug.cgi?id=15713
    Cc: <stable@kernel.org>
    Reported-by: Alex Shi <alex.shi@intel.com>
    Tested-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 55695c8d2f8a..6ac37664e8fe 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -75,12 +75,6 @@ struct kmem_cache {
 	int offset;		/* Free pointer offset. */
 	struct kmem_cache_order_objects oo;
 
-	/*
-	 * Avoid an extra cache line for UP, SMP and for the node local to
-	 * struct kmem_cache.
-	 */
-	struct kmem_cache_node local_node;
-
 	/* Allocation and freeing of slabs */
 	struct kmem_cache_order_objects max;
 	struct kmem_cache_order_objects min;
@@ -102,6 +96,9 @@ struct kmem_cache {
 	 */
 	int remote_node_defrag_ratio;
 	struct kmem_cache_node *node[MAX_NUMNODES];
+#else
+	/* Avoid an extra cache line for UP */
+	struct kmem_cache_node local_node;
 #endif
 };
 

commit 4581ced379736fd76432c754f999d26deb83fbb7
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed May 19 12:02:14 2010 +0100

    mm: Move ARCH_SLAB_MINALIGN and ARCH_KMALLOC_MINALIGN to <linux/slub_def.h>
    
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 0249d4175bac..55695c8d2f8a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -116,6 +116,14 @@ struct kmem_cache {
 
 #define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
+#ifndef ARCH_KMALLOC_MINALIGN
+#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+#endif
+
+#ifndef ARCH_SLAB_MINALIGN
+#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
+#endif
+
 /*
  * Maximum kmalloc object size handled by SLUB. Larger object allocations
  * are passed through to the page allocator. The page allocator "fastpath"

commit ff12059ed14b0773d7bbef86f98218ada6c20770
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Fri Dec 18 16:26:22 2009 -0600

    SLUB: this_cpu: Remove slub kmem_cache fields
    
    Remove the fields in struct kmem_cache_cpu that were used to cache data from
    struct kmem_cache when they were in different cachelines. The cacheline that
    holds the per cpu array pointer now also holds these values. We can cut down
    the struct kmem_cache_cpu size to almost half.
    
    The get_freepointer() and set_freepointer() functions that used to be only
    intended for the slow path now are also useful for the hot path since access
    to the size field does not require accessing an additional cacheline anymore.
    This results in consistent use of functions for setting the freepointer of
    objects throughout SLUB.
    
    Also we initialize all possible kmem_cache_cpu structures when a slab is
    created. No need to initialize them when a processor or node comes online.
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a78fb4ac2015..0249d4175bac 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -38,8 +38,6 @@ struct kmem_cache_cpu {
 	void **freelist;	/* Pointer to first free per cpu object */
 	struct page *page;	/* The slab from which we are allocating */
 	int node;		/* The node of the page (or -1 for debug) */
-	unsigned int offset;	/* Freepointer offset (in word units) */
-	unsigned int objsize;	/* Size of an object (from kmem_cache) */
 #ifdef CONFIG_SLUB_STATS
 	unsigned stat[NR_SLUB_STAT_ITEMS];
 #endif

commit 756dee75872a2a764b478e18076360b8a4ec9045
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Fri Dec 18 16:26:21 2009 -0600

    SLUB: Get rid of dynamic DMA kmalloc cache allocation
    
    Dynamic DMA kmalloc cache allocation is troublesome since the
    new percpu allocator does not support allocations in atomic contexts.
    Reserve some statically allocated kmalloc_cpu structures instead.
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 17ebe0f89bf3..a78fb4ac2015 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -131,11 +131,21 @@ struct kmem_cache {
 
 #define SLUB_PAGE_SHIFT (PAGE_SHIFT + 2)
 
+#ifdef CONFIG_ZONE_DMA
+#define SLUB_DMA __GFP_DMA
+/* Reserve extra caches for potential DMA use */
+#define KMALLOC_CACHES (2 * SLUB_PAGE_SHIFT - 6)
+#else
+/* Disable DMA functionality */
+#define SLUB_DMA (__force gfp_t)0
+#define KMALLOC_CACHES SLUB_PAGE_SHIFT
+#endif
+
 /*
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[SLUB_PAGE_SHIFT];
+extern struct kmem_cache kmalloc_caches[KMALLOC_CACHES];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -203,13 +213,6 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 	return &kmalloc_caches[index];
 }
 
-#ifdef CONFIG_ZONE_DMA
-#define SLUB_DMA __GFP_DMA
-#else
-/* Disable DMA functionality */
-#define SLUB_DMA (__force gfp_t)0
-#endif
-
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 

commit 9dfc6e68bfe6ee452efb1a4e9ca26a9007f2b864
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Fri Dec 18 16:26:20 2009 -0600

    SLUB: Use this_cpu operations in slub
    
    Using per cpu allocations removes the needs for the per cpu arrays in the
    kmem_cache struct. These could get quite big if we have to support systems
    with thousands of cpus. The use of this_cpu_xx operations results in:
    
    1. The size of kmem_cache for SMP configuration shrinks since we will only
       need 1 pointer instead of NR_CPUS. The same pointer can be used by all
       processors. Reduces cache footprint of the allocator.
    
    2. We can dynamically size kmem_cache according to the actual nodes in the
       system meaning less memory overhead for configurations that may potentially
       support up to 1k NUMA nodes / 4k cpus.
    
    3. We can remove the diddle widdle with allocating and releasing of
       kmem_cache_cpu structures when bringing up and shutting down cpus. The cpu
       alloc logic will do it all for us. Removes some portions of the cpu hotplug
       functionality.
    
    4. Fastpath performance increases since per cpu pointer lookups and
       address calculations are avoided.
    
    V7-V8
    - Convert missed get_cpu_slab() under CONFIG_SLUB_STATS
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 1e14beb23f9b..17ebe0f89bf3 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -69,6 +69,7 @@ struct kmem_cache_order_objects {
  * Slab cache management.
  */
 struct kmem_cache {
+	struct kmem_cache_cpu *cpu_slab;
 	/* Used for retriving partial slabs etc */
 	unsigned long flags;
 	int size;		/* The size of an object including meta data */
@@ -104,11 +105,6 @@ struct kmem_cache {
 	int remote_node_defrag_ratio;
 	struct kmem_cache_node *node[MAX_NUMNODES];
 #endif
-#ifdef CONFIG_SMP
-	struct kmem_cache_cpu *cpu_slab[NR_CPUS];
-#else
-	struct kmem_cache_cpu cpu_slab;
-#endif
 };
 
 /*

commit 0f24f1287a86b198c1e4bd4ce45e8565e40ff804
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Dec 11 15:45:30 2009 +0800

    tracing, slab: Define kmem_cache_alloc_notrace ifdef CONFIG_TRACING
    
    Define kmem_trace_alloc_{,node}_notrace() if CONFIG_TRACING is
    enabled, otherwise perf-kmem will show wrong stats ifndef
    CONFIG_KMEM_TRACE, because a kmalloc() memory allocation may
    be traced by both trace_kmalloc() and trace_kmem_cache_alloc().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: linux-mm@kvack.org <linux-mm@kvack.org>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    LKML-Reference: <4B21F89A.7000801@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5ad70a60fd74..1e14beb23f9b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -217,7 +217,7 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
-#ifdef CONFIG_KMEMTRACE
+#ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_notrace(struct kmem_cache *s, gfp_t gfpflags);
 #else
 static __always_inline void *
@@ -266,7 +266,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
-#ifdef CONFIG_KMEMTRACE
+#ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *s,
 					   gfp_t gfpflags,
 					   int node);

commit aceda773606f2506a25b91aaafae87b2e4315834
Merge: 0cc6d77e55ec 5086c389cb89 8a3d271deb0c
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Sep 14 20:19:06 2009 +0300

    Merge branches 'slab/cleanups' and 'slab/fixes' into for-linus

commit acdfcd04d9df7d084ff752f82afad6ed4ad5f363
Author: Aaro Koskinen <aaro.koskinen@nokia.com>
Date:   Fri Aug 28 14:28:54 2009 +0300

    SLUB: fix ARCH_KMALLOC_MINALIGN cases 64 and 256
    
    If the minalign is 64 bytes, then the 96 byte cache should not be created
    because it would conflict with the 128 byte cache.
    
    If the minalign is 256 bytes, patching the size_index table should not
    result in a buffer overrun.
    
    The calculation "(i - 1) / 8" used to access size_index[] is moved to
    a separate function as suggested by Christoph Lameter.
    
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Aaro Koskinen <aaro.koskinen@nokia.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 4dcbc2c71491..aa5d4a69d461 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -152,12 +152,10 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <= KMALLOC_MIN_SIZE)
 		return KMALLOC_SHIFT_LOW;
 
-#if KMALLOC_MIN_SIZE <= 64
-	if (size > 64 && size <= 96)
+	if (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)
 		return 1;
-	if (size > 128 && size <= 192)
+	if (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)
 		return 2;
-#endif
 	if (size <=          8) return 3;
 	if (size <=         16) return 4;
 	if (size <=         32) return 5;

commit bbff2e433e80fae72c8d00d482927d52ec19ba33
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Thu Aug 6 11:36:25 2009 +0300

    slab: remove duplicate kmem_cache_init_late() declarations
    
    kmem_cache_init_late() has been declared in slab.h
    
    CC: Nick Piggin <npiggin@suse.de>
    CC: Matt Mackall <mpm@selenic.com>
    CC: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index c1c862b1d01a..76c831693616 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -304,6 +304,4 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 }
 #endif
 
-void __init kmem_cache_init_late(void);
-
 #endif /* _LINUX_SLUB_DEF_H */

commit e4f7c0b44a8ac8935f223195af9ea637d0c08091
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 7 10:32:59 2009 +0100

    kmemleak: Trace the kmalloc_large* functions in slub
    
    The kmalloc_large() and kmalloc_large_node() functions were missed when
    adding the kmemleak hooks to the slub allocator. However, they should be
    traced to avoid false positives.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 4dcbc2c71491..c1c862b1d01a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -11,6 +11,7 @@
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
 #include <linux/kmemtrace.h>
+#include <linux/kmemleak.h>
 
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
@@ -233,6 +234,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 	unsigned int order = get_order(size);
 	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
 
+	kmemleak_alloc(ret, size, 1, flags);
 	trace_kmalloc(_THIS_IP_, ret, size, PAGE_SIZE << order, flags);
 
 	return ret;

commit 7e85ee0c1d15ca5f8bff0f514f158eba1742dd87
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Fri Jun 12 14:03:06 2009 +0300

    slab,slub: don't enable interrupts during early boot
    
    As explained by Benjamin Herrenschmidt:
    
      Oh and btw, your patch alone doesn't fix powerpc, because it's missing
      a whole bunch of GFP_KERNEL's in the arch code... You would have to
      grep the entire kernel for things that check slab_is_available() and
      even then you'll be missing some.
    
      For example, slab_is_available() didn't always exist, and so in the
      early days on powerpc, we used a mem_init_done global that is set form
      mem_init() (not perfect but works in practice). And we still have code
      using that to do the test.
    
    Therefore, mask out __GFP_WAIT, __GFP_IO, and __GFP_FS in the slab allocators
    in early boot code to avoid enabling interrupts.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index be5d40c43bd2..4dcbc2c71491 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -302,4 +302,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 }
 #endif
 
+void __init kmem_cache_init_late(void);
+
 #endif /* _LINUX_SLUB_DEF_H */

commit 02af61bb50f5d5f0322dbe5ab2a0d75808d25c7b
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Fri Apr 10 14:26:18 2009 +0800

    tracing, kmemtrace: Separate include/trace/kmemtrace.h to kmemtrace part and tracepoint part
    
    Impact: refactor code for future changes
    
    Current kmemtrace.h is used both as header file of kmemtrace and kmem's
    tracepoints definition.
    
    Tracepoints' definition file may be used by other code, and should only have
    definition of tracepoint.
    
    We can separate include/trace/kmemtrace.h into 2 files:
    
      include/linux/kmemtrace.h: header file for kmemtrace
      include/trace/kmem.h:      definition of kmem tracepoints
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <49DEE68A.5040902@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5046f90c1171..be5d40c43bd2 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -10,7 +10,7 @@
 #include <linux/gfp.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
-#include <trace/kmemtrace.h>
+#include <linux/kmemtrace.h>
 
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */

commit ca2b84cb3c4a0d4d2143b46ec072cdff5d1b3b87
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Mon Mar 23 15:12:24 2009 +0200

    kmemtrace: use tracepoints
    
    kmemtrace now uses tracepoints instead of markers. We no longer need to
    use format specifiers to pass arguments.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    [ folded: Use the new TP_PROTO and TP_ARGS to fix the build.     ]
    [ folded: fix build when CONFIG_KMEMTRACE is disabled.           ]
    [ folded: define tracepoints when CONFIG_TRACEPOINTS is enabled. ]
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    LKML-Reference: <ae61c0f37156db8ec8dc0d5778018edde60a92e3.1237813499.git.eduard.munteanu@linux360.ro>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a1f90528e70b..5046f90c1171 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -233,8 +233,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 	unsigned int order = get_order(size);
 	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
 
-	kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_, ret,
-			     size, PAGE_SIZE << order, flags);
+	trace_kmalloc(_THIS_IP_, ret, size, PAGE_SIZE << order, flags);
 
 	return ret;
 }
@@ -255,9 +254,7 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 
 			ret = kmem_cache_alloc_notrace(s, flags);
 
-			kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC,
-					     _THIS_IP_, ret,
-					     size, s->size, flags);
+			trace_kmalloc(_THIS_IP_, ret, size, s->size, flags);
 
 			return ret;
 		}
@@ -296,9 +293,8 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 
 		ret = kmem_cache_alloc_node_notrace(s, flags, node);
 
-		kmemtrace_mark_alloc_node(KMEMTRACE_TYPE_KMALLOC,
-					  _THIS_IP_, ret,
-					  size, s->size, flags, node);
+		trace_kmalloc_node(_THIS_IP_, ret,
+				   size, s->size, flags, node);
 
 		return ret;
 	}

commit 8302294f43250dc337108c51882a6007f2b1e2e0
Merge: 4fe70410d9a2 2e572895bf32
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 1 21:54:19 2009 +0200

    Merge branch 'tracing/core-v2' into tracing-for-linus
    
    Conflicts:
            include/linux/slub_def.h
            lib/Kconfig.debug
            mm/slob.c
            mm/slub.c

commit 15a5b0a4912d98a9615ef457c7bde8d08195a771
Merge: 8e0ee43bc2c3 6e9ed0cc4b96 6fb8f4243930 c0bdb232b23b 1a00df4a2cc0 e8120ff1ffc5
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Tue Mar 24 10:25:21 2009 +0200

    Merge branches 'topic/slob/cleanups', 'topic/slob/fixes', 'topic/slub/core', 'topic/slub/cleanups' and 'topic/slub/perf' into for-linus

commit 3b89d7d881a1dbb4da158f7eb5d6b3ceefc72810
Author: David Rientjes <rientjes@google.com>
Date:   Sun Feb 22 17:40:07 2009 -0800

    slub: move min_partial to struct kmem_cache
    
    Although it allows for better cacheline use, it is unnecessary to save a
    copy of the cache's min_partial value in each kmem_cache_node.
    
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2f5c16b1aacd..f20a89e4d52c 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -46,7 +46,6 @@ struct kmem_cache_cpu {
 struct kmem_cache_node {
 	spinlock_t list_lock;	/* Protect partial list and nr_partial */
 	unsigned long nr_partial;
-	unsigned long min_partial;
 	struct list_head partial;
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_t nr_slabs;
@@ -89,6 +88,7 @@ struct kmem_cache {
 	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
+	unsigned long min_partial;
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
 #ifdef CONFIG_SLUB_DEBUG

commit 057685cf57066bc8aaed68de1b1970e12f0075d2
Merge: 64b36ca7f408 fe1200b63d15
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 20 12:15:30 2009 +0100

    Merge branch 'for-ingo' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6 into tracing/kmemtrace
    
    Conflicts:
            mm/slub.c

commit fe1200b63d158b28eef6d4de1e5b5f99c681ba2f
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Tue Feb 17 12:05:07 2009 -0500

    SLUB: Introduce and use SLUB_MAX_SIZE and SLUB_PAGE_SHIFT constants
    
    As a preparational patch to bump up page allocator pass-through threshold,
    introduce two new constants SLUB_MAX_SIZE and SLUB_PAGE_SHIFT and convert
    mm/slub.c to use them.
    
    Reported-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Tested-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2f5c16b1aacd..986e09dcfd8f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -120,11 +120,24 @@ struct kmem_cache {
 
 #define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
+/*
+ * Maximum kmalloc object size handled by SLUB. Larger object allocations
+ * are passed through to the page allocator. The page allocator "fastpath"
+ * is relatively slow so we need this value sufficiently high so that
+ * performance critical objects are allocated through the SLUB fastpath.
+ *
+ * This should be dropped to PAGE_SIZE / 2 once the page allocator
+ * "fastpath" becomes competitive with the slab allocator fastpaths.
+ */
+#define SLUB_MAX_SIZE (PAGE_SIZE)
+
+#define SLUB_PAGE_SHIFT (PAGE_SHIFT + 1)
+
 /*
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[PAGE_SHIFT + 1];
+extern struct kmem_cache kmalloc_caches[SLUB_PAGE_SHIFT];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -212,7 +225,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
-		if (size > PAGE_SIZE)
+		if (size > SLUB_MAX_SIZE)
 			return kmalloc_large(size, flags);
 
 		if (!(flags & SLUB_DMA)) {
@@ -234,7 +247,7 @@ void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) &&
-		size <= PAGE_SIZE && !(flags & SLUB_DMA)) {
+		size <= SLUB_MAX_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)

commit 51735a7ca67531267a27b57e5fe20f7815192f9c
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Fri Feb 20 12:21:33 2009 +0200

    SLUB: Do not pass 8k objects through to the page allocator
    
    Increase the maximum object size in SLUB so that 8k objects are not
    passed through to the page allocator anymore. The network stack uses 8k
    objects for performance critical operations.
    
    The patch is motivated by a SLAB vs. SLUB regression in the netperf
    benchmark. The problem is that the kfree(skb->head) call in
    skb_release_data() that is subject to page allocator pass-through as the
    size passed to __alloc_skb() is larger than 4 KB in this test.
    
    As explained by Yanmin Zhang:
    
      I use 2.6.29-rc2 kernel to run netperf UDP-U-4k CPU_NUM client/server
      pair loopback testing on x86-64 machines. Comparing with SLUB, SLAB's
      result is about 2.3 times of SLUB's. After applying the reverting patch,
      the result difference between SLUB and SLAB becomes 1% which we might
      consider as fluctuation.
    
    [ penberg@cs.helsinki.fi: fix oops in kmalloc() ]
    Reported-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Tested-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 986e09dcfd8f..e217a7a68ea7 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -129,9 +129,9 @@ struct kmem_cache {
  * This should be dropped to PAGE_SIZE / 2 once the page allocator
  * "fastpath" becomes competitive with the slab allocator fastpaths.
  */
-#define SLUB_MAX_SIZE (PAGE_SIZE)
+#define SLUB_MAX_SIZE (2 * PAGE_SIZE)
 
-#define SLUB_PAGE_SHIFT (PAGE_SHIFT + 1)
+#define SLUB_PAGE_SHIFT (PAGE_SHIFT + 2)
 
 /*
  * We keep the general caches in an array of slab caches that are used for

commit ffadd4d0feb5376c82dc3a4104731b7ce2794edc
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Tue Feb 17 12:05:07 2009 -0500

    SLUB: Introduce and use SLUB_MAX_SIZE and SLUB_PAGE_SHIFT constants
    
    As a preparational patch to bump up page allocator pass-through threshold,
    introduce two new constants SLUB_MAX_SIZE and SLUB_PAGE_SHIFT and convert
    mm/slub.c to use them.
    
    Reported-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Tested-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2f5c16b1aacd..986e09dcfd8f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -120,11 +120,24 @@ struct kmem_cache {
 
 #define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
+/*
+ * Maximum kmalloc object size handled by SLUB. Larger object allocations
+ * are passed through to the page allocator. The page allocator "fastpath"
+ * is relatively slow so we need this value sufficiently high so that
+ * performance critical objects are allocated through the SLUB fastpath.
+ *
+ * This should be dropped to PAGE_SIZE / 2 once the page allocator
+ * "fastpath" becomes competitive with the slab allocator fastpaths.
+ */
+#define SLUB_MAX_SIZE (PAGE_SIZE)
+
+#define SLUB_PAGE_SHIFT (PAGE_SHIFT + 1)
+
 /*
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[PAGE_SHIFT + 1];
+extern struct kmem_cache kmalloc_caches[SLUB_PAGE_SHIFT];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -212,7 +225,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
-		if (size > PAGE_SIZE)
+		if (size > SLUB_MAX_SIZE)
 			return kmalloc_large(size, flags);
 
 		if (!(flags & SLUB_DMA)) {
@@ -234,7 +247,7 @@ void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) &&
-		size <= PAGE_SIZE && !(flags & SLUB_DMA)) {
+		size <= SLUB_MAX_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)

commit 36994e58a48fb8f9651c7dc845a6de298aba5bfc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Dec 29 13:42:23 2008 -0800

    tracing/kmemtrace: normalize the raw tracer event to the unified tracing API
    
    Impact: new tracer plugin
    
    This patch adapts kmemtrace raw events tracing to the unified tracing API.
    
    To enable and use this tracer, just do the following:
    
     echo kmemtrace > /debugfs/tracing/current_tracer
     cat /debugfs/tracing/trace
    
    You will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
    type_id 1 call_site 18446744071565527833 ptr 18446612134395152256
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164672 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164912 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345165152 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 0 call_site 18446744071566144042 ptr 18446612134346191680 bytes_req 1304 bytes_alloc 1312 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    
    That was to stay backward compatible with the format output produced in
    inux/tracepoint.h.
    
    This is the default ouput, but note that I tried something else.
    
    If you change an option:
    
    echo kmem_minimalistic > /debugfs/trace_options
    
    and then cat /debugfs/trace, you will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
       -      C                            0xffff88007c088780          file_free_rcu
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc780     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc870     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc960     -1   d_alloc
       +      K   1304   1312   000000d0   0xffff8800791d7340     -1   reiserfs_alloc_inode
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K    992   1000   000000d0   0xffff880079045b58     -1   alloc_inode
       +      K    768   1024   000080d0   0xffff88007c096400     -1   alloc_pipe_info
       +      K    240    240   000000d0   0xffff8800790dca50     -1   d_alloc
       +      K    272    320   000080d0   0xffff88007c088780     -1   get_empty_filp
       +      K    272    320   000080d0   0xffff88007c088000     -1   get_empty_filp
    
    Yeah I shall confess kmem_minimalistic should be: kmem_alternative.
    
    Whatever, I find it more readable but this a personal opinion of course.
    We can drop it if you want.
    
    On the ALLOC/FREE column, + means an allocation and - a free.
    
    On the type column, you have K = kmalloc, C = cache, P = page
    
    I would like the flags to be GFP_* strings but that would not be easy to not
    break the column with strings....
    
    About the node...it seems to always be -1. I don't know why but that shouldn't
    be difficult to find.
    
    I moved linux/tracepoint.h to trace/tracepoint.h as well. I think that would
    be more easy to find the tracer headers if they are all in their common
    directory.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index dc28432b5b9a..6b657f7dcb2b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -10,7 +10,7 @@
 #include <linux/gfp.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
-#include <linux/kmemtrace.h>
+#include <trace/kmemtrace.h>
 
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */

commit 5b882be4e00e53a44f47ad7eb997cac2938848bf
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Tue Aug 19 20:43:26 2008 +0300

    kmemtrace: SLUB hooks.
    
    This adds hooks for the SLUB allocator, to allow tracing with kmemtrace.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 2f5c16b1aacd..dc28432b5b9a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -10,6 +10,7 @@
 #include <linux/gfp.h>
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
+#include <linux/kmemtrace.h>
 
 enum stat_item {
 	ALLOC_FASTPATH,		/* Allocation from cpu slab */
@@ -204,13 +205,31 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
+#ifdef CONFIG_KMEMTRACE
+extern void *kmem_cache_alloc_notrace(struct kmem_cache *s, gfp_t gfpflags);
+#else
+static __always_inline void *
+kmem_cache_alloc_notrace(struct kmem_cache *s, gfp_t gfpflags)
+{
+	return kmem_cache_alloc(s, gfpflags);
+}
+#endif
+
 static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 {
-	return (void *)__get_free_pages(flags | __GFP_COMP, get_order(size));
+	unsigned int order = get_order(size);
+	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
+
+	kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC, _THIS_IP_, ret,
+			     size, PAGE_SIZE << order, flags);
+
+	return ret;
 }
 
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
+	void *ret;
+
 	if (__builtin_constant_p(size)) {
 		if (size > PAGE_SIZE)
 			return kmalloc_large(size, flags);
@@ -221,7 +240,13 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 			if (!s)
 				return ZERO_SIZE_PTR;
 
-			return kmem_cache_alloc(s, flags);
+			ret = kmem_cache_alloc_notrace(s, flags);
+
+			kmemtrace_mark_alloc(KMEMTRACE_TYPE_KMALLOC,
+					     _THIS_IP_, ret,
+					     size, s->size, flags);
+
+			return ret;
 		}
 	}
 	return __kmalloc(size, flags);
@@ -231,8 +256,24 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
+#ifdef CONFIG_KMEMTRACE
+extern void *kmem_cache_alloc_node_notrace(struct kmem_cache *s,
+					   gfp_t gfpflags,
+					   int node);
+#else
+static __always_inline void *
+kmem_cache_alloc_node_notrace(struct kmem_cache *s,
+			      gfp_t gfpflags,
+			      int node)
+{
+	return kmem_cache_alloc_node(s, gfpflags, node);
+}
+#endif
+
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
+	void *ret;
+
 	if (__builtin_constant_p(size) &&
 		size <= PAGE_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
@@ -240,7 +281,13 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 		if (!s)
 			return ZERO_SIZE_PTR;
 
-		return kmem_cache_alloc_node(s, flags, node);
+		ret = kmem_cache_alloc_node_notrace(s, flags, node);
+
+		kmemtrace_mark_alloc_node(KMEMTRACE_TYPE_KMALLOC,
+					  _THIS_IP_, ret,
+					  size, s->size, flags, node);
+
+		return ret;
 	}
 	return __kmalloc_node(size, flags, node);
 }

commit 5595cffc8248e4672c5803547445e85e4053c8fc
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Tue Aug 5 09:28:47 2008 +0300

    SLUB: dynamic per-cache MIN_PARTIAL
    
    This patch changes the static MIN_PARTIAL to a dynamic per-cache ->min_partial
    value that is calculated from object size. The bigger the object size, the more
    pages we keep on the partial list.
    
    I tested SLAB, SLUB, and SLUB with this patch on Jens Axboe's 'netio' example
    script of the fio benchmarking tool. The script stresses the networking
    subsystem which should also give a fairly good beating of kmalloc() et al.
    
    To run the test yourself, first clone the fio repository:
    
      git clone git://git.kernel.dk/fio.git
    
    and then run the following command n times on your machine:
    
      time ./fio examples/netio
    
    The results on my 2-way 64-bit x86 machine are as follows:
    
      [ the minimum, maximum, and average are captured from 50 individual runs ]
    
                     real time (seconds)
                     min      max      avg      sd
      SLAB           22.76    23.38    22.98    0.17
      SLUB           22.80    25.78    23.46    0.72
      SLUB (dynamic) 22.74    23.54    23.00    0.20
    
                     sys time (seconds)
                     min      max      avg      sd
      SLAB           6.90     8.28     7.70     0.28
      SLUB           7.42     16.95    8.89     2.28
      SLUB (dynamic) 7.17     8.64     7.73     0.29
    
                     user time (seconds)
                     min      max      avg      sd
      SLAB           36.89    38.11    37.50    0.29
      SLUB           30.85    37.99    37.06    1.67
      SLUB (dynamic) 36.75    38.07    37.59    0.32
    
    As you can see from the above numbers, this patch brings SLUB to the same level
    as SLAB for this particular workload fixing a ~2% regression. I'd expect this
    change to help similar workloads that allocate a lot of objects that are close
    to the size of a page.
    
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5bad61a93f65..2f5c16b1aacd 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -46,6 +46,7 @@ struct kmem_cache_cpu {
 struct kmem_cache_node {
 	spinlock_t list_lock;	/* Protect partial list and nr_partial */
 	unsigned long nr_partial;
+	unsigned long min_partial;
 	struct list_head partial;
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_t nr_slabs;

commit 51cc50685a4275c6a02653670af9f108a64e01cf
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Jul 25 19:45:34 2008 -0700

    SL*B: drop kmem cache argument from constructor
    
    Kmem cache passed to constructor is only needed for constructors that are
    themselves multiplexeres.  Nobody uses this "feature", nor does anybody uses
    passed kmem cache in non-trivial way, so pass only pointer to object.
    
    Non-trivial places are:
            arch/powerpc/mm/init_64.c
            arch/powerpc/mm/hugetlbpage.c
    
    This is flag day, yes.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Jon Tollefson <kniht@linux.vnet.ibm.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Matt Mackall <mpm@selenic.com>
    [akpm@linux-foundation.org: fix arch/powerpc/mm/hugetlbpage.c]
    [akpm@linux-foundation.org: fix mm/slab.c]
    [akpm@linux-foundation.org: fix ubifs]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index d117ea2825a9..5bad61a93f65 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -85,7 +85,7 @@ struct kmem_cache {
 	struct kmem_cache_order_objects min;
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
-	void (*ctor)(struct kmem_cache *, void *);
+	void (*ctor)(void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	const char *name;	/* Name (only for display!) */

commit cde53535991fbb5c34a1566f25955297c1487b8d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jul 4 09:59:22 2008 -0700

    Christoph has moved
    
    Remove all clameter@sgi.com addresses from the kernel tree since they will
    become invalid on June 27th.  Change my maintainer email address for the
    slab allocators to cl@linux-foundation.org (which will be the new email
    address for the future).
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index cef6f8fddd7d..d117ea2825a9 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -4,7 +4,7 @@
 /*
  * SLUB : A Slab allocator without object queues.
  *
- * (C) 2007 SGI, Christoph Lameter <clameter@sgi.com>
+ * (C) 2007 SGI, Christoph Lameter
  */
 #include <linux/types.h>
 #include <linux/gfp.h>

commit 41d54d3bf83f62d3ff5948cb788fe6007e66a0d0
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Thu Jul 3 09:14:26 2008 -0500

    slub: Do not use 192 byte sized cache if minimum alignment is 128 byte
    
    The 192 byte cache is not necessary if we have a basic alignment of 128
    byte. If it would be used then the 192 would be aligned to the next 128 byte
    boundary which would result in another 256 byte cache. Two 256 kmalloc caches
    cause sysfs to complain about a duplicate entry.
    
    MIPS needs 128 byte aligned kmalloc caches and spits out warnings on boot without
    this patch.
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 71e43a12ebbb..cef6f8fddd7d 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -137,10 +137,12 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <= KMALLOC_MIN_SIZE)
 		return KMALLOC_SHIFT_LOW;
 
+#if KMALLOC_MIN_SIZE <= 64
 	if (size > 64 && size <= 96)
 		return 1;
 	if (size > 128 && size <= 192)
 		return 2;
+#endif
 	if (size <=          8) return 3;
 	if (size <=         16) return 4;
 	if (size <=         32) return 5;

commit 65c3376aaca96c66aa76014aaf430398964b68cb
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 14 19:11:40 2008 +0300

    slub: Fallback to minimal order during slab page allocation
    
    If any higher order allocation fails then fall back the smallest order
    necessary to contain at least one object. This enables fallback for all
    allocations to order 0 pages. The fallback will waste more memory (objects
    will not fit neatly) and the fallback slabs will be not as efficient as larger
    slabs since they contain less objects.
    
    Note that SLAB also depends on order 1 allocations for some slabs that waste
    too much memory if forced into PAGE_SIZE'd page. SLUB now can now deal with
    failing order 1 allocs which SLAB cannot do.
    
    Add a new field min that will contain the objects for the smallest possible order
    for a slab cache.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 4236b5dee812..71e43a12ebbb 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -29,6 +29,7 @@ enum stat_item {
 	DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
 	DEACTIVATE_TO_TAIL,	/* Cpu slab was moved to the tail of partials */
 	DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
+	ORDER_FALLBACK,		/* Number of times fallback was necessary */
 	NR_SLUB_STAT_ITEMS };
 
 struct kmem_cache_cpu {
@@ -81,6 +82,7 @@ struct kmem_cache {
 
 	/* Allocation and freeing of slabs */
 	struct kmem_cache_order_objects max;
+	struct kmem_cache_order_objects min;
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(struct kmem_cache *, void *);

commit 205ab99dd103e3dd5b0964dad8a16dfe2db69b2e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 14 19:11:40 2008 +0300

    slub: Update statistics handling for variable order slabs
    
    Change the statistics to consider that slabs of the same slabcache
    can have different number of objects in them since they may be of
    different order.
    
    Provide a new sysfs field
    
            total_objects
    
    which shows the total objects that the allocated slabs of a slabcache
    could hold.
    
    Add a max field that holds the largest slab order that was ever used
    for a slab cache.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 4131e5fbd18b..4236b5dee812 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -48,6 +48,7 @@ struct kmem_cache_node {
 	struct list_head partial;
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_t nr_slabs;
+	atomic_long_t total_objects;
 	struct list_head full;
 #endif
 };
@@ -79,6 +80,7 @@ struct kmem_cache {
 	struct kmem_cache_node local_node;
 
 	/* Allocation and freeing of slabs */
+	struct kmem_cache_order_objects max;
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(struct kmem_cache *, void *);

commit 834f3d119234b35a1985a2449831d99356637937
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 14 19:11:31 2008 +0300

    slub: Add kmem_cache_order_objects struct
    
    Pack the order and the number of objects into a single word.
    This saves some memory in the kmem_cache_structure and more importantly
    allows us to fetch both values atomically.
    
    Later the slab orders become runtime configurable and we need to fetch these
    two items together in order to properly allocate a slab and initialize its
    objects.
    
    Fix the race by fetching the order and the number of objects in one word.
    
    [penberg@cs.helsinki.fi: fix memset() page order in new_slab()]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 79d59c937fac..4131e5fbd18b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -52,6 +52,15 @@ struct kmem_cache_node {
 #endif
 };
 
+/*
+ * Word size structure that can be atomically updated or read and that
+ * contains both the order and the number of objects that a slab of the
+ * given order would contain.
+ */
+struct kmem_cache_order_objects {
+	unsigned long x;
+};
+
 /*
  * Slab cache management.
  */
@@ -61,7 +70,7 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
-	int order;		/* Current preferred allocation order */
+	struct kmem_cache_order_objects oo;
 
 	/*
 	 * Avoid an extra cache line for UP, SMP and for the node local to
@@ -70,7 +79,6 @@ struct kmem_cache {
 	struct kmem_cache_node local_node;
 
 	/* Allocation and freeing of slabs */
-	int objects;		/* Number of objects in slab */
 	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(struct kmem_cache *, void *);

commit 0f389ec63077521166f071e1e970aed36147fd45
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 14 18:53:02 2008 +0300

    slub: No need for per node slab counters if !SLUB_DEBUG
    
    The per node counters are used mainly for showing data through the sysfs API.
    If that API is not compiled in then there is no point in keeping track of this
    data. Disable counters for the number of slabs and the number of total slabs
    if !SLUB_DEBUG. Incrementing the per node counters is also accessing a
    potentially contended cacheline so this could actually be a performance
    benefit to embedded systems.
    
    SLABINFO support is also affected. It now must depends on SLUB_DEBUG (which
    is on by default).
    
    Patch also avoids a check for a NULL kmem_cache_node pointer in new_slab()
    if the system is not compiled with NUMA support.
    
    [penberg@cs.helsinki.fi: fix oops and move ->nr_slabs into CONFIG_SLUB_DEBUG]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index b00c1c73eb0a..79d59c937fac 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -45,9 +45,9 @@ struct kmem_cache_cpu {
 struct kmem_cache_node {
 	spinlock_t list_lock;	/* Protect partial list and nr_partial */
 	unsigned long nr_partial;
-	atomic_long_t nr_slabs;
 	struct list_head partial;
 #ifdef CONFIG_SLUB_DEBUG
+	atomic_long_t nr_slabs;
 	struct list_head full;
 #endif
 };

commit 6446faa2ff30ca77c5b25e886bbbfb81c63f1c91
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Feb 15 23:45:26 2008 -0800

    slub: Fix up comments
    
    Provide comments and fix up various spelling / style issues.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 57deecc79d52..b00c1c73eb0a 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -61,7 +61,7 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
-	int order;
+	int order;		/* Current preferred allocation order */
 
 	/*
 	 * Avoid an extra cache line for UP, SMP and for the node local to
@@ -138,11 +138,11 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <=        512) return 9;
 	if (size <=       1024) return 10;
 	if (size <=   2 * 1024) return 11;
+	if (size <=   4 * 1024) return 12;
 /*
  * The following is only needed to support architectures with a larger page
  * size than 4k.
  */
-	if (size <=   4 * 1024) return 12;
 	if (size <=   8 * 1024) return 13;
 	if (size <=  16 * 1024) return 14;
 	if (size <=  32 * 1024) return 15;

commit 331dc558fa020451ff773973cee855fd721aa88e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Thu Feb 14 14:28:09 2008 -0800

    slub: Support 4k kmallocs again to compensate for page allocator slowness
    
    Currently we hand off PAGE_SIZEd kmallocs to the page allocator in the
    mistaken belief that the page allocator can handle these allocations
    effectively. However, measurements indicate a minimum slowdown by the
    factor of 8 (and that is only SMP, NUMA is much worse) vs the slub fastpath
    which causes regressions in tbench.
    
    Increase the number of kmalloc caches by one so that we again handle 4k
    kmallocs directly from slub. 4k page buffering for the page allocator
    will be performed by slub like done by slab.
    
    At some point the page allocator fastpath should be fixed. A lot of the kernel
    would benefit from a faster ability to allocate a single page. If that is
    done then the 4k allocs may again be forwarded to the page allocator and this
    patch could be reverted.
    
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 98be113cf935..57deecc79d52 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -111,7 +111,7 @@ struct kmem_cache {
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[PAGE_SHIFT];
+extern struct kmem_cache kmalloc_caches[PAGE_SHIFT + 1];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -197,7 +197,7 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
-		if (size > PAGE_SIZE / 2)
+		if (size > PAGE_SIZE)
 			return kmalloc_large(size, flags);
 
 		if (!(flags & SLUB_DMA)) {
@@ -219,7 +219,7 @@ void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) &&
-		size <= PAGE_SIZE / 2 && !(flags & SLUB_DMA)) {
+		size <= PAGE_SIZE && !(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)

commit b7a49f0d4c34166ae84089d9f145cfaae1b0eec5
Author: Christoph Lameter <clameter@sgi.com>
Date:   Thu Feb 14 14:21:32 2008 -0800

    slub: Determine gfpflags once and not every time a slab is allocated
    
    Currently we determine the gfp flags to pass to the page allocator
    each time a slab is being allocated.
    
    Determine the bits to be set at the time the slab is created. Store
    in a new allocflags field and add the flags in allocate_slab().
    
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a849c472b845..98be113cf935 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -71,6 +71,7 @@ struct kmem_cache {
 
 	/* Allocation and freeing of slabs */
 	int objects;		/* Number of objects in slab */
+	gfp_t allocflags;	/* gfp flags to use on each alloc */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(struct kmem_cache *, void *);
 	int inuse;		/* Offset to metadata */

commit eada35efcb2773cf49aa26277e056122e1a3405c
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Feb 11 22:47:46 2008 +0200

    slub: kmalloc page allocator pass-through cleanup
    
    This adds a proper function for kmalloc page allocator pass-through. While it
    simplifies any code that does slab tracing code a lot, I think it's a
    worthwhile cleanup in itself.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5e6d3d634d5b..a849c472b845 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -188,12 +188,16 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
+static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
+{
+	return (void *)__get_free_pages(flags | __GFP_COMP, get_order(size));
+}
+
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
 		if (size > PAGE_SIZE / 2)
-			return (void *)__get_free_pages(flags | __GFP_COMP,
-							get_order(size));
+			return kmalloc_large(size, flags);
 
 		if (!(flags & SLUB_DMA)) {
 			struct kmem_cache *s = kmalloc_slab(size);

commit 8ff12cfc009a2a38d87fa7058226fe197bb2696f
Author: Christoph Lameter <clameter@sgi.com>
Date:   Thu Feb 7 17:47:41 2008 -0800

    SLUB: Support for performance statistics
    
    The statistics provided here allow the monitoring of allocator behavior but
    at the cost of some (minimal) loss of performance. Counters are placed in
    SLUB's per cpu data structure. The per cpu structure may be extended by the
    statistics to grow larger than one cacheline which will increase the cache
    footprint of SLUB.
    
    There is a compile option to enable/disable the inclusion of the runtime
    statistics and its off by default.
    
    The slabinfo tool is enhanced to support these statistics via two options:
    
    -D      Switches the line of information displayed for a slab from size
            mode to activity mode.
    
    -A      Sorts the slabs displayed by activity. This allows the display of
            the slabs most important to the performance of a certain load.
    
    -r      Report option will report detailed statistics on
    
    Example (tbench load):
    
    slabinfo -AD            ->Shows the most active slabs
    
    Name                   Objects    Alloc     Free   %Fast
    skbuff_fclone_cache         33 111953835 111953835  99  99
    :0000192                  2666  5283688  5281047  99  99
    :0001024                   849  5247230  5246389  83  83
    vm_area_struct            1349   119642   118355  91  22
    :0004096                    15    66753    66751  98  98
    :0000064                  2067    25297    23383  98  78
    dentry                   10259    28635    18464  91  45
    :0000080                 11004    18950     8089  98  98
    :0000096                  1703    12358    10784  99  98
    :0000128                   762    10582     9875  94  18
    :0000512                   184     9807     9647  95  81
    :0002048                   479     9669     9195  83  65
    anon_vma                   777     9461     9002  99  71
    kmalloc-8                 6492     9981     5624  99  97
    :0000768                   258     7174     6931  58  15
    
    So the skbuff_fclone_cache is of highest importance for the tbench load.
    Pretty high load on the 192 sized slab. Look for the aliases
    
    slabinfo -a | grep 000192
    :0000192     <- xfs_btree_cur filp kmalloc-192 uid_cache tw_sock_TCP
            request_sock_TCPv6 tw_sock_TCPv6 skbuff_head_cache xfs_ili
    
    Likely skbuff_head_cache.
    
    
    Looking into the statistics of the skbuff_fclone_cache is possible through
    
    slabinfo skbuff_fclone_cache    ->-r option implied if cache name is mentioned
    
    
    .... Usual output ...
    
    Slab Perf Counter       Alloc     Free %Al %Fr
    --------------------------------------------------
    Fastpath             111953360 111946981  99  99
    Slowpath                 1044     7423   0   0
    Page Alloc                272      264   0   0
    Add partial                25      325   0   0
    Remove partial             86      264   0   0
    RemoteObj/SlabFrozen      350     4832   0   0
    Total                111954404 111954404
    
    Flushes       49 Refill        0
    Deactivate Full=325(92%) Empty=0(0%) ToHead=24(6%) ToTail=1(0%)
    
    Looks good because the fastpath is overwhelmingly taken.
    
    
    skbuff_head_cache:
    
    Slab Perf Counter       Alloc     Free %Al %Fr
    --------------------------------------------------
    Fastpath              5297262  5259882  99  99
    Slowpath                 4477    39586   0   0
    Page Alloc                937      824   0   0
    Add partial                 0     2515   0   0
    Remove partial           1691      824   0   0
    RemoteObj/SlabFrozen     2621     9684   0   0
    Total                 5301739  5299468
    
    Deactivate Full=2620(100%) Empty=0(0%) ToHead=0(0%) ToTail=0(0%)
    
    
    Descriptions of the output:
    
    Total:          The total number of allocation and frees that occurred for a
                    slab
    
    Fastpath:       The number of allocations/frees that used the fastpath.
    
    Slowpath:       Other allocations
    
    Page Alloc:     Number of calls to the page allocator as a result of slowpath
                    processing
    
    Add Partial:    Number of slabs added to the partial list through free or
                    alloc (occurs during cpuslab flushes)
    
    Remove Partial: Number of slabs removed from the partial list as a result of
                    allocations retrieving a partial slab or by a free freeing
                    the last object of a slab.
    
    RemoteObj/Froz: How many times were remotely freed object encountered when a
                    slab was about to be deactivated. Frozen: How many times was
                    free able to skip list processing because the slab was in use
                    as the cpuslab of another processor.
    
    Flushes:        Number of times the cpuslab was flushed on request
                    (kmem_cache_shrink, may result from races in __slab_alloc)
    
    Refill:         Number of times we were able to refill the cpuslab from
                    remotely freed objects for the same slab.
    
    Deactivate:     Statistics how slabs were deactivated. Shows how they were
                    put onto the partial list.
    
    In general fastpath is very good. Slowpath without partial list processing is
    also desirable. Any touching of partial list uses node specific locks which
    may potentially cause list lock contention.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index ddb1a706b144..5e6d3d634d5b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -11,12 +11,35 @@
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
 
+enum stat_item {
+	ALLOC_FASTPATH,		/* Allocation from cpu slab */
+	ALLOC_SLOWPATH,		/* Allocation by getting a new cpu slab */
+	FREE_FASTPATH,		/* Free to cpu slub */
+	FREE_SLOWPATH,		/* Freeing not to cpu slab */
+	FREE_FROZEN,		/* Freeing to frozen slab */
+	FREE_ADD_PARTIAL,	/* Freeing moves slab to partial list */
+	FREE_REMOVE_PARTIAL,	/* Freeing removes last object */
+	ALLOC_FROM_PARTIAL,	/* Cpu slab acquired from partial list */
+	ALLOC_SLAB,		/* Cpu slab acquired from page allocator */
+	ALLOC_REFILL,		/* Refill cpu slab from slab freelist */
+	FREE_SLAB,		/* Slab freed to the page allocator */
+	CPUSLAB_FLUSH,		/* Abandoning of the cpu slab */
+	DEACTIVATE_FULL,	/* Cpu slab was full when deactivated */
+	DEACTIVATE_EMPTY,	/* Cpu slab was empty when deactivated */
+	DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
+	DEACTIVATE_TO_TAIL,	/* Cpu slab was moved to the tail of partials */
+	DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
+	NR_SLUB_STAT_ITEMS };
+
 struct kmem_cache_cpu {
 	void **freelist;	/* Pointer to first free per cpu object */
 	struct page *page;	/* The slab from which we are allocating */
 	int node;		/* The node of the page (or -1 for debug) */
 	unsigned int offset;	/* Freepointer offset (in word units) */
 	unsigned int objsize;	/* Size of an object (from kmem_cache) */
+#ifdef CONFIG_SLUB_STATS
+	unsigned stat[NR_SLUB_STAT_ITEMS];
+#endif
 };
 
 struct kmem_cache_node {

commit da89b79ed06bac8e9b5b6874d4efc5382e1091de
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Jan 7 23:20:31 2008 -0800

    Explain kmem_cache_cpu fields
    
    Add some comments explaining the fields of the kmem_cache_cpu structure.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a06ee26193c2..ddb1a706b144 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -12,11 +12,11 @@
 #include <linux/kobject.h>
 
 struct kmem_cache_cpu {
-	void **freelist;
-	struct page *page;
-	int node;
-	unsigned int offset;
-	unsigned int objsize;
+	void **freelist;	/* Pointer to first free per cpu object */
+	struct page *page;	/* The slab from which we are allocating */
+	int node;		/* The node of the page (or -1 for debug) */
+	unsigned int offset;	/* Freepointer offset (in word units) */
+	unsigned int objsize;	/* Size of an object (from kmem_cache) */
 };
 
 struct kmem_cache_node {

commit 9824601ead957a29e35d539e43266c003f7b085b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Jan 7 23:20:26 2008 -0800

    SLUB: rename defrag to remote_node_defrag_ratio
    
    The NUMA defrag works by allocating objects from partial slabs on remote
    nodes.  Rename it to
    
            remote_node_defrag_ratio
    
    to be clear about this.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 40801e754afb..a06ee26193c2 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -59,7 +59,10 @@ struct kmem_cache {
 #endif
 
 #ifdef CONFIG_NUMA
-	int defrag_ratio;
+	/*
+	 * Defragmentation by allocating from a remote node.
+	 */
+	int remote_node_defrag_ratio;
 	struct kmem_cache_node *node[MAX_NUMNODES];
 #endif
 #ifdef CONFIG_SMP

commit 158a962422e4a54dc256b6a9b9562f3d30d34d9c
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Jan 2 13:04:48 2008 -0800

    Unify /proc/slabinfo configuration
    
    Both SLUB and SLAB really did almost exactly the same thing for
    /proc/slabinfo setup, using duplicate code and per-allocator #ifdef's.
    
    This just creates a common CONFIG_SLABINFO that is enabled by both SLUB
    and SLAB, and shares all the setup code.  Maybe SLOB will want this some
    day too.
    
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index b7d9408a00ff..40801e754afb 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -200,6 +200,4 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 }
 #endif
 
-extern const struct seq_operations slabinfo_op;
-
 #endif /* _LINUX_SLUB_DEF_H */

commit 57ed3eda977a215f054102b460ab0eb5d8d112e6
Author: Pekka J Enberg <penberg@cs.helsinki.fi>
Date:   Tue Jan 1 17:23:28 2008 +0100

    slub: provide /proc/slabinfo
    
    This adds a read-only /proc/slabinfo file on SLUB, that makes slabtop work.
    
    [ mingo@elte.hu: build fix. ]
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 40801e754afb..b7d9408a00ff 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -200,4 +200,6 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 }
 #endif
 
+extern const struct seq_operations slabinfo_op;
+
 #endif /* _LINUX_SLUB_DEF_H */

commit 4ba9b9d0ba0a49d91fa6417c7510ee36f48cf957
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 23:25:51 2007 -0700

    Slab API: remove useless ctor parameter and reorder parameters
    
    Slab constructors currently have a flags parameter that is never used.  And
    the order of the arguments is opposite to other slab functions.  The object
    pointer is placed before the kmem_cache pointer.
    
    Convert
    
            ctor(void *object, struct kmem_cache *s, unsigned long flags)
    
    to
    
            ctor(struct kmem_cache *s, void *object)
    
    throughout the kernel
    
    [akpm@linux-foundation.org: coupla fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index d65159d1d4f5..40801e754afb 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -49,7 +49,7 @@ struct kmem_cache {
 	/* Allocation and freeing of slabs */
 	int objects;		/* Number of objects in slab */
 	int refcount;		/* Refcount for slab cache destroy */
-	void (*ctor)(void *, struct kmem_cache *, unsigned long);
+	void (*ctor)(struct kmem_cache *, void *);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	const char *name;	/* Name (only for display!) */

commit 42a9fdbb12ac6c027b4b91ab9b5a60aa3a834489
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:26:09 2007 -0700

    SLUB: Optimize cacheline use for zeroing
    
    We touch a cacheline in the kmem_cache structure for zeroing to get the
    size. However, the hot paths in slab_alloc and slab_free do not reference
    any other fields in kmem_cache, so we may have to just bring in the
    cacheline for this one access.
    
    Add a new field to kmem_cache_cpu that contains the object size. That
    cacheline must already be used in the hotpaths. So we save one cacheline
    on every slab_alloc if we zero.
    
    We need to update the kmem_cache_cpu object size if an aliasing operation
    changes the objsize of an non debug slab.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f74716b59ce2..d65159d1d4f5 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -16,6 +16,7 @@ struct kmem_cache_cpu {
 	struct page *page;
 	int node;
 	unsigned int offset;
+	unsigned int objsize;
 };
 
 struct kmem_cache_node {

commit 4c93c355d5d563f300df7e61ef753d7a064411e9
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:26:08 2007 -0700

    SLUB: Place kmem_cache_cpu structures in a NUMA aware way
    
    The kmem_cache_cpu structures introduced are currently an array placed in the
    kmem_cache struct. Meaning the kmem_cache_cpu structures are overwhelmingly
    on the wrong node for systems with a higher amount of nodes. These are
    performance critical structures since the per node information has
    to be touched for every alloc and free in a slab.
    
    In order to place the kmem_cache_cpu structure optimally we put an array
    of pointers to kmem_cache_cpu structs in kmem_cache (similar to SLAB).
    
    However, the kmem_cache_cpu structures can now be allocated in a more
    intelligent way.
    
    We would like to put per cpu structures for the same cpu but different
    slab caches in cachelines together to save space and decrease the cache
    footprint. However, the slab allocators itself control only allocations
    per node. We set up a simple per cpu array for every processor with
    100 per cpu structures which is usually enough to get them all set up right.
    If we run out then we fall back to kmalloc_node. This also solves the
    bootstrap problem since we do not have to use slab allocator functions
    early in boot to get memory for the small per cpu structures.
    
    Pro:
            - NUMA aware placement improves memory performance
            - All global structures in struct kmem_cache become readonly
            - Dense packing of per cpu structures reduces cacheline
              footprint in SMP and NUMA.
            - Potential avoidance of exclusive cacheline fetches
              on the free and alloc hotpath since multiple kmem_cache_cpu
              structures are in one cacheline. This is particularly important
              for the kmalloc array.
    
    Cons:
            - Additional reference to one read only cacheline (per cpu
              array of pointers to kmem_cache_cpu) in both slab_alloc()
              and slab_free().
    
    [akinobu.mita@gmail.com: fix cpu hotplug offline/online path]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: "Pekka Enberg" <penberg@cs.helsinki.fi>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 92e10cf6d0e8..f74716b59ce2 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -16,8 +16,7 @@ struct kmem_cache_cpu {
 	struct page *page;
 	int node;
 	unsigned int offset;
-	/* Lots of wasted space */
-} ____cacheline_aligned_in_smp;
+};
 
 struct kmem_cache_node {
 	spinlock_t list_lock;	/* Protect partial list and nr_partial */
@@ -62,7 +61,11 @@ struct kmem_cache {
 	int defrag_ratio;
 	struct kmem_cache_node *node[MAX_NUMNODES];
 #endif
-	struct kmem_cache_cpu cpu_slab[NR_CPUS];
+#ifdef CONFIG_SMP
+	struct kmem_cache_cpu *cpu_slab[NR_CPUS];
+#else
+	struct kmem_cache_cpu cpu_slab;
+#endif
 };
 
 /*

commit b3fba8da653999c67d7517050f196e92da6f8d3b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:26:06 2007 -0700

    SLUB: Move page->offset to kmem_cache_cpu->offset
    
    We need the offset from the page struct during slab_alloc and slab_free. In
    both cases we also reference the cacheline of the kmem_cache_cpu structure.
    We can therefore move the offset field into the kmem_cache_cpu structure
    freeing up 16 bits in the page struct.
    
    Moving the offset allows an allocation from slab_alloc() without touching the
    page struct in the hot path.
    
    The only thing left in slab_free() that touches the page struct cacheline for
    per cpu freeing is the checking of SlabDebug(page). The next patch deals with
    that.
    
    Use the available 16 bits to broaden page->inuse. More than 64k objects per
    slab become possible and we can get rid of the checks for that limitation.
    
    No need anymore to shrink the order of slabs if we boot with 2M sized slabs
    (slub_min_order=9).
    
    No need anymore to switch off the offset calculation for very large slabs
    since the field in the kmem_cache_cpu structure is 32 bits and so the offset
    field can now handle slab sizes of up to 8GB.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 0a7ae25f7e8d..92e10cf6d0e8 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -15,6 +15,7 @@ struct kmem_cache_cpu {
 	void **freelist;
 	struct page *page;
 	int node;
+	unsigned int offset;
 	/* Lots of wasted space */
 } ____cacheline_aligned_in_smp;
 

commit dfb4f09609827301740ef0a11b37530d190f1681
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:26:05 2007 -0700

    SLUB: Avoid page struct cacheline bouncing due to remote frees to cpu slab
    
    A remote free may access the same page struct that also contains the lockless
    freelist for the cpu slab. If objects have a short lifetime and are freed by
    a different processor then remote frees back to the slab from which we are
    currently allocating are frequent. The cacheline with the page struct needs
    to be repeately acquired in exclusive mode by both the allocating thread and
    the freeing thread. If this is frequent enough then performance will suffer
    because of cacheline bouncing.
    
    This patchset puts the lockless_freelist pointer in its own cacheline. In
    order to make that happen we introduce a per cpu structure called
    kmem_cache_cpu.
    
    Instead of keeping an array of pointers to page structs we now keep an array
    to a per cpu structure that--among other things--contains the pointer to the
    lockless freelist. The freeing thread can then keep possession of exclusive
    access to the page struct cacheline while the allocating thread keeps its
    exclusive access to the cacheline containing the per cpu structure.
    
    This works as long as the allocating cpu is able to service its request
    from the lockless freelist. If the lockless freelist runs empty then the
    allocating thread needs to acquire exclusive access to the cacheline with
    the page struct lock the slab.
    
    The allocating thread will then check if new objects were freed to the per
    cpu slab. If so it will keep the slab as the cpu slab and continue with the
    recently remote freed objects. So the allocating thread can take a series
    of just freed remote pages and dish them out again. Ideally allocations
    could be just recycling objects in the same slab this way which will lead
    to an ideal allocation / remote free pattern.
    
    The number of objects that can be handled in this way is limited by the
    capacity of one slab. Increasing slab size via slub_min_objects/
    slub_max_order may increase the number of objects and therefore performance.
    
    If the allocating thread runs out of objects and finds that no objects were
    put back by the remote processor then it will retrieve a new slab (from the
    partial lists or from the page allocator) and start with a whole
    new set of objects while the remote thread may still be freeing objects to
    the old cpu slab. This may then repeat until the new slab is also exhausted.
    If remote freeing has freed objects in the earlier slab then that earlier
    slab will now be on the partial freelist and the allocating thread will
    pick that slab next for allocation. So the loop is extended. However,
    both threads need to take the list_lock to make the swizzling via
    the partial list happen.
    
    It is likely that this kind of scheme will keep the objects being passed
    around to a small set that can be kept in the cpu caches leading to increased
    performance.
    
    More code cleanups become possible:
    
    - Instead of passing a cpu we can now pass a kmem_cache_cpu structure around.
      Allows reducing the number of parameters to various functions.
    - Can define a new node_match() function for NUMA to encapsulate locality
      checks.
    
    Effect on allocations:
    
    Cachelines touched before this patch:
    
            Write:  page cache struct and first cacheline of object
    
    Cachelines touched after this patch:
    
            Write:  kmem_cache_cpu cacheline and first cacheline of object
            Read: page cache struct (but see later patch that avoids touching
                    that cacheline)
    
    The handling when the lockless alloc list runs empty gets to be a bit more
    complicated since another cacheline has now to be written to. But that is
    halfway out of the hot path.
    
    Effect on freeing:
    
    Cachelines touched before this patch:
    
            Write: page_struct and first cacheline of object
    
    Cachelines touched after this patch depending on how we free:
    
      Write(to cpu_slab):   kmem_cache_cpu struct and first cacheline of object
      Write(to other):      page struct and first cacheline of object
    
      Read(to cpu_slab):    page struct to id slab etc. (but see later patch that
                            avoids touching the page struct on free)
      Read(to other):       cpu local kmem_cache_cpu struct to verify its not
                            the cpu slab.
    
    Summary:
    
    Pro:
            - Distinct cachelines so that concurrent remote frees and local
              allocs on a cpuslab can occur without cacheline bouncing.
            - Avoids potential bouncing cachelines because of neighboring
              per cpu pointer updates in kmem_cache's cpu_slab structure since
              it now grows to a cacheline (Therefore remove the comment
              that talks about that concern).
    
    Cons:
            - Freeing objects now requires the reading of one additional
              cacheline. That can be mitigated for some cases by the following
              patches but its not possible to completely eliminate these
              references.
    
            - Memory usage grows slightly.
    
            The size of each per cpu object is blown up from one word
            (pointing to the page_struct) to one cacheline with various data.
            So this is NR_CPUS*NR_SLABS*L1_BYTES more memory use. Lets say
            NR_SLABS is 100 and a cache line size of 128 then we have just
            increased SLAB metadata requirements by 12.8k per cpu.
            (Another later patch reduces these requirements)
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 3b361b2906bd..0a7ae25f7e8d 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -11,6 +11,13 @@
 #include <linux/workqueue.h>
 #include <linux/kobject.h>
 
+struct kmem_cache_cpu {
+	void **freelist;
+	struct page *page;
+	int node;
+	/* Lots of wasted space */
+} ____cacheline_aligned_in_smp;
+
 struct kmem_cache_node {
 	spinlock_t list_lock;	/* Protect partial list and nr_partial */
 	unsigned long nr_partial;
@@ -54,7 +61,7 @@ struct kmem_cache {
 	int defrag_ratio;
 	struct kmem_cache_node *node[MAX_NUMNODES];
 #endif
-	struct page *cpu_slab[NR_CPUS];
+	struct kmem_cache_cpu cpu_slab[NR_CPUS];
 };
 
 /*

commit aadb4bc4a1f9108c1d0fbd121827c936c2ed4217
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:24:38 2007 -0700

    SLUB: direct pass through of page size or higher kmalloc requests
    
    This gets rid of all kmalloc caches larger than page size.  A kmalloc
    request larger than PAGE_SIZE > 2 is going to be passed through to the page
    allocator.  This works both inline where we will call __get_free_pages
    instead of kmem_cache_alloc and in __kmalloc.
    
    kfree is modified to check if the object is in a slab page. If not then
    the page is freed via the page allocator instead. Roughly similar to what
    SLOB does.
    
    Advantages:
    - Reduces memory overhead for kmalloc array
    - Large kmalloc operations are faster since they do not
      need to pass through the slab allocator to get to the
      page allocator.
    - Performance increase of 10%-20% on alloc and 50% on free for
      PAGE_SIZEd allocations.
      SLUB must call page allocator for each alloc anyways since
      the higher order pages which that allowed avoiding the page alloc calls
      are not available in a reliable way anymore. So we are basically removing
      useless slab allocator overhead.
    - Large kmallocs yields page aligned object which is what
      SLAB did. Bad things like using page sized kmalloc allocations to
      stand in for page allocate allocs can be transparently handled and are not
      distinguishable from page allocator uses.
    - Checking for too large objects can be removed since
      it is done by the page allocator.
    
    Drawbacks:
    - No accounting for large kmalloc slab allocations anymore
    - No debugging of large kmalloc slab allocations.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 74962077f632..3b361b2906bd 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -72,7 +72,7 @@ struct kmem_cache {
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
  */
-extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
+extern struct kmem_cache kmalloc_caches[PAGE_SHIFT];
 
 /*
  * Sorry that the following has to be that ugly but some versions of GCC
@@ -83,9 +83,6 @@ static __always_inline int kmalloc_index(size_t size)
 	if (!size)
 		return 0;
 
-	if (size > KMALLOC_MAX_SIZE)
-		return -1;
-
 	if (size <= KMALLOC_MIN_SIZE)
 		return KMALLOC_SHIFT_LOW;
 
@@ -102,6 +99,10 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <=        512) return 9;
 	if (size <=       1024) return 10;
 	if (size <=   2 * 1024) return 11;
+/*
+ * The following is only needed to support architectures with a larger page
+ * size than 4k.
+ */
 	if (size <=   4 * 1024) return 12;
 	if (size <=   8 * 1024) return 13;
 	if (size <=  16 * 1024) return 14;
@@ -109,13 +110,9 @@ static __always_inline int kmalloc_index(size_t size)
 	if (size <=  64 * 1024) return 16;
 	if (size <= 128 * 1024) return 17;
 	if (size <= 256 * 1024) return 18;
-	if (size <=  512 * 1024) return 19;
+	if (size <= 512 * 1024) return 19;
 	if (size <= 1024 * 1024) return 20;
 	if (size <=  2 * 1024 * 1024) return 21;
-	if (size <=  4 * 1024 * 1024) return 22;
-	if (size <=  8 * 1024 * 1024) return 23;
-	if (size <= 16 * 1024 * 1024) return 24;
-	if (size <= 32 * 1024 * 1024) return 25;
 	return -1;
 
 /*
@@ -140,19 +137,6 @@ static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 	if (index == 0)
 		return NULL;
 
-	/*
-	 * This function only gets expanded if __builtin_constant_p(size), so
-	 * testing it here shouldn't be needed.  But some versions of gcc need
-	 * help.
-	 */
-	if (__builtin_constant_p(size) && index < 0) {
-		/*
-		 * Generate a link failure. Would be great if we could
-		 * do something to stop the compile here.
-		 */
-		extern void __kmalloc_size_too_large(void);
-		__kmalloc_size_too_large();
-	}
 	return &kmalloc_caches[index];
 }
 
@@ -168,15 +152,21 @@ void *__kmalloc(size_t size, gfp_t flags);
 
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
-	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
-		struct kmem_cache *s = kmalloc_slab(size);
+	if (__builtin_constant_p(size)) {
+		if (size > PAGE_SIZE / 2)
+			return (void *)__get_free_pages(flags | __GFP_COMP,
+							get_order(size));
 
-		if (!s)
-			return ZERO_SIZE_PTR;
+		if (!(flags & SLUB_DMA)) {
+			struct kmem_cache *s = kmalloc_slab(size);
+
+			if (!s)
+				return ZERO_SIZE_PTR;
 
-		return kmem_cache_alloc(s, flags);
-	} else
-		return __kmalloc(size, flags);
+			return kmem_cache_alloc(s, flags);
+		}
+	}
+	return __kmalloc(size, flags);
 }
 
 #ifdef CONFIG_NUMA
@@ -185,15 +175,16 @@ void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
-	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
-		struct kmem_cache *s = kmalloc_slab(size);
+	if (__builtin_constant_p(size) &&
+		size <= PAGE_SIZE / 2 && !(flags & SLUB_DMA)) {
+			struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)
 			return ZERO_SIZE_PTR;
 
 		return kmem_cache_alloc_node(s, flags, node);
-	} else
-		return __kmalloc_node(size, flags, node);
+	}
+	return __kmalloc_node(size, flags, node);
 }
 #endif
 

commit aa137f9d29d30592774c727ec5cfcf9891e576fa
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Aug 31 00:48:45 2007 -0700

    SLUB: Force inlining for functions in slub_def.h
    
    Some compilers (especially older gcc releases) may skip inlining
    sometimes which will lead to link failures.  Force the inlining of
    keyfunctions in slub_def.h to avoid these issues.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Jan Dittmer <jdi@l4x.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 124270df8734..74962077f632 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -78,7 +78,7 @@ extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
  * Sorry that the following has to be that ugly but some versions of GCC
  * have trouble with constant propagation and loops.
  */
-static inline int kmalloc_index(size_t size)
+static __always_inline int kmalloc_index(size_t size)
 {
 	if (!size)
 		return 0;
@@ -133,7 +133,7 @@ static inline int kmalloc_index(size_t size)
  * This ought to end up with a global pointer to the right cache
  * in kmalloc_caches.
  */
-static inline struct kmem_cache *kmalloc_slab(size_t size)
+static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
 {
 	int index = kmalloc_index(size);
 
@@ -166,7 +166,7 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 
-static inline void *kmalloc(size_t size, gfp_t flags)
+static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
 		struct kmem_cache *s = kmalloc_slab(size);
@@ -183,7 +183,7 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
-static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
 		struct kmem_cache *s = kmalloc_slab(size);

commit d046943cbaf332f75284ad99f4b3e60bae7ffff2
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Jul 20 16:18:06 2007 +0100

    fix gfp_t annotations for slub
    
            Since we have use like ~SLUB_DMA, we ought to have the type
    set right in both cases.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 07f7e4cbcee3..124270df8734 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -160,7 +160,7 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 #define SLUB_DMA __GFP_DMA
 #else
 /* Disable DMA functionality */
-#define SLUB_DMA 0
+#define SLUB_DMA (__force gfp_t)0
 #endif
 
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);

commit 81cda6626178cd55297831296ba8ecedbfd8b52d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:29 2007 -0700

    Slab allocators: Cleanup zeroing allocations
    
    It becomes now easy to support the zeroing allocs with generic inline
    functions in slab.h.  Provide inline definitions to allow the continued use of
    kzalloc, kmem_cache_zalloc etc but remove other definitions of zeroing
    functions from the slab allocators and util.c.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index bae11111458f..07f7e4cbcee3 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -179,19 +179,6 @@ static inline void *kmalloc(size_t size, gfp_t flags)
 		return __kmalloc(size, flags);
 }
 
-static inline void *kzalloc(size_t size, gfp_t flags)
-{
-	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
-		struct kmem_cache *s = kmalloc_slab(size);
-
-		if (!s)
-			return ZERO_SIZE_PTR;
-
-		return kmem_cache_zalloc(s, flags);
-	} else
-		return __kzalloc(size, flags);
-}
-
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node);
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);

commit 0c710013200e72b5e0bc680ff4ec6bdac53c5ce8
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:24 2007 -0700

    SLUB: add some more inlines and #ifdef CONFIG_SLUB_DEBUG
    
    Add #ifdefs around data structures only needed if debugging is compiled into
    SLUB.
    
    Add inlines to small functions to reduce code size.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 579b0a22858e..bae11111458f 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -16,7 +16,9 @@ struct kmem_cache_node {
 	unsigned long nr_partial;
 	atomic_long_t nr_slabs;
 	struct list_head partial;
+#ifdef CONFIG_SLUB_DEBUG
 	struct list_head full;
+#endif
 };
 
 /*
@@ -44,7 +46,9 @@ struct kmem_cache {
 	int align;		/* Alignment */
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
+#ifdef CONFIG_SLUB_DEBUG
 	struct kobject kobj;	/* For sysfs */
+#endif
 
 #ifdef CONFIG_NUMA
 	int defrag_ratio;

commit 6cb8f91320d3e720351c21741da795fed580b21b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:22 2007 -0700

    Slab allocators: consistent ZERO_SIZE_PTR support and NULL result semantics
    
    Define ZERO_OR_NULL_PTR macro to be able to remove the checks from the
    allocators.  Move ZERO_SIZE_PTR related stuff into slab.h.
    
    Make ZERO_SIZE_PTR work for all slab allocators and get rid of the
    WARN_ON_ONCE(size == 0) that is still remaining in SLAB.
    
    Make slub return NULL like the other allocators if a too large memory segment
    is requested via __kmalloc.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a582f6771525..579b0a22858e 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -159,18 +159,6 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 #define SLUB_DMA 0
 #endif
 
-
-/*
- * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
- *
- * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
- *
- * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
- * Both make kfree a no-op.
- */
-#define ZERO_SIZE_PTR ((void *)16)
-
-
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
 void *__kmalloc(size_t size, gfp_t flags);
 

commit 6193a2ff180920f84ee06977165ebf32431fc2d2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Sun Jul 15 23:38:22 2007 -0700

    slob: initial NUMA support
    
    This adds preliminary NUMA support to SLOB, primarily aimed at systems with
    small nodes (tested all the way down to a 128kB SRAM block), whether
    asymmetric or otherwise.
    
    We follow the same conventions as SLAB/SLUB, preferring current node
    placement for new pages, or with explicit placement, if a node has been
    specified.  Presently on UP NUMA this has the side-effect of preferring
    node#0 allocations (since numa_node_id() == 0, though this could be
    reworked if we could hand off a pfn to determine node placement), so
    single-CPU NUMA systems will want to place smaller nodes further out in
    terms of node id.  Once a page has been bound to a node (via explicit node
    id typing), we only do block allocations from partial free pages that have
    a matching node id in the page flags.
    
    The current implementation does have some scalability problems, in that all
    partial free pages are tracked in the global freelist (with contention due
    to the single spinlock).  However, these are things that are being reworked
    for SMP scalability first, while things like per-node freelists can easily
    be built on top of this sort of functionality once it's been added.
    
    More background can be found in:
    
            http://marc.info/?l=linux-mm&m=118117916022379&w=2
            http://marc.info/?l=linux-mm&m=118170446306199&w=2
            http://marc.info/?l=linux-mm&m=118187859420048&w=2
    
    and subsequent threads.
    
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 6207a3d8da71..a582f6771525 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -171,6 +171,9 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 #define ZERO_SIZE_PTR ((void *)16)
 
 
+void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
+void *__kmalloc(size_t size, gfp_t flags);
+
 static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
@@ -198,7 +201,8 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 }
 
 #ifdef CONFIG_NUMA
-extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
+void *__kmalloc_node(size_t size, gfp_t flags, int node);
+void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node);
 
 static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {

commit 4b356be019d0c28f67af02809df7072c1c8f7d32
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Jun 16 10:16:13 2007 -0700

    SLUB: minimum alignment fixes
    
    If ARCH_KMALLOC_MINALIGN is set to a value greater than 8 (SLUBs smallest
    kmalloc cache) then SLUB may generate duplicate slabs in sysfs (yes again)
    because the object size is padded to reach ARCH_KMALLOC_MINALIGN.  Thus the
    size of the small slabs is all the same.
    
    No arch sets ARCH_KMALLOC_MINALIGN larger than 8 though except mips which
    for some reason wants a 128 byte alignment.
    
    This patch increases the size of the smallest cache if
    ARCH_KMALLOC_MINALIGN is greater than 8.  In that case more and more of the
    smallest caches are disabled.
    
    If we do that then the count of the active general caches that is displayed
    on boot is not correct anymore since we may skip elements of the kmalloc
    array.  So count them separately.
    
    This approach was tested by Havard yesterday.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a0ad37463d62..6207a3d8da71 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -28,7 +28,7 @@ struct kmem_cache {
 	int size;		/* The size of an object including meta data */
 	int objsize;		/* The size of an object without meta data */
 	int offset;		/* Free pointer offset. */
-	unsigned int order;
+	int order;
 
 	/*
 	 * Avoid an extra cache line for UP, SMP and for the node local to
@@ -56,7 +56,13 @@ struct kmem_cache {
 /*
  * Kmalloc subsystem.
  */
-#define KMALLOC_SHIFT_LOW 3
+#if defined(ARCH_KMALLOC_MINALIGN) && ARCH_KMALLOC_MINALIGN > 8
+#define KMALLOC_MIN_SIZE ARCH_KMALLOC_MINALIGN
+#else
+#define KMALLOC_MIN_SIZE 8
+#endif
+
+#define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)
 
 /*
  * We keep the general caches in an array of slab caches that are used for
@@ -76,6 +82,9 @@ static inline int kmalloc_index(size_t size)
 	if (size > KMALLOC_MAX_SIZE)
 		return -1;
 
+	if (size <= KMALLOC_MIN_SIZE)
+		return KMALLOC_SHIFT_LOW;
+
 	if (size > 64 && size <= 96)
 		return 1;
 	if (size > 128 && size <= 192)

commit 272c1d21d6fe42979068e14c04fb60fb6045ad74
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 8 13:46:49 2007 -0700

    SLUB: return ZERO_SIZE_PTR for kmalloc(0)
    
    Instead of returning the smallest available object return ZERO_SIZE_PTR.
    
    A ZERO_SIZE_PTR can be legitimately used as an object pointer as long as it
    is not deferenced.  The dereference of ZERO_SIZE_PTR causes a distinctive
    fault.  kfree can handle a ZERO_SIZE_PTR in the same way as NULL.
    
    This enables functions to use zero sized object. e.g. n = number of objects.
    
            objects = kmalloc(n * sizeof(object));
    
            for (i = 0; i < n; i++)
                    objects[i].x = y;
    
            kfree(objects);
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 0764c829d967..a0ad37463d62 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -70,11 +70,8 @@ extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
  */
 static inline int kmalloc_index(size_t size)
 {
-	/*
-	 * We should return 0 if size == 0 but we use the smallest object
-	 * here for SLAB legacy reasons.
-	 */
-	WARN_ON_ONCE(size == 0);
+	if (!size)
+		return 0;
 
 	if (size > KMALLOC_MAX_SIZE)
 		return -1;
@@ -153,13 +150,25 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 #define SLUB_DMA 0
 #endif
 
+
+/*
+ * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
+ *
+ * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
+ *
+ * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
+ * Both make kfree a no-op.
+ */
+#define ZERO_SIZE_PTR ((void *)16)
+
+
 static inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
 		struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)
-			return NULL;
+			return ZERO_SIZE_PTR;
 
 		return kmem_cache_alloc(s, flags);
 	} else
@@ -172,7 +181,7 @@ static inline void *kzalloc(size_t size, gfp_t flags)
 		struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)
-			return NULL;
+			return ZERO_SIZE_PTR;
 
 		return kmem_cache_zalloc(s, flags);
 	} else
@@ -188,7 +197,7 @@ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 		struct kmem_cache *s = kmalloc_slab(size);
 
 		if (!s)
-			return NULL;
+			return ZERO_SIZE_PTR;
 
 		return kmem_cache_alloc_node(s, flags, node);
 	} else

commit 0aa817f078b655d0ae36669169d73a5c8a388016
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:11:01 2007 -0700

    Slab allocators: define common size limitations
    
    Currently we have a maze of configuration variables that determine the
    maximum slab size.  Worst of all it seems to vary between SLAB and SLUB.
    
    So define a common maximum size for kmalloc.  For conveniences sake we use
    the maximum size ever supported which is 32 MB.  We limit the maximum size
    to a lower limit if MAX_ORDER does not allow such large allocations.
    
    For many architectures this patch will have the effect of adding large
    kmalloc sizes.  x86_64 adds 5 new kmalloc sizes.  So a small amount of
    memory will be needed for these caches (contemporary SLAB has dynamically
    sizeable node and cpu structure so the waste is less than in the past)
    
    Most architectures will then be able to allocate object with sizes up to
    MAX_ORDER.  We have had repeated breakage (in fact whenever we doubled the
    number of supported processors) on IA64 because one or the other struct
    grew beyond what the slab allocators supported.  This will avoid future
    issues and f.e.  avoid fixes for 2k and 4k cpu support.
    
    CONFIG_LARGE_ALLOCS is no longer necessary so drop it.
    
    It fixes sparc64 with SLAB.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index a9fb92862aaa..0764c829d967 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -58,17 +58,6 @@ struct kmem_cache {
  */
 #define KMALLOC_SHIFT_LOW 3
 
-#ifdef CONFIG_LARGE_ALLOCS
-#define KMALLOC_SHIFT_HIGH ((MAX_ORDER + PAGE_SHIFT) =< 25 ? \
-				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
-#else
-#if !defined(CONFIG_MMU) || NR_CPUS > 512 || MAX_NUMNODES > 256
-#define KMALLOC_SHIFT_HIGH 20
-#else
-#define KMALLOC_SHIFT_HIGH 18
-#endif
-#endif
-
 /*
  * We keep the general caches in an array of slab caches that are used for
  * 2^x bytes of allocations.
@@ -79,7 +68,7 @@ extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
  * Sorry that the following has to be that ugly but some versions of GCC
  * have trouble with constant propagation and loops.
  */
-static inline int kmalloc_index(int size)
+static inline int kmalloc_index(size_t size)
 {
 	/*
 	 * We should return 0 if size == 0 but we use the smallest object
@@ -87,7 +76,7 @@ static inline int kmalloc_index(int size)
 	 */
 	WARN_ON_ONCE(size == 0);
 
-	if (size > (1 << KMALLOC_SHIFT_HIGH))
+	if (size > KMALLOC_MAX_SIZE)
 		return -1;
 
 	if (size > 64 && size <= 96)
@@ -110,17 +99,13 @@ static inline int kmalloc_index(int size)
 	if (size <=  64 * 1024) return 16;
 	if (size <= 128 * 1024) return 17;
 	if (size <= 256 * 1024) return 18;
-#if KMALLOC_SHIFT_HIGH > 18
 	if (size <=  512 * 1024) return 19;
 	if (size <= 1024 * 1024) return 20;
-#endif
-#if KMALLOC_SHIFT_HIGH > 20
 	if (size <=  2 * 1024 * 1024) return 21;
 	if (size <=  4 * 1024 * 1024) return 22;
 	if (size <=  8 * 1024 * 1024) return 23;
 	if (size <= 16 * 1024 * 1024) return 24;
 	if (size <= 32 * 1024 * 1024) return 25;
-#endif
 	return -1;
 
 /*

commit ade3aff25fb2dce76e2a9b53e1334bd0a174f739
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed May 16 22:10:54 2007 -0700

    slub: fix handling of oversized slabs
    
    I'm getting zillions of undefined references to __kmalloc_size_too_large on
    alpha.  For some reason alpha is building out-of-line copies of kmalloc_slab()
    into lots of compilation units.
    
    It turns out that gcc just isn't smart enough to work out that
    __builtin_contant_p(size)==true implies that __builtin_contant_p(index)==true.
    
    So let's give it a bit of help.
    
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 5e2e7297dfaa..a9fb92862aaa 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -145,7 +145,12 @@ static inline struct kmem_cache *kmalloc_slab(size_t size)
 	if (index == 0)
 		return NULL;
 
-	if (index < 0) {
+	/*
+	 * This function only gets expanded if __builtin_constant_p(size), so
+	 * testing it here shouldn't be needed.  But some versions of gcc need
+	 * help.
+	 */
+	if (__builtin_constant_p(size) && index < 0) {
 		/*
 		 * Generate a link failure. Would be great if we could
 		 * do something to stop the compile here.

commit c59def9f222d44bb7e2f0a559f2906191a0862d7
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:10:50 2007 -0700

    Slab allocators: Drop support for destructors
    
    There is no user of destructors left.  There is no reason why we should keep
    checking for destructors calls in the slab allocators.
    
    The RFC for this patch was discussed at
    http://marc.info/?l=linux-kernel&m=117882364330705&w=2
    
    Destructors were mainly used for list management which required them to take a
    spinlock.  Taking a spinlock in a destructor is a bit risky since the slab
    allocators may run the destructors anytime they decide a slab is no longer
    needed.
    
    Patch drops destructor support.  Any attempt to use a destructor will BUG().
    
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index c6c1f4a120e3..5e2e7297dfaa 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -40,7 +40,6 @@ struct kmem_cache {
 	int objects;		/* Number of objects in slab */
 	int refcount;		/* Refcount for slab cache destroy */
 	void (*ctor)(void *, struct kmem_cache *, unsigned long);
-	void (*dtor)(void *, struct kmem_cache *, unsigned long);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
 	const char *name;	/* Name (only for display!) */

commit 1abd727ed7abf5c19e7d1760671475cbecbccb0e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue May 15 23:57:03 2007 -0700

    SLUB: It is legit to allocate a slab of the maximum permitted size
    
    Sorry I screwed up the comparison. It is only an error if we attempt
    to allocate a slab larger than the maximum allowed size.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index fd6627e2d115..c6c1f4a120e3 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -88,7 +88,7 @@ static inline int kmalloc_index(int size)
 	 */
 	WARN_ON_ONCE(size == 0);
 
-	if (size >= (1 << KMALLOC_SHIFT_HIGH))
+	if (size > (1 << KMALLOC_SHIFT_HIGH))
 		return -1;
 
 	if (size > 64 && size <= 96)

commit cfbf07f2a80b618c42a42c20d83647ea8fcceca0
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue May 15 01:42:06 2007 -0700

    SLUB: CONFIG_LARGE_ALLOCS must consider MAX_ORDER limit
    
    Take MAX_ORDER into consideration when determining KMALLOC_SHIFT_HIGH.
    Otherwise we may run into a situation where we attempt to create general
    slabs larger than MAX_ORDER.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index ea27065e80e6..fd6627e2d115 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -60,7 +60,8 @@ struct kmem_cache {
 #define KMALLOC_SHIFT_LOW 3
 
 #ifdef CONFIG_LARGE_ALLOCS
-#define KMALLOC_SHIFT_HIGH 25
+#define KMALLOC_SHIFT_HIGH ((MAX_ORDER + PAGE_SHIFT) =< 25 ? \
+				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
 #else
 #if !defined(CONFIG_MMU) || NR_CPUS > 512 || MAX_NUMNODES > 256
 #define KMALLOC_SHIFT_HIGH 20
@@ -87,6 +88,9 @@ static inline int kmalloc_index(int size)
 	 */
 	WARN_ON_ONCE(size == 0);
 
+	if (size >= (1 << KMALLOC_SHIFT_HIGH))
+		return -1;
+
 	if (size > 64 && size <= 96)
 		return 1;
 	if (size > 128 && size <= 192)

commit 643b113849d8faa68c9f01c3c9d929bfbffd50bd
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:42 2007 -0700

    slub: enable tracking of full slabs
    
    If slab tracking is on then build a list of full slabs so that we can verify
    the integrity of all slabs and are also able to built list of alloc/free
    callers.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index f8e0c86c48a9..ea27065e80e6 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -16,6 +16,7 @@ struct kmem_cache_node {
 	unsigned long nr_partial;
 	atomic_long_t nr_slabs;
 	struct list_head partial;
+	struct list_head full;
 };
 
 /*

commit 614410d5892af5f86d0ec14e28f9f6d5f4ac9e9b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:38 2007 -0700

    SLUB: allocate smallest object size if the user asks for 0 bytes
    
    Makes SLUB behave like SLAB in this area to avoid issues....
    
    Throw a stack dump to alert people.
    
    At some point the behavior should be switched back.  NULL is no memory as
    far as I can tell and if the use asked for 0 bytes then he need to get no
    memory.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index 30b154ce7289..f8e0c86c48a9 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -80,8 +80,12 @@ extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
  */
 static inline int kmalloc_index(int size)
 {
-	if (size == 0)
-		return 0;
+	/*
+	 * We should return 0 if size == 0 but we use the smallest object
+	 * here for SLAB legacy reasons.
+	 */
+	WARN_ON_ONCE(size == 0);
+
 	if (size > 64 && size <= 96)
 		return 1;
 	if (size > 128 && size <= 192)

commit 81819f0fc8285a2a5a921c019e3e3d7b6169d225
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:49:36 2007 -0700

    SLUB core
    
    This is a new slab allocator which was motivated by the complexity of the
    existing code in mm/slab.c. It attempts to address a variety of concerns
    with the existing implementation.
    
    A. Management of object queues
    
       A particular concern was the complex management of the numerous object
       queues in SLAB. SLUB has no such queues. Instead we dedicate a slab for
       each allocating CPU and use objects from a slab directly instead of
       queueing them up.
    
    B. Storage overhead of object queues
    
       SLAB Object queues exist per node, per CPU. The alien cache queue even
       has a queue array that contain a queue for each processor on each
       node. For very large systems the number of queues and the number of
       objects that may be caught in those queues grows exponentially. On our
       systems with 1k nodes / processors we have several gigabytes just tied up
       for storing references to objects for those queues  This does not include
       the objects that could be on those queues. One fears that the whole
       memory of the machine could one day be consumed by those queues.
    
    C. SLAB meta data overhead
    
       SLAB has overhead at the beginning of each slab. This means that data
       cannot be naturally aligned at the beginning of a slab block. SLUB keeps
       all meta data in the corresponding page_struct. Objects can be naturally
       aligned in the slab. F.e. a 128 byte object will be aligned at 128 byte
       boundaries and can fit tightly into a 4k page with no bytes left over.
       SLAB cannot do this.
    
    D. SLAB has a complex cache reaper
    
       SLUB does not need a cache reaper for UP systems. On SMP systems
       the per CPU slab may be pushed back into partial list but that
       operation is simple and does not require an iteration over a list
       of objects. SLAB expires per CPU, shared and alien object queues
       during cache reaping which may cause strange hold offs.
    
    E. SLAB has complex NUMA policy layer support
    
       SLUB pushes NUMA policy handling into the page allocator. This means that
       allocation is coarser (SLUB does interleave on a page level) but that
       situation was also present before 2.6.13. SLABs application of
       policies to individual slab objects allocated in SLAB is
       certainly a performance concern due to the frequent references to
       memory policies which may lead a sequence of objects to come from
       one node after another. SLUB will get a slab full of objects
       from one node and then will switch to the next.
    
    F. Reduction of the size of partial slab lists
    
       SLAB has per node partial lists. This means that over time a large
       number of partial slabs may accumulate on those lists. These can
       only be reused if allocator occur on specific nodes. SLUB has a global
       pool of partial slabs and will consume slabs from that pool to
       decrease fragmentation.
    
    G. Tunables
    
       SLAB has sophisticated tuning abilities for each slab cache. One can
       manipulate the queue sizes in detail. However, filling the queues still
       requires the uses of the spin lock to check out slabs. SLUB has a global
       parameter (min_slab_order) for tuning. Increasing the minimum slab
       order can decrease the locking overhead. The bigger the slab order the
       less motions of pages between per CPU and partial lists occur and the
       better SLUB will be scaling.
    
    G. Slab merging
    
       We often have slab caches with similar parameters. SLUB detects those
       on boot up and merges them into the corresponding general caches. This
       leads to more effective memory use. About 50% of all caches can
       be eliminated through slab merging. This will also decrease
       slab fragmentation because partial allocated slabs can be filled
       up again. Slab merging can be switched off by specifying
       slub_nomerge on boot up.
    
       Note that merging can expose heretofore unknown bugs in the kernel
       because corrupted objects may now be placed differently and corrupt
       differing neighboring objects. Enable sanity checks to find those.
    
    H. Diagnostics
    
       The current slab diagnostics are difficult to use and require a
       recompilation of the kernel. SLUB contains debugging code that
       is always available (but is kept out of the hot code paths).
       SLUB diagnostics can be enabled via the "slab_debug" option.
       Parameters can be specified to select a single or a group of
       slab caches for diagnostics. This means that the system is running
       with the usual performance and it is much more likely that
       race conditions can be reproduced.
    
    I. Resiliency
    
       If basic sanity checks are on then SLUB is capable of detecting
       common error conditions and recover as best as possible to allow the
       system to continue.
    
    J. Tracing
    
       Tracing can be enabled via the slab_debug=T,<slabcache> option
       during boot. SLUB will then protocol all actions on that slabcache
       and dump the object contents on free.
    
    K. On demand DMA cache creation.
    
       Generally DMA caches are not needed. If a kmalloc is used with
       __GFP_DMA then just create this single slabcache that is needed.
       For systems that have no ZONE_DMA requirement the support is
       completely eliminated.
    
    L. Performance increase
    
       Some benchmarks have shown speed improvements on kernbench in the
       range of 5-10%. The locking overhead of slub is based on the
       underlying base allocation size. If we can reliably allocate
       larger order pages then it is possible to increase slub
       performance much further. The anti-fragmentation patches may
       enable further performance increases.
    
    Tested on:
    i386 UP + SMP, x86_64 UP + SMP + NUMA emulation, IA64 NUMA + Simulator
    
    SLUB Boot options
    
    slub_nomerge            Disable merging of slabs
    slub_min_order=x        Require a minimum order for slab caches. This
                            increases the managed chunk size and therefore
                            reduces meta data and locking overhead.
    slub_min_objects=x      Mininum objects per slab. Default is 8.
    slub_max_order=x        Avoid generating slabs larger than order specified.
    slub_debug              Enable all diagnostics for all caches
    slub_debug=<options>    Enable selective options for all caches
    slub_debug=<o>,<cache>  Enable selective options for a certain set of
                            caches
    
    Available Debug options
    F               Double Free checking, sanity and resiliency
    R               Red zoning
    P               Object / padding poisoning
    U               Track last free / alloc
    T               Trace all allocs / frees (only use for individual slabs).
    
    To use SLUB: Apply this patch and then select SLUB as the default slab
    allocator.
    
    [hugh@veritas.com: fix an oops-causing locking error]
    [akpm@linux-foundation.org: various stupid cleanups and small fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
new file mode 100644
index 000000000000..30b154ce7289
--- /dev/null
+++ b/include/linux/slub_def.h
@@ -0,0 +1,201 @@
+#ifndef _LINUX_SLUB_DEF_H
+#define _LINUX_SLUB_DEF_H
+
+/*
+ * SLUB : A Slab allocator without object queues.
+ *
+ * (C) 2007 SGI, Christoph Lameter <clameter@sgi.com>
+ */
+#include <linux/types.h>
+#include <linux/gfp.h>
+#include <linux/workqueue.h>
+#include <linux/kobject.h>
+
+struct kmem_cache_node {
+	spinlock_t list_lock;	/* Protect partial list and nr_partial */
+	unsigned long nr_partial;
+	atomic_long_t nr_slabs;
+	struct list_head partial;
+};
+
+/*
+ * Slab cache management.
+ */
+struct kmem_cache {
+	/* Used for retriving partial slabs etc */
+	unsigned long flags;
+	int size;		/* The size of an object including meta data */
+	int objsize;		/* The size of an object without meta data */
+	int offset;		/* Free pointer offset. */
+	unsigned int order;
+
+	/*
+	 * Avoid an extra cache line for UP, SMP and for the node local to
+	 * struct kmem_cache.
+	 */
+	struct kmem_cache_node local_node;
+
+	/* Allocation and freeing of slabs */
+	int objects;		/* Number of objects in slab */
+	int refcount;		/* Refcount for slab cache destroy */
+	void (*ctor)(void *, struct kmem_cache *, unsigned long);
+	void (*dtor)(void *, struct kmem_cache *, unsigned long);
+	int inuse;		/* Offset to metadata */
+	int align;		/* Alignment */
+	const char *name;	/* Name (only for display!) */
+	struct list_head list;	/* List of slab caches */
+	struct kobject kobj;	/* For sysfs */
+
+#ifdef CONFIG_NUMA
+	int defrag_ratio;
+	struct kmem_cache_node *node[MAX_NUMNODES];
+#endif
+	struct page *cpu_slab[NR_CPUS];
+};
+
+/*
+ * Kmalloc subsystem.
+ */
+#define KMALLOC_SHIFT_LOW 3
+
+#ifdef CONFIG_LARGE_ALLOCS
+#define KMALLOC_SHIFT_HIGH 25
+#else
+#if !defined(CONFIG_MMU) || NR_CPUS > 512 || MAX_NUMNODES > 256
+#define KMALLOC_SHIFT_HIGH 20
+#else
+#define KMALLOC_SHIFT_HIGH 18
+#endif
+#endif
+
+/*
+ * We keep the general caches in an array of slab caches that are used for
+ * 2^x bytes of allocations.
+ */
+extern struct kmem_cache kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];
+
+/*
+ * Sorry that the following has to be that ugly but some versions of GCC
+ * have trouble with constant propagation and loops.
+ */
+static inline int kmalloc_index(int size)
+{
+	if (size == 0)
+		return 0;
+	if (size > 64 && size <= 96)
+		return 1;
+	if (size > 128 && size <= 192)
+		return 2;
+	if (size <=          8) return 3;
+	if (size <=         16) return 4;
+	if (size <=         32) return 5;
+	if (size <=         64) return 6;
+	if (size <=        128) return 7;
+	if (size <=        256) return 8;
+	if (size <=        512) return 9;
+	if (size <=       1024) return 10;
+	if (size <=   2 * 1024) return 11;
+	if (size <=   4 * 1024) return 12;
+	if (size <=   8 * 1024) return 13;
+	if (size <=  16 * 1024) return 14;
+	if (size <=  32 * 1024) return 15;
+	if (size <=  64 * 1024) return 16;
+	if (size <= 128 * 1024) return 17;
+	if (size <= 256 * 1024) return 18;
+#if KMALLOC_SHIFT_HIGH > 18
+	if (size <=  512 * 1024) return 19;
+	if (size <= 1024 * 1024) return 20;
+#endif
+#if KMALLOC_SHIFT_HIGH > 20
+	if (size <=  2 * 1024 * 1024) return 21;
+	if (size <=  4 * 1024 * 1024) return 22;
+	if (size <=  8 * 1024 * 1024) return 23;
+	if (size <= 16 * 1024 * 1024) return 24;
+	if (size <= 32 * 1024 * 1024) return 25;
+#endif
+	return -1;
+
+/*
+ * What we really wanted to do and cannot do because of compiler issues is:
+ *	int i;
+ *	for (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++)
+ *		if (size <= (1 << i))
+ *			return i;
+ */
+}
+
+/*
+ * Find the slab cache for a given combination of allocation flags and size.
+ *
+ * This ought to end up with a global pointer to the right cache
+ * in kmalloc_caches.
+ */
+static inline struct kmem_cache *kmalloc_slab(size_t size)
+{
+	int index = kmalloc_index(size);
+
+	if (index == 0)
+		return NULL;
+
+	if (index < 0) {
+		/*
+		 * Generate a link failure. Would be great if we could
+		 * do something to stop the compile here.
+		 */
+		extern void __kmalloc_size_too_large(void);
+		__kmalloc_size_too_large();
+	}
+	return &kmalloc_caches[index];
+}
+
+#ifdef CONFIG_ZONE_DMA
+#define SLUB_DMA __GFP_DMA
+#else
+/* Disable DMA functionality */
+#define SLUB_DMA 0
+#endif
+
+static inline void *kmalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
+		struct kmem_cache *s = kmalloc_slab(size);
+
+		if (!s)
+			return NULL;
+
+		return kmem_cache_alloc(s, flags);
+	} else
+		return __kmalloc(size, flags);
+}
+
+static inline void *kzalloc(size_t size, gfp_t flags)
+{
+	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
+		struct kmem_cache *s = kmalloc_slab(size);
+
+		if (!s)
+			return NULL;
+
+		return kmem_cache_zalloc(s, flags);
+	} else
+		return __kzalloc(size, flags);
+}
+
+#ifdef CONFIG_NUMA
+extern void *__kmalloc_node(size_t size, gfp_t flags, int node);
+
+static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
+		struct kmem_cache *s = kmalloc_slab(size);
+
+		if (!s)
+			return NULL;
+
+		return kmem_cache_alloc_node(s, flags, node);
+	} else
+		return __kmalloc_node(size, flags, node);
+}
+#endif
+
+#endif /* _LINUX_SLUB_DEF_H */
