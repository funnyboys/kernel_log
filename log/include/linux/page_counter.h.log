commit d1663a907bd348f912b7f7088e83ca1b6fd3309f
Author: Jakub Kicinski <kuba@kernel.org>
Date:   Mon Jun 1 21:49:49 2020 -0700

    mm/memcg: move cgroup high memory limit setting into struct page_counter
    
    High memory limit is currently recorded directly in struct mem_cgroup.
    We are about to add a high limit for swap, move the field to struct
    page_counter and add some helpers.
    
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Chris Down <chris@chrisdown.name>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/20200527195846.102707-4-kuba@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index bab7e57f659b..85bd413e784e 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -10,6 +10,7 @@ struct page_counter {
 	atomic_long_t usage;
 	unsigned long min;
 	unsigned long low;
+	unsigned long high;
 	unsigned long max;
 	struct page_counter *parent;
 
@@ -55,6 +56,13 @@ bool page_counter_try_charge(struct page_counter *counter,
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
 void page_counter_set_min(struct page_counter *counter, unsigned long nr_pages);
 void page_counter_set_low(struct page_counter *counter, unsigned long nr_pages);
+
+static inline void page_counter_set_high(struct page_counter *counter,
+					 unsigned long nr_pages)
+{
+	WRITE_ONCE(counter->high, nr_pages);
+}
+
 int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_memparse(const char *buf, const char *max,
 			  unsigned long *nr_pages);

commit bf8d5d52ffe89aac5b46ddb39dd1a4351fae5df4
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jun 7 17:07:46 2018 -0700

    memcg: introduce memory.min
    
    Memory controller implements the memory.low best-effort memory
    protection mechanism, which works perfectly in many cases and allows
    protecting working sets of important workloads from sudden reclaim.
    
    But its semantics has a significant limitation: it works only as long as
    there is a supply of reclaimable memory.  This makes it pretty useless
    against any sort of slow memory leaks or memory usage increases.  This
    is especially true for swapless systems.  If swap is enabled, memory
    soft protection effectively postpones problems, allowing a leaking
    application to fill all swap area, which makes no sense.  The only
    effective way to guarantee the memory protection in this case is to
    invoke the OOM killer.
    
    It's possible to handle this case in userspace by reacting on MEMCG_LOW
    events; but there is still a place for a fail-safe in-kernel mechanism
    to provide stronger guarantees.
    
    This patch introduces the memory.min interface for cgroup v2 memory
    controller.  It works very similarly to memory.low (sharing the same
    hierarchical behavior), except that it's not disabled if there is no
    more reclaimable memory in the system.
    
    If cgroup is not populated, its memory.min is ignored, because otherwise
    even the OOM killer wouldn't be able to reclaim the protected memory,
    and the system can stall.
    
    [guro@fb.com: s/low/min/ in docs]
    Link: http://lkml.kernel.org/r/20180510130758.GA9129@castle.DHCP.thefacebook.com
    Link: http://lkml.kernel.org/r/20180509180734.GA4856@castle.DHCP.thefacebook.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 7902a727d3b6..bab7e57f659b 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -8,10 +8,16 @@
 
 struct page_counter {
 	atomic_long_t usage;
-	unsigned long max;
+	unsigned long min;
 	unsigned long low;
+	unsigned long max;
 	struct page_counter *parent;
 
+	/* effective memory.min and memory.min usage tracking */
+	unsigned long emin;
+	atomic_long_t min_usage;
+	atomic_long_t children_min_usage;
+
 	/* effective memory.low and memory.low usage tracking */
 	unsigned long elow;
 	atomic_long_t low_usage;
@@ -47,8 +53,9 @@ bool page_counter_try_charge(struct page_counter *counter,
 			     unsigned long nr_pages,
 			     struct page_counter **fail);
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
-int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages);
+void page_counter_set_min(struct page_counter *counter, unsigned long nr_pages);
 void page_counter_set_low(struct page_counter *counter, unsigned long nr_pages);
+int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_memparse(const char *buf, const char *max,
 			  unsigned long *nr_pages);
 

commit 230671533d64631116be3ff9d407bd9ca5a58e1b
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jun 7 17:06:22 2018 -0700

    mm: memory.low hierarchical behavior
    
    This patch aims to address an issue in current memory.low semantics,
    which makes it hard to use it in a hierarchy, where some leaf memory
    cgroups are more valuable than others.
    
    For example, there are memcgs A, A/B, A/C, A/D and A/E:
    
      A      A/memory.low = 2G, A/memory.current = 6G
     //\\
    BC  DE   B/memory.low = 3G  B/memory.current = 2G
             C/memory.low = 1G  C/memory.current = 2G
             D/memory.low = 0   D/memory.current = 2G
             E/memory.low = 10G E/memory.current = 0
    
    If we apply memory pressure, B, C and D are reclaimed at the same pace
    while A's usage exceeds 2G.  This is obviously wrong, as B's usage is
    fully below B's memory.low, and C has 1G of protection as well.  Also, A
    is pushed to the size, which is less than A's 2G memory.low, which is
    also wrong.
    
    A simple bash script (provided below) can be used to reproduce
    the problem. Current results are:
      A:    1430097920
      A/B:  711929856
      A/C:  717426688
      A/D:  741376
      A/E:  0
    
    To address the issue a concept of effective memory.low is introduced.
    Effective memory.low is always equal or less than original memory.low.
    In a case, when there is no memory.low overcommittment (and also for
    top-level cgroups), these two values are equal.
    
    Otherwise it's a part of parent's effective memory.low, calculated as a
    cgroup's memory.low usage divided by sum of sibling's memory.low usages
    (under memory.low usage I mean the size of actually protected memory:
    memory.current if memory.current < memory.low, 0 otherwise).  It's
    necessary to track the actual usage, because otherwise an empty cgroup
    with memory.low set (A/E in my example) will affect actual memory
    distribution, which makes no sense.  To avoid traversing the cgroup tree
    twice, page_counters code is reused.
    
    Calculating effective memory.low can be done in the reclaim path, as we
    conveniently traversing the cgroup tree from top to bottom and check
    memory.low on each level.  So, it's a perfect place to calculate
    effective memory low and save it to use it for children cgroups.
    
    This also eliminates a need to traverse the cgroup tree from bottom to
    top each time to check if parent's guarantee is not exceeded.
    
    Setting/resetting effective memory.low is intentionally racy, but it's
    fine and shouldn't lead to any significant differences in actual memory
    distribution.
    
    With this patch applied results are matching the expectations:
      A:    2147930112
      A/B:  1428721664
      A/C:  718393344
      A/D:  815104
      A/E:  0
    
    Test script:
      #!/bin/bash
    
      CGPATH="/sys/fs/cgroup"
    
      truncate /file1 --size 2G
      truncate /file2 --size 2G
      truncate /file3 --size 2G
      truncate /file4 --size 50G
    
      mkdir "${CGPATH}/A"
      echo "+memory" > "${CGPATH}/A/cgroup.subtree_control"
      mkdir "${CGPATH}/A/B" "${CGPATH}/A/C" "${CGPATH}/A/D" "${CGPATH}/A/E"
    
      echo 2G > "${CGPATH}/A/memory.low"
      echo 3G > "${CGPATH}/A/B/memory.low"
      echo 1G > "${CGPATH}/A/C/memory.low"
      echo 0 > "${CGPATH}/A/D/memory.low"
      echo 10G > "${CGPATH}/A/E/memory.low"
    
      echo $$ > "${CGPATH}/A/B/cgroup.procs" && vmtouch -qt /file1
      echo $$ > "${CGPATH}/A/C/cgroup.procs" && vmtouch -qt /file2
      echo $$ > "${CGPATH}/A/D/cgroup.procs" && vmtouch -qt /file3
      echo $$ > "${CGPATH}/cgroup.procs" && vmtouch -qt /file4
    
      echo "A:   " `cat "${CGPATH}/A/memory.current"`
      echo "A/B: " `cat "${CGPATH}/A/B/memory.current"`
      echo "A/C: " `cat "${CGPATH}/A/C/memory.current"`
      echo "A/D: " `cat "${CGPATH}/A/D/memory.current"`
      echo "A/E: " `cat "${CGPATH}/A/E/memory.current"`
    
      rmdir "${CGPATH}/A/B" "${CGPATH}/A/C" "${CGPATH}/A/D" "${CGPATH}/A/E"
      rmdir "${CGPATH}/A"
      rm /file1 /file2 /file3 /file4
    
    Link: http://lkml.kernel.org/r/20180405185921.4942-2-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 94029dad9317..7902a727d3b6 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -9,8 +9,14 @@
 struct page_counter {
 	atomic_long_t usage;
 	unsigned long max;
+	unsigned long low;
 	struct page_counter *parent;
 
+	/* effective memory.low and memory.low usage tracking */
+	unsigned long elow;
+	atomic_long_t low_usage;
+	atomic_long_t children_low_usage;
+
 	/* legacy */
 	unsigned long watermark;
 	unsigned long failcnt;
@@ -42,6 +48,7 @@ bool page_counter_try_charge(struct page_counter *counter,
 			     struct page_counter **fail);
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages);
+void page_counter_set_low(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_memparse(const char *buf, const char *max,
 			  unsigned long *nr_pages);
 

commit bbec2e15170aae3e084d7d9afc730aeebe01b654
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jun 7 17:06:18 2018 -0700

    mm: rename page_counter's count/limit into usage/max
    
    This patch renames struct page_counter fields:
      count -> usage
      limit -> max
    
    and the corresponding functions:
      page_counter_limit() -> page_counter_set_max()
      mem_cgroup_get_limit() -> mem_cgroup_get_max()
      mem_cgroup_resize_limit() -> mem_cgroup_resize_max()
      memcg_update_kmem_limit() -> memcg_update_kmem_max()
      memcg_update_tcp_limit() -> memcg_update_tcp_max()
    
    The idea behind this renaming is to have the direct matching
    between memory cgroup knobs (low, high, max) and page_counters API.
    
    This is pure renaming, this patch doesn't bring any functional change.
    
    Link: http://lkml.kernel.org/r/20180405185921.4942-1-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index c15ab80ad32d..94029dad9317 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -7,8 +7,8 @@
 #include <asm/page.h>
 
 struct page_counter {
-	atomic_long_t count;
-	unsigned long limit;
+	atomic_long_t usage;
+	unsigned long max;
 	struct page_counter *parent;
 
 	/* legacy */
@@ -25,14 +25,14 @@ struct page_counter {
 static inline void page_counter_init(struct page_counter *counter,
 				     struct page_counter *parent)
 {
-	atomic_long_set(&counter->count, 0);
-	counter->limit = PAGE_COUNTER_MAX;
+	atomic_long_set(&counter->usage, 0);
+	counter->max = PAGE_COUNTER_MAX;
 	counter->parent = parent;
 }
 
 static inline unsigned long page_counter_read(struct page_counter *counter)
 {
-	return atomic_long_read(&counter->count);
+	return atomic_long_read(&counter->usage);
 }
 
 void page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
@@ -41,7 +41,7 @@ bool page_counter_try_charge(struct page_counter *counter,
 			     unsigned long nr_pages,
 			     struct page_counter **fail);
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
-int page_counter_limit(struct page_counter *counter, unsigned long limit);
+int page_counter_set_max(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_memparse(const char *buf, const char *max,
 			  unsigned long *nr_pages);
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 7e62920a3a94..c15ab80ad32d 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_PAGE_COUNTER_H
 #define _LINUX_PAGE_COUNTER_H
 

commit 6071ca5201066f4b2a61cfb693dd186d6bc6e9f3
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Nov 5 18:50:26 2015 -0800

    mm: page_counter: let page_counter_try_charge() return bool
    
    page_counter_try_charge() currently returns 0 on success and -ENOMEM on
    failure, which is surprising behavior given the function name.
    
    Make it follow the expected pattern of try_stuff() functions that return a
    boolean true to indicate success, or false for failure.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 17fa4f8de3a6..7e62920a3a94 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -36,9 +36,9 @@ static inline unsigned long page_counter_read(struct page_counter *counter)
 
 void page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
 void page_counter_charge(struct page_counter *counter, unsigned long nr_pages);
-int page_counter_try_charge(struct page_counter *counter,
-			    unsigned long nr_pages,
-			    struct page_counter **fail);
+bool page_counter_try_charge(struct page_counter *counter,
+			     unsigned long nr_pages,
+			     struct page_counter **fail);
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_limit(struct page_counter *counter, unsigned long limit);
 int page_counter_memparse(const char *buf, const char *max,

commit 650c5e565492f9092552bfe4d65935196c7d9567
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Feb 11 15:26:03 2015 -0800

    mm: page_counter: pull "-1" handling out of page_counter_memparse()
    
    The unified hierarchy interface for memory cgroups will no longer use "-1"
    to mean maximum possible resource value.  In preparation for this, make
    the string an argument and let the caller supply it.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 955421575d16..17fa4f8de3a6 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -41,7 +41,8 @@ int page_counter_try_charge(struct page_counter *counter,
 			    struct page_counter **fail);
 void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_limit(struct page_counter *counter, unsigned long limit);
-int page_counter_memparse(const char *buf, unsigned long *nr_pages);
+int page_counter_memparse(const char *buf, const char *max,
+			  unsigned long *nr_pages);
 
 static inline void page_counter_reset_watermark(struct page_counter *counter)
 {

commit 64f2199389414341ed3a570663f23616c131ba25
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Dec 10 15:42:45 2014 -0800

    mm: memcontrol: remove obsolete kmemcg pinning tricks
    
    As charges now pin the css explicitely, there is no more need for kmemcg
    to acquire a proxy reference for outstanding pages during offlining, or
    maintain state to identify such "dead" groups.
    
    This was the last user of the uncharge functions' return values, so remove
    them as well.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
index 7cce3be99ff3..955421575d16 100644
--- a/include/linux/page_counter.h
+++ b/include/linux/page_counter.h
@@ -34,12 +34,12 @@ static inline unsigned long page_counter_read(struct page_counter *counter)
 	return atomic_long_read(&counter->count);
 }
 
-int page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
+void page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
 void page_counter_charge(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_try_charge(struct page_counter *counter,
 			    unsigned long nr_pages,
 			    struct page_counter **fail);
-int page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
+void page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
 int page_counter_limit(struct page_counter *counter, unsigned long limit);
 int page_counter_memparse(const char *buf, unsigned long *nr_pages);
 

commit 3e32cb2e0a12b6915056ff04601cf1bb9b44f967
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Dec 10 15:42:31 2014 -0800

    mm: memcontrol: lockless page counters
    
    Memory is internally accounted in bytes, using spinlock-protected 64-bit
    counters, even though the smallest accounting delta is a page.  The
    counter interface is also convoluted and does too many things.
    
    Introduce a new lockless word-sized page counter API, then change all
    memory accounting over to it.  The translation from and to bytes then only
    happens when interfacing with userspace.
    
    The removed locking overhead is noticable when scaling beyond the per-cpu
    charge caches - on a 4-socket machine with 144-threads, the following test
    shows the performance differences of 288 memcgs concurrently running a
    page fault benchmark:
    
    vanilla:
    
       18631648.500498      task-clock (msec)         #  140.643 CPUs utilized            ( +-  0.33% )
             1,380,638      context-switches          #    0.074 K/sec                    ( +-  0.75% )
                24,390      cpu-migrations            #    0.001 K/sec                    ( +-  8.44% )
         1,843,305,768      page-faults               #    0.099 M/sec                    ( +-  0.00% )
    50,134,994,088,218      cycles                    #    2.691 GHz                      ( +-  0.33% )
       <not supported>      stalled-cycles-frontend
       <not supported>      stalled-cycles-backend
     8,049,712,224,651      instructions              #    0.16  insns per cycle          ( +-  0.04% )
     1,586,970,584,979      branches                  #   85.176 M/sec                    ( +-  0.05% )
         1,724,989,949      branch-misses             #    0.11% of all branches          ( +-  0.48% )
    
         132.474343877 seconds time elapsed                                          ( +-  0.21% )
    
    lockless:
    
       12195979.037525      task-clock (msec)         #  133.480 CPUs utilized            ( +-  0.18% )
               832,850      context-switches          #    0.068 K/sec                    ( +-  0.54% )
                15,624      cpu-migrations            #    0.001 K/sec                    ( +- 10.17% )
         1,843,304,774      page-faults               #    0.151 M/sec                    ( +-  0.00% )
    32,811,216,801,141      cycles                    #    2.690 GHz                      ( +-  0.18% )
       <not supported>      stalled-cycles-frontend
       <not supported>      stalled-cycles-backend
     9,999,265,091,727      instructions              #    0.30  insns per cycle          ( +-  0.10% )
     2,076,759,325,203      branches                  #  170.282 M/sec                    ( +-  0.12% )
         1,656,917,214      branch-misses             #    0.08% of all branches          ( +-  0.55% )
    
          91.369330729 seconds time elapsed                                          ( +-  0.45% )
    
    On top of improved scalability, this also gets rid of the icky long long
    types in the very heart of memcg, which is great for 32 bit and also makes
    the code a lot more readable.
    
    Notable differences between the old and new API:
    
    - res_counter_charge() and res_counter_charge_nofail() become
      page_counter_try_charge() and page_counter_charge() resp. to match
      the more common kernel naming scheme of try_do()/do()
    
    - res_counter_uncharge_until() is only ever used to cancel a local
      counter and never to uncharge bigger segments of a hierarchy, so
      it's replaced by the simpler page_counter_cancel()
    
    - res_counter_set_limit() is replaced by page_counter_limit(), which
      expects its callers to serialize against themselves
    
    - res_counter_memparse_write_strategy() is replaced by
      page_counter_limit(), which rounds down to the nearest page size -
      rather than up.  This is more reasonable for explicitely requested
      hard upper limits.
    
    - to keep charging light-weight, page_counter_try_charge() charges
      speculatively, only to roll back if the result exceeds the limit.
      Because of this, a failing bigger charge can temporarily lock out
      smaller charges that would otherwise succeed.  The error is bounded
      to the difference between the smallest and the biggest possible
      charge size, so for memcg, this means that a failing THP charge can
      send base page charges into reclaim upto 2MB (4MB) before the limit
      would have been reached.  This should be acceptable.
    
    [akpm@linux-foundation.org: add includes for WARN_ON_ONCE and memparse]
    [akpm@linux-foundation.org: add includes for WARN_ON_ONCE, memparse, strncmp, and PAGE_SIZE]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/page_counter.h b/include/linux/page_counter.h
new file mode 100644
index 000000000000..7cce3be99ff3
--- /dev/null
+++ b/include/linux/page_counter.h
@@ -0,0 +1,51 @@
+#ifndef _LINUX_PAGE_COUNTER_H
+#define _LINUX_PAGE_COUNTER_H
+
+#include <linux/atomic.h>
+#include <linux/kernel.h>
+#include <asm/page.h>
+
+struct page_counter {
+	atomic_long_t count;
+	unsigned long limit;
+	struct page_counter *parent;
+
+	/* legacy */
+	unsigned long watermark;
+	unsigned long failcnt;
+};
+
+#if BITS_PER_LONG == 32
+#define PAGE_COUNTER_MAX LONG_MAX
+#else
+#define PAGE_COUNTER_MAX (LONG_MAX / PAGE_SIZE)
+#endif
+
+static inline void page_counter_init(struct page_counter *counter,
+				     struct page_counter *parent)
+{
+	atomic_long_set(&counter->count, 0);
+	counter->limit = PAGE_COUNTER_MAX;
+	counter->parent = parent;
+}
+
+static inline unsigned long page_counter_read(struct page_counter *counter)
+{
+	return atomic_long_read(&counter->count);
+}
+
+int page_counter_cancel(struct page_counter *counter, unsigned long nr_pages);
+void page_counter_charge(struct page_counter *counter, unsigned long nr_pages);
+int page_counter_try_charge(struct page_counter *counter,
+			    unsigned long nr_pages,
+			    struct page_counter **fail);
+int page_counter_uncharge(struct page_counter *counter, unsigned long nr_pages);
+int page_counter_limit(struct page_counter *counter, unsigned long limit);
+int page_counter_memparse(const char *buf, unsigned long *nr_pages);
+
+static inline void page_counter_reset_watermark(struct page_counter *counter)
+{
+	counter->watermark = page_counter_read(counter);
+}
+
+#endif /* _LINUX_PAGE_COUNTER_H */
