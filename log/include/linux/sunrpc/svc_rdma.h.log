commit 08e3c9f181bfb78d842c4f432888116d66e07c2f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 29 16:00:12 2020 -0400

    svcrdma: Remove the SVCRDMA_DEBUG macro
    
    Clean up: Commit d21b05f101ae ("rdma: SVCRMDA Header File")
    introduced the SVCRDMA_DEBUG macro, but it doesn't seem to have been
    used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 8518c3f37e56..7ed82625dc0b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -48,7 +48,6 @@
 #include <linux/sunrpc/rpc_rdma.h>
 #include <rdma/ib_verbs.h>
 #include <rdma/rdma_cm.h>
-#define SVCRDMA_DEBUG
 
 /* Default and maximum inline threshold sizes */
 enum {

commit ea740bd5f58e2912e74f401fd01a9d6aa985ca05
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 20 17:32:41 2020 -0400

    svcrdma: Fix backchannel return code
    
    Way back when I was writing the RPC/RDMA server-side backchannel
    code, I misread the TCP backchannel reply handler logic. When
    svc_tcp_recvfrom() successfully receives a backchannel reply, it
    does not return -EAGAIN. It sets XPT_DATA and returns zero.
    
    Update svc_rdma_recvfrom() to return zero. Here, XPT_DATA doesn't
    need to be set again: it is set whenever a new message is received,
    behind a spin lock in a single threaded context.
    
    Also, if handling the cb reply is not successful, the message is
    simply dropped. There's no special message framing to deal with as
    there is in the TCP case.
    
    Now that the handle_bc_reply() return value is ignored, I've removed
    the dprintk call sites in the error exit of handle_bc_reply() in
    favor of trace points in other areas that already report the error
    cases.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cbcfbd0521e3..8518c3f37e56 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -160,9 +160,8 @@ struct svc_rdma_send_ctxt {
 };
 
 /* svc_rdma_backchannel.c */
-extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
-				    __be32 *rdma_resp,
-				    struct xdr_buf *rcvbuf);
+extern void svc_rdma_handle_bc_reply(struct svc_rqst *rqstp,
+				     struct svc_rdma_recv_ctxt *rctxt);
 
 /* svc_rdma_recvfrom.c */
 extern void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma);

commit 23cf1ee1f1869966b75518c59b5cbda4c6c92450
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 31 17:02:33 2020 -0400

    svcrdma: Fix leak of svc_rdma_recv_ctxt objects
    
    Utilize the xpo_release_rqst transport method to ensure that each
    rqstp's svc_rdma_recv_ctxt object is released even when the server
    cannot return a Reply for that rqstp.
    
    Without this fix, each RPC whose Reply cannot be sent leaks one
    svc_rdma_recv_ctxt. This is a 2.5KB structure, a 4KB DMA-mapped
    Receive buffer, and any pages that might be part of the Reply
    message.
    
    The leak is infrequent unless the network fabric is unreliable or
    Kerberos is in use, as GSS sequence window overruns, which result
    in connection loss, are more common on fast transports.
    
    Fixes: 3a88092ee319 ("svcrdma: Preserve Receive buffer until svc_rdma_sendto")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 78fe2ac6dc6c..cbcfbd0521e3 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -170,6 +170,7 @@ extern bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma);
 extern void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
 				   struct svc_rdma_recv_ctxt *ctxt);
 extern void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma);
+extern void svc_rdma_release_rqst(struct svc_rqst *rqstp);
 extern int svc_rdma_recvfrom(struct svc_rqst *);
 
 /* svc_rdma_rw.c */

commit 0dabe948f28274e7956a625a24f205016b810693
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 3 13:28:14 2020 -0500

    svcrdma: Avoid DMA mapping small RPC Replies
    
    On some platforms, DMA mapping part of a page is more costly than
    copying bytes. Indeed, not involving the I/O MMU can help the
    RPC/RDMA transport scale better for tiny I/Os across more RDMA
    devices. This is because interaction with the I/O MMU is eliminated
    for each of these small I/Os. Without the explicit unmapping, the
    NIC no longer needs to do a costly internal TLB shoot down for
    buffers that are just a handful of bytes.
    
    Since pull-up is now a more a frequent operation, I've introduced a
    trace point in the pull-up path. It can be used for debugging or
    user-space tools that count pull-up frequency.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index a3fa5b4fa2e4..78fe2ac6dc6c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -52,6 +52,7 @@
 
 /* Default and maximum inline threshold sizes */
 enum {
+	RPCRDMA_PULLUP_THRESH = RPCRDMA_V1_DEF_INLINE_SIZE >> 1,
 	RPCRDMA_DEF_INLINE_THRESH = 4096,
 	RPCRDMA_MAX_INLINE_THRESH = 65536
 };

commit aee4b74a3f273b54d136132fedf575ec464f4134
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 3 11:08:05 2020 -0500

    svcrdma: Fix double sync of transport header buffer
    
    Performance optimization: Avoid syncing the transport buffer twice
    when Reply buffer pull-up is necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d001aac13c2f..a3fa5b4fa2e4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -191,9 +191,6 @@ extern struct svc_rdma_send_ctxt *
 extern void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
 				   struct svc_rdma_send_ctxt *ctxt);
 extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
-extern void svc_rdma_sync_reply_hdr(struct svcxprt_rdma *rdma,
-				    struct svc_rdma_send_ctxt *ctxt,
-				    unsigned int len);
 extern int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
 				  struct svc_rdma_send_ctxt *sctxt,
 				  const struct svc_rdma_recv_ctxt *rctxt,

commit 6fd5034db45c9c0ca57c98f3d5b9a0ce5869eab3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:02:20 2020 -0500

    svcrdma: Refactor chunk list encoders
    
    Same idea as the receive-side changes I did a while back: use
    xdr_stream helpers rather than open-coding the XDR chunk list
    encoders. This builds the Reply transport header from beginning to
    end without backtracking.
    
    As additional clean-ups, fill in documenting comments for the XDR
    encoders and sprinkle some trace points in the new encoding
    functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c506732886b3..d001aac13c2f 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -149,6 +149,8 @@ struct svc_rdma_send_ctxt {
 	struct list_head	sc_list;
 	struct ib_send_wr	sc_send_wr;
 	struct ib_cqe		sc_cqe;
+	struct xdr_buf		sc_hdrbuf;
+	struct xdr_stream	sc_stream;
 	void			*sc_xprt_buf;
 	int			sc_page_count;
 	int			sc_cur_sge_no;

commit 4554755ed81bb690d709168550aba5b46447f069
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:02:19 2020 -0500

    svcrdma: Update synopsis of svc_rdma_map_reply_msg()
    
    Preparing for subsequent patches, no behavior change expected.
    
    Pass the RPC Call's svc_rdma_recv_ctxt deeper into the sendto()
    path. This enables passing more information about Requester-
    provided Write and Reply chunks into those lower-level functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 42b68126cc60..c506732886b3 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -193,8 +193,9 @@ extern void svc_rdma_sync_reply_hdr(struct svcxprt_rdma *rdma,
 				    struct svc_rdma_send_ctxt *ctxt,
 				    unsigned int len);
 extern int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
-				  struct svc_rdma_send_ctxt *ctxt,
-				  struct xdr_buf *xdr, __be32 *wr_lst);
+				  struct svc_rdma_send_ctxt *sctxt,
+				  const struct svc_rdma_recv_ctxt *rctxt,
+				  struct xdr_buf *xdr);
 extern int svc_rdma_sendto(struct svc_rqst *);
 extern int svc_rdma_read_payload(struct svc_rqst *rqstp, unsigned int offset,
 				 unsigned int length);

commit 6fa5785e78d39f03d9fa33dea4dad2e7caf21e1e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:02:19 2020 -0500

    svcrdma: Update synopsis of svc_rdma_send_reply_chunk()
    
    Preparing for subsequent patches, no behavior change expected.
    
    Pass the RPC Call's svc_rdma_recv_ctxt deeper into the sendto()
    path. This enables passing more information about Requester-
    provided Write and Reply chunks into the lower-level send
    functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index e714e4d90ac5..42b68126cc60 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -179,7 +179,7 @@ extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,
 				     unsigned int offset,
 				     unsigned long length);
 extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
-				     __be32 *rp_ch, bool writelist,
+				     const struct svc_rdma_recv_ctxt *rctxt,
 				     struct xdr_buf *xdr);
 
 /* svc_rdma_sendto.c */

commit 2fe8c446338e083a1f3c0ccaaaa20e7d48e71ebc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:01:08 2020 -0500

    svcrdma: De-duplicate code that locates Write and Reply chunks
    
    Cache the locations of the Requester-provided Write list and Reply
    chunk so that the Send path doesn't need to parse the Call header
    again.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c790dbb0dd90..e714e4d90ac5 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -138,6 +138,8 @@ struct svc_rdma_recv_ctxt {
 	unsigned int		rc_page_count;
 	unsigned int		rc_hdr_count;
 	u32			rc_inv_rkey;
+	__be32			*rc_write_list;
+	__be32			*rc_reply_chunk;
 	unsigned int		rc_read_payload_offset;
 	unsigned int		rc_read_payload_length;
 	struct page		*rc_pages[RPCSVC_MAXPAGES];

commit e604aad2cac7357162f661e45f2f60e46faa7b17
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:01:08 2020 -0500

    svcrdma: Use struct xdr_stream to decode ingress transport headers
    
    The logic that checks incoming network headers has to be scrupulous.
    
    De-duplicate: replace open-coded buffer overflow checks with the use
    of xdr_stream helpers that are used most everywhere else XDR
    decoding is done.
    
    One minor change to the sanity checks: instead of checking the
    length of individual segments, cap the length of the whole chunk
    to be sure it can fit in the set of pages available in rq_pages.
    This should be a better test of whether the server can handle the
    chunks in each request.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 04e4a34d1c6a..c790dbb0dd90 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -132,6 +132,7 @@ struct svc_rdma_recv_ctxt {
 	struct ib_sge		rc_recv_sge;
 	void			*rc_recv_buf;
 	struct xdr_buf		rc_arg;
+	struct xdr_stream	rc_stream;
 	bool			rc_temp;
 	u32			rc_byte_len;
 	unsigned int		rc_page_count;

commit 412055398b9e67e07347a936fc4a6adddabe9cf4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 14:45:53 2020 -0500

    nfsd: Fix NFSv4 READ on RDMA when using readv
    
    svcrdma expects that the payload falls precisely into the xdr_buf
    page vector. This does not seem to be the case for
    nfsd4_encode_readv().
    
    This code is called only when fops->splice_read is missing or when
    RQ_SPLICE_OK is clear, so it's not a noticeable problem in many
    common cases.
    
    Add new transport method: ->xpo_read_payload so that when a READ
    payload does not fit exactly in rq_res's page vector, the XDR
    encoder can inform the RPC transport exactly where that payload is,
    without the payload's XDR pad.
    
    That way, when a Write chunk is present, the transport knows what
    byte range in the Reply message is supposed to be matched with the
    chunk.
    
    Note that the Linux NFS server implementation of NFS/RDMA can
    currently handle only one Write chunk per RPC-over-RDMA message.
    This simplifies the implementation of this fix.
    
    Fixes: b04209806384 ("nfsd4: allow exotic read compounds")
    Buglink: https://bugzilla.kernel.org/show_bug.cgi?id=198053
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 40f65888dd38..04e4a34d1c6a 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -137,6 +137,8 @@ struct svc_rdma_recv_ctxt {
 	unsigned int		rc_page_count;
 	unsigned int		rc_hdr_count;
 	u32			rc_inv_rkey;
+	unsigned int		rc_read_payload_offset;
+	unsigned int		rc_read_payload_length;
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 
@@ -170,7 +172,9 @@ extern int svc_rdma_recv_read_chunk(struct svcxprt_rdma *rdma,
 				    struct svc_rqst *rqstp,
 				    struct svc_rdma_recv_ctxt *head, __be32 *p);
 extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,
-				     __be32 *wr_ch, struct xdr_buf *xdr);
+				     __be32 *wr_ch, struct xdr_buf *xdr,
+				     unsigned int offset,
+				     unsigned long length);
 extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
 				     __be32 *rp_ch, bool writelist,
 				     struct xdr_buf *xdr);
@@ -189,6 +193,8 @@ extern int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
 				  struct svc_rdma_send_ctxt *ctxt,
 				  struct xdr_buf *xdr, __be32 *wr_lst);
 extern int svc_rdma_sendto(struct svc_rqst *);
+extern int svc_rdma_read_payload(struct svc_rqst *rqstp, unsigned int offset,
+				 unsigned int length);
 
 /* svc_rdma_transport.c */
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);

commit 4866073e6ddf03066c925d3237903d7f4ca68982
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Aug 16 17:49:38 2019 -0400

    svcrdma: Use llist for managing cache of recv_ctxts
    
    Use a wait-free mechanism for managing the svc_rdma_recv_ctxts free
    list. Subsequently, sc_recv_lock can be eliminated.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index edb39900fe04..40f65888dd38 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -42,6 +42,7 @@
 
 #ifndef SVC_RDMA_H
 #define SVC_RDMA_H
+#include <linux/llist.h>
 #include <linux/sunrpc/xdr.h>
 #include <linux/sunrpc/svcsock.h>
 #include <linux/sunrpc/rpc_rdma.h>
@@ -107,8 +108,7 @@ struct svcxprt_rdma {
 	struct list_head     sc_read_complete_q;
 	struct work_struct   sc_work;
 
-	spinlock_t	     sc_recv_lock;
-	struct list_head     sc_recv_ctxts;
+	struct llist_head    sc_recv_ctxts;
 };
 /* sc_flags */
 #define RDMAXPRT_CONN_PENDING	3
@@ -125,6 +125,7 @@ enum {
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 
 struct svc_rdma_recv_ctxt {
+	struct llist_node	rc_node;
 	struct list_head	rc_list;
 	struct ib_recv_wr	rc_recv_wr;
 	struct ib_cqe		rc_cqe;

commit d6dfe43ec6062beea5ba1172b957e74a13c95b86
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Aug 16 17:48:36 2019 -0400

    svcrdma: Remove svc_rdma_wq
    
    Clean up: the system workqueue will work just as well.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 981f0d726ad4..edb39900fe04 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -200,7 +200,6 @@ extern struct svc_xprt_class svc_rdma_bc_class;
 #endif
 
 /* svc_rdma.c */
-extern struct workqueue_struct *svc_rdma_wq;
 extern int svc_rdma_init(void);
 extern void svc_rdma_cleanup(void);
 

commit 64e20ba204df539a76004114e08abf1156302e35
Author: Vasily Averin <vvs@virtuozzo.com>
Date:   Mon Dec 24 14:46:00 2018 +0300

    sunrpc: remove unused xpo_prep_reply_hdr callback
    
    xpo_prep_reply_hdr are not used now.
    
    It was defined for tcp transport only, however it cannot be
    called indirectly, so let's move it to its caller and
    remove unused callback.
    
    Signed-off-by: Vasily Averin <vvs@virtuozzo.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 7e22681333d0..981f0d726ad4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -193,7 +193,6 @@ extern int svc_rdma_sendto(struct svc_rqst *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
-extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);
 
 extern struct svc_xprt_class svc_rdma_class;
 #ifdef CONFIG_SUNRPC_BACKCHANNEL

commit 97bce63408f192712574a4d9d6dcab794eed3a79
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 27 11:11:35 2018 -0500

    svcrdma: Optimize the logic that selects the R_key to invalidate
    
    o Select the R_key to invalidate while the CPU cache still contains
      the received RPC Call transport header, rather than waiting until
      we're about to send the RPC Reply.
    
    o Choose Send With Invalidate if there is exactly one distinct R_key
      in the received transport header. If there's more than one, the
      client will have to perform local invalidation after it has
      already waited for remote invalidation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index e6e26918504c..7e22681333d0 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -135,6 +135,7 @@ struct svc_rdma_recv_ctxt {
 	u32			rc_byte_len;
 	unsigned int		rc_page_count;
 	unsigned int		rc_hdr_count;
+	u32			rc_inv_rkey;
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 

commit 3ae2cefb613b00d613677c05ffa384b4f660f468
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:16:11 2018 -0400

    svcrdma: Increase the default connection credit limit
    
    Reduce queuing on clients by allowing more credits by default.
    
    64 is the default NFSv4.1 slot table size on Linux clients. This
    size prevents the credit limit from putting RPC requests to sleep
    again after they have already slept waiting for a session slot.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index fd78f78df5c6..e6e26918504c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -113,13 +113,14 @@ struct svcxprt_rdma {
 /* sc_flags */
 #define RDMAXPRT_CONN_PENDING	3
 
-#define RPCRDMA_LISTEN_BACKLOG  10
-#define RPCRDMA_MAX_REQUESTS    32
-
-/* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our
- * current NFSv4.1 implementation supports one backchannel slot.
+/*
+ * Default connection parameters
  */
-#define RPCRDMA_MAX_BC_REQUESTS	2
+enum {
+	RPCRDMA_LISTEN_BACKLOG	= 10,
+	RPCRDMA_MAX_REQUESTS	= 64,
+	RPCRDMA_MAX_BC_REQUESTS	= 2,
+};
 
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 

commit 51cc257a1186f69dbb6faec1bd48cc7e1fc31079
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:31 2018 -0400

    svcrdma: Remove unused svc_rdma_op_ctxt
    
    Clean up: Eliminate a structure that is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 96b14a72d359..fd78f78df5c6 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -71,26 +71,6 @@ extern atomic_t rdma_stat_rq_prod;
 extern atomic_t rdma_stat_sq_poll;
 extern atomic_t rdma_stat_sq_prod;
 
-/*
- * Contexts are built when an RDMA request is created and are a
- * record of the resources that can be recovered when the request
- * completes.
- */
-struct svc_rdma_op_ctxt {
-	struct list_head list;
-	struct xdr_buf arg;
-	struct ib_cqe cqe;
-	u32 byte_len;
-	struct svcxprt_rdma *xprt;
-	enum dma_data_direction direction;
-	int count;
-	unsigned int mapped_sges;
-	int hdr_count;
-	struct ib_send_wr send_wr;
-	struct ib_sge sge[1 + RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];
-	struct page *pages[RPCSVC_MAXPAGES];
-};
-
 struct svcxprt_rdma {
 	struct svc_xprt      sc_xprt;		/* SVC transport structure */
 	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
@@ -111,7 +91,6 @@ struct svcxprt_rdma {
 
 	spinlock_t	     sc_send_lock;
 	struct list_head     sc_send_ctxts;
-	int		     sc_ctxt_used;
 	spinlock_t	     sc_rw_ctxt_lock;
 	struct list_head     sc_rw_ctxts;
 

commit 99722fe4d5a634707ced8d8f42b883b87a86b3c5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:25 2018 -0400

    svcrdma: Persistently allocate and DMA-map Send buffers
    
    While sending each RPC Reply, svc_rdma_sendto allocates and DMA-
    maps a separate buffer where the RPC/RDMA transport header is
    constructed. The buffer is unmapped and released in the Send
    completion handler. This is significant per-RPC overhead,
    especially for small RPCs.
    
    Instead, allocate and DMA-map a buffer, and cache it in each
    svc_rdma_send_ctxt. This buffer and its mapping can be re-used
    for each RPC, saving the cost of memory allocation and DMA
    mapping.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index a8bfc214614b..96b14a72d359 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -162,6 +162,7 @@ struct svc_rdma_send_ctxt {
 	struct list_head	sc_list;
 	struct ib_send_wr	sc_send_wr;
 	struct ib_cqe		sc_cqe;
+	void			*sc_xprt_buf;
 	int			sc_page_count;
 	int			sc_cur_sge_no;
 	struct page		*sc_pages[RPCSVC_MAXPAGES];
@@ -199,9 +200,12 @@ extern struct svc_rdma_send_ctxt *
 extern void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
 				   struct svc_rdma_send_ctxt *ctxt);
 extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
-extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
+extern void svc_rdma_sync_reply_hdr(struct svcxprt_rdma *rdma,
+				    struct svc_rdma_send_ctxt *ctxt,
+				    unsigned int len);
+extern int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,
 				  struct svc_rdma_send_ctxt *ctxt,
-				  __be32 *rdma_resp, unsigned int len);
+				  struct xdr_buf *xdr, __be32 *wr_lst);
 extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */

commit 986b78894b268f605e9ea055b99959bdce0e5945
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:15 2018 -0400

    svcrdma: Remove post_send_wr
    
    Clean up: Now that the send_wr is part of the svc_rdma_send_ctxt,
    svc_rdma_post_send_wr is nearly empty.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index bfb8824e31e1..a8bfc214614b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -202,9 +202,6 @@ extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
 extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
 				  struct svc_rdma_send_ctxt *ctxt,
 				  __be32 *rdma_resp, unsigned int len);
-extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
-				 struct svc_rdma_send_ctxt *ctxt,
-				 u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */

commit 25fd86eca11c26bad2aede6dd4709ff58f89c7cb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:09 2018 -0400

    svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt
    
    Receive buffers are always the same size, but each Send WR has a
    variable number of SGEs, based on the contents of the xdr_buf being
    sent.
    
    While assembling a Send WR, keep track of the number of SGEs so that
    we don't exceed the device's maximum, or walk off the end of the
    Send SGE array.
    
    For now the Send path just fails if it exceeds the maximum.
    
    The current logic in svc_rdma_accept bases the maximum number of
    Send SGEs on the largest NFS request that can be sent or received.
    In the transport layer, the limit is actually based on the
    capabilities of the underlying device, not on properties of the
    Upper Layer Protocol.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d3e2bb331264..bfb8824e31e1 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -96,7 +96,7 @@ struct svcxprt_rdma {
 	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
 	struct list_head     sc_accept_q;	/* Conn. waiting accept */
 	int		     sc_ord;		/* RDMA read limit */
-	int                  sc_max_sge;
+	int                  sc_max_send_sges;
 	bool		     sc_snd_w_inv;	/* OK to use Send With Invalidate */
 
 	atomic_t             sc_sq_avail;	/* SQEs ready to be consumed */
@@ -158,17 +158,14 @@ struct svc_rdma_recv_ctxt {
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 
-enum {
-	RPCRDMA_MAX_SGES	= 1 + (RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE),
-};
-
 struct svc_rdma_send_ctxt {
 	struct list_head	sc_list;
 	struct ib_send_wr	sc_send_wr;
 	struct ib_cqe		sc_cqe;
 	int			sc_page_count;
+	int			sc_cur_sge_no;
 	struct page		*sc_pages[RPCSVC_MAXPAGES];
-	struct ib_sge		sc_sges[RPCRDMA_MAX_SGES];
+	struct ib_sge		sc_sges[];
 };
 
 /* svc_rdma_backchannel.c */

commit 4201c7464753827803366b40e82eb050c04ebdef
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:04 2018 -0400

    svcrdma: Introduce svc_rdma_send_ctxt
    
    svc_rdma_op_ctxt's are pre-allocated and maintained on a per-xprt
    free list. This eliminates the overhead of calling kmalloc / kfree,
    both of which grab a globally shared lock that disables interrupts.
    Introduce a replacement to svc_rdma_op_ctxt's that is built
    especially for the svcrdma Send path.
    
    Subsequent patches will take advantage of this new structure by
    allocating real resources which are then cached in these objects.
    The allocations are freed when the transport is torn down.
    
    I've renamed the structure so that static type checking can be used
    to ensure that uses of op_ctxt and send_ctxt are not confused. As an
    additional clean up, structure fields are renamed to conform with
    kernel coding conventions.
    
    Additional clean ups:
    - Handle svc_rdma_send_ctxt_get allocation failure at each call
      site, rather than pre-allocating and hoping we guessed correctly
    - All send_ctxt_put call-sites request page freeing, so remove
      the @free_pages argument
    - All send_ctxt_put call-sites unmap SGEs, so fold that into
      svc_rdma_send_ctxt_put
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 8827b4e36c3c..d3e2bb331264 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -109,8 +109,8 @@ struct svcxprt_rdma {
 
 	struct ib_pd         *sc_pd;
 
-	spinlock_t	     sc_ctxt_lock;
-	struct list_head     sc_ctxts;
+	spinlock_t	     sc_send_lock;
+	struct list_head     sc_send_ctxts;
 	int		     sc_ctxt_used;
 	spinlock_t	     sc_rw_ctxt_lock;
 	struct list_head     sc_rw_ctxts;
@@ -158,6 +158,19 @@ struct svc_rdma_recv_ctxt {
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 
+enum {
+	RPCRDMA_MAX_SGES	= 1 + (RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE),
+};
+
+struct svc_rdma_send_ctxt {
+	struct list_head	sc_list;
+	struct ib_send_wr	sc_send_wr;
+	struct ib_cqe		sc_cqe;
+	int			sc_page_count;
+	struct page		*sc_pages[RPCSVC_MAXPAGES];
+	struct ib_sge		sc_sges[RPCRDMA_MAX_SGES];
+};
+
 /* svc_rdma_backchannel.c */
 extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    __be32 *rdma_resp,
@@ -183,24 +196,22 @@ extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
 				     struct xdr_buf *xdr);
 
 /* svc_rdma_sendto.c */
+extern void svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma);
+extern struct svc_rdma_send_ctxt *
+		svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma);
+extern void svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,
+				   struct svc_rdma_send_ctxt *ctxt);
+extern int svc_rdma_send(struct svcxprt_rdma *rdma, struct ib_send_wr *wr);
 extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
-				  struct svc_rdma_op_ctxt *ctxt,
+				  struct svc_rdma_send_ctxt *ctxt,
 				  __be32 *rdma_resp, unsigned int len);
 extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
-				 struct svc_rdma_op_ctxt *ctxt,
+				 struct svc_rdma_send_ctxt *ctxt,
 				 u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */
-extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);
-extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
-extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
-extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
-extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
 extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);

commit 232627905f12a05df75853c62451ce0886803cee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:59 2018 -0400

    svcrdma: Clean up Send SGE accounting
    
    Clean up: Since there's already a svc_rdma_op_ctxt being passed
    around with the running count of mapped SGEs, drop unneeded
    parameters to svc_rdma_post_send_wr().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 95530bc7bfab..8827b4e36c3c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -188,7 +188,7 @@ extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
 				  __be32 *rdma_resp, unsigned int len);
 extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
 				 struct svc_rdma_op_ctxt *ctxt,
-				 int num_sge, u32 inv_rkey);
+				 u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */

commit f016f305f98159a9131ce200ed3b4ed92133012c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:53 2018 -0400

    svcrdma: Refactor svc_rdma_dma_map_buf
    
    Clean up: svc_rdma_dma_map_buf does mostly the same thing as
    svc_rdma_dma_map_page, so let's fold these together.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 27cf59c7085f..95530bc7bfab 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -158,13 +158,6 @@ struct svc_rdma_recv_ctxt {
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 
-/* Track DMA maps for this transport and context */
-static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
-					   struct svc_rdma_op_ctxt *ctxt)
-{
-	ctxt->mapped_sges++;
-}
-
 /* svc_rdma_backchannel.c */
 extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    __be32 *rdma_resp,

commit eb5d7a622e0bbe3fd316b2325d3840a0e030a3c4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:48 2018 -0400

    svcrdma: Allocate recv_ctxt's on CPU handling Receives
    
    There is a significant latency penalty when processing an ingress
    Receive if the Receive buffer resides in memory that is not on the
    same NUMA node as the the CPU handling completions for a CQ.
    
    The system administrator and the device driver determine which CPU
    handles completions. This CPU does not change during life of the CQ.
    Further the Upper Layer does not have any visibility of which CPU it
    is.
    
    Allocating Receive buffers in the Receive completion handler
    guarantees that Receive buffers are allocated on the preferred NUMA
    node for that CQ.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 01baabfb863b..27cf59c7085f 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -151,6 +151,7 @@ struct svc_rdma_recv_ctxt {
 	struct ib_sge		rc_recv_sge;
 	void			*rc_recv_buf;
 	struct xdr_buf		rc_arg;
+	bool			rc_temp;
 	u32			rc_byte_len;
 	unsigned int		rc_page_count;
 	unsigned int		rc_hdr_count;

commit 3316f0631139c87631f2652c118da1a0354bd40d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:43 2018 -0400

    svcrdma: Persistently allocate and DMA-map Receive buffers
    
    The current Receive path uses an array of pages which are allocated
    and DMA mapped when each Receive WR is posted, and then handed off
    to the upper layer in rqstp::rq_arg. The page flip releases unused
    pages in the rq_pages pagelist. This mechanism introduces a
    significant amount of overhead.
    
    So instead, kmalloc the Receive buffer, and leave it DMA-mapped
    while the transport remains connected. This confers a number of
    benefits:
    
    * Each Receive WR requires only one receive SGE, no matter how large
      the inline threshold is. This helps the server-side NFS/RDMA
      transport operate on less capable RDMA devices.
    
    * The Receive buffer is left allocated and mapped all the time. This
      relieves svc_rdma_post_recv from the overhead of allocating and
      DMA-mapping a fresh buffer.
    
    * svc_rdma_wc_receive no longer has to DMA unmap the Receive buffer.
      It has to DMA sync only the number of bytes that were received.
    
    * svc_rdma_build_arg_xdr no longer has to free a page in rq_pages
      for each page in the Receive buffer, making it a constant-time
      function.
    
    * The Receive buffer is now plugged directly into the rq_arg's
      head[0].iov_vec, and can be larger than a page without spilling
      over into rq_arg's page list. This enables simplification of
      the RDMA Read path in subsequent patches.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f0bd0b6d8931..01baabfb863b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -148,12 +148,12 @@ struct svc_rdma_recv_ctxt {
 	struct list_head	rc_list;
 	struct ib_recv_wr	rc_recv_wr;
 	struct ib_cqe		rc_cqe;
+	struct ib_sge		rc_recv_sge;
+	void			*rc_recv_buf;
 	struct xdr_buf		rc_arg;
 	u32			rc_byte_len;
 	unsigned int		rc_page_count;
 	unsigned int		rc_hdr_count;
-	struct ib_sge		rc_sges[1 +
-					RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];
 	struct page		*rc_pages[RPCSVC_MAXPAGES];
 };
 

commit 1e5f4160745690a0476929d128a336cae95c1df9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:32 2018 -0400

    svcrdma: Simplify svc_rdma_recv_ctxt_put
    
    Currently svc_rdma_recv_ctxt_put's callers have to know whether they
    want to free the ctxt's pages or not. This means the human
    developers have to know when and why to set that free_pages
    argument.
    
    Instead, the ctxt should carry that information with it so that
    svc_rdma_recv_ctxt_put does the right thing no matter who is
    calling.
    
    We want to keep track of the number of pages in the Receive buffer
    separately from the number of pages pulled over by RDMA Read. This
    is so that the correct number of pages can be freed properly and
    that number is well-documented.
    
    So now, rc_hdr_count is the number of pages consumed by head[0]
    (ie., the page index where the Read chunk should start); and
    rc_page_count is always the number of pages that need to be released
    when the ctxt is put.
    
    The @free_pages argument is no longer needed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 3cb66319a814..f0bd0b6d8931 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -173,8 +173,7 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 extern void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma);
 extern bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma);
 extern void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
-				   struct svc_rdma_recv_ctxt *ctxt,
-				   int free_pages);
+				   struct svc_rdma_recv_ctxt *ctxt);
 extern void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma);
 extern int svc_rdma_recvfrom(struct svc_rqst *);
 

commit 2c577bfea85e421bfa91df16ccf5156361aa8d4b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:27 2018 -0400

    svcrdma: Remove sc_rq_depth
    
    Clean up: No need to retain rq_depth in struct svcrdma_xprt, it is
    used only in svc_rdma_accept().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 37f759d65348..3cb66319a814 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -101,7 +101,6 @@ struct svcxprt_rdma {
 
 	atomic_t             sc_sq_avail;	/* SQEs ready to be consumed */
 	unsigned int	     sc_sq_depth;	/* Depth of SQ */
-	unsigned int	     sc_rq_depth;	/* Depth of RQ */
 	__be32		     sc_fc_credits;	/* Forward credits */
 	u32		     sc_max_requests;	/* Max requests */
 	u32		     sc_max_bc_requests;/* Backward credits */

commit ecf85b2384ea5f7cb0577bf6143bc46d9ecfe4d3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:21 2018 -0400

    svcrdma: Introduce svc_rdma_recv_ctxt
    
    svc_rdma_op_ctxt's are pre-allocated and maintained on a per-xprt
    free list. This eliminates the overhead of calling kmalloc / kfree,
    both of which grab a globally shared lock that disables interrupts.
    To reduce contention further, separate the use of these objects in
    the Receive and Send paths in svcrdma.
    
    Subsequent patches will take advantage of this separation by
    allocating real resources which are then cached in these objects.
    The allocations are freed when the transport is torn down.
    
    I've renamed the structure so that static type checking can be used
    to ensure that uses of op_ctxt and recv_ctxt are not confused. As an
    additional clean up, structure fields are renamed to conform with
    kernel coding conventions.
    
    As a final clean up, helpers related to recv_ctxt are moved closer
    to the functions that use them.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 88da0c9bd7b1..37f759d65348 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -128,6 +128,9 @@ struct svcxprt_rdma {
 	unsigned long	     sc_flags;
 	struct list_head     sc_read_complete_q;
 	struct work_struct   sc_work;
+
+	spinlock_t	     sc_recv_lock;
+	struct list_head     sc_recv_ctxts;
 };
 /* sc_flags */
 #define RDMAXPRT_CONN_PENDING	3
@@ -142,6 +145,19 @@ struct svcxprt_rdma {
 
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 
+struct svc_rdma_recv_ctxt {
+	struct list_head	rc_list;
+	struct ib_recv_wr	rc_recv_wr;
+	struct ib_cqe		rc_cqe;
+	struct xdr_buf		rc_arg;
+	u32			rc_byte_len;
+	unsigned int		rc_page_count;
+	unsigned int		rc_hdr_count;
+	struct ib_sge		rc_sges[1 +
+					RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];
+	struct page		*rc_pages[RPCSVC_MAXPAGES];
+};
+
 /* Track DMA maps for this transport and context */
 static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 					   struct svc_rdma_op_ctxt *ctxt)
@@ -155,13 +171,19 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    struct xdr_buf *rcvbuf);
 
 /* svc_rdma_recvfrom.c */
+extern void svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma);
+extern bool svc_rdma_post_recvs(struct svcxprt_rdma *rdma);
+extern void svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,
+				   struct svc_rdma_recv_ctxt *ctxt,
+				   int free_pages);
+extern void svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma);
 extern int svc_rdma_recvfrom(struct svc_rqst *);
 
 /* svc_rdma_rw.c */
 extern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);
 extern int svc_rdma_recv_read_chunk(struct svcxprt_rdma *rdma,
 				    struct svc_rqst *rqstp,
-				    struct svc_rdma_op_ctxt *head, __be32 *p);
+				    struct svc_rdma_recv_ctxt *head, __be32 *p);
 extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,
 				     __be32 *wr_ch, struct xdr_buf *xdr);
 extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,

commit bcf3ffd405df6998914b248d2f22625544a4dd56
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:26:55 2018 -0400

    svcrdma: Add proper SPDX tags for NetApp-contributed source
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 7337e1221590..88da0c9bd7b1 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause */
 /*
  * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.
  *

commit 97cc3264508f33783ba21573204d7e0bf5b197e7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 20 17:05:15 2018 -0400

    svcrdma: Consult max_qp_init_rd_atom when accepting connections
    
    The target needs to return the lesser of the client's Inbound RDMA
    Read Queue Depth (IRD), provided in the connection parameters, and
    the local device's Outbound RDMA Read Queue Depth (ORD). The latter
    limit is max_qp_init_rd_atom, not max_qp_rd_atom.
    
    The svcrdma_ord value caps the ORD value for iWARP transports, which
    do not exchange ORD/IRD values at connection time. Since no other
    Linux kernel RDMA-enabled storage target sees fit to provide this
    cap, I'm removing it here too.
    
    initiator_depth is a u8, so ensure the computed ORD value does not
    overflow that field.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 4b731b046bcd..7337e1221590 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -132,9 +132,6 @@ struct svcxprt_rdma {
 #define RDMAXPRT_CONN_PENDING	3
 
 #define RPCRDMA_LISTEN_BACKLOG  10
-/* The default ORD value is based on two outstanding full-size writes with a
- * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
-#define RPCRDMA_ORD             (64/4)
 #define RPCRDMA_MAX_REQUESTS    32
 
 /* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our

commit 482725027ff32bc857f5527fb17feda5361265fe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 3 15:42:18 2018 -0500

    svcrdma: Post Receives in the Receive completion handler
    
    This change improves Receive efficiency by posting Receives only
    on the same CPU that handles Receive completion. Improved latency
    and throughput has been noted with this change.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 995c6fe9ee90..4b731b046bcd 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -185,8 +185,6 @@ extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);
 extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);
 extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
-extern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);
-extern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);

commit 9450ca8e2febb0000a5efd4f5870915d59ae62bc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:19:13 2017 -0400

    svcrdma: Clean up after converting svc_rdma_recvfrom to rdma_rw API
    
    Clean up: Registration mode details are now handled by the rdma_rw
    API, and thus can be removed from svcrdma.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index fd7775f70bb5..995c6fe9ee90 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -90,9 +90,6 @@ struct svc_rdma_op_ctxt {
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
-#define	SVCRDMA_DEVCAP_FAST_REG		1	/* fast mr registration */
-#define	SVCRDMA_DEVCAP_READ_W_INV	2	/* read w/ invalidate */
-
 struct svcxprt_rdma {
 	struct svc_xprt      sc_xprt;		/* SVC transport structure */
 	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
@@ -123,7 +120,6 @@ struct svcxprt_rdma {
 	struct ib_qp         *sc_qp;
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
-	u32		     sc_dev_caps;	/* distilled device caps */
 
 	spinlock_t	     sc_lock;		/* transport lock */
 

commit 463e63d7014442002399903af027b63ae38f6e77
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:18:57 2017 -0400

    svcrdma: Remove frmr cache
    
    Clean up: Now that the svc_rdma_recvfrom path uses the rdma_rw API,
    the details of Read sink buffer registration are dealt with by the
    kernel's RDMA core. This cache is no longer used, and can be
    removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 06d58a3f74bc..fd7775f70bb5 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -77,7 +77,6 @@ extern atomic_t rdma_stat_sq_prod;
  */
 struct svc_rdma_op_ctxt {
 	struct list_head list;
-	struct svc_rdma_fastreg_mr *frmr;
 	struct xdr_buf arg;
 	struct ib_cqe cqe;
 	u32 byte_len;
@@ -91,17 +90,6 @@ struct svc_rdma_op_ctxt {
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
-struct svc_rdma_fastreg_mr {
-	struct ib_mr *mr;
-	struct scatterlist *sg;
-	int sg_nents;
-	unsigned long access_flags;
-	enum dma_data_direction direction;
-	struct list_head frmr_list;
-};
-
-#define RDMACTXT_F_LAST_CTXT	2
-
 #define	SVCRDMA_DEVCAP_FAST_REG		1	/* fast mr registration */
 #define	SVCRDMA_DEVCAP_READ_W_INV	2	/* read w/ invalidate */
 
@@ -136,9 +124,6 @@ struct svcxprt_rdma {
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
 	u32		     sc_dev_caps;	/* distilled device caps */
-	unsigned int	     sc_frmr_pg_list_len;
-	struct list_head     sc_frmr_q;
-	spinlock_t	     sc_frmr_q_lock;
 
 	spinlock_t	     sc_lock;		/* transport lock */
 
@@ -210,9 +195,6 @@ extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
-extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
-extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
-			      struct svc_rdma_fastreg_mr *);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
 extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);

commit c84dc900d737a8d8f08768622226980ee863403b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:18:49 2017 -0400

    svcrdma: Remove unused Read completion handlers
    
    Clean up:
    
    The generic RDMA R/W API conversion of svc_rdma_recvfrom replaced
    the Register, Read, and Invalidate completion handlers. Remove the
    old ones, which are no longer used.
    
    These handlers shared some helper code with svc_rdma_wc_send. Fold
    the wc_common helper back into the one remaining completion handler.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index b1ba19ba1071..06d58a3f74bc 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -77,17 +77,15 @@ extern atomic_t rdma_stat_sq_prod;
  */
 struct svc_rdma_op_ctxt {
 	struct list_head list;
-	struct svc_rdma_op_ctxt *read_hdr;
 	struct svc_rdma_fastreg_mr *frmr;
-	int hdr_count;
 	struct xdr_buf arg;
 	struct ib_cqe cqe;
 	u32 byte_len;
 	struct svcxprt_rdma *xprt;
-	unsigned long flags;
 	enum dma_data_direction direction;
 	int count;
 	unsigned int mapped_sges;
+	int hdr_count;
 	struct ib_send_wr send_wr;
 	struct ib_sge sge[1 + RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];
 	struct page *pages[RPCSVC_MAXPAGES];

commit cafc739892f34b9090413179ca259409fc43bfae
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:18:33 2017 -0400

    svcrdma: Use generic RDMA R/W API in RPC Call path
    
    The current svcrdma recvfrom code path has a lot of detail about
    registration mode and the type of port (iWARP, IB, etc).
    
    Instead, use the RDMA core's generic R/W API. This shares code with
    other RDMA-enabled ULPs that manages the gory details of buffer
    registration and the posting of RDMA Read Work Requests.
    
    Since the Read list marshaling code is being replaced, I took the
    opportunity to replace C structure-based XDR encoding code with more
    portable code that uses pointer arithmetic.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cf5d5412298b..b1ba19ba1071 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -82,10 +82,7 @@ struct svc_rdma_op_ctxt {
 	int hdr_count;
 	struct xdr_buf arg;
 	struct ib_cqe cqe;
-	struct ib_cqe reg_cqe;
-	struct ib_cqe inv_cqe;
 	u32 byte_len;
-	u32 position;
 	struct svcxprt_rdma *xprt;
 	unsigned long flags;
 	enum dma_data_direction direction;
@@ -116,7 +113,6 @@ struct svcxprt_rdma {
 	struct list_head     sc_accept_q;	/* Conn. waiting accept */
 	int		     sc_ord;		/* RDMA read limit */
 	int                  sc_max_sge;
-	int                  sc_max_sge_rd;	/* max sge for read target */
 	bool		     sc_snd_w_inv;	/* OK to use Send With Invalidate */
 
 	atomic_t             sc_sq_avail;	/* SQEs ready to be consumed */
@@ -141,10 +137,6 @@ struct svcxprt_rdma {
 	struct ib_qp         *sc_qp;
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
-	int		     (*sc_reader)(struct svcxprt_rdma *,
-					  struct svc_rqst *,
-					  struct svc_rdma_op_ctxt *,
-					  int *, u32 *, u32, u32, u64, bool);
 	u32		     sc_dev_caps;	/* distilled device caps */
 	unsigned int	     sc_frmr_pg_list_len;
 	struct list_head     sc_frmr_q;
@@ -187,12 +179,6 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 
 /* svc_rdma_recvfrom.c */
 extern int svc_rdma_recvfrom(struct svc_rqst *);
-extern int rdma_read_chunk_lcl(struct svcxprt_rdma *, struct svc_rqst *,
-			       struct svc_rdma_op_ctxt *, int *, u32 *,
-			       u32, u32, u64, bool);
-extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
-				struct svc_rdma_op_ctxt *, int *, u32 *,
-				u32, u32, u64, bool);
 
 /* svc_rdma_rw.c */
 extern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);

commit 026d958b38c628a1b4ced534808945365e2747a5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:18:24 2017 -0400

    svcrdma: Add recvfrom helpers to svc_rdma_rw.c
    
    svc_rdma_rw.c already contains helpers for the sendto path.
    Introduce helpers for the recvfrom path.
    
    The plan is to replace the local NFSD bespoke code that constructs
    and posts RDMA Read Work Requests with calls to the rdma_rw API.
    This shares code with other RDMA-enabled ULPs that manages the gory
    details of buffer registration and posting Work Requests.
    
    This new code also puts all RDMA_NOMSG-specific logic in one place.
    
    Lastly, the use of rqstp->rq_arg.pages is deprecated in favor of
    using rqstp->rq_pages directly, for clarity.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 3ca991657889..cf5d5412298b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -196,6 +196,9 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 
 /* svc_rdma_rw.c */
 extern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);
+extern int svc_rdma_recv_read_chunk(struct svcxprt_rdma *rdma,
+				    struct svc_rqst *rqstp,
+				    struct svc_rdma_op_ctxt *head, __be32 *p);
 extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,
 				     __be32 *wr_ch, struct xdr_buf *xdr);
 extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,

commit a80a32341fbabd4276165a9ce4fa4c80168c0bef
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jun 23 17:17:35 2017 -0400

    svcrdma: Remove svc_rdma_marshal.c
    
    svc_rdma_marshal.c has one remaining exported function --
    svc_rdma_xdr_decode_req -- and it has a single call site. Take
    the same approach as the sendto path, and move this function
    into the source file where it is called.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f3787d800ba4..3ca991657889 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -185,9 +185,6 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    __be32 *rdma_resp,
 				    struct xdr_buf *rcvbuf);
 
-/* svc_rdma_marshal.c */
-extern int svc_rdma_xdr_decode_req(struct xdr_buf *);
-
 /* svc_rdma_recvfrom.c */
 extern int svc_rdma_recvfrom(struct svc_rqst *);
 extern int rdma_read_chunk_lcl(struct svcxprt_rdma *, struct svc_rqst *,

commit dadf3e435debb85dfcf28c157012047153a21a97
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:07:21 2017 -0400

    svcrdma: Clean out old XDR encoders
    
    Clean up: These have been replaced and are no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 479bb7f65233..f3787d800ba4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -187,10 +187,6 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 
 /* svc_rdma_marshal.c */
 extern int svc_rdma_xdr_decode_req(struct xdr_buf *);
-extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
-extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
-					    __be32, __be64, u32);
-extern unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp);
 
 /* svc_rdma_recvfrom.c */
 extern int svc_rdma_recvfrom(struct svc_rqst *);

commit 2cf32924c68a22783e6f630e1b5345a80aa1a376
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:07:13 2017 -0400

    svcrdma: Remove the req_map cache
    
    req_maps are no longer used by the send path and can thus be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f58c5349beb7..479bb7f65233 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -96,23 +96,6 @@ struct svc_rdma_op_ctxt {
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
-/*
- * NFS_ requests are mapped on the client side by the chunk lists in
- * the RPCRDMA header. During the fetching of the RPC from the client
- * and the writing of the reply to the client, the memory in the
- * client and the memory in the server must be mapped as contiguous
- * vaddr/len for access by the hardware. These data strucures keep
- * these mappings.
- *
- * For an RDMA_WRITE, the 'sge' maps the RPC REPLY. For RDMA_READ, the
- * 'sge' in the svc_rdma_req_map maps the server side RPC reply and the
- * 'ch' field maps the read-list of the RPCRDMA header to the 'sge'
- * mapping of the reply.
- */
-struct svc_rdma_chunk_sge {
-	int start;		/* sge no for this chunk */
-	int count;		/* sge count for this chunk */
-};
 struct svc_rdma_fastreg_mr {
 	struct ib_mr *mr;
 	struct scatterlist *sg;
@@ -121,15 +104,7 @@ struct svc_rdma_fastreg_mr {
 	enum dma_data_direction direction;
 	struct list_head frmr_list;
 };
-struct svc_rdma_req_map {
-	struct list_head free;
-	unsigned long count;
-	union {
-		struct kvec sge[RPCSVC_MAXPAGES];
-		struct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];
-		unsigned long lkey[RPCSVC_MAXPAGES];
-	};
-};
+
 #define RDMACTXT_F_LAST_CTXT	2
 
 #define	SVCRDMA_DEVCAP_FAST_REG		1	/* fast mr registration */
@@ -160,8 +135,6 @@ struct svcxprt_rdma {
 	int		     sc_ctxt_used;
 	spinlock_t	     sc_rw_ctxt_lock;
 	struct list_head     sc_rw_ctxts;
-	spinlock_t	     sc_map_lock;
-	struct list_head     sc_maps;
 
 	struct list_head     sc_rq_dto_q;
 	spinlock_t	     sc_rq_dto_lock;
@@ -237,8 +210,6 @@ extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
 				     struct xdr_buf *xdr);
 
 /* svc_rdma_sendto.c */
-extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
-			    struct svc_rdma_req_map *, bool);
 extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
 				  struct svc_rdma_op_ctxt *ctxt,
 				  __be32 *rdma_resp, unsigned int len);
@@ -259,9 +230,6 @@ extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
-extern struct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *);
-extern void svc_rdma_put_req_map(struct svcxprt_rdma *,
-				 struct svc_rdma_req_map *);
 extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
 extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
 			      struct svc_rdma_fastreg_mr *);

commit 68cc4636bbbca89b9fedcf46d8b6bee444fc5e4e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:07:05 2017 -0400

    svcrdma: Remove unused RDMA Write completion handler
    
    Clean up. All RDMA Write completions are now handled by
    svc_rdma_wc_write_ctx.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index e84b77556784..f58c5349beb7 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -249,7 +249,6 @@ extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */
 extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);
-extern void svc_rdma_wc_write(struct ib_cq *, struct ib_wc *);
 extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);
 extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);
 extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);

commit ded8d19641a605232ab48f5d27f542648beba3cc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:57 2017 -0400

    svcrdma: Reduce size of sge array in struct svc_rdma_op_ctxt
    
    The sge array in struct svc_rdma_op_ctxt is no longer used for
    sending RDMA Write WRs. It need only accommodate the construction of
    Send and Receive WRs. The maximum inline size is the largest payload
    it needs to handle now.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 44d642bbfce6..e84b77556784 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -48,6 +48,12 @@
 #include <rdma/rdma_cm.h>
 #define SVCRDMA_DEBUG
 
+/* Default and maximum inline threshold sizes */
+enum {
+	RPCRDMA_DEF_INLINE_THRESH = 4096,
+	RPCRDMA_MAX_INLINE_THRESH = 65536
+};
+
 /* RPC/RDMA parameters and stats */
 extern unsigned int svcrdma_ord;
 extern unsigned int svcrdma_max_requests;
@@ -86,7 +92,7 @@ struct svc_rdma_op_ctxt {
 	int count;
 	unsigned int mapped_sges;
 	struct ib_send_wr send_wr;
-	struct ib_sge sge[RPCSVC_MAXPAGES];
+	struct ib_sge sge[1 + RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
@@ -186,7 +192,6 @@ struct svcxprt_rdma {
  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
 #define RPCRDMA_ORD             (64/4)
 #define RPCRDMA_MAX_REQUESTS    32
-#define RPCRDMA_MAX_REQ_SIZE    4096
 
 /* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our
  * current NFSv4.1 implementation supports one backchannel slot.

commit f5821c76b2c9c2fb98b276c0bf6a101bfe9050a3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:49 2017 -0400

    svcrdma: Clean up RPC-over-RDMA backchannel reply processing
    
    Replace C structure-based XDR decoding with pointer arithmetic.
    Pointer arithmetic is considered more portable.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index a770d200f607..44d642bbfce6 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -204,7 +204,7 @@ static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 
 /* svc_rdma_backchannel.c */
 extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
-				    struct rpcrdma_msg *rmsgp,
+				    __be32 *rdma_resp,
 				    struct xdr_buf *rcvbuf);
 
 /* svc_rdma_marshal.c */

commit 6b19cc5ca2f78ebc88f5d39ba6a94197bb392fcc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:33 2017 -0400

    svcrdma: Clean up RDMA_ERROR path
    
    Now that svc_rdma_sendto has been renovated, svc_rdma_send_error can
    be refactored to reduce code duplication and remove C structure-
    based XDR encoding. It is also relocated to the source file that
    contains its only caller.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 599ee03ee3fb..a770d200f607 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -209,9 +209,6 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 
 /* svc_rdma_marshal.c */
 extern int svc_rdma_xdr_decode_req(struct xdr_buf *);
-extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
-				     struct rpcrdma_msg *,
-				     enum rpcrdma_errcode, __be32 *);
 extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
 extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
 					    __be32, __be64, u32);
@@ -244,8 +241,6 @@ extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
 				 struct svc_rdma_op_ctxt *ctxt,
 				 int num_sge, u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
-extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
-				int);
 
 /* svc_rdma_transport.c */
 extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);

commit 9a6a180b7867ceceeeab88a6f011bac23174b939
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:25 2017 -0400

    svcrdma: Use rdma_rw API in RPC reply path
    
    The current svcrdma sendto code path posts one RDMA Write WR at a
    time. Each of these Writes typically carries a small number of pages
    (for instance, up to 30 pages for mlx4 devices). That means a 1MB
    NFS READ reply requires 9 ib_post_send() calls for the Write WRs,
    and one for the Send WR carrying the actual RPC Reply message.
    
    Instead, use the new rdma_rw API. The details of Write WR chain
    construction and memory registration are taken care of in the RDMA
    core. svcrdma can focus on the details of the RPC-over-RDMA
    protocol. This gives three main benefits:
    
    1. All Write WRs for one RDMA segment are posted in a single chain.
    As few as one ib_post_send() for each Write chunk.
    
    2. The Write path can now use FRWR to register the Write buffers.
    If the device's maximum page list depth is large, this means a
    single Write WR is needed for each RPC's Write chunk data.
    
    3. The new code introduces support for RPCs that carry both a Write
    list and a Reply chunk. This combination can be used for an NFSv4
    READ where the data payload is large, and thus is removed from the
    Payload Stream, but the Payload Stream is still larger than the
    inline threshold.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index ca08671fb7e2..599ee03ee3fb 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -212,7 +212,6 @@ extern int svc_rdma_xdr_decode_req(struct xdr_buf *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 				     struct rpcrdma_msg *,
 				     enum rpcrdma_errcode, __be32 *);
-extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);
 extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
 extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
 					    __be32, __be64, u32);

commit f13193f50b64e2e0c87706b838d6b9895626a892
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:16 2017 -0400

    svcrdma: Introduce local rdma_rw API helpers
    
    The plan is to replace the local bespoke code that constructs and
    posts RDMA Read and Write Work Requests with calls to the rdma_rw
    API. This shares code with other RDMA-enabled ULPs that manages the
    gory details of buffer registration and posting Work Requests.
    
    Some design notes:
    
     o The structure of RPC-over-RDMA transport headers is flexible,
       allowing multiple segments per Reply with arbitrary alignment,
       each with a unique R_key. Write and Send WRs continue to be
       built and posted in separate code paths. However, one whole
       chunk (with one or more RDMA segments apiece) gets exactly
       one ib_post_send and one work completion.
    
     o svc_xprt reference counting is modified, since a chain of
       rdma_rw_ctx structs generates one completion, no matter how
       many Write WRs are posted.
    
     o The current code builds the transport header as it is construct-
       ing Write WRs. I've replaced that with marshaling of transport
       header data items in a separate step. This is because the exact
       structure of client-provided segments may not align with the
       components of the server's reply xdr_buf, or the pages in the
       page list. Thus parts of each client-provided segment may be
       written at different points in the send path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 11d5aa123f17..ca08671fb7e2 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -145,12 +145,15 @@ struct svcxprt_rdma {
 	u32		     sc_max_requests;	/* Max requests */
 	u32		     sc_max_bc_requests;/* Backward credits */
 	int                  sc_max_req_size;	/* Size of each RQ WR buf */
+	u8		     sc_port_num;
 
 	struct ib_pd         *sc_pd;
 
 	spinlock_t	     sc_ctxt_lock;
 	struct list_head     sc_ctxts;
 	int		     sc_ctxt_used;
+	spinlock_t	     sc_rw_ctxt_lock;
+	struct list_head     sc_rw_ctxts;
 	spinlock_t	     sc_map_lock;
 	struct list_head     sc_maps;
 
@@ -224,6 +227,14 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 				struct svc_rdma_op_ctxt *, int *, u32 *,
 				u32, u32, u64, bool);
 
+/* svc_rdma_rw.c */
+extern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);
+extern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,
+				     __be32 *wr_ch, struct xdr_buf *xdr);
+extern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,
+				     __be32 *rp_ch, bool writelist,
+				     struct xdr_buf *xdr);
+
 /* svc_rdma_sendto.c */
 extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
 			    struct svc_rdma_req_map *, bool);

commit b623589dbacbc786c2fffc85113a1dc1a331e2ca
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:05:52 2017 -0400

    svcrdma: Eliminate RPCRDMA_SQ_DEPTH_MULT
    
    The Send Queue depth is temporarily reduced to 1 SQE per credit. The
    new rdma_rw API does an internal computation, during QP creation, to
    increase the depth of the Send Queue to handle RDMA Read and Write
    operations.
    
    This change has to come before the NFSD code paths are updated to
    use the rdma_rw API. Without this patch, rdma_rw_init_qp() increases
    the size of the SQ too much, resulting in memory allocation failures
    during QP creation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 002a46d1faa1..11d5aa123f17 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -182,7 +182,6 @@ struct svcxprt_rdma {
 /* The default ORD value is based on two outstanding full-size writes with a
  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
 #define RPCRDMA_ORD             (64/4)
-#define RPCRDMA_SQ_DEPTH_MULT   8
 #define RPCRDMA_MAX_REQUESTS    32
 #define RPCRDMA_MAX_REQ_SIZE    4096
 

commit 6e6092ca305ad785c605d7e313727aad96c228a5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:05:44 2017 -0400

    svcrdma: Add svc_rdma_map_reply_hdr()
    
    Introduce a helper to DMA-map a reply's transport header before
    sending it. This will in part replace the map vector cache.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 287db5c179d8..002a46d1faa1 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -228,6 +228,9 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 /* svc_rdma_sendto.c */
 extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
 			    struct svc_rdma_req_map *, bool);
+extern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,
+				  struct svc_rdma_op_ctxt *ctxt,
+				  __be32 *rdma_resp, unsigned int len);
 extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
 				 struct svc_rdma_op_ctxt *ctxt,
 				 int num_sge, u32 inv_rkey);

commit 17f5f7f506aaca985b95df7ef7fc2ff49c36a8e9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:05:36 2017 -0400

    svcrdma: Move send_wr to svc_rdma_op_ctxt
    
    Clean up: Move the ib_send_wr off the stack, and move common code
    to post a Send Work Request into a helper.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index b105f73e3ca2..287db5c179d8 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -85,6 +85,7 @@ struct svc_rdma_op_ctxt {
 	enum dma_data_direction direction;
 	int count;
 	unsigned int mapped_sges;
+	struct ib_send_wr send_wr;
 	struct ib_sge sge[RPCSVC_MAXPAGES];
 	struct page *pages[RPCSVC_MAXPAGES];
 };
@@ -227,6 +228,9 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 /* svc_rdma_sendto.c */
 extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
 			    struct svc_rdma_req_map *, bool);
+extern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,
+				 struct svc_rdma_op_ctxt *ctxt,
+				 int num_sge, u32 inv_rkey);
 extern int svc_rdma_sendto(struct svc_rqst *);
 extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				int);

commit a3ab867fa64f9aedb3b01d570db5b43d2fc355fc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Feb 7 11:58:56 2017 -0500

    svcrdma: Combine list fields in struct svc_rdma_op_ctxt
    
    Clean up: The free list and the dto_q list fields are never used at
    the same time. Reduce the size of struct svc_rdma_op_ctxt by
    combining these fields.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f77a7bc1612c..b105f73e3ca2 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -70,7 +70,7 @@ extern atomic_t rdma_stat_sq_prod;
  * completes.
  */
 struct svc_rdma_op_ctxt {
-	struct list_head free;
+	struct list_head list;
 	struct svc_rdma_op_ctxt *read_hdr;
 	struct svc_rdma_fastreg_mr *frmr;
 	int hdr_count;
@@ -78,7 +78,6 @@ struct svc_rdma_op_ctxt {
 	struct ib_cqe cqe;
 	struct ib_cqe reg_cqe;
 	struct ib_cqe inv_cqe;
-	struct list_head dto_q;
 	u32 byte_len;
 	u32 position;
 	struct svcxprt_rdma *xprt;

commit aba7d14ba18c93a2ab37d50b057a885964ef285c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Feb 7 11:58:48 2017 -0500

    svcrdma: Remove unused sc_dto_q field
    
    Clean up. Commit be99bb11400c ("svcrdma: Use new CQ API for
    RPC-over-RDMA server send CQs") removed code that used the sc_dto_q
    field, but neglected to remove sc_dto_q at the same time.
    
    Fixes: be99bb11400c ("svcrdma: Use new CQ API for RPC-over- ...")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 25ecf92cae96..f77a7bc1612c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -172,7 +172,6 @@ struct svcxprt_rdma {
 
 	wait_queue_head_t    sc_send_wait;	/* SQ exhaustion waitlist */
 	unsigned long	     sc_flags;
-	struct list_head     sc_dto_q;		/* DTO tasklet I/O pending Q */
 	struct list_head     sc_read_complete_q;
 	struct work_struct   sc_work;
 };

commit 98fc21d3bfd55a36ce9eb7b32d1ce146f0d1696d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Feb 7 11:58:23 2017 -0500

    svcrdma: Clean up RPC-over-RDMA Reply header encoder
    
    Replace C structure-based XDR decoding with pointer arithmetic.
    Pointer arithmetic is considered more portable, and is used
    throughout the kernel's existing XDR encoders. The gcc optimizer
    generates similar assembler code either way.
    
    Byte-swapping before a memory store on x86 typically results in an
    instruction pipeline stall. Avoid byte-swapping when encoding a new
    header.
    
    svcrdma currently doesn't alter a connection's credit grant value
    after the connection has been accepted, so it is effectively a
    constant. Cache the byte-swapped value in a separate field.
    
    Christoph suggested pulling the header encoding logic into the only
    function that uses it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 551c51816352..25ecf92cae96 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -141,7 +141,8 @@ struct svcxprt_rdma {
 	atomic_t             sc_sq_avail;	/* SQEs ready to be consumed */
 	unsigned int	     sc_sq_depth;	/* Depth of SQ */
 	unsigned int	     sc_rq_depth;	/* Depth of RQ */
-	u32		     sc_max_requests;	/* Forward credits */
+	__be32		     sc_fc_credits;	/* Forward credits */
+	u32		     sc_max_requests;	/* Max requests */
 	u32		     sc_max_bc_requests;/* Backward credits */
 	int                  sc_max_req_size;	/* Size of each RQ WR buf */
 
@@ -214,10 +215,6 @@ extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);
 extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
 extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
 					    __be32, __be64, u32);
-extern void svc_rdma_xdr_encode_reply_header(struct svcxprt_rdma *,
-					     struct rpcrdma_msg *,
-					     struct rpcrdma_msg *,
-					     enum rpcrdma_proc);
 extern unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp);
 
 /* svc_rdma_recvfrom.c */

commit cbaf58032efca401834518b905f528ac912449e4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Feb 7 11:58:15 2017 -0500

    svcrdma: Another sendto chunk list parsing update
    
    Commit 5fdca6531434 ("svcrdma: Renovate sendto chunk list parsing")
    missed a spot. svc_rdma_xdr_get_reply_hdr_len() also assumes the
    Write list has only one Write chunk. There's no harm in making this
    code more general.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 757fb963696c..551c51816352 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -218,7 +218,7 @@ extern void svc_rdma_xdr_encode_reply_header(struct svcxprt_rdma *,
 					     struct rpcrdma_msg *,
 					     struct rpcrdma_msg *,
 					     enum rpcrdma_proc);
-extern int svc_rdma_xdr_get_reply_hdr_len(struct rpcrdma_msg *);
+extern unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp);
 
 /* svc_rdma_recvfrom.c */
 extern int svc_rdma_recvfrom(struct svc_rqst *);

commit 96a58f9c1921f28fab5ed008be791adacb540cc6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:05:07 2016 -0500

    svcrdma: Remove svc_rdma_op_ctxt::wc_status
    
    Clean up: Completion status is already reported in the individual
    completion handlers. Save a few bytes in struct svc_rdma_op_ctxt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 43d7c709d117..757fb963696c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -79,7 +79,6 @@ struct svc_rdma_op_ctxt {
 	struct ib_cqe reg_cqe;
 	struct ib_cqe inv_cqe;
 	struct list_head dto_q;
-	enum ib_wc_status wc_status;
 	u32 byte_len;
 	u32 position;
 	struct svcxprt_rdma *xprt;

commit dd6fd213b05e7a1f590b470500343dd97c3a32c1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:04:58 2016 -0500

    svcrdma: Remove DMA map accounting
    
    Clean up: sc_dma_used is not required for correct operation. It is
    simply a debugging tool to report when svcrdma has leaked DMA maps.
    
    However, manipulating an atomic has a measurable CPU cost, and DMA
    map accounting specific to svcrdma will be meaningless once svcrdma
    is converted to use the new generic r/w API.
    
    A similar kind of debug accounting can be done simply by enabling
    the IOMMU or by using CONFIG_DMA_API_DEBUG, CONFIG_IOMMU_DEBUG, and
    CONFIG_IOMMU_LEAK.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 601cb07aa746..43d7c709d117 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -148,7 +148,6 @@ struct svcxprt_rdma {
 
 	struct ib_pd         *sc_pd;
 
-	atomic_t	     sc_dma_used;
 	spinlock_t	     sc_ctxt_lock;
 	struct list_head     sc_ctxts;
 	int		     sc_ctxt_used;
@@ -200,7 +199,6 @@ static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
 					   struct svc_rdma_op_ctxt *ctxt)
 {
 	ctxt->mapped_sges++;
-	atomic_inc(&rdma->sc_dma_used);
 }
 
 /* svc_rdma_backchannel.c */

commit e4eb42cecc6dc546aac888ee4913d59121e886ee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:04:50 2016 -0500

    svcrdma: Remove BH-disabled spin locking in svc_rdma_send()
    
    svcrdma's current SQ accounting algorithm takes sc_lock and disables
    bottom-halves while posting all RDMA Read, Write, and Send WRs.
    
    This is relatively heavyweight serialization. And note that Write and
    Send are already fully serialized by the xpt_mutex.
    
    Using a single atomic_t should be all that is necessary to guarantee
    that ib_post_send() is called only when there is enough space on the
    send queue. This is what the other RDMA-enabled storage targets do.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 6aef63b9a669..601cb07aa746 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -139,7 +139,7 @@ struct svcxprt_rdma {
 	int                  sc_max_sge_rd;	/* max sge for read target */
 	bool		     sc_snd_w_inv;	/* OK to use Send With Invalidate */
 
-	atomic_t             sc_sq_count;	/* Number of SQ WR on queue */
+	atomic_t             sc_sq_avail;	/* SQEs ready to be consumed */
 	unsigned int	     sc_sq_depth;	/* Depth of SQ */
 	unsigned int	     sc_rq_depth;	/* Depth of RQ */
 	u32		     sc_max_requests;	/* Forward credits */

commit 5fdca6531434c1c1b2d584873afdda52e5ad448c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:04:42 2016 -0500

    svcrdma: Renovate sendto chunk list parsing
    
    The current sendto code appears to support clients that provide only
    one of a Read list, a Write list, or a Reply chunk. My reading of
    that code is that it doesn't support the following cases:
    
     - Read list + Write list
     - Read list + Reply chunk
     - Write list + Reply chunk
     - Read list + Write list + Reply chunk
    
    The protocol allows more than one Read or Write chunk in those
    lists. Some clients do send a Read list and Reply chunk
    simultaneously. NFSv4 WRITE uses a Read list for the data payload,
    and a Reply chunk because the GETATTR result in the reply can
    contain a large object like an ACL.
    
    Generalize one of the sendto code paths needed to support all of
    the above cases, and attempt to ensure that only one pass is done
    through the RPC Call's transport header to gather chunk list
    information for building the reply.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cc3ae16eac68..6aef63b9a669 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -236,8 +236,6 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
 			    struct svc_rdma_req_map *, bool);
 extern int svc_rdma_sendto(struct svc_rqst *);
-extern struct rpcrdma_read_chunk *
-	svc_rdma_get_read_chunk(struct rpcrdma_msg *);
 extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				int);
 

commit 25d55296dd3eac23adb2ae46b67b65bf73b22fb2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Sep 13 10:53:23 2016 -0400

    svcrdma: support Remote Invalidation
    
    Support Remote Invalidation. A private message is exchanged with
    the client upon RDMA transport connect that indicates whether
    Send With Invalidation may be used by the server to send RPC
    replies. The invalidate_rkey is arbitrarily chosen from among
    rkeys present in the RPC-over-RDMA header's chunk lists.
    
    Send With Invalidate improves performance only when clients can
    recognize, while processing an RPC reply, that an rkey has already
    been invalidated. That has been submitted as a separate change.
    
    In the future, the RPC-over-RDMA protocol might support Remote
    Invalidation properly. The protocol needs to enable signaling
    between peers to indicate when Remote Invalidation can be used
    for each individual RPC.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 3584bc8864c4..cc3ae16eac68 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -137,6 +137,7 @@ struct svcxprt_rdma {
 	int		     sc_ord;		/* RDMA read limit */
 	int                  sc_max_sge;
 	int                  sc_max_sge_rd;	/* max sge for read target */
+	bool		     sc_snd_w_inv;	/* OK to use Send With Invalidate */
 
 	atomic_t             sc_sq_count;	/* Number of SQ WR on queue */
 	unsigned int	     sc_sq_depth;	/* Depth of SQ */

commit cace564f8b6260e806f5e28d7f192fd0e0c603ed
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Sep 13 10:52:50 2016 -0400

    svcrdma: Tail iovec leaves an orphaned DMA mapping
    
    The ctxt's count field is overloaded to mean the number of pages in
    the ctxt->page array and the number of SGEs in the ctxt->sge array.
    Typically these two numbers are the same.
    
    However, when an inline RPC reply is constructed from an xdr_buf
    with a tail iovec, the head and tail often occupy the same page,
    but each are DMA mapped independently. In that case, ->count equals
    the number of pages, but it does not equal the number of SGEs.
    There's one more SGE, for the tail iovec. Hence there is one more
    DMA mapping than there are pages in the ctxt->page array.
    
    This isn't a real problem until the server's iommu is enabled. Then
    each RPC reply that has content in that iovec orphans a DMA mapping
    that consists of real resources.
    
    krb5i and krb5p always populate that tail iovec. After a couple
    million sent krb5i/p RPC replies, the NFS server starts behaving
    erratically. Reboot is needed to clear the problem.
    
    Fixes: 9d11b51ce7c1 ("svcrdma: Fix send_reply() scatter/gather set-up")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d6917b896d3a..3584bc8864c4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -86,6 +86,7 @@ struct svc_rdma_op_ctxt {
 	unsigned long flags;
 	enum dma_data_direction direction;
 	int count;
+	unsigned int mapped_sges;
 	struct ib_sge sge[RPCSVC_MAXPAGES];
 	struct page *pages[RPCSVC_MAXPAGES];
 };
@@ -193,6 +194,14 @@ struct svcxprt_rdma {
 
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 
+/* Track DMA maps for this transport and context */
+static inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,
+					   struct svc_rdma_op_ctxt *ctxt)
+{
+	ctxt->mapped_sges++;
+	atomic_inc(&rdma->sc_dma_used);
+}
+
 /* svc_rdma_backchannel.c */
 extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    struct rpcrdma_msg *rmsgp,

commit d9e4084f6c9746e51a78a4d7ebf4983023289b32
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 4 10:53:47 2016 -0400

    svcrdma: Generalize svc_rdma_xdr_decode_req()
    
    Clean up: Pass in just the piece of the svc_rqst that is needed
    here.
    
    While we're in the area, add an informative documenting comment.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 3081339968c3..d6917b896d3a 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -199,7 +199,7 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    struct xdr_buf *rcvbuf);
 
 /* svc_rdma_marshal.c */
-extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg *, struct svc_rqst *);
+extern int svc_rdma_xdr_decode_req(struct xdr_buf *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 				     struct rpcrdma_msg *,
 				     enum rpcrdma_errcode, __be32 *);

commit be99bb11400ce02552c35a6d3bf054de393ce30e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:07:22 2016 -0500

    svcrdma: Use new CQ API for RPC-over-RDMA server send CQs
    
    Calling ib_poll_cq() to sort through WCs during a completion is a
    common pattern amongst RDMA consumers. Since commit 14d3a3b2498e
    ("IB: add a proper completion queue abstraction"), WC sorting can
    be handled by the IB core.
    
    By converting to this new API, svcrdma is made a better neighbor to
    other RDMA consumers, as it allows the core to schedule the delivery
    of completions more fairly amongst all active consumers.
    
    This new API also aims each completion at a function that is
    specific to the WR's opcode. Thus the ctxt->wr_op field and the
    switch in process_context is replaced by a set of methods that
    handle each completion type.
    
    Because each ib_cqe carries a pointer to a completion method, the
    core can now post operations on a consumer's QP, and handle the
    completions itself.
    
    The server's rdma_stat_sq_poll and rdma_stat_sq_prod metrics are no
    longer updated.
    
    As a clean up, the cq_event_handler, the dto_tasklet, and all
    associated locking is removed, as they are no longer referenced or
    used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cf79ab86d3d4..3081339968c3 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -76,8 +76,9 @@ struct svc_rdma_op_ctxt {
 	int hdr_count;
 	struct xdr_buf arg;
 	struct ib_cqe cqe;
+	struct ib_cqe reg_cqe;
+	struct ib_cqe inv_cqe;
 	struct list_head dto_q;
-	enum ib_wr_opcode wr_op;
 	enum ib_wc_status wc_status;
 	u32 byte_len;
 	u32 position;
@@ -175,7 +176,6 @@ struct svcxprt_rdma {
 	struct work_struct   sc_work;
 };
 /* sc_flags */
-#define RDMAXPRT_SQ_PENDING	2
 #define RDMAXPRT_CONN_PENDING	3
 
 #define RPCRDMA_LISTEN_BACKLOG  10
@@ -232,6 +232,11 @@ extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				int);
 
 /* svc_rdma_transport.c */
+extern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);
+extern void svc_rdma_wc_write(struct ib_cq *, struct ib_wc *);
+extern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);
+extern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);
+extern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);

commit 8bd5ba86d9ba7169e137fc4f32c553080c056a02
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:07:13 2016 -0500

    svcrdma: Use new CQ API for RPC-over-RDMA server receive CQs
    
    Calling ib_poll_cq() to sort through WCs during a completion is a
    common pattern amongst RDMA consumers. Since commit 14d3a3b2498e
    ("IB: add a proper completion queue abstraction"), WC sorting can
    be handled by the IB core.
    
    By converting to this new API, svcrdma is made a better neighbor to
    other RDMA consumers, as it allows the core to schedule the delivery
    of completions more fairly amongst all active consumers.
    
    Because each ib_cqe carries a pointer to a completion method, the
    core can now post operations on a consumer's QP, and handle the
    completions itself.
    
    svcrdma receive completions no longer use the dto_tasklet. Each
    polled Receive WC is now handled individually in soft IRQ context.
    
    The server transport's rdma_stat_rq_poll and rdma_stat_rq_prod
    metrics are no longer updated.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c2b0d95602d8..cf79ab86d3d4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -75,6 +75,7 @@ struct svc_rdma_op_ctxt {
 	struct svc_rdma_fastreg_mr *frmr;
 	int hdr_count;
 	struct xdr_buf arg;
+	struct ib_cqe cqe;
 	struct list_head dto_q;
 	enum ib_wr_opcode wr_op;
 	enum ib_wc_status wc_status;
@@ -174,7 +175,6 @@ struct svcxprt_rdma {
 	struct work_struct   sc_work;
 };
 /* sc_flags */
-#define RDMAXPRT_RQ_PENDING	1
 #define RDMAXPRT_SQ_PENDING	2
 #define RDMAXPRT_CONN_PENDING	3
 

commit f3ea53fb3bc3908b6e9ef39e53a75b55df7f78f8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:06:47 2016 -0500

    svcrdma: Use correct XID in error replies
    
    When constructing an error reply, svc_rdma_xdr_encode_error()
    needs to view the client's request message so it can get the
    failing request's XID.
    
    svc_rdma_xdr_decode_req() is supposed to return a pointer to the
    client's request header. But if it fails to decode the client's
    message (and thus an error reply is needed) it does not return the
    pointer. The server then sends a bogus XID in the error reply.
    
    Instead, unconditionally generate the pointer to the client's header
    in svc_rdma_recvfrom(), and pass that pointer to both functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Tested-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 42e852230a03..c2b0d95602d8 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -199,7 +199,7 @@ extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
 				    struct xdr_buf *rcvbuf);
 
 /* svc_rdma_marshal.c */
-extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
+extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg *, struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 				     struct rpcrdma_msg *,
 				     enum rpcrdma_errcode, __be32 *);

commit a6081b82c533d78041acb76738716aa7dafb339a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:06:38 2016 -0500

    svcrdma: Make RDMA_ERROR messages work
    
    Fix several issues with svc_rdma_send_error():
    
     - Post a receive buffer to replace the one that was consumed by
       the incoming request
     - Posting a send should use DMA_TO_DEVICE, not DMA_FROM_DEVICE
     - No need to put_page _and_ free pages in svc_rdma_put_context
     - Make sure the sge is set up completely in case the error
       path goes through svc_rdma_unmap_dma()
     - Replace the use of ENOSYS, which has a reserved meaning
    
    Related fixes in svc_rdma_recvfrom():
    
     - Don't leak the ctxt associated with the incoming request
     - Don't close the connection after sending an error reply
     - Let svc_rdma_send_error() figure out the right header error code
    
    As a last clean up, move svc_rdma_send_error() to svc_rdma_sendto.c
    with other similar functions. There is some common logic in these
    functions that could someday be combined to reduce code duplication.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Tested-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index aef47dd2bd1a..42e852230a03 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -228,11 +228,11 @@ extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
 extern int svc_rdma_sendto(struct svc_rqst *);
 extern struct rpcrdma_read_chunk *
 	svc_rdma_get_read_chunk(struct rpcrdma_msg *);
+extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
+				int);
 
 /* svc_rdma_transport.c */
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
-extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
-				enum rpcrdma_errcode);
 extern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);

commit bf36387ad394ad4fc93ad85fdd4a95dfa583556a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:06:20 2016 -0500

    svcrdma: svc_rdma_post_recv() should close connection on error
    
    Clean up: Most svc_rdma_post_recv() call sites close the transport
    connection when a receive cannot be posted. Wrap that in a common
    helper.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Tested-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 40b678584041..aef47dd2bd1a 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -234,6 +234,7 @@ extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				enum rpcrdma_errcode);
 extern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);
+extern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);

commit f6763c29ab86c3ee27760a06e07bbeab47635b61
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:05:54 2016 -0500

    svcrdma: Do not send Write chunk XDR pad with inline content
    
    The NFS server's XDR encoders adds an XDR pad for content in the
    xdr_buf page list at the beginning of the xdr_buf's tail buffer.
    
    On RDMA transports, Write chunks are sent separately and without an
    XDR pad.
    
    If a Write chunk is being sent, strip off the pad in the tail buffer
    so that inline content following the Write chunk remains XDR-aligned
    when it is sent to the client.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=294
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 5322fea6fe4c..40b678584041 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -224,7 +224,7 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 
 /* svc_rdma_sendto.c */
 extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
-			    struct svc_rdma_req_map *);
+			    struct svc_rdma_req_map *, bool);
 extern int svc_rdma_sendto(struct svc_rqst *);
 extern struct rpcrdma_read_chunk *
 	svc_rdma_get_read_chunk(struct rpcrdma_msg *);

commit 5fe1043da84887369d32459514f2c7d98ff37936
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Jan 7 23:53:41 2016 -0800

    svc_rdma: use local_dma_lkey
    
    We now alwasy have a per-PD local_dma_lkey available.  Make use of that
    fact in svc_rdma and stop registering our own MR.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Acked-by: J. Bruce Fields <bfields@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index b13513a0caf4..5322fea6fe4c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -156,13 +156,11 @@ struct svcxprt_rdma {
 	struct ib_qp         *sc_qp;
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
-	struct ib_mr         *sc_phys_mr;	/* MR for server memory */
 	int		     (*sc_reader)(struct svcxprt_rdma *,
 					  struct svc_rqst *,
 					  struct svc_rdma_op_ctxt *,
 					  int *, u32 *, u32, u32, u64, bool);
 	u32		     sc_dev_caps;	/* distilled device caps */
-	u32		     sc_dma_lkey;	/* local dma key */
 	unsigned int	     sc_frmr_pg_list_len;
 	struct list_head     sc_frmr_q;
 	spinlock_t	     sc_frmr_q_lock;

commit 5d252f90a800cee5bc57c76d636ae60464f7a887
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:50:10 2016 -0500

    svcrdma: Add class for RDMA backwards direction transport
    
    To support the server-side of an NFSv4.1 backchannel on RDMA
    connections, add a transport class that enables backward
    direction messages on an existing forward channel connection.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 9a2c418dc690..b13513a0caf4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -195,6 +195,11 @@ struct svcxprt_rdma {
 
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 
+/* svc_rdma_backchannel.c */
+extern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,
+				    struct rpcrdma_msg *rmsgp,
+				    struct xdr_buf *rcvbuf);
+
 /* svc_rdma_marshal.c */
 extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,

commit 03fe9931536fe4782e9e34f7f499d588acd2015b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:50:02 2016 -0500

    svcrdma: Define maximum number of backchannel requests
    
    Extra resources for handling backchannel requests have to be
    pre-allocated when a transport instance is created. Set up
    additional fields in svcxprt_rdma to track these resources.
    
    The max_requests fields are elements of the RPC-over-RDMA
    protocol, so they should be u32. To ensure that unsigned
    arithmetic is used everywhere, some other fields in the
    svcxprt_rdma struct are updated.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index aeffa30655ce..9a2c418dc690 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -51,6 +51,7 @@
 /* RPC/RDMA parameters and stats */
 extern unsigned int svcrdma_ord;
 extern unsigned int svcrdma_max_requests;
+extern unsigned int svcrdma_max_bc_requests;
 extern unsigned int svcrdma_max_req_size;
 
 extern atomic_t rdma_stat_recv;
@@ -134,10 +135,11 @@ struct svcxprt_rdma {
 	int                  sc_max_sge;
 	int                  sc_max_sge_rd;	/* max sge for read target */
 
-	int                  sc_sq_depth;	/* Depth of SQ */
 	atomic_t             sc_sq_count;	/* Number of SQ WR on queue */
-
-	int                  sc_max_requests;	/* Depth of RQ */
+	unsigned int	     sc_sq_depth;	/* Depth of SQ */
+	unsigned int	     sc_rq_depth;	/* Depth of RQ */
+	u32		     sc_max_requests;	/* Forward credits */
+	u32		     sc_max_bc_requests;/* Backward credits */
 	int                  sc_max_req_size;	/* Size of each RQ WR buf */
 
 	struct ib_pd         *sc_pd;
@@ -186,6 +188,11 @@ struct svcxprt_rdma {
 #define RPCRDMA_MAX_REQUESTS    32
 #define RPCRDMA_MAX_REQ_SIZE    4096
 
+/* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our
+ * current NFSv4.1 implementation supports one backchannel slot.
+ */
+#define RPCRDMA_MAX_BC_REQUESTS	2
+
 #define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
 
 /* svc_rdma_marshal.c */

commit ba986c96f907a513215fb7f1c0a89261c97251ca
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:53 2016 -0500

    svcrdma: Make map_xdr non-static
    
    Pre-requisite to use map_xdr in the backchannel code.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 729ff356c18a..aeffa30655ce 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -213,6 +213,8 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 				u32, u32, u64, bool);
 
 /* svc_rdma_sendto.c */
+extern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,
+			    struct svc_rdma_req_map *);
 extern int svc_rdma_sendto(struct svc_rqst *);
 extern struct rpcrdma_read_chunk *
 	svc_rdma_get_read_chunk(struct rpcrdma_msg *);

commit 39b09a1a121cb22820c374f4e92f7ca34be1b75d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:37 2016 -0500

    svcrdma: Add gfp flags to svc_rdma_post_recv()
    
    svc_rdma_post_recv() allocates pages for receive buffers on-demand.
    It uses GFP_KERNEL so the allocator tries hard, and may sleep. But
    I'm about to add a call to svc_rdma_post_recv() from a function
    that may not sleep.
    
    Since all svc_rdma_post_recv() call sites can tolerate its failure,
    allow it to fail if the page allocator returns nothing. Longer term,
    receive buffers, being a finite resource per-connection, should be
    pre-allocated and re-used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 141edbbb73b3..729ff356c18a 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -221,7 +221,7 @@ extern struct rpcrdma_read_chunk *
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				enum rpcrdma_errcode);
-extern int svc_rdma_post_recv(struct svcxprt_rdma *);
+extern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);

commit 71810ef3271d1a06f7002c55c7e354d8c3233762
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:28 2016 -0500

    svcrdma: Remove unused req_map and ctxt kmem_caches
    
    Clean up.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 05bf4febad44..141edbbb73b3 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -242,6 +242,7 @@ extern struct svc_xprt_class svc_rdma_bc_class;
 #endif
 
 /* svc_rdma.c */
+extern struct workqueue_struct *svc_rdma_wq;
 extern int svc_rdma_init(void);
 extern void svc_rdma_cleanup(void);
 

commit 2fe81b239dbb00d0a2fd8858ac9dd4ef4a8841ee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:20 2016 -0500

    svcrdma: Improve allocation of struct svc_rdma_req_map
    
    To ensure this allocation cannot fail and will not sleep,
    pre-allocate the req_map structures per-connection.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index be2804b72cd8..05bf4febad44 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -113,6 +113,7 @@ struct svc_rdma_fastreg_mr {
 	struct list_head frmr_list;
 };
 struct svc_rdma_req_map {
+	struct list_head free;
 	unsigned long count;
 	union {
 		struct kvec sge[RPCSVC_MAXPAGES];
@@ -145,6 +146,8 @@ struct svcxprt_rdma {
 	spinlock_t	     sc_ctxt_lock;
 	struct list_head     sc_ctxts;
 	int		     sc_ctxt_used;
+	spinlock_t	     sc_map_lock;
+	struct list_head     sc_maps;
 
 	struct list_head     sc_rq_dto_q;
 	spinlock_t	     sc_rq_dto_lock;
@@ -223,8 +226,9 @@ extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
-extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
-extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
+extern struct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *);
+extern void svc_rdma_put_req_map(struct svcxprt_rdma *,
+				 struct svc_rdma_req_map *);
 extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
 extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
 			      struct svc_rdma_fastreg_mr *);

commit cc886c9ff1607eda04062bdcec963e2f8e6a3eb1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:12 2016 -0500

    svcrdma: Improve allocation of struct svc_rdma_op_ctxt
    
    When the maximum payload size of NFS READ and WRITE was increased
    by commit cc9a903d915c ("svcrdma: Change maximum server payload back
    to RPCSVC_MAXPAYLOAD"), the size of struct svc_rdma_op_ctxt
    increased to over 6KB (on x86_64). That makes allocating one of
    these from a kmem_cache more likely to fail in situations when
    system memory is exhausted.
    
    Since I'm about to add a caller where this allocation must always
    work _and_ it cannot sleep, pre-allocate ctxts for each connection.
    
    Another motivation for this change is that NFSv4.x servers are
    required by specification not to drop NFS requests. Pre-allocating
    memory resources reduces the likelihood of a drop.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f869807a0d0e..be2804b72cd8 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -69,6 +69,7 @@ extern atomic_t rdma_stat_sq_prod;
  * completes.
  */
 struct svc_rdma_op_ctxt {
+	struct list_head free;
 	struct svc_rdma_op_ctxt *read_hdr;
 	struct svc_rdma_fastreg_mr *frmr;
 	int hdr_count;
@@ -141,7 +142,10 @@ struct svcxprt_rdma {
 	struct ib_pd         *sc_pd;
 
 	atomic_t	     sc_dma_used;
-	atomic_t	     sc_ctxt_used;
+	spinlock_t	     sc_ctxt_lock;
+	struct list_head     sc_ctxts;
+	int		     sc_ctxt_used;
+
 	struct list_head     sc_rq_dto_q;
 	spinlock_t	     sc_rq_dto_lock;
 	struct ib_qp         *sc_qp;

commit e6604ecb70d4b1dbc0372c6518b51c25c4b135a1
Merge: 9d74288ca792 941c3ff3102c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 9 18:11:22 2015 -0800

    Merge tag 'nfs-for-4.4-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      New features:
       - RDMA client backchannel from Chuck
       - Support for NFSv4.2 file CLONE using the btrfs ioctl
    
      Bugfixes + cleanups:
       - Move socket data receive out of the bottom halves and into a
         workqueue
       - Refactor NFSv4 error handling so synchronous and asynchronous RPC
         handles errors identically.
       - Fix a panic when blocks or object layouts reads return a bad data
         length
       - Fix nfsroot so it can handle a 1024 byte long path.
       - Fix bad usage of page offset in bl_read_pagelist
       - Various NFSv4 callback cleanups+fixes
       - Fix GETATTR bitmap verification
       - Support hexadecimal number for sunrpc debug sysctl files"
    
    * tag 'nfs-for-4.4-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (53 commits)
      Sunrpc: Supports hexadecimal number for sysctl files of sunrpc debug
      nfs: Fix GETATTR bitmap verification
      nfs: Remove unused xdr page offsets in getacl/setacl arguments
      fs/nfs: remove unnecessary new_valid_dev check
      SUNRPC: fix variable type
      NFS: Enable client side NFSv4.1 backchannel to use other transports
      pNFS/flexfiles: Add support for FF_FLAGS_NO_IO_THRU_MDS
      pNFS/flexfiles: When mirrored, retry failed reads by switching mirrors
      SUNRPC: Remove the TCP-only restriction in bc_svc_process()
      svcrdma: Add backward direction service for RPC/RDMA transport
      xprtrdma: Handle incoming backward direction RPC calls
      xprtrdma: Add support for sending backward direction RPC replies
      xprtrdma: Pre-allocate Work Requests for backchannel
      xprtrdma: Pre-allocate backward rpc_rqst and send/receive buffers
      SUNRPC: Abstract backchannel operations
      xprtrdma: Saving IRQs no longer needed for rb_lock
      xprtrdma: Remove reply tasklet
      xprtrdma: Use workqueue to process RPC/RDMA replies
      xprtrdma: Replace send and receive arrays
      xprtrdma: Refactor reply handler error handling
      ...

commit 9468431962616c2449d47c482208a5967e011bf9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:28:16 2015 -0400

    svcrdma: Add backward direction service for RPC/RDMA transport
    
    On NFSv4.1 mount points, the Linux NFS client uses this transport
    endpoint to receive backward direction calls and route replies back
    to the NFSv4.1 server.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: "J. Bruce Fields" <bfields@fieldses.org>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 7ccc961f33e9..fb4013edcf57 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -228,9 +228,13 @@ extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
 			      struct svc_rdma_fastreg_mr *);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
-extern struct svc_xprt_class svc_rdma_class;
 extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);
 
+extern struct svc_xprt_class svc_rdma_class;
+#ifdef CONFIG_SUNRPC_BACKCHANNEL
+extern struct svc_xprt_class svc_rdma_bc_class;
+#endif
+
 /* svc_rdma.c */
 extern int svc_rdma_init(void);
 extern void svc_rdma_cleanup(void);

commit 412a15c0fe537c59c794d4e8134580b9cb984a0c
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:36 2015 +0300

    svcrdma: Port to new memory registration API
    
    Instead of maintaining a fastreg page list, keep an sg table
    and convert an array of pages to a sg list. Then call ib_map_mr_sg
    and construct ib_reg_wr.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Selvin Xavier <selvin.xavier@avagotech.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 7ccc961f33e9..1e4438ea2380 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -105,11 +105,9 @@ struct svc_rdma_chunk_sge {
 };
 struct svc_rdma_fastreg_mr {
 	struct ib_mr *mr;
-	void *kva;
-	struct ib_fast_reg_page_list *page_list;
-	int page_list_len;
+	struct scatterlist *sg;
+	int sg_nents;
 	unsigned long access_flags;
-	unsigned long map_len;
 	enum dma_data_direction direction;
 	struct list_head frmr_list;
 };

commit 26d2177e977c912863ac04f6c1a967e793ca3a56
Merge: a794b4f32921 d1178cbcdcf9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 9 08:33:31 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull inifiniband/rdma updates from Doug Ledford:
     "This is a fairly sizeable set of changes.  I've put them through a
      decent amount of testing prior to sending the pull request due to
      that.
    
      There are still a few fixups that I know are coming, but I wanted to
      go ahead and get the big, sizable chunk into your hands sooner rather
      than waiting for those last few fixups.
    
      Of note is the fact that this creates what is intended to be a
      temporary area in the drivers/staging tree specifically for some
      cleanups and additions that are coming for the RDMA stack.  We
      deprecated two drivers (ipath and amso1100) and are waiting to hear
      back if we can deprecate another one (ehca).  We also put Intel's new
      hfi1 driver into this area because it needs to be refactored and a
      transfer library created out of the factored out code, and then it and
      the qib driver and the soft-roce driver should all be modified to use
      that library.
    
      I expect drivers/staging/rdma to be around for three or four kernel
      releases and then to go away as all of the work is completed and final
      deletions of deprecated drivers are done.
    
      Summary of changes for 4.3:
    
       - Create drivers/staging/rdma
       - Move amso1100 driver to staging/rdma and schedule for deletion
       - Move ipath driver to staging/rdma and schedule for deletion
       - Add hfi1 driver to staging/rdma and set TODO for move to regular
         tree
       - Initial support for namespaces to be used on RDMA devices
       - Add RoCE GID table handling to the RDMA core caching code
       - Infrastructure to support handling of devices with differing read
         and write scatter gather capabilities
       - Various iSER updates
       - Kill off unsafe usage of global mr registrations
       - Update SRP driver
       - Misc  mlx4 driver updates
       - Support for the mr_alloc verb
       - Support for a netlink interface between kernel and user space cache
         daemon to speed path record queries and route resolution
       - Ininitial support for safe hot removal of verbs devices"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (136 commits)
      IB/ipoib: Suppress warning for send only join failures
      IB/ipoib: Clean up send-only multicast joins
      IB/srp: Fix possible protection fault
      IB/core: Move SM class defines from ib_mad.h to ib_smi.h
      IB/core: Remove unnecessary defines from ib_mad.h
      IB/hfi1: Add PSM2 user space header to header_install
      IB/hfi1: Add CSRs for CONFIG_SDMA_VERBOSITY
      mlx5: Fix incorrect wc pkey_index assignment for GSI messages
      IB/mlx5: avoid destroying a NULL mr in reg_user_mr error flow
      IB/uverbs: reject invalid or unknown opcodes
      IB/cxgb4: Fix if statement in pick_local_ip6adddrs
      IB/sa: Fix rdma netlink message flags
      IB/ucma: HW Device hot-removal support
      IB/mlx4_ib: Disassociate support
      IB/uverbs: Enable device removal when there are active user space applications
      IB/uverbs: Explicitly pass ib_dev to uverbs commands
      IB/uverbs: Fix race between ib_uverbs_open and remove_one
      IB/uverbs: Fix reference counting usage of event files
      IB/core: Make ib_dealloc_pd return void
      IB/srp: Create an insecure all physical rkey only if needed
      ...

commit bc3fe2e3769874dfa8674791e84c4a901ba9e48b
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jul 27 18:10:12 2015 -0500

    svcrdma: Use max_sge_rd for destination read depths
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cb94ee4181d4..83211bc9219e 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -132,6 +132,7 @@ struct svcxprt_rdma {
 	struct list_head     sc_accept_q;	/* Conn. waiting accept */
 	int		     sc_ord;		/* RDMA read limit */
 	int                  sc_max_sge;
+	int                  sc_max_sge_rd;	/* max sge for read target */
 
 	int                  sc_sq_depth;	/* Depth of SQ */
 	atomic_t             sc_sq_count;	/* Number of SQ WR on queue */

commit cc9a903d915c21626b6b2fbf8ed0ff16a7f82210
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Aug 7 16:55:46 2015 -0400

    svcrdma: Change maximum server payload back to RPCSVC_MAXPAYLOAD
    
    Both commit 0380a3f375 ("svcrdma: Add a separate "max data segs"
    macro for svcrdma") and commit 7e5be28827bf ("svcrdma: advertise
    the correct max payload") are incorrect. This commit reverts both
    changes, restoring the server's maximum payload size to 1MB.
    
    Commit 7e5be28827bf based the server's maximum payload on the
    _client's_ RPCRDMA_MAX_DATA_SEGS value. That was wrong.
    
    Commit 0380a3f375 tried to fix this so that the client maximum
    payload size could be raised without affecting the server, but
    managed to confuse matters more on the server side.
    
    More importantly, limiting the advertised maximum payload size was
    meant to be a workaround, not the actual fix. We need to revisit
    
      https://bugzilla.linux-nfs.org/show_bug.cgi?id=270
    
    A Linux client on a platform with 64KB pages can overrun and crash
    an x86_64 NFS/RDMA server when the r/wsize is 1MB. An x86/64 Linux
    client seems to work fine using 1MB reads and writes when the Linux
    server's maximum payload size is restored to 1MB.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=270
    Fixes: 0380a3f375 ("svcrdma: Add a separate "max data segs" macro")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 13af61b70417..d5ee6d8b7c58 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -172,13 +172,6 @@ struct svcxprt_rdma {
 #define RDMAXPRT_SQ_PENDING	2
 #define RDMAXPRT_CONN_PENDING	3
 
-#define RPCRDMA_MAX_SVC_SEGS	(64)	/* server max scatter/gather */
-#if RPCSVC_MAXPAYLOAD < (RPCRDMA_MAX_SVC_SEGS << PAGE_SHIFT)
-#define RPCRDMA_MAXPAYLOAD	RPCSVC_MAXPAYLOAD
-#else
-#define RPCRDMA_MAXPAYLOAD	(RPCRDMA_MAX_SVC_SEGS << PAGE_SHIFT)
-#endif
-
 #define RPCRDMA_LISTEN_BACKLOG  10
 /* The default ORD value is based on two outstanding full-size writes with a
  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
@@ -187,6 +180,8 @@ struct svcxprt_rdma {
 #define RPCRDMA_MAX_REQUESTS    32
 #define RPCRDMA_MAX_REQ_SIZE    4096
 
+#define RPCSVC_MAXPAYLOAD_RDMA	RPCSVC_MAXPAYLOAD
+
 /* svc_rdma_marshal.c */
 extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,

commit 31193fe5f6fb616711323f5d74ee5bb92aacba4a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jul 9 16:45:37 2015 -0400

    svcrdma: Remove svc_rdma_fastreg()
    
    Commit 0bf4828983df ("svcrdma: refactor marshalling logic") removed
    the last call site for svc_rdma_fastreg().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index ca4d86a6c947..13af61b70417 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -227,7 +227,6 @@ extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
 extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
 extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
-extern int svc_rdma_fastreg(struct svcxprt_rdma *, struct svc_rdma_fastreg_mr *);
 extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
 extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
 			      struct svc_rdma_fastreg_mr *);

commit 10dc4512185741a298cd7bc87e9968944f31a50d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jul 9 16:45:28 2015 -0400

    svcrdma: Clean up svc_rdma_get_reply_array()
    
    Kernel coding conventions frown upon having large nontrivial
    functions in header files, and the preference these days is to
    allow the compiler to make inlining decisions if possible.
    
    As these functions are re-homed into a .c file, be sure that
    comparisons with fields in struct rpcrdma_msg are with be32
    constants.
    
    This is a refactoring change; no behavior change is intended.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index cb94ee4181d4..ca4d86a6c947 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -213,6 +213,8 @@ extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
 
 /* svc_rdma_sendto.c */
 extern int svc_rdma_sendto(struct svc_rqst *);
+extern struct rpcrdma_read_chunk *
+	svc_rdma_get_read_chunk(struct rpcrdma_msg *);
 
 /* svc_rdma_transport.c */
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
@@ -238,83 +240,4 @@ extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);
 extern int svc_rdma_init(void);
 extern void svc_rdma_cleanup(void);
 
-/*
- * Returns the address of the first read chunk or <nul> if no read chunk is
- * present
- */
-static inline struct rpcrdma_read_chunk *
-svc_rdma_get_read_chunk(struct rpcrdma_msg *rmsgp)
-{
-	struct rpcrdma_read_chunk *ch =
-		(struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];
-
-	if (ch->rc_discrim == 0)
-		return NULL;
-
-	return ch;
-}
-
-/*
- * Returns the address of the first read write array element or <nul> if no
- * write array list is present
- */
-static inline struct rpcrdma_write_array *
-svc_rdma_get_write_array(struct rpcrdma_msg *rmsgp)
-{
-	if (rmsgp->rm_body.rm_chunks[0] != 0
-	    || rmsgp->rm_body.rm_chunks[1] == 0)
-		return NULL;
-
-	return (struct rpcrdma_write_array *)&rmsgp->rm_body.rm_chunks[1];
-}
-
-/*
- * Returns the address of the first reply array element or <nul> if no
- * reply array is present
- */
-static inline struct rpcrdma_write_array *
-svc_rdma_get_reply_array(struct rpcrdma_msg *rmsgp)
-{
-	struct rpcrdma_read_chunk *rch;
-	struct rpcrdma_write_array *wr_ary;
-	struct rpcrdma_write_array *rp_ary;
-
-	/* XXX: Need to fix when reply list may occur with read-list and/or
-	 * write list */
-	if (rmsgp->rm_body.rm_chunks[0] != 0 ||
-	    rmsgp->rm_body.rm_chunks[1] != 0)
-		return NULL;
-
-	rch = svc_rdma_get_read_chunk(rmsgp);
-	if (rch) {
-		while (rch->rc_discrim)
-			rch++;
-
-		/* The reply list follows an empty write array located
-		 * at 'rc_position' here. The reply array is at rc_target.
-		 */
-		rp_ary = (struct rpcrdma_write_array *)&rch->rc_target;
-
-		goto found_it;
-	}
-
-	wr_ary = svc_rdma_get_write_array(rmsgp);
-	if (wr_ary) {
-		rp_ary = (struct rpcrdma_write_array *)
-			&wr_ary->
-			wc_array[ntohl(wr_ary->wc_nchunks)].wc_target.rs_length;
-
-		goto found_it;
-	}
-
-	/* No read list, no write list */
-	rp_ary = (struct rpcrdma_write_array *)
-		&rmsgp->rm_body.rm_chunks[2];
-
- found_it:
-	if (rp_ary->wc_discrim == 0)
-		return NULL;
-
-	return rp_ary;
-}
 #endif

commit 0380a3f37540ad0582b3c749a74fc127af914689
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:32 2015 -0400

    svcrdma: Add a separate "max data segs macro for svcrdma
    
    The server and client maximum are architecturally independent.
    Allow changing one without affecting the other.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d26384b22126..cb94ee4181d4 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -172,6 +172,13 @@ struct svcxprt_rdma {
 #define RDMAXPRT_SQ_PENDING	2
 #define RDMAXPRT_CONN_PENDING	3
 
+#define RPCRDMA_MAX_SVC_SEGS	(64)	/* server max scatter/gather */
+#if RPCSVC_MAXPAYLOAD < (RPCRDMA_MAX_SVC_SEGS << PAGE_SHIFT)
+#define RPCRDMA_MAXPAYLOAD	RPCSVC_MAXPAYLOAD
+#else
+#define RPCRDMA_MAXPAYLOAD	(RPCRDMA_MAX_SVC_SEGS << PAGE_SHIFT)
+#endif
+
 #define RPCRDMA_LISTEN_BACKLOG  10
 /* The default ORD value is based on two outstanding full-size writes with a
  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */

commit b7e0b9a965a116341b4ef86ab98ea2843b218271
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:20 2015 -0400

    svcrdma: Replace GFP_KERNEL in a loop with GFP_NOFAIL
    
    At the 2015 LSF/MM, it was requested that memory allocation
    call sites that request GFP_KERNEL allocations in a loop should be
    annotated with __GFP_NOFAIL.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c03ca0a1b743..d26384b22126 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -211,7 +211,6 @@ extern int svc_rdma_sendto(struct svc_rqst *);
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
 extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
 				enum rpcrdma_errcode);
-struct page *svc_rdma_get_page(void);
 extern int svc_rdma_post_recv(struct svcxprt_rdma *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);

commit 30b7e246a6222f1fbad39b1451273375306fe1e2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:10 2015 -0400

    svcrdma: Keep rpcrdma_msg fields in network byte-order
    
    Fields in struct rpcrdma_msg are __be32. Don't byte-swap these
    fields when decoding RPC calls and then swap them back for the
    reply. For the most part, they can be left alone.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 8ad9b6d9d4e0..c03ca0a1b743 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -184,7 +184,7 @@ struct svcxprt_rdma {
 extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 				     struct rpcrdma_msg *,
-				     enum rpcrdma_errcode, u32 *);
+				     enum rpcrdma_errcode, __be32 *);
 extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);
 extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
 extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,

commit da7049f834c3582c1ed1a04889bda5b4121973c0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 13:49:07 2015 -0400

    svcrdma: Remove svc_rdma_xdr_decode_deferred_req()
    
    svc_rdma_xdr_decode_deferred_req() indexes an array with an
    un-byte-swapped value off the wire. Fortunately this function
    isn't used anywhere, so simply remove it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index df8edf8ec914..8ad9b6d9d4e0 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -182,7 +182,6 @@ struct svcxprt_rdma {
 
 /* svc_rdma_marshal.c */
 extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
-extern int svc_rdma_xdr_decode_deferred_req(struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 				     struct rpcrdma_msg *,
 				     enum rpcrdma_errcode, u32 *);

commit 61845143febe6b88349acad4732adc54894009a3
Merge: a26be149facb c23ae6017835
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 12 10:39:41 2015 -0800

    Merge branch 'for-3.20' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd updates from Bruce Fields:
     "The main change is the pNFS block server support from Christoph, which
      allows an NFS client connected to shared disk to do block IO to the
      shared disk in place of NFS reads and writes.  This also requires xfs
      patches, which should arrive soon through the xfs tree, barring
      unexpected problems.  Support for other filesystems is also possible
      if there's interest.
    
      Thanks also to Chuck Lever for continuing work to get NFS/RDMA into
      shape"
    
    * 'for-3.20' of git://linux-nfs.org/~bfields/linux: (32 commits)
      nfsd: default NFSv4.2 to on
      nfsd: pNFS block layout driver
      exportfs: add methods for block layout exports
      nfsd: add trace events
      nfsd: update documentation for pNFS support
      nfsd: implement pNFS layout recalls
      nfsd: implement pNFS operations
      nfsd: make find_any_file available outside nfs4state.c
      nfsd: make find/get/put file available outside nfs4state.c
      nfsd: make lookup/alloc/unhash_stid available outside nfs4state.c
      nfsd: add fh_fsid_match helper
      nfsd: move nfsd_fh_match to nfsfh.h
      fs: add FL_LAYOUT lease type
      fs: track fl_owner for leases
      nfs: add LAYOUT_TYPE_MAX enum value
      nfsd: factor out a helper to decode nfstime4 values
      sunrpc/lockd: fix references to the BKL
      nfsd: fix year-2038 nfs4 state problem
      svcrdma: Handle additional inline content
      svcrdma: Move read list XDR round-up logic
      ...

commit 284f4902a632584e8d73cf7d9363f819adf7240c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:02:13 2015 -0500

    xprtrdma: Modernize htonl and ntohl
    
    Clean up: Replace htonl and ntohl with the be32 equivalents.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 975da754c778..ddfe88f52219 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -63,8 +63,6 @@ extern atomic_t rdma_stat_rq_prod;
 extern atomic_t rdma_stat_sq_poll;
 extern atomic_t rdma_stat_sq_prod;
 
-#define RPCRDMA_VERSION 1
-
 /*
  * Contexts are built when an RDMA request is created and are a
  * record of the resources that can be recovered when the request

commit 0b056c224bea63060ce8a981e84193c93fac6f5d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jan 13 11:03:37 2015 -0500

    svcrdma: Support RDMA_NOMSG requests
    
    Currently the Linux server can not decode RDMA_NOMSG type requests.
    Operations whose length exceeds the fixed size of RDMA SEND buffers,
    like large NFSv4 CREATE(NF4LNK) operations, must be conveyed via
    RDMA_NOMSG.
    
    For an RDMA_MSG type request, the client sends the RPC/RDMA, RPC
    headers, and some or all of the NFS arguments via RDMA SEND.
    
    For an RDMA_NOMSG type request, the client sends just the RPC/RDMA
    header via RDMA SEND. The request's read list contains elements for
    the entire RPC message, including the RPC header.
    
    NFSD expects the RPC/RMDA header and RPC header to be contiguous in
    page zero of the XDR buffer. Add logic in the RDMA READ path to make
    the read list contents land where the server prefers, when the
    incoming message is a type RDMA_NOMSG message.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f161e309f25e..c343a94bc791 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -79,6 +79,7 @@ struct svc_rdma_op_ctxt {
 	enum ib_wr_opcode wr_op;
 	enum ib_wc_status wc_status;
 	u32 byte_len;
+	u32 position;
 	struct svcxprt_rdma *xprt;
 	unsigned long flags;
 	enum dma_data_direction direction;

commit e54524111f51eac1900cf91aca3d38a92a6b11c0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jan 13 11:03:20 2015 -0500

    svcrdma: Plant reader function in struct svcxprt_rdma
    
    The RDMA reader function doesn't change once an svcxprt_rdma is
    instantiated. Instead of checking sc_devcap during every incoming
    RPC, set the reader function once when the connection is accepted.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 2280325e4c88..f161e309f25e 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -150,6 +150,10 @@ struct svcxprt_rdma {
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
 	struct ib_mr         *sc_phys_mr;	/* MR for server memory */
+	int		     (*sc_reader)(struct svcxprt_rdma *,
+					  struct svc_rqst *,
+					  struct svc_rdma_op_ctxt *,
+					  int *, u32 *, u32, u32, u64, bool);
 	u32		     sc_dev_caps;	/* distilled device caps */
 	u32		     sc_dma_lkey;	/* local dma key */
 	unsigned int	     sc_frmr_pg_list_len;
@@ -195,6 +199,12 @@ extern int svc_rdma_xdr_get_reply_hdr_len(struct rpcrdma_msg *);
 
 /* svc_rdma_recvfrom.c */
 extern int svc_rdma_recvfrom(struct svc_rqst *);
+extern int rdma_read_chunk_lcl(struct svcxprt_rdma *, struct svc_rqst *,
+			       struct svc_rdma_op_ctxt *, int *, u32 *,
+			       u32, u32, u64, bool);
+extern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,
+				struct svc_rdma_op_ctxt *, int *, u32 *,
+				u32, u32, u64, bool);
 
 /* svc_rdma_sendto.c */
 extern int svc_rdma_sendto(struct svc_rqst *);

commit 2397aa8b515f7bd77c8d5698170b6a98fdd6721c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jan 13 11:02:54 2015 -0500

    svcrdma: Clean up read chunk counting
    
    The byte_count argument is not used, and the function is called
    only from one place.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 975da754c778..2280325e4c88 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -178,8 +178,6 @@ struct svcxprt_rdma {
 #define RPCRDMA_MAX_REQ_SIZE    4096
 
 /* svc_rdma_marshal.c */
-extern void svc_rdma_rcl_chunk_counts(struct rpcrdma_read_chunk *,
-				      int *, int *);
 extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
 extern int svc_rdma_xdr_decode_deferred_req(struct svc_rqst *);
 extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,

commit d9bb5a43277d2dcc514fa693f741bbc38e2e2271
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 22 17:48:04 2014 -0400

    svcrdma: Double the default credit limit
    
    The RDMA credit limit controls how many concurrent RPCs are allowed
    per connection.
    
    An NFS/RDMA client and server exchange their credit limits in the
    RPC/RDMA headers. The Linux client and the Solaris client and server
    allow 32 credits. The Linux server allows only 16, which limits its
    performance.
    
    Set the server's default credit limit to 32, like the other well-
    known implementations, so the out-of-the-shrinkwrap performance of
    the Linux server is better.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 5cf99a016368..975da754c778 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -174,8 +174,7 @@ struct svcxprt_rdma {
  * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
 #define RPCRDMA_ORD             (64/4)
 #define RPCRDMA_SQ_DEPTH_MULT   8
-#define RPCRDMA_MAX_THREADS     16
-#define RPCRDMA_MAX_REQUESTS    16
+#define RPCRDMA_MAX_REQUESTS    32
 #define RPCRDMA_MAX_REQ_SIZE    4096
 
 /* svc_rdma_marshal.c */

commit 0bf4828983dff062cd502f27ab8644b32774e72e
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Wed May 28 15:12:01 2014 -0500

    svcrdma: refactor marshalling logic
    
    This patch refactors the NFSRDMA server marshalling logic to
    remove the intermediary map structures.  It also fixes an existing bug
    where the NFSRDMA server was not minding the device fast register page
    list length limitations.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 0b8e3e6bdacf..5cf99a016368 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -115,14 +115,13 @@ struct svc_rdma_fastreg_mr {
 	struct list_head frmr_list;
 };
 struct svc_rdma_req_map {
-	struct svc_rdma_fastreg_mr *frmr;
 	unsigned long count;
 	union {
 		struct kvec sge[RPCSVC_MAXPAGES];
 		struct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];
+		unsigned long lkey[RPCSVC_MAXPAGES];
 	};
 };
-#define RDMACTXT_F_FAST_UNREG	1
 #define RDMACTXT_F_LAST_CTXT	2
 
 #define	SVCRDMA_DEVCAP_FAST_REG		1	/* fast mr registration */

commit 1fa9c4440c151c61eb3309579a85aae22c9adb6d
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Tue Feb 21 10:28:04 2012 +0300

    svcrdma: silence a Sparse warning
    
    Sparse complains that the definition function definition and the
    implementation aren't anotated the same way.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d205e9f938c6..0b8e3e6bdacf 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -190,7 +190,7 @@ extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
 extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);
 extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
 extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
-					    u32, u64, u32);
+					    __be32, __be64, u32);
 extern void svc_rdma_xdr_encode_reply_header(struct svcxprt_rdma *,
 					     struct rpcrdma_msg *,
 					     struct rpcrdma_msg *,

commit cec56c8ff5e28f58ff13041dca7853738ae577a1
Author: Tom Tucker <tom@ogc.us>
Date:   Wed Feb 15 11:30:00 2012 -0600

    svcrdma: Cleanup sparse warnings in the svcrdma module
    
    The svcrdma transport was un-marshalling requests in-place. This resulted
    in sparse warnings due to __beXX data containing both NBO and HBO data.
    
    The code has been restructured to do byte-swapping as the header is
    parsed instead of when the header is validated immediately after receipt.
    
    Also moved extern declarations for the workqueue and memory pools to the
    private header file.
    
    Signed-off-by: Tom Tucker <tom@ogc.us>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c14fe86dac59..d205e9f938c6 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -292,7 +292,7 @@ svc_rdma_get_reply_array(struct rpcrdma_msg *rmsgp)
 	if (wr_ary) {
 		rp_ary = (struct rpcrdma_write_array *)
 			&wr_ary->
-			wc_array[wr_ary->wc_nchunks].wc_target.rs_length;
+			wc_array[ntohl(wr_ary->wc_nchunks)].wc_target.rs_length;
 
 		goto found_it;
 	}

commit 146b6df6a537939570c5772ebd7db826fdbd5d82
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Tue Aug 12 15:12:10 2008 -0500

    svcrdma: Modify the RPC recv path to use FRMR when available
    
    RPCRDMA requests that specify a read-list are fetched with RDMA_READ. Using
    an FRMR to map the data sink improves NFSRDMA security on transports that
    place the RDMA_READ data sink LKEY on the wire because the valid lifetime
    of the MR is only the duration of the RDMA_READ. The LKEY is invalidated
    when the last RDMA_READ WR completes.
    
    Mapping the data sink also allows for very large amounts to data to be
    fetched with a single WR, so if the client is also using FRMR, the entire
    RPC read-list can be fetched with a single WR.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 1402d193b393..c14fe86dac59 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -212,6 +212,7 @@ extern int svc_rdma_post_recv(struct svcxprt_rdma *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
+extern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);
 extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
 extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
 extern int svc_rdma_fastreg(struct svcxprt_rdma *, struct svc_rdma_fastreg_mr *);

commit e1183210625cc8e02ce13eec78fb7a246567fc59
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Fri Oct 3 15:22:18 2008 -0500

    svcrdma: Add a service to register a Fast Reg MR with the device
    
    Fast Reg MR introduces a new WR type. Add a service to register the
    region with the adapter and update the completion handling to support
    completions with a NULL WR context.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 34252683671c..1402d193b393 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -214,6 +214,7 @@ extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
 extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
+extern int svc_rdma_fastreg(struct svcxprt_rdma *, struct svc_rdma_fastreg_mr *);
 extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
 extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
 			      struct svc_rdma_fastreg_mr *);

commit 64be8608c163bd480cf5ec4b34366f11e0f3c87f
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Mon Oct 6 14:45:18 2008 -0500

    svcrdma: Add FRMR get/put services
    
    Add services for the allocating, freeing, and unmapping Fast Reg MR. These
    services will be used by the transport connection setup, send and receive
    routines.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 49e458d98945..34252683671c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -214,6 +214,9 @@ extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
 extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
 extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
+extern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);
+extern void svc_rdma_put_frmr(struct svcxprt_rdma *,
+			      struct svc_rdma_fastreg_mr *);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
 extern struct svc_xprt_class svc_rdma_class;

commit 0d3ebb9ae9f9c887518fd4c81a68084111d154d7
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Tue Sep 30 13:06:13 2008 -0500

    svcrdma: Add Fast Reg MR Data Types
    
    Add data types to track Fast Reg Memory Regions. The core data type is
    svc_rdma_fastreg_mr that associates a device MR with a host kva and page
    list. A field is added to the WR context to keep track of the FRMR
    used to map the local memory for an RPC.
    
    An FRMR list and spin lock are added to the transport instance to keep
    track of all FRMR allocated for the transport. Also added are device
    capability flags to indicate what the memory registration
    capabilities are for the underlying device and whether or not fast
    memory registration is supported.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index dc05b54bd3a3..49e458d98945 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -72,6 +72,7 @@ extern atomic_t rdma_stat_sq_prod;
  */
 struct svc_rdma_op_ctxt {
 	struct svc_rdma_op_ctxt *read_hdr;
+	struct svc_rdma_fastreg_mr *frmr;
 	int hdr_count;
 	struct xdr_buf arg;
 	struct list_head dto_q;
@@ -103,16 +104,30 @@ struct svc_rdma_chunk_sge {
 	int start;		/* sge no for this chunk */
 	int count;		/* sge count for this chunk */
 };
+struct svc_rdma_fastreg_mr {
+	struct ib_mr *mr;
+	void *kva;
+	struct ib_fast_reg_page_list *page_list;
+	int page_list_len;
+	unsigned long access_flags;
+	unsigned long map_len;
+	enum dma_data_direction direction;
+	struct list_head frmr_list;
+};
 struct svc_rdma_req_map {
+	struct svc_rdma_fastreg_mr *frmr;
 	unsigned long count;
 	union {
 		struct kvec sge[RPCSVC_MAXPAGES];
 		struct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];
 	};
 };
-
+#define RDMACTXT_F_FAST_UNREG	1
 #define RDMACTXT_F_LAST_CTXT	2
 
+#define	SVCRDMA_DEVCAP_FAST_REG		1	/* fast mr registration */
+#define	SVCRDMA_DEVCAP_READ_W_INV	2	/* read w/ invalidate */
+
 struct svcxprt_rdma {
 	struct svc_xprt      sc_xprt;		/* SVC transport structure */
 	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
@@ -136,6 +151,11 @@ struct svcxprt_rdma {
 	struct ib_cq         *sc_rq_cq;
 	struct ib_cq         *sc_sq_cq;
 	struct ib_mr         *sc_phys_mr;	/* MR for server memory */
+	u32		     sc_dev_caps;	/* distilled device caps */
+	u32		     sc_dma_lkey;	/* local dma key */
+	unsigned int	     sc_frmr_pg_list_len;
+	struct list_head     sc_frmr_q;
+	spinlock_t	     sc_frmr_q_lock;
 
 	spinlock_t	     sc_lock;		/* transport lock */
 

commit 24b8b44780a2c53ecb738f4a1c08d114f5eda27c
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Aug 13 11:05:41 2008 -0500

    svcrdma: Fix race between svc_rdma_recvfrom thread and the dto_tasklet
    
    RDMA_READ completions are kept on a separate queue from the general
    I/O request queue. Since a separate lock is used to protect the RDMA_READ
    completion queue, a race exists between the dto_tasklet and the
    svc_rdma_recvfrom thread where the dto_tasklet sets the XPT_DATA
    bit and adds I/O to the read-completion queue. Concurrently, the
    recvfrom thread checks the generic queue, finds it empty and resets
    the XPT_DATA bit. A subsequent svc_xprt_enqueue will fail to enqueue
    the transport for I/O and cause the transport to "stall".
    
    The fix is to protect both lists with the same lock and set the XPT_DATA
    bit with this lock held.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@citi.umich.edu>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index ef2e3a20bf3b..dc05b54bd3a3 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -143,7 +143,6 @@ struct svcxprt_rdma {
 	unsigned long	     sc_flags;
 	struct list_head     sc_dto_q;		/* DTO tasklet I/O pending Q */
 	struct list_head     sc_read_complete_q;
-	spinlock_t           sc_read_complete_lock;
 	struct work_struct   sc_work;
 };
 /* sc_flags */

commit 8948896c9e098c6fd31a6a698a598a7cbd7fa40e
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed May 28 15:14:02 2008 -0500

    svcrdma: Change WR context get/put to use the kmem cache
    
    Change the WR context pool to be shared across mount points. This
    reduces the RDMA transport memory footprint significantly since
    idle mounts don't consume WR context memory.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d8d74c4ab504..ef2e3a20bf3b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -73,7 +73,6 @@ extern atomic_t rdma_stat_sq_prod;
 struct svc_rdma_op_ctxt {
 	struct svc_rdma_op_ctxt *read_hdr;
 	int hdr_count;
-	struct list_head free_list;
 	struct xdr_buf arg;
 	struct list_head dto_q;
 	enum ib_wr_opcode wr_op;
@@ -131,11 +130,6 @@ struct svcxprt_rdma {
 
 	atomic_t	     sc_dma_used;
 	atomic_t	     sc_ctxt_used;
-	struct list_head     sc_ctxt_free;
-	int		     sc_ctxt_cnt;
-	int		     sc_ctxt_bump;
-	int		     sc_ctxt_max;
-	spinlock_t	     sc_ctxt_lock;
 	struct list_head     sc_rq_dto_q;
 	spinlock_t	     sc_rq_dto_lock;
 	struct ib_qp         *sc_qp;

commit 779a48577ba88b6a7e9748a04b0b739f36c5e6f6
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Mon May 19 10:17:09 2008 -0500

    svcrdma: Remove unused wait q from svcrdma_xprt structure
    
    The sc_read_wait queue head is no longer used. Remove it.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index ab93afc03c43..d8d74c4ab504 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -119,7 +119,6 @@ struct svcxprt_rdma {
 	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
 	struct list_head     sc_accept_q;	/* Conn. waiting accept */
 	int		     sc_ord;		/* RDMA read limit */
-	wait_queue_head_t    sc_read_wait;
 	int                  sc_max_sge;
 
 	int                  sc_sq_depth;	/* Depth of SQ */

commit 87295b6c5c7fd7bbc0ce3e7f42d2adbbac7352b9
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed May 28 13:17:44 2008 -0500

    svcrdma: Add dma map count and WARN_ON
    
    Add a dma map count in order to verify that all DMA mapping resources
    have been freed when the transport is closed.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index fd5e8a1c17de..ab93afc03c43 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -130,6 +130,7 @@ struct svcxprt_rdma {
 
 	struct ib_pd         *sc_pd;
 
+	atomic_t	     sc_dma_used;
 	atomic_t	     sc_ctxt_used;
 	struct list_head     sc_ctxt_free;
 	int		     sc_ctxt_cnt;

commit f820c57ebf5493d4602cc00577c8b0fadd27a7b8
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Tue May 27 17:03:14 2008 -0500

    svcrdma: Use reply and chunk map for RDMA_READ processing
    
    Modify the RDMA_READ processing to use the reply and chunk list mapping data
    types. Also add a special purpose 'hdr_count' field in in the context to hold
    the header page count instead of overloading the SGE length field and
    corrupting the DMA map length.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index bd8749cc8084..fd5e8a1c17de 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -72,6 +72,7 @@ extern atomic_t rdma_stat_sq_prod;
  */
 struct svc_rdma_op_ctxt {
 	struct svc_rdma_op_ctxt *read_hdr;
+	int hdr_count;
 	struct list_head free_list;
 	struct xdr_buf arg;
 	struct list_head dto_q;

commit ab96dddbedf4bb8a7a0fe44012efc1d99598c36f
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed May 28 13:54:04 2008 -0500

    svcrdma: Add a type for keeping NFS RPC mapping
    
    Create a new data structure to hold the remote client address space
    to local server address space mapping.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 05eb4664d0dd..bd8749cc8084 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -86,6 +86,31 @@ struct svc_rdma_op_ctxt {
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
+/*
+ * NFS_ requests are mapped on the client side by the chunk lists in
+ * the RPCRDMA header. During the fetching of the RPC from the client
+ * and the writing of the reply to the client, the memory in the
+ * client and the memory in the server must be mapped as contiguous
+ * vaddr/len for access by the hardware. These data strucures keep
+ * these mappings.
+ *
+ * For an RDMA_WRITE, the 'sge' maps the RPC REPLY. For RDMA_READ, the
+ * 'sge' in the svc_rdma_req_map maps the server side RPC reply and the
+ * 'ch' field maps the read-list of the RPCRDMA header to the 'sge'
+ * mapping of the reply.
+ */
+struct svc_rdma_chunk_sge {
+	int start;		/* sge no for this chunk */
+	int count;		/* sge count for this chunk */
+};
+struct svc_rdma_req_map {
+	unsigned long count;
+	union {
+		struct kvec sge[RPCSVC_MAXPAGES];
+		struct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];
+	};
+};
+
 #define RDMACTXT_F_LAST_CTXT	2
 
 struct svcxprt_rdma {
@@ -173,6 +198,8 @@ extern int svc_rdma_post_recv(struct svcxprt_rdma *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
 extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
 extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
+extern struct svc_rdma_req_map *svc_rdma_get_req_map(void);
+extern void svc_rdma_put_req_map(struct svc_rdma_req_map *);
 extern void svc_sq_reap(struct svcxprt_rdma *);
 extern void svc_rq_reap(struct svcxprt_rdma *);
 extern struct svc_xprt_class svc_rdma_class;

commit 008fdbc57164b0ac237ad6ee2766944f02ac9c28
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed May 7 15:47:42 2008 -0500

    svcrdma: Change svc_rdma_send_error return type to void
    
    The svc_rdma_send_error function is called when an RPCRDMA protocol
    error is detected. This function attempts to post an error reply message.
    Since an error posting to a transport in error is ignored, change
    the return type to void.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index f5f15ae2438b..05eb4664d0dd 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -166,8 +166,8 @@ extern int svc_rdma_sendto(struct svc_rqst *);
 
 /* svc_rdma_transport.c */
 extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
-extern int svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
-			       enum rpcrdma_errcode);
+extern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
+				enum rpcrdma_errcode);
 struct page *svc_rdma_get_page(void);
 extern int svc_rdma_post_recv(struct svcxprt_rdma *);
 extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);

commit 8da91ea8de873ee8be82377ff18637d05e882058
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Apr 30 22:00:46 2008 -0500

    svcrdma: Move destroy to kernel thread
    
    Some providers may wait while destroying adapter resources.
    Since it is possible that the last reference is put on the
    dto_tasklet, the actual destroy must be scheduled as a work item.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index 701439064d21..f5f15ae2438b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -124,6 +124,7 @@ struct svcxprt_rdma {
 	struct list_head     sc_dto_q;		/* DTO tasklet I/O pending Q */
 	struct list_head     sc_read_complete_q;
 	spinlock_t           sc_read_complete_lock;
+	struct work_struct   sc_work;
 };
 /* sc_flags */
 #define RDMAXPRT_RQ_PENDING	1

commit 8740767376b32a7772607e1b2b07cde0c24120cc
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Apr 30 20:44:39 2008 -0500

    svcrdma: Use standard Linux lists for context cache
    
    Replace the one-off linked list implementation used to implement the
    context cache with the standard Linux list_head lists. Add a context
    counter to catch resource leaks. A WARN_ON will be added later to
    ensure that we've freed all contexts.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c447c417b37b..701439064d21 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -72,7 +72,7 @@ extern atomic_t rdma_stat_sq_prod;
  */
 struct svc_rdma_op_ctxt {
 	struct svc_rdma_op_ctxt *read_hdr;
-	struct svc_rdma_op_ctxt *next;
+	struct list_head free_list;
 	struct xdr_buf arg;
 	struct list_head dto_q;
 	enum ib_wr_opcode wr_op;
@@ -104,7 +104,8 @@ struct svcxprt_rdma {
 
 	struct ib_pd         *sc_pd;
 
-	struct svc_rdma_op_ctxt  *sc_ctxt_head;
+	atomic_t	     sc_ctxt_used;
+	struct list_head     sc_ctxt_free;
 	int		     sc_ctxt_cnt;
 	int		     sc_ctxt_bump;
 	int		     sc_ctxt_max;

commit 02e7452de74d308ca642f54f7e5ef801ced60a92
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Apr 30 19:50:56 2008 -0500

    svcrdma: Simplify RDMA_READ deferral buffer management
    
    An NFS_WRITE requires a set of RDMA_READ requests to fetch the write
    data from the client. There are two principal pieces of data that
    need to be tracked: the list of pages that comprise the completed RPC
    and the SGE of dma mapped pages to refer to this list of pages. Previously
    this whole bit was managed as a linked list of contexts with the
    context containing the page list buried in this list. This patch
    simplifies this processing by not keeping a linked list, but rather only
    a pionter from the last submitted RDMA_READ's context to the context
    that maps the set of pages that describe the RPC.  This significantly
    simplifies this code path. SGE contexts are cleaned up inline in the DTO
    path instead of at read completion time.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index d0011f3db90c..c447c417b37b 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -71,6 +71,7 @@ extern atomic_t rdma_stat_sq_prod;
  * completes.
  */
 struct svc_rdma_op_ctxt {
+	struct svc_rdma_op_ctxt *read_hdr;
 	struct svc_rdma_op_ctxt *next;
 	struct xdr_buf arg;
 	struct list_head dto_q;

commit 10a38c33f46d128d11e299acba744bc325cde420
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Apr 30 17:32:17 2008 -0500

    svcrdma: Remove unused READ_DONE context flags bit
    
    The RDMACTXT_F_READ_DONE bit is not longer used. Remove it.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
index c11bbcc081f9..d0011f3db90c 100644
--- a/include/linux/sunrpc/svc_rdma.h
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -85,7 +85,6 @@ struct svc_rdma_op_ctxt {
 	struct page *pages[RPCSVC_MAXPAGES];
 };
 
-#define RDMACTXT_F_READ_DONE	1
 #define RDMACTXT_F_LAST_CTXT	2
 
 struct svcxprt_rdma {

commit d21b05f101ae732d9bc322f13eea2f59c0aa60f5
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Wed Dec 12 16:13:17 2007 -0600

    rdma: SVCRMDA Header File
    
    This file defines the data types used by the SVCRDMA transport module.
    The principle data structure is the transport specific extension to
    the svcxprt structure.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>
    Acked-by: Neil Brown <neilb@suse.de>
    Signed-off-by: J. Bruce Fields <bfields@citi.umich.edu>

diff --git a/include/linux/sunrpc/svc_rdma.h b/include/linux/sunrpc/svc_rdma.h
new file mode 100644
index 000000000000..c11bbcc081f9
--- /dev/null
+++ b/include/linux/sunrpc/svc_rdma.h
@@ -0,0 +1,262 @@
+/*
+ * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the BSD-type
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *      Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *
+ *      Redistributions in binary form must reproduce the above
+ *      copyright notice, this list of conditions and the following
+ *      disclaimer in the documentation and/or other materials provided
+ *      with the distribution.
+ *
+ *      Neither the name of the Network Appliance, Inc. nor the names of
+ *      its contributors may be used to endorse or promote products
+ *      derived from this software without specific prior written
+ *      permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * Author: Tom Tucker <tom@opengridcomputing.com>
+ */
+
+#ifndef SVC_RDMA_H
+#define SVC_RDMA_H
+#include <linux/sunrpc/xdr.h>
+#include <linux/sunrpc/svcsock.h>
+#include <linux/sunrpc/rpc_rdma.h>
+#include <rdma/ib_verbs.h>
+#include <rdma/rdma_cm.h>
+#define SVCRDMA_DEBUG
+
+/* RPC/RDMA parameters and stats */
+extern unsigned int svcrdma_ord;
+extern unsigned int svcrdma_max_requests;
+extern unsigned int svcrdma_max_req_size;
+
+extern atomic_t rdma_stat_recv;
+extern atomic_t rdma_stat_read;
+extern atomic_t rdma_stat_write;
+extern atomic_t rdma_stat_sq_starve;
+extern atomic_t rdma_stat_rq_starve;
+extern atomic_t rdma_stat_rq_poll;
+extern atomic_t rdma_stat_rq_prod;
+extern atomic_t rdma_stat_sq_poll;
+extern atomic_t rdma_stat_sq_prod;
+
+#define RPCRDMA_VERSION 1
+
+/*
+ * Contexts are built when an RDMA request is created and are a
+ * record of the resources that can be recovered when the request
+ * completes.
+ */
+struct svc_rdma_op_ctxt {
+	struct svc_rdma_op_ctxt *next;
+	struct xdr_buf arg;
+	struct list_head dto_q;
+	enum ib_wr_opcode wr_op;
+	enum ib_wc_status wc_status;
+	u32 byte_len;
+	struct svcxprt_rdma *xprt;
+	unsigned long flags;
+	enum dma_data_direction direction;
+	int count;
+	struct ib_sge sge[RPCSVC_MAXPAGES];
+	struct page *pages[RPCSVC_MAXPAGES];
+};
+
+#define RDMACTXT_F_READ_DONE	1
+#define RDMACTXT_F_LAST_CTXT	2
+
+struct svcxprt_rdma {
+	struct svc_xprt      sc_xprt;		/* SVC transport structure */
+	struct rdma_cm_id    *sc_cm_id;		/* RDMA connection id */
+	struct list_head     sc_accept_q;	/* Conn. waiting accept */
+	int		     sc_ord;		/* RDMA read limit */
+	wait_queue_head_t    sc_read_wait;
+	int                  sc_max_sge;
+
+	int                  sc_sq_depth;	/* Depth of SQ */
+	atomic_t             sc_sq_count;	/* Number of SQ WR on queue */
+
+	int                  sc_max_requests;	/* Depth of RQ */
+	int                  sc_max_req_size;	/* Size of each RQ WR buf */
+
+	struct ib_pd         *sc_pd;
+
+	struct svc_rdma_op_ctxt  *sc_ctxt_head;
+	int		     sc_ctxt_cnt;
+	int		     sc_ctxt_bump;
+	int		     sc_ctxt_max;
+	spinlock_t	     sc_ctxt_lock;
+	struct list_head     sc_rq_dto_q;
+	spinlock_t	     sc_rq_dto_lock;
+	struct ib_qp         *sc_qp;
+	struct ib_cq         *sc_rq_cq;
+	struct ib_cq         *sc_sq_cq;
+	struct ib_mr         *sc_phys_mr;	/* MR for server memory */
+
+	spinlock_t	     sc_lock;		/* transport lock */
+
+	wait_queue_head_t    sc_send_wait;	/* SQ exhaustion waitlist */
+	unsigned long	     sc_flags;
+	struct list_head     sc_dto_q;		/* DTO tasklet I/O pending Q */
+	struct list_head     sc_read_complete_q;
+	spinlock_t           sc_read_complete_lock;
+};
+/* sc_flags */
+#define RDMAXPRT_RQ_PENDING	1
+#define RDMAXPRT_SQ_PENDING	2
+#define RDMAXPRT_CONN_PENDING	3
+
+#define RPCRDMA_LISTEN_BACKLOG  10
+/* The default ORD value is based on two outstanding full-size writes with a
+ * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */
+#define RPCRDMA_ORD             (64/4)
+#define RPCRDMA_SQ_DEPTH_MULT   8
+#define RPCRDMA_MAX_THREADS     16
+#define RPCRDMA_MAX_REQUESTS    16
+#define RPCRDMA_MAX_REQ_SIZE    4096
+
+/* svc_rdma_marshal.c */
+extern void svc_rdma_rcl_chunk_counts(struct rpcrdma_read_chunk *,
+				      int *, int *);
+extern int svc_rdma_xdr_decode_req(struct rpcrdma_msg **, struct svc_rqst *);
+extern int svc_rdma_xdr_decode_deferred_req(struct svc_rqst *);
+extern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,
+				     struct rpcrdma_msg *,
+				     enum rpcrdma_errcode, u32 *);
+extern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);
+extern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);
+extern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,
+					    u32, u64, u32);
+extern void svc_rdma_xdr_encode_reply_header(struct svcxprt_rdma *,
+					     struct rpcrdma_msg *,
+					     struct rpcrdma_msg *,
+					     enum rpcrdma_proc);
+extern int svc_rdma_xdr_get_reply_hdr_len(struct rpcrdma_msg *);
+
+/* svc_rdma_recvfrom.c */
+extern int svc_rdma_recvfrom(struct svc_rqst *);
+
+/* svc_rdma_sendto.c */
+extern int svc_rdma_sendto(struct svc_rqst *);
+
+/* svc_rdma_transport.c */
+extern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);
+extern int svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,
+			       enum rpcrdma_errcode);
+struct page *svc_rdma_get_page(void);
+extern int svc_rdma_post_recv(struct svcxprt_rdma *);
+extern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);
+extern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);
+extern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);
+extern void svc_sq_reap(struct svcxprt_rdma *);
+extern void svc_rq_reap(struct svcxprt_rdma *);
+extern struct svc_xprt_class svc_rdma_class;
+extern void svc_rdma_prep_reply_hdr(struct svc_rqst *);
+
+/* svc_rdma.c */
+extern int svc_rdma_init(void);
+extern void svc_rdma_cleanup(void);
+
+/*
+ * Returns the address of the first read chunk or <nul> if no read chunk is
+ * present
+ */
+static inline struct rpcrdma_read_chunk *
+svc_rdma_get_read_chunk(struct rpcrdma_msg *rmsgp)
+{
+	struct rpcrdma_read_chunk *ch =
+		(struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];
+
+	if (ch->rc_discrim == 0)
+		return NULL;
+
+	return ch;
+}
+
+/*
+ * Returns the address of the first read write array element or <nul> if no
+ * write array list is present
+ */
+static inline struct rpcrdma_write_array *
+svc_rdma_get_write_array(struct rpcrdma_msg *rmsgp)
+{
+	if (rmsgp->rm_body.rm_chunks[0] != 0
+	    || rmsgp->rm_body.rm_chunks[1] == 0)
+		return NULL;
+
+	return (struct rpcrdma_write_array *)&rmsgp->rm_body.rm_chunks[1];
+}
+
+/*
+ * Returns the address of the first reply array element or <nul> if no
+ * reply array is present
+ */
+static inline struct rpcrdma_write_array *
+svc_rdma_get_reply_array(struct rpcrdma_msg *rmsgp)
+{
+	struct rpcrdma_read_chunk *rch;
+	struct rpcrdma_write_array *wr_ary;
+	struct rpcrdma_write_array *rp_ary;
+
+	/* XXX: Need to fix when reply list may occur with read-list and/or
+	 * write list */
+	if (rmsgp->rm_body.rm_chunks[0] != 0 ||
+	    rmsgp->rm_body.rm_chunks[1] != 0)
+		return NULL;
+
+	rch = svc_rdma_get_read_chunk(rmsgp);
+	if (rch) {
+		while (rch->rc_discrim)
+			rch++;
+
+		/* The reply list follows an empty write array located
+		 * at 'rc_position' here. The reply array is at rc_target.
+		 */
+		rp_ary = (struct rpcrdma_write_array *)&rch->rc_target;
+
+		goto found_it;
+	}
+
+	wr_ary = svc_rdma_get_write_array(rmsgp);
+	if (wr_ary) {
+		rp_ary = (struct rpcrdma_write_array *)
+			&wr_ary->
+			wc_array[wr_ary->wc_nchunks].wc_target.rs_length;
+
+		goto found_it;
+	}
+
+	/* No read list, no write list */
+	rp_ary = (struct rpcrdma_write_array *)
+		&rmsgp->rm_body.rm_chunks[2];
+
+ found_it:
+	if (rp_ary->wc_discrim == 0)
+		return NULL;
+
+	return rp_ary;
+}
+#endif
