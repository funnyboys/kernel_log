commit 800bb1c8dc80bb4121446b56813067f3ea44edee
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Mar 16 20:32:14 2020 +0100

    mm: handle multiple owners of device private pages in migrate_vma
    
    Add a new src_owner field to struct migrate_vma.  If the field is set,
    only device private pages with page->pgmap->owner equal to that field are
    migrated.  If the field is not set only "normal" pages are migrated.
    
    Fixes: df6ad69838fc ("mm/device-public-memory: device memory cache coherent with CPU")
    Link: https://lore.kernel.org/r/20200316193216.920734-3-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Tested-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 72120061b7d4..3e546cbf03dd 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -196,6 +196,14 @@ struct migrate_vma {
 	unsigned long		npages;
 	unsigned long		start;
 	unsigned long		end;
+
+	/*
+	 * Set to the owner value also stored in page->pgmap->owner for
+	 * migrating out of device private memory.  If set only device
+	 * private pages with this owner are migrated.  If not set
+	 * device private pages are not migrated at all.
+	 */
+	void			*src_owner;
 };
 
 int migrate_vma_setup(struct migrate_vma *args);

commit 06d462beb470d361ffa8bd7b3d865509a8606987
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 14 09:59:27 2019 +0200

    mm: remove the unused MIGRATE_PFN_DEVICE flag
    
    No one ever checks this flag, and we could easily get that information
    from the page if needed.
    
    Link: https://lore.kernel.org/r/20190814075928.23766-10-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Tested-by: Ralph Campbell <rcampbell@nvidia.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 1e67dcfd318f..72120061b7d4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -166,7 +166,6 @@ static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 #define MIGRATE_PFN_MIGRATE	(1UL << 1)
 #define MIGRATE_PFN_LOCKED	(1UL << 2)
 #define MIGRATE_PFN_WRITE	(1UL << 3)
-#define MIGRATE_PFN_DEVICE	(1UL << 4)
 #define MIGRATE_PFN_SHIFT	6
 
 static inline struct page *migrate_pfn_to_page(unsigned long mpfn)

commit 2a915acf88ac0c4c583cd654bdc87371e38016c1
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 14 09:59:26 2019 +0200

    mm: remove the unused MIGRATE_PFN_ERROR flag
    
    Now that we can rely errors in the normal control flow there is no
    need for this flag, remove it.
    
    Link: https://lore.kernel.org/r/20190814075928.23766-9-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Tested-by: Ralph Campbell <rcampbell@nvidia.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 18156d379ebf..1e67dcfd318f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -167,7 +167,6 @@ static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 #define MIGRATE_PFN_LOCKED	(1UL << 2)
 #define MIGRATE_PFN_WRITE	(1UL << 3)
 #define MIGRATE_PFN_DEVICE	(1UL << 4)
-#define MIGRATE_PFN_ERROR	(1UL << 5)
 #define MIGRATE_PFN_SHIFT	6
 
 static inline struct page *migrate_pfn_to_page(unsigned long mpfn)

commit a7d1f22bb74f32cf3cd93f52776007e161f1a738
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 14 09:59:19 2019 +0200

    mm: turn migrate_vma upside down
    
    There isn't any good reason to pass callbacks to migrate_vma.  Instead
    we can just export the three steps done by this function to drivers and
    let them sequence the operation without callbacks.  This removes a lot
    of boilerplate code as-is, and will allow the drivers to drastically
    improve code flow and error handling further on.
    
    Link: https://lore.kernel.org/r/20190814075928.23766-2-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Tested-by: Ralph Campbell <rcampbell@nvidia.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7f04754c7f2b..18156d379ebf 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -182,107 +182,27 @@ static inline unsigned long migrate_pfn(unsigned long pfn)
 	return (pfn << MIGRATE_PFN_SHIFT) | MIGRATE_PFN_VALID;
 }
 
-/*
- * struct migrate_vma_ops - migrate operation callback
- *
- * @alloc_and_copy: alloc destination memory and copy source memory to it
- * @finalize_and_map: allow caller to map the successfully migrated pages
- *
- *
- * The alloc_and_copy() callback happens once all source pages have been locked,
- * unmapped and checked (checked whether pinned or not). All pages that can be
- * migrated will have an entry in the src array set with the pfn value of the
- * page and with the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set (other
- * flags might be set but should be ignored by the callback).
- *
- * The alloc_and_copy() callback can then allocate destination memory and copy
- * source memory to it for all those entries (ie with MIGRATE_PFN_VALID and
- * MIGRATE_PFN_MIGRATE flag set). Once these are allocated and copied, the
- * callback must update each corresponding entry in the dst array with the pfn
- * value of the destination page and with the MIGRATE_PFN_VALID and
- * MIGRATE_PFN_LOCKED flags set (destination pages must have their struct pages
- * locked, via lock_page()).
- *
- * At this point the alloc_and_copy() callback is done and returns.
- *
- * Note that the callback does not have to migrate all the pages that are
- * marked with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration
- * from device memory to system memory (ie the MIGRATE_PFN_DEVICE flag is also
- * set in the src array entry). If the device driver cannot migrate a device
- * page back to system memory, then it must set the corresponding dst array
- * entry to MIGRATE_PFN_ERROR. This will trigger a SIGBUS if CPU tries to
- * access any of the virtual addresses originally backed by this page. Because
- * a SIGBUS is such a severe result for the userspace process, the device
- * driver should avoid setting MIGRATE_PFN_ERROR unless it is really in an
- * unrecoverable state.
- *
- * For empty entry inside CPU page table (pte_none() or pmd_none() is true) we
- * do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus
- * allowing device driver to allocate device memory for those unback virtual
- * address. For this the device driver simply have to allocate device memory
- * and properly set the destination entry like for regular migration. Note that
- * this can still fails and thus inside the device driver must check if the
- * migration was successful for those entry inside the finalize_and_map()
- * callback just like for regular migration.
- *
- * THE alloc_and_copy() CALLBACK MUST NOT CHANGE ANY OF THE SRC ARRAY ENTRIES
- * OR BAD THINGS WILL HAPPEN !
- *
- *
- * The finalize_and_map() callback happens after struct page migration from
- * source to destination (destination struct pages are the struct pages for the
- * memory allocated by the alloc_and_copy() callback).  Migration can fail, and
- * thus the finalize_and_map() allows the driver to inspect which pages were
- * successfully migrated, and which were not. Successfully migrated pages will
- * have the MIGRATE_PFN_MIGRATE flag set for their src array entry.
- *
- * It is safe to update device page table from within the finalize_and_map()
- * callback because both destination and source page are still locked, and the
- * mmap_sem is held in read mode (hence no one can unmap the range being
- * migrated).
- *
- * Once callback is done cleaning up things and updating its page table (if it
- * chose to do so, this is not an obligation) then it returns. At this point,
- * the HMM core will finish up the final steps, and the migration is complete.
- *
- * THE finalize_and_map() CALLBACK MUST NOT CHANGE ANY OF THE SRC OR DST ARRAY
- * ENTRIES OR BAD THINGS WILL HAPPEN !
- */
-struct migrate_vma_ops {
-	void (*alloc_and_copy)(struct vm_area_struct *vma,
-			       const unsigned long *src,
-			       unsigned long *dst,
-			       unsigned long start,
-			       unsigned long end,
-			       void *private);
-	void (*finalize_and_map)(struct vm_area_struct *vma,
-				 const unsigned long *src,
-				 const unsigned long *dst,
-				 unsigned long start,
-				 unsigned long end,
-				 void *private);
+struct migrate_vma {
+	struct vm_area_struct	*vma;
+	/*
+	 * Both src and dst array must be big enough for
+	 * (end - start) >> PAGE_SHIFT entries.
+	 *
+	 * The src array must not be modified by the caller after
+	 * migrate_vma_setup(), and must not change the dst array after
+	 * migrate_vma_pages() returns.
+	 */
+	unsigned long		*dst;
+	unsigned long		*src;
+	unsigned long		cpages;
+	unsigned long		npages;
+	unsigned long		start;
+	unsigned long		end;
 };
 
-#if defined(CONFIG_MIGRATE_VMA_HELPER)
-int migrate_vma(const struct migrate_vma_ops *ops,
-		struct vm_area_struct *vma,
-		unsigned long start,
-		unsigned long end,
-		unsigned long *src,
-		unsigned long *dst,
-		void *private);
-#else
-static inline int migrate_vma(const struct migrate_vma_ops *ops,
-			      struct vm_area_struct *vma,
-			      unsigned long start,
-			      unsigned long end,
-			      unsigned long *src,
-			      unsigned long *dst,
-			      void *private)
-{
-	return -EINVAL;
-}
-#endif /* IS_ENABLED(CONFIG_MIGRATE_VMA_HELPER) */
+int migrate_vma_setup(struct migrate_vma *args);
+void migrate_vma_pages(struct migrate_vma *migrate);
+void migrate_vma_finalize(struct migrate_vma *migrate);
 
 #endif /* CONFIG_MIGRATION */
 

commit 371096949f0ad3950b06729989bd27de51b8c5f5
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Jul 18 15:58:46 2019 -0700

    mm: migrate: remove unused mode argument
    
    migrate_page_move_mapping() doesn't use the mode argument.  Remove it
    and update callers accordingly.
    
    Link: http://lkml.kernel.org/r/20190508210301.8472-1-keith.busch@intel.com
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Zi Yan <ziy@nvidia.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e13d9bf2f9a5..7f04754c7f2b 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -77,8 +77,7 @@ extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
 extern int migrate_page_move_mapping(struct address_space *mapping,
-		struct page *newpage, struct page *page, enum migrate_mode mode,
-		int extra_count);
+		struct page *newpage, struct page *page, int extra_count);
 #else
 
 static inline void putback_movable_pages(struct list_head *l) {}

commit ab41ee6879981b3d3a16a1079a33fa6fd043eb3c
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 28 00:39:20 2018 -0800

    mm: migrate: drop unused argument of migrate_page_move_mapping()
    
    All callers of migrate_page_move_mapping() now pass NULL for 'head'
    argument.  Drop it.
    
    Link: http://lkml.kernel.org/r/20181211172143.7358-7-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 617615fa11ce..e13d9bf2f9a5 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -77,8 +77,7 @@ extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
 extern int migrate_page_move_mapping(struct address_space *mapping,
-		struct page *newpage, struct page *page,
-		struct buffer_head *head, enum migrate_mode mode,
+		struct page *newpage, struct page *page, enum migrate_mode mode,
 		int extra_count);
 #else
 

commit 9a2f45ff320287d49a3cd90ce68cb58a6da6f5e1
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Dec 28 00:35:59 2018 -0800

    mm/debug.c: make "migrate_reason_names[]" const char *
    
    Those strings are immutable as well.
    
    Link: http://lkml.kernel.org/r/20181124090508.GB10877@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index f2b4abbca55e..617615fa11ce 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -29,7 +29,7 @@ enum migrate_reason {
 };
 
 /* In mm/debug.c; also keep sync with include/trace/events/migrate.h */
-extern char *migrate_reason_names[MR_TYPES];
+extern const char *migrate_reason_names[MR_TYPES];
 
 static inline struct page *new_page_nodemask(struct page *page,
 				int preferred_nid, nodemask_t *nodemask)

commit 94723aafb9e76414fada7c1c198733a86f01ea8f
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Apr 10 16:30:07 2018 -0700

    mm: unclutter THP migration
    
    THP migration is hacked into the generic migration with rather
    surprising semantic.  The migration allocation callback is supposed to
    check whether the THP can be migrated at once and if that is not the
    case then it allocates a simple page to migrate.  unmap_and_move then
    fixes that up by spliting the THP into small pages while moving the head
    page to the newly allocated order-0 page.  Remaning pages are moved to
    the LRU list by split_huge_page.  The same happens if the THP allocation
    fails.  This is really ugly and error prone [1].
    
    I also believe that split_huge_page to the LRU lists is inherently wrong
    because all tail pages are not migrated.  Some callers will just work
    around that by retrying (e.g.  memory hotplug).  There are other pfn
    walkers which are simply broken though.  e.g. madvise_inject_error will
    migrate head and then advances next pfn by the huge page size.
    do_move_page_to_node_array, queue_pages_range (migrate_pages, mbind),
    will simply split the THP before migration if the THP migration is not
    supported then falls back to single page migration but it doesn't handle
    tail pages if the THP migration path is not able to allocate a fresh THP
    so we end up with ENOMEM and fail the whole migration which is a
    questionable behavior.  Page compaction doesn't try to migrate large
    pages so it should be immune.
    
    This patch tries to unclutter the situation by moving the special THP
    handling up to the migrate_pages layer where it actually belongs.  We
    simply split the THP page into the existing list if unmap_and_move fails
    with ENOMEM and retry.  So we will _always_ migrate all THP subpages and
    specific migrate_pages users do not have to deal with this case in a
    special way.
    
    [1] http://lkml.kernel.org/r/20171121021855.50525-1-zi.yan@sent.com
    
    Link: http://lkml.kernel.org/r/20180103082555.14592-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Cc: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e0393240bf64..f2b4abbca55e 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -42,9 +42,9 @@ static inline struct page *new_page_nodemask(struct page *page,
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
 				preferred_nid, nodemask);
 
-	if (thp_migration_supported() && PageTransHuge(page)) {
-		order = HPAGE_PMD_ORDER;
+	if (PageTransHuge(page)) {
 		gfp_mask |= GFP_TRANSHUGE;
+		order = HPAGE_PMD_ORDER;
 	}
 
 	if (PageHighMem(page) || (zone_idx(page_zone(page)) == ZONE_MOVABLE))

commit 666feb21a0083e5b29ddd96588553ffa0cc357b6
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Apr 10 16:30:03 2018 -0700

    mm, migrate: remove reason argument from new_page_t
    
    No allocation callback is using this argument anymore.  new_page_node
    used to use this parameter to convey node_id resp.  migration error up
    to move_pages code (do_move_page_to_node_array).  The error status never
    made it into the final status field and we have a better way to
    communicate node id to the status field now.  All other allocation
    callbacks simply ignored the argument so we can drop it finally.
    
    [mhocko@suse.com: fix migration callback]
      Link: http://lkml.kernel.org/r/20180105085259.GH2801@dhcp22.suse.cz
    [akpm@linux-foundation.org: fix alloc_misplaced_dst_page()]
    [mhocko@kernel.org: fix build]
      Link: http://lkml.kernel.org/r/20180103091134.GB11319@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/20180103082555.14592-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Cc: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ab45f8a0d288..e0393240bf64 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,8 +7,7 @@
 #include <linux/migrate_mode.h>
 #include <linux/hugetlb.h>
 
-typedef struct page *new_page_t(struct page *page, unsigned long private,
-				int **reason);
+typedef struct page *new_page_t(struct page *page, unsigned long private);
 typedef void free_page_t(struct page *page, unsigned long private);
 
 /*

commit 310253514bbf179c5f82e20a7a4bbf07abc7f5ad
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu Apr 5 16:22:08 2018 -0700

    mm/migrate: rename migration reason MR_CMA to MR_CONTIG_RANGE
    
    alloc_contig_range() initiates compaction and eventual migration for the
    purpose of either CMA or HugeTLB allocations.  At present, the reason
    code remains the same MR_CMA for either of these cases.  Let's make it
    MR_CONTIG_RANGE which will appropriately reflect the reason code in both
    these cases.
    
    Link: http://lkml.kernel.org/r/20180202091518.18798-1-khandual@linux.vnet.ibm.com
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index a2246cf670ba..ab45f8a0d288 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -25,7 +25,7 @@ enum migrate_reason {
 	MR_SYSCALL,		/* also applies to cpusets */
 	MR_MEMPOLICY_MBIND,
 	MR_NUMA_MISPLACED,
-	MR_CMA,
+	MR_CONTIG_RANGE,
 	MR_TYPES
 };
 

commit 40a899ed16486455f964e46d1af31fd4fded21c1
Author: Zi Yan <zi.yan@cs.rutgers.edu>
Date:   Wed Nov 29 16:11:12 2017 -0800

    mm: migrate: fix an incorrect call of prep_transhuge_page()
    
    In https://lkml.org/lkml/2017/11/20/411, Andrea reported that during
    memory hotplug/hot remove prep_transhuge_page() is called incorrectly on
    non-THP pages for migration, when THP is on but THP migration is not
    enabled.  This leads to a bad state of target pages for migration.
    
    By inspecting the code, if called on a non-THP, prep_transhuge_page()
    will
    
     1) change the value of the mapping of (page + 2), since it is used for
        THP deferred list;
    
     2) change the lru value of (page + 1), since it is used for THP's dtor.
    
    Both can lead to data corruption of these two pages.
    
    Andrea said:
     "Pragmatically and from the point of view of the memory_hotplug subsys,
      the effect is a kernel crash when pages are being migrated during a
      memory hot remove offline and migration target pages are found in a
      bad state"
    
    This patch fixes it by only calling prep_transhuge_page() when we are
    certain that the target page is THP.
    
    Link: http://lkml.kernel.org/r/20171121021855.50525-1-zi.yan@sent.com
    Fixes: 8135d8926c08 ("mm: memory_hotplug: memory hotremove supports thp migration")
    Signed-off-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Reported-by: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Jérôme Glisse" <jglisse@redhat.com>
    Cc: <stable@vger.kernel.org>    [4.14]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 895ec0c4942e..a2246cf670ba 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -54,7 +54,7 @@ static inline struct page *new_page_nodemask(struct page *page,
 	new_page = __alloc_pages_nodemask(gfp_mask, order,
 				preferred_nid, nodemask);
 
-	if (new_page && PageTransHuge(page))
+	if (new_page && PageTransHuge(new_page))
 		prep_transhuge_page(new_page);
 
 	return new_page;

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 643c7ae7d7b4..895ec0c4942e 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_MIGRATE_H
 #define _LINUX_MIGRATE_H
 

commit 6b368cd4a44ce95b33f1d31f2f932e6ae707f319
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:32 2017 -0700

    mm/hmm: avoid bloating arch that do not make use of HMM
    
    This moves all new code including new page migration helper behind kernel
    Kconfig option so that there is no codee bloat for arch or user that do
    not want to use HMM or any of its associated features.
    
    arm allyesconfig (without all the patchset, then with and this patch):
       text    data     bss     dec     hex filename
    83721896        46511131        27582964        157815991       96814b7 ../without/vmlinux
    83722364        46511131        27582964        157816459       968168b vmlinux
    
    [jglisse@redhat.com: struct hmm is only use by HMM mirror functionality]
      Link: http://lkml.kernel.org/r/20170825213133.27286-1-jglisse@redhat.com
    [sfr@canb.auug.org.au: fix build (arm multi_v7_defconfig)]
      Link: http://lkml.kernel.org/r/20170828181849.323ab81b@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170818032858.7447-1-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index d4e6d12a0b40..643c7ae7d7b4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -265,6 +265,7 @@ struct migrate_vma_ops {
 				 void *private);
 };
 
+#if defined(CONFIG_MIGRATE_VMA_HELPER)
 int migrate_vma(const struct migrate_vma_ops *ops,
 		struct vm_area_struct *vma,
 		unsigned long start,
@@ -272,6 +273,18 @@ int migrate_vma(const struct migrate_vma_ops *ops,
 		unsigned long *src,
 		unsigned long *dst,
 		void *private);
+#else
+static inline int migrate_vma(const struct migrate_vma_ops *ops,
+			      struct vm_area_struct *vma,
+			      unsigned long start,
+			      unsigned long end,
+			      unsigned long *src,
+			      unsigned long *dst,
+			      void *private)
+{
+	return -EINVAL;
+}
+#endif /* IS_ENABLED(CONFIG_MIGRATE_VMA_HELPER) */
 
 #endif /* CONFIG_MIGRATION */
 

commit 8315ada7f095bfa2cae0cd1e915b95bf6226897d
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:21 2017 -0700

    mm/migrate: allow migrate_vma() to alloc new page on empty entry
    
    This allows callers of migrate_vma() to allocate new page for empty CPU
    page table entry (pte_none or back by zero page).  This is only for
    anonymous memory and it won't allow new page to be instanced if the
    userfaultfd is armed.
    
    This is useful to device driver that want to migrate a range of virtual
    address and would rather allocate new memory than having to fault later
    on.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-18-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 8dc8f0a3f1af..d4e6d12a0b40 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -218,6 +218,15 @@ static inline unsigned long migrate_pfn(unsigned long pfn)
  * driver should avoid setting MIGRATE_PFN_ERROR unless it is really in an
  * unrecoverable state.
  *
+ * For empty entry inside CPU page table (pte_none() or pmd_none() is true) we
+ * do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus
+ * allowing device driver to allocate device memory for those unback virtual
+ * address. For this the device driver simply have to allocate device memory
+ * and properly set the destination entry like for regular migration. Note that
+ * this can still fails and thus inside the device driver must check if the
+ * migration was successful for those entry inside the finalize_and_map()
+ * callback just like for regular migration.
+ *
  * THE alloc_and_copy() CALLBACK MUST NOT CHANGE ANY OF THE SRC ARRAY ENTRIES
  * OR BAD THINGS WILL HAPPEN !
  *

commit a5430dda8a3a1cdd532e37270e6f36436241b6e7
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:17 2017 -0700

    mm/migrate: support un-addressable ZONE_DEVICE page in migration
    
    Allow to unmap and restore special swap entry of un-addressable
    ZONE_DEVICE memory.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-17-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 8f73cebfc3f5..8dc8f0a3f1af 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -159,12 +159,18 @@ static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 
 #ifdef CONFIG_MIGRATION
 
+/*
+ * Watch out for PAE architecture, which has an unsigned long, and might not
+ * have enough bits to store all physical address and flags. So far we have
+ * enough room for all our flags.
+ */
 #define MIGRATE_PFN_VALID	(1UL << 0)
 #define MIGRATE_PFN_MIGRATE	(1UL << 1)
 #define MIGRATE_PFN_LOCKED	(1UL << 2)
 #define MIGRATE_PFN_WRITE	(1UL << 3)
-#define MIGRATE_PFN_ERROR	(1UL << 4)
-#define MIGRATE_PFN_SHIFT	5
+#define MIGRATE_PFN_DEVICE	(1UL << 4)
+#define MIGRATE_PFN_ERROR	(1UL << 5)
+#define MIGRATE_PFN_SHIFT	6
 
 static inline struct page *migrate_pfn_to_page(unsigned long mpfn)
 {

commit 8763cb45ab967a92a5ee49e9c544c0f0ea90e2d6
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:09 2017 -0700

    mm/migrate: new memory migration helper for use with device memory
    
    This patch add a new memory migration helpers, which migrate memory
    backing a range of virtual address of a process to different memory (which
    can be allocated through special allocator).  It differs from numa
    migration by working on a range of virtual address and thus by doing
    migration in chunk that can be large enough to use DMA engine or special
    copy offloading engine.
    
    Expected users are any one with heterogeneous memory where different
    memory have different characteristics (latency, bandwidth, ...).  As an
    example IBM platform with CAPI bus can make use of this feature to migrate
    between regular memory and CAPI device memory.  New CPU architecture with
    a pool of high performance memory not manage as cache but presented as
    regular memory (while being faster and with lower latency than DDR) will
    also be prime user of this patch.
    
    Migration to private device memory will be useful for device that have
    large pool of such like GPU, NVidia plans to use HMM for that.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-15-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Signed-off-by: Evgeny Baskakov <ebaskakov@nvidia.com>
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Mark Hairgrove <mhairgrove@nvidia.com>
    Signed-off-by: Sherry Cheung <SCheung@nvidia.com>
    Signed-off-by: Subhash Gutti <sgutti@nvidia.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7db4c812a2a6..8f73cebfc3f5 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -156,4 +156,108 @@ static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 }
 #endif /* CONFIG_NUMA_BALANCING && CONFIG_TRANSPARENT_HUGEPAGE*/
 
+
+#ifdef CONFIG_MIGRATION
+
+#define MIGRATE_PFN_VALID	(1UL << 0)
+#define MIGRATE_PFN_MIGRATE	(1UL << 1)
+#define MIGRATE_PFN_LOCKED	(1UL << 2)
+#define MIGRATE_PFN_WRITE	(1UL << 3)
+#define MIGRATE_PFN_ERROR	(1UL << 4)
+#define MIGRATE_PFN_SHIFT	5
+
+static inline struct page *migrate_pfn_to_page(unsigned long mpfn)
+{
+	if (!(mpfn & MIGRATE_PFN_VALID))
+		return NULL;
+	return pfn_to_page(mpfn >> MIGRATE_PFN_SHIFT);
+}
+
+static inline unsigned long migrate_pfn(unsigned long pfn)
+{
+	return (pfn << MIGRATE_PFN_SHIFT) | MIGRATE_PFN_VALID;
+}
+
+/*
+ * struct migrate_vma_ops - migrate operation callback
+ *
+ * @alloc_and_copy: alloc destination memory and copy source memory to it
+ * @finalize_and_map: allow caller to map the successfully migrated pages
+ *
+ *
+ * The alloc_and_copy() callback happens once all source pages have been locked,
+ * unmapped and checked (checked whether pinned or not). All pages that can be
+ * migrated will have an entry in the src array set with the pfn value of the
+ * page and with the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set (other
+ * flags might be set but should be ignored by the callback).
+ *
+ * The alloc_and_copy() callback can then allocate destination memory and copy
+ * source memory to it for all those entries (ie with MIGRATE_PFN_VALID and
+ * MIGRATE_PFN_MIGRATE flag set). Once these are allocated and copied, the
+ * callback must update each corresponding entry in the dst array with the pfn
+ * value of the destination page and with the MIGRATE_PFN_VALID and
+ * MIGRATE_PFN_LOCKED flags set (destination pages must have their struct pages
+ * locked, via lock_page()).
+ *
+ * At this point the alloc_and_copy() callback is done and returns.
+ *
+ * Note that the callback does not have to migrate all the pages that are
+ * marked with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration
+ * from device memory to system memory (ie the MIGRATE_PFN_DEVICE flag is also
+ * set in the src array entry). If the device driver cannot migrate a device
+ * page back to system memory, then it must set the corresponding dst array
+ * entry to MIGRATE_PFN_ERROR. This will trigger a SIGBUS if CPU tries to
+ * access any of the virtual addresses originally backed by this page. Because
+ * a SIGBUS is such a severe result for the userspace process, the device
+ * driver should avoid setting MIGRATE_PFN_ERROR unless it is really in an
+ * unrecoverable state.
+ *
+ * THE alloc_and_copy() CALLBACK MUST NOT CHANGE ANY OF THE SRC ARRAY ENTRIES
+ * OR BAD THINGS WILL HAPPEN !
+ *
+ *
+ * The finalize_and_map() callback happens after struct page migration from
+ * source to destination (destination struct pages are the struct pages for the
+ * memory allocated by the alloc_and_copy() callback).  Migration can fail, and
+ * thus the finalize_and_map() allows the driver to inspect which pages were
+ * successfully migrated, and which were not. Successfully migrated pages will
+ * have the MIGRATE_PFN_MIGRATE flag set for their src array entry.
+ *
+ * It is safe to update device page table from within the finalize_and_map()
+ * callback because both destination and source page are still locked, and the
+ * mmap_sem is held in read mode (hence no one can unmap the range being
+ * migrated).
+ *
+ * Once callback is done cleaning up things and updating its page table (if it
+ * chose to do so, this is not an obligation) then it returns. At this point,
+ * the HMM core will finish up the final steps, and the migration is complete.
+ *
+ * THE finalize_and_map() CALLBACK MUST NOT CHANGE ANY OF THE SRC OR DST ARRAY
+ * ENTRIES OR BAD THINGS WILL HAPPEN !
+ */
+struct migrate_vma_ops {
+	void (*alloc_and_copy)(struct vm_area_struct *vma,
+			       const unsigned long *src,
+			       unsigned long *dst,
+			       unsigned long start,
+			       unsigned long end,
+			       void *private);
+	void (*finalize_and_map)(struct vm_area_struct *vma,
+				 const unsigned long *src,
+				 const unsigned long *dst,
+				 unsigned long start,
+				 unsigned long end,
+				 void *private);
+};
+
+int migrate_vma(const struct migrate_vma_ops *ops,
+		struct vm_area_struct *vma,
+		unsigned long start,
+		unsigned long end,
+		unsigned long *src,
+		unsigned long *dst,
+		void *private);
+
+#endif /* CONFIG_MIGRATION */
+
 #endif /* _LINUX_MIGRATE_H */

commit 2916ecc0f9d435d849c98f4da50e453124c87531
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:06 2017 -0700

    mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY
    
    Introduce a new migration mode that allow to offload the copy to a device
    DMA engine.  This changes the workflow of migration and not all
    address_space migratepage callback can support this.
    
    This is intended to be use by migrate_vma() which itself is use for thing
    like HMM (see include/linux/hmm.h).
    
    No additional per-filesystem migratepage testing is needed.  I disables
    MIGRATE_SYNC_NO_COPY in all problematic migratepage() callback and i
    added comment in those to explain why (part of this patch).  The commit
    message is unclear it should say that any callback that wish to support
    this new mode need to be aware of the difference in the migration flow
    from other mode.
    
    Some of these callbacks do extra locking while copying (aio, zsmalloc,
    balloon, ...) and for DMA to be effective you want to copy multiple
    pages in one DMA operations.  But in the problematic case you can not
    easily hold the extra lock accross multiple call to this callback.
    
    Usual flow is:
    
    For each page {
     1 - lock page
     2 - call migratepage() callback
     3 - (extra locking in some migratepage() callback)
     4 - migrate page state (freeze refcount, update page cache, buffer
         head, ...)
     5 - copy page
     6 - (unlock any extra lock of migratepage() callback)
     7 - return from migratepage() callback
     8 - unlock page
    }
    
    The new mode MIGRATE_SYNC_NO_COPY:
     1 - lock multiple pages
    For each page {
     2 - call migratepage() callback
     3 - abort in all problematic migratepage() callback
     4 - migrate page state (freeze refcount, update page cache, buffer
         head, ...)
    } // finished all calls to migratepage() callback
     5 - DMA copy multiple pages
     6 - unlock all the pages
    
    To support MIGRATE_SYNC_NO_COPY in the problematic case we would need a
    new callback migratepages() (for instance) that deals with multiple
    pages in one transaction.
    
    Because the problematic cases are not important for current usage I did
    not wanted to complexify this patchset even more for no good reason.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-14-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ce15989521a1..7db4c812a2a6 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -72,6 +72,7 @@ extern void putback_movable_page(struct page *page);
 
 extern int migrate_prep(void);
 extern int migrate_prep_local(void);
+extern void migrate_page_states(struct page *newpage, struct page *page);
 extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
@@ -92,6 +93,10 @@ static inline int isolate_movable_page(struct page *page, isolate_mode_t mode)
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }
 
+static inline void migrate_page_states(struct page *newpage, struct page *page)
+{
+}
+
 static inline void migrate_page_copy(struct page *newpage,
 				     struct page *page) {}
 

commit 8135d8926c08e553e39b0b040c6d01f0daef0676
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Fri Sep 8 16:11:15 2017 -0700

    mm: memory_hotplug: memory hotremove supports thp migration
    
    This patch enables thp migration for memory hotremove.
    
    Link: http://lkml.kernel.org/r/20170717193955.20207-11-zi.yan@sent.com
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 3e0d405dc842..ce15989521a1 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -35,15 +35,28 @@ static inline struct page *new_page_nodemask(struct page *page,
 				int preferred_nid, nodemask_t *nodemask)
 {
 	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL;
+	unsigned int order = 0;
+	struct page *new_page = NULL;
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
 				preferred_nid, nodemask);
 
+	if (thp_migration_supported() && PageTransHuge(page)) {
+		order = HPAGE_PMD_ORDER;
+		gfp_mask |= GFP_TRANSHUGE;
+	}
+
 	if (PageHighMem(page) || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
 		gfp_mask |= __GFP_HIGHMEM;
 
-	return __alloc_pages_nodemask(gfp_mask, 0, preferred_nid, nodemask);
+	new_page = __alloc_pages_nodemask(gfp_mask, order,
+				preferred_nid, nodemask);
+
+	if (new_page && PageTransHuge(page))
+		prep_transhuge_page(new_page);
+
+	return new_page;
 }
 
 #ifdef CONFIG_MIGRATION

commit 0f55685627d6dd2beda55a82abc02297f0f8e5c2
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jul 12 14:36:58 2017 -0700

    mm, migration: do not trigger OOM killer when migrating memory
    
    Page migration (for memory hotplug, soft_offline_page or mbind) needs to
    allocate a new memory.  This can trigger an oom killer if the target
    memory is depleated.  Although quite unlikely, still possible,
    especially for the memory hotplug (offlining of memoery).
    
    Up to now we didn't really have reasonable means to back off.
    __GFP_NORETRY can fail just too easily and __GFP_THISNODE sticks to a
    single node and that is not suitable for all callers.
    
    But now that we have __GFP_RETRY_MAYFAIL we should use it.  It is
    preferable to fail the migration than disrupt the system by killing some
    processes.
    
    Link: http://lkml.kernel.org/r/20170623085345.11304-7-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Alex Belits <alex.belits@cavium.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: NeilBrown <neilb@suse.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 4634da521238..3e0d405dc842 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -34,7 +34,7 @@ extern char *migrate_reason_names[MR_TYPES];
 static inline struct page *new_page_nodemask(struct page *page,
 				int preferred_nid, nodemask_t *nodemask)
 {
-	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
+	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL;
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),

commit 3e59fcb0e8c1c40aecb60fa4c2d1543d6a097184
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:49:11 2017 -0700

    hugetlb: add support for preferred node to alloc_huge_page_nodemask
    
    alloc_huge_page_nodemask tries to allocate from any numa node in the
    allowed node mask starting from lower numa nodes.  This might lead to
    filling up those low NUMA nodes while others are not used.  We can
    reduce this risk by introducing a concept of the preferred node similar
    to what we have in the regular page allocator.  We will start allocating
    from the preferred nid and then iterate over all allowed nodes in the
    zonelist order until we try them all.
    
    This is mimicing the page allocator logic except it operates on per-node
    mempools.  dequeue_huge_page_vma already does this so distill the
    zonelist logic into a more generic dequeue_huge_page_nodemask and use it
    in alloc_huge_page_nodemask.
    
    This will allow us to use proper per numa distance fallback also for
    alloc_huge_page_node which can use alloc_huge_page_nodemask now and we
    can get rid of alloc_huge_page_node helper which doesn't have any user
    anymore.
    
    Link: http://lkml.kernel.org/r/20170622193034.28972-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index d9675b665cc4..4634da521238 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -38,7 +38,7 @@ static inline struct page *new_page_nodemask(struct page *page,
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
-				nodemask);
+				preferred_nid, nodemask);
 
 	if (PageHighMem(page) || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
 		gfp_mask |= __GFP_HIGHMEM;

commit 8b9132388964df2cfe151a88fd1dd8219dabf23c
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:48:47 2017 -0700

    mm: unify new_node_page and alloc_migrate_target
    
    Commit 394e31d2ceb4 ("mem-hotplug: alloc new page from a nearest
    neighbor node when mem-offline") has duplicated a large part of
    alloc_migrate_target with some hotplug specific special casing.
    
    To be more precise it tried to enfore the allocation from a different
    node than the original page.  As a result the two function diverged in
    their shared logic, e.g.  the hugetlb allocation strategy.
    
    Let's unify the two and express different NUMA requirements by the given
    nodemask.  new_node_page will simply exclude the node it doesn't care
    about and alloc_migrate_target will use all the available nodes.
    alloc_migrate_target will then learn to migrate hugetlb pages more
    sanely and use preallocated pool when possible.
    
    Please note that alloc_migrate_target used to call alloc_page resp.
    alloc_pages_current so the memory policy of the current context which is
    quite strange when we consider that it is used in the context of
    alloc_contig_range which just tries to migrate pages which stand in the
    way.
    
    Link: http://lkml.kernel.org/r/20170608074553.22152-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: zhong jiang <zhongjiang@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 48e24844b3c5..d9675b665cc4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -4,6 +4,7 @@
 #include <linux/mm.h>
 #include <linux/mempolicy.h>
 #include <linux/migrate_mode.h>
+#include <linux/hugetlb.h>
 
 typedef struct page *new_page_t(struct page *page, unsigned long private,
 				int **reason);
@@ -30,6 +31,21 @@ enum migrate_reason {
 /* In mm/debug.c; also keep sync with include/trace/events/migrate.h */
 extern char *migrate_reason_names[MR_TYPES];
 
+static inline struct page *new_page_nodemask(struct page *page,
+				int preferred_nid, nodemask_t *nodemask)
+{
+	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
+
+	if (PageHuge(page))
+		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
+				nodemask);
+
+	if (PageHighMem(page) || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
+		gfp_mask |= __GFP_HIGHMEM;
+
+	return __alloc_pages_nodemask(gfp_mask, 0, preferred_nid, nodemask);
+}
+
 #ifdef CONFIG_MIGRATION
 
 extern void putback_movable_pages(struct list_head *l);

commit 9927e3887642b976d9b391cd77d71388aa521e54
Author: Pushkar Jambhlekar <pushkar.iit@gmail.com>
Date:   Wed May 3 14:54:45 2017 -0700

    include/linux/migrate.h: add arg names to prototype
    
    It is preferred, and the rest of migrate.h gets it right.
    
    Link: http://lkml.kernel.org/r/1490336009-8024-1-git-send-email-pushkar.iit@gmail.com
    Signed-off-by: Pushkar Jambhlekar <pushkar.iit@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index fa76b516fa47..48e24844b3c5 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -33,8 +33,9 @@ extern char *migrate_reason_names[MR_TYPES];
 #ifdef CONFIG_MIGRATION
 
 extern void putback_movable_pages(struct list_head *l);
-extern int migrate_page(struct address_space *,
-			struct page *, struct page *, enum migrate_mode);
+extern int migrate_page(struct address_space *mapping,
+			struct page *newpage, struct page *page,
+			enum migrate_mode mode);
 extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 		unsigned long private, enum migrate_mode mode, int reason);
 extern int isolate_movable_page(struct page *page, isolate_mode_t mode);

commit cbae0170e5329c79c0a36a81fedd2800146aca12
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Fri Feb 24 14:57:32 2017 -0800

    mm/migration: make isolate_movable_page always defined
    
    Define isolate_movable_page as a static inline function when
    CONFIG_MIGRATION is not enable.  It should return -EBUSY here which
    means failed to isolate movable pages.
    
    This patch do not have any functional change but prepare for later
    patch.
    
    Link: http://lkml.kernel.org/r/1485867981-16037-3-git-send-email-ysxie@foxmail.com
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Suggested-by: Michal Hocko <mhocko@kernel.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Hanjun Guo <guohanjun@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 43d5deb0c4cc..fa76b516fa47 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -56,6 +56,8 @@ static inline int migrate_pages(struct list_head *l, new_page_t new,
 		free_page_t free, unsigned long private, enum migrate_mode mode,
 		int reason)
 	{ return -ENOSYS; }
+static inline int isolate_movable_page(struct page *page, isolate_mode_t mode)
+	{ return -EBUSY; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }

commit 9e5bcd610ffcedf5e485e78a72762810b25c7f25
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Fri Feb 24 14:57:29 2017 -0800

    mm/migration: make isolate_movable_page() return int type
    
    Patch series "HWPOISON: soft offlining for non-lru movable page", v6.
    
    After Minchan's commit bda807d44454 ("mm: migrate: support non-lru
    movable page migration"), some type of non-lru page like zsmalloc and
    virtio-balloon page also support migration.
    
    Therefore, we can:
    
    1) soft offlining no-lru movable pages, which means when memory
       corrected errors occur on a non-lru movable page, we can stop to use
       it by migrating data onto another page and disable the original
       (maybe half-broken) one.
    
    2) enable memory hotplug for non-lru movable pages, i.e. we may offline
       blocks, which include such pages, by using non-lru page migration.
    
    This patchset is heavily dependent on non-lru movable page migration.
    
    This patch (of 4):
    
    Change the return type of isolate_movable_page() from bool to int.  It
    will return 0 when isolate movable page successfully, and return -EBUSY
    when it isolates failed.
    
    There is no functional change within this patch but prepare for later
    patch.
    
    [xieyisheng1@huawei.com: v6]
      Link: http://lkml.kernel.org/r/1486108770-630-2-git-send-email-xieyisheng1@huawei.com
    Link: http://lkml.kernel.org/r/1485867981-16037-2-git-send-email-ysxie@foxmail.com
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Suggested-by: Michal Hocko <mhocko@kernel.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Hanjun Guo <guohanjun@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ae8d475a9385..43d5deb0c4cc 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -37,7 +37,7 @@ extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 		unsigned long private, enum migrate_mode mode, int reason);
-extern bool isolate_movable_page(struct page *page, isolate_mode_t mode);
+extern int isolate_movable_page(struct page *page, isolate_mode_t mode);
 extern void putback_movable_page(struct page *page);
 
 extern int migrate_prep(void);

commit dd4123f324bbaec7618b677b7bce2b11aee9594e
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 26 15:26:50 2016 -0700

    mm: fix build warnings in <linux/compaction.h>
    
    Randy reported below build error.
    
    > In file included from ../include/linux/balloon_compaction.h:48:0,
    >                  from ../mm/balloon_compaction.c:11:
    > ../include/linux/compaction.h:237:51: warning: 'struct node' declared inside parameter list [enabled by default]
    >  static inline int compaction_register_node(struct node *node)
    > ../include/linux/compaction.h:237:51: warning: its scope is only this definition or declaration, which is probably not what you want [enabled by default]
    > ../include/linux/compaction.h:242:54: warning: 'struct node' declared inside parameter list [enabled by default]
    >  static inline void compaction_unregister_node(struct node *node)
    >
    
    It was caused by non-lru page migration which needs compaction.h but
    compaction.h doesn't include any header to be standalone.
    
    I think proper header for non-lru page migration is migrate.h rather
    than compaction.h because migrate.h has already headers needed to work
    non-lru page migration indirectly like isolate_mode_t, migrate_mode
    MIGRATEPAGE_SUCCESS.
    
    [akpm@linux-foundation.org: revert mm-balloon-use-general-non-lru-movable-page-feature-fix.patch temp fix]
    Link: http://lkml.kernel.org/r/20160610003304.GE29779@bbox
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Gioh Kim <gi-oh.kim@profitbricks.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 404fbfefeb33..ae8d475a9385 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -71,6 +71,21 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 #endif /* CONFIG_MIGRATION */
 
+#ifdef CONFIG_COMPACTION
+extern int PageMovable(struct page *page);
+extern void __SetPageMovable(struct page *page, struct address_space *mapping);
+extern void __ClearPageMovable(struct page *page);
+#else
+static inline int PageMovable(struct page *page) { return 0; };
+static inline void __SetPageMovable(struct page *page,
+				struct address_space *mapping)
+{
+}
+static inline void __ClearPageMovable(struct page *page)
+{
+}
+#endif
+
 #ifdef CONFIG_NUMA_BALANCING
 extern bool pmd_trans_migrating(pmd_t pmd);
 extern int migrate_misplaced_page(struct page *page,

commit bda807d4445414e8e77da704f116bb0880fe0c76
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 26 15:23:05 2016 -0700

    mm: migrate: support non-lru movable page migration
    
    We have allowed migration for only LRU pages until now and it was enough
    to make high-order pages.  But recently, embedded system(e.g., webOS,
    android) uses lots of non-movable pages(e.g., zram, GPU memory) so we
    have seen several reports about troubles of small high-order allocation.
    For fixing the problem, there were several efforts (e,g,.  enhance
    compaction algorithm, SLUB fallback to 0-order page, reserved memory,
    vmalloc and so on) but if there are lots of non-movable pages in system,
    their solutions are void in the long run.
    
    So, this patch is to support facility to change non-movable pages with
    movable.  For the feature, this patch introduces functions related to
    migration to address_space_operations as well as some page flags.
    
    If a driver want to make own pages movable, it should define three
    functions which are function pointers of struct
    address_space_operations.
    
    1. bool (*isolate_page) (struct page *page, isolate_mode_t mode);
    
    What VM expects on isolate_page function of driver is to return *true*
    if driver isolates page successfully.  On returing true, VM marks the
    page as PG_isolated so concurrent isolation in several CPUs skip the
    page for isolation.  If a driver cannot isolate the page, it should
    return *false*.
    
    Once page is successfully isolated, VM uses page.lru fields so driver
    shouldn't expect to preserve values in that fields.
    
    2. int (*migratepage) (struct address_space *mapping,
                    struct page *newpage, struct page *oldpage, enum migrate_mode);
    
    After isolation, VM calls migratepage of driver with isolated page.  The
    function of migratepage is to move content of the old page to new page
    and set up fields of struct page newpage.  Keep in mind that you should
    indicate to the VM the oldpage is no longer movable via
    __ClearPageMovable() under page_lock if you migrated the oldpage
    successfully and returns 0.  If driver cannot migrate the page at the
    moment, driver can return -EAGAIN.  On -EAGAIN, VM will retry page
    migration in a short time because VM interprets -EAGAIN as "temporal
    migration failure".  On returning any error except -EAGAIN, VM will give
    up the page migration without retrying in this time.
    
    Driver shouldn't touch page.lru field VM using in the functions.
    
    3. void (*putback_page)(struct page *);
    
    If migration fails on isolated page, VM should return the isolated page
    to the driver so VM calls driver's putback_page with migration failed
    page.  In this function, driver should put the isolated page back to the
    own data structure.
    
    4. non-lru movable page flags
    
    There are two page flags for supporting non-lru movable page.
    
    * PG_movable
    
    Driver should use the below function to make page movable under
    page_lock.
    
            void __SetPageMovable(struct page *page, struct address_space *mapping)
    
    It needs argument of address_space for registering migration family
    functions which will be called by VM.  Exactly speaking, PG_movable is
    not a real flag of struct page.  Rather than, VM reuses page->mapping's
    lower bits to represent it.
    
            #define PAGE_MAPPING_MOVABLE 0x2
            page->mapping = page->mapping | PAGE_MAPPING_MOVABLE;
    
    so driver shouldn't access page->mapping directly.  Instead, driver
    should use page_mapping which mask off the low two bits of page->mapping
    so it can get right struct address_space.
    
    For testing of non-lru movable page, VM supports __PageMovable function.
    However, it doesn't guarantee to identify non-lru movable page because
    page->mapping field is unified with other variables in struct page.  As
    well, if driver releases the page after isolation by VM, page->mapping
    doesn't have stable value although it has PAGE_MAPPING_MOVABLE (Look at
    __ClearPageMovable).  But __PageMovable is cheap to catch whether page
    is LRU or non-lru movable once the page has been isolated.  Because LRU
    pages never can have PAGE_MAPPING_MOVABLE in page->mapping.  It is also
    good for just peeking to test non-lru movable pages before more
    expensive checking with lock_page in pfn scanning to select victim.
    
    For guaranteeing non-lru movable page, VM provides PageMovable function.
    Unlike __PageMovable, PageMovable functions validates page->mapping and
    mapping->a_ops->isolate_page under lock_page.  The lock_page prevents
    sudden destroying of page->mapping.
    
    Driver using __SetPageMovable should clear the flag via
    __ClearMovablePage under page_lock before the releasing the page.
    
    * PG_isolated
    
    To prevent concurrent isolation among several CPUs, VM marks isolated
    page as PG_isolated under lock_page.  So if a CPU encounters PG_isolated
    non-lru movable page, it can skip it.  Driver doesn't need to manipulate
    the flag because VM will set/clear it automatically.  Keep in mind that
    if driver sees PG_isolated page, it means the page have been isolated by
    VM so it shouldn't touch page.lru field.  PG_isolated is alias with
    PG_reclaim flag so driver shouldn't use the flag for own purpose.
    
    [opensource.ganesh@gmail.com: mm/compaction: remove local variable is_lru]
      Link: http://lkml.kernel.org/r/20160618014841.GA7422@leo-test
    Link: http://lkml.kernel.org/r/1464736881-24886-3-git-send-email-minchan@kernel.org
    Signed-off-by: Gioh Kim <gi-oh.kim@profitbricks.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Ganesh Mahendran <opensource.ganesh@gmail.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: John Einar Reitan <john.reitan@foss.arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 9b50325e4ddf..404fbfefeb33 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -37,6 +37,8 @@ extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 		unsigned long private, enum migrate_mode mode, int reason);
+extern bool isolate_movable_page(struct page *page, isolate_mode_t mode);
+extern void putback_movable_page(struct page *page);
 
 extern int migrate_prep(void);
 extern int migrate_prep_local(void);

commit 7cd12b4abfd2f8f42414c520bbd051a5b7dc7a8c
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Mar 15 14:56:18 2016 -0700

    mm, page_owner: track and print last migrate reason
    
    During migration, page_owner info is now copied with the rest of the
    page, so the stacktrace leading to free page allocation during migration
    is overwritten.  For debugging purposes, it might be however useful to
    know that the page has been migrated since its initial allocation.  This
    might happen many times during the lifetime for different reasons and
    fully tracking this, especially with stacktraces would incur extra
    memory costs.  As a compromise, store and print the migrate_reason of
    the last migration that occurred to the page.  This is enough to
    distinguish compaction, numa balancing etc.
    
    Example page_owner entry after the patch:
    
      Page allocated via order 0, mask 0x24200ca(GFP_HIGHUSER_MOVABLE)
      PFN 628753 type Movable Block 1228 type Movable Flags 0x1fffff80040030(dirty|lru|swapbacked)
       [<ffffffff811682c4>] __alloc_pages_nodemask+0x134/0x230
       [<ffffffff811b6325>] alloc_pages_vma+0xb5/0x250
       [<ffffffff81177491>] shmem_alloc_page+0x61/0x90
       [<ffffffff8117a438>] shmem_getpage_gfp+0x678/0x960
       [<ffffffff8117c2b9>] shmem_fallocate+0x329/0x440
       [<ffffffff811de600>] vfs_fallocate+0x140/0x230
       [<ffffffff811df434>] SyS_fallocate+0x44/0x70
       [<ffffffff8158cc2e>] entry_SYSCALL_64_fastpath+0x12/0x71
      Page has been migrated, last migrate reason: compaction
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index cac1c0904d5f..9b50325e4ddf 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -23,9 +23,13 @@ enum migrate_reason {
 	MR_SYSCALL,		/* also applies to cpusets */
 	MR_MEMPOLICY_MBIND,
 	MR_NUMA_MISPLACED,
-	MR_CMA
+	MR_CMA,
+	MR_TYPES
 };
 
+/* In mm/debug.c; also keep sync with include/trace/events/migrate.h */
+extern char *migrate_reason_names[MR_TYPES];
+
 #ifdef CONFIG_MIGRATION
 
 extern void putback_movable_pages(struct list_head *l);

commit 2a8e70026435ad97570a1e0a0c4c941e0f700a3e
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Apr 14 15:48:15 2015 -0700

    mm: numa: remove migrate_ratelimited
    
    This code is dead since commit 9e645ab6d089 ("sched/numa: Continue PTE
    scanning even if migrate rate limited") so remove it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 78baed5f2952..cac1c0904d5f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -69,7 +69,6 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 extern bool pmd_trans_migrating(pmd_t pmd);
 extern int migrate_misplaced_page(struct page *page,
 				  struct vm_area_struct *vma, int node);
-extern bool migrate_ratelimited(int node);
 #else
 static inline bool pmd_trans_migrating(pmd_t pmd)
 {
@@ -80,10 +79,6 @@ static inline int migrate_misplaced_page(struct page *page,
 {
 	return -EAGAIN; /* can't migrate now */
 }
-static inline bool migrate_ratelimited(int node)
-{
-	return false;
-}
 #endif /* CONFIG_NUMA_BALANCING */
 
 #if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_TRANSPARENT_HUGEPAGE)

commit 5d833062139d290adb8b62c093b654a01a353448
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Feb 12 14:58:16 2015 -0800

    mm: numa: do not dereference pmd outside of the lock during NUMA hinting fault
    
    Automatic NUMA balancing depends on being able to protect PTEs to trap a
    fault and gather reference locality information.  Very broadly speaking
    it would mark PTEs as not present and use another bit to distinguish
    between NUMA hinting faults and other types of faults.  It was
    universally loved by everybody and caused no problems whatsoever.  That
    last sentence might be a lie.
    
    This series is very heavily based on patches from Linus and Aneesh to
    replace the existing PTE/PMD NUMA helper functions with normal change
    protections.  I did alter and add parts of it but I consider them
    relatively minor contributions.  At their suggestion, acked-bys are in
    there but I've no problem converting them to Signed-off-by if requested.
    
    AFAIK, this has received no testing on ppc64 and I'm depending on Aneesh
    for that.  I tested trinity under kvm-tool and passed and ran a few
    other basic tests.  At the time of writing, only the short-lived tests
    have completed but testing of V2 indicated that long-term testing had no
    surprises.  In most cases I'm leaving out detail as it's not that
    interesting.
    
    specjbb single JVM: There was negligible performance difference in the
            benchmark itself for short runs. However, system activity is
            higher and interrupts are much higher over time -- possibly TLB
            flushes. Migrations are also higher. Overall, this is more overhead
            but considering the problems faced with the old approach I think
            we just have to suck it up and find another way of reducing the
            overhead.
    
    specjbb multi JVM: Negligible performance difference to the actual benchmark
            but like the single JVM case, the system overhead is noticeably
            higher.  Again, interrupts are a major factor.
    
    autonumabench: This was all over the place and about all that can be
            reasonably concluded is that it's different but not necessarily
            better or worse.
    
    autonumabench
                                         3.18.0-rc5            3.18.0-rc5
                                     mmotm-20141119         protnone-v3r3
    User    NUMA01               32380.24 (  0.00%)    21642.92 ( 33.16%)
    User    NUMA01_THEADLOCAL    22481.02 (  0.00%)    22283.22 (  0.88%)
    User    NUMA02                3137.00 (  0.00%)     3116.54 (  0.65%)
    User    NUMA02_SMT            1614.03 (  0.00%)     1543.53 (  4.37%)
    System  NUMA01                 322.97 (  0.00%)     1465.89 (-353.88%)
    System  NUMA01_THEADLOCAL       91.87 (  0.00%)       49.32 ( 46.32%)
    System  NUMA02                  37.83 (  0.00%)       14.61 ( 61.38%)
    System  NUMA02_SMT               7.36 (  0.00%)        7.45 ( -1.22%)
    Elapsed NUMA01                 716.63 (  0.00%)      599.29 ( 16.37%)
    Elapsed NUMA01_THEADLOCAL      553.98 (  0.00%)      539.94 (  2.53%)
    Elapsed NUMA02                  83.85 (  0.00%)       83.04 (  0.97%)
    Elapsed NUMA02_SMT              86.57 (  0.00%)       79.15 (  8.57%)
    CPU     NUMA01                4563.00 (  0.00%)     3855.00 ( 15.52%)
    CPU     NUMA01_THEADLOCAL     4074.00 (  0.00%)     4136.00 ( -1.52%)
    CPU     NUMA02                3785.00 (  0.00%)     3770.00 (  0.40%)
    CPU     NUMA02_SMT            1872.00 (  0.00%)     1959.00 ( -4.65%)
    
    System CPU usage of NUMA01 is worse but it's an adverse workload on this
    machine so I'm reluctant to conclude that it's a problem that matters.  On
    the other workloads that are sensible on this machine, system CPU usage is
    great.  Overall time to complete the benchmark is comparable
    
              3.18.0-rc5  3.18.0-rc5
            mmotm-20141119protnone-v3r3
    User        59612.50    48586.44
    System        460.22     1537.45
    Elapsed      1442.20     1304.29
    
    NUMA alloc hit                 5075182     5743353
    NUMA alloc miss                      0           0
    NUMA interleave hit                  0           0
    NUMA alloc local               5075174     5743339
    NUMA base PTE updates        637061448   443106883
    NUMA huge PMD updates          1243434      864747
    NUMA page range updates     1273699656   885857347
    NUMA hint faults               1658116     1214277
    NUMA hint local faults          959487      754113
    NUMA hint local percent             57          62
    NUMA pages migrated            5467056    61676398
    
    The NUMA pages migrated look terrible but when I looked at a graph of the
    activity over time I see that the massive spike in migration activity was
    during NUMA01.  This correlates with high system CPU usage and could be
    simply down to bad luck but any modifications that affect that workload
    would be related to scan rates and migrations, not the protection
    mechanism.  For all other workloads, migration activity was comparable.
    
    Overall, headline performance figures are comparable but the overhead is
    higher, mostly in interrupts.  To some extent, higher overhead from this
    approach was anticipated but not to this degree.  It's going to be
    necessary to reduce this again with a separate series in the future.  It's
    still worth going ahead with this series though as it's likely to avoid
    constant headaches with Xen and is probably easier to maintain.
    
    This patch (of 10):
    
    A transhuge NUMA hinting fault may find the page is migrating and should
    wait until migration completes.  The check is race-prone because the pmd
    is deferenced outside of the page lock and while the race is tiny, it'll
    be larger if the PMD is cleared while marking PMDs for hinting fault.
    This patch closes the race.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Kirill Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index fab9b32ace8e..78baed5f2952 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -67,7 +67,6 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 #ifdef CONFIG_NUMA_BALANCING
 extern bool pmd_trans_migrating(pmd_t pmd);
-extern void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd);
 extern int migrate_misplaced_page(struct page *page,
 				  struct vm_area_struct *vma, int node);
 extern bool migrate_ratelimited(int node);
@@ -76,9 +75,6 @@ static inline bool pmd_trans_migrating(pmd_t pmd)
 {
 	return false;
 }
-static inline void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd)
-{
-}
 static inline int migrate_misplaced_page(struct page *page,
 					 struct vm_area_struct *vma, int node)
 {

commit 50062175ffc844b8ff9664024c6416a37ad63c77
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu May 15 05:06:42 2014 -0400

    vm_area_operations: kill ->migrate()
    
    the only instance this method has ever grown was one in kernfs -
    one that call ->migrate() of another vm_ops if it exists.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 01aad3ed89ec..fab9b32ace8e 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -36,9 +36,6 @@ extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 
 extern int migrate_prep(void);
 extern int migrate_prep_local(void);
-extern int migrate_vmas(struct mm_struct *mm,
-		const nodemask_t *from, const nodemask_t *to,
-		unsigned long flags);
 extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
@@ -57,13 +54,6 @@ static inline int migrate_pages(struct list_head *l, new_page_t new,
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }
 
-static inline int migrate_vmas(struct mm_struct *mm,
-		const nodemask_t *from, const nodemask_t *to,
-		unsigned long flags)
-{
-	return -ENOSYS;
-}
-
 static inline void migrate_page_copy(struct page *newpage,
 				     struct page *page) {}
 

commit d6d86c0a7f8ddc5b38cf089222cb1d9540762dc2
Author: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
Date:   Thu Oct 9 15:29:27 2014 -0700

    mm/balloon_compaction: redesign ballooned pages management
    
    Sasha Levin reported KASAN splash inside isolate_migratepages_range().
    Problem is in the function __is_movable_balloon_page() which tests
    AS_BALLOON_MAP in page->mapping->flags.  This function has no protection
    against anonymous pages.  As result it tried to check address space flags
    inside struct anon_vma.
    
    Further investigation shows more problems in current implementation:
    
    * Special branch in __unmap_and_move() never works:
      balloon_page_movable() checks page flags and page_count.  In
      __unmap_and_move() page is locked, reference counter is elevated, thus
      balloon_page_movable() always fails.  As a result execution goes to the
      normal migration path.  virtballoon_migratepage() returns
      MIGRATEPAGE_BALLOON_SUCCESS instead of MIGRATEPAGE_SUCCESS,
      move_to_new_page() thinks this is an error code and assigns
      newpage->mapping to NULL.  Newly migrated page lose connectivity with
      balloon an all ability for further migration.
    
    * lru_lock erroneously required in isolate_migratepages_range() for
      isolation ballooned page.  This function releases lru_lock periodically,
      this makes migration mostly impossible for some pages.
    
    * balloon_page_dequeue have a tight race with balloon_page_isolate:
      balloon_page_isolate could be executed in parallel with dequeue between
      picking page from list and locking page_lock.  Race is rare because they
      use trylock_page() for locking.
    
    This patch fixes all of them.
    
    Instead of fake mapping with special flag this patch uses special state of
    page->_mapcount: PAGE_BALLOON_MAPCOUNT_VALUE = -256.  Buddy allocator uses
    PAGE_BUDDY_MAPCOUNT_VALUE = -128 for similar purpose.  Storing mark
    directly in struct page makes everything safer and easier.
    
    PagePrivate is used to mark pages present in page list (i.e.  not
    isolated, like PageLRU for normal pages).  It replaces special rules for
    reference counter and makes balloon migration similar to migration of
    normal pages.  This flag is protected by page_lock together with link to
    the balloon device.
    
    Signed-off-by: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/p/53E6CEAA.9020105@oracle.com
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: <stable@vger.kernel.org>    [3.8+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index b66fd10f4b93..01aad3ed89ec 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -13,18 +13,9 @@ typedef void free_page_t(struct page *page, unsigned long private);
  * Return values from addresss_space_operations.migratepage():
  * - negative errno on page migration failure;
  * - zero on page migration success;
- *
- * The balloon page migration introduces this special case where a 'distinct'
- * return code is used to flag a successful page migration to unmap_and_move().
- * This approach is necessary because page migration can race against balloon
- * deflation procedure, and for such case we could introduce a nasty page leak
- * if a successfully migrated balloon page gets released concurrently with
- * migration's unmap_and_move() wrap-up steps.
  */
 #define MIGRATEPAGE_SUCCESS		0
-#define MIGRATEPAGE_BALLOON_SUCCESS	1 /* special ret code for balloon page
-					   * sucessful migration case.
-					   */
+
 enum migrate_reason {
 	MR_COMPACTION,
 	MR_MEMORY_FAILURE,

commit 1c93923cc264105418e6ead149c76bd88302eff4
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Oct 9 15:27:59 2014 -0700

    include/linux/migrate.h: remove migrate_page #define
    
    This is designed to avoid a few ifdefs in .c files but it's obnoxious
    because it can cause unsuspecting "migrate_page" symbols to get turned into
    "NULL".
    
    Just nuke it and use the ifdefs.
    
    Cc: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index a2901c414664..b66fd10f4b93 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -82,9 +82,6 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 	return -ENOSYS;
 }
 
-/* Possible settings for the migrate_page() method in address_operations */
-#define migrate_page NULL
-
 #endif /* CONFIG_MIGRATION */
 
 #ifdef CONFIG_NUMA_BALANCING

commit 68711a746345c44ae00c64d8dbac6a9ce13ac54a
Author: David Rientjes <rientjes@google.com>
Date:   Wed Jun 4 16:08:25 2014 -0700

    mm, migration: add destination page freeing callback
    
    Memory migration uses a callback defined by the caller to determine how to
    allocate destination pages.  When migration fails for a source page,
    however, it frees the destination page back to the system.
    
    This patch adds a memory migration callback defined by the caller to
    determine how to free destination pages.  If a caller, such as memory
    compaction, builds its own freelist for migration targets, this can reuse
    already freed memory instead of scanning additional memory.
    
    If the caller provides a function to handle freeing of destination pages,
    it is called when page migration fails.  If the caller passes NULL then
    freeing back to the system will be handled as usual.  This patch
    introduces no functional change.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 84a31ad0b791..a2901c414664 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -5,7 +5,9 @@
 #include <linux/mempolicy.h>
 #include <linux/migrate_mode.h>
 
-typedef struct page *new_page_t(struct page *, unsigned long private, int **);
+typedef struct page *new_page_t(struct page *page, unsigned long private,
+				int **reason);
+typedef void free_page_t(struct page *page, unsigned long private);
 
 /*
  * Return values from addresss_space_operations.migratepage():
@@ -38,7 +40,7 @@ enum migrate_reason {
 extern void putback_movable_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
-extern int migrate_pages(struct list_head *l, new_page_t x,
+extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 		unsigned long private, enum migrate_mode mode, int reason);
 
 extern int migrate_prep(void);
@@ -56,8 +58,9 @@ extern int migrate_page_move_mapping(struct address_space *mapping,
 #else
 
 static inline void putback_movable_pages(struct list_head *l) {}
-static inline int migrate_pages(struct list_head *l, new_page_t x,
-		unsigned long private, enum migrate_mode mode, int reason)
+static inline int migrate_pages(struct list_head *l, new_page_t new,
+		free_page_t free, unsigned long private, enum migrate_mode mode,
+		int reason)
 	{ return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }

commit 78d5506e82b21a1a1de68c24182db2c2fe521422
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Jan 21 15:51:18 2014 -0800

    mm/migrate: remove unused function, fail_migrate_page()
    
    fail_migrate_page() isn't used anywhere, so remove it.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 2a411bc0097f..84a31ad0b791 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -41,9 +41,6 @@ extern int migrate_page(struct address_space *,
 extern int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, enum migrate_mode mode, int reason);
 
-extern int fail_migrate_page(struct address_space *,
-			struct page *, struct page *);
-
 extern int migrate_prep(void);
 extern int migrate_prep_local(void);
 extern int migrate_vmas(struct mm_struct *mm,
@@ -84,7 +81,6 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 /* Possible settings for the migrate_page() method in address_operations */
 #define migrate_page NULL
-#define fail_migrate_page NULL
 
 #endif /* CONFIG_MIGRATION */
 

commit 59c82b70dcd9cc273c21fae5abc29e41fc732a17
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Jan 21 15:51:17 2014 -0800

    mm/migrate: remove putback_lru_pages, fix comment on putback_movable_pages
    
    Some part of putback_lru_pages() and putback_movable_pages() is
    duplicated, so it could confuse us what we should use.  We can remove
    putback_lru_pages() since it is not really needed now.  This makes us
    undestand and maintain the code more easily.
    
    And comment on putback_movable_pages() is stale now, so fix it.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index f015c059e159..2a411bc0097f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -35,7 +35,6 @@ enum migrate_reason {
 
 #ifdef CONFIG_MIGRATION
 
-extern void putback_lru_pages(struct list_head *l);
 extern void putback_movable_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
@@ -59,7 +58,6 @@ extern int migrate_page_move_mapping(struct address_space *mapping,
 		int extra_count);
 #else
 
-static inline void putback_lru_pages(struct list_head *l) {}
 static inline void putback_movable_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, enum migrate_mode mode, int reason)

commit 8e321fefb0e60bae4e2a28d20fc4fa30758d27c6
Author: Benjamin LaHaise <bcrl@kvack.org>
Date:   Sat Dec 21 17:56:08 2013 -0500

    aio/migratepages: make aio migrate pages sane
    
    The arbitrary restriction on page counts offered by the core
    migrate_page_move_mapping() code results in rather suspicious looking
    fiddling with page reference counts in the aio_migratepage() operation.
    To fix this, make migrate_page_move_mapping() take an extra_count parameter
    that allows aio to tell the code about its own reference count on the page
    being migrated.
    
    While cleaning up aio_migratepage(), make it validate that the old page
    being passed in is actually what aio_migratepage() expects to prevent
    misbehaviour in the case of races.
    
    Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index b7717d74da7f..f015c059e159 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -55,7 +55,8 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
 extern int migrate_page_move_mapping(struct address_space *mapping,
 		struct page *newpage, struct page *page,
-		struct buffer_head *head, enum migrate_mode mode);
+		struct buffer_head *head, enum migrate_mode mode,
+		int extra_count);
 #else
 
 static inline void putback_lru_pages(struct list_head *l) {}

commit de466bd628e8d663fdf3f791bc8db318ee85c714
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Dec 18 17:08:42 2013 -0800

    mm: numa: avoid unnecessary disruption of NUMA hinting during migration
    
    do_huge_pmd_numa_page() handles the case where there is parallel THP
    migration.  However, by the time it is checked the NUMA hinting
    information has already been disrupted.  This patch adds an earlier
    check with some helpers.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index f5096b58b20d..b7717d74da7f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -90,10 +90,19 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 #endif /* CONFIG_MIGRATION */
 
 #ifdef CONFIG_NUMA_BALANCING
+extern bool pmd_trans_migrating(pmd_t pmd);
+extern void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd);
 extern int migrate_misplaced_page(struct page *page,
 				  struct vm_area_struct *vma, int node);
 extern bool migrate_ratelimited(int node);
 #else
+static inline bool pmd_trans_migrating(pmd_t pmd)
+{
+	return false;
+}
+static inline void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd)
+{
+}
 static inline int migrate_misplaced_page(struct page *page,
 					 struct vm_area_struct *vma, int node)
 {

commit 1bc115d87dffd1c43bdc3c9c9d1e3a51c195d18e
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:05 2013 +0100

    mm: numa: Scan pages with elevated page_mapcount
    
    Currently automatic NUMA balancing is unable to distinguish between false
    shared versus private pages except by ignoring pages with an elevated
    page_mapcount entirely. This avoids shared pages bouncing between the
    nodes whose task is using them but that is ignored quite a lot of data.
    
    This patch kicks away the training wheels in preparation for adding support
    for identifying shared/private pages is now in place. The ordering is so
    that the impact of the shared/private detection can be easily measured. Note
    that the patch does not migrate shared, file-backed within vmas marked
    VM_EXEC as these are generally shared library pages. Migrating such pages
    is not beneficial as there is an expectation they are read-shared between
    caches and iTLB and iCache pressure is generally low.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-28-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 8d3c57fdf221..f5096b58b20d 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -90,11 +90,12 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 #endif /* CONFIG_MIGRATION */
 
 #ifdef CONFIG_NUMA_BALANCING
-extern int migrate_misplaced_page(struct page *page, int node);
-extern int migrate_misplaced_page(struct page *page, int node);
+extern int migrate_misplaced_page(struct page *page,
+				  struct vm_area_struct *vma, int node);
 extern bool migrate_ratelimited(int node);
 #else
-static inline int migrate_misplaced_page(struct page *page, int node)
+static inline int migrate_misplaced_page(struct page *page,
+					 struct vm_area_struct *vma, int node)
 {
 	return -EAGAIN; /* can't migrate now */
 }

commit 9bf12df31f282e845b3dfaac1e5d5376a041da22
Merge: 399a946edbbe d9b2c8714aef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 13 10:55:58 2013 -0700

    Merge git://git.kvack.org/~bcrl/aio-next
    
    Pull aio changes from Ben LaHaise:
     "First off, sorry for this pull request being late in the merge window.
      Al had raised a couple of concerns about 2 items in the series below.
      I addressed the first issue (the race introduced by Gu's use of
      mm_populate()), but he has not provided any further details on how he
      wants to rework the anon_inode.c changes (which were sent out months
      ago but have yet to be commented on).
    
      The bulk of the changes have been sitting in the -next tree for a few
      months, with all the issues raised being addressed"
    
    * git://git.kvack.org/~bcrl/aio-next: (22 commits)
      aio: rcu_read_lock protection for new rcu_dereference calls
      aio: fix race in ring buffer page lookup introduced by page migration support
      aio: fix rcu sparse warnings introduced by ioctx table lookup patch
      aio: remove unnecessary debugging from aio_free_ring()
      aio: table lookup: verify ctx pointer
      staging/lustre: kiocb->ki_left is removed
      aio: fix error handling and rcu usage in "convert the ioctx list to table lookup v3"
      aio: be defensive to ensure request batching is non-zero instead of BUG_ON()
      aio: convert the ioctx list to table lookup v3
      aio: double aio_max_nr in calculations
      aio: Kill ki_dtor
      aio: Kill ki_users
      aio: Kill unneeded kiocb members
      aio: Kill aio_rw_vect_retry()
      aio: Don't use ctx->tail unnecessarily
      aio: io_cancel() no longer returns the io_event
      aio: percpu ioctx refcount
      aio: percpu reqs_available
      aio: reqs_active -> reqs_available
      aio: fix build when migration is disabled
      ...

commit b8ec1cee5a4375c1244b85709138a2eac2d89cb6
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:01 2013 -0700

    mm: soft-offline: use migrate_pages() instead of migrate_huge_page()
    
    Currently migrate_huge_page() takes a pointer to a hugepage to be migrated
    as an argument, instead of taking a pointer to the list of hugepages to be
    migrated.  This behavior was introduced in commit 189ebff28 ("hugetlb:
    simplify migrate_huge_page()"), and was OK because until now hugepage
    migration is enabled only for soft-offlining which migrates only one
    hugepage in a single call.
    
    But the situation will change in the later patches in this series which
    enable other users of page migration to support hugepage migration.  They
    can kick migration for both of normal pages and hugepages in a single
    call, so we need to go back to original implementation which uses linked
    lists to collect the hugepages to be migrated.
    
    With this patch, soft_offline_huge_page() switches to use migrate_pages(),
    and migrate_huge_page() is not used any more.  So let's remove it.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index a405d3dc0f61..6fe521420631 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -41,8 +41,6 @@ extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, enum migrate_mode mode, int reason);
-extern int migrate_huge_page(struct page *, new_page_t x,
-		unsigned long private, enum migrate_mode mode);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -62,9 +60,6 @@ static inline void putback_movable_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, enum migrate_mode mode, int reason)
 	{ return -ENOSYS; }
-static inline int migrate_huge_page(struct page *page, new_page_t x,
-		unsigned long private, enum migrate_mode mode)
-	{ return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }

commit 36bc08cc01709b4a9bb563b35aa530241ddc63e3
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Tue Jul 16 17:56:16 2013 +0800

    fs/aio: Add support to aio ring pages migration
    
    As the aio job will pin the ring pages, that will lead to mem migrated
    failed. In order to fix this problem we use an anon inode to manage the aio ring
    pages, and  setup the migratepage callback in the anon inode's address space, so
    that when mem migrating the aio ring pages will be moved to other mem node safely.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index a405d3dc0f61..c407d88f5979 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -55,6 +55,9 @@ extern int migrate_vmas(struct mm_struct *mm,
 extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
+extern int migrate_page_move_mapping(struct address_space *mapping,
+		struct page *newpage, struct page *page,
+		struct buffer_head *head, enum migrate_mode mode);
 #else
 
 static inline void putback_lru_pages(struct list_head *l) {}

commit 9c620e2bc5aa4256c102ada34e6c76204ed5898b
Author: Hugh Dickins <hughd@google.com>
Date:   Fri Feb 22 16:35:14 2013 -0800

    mm: remove offlining arg to migrate_pages
    
    No functional change, but the only purpose of the offlining argument to
    migrate_pages() etc, was to ensure that __unmap_and_move() could migrate a
    KSM page for memory hotremove (which took ksm_thread_mutex) but not for
    other callers.  Now all cases are safe, remove the arg.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Petr Holasek <pholasek@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Izik Eidus <izik.eidus@ravellosystems.com>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 1e9f627967a3..a405d3dc0f61 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -40,11 +40,9 @@ extern void putback_movable_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t x,
-			unsigned long private, bool offlining,
-			enum migrate_mode mode, int reason);
+		unsigned long private, enum migrate_mode mode, int reason);
 extern int migrate_huge_page(struct page *, new_page_t x,
-			unsigned long private, bool offlining,
-			enum migrate_mode mode);
+		unsigned long private, enum migrate_mode mode);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -62,11 +60,11 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline void putback_movable_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
-		unsigned long private, bool offlining,
-		enum migrate_mode mode, int reason) { return -ENOSYS; }
+		unsigned long private, enum migrate_mode mode, int reason)
+	{ return -ENOSYS; }
 static inline int migrate_huge_page(struct page *page, new_page_t x,
-		unsigned long private, bool offlining,
-		enum migrate_mode mode) { return -ENOSYS; }
+		unsigned long private, enum migrate_mode mode)
+	{ return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 5733c7d11dff44e98d2ca16617886a78086b354f
Author: Rafael Aquini <aquini@redhat.com>
Date:   Tue Dec 11 16:02:47 2012 -0800

    mm: introduce putback_movable_pages()
    
    The PATCH "mm: introduce compaction and migration for virtio ballooned pages"
    hacks around putback_lru_pages() in order to allow ballooned pages to be
    re-inserted on balloon page list as if a ballooned page was like a LRU page.
    
    As ballooned pages are not legitimate LRU pages, this patch introduces
    putback_movable_pages() to properly cope with cases where the isolated
    pageset contains ballooned pages and LRU pages, thus fixing the mentioned
    inelegant hack around putback_lru_pages().
    
    Signed-off-by: Rafael Aquini <aquini@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ce42847ed35f..0b5865c61efd 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -27,6 +27,7 @@ typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 #ifdef CONFIG_MIGRATION
 
 extern void putback_lru_pages(struct list_head *l);
+extern void putback_movable_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t x,
@@ -50,6 +51,7 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 #else
 
 static inline void putback_lru_pages(struct list_head *l) {}
+static inline void putback_movable_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, bool offlining,
 		enum migrate_mode mode) { return -ENOSYS; }

commit 18468d93e53b037e1a04ec58398eab763d054064
Author: Rafael Aquini <aquini@redhat.com>
Date:   Tue Dec 11 16:02:38 2012 -0800

    mm: introduce a common interface for balloon pages mobility
    
    Memory fragmentation introduced by ballooning might reduce significantly
    the number of 2MB contiguous memory blocks that can be used within a guest,
    thus imposing performance penalties associated with the reduced number of
    transparent huge pages that could be used by the guest workload.
    
    This patch introduces a common interface to help a balloon driver on
    making its page set movable to compaction, and thus allowing the system
    to better leverage the compation efforts on memory defragmentation.
    
    [akpm@linux-foundation.org: use PAGE_FLAGS_CHECK_AT_PREP, s/__balloon_page_flags/page_flags_cleared/, small cleanups]
    [rientjes@google.com: allow balloon compaction for any system with memory compaction enabled, which is the defconfig]
    Signed-off-by: Rafael Aquini <aquini@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index a4e886d17f87..ce42847ed35f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -11,8 +11,18 @@ typedef struct page *new_page_t(struct page *, unsigned long private, int **);
  * Return values from addresss_space_operations.migratepage():
  * - negative errno on page migration failure;
  * - zero on page migration success;
+ *
+ * The balloon page migration introduces this special case where a 'distinct'
+ * return code is used to flag a successful page migration to unmap_and_move().
+ * This approach is necessary because page migration can race against balloon
+ * deflation procedure, and for such case we could introduce a nasty page leak
+ * if a successfully migrated balloon page gets released concurrently with
+ * migration's unmap_and_move() wrap-up steps.
  */
 #define MIGRATEPAGE_SUCCESS		0
+#define MIGRATEPAGE_BALLOON_SUCCESS	1 /* special ret code for balloon page
+					   * sucessful migration case.
+					   */
 
 #ifdef CONFIG_MIGRATION
 

commit 78bd52097d04205a33a8014a1b8ac01cf1ae9d06
Author: Rafael Aquini <aquini@redhat.com>
Date:   Tue Dec 11 16:02:31 2012 -0800

    mm: adjust address_space_operations.migratepage() return code
    
    Memory fragmentation introduced by ballooning might reduce significantly
    the number of 2MB contiguous memory blocks that can be used within a
    guest, thus imposing performance penalties associated with the reduced
    number of transparent huge pages that could be used by the guest workload.
    
    This patch-set follows the main idea discussed at 2012 LSFMMS session:
    "Ballooning for transparent huge pages" -- http://lwn.net/Articles/490114/
    to introduce the required changes to the virtio_balloon driver, as well as
    the changes to the core compaction & migration bits, in order to make
    those subsystems aware of ballooned pages and allow memory balloon pages
    become movable within a guest, thus avoiding the aforementioned
    fragmentation issue
    
    Following are numbers that prove this patch benefits on allowing
    compaction to be more effective at memory ballooned guests.
    
    Results for STRESS-HIGHALLOC benchmark, from Mel Gorman's mmtests suite,
    running on a 4gB RAM KVM guest which was ballooning 512mB RAM in 64mB
    chunks, at every minute (inflating/deflating), while test was running:
    
    ===BEGIN stress-highalloc
    
    STRESS-HIGHALLOC
                     highalloc-3.7     highalloc-3.7
                         rc4-clean         rc4-patch
    Pass 1          55.00 ( 0.00%)    62.00 ( 7.00%)
    Pass 2          54.00 ( 0.00%)    62.00 ( 8.00%)
    while Rested    75.00 ( 0.00%)    80.00 ( 5.00%)
    
    MMTests Statistics: duration
                     3.7         3.7
               rc4-clean   rc4-patch
    User         1207.59     1207.46
    System       1300.55     1299.61
    Elapsed      2273.72     2157.06
    
    MMTests Statistics: vmstat
                                    3.7         3.7
                              rc4-clean   rc4-patch
    Page Ins                    3581516     2374368
    Page Outs                  11148692    10410332
    Swap Ins                         80          47
    Swap Outs                      3641         476
    Direct pages scanned          37978       33826
    Kswapd pages scanned        1828245     1342869
    Kswapd pages reclaimed      1710236     1304099
    Direct pages reclaimed        32207       31005
    Kswapd efficiency               93%         97%
    Kswapd velocity             804.077     622.546
    Direct efficiency               84%         91%
    Direct velocity              16.703      15.682
    Percentage direct scans          2%          2%
    Page writes by reclaim        79252        9704
    Page writes file              75611        9228
    Page writes anon               3641         476
    Page reclaim immediate        16764       11014
    Page rescued immediate            0           0
    Slabs scanned               2171904     2152448
    Direct inode steals             385        2261
    Kswapd inode steals          659137      609670
    Kswapd skipped wait               1          69
    THP fault alloc                 546         631
    THP collapse alloc              361         339
    THP splits                      259         263
    THP fault fallback               98          50
    THP collapse fail                20          17
    Compaction stalls               747         499
    Compaction success              244         145
    Compaction failures             503         354
    Compaction pages moved       370888      474837
    Compaction move failure       77378       65259
    
    ===END stress-highalloc
    
    This patch:
    
    Introduce MIGRATEPAGE_SUCCESS as the default return code for
    address_space_operations.migratepage() method and documents the expected
    return code for the same method in failure cases.
    
    Signed-off-by: Rafael Aquini <aquini@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ce7e6671968b..a4e886d17f87 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,6 +7,13 @@
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
+/*
+ * Return values from addresss_space_operations.migratepage():
+ * - negative errno on page migration failure;
+ * - zero on page migration success;
+ */
+#define MIGRATEPAGE_SUCCESS		0
+
 #ifdef CONFIG_MIGRATION
 
 extern void putback_lru_pages(struct list_head *l);

commit 220018d388b8ab1fca1c5f0c6474bab47ad2c9c0
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Dec 5 09:32:56 2012 +0000

    mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
    
    Commit "Add THP migration for the NUMA working set scanning fault case"
    breaks the build because HPAGE_PMD_SHIFT and HPAGE_PMD_MASK defined to
    explode without CONFIG_TRANSPARENT_HUGEPAGE:
    
    mm/migrate.c: In function 'migrate_misplaced_transhuge_page_put':
    mm/migrate.c:1549: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    mm/migrate.c:1564: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    mm/migrate.c:1566: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    mm/migrate.c:1573: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    mm/migrate.c:1606: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    mm/migrate.c:1648: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
    
    CONFIG_NUMA_BALANCING allows compilation without enabling transparent
    hugepages, so define the dummy function for such a configuration and only
    define migrate_misplaced_transhuge_page_put() when transparent hugepages
    are enabled.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 51eac4bdc606..d52afb9a790c 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -79,12 +79,6 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 extern int migrate_misplaced_page(struct page *page, int node);
 extern int migrate_misplaced_page(struct page *page, int node);
 extern bool migrate_ratelimited(int node);
-extern int migrate_misplaced_transhuge_page(struct mm_struct *mm,
-			struct vm_area_struct *vma,
-			pmd_t *pmd, pmd_t entry,
-			unsigned long address,
-			struct page *page, int node);
-
 #else
 static inline int migrate_misplaced_page(struct page *page, int node)
 {
@@ -94,7 +88,15 @@ static inline bool migrate_ratelimited(int node)
 {
 	return false;
 }
+#endif /* CONFIG_NUMA_BALANCING */
 
+#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_TRANSPARENT_HUGEPAGE)
+extern int migrate_misplaced_transhuge_page(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			pmd_t *pmd, pmd_t entry,
+			unsigned long address,
+			struct page *page, int node);
+#else
 static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 			struct vm_area_struct *vma,
 			pmd_t *pmd, pmd_t entry,
@@ -103,6 +105,6 @@ static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 {
 	return -EAGAIN;
 }
-#endif /* CONFIG_NUMA_BALANCING */
+#endif /* CONFIG_NUMA_BALANCING && CONFIG_TRANSPARENT_HUGEPAGE*/
 
 #endif /* _LINUX_MIGRATE_H */

commit b32967ff101a7508f70be8de59b278d4df92fa00
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Nov 19 12:35:47 2012 +0000

    mm: numa: Add THP migration for the NUMA working set scanning fault case.
    
    Note: This is very heavily based on a patch from Peter Zijlstra with
            fixes from Ingo Molnar, Hugh Dickins and Johannes Weiner.  That patch
            put a lot of migration logic into mm/huge_memory.c where it does
            not belong. This version puts tries to share some of the migration
            logic with migrate_misplaced_page.  However, it should be noted
            that now migrate.c is doing more with the pagetable manipulation
            than is preferred. The end result is barely recognisable so as
            before, the signed-offs had to be removed but will be re-added if
            the original authors are ok with it.
    
    Add THP migration for the NUMA working set scanning fault case.
    
    It uses the page lock to serialize. No migration pte dance is
    necessary because the pte is already unmapped when we decide
    to migrate.
    
    [dhillf@gmail.com: Fix memory leak on isolation failure]
    [dhillf@gmail.com: Fix transfer of last_nid information]
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 91556889adac..51eac4bdc606 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -79,6 +79,12 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 extern int migrate_misplaced_page(struct page *page, int node);
 extern int migrate_misplaced_page(struct page *page, int node);
 extern bool migrate_ratelimited(int node);
+extern int migrate_misplaced_transhuge_page(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			pmd_t *pmd, pmd_t entry,
+			unsigned long address,
+			struct page *page, int node);
+
 #else
 static inline int migrate_misplaced_page(struct page *page, int node)
 {
@@ -88,6 +94,15 @@ static inline bool migrate_ratelimited(int node)
 {
 	return false;
 }
+
+static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			pmd_t *pmd, pmd_t entry,
+			unsigned long address,
+			struct page *page, int node)
+{
+	return -EAGAIN;
+}
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* _LINUX_MIGRATE_H */

commit e14808b49f55e0e1135da5e4a154a540dd9f3662
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Nov 19 10:59:15 2012 +0000

    mm: numa: Rate limit setting of pte_numa if node is saturated
    
    If there are a large number of NUMA hinting faults and all of them
    are resulting in migrations it may indicate that memory is just
    bouncing uselessly around. NUMA balancing cost is likely exceeding
    any benefit from locality. Rate limit the PTE updates if the node
    is migration rate-limited. As noted in the comments, this distorts
    the NUMA faulting statistics.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index f0d0313eea6f..91556889adac 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -77,11 +77,17 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 #ifdef CONFIG_NUMA_BALANCING
 extern int migrate_misplaced_page(struct page *page, int node);
+extern int migrate_misplaced_page(struct page *page, int node);
+extern bool migrate_ratelimited(int node);
 #else
 static inline int migrate_misplaced_page(struct page *page, int node)
 {
 	return -EAGAIN; /* can't migrate now */
 }
+static inline bool migrate_ratelimited(int node)
+{
+	return false;
+}
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* _LINUX_MIGRATE_H */

commit 7039e1dbec6eeaa8ecab43a82d6589eeced995c3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 25 14:16:34 2012 +0200

    mm: migrate: Introduce migrate_misplaced_page()
    
    Note: This was originally based on Peter's patch "mm/migrate: Introduce
            migrate_misplaced_page()" but borrows extremely heavily from Andrea's
            "autonuma: memory follows CPU algorithm and task/mm_autonuma stats
            collection". The end result is barely recognisable so signed-offs
            had to be dropped. If original authors are ok with it, I'll
            re-add the signed-off-bys.
    
    Add migrate_misplaced_page() which deals with migrating pages from
    faults.
    
    Based-on-work-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Based-on-work-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Based-on-work-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 9d1c159e2427..f0d0313eea6f 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -13,6 +13,7 @@ enum migrate_reason {
 	MR_MEMORY_HOTPLUG,
 	MR_SYSCALL,		/* also applies to cpusets */
 	MR_MEMPOLICY_MBIND,
+	MR_NUMA_MISPLACED,
 	MR_CMA
 };
 
@@ -73,4 +74,14 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 #define fail_migrate_page NULL
 
 #endif /* CONFIG_MIGRATION */
+
+#ifdef CONFIG_NUMA_BALANCING
+extern int migrate_misplaced_page(struct page *page, int node);
+#else
+static inline int migrate_misplaced_page(struct page *page, int node)
+{
+	return -EAGAIN; /* can't migrate now */
+}
+#endif /* CONFIG_NUMA_BALANCING */
+
 #endif /* _LINUX_MIGRATE_H */

commit 7b2a2d4a18fffac3c4872021529b0657896db788
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Oct 19 14:07:31 2012 +0100

    mm: migrate: Add a tracepoint for migrate_pages
    
    The pgmigrate_success and pgmigrate_fail vmstat counters tells the user
    about migration activity but not the type or the reason. This patch adds
    a tracepoint to identify the type of page migration and why the page is
    being migrated.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ce7e6671968b..9d1c159e2427 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,6 +7,15 @@
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
+enum migrate_reason {
+	MR_COMPACTION,
+	MR_MEMORY_FAILURE,
+	MR_MEMORY_HOTPLUG,
+	MR_SYSCALL,		/* also applies to cpusets */
+	MR_MEMPOLICY_MBIND,
+	MR_CMA
+};
+
 #ifdef CONFIG_MIGRATION
 
 extern void putback_lru_pages(struct list_head *l);
@@ -14,7 +23,7 @@ extern int migrate_page(struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t x,
 			unsigned long private, bool offlining,
-			enum migrate_mode mode);
+			enum migrate_mode mode, int reason);
 extern int migrate_huge_page(struct page *, new_page_t x,
 			unsigned long private, bool offlining,
 			enum migrate_mode mode);
@@ -35,7 +44,7 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, bool offlining,
-		enum migrate_mode mode) { return -ENOSYS; }
+		enum migrate_mode mode, int reason) { return -ENOSYS; }
 static inline int migrate_huge_page(struct page *page, new_page_t x,
 		unsigned long private, bool offlining,
 		enum migrate_mode mode) { return -ENOSYS; }

commit 189ebff2894a9d0f4e250dd1e154d282ef0a6779
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:06 2012 -0700

    hugetlb: simplify migrate_huge_page()
    
    Since we migrate only one hugepage, don't use linked list for passing the
    page around.  Directly pass the page that need to be migrated as argument.
    This also removes the usage of page->lru in the migrate path.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 855c337b20c3..ce7e6671968b 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -15,7 +15,7 @@ extern int migrate_page(struct address_space *,
 extern int migrate_pages(struct list_head *l, new_page_t x,
 			unsigned long private, bool offlining,
 			enum migrate_mode mode);
-extern int migrate_huge_pages(struct list_head *l, new_page_t x,
+extern int migrate_huge_page(struct page *, new_page_t x,
 			unsigned long private, bool offlining,
 			enum migrate_mode mode);
 
@@ -36,7 +36,7 @@ static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, bool offlining,
 		enum migrate_mode mode) { return -ENOSYS; }
-static inline int migrate_huge_pages(struct list_head *l, new_page_t x,
+static inline int migrate_huge_page(struct page *page, new_page_t x,
 		unsigned long private, bool offlining,
 		enum migrate_mode mode) { return -ENOSYS; }
 

commit ce1744f4ed20ca873360e54502f8a71564ef7cc6
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Wed Mar 21 16:33:59 2012 -0700

    mm: replace PAGE_MIGRATION with IS_ENABLED(CONFIG_MIGRATION)
    
    Since commit 2a11c8ea20bf ("kconfig: Introduce IS_ENABLED(),
    IS_BUILTIN() and IS_MODULE()") there is a generic grep-friendly method
    for checking config options in C expressions.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 05ed2828a553..855c337b20c3 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -8,7 +8,6 @@
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 #ifdef CONFIG_MIGRATION
-#define PAGE_MIGRATION 1
 
 extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
@@ -32,7 +31,6 @@ extern void migrate_page_copy(struct page *newpage, struct page *page);
 extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page);
 #else
-#define PAGE_MIGRATION 0
 
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,

commit 6536e3123e5d3371a6f52e32a3d0694bcc987702
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Jan 20 14:33:53 2012 -0800

    mm: fix warnings regarding enum migrate_mode
    
    sparc64 allmodconfig:
    
    In file included from include/linux/compat.h:15,
                     from /usr/src/25/arch/sparc/include/asm/siginfo.h:19,
                     from include/linux/signal.h:5,
                     from include/linux/sched.h:73,
                     from arch/sparc/kernel/asm-offsets.c:13:
    include/linux/fs.h:618: warning: parameter has incomplete type
    
    It seems that my sparc64 compiler (gcc-3.4.5) doesn't like the forward
    declaration of enums.
    
    Fix this by moving the "enum migrate_mode" definition into its own header
    file.
    
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andy Isaacson <adi@hexapodia.org>
    Cc: Nai Xia <nai.xia@gmail.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index eaf867412f7a..05ed2828a553 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -3,22 +3,10 @@
 
 #include <linux/mm.h>
 #include <linux/mempolicy.h>
+#include <linux/migrate_mode.h>
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
-/*
- * MIGRATE_ASYNC means never block
- * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
- *	on most operations but not ->writepage as the potential stall time
- *	is too significant
- * MIGRATE_SYNC will block when migrating pages
- */
-enum migrate_mode {
-	MIGRATE_ASYNC,
-	MIGRATE_SYNC_LIGHT,
-	MIGRATE_SYNC,
-};
-
 #ifdef CONFIG_MIGRATION
 #define PAGE_MIGRATION 1
 

commit a6bc32b899223a877f595ef9ddc1e89ead5072b8
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Jan 12 17:19:43 2012 -0800

    mm: compaction: introduce sync-light migration for use by compaction
    
    This patch adds a lightweight sync migrate operation MIGRATE_SYNC_LIGHT
    mode that avoids writing back pages to backing storage.  Async compaction
    maps to MIGRATE_ASYNC while sync compaction maps to MIGRATE_SYNC_LIGHT.
    For other migrate_pages users such as memory hotplug, MIGRATE_SYNC is
    used.
    
    This avoids sync compaction stalling for an excessive length of time,
    particularly when copying files to a USB stick where there might be a
    large number of dirty pages backed by a filesystem that does not support
    ->writepages.
    
    [aarcange@redhat.com: This patch is heavily based on Andrea's work]
    [akpm@linux-foundation.org: fix fs/nfs/write.c build]
    [akpm@linux-foundation.org: fix fs/btrfs/disk-io.c build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andy Isaacson <adi@hexapodia.org>
    Cc: Nai Xia <nai.xia@gmail.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 14e6d2a88475..eaf867412f7a 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -6,18 +6,31 @@
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
+/*
+ * MIGRATE_ASYNC means never block
+ * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
+ *	on most operations but not ->writepage as the potential stall time
+ *	is too significant
+ * MIGRATE_SYNC will block when migrating pages
+ */
+enum migrate_mode {
+	MIGRATE_ASYNC,
+	MIGRATE_SYNC_LIGHT,
+	MIGRATE_SYNC,
+};
+
 #ifdef CONFIG_MIGRATION
 #define PAGE_MIGRATION 1
 
 extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
-			struct page *, struct page *, bool);
+			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t x,
 			unsigned long private, bool offlining,
-			bool sync);
+			enum migrate_mode mode);
 extern int migrate_huge_pages(struct list_head *l, new_page_t x,
 			unsigned long private, bool offlining,
-			bool sync);
+			enum migrate_mode mode);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -36,10 +49,10 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, bool offlining,
-		bool sync) { return -ENOSYS; }
+		enum migrate_mode mode) { return -ENOSYS; }
 static inline int migrate_huge_pages(struct list_head *l, new_page_t x,
 		unsigned long private, bool offlining,
-		bool sync) { return -ENOSYS; }
+		enum migrate_mode mode) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }

commit b969c4ab9f182a6e1b2a0848be349f99714947b0
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Jan 12 17:19:34 2012 -0800

    mm: compaction: determine if dirty pages can be migrated without blocking within ->migratepage
    
    Asynchronous compaction is used when allocating transparent hugepages to
    avoid blocking for long periods of time.  Due to reports of stalling,
    there was a debate on disabling synchronous compaction but this severely
    impacted allocation success rates.  Part of the reason was that many dirty
    pages are skipped in asynchronous compaction by the following check;
    
            if (PageDirty(page) && !sync &&
                    mapping->a_ops->migratepage != migrate_page)
                            rc = -EBUSY;
    
    This skips over all mapping aops using buffer_migrate_page() even though
    it is possible to migrate some of these pages without blocking.  This
    patch updates the ->migratepage callback with a "sync" parameter.  It is
    the responsibility of the callback to fail gracefully if migration would
    block.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andy Isaacson <adi@hexapodia.org>
    Cc: Nai Xia <nai.xia@gmail.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e39aeecfe9a2..14e6d2a88475 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -11,7 +11,7 @@ typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
-			struct page *, struct page *);
+			struct page *, struct page *, bool);
 extern int migrate_pages(struct list_head *l, new_page_t x,
 			unsigned long private, bool offlining,
 			bool sync);

commit 7f0f24967b0349798803260b2e4bf347cffa1990
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:45:58 2011 -0800

    mm: migration: cleanup migrate_pages API by matching types for offlining and sync
    
    With the introduction of the boolean sync parameter, the API looks a
    little inconsistent as offlining is still an int.  Convert offlining to a
    bool for the sake of being tidy.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index fa31902803fd..e39aeecfe9a2 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -13,10 +13,10 @@ extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
 extern int migrate_pages(struct list_head *l, new_page_t x,
-			unsigned long private, int offlining,
+			unsigned long private, bool offlining,
 			bool sync);
 extern int migrate_huge_pages(struct list_head *l, new_page_t x,
-			unsigned long private, int offlining,
+			unsigned long private, bool offlining,
 			bool sync);
 
 extern int fail_migrate_page(struct address_space *,
@@ -35,10 +35,10 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
-		unsigned long private, int offlining,
+		unsigned long private, bool offlining,
 		bool sync) { return -ENOSYS; }
 static inline int migrate_huge_pages(struct list_head *l, new_page_t x,
-		unsigned long private, int offlining,
+		unsigned long private, bool offlining,
 		bool sync) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }

commit 77f1fe6b08b13a87391549c8a820ddc817b6f50e
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:45:57 2011 -0800

    mm: migration: allow migration to operate asynchronously and avoid synchronous compaction in the faster path
    
    Migration synchronously waits for writeback if the initial passes fails.
    Callers of memory compaction do not necessarily want this behaviour if the
    caller is latency sensitive or expects that synchronous migration is not
    going to have a significantly better success rate.
    
    This patch adds a sync parameter to migrate_pages() allowing the caller to
    indicate if wait_on_page_writeback() is allowed within migration or not.
    For reclaim/compaction, try_to_compact_pages() is first called
    asynchronously, direct reclaim runs and then try_to_compact_pages() is
    called synchronously as there is a greater expectation that it'll succeed.
    
    [akpm@linux-foundation.org: build/merge fix]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 085527fb8261..fa31902803fd 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -13,9 +13,11 @@ extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
 extern int migrate_pages(struct list_head *l, new_page_t x,
-			unsigned long private, int offlining);
+			unsigned long private, int offlining,
+			bool sync);
 extern int migrate_huge_pages(struct list_head *l, new_page_t x,
-			unsigned long private, int offlining);
+			unsigned long private, int offlining,
+			bool sync);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -33,9 +35,11 @@ extern int migrate_huge_page_move_mapping(struct address_space *mapping,
 
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
-		unsigned long private, int offlining) { return -ENOSYS; }
+		unsigned long private, int offlining,
+		bool sync) { return -ENOSYS; }
 static inline int migrate_huge_pages(struct list_head *l, new_page_t x,
-		unsigned long private, int offlining) { return -ENOSYS; }
+		unsigned long private, int offlining,
+		bool sync) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }

commit 6f39ce056ab2ab2d29b2fae4aed61ed0b485972f
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Sep 30 11:54:51 2010 +0900

    Fix build error with !CONFIG_MIGRATION
    
    migrate_huge_page_move_mapping() is declared as "extern int ..."
    in include/linux/migrate.h for !CONFIG_MIGRATION,
    which causes the build error like below:
    
      mm/mprotect.o: In function `migrate_huge_page_move_mapping':
      mprotect.c:(.text+0x0): multiple definition of `migrate_huge_page_move_mapping'
      mm/shmem.o:shmem.c:(.text+0x0): first defined here
      mm/rmap.o: In function `migrate_huge_page_move_mapping':
      rmap.c:(.text+0x0): multiple definition of `migrate_huge_page_move_mapping'
      mm/shmem.o:shmem.c:(.text+0x0): first defined here
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 3c1941e40e61..085527fb8261 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -50,7 +50,7 @@ static inline int migrate_vmas(struct mm_struct *mm,
 static inline void migrate_page_copy(struct page *newpage,
 				     struct page *page) {}
 
-extern int migrate_huge_page_move_mapping(struct address_space *mapping,
+static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 				  struct page *newpage, struct page *page)
 {
 	return -ENOSYS;

commit 290408d4a25002f099efeee7b6a5778d431154d6
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 8 10:19:35 2010 +0900

    hugetlb: hugepage migration core
    
    This patch extends page migration code to support hugepage migration.
    One of the potential users of this feature is soft offlining which
    is triggered by memory corrected errors (added by the next patch.)
    
    Todo:
    - there are other users of page migration such as memory policy,
      memory hotplug and memocy compaction.
      They are not ready for hugepage support for now.
    
    ChangeLog since v4:
    - define migrate_huge_pages()
    - remove changes on isolation/putback_lru_page()
    
    ChangeLog since v2:
    - refactor isolate/putback_lru_page() to handle hugepage
    - add comment about race on unmap_and_move_huge_page()
    
    ChangeLog since v1:
    - divide migration code path for hugepage
    - define routine checking migration swap entry for hugetlb
    - replace "goto" with "if/else" in remove_migration_pte()
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7238231b8dd4..3c1941e40e61 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -14,6 +14,8 @@ extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
 extern int migrate_pages(struct list_head *l, new_page_t x,
 			unsigned long private, int offlining);
+extern int migrate_huge_pages(struct list_head *l, new_page_t x,
+			unsigned long private, int offlining);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -23,12 +25,17 @@ extern int migrate_prep_local(void);
 extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
+extern void migrate_page_copy(struct page *newpage, struct page *page);
+extern int migrate_huge_page_move_mapping(struct address_space *mapping,
+				  struct page *newpage, struct page *page);
 #else
 #define PAGE_MIGRATION 0
 
 static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, int offlining) { return -ENOSYS; }
+static inline int migrate_huge_pages(struct list_head *l, new_page_t x,
+		unsigned long private, int offlining) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 static inline int migrate_prep_local(void) { return -ENOSYS; }
@@ -40,6 +47,15 @@ static inline int migrate_vmas(struct mm_struct *mm,
 	return -ENOSYS;
 }
 
+static inline void migrate_page_copy(struct page *newpage,
+				     struct page *page) {}
+
+extern int migrate_huge_page_move_mapping(struct address_space *mapping,
+				  struct page *newpage, struct page *page)
+{
+	return -ENOSYS;
+}
+
 /* Possible settings for the migrate_page() method in address_operations */
 #define migrate_page NULL
 #define fail_migrate_page NULL

commit 748446bb6b5a9390b546af38ec899c868a9dbcf0
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon May 24 14:32:27 2010 -0700

    mm: compaction: memory compaction core
    
    This patch is the core of a mechanism which compacts memory in a zone by
    relocating movable pages towards the end of the zone.
    
    A single compaction run involves a migration scanner and a free scanner.
    Both scanners operate on pageblock-sized areas in the zone.  The migration
    scanner starts at the bottom of the zone and searches for all movable
    pages within each area, isolating them onto a private list called
    migratelist.  The free scanner starts at the top of the zone and searches
    for suitable areas and consumes the free pages within making them
    available for the migration scanner.  The pages isolated for migration are
    then migrated to the newly isolated free pages.
    
    [aarcange@redhat.com: Fix unsafe optimisation]
    [mel@csn.ul.ie: do not schedule work on other CPUs for compaction]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7a07b17d27c5..7238231b8dd4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -19,6 +19,7 @@ extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
 
 extern int migrate_prep(void);
+extern int migrate_prep_local(void);
 extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
@@ -30,6 +31,7 @@ static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, int offlining) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
+static inline int migrate_prep_local(void) { return -ENOSYS; }
 
 static inline int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,

commit e13861d822f8f443ca0c020ea8fc2dc01039cd63
Author: Minchan Kim <minchan.kim@gmail.com>
Date:   Mon May 24 14:31:59 2010 -0700

    mm: remove return value of putback_lru_pages()
    
    putback_lru_page() never can fail.  So it doesn't matter count of "the
    number of pages put back".
    
    In addition, users of this functions don't use return value.
    
    Let's remove unnecessary code.
    
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7f085c97c799..7a07b17d27c5 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -9,7 +9,7 @@ typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 #ifdef CONFIG_MIGRATION
 #define PAGE_MIGRATION 1
 
-extern int putback_lru_pages(struct list_head *l);
+extern void putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
 extern int migrate_pages(struct list_head *l, new_page_t x,
@@ -25,7 +25,7 @@ extern int migrate_vmas(struct mm_struct *mm,
 #else
 #define PAGE_MIGRATION 0
 
-static inline int putback_lru_pages(struct list_head *l) { return 0; }
+static inline void putback_lru_pages(struct list_head *l) {}
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private, int offlining) { return -ENOSYS; }
 

commit 62b61f611eb5e20f7e9f8619bfd03bdfe8af6348
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Dec 14 17:59:33 2009 -0800

    ksm: memory hotremove migration only
    
    The previous patch enables page migration of ksm pages, but that soon gets
    into trouble: not surprising, since we're using the ksm page lock to lock
    operations on its stable_node, but page migration switches the page whose
    lock is to be used for that.  Another layer of locking would fix it, but
    do we need that yet?
    
    Do we actually need page migration of ksm pages?  Yes, memory hotremove
    needs to offline sections of memory: and since we stopped allocating ksm
    pages with GFP_HIGHUSER, they will tend to be GFP_HIGHUSER_MOVABLE
    candidates for migration.
    
    But KSM is currently unconscious of NUMA issues, happily merging pages
    from different NUMA nodes: at present the rule must be, not to use
    MADV_MERGEABLE where you care about NUMA.  So no, NUMA page migration of
    ksm pages does not make sense yet.
    
    So, to complete support for ksm swapping we need to make hotremove safe.
    ksm_memory_callback() take ksm_thread_mutex when MEM_GOING_OFFLINE and
    release it when MEM_OFFLINE or MEM_CANCEL_OFFLINE.  But if mapped pages
    are freed before migration reaches them, stable_nodes may be left still
    pointing to struct pages which have been removed from the system: the
    stable_node needs to identify a page by pfn rather than page pointer, then
    it can safely prune them when MEM_OFFLINE.
    
    And make NUMA migration skip PageKsm pages where it skips PageReserved.
    But it's only when we reach unmap_and_move() that the page lock is taken
    and we can be sure that raised pagecount has prevented a PageAnon from
    being upgraded: so add offlining arg to migrate_pages(), to migrate ksm
    page when offlining (has sufficient locking) but reject it otherwise.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Izik Eidus <ieidus@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Chris Wright <chrisw@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 527602cdea1c..7f085c97c799 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -12,7 +12,8 @@ typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
-extern int migrate_pages(struct list_head *l, new_page_t x, unsigned long);
+extern int migrate_pages(struct list_head *l, new_page_t x,
+			unsigned long private, int offlining);
 
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -26,10 +27,7 @@ extern int migrate_vmas(struct mm_struct *mm,
 
 static inline int putback_lru_pages(struct list_head *l) { return 0; }
 static inline int migrate_pages(struct list_head *l, new_page_t x,
-		unsigned long private) { return -ENOSYS; }
-
-static inline int migrate_pages_to(struct list_head *pagelist,
-			struct vm_area_struct *vma, int dest) { return 0; }
+		unsigned long private, int offlining) { return -ENOSYS; }
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 

commit 64cdd548ffe26849d4cd113ac640f60606063b14
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jan 6 14:39:16 2009 -0800

    mm: cleanup: remove #ifdef CONFIG_MIGRATION
    
    #ifdef in *.c file decrease source readability a bit.  removing is better.
    
    This patch doesn't have any functional change.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 3f34005068d4..527602cdea1c 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,6 +7,8 @@
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 #ifdef CONFIG_MIGRATION
+#define PAGE_MIGRATION 1
+
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -20,6 +22,8 @@ extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
 #else
+#define PAGE_MIGRATION 0
+
 static inline int putback_lru_pages(struct list_head *l) { return 0; }
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private) { return -ENOSYS; }

commit 62695a84eb8f2e718bf4dfb21700afaa7a08e0ea
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:26:09 2008 -0700

    vmscan: move isolate_lru_page() to vmscan.c
    
    On large memory systems, the VM can spend way too much time scanning
    through pages that it cannot (or should not) evict from memory.  Not only
    does it use up CPU time, but it also provokes lock contention and can
    leave large systems under memory presure in a catatonic state.
    
    This patch series improves VM scalability by:
    
    1) putting filesystem backed, swap backed and unevictable pages
       onto their own LRUs, so the system only scans the pages that it
       can/should evict from memory
    
    2) switching to two handed clock replacement for the anonymous LRUs,
       so the number of pages that need to be scanned when the system
       starts swapping is bound to a reasonable number
    
    3) keeping unevictable pages off the LRU completely, so the
       VM does not waste CPU time scanning them. ramfs, ramdisk,
       SHM_LOCKED shared memory segments and mlock()ed VMA pages
       are keept on the unevictable list.
    
    This patch:
    
    isolate_lru_page logically belongs to be in vmscan.c than migrate.c.
    
    It is tough, because we don't need that function without memory migration
    so there is a valid argument to have it in migrate.c.  However a
    subsequent patch needs to make use of it in the core mm, so we can happily
    move it to vmscan.c.
    
    Also, make the function a little more generic by not requiring that it
    adds an isolated page to a given list.  Callers can do that.
    
            Note that we now have '__isolate_lru_page()', that does
            something quite different, visible outside of vmscan.c
            for use with memory controller.  Methinks we need to
            rationalize these names/purposes.       --lts
    
    [akpm@linux-foundation.org: fix mm/memory_hotplug.c build]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 03aea612d284..3f34005068d4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,7 +7,6 @@
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 #ifdef CONFIG_MIGRATION
-extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
@@ -21,8 +20,6 @@ extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
 #else
-static inline int isolate_lru_page(struct page *p, struct list_head *list)
-					{ return -ENOSYS; }
 static inline int putback_lru_pages(struct list_head *l) { return 0; }
 static inline int migrate_pages(struct list_head *l, new_page_t x,
 		unsigned long private) { return -ENOSYS; }

commit 83d1674a946141c3c59d430e96c224f7937e6158
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jul 23 21:28:22 2008 -0700

    mm: make CONFIG_MIGRATION available w/o CONFIG_NUMA
    
    We'd like to support CONFIG_MEMORY_HOTREMOVE on s390, which depends on
    CONFIG_MIGRATION.  So far, CONFIG_MIGRATION is only available with NUMA
    support.
    
    This patch makes CONFIG_MIGRATION selectable for architectures that define
    ARCH_ENABLE_MEMORY_HOTREMOVE.  When MIGRATION is enabled w/o NUMA, the
    kernel won't compile because migrate_vmas() does not know about
    vm_ops->migrate() and vma_migratable() does not know about policy_zone.
    To fix this, those two functions can be restricted to '#ifdef CONFIG_NUMA'
    because they are not being used w/o NUMA.  vma_migratable() is moved over
    from migrate.h to mempolicy.h.
    
    [kosaki.motohiro@jp.fujitsu.com: build fix]
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: KOSAKI Motorhiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e10a90a93b5d..03aea612d284 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -3,28 +3,10 @@
 
 #include <linux/mm.h>
 #include <linux/mempolicy.h>
-#include <linux/pagemap.h>
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 #ifdef CONFIG_MIGRATION
-/* Check if a vma is migratable */
-static inline int vma_migratable(struct vm_area_struct *vma)
-{
-	if (vma->vm_flags & (VM_IO|VM_HUGETLB|VM_PFNMAP|VM_RESERVED))
-		return 0;
-	/*
-	 * Migration allocates pages in the highest zone. If we cannot
-	 * do so then migration (at least from node to node) is not
-	 * possible.
-	 */
-	if (vma->vm_file &&
-		gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
-								< policy_zone)
-			return 0;
-	return 1;
-}
-
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
@@ -39,9 +21,6 @@ extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
 #else
-static inline int vma_migratable(struct vm_area_struct *vma)
-					{ return 0; }
-
 static inline int isolate_lru_page(struct page *p, struct list_head *list)
 					{ return -ENOSYS; }
 static inline int putback_lru_pages(struct list_head *l) { return 0; }

commit 906e0be197232c219197d058ef5095baa7764cd4
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:50:20 2007 -0700

    page migration: Only migrate pages if allocation in the highest zone is possible
    
    Address spaces contain an allocation flag that specifies restriction on the
    zone for pages placed in the mapping.  I.e.  some device may require pages
    to be allocated from a DMA zone.  Block devices may not be able to use
    pages from HIGHMEM.
    
    Memory policies and the common use of page migration works only on the
    highest zone.  If the address space does not allow allocation from the
    highest zone then the pages in the address space are not migratable simply
    because we can only allocate memory for a specified node if we allow
    allocation for the highest zone on each node.
    
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 75e55dcdeb18..e10a90a93b5d 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -2,18 +2,29 @@
 #define _LINUX_MIGRATE_H
 
 #include <linux/mm.h>
+#include <linux/mempolicy.h>
+#include <linux/pagemap.h>
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
+#ifdef CONFIG_MIGRATION
 /* Check if a vma is migratable */
 static inline int vma_migratable(struct vm_area_struct *vma)
 {
 	if (vma->vm_flags & (VM_IO|VM_HUGETLB|VM_PFNMAP|VM_RESERVED))
 		return 0;
+	/*
+	 * Migration allocates pages in the highest zone. If we cannot
+	 * do so then migration (at least from node to node) is not
+	 * possible.
+	 */
+	if (vma->vm_file &&
+		gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
+								< policy_zone)
+			return 0;
 	return 1;
 }
 
-#ifdef CONFIG_MIGRATION
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
@@ -28,6 +39,8 @@ extern int migrate_vmas(struct mm_struct *mm,
 		const nodemask_t *from, const nodemask_t *to,
 		unsigned long flags);
 #else
+static inline int vma_migratable(struct vm_area_struct *vma)
+					{ return 0; }
 
 static inline int isolate_lru_page(struct page *p, struct list_head *list)
 					{ return -ENOSYS; }

commit 0dc952dc3e6d96d554a19fa7bee3f3b1d55e3cff
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Mon Mar 5 00:30:33 2007 -0800

    [PATCH] Page migration: Fix vma flag checking
    
    Currently we do not check for vma flags if sys_move_pages is called to move
    individual pages.  If sys_migrate_pages is called to move pages then we
    check for vm_flags that indicate a non migratable vma but that still
    includes VM_LOCKED and we can migrate mlocked pages.
    
    Extract the vma_migratable check from mm/mempolicy.c, fix it and put it
    into migrate.h so that is can be used from both locations.
    
    Problem was spotted by Lee Schermerhorn
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 48148e0cdbd1..75e55dcdeb18 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -5,6 +5,14 @@
 
 typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
+/* Check if a vma is migratable */
+static inline int vma_migratable(struct vm_area_struct *vma)
+{
+	if (vma->vm_flags & (VM_IO|VM_HUGETLB|VM_PFNMAP|VM_RESERVED))
+		return 0;
+	return 1;
+}
+
 #ifdef CONFIG_MIGRATION
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);

commit 7b2259b3e53f128c10a9fded0965e69d4a949847
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Jun 25 05:46:48 2006 -0700

    [PATCH] page migration: Support a vma migration function
    
    Hooks for calling vma specific migration functions
    
    With this patch a vma may define a vma->vm_ops->migrate function.  That
    function may perform page migration on its own (some vmas may not contain page
    structs and therefore cannot be handled by regular page migration.  Pages in a
    vma may require special preparatory treatment before migration is possible
    etc) .  Only mmap_sem is held when the migration function is called.  The
    migrate() function gets passed two sets of nodemasks describing the source and
    the target of the migration.  The flags parameter either contains
    
    MPOL_MF_MOVE    which means that only pages used exclusively by
                    the specified mm should be moved
    
    or
    
    MPOL_MF_MOVE_ALL which means that pages shared with other processes
                    should also be moved.
    
    The migration function returns 0 on success or an error condition.  An error
    condition will prevent regular page migration from occurring.
    
    On its own this patch cannot be included since there are no users for this
    functionality.  But it seems that the uncached allocator will need this
    functionality at some point.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 5dba23a1c0d0..48148e0cdbd1 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -16,7 +16,9 @@ extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
 
 extern int migrate_prep(void);
-
+extern int migrate_vmas(struct mm_struct *mm,
+		const nodemask_t *from, const nodemask_t *to,
+		unsigned long flags);
 #else
 
 static inline int isolate_lru_page(struct page *p, struct list_head *list)
@@ -30,6 +32,13 @@ static inline int migrate_pages_to(struct list_head *pagelist,
 
 static inline int migrate_prep(void) { return -ENOSYS; }
 
+static inline int migrate_vmas(struct mm_struct *mm,
+		const nodemask_t *from, const nodemask_t *to,
+		unsigned long flags)
+{
+	return -ENOSYS;
+}
+
 /* Possible settings for the migrate_page() method in address_operations */
 #define migrate_page NULL
 #define fail_migrate_page NULL

commit 742755a1d8ce2b548428f7aacf1758b4bba50080
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 23 02:03:55 2006 -0700

    [PATCH] page migration: sys_move_pages(): support moving of individual pages
    
    move_pages() is used to move individual pages of a process. The function can
    be used to determine the location of pages and to move them onto the desired
    node. move_pages() returns status information for each page.
    
    long move_pages(pid, number_of_pages_to_move,
                    addresses_of_pages[],
                    nodes[] or NULL,
                    status[],
                    flags);
    
    The addresses of pages is an array of void * pointing to the
    pages to be moved.
    
    The nodes array contains the node numbers that the pages should be moved
    to. If a NULL is passed instead of an array then no pages are moved but
    the status array is updated. The status request may be used to determine
    the page state before issuing another move_pages() to move pages.
    
    The status array will contain the state of all individual page migration
    attempts when the function terminates. The status array is only valid if
    move_pages() completed successfullly.
    
    Possible page states in status[]:
    
    0..MAX_NUMNODES The page is now on the indicated node.
    
    -ENOENT         Page is not present
    
    -EACCES         Page is mapped by multiple processes and can only
                    be moved if MPOL_MF_MOVE_ALL is specified.
    
    -EPERM          The page has been mlocked by a process/driver and
                    cannot be moved.
    
    -EBUSY          Page is busy and cannot be moved. Try again later.
    
    -EFAULT         Invalid address (no VMA or zero page).
    
    -ENOMEM         Unable to allocate memory on target node.
    
    -EIO            Unable to write back page. The page must be written
                    back in order to move it since the page is dirty and the
                    filesystem does not provide a migration function that
                    would allow the moving of dirty pages.
    
    -EINVAL         A dirty page cannot be moved. The filesystem does not provide
                    a migration function and has no ability to write back pages.
    
    The flags parameter indicates what types of pages to move:
    
    MPOL_MF_MOVE    Move pages that are only mapped by the process.
    
    MPOL_MF_MOVE_ALL Also move pages that are mapped by multiple processes.
                    Requires sufficient capabilities.
    
    Possible return codes from move_pages()
    
    -ENOENT         No pages found that would require moving. All pages
                    are either already on the target node, not present, had an
                    invalid address or could not be moved because they were
                    mapped by multiple processes.
    
    -EINVAL         Flags other than MPOL_MF_MOVE(_ALL) specified or an attempt
                    to migrate pages in a kernel thread.
    
    -EPERM          MPOL_MF_MOVE_ALL specified without sufficient priviledges.
                    or an attempt to move a process belonging to another user.
    
    -EACCES         One of the target nodes is not allowed by the current cpuset.
    
    -ENODEV         One of the target nodes is not online.
    
    -ESRCH          Process does not exist.
    
    -E2BIG          Too many pages to move.
    
    -ENOMEM         Not enough memory to allocate control array.
    
    -EFAULT         Parameters could not be accessed.
    
    A test program for move_pages() may be found with the patches
    on ftp.kernel.org:/pub/linux/kernel/people/christoph/pmig/patches-2.6.17-rc4-mm3
    
    From: Christoph Lameter <clameter@sgi.com>
    
      Detailed results for sys_move_pages()
    
      Pass a pointer to an integer to get_new_page() that may be used to
      indicate where the completion status of a migration operation should be
      placed.  This allows sys_move_pags() to report back exactly what happened to
      each page.
    
      Wish there would be a better way to do this. Looks a bit hacky.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Jes Sorensen <jes@trained-monkey.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 5b95d6568dc4..5dba23a1c0d0 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -3,7 +3,7 @@
 
 #include <linux/mm.h>
 
-typedef struct page *new_page_t(struct page *, unsigned long private);
+typedef struct page *new_page_t(struct page *, unsigned long private, int **);
 
 #ifdef CONFIG_MIGRATION
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);

commit 95a402c3847cc16f4ba03013cd01404fa0f14c2e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 23 02:03:53 2006 -0700

    [PATCH] page migration: use allocator function for migrate_pages()
    
    Instead of passing a list of new pages, pass a function to allocate a new
    page.  This allows the correct placement of MPOL_INTERLEAVE pages during page
    migration.  It also further simplifies the callers of migrate pages.
    migrate_pages() becomes similar to migrate_pages_to() so drop
    migrate_pages_to().  The batching of new page allocations becomes unnecessary.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Jes Sorensen <jes@trained-monkey.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 83af25949fa9..5b95d6568dc4 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -3,14 +3,15 @@
 
 #include <linux/mm.h>
 
+typedef struct page *new_page_t(struct page *, unsigned long private);
+
 #ifdef CONFIG_MIGRATION
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
-extern int migrate_pages(struct list_head *l, struct list_head *t);
-extern int migrate_pages_to(struct list_head *pagelist,
-			struct vm_area_struct *vma, int dest);
+extern int migrate_pages(struct list_head *l, new_page_t x, unsigned long);
+
 extern int fail_migrate_page(struct address_space *,
 			struct page *, struct page *);
 
@@ -21,8 +22,8 @@ extern int migrate_prep(void);
 static inline int isolate_lru_page(struct page *p, struct list_head *list)
 					{ return -ENOSYS; }
 static inline int putback_lru_pages(struct list_head *l) { return 0; }
-static inline int migrate_pages(struct list_head *l, struct list_head *t)
-					{ return -ENOSYS; }
+static inline int migrate_pages(struct list_head *l, new_page_t x,
+		unsigned long private) { return -ENOSYS; }
 
 static inline int migrate_pages_to(struct list_head *pagelist,
 			struct vm_area_struct *vma, int dest) { return 0; }

commit aaa994b300a172afafab47938804836b923e5ef7
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 23 02:03:52 2006 -0700

    [PATCH] page migration: handle freeing of pages in migrate_pages()
    
    Do not leave pages on the lists passed to migrate_pages().  Seems that we will
    not need any postprocessing of pages.  This will simplify the handling of
    pages by the callers of migrate_pages().
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Jes Sorensen <jes@trained-monkey.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 287c47b5e5df..83af25949fa9 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -8,8 +8,7 @@ extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct address_space *,
 			struct page *, struct page *);
-extern int migrate_pages(struct list_head *l, struct list_head *t,
-		struct list_head *moved, struct list_head *failed);
+extern int migrate_pages(struct list_head *l, struct list_head *t);
 extern int migrate_pages_to(struct list_head *pagelist,
 			struct vm_area_struct *vma, int dest);
 extern int fail_migrate_page(struct address_space *,
@@ -22,8 +21,8 @@ extern int migrate_prep(void);
 static inline int isolate_lru_page(struct page *p, struct list_head *list)
 					{ return -ENOSYS; }
 static inline int putback_lru_pages(struct list_head *l) { return 0; }
-static inline int migrate_pages(struct list_head *l, struct list_head *t,
-	struct list_head *moved, struct list_head *failed) { return -ENOSYS; }
+static inline int migrate_pages(struct list_head *l, struct list_head *t)
+					{ return -ENOSYS; }
 
 static inline int migrate_pages_to(struct list_head *pagelist,
 			struct vm_area_struct *vma, int dest) { return 0; }

commit 2d1db3b1170db4e8bf0531dd636742269c2cf579
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 23 02:03:33 2006 -0700

    [PATCH] page migration cleanup: pass "mapping" to migration functions
    
    Change handling of address spaces.
    
    Pass a pointer to the address space in which the page is migrated to all
    migration function.  This avoids repeatedly having to retrieve the address
    space pointer from the page and checking it for validity.  The old page
    mapping will change once migration has gone to a certain step, so it is less
    confusing to have the pointer always available.
    
    Move the setting of the mapping and index for the new page into
    migrate_pages().
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e8d3b08cc354..287c47b5e5df 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -6,12 +6,14 @@
 #ifdef CONFIG_MIGRATION
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
-extern int migrate_page(struct page *, struct page *);
+extern int migrate_page(struct address_space *,
+			struct page *, struct page *);
 extern int migrate_pages(struct list_head *l, struct list_head *t,
 		struct list_head *moved, struct list_head *failed);
 extern int migrate_pages_to(struct list_head *pagelist,
 			struct vm_area_struct *vma, int dest);
-extern int fail_migrate_page(struct page *, struct page *);
+extern int fail_migrate_page(struct address_space *,
+			struct page *, struct page *);
 
 extern int migrate_prep(void);
 

commit e7340f73307abed9283d0a07570d06e228c205dd
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 23 02:03:29 2006 -0700

    [PATCH] page migration cleanup: remove useless definitions
    
    Remove the export for migrate_page_remove_references() and migrate_page_copy()
    that are unlikely to be used directly by filesystems implementing migration.
    The export was useful when buffer_migrate_page() lived in fs/buffer.c but it
    has now been moved to migrate.c in the migration reorg.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 6789c4940c9c..e8d3b08cc354 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -7,8 +7,6 @@
 extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
 extern int putback_lru_pages(struct list_head *l);
 extern int migrate_page(struct page *, struct page *);
-extern void migrate_page_copy(struct page *, struct page *);
-extern int migrate_page_remove_references(struct page *, struct page *, int);
 extern int migrate_pages(struct list_head *l, struct list_head *t,
 		struct list_head *moved, struct list_head *failed);
 extern int migrate_pages_to(struct list_head *pagelist,

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index ff0a64073ebc..6789c4940c9c 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -1,7 +1,6 @@
 #ifndef _LINUX_MIGRATE_H
 #define _LINUX_MIGRATE_H
 
-#include <linux/config.h>
 #include <linux/mm.h>
 
 #ifdef CONFIG_MIGRATION

commit 9bf9e89c3d147ca8cf9622d2d053684fba77a464
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Mar 31 02:29:56 2006 -0800

    [PATCH] migrate_pages_to() must be defined for the no swap case
    
    Fix migrate_pages_to() definition.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 7d09962c3c0b..ff0a64073ebc 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -12,7 +12,7 @@ extern void migrate_page_copy(struct page *, struct page *);
 extern int migrate_page_remove_references(struct page *, struct page *, int);
 extern int migrate_pages(struct list_head *l, struct list_head *t,
 		struct list_head *moved, struct list_head *failed);
-int migrate_pages_to(struct list_head *pagelist,
+extern int migrate_pages_to(struct list_head *pagelist,
 			struct vm_area_struct *vma, int dest);
 extern int fail_migrate_page(struct page *, struct page *);
 
@@ -26,6 +26,9 @@ static inline int putback_lru_pages(struct list_head *l) { return 0; }
 static inline int migrate_pages(struct list_head *l, struct list_head *t,
 	struct list_head *moved, struct list_head *failed) { return -ENOSYS; }
 
+static inline int migrate_pages_to(struct list_head *pagelist,
+			struct vm_area_struct *vma, int dest) { return 0; }
+
 static inline int migrate_prep(void) { return -ENOSYS; }
 
 /* Possible settings for the migrate_page() method in address_operations */

commit b20a35035f983f4ac7e29c4a68f30e43510007e0
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Mar 22 00:09:12 2006 -0800

    [PATCH] page migration reorg
    
    Centralize the page migration functions in anticipation of additional
    tinkering.  Creates a new file mm/migrate.c
    
    1. Extract buffer_migrate_page() from fs/buffer.c
    
    2. Extract central migration code from vmscan.c
    
    3. Extract some components from mempolicy.c
    
    4. Export pageout() and remove_from_swap() from vmscan.c
    
    5. Make it possible to configure NUMA systems without page migration
       and non-NUMA systems with page migration.
    
    I had to so some #ifdeffing in mempolicy.c that may need a cleanup.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/migrate.h b/include/linux/migrate.h
new file mode 100644
index 000000000000..7d09962c3c0b
--- /dev/null
+++ b/include/linux/migrate.h
@@ -0,0 +1,36 @@
+#ifndef _LINUX_MIGRATE_H
+#define _LINUX_MIGRATE_H
+
+#include <linux/config.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_MIGRATION
+extern int isolate_lru_page(struct page *p, struct list_head *pagelist);
+extern int putback_lru_pages(struct list_head *l);
+extern int migrate_page(struct page *, struct page *);
+extern void migrate_page_copy(struct page *, struct page *);
+extern int migrate_page_remove_references(struct page *, struct page *, int);
+extern int migrate_pages(struct list_head *l, struct list_head *t,
+		struct list_head *moved, struct list_head *failed);
+int migrate_pages_to(struct list_head *pagelist,
+			struct vm_area_struct *vma, int dest);
+extern int fail_migrate_page(struct page *, struct page *);
+
+extern int migrate_prep(void);
+
+#else
+
+static inline int isolate_lru_page(struct page *p, struct list_head *list)
+					{ return -ENOSYS; }
+static inline int putback_lru_pages(struct list_head *l) { return 0; }
+static inline int migrate_pages(struct list_head *l, struct list_head *t,
+	struct list_head *moved, struct list_head *failed) { return -ENOSYS; }
+
+static inline int migrate_prep(void) { return -ENOSYS; }
+
+/* Possible settings for the migrate_page() method in address_operations */
+#define migrate_page NULL
+#define fail_migrate_page NULL
+
+#endif /* CONFIG_MIGRATION */
+#endif /* _LINUX_MIGRATE_H */
