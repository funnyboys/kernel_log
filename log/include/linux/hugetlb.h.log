commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0cced410e0bd..50650d0d01b9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -9,7 +9,7 @@
 #include <linux/cgroup.h>
 #include <linux/list.h>
 #include <linux/kref.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 
 struct ctl_table;
 struct user_struct;

commit ee01c4d72adffb7d424535adf630f2955748fa8b
Merge: c444eb564fb1 09587a09ada2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 20:24:15 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
     "More mm/ work, plenty more to come
    
      Subsystems affected by this patch series: slub, memcg, gup, kasan,
      pagealloc, hugetlb, vmscan, tools, mempolicy, memblock, hugetlbfs,
      thp, mmap, kconfig"
    
    * akpm: (131 commits)
      arm64: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      x86: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      riscv: support DEBUG_WX
      mm: add DEBUG_WX support
      drivers/base/memory.c: cache memory blocks in xarray to accelerate lookup
      mm/thp: rename pmd_mknotpresent() as pmd_mkinvalid()
      powerpc/mm: drop platform defined pmd_mknotpresent()
      mm: thp: don't need to drain lru cache when splitting and mlocking THP
      hugetlbfs: get unmapped area below TASK_UNMAPPED_BASE for hugetlbfs
      sparc32: register memory occupied by kernel as memblock.memory
      include/linux/memblock.h: fix minor typo and unclear comment
      mm, mempolicy: fix up gup usage in lookup_node
      tools/vm/page_owner_sort.c: filter out unneeded line
      mm: swap: memcg: fix memcg stats for huge pages
      mm: swap: fix vmstats for huge pages
      mm: vmscan: limit the range of LRU type balancing
      mm: vmscan: reclaim writepage is IO cost
      mm: vmscan: determine anon/file pressure balance at the reclaim root
      mm: balance LRU lists based on relative thrashing
      mm: only count actual rotations as LRU reclaim cost
      ...

commit 5be993432821750f382809df5e20bf4c129b24f7
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Jun 3 16:01:05 2020 -0700

    mm/hugetlb: define a generic fallback for arch_clear_hugepage_flags()
    
    There are multiple similar definitions for arch_clear_hugepage_flags() on
    various platforms.  Lets just add it's generic fallback definition for
    platforms that do not override.  This help reduce code duplication.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1588907271-11920-4-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index bf97b17ab206..2e66b71c1236 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -600,6 +600,11 @@ static inline int is_hugepage_only_range(struct mm_struct *mm,
 #define is_hugepage_only_range is_hugepage_only_range
 #endif
 
+#ifndef arch_clear_hugepage_flags
+static inline void arch_clear_hugepage_flags(struct page *page) { }
+#define arch_clear_hugepage_flags arch_clear_hugepage_flags
+#endif
+
 #ifndef arch_make_huge_pte
 static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 				       struct page *page, int writable)

commit b0eae98c66fe4ccac3a5a79a1479c057f2c7cdd7
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Jun 3 16:01:01 2020 -0700

    mm/hugetlb: define a generic fallback for is_hugepage_only_range()
    
    There are multiple similar definitions for is_hugepage_only_range() on
    various platforms.  Lets just add it's generic fallback definition for
    platforms that do not override.  This help reduce code duplication.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1588907271-11920-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0c13706054ef..bf97b17ab206 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -591,6 +591,15 @@ static inline unsigned int blocks_per_huge_page(struct hstate *h)
 
 #include <asm/hugetlb.h>
 
+#ifndef is_hugepage_only_range
+static inline int is_hugepage_only_range(struct mm_struct *mm,
+					unsigned long addr, unsigned long len)
+{
+	return 0;
+}
+#define is_hugepage_only_range is_hugepage_only_range
+#endif
+
 #ifndef arch_make_huge_pte
 static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 				       struct page *page, int writable)

commit 359f25443a8dada0fb709dd044a422017031790f
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Jun 3 16:00:38 2020 -0700

    hugetlbfs: move hugepagesz= parsing to arch independent code
    
    Now that architectures provide arch_hugetlb_valid_size(), parsing of
    "hugepagesz=" can be done in architecture independent code.  Create a
    single routine to handle hugepagesz= parsing and remove all arch specific
    routines.  We can also remove the interface hugetlb_bad_size() as this is
    no longer used outside arch independent code.
    
    This also provides consistent behavior of hugetlbfs command line options.
    The hugepagesz= option should only be specified once for a specific size,
    but some architectures allow multiple instances.  This appears to be more
    of an oversight when code was added by some architectures to set up ALL
    huge pages sizes.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Sandipan Das <sandipan@linux.ibm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Acked-by: Mina Almasry <almasrymina@google.com>
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>  [s390]
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Longpeng <longpeng2@huawei.com>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Anders Roxell <anders.roxell@linaro.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200417185049.275845-3-mike.kravetz@oracle.com
    Link: http://lkml.kernel.org/r/20200428205614.246260-3-mike.kravetz@oracle.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 2eb15f5ab01e..0c13706054ef 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -519,7 +519,6 @@ int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 int __init __alloc_bootmem_huge_page(struct hstate *h);
 int __init alloc_bootmem_huge_page(struct hstate *h);
 
-void __init hugetlb_bad_size(void);
 void __init hugetlb_add_hstate(unsigned order);
 bool __init arch_hugetlb_valid_size(unsigned long size);
 struct hstate *size_to_hstate(unsigned long size);

commit ae94da898133947c2d1f005da10838478e4548db
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Jun 3 16:00:34 2020 -0700

    hugetlbfs: add arch_hugetlb_valid_size
    
    Patch series "Clean up hugetlb boot command line processing", v4.
    
    Longpeng(Mike) reported a weird message from hugetlb command line
    processing and proposed a solution [1].  While the proposed patch does
    address the specific issue, there are other related issues in command line
    processing.  As hugetlbfs evolved, updates to command line processing have
    been made to meet immediate needs and not necessarily in a coordinated
    manner.  The result is that some processing is done in arch specific code,
    some is done in arch independent code and coordination is problematic.
    Semantics can vary between architectures.
    
    The patch series does the following:
    - Define arch specific arch_hugetlb_valid_size routine used to validate
      passed huge page sizes.
    - Move hugepagesz= command line parsing out of arch specific code and into
      an arch independent routine.
    - Clean up command line processing to follow desired semantics and
      document those semantics.
    
    [1] https://lore.kernel.org/linux-mm/20200305033014.1152-1-longpeng2@huawei.com
    
    This patch (of 3):
    
    The architecture independent routine hugetlb_default_setup sets up the
    default huge pages size.  It has no way to verify if the passed value is
    valid, so it accepts it and attempts to validate at a later time.  This
    requires undocumented cooperation between the arch specific and arch
    independent code.
    
    For architectures that support more than one huge page size, provide a
    routine arch_hugetlb_valid_size to validate a huge page size.
    hugetlb_default_setup can use this to validate passed values.
    
    arch_hugetlb_valid_size will also be used in a subsequent patch to move
    processing of the "hugepagesz=" in arch specific code to a common routine
    in arch independent code.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>  [s390]
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Longpeng <longpeng2@huawei.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Mina Almasry <almasrymina@google.com>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Anders Roxell <anders.roxell@linaro.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200428205614.246260-1-mike.kravetz@oracle.com
    Link: http://lkml.kernel.org/r/20200428205614.246260-2-mike.kravetz@oracle.com
    Link: http://lkml.kernel.org/r/20200417185049.275845-1-mike.kravetz@oracle.com
    Link: http://lkml.kernel.org/r/20200417185049.275845-2-mike.kravetz@oracle.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 43a1cef8f0f1..2eb15f5ab01e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -521,6 +521,7 @@ int __init alloc_bootmem_huge_page(struct hstate *h);
 
 void __init hugetlb_bad_size(void);
 void __init hugetlb_add_hstate(unsigned order);
+bool __init arch_hugetlb_valid_size(unsigned long size);
 struct hstate *size_to_hstate(unsigned long size);
 
 #ifndef HUGE_MAX_HSTATE

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 43a1cef8f0f1..92c21c5ccc58 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -105,14 +105,13 @@ struct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
-int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
-int hugetlb_overcommit_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
-int hugetlb_treat_movable_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
-
-#ifdef CONFIG_NUMA
-int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
-#endif
+int hugetlb_sysctl_handler(struct ctl_table *, int, void *, size_t *, loff_t *);
+int hugetlb_overcommit_handler(struct ctl_table *, int, void *, size_t *,
+		loff_t *);
+int hugetlb_treat_movable_handler(struct ctl_table *, int, void *, size_t *,
+		loff_t *);
+int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int, void *, size_t *,
+		loff_t *);
 
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,

commit cf11e85fc08cc6a4fe3ac2ba2e610c962bf20bc3
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 10 14:32:45 2020 -0700

    mm: hugetlb: optionally allocate gigantic hugepages using cma
    
    Commit 944d9fec8d7a ("hugetlb: add support for gigantic page allocation
    at runtime") has added the run-time allocation of gigantic pages.
    
    However it actually works only at early stages of the system loading,
    when the majority of memory is free.  After some time the memory gets
    fragmented by non-movable pages, so the chances to find a contiguous 1GB
    block are getting close to zero.  Even dropping caches manually doesn't
    help a lot.
    
    At large scale rebooting servers in order to allocate gigantic hugepages
    is quite expensive and complex.  At the same time keeping some constant
    percentage of memory in reserved hugepages even if the workload isn't
    using it is a big waste: not all workloads can benefit from using 1 GB
    pages.
    
    The following solution can solve the problem:
    1) On boot time a dedicated cma area* is reserved. The size is passed
       as a kernel argument.
    2) Run-time allocations of gigantic hugepages are performed using the
       cma allocator and the dedicated cma area
    
    In this case gigantic hugepages can be allocated successfully with a
    high probability, however the memory isn't completely wasted if nobody
    is using 1GB hugepages: it can be used for pagecache, anon memory, THPs,
    etc.
    
    * On a multi-node machine a per-node cma area is allocated on each node.
      Following gigantic hugetlb allocation are using the first available
      numa node if the mask isn't specified by a user.
    
    Usage:
    1) configure the kernel to allocate a cma area for hugetlb allocations:
       pass hugetlb_cma=10G as a kernel argument
    
    2) allocate hugetlb pages as usual, e.g.
       echo 10 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
    
    If the option isn't enabled or the allocation of the cma area failed,
    the current behavior of the system is preserved.
    
    x86 and arm-64 are covered by this patch, other architectures can be
    trivially added later.
    
    The patch contains clean-ups and fixes proposed and implemented by Aslan
    Bakirov and Randy Dunlap.  It also contains ideas and suggestions
    proposed by Rik van Riel, Michal Hocko and Mike Kravetz.  Thanks!
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Andreas Schaufler <andreas.schaufler@gmx.de>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Cc: Aslan Bakirov <aslan@fb.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Link: http://lkml.kernel.org/r/20200407163840.92263-3-guro@fb.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5ea05879a0a9..43a1cef8f0f1 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -895,4 +895,16 @@ static inline spinlock_t *huge_pte_lock(struct hstate *h,
 	return ptl;
 }
 
+#if defined(CONFIG_HUGETLB_PAGE) && defined(CONFIG_CMA)
+extern void __init hugetlb_cma_reserve(int order);
+extern void __init hugetlb_cma_check(void);
+#else
+static inline __init void hugetlb_cma_reserve(int order)
+{
+}
+static inline __init void hugetlb_cma_check(void)
+{
+}
+#endif
+
 #endif /* _LINUX_HUGETLB_H */

commit bb297bb2de517e41199185021f043bbc5d75b377
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Apr 1 21:11:54 2020 -0700

    mm/hugetlb: fix build failure with HUGETLB_PAGE but not HUGEBTLBFS
    
    When CONFIG_HUGETLB_PAGE is set but not CONFIG_HUGETLBFS, the following
    build failure is encoutered:
    
      In file included from arch/powerpc/mm/fault.c:33:0:
      include/linux/hugetlb.h: In function 'hstate_inode':
      include/linux/hugetlb.h:477:9: error: implicit declaration of function 'HUGETLBFS_SB' [-Werror=implicit-function-declaration]
        return HUGETLBFS_SB(i->i_sb)->hstate;
               ^
      include/linux/hugetlb.h:477:30: error: invalid type argument of '->' (have 'int')
        return HUGETLBFS_SB(i->i_sb)->hstate;
                                    ^
    
    Gate hstate_inode() with CONFIG_HUGETLBFS instead of CONFIG_HUGETLB_PAGE.
    
    Fixes: a137e1cc6d6e ("hugetlbfs: per mount huge page sizes")
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andi Kleen <ak@suse.de>
    Link: http://lkml.kernel.org/r/7e8c3a3c9a587b9cd8a2f146df32a421b961f3a2.1584432148.git.christophe.leroy@c-s.fr
    Link: https://patchwork.ozlabs.org/patch/1255548/#2386036
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 219962c9517e..5ea05879a0a9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -443,7 +443,10 @@ static inline bool is_file_hugepages(struct file *file)
 	return is_file_shm_hugepages(file);
 }
 
-
+static inline struct hstate *hstate_inode(struct inode *i)
+{
+	return HUGETLBFS_SB(i->i_sb)->hstate;
+}
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)			false
@@ -455,6 +458,10 @@ hugetlb_file_setup(const char *name, size_t size, vm_flags_t acctflag,
 	return ERR_PTR(-ENOSYS);
 }
 
+static inline struct hstate *hstate_inode(struct inode *i)
+{
+	return NULL;
+}
 #endif /* !CONFIG_HUGETLBFS */
 
 #ifdef HAVE_ARCH_HUGETLB_UNMAPPED_AREA
@@ -525,11 +532,6 @@ extern unsigned int default_hstate_idx;
 
 #define default_hstate (hstates[default_hstate_idx])
 
-static inline struct hstate *hstate_inode(struct inode *i)
-{
-	return HUGETLBFS_SB(i->i_sb)->hstate;
-}
-
 static inline struct hstate *hstate_file(struct file *f)
 {
 	return hstate_inode(file_inode(f));
@@ -782,11 +784,6 @@ static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
 	return NULL;
 }
 
-static inline struct hstate *hstate_inode(struct inode *i)
-{
-	return NULL;
-}
-
 static inline struct hstate *page_hstate(struct page *page)
 {
 	return NULL;

commit 075a61d07a8eca2fe980acd94105ed5d6429c55d
Author: Mina Almasry <almasrymina@google.com>
Date:   Wed Apr 1 21:11:28 2020 -0700

    hugetlb_cgroup: add accounting for shared mappings
    
    For shared mappings, the pointer to the hugetlb_cgroup to uncharge lives
    in the resv_map entries, in file_region->reservation_counter.
    
    After a call to region_chg, we charge the approprate hugetlb_cgroup, and
    if successful, we pass on the hugetlb_cgroup info to a follow up
    region_add call.  When a file_region entry is added to the resv_map via
    region_add, we put the pointer to that cgroup in
    file_region->reservation_counter.  If charging doesn't succeed, we report
    the error to the caller, so that the kernel fails the reservation.
    
    On region_del, which is when the hugetlb memory is unreserved, we also
    uncharge the file_region->reservation_counter.
    
    [akpm@linux-foundation.org: forward declare struct file_region]
    Signed-off-by: Mina Almasry <almasrymina@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Sandipan Das <sandipan@linux.ibm.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Link: http://lkml.kernel.org/r/20200211213128.73302-5-almasrymina@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 8f8c0b915fbb..219962c9517e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -57,6 +57,41 @@ struct resv_map {
 	struct cgroup_subsys_state *css;
 #endif
 };
+
+/*
+ * Region tracking -- allows tracking of reservations and instantiated pages
+ *                    across the pages in a mapping.
+ *
+ * The region data structures are embedded into a resv_map and protected
+ * by a resv_map's lock.  The set of regions within the resv_map represent
+ * reservations for huge pages, or huge pages that have already been
+ * instantiated within the map.  The from and to elements are huge page
+ * indicies into the associated mapping.  from indicates the starting index
+ * of the region.  to represents the first index past the end of  the region.
+ *
+ * For example, a file region structure with from == 0 and to == 4 represents
+ * four huge pages in a mapping.  It is important to note that the to element
+ * represents the first element past the end of the region. This is used in
+ * arithmetic as 4(to) - 0(from) = 4 huge pages in the region.
+ *
+ * Interval notation of the form [from, to) will be used to indicate that
+ * the endpoint from is inclusive and to is exclusive.
+ */
+struct file_region {
+	struct list_head link;
+	long from;
+	long to;
+#ifdef CONFIG_CGROUP_HUGETLB
+	/*
+	 * On shared mappings, each reserved region appears as a struct
+	 * file_region in resv_map. These fields hold the info needed to
+	 * uncharge each reservation.
+	 */
+	struct page_counter *reservation_counter;
+	struct cgroup_subsys_state *css;
+#endif
+};
+
 extern struct resv_map *resv_map_alloc(void);
 void resv_map_release(struct kref *ref);
 

commit e9fe92ae0cd28aac5cf6d3fb8442825c22fbd3a6
Author: Mina Almasry <almasrymina@google.com>
Date:   Wed Apr 1 21:11:21 2020 -0700

    hugetlb_cgroup: add reservation accounting for private mappings
    
    Normally the pointer to the cgroup to uncharge hangs off the struct page,
    and gets queried when it's time to free the page.  With hugetlb_cgroup
    reservations, this is not possible.  Because it's possible for a page to
    be reserved by one task and actually faulted in by another task.
    
    The best place to put the hugetlb_cgroup pointer to uncharge for
    reservations is in the resv_map.  But, because the resv_map has different
    semantics for private and shared mappings, the code patch to
    charge/uncharge shared and private mappings is different.  This patch
    implements charging and uncharging for private mappings.
    
    For private mappings, the counter to uncharge is in
    resv_map->reservation_counter.  On initializing the resv_map this is set
    to NULL.  On reservation of a region in private mapping, the tasks
    hugetlb_cgroup is charged and the hugetlb_cgroup is placed is
    resv_map->reservation_counter.
    
    On hugetlb_vm_op_close, we uncharge resv_map->reservation_counter.
    
    [akpm@linux-foundation.org: forward declare struct resv_map]
    Signed-off-by: Mina Almasry <almasrymina@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Sandipan Das <sandipan@linux.ibm.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Link: http://lkml.kernel.org/r/20200211213128.73302-3-almasrymina@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 83ae629c9beb..8f8c0b915fbb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -46,6 +46,16 @@ struct resv_map {
 	long adds_in_progress;
 	struct list_head region_cache;
 	long region_cache_count;
+#ifdef CONFIG_CGROUP_HUGETLB
+	/*
+	 * On private mappings, the counter to uncharge reservations is stored
+	 * here. If these fields are 0, then either the mapping is shared, or
+	 * cgroup accounting is disabled for this resv_map.
+	 */
+	struct page_counter *reservation_counter;
+	unsigned long pages_per_hpage;
+	struct cgroup_subsys_state *css;
+#endif
 };
 extern struct resv_map *resv_map_alloc(void);
 void resv_map_release(struct kref *ref);

commit cdc2fcfea79b9873bb63159f8ed973f4046018c8
Author: Mina Almasry <almasrymina@google.com>
Date:   Wed Apr 1 21:11:11 2020 -0700

    hugetlb_cgroup: add hugetlb_cgroup reservation counter
    
    These counters will track hugetlb reservations rather than hugetlb memory
    faulted in.  This patch only adds the counter, following patches add the
    charging and uncharging of the counter.
    
    This is patch 1 of an 9 patch series.
    
    Problem:
    
    Currently tasks attempting to reserve more hugetlb memory than is
    available get a failure at mmap/shmget time.  This is thanks to Hugetlbfs
    Reservations [1].  However, if a task attempts to reserve more hugetlb
    memory than its hugetlb_cgroup limit allows, the kernel will allow the
    mmap/shmget call, but will SIGBUS the task when it attempts to fault in
    the excess memory.
    
    We have users hitting their hugetlb_cgroup limits and thus we've been
    looking at this failure mode.  We'd like to improve this behavior such
    that users violating the hugetlb_cgroup limits get an error on mmap/shmget
    time, rather than getting SIGBUS'd when they try to fault the excess
    memory in.  This gives the user an opportunity to fallback more gracefully
    to non-hugetlbfs memory for example.
    
    The underlying problem is that today's hugetlb_cgroup accounting happens
    at hugetlb memory *fault* time, rather than at *reservation* time.  Thus,
    enforcing the hugetlb_cgroup limit only happens at fault time, and the
    offending task gets SIGBUS'd.
    
    Proposed Solution:
    
    A new page counter named
    'hugetlb.xMB.rsvd.[limit|usage|max_usage]_in_bytes'. This counter has
    slightly different semantics than
    'hugetlb.xMB.[limit|usage|max_usage]_in_bytes':
    
    - While usage_in_bytes tracks all *faulted* hugetlb memory,
      rsvd.usage_in_bytes tracks all *reserved* hugetlb memory and hugetlb
      memory faulted in without a prior reservation.
    
    - If a task attempts to reserve more memory than limit_in_bytes allows,
      the kernel will allow it to do so.  But if a task attempts to reserve
      more memory than rsvd.limit_in_bytes, the kernel will fail this
      reservation.
    
    This proposal is implemented in this patch series, with tests to verify
    functionality and show the usage.
    
    Alternatives considered:
    
    1. A new cgroup, instead of only a new page_counter attached to the
       existing hugetlb_cgroup.  Adding a new cgroup seemed like a lot of code
       duplication with hugetlb_cgroup.  Keeping hugetlb related page counters
       under hugetlb_cgroup seemed cleaner as well.
    
    2. Instead of adding a new counter, we considered adding a sysctl that
       modifies the behavior of hugetlb.xMB.[limit|usage]_in_bytes, to do
       accounting at reservation time rather than fault time.  Adding a new
       page_counter seems better as userspace could, if it wants, choose to
       enforce different cgroups differently: one via limit_in_bytes, and
       another via rsvd.limit_in_bytes.  This could be very useful if you're
       transitioning how hugetlb memory is partitioned on your system one
       cgroup at a time, for example.  Also, someone may find usage for both
       limit_in_bytes and rsvd.limit_in_bytes concurrently, and this approach
       gives them the option to do so.
    
    Testing:
    - Added tests passing.
    - Used libhugetlbfs for regression testing.
    
    [1]: https://www.kernel.org/doc/html/latest/vm/hugetlbfs_reserv.html
    
    Signed-off-by: Mina Almasry <almasrymina@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Sandipan Das <sandipan@linux.ibm.com>
    Link: http://lkml.kernel.org/r/20200211213128.73302-1-almasrymina@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 4ba379eae152..83ae629c9beb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -440,8 +440,8 @@ struct hstate {
 	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
 #ifdef CONFIG_CGROUP_HUGETLB
 	/* cgroup control files */
-	struct cftype cgroup_files_dfl[5];
-	struct cftype cgroup_files_legacy[5];
+	struct cftype cgroup_files_dfl[7];
+	struct cftype cgroup_files_legacy[9];
 #endif
 	char name[HSTATE_NAME_LEN];
 };

commit c0d0381ade79885c04a04c303284b040616b116e
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Apr 1 21:11:05 2020 -0700

    hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization
    
    Patch series "hugetlbfs: use i_mmap_rwsem for more synchronization", v2.
    
    While discussing the issue with huge_pte_offset [1], I remembered that
    there were more outstanding hugetlb races.  These issues are:
    
    1) For shared pmds, huge PTE pointers returned by huge_pte_alloc can become
       invalid via a call to huge_pmd_unshare by another thread.
    2) hugetlbfs page faults can race with truncation causing invalid global
       reserve counts and state.
    
    A previous attempt was made to use i_mmap_rwsem in this manner as
    described at [2].  However, those patches were reverted starting with [3]
    due to locking issues.
    
    To effectively use i_mmap_rwsem to address the above issues it needs to be
    held (in read mode) during page fault processing.  However, during fault
    processing we need to lock the page we will be adding.  Lock ordering
    requires we take page lock before i_mmap_rwsem.  Waiting until after
    taking the page lock is too late in the fault process for the
    synchronization we want to do.
    
    To address this lock ordering issue, the following patches change the lock
    ordering for hugetlb pages.  This is not too invasive as hugetlbfs
    processing is done separate from core mm in many places.  However, I don't
    really like this idea.  Much ugliness is contained in the new routine
    hugetlb_page_mapping_lock_write() of patch 1.
    
    The only other way I can think of to address these issues is by catching
    all the races.  After catching a race, cleanup, backout, retry ...  etc,
    as needed.  This can get really ugly, especially for huge page
    reservations.  At one time, I started writing some of the reservation
    backout code for page faults and it got so ugly and complicated I went
    down the path of adding synchronization to avoid the races.  Any other
    suggestions would be welcome.
    
    [1] https://lore.kernel.org/linux-mm/1582342427-230392-1-git-send-email-longpeng2@huawei.com/
    [2] https://lore.kernel.org/linux-mm/20181222223013.22193-1-mike.kravetz@oracle.com/
    [3] https://lore.kernel.org/linux-mm/20190103235452.29335-1-mike.kravetz@oracle.com
    [4] https://lore.kernel.org/linux-mm/1584028670.7365.182.camel@lca.pw/
    [5] https://lore.kernel.org/lkml/20200312183142.108df9ac@canb.auug.org.au/
    
    This patch (of 2):
    
    While looking at BUGs associated with invalid huge page map counts, it was
    discovered and observed that a huge pte pointer could become 'invalid' and
    point to another task's page table.  Consider the following:
    
    A task takes a page fault on a shared hugetlbfs file and calls
    huge_pte_alloc to get a ptep.  Suppose the returned ptep points to a
    shared pmd.
    
    Now, another task truncates the hugetlbfs file.  As part of truncation, it
    unmaps everyone who has the file mapped.  If the range being truncated is
    covered by a shared pmd, huge_pmd_unshare will be called.  For all but the
    last user of the shared pmd, huge_pmd_unshare will clear the pud pointing
    to the pmd.  If the task in the middle of the page fault is not the last
    user, the ptep returned by huge_pte_alloc now points to another task's
    page table or worse.  This leads to bad things such as incorrect page
    map/reference counts or invalid memory references.
    
    To fix, expand the use of i_mmap_rwsem as follows:
    - i_mmap_rwsem is held in read mode whenever huge_pmd_share is called.
      huge_pmd_share is only called via huge_pte_alloc, so callers of
      huge_pte_alloc take i_mmap_rwsem before calling.  In addition, callers
      of huge_pte_alloc continue to hold the semaphore until finished with
      the ptep.
    - i_mmap_rwsem is held in write mode whenever huge_pmd_unshare is called.
    
    One problem with this scheme is that it requires taking i_mmap_rwsem
    before taking the page lock during page faults.  This is not the order
    specified in the rest of mm code.  Handling of hugetlbfs pages is mostly
    isolated today.  Therefore, we use this alternative locking order for
    PageHuge() pages.
    
             mapping->i_mmap_rwsem
               hugetlb_fault_mutex (hugetlbfs specific page fault mutex)
                 page->flags PG_locked (lock_page)
    
    To help with lock ordering issues, hugetlb_page_mapping_lock_write() is
    introduced to write lock the i_mmap_rwsem associated with a page.
    
    In most cases it is easy to get address_space via vma->vm_file->f_mapping.
    However, in the case of migration or memory errors for anon pages we do
    not have an associated vma.  A new routine _get_hugetlb_page_mapping()
    will use anon_vma to get address_space in these cases.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Prakash Sangappa <prakash.sangappa@oracle.com>
    Link: http://lkml.kernel.org/r/20200316205756.146666-2-mike.kravetz@oracle.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1e897e4168ac..4ba379eae152 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -109,6 +109,8 @@ u32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx);
 
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 
+struct address_space *hugetlb_page_mapping_lock_write(struct page *hpage);
+
 extern int sysctl_hugetlb_shm_group;
 extern struct list_head huge_boot_pages;
 
@@ -151,6 +153,12 @@ static inline unsigned long hugetlb_total_pages(void)
 	return 0;
 }
 
+static inline struct address_space *hugetlb_page_mapping_lock_write(
+							struct page *hpage)
+{
+	return NULL;
+}
+
 static inline int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr,
 					pte_t *ptep)
 {

commit faced7e0806cf44095a2833ad53ff59c39e6748d
Author: Giuseppe Scrivano <gscrivan@redhat.com>
Date:   Mon Dec 16 20:38:31 2019 +0100

    mm: hugetlb controller for cgroups v2
    
    In the effort of supporting cgroups v2 into Kubernetes, I stumped on
    the lack of the hugetlb controller.
    
    When the controller is enabled, it exposes four new files for each
    hugetlb size on non-root cgroups:
    
    - hugetlb.<hugepagesize>.current
    - hugetlb.<hugepagesize>.max
    - hugetlb.<hugepagesize>.events
    - hugetlb.<hugepagesize>.events.local
    
    The differences with the legacy hierarchy are in the file names and
    using the value "max" instead of "-1" to disable a limit.
    
    The file .limit_in_bytes is renamed to .max.
    
    The file .usage_in_bytes is renamed to .current.
    
    .failcnt is not provided as a single file anymore, but its value can
    be read through the new flat-keyed files .events and .events.local,
    through the "max" key.
    
    Signed-off-by: Giuseppe Scrivano <gscrivan@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 31d4920994b9..1e897e4168ac 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -432,7 +432,8 @@ struct hstate {
 	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
 #ifdef CONFIG_CGROUP_HUGETLB
 	/* cgroup control files */
-	struct cftype cgroup_files[5];
+	struct cftype cgroup_files_dfl[5];
+	struct cftype cgroup_files_legacy[5];
 #endif
 	char name[HSTATE_NAME_LEN];
 };

commit 188b04a7d93860fd100b2671600b8ad81fb0a842
Author: Wei Yang <richardw.yang@linux.intel.com>
Date:   Sat Nov 30 17:57:02 2019 -0800

    hugetlb: remove unused hstate in hugetlb_fault_mutex_hash()
    
    The first parameter hstate in function hugetlb_fault_mutex_hash() is not
    used anymore.
    
    This patch removes it.
    
    [akpm@linux-foundation.org: various build fixes]
    [cai@lca.pw: fix a GCC compilation warning]
     Link: http://lkml.kernel.org/r/1570544108-32331-1-git-send-email-cai@lca.pw
    Link: http://lkml.kernel.org/r/20191005003302.785-1-richardw.yang@linux.intel.com
    Signed-off-by: Wei Yang <richardw.yang@linux.intel.com>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 159d2012cdb1..31d4920994b9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -105,8 +105,7 @@ void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason);
 void free_huge_page(struct page *page);
 void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
-u32 hugetlb_fault_mutex_hash(struct hstate *h, struct address_space *mapping,
-				pgoff_t idx);
+u32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx);
 
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 

commit 1f9dccb25b8fb48778149a002bb25d4ac2899633
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Sat Nov 30 17:56:40 2019 -0800

    hugetlbfs: convert macros to static inline, fix sparse warning
    
    huge_pte_offset() produced a sparse warning due to an improper return
    type when the kernel was built with !CONFIG_HUGETLB_PAGE.  Fix the bad
    type and also convert all the macros in this block to static inline
    wrappers.  Two existing wrappers in this block had lines in excess of 80
    columns so clean those up as well.
    
    No functional change.
    
    Link: http://lkml.kernel.org/r/20191112194558.139389-3-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reported-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d3814bd686ba..159d2012cdb1 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -164,38 +164,130 @@ static inline void adjust_range_if_pmd_sharing_possible(
 {
 }
 
-#define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n)	({ BUG(); 0; })
-#define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
-#define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
+static inline long follow_hugetlb_page(struct mm_struct *mm,
+			struct vm_area_struct *vma, struct page **pages,
+			struct vm_area_struct **vmas, unsigned long *position,
+			unsigned long *nr_pages, long i, unsigned int flags,
+			int *nonblocking)
+{
+	BUG();
+	return 0;
+}
+
+static inline struct page *follow_huge_addr(struct mm_struct *mm,
+					unsigned long address, int write)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+static inline int copy_hugetlb_page_range(struct mm_struct *dst,
+			struct mm_struct *src, struct vm_area_struct *vma)
+{
+	BUG();
+	return 0;
+}
+
 static inline void hugetlb_report_meminfo(struct seq_file *m)
 {
 }
-#define hugetlb_report_node_meminfo(n, buf)	0
+
+static inline int hugetlb_report_node_meminfo(int nid, char *buf)
+{
+	return 0;
+}
+
 static inline void hugetlb_show_meminfo(void)
 {
 }
-#define follow_huge_pd(vma, addr, hpd, flags, pdshift) NULL
-#define follow_huge_pmd(mm, addr, pmd, flags)	NULL
-#define follow_huge_pud(mm, addr, pud, flags)	NULL
-#define follow_huge_pgd(mm, addr, pgd, flags)	NULL
-#define prepare_hugepage_range(file, addr, len)	(-EINVAL)
-#define pmd_huge(x)	0
-#define pud_huge(x)	0
-#define is_hugepage_only_range(mm, addr, len)	0
-#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
-#define hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma, dst_addr, \
-				src_addr, pagep)	({ BUG(); 0; })
-#define huge_pte_offset(mm, address, sz)	0
+
+static inline struct page *follow_huge_pd(struct vm_area_struct *vma,
+				unsigned long address, hugepd_t hpd, int flags,
+				int pdshift)
+{
+	return NULL;
+}
+
+static inline struct page *follow_huge_pmd(struct mm_struct *mm,
+				unsigned long address, pmd_t *pmd, int flags)
+{
+	return NULL;
+}
+
+static inline struct page *follow_huge_pud(struct mm_struct *mm,
+				unsigned long address, pud_t *pud, int flags)
+{
+	return NULL;
+}
+
+static inline struct page *follow_huge_pgd(struct mm_struct *mm,
+				unsigned long address, pgd_t *pgd, int flags)
+{
+	return NULL;
+}
+
+static inline int prepare_hugepage_range(struct file *file,
+				unsigned long addr, unsigned long len)
+{
+	return -EINVAL;
+}
+
+static inline int pmd_huge(pmd_t pmd)
+{
+	return 0;
+}
+
+static inline int pud_huge(pud_t pud)
+{
+	return 0;
+}
+
+static inline int is_hugepage_only_range(struct mm_struct *mm,
+					unsigned long addr, unsigned long len)
+{
+	return 0;
+}
+
+static inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,
+				unsigned long addr, unsigned long end,
+				unsigned long floor, unsigned long ceiling)
+{
+	BUG();
+}
+
+static inline int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
+						pte_t *dst_pte,
+						struct vm_area_struct *dst_vma,
+						unsigned long dst_addr,
+						unsigned long src_addr,
+						struct page **pagep)
+{
+	BUG();
+	return 0;
+}
+
+static inline pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr,
+					unsigned long sz)
+{
+	return NULL;
+}
 
 static inline bool isolate_huge_page(struct page *page, struct list_head *list)
 {
 	return false;
 }
-#define putback_active_hugepage(p)	do {} while (0)
-#define move_hugetlb_state(old, new, reason)	do {} while (0)
 
-static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
-		unsigned long address, unsigned long end, pgprot_t newprot)
+static inline void putback_active_hugepage(struct page *page)
+{
+}
+
+static inline void move_hugetlb_state(struct page *oldpage,
+					struct page *newpage, int reason)
+{
+}
+
+static inline unsigned long hugetlb_change_protection(
+			struct vm_area_struct *vma, unsigned long address,
+			unsigned long end, pgprot_t newprot)
 {
 	return 0;
 }
@@ -213,9 +305,10 @@ static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
 {
 	BUG();
 }
+
 static inline vm_fault_t hugetlb_fault(struct mm_struct *mm,
-				struct vm_area_struct *vma, unsigned long address,
-				unsigned int flags)
+			struct vm_area_struct *vma, unsigned long address,
+			unsigned int flags)
 {
 	BUG();
 	return 0;

commit 552546366a30d88bd1d6f5efe848b2ab50fd57e5
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Sat Nov 30 17:56:30 2019 -0800

    hugetlbfs: hugetlb_fault_mutex_hash() cleanup
    
    A new clang diagnostic (-Wsizeof-array-div) warns about the calculation
    to determine the number of u32's in an array of unsigned longs.
    Suppress warning by adding parentheses.
    
    While looking at the above issue, noticed that the 'address' parameter
    to hugetlb_fault_mutex_hash is no longer used.  So, remove it from the
    definition and all callers.
    
    No functional change.
    
    Link: http://lkml.kernel.org/r/20190919011847.18400-1-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reported-by: Nathan Chancellor <natechancellor@gmail.com>
    Reviewed-by: Nathan Chancellor <natechancellor@gmail.com>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Ilie Halip <ilie.halip@gmail.com>
    Cc: David Bolvansky <david.bolvansky@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 53fc34f930d0..d3814bd686ba 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -106,7 +106,7 @@ void free_huge_page(struct page *page);
 void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
 u32 hugetlb_fault_mutex_hash(struct hstate *h, struct address_space *mapping,
-				pgoff_t idx, unsigned long address);
+				pgoff_t idx);
 
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 

commit a50b854e073cd3335bbbada8dcff83a857297dd7
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Sep 23 15:34:25 2019 -0700

    mm: introduce page_size()
    
    Patch series "Make working with compound pages easier", v2.
    
    These three patches add three helpers and convert the appropriate
    places to use them.
    
    This patch (of 3):
    
    It's unnecessarily hard to find out the size of a potentially huge page.
    Replace 'PAGE_SIZE << compound_order(page)' with page_size(page).
    
    Link: http://lkml.kernel.org/r/20190721104612.19120-2-willy@infradead.org
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index edfca4278319..53fc34f930d0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -454,7 +454,7 @@ static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 static inline struct hstate *page_hstate(struct page *page)
 {
 	VM_BUG_ON_PAGE(!PageHuge(page), page);
-	return size_to_hstate(PAGE_SIZE << compound_order(page));
+	return size_to_hstate(page_size(page));
 }
 
 static inline unsigned hstate_index_to_shift(unsigned index)

commit cbd34da7dc9afd521e0bea5e7d12701f4a9da7c7
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jul 11 20:57:28 2019 -0700

    mm: move the powerpc hugepd code to mm/gup.c
    
    While only powerpc supports the hugepd case, the code is pretty generic
    and I'd like to keep all GUP internals in one place.
    
    Link: http://lkml.kernel.org/r/20190625143715.1689-15-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f895a79c6f5c..edfca4278319 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -16,29 +16,11 @@ struct user_struct;
 struct mmu_gather;
 
 #ifndef is_hugepd
-/*
- * Some architectures requires a hugepage directory format that is
- * required to support multiple hugepage sizes. For example
- * a4fe3ce76 "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
- * introduced the same on powerpc. This allows for a more flexible hugepage
- * pagetable layout.
- */
 typedef struct { unsigned long pd; } hugepd_t;
 #define is_hugepd(hugepd) (0)
 #define __hugepd(x) ((hugepd_t) { (x) })
-static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-			      unsigned pdshift, unsigned long end,
-			      int write, struct page **pages, int *nr)
-{
-	return 0;
-}
-#else
-extern int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-		       unsigned pdshift, unsigned long end,
-		       int write, struct page **pages, int *nr);
 #endif
 
-
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>

commit 442a5a9a9295bfd9b0cffd0691ef8a6ce81db7c4
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jul 11 20:54:40 2019 -0700

    mm: make !CONFIG_HUGE_PAGE wrappers into static inlines
    
    Instead of using defines, which loses type safety and provokes unused
    variable warnings from gcc, put the constants into static inlines.
    
    Link: http://lkml.kernel.org/r/20190522235102.GA15370@mellanox.com
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index edf476c8cfb9..f895a79c6f5c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -608,22 +608,92 @@ static inline void huge_ptep_modify_prot_commit(struct vm_area_struct *vma,
 
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
-#define alloc_huge_page(v, a, r) NULL
-#define alloc_huge_page_node(h, nid) NULL
-#define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL
-#define alloc_huge_page_vma(h, vma, address) NULL
-#define alloc_bootmem_huge_page(h) NULL
-#define hstate_file(f) NULL
-#define hstate_sizelog(s) NULL
-#define hstate_vma(v) NULL
-#define hstate_inode(i) NULL
-#define page_hstate(page) NULL
-#define huge_page_size(h) PAGE_SIZE
-#define huge_page_mask(h) PAGE_MASK
-#define vma_kernel_pagesize(v) PAGE_SIZE
-#define vma_mmu_pagesize(v) PAGE_SIZE
-#define huge_page_order(h) 0
-#define huge_page_shift(h) PAGE_SHIFT
+
+static inline struct page *alloc_huge_page(struct vm_area_struct *vma,
+					   unsigned long addr,
+					   int avoid_reserve)
+{
+	return NULL;
+}
+
+static inline struct page *alloc_huge_page_node(struct hstate *h, int nid)
+{
+	return NULL;
+}
+
+static inline struct page *
+alloc_huge_page_nodemask(struct hstate *h, int preferred_nid, nodemask_t *nmask)
+{
+	return NULL;
+}
+
+static inline struct page *alloc_huge_page_vma(struct hstate *h,
+					       struct vm_area_struct *vma,
+					       unsigned long address)
+{
+	return NULL;
+}
+
+static inline int __alloc_bootmem_huge_page(struct hstate *h)
+{
+	return 0;
+}
+
+static inline struct hstate *hstate_file(struct file *f)
+{
+	return NULL;
+}
+
+static inline struct hstate *hstate_sizelog(int page_size_log)
+{
+	return NULL;
+}
+
+static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
+{
+	return NULL;
+}
+
+static inline struct hstate *hstate_inode(struct inode *i)
+{
+	return NULL;
+}
+
+static inline struct hstate *page_hstate(struct page *page)
+{
+	return NULL;
+}
+
+static inline unsigned long huge_page_size(struct hstate *h)
+{
+	return PAGE_SIZE;
+}
+
+static inline unsigned long huge_page_mask(struct hstate *h)
+{
+	return PAGE_MASK;
+}
+
+static inline unsigned long vma_kernel_pagesize(struct vm_area_struct *vma)
+{
+	return PAGE_SIZE;
+}
+
+static inline unsigned long vma_mmu_pagesize(struct vm_area_struct *vma)
+{
+	return PAGE_SIZE;
+}
+
+static inline unsigned int huge_page_order(struct hstate *h)
+{
+	return 0;
+}
+
+static inline unsigned int huge_page_shift(struct hstate *h)
+{
+	return PAGE_SHIFT;
+}
+
 static inline bool hstate_is_gigantic(struct hstate *h)
 {
 	return false;

commit 1b426bac66e6cc83c9f2d92b96e4e72acf43419a
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Mon May 13 17:19:41 2019 -0700

    hugetlb: use same fault hash key for shared and private mappings
    
    hugetlb uses a fault mutex hash table to prevent page faults of the
    same pages concurrently.  The key for shared and private mappings is
    different.  Shared keys off address_space and file index.  Private keys
    off mm and virtual address.  Consider a private mappings of a populated
    hugetlbfs file.  A fault will map the page from the file and if needed
    do a COW to map a writable page.
    
    Hugetlbfs hole punch uses the fault mutex to prevent mappings of file
    pages.  It uses the address_space file index key.  However, private
    mappings will use a different key and could race with this code to map
    the file page.  This causes problems (BUG) for the page cache remove
    code as it expects the page to be unmapped.  A sample stack is:
    
    page dumped because: VM_BUG_ON_PAGE(page_mapped(page))
    kernel BUG at mm/filemap.c:169!
    ...
    RIP: 0010:unaccount_page_cache_page+0x1b8/0x200
    ...
    Call Trace:
    __delete_from_page_cache+0x39/0x220
    delete_from_page_cache+0x45/0x70
    remove_inode_hugepages+0x13c/0x380
    ? __add_to_page_cache_locked+0x162/0x380
    hugetlbfs_fallocate+0x403/0x540
    ? _cond_resched+0x15/0x30
    ? __inode_security_revalidate+0x5d/0x70
    ? selinux_file_permission+0x100/0x130
    vfs_fallocate+0x13f/0x270
    ksys_fallocate+0x3c/0x80
    __x64_sys_fallocate+0x1a/0x20
    do_syscall_64+0x5b/0x180
    entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    There seems to be another potential COW issue/race with this approach
    of different private and shared keys as noted in commit 8382d914ebf7
    ("mm, hugetlb: improve page-fault scalability").
    
    Since every hugetlb mapping (even anon and private) is actually a file
    mapping, just use the address_space index key for all mappings.  This
    results in potentially more hash collisions.  However, this should not
    be the common case.
    
    Link: http://lkml.kernel.org/r/20190328234704.27083-3-mike.kravetz@oracle.com
    Link: http://lkml.kernel.org/r/20190412165235.t4sscoujczfhuiyt@linux-r8p5
    Fixes: b5cec28d36f5 ("hugetlbfs: truncate_hugepages() takes a range of pages")
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 11943b60f208..edf476c8cfb9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -123,9 +123,7 @@ void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason);
 void free_huge_page(struct page *page);
 void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
-u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
-				struct vm_area_struct *vma,
-				struct address_space *mapping,
+u32 hugetlb_fault_mutex_hash(struct hstate *h, struct address_space *mapping,
 				pgoff_t idx, unsigned long address);
 
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);

commit a953e7721fa9999fd628885ed451e16641a23d1e
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Thu Mar 28 20:43:51 2019 -0700

    include/linux/hugetlb.h: convert to use vm_fault_t
    
    kbuild produces the below warning:
    
      tree: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git master
      head:   5453a3df2a5eb49bc24615d4cf0d66b2aae05e5f
      commit 3d3539018d2c ("mm: create the new vm_fault_t type")
      reproduce:
            # apt-get install sparse
            git checkout 3d3539018d2cbd12e5af4a132636ee7fd8d43ef0
            make ARCH=x86_64 allmodconfig
            make C=1 CF='-fdiagnostic-prefix -D__CHECK_ENDIAN__'
    
      >> mm/memory.c:3968:21: sparse: incorrect type in assignment (different
      >> base types) @@    expected restricted vm_fault_t [usertype] ret @@
      >> got e] ret @@
         mm/memory.c:3968:21:    expected restricted vm_fault_t [usertype] ret
         mm/memory.c:3968:21:    got int
    
    This patch converts to return vm_fault_t type for hugetlb_fault() when
    CONFIG_HUGETLB_PAGE=n.
    
    Regarding the sparse warning, Luc said:
    
    : This is the expected behaviour.  The constant 0 is magic regarding bitwise
    : types but ({ ...; 0; }) is not, it is just an ordinary expression of type
    : 'int'.
    :
    : So, IMHO, Souptick's patch is the right thing to do.
    
    Link: http://lkml.kernel.org/r/20190318162604.GA31553@jordon-HP-15-Notebook-PC
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ea35263eb76b..11943b60f208 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -203,7 +203,6 @@ static inline void hugetlb_show_meminfo(void)
 #define pud_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
-#define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma, dst_addr, \
 				src_addr, pagep)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address, sz)	0
@@ -234,6 +233,13 @@ static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
 {
 	BUG();
 }
+static inline vm_fault_t hugetlb_fault(struct mm_struct *mm,
+				struct vm_area_struct *vma, unsigned long address,
+				unsigned int flags)
+{
+	BUG();
+	return 0;
+}
 
 #endif /* !CONFIG_HUGETLB_PAGE */
 /*

commit 9a4e9f3b2d7393d50256762c21e7466b4b6b1c9c
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Mar 5 15:47:44 2019 -0800

    mm: update get_user_pages_longterm to migrate pages allocated from CMA region
    
    This patch updates get_user_pages_longterm to migrate pages allocated
    out of CMA region.  This makes sure that we don't keep non-movable pages
    (due to page reference count) in the CMA area.
    
    This will be used by ppc64 in a later patch to avoid pinning pages in
    the CMA region.  ppc64 uses CMA region for allocation of the hardware
    page table (hash page table) and not able to migrate pages out of CMA
    region results in page table allocation failures.
    
    One case where we hit this easy is when a guest using a VFIO passthrough
    device.  VFIO locks all the guest's memory and if the guest memory is
    backed by CMA region, it becomes unmovable resulting in fragmenting the
    CMA and possibly preventing other guests from allocation a large enough
    hash page table.
    
    NOTE: We allocate the new page without using __GFP_THISNODE
    
    Link: http://lkml.kernel.org/r/20190114095438.32470-3-aneesh.kumar@linux.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 54c317c8355f..ea35263eb76b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -371,6 +371,8 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 				nodemask_t *nmask);
 struct page *alloc_huge_page_vma(struct hstate *h, struct vm_area_struct *vma,
 				unsigned long address);
+struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
+				     int nid, nodemask_t *nmask);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 

commit 023bdd00235eb0dcb71fd98f0b8347a9bb85d417
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Mar 5 15:46:37 2019 -0800

    mm/hugetlb: add prot_modify_start/commit sequence for hugetlb update
    
    Architectures like ppc64 require to do a conditional tlb flush based on
    the old and new value of pte.  Follow the regular pte change protection
    sequence for hugetlb too.  This allows the architectures to override the
    update sequence.
    
    Link: http://lkml.kernel.org/r/20190116085035.29729-5-aneesh.kumar@linux.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 4cc3871b65fc..54c317c8355f 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -580,6 +580,26 @@ static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr
 	set_huge_pte_at(mm, addr, ptep, pte);
 }
 #endif
+
+#ifndef huge_ptep_modify_prot_start
+#define huge_ptep_modify_prot_start huge_ptep_modify_prot_start
+static inline pte_t huge_ptep_modify_prot_start(struct vm_area_struct *vma,
+						unsigned long addr, pte_t *ptep)
+{
+	return huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);
+}
+#endif
+
+#ifndef huge_ptep_modify_prot_commit
+#define huge_ptep_modify_prot_commit huge_ptep_modify_prot_commit
+static inline void huge_ptep_modify_prot_commit(struct vm_area_struct *vma,
+						unsigned long addr, pte_t *ptep,
+						pte_t old_pte, pte_t pte)
+{
+	set_huge_pte_at(vma->vm_mm, addr, ptep, pte);
+}
+#endif
+
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL

commit e693de186414ae66f2a316ff9befcd2b7a6d07b6
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:43:51 2019 -0800

    mm/hugetlb: enable arch specific huge page size support for migration
    
    Architectures like arm64 have HugeTLB page sizes which are different
    than generic sizes at PMD, PUD, PGD level and implemented via contiguous
    bits.  At present these special size HugeTLB pages cannot be identified
    through macros like (PMD|PUD|PGDIR)_SHIFT and hence chosen not be
    migrated.
    
    Enabling migration support for these special HugeTLB page sizes along
    with the generic ones (PMD|PUD|PGD) would require identifying all of
    them on a given platform.  A platform specific hook can precisely
    enumerate all huge page sizes supported for migration.  Instead of
    comparing against standard huge page orders let
    hugetlb_migration_support() function call a platform hook
    arch_hugetlb_migration_support().  Default definition for the platform
    hook maintains existing semantics which checks standard huge page order.
    But an architecture can choose to override the default and provide
    support for a comprehensive set of huge page sizes.
    
    Link: http://lkml.kernel.org/r/1545121450-1663-4-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 70bcd8973323..4cc3871b65fc 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -493,18 +493,29 @@ static inline pgoff_t basepage_index(struct page *page)
 extern int dissolve_free_huge_page(struct page *page);
 extern int dissolve_free_huge_pages(unsigned long start_pfn,
 				    unsigned long end_pfn);
-static inline bool hugepage_migration_supported(struct hstate *h)
-{
+
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
+#ifndef arch_hugetlb_migration_supported
+static inline bool arch_hugetlb_migration_supported(struct hstate *h)
+{
 	if ((huge_page_shift(h) == PMD_SHIFT) ||
 		(huge_page_shift(h) == PUD_SHIFT) ||
 			(huge_page_shift(h) == PGDIR_SHIFT))
 		return true;
 	else
 		return false;
+}
+#endif
 #else
+static inline bool arch_hugetlb_migration_supported(struct hstate *h)
+{
 	return false;
+}
 #endif
+
+static inline bool hugepage_migration_supported(struct hstate *h)
+{
+	return arch_hugetlb_migration_supported(h);
 }
 
 /*

commit 9b553bf5eb99dd1b2d8ae23136da46da5c205dfd
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:43:48 2019 -0800

    mm/hugetlb: enable PUD level huge page migration
    
    Architectures like arm64 have PUD level HugeTLB pages for certain configs
    (1GB huge page is PUD based on ARM64_4K_PAGES base page size) that can
    be enabled for migration.  It can be achieved through checking for
    PUD_SHIFT order based HugeTLB pages during migration.
    
    Link: http://lkml.kernel.org/r/1545121450-1663-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1b858d795731..70bcd8973323 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -497,7 +497,8 @@ static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
 	if ((huge_page_shift(h) == PMD_SHIFT) ||
-		(huge_page_shift(h) == PGDIR_SHIFT))
+		(huge_page_shift(h) == PUD_SHIFT) ||
+			(huge_page_shift(h) == PGDIR_SHIFT))
 		return true;
 	else
 		return false;

commit 7ed2c31dabdeb3ee6abe8ff5aac7287821a50cba
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:43:44 2019 -0800

    mm/hugetlb: distinguish between migratability and movability
    
    Patch series "arm64/mm: Enable HugeTLB migration", v4.
    
    This patch series enables HugeTLB migration support for all supported
    huge page sizes at all levels including contiguous bit implementation.
    Following HugeTLB migration support matrix has been enabled with this
    patch series.  All permutations have been tested except for the 16GB.
    
               CONT PTE    PMD    CONT PMD    PUD
               --------    ---    --------    ---
      4K:         64K     2M         32M     1G
      16K:         2M    32M          1G
      64K:         2M   512M         16G
    
    First the series adds migration support for PUD based huge pages.  It
    then adds a platform specific hook to query an architecture if a given
    huge page size is supported for migration while also providing a default
    fallback option preserving the existing semantics which just checks for
    (PMD|PUD|PGDIR)_SHIFT macros.  The last two patches enables HugeTLB
    migration on arm64 and subscribe to this new platform specific hook by
    defining an override.
    
    The second patch differentiates between movability and migratability
    aspects of huge pages and implements hugepage_movable_supported() which
    can then be used during allocation to decide whether to place the huge
    page in movable zone or not.
    
    This patch (of 5):
    
    During huge page allocation it's migratability is checked to determine
    if it should be placed under movable zones with GFP_HIGHUSER_MOVABLE.
    But the movability aspect of the huge page could depend on other factors
    than just migratability.  Movability in itself is a distinct property
    which should not be tied with migratability alone.
    
    This differentiates these two and implements an enhanced movability check
    which also considers huge page size to determine if it is feasible to be
    placed under a movable zone.  At present it just checks for gigantic pages
    but going forward it can incorporate other enhanced checks.
    
    Link: http://lkml.kernel.org/r/1545121450-1663-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Suggested-by: Michal Hocko <mhocko@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 087fd5f48c91..1b858d795731 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -506,6 +506,31 @@ static inline bool hugepage_migration_supported(struct hstate *h)
 #endif
 }
 
+/*
+ * Movability check is different as compared to migration check.
+ * It determines whether or not a huge page should be placed on
+ * movable zone or not. Movability of any huge page should be
+ * required only if huge page size is supported for migration.
+ * There wont be any reason for the huge page to be movable if
+ * it is not migratable to start with. Also the size of the huge
+ * page should be large enough to be placed under a movable zone
+ * and still feasible enough to be migratable. Just the presence
+ * in movable zone does not make the migration feasible.
+ *
+ * So even though large huge page sizes like the gigantic ones
+ * are migratable they should not be movable because its not
+ * feasible to migrate them from movable zone.
+ */
+static inline bool hugepage_movable_supported(struct hstate *h)
+{
+	if (!hugepage_migration_supported(h))
+		return false;
+
+	if (hstate_is_gigantic(h))
+		return false;
+	return true;
+}
+
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 					   struct mm_struct *mm, pte_t *pte)
 {
@@ -602,6 +627,11 @@ static inline bool hugepage_migration_supported(struct hstate *h)
 	return false;
 }
 
+static inline bool hugepage_movable_supported(struct hstate *h)
+{
+	return false;
+}
+
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 					   struct mm_struct *mm, pte_t *pte)
 {

commit 017b1660df89f5fb4bfe66c34e35f7d2031100c7
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Fri Oct 5 15:51:29 2018 -0700

    mm: migration: fix migration of huge PMD shared pages
    
    The page migration code employs try_to_unmap() to try and unmap the source
    page.  This is accomplished by using rmap_walk to find all vmas where the
    page is mapped.  This search stops when page mapcount is zero.  For shared
    PMD huge pages, the page map count is always 1 no matter the number of
    mappings.  Shared mappings are tracked via the reference count of the PMD
    page.  Therefore, try_to_unmap stops prematurely and does not completely
    unmap all mappings of the source page.
    
    This problem can result is data corruption as writes to the original
    source page can happen after contents of the page are copied to the target
    page.  Hence, data is lost.
    
    This problem was originally seen as DB corruption of shared global areas
    after a huge page was soft offlined due to ECC memory errors.  DB
    developers noticed they could reproduce the issue by (hotplug) offlining
    memory used to back huge pages.  A simple testcase can reproduce the
    problem by creating a shared PMD mapping (note that this must be at least
    PUD_SIZE in size and PUD_SIZE aligned (1GB on x86)), and using
    migrate_pages() to migrate process pages between nodes while continually
    writing to the huge pages being migrated.
    
    To fix, have the try_to_unmap_one routine check for huge PMD sharing by
    calling huge_pmd_unshare for hugetlbfs huge pages.  If it is a shared
    mapping it will be 'unshared' which removes the page table entry and drops
    the reference on the PMD page.  After this, flush caches and TLB.
    
    mmu notifiers are called before locking page tables, but we can not be
    sure of PMD sharing until page tables are locked.  Therefore, check for
    the possibility of PMD sharing before locking so that notifiers can
    prepare for the worst possible case.
    
    Link: http://lkml.kernel.org/r/20180823205917.16297-2-mike.kravetz@oracle.com
    [mike.kravetz@oracle.com: make _range_in_vma() a static inline]
      Link: http://lkml.kernel.org/r/6063f215-a5c8-2f0c-465a-2c515ddc952d@oracle.com
    Fixes: 39dde65c9940 ("shared page table for hugetlb page")
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6b68e345f0ca..087fd5f48c91 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -140,6 +140,8 @@ pte_t *huge_pte_alloc(struct mm_struct *mm,
 pte_t *huge_pte_offset(struct mm_struct *mm,
 		       unsigned long addr, unsigned long sz);
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
+void adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,
+				unsigned long *start, unsigned long *end);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
 struct page *follow_huge_pd(struct vm_area_struct *vma,
@@ -170,6 +172,18 @@ static inline unsigned long hugetlb_total_pages(void)
 	return 0;
 }
 
+static inline int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr,
+					pte_t *ptep)
+{
+	return 0;
+}
+
+static inline void adjust_range_if_pmd_sharing_possible(
+				struct vm_area_struct *vma,
+				unsigned long *start, unsigned long *end)
+{
+}
+
 #define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n)	({ BUG(); 0; })
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })

commit 2b7403035459c75e193c6b04a293e518a4212de0
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Thu Aug 23 17:01:36 2018 -0700

    mm: Change return type int to vm_fault_t for fault handlers
    
    Use new return type vm_fault_t for fault handler.  For now, this is just
    documenting that the function returns a VM_FAULT value rather than an
    errno.  Once all instances are converted, vm_fault_t will become a
    distinct type.
    
    Ref-> commit 1c8f422059ae ("mm: change return type to vm_fault_t")
    
    The aim is to change the return type of finish_fault() and
    handle_mm_fault() to vm_fault_t type.  As part of that clean up return
    type of all other recursively called functions have been changed to
    vm_fault_t type.
    
    The places from where handle_mm_fault() is getting invoked will be
    change to vm_fault_t type but in a separate patch.
    
    vmf_error() is the newly introduce inline function in 4.17-rc6.
    
    [akpm@linux-foundation.org: don't shadow outer local `ret' in __do_huge_pmd_anonymous_page()]
    Link: http://lkml.kernel.org/r/20180604171727.GA20279@jordon-HP-15-Notebook-PC
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index c39d9170a8a0..6b68e345f0ca 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -105,7 +105,7 @@ void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
 void hugetlb_show_meminfo(void);
 unsigned long hugetlb_total_pages(void);
-int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags);
 int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm, pte_t *dst_pte,
 				struct vm_area_struct *dst_vma,

commit 40d18ebffb3974272a920c41f2d74431152cae98
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Fri Aug 17 15:49:07 2018 -0700

    mm/hugetlb: remove gigantic page support for HIGHMEM
    
    This reverts ee8f248d266e ("hugetlb: add phys addr to struct
    huge_bootmem_page").
    
    At one time powerpc used this field and supporting code.  However that
    was removed with commit 79cc38ded1e1 ("powerpc/mm/hugetlb: Add support
    for reserving gigantic huge pages via kernel command line").
    
    There are no users of this field and supporting code, so remove it.
    
    Link: http://lkml.kernel.org/r/20180711195913.1294-1-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Cannon Matthews <cannonmatthews@google.com>
    Cc: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 36fa6a2a82e3..c39d9170a8a0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -348,9 +348,6 @@ struct hstate {
 struct huge_bootmem_page {
 	struct list_head list;
 	struct hstate *hstate;
-#ifdef CONFIG_HIGHMEM
-	phys_addr_t phys;
-#endif
 };
 
 struct page *alloc_huge_page(struct vm_area_struct *vma,

commit 389c8178d0904f944887ccca2256ff9d79c12e8e
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jan 31 16:21:03 2018 -0800

    hugetlb, mbind: fall back to default policy if vma is NULL
    
    Dan Carpenter has noticed that mbind migration callback (new_page) can
    get a NULL vma pointer and choke on it inside alloc_huge_page_vma which
    relies on the VMA to get the hstate.  We used to BUG_ON this case but
    the BUG_+ON has been removed recently by "hugetlb, mempolicy: fix the
    mbind hugetlb migration".
    
    The proper way to handle this is to get the hstate from the migrated
    page and rely on huge_node (resp.  get_vma_policy) do the right thing
    with null VMA.  We are currently falling back to the default mempolicy
    in that case which is in line what THP path is doing here.
    
    Link: http://lkml.kernel.org/r/20180110104712.GR1732@dhcp22.suse.cz
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 612a29b7f6c6..36fa6a2a82e3 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -358,7 +358,8 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 				nodemask_t *nmask);
-struct page *alloc_huge_page_vma(struct vm_area_struct *vma, unsigned long address);
+struct page *alloc_huge_page_vma(struct hstate *h, struct vm_area_struct *vma,
+				unsigned long address);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
@@ -536,7 +537,7 @@ struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL
-#define alloc_huge_page_vma(vma, address) NULL
+#define alloc_huge_page_vma(h, vma, address) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
 #define hstate_sizelog(s) NULL

commit ebd637235890a3fa6a6d4bb57522098f2f59c693
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jan 31 16:21:00 2018 -0800

    hugetlb, mempolicy: fix the mbind hugetlb migration
    
    do_mbind migration code relies on alloc_huge_page_noerr for hugetlb
    pages.  alloc_huge_page_noerr uses alloc_huge_page which is a highlevel
    allocation function which has to take care of reserves, overcommit or
    hugetlb cgroup accounting.  None of that is really required for the page
    migration because the new page is only temporal and either will replace
    the original page or it will be dropped.  This is essentially as for
    other migration call paths and there shouldn't be any reason to handle
    mbind in a special way.
    
    The current implementation is even suboptimal because the migration
    might fail just because the hugetlb cgroup limit is reached, or the
    overcommit is saturated.
    
    Fix this by making mbind like other hugetlb migration paths.  Add a new
    migration helper alloc_huge_page_vma as a wrapper around
    alloc_huge_page_nodemask with additional mempolicy handling.
    
    alloc_huge_page_noerr has no more users and it can go.
    
    Link: http://lkml.kernel.org/r/20180103093213.26329-7-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 66992348531e..612a29b7f6c6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -356,10 +356,9 @@ struct huge_bootmem_page {
 struct page *alloc_huge_page(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
-struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
-				unsigned long addr, int avoid_reserve);
 struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 				nodemask_t *nmask);
+struct page *alloc_huge_page_vma(struct vm_area_struct *vma, unsigned long address);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
@@ -537,7 +536,7 @@ struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL
-#define alloc_huge_page_noerr(v, a, r) NULL
+#define alloc_huge_page_vma(vma, address) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
 #define hstate_sizelog(s) NULL

commit ab5ac90aecf5685eb630c42c396f5f14726b0afd
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jan 31 16:20:48 2018 -0800

    mm, hugetlb: do not rely on overcommit limit during migration
    
    hugepage migration relies on __alloc_buddy_huge_page to get a new page.
    This has 2 main disadvantages.
    
    1) it doesn't allow to migrate any huge page if the pool is used
       completely which is not an exceptional case as the pool is static and
       unused memory is just wasted.
    
    2) it leads to a weird semantic when migration between two numa nodes
       might increase the pool size of the destination NUMA node while the
       page is in use.  The issue is caused by per NUMA node surplus pages
       tracking (see free_huge_page).
    
    Address both issues by changing the way how we allocate and account
    pages allocated for migration.  Those should temporal by definition.  So
    we mark them that way (we will abuse page flags in the 3rd page) and
    update free_huge_page to free such pages to the page allocator.  Page
    migration path then just transfers the temporal status from the new page
    to the old one which will be freed on the last reference.  The global
    surplus count will never change during this path but we still have to be
    careful when migrating a per-node suprlus page.  This is now handled in
    move_hugetlb_state which is called from the migration path and it copies
    the hugetlb specific page state and fixes up the accounting when needed
    
    Rename __alloc_buddy_huge_page to __alloc_surplus_huge_page to better
    reflect its purpose.  The new allocation routine for the migration path
    is __alloc_migrate_huge_page.
    
    The user visible effect of this patch is that migrated pages are really
    temporal and they travel between NUMA nodes as per the migration
    request:
    
    Before migration
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:1
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
    
    After
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:1
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
    
    with the previous implementation, both nodes would have nr_hugepages:1
    until the page is freed.
    
    Link: http://lkml.kernel.org/r/20180103093213.26329-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 944e6e8bd572..66992348531e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -119,6 +119,7 @@ long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
 						long freed);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
+void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason);
 void free_huge_page(struct page *page);
 void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
@@ -157,6 +158,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
 bool is_hugetlb_entry_migration(pte_t pte);
+
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
@@ -197,6 +199,7 @@ static inline bool isolate_huge_page(struct page *page, struct list_head *list)
 	return false;
 }
 #define putback_active_hugepage(p)	do {} while (0)
+#define move_hugetlb_state(old, new, reason)	do {} while (0)
 
 static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot)

commit ff62a34210441103108d435ae8a00a777c4dcb99
Author: Marc-André Lureau <marcandre.lureau@redhat.com>
Date:   Wed Jan 31 16:19:25 2018 -0800

    hugetlb: implement memfd sealing
    
    Implements memfd sealing, similar to shmem:
     - WRITE: deny fallocate(PUNCH_HOLE). mmap() write is denied in
       memfd_add_seals(). write() doesn't exist for hugetlbfs.
     - SHRINK: added similar check as shmem_setattr()
     - GROW: added similar check as shmem_setattr() & shmem_fallocate()
    
    Except write() operation that doesn't exist with hugetlbfs, that should
    make sealing as close as it can be to shmem support.
    
    Link: http://lkml.kernel.org/r/20171107122800.25517-5-marcandre.lureau@redhat.com
    Signed-off-by: Marc-André Lureau <marcandre.lureau@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d02301e3f232..944e6e8bd572 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -273,6 +273,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 struct hugetlbfs_inode_info {
 	struct shared_policy policy;
 	struct inode vfs_inode;
+	unsigned int seals;
 };
 
 static inline struct hugetlbfs_inode_info *HUGETLBFS_I(struct inode *inode)

commit da14c1e524a56d62b846f73ae44fd722d63747b6
Author: Marc-André Lureau <marcandre.lureau@redhat.com>
Date:   Wed Jan 31 16:19:22 2018 -0800

    hugetlb: expose hugetlbfs_inode_info in header
    
    hugetlbfs inode information will need to be accessed by code in
    mm/shmem.c for file sealing operations.  Move inode information
    definition from .c file to header for needed access.
    
    Link: http://lkml.kernel.org/r/20171107122800.25517-4-marcandre.lureau@redhat.com
    Signed-off-by: Marc-André Lureau <marcandre.lureau@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6fcf140188d0..d02301e3f232 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -270,6 +270,16 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 	return sb->s_fs_info;
 }
 
+struct hugetlbfs_inode_info {
+	struct shared_policy policy;
+	struct inode vfs_inode;
+};
+
+static inline struct hugetlbfs_inode_info *HUGETLBFS_I(struct inode *inode)
+{
+	return container_of(inode, struct hugetlbfs_inode_info, vfs_inode);
+}
+
 extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,

commit d6cb41cc44c63492702281b1d329955ca767d399
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jan 31 16:17:10 2018 -0800

    mm, hugetlb: remove hugepages_treat_as_movable sysctl
    
    hugepages_treat_as_movable has been introduced by 396faf0303d2 ("Allow
    huge page allocations to use GFP_HIGH_MOVABLE") to allow hugetlb
    allocations from ZONE_MOVABLE even when hugetlb pages were not
    migrateable.  The purpose of the movable zone was different at the time.
    It aimed at reducing memory fragmentation and hugetlb pages being long
    lived and large werre not contributing to the fragmentation so it was
    acceptable to use the zone back then.
    
    Things have changed though and the primary purpose of the zone became
    migratability guarantee.  If we allow non migrateable hugetlb pages to
    be in ZONE_MOVABLE memory hotplug might fail to offline the memory.
    
    Remove the knob and only rely on hugepage_migration_supported to allow
    movable zones.
    
    Mel said:
    
    : Primarily it was aimed at allowing the hugetlb pool to safely shrink with
    : the ability to grow it again.  The use case was for batched jobs, some of
    : which needed huge pages and others that did not but didn't want the memory
    : useless pinned in the huge pages pool.
    :
    : I suspect that more users rely on THP than hugetlbfs for flexible use of
    : huge pages with fallback options so I think that removing the option
    : should be ok.
    
    Link: http://lkml.kernel.org/r/20171003072619.8654-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Alexandru Moise <00moses.alexander00@gmail.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Alexandru Moise <00moses.alexander00@gmail.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 82a25880714a..6fcf140188d0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -129,7 +129,6 @@ u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
 
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 
-extern int hugepages_treat_as_movable;
 extern int sysctl_hugetlb_shm_group;
 extern struct list_head huge_boot_pages;
 

commit 1501899a898dfb5477c55534bdfd734c046da06d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Nov 29 16:10:06 2017 -0800

    mm: fix device-dax pud write-faults triggered by get_user_pages()
    
    Currently only get_user_pages_fast() can safely handle the writable gup
    case due to its use of pud_access_permitted() to check whether the pud
    entry is writable.  In the gup slow path pud_write() is used instead of
    pud_access_permitted() and to date it has been unimplemented, just calls
    BUG_ON().
    
        kernel BUG at ./include/linux/hugetlb.h:244!
        [..]
        RIP: 0010:follow_devmap_pud+0x482/0x490
        [..]
        Call Trace:
         follow_page_mask+0x28c/0x6e0
         __get_user_pages+0xe4/0x6c0
         get_user_pages_unlocked+0x130/0x1b0
         get_user_pages_fast+0x89/0xb0
         iov_iter_get_pages_alloc+0x114/0x4a0
         nfs_direct_read_schedule_iovec+0xd2/0x350
         ? nfs_start_io_direct+0x63/0x70
         nfs_file_direct_read+0x1e0/0x250
         nfs_file_read+0x90/0xc0
    
    For now this just implements a simple check for the _PAGE_RW bit similar
    to pmd_write.  However, this implies that the gup-slow-path check is
    missing the extra checks that the gup-fast-path performs with
    pud_access_permitted.  Later patches will align all checks to use the
    'access_permitted' helper if the architecture provides it.
    
    Note that the generic 'access_permitted' helper fallback is the simple
    _PAGE_RW check on architectures that do not define the
    'access_permitted' helper(s).
    
    [dan.j.williams@intel.com: fix powerpc compile error]
      Link: http://lkml.kernel.org/r/151129126165.37405.16031785266675461397.stgit@dwillia2-desk3.amr.corp.intel.com
    Link: http://lkml.kernel.org/r/151043109938.2842.14834662818213616199.stgit@dwillia2-desk3.amr.corp.intel.com
    Fixes: a00cc7d9dd93 ("mm, x86: add support for PUD-sized transparent hugepages")
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>  [x86]
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index fbf5b31d47ee..82a25880714a 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -239,14 +239,6 @@ static inline int pgd_write(pgd_t pgd)
 }
 #endif
 
-#ifndef pud_write
-static inline int pud_write(pud_t pud)
-{
-	BUG();
-	return 0;
-}
-#endif
-
 #define HUGETLB_ANON_FILE "anon_hugepage"
 
 enum {

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 8bbbd37ab105..fbf5b31d47ee 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_HUGETLB_H
 #define _LINUX_HUGETLB_H
 

commit e24a1307ba1f99fc62a0bd61d5e87fcfb6d5503d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Jul 28 10:31:25 2017 +0530

    mm/hugetlb: Allow arch to override and call the weak function
    
    When running in guest mode ppc64 supports a different mechanism for hugetlb
    allocation/reservation. The LPAR management application called HMC can
    be used to reserve a set of hugepages and we pass the details of
    reserved pages via device tree to the guest. (more details in
    htab_dt_scan_hugepage_blocks()) . We do the memblock_reserve of the range
    and later in the boot sequence, we add the reserved range to huge_boot_pages.
    
    But to enable 16G hugetlb on baremetal config (when we are not running as guest)
    we want to do memblock reservation during boot. Generic code already does this
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0ed8e41aaf11..8bbbd37ab105 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -358,6 +358,7 @@ int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
 /* arch callback */
+int __init __alloc_bootmem_huge_page(struct hstate *h);
 int __init alloc_bootmem_huge_page(struct hstate *h);
 
 void __init hugetlb_bad_size(void);

commit 78dcf73421a879d22319d3889119945b85954a68
Merge: 93ff81859733 fdb254db21bb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 15 12:00:42 2017 -0700

    Merge branch 'work.mount' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull ->s_options removal from Al Viro:
     "Preparations for fsmount/fsopen stuff (coming next cycle). Everything
      gets moved to explicit ->show_options(), killing ->s_options off +
      some cosmetic bits around fs/namespace.c and friends. Basically, the
      stuff needed to work with fsmount series with minimum of conflicts
      with other work.
    
      It's not strictly required for this merge window, but it would reduce
      the PITA during the coming cycle, so it would be nice to have those
      bits and pieces out of the way"
    
    * 'work.mount' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      isofs: Fix isofs_show_options()
      VFS: Kill off s_options and helpers
      orangefs: Implement show_options
      9p: Implement show_options
      isofs: Implement show_options
      afs: Implement show_options
      affs: Implement show_options
      befs: Implement show_options
      spufs: Implement show_options
      bpf: Implement show_options
      ramfs: Implement show_options
      pstore: Implement show_options
      omfs: Implement show_options
      hugetlbfs: Implement show_options
      VFS: Don't use save/replace_mount_options if not using generic_show_options
      VFS: Provide empty name qstr
      VFS: Make get_filesystem() return the affected filesystem
      VFS: Clean up whitespace in fs/namespace.c and fs/super.c
      Provide a function to create a NUL-terminated string from unterminated data

commit 3e59fcb0e8c1c40aecb60fa4c2d1543d6a097184
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:49:11 2017 -0700

    hugetlb: add support for preferred node to alloc_huge_page_nodemask
    
    alloc_huge_page_nodemask tries to allocate from any numa node in the
    allowed node mask starting from lower numa nodes.  This might lead to
    filling up those low NUMA nodes while others are not used.  We can
    reduce this risk by introducing a concept of the preferred node similar
    to what we have in the regular page allocator.  We will start allocating
    from the preferred nid and then iterate over all allowed nodes in the
    zonelist order until we try them all.
    
    This is mimicing the page allocator logic except it operates on per-node
    mempools.  dequeue_huge_page_vma already does this so distill the
    zonelist logic into a more generic dequeue_huge_page_nodemask and use it
    in alloc_huge_page_nodemask.
    
    This will allow us to use proper per numa distance fallback also for
    alloc_huge_page_node which can use alloc_huge_page_nodemask now and we
    can get rid of alloc_huge_page_node helper which doesn't have any user
    anymore.
    
    Link: http://lkml.kernel.org/r/20170622193034.28972-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 66b621469f52..8d9fe131a240 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -349,7 +349,8 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
-struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);
+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
+				nodemask_t *nmask);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
@@ -525,7 +526,7 @@ static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
-#define alloc_huge_page_nodemask(h, nmask) NULL
+#define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL

commit aaf14e40a33a2c9350471387031ca40c00f5a006
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:49:08 2017 -0700

    mm, hugetlb: unclutter hugetlb allocation layers
    
    Patch series "mm, hugetlb: allow proper node fallback dequeue".
    
    While working on a hugetlb migration issue addressed in a separate
    patchset[1] I have noticed that the hugetlb allocations from the
    preallocated pool are quite subotimal.
    
     [1] //lkml.kernel.org/r/20170608074553.22152-1-mhocko@kernel.org
    
    There is no fallback mechanism implemented and no notion of preferred
    node.  I have tried to work around it but Vlastimil was right to push
    back for a more robust solution.  It seems that such a solution is to
    reuse zonelist approach we use for the page alloctor.
    
    This series has 3 patches.  The first one tries to make hugetlb
    allocation layers more clear.  The second one implements the zonelist
    hugetlb pool allocation and introduces a preferred node semantic which
    is used by the migration callbacks.  The last patch is a clean up.
    
    This patch (of 3):
    
    Hugetlb allocation path for fresh huge pages is unnecessarily complex
    and it mixes different interfaces between layers.
    
    __alloc_buddy_huge_page is the central place to perform a new
    allocation.  It checks for the hugetlb overcommit and then relies on
    __hugetlb_alloc_buddy_huge_page to invoke the page allocator.  This is
    all good except that __alloc_buddy_huge_page pushes vma and address down
    the callchain and so __hugetlb_alloc_buddy_huge_page has to deal with
    two different allocation modes - one for memory policy and other node
    specific (or to make it more obscure node non-specific) requests.
    
    This just screams for a reorganization.
    
    This patch pulls out all the vma specific handling up to
    __alloc_buddy_huge_page_with_mpol where it belongs.
    __alloc_buddy_huge_page will get nodemask argument and
    __hugetlb_alloc_buddy_huge_page will become a trivial wrapper over the
    page allocator.
    
    In short:
    __alloc_buddy_huge_page_with_mpol - memory policy handling
      __alloc_buddy_huge_page - overcommit handling and accounting
        __hugetlb_alloc_buddy_huge_page - page allocator layer
    
    Also note that __hugetlb_alloc_buddy_huge_page and its cpuset retry loop
    is not really needed because the page allocator already handles the
    cpusets update.
    
    Finally __hugetlb_alloc_buddy_huge_page had a special case for node
    specific allocations (when no policy is applied and there is a node
    given).  This has relied on __GFP_THISNODE to not fallback to a different
    node.  alloc_huge_page_node is the only caller which relies on this
    behavior so move the __GFP_THISNODE there.
    
    Not only does this remove quite some code it also should make those
    layers easier to follow and clear wrt responsibilities.
    
    Link: http://lkml.kernel.org/r/20170622193034.28972-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 8fd0725d3f30..66b621469f52 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -349,7 +349,7 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);
+struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 

commit 4db9b2efe94967be34e3b136a93251a3c1736dd5
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:48:44 2017 -0700

    hugetlb, memory_hotplug: prefer to use reserved pages for migration
    
    new_node_page will try to use the origin's next NUMA node as the
    migration destination for hugetlb pages.  If such a node doesn't have
    any preallocated pool it falls back to __alloc_buddy_huge_page_no_mpol
    to allocate a surplus page instead.  This is quite subotpimal for any
    configuration when hugetlb pages are no distributed to all NUMA nodes
    evenly.  Say we have a hotplugable node 4 and spare hugetlb pages are
    node 0
    
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:10000
      /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node2/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node3/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node4/hugepages/hugepages-2048kB/nr_hugepages:10000
      /sys/devices/system/node/node5/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node6/hugepages/hugepages-2048kB/nr_hugepages:0
      /sys/devices/system/node/node7/hugepages/hugepages-2048kB/nr_hugepages:0
    
    Now we consume the whole pool on node 4 and try to offline this node.
    All the allocated pages should be moved to node0 which has enough
    preallocated pages to hold them.  With the current implementation
    offlining very likely fails because hugetlb allocations during runtime
    are much less reliable.
    
    Fix this by reusing the nodemask which excludes migration source and try
    to find a first node which has a page in the preallocated pool first and
    fall back to __alloc_buddy_huge_page_no_mpol only when the whole pool is
    consumed.
    
    [akpm@linux-foundation.org: remove bogus arg from alloc_huge_page_nodemask() stub]
    Link: http://lkml.kernel.org/r/20170608074553.22152-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: zhong jiang <zhongjiang@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 57f700ac127e..8fd0725d3f30 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -349,6 +349,7 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
+struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
@@ -524,6 +525,7 @@ static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
+#define alloc_huge_page_nodemask(h, nmask) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL

commit ddd40d8a2c4ef8f2152ea6d227e11475cf7e5bfa
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Mon Jul 10 15:47:53 2017 -0700

    mm: hugetlb: delete dequeue_hwpoisoned_huge_page()
    
    dequeue_hwpoisoned_huge_page() is no longer used, so let's remove it.
    
    Link: http://lkml.kernel.org/r/1496305019-5493-9-git-send-email-n-horiguchi@ah.jp.nec.com
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 668ab1742ef6..57f700ac127e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -116,7 +116,6 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						vm_flags_t vm_flags);
 long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
 						long freed);
-int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 void free_huge_page(struct page *page);
@@ -192,10 +191,6 @@ static inline void hugetlb_show_meminfo(void)
 #define hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma, dst_addr, \
 				src_addr, pagep)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address, sz)	0
-static inline int dequeue_hwpoisoned_huge_page(struct page *page)
-{
-	return 0;
-}
 
 static inline bool isolate_huge_page(struct page *page, struct list_head *list)
 {

commit c3114a84f7f96c9d5c73c8bfa7e21ff42fda97e2
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Mon Jul 10 15:47:41 2017 -0700

    mm: hugetlb: soft-offline: dissolve source hugepage after successful migration
    
    Currently hugepage migrated by soft-offline (i.e.  due to correctable
    memory errors) is contained as a hugepage, which means many non-error
    pages in it are unreusable, i.e.  wasted.
    
    This patch solves this issue by dissolving source hugepages into buddy.
    As done in previous patch, PageHWPoison is set only on a head page of
    the error hugepage.  Then in dissoliving we move the PageHWPoison flag
    to the raw error page so that all healthy subpages return back to buddy.
    
    [arnd@arndb.de: fix warnings: replace some macros with inline functions]
      Link: http://lkml.kernel.org/r/20170609102544.2947326-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/1496305019-5493-5-git-send-email-n-horiguchi@ah.jp.nec.com
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 46bfb702e7d6..668ab1742ef6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -472,6 +472,7 @@ static inline pgoff_t basepage_index(struct page *page)
 	return __basepage_index(page);
 }
 
+extern int dissolve_free_huge_page(struct page *page);
 extern int dissolve_free_huge_pages(unsigned long start_pfn,
 				    unsigned long end_pfn);
 static inline bool hugepage_migration_supported(struct hstate *h)
@@ -550,15 +551,37 @@ static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1;
 }
-#define hstate_index_to_shift(index) 0
-#define hstate_index(h) 0
+
+static inline unsigned hstate_index_to_shift(unsigned index)
+{
+	return 0;
+}
+
+static inline int hstate_index(struct hstate *h)
+{
+	return 0;
+}
 
 static inline pgoff_t basepage_index(struct page *page)
 {
 	return page->index;
 }
-#define dissolve_free_huge_pages(s, e)	0
-#define hugepage_migration_supported(h)	false
+
+static inline int dissolve_free_huge_page(struct page *page)
+{
+	return 0;
+}
+
+static inline int dissolve_free_huge_pages(unsigned long start_pfn,
+					   unsigned long end_pfn)
+{
+	return 0;
+}
+
+static inline bool hugepage_migration_supported(struct hstate *h)
+{
+	return false;
+}
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 					   struct mm_struct *mm, pte_t *pte)

commit e5251fd43007f9e1155331f0fa30685604a8e3a1
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Thu Jul 6 15:39:50 2017 -0700

    mm/hugetlb: introduce set_huge_swap_pte_at() helper
    
    set_huge_pte_at(), an architecture callback to populate hugepage ptes,
    does not provide the range of virtual memory that is targeted.  This
    leads to ambiguity when dealing with swap entries on architectures that
    support hugepages consisting of contiguous ptes.
    
    Fix the problem by introducing an overridable helper that is called when
    populating the page tables with swap entries.  The size of the targeted
    region is provided to the helper to help determine the number of entries
    to be updated.
    
    Provide a default implementation that maintains the current behaviour.
    
    [punit.agrawal@arm.com: v4]
      Link: http://lkml.kernel.org/r/20170524115409.31309-8-punit.agrawal@arm.com
    [punit.agrawal@arm.com: add an empty definition for set_huge_swap_pte_at()]
      Link: http://lkml.kernel.org/r/20170525171331.31469-1-punit.agrawal@arm.com
    Link: http://lkml.kernel.org/r/20170522133604.11392-6-punit.agrawal@arm.com
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 31e665fbcf76..46bfb702e7d6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -516,6 +516,14 @@ static inline void hugetlb_count_sub(long l, struct mm_struct *mm)
 {
 	atomic_long_sub(l, &mm->hugetlb_usage);
 }
+
+#ifndef set_huge_swap_pte_at
+static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr,
+					pte_t *ptep, pte_t pte, unsigned long sz)
+{
+	set_huge_pte_at(mm, addr, ptep, pte);
+}
+#endif
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
@@ -565,6 +573,11 @@ static inline void hugetlb_report_usage(struct seq_file *f, struct mm_struct *m)
 static inline void hugetlb_count_sub(long l, struct mm_struct *mm)
 {
 }
+
+static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr,
+					pte_t *ptep, pte_t pte, unsigned long sz)
+{
+}
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 static inline spinlock_t *huge_pte_lock(struct hstate *h,

commit 7868a2087ec13ec4a5df0c5e00999863be132ba8
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Thu Jul 6 15:39:42 2017 -0700

    mm/hugetlb: add size parameter to huge_pte_offset()
    
    A poisoned or migrated hugepage is stored as a swap entry in the page
    tables.  On architectures that support hugepages consisting of
    contiguous page table entries (such as on arm64) this leads to ambiguity
    in determining the page table entry to return in huge_pte_offset() when
    a poisoned entry is encountered.
    
    Let's remove the ambiguity by adding a size parameter to convey
    additional information about the requested address.  Also fixup the
    definition/usage of huge_pte_offset() throughout the tree.
    
    Link: http://lkml.kernel.org/r/20170522133604.11392-4-punit.agrawal@arm.com
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: James Hogan <james.hogan@imgtec.com> (odd fixer:METAG ARCHITECTURE)
    Cc: Ralf Baechle <ralf@linux-mips.org> (supporter:MIPS)
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index c92a1f0c7240..31e665fbcf76 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -137,7 +137,8 @@ extern struct list_head huge_boot_pages;
 
 pte_t *huge_pte_alloc(struct mm_struct *mm,
 			unsigned long addr, unsigned long sz);
-pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);
+pte_t *huge_pte_offset(struct mm_struct *mm,
+		       unsigned long addr, unsigned long sz);
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
@@ -190,7 +191,7 @@ static inline void hugetlb_show_meminfo(void)
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma, dst_addr, \
 				src_addr, pagep)	({ BUG(); 0; })
-#define huge_pte_offset(mm, address)	0
+#define huge_pte_offset(mm, address, sz)	0
 static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 {
 	return 0;

commit 4dc71451a2078efcad2f66bd6ef130d2296827b1
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:56 2017 -0700

    mm/follow_page_mask: add support for hugepage directory entry
    
    Architectures like ppc64 supports hugepage size that is not mapped to
    any of of the page table levels.  Instead they add an alternate page
    table entry format called hugepage directory (hugepd).  hugepd indicates
    that the page table entry maps to a set of hugetlb pages.  Add support
    for this in generic follow_page_mask code.  We already support this
    format in the generic gup code.
    
    The default implementation prints warning and returns NULL.  We will add
    ppc64 support in later patches
    
    Link: http://lkml.kernel.org/r/1494926612-23928-7-git-send-email-aneesh.kumar@linux.vnet.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f01427c79947..c92a1f0c7240 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -141,6 +141,9 @@ pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
+struct page *follow_huge_pd(struct vm_area_struct *vma,
+			    unsigned long address, hugepd_t hpd,
+			    int flags, int pdshift);
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int flags);
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
@@ -175,6 +178,7 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 static inline void hugetlb_show_meminfo(void)
 {
 }
+#define follow_huge_pd(vma, addr, hpd, flags, pdshift) NULL
 #define follow_huge_pmd(mm, addr, pmd, flags)	NULL
 #define follow_huge_pud(mm, addr, pud, flags)	NULL
 #define follow_huge_pgd(mm, addr, pgd, flags)	NULL

commit e22992923f741c951b830121655b58342fce202e
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:53 2017 -0700

    mm/hugetlb: move default definition of hugepd_t earlier in the header
    
    This enable to use the hugepd_t type early.  No functional change in
    this patch.
    
    Link: http://lkml.kernel.org/r/1494926612-23928-6-git-send-email-aneesh.kumar@linux.vnet.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 3656ce605dc9..f01427c79947 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -14,6 +14,30 @@ struct ctl_table;
 struct user_struct;
 struct mmu_gather;
 
+#ifndef is_hugepd
+/*
+ * Some architectures requires a hugepage directory format that is
+ * required to support multiple hugepage sizes. For example
+ * a4fe3ce76 "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
+ * introduced the same on powerpc. This allows for a more flexible hugepage
+ * pagetable layout.
+ */
+typedef struct { unsigned long pd; } hugepd_t;
+#define is_hugepd(hugepd) (0)
+#define __hugepd(x) ((hugepd_t) { (x) })
+static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+			      unsigned pdshift, unsigned long end,
+			      int write, struct page **pages, int *nr)
+{
+	return 0;
+}
+#else
+extern int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+		       unsigned pdshift, unsigned long end,
+		       int write, struct page **pages, int *nr);
+#endif
+
+
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>
@@ -222,29 +246,6 @@ static inline int pud_write(pud_t pud)
 }
 #endif
 
-#ifndef is_hugepd
-/*
- * Some architectures requires a hugepage directory format that is
- * required to support multiple hugepage sizes. For example
- * a4fe3ce76 "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
- * introduced the same on powerpc. This allows for a more flexible hugepage
- * pagetable layout.
- */
-typedef struct { unsigned long pd; } hugepd_t;
-#define is_hugepd(hugepd) (0)
-#define __hugepd(x) ((hugepd_t) { (x) })
-static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-			      unsigned pdshift, unsigned long end,
-			      int write, struct page **pages, int *nr)
-{
-	return 0;
-}
-#else
-extern int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-		       unsigned pdshift, unsigned long end,
-		       int write, struct page **pages, int *nr);
-#endif
-
 #define HUGETLB_ANON_FILE "anon_hugepage"
 
 enum {

commit faaa5b62d3f7907e217b179556038f9f8e157ee0
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:50 2017 -0700

    mm/follow_page_mask: add support for hugetlb pgd entries
    
    ppc64 supports pgd hugetlb entries.  Add code to handle hugetlb pgd
    entries to follow_page_mask so that ppc64 can switch to it to handle
    hugetlbe entries.
    
    Link: http://lkml.kernel.org/r/1494926612-23928-5-git-send-email-aneesh.kumar@linux.vnet.ibm.com
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index aa1df49b9a14..3656ce605dc9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -121,6 +121,9 @@ struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int flags);
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int flags);
+struct page *follow_huge_pgd(struct mm_struct *mm, unsigned long address,
+			     pgd_t *pgd, int flags);
+
 int pmd_huge(pmd_t pmd);
 int pud_huge(pud_t pud);
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
@@ -150,6 +153,7 @@ static inline void hugetlb_show_meminfo(void)
 }
 #define follow_huge_pmd(mm, addr, pmd, flags)	NULL
 #define follow_huge_pud(mm, addr, pud, flags)	NULL
+#define follow_huge_pgd(mm, addr, pgd, flags)	NULL
 #define prepare_hugepage_range(file, addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define pud_huge(x)	0

commit d5ed7444dafb94b6877410d1b66a846eb7184a09
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:47 2017 -0700

    mm/hugetlb: export hugetlb_entry_migration helper
    
    We will be using this later from the ppc64 code.  Change the return type
    to bool.
    
    Link: http://lkml.kernel.org/r/1494926612-23928-4-git-send-email-aneesh.kumar@linux.vnet.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5f539f985e2a..aa1df49b9a14 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -126,6 +126,7 @@ int pud_huge(pud_t pud);
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
+bool is_hugetlb_entry_migration(pte_t pte);
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)

commit 94310cbcaa3c2bc1b790ba997270f28dc173d8ce
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:38 2017 -0700

    mm/madvise: enable (soft|hard) offline of HugeTLB pages at PGD level
    
    Though migrating gigantic HugeTLB pages does not sound much like real
    world use case, they can be affected by memory errors.  Hence migration
    at the PGD level HugeTLB pages should be supported just to enable soft
    and hard offline use cases.
    
    While allocating the new gigantic HugeTLB page, it should not matter
    whether new page comes from the same node or not.  There would be very
    few gigantic pages on the system afterall, we should not be bothered
    about node locality when trying to save a big page from crashing.
    
    This change renames dequeu_huge_page_node() function as dequeue_huge
    _page_node_exact() preserving it's original functionality.  Now the new
    dequeue_huge_page_node() function scans through all available online nodes
    to allocate a huge page for the NUMA_NO_NODE case and just falls back
    calling dequeu_huge_page_node_exact() for all other cases.
    
    [arnd@arndb.de: make hstate_is_gigantic() inline]
      Link: http://lkml.kernel.org/r/20170522124748.3911296-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/20170516100509.20122-1-khandual@linux.vnet.ibm.com
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b857fc8cc2ec..5f539f985e2a 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -466,7 +466,11 @@ extern int dissolve_free_huge_pages(unsigned long start_pfn,
 static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
-	return huge_page_shift(h) == PMD_SHIFT;
+	if ((huge_page_shift(h) == PMD_SHIFT) ||
+		(huge_page_shift(h) == PGDIR_SHIFT))
+		return true;
+	else
+		return false;
 #else
 	return false;
 #endif
@@ -518,6 +522,11 @@ struct hstate {};
 #define vma_mmu_pagesize(v) PAGE_SIZE
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
+static inline bool hstate_is_gigantic(struct hstate *h)
+{
+	return false;
+}
+
 static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1;

commit 4a25220d4e43bb2461823dbc7eb1502a34087958
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jul 5 16:24:18 2017 +0100

    hugetlbfs: Implement show_options
    
    Implement the show_options superblock op for hugetlbfs as part of a bid to
    get rid of s_options and generic_show_options() to make it easier to
    implement a context-based mount where the mount options can be passed
    individually over a file descriptor.
    
    Note that the uid and gid should possibly be displayed relative to the
    viewer's user namespace.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Nadia Yvette Chambers <nyc@holomorphy.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b857fc8cc2ec..3b6eeaad2f77 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -262,6 +262,9 @@ struct hugetlbfs_sb_info {
 	spinlock_t	stat_lock;
 	struct hstate *hstate;
 	struct hugepage_subpool *spool;
+	kuid_t	uid;
+	kgid_t	gid;
+	umode_t mode;
 };
 
 static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)

commit c2febafc67734a62196c1b9dfba926412d4077ba
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 9 17:24:07 2017 +0300

    mm: convert generic code to 5-level paging
    
    Convert all non-architecture-specific code to 5-level paging.
    
    It's mostly mechanical adding handling one more page table level in
    places where we deal with pud_t.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 503099d8aada..b857fc8cc2ec 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -122,7 +122,7 @@ struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int flags);
 int pmd_huge(pmd_t pmd);
-int pud_huge(pud_t pmd);
+int pud_huge(pud_t pud);
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
@@ -197,6 +197,9 @@ static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
 #ifndef pgd_huge
 #define pgd_huge(x)	0
 #endif
+#ifndef p4d_huge
+#define p4d_huge(x)	0
+#endif
 
 #ifndef pgd_write
 static inline int pgd_write(pgd_t pgd)

commit 87ffc118b54dcd4cc642723603d944673248152f
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Feb 22 15:43:13 2017 -0800

    userfaultfd: hugetlbfs: gup: support VM_FAULT_RETRY
    
    Add support for VM_FAULT_RETRY to follow_hugetlb_page() so that
    get_user_pages_unlocked/locked and "nonblocking/FOLL_NOWAIT" features
    will work on hugetlbfs.
    
    This is required for fully functional userfaultfd non-present support on
    hugetlbfs.
    
    Link: http://lkml.kernel.org/r/20161216144821.5183-25-aarcange@redhat.com
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michael Rapoport <RAPOPORT@il.ibm.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index aab2fff3e269..503099d8aada 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -65,7 +65,8 @@ int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 struct page **, struct vm_area_struct **,
-			 unsigned long *, unsigned long *, long, unsigned int);
+			 unsigned long *, unsigned long *, long, unsigned int,
+			 int *);
 void unmap_hugepage_range(struct vm_area_struct *,
 			  unsigned long, unsigned long, struct page *);
 void __unmap_hugepage_range_final(struct mmu_gather *tlb,
@@ -136,7 +137,7 @@ static inline unsigned long hugetlb_total_pages(void)
 	return 0;
 }
 
-#define follow_hugetlb_page(m,v,p,vs,a,b,i,w)	({ BUG(); 0; })
+#define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n)	({ BUG(); 0; })
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 static inline void hugetlb_report_meminfo(struct seq_file *m)

commit 8fb5debc5fcd450470cdd789c2d80ef95ebb8cf4
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Feb 22 15:42:52 2017 -0800

    userfaultfd: hugetlbfs: add hugetlb_mcopy_atomic_pte for userfaultfd support
    
    hugetlb_mcopy_atomic_pte is the low level routine that implements the
    userfaultfd UFFDIO_COPY command.  It is based on the existing
    mcopy_atomic_pte routine with modifications for huge pages.
    
    Link: http://lkml.kernel.org/r/20161216144821.5183-18-aarcange@redhat.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michael Rapoport <RAPOPORT@il.ibm.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 48c76d612d40..aab2fff3e269 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -81,6 +81,11 @@ void hugetlb_show_meminfo(void);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags);
+int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm, pte_t *dst_pte,
+				struct vm_area_struct *dst_vma,
+				unsigned long dst_addr,
+				unsigned long src_addr,
+				struct page **pagep);
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						vm_flags_t vm_flags);
@@ -149,6 +154,8 @@ static inline void hugetlb_show_meminfo(void)
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
+#define hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma, dst_addr, \
+				src_addr, pagep)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address)	0
 static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 {

commit 72e2936c04f7d2a4bf87d7f72d3bf11cf91ebb47
Author: zhong jiang <zhongjiang@huawei.com>
Date:   Fri Oct 7 17:02:01 2016 -0700

    mm: remove unnecessary condition in remove_inode_hugepages
    
    When the huge page is added to the page cahce (huge_add_to_page_cache),
    the page private flag will be cleared.  since this code
    (remove_inode_hugepages) will only be called for pages in the page
    cahce, PagePrivate(page) will always be false.
    
    The patch remove the code without any functional change.
    
    Link: http://lkml.kernel.org/r/1475113323-29368-1-git-send-email-zhongjiang@huawei.com
    Signed-off-by: zhong jiang <zhongjiang@huawei.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Tested-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index fe99e6f956e2..48c76d612d40 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -90,7 +90,7 @@ int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 void free_huge_page(struct page *page);
-void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve);
+void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
 u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
 				struct vm_area_struct *vma,

commit 082d5b6b60e9f25e1511557fcfcb21eedd267446
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Fri Oct 7 17:01:10 2016 -0700

    mm/hugetlb: check for reserved hugepages during memory offline
    
    In dissolve_free_huge_pages(), free hugepages will be dissolved without
    making sure that there are enough of them left to satisfy hugepage
    reservations.
    
    Fix this by adding a return value to dissolve_free_huge_pages() and
    checking h->free_huge_pages vs.  h->resv_huge_pages.  Note that this may
    lead to the situation where dissolve_free_huge_page() returns an error
    and all free hugepages that were dissolved before that error are lost,
    while the memory block still cannot be set offline.
    
    Fixes: c8721bbb ("mm: memory-hotplug: enable memory hotplug to handle hugepage")
    Link: http://lkml.kernel.org/r/20160926172811.94033-3-gerald.schaefer@de.ibm.com
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rui Teng <rui.teng@linux.vnet.ibm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index c26d4638f665..fe99e6f956e2 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -450,8 +450,8 @@ static inline pgoff_t basepage_index(struct page *page)
 	return __basepage_index(page);
 }
 
-extern void dissolve_free_huge_pages(unsigned long start_pfn,
-				     unsigned long end_pfn);
+extern int dissolve_free_huge_pages(unsigned long start_pfn,
+				    unsigned long end_pfn);
 static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
@@ -518,7 +518,7 @@ static inline pgoff_t basepage_index(struct page *page)
 {
 	return page->index;
 }
-#define dissolve_free_huge_pages(s, e)	do {} while (0)
+#define dissolve_free_huge_pages(s, e)	0
 #define hugepage_migration_supported(h)	false
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,

commit d70c17d436b3fb9dbdae8c93bf908a6110b0cb4f
Author: Chen Gang <chengang@emindsoft.com.cn>
Date:   Fri May 20 16:58:01 2016 -0700

    include/linux/hugetlb.h: use bool instead of int for hugepage_migration_supported()
    
    It is used as a pure bool function within kernel source wide.
    
    Signed-off-by: Chen Gang <gang.chen.5i5j@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7ef4b635015d..c26d4638f665 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -452,12 +452,12 @@ static inline pgoff_t basepage_index(struct page *page)
 
 extern void dissolve_free_huge_pages(unsigned long start_pfn,
 				     unsigned long end_pfn);
-static inline int hugepage_migration_supported(struct hstate *h)
+static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
 	return huge_page_shift(h) == PMD_SHIFT;
 #else
-	return 0;
+	return false;
 #endif
 }
 
@@ -519,7 +519,7 @@ static inline pgoff_t basepage_index(struct page *page)
 	return page->index;
 }
 #define dissolve_free_huge_pages(s, e)	do {} while (0)
-#define hugepage_migration_supported(h)	0
+#define hugepage_migration_supported(h)	false
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 					   struct mm_struct *mm, pte_t *pte)

commit 7fab358d90e6ba9d9cb702bee0c8a5f5c13bb6df
Author: Chen Gang <gang.chen.5i5j@gmail.com>
Date:   Fri May 20 16:57:59 2016 -0700

    include/linux/hugetlb*.h: clean up code
    
    Macro HUGETLBFS_SB is clear enough, so one statement is clearer than 3
    lines statements.
    
    Remove redundant return statements for non-return functions, which can
    save lines, at least.
    
    Signed-off-by: Chen Gang <gang.chen.5i5j@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e44c57876e89..7ef4b635015d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -353,9 +353,7 @@ extern unsigned int default_hstate_idx;
 
 static inline struct hstate *hstate_inode(struct inode *i)
 {
-	struct hugetlbfs_sb_info *hsb;
-	hsb = HUGETLBFS_SB(i->i_sb);
-	return hsb->hstate;
+	return HUGETLBFS_SB(i->i_sb)->hstate;
 }
 
 static inline struct hstate *hstate_file(struct file *f)

commit 9fee021d15ddd884d40d1540913474e8112313fe
Author: Vaishali Thakkar <vaishali.thakkar@oracle.com>
Date:   Thu May 19 17:11:04 2016 -0700

    mm/hugetlb: introduce hugetlb_bad_size()
    
    When any unsupported hugepage size is specified, 'hugepagesz=' and
    'hugepages=' should be ignored during command line parsing until any
    supported hugepage size is found.  But currently incorrect number of
    hugepages are allocated when unsupported size is specified as it fails
    to ignore the 'hugepages=' command.
    
    Test case:
    
    Note that this is specific to x86 architecture.
    
    Boot the kernel with command line option 'hugepagesz=256M hugepages=X'.
    After boot, dmesg output shows that X number of hugepages of the size 2M
    is pre-allocated instead of 0.
    
    So, to handle such command line options, introduce new routine
    hugetlb_bad_size.  The routine hugetlb_bad_size sets the global variable
    parsed_valid_hugepagesz.  We are using parsed_valid_hugepagesz to save
    the state when unsupported hugepagesize is found so that we can ignore
    the 'hugepages=' parameters after that and then reset the variable when
    supported hugepage size is found.
    
    The routine hugetlb_bad_size can be called while setting 'hugepagesz='
    parameter in an architecture specific code.
    
    Signed-off-by: Vaishali Thakkar <vaishali.thakkar@oracle.com>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7d953c2542a8..e44c57876e89 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -338,6 +338,7 @@ int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 /* arch callback */
 int __init alloc_bootmem_huge_page(struct hstate *h);
 
+void __init hugetlb_bad_size(void);
 void __init hugetlb_add_hstate(unsigned order);
 struct hstate *size_to_hstate(unsigned long size);
 

commit 888cdbc2c9a76a0e450f533b1957cdbfe7d483d5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:32 2016 -0800

    hugetlb: fix compile error on tile
    
    Inlude asm/pgtable.h to get the definition for pud_t to fix:
    
      include/linux/hugetlb.h:203:29: error: unknown type name 'pud_t'
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e76574d8f9b5..7d953c2542a8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -8,6 +8,7 @@
 #include <linux/cgroup.h>
 #include <linux/list.h>
 #include <linux/kref.h>
+#include <asm/pgtable.h>
 
 struct ctl_table;
 struct user_struct;

commit 719ff32162865522273d8fa975bf99b32875738b
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu Jan 14 15:18:51 2016 -0800

    include/linux/hugetlb.h: is_file_hugepages() can be boolean
    
    Make is_file_hugepages() return bool to improve readability due to this
    particular function only using either one or zero as its return value.
    
    This patch also removed the if condition to make is_file_hugepages
    return directly.
    
    No functional change.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b0eb06423d5e..e76574d8f9b5 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -263,20 +263,18 @@ struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 				struct user_struct **user, int creat_flags,
 				int page_size_log);
 
-static inline int is_file_hugepages(struct file *file)
+static inline bool is_file_hugepages(struct file *file)
 {
 	if (file->f_op == &hugetlbfs_file_operations)
-		return 1;
-	if (is_file_shm_hugepages(file))
-		return 1;
+		return true;
 
-	return 0;
+	return is_file_shm_hugepages(file);
 }
 
 
 #else /* !CONFIG_HUGETLBFS */
 
-#define is_file_hugepages(file)			0
+#define is_file_hugepages(file)			false
 static inline struct file *
 hugetlb_file_setup(const char *name, size_t size, vm_flags_t acctflag,
 		struct user_struct **user, int creat_flags,

commit 66b3923a1a0f77a563b43f43f6ad091354abbfe9
Author: David Woods <dwoods@ezchip.com>
Date:   Thu Dec 17 14:31:26 2015 -0500

    arm64: hugetlb: add support for PTE contiguous bit
    
    The arm64 MMU supports a Contiguous bit which is a hint that the TTE
    is one of a set of contiguous entries which can be cached in a single
    TLB entry.  Supporting this bit adds new intermediate huge page sizes.
    
    The set of huge page sizes available depends on the base page size.
    Without using contiguous pages the huge page sizes are as follows.
    
     4KB:   2MB  1GB
    64KB: 512MB
    
    With a 4KB granule, the contiguous bit groups together sets of 16 pages
    and with a 64KB granule it groups sets of 32 pages.  This enables two new
    huge page sizes in each case, so that the full set of available sizes
    is as follows.
    
     4KB:  64KB   2MB  32MB  1GB
    64KB:   2MB 512MB  16GB
    
    If a 16KB granule is used then the contiguous bit groups 128 pages
    at the PTE level and 32 pages at the PMD level.
    
    If the base page size is set to 64KB then 2MB pages are enabled by
    default.  It is possible in the future to make 2MB the default huge
    page size for both 4KB and 64KB granules.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Reviewed-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: David Woods <dwoods@ezchip.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 685c262e0be8..b0eb06423d5e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -96,9 +96,7 @@ u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
 				struct address_space *mapping,
 				pgoff_t idx, unsigned long address);
 
-#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
-#endif
 
 extern int hugepages_treat_as_movable;
 extern int sysctl_hugetlb_shm_group;

commit 5d317b2b6536592a9b51fe65faed43d65ca9158e
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Nov 5 18:47:14 2015 -0800

    mm: hugetlb: proc: add HugetlbPages field to /proc/PID/status
    
    Currently there's no easy way to get per-process usage of hugetlb pages,
    which is inconvenient because userspace applications which use hugetlb
    typically want to control their processes on the basis of how much memory
    (including hugetlb) they use.  So this patch simply provides easy access
    to the info via /proc/PID/status.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Joern Engel <joern@logfs.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5e35379f58a5..685c262e0be8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -483,6 +483,17 @@ static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 #define hugepages_supported() (HPAGE_SHIFT != 0)
 #endif
 
+void hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm);
+
+static inline void hugetlb_count_add(long l, struct mm_struct *mm)
+{
+	atomic_long_add(l, &mm->hugetlb_usage);
+}
+
+static inline void hugetlb_count_sub(long l, struct mm_struct *mm)
+{
+	atomic_long_sub(l, &mm->hugetlb_usage);
+}
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
@@ -519,6 +530,14 @@ static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 {
 	return &mm->page_table_lock;
 }
+
+static inline void hugetlb_report_usage(struct seq_file *f, struct mm_struct *m)
+{
+}
+
+static inline void hugetlb_count_sub(long l, struct mm_struct *mm)
+{
+}
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 static inline spinlock_t *huge_pte_lock(struct hstate *h,

commit 70c3547e36f5c9fbc4caecfeca98f0effa6932c5
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Tue Sep 8 15:01:54 2015 -0700

    hugetlbfs: add hugetlbfs_fallocate()
    
    This is based on the shmem version, but it has diverged quite a bit.  We
    have no swap to worry about, nor the new file sealing.  Add
    synchronication via the fault mutex table to coordinate page faults,
    fallocate allocation and fallocate hole punch.
    
    What this allows us to do is move physical memory in and out of a
    hugetlbfs file without having it mapped.  This also gives us the ability
    to support MADV_REMOVE since it is currently implemented using
    fallocate().  MADV_REMOVE lets madvise() remove pages from the middle of
    a hugetlbfs file, which wasn't possible before.
    
    hugetlbfs fallocate only operates on whole huge pages.
    
    Based on code by Dave Hansen.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1222fb07a746..5e35379f58a5 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -330,6 +330,8 @@ struct huge_bootmem_page {
 #endif
 };
 
+struct page *alloc_huge_page(struct vm_area_struct *vma,
+				unsigned long addr, int avoid_reserve);
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
@@ -483,6 +485,7 @@ static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
+#define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL

commit ab76ad540a50191308e5bb6b5e2d9e26c78616d3
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Tue Sep 8 15:01:50 2015 -0700

    hugetlbfs: New huge_add_to_page_cache helper routine
    
    Currently, there is only a single place where hugetlbfs pages are added
    to the page cache.  The new fallocate code be adding a second one, so
    break the functionality out into its own helper.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 35afca1692fb..1222fb07a746 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -333,6 +333,8 @@ struct huge_bootmem_page {
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
+int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
+			pgoff_t idx);
 
 /* arch callback */
 int __init alloc_bootmem_huge_page(struct hstate *h);

commit b5cec28d36f5ee6b4e6f68a0a40aa1e4045d6d99
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Tue Sep 8 15:01:41 2015 -0700

    hugetlbfs: truncate_hugepages() takes a range of pages
    
    Modify truncate_hugepages() to take a range of pages (start, end)
    instead of simply start.  If an end value of LLONG_MAX is passed, the
    current "truncate" functionality is maintained.  Existing callers are
    modified to pass LLONG_MAX as end of range.  By keying off end ==
    LLONG_MAX, the routine behaves differently for truncate and hole punch.
    Page removal is now synchronized with page allocation via faults by
    using the fault mutex table.  The hole punch case can experience the
    rare region_del error and must handle accordingly.
    
    Add the routine hugetlb_fix_reserve_counts to fix up reserve counts in
    the case where region_del returns an error.
    
    Since the routine handles more than just the truncate case, it is
    renamed to remove_inode_hugepages().  To be consistent, the routine
    truncate_huge_page() is renamed remove_huge_page().
    
    Downstream of remove_inode_hugepages(), the routine
    hugetlb_unreserve_pages() is also modified to take a range of pages.
    hugetlb_unreserve_pages is modified to detect an error from region_del and
    pass it back to the caller.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 530cf6fc24c7..35afca1692fb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -83,11 +83,13 @@ int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						vm_flags_t vm_flags);
-void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
+long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
+						long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 void free_huge_page(struct page *page);
+void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve);
 extern struct mutex *hugetlb_fault_mutex_table;
 u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
 				struct vm_area_struct *vma,

commit c672c7f29f2fdb73e1f72911bf499675c81fcdbb
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Tue Sep 8 15:01:35 2015 -0700

    mm/hugetlb: expose hugetlb fault mutex for use by fallocate
    
    hugetlb page faults are currently synchronized by the table of mutexes
    (htlb_fault_mutex_table).  fallocate code will need to synchronize with
    the page fault code when it allocates or deletes pages.  Expose
    interfaces so that fallocate operations can be synchronized with page
    faults.  Minor name changes to be more consistent with other global
    hugetlb symbols.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e2d94960b38b..530cf6fc24c7 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -88,6 +88,11 @@ int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 void free_huge_page(struct page *page);
+extern struct mutex *hugetlb_fault_mutex_table;
+u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
+				struct vm_area_struct *vma,
+				struct address_space *mapping,
+				pgoff_t idx, unsigned long address);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);

commit 5e9113731a3ce616e8b5aa128ffc1aeaa4942571
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Tue Sep 8 15:01:28 2015 -0700

    mm/hugetlb: add cache of descriptors to resv_map for region_add
    
    hugetlbfs is used today by applications that want a high degree of
    control over huge page usage.  Often, large hugetlbfs files are used to
    map a large number huge pages into the application processes.  The
    applications know when page ranges within these large files will no
    longer be used, and ideally would like to release them back to the
    subpool or global pools for other uses.  The fallocate() system call
    provides an interface for preallocation and hole punching within files.
    This patch set adds fallocate functionality to hugetlbfs.
    
    fallocate hole punch will want to remove a specific range of pages.
    When pages are removed, their associated entries in the region/reserve
    map will also be removed.  This will break an assumption in the
    region_chg/region_add calling sequence.  If a new region descriptor must
    be allocated, it is done as part of the region_chg processing.  In this
    way, region_add can not fail because it does not need to attempt an
    allocation.
    
    To prepare for fallocate hole punch, create a "cache" of descriptors
    that can be used by region_add if necessary.  region_chg will ensure
    there are sufficient entries in the cache.  It will be necessary to
    track the number of in progress add operations to know a sufficient
    number of descriptors reside in the cache.  A new routine region_abort
    is added to adjust this in progress count when add operations are
    aborted.  vma_abort_reservation is also added for callers creating
    reservations with vma_needs_reservation/vma_commit_reservation.
    
    [akpm@linux-foundation.org: fix typo in comment, use more cols]
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d891f949466a..e2d94960b38b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -35,6 +35,9 @@ struct resv_map {
 	struct kref refs;
 	spinlock_t lock;
 	struct list_head regions;
+	long adds_in_progress;
+	struct list_head region_cache;
+	long region_cache_count;
 };
 extern struct resv_map *resv_map_alloc(void);
 void resv_map_release(struct kref *ref);

commit 2531c8cf56a640cd7d17057df8484e570716a450
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Jul 17 16:23:37 2015 -0700

    mm: hugetlb: allow hugepages_supported to be architecture specific
    
    s390 has a constant hugepage size, by setting HPAGE_SHIFT we also change
    e.g. the pageblock_order, which should be independent in respect to
    hugepage support.
    
    With this patch every architecture is free to define how to check
    for hugepage support.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 205026175c42..d891f949466a 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -460,15 +460,14 @@ static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 	return &mm->page_table_lock;
 }
 
-static inline bool hugepages_supported(void)
-{
-	/*
-	 * Some platform decide whether they support huge pages at boot
-	 * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when
-	 * there is no such support
-	 */
-	return HPAGE_SHIFT != 0;
-}
+#ifndef hugepages_supported
+/*
+ * Some platform decide whether they support huge pages at boot
+ * time. Some of them, such as powerpc, set HPAGE_SHIFT to 0
+ * when there is no such support
+ */
+#define hugepages_supported() (HPAGE_SHIFT != 0)
+#endif
 
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};

commit 7e1f049efb86bd86c06b80eeac0ef80cdeb8c0e7
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Apr 15 16:14:41 2015 -0700

    mm: hugetlb: cleanup using paeg_huge_active()
    
    Now we have an easy access to hugepages' activeness, so existing helpers to
    get the information can be cleaned up.
    
    [akpm@linux-foundation.org: s/PageHugeActive/page_huge_active/]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Hugh Dickins <hughd@google.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5713c49a4a5c..205026175c42 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -84,7 +84,6 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
-bool is_hugepage_active(struct page *page);
 void free_huge_page(struct page *page);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
@@ -152,7 +151,6 @@ static inline bool isolate_huge_page(struct page *page, struct list_head *list)
 	return false;
 }
 #define putback_active_hugepage(p)	do {} while (0)
-#define is_hugepage_active(x)	false
 
 static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot)

commit 7ca02d0ae586fe7df59632966a64f3f1a756ef05
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Apr 15 16:13:42 2015 -0700

    hugetlbfs: accept subpool min_size mount option and setup accordingly
    
    Make 'min_size=<value>' be an option when mounting a hugetlbfs.  This
    option takes the same value as the 'size' option.  min_size can be
    specified without specifying size.  If both are specified, min_size must
    be less that or equal to size else the mount will fail.  If min_size is
    specified, then at mount time an attempt is made to reserve min_size
    pages.  If the reservation fails, the mount fails.  At umount time, the
    reserved pages are released.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d1a77b87408d..5713c49a4a5c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -44,7 +44,8 @@ extern int hugetlb_max_hstate __read_mostly;
 #define for_each_hstate(h) \
 	for ((h) = hstates; (h) < &hstates[hugetlb_max_hstate]; (h)++)
 
-struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
+struct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,
+						long min_hpages);
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);

commit c6a918200c4f4ebf74b7e1ae4fea9115c7b297f8
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Apr 15 16:13:36 2015 -0700

    hugetlbfs: add minimum size tracking fields to subpool structure
    
    hugetlbfs allocates huge pages from the global pool as needed.  Even if
    the global pool contains a sufficient number pages for the filesystem size
    at mount time, those global pages could be grabbed for some other use.  As
    a result, filesystem huge page allocations may fail due to lack of pages.
    
    Applications such as a database want to use huge pages for performance
    reasons.  hugetlbfs filesystem semantics with ownership and modes work
    well to manage access to a pool of huge pages.  However, the application
    would like some reasonable assurance that allocations will not fail due to
    a lack of huge pages.  At application startup time, the application would
    like to configure itself to use a specific number of huge pages.  Before
    starting, the application can check to make sure that enough huge pages
    exist in the system global pools.  However, there are no guarantees that
    those pages will be available when needed by the application.  What the
    application wants is exclusive use of a subset of huge pages.
    
    Add a new hugetlbfs mount option 'min_size=<value>' to indicate that the
    specified number of pages will be available for use by the filesystem.  At
    mount time, this number of huge pages will be reserved for exclusive use
    of the filesystem.  If there is not a sufficient number of free pages, the
    mount will fail.  As pages are allocated to and freeed from the
    filesystem, the number of reserved pages is adjusted so that the specified
    minimum is maintained.
    
    This patch (of 4):
    
    Add a field to the subpool structure to indicate the minimimum number of
    huge pages to always be used by this subpool.  This minimum count includes
    allocated pages as well as reserved pages.  If the minimum number of pages
    for the subpool have not been allocated, pages are reserved up to this
    minimum.  An additional field (rsv_hpages) is used to track the number of
    pages reserved to meet this minimum size.  The hstate pointer in the
    subpool is convenient to have when reserving and unreserving the pages.
    
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1a782733a420..d1a77b87408d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -22,7 +22,13 @@ struct mmu_gather;
 struct hugepage_subpool {
 	spinlock_t lock;
 	long count;
-	long max_hpages, used_hpages;
+	long max_hpages;	/* Maximum huge pages or -1 if no maximum. */
+	long used_hpages;	/* Used count against maximum, includes */
+				/* both alloced and reserved pages. */
+	struct hstate *hstate;
+	long min_hpages;	/* Minimum huge pages or -1 if no minimum. */
+	long rsv_hpages;	/* Pages reserved against global pool to */
+				/* sasitfy minimum size. */
 };
 
 struct resv_map {

commit e8c6158fef15a1532bd5242a0cd88565eedabe61
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Apr 15 16:13:08 2015 -0700

    mm: consolidate all page-flags helpers in <linux/page-flags.h>
    
    Currently we take a naive approach to page flags on compound pages - we
    set the flag on the page without consideration if the flag makes sense
    for tail page or for compound page in general.  This patchset try to
    sort this out by defining per-flag policy on what need to be done if
    page-flag helper operate on compound page.
    
    The last patch in the patchset also sanitizes usege of page->mapping for
    tail pages.  We don't define the meaning of page->mapping for tail
    pages.  Currently it's always NULL, which can be inconsistent with head
    page and potentially lead to problems.
    
    For now I caught one case of illegal usage of page flags or ->mapping:
    sound subsystem allocates pages with __GFP_COMP and maps them with PTEs.
    It leads to setting dirty bit on tail pages and access to tail_page's
    ->mapping.  I don't see any bad behaviour caused by this, but worth
    fixing anyway.
    
    This patchset makes more sense if you take my THP refcounting into
    account: we will see more compound pages mapped with PTEs and we need to
    define behaviour of flags on compound pages to avoid bugs.
    
    This patch (of 16):
    
    We have page-flags helper function declarations/definitions spread over
    several header files.  Let's consolidate them in <linux/page-flags.h>.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7b5785032049..1a782733a420 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -41,8 +41,6 @@ extern int hugetlb_max_hstate __read_mostly;
 struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 
-int PageHuge(struct page *page);
-
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
 int hugetlb_overcommit_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
@@ -109,11 +107,6 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 
 #else /* !CONFIG_HUGETLB_PAGE */
 
-static inline int PageHuge(struct page *page)
-{
-	return 0;
-}
-
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
 }

commit e66f17ff71772b209eed39de35aaa99ba819c93d
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Feb 11 15:25:22 2015 -0800

    mm/hugetlb: take page table lock in follow_huge_pmd()
    
    We have a race condition between move_pages() and freeing hugepages, where
    move_pages() calls follow_page(FOLL_GET) for hugepages internally and
    tries to get its refcount without preventing concurrent freeing.  This
    race crashes the kernel, so this patch fixes it by moving FOLL_GET code
    for hugepages into follow_huge_pmd() with taking the page table lock.
    
    This patch intentionally removes page==NULL check after pte_page.
    This is justified because pte_page() never returns NULL for any
    architectures or configurations.
    
    This patch changes the behavior of follow_huge_pmd() for tail pages and
    then tail pages can be pinned/returned.  So the caller must be changed to
    properly handle the returned tail pages.
    
    We could have a choice to add the similar locking to
    follow_huge_(addr|pud) for consistency, but it's not necessary because
    currently these functions don't support FOLL_GET flag, so let's leave it
    for future development.
    
    Here is the reproducer:
    
      $ cat movepages.c
      #include <stdio.h>
      #include <stdlib.h>
      #include <numaif.h>
    
      #define ADDR_INPUT      0x700000000000UL
      #define HPS             0x200000
      #define PS              0x1000
    
      int main(int argc, char *argv[]) {
              int i;
              int nr_hp = strtol(argv[1], NULL, 0);
              int nr_p  = nr_hp * HPS / PS;
              int ret;
              void **addrs;
              int *status;
              int *nodes;
              pid_t pid;
    
              pid = strtol(argv[2], NULL, 0);
              addrs  = malloc(sizeof(char *) * nr_p + 1);
              status = malloc(sizeof(char *) * nr_p + 1);
              nodes  = malloc(sizeof(char *) * nr_p + 1);
    
              while (1) {
                      for (i = 0; i < nr_p; i++) {
                              addrs[i] = (void *)ADDR_INPUT + i * PS;
                              nodes[i] = 1;
                              status[i] = 0;
                      }
                      ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
                                            MPOL_MF_MOVE_ALL);
                      if (ret == -1)
                              err("move_pages");
    
                      for (i = 0; i < nr_p; i++) {
                              addrs[i] = (void *)ADDR_INPUT + i * PS;
                              nodes[i] = 0;
                              status[i] = 0;
                      }
                      ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
                                            MPOL_MF_MOVE_ALL);
                      if (ret == -1)
                              err("move_pages");
              }
              return 0;
      }
    
      $ cat hugepage.c
      #include <stdio.h>
      #include <sys/mman.h>
      #include <string.h>
    
      #define ADDR_INPUT      0x700000000000UL
      #define HPS             0x200000
    
      int main(int argc, char *argv[]) {
              int nr_hp = strtol(argv[1], NULL, 0);
              char *p;
    
              while (1) {
                      p = mmap((void *)ADDR_INPUT, nr_hp * HPS, PROT_READ | PROT_WRITE,
                               MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0);
                      if (p != (void *)ADDR_INPUT) {
                              perror("mmap");
                              break;
                      }
                      memset(p, 0, nr_hp * HPS);
                      munmap(p, nr_hp * HPS);
              }
      }
    
      $ sysctl vm.nr_hugepages=40
      $ ./hugepage 10 &
      $ ./movepages 10 $(pgrep -f hugepage)
    
    Fixes: e632a938d914 ("mm: migrate: add hugepage migration code to move_pages()")
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reported-by: Hugh Dickins <hughd@google.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: <stable@vger.kernel.org>    [3.12+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7d7856359920..7b5785032049 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -99,9 +99,9 @@ int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
-				pmd_t *pmd, int write);
+				pmd_t *pmd, int flags);
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
-				pud_t *pud, int write);
+				pud_t *pud, int flags);
 int pmd_huge(pmd_t pmd);
 int pud_huge(pud_t pmd);
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
@@ -133,8 +133,8 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 static inline void hugetlb_show_meminfo(void)
 {
 }
-#define follow_huge_pmd(mm, addr, pmd, write)	NULL
-#define follow_huge_pud(mm, addr, pud, write)	NULL
+#define follow_huge_pmd(mm, addr, pmd, flags)	NULL
+#define follow_huge_pud(mm, addr, pud, flags)	NULL
 #define prepare_hugepage_range(file, addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define pud_huge(x)	0

commit 753162cd849c45580fb5aaa7f3597c81e74e391c
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Tue Feb 10 14:11:36 2015 -0800

    mm: hugetlb: fix type of hugetlb_treat_as_movable variable
    
    hugetlb_treat_as_movable declared as unsigned long, but
    proc_dointvec() used for parsing it:
    
    static struct ctl_table vm_table[] = {
    ...
            {
                    .procname       = "hugepages_treat_as_movable",
                    .data           = &hugepages_treat_as_movable,
                    .maxlen         = sizeof(int),
                    .mode           = 0644,
                    .proc_handler   = proc_dointvec,
            },
    
    This seems harmless, but it's better to use int type here.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 431b7fc605c9..7d7856359920 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -86,7 +86,7 @@ void free_huge_page(struct page *page);
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 #endif
 
-extern unsigned long hugepages_treat_as_movable;
+extern int hugepages_treat_as_movable;
 extern int sysctl_hugetlb_shm_group;
 extern struct list_head huge_boot_pages;
 

commit 140cd7fb04a4a2bc09a30980bc8104cc89e09330
Merge: 27afc5dbda52 56548fc0e86c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 11 17:48:14 2014 -0800

    Merge tag 'powerpc-3.19-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Some nice cleanups like removing bootmem, and removal of
      __get_cpu_var().
    
      There is one patch to mm/gup.c.  This is the generic GUP
      implementation, but is only used by us and arm(64).  We have an ack
      from Steve Capper, and although we didn't get an ack from Andrew he
      told us to take the patch through the powerpc tree.
    
      There's one cxl patch.  This is in drivers/misc, but Greg said he was
      happy for us to manage fixes for it.
    
      There is an infrastructure patch to support an IPMI driver for OPAL.
    
      There is also an RTC driver for OPAL.  We weren't able to get any
      response from the RTC maintainer, Alessandro Zummo, so in the end we
      just merged the driver.
    
      The usual batch of Freescale updates from Scott"
    
    * tag 'powerpc-3.19-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (101 commits)
      powerpc/powernv: Return to cpu offline loop when finished in KVM guest
      powerpc/book3s: Fix partial invalidation of TLBs in MCE code.
      powerpc/mm: don't do tlbie for updatepp request with NO HPTE fault
      powerpc/xmon: Cleanup the breakpoint flags
      powerpc/xmon: Enable HW instruction breakpoint on POWER8
      powerpc/mm/thp: Use tlbiel if possible
      powerpc/mm/thp: Remove code duplication
      powerpc/mm/hugetlb: Sanity check gigantic hugepage count
      powerpc/oprofile: Disable pagefaults during user stack read
      powerpc/mm: Check for matching hpte without taking hpte lock
      powerpc: Drop useless warning in eeh_init()
      powerpc/powernv: Cleanup unused MCE definitions/declarations.
      powerpc/eeh: Dump PHB diag-data early
      powerpc/eeh: Recover EEH error on ownership change for BCM5719
      powerpc/eeh: Set EEH_PE_RESET on PE reset
      powerpc/eeh: Refactor eeh_reset_pe()
      powerpc: Remove more traces of bootmem
      powerpc/pseries: Initialise nvram_pstore_info's buf_lock
      cxl: Name interrupts in /proc/interrupt
      cxl: Return error to PSL if IRQ demultiplexing fails & print clearer warning
      ...

commit 97ad2be1daf8e6f2d297aa349101b340e1327917
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Wed Dec 10 15:44:13 2014 -0800

    mm, hugetlb: correct bit shift in hstate_sizelog()
    
    hstate_sizelog() would shift left an int rather than long, triggering
    undefined behaviour and passing an incorrect value when the requested
    page size was more than 4GB, thus breaking >4GB pages.
    
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andrey Ryabinin <a.ryabinin@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6e6d338641fe..cdd149ca5cc0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -311,7 +311,8 @@ static inline struct hstate *hstate_sizelog(int page_size_log)
 {
 	if (!page_size_log)
 		return &default_hstate;
-	return size_to_hstate(1 << page_size_log);
+
+	return size_to_hstate(1UL << page_size_log);
 }
 
 static inline struct hstate *hstate_vma(struct vm_area_struct *vma)

commit f30c59e921f12b209852e1ddc197dc6a8fb0142b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Nov 5 21:57:40 2014 +0530

    mm: Update generic gup implementation to handle hugepage directory
    
    Update generic gup implementation with powerpc specific details.
    On powerpc at pmd level we can have hugepte, normal pmd pointer
    or a pointer to the hugepage directory.
    
    Tested-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6e6d338641fe..e6b62f30ab21 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -175,6 +175,52 @@ static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
 }
 
 #endif /* !CONFIG_HUGETLB_PAGE */
+/*
+ * hugepages at page global directory. If arch support
+ * hugepages at pgd level, they need to define this.
+ */
+#ifndef pgd_huge
+#define pgd_huge(x)	0
+#endif
+
+#ifndef pgd_write
+static inline int pgd_write(pgd_t pgd)
+{
+	BUG();
+	return 0;
+}
+#endif
+
+#ifndef pud_write
+static inline int pud_write(pud_t pud)
+{
+	BUG();
+	return 0;
+}
+#endif
+
+#ifndef is_hugepd
+/*
+ * Some architectures requires a hugepage directory format that is
+ * required to support multiple hugepage sizes. For example
+ * a4fe3ce76 "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
+ * introduced the same on powerpc. This allows for a more flexible hugepage
+ * pagetable layout.
+ */
+typedef struct { unsigned long pd; } hugepd_t;
+#define is_hugepd(hugepd) (0)
+#define __hugepd(x) ((hugepd_t) { (x) })
+static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+			      unsigned pdshift, unsigned long end,
+			      int write, struct page **pages, int *nr)
+{
+	return 0;
+}
+#else
+extern int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+		       unsigned pdshift, unsigned long end,
+		       int write, struct page **pages, int *nr);
+#endif
 
 #define HUGETLB_ANON_FILE "anon_hugepage"
 

commit ed4d4902ebdd7ca8b5a51daaf6bebf4b172895cc
Author: David Rientjes <rientjes@google.com>
Date:   Wed Aug 6 16:06:54 2014 -0700

    mm, hugetlb: remove hugetlb_zero and hugetlb_infinity
    
    They are unnecessary: "zero" can be used in place of "hugetlb_zero" and
    passing extra2 == NULL is equivalent to infinity.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index a23c096b3080..6e6d338641fe 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -87,7 +87,6 @@ pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
 #endif
 
 extern unsigned long hugepages_treat_as_movable;
-extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 extern struct list_head huge_boot_pages;
 

commit 8f1d26d0e59b9676587c54578f976709b625d6e9
Author: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
Date:   Wed Jul 30 16:08:39 2014 -0700

    kexec: export free_huge_page to VMCOREINFO
    
    PG_head_mask was added into VMCOREINFO to filter huge pages in b3acc56bfe1
    ("kexec: save PG_head_mask in VMCOREINFO"), but makedumpfile still need
    another symbol to filter *hugetlbfs* pages.
    
    If a user hope to filter user pages, makedumpfile tries to exclude them by
    checking the condition whether the page is anonymous, but hugetlbfs pages
    aren't anonymous while they also be user pages.
    
    We know it's possible to detect them in the same way as PageHuge(),
    so we need the start address of free_huge_page():
    
        int PageHuge(struct page *page)
        {
                if (!PageCompound(page))
                        return 0;
    
                page = compound_head(page);
                return get_compound_page_dtor(page) == free_huge_page;
        }
    
    For that reason, this patch changes free_huge_page() into public
    to export it to VMCOREINFO.
    
    Signed-off-by: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Acked-by: Baoquan He <bhe@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 255cd5cc0754..a23c096b3080 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -80,6 +80,7 @@ int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 bool is_hugepage_active(struct page *page);
+void free_huge_page(struct page *page);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);

commit 100873d7a777b67ad35197c5a998b5e778f8bf3f
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Jun 4 16:10:56 2014 -0700

    hugetlb: rename hugepage_migration_support() to ..._supported()
    
    We already have a function named hugepages_supported(), and the similar
    name hugepage_migration_support() is a bit unconfortable, so let's rename
    it hugepage_migration_supported().
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 35786ee36f06..255cd5cc0754 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -397,7 +397,7 @@ static inline pgoff_t basepage_index(struct page *page)
 
 extern void dissolve_free_huge_pages(unsigned long start_pfn,
 				     unsigned long end_pfn);
-static inline int hugepage_migration_support(struct hstate *h)
+static inline int hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
 	return huge_page_shift(h) == PMD_SHIFT;
@@ -453,7 +453,7 @@ static inline pgoff_t basepage_index(struct page *page)
 	return page->index;
 }
 #define dissolve_free_huge_pages(s, e)	do {} while (0)
-#define hugepage_migration_support(h)	0
+#define hugepage_migration_supported(h)	0
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 					   struct mm_struct *mm, pte_t *pte)

commit bae7f4ae14d47008a11b4358b167cb0ae186c06a
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Jun 4 16:07:08 2014 -0700

    hugetlb: add hstate_is_gigantic()
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reviewed-by: Davidlohr Bueso <davidlohr@hp.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d0bad1a8b0bd..35786ee36f06 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -343,6 +343,11 @@ static inline unsigned huge_page_shift(struct hstate *h)
 	return h->order + PAGE_SHIFT;
 }
 
+static inline bool hstate_is_gigantic(struct hstate *h)
+{
+	return huge_page_order(h) >= MAX_ORDER;
+}
+
 static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1 << h->order;

commit c177c81e09e517bbf75b67762cdab1b83aba6976
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Jun 4 16:05:35 2014 -0700

    hugetlb: restrict hugepage_migration_support() to x86_64
    
    Currently hugepage migration is available for all archs which support
    pmd-level hugepage, but testing is done only for x86_64 and there're
    bugs for other archs.  So to avoid breaking such archs, this patch
    limits the availability strictly to x86_64 until developers of other
    archs get interested in enabling this feature.
    
    Simply disabling hugepage migration on non-x86_64 archs is not enough to
    fix the reported problem where sys_move_pages() hits the BUG_ON() in
    follow_page(FOLL_GET), so let's fix this by checking if hugepage
    migration is supported in vma_migratable().
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: <stable@vger.kernel.org>    [3.12+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b65166de1d9d..d0bad1a8b0bd 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -392,15 +392,13 @@ static inline pgoff_t basepage_index(struct page *page)
 
 extern void dissolve_free_huge_pages(unsigned long start_pfn,
 				     unsigned long end_pfn);
-int pmd_huge_support(void);
-/*
- * Currently hugepage migration is enabled only for pmd-based hugepage.
- * This function will be updated when hugepage migration is more widely
- * supported.
- */
 static inline int hugepage_migration_support(struct hstate *h)
 {
-	return pmd_huge_support() && (huge_page_shift(h) == PMD_SHIFT);
+#ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
+	return huge_page_shift(h) == PMD_SHIFT;
+#else
+	return 0;
+#endif
 }
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
@@ -450,7 +448,6 @@ static inline pgoff_t basepage_index(struct page *page)
 	return page->index;
 }
 #define dissolve_free_huge_pages(s, e)	do {} while (0)
-#define pmd_huge_support()	0
 #define hugepage_migration_support(h)	0
 
 static inline spinlock_t *huge_pte_lockptr(struct hstate *h,

commit 457c1b27ed56ec472d202731b12417bff023594a
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Tue May 6 12:50:00 2014 -0700

    hugetlb: ensure hugepage access is denied if hugepages are not supported
    
    Currently, I am seeing the following when I `mount -t hugetlbfs /none
    /dev/hugetlbfs`, and then simply do a `ls /dev/hugetlbfs`.  I think it's
    related to the fact that hugetlbfs is properly not correctly setting
    itself up in this state?:
    
      Unable to handle kernel paging request for data at address 0x00000031
      Faulting instruction address: 0xc000000000245710
      Oops: Kernel access of bad area, sig: 11 [#1]
      SMP NR_CPUS=2048 NUMA pSeries
      ....
    
    In KVM guests on Power, in a guest not backed by hugepages, we see the
    following:
    
      AnonHugePages:         0 kB
      HugePages_Total:       0
      HugePages_Free:        0
      HugePages_Rsvd:        0
      HugePages_Surp:        0
      Hugepagesize:         64 kB
    
    HPAGE_SHIFT == 0 in this configuration, which indicates that hugepages
    are not supported at boot-time, but this is only checked in
    hugetlb_init().  Extract the check to a helper function, and use it in a
    few relevant places.
    
    This does make hugetlbfs not supported (not registered at all) in this
    environment.  I believe this is fine, as there are no valid hugepages
    and that won't change at runtime.
    
    [akpm@linux-foundation.org: use pr_info(), per Mel]
    [akpm@linux-foundation.org: fix build when HPAGE_SHIFT is undefined]
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5b337cf8fb86..b65166de1d9d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -412,6 +412,16 @@ static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 	return &mm->page_table_lock;
 }
 
+static inline bool hugepages_supported(void)
+{
+	/*
+	 * Some platform decide whether they support huge pages at boot
+	 * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when
+	 * there is no such support
+	 */
+	return HPAGE_SHIFT != 0;
+}
+
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL

commit 7b24d8616be33616efd41ff67d3c76362c60ca84
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Thu Apr 3 14:47:27 2014 -0700

    mm, hugetlb: fix race in region tracking
    
    There is a race condition if we map a same file on different processes.
    Region tracking is protected by mmap_sem and hugetlb_instantiation_mutex.
    When we do mmap, we don't grab a hugetlb_instantiation_mutex, but only
    mmap_sem (exclusively).  This doesn't prevent other tasks from modifying
    the region structure, so it can be modified by two processes
    concurrently.
    
    To solve this, introduce a spinlock to resv_map and make region
    manipulation function grab it before they do actual work.
    
    [davidlohr@hp.com: updated changelog]
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Suggested-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f62c2f6c6059..5b337cf8fb86 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -27,6 +27,7 @@ struct hugepage_subpool {
 
 struct resv_map {
 	struct kref refs;
+	spinlock_t lock;
 	struct list_head regions;
 };
 extern struct resv_map *resv_map_alloc(void);

commit 9119a41e9091fb3a8204039d595bcdae24193c57
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Apr 3 14:47:25 2014 -0700

    mm, hugetlb: unify region structure handling
    
    Currently, to track reserved and allocated regions, we use two different
    ways, depending on the mapping.  For MAP_SHARED, we use
    address_mapping's private_list and, while for MAP_PRIVATE, we use a
    resv_map.
    
    Now, we are preparing to change a coarse grained lock which protect a
    region structure to fine grained lock, and this difference hinder it.
    So, before changing it, unify region structure handling, consistently
    using a resv_map regardless of the kind of mapping.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 8c43cc469d78..f62c2f6c6059 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -6,6 +6,8 @@
 #include <linux/fs.h>
 #include <linux/hugetlb_inline.h>
 #include <linux/cgroup.h>
+#include <linux/list.h>
+#include <linux/kref.h>
 
 struct ctl_table;
 struct user_struct;
@@ -23,6 +25,13 @@ struct hugepage_subpool {
 	long max_hpages, used_hpages;
 };
 
+struct resv_map {
+	struct kref refs;
+	struct list_head regions;
+};
+extern struct resv_map *resv_map_alloc(void);
+void resv_map_release(struct kref *ref);
+
 extern spinlock_t hugetlb_lock;
 extern int hugetlb_max_hstate __read_mostly;
 #define for_each_hstate(h) \

commit 309381feaee564281c3d9e90fbca8963bb7428ad
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Thu Jan 23 15:52:54 2014 -0800

    mm: dump page when hitting a VM_BUG_ON using VM_BUG_ON_PAGE
    
    Most of the VM_BUG_ON assertions are performed on a page.  Usually, when
    one of these assertions fails we'll get a BUG_ON with a call stack and
    the registers.
    
    I've recently noticed based on the requests to add a small piece of code
    that dumps the page to various VM_BUG_ON sites that the page dump is
    quite useful to people debugging issues in mm.
    
    This patch adds a VM_BUG_ON_PAGE(cond, page) which beyond doing what
    VM_BUG_ON() does, also dumps the page before executing the actual
    BUG_ON.
    
    [akpm@linux-foundation.org: fix up includes]
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d01cc972a1d9..8c43cc469d78 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -2,6 +2,7 @@
 #define _LINUX_HUGETLB_H
 
 #include <linux/mm_types.h>
+#include <linux/mmdebug.h>
 #include <linux/fs.h>
 #include <linux/hugetlb_inline.h>
 #include <linux/cgroup.h>
@@ -354,7 +355,7 @@ static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 
 static inline struct hstate *page_hstate(struct page *page)
 {
-	VM_BUG_ON(!PageHuge(page));
+	VM_BUG_ON_PAGE(!PageHuge(page), page);
 	return size_to_hstate(PAGE_SIZE << compound_order(page));
 }
 

commit 44518d2b32646e37b4b7a0813bbbe98dc21c7f8f
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Tue Jan 21 15:48:54 2014 -0800

    mm: tail page refcounting optimization for slab and hugetlbfs
    
    This skips the _mapcount mangling for slab and hugetlbfs pages.
    
    The main trouble in doing this is to guarantee that PageSlab and
    PageHeadHuge remains constant for all get_page/put_page run on the tail
    of slab or hugetlbfs compound pages.  Otherwise if they're set during
    get_page but not set during put_page, the _mapcount of the tail page
    would underflow.
    
    PageHeadHuge will remain true until the compound page is released and
    enters the buddy allocator so it won't risk to change even if the tail
    page is the last reference left on the page.
    
    PG_slab instead is cleared before the slab frees the head page with
    put_page, so if the tail pin is released after the slab freed the page,
    we would have a problem.  But in the slab case the tail pin cannot be
    the last reference left on the page.  This is because the slab code is
    free to reuse the compound page after a kfree/kmem_cache_free without
    having to check if there's any tail pin left.  In turn all tail pins
    must be always released while the head is still pinned by the slab code
    and so we know PG_slab will be still set too.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Pravin Shelar <pshelar@nicira.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 251233c1494d..d01cc972a1d9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -31,7 +31,6 @@ struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 
 int PageHuge(struct page *page);
-int PageHeadHuge(struct page *page_head);
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
@@ -104,11 +103,6 @@ static inline int PageHuge(struct page *page)
 	return 0;
 }
 
-static inline int PageHeadHuge(struct page *page_head)
-{
-	return 0;
-}
-
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
 }

commit 0e147aed4c250766e657855db55a395d2c8008a5
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Jan 21 15:48:48 2014 -0800

    mm: hugetlbfs: Add some VM_BUG_ON()s to catch non-hugetlbfs pages
    
    Dave Jiang reported that he was seeing oopses when running NUMA systems
    and default_hugepagesz=1G.  I traced the issue down to
    migrate_page_copy() trying to use the same code for hugetlb pages and
    transparent hugepages.  It should not have been trying to pass thp pages
    in there.
    
    So, add some VM_BUG_ON()s for the next hapless VM developer that tries
    the same thing.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Tested-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index bd7e98752222..251233c1494d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -360,6 +360,7 @@ static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 
 static inline struct hstate *page_hstate(struct page *page)
 {
+	VM_BUG_ON(!PageHuge(page));
 	return size_to_hstate(PAGE_SIZE << compound_order(page));
 }
 

commit f40386a4e976acb2bd3e0f9ead11e8e769acbe98
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Dec 12 17:12:19 2013 -0800

    include/linux/hugetlb.h: make isolate_huge_page() an inline
    
    With CONFIG_HUGETLBFS=n:
    
      mm/migrate.c: In function `do_move_page_to_node_array':
      include/linux/hugetlb.h:140:33: warning: statement with no effect [-Wunused-value]
       #define isolate_huge_page(p, l) false
                                       ^
      mm/migrate.c:1170:4: note: in expansion of macro `isolate_huge_page'
          isolate_huge_page(page, &pagelist);
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 9649ff0c63f8..bd7e98752222 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -142,7 +142,10 @@ static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 	return 0;
 }
 
-#define isolate_huge_page(p, l) false
+static inline bool isolate_huge_page(struct page *page, struct list_head *list)
+{
+	return false;
+}
 #define putback_active_hugepage(p)	do {} while (0)
 #define is_hugepage_active(x)	false
 

commit 27c73ae759774e63313c1fbfeb17ba076cea64c5
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Nov 21 14:32:02 2013 -0800

    mm: hugetlbfs: fix hugetlbfs optimization
    
    Commit 7cb2ef56e6a8 ("mm: fix aio performance regression for database
    caused by THP") can cause dereference of a dangling pointer if
    split_huge_page runs during PageHuge() if there are updates to the
    tail_page->private field.
    
    Also it is repeating compound_head twice for hugetlbfs and it is running
    compound_head+compound_trans_head for THP when a single one is needed in
    both cases.
    
    The new code within the PageSlab() check doesn't need to verify that the
    THP page size is never bigger than the smallest hugetlbfs page size, to
    avoid memory corruption.
    
    A longstanding theoretical race condition was found while fixing the
    above (see the change right after the skip_unlock label, that is
    relevant for the compound_lock path too).
    
    By re-establishing the _mapcount tail refcounting for all compound
    pages, this also fixes the below problem:
    
      echo 0 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
    
      BUG: Bad page state in process bash  pfn:59a01
      page:ffffea000139b038 count:0 mapcount:10 mapping:          (null) index:0x0
      page flags: 0x1c00000000008000(tail)
      Modules linked in:
      CPU: 6 PID: 2018 Comm: bash Not tainted 3.12.0+ #25
      Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
      Call Trace:
        dump_stack+0x55/0x76
        bad_page+0xd5/0x130
        free_pages_prepare+0x213/0x280
        __free_pages+0x36/0x80
        update_and_free_page+0xc1/0xd0
        free_pool_huge_page+0xc2/0xe0
        set_max_huge_pages.part.58+0x14c/0x220
        nr_hugepages_store_common.isra.60+0xd0/0xf0
        nr_hugepages_store+0x13/0x20
        kobj_attr_store+0xf/0x20
        sysfs_write_file+0x189/0x1e0
        vfs_write+0xc5/0x1f0
        SyS_write+0x55/0xb0
        system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Khalid Aziz <khalid.aziz@oracle.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Tested-by: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Pravin Shelar <pshelar@nicira.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 85e0c58bdfdf..9649ff0c63f8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -31,6 +31,7 @@ struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 
 int PageHuge(struct page *page);
+int PageHeadHuge(struct page *page_head);
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
@@ -103,6 +104,11 @@ static inline int PageHuge(struct page *page)
 	return 0;
 }
 
+static inline int PageHeadHuge(struct page *page_head)
+{
+	return 0;
+}
+
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
 }

commit 30b0a105d9f7141e4cbf72ae5511832457d89788
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Nov 21 14:31:58 2013 -0800

    mm: thp: give transparent hugepage code a separate copy_page
    
    Right now, the migration code in migrate_page_copy() uses copy_huge_page()
    for hugetlbfs and thp pages:
    
           if (PageHuge(page) || PageTransHuge(page))
                    copy_huge_page(newpage, page);
    
    So, yay for code reuse.  But:
    
      void copy_huge_page(struct page *dst, struct page *src)
      {
            struct hstate *h = page_hstate(src);
    
    and a non-hugetlbfs page has no page_hstate().  This works 99% of the
    time because page_hstate() determines the hstate from the page order
    alone.  Since the page order of a THP page matches the default hugetlbfs
    page order, it works.
    
    But, if you change the default huge page size on the boot command-line
    (say default_hugepagesz=1G), then we might not even *have* a 2MB hstate
    so page_hstate() returns null and copy_huge_page() oopses pretty fast
    since copy_huge_page() dereferences the hstate:
    
      void copy_huge_page(struct page *dst, struct page *src)
      {
            struct hstate *h = page_hstate(src);
            if (unlikely(pages_per_huge_page(h) > MAX_ORDER_NR_PAGES)) {
      ...
    
    Mel noticed that the migration code is really the only user of these
    functions.  This moves all the copy code over to migrate.c and makes
    copy_huge_page() work for THP by checking for it explicitly.
    
    I believe the bug was introduced in commit b32967ff101a ("mm: numa: Add
    THP migration for the NUMA working set scanning fault case")
    
    [akpm@linux-foundation.org: fix coding-style and comment text, per Naoya Horiguchi]
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Tested-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index acd2010328f3..85e0c58bdfdf 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -69,7 +69,6 @@ int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 bool is_hugepage_active(struct page *page);
-void copy_huge_page(struct page *dst, struct page *src);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
@@ -140,9 +139,6 @@ static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 #define isolate_huge_page(p, l) false
 #define putback_active_hugepage(p)	do {} while (0)
 #define is_hugepage_active(x)	false
-static inline void copy_huge_page(struct page *dst, struct page *src)
-{
-}
 
 static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot)

commit cb900f41215447433cbc456d1c4294e858a84d7c
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:02 2013 -0800

    mm, hugetlb: convert hugetlbfs to use split pmd lock
    
    Hugetlb supports multiple page sizes. We use split lock only for PMD
    level, but not for PUD.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Alex Thorlton <athorlton@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "Eric W . Biederman" <ebiederm@xmission.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0393270466c3..acd2010328f3 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -392,6 +392,15 @@ static inline int hugepage_migration_support(struct hstate *h)
 	return pmd_huge_support() && (huge_page_shift(h) == PMD_SHIFT);
 }
 
+static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
+					   struct mm_struct *mm, pte_t *pte)
+{
+	if (huge_page_size(h) == PMD_SIZE)
+		return pmd_lockptr(mm, (pmd_t *) pte);
+	VM_BUG_ON(huge_page_size(h) == PAGE_SIZE);
+	return &mm->page_table_lock;
+}
+
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
@@ -401,6 +410,7 @@ struct hstate {};
 #define hstate_sizelog(s) NULL
 #define hstate_vma(v) NULL
 #define hstate_inode(i) NULL
+#define page_hstate(page) NULL
 #define huge_page_size(h) PAGE_SIZE
 #define huge_page_mask(h) PAGE_MASK
 #define vma_kernel_pagesize(v) PAGE_SIZE
@@ -421,6 +431,22 @@ static inline pgoff_t basepage_index(struct page *page)
 #define dissolve_free_huge_pages(s, e)	do {} while (0)
 #define pmd_huge_support()	0
 #define hugepage_migration_support(h)	0
+
+static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
+					   struct mm_struct *mm, pte_t *pte)
+{
+	return &mm->page_table_lock;
+}
 #endif	/* CONFIG_HUGETLB_PAGE */
 
+static inline spinlock_t *huge_pte_lock(struct hstate *h,
+					struct mm_struct *mm, pte_t *pte)
+{
+	spinlock_t *ptl;
+
+	ptl = huge_pte_lockptr(h, mm, pte);
+	spin_lock(ptl);
+	return ptl;
+}
+
 #endif /* _LINUX_HUGETLB_H */

commit 83467efbdb7948146581a56cbd683a22a0684bbb
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:11 2013 -0700

    mm: migrate: check movability of hugepage in unmap_and_move_huge_page()
    
    Currently hugepage migration works well only for pmd-based hugepages
    (mainly due to lack of testing,) so we had better not enable migration of
    other levels of hugepages until we are ready for it.
    
    Some users of hugepage migration (mbind, move_pages, and migrate_pages) do
    page table walk and check pud/pmd_huge() there, so they are safe.  But the
    other users (softoffline and memory hotremove) don't do this, so without
    this patch they can try to migrate unexpected types of hugepages.
    
    To prevent this, we introduce hugepage_migration_support() as an
    architecture dependent check of whether hugepage are implemented on a pmd
    basis or not.  And on some architecture multiple sizes of hugepages are
    available, so hugepage_migration_support() also checks hugepage size.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 2e02c4ed1035..0393270466c3 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -381,6 +381,16 @@ static inline pgoff_t basepage_index(struct page *page)
 
 extern void dissolve_free_huge_pages(unsigned long start_pfn,
 				     unsigned long end_pfn);
+int pmd_huge_support(void);
+/*
+ * Currently hugepage migration is enabled only for pmd-based hugepage.
+ * This function will be updated when hugepage migration is more widely
+ * supported.
+ */
+static inline int hugepage_migration_support(struct hstate *h)
+{
+	return pmd_huge_support() && (huge_page_shift(h) == PMD_SHIFT);
+}
 
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
@@ -409,6 +419,8 @@ static inline pgoff_t basepage_index(struct page *page)
 	return page->index;
 }
 #define dissolve_free_huge_pages(s, e)	do {} while (0)
+#define pmd_huge_support()	0
+#define hugepage_migration_support(h)	0
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 #endif /* _LINUX_HUGETLB_H */

commit c8721bbbdd36382de51cd6b7a56322e0acca2414
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:09 2013 -0700

    mm: memory-hotplug: enable memory hotplug to handle hugepage
    
    Until now we can't offline memory blocks which contain hugepages because a
    hugepage is considered as an unmovable page.  But now with this patch
    series, a hugepage has become movable, so by using hugepage migration we
    can offline such memory blocks.
    
    What's different from other users of hugepage migration is that we need to
    decompose all the hugepages inside the target memory block into free buddy
    pages after hugepage migration, because otherwise free hugepages remaining
    in the memory block intervene the memory offlining.  For this reason we
    introduce new functions dissolve_free_huge_page() and
    dissolve_free_huge_pages().
    
    Other than that, what this patch does is straightforwardly to add hugepage
    migration code, that is, adding hugepage code to the functions which scan
    over pfn and collect hugepages to be migrated, and adding a hugepage
    allocation function to alloc_migrate_target().
    
    As for larger hugepages (1GB for x86_64), it's not easy to do hotremove
    over them because it's larger than memory block.  So we now simply leave
    it to fail as it is.
    
    [yongjun_wei@trendmicro.com.cn: remove duplicated include]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d1db00790a84..2e02c4ed1035 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -68,6 +68,7 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
+bool is_hugepage_active(struct page *page);
 void copy_huge_page(struct page *dst, struct page *src);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
@@ -138,6 +139,7 @@ static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 
 #define isolate_huge_page(p, l) false
 #define putback_active_hugepage(p)	do {} while (0)
+#define is_hugepage_active(x)	false
 static inline void copy_huge_page(struct page *dst, struct page *src)
 {
 }
@@ -377,6 +379,9 @@ static inline pgoff_t basepage_index(struct page *page)
 	return __basepage_index(page);
 }
 
+extern void dissolve_free_huge_pages(unsigned long start_pfn,
+				     unsigned long end_pfn);
+
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
@@ -403,6 +408,7 @@ static inline pgoff_t basepage_index(struct page *page)
 {
 	return page->index;
 }
+#define dissolve_free_huge_pages(s, e)	do {} while (0)
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 #endif /* _LINUX_HUGETLB_H */

commit 74060e4d78795c7c43805133cb717d82533d4e0d
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:06 2013 -0700

    mm: mbind: add hugepage migration code to mbind()
    
    Extend do_mbind() to handle vma with VM_HUGETLB set.  We will be able to
    migrate hugepage with mbind(2) after applying the enablement patch which
    comes later in this series.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index bc8d8370cd0d..d1db00790a84 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -265,6 +265,8 @@ struct huge_bootmem_page {
 };
 
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
+struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
+				unsigned long addr, int avoid_reserve);
 
 /* arch callback */
 int __init alloc_bootmem_huge_page(struct hstate *h);
@@ -378,6 +380,7 @@ static inline pgoff_t basepage_index(struct page *page)
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
+#define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
 #define hstate_sizelog(s) NULL

commit 31caf665e666b51fe36efd1e54031ed29e86c0b4
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:21:59 2013 -0700

    mm: migrate: make core migration code aware of hugepage
    
    Currently hugepage migration is available only for soft offlining, but
    it's also useful for some other users of page migration (clearly because
    users of hugepage can enjoy the benefit of mempolicy and memory hotplug.)
    So this patchset tries to extend such users to support hugepage migration.
    
    The target of this patchset is to enable hugepage migration for NUMA
    related system calls (migrate_pages(2), move_pages(2), and mbind(2)), and
    memory hotplug.
    
    This patchset does not add hugepage migration for memory compaction,
    because users of memory compaction mainly expect to construct thp by
    arranging raw pages, and there's little or no need to compact hugepages.
    CMA, another user of page migration, can have benefit from hugepage
    migration, but is not enabled to support it for now (just because of lack
    of testing and expertise in CMA.)
    
    Hugepage migration of non pmd-based hugepage (for example 1GB hugepage in
    x86_64, or hugepages in architectures like ia64) is not enabled for now
    (again, because of lack of testing.)
    
    As for how these are achived, I extended the API (migrate_pages()) to
    handle hugepage (with patch 1 and 2) and adjusted code of each caller to
    check and collect movable hugepages (with patch 3-7).  Remaining 2 patches
    are kind of miscellaneous ones to avoid unexpected behavior.  Patch 8 is
    about making sure that we only migrate pmd-based hugepages.  And patch 9
    is about choosing appropriate zone for hugepage allocation.
    
    My test is mainly functional one, simply kicking hugepage migration via
    each entry point and confirm that migration is done correctly.  Test code
    is available here:
    
      git://github.com/Naoya-Horiguchi/test_hugepage_migration_extension.git
    
    And I always run libhugetlbfs test when changing hugetlbfs's code.  With
    this patchset, no regression was found in the test.
    
    This patch (of 9):
    
    Before enabling each user of page migration to support hugepage,
    this patch enables the list of pages for migration to link not only
    LRU pages, but also hugepages. As a result, putback_movable_pages()
    and migrate_pages() can handle both of LRU pages and hugepages.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index c2b1801a160b..bc8d8370cd0d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -66,6 +66,8 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						vm_flags_t vm_flags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
+bool isolate_huge_page(struct page *page, struct list_head *list);
+void putback_active_hugepage(struct page *page);
 void copy_huge_page(struct page *dst, struct page *src);
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
@@ -134,6 +136,8 @@ static inline int dequeue_hwpoisoned_huge_page(struct page *page)
 	return 0;
 }
 
+#define isolate_huge_page(p, l) false
+#define putback_active_hugepage(p)	do {} while (0)
 static inline void copy_huge_page(struct page *dst, struct page *src)
 {
 }

commit 5f1e31d2f5977d910d0b2f5018173e99241d1940
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:02:42 2013 -0700

    mm/hugetlb: remove hugetlb_prefault
    
    hugetlb_prefault() is not used any more, this patch removes it.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 89d4fbf681e7..c2b1801a160b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -55,7 +55,6 @@ void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 				unsigned long start, unsigned long end,
 				struct page *ref_page);
-int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
 void hugetlb_show_meminfo(void);
@@ -114,7 +113,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define follow_hugetlb_page(m,v,p,vs,a,b,i,w)	({ BUG(); 0; })
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
-#define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
 static inline void hugetlb_report_meminfo(struct seq_file *m)
 {
 }

commit 1873e50028ce87dd9014049c86d71a898fa02166
Merge: fb2af0020a51 aa729dccb5e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 10:31:38 2013 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull ARM64 updates from Catalin Marinas:
     "Main features:
       - KVM and Xen ports to AArch64
       - Hugetlbfs and transparent huge pages support for arm64
       - Applied Micro X-Gene Kconfig entry and dts file
       - Cache flushing improvements
    
      For arm64 huge pages support, there are x86 changes moving part of
      arch/x86/mm/hugetlbpage.c into mm/hugetlb.c to be re-used by arm64"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (66 commits)
      arm64: Add initial DTS for APM X-Gene Storm SOC and APM Mustang board
      arm64: Add defines for APM ARMv8 implementation
      arm64: Enable APM X-Gene SOC family in the defconfig
      arm64: Add Kconfig option for APM X-Gene SOC family
      arm64/Makefile: provide vdso_install target
      ARM64: mm: THP support.
      ARM64: mm: Raise MAX_ORDER for 64KB pages and THP.
      ARM64: mm: HugeTLB support.
      ARM64: mm: Move PTE_PROT_NONE bit.
      ARM64: mm: Make PAGE_NONE pages read only and no-execute.
      ARM64: mm: Restore memblock limit when map_mem finished.
      mm: thp: Correct the HPAGE_PMD_ORDER check.
      x86: mm: Remove general hugetlb code from x86.
      mm: hugetlb: Copy general hugetlb code from x86 to mm.
      x86: mm: Remove x86 version of huge_pmd_share.
      mm: hugetlb: Copy huge_pmd_share from x86 to mm.
      arm64: KVM: document kernel object mappings in HYP
      arm64: KVM: MAINTAINERS update
      arm64: KVM: userspace API documentation
      arm64: KVM: enable initialization of a 32bit vcpu
      ...

commit 13d60f4b6ab5b702dc8d2ee20999f98a93728aec
Author: Zhang Yi <wetpzy@gmail.com>
Date:   Tue Jun 25 21:19:31 2013 +0800

    futex: Take hugepages into account when generating futex_key
    
    The futex_keys of process shared futexes are generated from the page
    offset, the mapping host and the mapping index of the futex user space
    address. This should result in an unique identifier for each futex.
    
    Though this is not true when futexes are located in different subpages
    of an hugepage. The reason is, that the mapping index for all those
    futexes evaluates to the index of the base page of the hugetlbfs
    mapping. So a futex at offset 0 of the hugepage mapping and another
    one at offset PAGE_SIZE of the same hugepage mapping have identical
    futex_keys. This happens because the futex code blindly uses
    page->index.
    
    Steps to reproduce the bug:
    
    1. Map a file from hugetlbfs. Initialize pthread_mutex1 at offset 0
       and pthread_mutex2 at offset PAGE_SIZE of the hugetlbfs
       mapping.
    
       The mutexes must be initialized as PTHREAD_PROCESS_SHARED because
       PTHREAD_PROCESS_PRIVATE mutexes are not affected by this issue as
       their keys solely depend on the user space address.
    
    2. Lock mutex1 and mutex2
    
    3. Create thread1 and in the thread function lock mutex1, which
       results in thread1 blocking on the locked mutex1.
    
    4. Create thread2 and in the thread function lock mutex2, which
       results in thread2 blocking on the locked mutex2.
    
    5. Unlock mutex2. Despite the fact that mutex2 got unlocked, thread2
       still blocks on mutex2 because the futex_key points to mutex1.
    
    To solve this issue we need to take the normal page index of the page
    which contains the futex into account, if the futex is in an hugetlbfs
    mapping. In other words, we calculate the normal page mapping index of
    the subpage in the hugetlbfs mapping.
    
    Mappings which are not based on hugetlbfs are not affected and still
    use page->index.
    
    Thanks to Mel Gorman who provided a patch for adding proper evaluation
    functions to the hugetlbfs code to avoid exposing hugetlbfs specific
    details to the futex code.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Zhang Yi <zhang.yi20@zte.com.cn>
    Reviewed-by: Jiang Biao <jiang.biao2@zte.com.cn>
    Tested-by: Ma Chenggong <ma.chenggong@zte.com.cn>
    Reviewed-by: 'Mel Gorman' <mgorman@suse.de>
    Acked-by: 'Darren Hart' <dvhart@linux.intel.com>
    Cc: 'Peter Zijlstra' <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/000101ce71a6%24a83c5880%24f8b50980%24@com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6b4890fa57e7..feaf0c7fb7d8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -358,6 +358,17 @@ static inline int hstate_index(struct hstate *h)
 	return h - hstates;
 }
 
+pgoff_t __basepage_index(struct page *page);
+
+/* Return page->index in PAGE_SIZE units */
+static inline pgoff_t basepage_index(struct page *page)
+{
+	if (!PageCompound(page))
+		return page->index;
+
+	return __basepage_index(page);
+}
+
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
@@ -378,6 +389,11 @@ static inline unsigned int pages_per_huge_page(struct hstate *h)
 }
 #define hstate_index_to_shift(index) 0
 #define hstate_index(h) 0
+
+static inline pgoff_t basepage_index(struct page *page)
+{
+	return page->index;
+}
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 #endif /* _LINUX_HUGETLB_H */

commit 3212b535f200c85b5a67cbfaea18431da71b5c72
Author: Steve Capper <steve.capper@linaro.org>
Date:   Tue Apr 23 12:35:02 2013 +0100

    mm: hugetlb: Copy huge_pmd_share from x86 to mm.
    
    Under x86, multiple puds can be made to reference the same bank of
    huge pmds provided that they represent a full PUD_SIZE of shared
    huge memory that is aligned to a PUD_SIZE boundary.
    
    The code to share pmds does not require any architecture specific
    knowledge other than the fact that pmds can be indexed, thus can
    be beneficial to some other architectures.
    
    This patch copies the huge pmd sharing (and unsharing) logic from
    x86/ to mm/ and introduces a new config option to activate it:
    CONFIG_ARCH_WANTS_HUGE_PMD_SHARE
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6b4890fa57e7..981546ad231c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -69,6 +69,10 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
 void copy_huge_page(struct page *dst, struct page *src);
 
+#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
+pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
+#endif
+
 extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;

commit af73e4d9506d3b797509f3c030e7dcd554f7d9c4
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Tue May 7 16:18:13 2013 -0700

    hugetlbfs: fix mmap failure in unaligned size request
    
    The current kernel returns -EINVAL unless a given mmap length is
    "almost" hugepage aligned.  This is because in sys_mmap_pgoff() the
    given length is passed to vm_mmap_pgoff() as it is without being aligned
    with hugepage boundary.
    
    This is a regression introduced in commit 40716e29243d ("hugetlbfs: fix
    alignment of huge page requests"), where alignment code is pushed into
    hugetlb_file_setup() and the variable len in caller side is not changed.
    
    To fix this, this patch partially reverts that commit, and adds
    alignment code in caller side.  And it also introduces hstate_sizelog()
    in order to get proper hstate to specified hugepage size.
    
    Addresses https://bugzilla.kernel.org/show_bug.cgi?id=56881
    
    [akpm@linux-foundation.org: fix warning when CONFIG_HUGETLB_PAGE=n]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: <iceman_dvd@yahoo.com>
    Cc: Steven Truelove <steven.truelove@utoronto.ca>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 3a62df310f2e..6b4890fa57e7 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -189,8 +189,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_file_setup(const char *name, unsigned long addr,
-				size_t size, vm_flags_t acct,
+struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 				struct user_struct **user, int creat_flags,
 				int page_size_log);
 
@@ -209,8 +208,8 @@ static inline int is_file_hugepages(struct file *file)
 
 #define is_file_hugepages(file)			0
 static inline struct file *
-hugetlb_file_setup(const char *name, unsigned long addr, size_t size,
-		vm_flags_t acctflag, struct user_struct **user, int creat_flags,
+hugetlb_file_setup(const char *name, size_t size, vm_flags_t acctflag,
+		struct user_struct **user, int creat_flags,
 		int page_size_log)
 {
 	return ERR_PTR(-ENOSYS);
@@ -288,6 +287,13 @@ static inline struct hstate *hstate_file(struct file *f)
 	return hstate_inode(file_inode(f));
 }
 
+static inline struct hstate *hstate_sizelog(int page_size_log)
+{
+	if (!page_size_log)
+		return &default_hstate;
+	return size_to_hstate(1 << page_size_log);
+}
+
 static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
 {
 	return hstate_file(vma->vm_file);
@@ -352,11 +358,12 @@ static inline int hstate_index(struct hstate *h)
 	return h - hstates;
 }
 
-#else
+#else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
+#define hstate_sizelog(s) NULL
 #define hstate_vma(v) NULL
 #define hstate_inode(i) NULL
 #define huge_page_size(h) PAGE_SIZE
@@ -371,6 +378,6 @@ static inline unsigned int pages_per_huge_page(struct hstate *h)
 }
 #define hstate_index_to_shift(index) 0
 #define hstate_index(h) 0
-#endif
+#endif	/* CONFIG_HUGETLB_PAGE */
 
 #endif /* _LINUX_HUGETLB_H */

commit 949f7ec5760b021da3cccc1eaeb0671270e4238f
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 29 15:07:48 2013 -0700

    mm, hugetlb: include hugepages in meminfo
    
    Particularly in oom conditions, it's troublesome that hugetlb memory is
    not displayed.  All other meminfo that is emitted will not add up to
    what is expected, and there is no artifact left in the kernel log to
    show that a potentially significant amount of memory is actually
    allocated as hugepages which are not available to be reclaimed.
    
    Booting with hugepages=8192 on the command line, this memory is now
    shown in oom conditions.  For example, with echo m >
    /proc/sysrq-trigger:
    
      Node 0 hugepages_total=2048 hugepages_free=2048 hugepages_surp=0 hugepages_size=2048kB
      Node 1 hugepages_total=2048 hugepages_free=2048 hugepages_surp=0 hugepages_size=2048kB
      Node 2 hugepages_total=2048 hugepages_free=2048 hugepages_surp=0 hugepages_size=2048kB
      Node 3 hugepages_total=2048 hugepages_free=2048 hugepages_surp=0 hugepages_size=2048kB
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 16e4e9a643fb..3a62df310f2e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -58,6 +58,7 @@ void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
+void hugetlb_show_meminfo(void);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags);
@@ -114,6 +115,9 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 {
 }
 #define hugetlb_report_node_meminfo(n, buf)	0
+static inline void hugetlb_show_meminfo(void)
+{
+}
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
 #define follow_huge_pud(mm, addr, pud, write)	NULL
 #define prepare_hugepage_range(file, addr, len)	(-EINVAL)

commit d895cb1af15c04c522a25c79cc429076987c089b
Merge: 9626357371b5 d3d009cb965e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 20:16:07 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs pile (part one) from Al Viro:
     "Assorted stuff - cleaning namei.c up a bit, fixing ->d_name/->d_parent
      locking violations, etc.
    
      The most visible changes here are death of FS_REVAL_DOT (replaced with
      "has ->d_weak_revalidate()") and a new helper getting from struct file
      to inode.  Some bits of preparation to xattr method interface changes.
    
      Misc patches by various people sent this cycle *and* ocfs2 fixes from
      several cycles ago that should've been upstream right then.
    
      PS: the next vfs pile will be xattr stuff."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (46 commits)
      saner proc_get_inode() calling conventions
      proc: avoid extra pde_put() in proc_fill_super()
      fs: change return values from -EACCES to -EPERM
      fs/exec.c: make bprm_mm_init() static
      ocfs2/dlm: use GFP_ATOMIC inside a spin_lock
      ocfs2: fix possible use-after-free with AIO
      ocfs2: Fix oops in ocfs2_fast_symlink_readpage() code path
      get_empty_filp()/alloc_file() leave both ->f_pos and ->f_version zero
      target: writev() on single-element vector is pointless
      export kernel_write(), convert open-coded instances
      fs: encode_fh: return FILEID_INVALID if invalid fid_type
      kill f_vfsmnt
      vfs: kill FS_REVAL_DOT by adding a d_weak_revalidate dentry op
      nfsd: handle vfs_getattr errors in acl protocol
      switch vfs_getattr() to struct path
      default SET_PERSONALITY() in linux/elf.h
      ceph: prepopulate inodes only when request is aborted
      d_hash_and_lookup(): export, switch open-coded instances
      9p: switch v9fs_set_create_acl() to inode+fid, do it before d_instantiate()
      9p: split dropping the acls from v9fs_set_create_acl()
      ...

commit 28a35716d317980ae9bc2ff2f84c33a3cda9e884
Author: Michel Lespinasse <walken@google.com>
Date:   Fri Feb 22 16:35:55 2013 -0800

    mm: use long type for page counts in mm_populate() and get_user_pages()
    
    Use long type for page counts in mm_populate() so as to avoid integer
    overflow when running the following test code:
    
    int main(void) {
      void *p = mmap(NULL, 0x100000000000, PROT_READ,
                     MAP_PRIVATE | MAP_ANON, -1, 0);
      printf("p: %p\n", p);
      mlockall(MCL_CURRENT);
      printf("done\n");
      return 0;
    }
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0c80d3f57a5b..eedc334fb6f5 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -43,9 +43,9 @@ int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
 #endif
 
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
-			struct page **, struct vm_area_struct **,
-			unsigned long *, int *, int, unsigned int flags);
+long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
+			 struct page **, struct vm_area_struct **,
+			 unsigned long *, unsigned long *, long, unsigned int);
 void unmap_hugepage_range(struct vm_area_struct *,
 			  unsigned long, unsigned long, struct page *);
 void __unmap_hugepage_range_final(struct mmu_gather *tlb,

commit 496ad9aa8ef448058e36ca7a787c61f2e63f0f54
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jan 23 17:07:38 2013 -0500

    new helper: file_inode(file)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0c80d3f57a5b..70832951f97c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -281,7 +281,7 @@ static inline struct hstate *hstate_inode(struct inode *i)
 
 static inline struct hstate *hstate_file(struct file *f)
 {
-	return hstate_inode(f->f_dentry->d_inode);
+	return hstate_inode(file_inode(f));
 }
 
 static inline struct hstate *hstate_vma(struct vm_area_struct *vma)

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 42d7395feb56f0655cd8b68e06fc6063823449f8
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Dec 11 16:01:34 2012 -0800

    mm: support more pagesizes for MAP_HUGETLB/SHM_HUGETLB
    
    There was some desire in large applications using MAP_HUGETLB or
    SHM_HUGETLB to use 1GB huge pages on some mappings, and stay with 2MB on
    others.  This is useful together with NUMA policy: use 2MB interleaving
    on some mappings, but 1GB on local mappings.
    
    This patch extends the IPC/SHM syscall interfaces slightly to allow
    specifying the page size.
    
    It borrows some upper bits in the existing flag arguments and allows
    encoding the log of the desired page size in addition to the *_HUGETLB
    flag.  When 0 is specified the default size is used, this makes the
    change fully compatible.
    
    Extending the internal hugetlb code to handle this is straight forward.
    Instead of a single mount it just keeps an array of them and selects the
    right mount based on the specified page size.  When no page size is
    specified it uses the mount of the default page size.
    
    The change is not visible in /proc/mounts because internal mounts don't
    appear there.  It also has very little overhead: the additional mounts
    just consume a super block, but not more memory when not used.
    
    I also exported the new flags to the user headers (they were previously
    under __KERNEL__).  Right now only symbols for x86 and some other
    architecture for 1GB and 2MB are defined.  The interface should already
    work for all other architectures though.  Only architectures that define
    multiple hugetlb sizes actually need it (that is currently x86, tile,
    powerpc).  However tile and powerpc have user configurable hugetlb
    sizes, so it's not easy to add defines.  A program on those
    architectures would need to query sysfs and use the appropiate log2.
    
    [akpm@linux-foundation.org: cleanups]
    [rientjes@google.com: fix build]
    [akpm@linux-foundation.org: checkpatch fixes]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 225164842ab6..3e7fa1acf09c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -183,7 +183,8 @@ extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, unsigned long addr,
 				size_t size, vm_flags_t acct,
-				struct user_struct **user, int creat_flags);
+				struct user_struct **user, int creat_flags,
+				int page_size_log);
 
 static inline int is_file_hugepages(struct file *file)
 {
@@ -195,12 +196,14 @@ static inline int is_file_hugepages(struct file *file)
 	return 0;
 }
 
+
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)			0
 static inline struct file *
 hugetlb_file_setup(const char *name, unsigned long addr, size_t size,
-		vm_flags_t acctflag, struct user_struct **user, int creat_flags)
+		vm_flags_t acctflag, struct user_struct **user, int creat_flags,
+		int page_size_log)
 {
 	return ERR_PTR(-ENOSYS);
 }

commit 7da4d641c58d201c3cc1835c05ca1a7fa26f0856
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 19 03:14:23 2012 +0100

    mm: Count the number of pages affected in change_protection()
    
    This will be used for three kinds of purposes:
    
     - to optimize mprotect()
    
     - to speed up working set scanning for working set areas that
       have not been touched
    
     - to more accurately scan per real working set
    
    No change in functionality from this patch.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 225164842ab6..06e691baab86 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -87,7 +87,7 @@ struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int write);
 int pmd_huge(pmd_t pmd);
 int pud_huge(pud_t pmd);
-void hugetlb_change_protection(struct vm_area_struct *vma,
+unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
 #else /* !CONFIG_HUGETLB_PAGE */
@@ -132,7 +132,11 @@ static inline void copy_huge_page(struct page *dst, struct page *src)
 {
 }
 
-#define hugetlb_change_protection(vma, address, end, newprot)
+static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
+		unsigned long address, unsigned long end, pgprot_t newprot)
+{
+	return 0;
+}
 
 static inline void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 			struct vm_area_struct *vma, unsigned long start,

commit d833352a4338dc31295ed832a30c9ccff5c7a183
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:46:20 2012 -0700

    mm: hugetlbfs: close race during teardown of hugetlbfs shared page tables
    
    If a process creates a large hugetlbfs mapping that is eligible for page
    table sharing and forks heavily with children some of whom fault and
    others which destroy the mapping then it is possible for page tables to
    get corrupted.  Some teardowns of the mapping encounter a "bad pmd" and
    output a message to the kernel log.  The final teardown will trigger a
    BUG_ON in mm/filemap.c.
    
    This was reproduced in 3.4 but is known to have existed for a long time
    and goes back at least as far as 2.6.37.  It was probably was introduced
    in 2.6.20 by [39dde65c: shared page table for hugetlb page].  The messages
    look like this;
    
    [  ..........] Lots of bad pmd messages followed by this
    [  127.164256] mm/memory.c:391: bad pmd ffff880412e04fe8(80000003de4000e7).
    [  127.164257] mm/memory.c:391: bad pmd ffff880412e04ff0(80000003de6000e7).
    [  127.164258] mm/memory.c:391: bad pmd ffff880412e04ff8(80000003de0000e7).
    [  127.186778] ------------[ cut here ]------------
    [  127.186781] kernel BUG at mm/filemap.c:134!
    [  127.186782] invalid opcode: 0000 [#1] SMP
    [  127.186783] CPU 7
    [  127.186784] Modules linked in: af_packet cpufreq_conservative cpufreq_userspace cpufreq_powersave acpi_cpufreq mperf ext3 jbd dm_mod coretemp crc32c_intel usb_storage ghash_clmulni_intel aesni_intel i2c_i801 r8169 mii uas sr_mod cdrom sg iTCO_wdt iTCO_vendor_support shpchp serio_raw cryptd aes_x86_64 e1000e pci_hotplug dcdbas aes_generic container microcode ext4 mbcache jbd2 crc16 sd_mod crc_t10dif i915 drm_kms_helper drm i2c_algo_bit ehci_hcd ahci libahci usbcore rtc_cmos usb_common button i2c_core intel_agp video intel_gtt fan processor thermal thermal_sys hwmon ata_generic pata_atiixp libata scsi_mod
    [  127.186801]
    [  127.186802] Pid: 9017, comm: hugetlbfs-test Not tainted 3.4.0-autobuild #53 Dell Inc. OptiPlex 990/06D7TR
    [  127.186804] RIP: 0010:[<ffffffff810ed6ce>]  [<ffffffff810ed6ce>] __delete_from_page_cache+0x15e/0x160
    [  127.186809] RSP: 0000:ffff8804144b5c08  EFLAGS: 00010002
    [  127.186810] RAX: 0000000000000001 RBX: ffffea000a5c9000 RCX: 00000000ffffffc0
    [  127.186811] RDX: 0000000000000000 RSI: 0000000000000009 RDI: ffff88042dfdad00
    [  127.186812] RBP: ffff8804144b5c18 R08: 0000000000000009 R09: 0000000000000003
    [  127.186813] R10: 0000000000000000 R11: 000000000000002d R12: ffff880412ff83d8
    [  127.186814] R13: ffff880412ff83d8 R14: 0000000000000000 R15: ffff880412ff83d8
    [  127.186815] FS:  00007fe18ed2c700(0000) GS:ffff88042dce0000(0000) knlGS:0000000000000000
    [  127.186816] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    [  127.186817] CR2: 00007fe340000503 CR3: 0000000417a14000 CR4: 00000000000407e0
    [  127.186818] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [  127.186819] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    [  127.186820] Process hugetlbfs-test (pid: 9017, threadinfo ffff8804144b4000, task ffff880417f803c0)
    [  127.186821] Stack:
    [  127.186822]  ffffea000a5c9000 0000000000000000 ffff8804144b5c48 ffffffff810ed83b
    [  127.186824]  ffff8804144b5c48 000000000000138a 0000000000001387 ffff8804144b5c98
    [  127.186825]  ffff8804144b5d48 ffffffff811bc925 ffff8804144b5cb8 0000000000000000
    [  127.186827] Call Trace:
    [  127.186829]  [<ffffffff810ed83b>] delete_from_page_cache+0x3b/0x80
    [  127.186832]  [<ffffffff811bc925>] truncate_hugepages+0x115/0x220
    [  127.186834]  [<ffffffff811bca43>] hugetlbfs_evict_inode+0x13/0x30
    [  127.186837]  [<ffffffff811655c7>] evict+0xa7/0x1b0
    [  127.186839]  [<ffffffff811657a3>] iput_final+0xd3/0x1f0
    [  127.186840]  [<ffffffff811658f9>] iput+0x39/0x50
    [  127.186842]  [<ffffffff81162708>] d_kill+0xf8/0x130
    [  127.186843]  [<ffffffff81162812>] dput+0xd2/0x1a0
    [  127.186845]  [<ffffffff8114e2d0>] __fput+0x170/0x230
    [  127.186848]  [<ffffffff81236e0e>] ? rb_erase+0xce/0x150
    [  127.186849]  [<ffffffff8114e3ad>] fput+0x1d/0x30
    [  127.186851]  [<ffffffff81117db7>] remove_vma+0x37/0x80
    [  127.186853]  [<ffffffff81119182>] do_munmap+0x2d2/0x360
    [  127.186855]  [<ffffffff811cc639>] sys_shmdt+0xc9/0x170
    [  127.186857]  [<ffffffff81410a39>] system_call_fastpath+0x16/0x1b
    [  127.186858] Code: 0f 1f 44 00 00 48 8b 43 08 48 8b 00 48 8b 40 28 8b b0 40 03 00 00 85 f6 0f 88 df fe ff ff 48 89 df e8 e7 cb 05 00 e9 d2 fe ff ff <0f> 0b 55 83 e2 fd 48 89 e5 48 83 ec 30 48 89 5d d8 4c 89 65 e0
    [  127.186868] RIP  [<ffffffff810ed6ce>] __delete_from_page_cache+0x15e/0x160
    [  127.186870]  RSP <ffff8804144b5c08>
    [  127.186871] ---[ end trace 7cbac5d1db69f426 ]---
    
    The bug is a race and not always easy to reproduce.  To reproduce it I was
    doing the following on a single socket I7-based machine with 16G of RAM.
    
    $ hugeadm --pool-pages-max DEFAULT:13G
    $ echo $((18*1048576*1024)) > /proc/sys/kernel/shmmax
    $ echo $((18*1048576*1024)) > /proc/sys/kernel/shmall
    $ for i in `seq 1 9000`; do ./hugetlbfs-test; done
    
    On my particular machine, it usually triggers within 10 minutes but
    enabling debug options can change the timing such that it never hits.
    Once the bug is triggered, the machine is in trouble and needs to be
    rebooted.  The machine will respond but processes accessing proc like "ps
    aux" will hang due to the BUG_ON.  shutdown will also hang and needs a
    hard reset or a sysrq-b.
    
    The basic problem is a race between page table sharing and teardown.  For
    the most part page table sharing depends on i_mmap_mutex.  In some cases,
    it is also taking the mm->page_table_lock for the PTE updates but with
    shared page tables, it is the i_mmap_mutex that is more important.
    
    Unfortunately it appears to be also insufficient. Consider the following
    situation
    
    Process A                                       Process B
    ---------                                       ---------
    hugetlb_fault                                   shmdt
                                                    LockWrite(mmap_sem)
                                                      do_munmap
                                                        unmap_region
                                                          unmap_vmas
                                                            unmap_single_vma
                                                              unmap_hugepage_range
                                                                Lock(i_mmap_mutex)
                                                                Lock(mm->page_table_lock)
                                                                huge_pmd_unshare/unmap tables <--- (1)
                                                                Unlock(mm->page_table_lock)
                                                                Unlock(i_mmap_mutex)
      huge_pte_alloc                                      ...
        Lock(i_mmap_mutex)                                ...
        vma_prio_walk, find svma, spte                    ...
        Lock(mm->page_table_lock)                         ...
        share spte                                        ...
        Unlock(mm->page_table_lock)                       ...
        Unlock(i_mmap_mutex)                              ...
      hugetlb_no_page                                                                         <--- (2)
                                                          free_pgtables
                                                            unlink_file_vma
                                                            hugetlb_free_pgd_range
                                                        remove_vma_list
    
    In this scenario, it is possible for Process A to share page tables with
    Process B that is trying to tear them down.  The i_mmap_mutex on its own
    does not prevent Process A walking Process B's page tables.  At (1) above,
    the page tables are not shared yet so it unmaps the PMDs.  Process A sets
    up page table sharing and at (2) faults a new entry.  Process B then trips
    up on it in free_pgtables.
    
    This patch fixes the problem by adding a new function
    __unmap_hugepage_range_final that is only called when the VMA is about to
    be destroyed.  This function clears VM_MAYSHARE during
    unmap_hugepage_range() under the i_mmap_mutex.  This makes the VMA
    ineligible for sharing and avoids the race.  Superficially this looks like
    it would then be vunerable to truncate and madvise issues but hugetlbfs
    has its own truncate handlers so does not use unmap_mapping_range() and
    does not support madvise(DONTNEED).
    
    This should be treated as a -stable candidate if it is merged.
    
    Test program is as follows. The test case was mostly written by Michal
    Hocko with a few minor changes to reproduce this bug.
    
    ==== CUT HERE ====
    
    static size_t huge_page_size = (2UL << 20);
    static size_t nr_huge_page_A = 512;
    static size_t nr_huge_page_B = 5632;
    
    unsigned int get_random(unsigned int max)
    {
            struct timeval tv;
    
            gettimeofday(&tv, NULL);
            srandom(tv.tv_usec);
            return random() % max;
    }
    
    static void play(void *addr, size_t size)
    {
            unsigned char *start = addr,
                          *end = start + size,
                          *a;
            start += get_random(size/2);
    
            /* we could itterate on huge pages but let's give it more time. */
            for (a = start; a < end; a += 4096)
                    *a = 0;
    }
    
    int main(int argc, char **argv)
    {
            key_t key = IPC_PRIVATE;
            size_t sizeA = nr_huge_page_A * huge_page_size;
            size_t sizeB = nr_huge_page_B * huge_page_size;
            int shmidA, shmidB;
            void *addrA = NULL, *addrB = NULL;
            int nr_children = 300, n = 0;
    
            if ((shmidA = shmget(key, sizeA, IPC_CREAT|SHM_HUGETLB|0660)) == -1) {
                    perror("shmget:");
                    return 1;
            }
    
            if ((addrA = shmat(shmidA, addrA, SHM_R|SHM_W)) == (void *)-1UL) {
                    perror("shmat");
                    return 1;
            }
            if ((shmidB = shmget(key, sizeB, IPC_CREAT|SHM_HUGETLB|0660)) == -1) {
                    perror("shmget:");
                    return 1;
            }
    
            if ((addrB = shmat(shmidB, addrB, SHM_R|SHM_W)) == (void *)-1UL) {
                    perror("shmat");
                    return 1;
            }
    
    fork_child:
            switch(fork()) {
                    case 0:
                            switch (n%3) {
                            case 0:
                                    play(addrA, sizeA);
                                    break;
                            case 1:
                                    play(addrB, sizeB);
                                    break;
                            case 2:
                                    break;
                            }
                            break;
                    case -1:
                            perror("fork:");
                            break;
                    default:
                            if (++n < nr_children)
                                    goto fork_child;
                            play(addrA, sizeA);
                            break;
            }
            shmdt(addrA);
            shmdt(addrB);
            do {
                    wait(NULL);
            } while (--n > 0);
            shmctl(shmidA, IPC_RMID, NULL);
            shmctl(shmidB, IPC_RMID, NULL);
            return 0;
    }
    
    [akpm@linux-foundation.org: name the declaration's args, fix CONFIG_HUGETLBFS=n build]
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f9db20bfa9fc..225164842ab6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -48,6 +48,10 @@ int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			unsigned long *, int *, int, unsigned int flags);
 void unmap_hugepage_range(struct vm_area_struct *,
 			  unsigned long, unsigned long, struct page *);
+void __unmap_hugepage_range_final(struct mmu_gather *tlb,
+			  struct vm_area_struct *vma,
+			  unsigned long start, unsigned long end,
+			  struct page *ref_page);
 void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 				unsigned long start, unsigned long end,
 				struct page *ref_page);
@@ -130,6 +134,13 @@ static inline void copy_huge_page(struct page *dst, struct page *src)
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 
+static inline void __unmap_hugepage_range_final(struct mmu_gather *tlb,
+			struct vm_area_struct *vma, unsigned long start,
+			unsigned long end, struct page *ref_page)
+{
+	BUG();
+}
+
 static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
 			struct vm_area_struct *vma, unsigned long start,
 			unsigned long end, struct page *ref_page)

commit abb8206cb07734d0b7bf033c715995d6371a94c3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:24 2012 -0700

    hugetlb/cgroup: add hugetlb cgroup control files
    
    Add the control files for hugetlb controller
    
    [akpm@linux-foundation.org: s/CONFIG_CGROUP_HUGETLB_RES_CTLR/CONFIG_MEMCG_HUGETLB/g]
    [akpm@linux-foundation.org: s/CONFIG_MEMCG_HUGETLB/CONFIG_CGROUP_HUGETLB/]
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 3d677dd41898..f9db20bfa9fc 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -4,6 +4,7 @@
 #include <linux/mm_types.h>
 #include <linux/fs.h>
 #include <linux/hugetlb_inline.h>
+#include <linux/cgroup.h>
 
 struct ctl_table;
 struct user_struct;
@@ -221,6 +222,10 @@ struct hstate {
 	unsigned int nr_huge_pages_node[MAX_NUMNODES];
 	unsigned int free_huge_pages_node[MAX_NUMNODES];
 	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
+#ifdef CONFIG_CGROUP_HUGETLB
+	/* cgroup control files */
+	struct cftype cgroup_files[5];
+#endif
 	char name[HSTATE_NAME_LEN];
 };
 

commit c3f38a38715e9b4e7b1dda6840a1375f6894744d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:10 2012 -0700

    hugetlb: make some static variables global
    
    We will use them later in hugetlb_cgroup.c
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ed550d819044..3d677dd41898 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -21,6 +21,11 @@ struct hugepage_subpool {
 	long max_hpages, used_hpages;
 };
 
+extern spinlock_t hugetlb_lock;
+extern int hugetlb_max_hstate __read_mostly;
+#define for_each_hstate(h) \
+	for ((h) = hstates; (h) < &hstates[hugetlb_max_hstate]; (h)++)
+
 struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
 void hugepage_put_subpool(struct hugepage_subpool *spool);
 

commit 0edaecfab218d747d30de4575e911907371e2cd2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:07 2012 -0700

    hugetlb: add a list for tracking in-use HugeTLB pages
    
    hugepage_activelist will be used to track currently used HugeTLB pages.
    We need to find the in-use HugeTLB pages to support HugeTLB cgroup removal.
    On cgroup removal we update the page's HugeTLB cgroup to point to parent
    cgroup.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0f23c1840c9b..ed550d819044 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -211,6 +211,7 @@ struct hstate {
 	unsigned long resv_huge_pages;
 	unsigned long surplus_huge_pages;
 	unsigned long nr_overcommit_huge_pages;
+	struct list_head hugepage_activelist;
 	struct list_head hugepage_freelists[MAX_NUMNODES];
 	unsigned int nr_huge_pages_node[MAX_NUMNODES];
 	unsigned int free_huge_pages_node[MAX_NUMNODES];

commit 24669e58477e2752c1fbca9c1c988e9dd0d79d15
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:03 2012 -0700

    hugetlb: use mmu_gather instead of a temporary linked list for accumulating pages
    
    Use a mmu_gather instead of a temporary linked list for accumulating pages
    when we unmap a hugepage range
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 217f52859fa7..0f23c1840c9b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -7,6 +7,7 @@
 
 struct ctl_table;
 struct user_struct;
+struct mmu_gather;
 
 #ifdef CONFIG_HUGETLB_PAGE
 
@@ -40,9 +41,10 @@ int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			struct page **, struct vm_area_struct **,
 			unsigned long *, int *, int, unsigned int flags);
 void unmap_hugepage_range(struct vm_area_struct *,
-			unsigned long, unsigned long, struct page *);
-void __unmap_hugepage_range(struct vm_area_struct *,
-			unsigned long, unsigned long, struct page *);
+			  unsigned long, unsigned long, struct page *);
+void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
+				unsigned long start, unsigned long end,
+				struct page *ref_page);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
@@ -98,7 +100,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
-#define unmap_hugepage_range(vma, start, end, page)	BUG()
 static inline void hugetlb_report_meminfo(struct seq_file *m)
 {
 }
@@ -112,13 +113,24 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address)	0
-#define dequeue_hwpoisoned_huge_page(page)	0
+static inline int dequeue_hwpoisoned_huge_page(struct page *page)
+{
+	return 0;
+}
+
 static inline void copy_huge_page(struct page *dst, struct page *src)
 {
 }
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 
+static inline void __unmap_hugepage_range(struct mmu_gather *tlb,
+			struct vm_area_struct *vma, unsigned long start,
+			unsigned long end, struct page *ref_page)
+{
+	BUG();
+}
+
 #endif /* !CONFIG_HUGETLB_PAGE */
 
 #define HUGETLB_ANON_FILE "anon_hugepage"

commit 972dc4de13f667a7df27ee32573b2e6fc6cc8434
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:42:00 2012 -0700

    hugetlb: add an inline helper for finding hstate index
    
    Add an inline helper and use it in the code.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d5d6bbe2259e..217f52859fa7 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -302,6 +302,11 @@ static inline unsigned hstate_index_to_shift(unsigned index)
 	return hstates[index].order + PAGE_SHIFT;
 }
 
+static inline int hstate_index(struct hstate *h)
+{
+	return h - hstates;
+}
+
 #else
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
@@ -320,6 +325,7 @@ static inline unsigned int pages_per_huge_page(struct hstate *h)
 	return 1;
 }
 #define hstate_index_to_shift(index) 0
+#define hstate_index(h) 0
 #endif
 
 #endif /* _LINUX_HUGETLB_H */

commit d9ed9faac283a3be73f0e11a2ef49ee55aece4db
Author: Chris Metcalf <cmetcalf@tilera.com>
Date:   Sun Apr 1 14:01:34 2012 -0400

    mm: add new arch_make_huge_pte() method for tile support
    
    The tile support for multiple-size huge pages requires tagging
    the hugetlb PTE with a "super" bit for PTEs that are multiples of
    the basic size of a pagetable span.  To set that bit properly
    we need to tweak the PTe in make_huge_pte() based on the vma.
    
    This change provides the API for a subsequent tile-specific
    change to use.
    
    Reviewed-by: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Chris Metcalf <cmetcalf@tilera.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 000837e126e6..d5d6bbe2259e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -284,6 +284,14 @@ static inline unsigned int blocks_per_huge_page(struct hstate *h)
 
 #include <asm/hugetlb.h>
 
+#ifndef arch_make_huge_pte
+static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
+				       struct page *page, int writable)
+{
+	return entry;
+}
+#endif
+
 static inline struct hstate *page_hstate(struct page *page)
 {
 	return size_to_hstate(PAGE_SIZE << compound_order(page));

commit 40716e29243de46720e5773797791466c28904ec
Author: Steven Truelove <steven.truelove@utoronto.ca>
Date:   Wed Mar 21 16:34:14 2012 -0700

    hugetlbfs: fix alignment of huge page requests
    
    When calling shmget() with SHM_HUGETLB, shmget aligns the request size to
    PAGE_SIZE, but this is not sufficient.
    
    Modify hugetlb_file_setup() to align requests to the huge page size, and
    to accept an address argument so that all alignment checks can be
    performed in hugetlb_file_setup(), rather than in its callers.  Change
    newseg() and mmap_pgoff() to match the new prototype and eliminate a now
    redundant alignment check.
    
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Steven Truelove <steven.truelove@utoronto.ca>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index cf0181738c9e..000837e126e6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -152,7 +152,8 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
+struct file *hugetlb_file_setup(const char *name, unsigned long addr,
+				size_t size, vm_flags_t acct,
 				struct user_struct **user, int creat_flags);
 
 static inline int is_file_hugepages(struct file *file)
@@ -168,7 +169,8 @@ static inline int is_file_hugepages(struct file *file)
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)			0
-static inline struct file *hugetlb_file_setup(const char *name, size_t size,
+static inline struct file *
+hugetlb_file_setup(const char *name, unsigned long addr, size_t size,
 		vm_flags_t acctflag, struct user_struct **user, int creat_flags)
 {
 	return ERR_PTR(-ENOSYS);

commit 90481622d75715bfcb68501280a917dbfe516029
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 21 16:34:12 2012 -0700

    hugepages: fix use after free bug in "quota" handling
    
    hugetlbfs_{get,put}_quota() are badly named.  They don't interact with the
    general quota handling code, and they don't much resemble its behaviour.
    Rather than being about maintaining limits on on-disk block usage by
    particular users, they are instead about maintaining limits on in-memory
    page usage (including anonymous MAP_PRIVATE copied-on-write pages)
    associated with a particular hugetlbfs filesystem instance.
    
    Worse, they work by having callbacks to the hugetlbfs filesystem code from
    the low-level page handling code, in particular from free_huge_page().
    This is a layering violation of itself, but more importantly, if the
    kernel does a get_user_pages() on hugepages (which can happen from KVM
    amongst others), then the free_huge_page() can be delayed until after the
    associated inode has already been freed.  If an unmount occurs at the
    wrong time, even the hugetlbfs superblock where the "quota" limits are
    stored may have been freed.
    
    Andrew Barry proposed a patch to fix this by having hugepages, instead of
    storing a pointer to their address_space and reaching the superblock from
    there, had the hugepages store pointers directly to the superblock,
    bumping the reference count as appropriate to avoid it being freed.
    Andrew Morton rejected that version, however, on the grounds that it made
    the existing layering violation worse.
    
    This is a reworked version of Andrew's patch, which removes the extra, and
    some of the existing, layering violation.  It works by introducing the
    concept of a hugepage "subpool" at the lower hugepage mm layer - that is a
    finite logical pool of hugepages to allocate from.  hugetlbfs now creates
    a subpool for each filesystem instance with a page limit set, and a
    pointer to the subpool gets added to each allocated hugepage, instead of
    the address_space pointer used now.  The subpool has its own lifetime and
    is only freed once all pages in it _and_ all other references to it (i.e.
    superblocks) are gone.
    
    subpools are optional - a NULL subpool pointer is taken by the code to
    mean that no subpool limits are in effect.
    
    Previous discussion of this bug found in:  "Fix refcounting in hugetlbfs
    quota handling.". See:  https://lkml.org/lkml/2011/8/11/28 or
    http://marc.info/?l=linux-mm&m=126928970510627&w=1
    
    v2: Fixed a bug spotted by Hillf Danton, and removed the extra parameter to
    alloc_huge_page() - since it already takes the vma, it is not necessary.
    
    Signed-off-by: Andrew Barry <abarry@cray.com>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7adc4923e7ac..cf0181738c9e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -14,6 +14,15 @@ struct user_struct;
 #include <linux/shm.h>
 #include <asm/tlbflush.h>
 
+struct hugepage_subpool {
+	spinlock_t lock;
+	long count;
+	long max_hpages, used_hpages;
+};
+
+struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
+void hugepage_put_subpool(struct hugepage_subpool *spool);
+
 int PageHuge(struct page *page);
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
@@ -129,12 +138,11 @@ enum {
 
 #ifdef CONFIG_HUGETLBFS
 struct hugetlbfs_sb_info {
-	long	max_blocks;   /* blocks allowed */
-	long	free_blocks;  /* blocks free */
 	long	max_inodes;   /* inodes allowed */
 	long	free_inodes;  /* inodes free */
 	spinlock_t	stat_lock;
 	struct hstate *hstate;
+	struct hugepage_subpool *spool;
 };
 
 static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
@@ -146,8 +154,6 @@ extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 				struct user_struct **user, int creat_flags);
-int hugetlb_get_quota(struct address_space *mapping, long delta);
-void hugetlb_put_quota(struct address_space *mapping, long delta);
 
 static inline int is_file_hugepages(struct file *file)
 {

commit a1d776ee3147cec2a54a645e92eb2e3e2f65a137
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 21 16:34:12 2012 -0700

    hugetlb: cleanup hugetlb.h
    
    Make a couple of small cleanups to linux/include/hugetlb.h.  The
    set_file_hugepages() function, which was not used anywhere is removed,
    and the hugetlbfs_config and hugetlbfs_inode_info structures with its
    HUGETLBFS_I helper function are moved into inode.c, the only place they
    were used.
    
    These structures are really linked to the hugetlbfs filesystem
    specifically not to hugepage mm handling in general, so they belong in
    the filesystem code not in a generally available header.
    
    It would be nice to move the hugetlbfs_sb_info (superblock) structure in
    there as well, but it's currently needed in a number of places via the
    hstate_vma() and hstate_inode().
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Andrew Barry <abarry@cray.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d9d6c868b86b..7adc4923e7ac 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -128,15 +128,6 @@ enum {
 };
 
 #ifdef CONFIG_HUGETLBFS
-struct hugetlbfs_config {
-	uid_t   uid;
-	gid_t   gid;
-	umode_t mode;
-	long	nr_blocks;
-	long	nr_inodes;
-	struct hstate *hstate;
-};
-
 struct hugetlbfs_sb_info {
 	long	max_blocks;   /* blocks allowed */
 	long	free_blocks;  /* blocks free */
@@ -146,17 +137,6 @@ struct hugetlbfs_sb_info {
 	struct hstate *hstate;
 };
 
-
-struct hugetlbfs_inode_info {
-	struct shared_policy policy;
-	struct inode vfs_inode;
-};
-
-static inline struct hugetlbfs_inode_info *HUGETLBFS_I(struct inode *inode)
-{
-	return container_of(inode, struct hugetlbfs_inode_info, vfs_inode);
-}
-
 static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 {
 	return sb->s_fs_info;
@@ -179,14 +159,9 @@ static inline int is_file_hugepages(struct file *file)
 	return 0;
 }
 
-static inline void set_file_hugepages(struct file *file)
-{
-	file->f_op = &hugetlbfs_file_operations;
-}
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)			0
-#define set_file_hugepages(file)		BUG()
 static inline struct file *hugetlb_file_setup(const char *name, size_t size,
 		vm_flags_t acctflag, struct user_struct **user, int creat_flags)
 {

commit a5c86e986f0b2fe779f13cf53ce6e9f467b03950
Author: David Rientjes <rientjes@google.com>
Date:   Sat Nov 19 02:33:57 2011 -0800

    hugetlb: remove dummy definitions of HPAGE_MASK and HPAGE_SIZE
    
    Dummy, non-zero definitions for HPAGE_MASK and HPAGE_SIZE were added in
    51c6f666fceb ("mm: ZAP_BLOCK causes redundant work") to avoid a divide
    by zero in generic kernel code.
    
    That code has since been removed, but probably should never have been
    added in the first place: we don't want HPAGE_SIZE to act like PAGE_SIZE
    for code that is working with hugepages, for example, when the
    dependency on CONFIG_HUGETLB_PAGE has not been fulfilled.
    
    Because hugepage size can differ from architecture to architecture, each
    is required to have their own definitions for both HPAGE_MASK and
    HPAGE_SIZE.  This is always done in arch/*/include/asm/page.h.
    
    So, just remove the dummy and dangerous definitions since they are no
    longer needed and reveals the correct dependencies.  Tested on
    architectures using the definitions with allyesconfig: x86 (even with
    thp), hppa, mips, powerpc, s390, sh3, sh4, sparc, and sparc64, and with
    defconfig on ia64.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 19644e0016bd..d9d6c868b86b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -110,11 +110,6 @@ static inline void copy_huge_page(struct page *dst, struct page *src)
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 
-#ifndef HPAGE_MASK
-#define HPAGE_MASK	PAGE_MASK		/* Keep the compiler happy */
-#define HPAGE_SIZE	PAGE_SIZE
-#endif
-
 #endif /* !CONFIG_HUGETLB_PAGE */
 
 #define HUGETLB_ANON_FILE "anon_hugepage"

commit ee8f248d266ec6966c0ce6b7dec24de43dcc1b58
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Jul 25 17:11:50 2011 -0700

    hugetlb: add phys addr to struct huge_bootmem_page
    
    This is needed on HIGHMEM systems - we don't always have a virtual
    address so store the physical address and map it in as needed.
    
    [akpm@linux-foundation.org: cleanup]
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 59225ef27d15..19644e0016bd 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -231,6 +231,9 @@ struct hstate {
 struct huge_bootmem_page {
 	struct list_head list;
 	struct hstate *hstate;
+#ifdef CONFIG_HIGHMEM
+	phys_addr_t phys;
+#endif
 };
 
 struct page *alloc_huge_page_node(struct hstate *h, int nid);

commit be93d8cfbae1996052e91b2883d306a5d9d0fe18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 26 12:03:50 2011 -0700

    Fix build with !HUGETLBFS
    
    I stupidly broke the case of CONFIG_HUGETLBFS=n when doing the
    conversion to vm_flags_t in commit ca16d140af91 ("mm: don't access
    vm_flags as 'int'").  And my 'allyesconfig' build didn't find it, for
    obvious reasons..
    
    Include <linux/mm_types.h> in <linux/hugetlb.h>.  The problem could have
    been avoided by just turning the hugetlb_file_setup() error wrapper into
    a macro, but mm_types.h is a reasonable include in this file.
    
    Reported-by: Richard -rw- Weinberger <richard.weinberger@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index cf8931e1dd9d..59225ef27d15 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -1,6 +1,7 @@
 #ifndef _LINUX_HUGETLB_H
 #define _LINUX_HUGETLB_H
 
+#include <linux/mm_types.h>
 #include <linux/fs.h>
 #include <linux/hugetlb_inline.h>
 

commit ca16d140af91febe25daeb9e032bf8bd46b8c31f
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu May 26 19:16:19 2011 +0900

    mm: don't access vm_flags as 'int'
    
    The type of vma->vm_flags is 'unsigned long'. Neither 'int' nor
    'unsigned int'. This patch fixes such misuse.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    [ Changed to use a typedef - we'll extend it to cover more cases
      later, since there has been discussion about making it a 64-bit
      type..                      - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 943c76b3d4bb..cf8931e1dd9d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -41,7 +41,7 @@ int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, unsigned int flags);
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
-						int acctflags);
+						vm_flags_t vm_flags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 int dequeue_hwpoisoned_huge_page(struct page *page);
 void copy_huge_page(struct page *dst, struct page *src);
@@ -168,7 +168,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern const struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_file_setup(const char *name, size_t size, int acct,
+struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 				struct user_struct **user, int creat_flags);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
@@ -192,7 +192,7 @@ static inline void set_file_hugepages(struct file *file)
 #define is_file_hugepages(file)			0
 #define set_file_hugepages(file)		BUG()
 static inline struct file *hugetlb_file_setup(const char *name, size_t size,
-		int acctflag, struct user_struct **user, int creat_flags)
+		vm_flags_t acctflag, struct user_struct **user, int creat_flags)
 {
 	return ERR_PTR(-ENOSYS);
 }

commit aa50d3a7aa8147b9e14dc9d5972a5d2359db4ef8
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Oct 6 21:45:00 2010 +0200

    Encode huge page size for VM_FAULT_HWPOISON errors
    
    This fixes a problem introduced with the hugetlb hwpoison handling
    
    The user space SIGBUS signalling wants to know the size of the hugepage
    that caused a HWPOISON fault.
    
    Unfortunately the architecture page fault handlers do not have easy
    access to the struct page.
    
    Pass the information out in the fault error code instead.
    
    I added a separate VM_FAULT_HWPOISON_LARGE bit for this case and encode
    the hpage index in some free upper bits of the fault code. The small
    page hwpoison keeps stays with the VM_FAULT_HWPOISON name to minimize
    changes.
    
    Also add code to hugetlb.h to convert that index into a page shift.
    
    Will be used in a further patch.
    
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: fengguang.wu@intel.com
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 796f30e00806..943c76b3d4bb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -307,6 +307,11 @@ static inline struct hstate *page_hstate(struct page *page)
 	return size_to_hstate(PAGE_SIZE << compound_order(page));
 }
 
+static inline unsigned hstate_index_to_shift(unsigned index)
+{
+	return hstates[index].order + PAGE_SHIFT;
+}
+
 #else
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
@@ -324,6 +329,7 @@ static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1;
 }
+#define hstate_index_to_shift(index) 0
 #endif
 
 #endif /* _LINUX_HUGETLB_H */

commit 6de2b1aab94355482bd2accdc115666509667458
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 8 10:19:36 2010 +0900

    HWPOISON, hugetlb: add free check to dequeue_hwpoison_huge_page()
    
    This check is necessary to avoid race between dequeue and allocation,
    which can cause a free hugepage to be dequeued twice and get kernel unstable.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 9e51f77d44ca..796f30e00806 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -43,7 +43,7 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						int acctflags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
-void __isolate_hwpoisoned_huge_page(struct page *page);
+int dequeue_hwpoisoned_huge_page(struct page *page);
 void copy_huge_page(struct page *dst, struct page *src);
 
 extern unsigned long hugepages_treat_as_movable;
@@ -102,7 +102,7 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address)	0
-#define __isolate_hwpoisoned_huge_page(page)	0
+#define dequeue_hwpoisoned_huge_page(page)	0
 static inline void copy_huge_page(struct page *dst, struct page *src)
 {
 }

commit 0ebabb416f585ace711769057422af4bbc9d1110
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 8 10:19:34 2010 +0900

    hugetlb: redefine hugepage copy functions
    
    This patch modifies hugepage copy functions to have only destination
    and source hugepages as arguments for later use.
    The old ones are renamed from copy_{gigantic,huge}_page() to
    copy_user_{gigantic,huge}_page().
    This naming convention is consistent with that between copy_highpage()
    and copy_user_highpage().
    
    ChangeLog since v4:
    - add blank line between local declaration and code
    - remove unnecessary might_sleep()
    
    ChangeLog since v2:
    - change copy_huge_page() from macro to inline dummy function
      to avoid compile warning when !CONFIG_HUGETLB_PAGE.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0b73c536afd2..9e51f77d44ca 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -44,6 +44,7 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						int acctflags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 void __isolate_hwpoisoned_huge_page(struct page *page);
+void copy_huge_page(struct page *dst, struct page *src);
 
 extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
@@ -102,6 +103,9 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address)	0
 #define __isolate_hwpoisoned_huge_page(page)	0
+static inline void copy_huge_page(struct page *dst, struct page *src)
+{
+}
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 

commit bf50bab2b34483316162443587b8467952e07730
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 8 10:19:33 2010 +0900

    hugetlb: add allocate function for hugepage migration
    
    We can't use existing hugepage allocation functions to allocate hugepage
    for page migration, because page migration can happen asynchronously with
    the running processes and page migration users should call the allocation
    function with physical addresses (not virtual addresses) as arguments.
    
    ChangeLog since v3:
    - unify alloc_buddy_huge_page() and alloc_buddy_huge_page_node()
    
    ChangeLog since v2:
    - remove unnecessary get/put_mems_allowed() (thanks to David Rientjes)
    
    ChangeLog since v1:
    - add comment on top of alloc_huge_page_no_vma()
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f479700df61b..0b73c536afd2 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -228,6 +228,8 @@ struct huge_bootmem_page {
 	struct hstate *hstate;
 };
 
+struct page *alloc_huge_page_node(struct hstate *h, int nid);
+
 /* arch callback */
 int __init alloc_bootmem_huge_page(struct hstate *h);
 
@@ -303,6 +305,7 @@ static inline struct hstate *page_hstate(struct page *page)
 
 #else
 struct hstate {};
+#define alloc_huge_page_node(h, nid) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
 #define hstate_vma(v) NULL

commit 93f70f900da36fbc19c13c2aa04b2e468c8d00fb
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Fri May 28 09:29:20 2010 +0900

    HWPOISON, hugetlb: isolate corrupted hugepage
    
    If error hugepage is not in-use, we can fully recovery from error
    by dequeuing it from freelist, so return RECOVERY.
    Otherwise whether or not we can recovery depends on user processes,
    so return DELAYED.
    
    Dependency:
      "HWPOISON, hugetlb: enable error handling path for hugepage"
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e688fd89354d..f479700df61b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -43,6 +43,7 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						int acctflags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
+void __isolate_hwpoisoned_huge_page(struct page *page);
 
 extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
@@ -100,6 +101,7 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 #define huge_pte_offset(mm, address)	0
+#define __isolate_hwpoisoned_huge_page(page)	0
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 

commit 0fe6e20b9c4c53b3e97096ee73a0857f60aad43f
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Fri May 28 09:29:16 2010 +0900

    hugetlb, rmap: add reverse mapping for hugepage
    
    This patch adds reverse mapping feature for hugepage by introducing
    mapcount for shared/private-mapped hugepage and anon_vma for
    private-mapped hugepage.
    
    While hugepage is not currently swappable, reverse mapping can be useful
    for memory error handler.
    
    Without this patch, memory error handler cannot identify processes
    using the bad hugepage nor unmap it from them. That is:
    - for shared hugepage:
      we can collect processes using a hugepage through pagecache,
      but can not unmap the hugepage because of the lack of mapcount.
    - for privately mapped hugepage:
      we can neither collect processes nor unmap the hugepage.
    This patch solves these problems.
    
    This patch include the bug fix given by commit 23be7468e8, so reverts it.
    
    Dependency:
      "hugetlb: move definition of is_vm_hugetlb_page() to hugepage_inline.h"
    
    ChangeLog since May 24.
    - create hugetlb_inline.h and move is_vm_hugetlb_index() in it.
    - move functions setting up anon_vma for hugepage into mm/rmap.c.
    
    ChangeLog since May 13.
    - rebased to 2.6.34
    - fix logic error (in case that private mapping and shared mapping coexist)
    - move is_vm_hugetlb_page() into include/linux/mm.h to use this function
      from linear_page_index()
    - define and use linear_hugepage_index() instead of compound_order()
    - use page_move_anon_rmap() in hugetlb_cow()
    - copy exclusive switch of __set_page_anon_rmap() into hugepage counterpart.
    - revert commit 24be7468 completely
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Acked-by: Fengguang Wu <fengguang.wu@intel.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d47a7c41745d..e688fd89354d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -99,6 +99,7 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
+#define huge_pte_offset(mm, address)	0
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 

commit 8edf344c66a3f214d709dad1421c29d678915b3f
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Fri May 28 09:29:15 2010 +0900

    hugetlb: move definition of is_vm_hugetlb_page() to hugepage_inline.h
    
    is_vm_hugetlb_page() is a widely used inline function to insert hooks
    into hugetlb code.
    But we can't use it in pagemap.h because of circular dependency of
    the header files. This patch removes this limitation.
    
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 78b4bc64c006..d47a7c41745d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -2,6 +2,7 @@
 #define _LINUX_HUGETLB_H
 
 #include <linux/fs.h>
+#include <linux/hugetlb_inline.h>
 
 struct ctl_table;
 struct user_struct;
@@ -14,11 +15,6 @@ struct user_struct;
 
 int PageHuge(struct page *page);
 
-static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
-{
-	return vma->vm_flags & VM_HUGETLB;
-}
-
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
 int hugetlb_overcommit_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
@@ -77,11 +73,6 @@ static inline int PageHuge(struct page *page)
 	return 0;
 }
 
-static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
-{
-	return 0;
-}
-
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
 }

commit 06808b0827e1cd14eedc96bac2655d5b37ac246c
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Dec 14 17:58:21 2009 -0800

    hugetlb: derive huge pages nodes allowed from task mempolicy
    
    This patch derives a "nodes_allowed" node mask from the numa mempolicy of
    the task modifying the number of persistent huge pages to control the
    allocation, freeing and adjusting of surplus huge pages when the pool page
    count is modified via the new sysctl or sysfs attribute
    "nr_hugepages_mempolicy".  The nodes_allowed mask is derived as follows:
    
    * For "default" [NULL] task mempolicy, a NULL nodemask_t pointer
      is produced.  This will cause the hugetlb subsystem to use
      node_online_map as the "nodes_allowed".  This preserves the
      behavior before this patch.
    * For "preferred" mempolicy, including explicit local allocation,
      a nodemask with the single preferred node will be produced.
      "local" policy will NOT track any internode migrations of the
      task adjusting nr_hugepages.
    * For "bind" and "interleave" policy, the mempolicy's nodemask
      will be used.
    * Other than to inform the construction of the nodes_allowed node
      mask, the actual mempolicy mode is ignored.  That is, all modes
      behave like interleave over the resulting nodes_allowed mask
      with no "fallback".
    
    See the updated documentation [next patch] for more information
    about the implications of this patch.
    
    Examples:
    
    Starting with:
    
            Node 0 HugePages_Total:     0
            Node 1 HugePages_Total:     0
            Node 2 HugePages_Total:     0
            Node 3 HugePages_Total:     0
    
    Default behavior [with or without this patch] balances persistent
    hugepage allocation across nodes [with sufficient contiguous memory]:
    
            sysctl vm.nr_hugepages[_mempolicy]=32
    
    yields:
    
            Node 0 HugePages_Total:     8
            Node 1 HugePages_Total:     8
            Node 2 HugePages_Total:     8
            Node 3 HugePages_Total:     8
    
    Of course, we only have nr_hugepages_mempolicy with the patch,
    but with default mempolicy, nr_hugepages_mempolicy behaves the
    same as nr_hugepages.
    
    Applying mempolicy--e.g., with numactl [using '-m' a.k.a.
    '--membind' because it allows multiple nodes to be specified
    and it's easy to type]--we can allocate huge pages on
    individual nodes or sets of nodes.  So, starting from the
    condition above, with 8 huge pages per node, add 8 more to
    node 2 using:
    
            numactl -m 2 sysctl vm.nr_hugepages_mempolicy=40
    
    This yields:
    
            Node 0 HugePages_Total:     8
            Node 1 HugePages_Total:     8
            Node 2 HugePages_Total:    16
            Node 3 HugePages_Total:     8
    
    The incremental 8 huge pages were restricted to node 2 by the
    specified mempolicy.
    
    Similarly, we can use mempolicy to free persistent huge pages
    from specified nodes:
    
            numactl -m 0,1 sysctl vm.nr_hugepages_mempolicy=32
    
    yields:
    
            Node 0 HugePages_Total:     4
            Node 1 HugePages_Total:     4
            Node 2 HugePages_Total:    16
            Node 3 HugePages_Total:     8
    
    The 8 huge pages freed were balanced over nodes 0 and 1.
    
    [rientjes@google.com: accomodate reworked NODEMASK_ALLOC]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Andi Kleen <andi@firstfloor.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@canonical.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 41a59afc70fa..78b4bc64c006 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -23,6 +23,12 @@ void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
 int hugetlb_overcommit_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
 int hugetlb_treat_movable_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
+
+#ifdef CONFIG_NUMA
+int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
+					void __user *, size_t *, loff_t *);
+#endif
+
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			struct page **, struct vm_area_struct **,

commit f0f37e2f77731b3473fa6bd5ee53255d9a9cdb40
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sun Sep 27 22:29:37 2009 +0400

    const: mark struct vm_struct_operations
    
    * mark struct vm_area_struct::vm_ops as const
    * mark vm_ops in AGP code
    
    But leave TTM code alone, something is fishy there with global vm_ops
    being used.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 16937995abd4..41a59afc70fa 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -163,7 +163,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 }
 
 extern const struct file_operations hugetlbfs_file_operations;
-extern struct vm_operations_struct hugetlb_vm_ops;
+extern const struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, size_t size, int acct,
 				struct user_struct **user, int creat_flags);
 int hugetlb_get_quota(struct address_space *mapping, long delta);

commit e9ea0e2d1d00959b451dfc239df126d3c6a22043
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Sep 24 14:47:45 2009 -0700

    hugetlb_file_setup(): use C, not cpp
    
    Why macros are always wrong:
    
      mm/mmap.c: In function 'do_mmap_pgoff':
      mm/mmap.c:953: warning: unused variable 'user'
    
    also, move a couple of struct forward-decls outside `#ifdef
    CONFIG_HUGETLB_PAGE' - it's pointless and frequently harmful to make these
    conditional (eg, this patch needed `struct user_struct').
    
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@canonical.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: Eric B Munson <ebmunson@us.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 11ab19ac6b3d..16937995abd4 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -3,15 +3,15 @@
 
 #include <linux/fs.h>
 
+struct ctl_table;
+struct user_struct;
+
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>
 #include <linux/shm.h>
 #include <asm/tlbflush.h>
 
-struct ctl_table;
-struct user_struct;
-
 int PageHuge(struct page *page);
 
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
@@ -187,7 +187,11 @@ static inline void set_file_hugepages(struct file *file)
 
 #define is_file_hugepages(file)			0
 #define set_file_hugepages(file)		BUG()
-#define hugetlb_file_setup(name,size,acct,user,creat)	ERR_PTR(-ENOSYS)
+static inline struct file *hugetlb_file_setup(const char *name, size_t size,
+		int acctflag, struct user_struct **user, int creat_flags)
+{
+	return ERR_PTR(-ENOSYS);
+}
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 176e7ee73eff..11ab19ac6b3d 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -20,9 +20,9 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 }
 
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
-int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
-int hugetlb_overcommit_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
-int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
+int hugetlb_sysctl_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
+int hugetlb_overcommit_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
+int hugetlb_treat_movable_handler(struct ctl_table *, int, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			struct page **, struct vm_area_struct **,

commit 4e52780d41a741fb4861ae1df2413dd816ec11b1
Author: Eric B Munson <ebmunson@us.ibm.com>
Date:   Mon Sep 21 17:03:47 2009 -0700

    hugetlb: add MAP_HUGETLB for mmaping pseudo-anonymous huge page regions
    
    Add a flag for mmap that will be used to request a huge page region that
    will look like anonymous memory to userspace.  This is accomplished by
    using a file on the internal vfsmount.  MAP_HUGETLB is a modifier of
    MAP_ANONYMOUS and so must be specified with it.  The region will behave
    the same as a MAP_ANONYMOUS region using small pages.
    
    [akpm@linux-foundation.org: fix arch definitions of MAP_HUGETLB]
    Signed-off-by: Eric B Munson <ebmunson@us.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f6505ad86657..176e7ee73eff 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -112,12 +112,19 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 
 #endif /* !CONFIG_HUGETLB_PAGE */
 
+#define HUGETLB_ANON_FILE "anon_hugepage"
+
 enum {
 	/*
 	 * The file will be used as an shm file so shmfs accounting rules
 	 * apply
 	 */
 	HUGETLB_SHMFS_INODE     = 1,
+	/*
+	 * The file is being created on the internal vfs mount and shmfs
+	 * accounting rules do not apply
+	 */
+	HUGETLB_ANONHUGE_INODE  = 2,
 };
 
 #ifdef CONFIG_HUGETLBFS

commit 6bfde05bf5c9682e255c6a2c669dc80f91af6296
Author: Eric B Munson <ebmunson@us.ibm.com>
Date:   Mon Sep 21 17:03:43 2009 -0700

    hugetlbfs: allow the creation of files suitable for MAP_PRIVATE on the vfs internal mount
    
    This patchset adds a flag to mmap that allows the user to request that an
    anonymous mapping be backed with huge pages.  This mapping will borrow
    functionality from the huge page shm code to create a file on the kernel
    internal mount and use it to approximate an anonymous mapping.  The
    MAP_HUGETLB flag is a modifier to MAP_ANONYMOUS and will not work without
    both flags being preset.
    
    A new flag is necessary because there is no other way to hook into huge
    pages without creating a file on a hugetlbfs mount which wouldn't be
    MAP_ANONYMOUS.
    
    To userspace, this mapping will behave just like an anonymous mapping
    because the file is not accessible outside of the kernel.
    
    This patchset is meant to simplify the programming model.  Presently there
    is a large chunk of boiler platecode, contained in libhugetlbfs, required
    to create private, hugepage backed mappings.  This patch set would allow
    use of hugepages without linking to libhugetlbfs or having hugetblfs
    mounted.
    
    Unification of the VM code would provide these same benefits, but it has
    been resisted each time that it has been suggested for several reasons: it
    would break PAGE_SIZE assumptions across the kernel, it makes page-table
    abstractions really expensive, and it does not provide any benefit on
    architectures that do not support huge pages, incurring fast path
    penalties without providing any benefit on these architectures.
    
    This patch:
    
    There are two means of creating mappings backed by huge pages:
    
            1. mmap() a file created on hugetlbfs
            2. Use shm which creates a file on an internal mount which essentially
               maps it MAP_SHARED
    
    The internal mount is only used for shared mappings but there is very
    little that stops it being used for private mappings. This patch extends
    hugetlbfs_file_setup() to deal with the creation of files that will be
    mapped MAP_PRIVATE on the internal hugetlbfs mount. This extended API is
    used in a subsequent patch to implement the MAP_HUGETLB mmap() flag.
    
    Signed-off-by: Eric Munson <ebmunson@us.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e7f0fabfa1c2..f6505ad86657 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -112,6 +112,14 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 
 #endif /* !CONFIG_HUGETLB_PAGE */
 
+enum {
+	/*
+	 * The file will be used as an shm file so shmfs accounting rules
+	 * apply
+	 */
+	HUGETLB_SHMFS_INODE     = 1,
+};
+
 #ifdef CONFIG_HUGETLBFS
 struct hugetlbfs_config {
 	uid_t   uid;
@@ -150,7 +158,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, size_t size, int acct,
-						struct user_struct **user);
+				struct user_struct **user, int creat_flags);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
 
@@ -172,7 +180,7 @@ static inline void set_file_hugepages(struct file *file)
 
 #define is_file_hugepages(file)			0
 #define set_file_hugepages(file)		BUG()
-#define hugetlb_file_setup(name,size,acct,user)	ERR_PTR(-ENOSYS)
+#define hugetlb_file_setup(name,size,acct,user,creat)	ERR_PTR(-ENOSYS)
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 2a15efc953b26ad57d7d38b9e6782d57e53b4ab2
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:03:27 2009 -0700

    mm: follow_hugetlb_page flags
    
    follow_hugetlb_page() shouldn't be guessing about the coredump case
    either: pass the foll_flags down to it, instead of just the write bit.
    
    Remove that obscure huge_zeropage_ok() test.  The decision is easy,
    though unlike the non-huge case - here vm_ops->fault is always set.
    But we know that a fault would serve up zeroes, unless there's
    already a hugetlbfs pagecache page to back the range.
    
    (Alternatively, since hugetlb pages aren't swapped out under pressure,
    you could save more dump space by arguing that a page not yet faulted
    into this process cannot be relevant to the dump; but that would be
    more surprising.)
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 16cdb75a543a..e7f0fabfa1c2 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -24,7 +24,9 @@ int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *
 int hugetlb_overcommit_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
+			struct page **, struct vm_area_struct **,
+			unsigned long *, int *, int, unsigned int flags);
 void unmap_hugepage_range(struct vm_area_struct *,
 			unsigned long, unsigned long, struct page *);
 void __unmap_hugepage_range(struct vm_area_struct *,

commit e8c5c8249878fb6564125680a1d15e06adbd5639
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Sep 21 17:01:22 2009 -0700

    hugetlb: balance freeing of huge pages across nodes
    
    Free huges pages from nodes in round robin fashion in an attempt to keep
    [persistent a.k.a static] hugepages balanced across nodes
    
    New function free_pool_huge_page() is modeled on and performs roughly the
    inverse of alloc_fresh_huge_page().  Replaces dequeue_huge_page() which
    now has no callers, so this patch removes it.
    
    Helper function hstate_next_node_to_free() uses new hstate member
    next_to_free_nid to distribute "frees" across all nodes with huge pages.
    
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@canonical.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5cbc620bdfe0..16cdb75a543a 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -185,7 +185,8 @@ unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
 #define HSTATE_NAME_LEN 32
 /* Defines one hugetlb page size */
 struct hstate {
-	int hugetlb_next_nid;
+	int next_nid_to_alloc;
+	int next_nid_to_free;
 	unsigned int order;
 	unsigned long mask;
 	unsigned long max_huge_pages;

commit 353d5c30c666580347515da609dd74a2b8e9b828
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Aug 24 16:30:28 2009 +0100

    mm: fix hugetlb bug due to user_shm_unlock call
    
    2.6.30's commit 8a0bdec194c21c8fdef840989d0d7b742bb5d4bc removed
    user_shm_lock() calls in hugetlb_file_setup() but left the
    user_shm_unlock call in shm_destroy().
    
    In detail:
    Assume that can_do_hugetlb_shm() returns true and hence user_shm_lock()
    is not called in hugetlb_file_setup(). However, user_shm_unlock() is
    called in any case in shm_destroy() and in the following
    atomic_dec_and_lock(&up->__count) in free_uid() is executed and if
    up->__count gets zero, also cleanup_user_struct() is scheduled.
    
    Note that sched_destroy_user() is empty if CONFIG_USER_SCHED is not set.
    However, the ref counter up->__count gets unexpectedly non-positive and
    the corresponding structs are freed even though there are live
    references to them, resulting in a kernel oops after a lots of
    shmget(SHM_HUGETLB)/shmctl(IPC_RMID) cycles and CONFIG_USER_SCHED set.
    
    Hugh changed Stefan's suggested patch: can_do_hugetlb_shm() at the
    time of shm_destroy() may give a different answer from at the time
    of hugetlb_file_setup().  And fixed newseg()'s no_id error path,
    which has missed user_shm_unlock() ever since it came in 2.6.9.
    
    Reported-by: Stefan Huber <shuber2@gmail.com>
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Tested-by: Stefan Huber <shuber2@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 2723513a5651..5cbc620bdfe0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -10,6 +10,7 @@
 #include <asm/tlbflush.h>
 
 struct ctl_table;
+struct user_struct;
 
 int PageHuge(struct page *page);
 
@@ -146,7 +147,8 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_file_setup(const char *name, size_t, int);
+struct file *hugetlb_file_setup(const char *name, size_t size, int acct,
+						struct user_struct **user);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
 
@@ -168,7 +170,7 @@ static inline void set_file_hugepages(struct file *file)
 
 #define is_file_hugepages(file)			0
 #define set_file_hugepages(file)		BUG()
-#define hugetlb_file_setup(name,size,acctflag)	ERR_PTR(-ENOSYS)
+#define hugetlb_file_setup(name,size,acct,user)	ERR_PTR(-ENOSYS)
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 788c7df451467df71638dd79a2d63d78c6e13b9c
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Tue Jun 23 13:49:05 2009 +0100

    hugetlb: fault flags instead of write_access
    
    handle_mm_fault() is now passing fault flags rather than write_access
    down to hugetlb_fault(), so better recognize that in hugetlb_fault(),
    and in hugetlb_no_page().
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Acked-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index a05a5ef33391..2723513a5651 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -33,7 +33,7 @@ void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
-			unsigned long address, int write_access);
+			unsigned long address, unsigned int flags);
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						int acctflags);
@@ -98,7 +98,7 @@ static inline void hugetlb_report_meminfo(struct seq_file *m)
 #define pud_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
-#define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
+#define hugetlb_fault(mm, vma, addr, flags)	({ BUG(); 0; })
 
 #define hugetlb_change_protection(vma, address, end, newprot)
 

commit 20a0307c0396c2edb651401d2f2db193dda2f3c9
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Jun 16 15:32:22 2009 -0700

    mm: introduce PageHuge() for testing huge/gigantic pages
    
    A series of patches to enhance the /proc/pagemap interface and to add a
    userspace executable which can be used to present the pagemap data.
    
    Export 10 more flags to end users (and more for kernel developers):
    
            11. KPF_MMAP            (pseudo flag) memory mapped page
            12. KPF_ANON            (pseudo flag) memory mapped page (anonymous)
            13. KPF_SWAPCACHE       page is in swap cache
            14. KPF_SWAPBACKED      page is swap/RAM backed
            15. KPF_COMPOUND_HEAD   (*)
            16. KPF_COMPOUND_TAIL   (*)
            17. KPF_HUGE            hugeTLB pages
            18. KPF_UNEVICTABLE     page is in the unevictable LRU list
            19. KPF_HWPOISON        hardware detected corruption
            20. KPF_NOPAGE          (pseudo flag) no page frame at the address
    
            (*) For compound pages, exporting _both_ head/tail info enables
                users to tell where a compound page starts/ends, and its order.
    
    a simple demo of the page-types tool
    
    # ./page-types -h
    page-types [options]
                -r|--raw                  Raw mode, for kernel developers
                -a|--addr    addr-spec    Walk a range of pages
                -b|--bits    bits-spec    Walk pages with specified bits
                -l|--list                 Show page details in ranges
                -L|--list-each            Show page details one by one
                -N|--no-summary           Don't show summay info
                -h|--help                 Show this usage message
    addr-spec:
                N                         one page at offset N (unit: pages)
                N+M                       pages range from N to N+M-1
                N,M                       pages range from N to M-1
                N,                        pages range from N to end
                ,M                        pages range from 0 to M
    bits-spec:
                bit1,bit2                 (flags & (bit1|bit2)) != 0
                bit1,bit2=bit1            (flags & (bit1|bit2)) == bit1
                bit1,~bit2                (flags & (bit1|bit2)) == bit1
                =bit1,bit2                flags == (bit1|bit2)
    bit-names:
              locked              error         referenced           uptodate
               dirty                lru             active               slab
           writeback            reclaim              buddy               mmap
           anonymous          swapcache         swapbacked      compound_head
       compound_tail               huge        unevictable           hwpoison
              nopage           reserved(r)         mlocked(r)    mappedtodisk(r)
             private(r)       private_2(r)   owner_private(r)            arch(r)
            uncached(r)       readahead(o)       slob_free(o)     slub_frozen(o)
          slub_debug(o)
                                       (r) raw mode bits  (o) overloaded bits
    
    # ./page-types
                 flags      page-count       MB  symbolic-flags                     long-symbolic-flags
    0x0000000000000000          487369     1903  _________________________________
    0x0000000000000014               5        0  __R_D____________________________  referenced,dirty
    0x0000000000000020               1        0  _____l___________________________  lru
    0x0000000000000024              34        0  __R__l___________________________  referenced,lru
    0x0000000000000028            3838       14  ___U_l___________________________  uptodate,lru
    0x0001000000000028              48        0  ___U_l_______________________I___  uptodate,lru,readahead
    0x000000000000002c            6478       25  __RU_l___________________________  referenced,uptodate,lru
    0x000100000000002c              47        0  __RU_l_______________________I___  referenced,uptodate,lru,readahead
    0x0000000000000040            8344       32  ______A__________________________  active
    0x0000000000000060               1        0  _____lA__________________________  lru,active
    0x0000000000000068             348        1  ___U_lA__________________________  uptodate,lru,active
    0x0001000000000068              12        0  ___U_lA______________________I___  uptodate,lru,active,readahead
    0x000000000000006c             988        3  __RU_lA__________________________  referenced,uptodate,lru,active
    0x000100000000006c              48        0  __RU_lA______________________I___  referenced,uptodate,lru,active,readahead
    0x0000000000004078               1        0  ___UDlA_______b__________________  uptodate,dirty,lru,active,swapbacked
    0x000000000000407c              34        0  __RUDlA_______b__________________  referenced,uptodate,dirty,lru,active,swapbacked
    0x0000000000000400             503        1  __________B______________________  buddy
    0x0000000000000804               1        0  __R________M_____________________  referenced,mmap
    0x0000000000000828            1029        4  ___U_l_____M_____________________  uptodate,lru,mmap
    0x0001000000000828              43        0  ___U_l_____M_________________I___  uptodate,lru,mmap,readahead
    0x000000000000082c             382        1  __RU_l_____M_____________________  referenced,uptodate,lru,mmap
    0x000100000000082c              12        0  __RU_l_____M_________________I___  referenced,uptodate,lru,mmap,readahead
    0x0000000000000868             192        0  ___U_lA____M_____________________  uptodate,lru,active,mmap
    0x0001000000000868              12        0  ___U_lA____M_________________I___  uptodate,lru,active,mmap,readahead
    0x000000000000086c             800        3  __RU_lA____M_____________________  referenced,uptodate,lru,active,mmap
    0x000100000000086c              31        0  __RU_lA____M_________________I___  referenced,uptodate,lru,active,mmap,readahead
    0x0000000000004878               2        0  ___UDlA____M__b__________________  uptodate,dirty,lru,active,mmap,swapbacked
    0x0000000000001000             492        1  ____________a____________________  anonymous
    0x0000000000005808               4        0  ___U_______Ma_b__________________  uptodate,mmap,anonymous,swapbacked
    0x0000000000005868            2839       11  ___U_lA____Ma_b__________________  uptodate,lru,active,mmap,anonymous,swapbacked
    0x000000000000586c              30        0  __RU_lA____Ma_b__________________  referenced,uptodate,lru,active,mmap,anonymous,swapbacked
                 total          513968     2007
    
    # ./page-types -r
                 flags      page-count       MB  symbolic-flags                     long-symbolic-flags
    0x0000000000000000          468002     1828  _________________________________
    0x0000000100000000           19102       74  _____________________r___________  reserved
    0x0000000000008000              41        0  _______________H_________________  compound_head
    0x0000000000010000             188        0  ________________T________________  compound_tail
    0x0000000000008014               1        0  __R_D__________H_________________  referenced,dirty,compound_head
    0x0000000000010014               4        0  __R_D___________T________________  referenced,dirty,compound_tail
    0x0000000000000020               1        0  _____l___________________________  lru
    0x0000000800000024              34        0  __R__l__________________P________  referenced,lru,private
    0x0000000000000028            3794       14  ___U_l___________________________  uptodate,lru
    0x0001000000000028              46        0  ___U_l_______________________I___  uptodate,lru,readahead
    0x0000000400000028              44        0  ___U_l_________________d_________  uptodate,lru,mappedtodisk
    0x0001000400000028               2        0  ___U_l_________________d_____I___  uptodate,lru,mappedtodisk,readahead
    0x000000000000002c            6434       25  __RU_l___________________________  referenced,uptodate,lru
    0x000100000000002c              47        0  __RU_l_______________________I___  referenced,uptodate,lru,readahead
    0x000000040000002c              14        0  __RU_l_________________d_________  referenced,uptodate,lru,mappedtodisk
    0x000000080000002c              30        0  __RU_l__________________P________  referenced,uptodate,lru,private
    0x0000000800000040            8124       31  ______A_________________P________  active,private
    0x0000000000000040             219        0  ______A__________________________  active
    0x0000000800000060               1        0  _____lA_________________P________  lru,active,private
    0x0000000000000068             322        1  ___U_lA__________________________  uptodate,lru,active
    0x0001000000000068              12        0  ___U_lA______________________I___  uptodate,lru,active,readahead
    0x0000000400000068              13        0  ___U_lA________________d_________  uptodate,lru,active,mappedtodisk
    0x0000000800000068              12        0  ___U_lA_________________P________  uptodate,lru,active,private
    0x000000000000006c             977        3  __RU_lA__________________________  referenced,uptodate,lru,active
    0x000100000000006c              48        0  __RU_lA______________________I___  referenced,uptodate,lru,active,readahead
    0x000000040000006c               5        0  __RU_lA________________d_________  referenced,uptodate,lru,active,mappedtodisk
    0x000000080000006c               3        0  __RU_lA_________________P________  referenced,uptodate,lru,active,private
    0x0000000c0000006c               3        0  __RU_lA________________dP________  referenced,uptodate,lru,active,mappedtodisk,private
    0x0000000c00000068               1        0  ___U_lA________________dP________  uptodate,lru,active,mappedtodisk,private
    0x0000000000004078               1        0  ___UDlA_______b__________________  uptodate,dirty,lru,active,swapbacked
    0x000000000000407c              34        0  __RUDlA_______b__________________  referenced,uptodate,dirty,lru,active,swapbacked
    0x0000000000000400             538        2  __________B______________________  buddy
    0x0000000000000804               1        0  __R________M_____________________  referenced,mmap
    0x0000000000000828            1029        4  ___U_l_____M_____________________  uptodate,lru,mmap
    0x0001000000000828              43        0  ___U_l_____M_________________I___  uptodate,lru,mmap,readahead
    0x000000000000082c             382        1  __RU_l_____M_____________________  referenced,uptodate,lru,mmap
    0x000100000000082c              12        0  __RU_l_____M_________________I___  referenced,uptodate,lru,mmap,readahead
    0x0000000000000868             192        0  ___U_lA____M_____________________  uptodate,lru,active,mmap
    0x0001000000000868              12        0  ___U_lA____M_________________I___  uptodate,lru,active,mmap,readahead
    0x000000000000086c             800        3  __RU_lA____M_____________________  referenced,uptodate,lru,active,mmap
    0x000100000000086c              31        0  __RU_lA____M_________________I___  referenced,uptodate,lru,active,mmap,readahead
    0x0000000000004878               2        0  ___UDlA____M__b__________________  uptodate,dirty,lru,active,mmap,swapbacked
    0x0000000000001000             492        1  ____________a____________________  anonymous
    0x0000000000005008               2        0  ___U________a_b__________________  uptodate,anonymous,swapbacked
    0x0000000000005808               4        0  ___U_______Ma_b__________________  uptodate,mmap,anonymous,swapbacked
    0x000000000000580c               1        0  __RU_______Ma_b__________________  referenced,uptodate,mmap,anonymous,swapbacked
    0x0000000000005868            2839       11  ___U_lA____Ma_b__________________  uptodate,lru,active,mmap,anonymous,swapbacked
    0x000000000000586c              29        0  __RU_lA____Ma_b__________________  referenced,uptodate,lru,active,mmap,anonymous,swapbacked
                 total          513968     2007
    
    # ./page-types --raw --list --no-summary --bits reserved
    offset  count   flags
    0       15      _____________________r___________
    31      4       _____________________r___________
    159     97      _____________________r___________
    4096    2067    _____________________r___________
    6752    2390    _____________________r___________
    9355    3       _____________________r___________
    9728    14526   _____________________r___________
    
    This patch:
    
    Introduce PageHuge(), which identifies huge/gigantic pages by their
    dedicated compound destructor functions.
    
    Also move prep_compound_gigantic_page() to hugetlb.c and make
    __free_pages_ok() non-static.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 03be7f29ca01..a05a5ef33391 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -11,6 +11,8 @@
 
 struct ctl_table;
 
+int PageHuge(struct page *page);
+
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 {
 	return vma->vm_flags & VM_HUGETLB;
@@ -61,6 +63,11 @@ void hugetlb_change_protection(struct vm_area_struct *vma,
 
 #else /* !CONFIG_HUGETLB_PAGE */
 
+static inline int PageHuge(struct page *page)
+{
+	return 0;
+}
+
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 {
 	return 0;

commit 1db8508cf483dc1ecf66141f90a7c03659d69512
Author: Stefan Richter <stefanr@s5r6.in-berlin.de>
Date:   Tue Feb 10 23:27:32 2009 +0100

    hugetlbfs: fix build failure with !CONFIG_HUGETLBFS
    
    Fix regression due to 5a6fe125950676015f5108fb71b2a67441755003,
    "Do not account for the address space used by hugetlbfs using VM_ACCOUNT"
    which added an argument to the function hugetlb_file_setup() but not to
    the macro hugetlb_file_setup().
    
    Reported-by: Chris Clayton <chris2553@googlemail.com>
    Signed-off-by: Stefan Richter <stefanr@s5r6.in-berlin.de>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index af09660001c7..03be7f29ca01 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -159,9 +159,9 @@ static inline void set_file_hugepages(struct file *file)
 }
 #else /* !CONFIG_HUGETLBFS */
 
-#define is_file_hugepages(file)		0
-#define set_file_hugepages(file)	BUG()
-#define hugetlb_file_setup(name,size)	ERR_PTR(-ENOSYS)
+#define is_file_hugepages(file)			0
+#define set_file_hugepages(file)		BUG()
+#define hugetlb_file_setup(name,size,acctflag)	ERR_PTR(-ENOSYS)
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 5a6fe125950676015f5108fb71b2a67441755003
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Feb 10 14:02:27 2009 +0000

    Do not account for the address space used by hugetlbfs using VM_ACCOUNT
    
    When overcommit is disabled, the core VM accounts for pages used by anonymous
    shared, private mappings and special mappings. It keeps track of VMAs that
    should be accounted for with VM_ACCOUNT and VMAs that never had a reserve
    with VM_NORESERVE.
    
    Overcommit for hugetlbfs is much riskier than overcommit for base pages
    due to contiguity requirements. It avoids overcommiting on both shared and
    private mappings using reservation counters that are checked and updated
    during mmap(). This ensures (within limits) that hugepages exist in the
    future when faults occurs or it is too easy to applications to be SIGKILLed.
    
    As hugetlbfs makes its own reservations of a different unit to the base page
    size, VM_ACCOUNT should never be set. Even if the units were correct, we would
    double account for the usage in the core VM and hugetlbfs. VM_NORESERVE may
    be set because an application can request no reserves be made for hugetlbfs
    at the risk of getting killed later.
    
    With commit fc8744adc870a8d4366908221508bb113d8b72ee, VM_NORESERVE and
    VM_ACCOUNT are getting unconditionally set for hugetlbfs-backed mappings. This
    breaks the accounting for both the core VM and hugetlbfs, can trigger an
    OOM storm when hugepage pools are too small lockups and corrupted counters
    otherwise are used. This patch brings hugetlbfs more in line with how the
    core VM treats VM_NORESERVE but prevents VM_ACCOUNT being set.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f1d2fba19ea0..af09660001c7 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -33,7 +33,8 @@ unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write_access);
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
-						struct vm_area_struct *vma);
+						struct vm_area_struct *vma,
+						int acctflags);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long hugepages_treat_as_movable;
@@ -138,7 +139,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_file_setup(const char *name, size_t);
+struct file *hugetlb_file_setup(const char *name, size_t, int);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
 

commit 3340289ddf29ca75c3acfb3a6b72f234b2f74d5c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jan 6 14:38:54 2009 -0800

    mm: report the MMU pagesize in /proc/pid/smaps
    
    The KernelPageSize entry in /proc/pid/smaps is the pagesize used by the
    kernel to back a VMA.  This matches the size used by the MMU in the
    majority of cases.  However, one counter-example occurs on PPC64 kernels
    whereby a kernel using 64K as a base pagesize may still use 4K pages for
    the MMU on older processor.  To distinguish, this patch reports
    MMUPageSize as the pagesize used by the MMU in /proc/pid/smaps.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: "KOSAKI Motohiro" <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 648e1e25979e..f1d2fba19ea0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -235,6 +235,8 @@ static inline unsigned long huge_page_size(struct hstate *h)
 
 extern unsigned long vma_kernel_pagesize(struct vm_area_struct *vma);
 
+extern unsigned long vma_mmu_pagesize(struct vm_area_struct *vma);
+
 static inline unsigned long huge_page_mask(struct hstate *h)
 {
 	return h->mask;
@@ -276,6 +278,7 @@ struct hstate {};
 #define huge_page_size(h) PAGE_SIZE
 #define huge_page_mask(h) PAGE_MASK
 #define vma_kernel_pagesize(v) PAGE_SIZE
+#define vma_mmu_pagesize(v) PAGE_SIZE
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
 static inline unsigned int pages_per_huge_page(struct hstate *h)

commit 08fba69986e20c1c9e5fe2e6064d146cc4f42480
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jan 6 14:38:53 2009 -0800

    mm: report the pagesize backing a VMA in /proc/pid/smaps
    
    It is useful to verify a hugepage-aware application is using the expected
    pagesizes for its memory regions. This patch creates an entry called
    KernelPageSize in /proc/pid/smaps that is the size of page used by the
    kernel to back a VMA. The entry is not called PageSize as it is possible
    the MMU uses a different size. This extension should not break any sensible
    parser that skips lines containing unrecognised information.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: "KOSAKI Motohiro" <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e1c8afc002c0..648e1e25979e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -233,6 +233,8 @@ static inline unsigned long huge_page_size(struct hstate *h)
 	return (unsigned long)PAGE_SIZE << h->order;
 }
 
+extern unsigned long vma_kernel_pagesize(struct vm_area_struct *vma);
+
 static inline unsigned long huge_page_mask(struct hstate *h)
 {
 	return h->mask;
@@ -273,6 +275,7 @@ struct hstate {};
 #define hstate_inode(i) NULL
 #define huge_page_size(h) PAGE_SIZE
 #define huge_page_mask(h) PAGE_MASK
+#define vma_kernel_pagesize(v) PAGE_SIZE
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
 static inline unsigned int pages_per_huge_page(struct hstate *h)

commit e1759c215bee5abbcb6cb066590ab20905154ed5
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Oct 15 23:50:22 2008 +0400

    proc: switch /proc/meminfo to seq_file
    
    and move it to fs/proc/meminfo.c while I'm at it.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 32e0ef0f6e1f..e1c8afc002c0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -27,7 +27,7 @@ void unmap_hugepage_range(struct vm_area_struct *,
 void __unmap_hugepage_range(struct vm_area_struct *,
 			unsigned long, unsigned long, struct page *);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
-int hugetlb_report_meminfo(char *);
+void hugetlb_report_meminfo(struct seq_file *);
 int hugetlb_report_node_meminfo(int, char *);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
@@ -79,7 +79,9 @@ static inline unsigned long hugetlb_total_pages(void)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
 #define unmap_hugepage_range(vma, start, end, page)	BUG()
-#define hugetlb_report_meminfo(buf)		0
+static inline void hugetlb_report_meminfo(struct seq_file *m)
+{
+}
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
 #define follow_huge_pud(mm, addr, pud, write)	NULL

commit 510a35d4a47802f4a0028aa6bd2ca2170da5e32f
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sat Jul 26 15:22:27 2008 -0700

    hugetlb: remove unused variable warning
    
    Remove the following warning when CONFIG_HUGETLB_PAGE is not set:
    
            ipc/shm.c: In function `shm_get_stat':
            ipc/shm.c:565: warning: unused variable `h'
    
    [akpm@linux-foundation.org: use tabs, not spaces]
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 9a71d4cc88c8..32e0ef0f6e1f 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -273,7 +273,10 @@ struct hstate {};
 #define huge_page_mask(h) PAGE_MASK
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
-#define pages_per_huge_page(h) 1
+static inline unsigned int pages_per_huge_page(struct hstate *h)
+{
+	return 1;
+}
 #endif
 
 #endif /* _LINUX_HUGETLB_H */

commit 53ba51d21d6e048424ab8aadfebdb1f25ae07b60
Author: Jon Tollefson <kniht@linux.vnet.ibm.com>
Date:   Wed Jul 23 21:27:52 2008 -0700

    hugetlb: allow arch overridden hugepage allocation
    
    Allow alloc_bootmem_huge_page() to be overridden by architectures that
    can't always use bootmem.  This requires huge_boot_pages to be available
    for use by this function.
    
    This is required for powerpc 16G pages, which have to be reserved prior to
    boot-time.  The location of these pages are indicated in the device tree.
    
    Acked-by: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Jon Tollefson <kniht@linux.vnet.ibm.com>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b2c17f62cacb..9a71d4cc88c8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -39,6 +39,7 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
+extern struct list_head huge_boot_pages;
 
 /* arch callbacks */
 
@@ -188,6 +189,14 @@ struct hstate {
 	char name[HSTATE_NAME_LEN];
 };
 
+struct huge_bootmem_page {
+	struct list_head list;
+	struct hstate *hstate;
+};
+
+/* arch callback */
+int __init alloc_bootmem_huge_page(struct hstate *h);
+
 void __init hugetlb_add_hstate(unsigned order);
 struct hstate *size_to_hstate(unsigned long size);
 
@@ -256,6 +265,7 @@ static inline struct hstate *page_hstate(struct page *page)
 
 #else
 struct hstate {};
+#define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
 #define hstate_vma(v) NULL
 #define hstate_inode(i) NULL

commit ceb868796181dc95ea01a110e123afd391639873
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jul 23 21:27:50 2008 -0700

    hugetlb: introduce pud_huge
    
    Straight forward extensions for huge pages located in the PUD instead of
    PMDs.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 58c0de32e7f0..b2c17f62cacb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -50,7 +50,10 @@ struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int write);
+struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
+				pud_t *pud, int write);
 int pmd_huge(pmd_t pmd);
+int pud_huge(pud_t pmd);
 void hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
@@ -78,8 +81,10 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
+#define follow_huge_pud(mm, addr, pud, write)	NULL
 #define prepare_hugepage_range(file, addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
+#define pud_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })

commit a3437870160cf2caaac6bdd76c7377a5a4145a8c
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Wed Jul 23 21:27:44 2008 -0700

    hugetlb: new sysfs interface
    
    Provide new hugepages user APIs that are more suited to multiple hstates
    in sysfs.  There is a new directory, /sys/kernel/hugepages.  Underneath
    that directory there will be a directory per-supported hugepage size,
    e.g.:
    
    /sys/kernel/hugepages/hugepages-64kB
    /sys/kernel/hugepages/hugepages-16384kB
    /sys/kernel/hugepages/hugepages-16777216kB
    
    corresponding to 64k, 16m and 16g respectively.  Within each
    hugepages-size directory there are a number of files, corresponding to the
    tracked counters in the hstate, e.g.:
    
    /sys/kernel/hugepages/hugepages-64/nr_hugepages
    /sys/kernel/hugepages/hugepages-64/nr_overcommit_hugepages
    /sys/kernel/hugepages/hugepages-64/free_hugepages
    /sys/kernel/hugepages/hugepages-64/resv_hugepages
    /sys/kernel/hugepages/hugepages-64/surplus_hugepages
    
    Of these files, the first two are read-write and the latter three are
    read-only.  The size of the hugepage being manipulated is trivially
    deducible from the enclosing directory and is always expressed in kB (to
    match meminfo).
    
    [dave@linux.vnet.ibm.com: fix build]
    [nacc@us.ibm.com: hugetlb: hang off of /sys/kernel/mm rather than /sys/kernel]
    [nacc@us.ibm.com: hugetlb: remove CONFIG_SYSFS dependency]
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ba9263e631b9..58c0de32e7f0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -164,6 +164,7 @@ unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
 
 #ifdef CONFIG_HUGETLB_PAGE
 
+#define HSTATE_NAME_LEN 32
 /* Defines one hugetlb page size */
 struct hstate {
 	int hugetlb_next_nid;
@@ -179,6 +180,7 @@ struct hstate {
 	unsigned int nr_huge_pages_node[MAX_NUMNODES];
 	unsigned int free_huge_pages_node[MAX_NUMNODES];
 	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
+	char name[HSTATE_NAME_LEN];
 };
 
 void __init hugetlb_add_hstate(unsigned order);

commit a137e1cc6d6e7d315fef03962a2a5a113348b13b
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jul 23 21:27:43 2008 -0700

    hugetlbfs: per mount huge page sizes
    
    Add the ability to configure the hugetlb hstate used on a per mount basis.
    
    - Add a new pagesize= option to the hugetlbfs mount that allows setting
      the page size
    - This option causes the mount code to find the hstate corresponding to the
      specified size, and sets up a pointer to the hstate in the mount's
      superblock.
    - Change the hstate accessors to use this information rather than the
      global_hstate they were using (requires a slight change in mm/memory.c
      so we don't NULL deref in the error-unmap path -- see comments).
    
    [np: take hstate out of hugetlbfs inode and vma->vm_private_data]
    
    Acked-by: Adam Litke <agl@us.ibm.com>
    Acked-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b75bdb4deba3..ba9263e631b9 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -100,6 +100,7 @@ struct hugetlbfs_config {
 	umode_t mode;
 	long	nr_blocks;
 	long	nr_inodes;
+	struct hstate *hstate;
 };
 
 struct hugetlbfs_sb_info {
@@ -108,6 +109,7 @@ struct hugetlbfs_sb_info {
 	long	max_inodes;   /* inodes allowed */
 	long	free_inodes;  /* inodes free */
 	spinlock_t	stat_lock;
+	struct hstate *hstate;
 };
 
 
@@ -191,19 +193,21 @@ extern unsigned int default_hstate_idx;
 
 #define default_hstate (hstates[default_hstate_idx])
 
-static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
+static inline struct hstate *hstate_inode(struct inode *i)
 {
-	return &default_hstate;
+	struct hugetlbfs_sb_info *hsb;
+	hsb = HUGETLBFS_SB(i->i_sb);
+	return hsb->hstate;
 }
 
 static inline struct hstate *hstate_file(struct file *f)
 {
-	return &default_hstate;
+	return hstate_inode(f->f_dentry->d_inode);
 }
 
-static inline struct hstate *hstate_inode(struct inode *i)
+static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
 {
-	return &default_hstate;
+	return hstate_file(vma->vm_file);
 }
 
 static inline unsigned long huge_page_size(struct hstate *h)

commit e5ff215941d59f8ae6bf58f6428dc5c26745a612
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jul 23 21:27:42 2008 -0700

    hugetlb: multiple hstates for multiple page sizes
    
    Add basic support for more than one hstate in hugetlbfs.  This is the key
    to supporting multiple hugetlbfs page sizes at once.
    
    - Rather than a single hstate, we now have an array, with an iterator
    - default_hstate continues to be the struct hstate which we use by default
    - Add functions for architectures to register new hstates
    
    [akpm@linux-foundation.org: coding-style fixes]
    Acked-by: Adam Litke <agl@us.ibm.com>
    Acked-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ad2271e11f9b..b75bdb4deba3 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -36,8 +36,6 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
-extern unsigned long max_huge_pages;
-extern unsigned long sysctl_overcommit_huge_pages;
 extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
@@ -181,7 +179,17 @@ struct hstate {
 	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
 };
 
-extern struct hstate default_hstate;
+void __init hugetlb_add_hstate(unsigned order);
+struct hstate *size_to_hstate(unsigned long size);
+
+#ifndef HUGE_MAX_HSTATE
+#define HUGE_MAX_HSTATE 1
+#endif
+
+extern struct hstate hstates[HUGE_MAX_HSTATE];
+extern unsigned int default_hstate_idx;
+
+#define default_hstate (hstates[default_hstate_idx])
 
 static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
 {
@@ -230,6 +238,11 @@ static inline unsigned int blocks_per_huge_page(struct hstate *h)
 
 #include <asm/hugetlb.h>
 
+static inline struct hstate *page_hstate(struct page *page)
+{
+	return size_to_hstate(PAGE_SIZE << compound_order(page));
+}
+
 #else
 struct hstate {};
 #define hstate_file(f) NULL

commit a5516438959d90b071ff0a484ce4f3f523dc3152
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jul 23 21:27:41 2008 -0700

    hugetlb: modular state for hugetlb page size
    
    The goal of this patchset is to support multiple hugetlb page sizes.  This
    is achieved by introducing a new struct hstate structure, which
    encapsulates the important hugetlb state and constants (eg.  huge page
    size, number of huge pages currently allocated, etc).
    
    The hstate structure is then passed around the code which requires these
    fields, they will do the right thing regardless of the exact hstate they
    are operating on.
    
    This patch adds the hstate structure, with a single global instance of it
    (default_hstate), and does the basic work of converting hugetlb to use the
    hstate.
    
    Future patches will add more hstate structures to allow for different
    hugetlbfs mounts to have different page sizes.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Acked-by: Adam Litke <agl@us.ibm.com>
    Acked-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index abbc187193a1..ad2271e11f9b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -8,7 +8,6 @@
 #include <linux/mempolicy.h>
 #include <linux/shm.h>
 #include <asm/tlbflush.h>
-#include <asm/hugetlb.h>
 
 struct ctl_table;
 
@@ -45,7 +44,8 @@ extern int sysctl_hugetlb_shm_group;
 
 /* arch callbacks */
 
-pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr);
+pte_t *huge_pte_alloc(struct mm_struct *mm,
+			unsigned long addr, unsigned long sz);
 pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
@@ -80,7 +80,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
-#define prepare_hugepage_range(addr,len)	(-EINVAL)
+#define prepare_hugepage_range(file, addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
@@ -134,8 +134,6 @@ struct file *hugetlb_file_setup(const char *name, size_t);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
 
-#define BLOCKS_PER_HUGEPAGE	(HPAGE_SIZE / 512)
-
 static inline int is_file_hugepages(struct file *file)
 {
 	if (file->f_op == &hugetlbfs_file_operations)
@@ -164,4 +162,84 @@ unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
 					unsigned long flags);
 #endif /* HAVE_ARCH_HUGETLB_UNMAPPED_AREA */
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+/* Defines one hugetlb page size */
+struct hstate {
+	int hugetlb_next_nid;
+	unsigned int order;
+	unsigned long mask;
+	unsigned long max_huge_pages;
+	unsigned long nr_huge_pages;
+	unsigned long free_huge_pages;
+	unsigned long resv_huge_pages;
+	unsigned long surplus_huge_pages;
+	unsigned long nr_overcommit_huge_pages;
+	struct list_head hugepage_freelists[MAX_NUMNODES];
+	unsigned int nr_huge_pages_node[MAX_NUMNODES];
+	unsigned int free_huge_pages_node[MAX_NUMNODES];
+	unsigned int surplus_huge_pages_node[MAX_NUMNODES];
+};
+
+extern struct hstate default_hstate;
+
+static inline struct hstate *hstate_vma(struct vm_area_struct *vma)
+{
+	return &default_hstate;
+}
+
+static inline struct hstate *hstate_file(struct file *f)
+{
+	return &default_hstate;
+}
+
+static inline struct hstate *hstate_inode(struct inode *i)
+{
+	return &default_hstate;
+}
+
+static inline unsigned long huge_page_size(struct hstate *h)
+{
+	return (unsigned long)PAGE_SIZE << h->order;
+}
+
+static inline unsigned long huge_page_mask(struct hstate *h)
+{
+	return h->mask;
+}
+
+static inline unsigned int huge_page_order(struct hstate *h)
+{
+	return h->order;
+}
+
+static inline unsigned huge_page_shift(struct hstate *h)
+{
+	return h->order + PAGE_SHIFT;
+}
+
+static inline unsigned int pages_per_huge_page(struct hstate *h)
+{
+	return 1 << h->order;
+}
+
+static inline unsigned int blocks_per_huge_page(struct hstate *h)
+{
+	return huge_page_size(h) / 512;
+}
+
+#include <asm/hugetlb.h>
+
+#else
+struct hstate {};
+#define hstate_file(f) NULL
+#define hstate_vma(v) NULL
+#define hstate_inode(i) NULL
+#define huge_page_size(h) PAGE_SIZE
+#define huge_page_mask(h) PAGE_MASK
+#define huge_page_order(h) 0
+#define huge_page_shift(h) PAGE_SHIFT
+#define pages_per_huge_page(h) 1
+#endif
+
 #endif /* _LINUX_HUGETLB_H */

commit 04f2cbe35699d22dbf428373682ead85ca1240f5
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Jul 23 21:27:25 2008 -0700

    hugetlb: guarantee that COW faults for a process that called mmap(MAP_PRIVATE) on hugetlbfs will succeed
    
    After patch 2 in this series, a process that successfully calls mmap() for
    a MAP_PRIVATE mapping will be guaranteed to successfully fault until a
    process calls fork().  At that point, the next write fault from the parent
    could fail due to COW if the child still has a reference.
    
    We only reserve pages for the parent but a copy must be made to avoid
    leaking data from the parent to the child after fork().  Reserves could be
    taken for both parent and child at fork time to guarantee faults but if
    the mapping is large it is highly likely we will not have sufficient pages
    for the reservation, and it is common to fork only to exec() immediatly
    after.  A failure here would be very undesirable.
    
    Note that the current behaviour of mainline with MAP_PRIVATE pages is
    pretty bad.  The following situation is allowed to occur today.
    
    1. Process calls mmap(MAP_PRIVATE)
    2. Process calls mlock() to fault all pages and makes sure it succeeds
    3. Process forks()
    4. Process writes to MAP_PRIVATE mapping while child still exists
    5. If the COW fails at this point, the process gets SIGKILLed even though it
       had taken care to ensure the pages existed
    
    This patch improves the situation by guaranteeing the reliability of the
    process that successfully calls mmap().  When the parent performs COW, it
    will try to satisfy the allocation without using reserves.  If that fails
    the parent will steal the page leaving any children without a page.
    Faults from the child after that point will result in failure.  If the
    child COW happens first, an attempt will be made to allocate the page
    without reserves and the child will get SIGKILLed on failure.
    
    To summarise the new behaviour:
    
    1. If the original mapper performs COW on a private mapping with multiple
       references, it will attempt to allocate a hugepage from the pool or
       the buddy allocator without using the existing reserves. On fail, VMAs
       mapping the same area are traversed and the page being COW'd is unmapped
       where found. It will then steal the original page as the last mapper in
       the normal way.
    
    2. The VMAs the pages were unmapped from are flagged to note that pages
       with data no longer exist. Future no-page faults on those VMAs will
       terminate the process as otherwise it would appear that data was corrupted.
       A warning is printed to the console that this situation occured.
    
    2. If the child performs COW first, it will attempt to satisfy the COW
       from the pool if there are enough pages or via the buddy allocator if
       overcommit is allowed and the buddy allocator can satisfy the request. If
       it fails, the child will be killed.
    
    If the pool is large enough, existing applications will not notice that
    the reserves were a factor.  Existing applications depending on the
    no-reserves been set are unlikely to exist as for much of the history of
    hugetlbfs, pages were prefaulted at mmap(), allocating the pages at that
    point or failing the mmap().
    
    [npiggin@suse.de: fix CONFIG_HUGETLB=n build]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 185b14c9f021..abbc187193a1 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -23,8 +23,10 @@ int hugetlb_overcommit_handler(struct ctl_table *, int, struct file *, void __us
 int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);
-void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
-void __unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+void unmap_hugepage_range(struct vm_area_struct *,
+			unsigned long, unsigned long, struct page *);
+void __unmap_hugepage_range(struct vm_area_struct *,
+			unsigned long, unsigned long, struct page *);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);
@@ -74,7 +76,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
-#define unmap_hugepage_range(vma, start, end)	BUG()
+#define unmap_hugepage_range(vma, start, end, page)	BUG()
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL

commit a1e78772d72b2616ed20e54896e68e0e7044854e
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Jul 23 21:27:23 2008 -0700

    hugetlb: reserve huge pages for reliable MAP_PRIVATE hugetlbfs mappings until fork()
    
    This patch reserves huge pages at mmap() time for MAP_PRIVATE mappings in
    a similar manner to the reservations taken for MAP_SHARED mappings.  The
    reserve count is accounted both globally and on a per-VMA basis for
    private mappings.  This guarantees that a process that successfully calls
    mmap() will successfully fault all pages in the future unless fork() is
    called.
    
    The characteristics of private mappings of hugetlbfs files behaviour after
    this patch are;
    
    1. The process calling mmap() is guaranteed to succeed all future faults until
       it forks().
    2. On fork(), the parent may die due to SIGKILL on writes to the private
       mapping if enough pages are not available for the COW. For reasonably
       reliable behaviour in the face of a small huge page pool, children of
       hugepage-aware processes should not reference the mappings; such as
       might occur when fork()ing to exec().
    3. On fork(), the child VMAs inherit no reserves. Reads on pages already
       faulted by the parent will succeed. Successful writes will depend on enough
       huge pages being free in the pool.
    4. Quotas of the hugetlbfs mount are checked at reserve time for the mapper
       and at fault time otherwise.
    
    Before this patch, all reads or writes in the child potentially needs page
    allocations that can later lead to the death of the parent.  This applies
    to reads and writes of uninstantiated pages as well as COW.  After the
    patch it is only a write to an instantiated page that causes problems.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index a79e80b689d8..185b14c9f021 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -17,6 +17,7 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_HUGETLB;
 }
 
+void reset_vma_resv_huge_pages(struct vm_area_struct *vma);
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int hugetlb_overcommit_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
@@ -30,7 +31,8 @@ int hugetlb_report_node_meminfo(int, char *);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write_access);
-int hugetlb_reserve_pages(struct inode *inode, long from, long to);
+int hugetlb_reserve_pages(struct inode *inode, long from, long to,
+						struct vm_area_struct *vma);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
@@ -58,6 +60,11 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 {
 	return 0;
 }
+
+static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
+{
+}
+
 static inline unsigned long hugetlb_total_pages(void)
 {
 	return 0;

commit 6d779079bfd1196e077bb1d0a906c37ae770b102
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Mon Apr 28 02:13:27 2008 -0700

    hugetlbfs: architecture header cleanup
    
    This patch moves all architecture functions for hugetlb to architecture header
    files (include/asm-foo/hugetlb.h) and converts all macros to inline functions.
     It also removes (!) ARCH_HAS_HUGEPAGE_ONLY_RANGE,
    ARCH_HAS_HUGETLB_FREE_PGD_RANGE, ARCH_HAS_PREPARE_HUGEPAGE_RANGE,
    ARCH_HAS_SETCLEAR_HUGE_PTE and ARCH_HAS_HUGETLB_PREFAULT_HOOK.
    
    Getting rid of the ARCH_HAS_xxx #ifdef and macro fugliness should increase
    readability and maintainability, at the price of some code duplication.  An
    asm-generic common part would have reduced the loc, but we would end up with
    new ARCH_HAS_xxx defines eventually.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index addca4cd4f11..a79e80b689d8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -8,6 +8,7 @@
 #include <linux/mempolicy.h>
 #include <linux/shm.h>
 #include <asm/tlbflush.h>
+#include <asm/hugetlb.h>
 
 struct ctl_table;
 
@@ -51,51 +52,6 @@ int pmd_huge(pmd_t pmd);
 void hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
-#ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
-#define is_hugepage_only_range(mm, addr, len)	0
-#endif
-
-#ifndef ARCH_HAS_HUGETLB_FREE_PGD_RANGE
-#define hugetlb_free_pgd_range	free_pgd_range
-#else
-void hugetlb_free_pgd_range(struct mmu_gather **tlb, unsigned long addr,
-			    unsigned long end, unsigned long floor,
-			    unsigned long ceiling);
-#endif
-
-#ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE
-/*
- * If the arch doesn't supply something else, assume that hugepage
- * size aligned regions are ok without further preparation.
- */
-static inline int prepare_hugepage_range(unsigned long addr, unsigned long len)
-{
-	if (len & ~HPAGE_MASK)
-		return -EINVAL;
-	if (addr & ~HPAGE_MASK)
-		return -EINVAL;
-	return 0;
-}
-#else
-int prepare_hugepage_range(unsigned long addr, unsigned long len);
-#endif
-
-#ifndef ARCH_HAS_SETCLEAR_HUGE_PTE
-#define set_huge_pte_at(mm, addr, ptep, pte)	set_pte_at(mm, addr, ptep, pte)
-#define huge_ptep_get_and_clear(mm, addr, ptep) ptep_get_and_clear(mm, addr, ptep)
-#else
-void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
-		     pte_t *ptep, pte_t pte);
-pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
-			      pte_t *ptep);
-#endif
-
-#ifndef ARCH_HAS_HUGETLB_PREFAULT_HOOK
-#define hugetlb_prefault_arch_hook(mm)		do { } while (0)
-#else
-void hugetlb_prefault_arch_hook(struct mm_struct *mm);
-#endif
-
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)

commit 064d9efe947542097be669581f82d6b097e81d1a
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Wed Feb 13 15:03:19 2008 -0800

    hugetlb: fix overcommit locking
    
    proc_doulongvec_minmax() calls copy_to_user()/copy_from_user(), so we can't
    hold hugetlb_lock over the call.  Use a dummy variable to store the sysctl
    result, like in hugetlb_sysctl_handler(), then grab the lock to update
    nr_overcommit_huge_pages.
    
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Reported-by: Miles Lane <miles.lane@gmail.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 7ca198b379af..addca4cd4f11 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -33,8 +33,8 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
+extern unsigned long sysctl_overcommit_huge_pages;
 extern unsigned long hugepages_treat_as_movable;
-extern unsigned long nr_overcommit_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 

commit a3d0c6aa1bb342b9b2c7b123b52ac2f48a4d4d0a
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Fri Feb 8 04:18:18 2008 -0800

    hugetlb: add locking for overcommit sysctl
    
    When I replaced hugetlb_dynamic_pool with nr_overcommit_hugepages I used
    proc_doulongvec_minmax() directly.  However, hugetlb.c's locking rules
    require that all counter modifications occur under the hugetlb_lock.  Add a
    callback into the hugetlb code similar to the one for nr_hugepages.  Grab
    the lock around the manipulation of nr_overcommit_hugepages in
    proc_doulongvec_minmax().
    
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 30d606afcafe..7ca198b379af 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -17,6 +17,7 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 }
 
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
+int hugetlb_overcommit_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);

commit 368d2c6358c3c62b3820a8a73f9fe9c8b540cdea
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Mon Dec 17 16:20:22 2007 -0800

    Revert "hugetlb: Add hugetlb_dynamic_pool sysctl"
    
    This reverts commit 54f9f80d6543fb7b157d3b11e2e7911dc1379790 ("hugetlb:
    Add hugetlb_dynamic_pool sysctl")
    
    Given the new sysctl nr_overcommit_hugepages, the boolean dynamic pool
    sysctl is not needed, as its semantics can be expressed by 0 in the
    overcommit sysctl (no dynamic pool) and non-0 in the overcommit sysctl
    (pool enabled).
    
    (Needed in 2.6.24 since it reverts a post-2.6.23 userspace-visible change)
    
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f7bc869a29b8..30d606afcafe 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -33,7 +33,6 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
 extern unsigned long hugepages_treat_as_movable;
-extern int hugetlb_dynamic_pool;
 extern unsigned long nr_overcommit_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;

commit d1c3fb1f8f29c41b0d098d7cfb3c32939043631f
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Mon Dec 17 16:20:12 2007 -0800

    hugetlb: introduce nr_overcommit_hugepages sysctl
    
    hugetlb: introduce nr_overcommit_hugepages sysctl
    
    While examining the code to support /proc/sys/vm/hugetlb_dynamic_pool, I
    became convinced that having a boolean sysctl was insufficient:
    
    1) To support per-node control of hugepages, I have previously submitted
    patches to add a sysfs attribute related to nr_hugepages. However, with
    a boolean global value and per-mount quota enforcement constraining the
    dynamic pool, adding corresponding control of the dynamic pool on a
    per-node basis seems inconsistent to me.
    
    2) Administration of the hugetlb dynamic pool with multiple hugetlbfs
    mount points is, arguably, more arduous than it needs to be. Each quota
    would need to be set separately, and the sum would need to be monitored.
    
    To ease the administration, and to help make the way for per-node
    control of the static & dynamic hugepage pool, I added a separate
    sysctl, nr_overcommit_hugepages. This value serves as a high watermark
    for the overall hugepage pool, while nr_hugepages serves as a low
    watermark. The boolean sysctl can then be removed, as the condition
    
            nr_overcommit_hugepages > 0
    
    indicates the same administrative setting as
    
            hugetlb_dynamic_pool == 1
    
    Quotas still serve as local enforcement of the size of the pool on a
    per-mount basis.
    
    A few caveats:
    
    1) There is a race whereby the global surplus huge page counter is
    incremented before a hugepage has allocated. Another process could then
    try grow the pool, and fail to convert a surplus huge page to a normal
    huge page and instead allocate a fresh huge page. I believe this is
    benign, as no memory is leaked (the actual pages are still tracked
    correctly) and the counters won't go out of sync.
    
    2) Shrinking the static pool while a surplus is in effect will allow the
    number of surplus huge pages to exceed the overcommit value. As long as
    this condition holds, however, no more surplus huge pages will be
    allowed on the system until one of the two sysctls are increased
    sufficiently, or the surplus huge pages go out of use and are freed.
    
    Successfully tested on x86_64 with the current libhugetlbfs snapshot,
    modified to use the new sysctl.
    
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 24968790bc3e..f7bc869a29b8 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -34,6 +34,7 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 extern unsigned long max_huge_pages;
 extern unsigned long hugepages_treat_as_movable;
 extern int hugetlb_dynamic_pool;
+extern unsigned long nr_overcommit_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 

commit 45c682a68a87251d9a01383ce076ab21ee09812e
Author: Ken Chen <kenchen@google.com>
Date:   Wed Nov 14 16:59:44 2007 -0800

    hugetlb: fix i_blocks accounting
    
    For administrative purpose, we want to query actual block usage for
    hugetlbfs file via fstat.  Currently, hugetlbfs always return 0.  Fix that
    up since kernel already has all the information to track it properly.
    
    Signed-off-by: Ken Chen <kenchen@google.com>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 8104e5af75ab..24968790bc3e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -168,6 +168,8 @@ struct file *hugetlb_file_setup(const char *name, size_t);
 int hugetlb_get_quota(struct address_space *mapping, long delta);
 void hugetlb_put_quota(struct address_space *mapping, long delta);
 
+#define BLOCKS_PER_HUGEPAGE	(HPAGE_SIZE / 512)
+
 static inline int is_file_hugepages(struct file *file)
 {
 	if (file->f_op == &hugetlbfs_file_operations)

commit 9a119c056dc2a9970901954a6d561d50a95e528d
Author: Adam Litke <agl@us.ibm.com>
Date:   Wed Nov 14 16:59:41 2007 -0800

    hugetlb: allow bulk updating in hugetlb_*_quota()
    
    Add a second parameter 'delta' to hugetlb_get_quota and hugetlb_put_quota to
    allow bulk updating of the sbinfo->free_blocks counter.  This will be used by
    the next patch in the series.
    
    Signed-off-by: Adam Litke <agl@us.ibm.com>
    Cc: Ken Chen <kenchen@google.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: David Gibson <hermes@gibson.dropbear.id.au>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e22368656b38..8104e5af75ab 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -165,8 +165,8 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_file_setup(const char *name, size_t);
-int hugetlb_get_quota(struct address_space *mapping);
-void hugetlb_put_quota(struct address_space *mapping);
+int hugetlb_get_quota(struct address_space *mapping, long delta);
+void hugetlb_put_quota(struct address_space *mapping, long delta);
 
 static inline int is_file_hugepages(struct file *file)
 {

commit 5b23dbe8173c212d6a326e35347b038705603d39
Author: Adam Litke <agl@us.ibm.com>
Date:   Wed Nov 14 16:59:33 2007 -0800

    hugetlb: follow_hugetlb_page() for write access
    
    When calling get_user_pages(), a write flag is passed in by the caller to
    indicate if write access is required on the faulted-in pages.  Currently,
    follow_hugetlb_page() ignores this flag and always faults pages for
    read-only access.  This can cause data corruption because a device driver
    that calls get_user_pages() with write set will not expect COW faults to
    occur on the returned pages.
    
    This patch passes the write flag down to follow_hugetlb_page() and makes
    sure hugetlb_fault() is called with the right write_access parameter.
    
    [ezk@cs.sunysb.edu: build fix]
    Signed-off-by: Adam Litke <agl@us.ibm.com>
    Reviewed-by: Ken Chen <kenchen@google.com>
    Cc: David Gibson <hermes@gibson.dropbear.id.au>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Erez Zadok <ezk@cs.sunysb.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ea0f50bfbe03..e22368656b38 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -19,7 +19,7 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 void __unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
@@ -106,7 +106,7 @@ static inline unsigned long hugetlb_total_pages(void)
 	return 0;
 }
 
-#define follow_hugetlb_page(m,v,p,vs,a,b,i)	({ BUG(); 0; })
+#define follow_hugetlb_page(m,v,p,vs,a,b,i,w)	({ BUG(); 0; })
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })

commit 54f9f80d6543fb7b157d3b11e2e7911dc1379790
Author: Adam Litke <agl@us.ibm.com>
Date:   Tue Oct 16 01:26:20 2007 -0700

    hugetlb: Add hugetlb_dynamic_pool sysctl
    
    The maximum size of the huge page pool can be controlled using the overall
    size of the hugetlb filesystem (via its 'size' mount option).  However in the
    common case the this will not be set as the pool is traditionally fixed in
    size at boot time.  In order to maintain the expected semantics, we need to
    prevent the pool expanding by default.
    
    This patch introduces a new sysctl controlling dynamic pool resizing.  When
    this is enabled the pool will expand beyond its base size up to the size of
    the hugetlb filesystem.  It is disabled by default.
    
    Signed-off-by: Adam Litke <agl@us.ibm.com>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Dave McCracken <dave.mccracken@oracle.com>
    Cc: William Irwin <bill.irwin@oracle.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Ken Chen <kenchen@google.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 3a19b032c0eb..ea0f50bfbe03 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -33,6 +33,7 @@ void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
 extern unsigned long hugepages_treat_as_movable;
+extern int hugetlb_dynamic_pool;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 

commit dec4ad86c2fbea062e9ef9caa6d6e79f7c5e0b12
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Aug 30 23:56:40 2007 -0700

    hugepage: fix broken check for offset alignment in hugepage mappings
    
    For hugepage mappings, the file offset, like the address and size, needs to
    be aligned to the size of a hugepage.
    
    In commit 68589bc353037f233fe510ad9ff432338c95db66, the check for this was
    moved into prepare_hugepage_range() along with the address and size checks.
     But since BenH's rework of the get_unmapped_area() paths leading up to
    commit 4b1d89290b62bb2db476c94c82cf7442aab440c8, prepare_hugepage_range()
    is only called for MAP_FIXED mappings, not for other mappings.  This means
    we're no longer ever checking for an aligned offset - I've confirmed that
    mmap() will (apparently) succeed with a misaligned offset on both powerpc
    and i386 at least.
    
    This patch restores the check, removing it from prepare_hugepage_range()
    and putting it back into hugetlbfs_file_mmap().  I'm putting it there,
    rather than in the get_unmapped_area() path so it only needs to go in one
    place, than separately in the half-dozen or so arch-specific
    implementations of hugetlb_get_unmapped_area().
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e6a71c82d204..3a19b032c0eb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -66,11 +66,8 @@ void hugetlb_free_pgd_range(struct mmu_gather **tlb, unsigned long addr,
  * If the arch doesn't supply something else, assume that hugepage
  * size aligned regions are ok without further preparation.
  */
-static inline int prepare_hugepage_range(unsigned long addr, unsigned long len,
-						pgoff_t pgoff)
+static inline int prepare_hugepage_range(unsigned long addr, unsigned long len)
 {
-	if (pgoff & (~HPAGE_MASK >> PAGE_SHIFT))
-		return -EINVAL;
 	if (len & ~HPAGE_MASK)
 		return -EINVAL;
 	if (addr & ~HPAGE_MASK)
@@ -78,8 +75,7 @@ static inline int prepare_hugepage_range(unsigned long addr, unsigned long len,
 	return 0;
 }
 #else
-int prepare_hugepage_range(unsigned long addr, unsigned long len,
-						pgoff_t pgoff);
+int prepare_hugepage_range(unsigned long addr, unsigned long len);
 #endif
 
 #ifndef ARCH_HAS_SETCLEAR_HUGE_PTE
@@ -117,7 +113,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
-#define prepare_hugepage_range(addr,len,pgoff)	(-EINVAL)
+#define prepare_hugepage_range(addr,len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })

commit 4e950f6f0189f65f8bf069cf2272649ef418f5e4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jul 30 02:36:13 2007 +0400

    Remove fs.h from mm.h
    
    Remove fs.h from mm.h. For this,
     1) Uninline vma_wants_writenotify(). It's pretty huge anyway.
     2) Add back fs.h or less bloated headers (err.h) to files that need it.
    
    As result, on x86_64 allyesconfig, fs.h dependencies cut down from 3929 files
    rebuilt down to 3444 (-12.3%).
    
    Cross-compile tested without regressions on my two usual configs and (sigh):
    
    alpha              arm-mx1ads        mips-bigsur          powerpc-ebony
    alpha-allnoconfig  arm-neponset      mips-capcella        powerpc-g5
    alpha-defconfig    arm-netwinder     mips-cobalt          powerpc-holly
    alpha-up           arm-netx          mips-db1000          powerpc-iseries
    arm                arm-ns9xxx        mips-db1100          powerpc-linkstation
    arm-assabet        arm-omap_h2_1610  mips-db1200          powerpc-lite5200
    arm-at91rm9200dk   arm-onearm        mips-db1500          powerpc-maple
    arm-at91rm9200ek   arm-picotux200    mips-db1550          powerpc-mpc7448_hpc2
    arm-at91sam9260ek  arm-pleb          mips-ddb5477         powerpc-mpc8272_ads
    arm-at91sam9261ek  arm-pnx4008       mips-decstation      powerpc-mpc8313_rdb
    arm-at91sam9263ek  arm-pxa255-idp    mips-e55             powerpc-mpc832x_mds
    arm-at91sam9rlek   arm-realview      mips-emma2rh         powerpc-mpc832x_rdb
    arm-ateb9200       arm-realview-smp  mips-excite          powerpc-mpc834x_itx
    arm-badge4         arm-rpc           mips-fulong          powerpc-mpc834x_itxgp
    arm-carmeva        arm-s3c2410       mips-ip22            powerpc-mpc834x_mds
    arm-cerfcube       arm-shannon       mips-ip27            powerpc-mpc836x_mds
    arm-clps7500       arm-shark         mips-ip32            powerpc-mpc8540_ads
    arm-collie         arm-simpad        mips-jazz            powerpc-mpc8544_ds
    arm-corgi          arm-spitz         mips-jmr3927         powerpc-mpc8560_ads
    arm-csb337         arm-trizeps4      mips-malta           powerpc-mpc8568mds
    arm-csb637         arm-versatile     mips-mipssim         powerpc-mpc85xx_cds
    arm-ebsa110        i386              mips-mpc30x          powerpc-mpc8641_hpcn
    arm-edb7211        i386-allnoconfig  mips-msp71xx         powerpc-mpc866_ads
    arm-em_x270        i386-defconfig    mips-ocelot          powerpc-mpc885_ads
    arm-ep93xx         i386-up           mips-pb1100          powerpc-pasemi
    arm-footbridge     ia64              mips-pb1500          powerpc-pmac32
    arm-fortunet       ia64-allnoconfig  mips-pb1550          powerpc-ppc64
    arm-h3600          ia64-bigsur       mips-pnx8550-jbs     powerpc-prpmc2800
    arm-h7201          ia64-defconfig    mips-pnx8550-stb810  powerpc-ps3
    arm-h7202          ia64-gensparse    mips-qemu            powerpc-pseries
    arm-hackkit        ia64-sim          mips-rbhma4200       powerpc-up
    arm-integrator     ia64-sn2          mips-rbhma4500       s390
    arm-iop13xx        ia64-tiger        mips-rm200           s390-allnoconfig
    arm-iop32x         ia64-up           mips-sb1250-swarm    s390-defconfig
    arm-iop33x         ia64-zx1          mips-sead            s390-up
    arm-ixp2000        m68k              mips-tb0219          sparc
    arm-ixp23xx        m68k-amiga        mips-tb0226          sparc-allnoconfig
    arm-ixp4xx         m68k-apollo       mips-tb0287          sparc-defconfig
    arm-jornada720     m68k-atari        mips-workpad         sparc-up
    arm-kafa           m68k-bvme6000     mips-wrppmc          sparc64
    arm-kb9202         m68k-hp300        mips-yosemite        sparc64-allnoconfig
    arm-ks8695         m68k-mac          parisc               sparc64-defconfig
    arm-lart           m68k-mvme147      parisc-allnoconfig   sparc64-up
    arm-lpd270         m68k-mvme16x      parisc-defconfig     um-x86_64
    arm-lpd7a400       m68k-q40          parisc-up            x86_64
    arm-lpd7a404       m68k-sun3         powerpc              x86_64-allnoconfig
    arm-lubbock        m68k-sun3x        powerpc-cell         x86_64-defconfig
    arm-lusl7200       mips              powerpc-celleb       x86_64-up
    arm-mainstone      mips-atlas        powerpc-chrp32
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 49b7053043ad..e6a71c82d204 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -1,6 +1,8 @@
 #ifndef _LINUX_HUGETLB_H
 #define _LINUX_HUGETLB_H
 
+#include <linux/fs.h>
+
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>

commit 396faf0303d273219db5d7eb4a2879ad977ed185
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jul 17 04:03:13 2007 -0700

    Allow huge page allocations to use GFP_HIGH_MOVABLE
    
    Huge pages are not movable so are not allocated from ZONE_MOVABLE.  However,
    as ZONE_MOVABLE will always have pages that can be migrated or reclaimed, it
    can be used to satisfy hugepage allocations even when the system has been
    running a long time.  This allows an administrator to resize the hugepage pool
    at runtime depending on the size of ZONE_MOVABLE.
    
    This patch adds a new sysctl called hugepages_treat_as_movable.  When a
    non-zero value is written to it, future allocations for the huge page pool
    will use ZONE_MOVABLE.  Despite huge pages being non-movable, we do not
    introduce additional external fragmentation of note as huge pages are always
    the largest contiguous block we care about.
    
    [akpm@linux-foundation.org: various fixes]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 2c13715e9dde..49b7053043ad 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -15,6 +15,7 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 }
 
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
+int hugetlb_treat_movable_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
@@ -29,6 +30,7 @@ int hugetlb_reserve_pages(struct inode *inode, long from, long to);
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
+extern unsigned long hugepages_treat_as_movable;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 

commit 9d66586f7723b73c5925c7c7819c260484627851
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jun 16 10:16:16 2007 -0700

    shm: fix the filename of hugetlb sysv shared memory
    
    Some user space tools need to identify SYSV shared memory when examining
    /proc/<pid>/maps.  To do so they look for a block device with major zero, a
    dentry named SYSV<sysv key>, and having the minor of the internal sysv
    shared memory kernel mount.
    
    To help these tools and to make it easier for people just browsing
    /proc/<pid>/maps this patch modifies hugetlb sysv shared memory to use the
    SYSV<key> dentry naming convention.
    
    User space tools will still have to be aware that hugetlb sysv shared
    memory lives on a different internal kernel mount and so has a different
    block device minor number from the rest of sysv shared memory.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: "Serge E. Hallyn" <serge@hallyn.com>
    Cc: Albert Cahalan <acahalan@gmail.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index b4570b62ab85..2c13715e9dde 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -163,7 +163,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
-struct file *hugetlb_zero_setup(size_t);
+struct file *hugetlb_file_setup(const char *name, size_t);
 int hugetlb_get_quota(struct address_space *mapping);
 void hugetlb_put_quota(struct address_space *mapping);
 
@@ -185,7 +185,7 @@ static inline void set_file_hugepages(struct file *file)
 
 #define is_file_hugepages(file)		0
 #define set_file_hugepages(file)	BUG()
-#define hugetlb_zero_setup(size)	ERR_PTR(-ENOSYS)
+#define hugetlb_file_setup(name,size)	ERR_PTR(-ENOSYS)
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit d2ba27e8007b35d24764c0877ab2428e00a5c5ab
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sun May 6 14:49:00 2007 -0700

    proper prototype for hugetlb_get_unmapped_area()
    
    Add a proper prototype for hugetlb_get_unmapped_area() in
    include/linux/hugetlb.h.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: William Irwin <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 3f3e7a648da3..b4570b62ab85 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -189,4 +189,10 @@ static inline void set_file_hugepages(struct file *file)
 
 #endif /* !CONFIG_HUGETLBFS */
 
+#ifdef HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+					unsigned long len, unsigned long pgoff,
+					unsigned long flags);
+#endif /* HAVE_ARCH_HUGETLB_UNMAPPED_AREA */
+
 #endif /* _LINUX_HUGETLB_H */

commit 516dffdcd8827a40532798602830dfcfc672294c
Author: Adam Litke <agl@us.ibm.com>
Date:   Thu Mar 1 15:46:08 2007 -0800

    [PATCH] Fix get_unmapped_area and fsync for hugetlb shm segments
    
    This patch provides the following hugetlb-related fixes to the recent stacked
    shm files changes:
     - Update is_file_hugepages() so it will reconize hugetlb shm segments.
     - get_unmapped_area must be called with the nested file struct to handle
       the sfd->file->f_ops->get_unmapped_area == NULL case.
     - The fsync f_op must be wrapped since it is specified in the hugetlbfs
       f_ops.
    
    This is based on proposed fixes from Eric Biederman that were debugged and
    tested by me.  Without it, attempting to use hugetlb shared memory segments
    on powerpc (and likely ia64) will kill your box.
    
    Signed-off-by: Adam Litke <agl@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: William Irwin <bill.irwin@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index a60995afe334..3f3e7a648da3 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -4,6 +4,7 @@
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>
+#include <linux/shm.h>
 #include <asm/tlbflush.h>
 
 struct ctl_table;
@@ -168,7 +169,12 @@ void hugetlb_put_quota(struct address_space *mapping);
 
 static inline int is_file_hugepages(struct file *file)
 {
-	return file->f_op == &hugetlbfs_file_operations;
+	if (file->f_op == &hugetlbfs_file_operations)
+		return 1;
+	if (is_file_shm_hugepages(file))
+		return 1;
+
+	return 0;
 }
 
 static inline void set_file_hugepages(struct file *file)

commit 39dde65c9940c97fcd178a3d2b1c57ed8b7b68aa
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Wed Dec 6 20:32:03 2006 -0800

    [PATCH] shared page table for hugetlb page
    
    Following up with the work on shared page table done by Dave McCracken.  This
    set of patch target shared page table for hugetlb memory only.
    
    The shared page table is particular useful in the situation of large number of
    independent processes sharing large shared memory segments.  In the normal
    page case, the amount of memory saved from process' page table is quite
    significant.  For hugetlb, the saving on page table memory is not the primary
    objective (as hugetlb itself already cuts down page table overhead
    significantly), instead, the purpose of using shared page table on hugetlb is
    to allow faster TLB refill and smaller cache pollution upon TLB miss.
    
    With PT sharing, pte entries are shared among hundreds of processes, the cache
    consumption used by all the page table is smaller and in return, application
    gets much higher cache hit ratio.  One other effect is that cache hit ratio
    with hardware page walker hitting on pte in cache will be higher and this
    helps to reduce tlb miss latency.  These two effects contribute to higher
    application performance.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Cc: Dave McCracken <dmccr@us.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Adam Litke <agl@us.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ace64e57e17f..a60995afe334 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -35,6 +35,7 @@ extern int sysctl_hugetlb_shm_group;
 
 pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr);
 pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);
+int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,

commit 68589bc353037f233fe510ad9ff432338c95db66
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Nov 14 02:03:32 2006 -0800

    [PATCH] hugetlb: prepare_hugepage_range check offset too
    
    (David:)
    
    If hugetlbfs_file_mmap() returns a failure to do_mmap_pgoff() - for example,
    because the given file offset is not hugepage aligned - then do_mmap_pgoff
    will go to the unmap_and_free_vma backout path.
    
    But at this stage the vma hasn't been marked as hugepage, and the backout path
    will call unmap_region() on it.  That will eventually call down to the
    non-hugepage version of unmap_page_range().  On ppc64, at least, that will
    cause serious problems if there are any existing hugepage pagetable entries in
    the vicinity - for example if there are any other hugepage mappings under the
    same PUD.  unmap_page_range() will trigger a bad_pud() on the hugepage pud
    entries.  I suspect this will also cause bad problems on ia64, though I don't
    have a machine to test it on.
    
    (Hugh:)
    
    prepare_hugepage_range() should check file offset alignment when it checks
    virtual address and length, to stop MAP_FIXED with a bad huge offset from
    unmapping before it fails further down.  PowerPC should apply the same
    prepare_hugepage_range alignment checks as ia64 and all the others do.
    
    Then none of the alignment checks in hugetlbfs_file_mmap are required (nor
    is the check for too small a mapping); but even so, move up setting of
    VM_HUGETLB and add a comment to warn of what David Gibson discovered - if
    hugetlbfs_file_mmap fails before setting it, do_mmap_pgoff's unmap_region
    when unwinding from error will go the non-huge way, which may cause bad
    behaviour on architectures (powerpc and ia64) which segregate their huge
    mappings into a separate region of the address space.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5081d27bfa27..ace64e57e17f 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -60,8 +60,11 @@ void hugetlb_free_pgd_range(struct mmu_gather **tlb, unsigned long addr,
  * If the arch doesn't supply something else, assume that hugepage
  * size aligned regions are ok without further preparation.
  */
-static inline int prepare_hugepage_range(unsigned long addr, unsigned long len)
+static inline int prepare_hugepage_range(unsigned long addr, unsigned long len,
+						pgoff_t pgoff)
 {
+	if (pgoff & (~HPAGE_MASK >> PAGE_SHIFT))
+		return -EINVAL;
 	if (len & ~HPAGE_MASK)
 		return -EINVAL;
 	if (addr & ~HPAGE_MASK)
@@ -69,7 +72,8 @@ static inline int prepare_hugepage_range(unsigned long addr, unsigned long len)
 	return 0;
 }
 #else
-int prepare_hugepage_range(unsigned long addr, unsigned long len);
+int prepare_hugepage_range(unsigned long addr, unsigned long len,
+						pgoff_t pgoff);
 #endif
 
 #ifndef ARCH_HAS_SETCLEAR_HUGE_PTE
@@ -107,7 +111,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
-#define prepare_hugepage_range(addr, len)	(-EINVAL)
+#define prepare_hugepage_range(addr,len,pgoff)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })

commit 502717f4e112b18d9c37753a32f675bec9f2838b
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Wed Oct 11 01:20:46 2006 -0700

    [PATCH] hugetlb: fix linked list corruption in unmap_hugepage_range()
    
    commit fe1668ae5bf0145014c71797febd9ad5670d5d05 causes kernel to oops with
    libhugetlbfs test suite.  The problem is that hugetlb pages can be shared
    by multiple mappings.  Multiple threads can fight over page->lru in the
    unmap path and bad things happen.  We now serialize __unmap_hugepage_range
    to void concurrent linked list manipulation.  Such serialization is also
    needed for shared page table page on hugetlb area.  This patch will fixed
    the bug and also serve as a prepatch for shared page table.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index c25a38d8f600..5081d27bfa27 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -17,6 +17,7 @@ int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+void __unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);

commit a43a8c39bbb493c9e93f6764b350de2e33e18e92
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Fri Jun 23 02:03:15 2006 -0700

    [PATCH] tightening hugetlb strict accounting
    
    Current hugetlb strict accounting for shared mapping always assume mapping
    starts at zero file offset and reserves pages between zero and size of the
    file.  This assumption often reserves (or lock down) a lot more pages then
    necessary if application maps at none zero file offset.  libhugetlbfs is
    one example that requires proper reservation on shared mapping starts at
    none zero offset.
    
    This patch extends the reservation and hugetlb strict accounting to support
    any arbitrary pair of (offset, len), resulting a much more robust and
    accurate scheme.  More importantly, it won't lock down any hugetlb pages
    outside file mapping.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 4c5e610fe442..c25a38d8f600 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -23,6 +23,8 @@ int hugetlb_report_node_meminfo(int, char *);
 unsigned long hugetlb_total_pages(void);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write_access);
+int hugetlb_reserve_pages(struct inode *inode, long from, long to);
+void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
 
 extern unsigned long max_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
@@ -139,8 +141,6 @@ struct hugetlbfs_sb_info {
 
 struct hugetlbfs_inode_info {
 	struct shared_policy policy;
-	/* Protected by the (global) hugetlb_lock */
-	unsigned long prereserved_hpages;
 	struct inode vfs_inode;
 };
 
@@ -157,10 +157,6 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_zero_setup(size_t);
-int hugetlb_extend_reservation(struct hugetlbfs_inode_info *info,
-			       unsigned long atleast_hpages);
-void hugetlb_truncate_reservation(struct hugetlbfs_inode_info *info,
-				  unsigned long atmost_hpages);
 int hugetlb_get_quota(struct address_space *mapping);
 void hugetlb_put_quota(struct address_space *mapping);
 

commit 4b6f5d20b04dcbc3d888555522b90ba6d36c4106
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Tue Mar 28 01:56:42 2006 -0800

    [PATCH] Make most file operations structs in fs/ const
    
    This is a conversion to make the various file_operations structs in fs/
    const.  Basically a regexp job, with a few manual fixups
    
    The goal is both to increase correctness (harder to accidentally write to
    shared datastructures) and reducing the false sharing of cachelines with
    things that get dirty in .data (while .rodata is nicely read only and thus
    cache clean)
    
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d6f1019625af..4c5e610fe442 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -154,7 +154,7 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 	return sb->s_fs_info;
 }
 
-extern struct file_operations hugetlbfs_file_operations;
+extern const struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_zero_setup(size_t);
 int hugetlb_extend_reservation(struct hugetlbfs_inode_info *info,

commit 42b88befd6e0dae1a5fe04c03925037fa890e1f3
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 22 00:09:01 2006 -0800

    [PATCH] hugepage: is_aligned_hugepage_range() cleanup
    
    Quite a long time back, prepare_hugepage_range() replaced
    is_aligned_hugepage_range() as the callback from mm/mmap.c to arch code to
    verify if an address range is suitable for a hugepage mapping.
    is_aligned_hugepage_range() stuck around, but only to implement
    prepare_hugepage_range() on archs which didn't implement their own.
    
    Most archs (everything except ia64 and powerpc) used the same
    implementation of is_aligned_hugepage_range().  On powerpc, which
    implements its own prepare_hugepage_range(), the custom version was never
    used.
    
    In addition, "is_aligned_hugepage_range()" was a bad name, because it
    suggests it returns true iff the given range is a good hugepage range,
    whereas in fact it returns 0-or-error (so the sense is reversed).
    
    This patch cleans up by abolishing is_aligned_hugepage_range().  Instead
    prepare_hugepage_range() is defined directly.  Most archs use the default
    version, which simply checks the given region is aligned to the size of a
    hugepage.  ia64 and powerpc define custom versions.  The ia64 one simply
    checks that the range is in the correct address space region in addition to
    being suitably aligned.  The powerpc version (just as previously) checks
    for suitable addresses, and if necessary performs low-level MMU frobbing to
    set up new areas for use by hugepages.
    
    No libhugetlbfs testsuite regressions on ppc64 (POWER5 LPAR).
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Zhang Yanmin <yanmin.zhang@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5db25ffdb3eb..d6f1019625af 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -36,7 +36,6 @@ struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int write);
-int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
 int pmd_huge(pmd_t pmd);
 void hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
@@ -54,8 +53,18 @@ void hugetlb_free_pgd_range(struct mmu_gather **tlb, unsigned long addr,
 #endif
 
 #ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE
-#define prepare_hugepage_range(addr, len)	\
-	is_aligned_hugepage_range(addr, len)
+/*
+ * If the arch doesn't supply something else, assume that hugepage
+ * size aligned regions are ok without further preparation.
+ */
+static inline int prepare_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	return 0;
+}
 #else
 int prepare_hugepage_range(unsigned long addr, unsigned long len);
 #endif
@@ -95,7 +104,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
-#define is_aligned_hugepage_range(addr, len)	0
 #define prepare_hugepage_range(addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0

commit 3915bcf38fe0b6d130b4bbde97804f29a0becf32
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 22 00:08:59 2006 -0800

    [PATCH] hugepage: Move hugetlb_free_pgd_range() prototype to hugetlb.h
    
    The optional hugepage callback, hugetlb_free_pgd_range() is presently
    implemented non-trivially only on ia64 (but I plan to add one for powerpc
    shortly).  It has its own prototype for the function in asm-ia64/pgtable.h.
     However, since the function is called from generic code, it make sense for
    its prototype to be in the generic hugetlb.h header file, as the protypes
    other arch callbacks already are (prepare_hugepage_range(),
    set_huge_pte_at(), etc.).  This patch makes it so.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e465fbf1ef5f..5db25ffdb3eb 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -47,6 +47,10 @@ void hugetlb_change_protection(struct vm_area_struct *vma,
 
 #ifndef ARCH_HAS_HUGETLB_FREE_PGD_RANGE
 #define hugetlb_free_pgd_range	free_pgd_range
+#else
+void hugetlb_free_pgd_range(struct mmu_gather **tlb, unsigned long addr,
+			    unsigned long end, unsigned long floor,
+			    unsigned long ceiling);
 #endif
 
 #ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE

commit 9da61aef0fd5b17dd4bf4baf33db12c470def774
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 22 00:08:57 2006 -0800

    [PATCH] hugepage: Fix hugepage logic in free_pgtables()
    
    free_pgtables() has special logic to call hugetlb_free_pgd_range() instead
    of the normal free_pgd_range() on hugepage VMAs.  However, the test it uses
    to do so is incorrect: it calls is_hugepage_only_range on a hugepage sized
    range at the start of the vma.  is_hugepage_only_range() will return true
    if the given range has any intersection with a hugepage address region, and
    in this case the given region need not be hugepage aligned.  So, for
    example, this test can return true if called on, say, a 4k VMA immediately
    preceding a (nicely aligned) hugepage VMA.
    
    At present we get away with this because the powerpc version of
    hugetlb_free_pgd_range() is just a call to free_pgd_range().  On ia64 (the
    only other arch with a non-trivial is_hugepage_only_range()) we get away
    with it for a different reason; the hugepage area is not contiguous with
    the rest of the user address space, and VMAs are not permitted in between,
    so the test can't return a false positive there.
    
    Nonetheless this should be fixed.  We do that in the patch below by
    replacing the is_hugepage_only_range() test with an explicit test of the
    VMA using is_vm_hugetlb_page().
    
    This in turn changes behaviour for platforms where is_hugepage_only_range()
    returns false always (everything except powerpc and ia64).  We address this
    by ensuring that hugetlb_free_pgd_range() is defined to be identical to
    free_pgd_range() (instead of a no-op) on everything except ia64.  Even so,
    it will prevent some otherwise possible coalescing of calls down to
    free_pgd_range().  Since this only happens for hugepage VMAs, removing this
    small optimization seems unlikely to cause any trouble.
    
    This patch causes no regressions on the libhugetlbfs testsuite - ppc64
    POWER5 (8-way), ppc64 G5 (2-way) and i386 Pentium M (UP).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 5d84c368ffe4..e465fbf1ef5f 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -43,8 +43,10 @@ void hugetlb_change_protection(struct vm_area_struct *vma,
 
 #ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
 #define is_hugepage_only_range(mm, addr, len)	0
-#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
-						do { } while (0)
+#endif
+
+#ifndef ARCH_HAS_HUGETLB_FREE_PGD_RANGE
+#define hugetlb_free_pgd_range	free_pgd_range
 #endif
 
 #ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE
@@ -93,8 +95,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define prepare_hugepage_range(addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
-#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
-						do { } while (0)
+#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) ({BUG(); 0; })
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 
 #define hugetlb_change_protection(vma, address, end, newprot)

commit 27a85ef1b81300cfff06b4c8037e9914dfb09acc
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 22 00:08:56 2006 -0800

    [PATCH] hugepage: Make {alloc,free}_huge_page() local
    
    Originally, mm/hugetlb.c just handled the hugepage physical allocation path
    and its {alloc,free}_huge_page() functions were used from the arch specific
    hugepage code.  These days those functions are only used with mm/hugetlb.c
    itself.  Therefore, this patch makes them static and removes their
    prototypes from hugetlb.h.  This requires a small rearrangement of code in
    mm/hugetlb.c to avoid a forward declaration.
    
    This patch causes no regressions on the libhugetlbfs testsuite (ppc64,
    POWER5).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index cafe73eecb05..5d84c368ffe4 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -21,8 +21,6 @@ int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);
 unsigned long hugetlb_total_pages(void);
-struct page *alloc_huge_page(struct vm_area_struct *, unsigned long);
-void free_huge_page(struct page *);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write_access);
 
@@ -97,8 +95,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
 						do { } while (0)
-#define alloc_huge_page(vma, addr)		({ NULL; })
-#define free_huge_page(p)			({ (void)(p); BUG(); })
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 
 #define hugetlb_change_protection(vma, address, end, newprot)

commit b45b5bd65f668a665db40d093e4e1fe563533608
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Mar 22 00:08:55 2006 -0800

    [PATCH] hugepage: Strict page reservation for hugepage inodes
    
    These days, hugepages are demand-allocated at first fault time.  There's a
    somewhat dubious (and racy) heuristic when making a new mmap() to check if
    there are enough available hugepages to fully satisfy that mapping.
    
    A particularly obvious case where the heuristic breaks down is where a
    process maps its hugepages not as a single chunk, but as a bunch of
    individually mmap()ed (or shmat()ed) blocks without touching and
    instantiating the pages in between allocations.  In this case the size of
    each block is compared against the total number of available hugepages.
    It's thus easy for the process to become overcommitted, because each block
    mapping will succeed, although the total number of hugepages required by
    all blocks exceeds the number available.  In particular, this defeats such
    a program which will detect a mapping failure and adjust its hugepage usage
    downward accordingly.
    
    The patch below addresses this problem, by strictly reserving a number of
    physical hugepages for hugepage inodes which have been mapped, but not
    instatiated.  MAP_SHARED mappings are thus "safe" - they will fail on
    mmap(), not later with an OOM SIGKILL.  MAP_PRIVATE mappings can still
    trigger an OOM.  (Actually SHARED mappings can technically still OOM, but
    only if the sysadmin explicitly reduces the hugepage pool between mapping
    and instantiation)
    
    This patch appears to address the problem at hand - it allows DB2 to start
    correctly, for instance, which previously suffered the failure described
    above.
    
    This patch causes no regressions on the libhugetblfs testsuite, and makes a
    test (designed to catch this problem) pass which previously failed (ppc64,
    POWER5).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index fa83836b63d2..cafe73eecb05 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -20,7 +20,6 @@ void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long)
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);
-int is_hugepage_mem_enough(size_t);
 unsigned long hugetlb_total_pages(void);
 struct page *alloc_huge_page(struct vm_area_struct *, unsigned long);
 void free_huge_page(struct page *);
@@ -89,7 +88,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
 #define unmap_hugepage_range(vma, start, end)	BUG()
-#define is_hugepage_mem_enough(size)		0
 #define hugetlb_report_meminfo(buf)		0
 #define hugetlb_report_node_meminfo(n, buf)	0
 #define follow_huge_pmd(mm, addr, pmd, write)	NULL
@@ -132,6 +130,8 @@ struct hugetlbfs_sb_info {
 
 struct hugetlbfs_inode_info {
 	struct shared_policy policy;
+	/* Protected by the (global) hugetlb_lock */
+	unsigned long prereserved_hpages;
 	struct inode vfs_inode;
 };
 
@@ -148,6 +148,10 @@ static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
 extern struct file_operations hugetlbfs_file_operations;
 extern struct vm_operations_struct hugetlb_vm_ops;
 struct file *hugetlb_zero_setup(size_t);
+int hugetlb_extend_reservation(struct hugetlbfs_inode_info *info,
+			       unsigned long atleast_hpages);
+void hugetlb_truncate_reservation(struct hugetlbfs_inode_info *info,
+				  unsigned long atmost_hpages);
 int hugetlb_get_quota(struct address_space *mapping);
 void hugetlb_put_quota(struct address_space *mapping);
 

commit 8f860591ffb29738cf5539b6fbf27f50dcdeb380
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Wed Mar 22 00:08:50 2006 -0800

    [PATCH] Enable mprotect on huge pages
    
    2.6.16-rc3 uses hugetlb on-demand paging, but it doesn_t support hugetlb
    mprotect.
    
    From: David Gibson <david@gibson.dropbear.id.au>
    
      Remove a test from the mprotect() path which checks that the mprotect()ed
      range on a hugepage VMA is hugepage aligned (yes, really, the sense of
      is_aligned_hugepage_range() is the opposite of what you'd guess :-/).
    
      In fact, we don't need this test.  If the given addresses match the
      beginning/end of a hugepage VMA they must already be suitably aligned.  If
      they don't, then mprotect_fixup() will attempt to split the VMA.  The very
      first test in split_vma() will check for a badly aligned address on a
      hugepage VMA and return -EINVAL if necessary.
    
    From: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    
      On i386 and x86-64, pte flag _PAGE_PSE collides with _PAGE_PROTNONE.  The
      identify of hugetlb pte is lost when changing page protection via mprotect.
      A page fault occurs later will trigger a bug check in huge_pte_alloc().
    
      The fix is to always make new pte a hugetlb pte and also to clean up
      legacy code where _PAGE_PRESENT is forced on in the pre-faulting day.
    
    Signed-off-by: Zhang Yanmin <yanmin.zhang@intel.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 68d82ad6b17c..fa83836b63d2 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -41,6 +41,8 @@ struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int write);
 int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
 int pmd_huge(pmd_t pmd);
+void hugetlb_change_protection(struct vm_area_struct *vma,
+		unsigned long address, unsigned long end, pgprot_t newprot);
 
 #ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
 #define is_hugepage_only_range(mm, addr, len)	0
@@ -101,6 +103,8 @@ static inline unsigned long hugetlb_total_pages(void)
 #define free_huge_page(p)			({ (void)(p); BUG(); })
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 
+#define hugetlb_change_protection(vma, address, end, newprot)
+
 #ifndef HPAGE_MASK
 #define HPAGE_MASK	PAGE_MASK		/* Keep the compiler happy */
 #define HPAGE_SIZE	PAGE_SIZE

commit 5da7ca86078964cbfe6c83efc1205904587706fe
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Fri Jan 6 00:10:46 2006 -0800

    [PATCH] Add NUMA policy support for huge pages.
    
    The huge_zonelist() function in the memory policy layer provides an list of
    zones ordered by NUMA distance.  The hugetlb layer will walk that list looking
    for a zone that has available huge pages but is also in the nodeset of the
    current cpuset.
    
    This patch does not contain the folding of find_or_alloc_huge_page() that was
    controversial in the earlier discussion.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@muc.de>
    Acked-by: William Lee Irwin III <wli@holomorphy.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 1056717ee501..68d82ad6b17c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -22,7 +22,7 @@ int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);
 int is_hugepage_mem_enough(size_t);
 unsigned long hugetlb_total_pages(void);
-struct page *alloc_huge_page(void);
+struct page *alloc_huge_page(struct vm_area_struct *, unsigned long);
 void free_huge_page(struct page *);
 int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, int write_access);
@@ -97,7 +97,7 @@ static inline unsigned long hugetlb_total_pages(void)
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
 						do { } while (0)
-#define alloc_huge_page()			({ NULL; })
+#define alloc_huge_page(vma, addr)		({ NULL; })
 #define free_huge_page(p)			({ (void)(p); BUG(); })
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 

commit 51c6f666fceb3184eeff045dad4432b602cd648e
Author: Robin Holt <holt@sgi.com>
Date:   Sun Nov 13 16:06:42 2005 -0800

    [PATCH] mm: ZAP_BLOCK causes redundant work
    
    The address based work estimate for unmapping (for lockbreak) is and always
    was horribly inefficient for sparse mappings.  The problem is most simply
    explained with an example:
    
    If we find a pgd is clear, we still have to call into unmap_page_range
    PGDIR_SIZE / ZAP_BLOCK_SIZE times, each time checking the clear pgd, in
    order to progress the working address to the next pgd.
    
    The fundamental way to solve the problem is to keep track of the end
    address we've processed and pass it back to the higher layers.
    
    From: Nick Piggin <npiggin@suse.de>
    
      Modification to completely get away from address based work estimate
      and instead use an abstract count, with a very small cost for empty
      entries as opposed to present pages.
    
      On 2.6.14-git2, ppc64, and CONFIG_PREEMPT=y, mapping and unmapping 1TB
      of virtual address space takes 1.69s; with the following patch applied,
      this operation can be done 1000 times in less than 0.01s
    
    From: Andrew Morton <akpm@osdl.org>
    
    With CONFIG_HUTETLB_PAGE=n:
    
    mm/memory.c: In function `unmap_vmas':
    mm/memory.c:779: warning: division by zero
    
    Due to
    
                            zap_work -= (end - start) /
                                            (HPAGE_SIZE / PAGE_SIZE);
    
    So make the dummy HPAGE_SIZE non-zero
    
    Signed-off-by: Robin Holt <holt@sgi.com>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0cea162b08c0..1056717ee501 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -102,8 +102,8 @@ static inline unsigned long hugetlb_total_pages(void)
 #define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 
 #ifndef HPAGE_MASK
-#define HPAGE_MASK	0		/* Keep the compiler happy */
-#define HPAGE_SIZE	0
+#define HPAGE_MASK	PAGE_MASK		/* Keep the compiler happy */
+#define HPAGE_SIZE	PAGE_SIZE
 #endif
 
 #endif /* !CONFIG_HUGETLB_PAGE */

commit 508034a32b819a2d40aa7ac0dbc8cd2e044c2de6
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:30 2005 -0700

    [PATCH] mm: unmap_vmas with inner ptlock
    
    Remove the page_table_lock from around the calls to unmap_vmas, and replace
    the pte_offset_map in zap_pte_range by pte_offset_map_lock: all callers are
    now safe to descend without page_table_lock.
    
    Don't attempt fancy locking for hugepages, just take page_table_lock in
    unmap_hugepage_range.  Which makes zap_hugepage_range, and the hugetlb test in
    zap_page_range, redundant: unmap_vmas calls unmap_hugepage_range anyway.  Nor
    does unmap_vmas have much use for its mm arg now.
    
    The tlb_start_vma and tlb_end_vma in unmap_page_range are now called without
    page_table_lock: if they're implemented at all, they typically come down to
    flush_cache_range (usually done outside page_table_lock) and flush_tlb_range
    (which we already audited for the mprotect case).
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index d664330d900e..0cea162b08c0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -16,7 +16,6 @@ static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
-void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
 int hugetlb_report_meminfo(char *);
@@ -87,7 +86,6 @@ static inline unsigned long hugetlb_total_pages(void)
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
-#define zap_hugepage_range(vma, start, len)	BUG()
 #define unmap_hugepage_range(vma, start, end)	BUG()
 #define is_hugepage_mem_enough(size)		0
 #define hugetlb_report_meminfo(buf)		0

commit ac9b9c667c2e1194e22ebe0a441ae1c37aaa9b90
Author: Hugh Dickins <hugh@veritas.com>
Date:   Thu Oct 20 16:24:28 2005 +0100

    [PATCH] Fix handling spurious page fault for hugetlb region
    
    This reverts commit 3359b54c8c07338f3a863d1109b42eebccdcf379 and
    replaces it with a cleaner version that is purely based on page table
    operations, so that the synchronization between inode size and hugetlb
    mappings becomes moot.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 42cb7d70f9ac..d664330d900e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -25,6 +25,8 @@ int is_hugepage_mem_enough(size_t);
 unsigned long hugetlb_total_pages(void);
 struct page *alloc_huge_page(void);
 void free_huge_page(struct page *);
+int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+			unsigned long address, int write_access);
 
 extern unsigned long max_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
@@ -99,6 +101,7 @@ static inline unsigned long hugetlb_total_pages(void)
 						do { } while (0)
 #define alloc_huge_page()			({ NULL; })
 #define free_huge_page(p)			({ (void)(p); BUG(); })
+#define hugetlb_fault(mm, vma, addr, write)	({ BUG(); 0; })
 
 #ifndef HPAGE_MASK
 #define HPAGE_MASK	0		/* Keep the compiler happy */
@@ -155,24 +158,11 @@ static inline void set_file_hugepages(struct file *file)
 {
 	file->f_op = &hugetlbfs_file_operations;
 }
-
-static inline int valid_hugetlb_file_off(struct vm_area_struct *vma, 
-					  unsigned long address) 
-{
-	struct inode *inode = vma->vm_file->f_dentry->d_inode;
-	loff_t file_off = address - vma->vm_start;
-	
-	file_off += (vma->vm_pgoff << PAGE_SHIFT);
-	
-	return (file_off < inode->i_size);
-}
-
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)		0
 #define set_file_hugepages(file)	BUG()
 #define hugetlb_zero_setup(size)	ERR_PTR(-ENOSYS)
-#define valid_hugetlb_file_off(vma, address) 	0
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 3359b54c8c07338f3a863d1109b42eebccdcf379
Author: Seth, Rohit <rohit.seth@intel.com>
Date:   Tue Oct 18 14:15:12 2005 -0700

    [PATCH] Handle spurious page fault for hugetlb region
    
    The hugetlb pages are currently pre-faulted.  At the time of mmap of
    hugepages, we populate the new PTEs.  It is possible that HW has already
    cached some of the unused PTEs internally.  These stale entries never
    get a chance to be purged in existing control flow.
    
    This patch extends the check in page fault code for hugepages.  Check if
    a faulted address falls with in size for the hugetlb file backing it.
    We return VM_FAULT_MINOR for these cases (assuming that the arch
    specific page-faulting code purges the stale entry for the archs that
    need it).
    
    Signed-off-by: Rohit Seth <rohit.seth@intel.com>
    
    [ This is apparently arguably an ia64 port bug. But the code won't
      hurt, and for now it fixes a real problem on some ia64 machines ]
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index e670b0d13fe0..42cb7d70f9ac 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -155,11 +155,24 @@ static inline void set_file_hugepages(struct file *file)
 {
 	file->f_op = &hugetlbfs_file_operations;
 }
+
+static inline int valid_hugetlb_file_off(struct vm_area_struct *vma, 
+					  unsigned long address) 
+{
+	struct inode *inode = vma->vm_file->f_dentry->d_inode;
+	loff_t file_off = address - vma->vm_start;
+	
+	file_off += (vma->vm_pgoff << PAGE_SHIFT);
+	
+	return (file_off < inode->i_size);
+}
+
 #else /* !CONFIG_HUGETLBFS */
 
 #define is_file_hugepages(file)		0
 #define set_file_hugepages(file)	BUG()
 #define hugetlb_zero_setup(size)	ERR_PTR(-ENOSYS)
+#define valid_hugetlb_file_off(vma, address) 	0
 
 #endif /* !CONFIG_HUGETLBFS */
 

commit 0e5c9f39f64d8a55c5db37a5ea43e37d3422fd92
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Sat Sep 3 15:55:02 2005 -0700

    [PATCH] remove hugetlb_clean_stale_pgtable() and fix huge_pte_alloc()
    
    I don't think we need to call hugetlb_clean_stale_pgtable() anymore
    in 2.6.13 because of the rework with free_pgtables().  It now collect
    all the pte page at the time of munmap.  It used to only collect page
    table pages when entire one pgd can be freed and left with staled pte
    pages.  Not anymore with 2.6.13.  This function will never be called
    and We should turn it into a BUG_ON.
    
    I also spotted two problems here, not Adam's fault :-)
    (1) in huge_pte_alloc(), it looks like a bug to me that pud is not
        checked before calling pmd_alloc()
    (2) in hugetlb_clean_stale_pgtable(), it also missed a call to
        pmd_free_tlb.  I think a tlb flush is required to flush the mapping
        for the page table itself when we clear out the pmd pointing to a
        pte page.  However, since hugetlb_clean_stale_pgtable() is never
        called, so it won't trigger the bug.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index f529d1442815..e670b0d13fe0 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -70,12 +70,6 @@ pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 void hugetlb_prefault_arch_hook(struct mm_struct *mm);
 #endif
 
-#ifndef ARCH_HAS_HUGETLB_CLEAN_STALE_PGTABLE
-#define hugetlb_clean_stale_pgtable(pte)	BUG()
-#else
-void hugetlb_clean_stale_pgtable(pte_t *pte);
-#endif
-
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)

commit 63551ae0feaaa23807ebea60de1901564bbef32e
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Jun 21 17:14:44 2005 -0700

    [PATCH] Hugepage consolidation
    
    A lot of the code in arch/*/mm/hugetlbpage.c is quite similar.  This patch
    attempts to consolidate a lot of the code across the arch's, putting the
    combined version in mm/hugetlb.c.  There are a couple of uglyish hacks in
    order to covert all the hugepage archs, but the result is a very large
    reduction in the total amount of code.  It also means things like hugepage
    lazy allocation could be implemented in one place, instead of six.
    
    Tested, at least a little, on ppc64, i386 and x86_64.
    
    Notes:
            - this patch changes the meaning of set_huge_pte() to be more
              analagous to set_pte()
            - does SH4 need s special huge_ptep_get_and_clear()??
    
    Acked-by: William Lee Irwin <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 6af1ae4a8211..f529d1442815 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -4,6 +4,7 @@
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>
+#include <asm/tlbflush.h>
 
 struct ctl_table;
 
@@ -22,12 +23,6 @@ int hugetlb_report_meminfo(char *);
 int hugetlb_report_node_meminfo(int, char *);
 int is_hugepage_mem_enough(size_t);
 unsigned long hugetlb_total_pages(void);
-struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
-			      int write);
-struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
-				pmd_t *pmd, int write);
-int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
-int pmd_huge(pmd_t pmd);
 struct page *alloc_huge_page(void);
 void free_huge_page(struct page *);
 
@@ -35,6 +30,17 @@ extern unsigned long max_huge_pages;
 extern const unsigned long hugetlb_zero, hugetlb_infinity;
 extern int sysctl_hugetlb_shm_group;
 
+/* arch callbacks */
+
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr);
+pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);
+struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
+			      int write);
+struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+				pmd_t *pmd, int write);
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
+int pmd_huge(pmd_t pmd);
+
 #ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
 #define is_hugepage_only_range(mm, addr, len)	0
 #define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
@@ -48,6 +54,28 @@ extern int sysctl_hugetlb_shm_group;
 int prepare_hugepage_range(unsigned long addr, unsigned long len);
 #endif
 
+#ifndef ARCH_HAS_SETCLEAR_HUGE_PTE
+#define set_huge_pte_at(mm, addr, ptep, pte)	set_pte_at(mm, addr, ptep, pte)
+#define huge_ptep_get_and_clear(mm, addr, ptep) ptep_get_and_clear(mm, addr, ptep)
+#else
+void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+		     pte_t *ptep, pte_t pte);
+pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep);
+#endif
+
+#ifndef ARCH_HAS_HUGETLB_PREFAULT_HOOK
+#define hugetlb_prefault_arch_hook(mm)		do { } while (0)
+#else
+void hugetlb_prefault_arch_hook(struct mm_struct *mm);
+#endif
+
+#ifndef ARCH_HAS_HUGETLB_CLEAN_STALE_PGTABLE
+#define hugetlb_clean_stale_pgtable(pte)	BUG()
+#else
+void hugetlb_clean_stale_pgtable(pte_t *pte);
+#endif
+
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)

commit 3bf5ee95648c694bac4d13529563c230cd4fe5f2
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Apr 19 13:29:16 2005 -0700

    [PATCH] freepgt: hugetlb_free_pgd_range
    
    ia64 and ppc64 had hugetlb_free_pgtables functions which were no longer being
    called, and it wasn't obvious what to do about them.
    
    The ppc64 case turns out to be easy: the associated tables are noted elsewhere
    and freed later, safe to either skip its hugetlb areas or go through the
    motions of freeing nothing.  Since ia64 does need a special case, restore to
    ppc64 the special case of skipping them.
    
    The ia64 hugetlb case has been broken since pgd_addr_end went in, though it
    probably appeared to work okay if you just had one such area; in fact it's
    been broken much longer if you consider a long munmap spanning from another
    region into the hugetlb region.
    
    In the ia64 hugetlb region, more virtual address bits are available than in
    the other regions, yet the page tables are structured the same way: the page
    at the bottom is larger.  Here we need to scale down each addr before passing
    it to the standard free_pgd_range.  Was about to write a hugely_scaled_down
    macro, but found htlbpage_to_page already exists for just this purpose.  Fixed
    off-by-one in ia64 is_hugepage_only_range.
    
    Uninline free_pgd_range to make it available to ia64.  Make sure the
    vma-gathering loop in free_pgtables cannot join a hugepage_only_range to any
    other (safe to join huges?  probably but don't bother).
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index ae45676d27ba..6af1ae4a8211 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -37,7 +37,8 @@ extern int sysctl_hugetlb_shm_group;
 
 #ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
 #define is_hugepage_only_range(mm, addr, len)	0
-#define hugetlb_free_pgtables(tlb, prev, start, end) do { } while (0)
+#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
+						do { } while (0)
 #endif
 
 #ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE
@@ -72,7 +73,8 @@ static inline unsigned long hugetlb_total_pages(void)
 #define prepare_hugepage_range(addr, len)	(-EINVAL)
 #define pmd_huge(x)	0
 #define is_hugepage_only_range(mm, addr, len)	0
-#define hugetlb_free_pgtables(tlb, prev, start, end) do { } while (0)
+#define hugetlb_free_pgd_range(tlb, addr, end, floor, ceiling) \
+						do { } while (0)
 #define alloc_huge_page()			({ NULL; })
 #define free_huge_page(p)			({ (void)(p); BUG(); })
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
new file mode 100644
index 000000000000..ae45676d27ba
--- /dev/null
+++ b/include/linux/hugetlb.h
@@ -0,0 +1,142 @@
+#ifndef _LINUX_HUGETLB_H
+#define _LINUX_HUGETLB_H
+
+#ifdef CONFIG_HUGETLB_PAGE
+
+#include <linux/mempolicy.h>
+
+struct ctl_table;
+
+static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
+{
+	return vma->vm_flags & VM_HUGETLB;
+}
+
+int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void __user *, size_t *, loff_t *);
+int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
+int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
+int hugetlb_report_meminfo(char *);
+int hugetlb_report_node_meminfo(int, char *);
+int is_hugepage_mem_enough(size_t);
+unsigned long hugetlb_total_pages(void);
+struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
+			      int write);
+struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+				pmd_t *pmd, int write);
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len);
+int pmd_huge(pmd_t pmd);
+struct page *alloc_huge_page(void);
+void free_huge_page(struct page *);
+
+extern unsigned long max_huge_pages;
+extern const unsigned long hugetlb_zero, hugetlb_infinity;
+extern int sysctl_hugetlb_shm_group;
+
+#ifndef ARCH_HAS_HUGEPAGE_ONLY_RANGE
+#define is_hugepage_only_range(mm, addr, len)	0
+#define hugetlb_free_pgtables(tlb, prev, start, end) do { } while (0)
+#endif
+
+#ifndef ARCH_HAS_PREPARE_HUGEPAGE_RANGE
+#define prepare_hugepage_range(addr, len)	\
+	is_aligned_hugepage_range(addr, len)
+#else
+int prepare_hugepage_range(unsigned long addr, unsigned long len);
+#endif
+
+#else /* !CONFIG_HUGETLB_PAGE */
+
+static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
+{
+	return 0;
+}
+static inline unsigned long hugetlb_total_pages(void)
+{
+	return 0;
+}
+
+#define follow_hugetlb_page(m,v,p,vs,a,b,i)	({ BUG(); 0; })
+#define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
+#define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
+#define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
+#define zap_hugepage_range(vma, start, len)	BUG()
+#define unmap_hugepage_range(vma, start, end)	BUG()
+#define is_hugepage_mem_enough(size)		0
+#define hugetlb_report_meminfo(buf)		0
+#define hugetlb_report_node_meminfo(n, buf)	0
+#define follow_huge_pmd(mm, addr, pmd, write)	NULL
+#define is_aligned_hugepage_range(addr, len)	0
+#define prepare_hugepage_range(addr, len)	(-EINVAL)
+#define pmd_huge(x)	0
+#define is_hugepage_only_range(mm, addr, len)	0
+#define hugetlb_free_pgtables(tlb, prev, start, end) do { } while (0)
+#define alloc_huge_page()			({ NULL; })
+#define free_huge_page(p)			({ (void)(p); BUG(); })
+
+#ifndef HPAGE_MASK
+#define HPAGE_MASK	0		/* Keep the compiler happy */
+#define HPAGE_SIZE	0
+#endif
+
+#endif /* !CONFIG_HUGETLB_PAGE */
+
+#ifdef CONFIG_HUGETLBFS
+struct hugetlbfs_config {
+	uid_t   uid;
+	gid_t   gid;
+	umode_t mode;
+	long	nr_blocks;
+	long	nr_inodes;
+};
+
+struct hugetlbfs_sb_info {
+	long	max_blocks;   /* blocks allowed */
+	long	free_blocks;  /* blocks free */
+	long	max_inodes;   /* inodes allowed */
+	long	free_inodes;  /* inodes free */
+	spinlock_t	stat_lock;
+};
+
+
+struct hugetlbfs_inode_info {
+	struct shared_policy policy;
+	struct inode vfs_inode;
+};
+
+static inline struct hugetlbfs_inode_info *HUGETLBFS_I(struct inode *inode)
+{
+	return container_of(inode, struct hugetlbfs_inode_info, vfs_inode);
+}
+
+static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
+{
+	return sb->s_fs_info;
+}
+
+extern struct file_operations hugetlbfs_file_operations;
+extern struct vm_operations_struct hugetlb_vm_ops;
+struct file *hugetlb_zero_setup(size_t);
+int hugetlb_get_quota(struct address_space *mapping);
+void hugetlb_put_quota(struct address_space *mapping);
+
+static inline int is_file_hugepages(struct file *file)
+{
+	return file->f_op == &hugetlbfs_file_operations;
+}
+
+static inline void set_file_hugepages(struct file *file)
+{
+	file->f_op = &hugetlbfs_file_operations;
+}
+#else /* !CONFIG_HUGETLBFS */
+
+#define is_file_hugepages(file)		0
+#define set_file_hugepages(file)	BUG()
+#define hugetlb_zero_setup(size)	ERR_PTR(-ENOSYS)
+
+#endif /* !CONFIG_HUGETLBFS */
+
+#endif /* _LINUX_HUGETLB_H */
