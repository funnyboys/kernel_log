commit b58e733fd774f3f4b49d9e7640d172a57e35200e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 15 18:24:27 2020 +0200

    rcu: Fixup noinstr warnings
    
    A KCSAN build revealed we have explicit annoations through atomic_*()
    usage, switch to arch_atomic_*() for the respective functions.
    
    vmlinux.o: warning: objtool: rcu_nmi_exit()+0x4d: call to __kcsan_check_access() leaves .noinstr.text section
    vmlinux.o: warning: objtool: rcu_dynticks_eqs_enter()+0x25: call to __kcsan_check_access() leaves .noinstr.text section
    vmlinux.o: warning: objtool: rcu_nmi_enter()+0x4f: call to __kcsan_check_access() leaves .noinstr.text section
    vmlinux.o: warning: objtool: rcu_dynticks_eqs_exit()+0x2a: call to __kcsan_check_access() leaves .noinstr.text section
    vmlinux.o: warning: objtool: __rcu_is_watching()+0x25: call to __kcsan_check_access() leaves .noinstr.text section
    
    Additionally, without the NOP in instrumentation_begin(), objtool would
    not detect the lack of the 'else instrumentation_begin();' branch in
    rcu_nmi_enter().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 30827f82ad62..204e76856435 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -123,7 +123,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #ifdef CONFIG_DEBUG_ENTRY
 /* Begin/end of an instrumentation safe region */
 #define instrumentation_begin() ({					\
-	asm volatile("%c0:\n\t"						\
+	asm volatile("%c0: nop\n\t"						\
 		     ".pushsection .discard.instr_begin\n\t"		\
 		     ".long %c0b - .\n\t"				\
 		     ".popsection\n\t" : : "i" (__COUNTER__));		\

commit eb73876c74313231c35cee6310f8ad62c56fa2b3
Author: Marco Elver <elver@google.com>
Date:   Thu May 21 16:20:46 2020 +0200

    compiler.h: Move function attributes to compiler_types.h
    
    Cleanup and move the KASAN and KCSAN related function attributes to
    compiler_types.h, where the rest of the same kind live.
    
    No functional change intended.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200521142047.169334-11-elver@google.com

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f0bfbe82f0fe..30827f82ad62 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -303,35 +303,6 @@ do {									\
 	__WRITE_ONCE(x, val);						\
 } while (0)
 
-#ifdef CONFIG_KASAN
-/*
- * We can't declare function 'inline' because __no_sanitize_address conflicts
- * with inlining. Attempt to inline it may cause a build failure.
- *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
- * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
- */
-# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused
-# define __no_sanitize_or_inline __no_kasan_or_inline
-#else
-# define __no_kasan_or_inline __always_inline
-#endif
-
-#define __no_kcsan __no_sanitize_thread
-#ifdef __SANITIZE_THREAD__
-/*
- * Rely on __SANITIZE_THREAD__ instead of CONFIG_KCSAN, to avoid not inlining in
- * compilation units where instrumentation is disabled.
- */
-# define __no_kcsan_or_inline __no_kcsan notrace __maybe_unused
-# define __no_sanitize_or_inline __no_kcsan_or_inline
-#else
-# define __no_kcsan_or_inline __always_inline
-#endif
-
-#ifndef __no_sanitize_or_inline
-#define __no_sanitize_or_inline __always_inline
-#endif
-
 static __no_sanitize_or_inline
 unsigned long __read_once_word_nocheck(const void *addr)
 {

commit 95c094fccb85422eff3c12930ebebbda9278f76b
Author: Marco Elver <elver@google.com>
Date:   Thu May 21 16:20:45 2020 +0200

    compiler.h: Avoid nested statement expression in data_race()
    
    It appears that compilers have trouble with nested statement
    expressions. Therefore, remove one level of statement expression nesting
    from the data_race() macro. This will help avoiding potential problems
    in the future as its usage increases.
    
    Reported-by: Borislav Petkov <bp@suse.de>
    Reported-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Tested-by: Nick Desaulniers <ndesaulniers@google.com>
    Link: https://lkml.kernel.org/r/20200520221712.GA21166@zn.tnic
    Link: https://lkml.kernel.org/r/20200521142047.169334-10-elver@google.com

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 7b090d263fec..f0bfbe82f0fe 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -264,12 +264,12 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  */
 #define data_race(expr)							\
 ({									\
-	__kcsan_disable_current();					\
-	({								\
-		__unqual_scalar_typeof(({ expr; })) __v = ({ expr; });	\
-		__kcsan_enable_current();				\
-		__v;							\
+	__unqual_scalar_typeof(({ expr; })) __v = ({			\
+		__kcsan_disable_current();				\
+		expr;							\
 	});								\
+	__kcsan_enable_current();					\
+	__v;								\
 })
 
 /*

commit 44b97dccb2291a56454549827adc5e99d94811f3
Author: Marco Elver <elver@google.com>
Date:   Thu May 21 16:20:44 2020 +0200

    compiler.h: Remove data_race() and unnecessary checks from {READ,WRITE}_ONCE()
    
    The volatile accesses no longer need to be wrapped in data_race()
    because compilers that emit instrumentation distinguishing volatile
    accesses are required for KCSAN.
    
    Consequently, the explicit kcsan_check_atomic*() are no longer required
    either since the compiler emits instrumentation distinguishing the
    volatile accesses.
    
    Finally, simplify __READ_ONCE_SCALAR() and remove __WRITE_ONCE_SCALAR().
    
     [ bp: Convert commit message to passive voice. ]
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200521142047.169334-9-elver@google.com

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 6d307c0aa4b4..7b090d263fec 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -281,9 +281,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 
 #define __READ_ONCE_SCALAR(x)						\
 ({									\
-	typeof(x) *__xp = &(x);						\
-	__unqual_scalar_typeof(x) __x = data_race(__READ_ONCE(*__xp));	\
-	kcsan_check_atomic_read(__xp, sizeof(*__xp));			\
+	__unqual_scalar_typeof(x) __x = __READ_ONCE(x);			\
 	smp_read_barrier_depends();					\
 	(typeof(x))__x;							\
 })
@@ -299,17 +297,10 @@ do {									\
 	*(volatile typeof(x) *)&(x) = (val);				\
 } while (0)
 
-#define __WRITE_ONCE_SCALAR(x, val)					\
-do {									\
-	typeof(x) *__xp = &(x);						\
-	kcsan_check_atomic_write(__xp, sizeof(*__xp));			\
-	data_race(({ __WRITE_ONCE(*__xp, val); 0; }));			\
-} while (0)
-
 #define WRITE_ONCE(x, val)						\
 do {									\
 	compiletime_assert_rwonce_type(x);				\
-	__WRITE_ONCE_SCALAR(x, val);					\
+	__WRITE_ONCE(x, val);						\
 } while (0)
 
 #ifdef CONFIG_KASAN

commit e3b779d9ebe82b4be0121f25f27632844bb86d96
Author: Marco Elver <elver@google.com>
Date:   Thu May 21 16:20:41 2020 +0200

    kcsan: Remove 'noinline' from __no_kcsan_or_inline
    
    Some compilers incorrectly inline small __no_kcsan functions, which then
    results in instrumenting the accesses. For this reason, the 'noinline'
    attribute was added to __no_kcsan_or_inline. All known versions of GCC
    are affected by this. Supported versions of Clang are unaffected, and
    never inline a no_sanitize function.
    
    However, the attribute 'noinline' in __no_kcsan_or_inline causes
    unexpected code generation in functions that are __no_kcsan and call a
    __no_kcsan_or_inline function.
    
    In certain situations it is expected that the __no_kcsan_or_inline
    function is actually inlined by the __no_kcsan function, and *no* calls
    are emitted. By removing the 'noinline' attribute, give the compiler
    the ability to inline and generate the expected code in __no_kcsan
    functions.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/CANpmjNNOpJk0tprXKB_deiNAv_UmmORf1-2uajLhnLWQQ1hvoA@mail.gmail.com
    Link: https://lkml.kernel.org/r/20200521142047.169334-6-elver@google.com

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f09ebbf16562..6d307c0aa4b4 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -329,11 +329,9 @@ do {									\
 #ifdef __SANITIZE_THREAD__
 /*
  * Rely on __SANITIZE_THREAD__ instead of CONFIG_KCSAN, to avoid not inlining in
- * compilation units where instrumentation is disabled. The attribute 'noinline'
- * is required for older compilers, where implicit inlining of very small
- * functions renders __no_sanitize_thread ineffective.
+ * compilation units where instrumentation is disabled.
  */
-# define __no_kcsan_or_inline __no_kcsan noinline notrace __maybe_unused
+# define __no_kcsan_or_inline __no_kcsan notrace __maybe_unused
 # define __no_sanitize_or_inline __no_kcsan_or_inline
 #else
 # define __no_kcsan_or_inline __always_inline

commit 37d1a04b13a6d2fec91a6813fc034947a27db034
Merge: 37f8173dd849 97a9474aeb78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 11 20:02:46 2020 +0200

    Rebase locking/kcsan to locking/urgent
    
    Merge the state of the locking kcsan branch before the read/write_once()
    and the atomics modifications got merged.
    
    Squash the fallout of the rebase on top of the read/write once and atomic
    fallback work into the merge. The history of the original branch is
    preserved in tag locking-kcsan-2020-06-02.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 4152d146ee2169653297e03b9fa2e0f476923959
Merge: 78c24f7beeae b398ace5d2ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 14:46:54 2020 -0700

    Merge branch 'rwonce/rework' of git://git.kernel.org/pub/scm/linux/kernel/git/will/linux
    
    Pull READ/WRITE_ONCE rework from Will Deacon:
     "This the READ_ONCE rework I've been working on for a while, which
      bumps the minimum GCC version and improves code-gen on arm64 when
      stack protector is enabled"
    
    [ Side note: I'm _really_ tempted to raise the minimum gcc version to
      4.9, so that we can just say that we require _Generic() support.
    
      That would allow us to more cleanly handle a lot of the cases where we
      depend on very complex macros with 'sizeof' or __builtin_choose_expr()
      with __builtin_types_compatible_p() etc.
    
      This branch has a workaround for sparse not handling _Generic(),
      either, but that was already fixed in the sparse development branch,
      so it's really just gcc-4.9 that we'd require.   - Linus ]
    
    * 'rwonce/rework' of git://git.kernel.org/pub/scm/linux/kernel/git/will/linux:
      compiler_types.h: Use unoptimized __unqual_scalar_typeof for sparse
      compiler_types.h: Optimize __unqual_scalar_typeof compilation time
      compiler.h: Enforce that READ_ONCE_NOCHECK() access size is sizeof(long)
      compiler-types.h: Include naked type in __pick_integer_type() match
      READ_ONCE: Fix comment describing 2x32-bit atomicity
      gcov: Remove old GCC 3.4 support
      arm64: barrier: Use '__unqual_scalar_typeof' for acquire/release macros
      locking/barriers: Use '__unqual_scalar_typeof' for load-acquire macros
      READ_ONCE: Drop pointer qualifiers when reading from scalar types
      READ_ONCE: Enforce atomicity for {READ,WRITE}_ONCE() memory accesses
      READ_ONCE: Simplify implementations of {READ,WRITE}_ONCE()
      arm64: csum: Disable KASAN for do_csum()
      fault_inject: Don't rely on "return value" from WRITE_ONCE()
      net: tls: Avoid assigning 'const' pointer to non-const pointer
      netfilter: Avoid assigning 'const' pointer to non-const pointer
      compiler/gcc: Raise minimum GCC version for kernel builds to 4.8

commit b16d8ecf4fa17e16fff20638364f9bd2205615e7
Author: Will Deacon <will@kernel.org>
Date:   Fri Jun 5 11:19:46 2020 +0100

    compiler.h: Enforce that READ_ONCE_NOCHECK() access size is sizeof(long)
    
    READ_ONCE_NOCHECK() unconditionally performs a sizeof(long)-sized access,
    so enforce that the size of the pointed-to object that we are loading
    from is the same size as 'long'.
    
    Reported-by: Marco Elver <elver@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 657e4fd38a77..a0aa56e6b782 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -254,9 +254,12 @@ unsigned long __read_once_word_nocheck(const void *addr)
  */
 #define READ_ONCE_NOCHECK(x)						\
 ({									\
-	unsigned long __x = __read_once_word_nocheck(&(x));		\
+	unsigned long __x;						\
+	compiletime_assert(sizeof(x) == sizeof(__x),			\
+		"Unsupported access size for READ_ONCE_NOCHECK().");	\
+	__x = __read_once_word_nocheck(&(x));				\
 	smp_read_barrier_depends();					\
-	__x;								\
+	(typeof(x))__x;							\
 })
 
 static __no_kasan_or_inline

commit 5872f1a2e5c783783d51e96468f0ff6aede61182
Author: Will Deacon <will@kernel.org>
Date:   Mon May 11 21:59:51 2020 +0100

    READ_ONCE: Fix comment describing 2x32-bit atomicity
    
    READ_ONCE() permits 64-bit accesses on 32-bit architectures, since this
    crops up in a few places and is generally harmless because either the
    upper bits are always zero (e.g. for a virtual address or 32-bit time_t)
    or the architecture provides 64-bit atomicity anyway.
    
    Update the corresponding comment above compiletime_assert_rwonce_type(),
    which incorrectly states that 32-bit x86 provides 64-bit atomicity, and
    instead reference 32-bit Armv7 with LPAE.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c363d8debc43..657e4fd38a77 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -332,9 +332,9 @@ static inline void *offset_to_ptr(const int *off)
 
 /*
  * Yes, this permits 64-bit accesses on 32-bit architectures. These will
- * actually be atomic in many cases (namely x86), but for others we rely on
- * the access being split into 2x32-bit accesses for a 32-bit quantity (e.g.
- * a virtual address) and a strong prevailing wind.
+ * actually be atomic in some cases (namely Armv7 + LPAE), but for others we
+ * rely on the access being split into 2x32-bit accesses for a 32-bit quantity
+ * (e.g. a virtual address) and a strong prevailing wind.
  */
 #define compiletime_assert_rwonce_type(t)					\
 	compiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),	\

commit 0bd957eb11cfeef23fcc240edde6dfe431731e69
Merge: 9bf9511e3d9f 66e9b0717102
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 12:45:04 2020 -0700

    Merge tag 'core-kprobes-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull kprobes updates from Ingo Molnar:
     "Various kprobes updates, mostly centered around cleaning up the
      no-instrumentation logic.
    
      Instead of the current per debug facility blacklist, use the more
      generic .noinstr.text approach, combined with a 'noinstr' marker for
      functions.
    
      Also add instrumentation_begin()/end() to better manage the exact
      place in entry code where instrumentation may be used.
    
      And add a kprobes blacklist for modules"
    
    * tag 'core-kprobes-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      kprobes: Prevent probes in .noinstr.text section
      vmlinux.lds.h: Create section for protection against instrumentation
      samples/kprobes: Add __kprobes and NOKPROBE_SYMBOL() for handlers.
      kprobes: Support NOKPROBE_SYMBOL() in modules
      kprobes: Support __kprobes blacklist in modules
      kprobes: Lock kprobe_mutex while showing kprobe_blacklist

commit 6553896666433e7efec589838b400a2a652b3ffa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 9 22:47:17 2020 +0100

    vmlinux.lds.h: Create section for protection against instrumentation
    
    Some code pathes, especially the low level entry code, must be protected
    against instrumentation for various reasons:
    
     - Low level entry code can be a fragile beast, especially on x86.
    
     - With NO_HZ_FULL RCU state needs to be established before using it.
    
    Having a dedicated section for such code allows to validate with tooling
    that no unsafe functions are invoked.
    
    Add the .noinstr.text section and the noinstr attribute to mark
    functions. noinstr implies notrace. Kprobes will gain a section check
    later.
    
    Provide also a set of markers: instrumentation_begin()/end()
    
    These are used to mark code inside a noinstr function which calls
    into regular instrumentable text section as safe.
    
    The instrumentation markers are only active when CONFIG_DEBUG_ENTRY is
    enabled as the end marker emits a NOP to prevent the compiler from merging
    the annotation points. This means the objtool verification requires a
    kernel compiled with this option.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134100.075416272@linutronix.de

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 034b0a644efc..e9ead0505671 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -120,12 +120,65 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 /* Annotate a C jump table to allow objtool to follow the code flow */
 #define __annotate_jump_table __section(.rodata..c_jump_table)
 
+#ifdef CONFIG_DEBUG_ENTRY
+/* Begin/end of an instrumentation safe region */
+#define instrumentation_begin() ({					\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.instr_begin\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
+})
+
+/*
+ * Because instrumentation_{begin,end}() can nest, objtool validation considers
+ * _begin() a +1 and _end() a -1 and computes a sum over the instructions.
+ * When the value is greater than 0, we consider instrumentation allowed.
+ *
+ * There is a problem with code like:
+ *
+ * noinstr void foo()
+ * {
+ *	instrumentation_begin();
+ *	...
+ *	if (cond) {
+ *		instrumentation_begin();
+ *		...
+ *		instrumentation_end();
+ *	}
+ *	bar();
+ *	instrumentation_end();
+ * }
+ *
+ * If instrumentation_end() would be an empty label, like all the other
+ * annotations, the inner _end(), which is at the end of a conditional block,
+ * would land on the instruction after the block.
+ *
+ * If we then consider the sum of the !cond path, we'll see that the call to
+ * bar() is with a 0-value, even though, we meant it to happen with a positive
+ * value.
+ *
+ * To avoid this, have _end() be a NOP instruction, this ensures it will be
+ * part of the condition block and does not escape.
+ */
+#define instrumentation_end() ({					\
+	asm volatile("%c0: nop\n\t"					\
+		     ".pushsection .discard.instr_end\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
+})
+#endif /* CONFIG_DEBUG_ENTRY */
+
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
 #define __annotate_jump_table
 #endif
 
+#ifndef instrumentation_begin
+#define instrumentation_begin()		do { } while(0)
+#define instrumentation_end()		do { } while(0)
+#endif
+
 #ifndef ASM_UNREACHABLE
 # define ASM_UNREACHABLE
 #endif

commit a9a3ed1eff3601b63aea4fb462d8b3b92c7c1e7e
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Apr 22 18:11:30 2020 +0200

    x86: Fix early boot crash on gcc-10, third try
    
    ... or the odyssey of trying to disable the stack protector for the
    function which generates the stack canary value.
    
    The whole story started with Sergei reporting a boot crash with a kernel
    built with gcc-10:
    
      Kernel panic — not syncing: stack-protector: Kernel stack is corrupted in: start_secondary
      CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.6.0-rc5—00235—gfffb08b37df9 #139
      Hardware name: Gigabyte Technology Co., Ltd. To be filled by O.E.M./H77M—D3H, BIOS F12 11/14/2013
      Call Trace:
        dump_stack
        panic
        ? start_secondary
        __stack_chk_fail
        start_secondary
        secondary_startup_64
      -—-[ end Kernel panic — not syncing: stack—protector: Kernel stack is corrupted in: start_secondary
    
    This happens because gcc-10 tail-call optimizes the last function call
    in start_secondary() - cpu_startup_entry() - and thus emits a stack
    canary check which fails because the canary value changes after the
    boot_init_stack_canary() call.
    
    To fix that, the initial attempt was to mark the one function which
    generates the stack canary with:
    
      __attribute__((optimize("-fno-stack-protector"))) ... start_secondary(void *unused)
    
    however, using the optimize attribute doesn't work cumulatively
    as the attribute does not add to but rather replaces previously
    supplied optimization options - roughly all -fxxx options.
    
    The key one among them being -fno-omit-frame-pointer and thus leading to
    not present frame pointer - frame pointer which the kernel needs.
    
    The next attempt to prevent compilers from tail-call optimizing
    the last function call cpu_startup_entry(), shy of carving out
    start_secondary() into a separate compilation unit and building it with
    -fno-stack-protector, was to add an empty asm("").
    
    This current solution was short and sweet, and reportedly, is supported
    by both compilers but we didn't get very far this time: future (LTO?)
    optimization passes could potentially eliminate this, which leads us
    to the third attempt: having an actual memory barrier there which the
    compiler cannot ignore or move around etc.
    
    That should hold for a long time, but hey we said that about the other
    two solutions too so...
    
    Reported-by: Sergei Trofimovich <slyfox@gentoo.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Tested-by: Kalle Valo <kvalo@codeaurora.org>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200314164451.346497-1-slyfox@gentoo.org

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 034b0a644efc..448c91bf543b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -356,4 +356,10 @@ static inline void *offset_to_ptr(const int *off)
 /* &a[0] degrades to a pointer: a different type from an array */
 #define __must_be_array(a)	BUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))
 
+/*
+ * This is needed in functions which generate the stack canary, see
+ * arch/x86/kernel/smpboot.c::start_secondary() for an example.
+ */
+#define prevent_tail_call_optimization()	mb()
+
 #endif /* __LINUX_COMPILER_H */

commit 97a9474aeb789183a1d0712e66a4283860279ac9
Merge: 3b02a051d25d 50a19ad4b1ec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 8 14:58:28 2020 +0200

    Merge branch 'kcsan-for-tip' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into locking/kcsan
    
    Pull KCSAN updates from Paul McKenney.

commit dee081bf8f824cabeb7c7495367d5dad0a444eb1
Author: Will Deacon <will@kernel.org>
Date:   Thu Dec 19 14:22:31 2019 +0000

    READ_ONCE: Drop pointer qualifiers when reading from scalar types
    
    Passing a volatile-qualified pointer to READ_ONCE() is an absolute
    trainwreck for code generation: the use of 'typeof()' to define a
    temporary variable inside the macro means that the final evaluation in
    macro scope ends up forcing a read back from the stack. When stack
    protector is enabled (the default for arm64, at least), this causes
    the compiler to vomit up all sorts of junk.
    
    Unfortunately, dropping pointer qualifiers inside the macro poses quite
    a challenge, especially since the pointed-to type is permitted to be an
    aggregate, and this is relied upon by mm/ code accessing things like
    'pmd_t'. Based on numerous hacks and discussions on the mailing list,
    this is the best I've managed to come up with.
    
    Introduce '__unqual_scalar_typeof()' which takes an expression and, if
    the expression is an optionally qualified 8, 16, 32 or 64-bit scalar
    type, evaluates to the unqualified type. Other input types, including
    aggregates, remain unchanged. Hopefully READ_ONCE() on volatile aggregate
    pointers isn't something we do on a fast-path.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 50bb2461648f..c363d8debc43 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -203,13 +203,13 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * atomicity or dependency ordering guarantees. Note that this may result
  * in tears!
  */
-#define __READ_ONCE(x)	(*(const volatile typeof(x) *)&(x))
+#define __READ_ONCE(x)	(*(const volatile __unqual_scalar_typeof(x) *)&(x))
 
 #define __READ_ONCE_SCALAR(x)						\
 ({									\
-	typeof(x) __x = __READ_ONCE(x);					\
+	__unqual_scalar_typeof(x) __x = __READ_ONCE(x);			\
 	smp_read_barrier_depends();					\
-	__x;								\
+	(typeof(x))__x;							\
 })
 
 #define READ_ONCE(x)							\

commit 9e343b467c70379e66b8b771d96f03ae23eba351
Author: Will Deacon <will@kernel.org>
Date:   Fri Dec 13 14:47:02 2019 +0000

    READ_ONCE: Enforce atomicity for {READ,WRITE}_ONCE() memory accesses
    
    {READ,WRITE}_ONCE() cannot guarantee atomicity for arbitrary data sizes.
    This can be surprising to callers that might incorrectly be expecting
    atomicity for accesses to aggregate structures, although there are other
    callers where tearing is actually permissable (e.g. if they are using
    something akin to sequence locking to protect the access).
    
    Linus sayeth:
    
      | We could also look at being stricter for the normal READ/WRITE_ONCE(),
      | and require that they are
      |
      | (a) regular integer types
      |
      | (b) fit in an atomic word
      |
      | We actually did (b) for a while, until we noticed that we do it on
      | loff_t's etc and relaxed the rules. But maybe we could have a
      | "non-atomic" version of READ/WRITE_ONCE() that is used for the
      | questionable cases?
    
    The slight snag is that we also have to support 64-bit accesses on 32-bit
    architectures, as these appear to be widespread and tend to work out ok
    if either the architecture supports atomic 64-bit accesses (x86, armv7)
    or if the variable being accesses represents a virtual address and
    therefore only requires 32-bit atomicity in practice.
    
    Take a step in that direction by introducing a variant of
    'compiletime_assert_atomic_type()' and use it to check the pointer
    argument to {READ,WRITE}_ONCE(). Expose __{READ,WRITE}_ONCE() variants
    which are allowed to tear and convert the one broken caller over to the
    new macros.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 338111a448d0..50bb2461648f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -198,20 +198,37 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #include <asm/barrier.h>
 #include <linux/kasan-checks.h>
 
-#define __READ_ONCE(x)	(*(volatile typeof(x) *)&(x))
+/*
+ * Use __READ_ONCE() instead of READ_ONCE() if you do not require any
+ * atomicity or dependency ordering guarantees. Note that this may result
+ * in tears!
+ */
+#define __READ_ONCE(x)	(*(const volatile typeof(x) *)&(x))
 
-#define READ_ONCE(x)							\
+#define __READ_ONCE_SCALAR(x)						\
 ({									\
 	typeof(x) __x = __READ_ONCE(x);					\
 	smp_read_barrier_depends();					\
 	__x;								\
 })
 
-#define WRITE_ONCE(x, val)				\
+#define READ_ONCE(x)							\
+({									\
+	compiletime_assert_rwonce_type(x);				\
+	__READ_ONCE_SCALAR(x);						\
+})
+
+#define __WRITE_ONCE(x, val)				\
 do {							\
 	*(volatile typeof(x) *)&(x) = (val);		\
 } while (0)
 
+#define WRITE_ONCE(x, val)				\
+do {							\
+	compiletime_assert_rwonce_type(x);		\
+	__WRITE_ONCE(x, val);				\
+} while (0)
+
 #ifdef CONFIG_KASAN
 /*
  * We can't declare function 'inline' because __no_sanitize_address conflicts
@@ -313,6 +330,16 @@ static inline void *offset_to_ptr(const int *off)
 	compiletime_assert(__native_word(t),				\
 		"Need native word sized stores/loads for atomicity.")
 
+/*
+ * Yes, this permits 64-bit accesses on 32-bit architectures. These will
+ * actually be atomic in many cases (namely x86), but for others we rely on
+ * the access being split into 2x32-bit accesses for a 32-bit quantity (e.g.
+ * a virtual address) and a strong prevailing wind.
+ */
+#define compiletime_assert_rwonce_type(t)					\
+	compiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),	\
+		"Unsupported access size for {READ,WRITE}_ONCE().")
+
 /* &a[0] degrades to a pointer: a different type from an array */
 #define __must_be_array(a)	BUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))
 

commit a5460b5e5fb82656807840d40d3deaecad094044
Author: Will Deacon <will@kernel.org>
Date:   Mon Dec 16 16:51:45 2019 +0000

    READ_ONCE: Simplify implementations of {READ,WRITE}_ONCE()
    
    The implementations of {READ,WRITE}_ONCE() suffer from a significant
    amount of indirection and complexity due to a historic GCC bug:
    
    https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58145
    
    which was originally worked around by 230fa253df63 ("kernel: Provide
    READ_ONCE and ASSIGN_ONCE").
    
    Since GCC 4.8 is fairly vintage at this point and we emit a warning if
    we detect it during the build, return {READ,WRITE}_ONCE() to their former
    glory with an implementation that is easier to understand and, crucially,
    more amenable to optimisation. A side effect of this simplification is
    that WRITE_ONCE() no longer returns a value, but nobody seems to be
    relying on that and the new behaviour is aligned with smp_store_release().
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 034b0a644efc..338111a448d0 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -177,60 +177,6 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 # define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)
 #endif
 
-#include <uapi/linux/types.h>
-
-#define __READ_ONCE_SIZE						\
-({									\
-	switch (size) {							\
-	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;		\
-	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;		\
-	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;		\
-	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;		\
-	default:							\
-		barrier();						\
-		__builtin_memcpy((void *)res, (const void *)p, size);	\
-		barrier();						\
-	}								\
-})
-
-static __always_inline
-void __read_once_size(const volatile void *p, void *res, int size)
-{
-	__READ_ONCE_SIZE;
-}
-
-#ifdef CONFIG_KASAN
-/*
- * We can't declare function 'inline' because __no_sanitize_address confilcts
- * with inlining. Attempt to inline it may cause a build failure.
- * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
- * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
- */
-# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused
-#else
-# define __no_kasan_or_inline __always_inline
-#endif
-
-static __no_kasan_or_inline
-void __read_once_size_nocheck(const volatile void *p, void *res, int size)
-{
-	__READ_ONCE_SIZE;
-}
-
-static __always_inline void __write_once_size(volatile void *p, void *res, int size)
-{
-	switch (size) {
-	case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
-	case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
-	case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
-	case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
-	default:
-		barrier();
-		__builtin_memcpy((void *)p, (const void *)res, size);
-		barrier();
-	}
-}
-
 /*
  * Prevent the compiler from merging or refetching reads or writes. The
  * compiler is also forbidden from reordering successive instances of
@@ -240,11 +186,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * statements.
  *
  * These two macros will also work on aggregate data types like structs or
- * unions. If the size of the accessed data type exceeds the word size of
- * the machine (e.g., 32 bits or 64 bits) READ_ONCE() and WRITE_ONCE() will
- * fall back to memcpy(). There's at least two memcpy()s: one for the
- * __builtin_memcpy() and then one for the macro doing the copy of variable
- * - '__u' allocated on the stack.
+ * unions.
  *
  * Their two major use cases are: (1) Mediating communication between
  * process-level code and irq/NMI handlers, all running on the same CPU,
@@ -256,23 +198,49 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 #include <asm/barrier.h>
 #include <linux/kasan-checks.h>
 
-#define __READ_ONCE(x, check)						\
+#define __READ_ONCE(x)	(*(volatile typeof(x) *)&(x))
+
+#define READ_ONCE(x)							\
 ({									\
-	union { typeof(x) __val; char __c[1]; } __u;			\
-	if (check)							\
-		__read_once_size(&(x), __u.__c, sizeof(x));		\
-	else								\
-		__read_once_size_nocheck(&(x), __u.__c, sizeof(x));	\
-	smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \
-	__u.__val;							\
+	typeof(x) __x = __READ_ONCE(x);					\
+	smp_read_barrier_depends();					\
+	__x;								\
 })
-#define READ_ONCE(x) __READ_ONCE(x, 1)
+
+#define WRITE_ONCE(x, val)				\
+do {							\
+	*(volatile typeof(x) *)&(x) = (val);		\
+} while (0)
+
+#ifdef CONFIG_KASAN
+/*
+ * We can't declare function 'inline' because __no_sanitize_address conflicts
+ * with inlining. Attempt to inline it may cause a build failure.
+ *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
+ * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
+ */
+# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused
+#else
+# define __no_kasan_or_inline __always_inline
+#endif
+
+static __no_kasan_or_inline
+unsigned long __read_once_word_nocheck(const void *addr)
+{
+	return __READ_ONCE(*(unsigned long *)addr);
+}
 
 /*
- * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
- * to hide memory access from KASAN.
+ * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a
+ * word from memory atomically but without telling KASAN. This is usually
+ * used by unwinding code when walking the stack of a running process.
  */
-#define READ_ONCE_NOCHECK(x) __READ_ONCE(x, 0)
+#define READ_ONCE_NOCHECK(x)						\
+({									\
+	unsigned long __x = __read_once_word_nocheck(&(x));		\
+	smp_read_barrier_depends();					\
+	__x;								\
+})
 
 static __no_kasan_or_inline
 unsigned long read_word_at_a_time(const void *addr)
@@ -281,14 +249,6 @@ unsigned long read_word_at_a_time(const void *addr)
 	return *(unsigned long *)addr;
 }
 
-#define WRITE_ONCE(x, val) \
-({							\
-	union { typeof(x) __val; char __c[1]; } __u =	\
-		{ .__val = (__force typeof(x)) (val) }; \
-	__write_once_size(&(x), __u.__c, sizeof(x));	\
-	__u.__val;					\
-})
-
 #endif /* __KERNEL__ */
 
 /*

commit d071e91361bbfef524ae8abf7e560fb294d0ad64
Author: Marco Elver <elver@google.com>
Date:   Tue Mar 31 21:32:33 2020 +0200

    kcsan: Change data_race() to no longer require marking racing accesses
    
    Thus far, accesses marked with data_race() would still require the
    racing access to be marked in some way (be it with READ_ONCE(),
    WRITE_ONCE(), or data_race() itself), as otherwise KCSAN would still
    report a data race.  This requirement, however, seems to be unintuitive,
    and some valid use-cases demand *not* marking other accesses, as it
    might hide more serious bugs (e.g. diagnostic reads).
    
    Therefore, this commit changes data_race() to no longer require marking
    racing accesses (although it's still recommended if possible).
    
    The alternative would have been introducing another variant of
    data_race(), however, since usage of data_race() already needs to be
    carefully reasoned about, distinguishing between these cases likely adds
    more complexity in the wrong place.
    
    Link: https://lkml.kernel.org/r/20200331131002.GA30975@willie-the-truck
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: Qian Cai <cai@lca.pw>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f504edebd5d7..1729bd17e9b7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -326,9 +326,9 @@ unsigned long read_word_at_a_time(const void *addr)
 #define data_race(expr)                                                        \
 	({                                                                     \
 		typeof(({ expr; })) __val;                                     \
-		kcsan_nestable_atomic_begin();                                 \
+		kcsan_disable_current();                                       \
 		__val = ({ expr; });                                           \
-		kcsan_nestable_atomic_end();                                   \
+		kcsan_enable_current();                                        \
 		__val;                                                         \
 	})
 #else

commit 3b02a051d25d9600e9d403ad3043aed7de00160e
Merge: f5d2313bd3c5 8f3d9f354286
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 13 09:44:39 2020 +0200

    Merge tag 'v5.7-rc1' into locking/kcsan, to resolve conflicts and refresh
    
    Resolve these conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/Makefile
    
    Do a minor "evil merge" to move the KCSAN entry up a bit by a few lines
    in the Kconfig to reduce the probability of future conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit af9c5d2e3b355854ff0e4acfbfbfadcd5198a349
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Apr 6 20:09:37 2020 -0700

    compiler.h: fix error in BUILD_BUG_ON() reporting
    
    compiletime_assert() uses __LINE__ to create a unique function name.  This
    means that if you have more than one BUILD_BUG_ON() in the same source
    line (which can happen if they appear e.g.  in a macro), then the error
    message from the compiler might output the wrong condition.
    
    For this source file:
    
            #include <linux/build_bug.h>
    
            #define macro() \
                    BUILD_BUG_ON(1); \
                    BUILD_BUG_ON(0);
    
            void foo()
            {
                    macro();
            }
    
    gcc would output:
    
    ./include/linux/compiler.h:350:38: error: call to `__compiletime_assert_9' declared with attribute error: BUILD_BUG_ON failed: 0
      _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
    
    However, it was not the BUILD_BUG_ON(0) that failed, so it should say 1
    instead of 0. With this patch, we use __COUNTER__ instead of __LINE__, so
    each BUILD_BUG_ON() gets a different function name and the correct
    condition is printed:
    
    ./include/linux/compiler.h:350:38: error: call to `__compiletime_assert_0' declared with attribute error: BUILD_BUG_ON failed: 1
      _compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)
    
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Reviewed-by: Daniel Santos <daniel.santos@pobox.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Ian Abbott <abbotti@mev.co.uk>
    Cc: Joe Perches <joe@perches.com>
    Link: http://lkml.kernel.org/r/20200331112637.25047-1-vegard.nossum@oracle.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 5e88e7e33abe..034b0a644efc 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -347,7 +347,7 @@ static inline void *offset_to_ptr(const int *off)
  * compiler has support to do so.
  */
 #define compiletime_assert(condition, msg) \
-	_compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
+	_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)
 
 #define compiletime_assert_atomic_type(t)				\
 	compiletime_assert(__native_word(t),				\

commit b968a08f242d51982e46041c506115b5e11a7570
Author: Marco Elver <elver@google.com>
Date:   Tue Feb 11 17:04:20 2020 +0100

    compiler.h, seqlock.h: Remove unnecessary kcsan.h includes
    
    No we longer have to include kcsan.h, since the required KCSAN interface
    for both compiler.h and seqlock.h are now provided by kcsan-checks.h.
    
    Acked-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c1bdf37571cb..f504edebd5d7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -313,8 +313,6 @@ unsigned long read_word_at_a_time(const void *addr)
 	__u.__val;					\
 })
 
-#include <linux/kcsan.h>
-
 /**
  * data_race - mark an expression as containing intentional data races
  *

commit 7ad900d35b49af5a05f595d2274c32e69e01b055
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Feb 3 14:42:18 2020 -0800

    kcsan: Add docbook header for data_race()
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Marco Elver <elver@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8c0beb10c1dd..c1bdf37571cb 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -315,13 +315,15 @@ unsigned long read_word_at_a_time(const void *addr)
 
 #include <linux/kcsan.h>
 
-/*
- * data_race(): macro to document that accesses in an expression may conflict with
- * other concurrent accesses resulting in data races, but the resulting
- * behaviour is deemed safe regardless.
+/**
+ * data_race - mark an expression as containing intentional data races
+ *
+ * This data_race() macro is useful for situations in which data races
+ * should be forgiven.  One example is diagnostic code that accesses
+ * shared variables but is not a part of the core synchronization design.
  *
- * This macro *does not* affect normal code generation, but is a hint to tooling
- * that data races here should be ignored.
+ * This macro *does not* affect normal code generation, but is a hint
+ * to tooling that data races here are to be ignored.
  */
 #define data_race(expr)                                                        \
 	({                                                                     \

commit e33f9a169747880a008dd5e7b934fc592e91cd63
Author: Marco Elver <elver@google.com>
Date:   Thu Dec 12 01:07:09 2019 +0100

    kcsan: Add __no_kcsan function attribute
    
    Since the use of -fsanitize=thread is an implementation detail of KCSAN,
    the name __no_sanitize_thread could be misleading if used widely.
    Instead, we introduce the __no_kcsan attribute which is shorter and more
    accurate in the context of KCSAN.
    
    This matches the attribute name __no_kcsan_or_inline. The use of
    __kcsan_or_inline itself is still required for __always_inline functions
    to retain compatibility with older compilers.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ad8c76144a3c..8c0beb10c1dd 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -207,12 +207,15 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 # define __no_kasan_or_inline __always_inline
 #endif
 
+#define __no_kcsan __no_sanitize_thread
 #ifdef __SANITIZE_THREAD__
 /*
  * Rely on __SANITIZE_THREAD__ instead of CONFIG_KCSAN, to avoid not inlining in
- * compilation units where instrumentation is disabled.
+ * compilation units where instrumentation is disabled. The attribute 'noinline'
+ * is required for older compilers, where implicit inlining of very small
+ * functions renders __no_sanitize_thread ineffective.
  */
-# define __no_kcsan_or_inline __no_sanitize_thread notrace __maybe_unused
+# define __no_kcsan_or_inline __no_kcsan noinline notrace __maybe_unused
 # define __no_sanitize_or_inline __no_kcsan_or_inline
 #else
 # define __no_kcsan_or_inline __always_inline

commit 5cbaefe9743bf14c9d3106db0cc19f8cb0a3ca22
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 20 10:41:43 2019 +0100

    kcsan: Improve various small stylistic details
    
    Tidy up a few bits:
    
      - Fix typos and grammar, improve wording.
    
      - Remove spurious newlines that are col80 warning artifacts where the
        resulting line-break is worse than the disease it's curing.
    
      - Use core kernel coding style to improve readability and reduce
        spurious code pattern variations.
    
      - Use better vertical alignment for structure definitions and initialization
        sequences.
    
      - Misc other small details.
    
    No change in functionality intended.
    
    Cc: linux-kernel@vger.kernel.org
    Cc: Marco Elver <elver@google.com>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 7d3e77781578..ad8c76144a3c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -313,7 +313,7 @@ unsigned long read_word_at_a_time(const void *addr)
 #include <linux/kcsan.h>
 
 /*
- * data_race: macro to document that accesses in an expression may conflict with
+ * data_race(): macro to document that accesses in an expression may conflict with
  * other concurrent accesses resulting in data races, but the resulting
  * behaviour is deemed safe regardless.
  *

commit c48981eeb0d56e107691df590007d6699441a689
Author: Marco Elver <elver@google.com>
Date:   Thu Nov 14 19:02:55 2019 +0100

    include/linux/compiler.h: Introduce data_race(expr) macro
    
    This introduces the data_race(expr) macro, which can be used to annotate
    expressions for purposes of (1) documenting, and (2) giving tooling such
    as KCSAN information about which data races are deemed "safe".
    
    More context:
    http://lkml.kernel.org/r/CAHk-=wg5CkOEF8DTez1Qu0XTEFw_oHhxN98bDnFqbY7HL5AB2g@mail.gmail.com
    
    Signed-off-by: Marco Elver <elver@google.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c42fa83f23fb..7d3e77781578 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -310,6 +310,26 @@ unsigned long read_word_at_a_time(const void *addr)
 	__u.__val;					\
 })
 
+#include <linux/kcsan.h>
+
+/*
+ * data_race: macro to document that accesses in an expression may conflict with
+ * other concurrent accesses resulting in data races, but the resulting
+ * behaviour is deemed safe regardless.
+ *
+ * This macro *does not* affect normal code generation, but is a hint to tooling
+ * that data races here should be ignored.
+ */
+#define data_race(expr)                                                        \
+	({                                                                     \
+		typeof(({ expr; })) __val;                                     \
+		kcsan_nestable_atomic_begin();                                 \
+		__val = ({ expr; });                                           \
+		kcsan_nestable_atomic_end();                                   \
+		__val;                                                         \
+	})
+#else
+
 #endif /* __KERNEL__ */
 
 /*

commit dfd402a4c4baae42398ce9180ff424d589b8bffc
Author: Marco Elver <elver@google.com>
Date:   Thu Nov 14 19:02:54 2019 +0100

    kcsan: Add Kernel Concurrency Sanitizer infrastructure
    
    Kernel Concurrency Sanitizer (KCSAN) is a dynamic data-race detector for
    kernel space. KCSAN is a sampling watchpoint-based data-race detector.
    See the included Documentation/dev-tools/kcsan.rst for more details.
    
    This patch adds basic infrastructure, but does not yet enable KCSAN for
    any architecture.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Acked-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 5e88e7e33abe..c42fa83f23fb 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -178,6 +178,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #endif
 
 #include <uapi/linux/types.h>
+#include <linux/kcsan-checks.h>
 
 #define __READ_ONCE_SIZE						\
 ({									\
@@ -193,12 +194,6 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	}								\
 })
 
-static __always_inline
-void __read_once_size(const volatile void *p, void *res, int size)
-{
-	__READ_ONCE_SIZE;
-}
-
 #ifdef CONFIG_KASAN
 /*
  * We can't declare function 'inline' because __no_sanitize_address confilcts
@@ -207,18 +202,44 @@ void __read_once_size(const volatile void *p, void *res, int size)
  * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
  */
 # define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused
+# define __no_sanitize_or_inline __no_kasan_or_inline
 #else
 # define __no_kasan_or_inline __always_inline
 #endif
 
-static __no_kasan_or_inline
+#ifdef __SANITIZE_THREAD__
+/*
+ * Rely on __SANITIZE_THREAD__ instead of CONFIG_KCSAN, to avoid not inlining in
+ * compilation units where instrumentation is disabled.
+ */
+# define __no_kcsan_or_inline __no_sanitize_thread notrace __maybe_unused
+# define __no_sanitize_or_inline __no_kcsan_or_inline
+#else
+# define __no_kcsan_or_inline __always_inline
+#endif
+
+#ifndef __no_sanitize_or_inline
+#define __no_sanitize_or_inline __always_inline
+#endif
+
+static __no_kcsan_or_inline
+void __read_once_size(const volatile void *p, void *res, int size)
+{
+	kcsan_check_atomic_read(p, size);
+	__READ_ONCE_SIZE;
+}
+
+static __no_sanitize_or_inline
 void __read_once_size_nocheck(const volatile void *p, void *res, int size)
 {
 	__READ_ONCE_SIZE;
 }
 
-static __always_inline void __write_once_size(volatile void *p, void *res, int size)
+static __no_kcsan_or_inline
+void __write_once_size(volatile void *p, void *res, int size)
 {
+	kcsan_check_atomic_write(p, size);
+
 	switch (size) {
 	case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
 	case 2: *(volatile __u16 *)p = *(__u16 *)res; break;

commit bfafddd8de426d894fcf3e062370b1efaa195ebc
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Wed Aug 28 15:55:23 2019 -0700

    include/linux/compiler.h: fix Oops for Clang-compiled kernels
    
    GCC unescapes escaped string section names while Clang does not. Because
    __section uses the `#` stringification operator for the section name, it
    doesn't need to be escaped.
    
    This fixes an Oops observed in distro's that use systemd and not
    net.core.bpf_jit_enable=1, when their kernels are compiled with Clang.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/619
    Link: https://bugs.llvm.org/show_bug.cgi?id=42950
    Link: https://marc.info/?l=linux-netdev&m=156412960619946&w=2
    Link: https://lore.kernel.org/lkml/20190904181740.GA19688@gmail.com/
    Acked-by: Will Deacon <will@kernel.org>
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    [Cherry-picked from the __section cleanup series for 5.3]
    [Adjusted commit message]
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f0fd5636fddb..5e88e7e33abe 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -24,7 +24,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 			long ______r;					\
 			static struct ftrace_likely_data		\
 				__aligned(4)				\
-				__section("_ftrace_annotated_branch")	\
+				__section(_ftrace_annotated_branch)	\
 				______f = {				\
 				.data.func = __func__,			\
 				.data.file = __FILE__,			\
@@ -60,7 +60,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #define __trace_if_value(cond) ({			\
 	static struct ftrace_branch_data		\
 		__aligned(4)				\
-		__section("_ftrace_branch")		\
+		__section(_ftrace_branch)		\
 		__if_trace = {				\
 			.func = __func__,		\
 			.file = __FILE__,		\
@@ -118,7 +118,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	".popsection\n\t"
 
 /* Annotate a C jump table to allow objtool to follow the code flow */
-#define __annotate_jump_table __section(".rodata..c_jump_table")
+#define __annotate_jump_table __section(.rodata..c_jump_table)
 
 #else
 #define annotate_reachable()
@@ -298,7 +298,7 @@ unsigned long read_word_at_a_time(const void *addr)
  * visible to the compiler.
  */
 #define __ADDRESSABLE(sym) \
-	static void * __section(".discard.addressable") __used \
+	static void * __section(.discard.addressable) __used \
 		__PASTE(__addressable_##sym, __LINE__) = (void *)&sym;
 
 /**

commit 87b512def792579641499d9bef1d640994ea9c18
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Jun 27 20:50:46 2019 -0500

    objtool: Add support for C jump tables
    
    Objtool doesn't know how to read C jump tables, so it has to whitelist
    functions which use them, causing missing ORC unwinder data for such
    functions, e.g. ___bpf_prog_run().
    
    C jump tables are very similar to GCC switch jump tables, which objtool
    already knows how to read.  So adding support for C jump tables is easy.
    It just needs to be able to find the tables and distinguish them from
    other data.
    
    To allow the jump tables to be found, create an __annotate_jump_table
    macro which can be used to annotate them.
    
    The annotation is done by placing the jump table in an
    .rodata..c_jump_table section.  The '.rodata' prefix ensures that the data
    will be placed in the rodata section by the vmlinux linker script.  The
    double periods are part of an existing convention which distinguishes
    kernel sections from GCC sections.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Kairui Song <kasong@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lkml.kernel.org/r/0ba2ca30442b16b97165992381ce643dc27b3d1a.1561685471.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8aaf7cd026b0..f0fd5636fddb 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -116,9 +116,14 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	".pushsection .discard.unreachable\n\t"				\
 	".long 999b - .\n\t"						\
 	".popsection\n\t"
+
+/* Annotate a C jump table to allow objtool to follow the code flow */
+#define __annotate_jump_table __section(".rodata..c_jump_table")
+
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
+#define __annotate_jump_table
 #endif
 
 #ifndef ASM_UNREACHABLE

commit a15fd609ad53a631a927c6680e8fb606f42a712b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 20 10:26:17 2019 -0700

    tracing: Simplify "if" macro code
    
    Peter Zijlstra noticed that with CONFIG_PROFILE_ALL_BRANCHES, the "if"
    macro converts the conditional to an array index.  This can cause GCC
    to create horrible code.  When there are nested ifs, the generated code
    uses register values to encode branching decisions.
    
    Josh Poimboeuf found that replacing the define "if" macro from using
    the condition as an array index and incrementing the branch statics
    with an if statement itself, reduced the asm complexity and shrinks the
    generated code quite a bit.
    
    But this can be simplified even further by replacing the internal if
    statement with a ternary operator.
    
    Link: https://lkml.kernel.org/r/20190307174802.46fmpysxyo35hh43@treble
    Link: http://lkml.kernel.org/r/CAHk-=wiALN3jRuzARpwThN62iKd476Xj-uom+YnLZ4=eqcz7xQ@mail.gmail.com
    
    Reported-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reported-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 445348facea9..8aaf7cd026b0 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -53,23 +53,24 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * "Define 'is'", Bill Clinton
  * "Define 'if'", Steven Rostedt
  */
-#define if(cond, ...) __trace_if( (cond , ## __VA_ARGS__) )
-#define __trace_if(cond) \
-	if (__builtin_constant_p(!!(cond)) ? !!(cond) :			\
-	({								\
-		int ______r;						\
-		static struct ftrace_branch_data			\
-			__aligned(4)					\
-			__section("_ftrace_branch")			\
-			______f = {					\
-				.func = __func__,			\
-				.file = __FILE__,			\
-				.line = __LINE__,			\
-			};						\
-		______r = !!(cond);					\
-		______f.miss_hit[______r]++;					\
-		______r;						\
-	}))
+#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )
+
+#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))
+
+#define __trace_if_value(cond) ({			\
+	static struct ftrace_branch_data		\
+		__aligned(4)				\
+		__section("_ftrace_branch")		\
+		__if_trace = {				\
+			.func = __func__,		\
+			.file = __FILE__,		\
+			.line = __LINE__,		\
+		};					\
+	(cond) ?					\
+		(__if_trace.miss_hit[1]++,1) :		\
+		(__if_trace.miss_hit[0]++,0);		\
+})
+
 #endif /* CONFIG_PROFILE_ALL_BRANCHES */
 
 #else

commit 3e2ffd655cc6a694608d997738989ff5572a8266
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Wed Jan 2 15:57:49 2019 -0500

    include/linux/compiler*.h: fix OPTIMIZER_HIDE_VAR
    
    Since commit 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h
    mutually exclusive") clang no longer reuses the OPTIMIZER_HIDE_VAR macro
    from compiler-gcc - instead it gets the version in
    include/linux/compiler.h.  Unfortunately that version doesn't actually
    prevent compiler from optimizing out the variable.
    
    Fix up by moving the macro out from compiler-gcc.h to compiler.h.
    Compilers without incline asm support will keep working
    since it's protected by an ifdef.
    
    Also fix up comments to match reality since we are no longer overriding
    any macros.
    
    Build-tested with gcc and clang.
    
    Fixes: 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h mutually exclusive")
    Cc: Eli Friedman <efriedma@codeaurora.org>
    Cc: Joe Perches <joe@perches.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index fc5004a4b07d..445348facea9 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -161,7 +161,9 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #endif
 
 #ifndef OPTIMIZER_HIDE_VAR
-#define OPTIMIZER_HIDE_VAR(var) barrier()
+/* Make the optimizer believe the variable can be manipulated arbitrarily. */
+#define OPTIMIZER_HIDE_VAR(var)						\
+	__asm__ ("" : "=r" (var) : "0" (var))
 #endif
 
 /* Not-quite-unique ID. */

commit 96af6cd02a10b96108fc415a213441e7fb9f4f9b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Dec 19 11:23:27 2018 +0100

    Revert "x86/objtool: Use asm macros to work around GCC inlining bugs"
    
    This reverts commit c06c4d8090513f2974dfdbed2ac98634357ac475.
    
    See this commit for details about the revert:
    
      e769742d3584 ("Revert "x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs"")
    
    Reported-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Richard Biener <rguenther@suse.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Nadav Amit <namit@vmware.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1cf127..fc5004a4b07d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */

commit 163c8d54a997153ee1a1e07fcac087492ad85b37
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Nov 5 07:36:28 2018 +0100

    compiler: remove __no_sanitize_address_or_inline again
    
    The __no_sanitize_address_or_inline and __no_kasan_or_inline defines
    are almost identical. The only difference is that __no_kasan_or_inline
    does not have the 'notrace' attribute.
    
    To be able to replace __no_sanitize_address_or_inline with the older
    definition, add 'notrace' to __no_kasan_or_inline and change to two
    users of __no_sanitize_address_or_inline in the s390 code.
    
    The 'notrace' option is necessary for e.g. the __load_psw_mask function
    in arch/s390/include/asm/processor.h. Without the option it is possible
    to trace __load_psw_mask which leads to kernel stack overflow.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Pointed-out-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 18c80cfa4fc4..06396c1cf127 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -189,7 +189,7 @@ void __read_once_size(const volatile void *p, void *res, int size)
  * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
  * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
  */
-# define __no_kasan_or_inline __no_sanitize_address __maybe_unused
+# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused
 #else
 # define __no_kasan_or_inline __always_inline
 #endif

commit e468f5c06b5ebef3f6f3c187e51aa6daab667e57
Merge: baa888d25ea6 1ff2fea5e30c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 1 18:34:46 2018 -0700

    Merge tag 'compiler-attributes-for-linus-4.20-rc1' of https://github.com/ojeda/linux
    
    Pull compiler attribute updates from Miguel Ojeda:
     "This is an effort to disentangle the include/linux/compiler*.h headers
      and bring them up to date.
    
      The main idea behind the series is to use feature checking macros
      (i.e. __has_attribute) instead of compiler version checks (e.g.
      GCC_VERSION), which are compiler-agnostic (so they can be shared,
      reducing the size of compiler-specific headers) and version-agnostic.
    
      Other related improvements have been performed in the headers as well,
      which on top of the use of __has_attribute it has amounted to a
      significant simplification of these headers (e.g. GCC_VERSION is now
      only guarding a few non-attribute macros).
    
      This series should also help the efforts to support compiling the
      kernel with clang and icc. A fair amount of documentation and comments
      have also been added, clarified or removed; and the headers are now
      more readable, which should help kernel developers in general.
    
      The series was triggered due to the move to gcc >= 4.6. In turn, this
      series has also triggered Sparse to gain the ability to recognize
      __has_attribute on its own.
    
      Finally, the __nonstring variable attribute series has been also
      applied on top; plus two related patches from Nick Desaulniers for
      unreachable() that came a bit afterwards"
    
    * tag 'compiler-attributes-for-linus-4.20-rc1' of https://github.com/ojeda/linux:
      compiler-gcc: remove comment about gcc 4.5 from unreachable()
      compiler.h: update definition of unreachable()
      Compiler Attributes: ext4: remove local __nonstring definition
      Compiler Attributes: auxdisplay: panel: use __nonstring
      Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
      Compiler Attributes: add support for __nonstring (gcc >= 8)
      Compiler Attributes: add MAINTAINERS entry
      Compiler Attributes: add Doc/process/programming-language.rst
      Compiler Attributes: remove uses of __attribute__ from compiler.h
      Compiler Attributes: KENTRY used twice the "used" attribute
      Compiler Attributes: use feature checks instead of version checks
      Compiler Attributes: add missing SPDX ID in compiler_types.h
      Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
      Compiler Attributes: homogenize __must_be_array
      Compiler Attributes: remove unneeded tests
      Compiler Attributes: always use the extra-underscores syntax
      Compiler Attributes: remove unused attributes

commit 746bb4ed6d626f3f9e431a7f9b20504538e62ded
Merge: ac747c0715f2 0bb95f80a38f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 28 13:26:45 2018 -0700

    Merge tag 'vla-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull VLA removal from Kees Cook:
     "Globally warn on VLA use.
    
      This turns on "-Wvla" globally now that the last few trees with their
      VLA removals have landed (crypto, block, net, and powerpc).
    
      Arnd mentioned that there may be a couple more VLAs hiding in
      hard-to-find randconfigs, but nothing big has shaken out in the last
      month or so in linux-next.
    
      We should be basically VLA-free now! Wheee. :)
    
      Summary:
    
       - Remove unused fallback for BUILD_BUG_ON (which technically contains
         a VLA)
    
       - Lift -Wvla to the top-level Makefile"
    
    * tag 'vla-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      Makefile: Globally enable VLA warning
      compiler.h: give up __compiletime_assert_fallback()

commit fe0640eb30b7da261ae84d252ed9ed3c7e68dfd8
Author: ndesaulniers@google.com <ndesaulniers@google.com>
Date:   Mon Oct 15 10:22:21 2018 -0700

    compiler.h: update definition of unreachable()
    
    Fixes the objtool warning seen with Clang:
    arch/x86/mm/fault.o: warning: objtool: no_context()+0x220: unreachable
    instruction
    
    Fixes commit 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h
    mutually exclusive")
    
    Josh noted that the fallback definition was meant to work around a
    pre-gcc-4.6 bug. GCC still needs to work around
    https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82365, so compiler-gcc.h
    defines its own version of unreachable().  Clang and ICC can use this
    shared definition.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/204
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index b5fb034fa6fa..2e0b6322588b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -124,7 +124,10 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 # define ASM_UNREACHABLE
 #endif
 #ifndef unreachable
-# define unreachable() do { annotate_reachable(); do { } while (1); } while (0)
+# define unreachable() do {		\
+	annotate_unreachable();		\
+	__builtin_unreachable();	\
+} while (0)
 #endif
 
 /*

commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Sun Aug 26 03:16:29 2018 +0900

    compiler.h: give up __compiletime_assert_fallback()
    
    __compiletime_assert_fallback() is supposed to stop building earlier
    by using the negative-array-size method in case the compiler does not
    support "error" attribute, but has never worked like that.
    
    You can simply try:
    
        BUILD_BUG_ON(1);
    
    GCC immediately terminates the build, but Clang does not report
    anything because Clang does not support the "error" attribute now.
    It will later fail at link time, but __compiletime_assert_fallback()
    is not working at least.
    
    The root cause is commit 1d6a0d19c855 ("bug.h: prevent double evaluation
    of `condition' in BUILD_BUG_ON").  Prior to that commit, BUILD_BUG_ON()
    was checked by the negative-array-size method *and* the link-time trick.
    Since that commit, the negative-array-size is not effective because
    '__cond' is no longer constant.  As the comment in <linux/build_bug.h>
    says, GCC (and Clang as well) only emits the error for obvious cases.
    
    When '__cond' is a variable,
    
        ((void)sizeof(char[1 - 2 * __cond]))
    
    ... is not obvious for the compiler to know the array size is negative.
    
    Reverting that commit would break BUILD_BUG() because negative-size-array
    is evaluated before the code is optimized out.
    
    Let's give up __compiletime_assert_fallback().  This commit does not
    change the current behavior since it just rips off the useless code.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 681d866efb1e..87c776c3ce73 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -314,29 +314,14 @@ static inline void *offset_to_ptr(const int *off)
 #endif
 #ifndef __compiletime_error
 # define __compiletime_error(message)
-/*
- * Sparse complains of variable sized arrays due to the temporary variable in
- * __compiletime_assert. Unfortunately we can't just expand it out to make
- * sparse see a constant array size without breaking compiletime_assert on old
- * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
- */
-# ifndef __CHECKER__
-#  define __compiletime_error_fallback(condition) \
-	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
-# endif
-#endif
-#ifndef __compiletime_error_fallback
-# define __compiletime_error_fallback(condition) do { } while (0)
 #endif
 
 #ifdef __OPTIMIZE__
 # define __compiletime_assert(condition, msg, prefix, suffix)		\
 	do {								\
-		int __cond = !(condition);				\
 		extern void prefix ## suffix(void) __compiletime_error(msg); \
-		if (__cond)						\
+		if (!(condition))					\
 			prefix ## suffix();				\
-		__compiletime_error_fallback(__cond);			\
 	} while (0)
 #else
 # define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)

commit c06c4d8090513f2974dfdbed2ac98634357ac475
Author: Nadav Amit <namit@vmware.com>
Date:   Wed Oct 3 14:30:53 2018 -0700

    x86/objtool: Use asm macros to work around GCC inlining bugs
    
    As described in:
    
      77b0bf55bc67: ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
    
    GCC's inlining heuristics are broken with common asm() patterns used in
    kernel code, resulting in the effective disabling of inlining.
    
    In the case of objtool the resulting borkage can be significant, since all the
    annotations of objtool are discarded during linkage and never inlined,
    yet GCC bogusly considers most functions affected by objtool annotations
    as 'too large'.
    
    The workaround is to set an assembly macro and call it from the inline
    assembly block. As a result GCC considers the inline assembly block as
    a single instruction. (Which it isn't, but that's the best we can get.)
    
    This increases the kernel size slightly:
    
          text     data     bss      dec     hex filename
      18140829 10224724 2957312 31322865 1ddf2f1 ./vmlinux before
      18140970 10225412 2957312 31323694 1ddf62e ./vmlinux after (+829)
    
    The number of static text symbols (i.e. non-inlined functions) is reduced:
    
      Before:  40321
      After:   40302 (-19)
    
    [ mingo: Rewrote the changelog. ]
    
    Tested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-sparse@vger.kernel.org
    Link: http://lkml.kernel.org/r/20181003213100.189959-4-namit@vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 681d866efb1e..1921545c6351 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,22 +99,13 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("%c0:\n\t"						\
-		     ".pushsection .discard.reachable\n\t"		\
-		     ".long %c0b - .\n\t"				\
-		     ".popsection\n\t" : : "i" (__COUNTER__));		\
+	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
+		     : : "i" (__COUNTER__));				\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("%c0:\n\t"						\
-		     ".pushsection .discard.unreachable\n\t"		\
-		     ".long %c0b - .\n\t"				\
-		     ".popsection\n\t" : : "i" (__COUNTER__));		\
+	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
+		     : : "i" (__COUNTER__));				\
 })
-#define ASM_UNREACHABLE							\
-	"999:\n\t"							\
-	".pushsection .discard.unreachable\n\t"				\
-	".long 999b - .\n\t"						\
-	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -299,6 +290,45 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
+#else /* __ASSEMBLY__ */
+
+#ifdef __KERNEL__
+#ifndef LINKER_SCRIPT
+
+#ifdef CONFIG_STACK_VALIDATION
+.macro ANNOTATE_UNREACHABLE counter:req
+\counter:
+	.pushsection .discard.unreachable
+	.long \counter\()b -.
+	.popsection
+.endm
+
+.macro ANNOTATE_REACHABLE counter:req
+\counter:
+	.pushsection .discard.reachable
+	.long \counter\()b -.
+	.popsection
+.endm
+
+.macro ASM_UNREACHABLE
+999:
+	.pushsection .discard.unreachable
+	.long 999b - .
+	.popsection
+.endm
+#else /* CONFIG_STACK_VALIDATION */
+.macro ANNOTATE_UNREACHABLE counter:req
+.endm
+
+.macro ANNOTATE_REACHABLE counter:req
+.endm
+
+.macro ASM_UNREACHABLE
+.endm
+#endif /* CONFIG_STACK_VALIDATION */
+
+#endif /* LINKER_SCRIPT */
+#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 #ifndef __optimize

commit e04462fb82f8dd98288c0e7ab1eec79c92537d25
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Mon Sep 3 19:17:50 2018 +0200

    Compiler Attributes: remove uses of __attribute__ from compiler.h
    
    Suggested-by: Nick Desaulniers <ndesaulniers@google.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 17ee9165ca51..b5fb034fa6fa 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -23,8 +23,8 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #define __branch_check__(x, expect, is_constant) ({			\
 			long ______r;					\
 			static struct ftrace_likely_data		\
-				__attribute__((__aligned__(4)))		\
-				__attribute__((__section__("_ftrace_annotated_branch"))) \
+				__aligned(4)				\
+				__section("_ftrace_annotated_branch")	\
 				______f = {				\
 				.data.func = __func__,			\
 				.data.file = __FILE__,			\
@@ -59,8 +59,8 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	({								\
 		int ______r;						\
 		static struct ftrace_branch_data			\
-			__attribute__((__aligned__(4)))			\
-			__attribute__((__section__("_ftrace_branch")))	\
+			__aligned(4)					\
+			__section("_ftrace_branch")			\
 			______f = {					\
 				.func = __func__,			\
 				.file = __FILE__,			\
@@ -146,7 +146,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	extern typeof(sym) sym;					\
 	static const unsigned long __kentry_##sym		\
 	__used							\
-	__attribute__((__section__("___kentry" "+" #sym )))	\
+	__section("___kentry" "+" #sym )			\
 	= (unsigned long)&sym;
 #endif
 
@@ -287,7 +287,7 @@ unsigned long read_word_at_a_time(const void *addr)
  * visible to the compiler.
  */
 #define __ADDRESSABLE(sym) \
-	static void * __attribute__((__section__(".discard.addressable"), used)) \
+	static void * __section(".discard.addressable") __used \
 		__PASTE(__addressable_##sym, __LINE__) = (void *)&sym;
 
 /**

commit 06e3727e02f9ee9cf571692cd5c74fc5a8a2af52
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Mon Sep 3 19:22:13 2018 +0200

    Compiler Attributes: KENTRY used twice the "used" attribute
    
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 4030a2940d6b..17ee9165ca51 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -146,7 +146,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	extern typeof(sym) sym;					\
 	static const unsigned long __kentry_##sym		\
 	__used							\
-	__attribute__((__section__("___kentry" "+" #sym ), used))	\
+	__attribute__((__section__("___kentry" "+" #sym )))	\
 	= (unsigned long)&sym;
 #endif
 

commit 989bd5000f36052df604888ed12bb6ef390786b7
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Fri Aug 31 18:00:16 2018 +0200

    Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
    
    Sparse knows about a few more attributes now, so we can remove
    the __CHECKER__ conditions from them (which, in turn, allow us
    to move some of them later on to compiler_attributes.h).
    
      * assume_aligned: since sparse's commit ffc860b ("sparse:
        ignore __assume_aligned__ attribute"), included in 0.5.1
    
      * error: since sparse's commit 0a04210 ("sparse: Add 'error'
        to ignored attributes"), included in 0.5.0
    
      * hotpatch: since sparse's commit 6043210 ("sparse/parse.c:
        ignore hotpatch attribute"), included in 0.5.1
    
      * warning: since sparse's commit 977365d ("Avoid "attribute
        'warning': unknown attribute" warning"), included in 0.4.2
    
    On top of that, __must_be_array does not need it either because:
    
      * Even ancient versions of sparse do not have a problem
    
      * BUILD_BUG_ON_ZERO() is currently disabled for __CHECKER__
    
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 165b1d5683ed..4030a2940d6b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -357,11 +357,7 @@ static inline void *offset_to_ptr(const int *off)
 	compiletime_assert(__native_word(t),				\
 		"Need native word sized stores/loads for atomicity.")
 
-#ifdef __CHECKER__
-#define __must_be_array(a)	0
-#else
 /* &a[0] degrades to a pointer: a different type from an array */
 #define __must_be_array(a)	BUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))
-#endif
 
 #endif /* __LINUX_COMPILER_H */

commit ec0bbef66f867854691d5af18c2231d746958e0e
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Thu Aug 30 19:25:14 2018 +0200

    Compiler Attributes: homogenize __must_be_array
    
    Different definitions of __must_be_array:
    
      * gcc: disabled for __CHECKER__
    
      * clang: same definition as gcc's, but without __CHECKER__
    
      * intel: the comment claims __builtin_types_compatible_p()
        is unsupported; but icc seems to support it since 13.0.1
        (released in 2012). See https://godbolt.org/z/S0l6QQ
    
    Therefore, we can remove all of them and have a single definition
    in compiler.h
    
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ec4a28bad2c6..165b1d5683ed 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -357,4 +357,11 @@ static inline void *offset_to_ptr(const int *off)
 	compiletime_assert(__native_word(t),				\
 		"Need native word sized stores/loads for atomicity.")
 
+#ifdef __CHECKER__
+#define __must_be_array(a)	0
+#else
+/* &a[0] degrades to a pointer: a different type from an array */
+#define __must_be_array(a)	BUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))
+#endif
+
 #endif /* __LINUX_COMPILER_H */

commit 5c67a52f3da0f0d22764f2daec417702695a8112
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Thu Aug 30 19:13:37 2018 +0200

    Compiler Attributes: always use the extra-underscores syntax
    
    The attribute syntax optionally allows to surround attribute names
    with "__" in order to avoid collisions with macros of the same name
    (see https://gcc.gnu.org/onlinedocs/gcc/Attribute-Syntax.html).
    
    This homogenizes all attributes to use the syntax with underscores.
    While there are currently only a handful of cases of some TUs defining
    macros like "error" which may collide with the attributes,
    this should prevent futures surprises.
    
    This has been done only for "standard" attributes supported by
    the major compilers. In other words, those of third-party tools
    (e.g. sparse, plugins...) have not been changed for the moment.
    
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 7c0157d50964..ec4a28bad2c6 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -24,7 +24,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 			long ______r;					\
 			static struct ftrace_likely_data		\
 				__attribute__((__aligned__(4)))		\
-				__attribute__((section("_ftrace_annotated_branch"))) \
+				__attribute__((__section__("_ftrace_annotated_branch"))) \
 				______f = {				\
 				.data.func = __func__,			\
 				.data.file = __FILE__,			\
@@ -60,7 +60,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 		int ______r;						\
 		static struct ftrace_branch_data			\
 			__attribute__((__aligned__(4)))			\
-			__attribute__((section("_ftrace_branch")))	\
+			__attribute__((__section__("_ftrace_branch")))	\
 			______f = {					\
 				.func = __func__,			\
 				.file = __FILE__,			\
@@ -146,7 +146,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	extern typeof(sym) sym;					\
 	static const unsigned long __kentry_##sym		\
 	__used							\
-	__attribute__((section("___kentry" "+" #sym ), used))	\
+	__attribute__((__section__("___kentry" "+" #sym ), used))	\
 	= (unsigned long)&sym;
 #endif
 
@@ -287,7 +287,7 @@ unsigned long read_word_at_a_time(const void *addr)
  * visible to the compiler.
  */
 #define __ADDRESSABLE(sym) \
-	static void * __attribute__((section(".discard.addressable"), used)) \
+	static void * __attribute__((__section__(".discard.addressable"), used)) \
 		__PASTE(__addressable_##sym, __LINE__) = (void *)&sym;
 
 /**

commit 29efbc6aea9d9bd9aa9870a9afc1882046303cf9
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Thu Aug 30 19:07:27 2018 +0200

    Compiler Attributes: remove unused attributes
    
    __optimize and __deprecate_for_modules are unused in
    the whole kernel tree. Simply drop them.
    
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com> # on top of v4.19-rc5, clang 7
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 681d866efb1e..7c0157d50964 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -301,10 +301,6 @@ static inline void *offset_to_ptr(const int *off)
 
 #endif /* __ASSEMBLY__ */
 
-#ifndef __optimize
-# define __optimize(level)
-#endif
-
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1

commit 7290d58095712a89f845e1bca05334796dd49ed2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Aug 21 21:56:09 2018 -0700

    module: use relative references for __ksymtab entries
    
    An ordinary arm64 defconfig build has ~64 KB worth of __ksymtab entries,
    each consisting of two 64-bit fields containing absolute references, to
    the symbol itself and to a char array containing its name, respectively.
    
    When we build the same configuration with KASLR enabled, we end up with an
    additional ~192 KB of relocations in the .init section, i.e., one 24 byte
    entry for each absolute reference, which all need to be processed at boot
    time.
    
    Given how the struct kernel_symbol that describes each entry is completely
    local to module.c (except for the references emitted by EXPORT_SYMBOL()
    itself), we can easily modify it to contain two 32-bit relative references
    instead.  This reduces the size of the __ksymtab section by 50% for all
    64-bit architectures, and gets rid of the runtime relocations entirely for
    architectures implementing KASLR, either via standard PIE linking (arm64)
    or using custom host tools (x86).
    
    Note that the binary search involving __ksymtab contents relies on each
    section being sorted by symbol name.  This is implemented based on the
    input section names, not the names in the ksymtab entries, so this patch
    does not interfere with that.
    
    Given that the use of place-relative relocations requires support both in
    the toolchain and in the module loader, we cannot enable this feature for
    all architectures.  So make it dependent on whether
    CONFIG_HAVE_ARCH_PREL32_RELOCATIONS is defined.
    
    Link: http://lkml.kernel.org/r/20180704083651.24360-4-ard.biesheuvel@linaro.org
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Jessica Yu <jeyu@kernel.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morris <james.morris@microsoft.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: "Serge E. Hallyn" <serge@hallyn.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c8eab637a2a7..681d866efb1e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -280,6 +280,25 @@ unsigned long read_word_at_a_time(const void *addr)
 
 #endif /* __KERNEL__ */
 
+/*
+ * Force the compiler to emit 'sym' as a symbol, so that we can reference
+ * it from inline assembler. Necessary in case 'sym' could be inlined
+ * otherwise, or eliminated entirely due to lack of references that are
+ * visible to the compiler.
+ */
+#define __ADDRESSABLE(sym) \
+	static void * __attribute__((section(".discard.addressable"), used)) \
+		__PASTE(__addressable_##sym, __LINE__) = (void *)&sym;
+
+/**
+ * offset_to_ptr - convert a relative memory offset to an absolute pointer
+ * @off:	the address of the 32-bit offset value
+ */
+static inline void *offset_to_ptr(const int *off)
+{
+	return (void *)((unsigned long)off + *off);
+}
+
 #endif /* __ASSEMBLY__ */
 
 #ifndef __optimize

commit 203583990cb675aeddff6d7623f68268068c078c
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Tue Aug 21 21:55:45 2018 -0700

    linux/compiler.h: don't use bool
    
    Appararently, it's possible to have a non-trivial TU include a few
    headers, including linux/build_bug.h, without ending up with
    linux/types.h.  So the 0day bot sent me
    
    config: um-x86_64_defconfig (attached as .config)
    
    >> include/linux/compiler.h:316:3: error: unknown type name 'bool'; did you mean '_Bool'?
          bool __cond = !(condition);    \
    
    for something I'm working on.
    
    Rather than contributing to the #include madness and including
    linux/types.h from compiler.h, just use int.
    
    Link: http://lkml.kernel.org/r/20180817101036.20969-1-linux@rasmusvillemoes.dk
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Christopher Li <sparse@chrisli.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 42506e4d1f53..c8eab637a2a7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -313,7 +313,7 @@ unsigned long read_word_at_a_time(const void *addr)
 #ifdef __OPTIMIZE__
 # define __compiletime_assert(condition, msg, prefix, suffix)		\
 	do {								\
-		bool __cond = !(condition);				\
+		int __cond = !(condition);				\
 		extern void prefix ## suffix(void) __compiletime_error(msg); \
 		if (__cond)						\
 			prefix ## suffix();				\

commit 2026d35741f2c3ece73c11eb7e4a15d7c2df9ebe
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Wed May 30 08:19:22 2018 -0400

    branch-check: fix long->int truncation when profiling branches
    
    The function __builtin_expect returns long type (see the gcc
    documentation), and so do macros likely and unlikely. Unfortunatelly, when
    CONFIG_PROFILE_ANNOTATED_BRANCHES is selected, the macros likely and
    unlikely expand to __branch_check__ and __branch_check__ truncates the
    long type to int. This unintended truncation may cause bugs in various
    kernel code (we found a bug in dm-writecache because of it), so it's
    better to fix __branch_check__ to return long.
    
    Link: http://lkml.kernel.org/r/alpine.LRH.2.02.1805300818140.24812@file01.intranet.prod.int.rdu2.redhat.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Fixes: 1f0d69a9fc815 ("tracing: profile likely and unlikely annotations")
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ab4711c63601..42506e4d1f53 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -21,7 +21,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #define unlikely_notrace(x)	__builtin_expect(!!(x), 0)
 
 #define __branch_check__(x, expect, is_constant) ({			\
-			int ______r;					\
+			long ______r;					\
 			static struct ftrace_likely_data		\
 				__attribute__((__aligned__(4)))		\
 				__attribute__((section("_ftrace_annotated_branch"))) \

commit 173a3efd3edb2ef6ef07471397c5f542a360e9c1
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 21 14:45:54 2018 -0800

    bug.h: work around GCC PR82365 in BUG()
    
    Looking at functions with large stack frames across all architectures
    led me discovering that BUG() suffers from the same problem as
    fortify_panic(), which I've added a workaround for already.
    
    In short, variables that go out of scope by calling a noreturn function
    or __builtin_unreachable() keep using stack space in functions
    afterwards.
    
    A workaround that was identified is to insert an empty assembler
    statement just before calling the function that doesn't return.  I'm
    adding a macro "barrier_before_unreachable()" to document this, and
    insert calls to that in all instances of BUG() that currently suffer
    from this problem.
    
    The files that saw the largest change from this had these frame sizes
    before, and much less with my patch:
    
      fs/ext4/inode.c:82:1: warning: the frame size of 1672 bytes is larger than 800 bytes [-Wframe-larger-than=]
      fs/ext4/namei.c:434:1: warning: the frame size of 904 bytes is larger than 800 bytes [-Wframe-larger-than=]
      fs/ext4/super.c:2279:1: warning: the frame size of 1160 bytes is larger than 800 bytes [-Wframe-larger-than=]
      fs/ext4/xattr.c:146:1: warning: the frame size of 1168 bytes is larger than 800 bytes [-Wframe-larger-than=]
      fs/f2fs/inode.c:152:1: warning: the frame size of 1424 bytes is larger than 800 bytes [-Wframe-larger-than=]
      net/netfilter/ipvs/ip_vs_core.c:1195:1: warning: the frame size of 1068 bytes is larger than 800 bytes [-Wframe-larger-than=]
      net/netfilter/ipvs/ip_vs_core.c:395:1: warning: the frame size of 1084 bytes is larger than 800 bytes [-Wframe-larger-than=]
      net/netfilter/ipvs/ip_vs_ftp.c:298:1: warning: the frame size of 928 bytes is larger than 800 bytes [-Wframe-larger-than=]
      net/netfilter/ipvs/ip_vs_ftp.c:418:1: warning: the frame size of 908 bytes is larger than 800 bytes [-Wframe-larger-than=]
      net/netfilter/ipvs/ip_vs_lblcr.c:718:1: warning: the frame size of 960 bytes is larger than 800 bytes [-Wframe-larger-than=]
      drivers/net/xen-netback/netback.c:1500:1: warning: the frame size of 1088 bytes is larger than 800 bytes [-Wframe-larger-than=]
    
    In case of ARC and CRIS, it turns out that the BUG() implementation
    actually does return (or at least the compiler thinks it does),
    resulting in lots of warnings about uninitialized variable use and
    leaving noreturn functions, such as:
    
      block/cfq-iosched.c: In function 'cfq_async_queue_prio':
      block/cfq-iosched.c:3804:1: error: control reaches end of non-void function [-Werror=return-type]
      include/linux/dmaengine.h: In function 'dma_maxpq':
      include/linux/dmaengine.h:1123:1: error: control reaches end of non-void function [-Werror=return-type]
    
    This makes them call __builtin_trap() instead, which should normally
    dump the stack and kill the current process, like some of the other
    architectures already do.
    
    I tried adding barrier_before_unreachable() to panic() and
    fortify_panic() as well, but that had very little effect, so I'm not
    submitting that patch.
    
    Vineet said:
    
    : For ARC, it is double win.
    :
    : 1. Fixes 3 -Wreturn-type warnings
    :
    : | ../net/core/ethtool.c:311:1: warning: control reaches end of non-void function
    : [-Wreturn-type]
    : | ../kernel/sched/core.c:3246:1: warning: control reaches end of non-void function
    : [-Wreturn-type]
    : | ../include/linux/sunrpc/svc_xprt.h:180:1: warning: control reaches end of
    : non-void function [-Wreturn-type]
    :
    : 2.  bloat-o-meter reports code size improvements as gcc elides the
    :    generated code for stack return.
    
    Link: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82365
    Link: http://lkml.kernel.org/r/20171219114112.939391-1-arnd@arndb.de
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>    [arch/arc]
    Tested-by: Vineet Gupta <vgupta@synopsys.com>   [arch/arc]
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e835fc0423ec..ab4711c63601 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -86,6 +86,11 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 # define barrier_data(ptr) barrier()
 #endif
 
+/* workaround for GCC PR82365 if needed */
+#ifndef barrier_before_unreachable
+# define barrier_before_unreachable() do { } while (0)
+#endif
+
 /* Unreachable code */
 #ifdef CONFIG_STACK_VALIDATION
 /*

commit 178e834c47b0d01352c48730235aae69898fbc02
Merge: 7928b2cbe55b 2e7d1d61ea6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 12 08:57:21 2018 -0800

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto fixes from Herbert Xu:
     "This fixes the following issues:
    
       - oversize stack frames on mn10300 in sha3-generic
    
       - warning on old compilers in sha3-generic
    
       - API error in sun4i_ss_prng
    
       - potential dead-lock in sun4i_ss_prng
    
       - null-pointer dereference in sha512-mb
    
       - endless loop when DECO acquire fails in caam
    
       - kernel oops when hashing empty message in talitos"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6:
      crypto: sun4i_ss_prng - convert lock to _bh in sun4i_ss_prng_generate
      crypto: sun4i_ss_prng - fix return value of sun4i_ss_prng_generate
      crypto: caam - fix endless loop when DECO acquire fails
      crypto: sha3-generic - Use __optimize to support old compilers
      compiler-gcc.h: __nostackprotector needs gcc-4.4 and up
      compiler-gcc.h: Introduce __optimize function attribute
      crypto: sha3-generic - deal with oversize stack frames
      crypto: talitos - fix Kernel Oops on hashing an empty file
      crypto: sha512-mb - initialize pending lengths correctly

commit df5d45aa08f848b79caf395211b222790534ccc7
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Thu Feb 1 11:21:58 2018 +0100

    compiler-gcc.h: Introduce __optimize function attribute
    
    Create a new function attribute __optimize, which allows to specify an
    optimization level on a per-function basis.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 188ed9f65517..cdc629f20e20 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -271,6 +271,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 
 #endif /* __ASSEMBLY__ */
 
+#ifndef __optimize
+# define __optimize(level)
+#endif
+
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1

commit 7f1e541fc8d57a143dd5df1d0a1276046e08c083
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu Feb 1 21:00:49 2018 +0300

    compiler.h: Add read_word_at_a_time() function.
    
    Sometimes we know that it's safe to do potentially out-of-bounds access
    because we know it won't cross a page boundary.  Still, KASAN will
    report this as a bug.
    
    Add read_word_at_a_time() function which is supposed to be used in such
    cases.  In read_word_at_a_time() KASAN performs relaxed check - only the
    first byte of access is validated.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 3035990a8070..c2cc57a2f508 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -238,6 +238,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * required ordering.
  */
 #include <asm/barrier.h>
+#include <linux/kasan-checks.h>
 
 #define __READ_ONCE(x, check)						\
 ({									\
@@ -257,6 +258,13 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  */
 #define READ_ONCE_NOCHECK(x) __READ_ONCE(x, 0)
 
+static __no_kasan_or_inline
+unsigned long read_word_at_a_time(const void *addr)
+{
+	kasan_check_read(addr, 1);
+	return *(unsigned long *)addr;
+}
+
 #define WRITE_ONCE(x, val) \
 ({							\
 	union { typeof(x) __val; char __c[1]; } __u =	\

commit bdb5ac801af3d81d36732c2f640d6a1d3df83826
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Thu Feb 1 21:00:48 2018 +0300

    compiler.h, kasan: Avoid duplicating __read_once_size_nocheck()
    
    Instead of having two identical __read_once_size_nocheck() functions
    with different attributes, consolidate all the difference in new macro
    __no_kasan_or_inline and use it. No functional changes.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 52e611ab9a6c..3035990a8070 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -185,23 +185,21 @@ void __read_once_size(const volatile void *p, void *res, int size)
 
 #ifdef CONFIG_KASAN
 /*
- * This function is not 'inline' because __no_sanitize_address confilcts
+ * We can't declare function 'inline' because __no_sanitize_address confilcts
  * with inlining. Attempt to inline it may cause a build failure.
  * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
  * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
  */
-static __no_sanitize_address __maybe_unused
-void __read_once_size_nocheck(const volatile void *p, void *res, int size)
-{
-	__READ_ONCE_SIZE;
-}
+# define __no_kasan_or_inline __no_sanitize_address __maybe_unused
 #else
-static __always_inline
+# define __no_kasan_or_inline __always_inline
+#endif
+
+static __no_kasan_or_inline
 void __read_once_size_nocheck(const volatile void *p, void *res, int size)
 {
 	__READ_ONCE_SIZE;
 }
-#endif
 
 static __always_inline void __write_once_size(volatile void *p, void *res, int size)
 {

commit b899a850431e2dd0943205a63a68573f3e312d0d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Nov 27 10:38:23 2017 +0000

    compiler.h: Remove ACCESS_ONCE()
    
    There are no longer any kernelspace uses of ACCESS_ONCE(), so we can
    remove the definition from <linux/compiler.h>.
    
    This patch removes the ACCESS_ONCE() definition, and updates comments
    which referred to it. At the same time, some inconsistent and redundant
    whitespace is removed from comments.
    
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: apw@canonical.com
    Link: http://lkml.kernel.org/r/20171127103824.36526-4-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 188ed9f65517..52e611ab9a6c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -220,21 +220,21 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 /*
  * Prevent the compiler from merging or refetching reads or writes. The
  * compiler is also forbidden from reordering successive instances of
- * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
- * compiler is aware of some particular ordering.  One way to make the
- * compiler aware of ordering is to put the two invocations of READ_ONCE,
- * WRITE_ONCE or ACCESS_ONCE() in different C statements.
+ * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some
+ * particular ordering. One way to make the compiler aware of ordering is to
+ * put the two invocations of READ_ONCE or WRITE_ONCE in different C
+ * statements.
  *
- * In contrast to ACCESS_ONCE these two macros will also work on aggregate
- * data types like structs or unions. If the size of the accessed data
- * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
- * READ_ONCE() and WRITE_ONCE() will fall back to memcpy(). There's at
- * least two memcpy()s: one for the __builtin_memcpy() and then one for
- * the macro doing the copy of variable - '__u' allocated on the stack.
+ * These two macros will also work on aggregate data types like structs or
+ * unions. If the size of the accessed data type exceeds the word size of
+ * the machine (e.g., 32 bits or 64 bits) READ_ONCE() and WRITE_ONCE() will
+ * fall back to memcpy(). There's at least two memcpy()s: one for the
+ * __builtin_memcpy() and then one for the macro doing the copy of variable
+ * - '__u' allocated on the stack.
  *
  * Their two major use cases are: (1) Mediating communication between
  * process-level code and irq/NMI handlers, all running on the same CPU,
- * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
+ * and (2) Ensuring that the compiler does not fold, spindle, or otherwise
  * mutilate accesses that either do not require ordering or that interact
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
@@ -327,29 +327,4 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	compiletime_assert(__native_word(t),				\
 		"Need native word sized stores/loads for atomicity.")
 
-/*
- * Prevent the compiler from merging or refetching accesses.  The compiler
- * is also forbidden from reordering successive instances of ACCESS_ONCE(),
- * but only when the compiler is aware of some particular ordering.  One way
- * to make the compiler aware of ordering is to put the two invocations of
- * ACCESS_ONCE() in different C statements.
- *
- * ACCESS_ONCE will only work on scalar types. For union types, ACCESS_ONCE
- * on a union member will work as long as the size of the member matches the
- * size of the union and the size is smaller than word size.
- *
- * The major use cases of ACCESS_ONCE used to be (1) Mediating communication
- * between process-level code and irq/NMI handlers, all running on the same CPU,
- * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
- * mutilate accesses that either do not require ordering or that interact
- * with an explicit memory barrier or atomic instruction that provides the
- * required ordering.
- *
- * If possible use READ_ONCE()/WRITE_ONCE() instead.
- */
-#define __ACCESS_ONCE(x) ({ \
-	 __maybe_unused typeof(x) __var = (__force typeof(x)) 0; \
-	(volatile typeof(x) *)&(x); })
-#define ACCESS_ONCE(x) (*__ACCESS_ONCE(x))
-
 #endif /* __LINUX_COMPILER_H */

commit 050ab10a645097e47c01cfab3ccf24167e9b266b
Merge: 9eb719855f6c b29c6ef7bb12
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 14 07:21:44 2017 +0100

    Merge branch 'linus' into core/objtool, to pick up dependent commits
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 10259821ac47dbefa6f83ae57f1fa9f1f2c54b3d
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Nov 6 07:17:38 2017 -0600

    objtool: Make unreachable annotation inline asms explicitly volatile
    
    Add 'volatile' to the unreachable annotation macro inline asm
    statements.  They're already implicitly volatile because they don't have
    output constraints, but it's clearer and more robust to make them
    explicitly volatile.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/28659257b7a6adf4a7f65920dad70b2b0226e996.1509974104.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f8734fca54ce..4fac29cdffd1 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -193,16 +193,16 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm("%c0:\n\t"							\
-	    ".pushsection .discard.reachable\n\t"			\
-	    ".long %c0b - .\n\t"					\
-	    ".popsection\n\t" : : "i" (__COUNTER__));			\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm("%c0:\n\t"							\
-	    ".pushsection .discard.unreachable\n\t"			\
-	    ".long %c0b - .\n\t"					\
-	    ".popsection\n\t" : : "i" (__COUNTER__));			\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define ASM_UNREACHABLE							\
 	"999:\n\t"							\

commit d0c2e691d1cbe43662df3a08a4933f13acc352c3
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Nov 6 07:17:37 2017 -0600

    objtool: Add a comment for the unreachable annotation macros
    
    Add a comment for the unreachable annotation macros to explain their
    purpose and the '__COUNTER__' label hack.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1570e48d9f87e0fc6f0126c32e7e1de6e109cb67.1509974104.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 202710420d6d..f8734fca54ce 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -187,6 +187,11 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 
 /* Unreachable code */
 #ifdef CONFIG_STACK_VALIDATION
+/*
+ * These macros help objtool understand GCC code flow for unreachable code.
+ * The __COUNTER__ based labels are a hack to make each instance of the macros
+ * unique, to convince GCC not to merge duplicate inline asm statements.
+ */
 #define annotate_reachable() ({						\
 	asm("%c0:\n\t"							\
 	    ".pushsection .discard.reachable\n\t"			\

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ec1e1b6109171d1890a437481c35b2b56d2327b8
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Nov 3 17:19:41 2017 -0500

    objtool: Prevent GCC from merging annotate_unreachable(), take 2
    
    This fixes the following warning with GCC 4.6:
    
      mm/migrate.o: warning: objtool: migrate_misplaced_transhuge_page()+0x71: unreachable instruction
    
    The problem is that the compiler merged identical annotate_unreachable()
    inline asm blocks, resulting in a missing 'unreachable' annotation.
    
    This problem happened before, and was partially fixed with:
    
      3d1e236022cc ("objtool: Prevent GCC from merging annotate_unreachable()")
    
    That commit tried to ensure that each instance of the
    annotate_unreachable() inline asm statement has a unique label.  It used
    the __LINE__ macro to generate the label number.  However, even the line
    number isn't necessarily unique when used in an inline function with
    multiple callers (in this case, __alloc_pages_node()'s use of
    VM_BUG_ON).
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kbuild-all@01.org
    Cc: tipbuild@zytor.com
    Fixes: 3d1e236022cc ("objtool: Prevent GCC from merging annotate_unreachable()")
    Link: http://lkml.kernel.org/r/20171103221941.cajpwszir7ujxyc4@treble
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index fd8697aa4f73..202710420d6d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -191,13 +191,13 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	asm("%c0:\n\t"							\
 	    ".pushsection .discard.reachable\n\t"			\
 	    ".long %c0b - .\n\t"					\
-	    ".popsection\n\t" : : "i" (__LINE__));			\
+	    ".popsection\n\t" : : "i" (__COUNTER__));			\
 })
 #define annotate_unreachable() ({					\
 	asm("%c0:\n\t"							\
 	    ".pushsection .discard.unreachable\n\t"			\
 	    ".long %c0b - .\n\t"					\
-	    ".popsection\n\t" : : "i" (__LINE__));			\
+	    ".popsection\n\t" : : "i" (__COUNTER__));			\
 })
 #define ASM_UNREACHABLE							\
 	"999:\n\t"							\

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e95a2631e545..fd8697aa4f73 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __LINUX_COMPILER_H
 #define __LINUX_COMPILER_H
 

commit 59ecbbe7b31cd2d86ff9a9f461a00f7e7533aedc
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 24 11:22:49 2017 +0100

    locking/barriers: Kill lockless_dereference()
    
    lockless_dereference() is a nice idea, but it gained little traction in
    kernel code since its introduction three years ago. This is partly
    because it's a pain to type, but also because using READ_ONCE() instead
    has worked correctly on all architectures apart from Alpha, which is a
    fully supported but somewhat niche architecture these days.
    
    Now that READ_ONCE() has been upgraded to contain an implicit
    smp_read_barrier_depends() and the few callers of lockless_dereference()
    have been converted, we can remove lockless_dereference() altogether.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1508840570-22169-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 7d7b77da9716..5a1cab48442c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -346,24 +346,4 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	(volatile typeof(x) *)&(x); })
 #define ACCESS_ONCE(x) (*__ACCESS_ONCE(x))
 
-/**
- * lockless_dereference() - safely load a pointer for later dereference
- * @p: The pointer to load
- *
- * Similar to rcu_dereference(), but for situations where the pointed-to
- * object's lifetime is managed by something other than RCU.  That
- * "something other" might be reference counting or simple immortality.
- *
- * The seemingly unused variable ___typecheck_p validates that @p is
- * indeed a pointer type by using a pointer to typeof(*p) as the type.
- * Taking a pointer to typeof(*p) again is needed in case p is void *.
- */
-#define lockless_dereference(p) \
-({ \
-	typeof(p) _________p1 = READ_ONCE(p); \
-	typeof(*(p)) *___typecheck_p __maybe_unused; \
-	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
-	(_________p1); \
-})
-
 #endif /* __LINUX_COMPILER_H */

commit 76ebbe78f7390aee075a7f3768af197ded1bdfbb
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 24 11:22:47 2017 +0100

    locking/barriers: Add implicit smp_read_barrier_depends() to READ_ONCE()
    
    In preparation for the removal of lockless_dereference(), which is the
    same as READ_ONCE() on all architectures other than Alpha, add an
    implicit smp_read_barrier_depends() to READ_ONCE() so that it can be
    used to head dependency chains on all architectures.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1508840570-22169-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 08083186e54f..7d7b77da9716 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -242,6 +242,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 		__read_once_size(&(x), __u.__c, sizeof(x));		\
 	else								\
 		__read_once_size_nocheck(&(x), __u.__c, sizeof(x));	\
+	smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \
 	__u.__val;							\
 })
 #define READ_ONCE(x) __READ_ONCE(x, 1)

commit d15155824c5014803d91b829736d249c500bdda6
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 24 11:22:46 2017 +0100

    linux/compiler.h: Split into compiler.h and compiler_types.h
    
    linux/compiler.h is included indirectly by linux/types.h via
    uapi/linux/types.h -> uapi/linux/posix_types.h -> linux/stddef.h
    -> uapi/linux/stddef.h and is needed to provide a proper definition of
    offsetof.
    
    Unfortunately, compiler.h requires a definition of
    smp_read_barrier_depends() for defining lockless_dereference() and soon
    for defining READ_ONCE(), which means that all
    users of READ_ONCE() will need to include asm/barrier.h to avoid splats
    such as:
    
       In file included from include/uapi/linux/stddef.h:1:0,
                        from include/linux/stddef.h:4,
                        from arch/h8300/kernel/asm-offsets.c:11:
       include/linux/list.h: In function 'list_empty':
    >> include/linux/compiler.h:343:2: error: implicit declaration of function 'smp_read_barrier_depends' [-Werror=implicit-function-declaration]
         smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \
         ^
    
    A better alternative is to include asm/barrier.h in linux/compiler.h,
    but this requires a type definition for "bool" on some architectures
    (e.g. x86), which is defined later by linux/types.h. Type "bool" is also
    used directly in linux/compiler.h, so the whole thing is pretty fragile.
    
    This patch splits compiler.h in two: compiler_types.h contains type
    annotations, definitions and the compiler-specific parts, whereas
    compiler.h #includes compiler-types.h and additionally defines macros
    such as {READ,WRITE.ACCESS}_ONCE().
    
    uapi/linux/stddef.h and linux/linkage.h are then moved over to include
    linux/compiler_types.h, which fixes the build for h8 and blackfin.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1508840570-22169-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e95a2631e545..08083186e54f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -1,111 +1,12 @@
 #ifndef __LINUX_COMPILER_H
 #define __LINUX_COMPILER_H
 
-#ifndef __ASSEMBLY__
+#include <linux/compiler_types.h>
 
-#ifdef __CHECKER__
-# define __user		__attribute__((noderef, address_space(1)))
-# define __kernel	__attribute__((address_space(0)))
-# define __safe		__attribute__((safe))
-# define __force	__attribute__((force))
-# define __nocast	__attribute__((nocast))
-# define __iomem	__attribute__((noderef, address_space(2)))
-# define __must_hold(x)	__attribute__((context(x,1,1)))
-# define __acquires(x)	__attribute__((context(x,0,1)))
-# define __releases(x)	__attribute__((context(x,1,0)))
-# define __acquire(x)	__context__(x,1)
-# define __release(x)	__context__(x,-1)
-# define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
-# define __percpu	__attribute__((noderef, address_space(3)))
-# define __rcu		__attribute__((noderef, address_space(4)))
-# define __private	__attribute__((noderef))
-extern void __chk_user_ptr(const volatile void __user *);
-extern void __chk_io_ptr(const volatile void __iomem *);
-# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))
-#else /* __CHECKER__ */
-# ifdef STRUCTLEAK_PLUGIN
-#  define __user __attribute__((user))
-# else
-#  define __user
-# endif
-# define __kernel
-# define __safe
-# define __force
-# define __nocast
-# define __iomem
-# define __chk_user_ptr(x) (void)0
-# define __chk_io_ptr(x) (void)0
-# define __builtin_warning(x, y...) (1)
-# define __must_hold(x)
-# define __acquires(x)
-# define __releases(x)
-# define __acquire(x) (void)0
-# define __release(x) (void)0
-# define __cond_lock(x,c) (c)
-# define __percpu
-# define __rcu
-# define __private
-# define ACCESS_PRIVATE(p, member) ((p)->member)
-#endif /* __CHECKER__ */
-
-/* Indirect macros required for expanded argument pasting, eg. __LINE__. */
-#define ___PASTE(a,b) a##b
-#define __PASTE(a,b) ___PASTE(a,b)
+#ifndef __ASSEMBLY__
 
 #ifdef __KERNEL__
 
-#ifdef __GNUC__
-#include <linux/compiler-gcc.h>
-#endif
-
-#if defined(CC_USING_HOTPATCH) && !defined(__CHECKER__)
-#define notrace __attribute__((hotpatch(0,0)))
-#else
-#define notrace __attribute__((no_instrument_function))
-#endif
-
-/* Intel compiler defines __GNUC__. So we will overwrite implementations
- * coming from above header files here
- */
-#ifdef __INTEL_COMPILER
-# include <linux/compiler-intel.h>
-#endif
-
-/* Clang compiler defines __GNUC__. So we will overwrite implementations
- * coming from above header files here
- */
-#ifdef __clang__
-#include <linux/compiler-clang.h>
-#endif
-
-/*
- * Generic compiler-dependent macros required for kernel
- * build go below this comment. Actual compiler/compiler version
- * specific implementations come from the above header files
- */
-
-struct ftrace_branch_data {
-	const char *func;
-	const char *file;
-	unsigned line;
-	union {
-		struct {
-			unsigned long correct;
-			unsigned long incorrect;
-		};
-		struct {
-			unsigned long miss;
-			unsigned long hit;
-		};
-		unsigned long miss_hit[2];
-	};
-};
-
-struct ftrace_likely_data {
-	struct ftrace_branch_data	data;
-	unsigned long			constant;
-};
-
 /*
  * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
  * to disable branch tracing on a per file basis.
@@ -332,6 +233,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
  */
+#include <asm/barrier.h>
 
 #define __READ_ONCE(x, check)						\
 ({									\
@@ -362,167 +264,6 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 
 #endif /* __ASSEMBLY__ */
 
-#ifdef __KERNEL__
-/*
- * Allow us to mark functions as 'deprecated' and have gcc emit a nice
- * warning for each use, in hopes of speeding the functions removal.
- * Usage is:
- * 		int __deprecated foo(void)
- */
-#ifndef __deprecated
-# define __deprecated		/* unimplemented */
-#endif
-
-#ifdef MODULE
-#define __deprecated_for_modules __deprecated
-#else
-#define __deprecated_for_modules
-#endif
-
-#ifndef __must_check
-#define __must_check
-#endif
-
-#ifndef CONFIG_ENABLE_MUST_CHECK
-#undef __must_check
-#define __must_check
-#endif
-#ifndef CONFIG_ENABLE_WARN_DEPRECATED
-#undef __deprecated
-#undef __deprecated_for_modules
-#define __deprecated
-#define __deprecated_for_modules
-#endif
-
-#ifndef __malloc
-#define __malloc
-#endif
-
-/*
- * Allow us to avoid 'defined but not used' warnings on functions and data,
- * as well as force them to be emitted to the assembly file.
- *
- * As of gcc 3.4, static functions that are not marked with attribute((used))
- * may be elided from the assembly file.  As of gcc 3.4, static data not so
- * marked will not be elided, but this may change in a future gcc version.
- *
- * NOTE: Because distributions shipped with a backported unit-at-a-time
- * compiler in gcc 3.3, we must define __used to be __attribute__((used))
- * for gcc >=3.3 instead of 3.4.
- *
- * In prior versions of gcc, such functions and data would be emitted, but
- * would be warned about except with attribute((unused)).
- *
- * Mark functions that are referenced only in inline assembly as __used so
- * the code is emitted even though it appears to be unreferenced.
- */
-#ifndef __used
-# define __used			/* unimplemented */
-#endif
-
-#ifndef __maybe_unused
-# define __maybe_unused		/* unimplemented */
-#endif
-
-#ifndef __always_unused
-# define __always_unused	/* unimplemented */
-#endif
-
-#ifndef noinline
-#define noinline
-#endif
-
-/*
- * Rather then using noinline to prevent stack consumption, use
- * noinline_for_stack instead.  For documentation reasons.
- */
-#define noinline_for_stack noinline
-
-#ifndef __always_inline
-#define __always_inline inline
-#endif
-
-#endif /* __KERNEL__ */
-
-/*
- * From the GCC manual:
- *
- * Many functions do not examine any values except their arguments,
- * and have no effects except the return value.  Basically this is
- * just slightly more strict class than the `pure' attribute above,
- * since function is not allowed to read global memory.
- *
- * Note that a function that has pointer arguments and examines the
- * data pointed to must _not_ be declared `const'.  Likewise, a
- * function that calls a non-`const' function usually must not be
- * `const'.  It does not make sense for a `const' function to return
- * `void'.
- */
-#ifndef __attribute_const__
-# define __attribute_const__	/* unimplemented */
-#endif
-
-#ifndef __designated_init
-# define __designated_init
-#endif
-
-#ifndef __latent_entropy
-# define __latent_entropy
-#endif
-
-#ifndef __randomize_layout
-# define __randomize_layout __designated_init
-#endif
-
-#ifndef __no_randomize_layout
-# define __no_randomize_layout
-#endif
-
-#ifndef randomized_struct_fields_start
-# define randomized_struct_fields_start
-# define randomized_struct_fields_end
-#endif
-
-/*
- * Tell gcc if a function is cold. The compiler will assume any path
- * directly leading to the call is unlikely.
- */
-
-#ifndef __cold
-#define __cold
-#endif
-
-/* Simple shorthand for a section definition */
-#ifndef __section
-# define __section(S) __attribute__ ((__section__(#S)))
-#endif
-
-#ifndef __visible
-#define __visible
-#endif
-
-#ifndef __nostackprotector
-# define __nostackprotector
-#endif
-
-/*
- * Assume alignment of return value.
- */
-#ifndef __assume_aligned
-#define __assume_aligned(a, ...)
-#endif
-
-
-/* Are two types/vars the same type (ignoring qualifiers)? */
-#ifndef __same_type
-# define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
-#endif
-
-/* Is this type a native word size -- useful for atomic operations */
-#ifndef __native_word
-# define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
-#endif
-
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1

commit b1b6f83ac938d176742c85757960dec2cf10e468
Merge: 5f82e71a001d 9e52fc2b50de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 12:21:28 2017 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm changes from Ingo Molnar:
     "PCID support, 5-level paging support, Secure Memory Encryption support
    
      The main changes in this cycle are support for three new, complex
      hardware features of x86 CPUs:
    
       - Add 5-level paging support, which is a new hardware feature on
         upcoming Intel CPUs allowing up to 128 PB of virtual address space
         and 4 PB of physical RAM space - a 512-fold increase over the old
         limits. (Supercomputers of the future forecasting hurricanes on an
         ever warming planet can certainly make good use of more RAM.)
    
         Many of the necessary changes went upstream in previous cycles,
         v4.14 is the first kernel that can enable 5-level paging.
    
         This feature is activated via CONFIG_X86_5LEVEL=y - disabled by
         default.
    
         (By Kirill A. Shutemov)
    
       - Add 'encrypted memory' support, which is a new hardware feature on
         upcoming AMD CPUs ('Secure Memory Encryption', SME) allowing system
         RAM to be encrypted and decrypted (mostly) transparently by the
         CPU, with a little help from the kernel to transition to/from
         encrypted RAM. Such RAM should be more secure against various
         attacks like RAM access via the memory bus and should make the
         radio signature of memory bus traffic harder to intercept (and
         decrypt) as well.
    
         This feature is activated via CONFIG_AMD_MEM_ENCRYPT=y - disabled
         by default.
    
         (By Tom Lendacky)
    
       - Enable PCID optimized TLB flushing on newer Intel CPUs: PCID is a
         hardware feature that attaches an address space tag to TLB entries
         and thus allows to skip TLB flushing in many cases, even if we
         switch mm's.
    
         (By Andy Lutomirski)
    
      All three of these features were in the works for a long time, and
      it's coincidence of the three independent development paths that they
      are all enabled in v4.14 at once"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (65 commits)
      x86/mm: Enable RCU based page table freeing (CONFIG_HAVE_RCU_TABLE_FREE=y)
      x86/mm: Use pr_cont() in dump_pagetable()
      x86/mm: Fix SME encryption stack ptr handling
      kvm/x86: Avoid clearing the C-bit in rsvd_bits()
      x86/CPU: Align CR3 defines
      x86/mm, mm/hwpoison: Clear PRESENT bit for kernel 1:1 mappings of poison pages
      acpi, x86/mm: Remove encryption mask from ACPI page protection type
      x86/mm, kexec: Fix memory corruption with SME on successive kexecs
      x86/mm/pkeys: Fix typo in Documentation/x86/protection-keys.txt
      x86/mm/dump_pagetables: Speed up page tables dump for CONFIG_KASAN=y
      x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID
      x86: Enable 5-level paging support via CONFIG_X86_5LEVEL=y
      x86/mm: Allow userspace have mappings above 47-bit
      x86/mm: Prepare to expose larger address space to userspace
      x86/mpx: Do not allow MPX if we have mappings above 47-bit
      x86/mm: Rename tasksize_32bit/64bit to task_size_32bit/64bit()
      x86/xen: Redefine XEN_ELFNOTE_INIT_P2M using PUD_SIZE * PTRS_PER_PUD
      x86/mm/dump_pagetables: Fix printout of p4d level
      x86/mm/dump_pagetables: Generalize address normalization
      x86/boot: Fix memremap() related build failure
      ...

commit b0c79f49c343cda8954b3322984c32f258ca4ccb
Merge: f213a6c84c1b dd88a0a0c861
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 09:52:57 2017 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
    
     - Introduce the ORC unwinder, which can be enabled via
       CONFIG_ORC_UNWINDER=y.
    
       The ORC unwinder is a lightweight, Linux kernel specific debuginfo
       implementation, which aims to be DWARF done right for unwinding.
       Objtool is used to generate the ORC unwinder tables during build, so
       the data format is flexible and kernel internal: there's no
       dependency on debuginfo created by an external toolchain.
    
       The ORC unwinder is almost two orders of magnitude faster than the
       (out of tree) DWARF unwinder - which is important for perf call graph
       profiling. It is also significantly simpler and is coded defensively:
       there has not been a single ORC related kernel crash so far, even
       with early versions. (knock on wood!)
    
       But the main advantage is that enabling the ORC unwinder allows
       CONFIG_FRAME_POINTERS to be turned off - which speeds up the kernel
       measurably:
    
       With frame pointers disabled, GCC does not have to add frame pointer
       instrumentation code to every function in the kernel. The kernel's
       .text size decreases by about 3.2%, resulting in better cache
       utilization and fewer instructions executed, resulting in a broad
       kernel-wide speedup. Average speedup of system calls should be
       roughly in the 1-3% range - measurements by Mel Gorman [1] have shown
       a speedup of 5-10% for some function execution intense workloads.
    
       The main cost of the unwinder is that the unwinder data has to be
       stored in RAM: the memory cost is 2-4MB of RAM, depending on kernel
       config - which is a modest cost on modern x86 systems.
    
       Given how young the ORC unwinder code is it's not enabled by default
       - but given the performance advantages the plan is to eventually make
       it the default unwinder on x86.
    
       See Documentation/x86/orc-unwinder.txt for more details.
    
     - Remove lguest support: its intended role was that of a temporary
       proof of concept for virtualization, plus its removal will enable the
       reduction (removal) of the paravirt API as well, so Rusty agreed to
       its removal. (Juergen Gross)
    
     - Clean up and fix FSGS related functionality (Andy Lutomirski)
    
     - Clean up IO access APIs (Andy Shevchenko)
    
     - Enhance the symbol namespace (Jiri Slaby)
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (47 commits)
      objtool: Handle GCC stack pointer adjustment bug
      x86/entry/64: Use ENTRY() instead of ALIGN+GLOBAL for stub32_clone()
      x86/fpu/math-emu: Add ENDPROC to functions
      x86/boot/64: Extract efi_pe_entry() from startup_64()
      x86/boot/32: Extract efi_pe_entry() from startup_32()
      x86/lguest: Remove lguest support
      x86/paravirt/xen: Remove xen_patch()
      objtool: Fix objtool fallthrough detection with function padding
      x86/xen/64: Fix the reported SS and CS in SYSCALL
      objtool: Track DRAP separately from callee-saved registers
      objtool: Fix validate_branch() return codes
      x86: Clarify/fix no-op barriers for text_poke_bp()
      x86/switch_to/64: Rewrite FS/GS switching yet again to fix AMD CPUs
      selftests/x86/fsgsbase: Test selectors 1, 2, and 3
      x86/fsgsbase/64: Report FSBASE and GSBASE correctly in core dumps
      x86/fsgsbase/64: Fully initialize FS and GS state in start_thread_common
      x86/asm: Fix UNWIND_HINT_REGS macro for older binutils
      x86/asm/32: Fix regs_get_register() on segment registers
      x86/xen/64: Rearrange the SYSCALL entries
      x86/asm/32: Remove a bunch of '& 0xffff' from pt_regs segment reads
      ...

commit c03567a8e8d5cf2aaca40e605c48f319dc2ead57
Author: Joe Stringer <joe@ovn.org>
Date:   Thu Aug 31 16:15:33 2017 -0700

    include/linux/compiler.h: don't perform compiletime_assert with -O0
    
    Commit c7acec713d14 ("kernel.h: handle pointers to arrays better in
    container_of()") made use of __compiletime_assert() from container_of()
    thus increasing the usage of this macro, allowing developers to notice
    type conflicts in usage of container_of() at compile time.
    
    However, the implementation of __compiletime_assert relies on compiler
    optimizations to report an error.  This means that if a developer uses
    "-O0" with any code that performs container_of(), the compiler will always
    report an error regardless of whether there is an actual problem in the
    code.
    
    This patch disables compile_time_assert when optimizations are disabled to
    allow such code to compile with CFLAGS="-O0".
    
    Example compilation failure:
    
    ./include/linux/compiler.h:547:38: error: call to `__compiletime_assert_94' declared with attribute error: pointer type mismatch in container_of()
      _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
                                          ^
    ./include/linux/compiler.h:530:4: note: in definition of macro `__compiletime_assert'
        prefix ## suffix();    \
        ^~~~~~
    ./include/linux/compiler.h:547:2: note: in expansion of macro `_compiletime_assert'
      _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
      ^~~~~~~~~~~~~~~~~~~
    ./include/linux/build_bug.h:46:37: note: in expansion of macro `compiletime_assert'
     #define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
                                         ^~~~~~~~~~~~~~~~~~
    ./include/linux/kernel.h:860:2: note: in expansion of macro `BUILD_BUG_ON_MSG'
      BUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) && \
      ^~~~~~~~~~~~~~~~
    
    [akpm@linux-foundation.org: use do{}while(0), per Michal]
    Link: http://lkml.kernel.org/r/20170829230114.11662-1-joe@ovn.org
    Fixes: c7acec713d14c6c ("kernel.h: handle pointers to arrays better in container_of()")
    Signed-off-by: Joe Stringer <joe@ovn.org>
    Cc: Ian Abbott <abbotti@mev.co.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index eca8ad75e28b..043b60de041e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -517,7 +517,8 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 # define __compiletime_error_fallback(condition) do { } while (0)
 #endif
 
-#define __compiletime_assert(condition, msg, prefix, suffix)		\
+#ifdef __OPTIMIZE__
+# define __compiletime_assert(condition, msg, prefix, suffix)		\
 	do {								\
 		bool __cond = !(condition);				\
 		extern void prefix ## suffix(void) __compiletime_error(msg); \
@@ -525,6 +526,9 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 			prefix ## suffix();				\
 		__compiletime_error_fallback(__cond);			\
 	} while (0)
+#else
+# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)
+#endif
 
 #define _compiletime_assert(condition, msg, prefix, suffix) \
 	__compiletime_assert(condition, msg, prefix, suffix)

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1d0f49e14007a5426eb7e9e5808168cdb77b3e7f
Merge: 99504819fc64 e93c17301ac5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Aug 10 13:14:15 2017 +0200

    Merge branch 'x86/urgent' into x86/asm, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 649ea4d5a624f061a111b1f1cb0e47cfdc3ac21b
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Thu Jul 27 15:56:53 2017 -0500

    objtool: Assume unannotated UD2 instructions are dead ends
    
    Arnd reported some false positive warnings with GCC 7:
    
      drivers/hid/wacom_wac.o: warning: objtool: wacom_bpt3_touch()+0x2a5: stack state mismatch: cfa1=7+8 cfa2=6+16
      drivers/iio/adc/vf610_adc.o: warning: objtool: vf610_adc_calculate_rates() falls through to next function vf610_adc_sample_set()
      drivers/pwm/pwm-hibvt.o: warning: objtool: hibvt_pwm_get_state() falls through to next function hibvt_pwm_remove()
      drivers/pwm/pwm-mediatek.o: warning: objtool: mtk_pwm_config() falls through to next function mtk_pwm_enable()
      drivers/spi/spi-bcm2835.o: warning: objtool: .text: unexpected end of section
      drivers/spi/spi-bcm2835aux.o: warning: objtool: .text: unexpected end of section
      drivers/watchdog/digicolor_wdt.o: warning: objtool: dc_wdt_get_timeleft() falls through to next function dc_wdt_restart()
    
    When GCC 7 detects a potential divide-by-zero condition, it sometimes
    inserts a UD2 instruction for the case where the divisor is zero,
    instead of letting the hardware trap on the divide instruction.
    
    Objtool doesn't consider UD2 to be fatal unless it's annotated with
    unreachable().  So it considers the GCC-generated UD2 to be non-fatal,
    and it tries to follow the control flow past the UD2 and gets
    confused.
    
    Previously, objtool *did* assume UD2 was always a dead end.  That
    changed with the following commit:
    
      d1091c7fa3d5 ("objtool: Improve detection of BUG() and other dead ends")
    
    The motivation behind that change was that Peter was planning on using
    UD2 for __WARN(), which is *not* a dead end.  However, it turns out
    that some emulators rely on UD2 being fatal, so he ended up using
    'ud0' instead:
    
      9a93848fe787 ("x86/debug: Implement __WARN() using UD0")
    
    For GCC 4.5+, it should be safe to go back to the previous assumption
    that UD2 is fatal, even when it's not annotated with unreachable().
    
    But for pre-4.5 versions of GCC, the unreachable() macro isn't
    supported, so such cases of UD2 need to be explicitly annotated as
    reachable.
    
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: d1091c7fa3d5 ("objtool: Improve detection of BUG() and other dead ends")
    Link: http://lkml.kernel.org/r/e57fa9dfede25f79487da8126ee9cdf7b856db65.1501188854.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 641f5912d75f..70f070ddd7c7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -185,11 +185,34 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #endif
 
 /* Unreachable code */
+#ifdef CONFIG_STACK_VALIDATION
+#define annotate_reachable() ({						\
+	asm("%c0:\n\t"							\
+	    ".pushsection .discard.reachable\n\t"			\
+	    ".long %c0b - .\n\t"					\
+	    ".popsection\n\t" : : "i" (__LINE__));			\
+})
+#define annotate_unreachable() ({					\
+	asm("%c0:\n\t"							\
+	    ".pushsection .discard.unreachable\n\t"			\
+	    ".long %c0b - .\n\t"					\
+	    ".popsection\n\t" : : "i" (__LINE__));			\
+})
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
+#else
+#define annotate_reachable()
+#define annotate_unreachable()
+#endif
+
 #ifndef ASM_UNREACHABLE
 # define ASM_UNREACHABLE
 #endif
 #ifndef unreachable
-# define unreachable() do { } while (1)
+# define unreachable() do { annotate_reachable(); do { } while (1); } while (0)
 #endif
 
 /*

commit aa5d1b81500e6059190f18fe25a7617682321910
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jul 24 11:35:48 2017 -0700

    x86/asm: Add ASM_UNREACHABLE
    
    This creates an unreachable annotation in asm for CONFIG_STACK_VALIDATION=y.
    While here, adjust earlier uses of \t\n into \n\t.
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Elena Reshetova <elena.reshetova@intel.com>
    Cc: Eric Biggers <ebiggers3@gmail.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Hans Liljestrand <ishkamiel@gmail.com>
    Cc: James Bottomley <James.Bottomley@hansenpartnership.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Serge E. Hallyn <serge@hallyn.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arozansk@redhat.com
    Cc: axboe@kernel.dk
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1500921349-10803-3-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 219f82f3ec1a..641f5912d75f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -185,6 +185,9 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 #endif
 
 /* Unreachable code */
+#ifndef ASM_UNREACHABLE
+# define ASM_UNREACHABLE
+#endif
 #ifndef unreachable
 # define unreachable() do { } while (1)
 #endif

commit e06fdaf40a5c021dd4a2ec797e8b724f07360070
Merge: a90c6ac2b565 8acdf5055974
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 19 08:55:18 2017 -0700

    Merge tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull structure randomization updates from Kees Cook:
     "Now that IPC and other changes have landed, enable manual markings for
      randstruct plugin, including the task_struct.
    
      This is the rest of what was staged in -next for the gcc-plugins, and
      comes in three patches, largest first:
    
       - mark "easy" structs with __randomize_layout
    
       - mark task_struct with an optional anonymous struct to isolate the
         __randomize_layout section
    
       - mark structs to opt _out_ of automated marking (which will come
         later)
    
      And, FWIW, this continues to pass allmodconfig (normal and patched to
      enable gcc-plugins) builds of x86_64, i386, arm64, arm, powerpc, and
      s390 for me"
    
    * tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      randstruct: opt-out externally exposed function pointer structs
      task_struct: Allow randomized layout
      randstruct: Mark various structs for randomization

commit 7375ae3a0b79ea072f4c672039f08f5db633b9e1
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:34 2017 -0500

    compiler-gcc.h: Introduce __nostackprotector function attribute
    
    Create a new function attribute, __nostackprotector, that can used to turn off
    stack protection on a per function basis.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/0576fd5c74440ad0250f16ac6609ecf587812456.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 219f82f3ec1a..3f8c88e29a46 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -470,6 +470,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 #define __visible
 #endif
 
+#ifndef __nostackprotector
+# define __nostackprotector
+#endif
+
 /*
  * Assume alignment of return value.
  */

commit 59005b0c59a164101b0273e4bda212c809dc2246
Merge: 2cc7b4ca7d01 d1185a8c5dd2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 11:46:59 2017 -0700

    Merge tag 'gcc-plugins-v4.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull GCC plugin updates from Kees Cook:
     "The big part is the randstruct plugin infrastructure.
    
      This is the first of two expected pull requests for randstruct since
      there are dependencies in other trees that would be easier to merge
      once those have landed. Notably, the IPC allocation refactoring in
      -mm, and many trivial merge conflicts across several trees when
      applying the __randomize_layout annotation.
    
      As a result, it seemed like I should send this now since it is
      relatively self-contained, and once the rest of the trees have landed,
      send the annotation patches. I'm expecting the final phase of
      randstruct (automatic struct selection) will land for v4.14, but if
      its other tree dependencies actually make it for v4.13, I can send
      that merge request too.
    
      Summary:
    
      - typo fix in Kconfig (Jean Delvare)
    
      - randstruct infrastructure"
    
    * tag 'gcc-plugins-v4.13-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      ARM: Prepare for randomized task_struct
      randstruct: Whitelist NIU struct page overloading
      randstruct: Whitelist big_key path struct overloading
      randstruct: Whitelist UNIXCB cast
      randstruct: Whitelist struct security_hook_heads cast
      gcc-plugins: Add the randstruct plugin
      Fix English in description of GCC_PLUGIN_STRUCTLEAK
      compiler: Add __designated_init annotation
      gcc-plugins: Detail c-common.h location for GCC 4.6

commit 29e48ce87f1eaaa4b1fe3d9af90c586ac2d1fb74
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 5 22:43:33 2017 -0700

    task_struct: Allow randomized layout
    
    This marks most of the layout of task_struct as randomizable, but leaves
    thread_info and scheduler state untouched at the start, and thread_struct
    untouched at the end.
    
    Other parts of the kernel use unnamed structures, but the 0-day builder
    using gcc-4.4 blows up on static initializers. Officially, it's documented
    as only working on gcc 4.6 and later, which further confuses me:
            https://gcc.gnu.org/wiki/C11Status
    The structure layout randomization already requires gcc 4.7, but instead
    of depending on the plugin being enabled, just check the gcc versions
    for wider build testing. At Linus's suggestion, the marking is hidden
    in a macro to reduce how ugly it looks. Additionally, indenting is left
    unchanged since it would make things harder to read.
    
    Randomization of task_struct is modified from Brad Spengler/PaX Team's
    code in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 55ee9ee814f8..0b4ac3e8c63e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -456,6 +456,11 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 # define __no_randomize_layout
 #endif
 
+#ifndef randomized_struct_fields_start
+# define randomized_struct_fields_start
+# define randomized_struct_fields_end
+#endif
+
 /*
  * Tell gcc if a function is cold. The compiler will assume any path
  * directly leading to the call is unlikely.

commit 313dd1b629219db50cad532dba6a3b3b22ffe622
Author: Kees Cook <keescook@chromium.org>
Date:   Fri May 5 23:37:45 2017 -0700

    gcc-plugins: Add the randstruct plugin
    
    This randstruct plugin is modified from Brad Spengler/PaX Team's code
    in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    The randstruct GCC plugin randomizes the layout of selected structures
    at compile time, as a probabilistic defense against attacks that need to
    know the layout of structures within the kernel. This is most useful for
    "in-house" kernel builds where neither the randomization seed nor other
    build artifacts are made available to an attacker. While less useful for
    distribution kernels (where the randomization seed must be exposed for
    third party kernel module builds), it still has some value there since now
    all kernel builds would need to be tracked by an attacker.
    
    In more performance sensitive scenarios, GCC_PLUGIN_RANDSTRUCT_PERFORMANCE
    can be selected to make a best effort to restrict randomization to
    cacheline-sized groups of elements, and will not randomize bitfields. This
    comes at the cost of reduced randomization.
    
    Two annotations are defined,__randomize_layout and __no_randomize_layout,
    which respectively tell the plugin to either randomize or not to
    randomize instances of the struct in question. Follow-on patches enable
    the auto-detection logic for selecting structures for randomization
    that contain only function pointers. It is disabled here to assist with
    bisection.
    
    Since any randomized structs must be initialized using designated
    initializers, __randomize_layout includes the __designated_init annotation
    even when the plugin is disabled so that all builds will require
    the needed initialization. (With the plugin enabled, annotations for
    automatically chosen structures are marked as well.)
    
    The main differences between this implemenation and grsecurity are:
    - disable automatic struct selection (to be enabled in follow-up patch)
    - add designated_init attribute at runtime and for manual marking
    - clarify debugging output to differentiate bad cast warnings
    - add whitelisting infrastructure
    - support gcc 7's DECL_ALIGN and DECL_MODE changes (Laura Abbott)
    - raise minimum required GCC version to 4.7
    
    Earlier versions of this patch series were ported by Michael Leibowitz.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 80a1dea36cbe..55ee9ee814f8 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -448,6 +448,14 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 # define __latent_entropy
 #endif
 
+#ifndef __randomize_layout
+# define __randomize_layout __designated_init
+#endif
+
+#ifndef __no_randomize_layout
+# define __no_randomize_layout
+#endif
+
 /*
  * Tell gcc if a function is cold. The compiler will assume any path
  * directly leading to the call is unlikely.

commit 41a2901e7d220875752a8c870e0b53288a578c20
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri May 12 15:56:35 2017 -0700

    rcu: Remove SPARSE_RCU_POINTER Kconfig option
    
    The sparse-based checking for non-RCU accesses to RCU-protected pointers
    has been around for a very long time, and it is now the only type of
    sparse-based checking that is optional.  This commit therefore makes
    it unconditional.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f8110051188f..707242fdbb89 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -17,11 +17,7 @@
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 # define __percpu	__attribute__((noderef, address_space(3)))
-#ifdef CONFIG_SPARSE_RCU_POINTER
 # define __rcu		__attribute__((noderef, address_space(4)))
-#else /* CONFIG_SPARSE_RCU_POINTER */
-# define __rcu
-#endif /* CONFIG_SPARSE_RCU_POINTER */
 # define __private	__attribute__((noderef))
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);

commit 0aa5e49c6845ecd82531341085f367767c9f419a
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 5 09:49:19 2017 -0700

    compiler: Add __designated_init annotation
    
    This allows structure annotations for requiring designated initialization
    in GCC 5.1.0 and later:
    https://gcc.gnu.org/onlinedocs/gcc/Designated-Inits.html
    
    The structure randomization layout plugin will be using this to help
    identify structures that need this form of initialization.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f8110051188f..80a1dea36cbe 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -440,6 +440,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 # define __attribute_const__	/* unimplemented */
 #endif
 
+#ifndef __designated_init
+# define __designated_init
+#endif
+
 #ifndef __latent_entropy
 # define __latent_entropy
 #endif

commit 86292b33d4b79ee03e2f43ea0381ef85f077c760
Merge: 1ac884f173d4 3e761a42e19c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 27 23:09:29 2017 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge yet more updates from Andrew Morton:
    
     - a few MM remainders
    
     - misc things
    
     - autofs updates
    
     - signals
    
     - affs updates
    
     - ipc
    
     - nilfs2
    
     - spelling.txt updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (78 commits)
      mm, x86: fix HIGHMEM64 && PARAVIRT build config for native_pud_clear()
      mm: add arch-independent testcases for RODATA
      hfs: atomically read inode size
      mm: clarify mm_struct.mm_{users,count} documentation
      mm: use mmget_not_zero() helper
      mm: add new mmget() helper
      mm: add new mmgrab() helper
      checkpatch: warn when formats use %Z and suggest %z
      lib/vsprintf.c: remove %Z support
      scripts/spelling.txt: add some typo-words
      scripts/spelling.txt: add "followings" pattern and fix typo instances
      scripts/spelling.txt: add "therfore" pattern and fix typo instances
      scripts/spelling.txt: add "overwriten" pattern and fix typo instances
      scripts/spelling.txt: add "overwritting" pattern and fix typo instances
      scripts/spelling.txt: add "deintialize(d)" pattern and fix typo instances
      scripts/spelling.txt: add "disassocation" pattern and fix typo instances
      scripts/spelling.txt: add "omited" pattern and fix typo instances
      scripts/spelling.txt: add "explictely" pattern and fix typo instances
      scripts/spelling.txt: add "applys" pattern and fix typo instances
      scripts/spelling.txt: add "configuartion" pattern and fix typo instances
      ...

commit 7d134b2ce639448199052fd573a324f7e7cd5ed8
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Mon Feb 27 14:26:56 2017 -0800

    kprobes: move kprobe declarations to asm-generic/kprobes.h
    
    Often all is needed is these small helpers, instead of compiler.h or a
    full kprobes.h.  This is important for asm helpers, in fact even some
    asm/kprobes.h make use of these helpers...  instead just keep a generic
    asm file with helpers useful for asm code with the least amount of
    clutter as possible.
    
    Likewise we need now to also address what to do about this file for both
    when architectures have CONFIG_HAVE_KPROBES, and when they do not.  Then
    for when architectures have CONFIG_HAVE_KPROBES but have disabled
    CONFIG_KPROBES.
    
    Right now most asm/kprobes.h do not have guards against CONFIG_KPROBES,
    this means most architecture code cannot include asm/kprobes.h safely.
    Correct this and add guards for architectures missing them.
    Additionally provide architectures that not have kprobes support with
    the default asm-generic solution.  This lets us force asm/kprobes.h on
    the header include/linux/kprobes.h always, but most importantly we can
    now safely include just asm/kprobes.h on architecture code without
    bringing the full kitchen sink of header files.
    
    Two architectures already provided a guard against CONFIG_KPROBES on its
    kprobes.h: sh, arch.  The rest of the architectures needed gaurds added.
    We avoid including any not-needed headers on asm/kprobes.h unless
    kprobes have been enabled.
    
    In a subsequent atomic change we can try now to remove compiler.h from
    include/linux/kprobes.h.
    
    During this sweep I've also identified a few architectures defining a
    common macro needed for both kprobes and ftrace, that of the definition
    of the breakput instruction up.  Some refer to this as
    BREAKPOINT_INSTRUCTION.  This must be kept outside of the #ifdef
    CONFIG_KPROBES guard.
    
    [mcgrof@kernel.org: fix arm64 build]
      Link: http://lkml.kernel.org/r/CAB=NE6X1WMByuARS4mZ1g9+W=LuVBnMDnh_5zyN0CLADaVh=Jw@mail.gmail.com
    [sfr@canb.auug.org.au: fixup for kprobes declarations moving]
      Link: http://lkml.kernel.org/r/20170214165933.13ebd4f4@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170203233139.32682-1-mcgrof@kernel.org
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 91c30cba984e..b2eb9c0a68c4 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -570,12 +570,4 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	(_________p1); \
 })
 
-/* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */
-#ifdef CONFIG_KPROBES
-# define __kprobes	__attribute__((__section__(".kprobes.text")))
-# define nokprobe_inline	__always_inline
-#else
-# define __kprobes
-# define nokprobe_inline	inline
-#endif
 #endif /* __LINUX_COMPILER_H */

commit 79b17ea740d9fab178d6a1aa15d848b5e6c01b82
Merge: e5d56efc97f8 67d04bb2bcbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 27 13:26:17 2017 -0800

    Merge tag 'trace-v4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This release has no new tracing features, just clean ups, minor fixes
      and small optimizations"
    
    * tag 'trace-v4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (25 commits)
      tracing: Remove outdated ring buffer comment
      tracing/probes: Fix a warning message to show correct maximum length
      tracing: Fix return value check in trace_benchmark_reg()
      tracing: Use modern function declaration
      jump_label: Reduce the size of struct static_key
      tracing/probe: Show subsystem name in messages
      tracing/hwlat: Update old comment about migration
      timers: Make flags output in the timer_start tracepoint useful
      tracing: Have traceprobe_probes_write() not access userspace unnecessarily
      tracing: Have COMM event filter key be treated as a string
      ftrace: Have set_graph_function handle multiple functions in one write
      ftrace: Do not hold references of ftrace_graph_{notrace_}hash out of graph_lock
      tracing: Reset parser->buffer to allow multiple "puts"
      ftrace: Have set_graph_functions handle write with RDWR
      ftrace: Reset fgd->hash in ftrace_graph_write()
      ftrace: Replace (void *)1 with a meaningful macro name FTRACE_GRAPH_EMPTY
      ftrace: Create a slight optimization on searching the ftrace_hash
      tracing: Add ftrace_hash_key() helper function
      ftrace: Convert graph filter to use hash tables
      ftrace: Expose ftrace_hash_empty and ftrace_lookup_ip
      ...

commit 134e6a034cb004ed5acd3048792de70ced1c6cf5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 19 08:57:14 2017 -0500

    tracing: Show number of constants profiled in likely profiler
    
    Now that constants are traced, it is useful to see the number of constants
    that are traced in the likely/unlikely profiler in order to know if they
    should be ignored or not.
    
    The likely/unlikely will display a number after the "correct" number if a
    "constant" count exists.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index bbbe1570de1c..a73cc9afa784 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -101,13 +101,18 @@ struct ftrace_branch_data {
 	};
 };
 
+struct ftrace_likely_data {
+	struct ftrace_branch_data	data;
+	unsigned long			constant;
+};
+
 /*
  * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
  * to disable branch tracing on a per file basis.
  */
 #if defined(CONFIG_TRACE_BRANCH_PROFILING) \
     && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)
-void ftrace_likely_update(struct ftrace_branch_data *f, int val,
+void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 			  int expect, int is_constant);
 
 #define likely_notrace(x)	__builtin_expect(!!(x), 1)
@@ -115,13 +120,13 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val,
 
 #define __branch_check__(x, expect, is_constant) ({			\
 			int ______r;					\
-			static struct ftrace_branch_data		\
+			static struct ftrace_likely_data		\
 				__attribute__((__aligned__(4)))		\
 				__attribute__((section("_ftrace_annotated_branch"))) \
 				______f = {				\
-				.func = __func__,			\
-				.file = __FILE__,			\
-				.line = __LINE__,			\
+				.data.func = __func__,			\
+				.data.file = __FILE__,			\
+				.data.line = __LINE__,			\
 			};						\
 			______r = __builtin_expect(!!(x), expect);	\
 			ftrace_likely_update(&______f, ______r,		\

commit c61f13eaa1ee17728c41370100d2d45c254ce76f
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Jan 13 11:14:39 2017 -0800

    gcc-plugins: Add structleak for more stack initialization
    
    This plugin detects any structures that contain __user attributes and
    makes sure it is being fully initialized so that a specific class of
    information exposure is eliminated. (This plugin was originally designed
    to block the exposure of siginfo in CVE-2013-2141.)
    
    Ported from grsecurity/PaX. This version adds a verbose option to the
    plugin and the Kconfig.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index cf0fa5d86059..91c30cba984e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -27,7 +27,11 @@ extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
 # define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))
 #else /* __CHECKER__ */
-# define __user
+# ifdef STRUCTLEAK_PLUGIN
+#  define __user __attribute__((user))
+# else
+#  define __user
+# endif
 # define __kernel
 # define __safe
 # define __force

commit d45ae1f7041ac52ade6c5ec76d96bbed765d67aa
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jan 17 12:29:35 2017 -0500

    tracing: Process constants for (un)likely() profiler
    
    When running the likely/unlikely profiler, one of the results did not look
    accurate. It noted that the unlikely() in link_path_walk() was 100%
    incorrect. When I added a trace_printk() to see what was happening there, it
    became 80% correct! Looking deeper into what whas happening, I found that
    gcc split that if statement into two paths. One where the if statement
    became a constant, the other path a variable. The other path had the if
    statement always hit (making the unlikely there, always false), but since
    the #define unlikely() has:
    
      #define unlikely() (__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 0))
    
    Where constants are ignored by the branch profiler, the "constant" path
    made by the compiler was ignored, even though it was hit 80% of the time.
    
    By just passing the constant value to the __branch_check__() function and
    tracing it out of line (as always correct, as likely/unlikely isn't a factor
    for constants), then we get back the accurate readings of branches that were
    optimized by gcc causing part of the execution to become constant.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index cf0fa5d86059..bbbe1570de1c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -107,12 +107,13 @@ struct ftrace_branch_data {
  */
 #if defined(CONFIG_TRACE_BRANCH_PROFILING) \
     && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)
-void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
+void ftrace_likely_update(struct ftrace_branch_data *f, int val,
+			  int expect, int is_constant);
 
 #define likely_notrace(x)	__builtin_expect(!!(x), 1)
 #define unlikely_notrace(x)	__builtin_expect(!!(x), 0)
 
-#define __branch_check__(x, expect) ({					\
+#define __branch_check__(x, expect, is_constant) ({			\
 			int ______r;					\
 			static struct ftrace_branch_data		\
 				__attribute__((__aligned__(4)))		\
@@ -122,8 +123,9 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 				.file = __FILE__,			\
 				.line = __LINE__,			\
 			};						\
-			______r = likely_notrace(x);			\
-			ftrace_likely_update(&______f, ______r, expect); \
+			______r = __builtin_expect(!!(x), expect);	\
+			ftrace_likely_update(&______f, ______r,		\
+					     expect, is_constant);	\
 			______r;					\
 		})
 
@@ -133,10 +135,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
  * written by Daniel Walker.
  */
 # ifndef likely
-#  define likely(x)	(__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 1))
+#  define likely(x)	(__branch_check__(x, 1, __builtin_constant_p(x)))
 # endif
 # ifndef unlikely
-#  define unlikely(x)	(__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 0))
+#  define unlikely(x)	(__branch_check__(x, 0, __builtin_constant_p(x)))
 # endif
 
 #ifdef CONFIG_PROFILE_ALL_BRANCHES

commit 9ffc66941df278c9f4df979b6bcf6c6ddafedd16
Merge: 133d970e0dad 0766f788eb72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 15 10:03:15 2016 -0700

    Merge tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull gcc plugins update from Kees Cook:
     "This adds a new gcc plugin named "latent_entropy". It is designed to
      extract as much possible uncertainty from a running system at boot
      time as possible, hoping to capitalize on any possible variation in
      CPU operation (due to runtime data differences, hardware differences,
      SMP ordering, thermal timing variation, cache behavior, etc).
    
      At the very least, this plugin is a much more comprehensive example
      for how to manipulate kernel code using the gcc plugin internals"
    
    * tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      latent_entropy: Mark functions with __latent_entropy
      gcc-plugins: Add latent_entropy plugin

commit 84d69848c97faab0c25aa2667b273404d2e2a64a
Merge: d4d24d2d0a7e 590abbdd2733
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 14 14:26:58 2016 -0700

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild
    
    Pull kbuild updates from Michal Marek:
    
     - EXPORT_SYMBOL for asm source by Al Viro.
    
       This does bring a regression, because genksyms no longer generates
       checksums for these symbols (CONFIG_MODVERSIONS). Nick Piggin is
       working on a patch to fix this.
    
       Plus, we are talking about functions like strcpy(), which rarely
       change prototypes.
    
     - Fixes for PPC fallout of the above by Stephen Rothwell and Nick
       Piggin
    
     - fixdep speedup by Alexey Dobriyan.
    
     - preparatory work by Nick Piggin to allow architectures to build with
       -ffunction-sections, -fdata-sections and --gc-sections
    
     - CONFIG_THIN_ARCHIVES support by Stephen Rothwell
    
     - fix for filenames with colons in the initramfs source by me.
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild: (22 commits)
      initramfs: Escape colons in depfile
      ppc: there is no clear_pages to export
      powerpc/64: whitelist unresolved modversions CRCs
      kbuild: -ffunction-sections fix for archs with conflicting sections
      kbuild: add arch specific post-link Makefile
      kbuild: allow archs to select link dead code/data elimination
      kbuild: allow architectures to use thin archives instead of ld -r
      kbuild: Regenerate genksyms lexer
      kbuild: genksyms fix for typeof handling
      fixdep: faster CONFIG_ search
      ia64: move exports to definitions
      sparc32: debride memcpy.S a bit
      [sparc] unify 32bit and 64bit string.h
      sparc: move exports to definitions
      ppc: move exports to definitions
      arm: move exports to definitions
      s390: move exports to definitions
      m68k: move exports to definitions
      alpha: move exports to actual definitions
      x86: move exports to actual definitions
      ...

commit 0766f788eb727e2e330d55d30545db65bcf2623f
Author: Emese Revfy <re.emese@gmail.com>
Date:   Mon Jun 20 20:42:34 2016 +0200

    latent_entropy: Mark functions with __latent_entropy
    
    The __latent_entropy gcc attribute can be used only on functions and
    variables.  If it is on a function then the plugin will instrument it for
    gathering control-flow entropy. If the attribute is on a variable then
    the plugin will initialize it with random contents.  The variable must
    be an integer, an integer array type or a structure with integer fields.
    
    These specific functions have been selected because they are init
    functions (to help gather boot-time entropy), are called at unpredictable
    times, or they have variable loops, each of which provide some level of
    latent entropy.
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    [kees: expanded commit message]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 668569844d37..ceaddaf76ff1 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -406,6 +406,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 # define __attribute_const__	/* unimplemented */
 #endif
 
+#ifndef __latent_entropy
+# define __latent_entropy
+#endif
+
 /*
  * Tell gcc if a function is cold. The compiler will assume any path
  * directly leading to the call is unlikely.

commit b67067f1176df6ee727450546b58704e4b588563
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Aug 24 22:29:20 2016 +1000

    kbuild: allow archs to select link dead code/data elimination
    
    Introduce LD_DEAD_CODE_DATA_ELIMINATION option for architectures to
    select to build with -ffunction-sections, -fdata-sections, and link
    with --gc-sections. It requires some work (documented) to ensure all
    unreferenced entrypoints are live, and requires toolchain and build
    verification, so it is made a per-arch option for now.
    
    On a random powerpc64le build, this yelds a significant size saving,
    it boots and runs fine, but there is a lot I haven't tested as yet, so
    these savings may be reduced if there are bugs in the link.
    
        text      data        bss        dec   filename
    11169741   1180744    1923176   14273661   vmlinux
    10445269   1004127    1919707   13369103   vmlinux.dce
    
    ~700K text, ~170K data, 6% removed from kernel image size.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1bb954842725..86130cded110 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -182,6 +182,29 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define unreachable() do { } while (1)
 #endif
 
+/*
+ * KENTRY - kernel entry point
+ * This can be used to annotate symbols (functions or data) that are used
+ * without their linker symbol being referenced explicitly. For example,
+ * interrupt vector handlers, or functions in the kernel image that are found
+ * programatically.
+ *
+ * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those
+ * are handled in their own way (with KEEP() in linker scripts).
+ *
+ * KENTRY can be avoided if the symbols in question are marked as KEEP() in the
+ * linker script. For example an architecture could KEEP() its entire
+ * boot/exception vector code rather than annotate each function and data.
+ */
+#ifndef KENTRY
+# define KENTRY(sym)						\
+	extern typeof(sym) sym;					\
+	static const unsigned long __kentry_##sym		\
+	__used							\
+	__attribute__((section("___kentry" "+" #sym ), used))	\
+	= (unsigned long)&sym;
+#endif
+
 #ifndef RELOC_HIDE
 # define RELOC_HIDE(ptr, off)					\
   ({ unsigned long __ptr;					\

commit d7127b5e5fa0551be21b86640f1648b224e36d43
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Fri Aug 26 08:16:00 2016 +0200

    locking/barriers: Don't use sizeof(void) in lockless_dereference()
    
    My previous commit:
    
      112dc0c8069e ("locking/barriers: Suppress sparse warnings in lockless_dereference()")
    
    caused sparse to complain that (in radix-tree.h) we use sizeof(void)
    since that rcu_dereference()s a void *.
    
    Really, all we need is to have the expression *p in here somewhere
    to make sure p is a pointer type, and sizeof(*p) was the thing that
    came to my mind first to make sure that's done without really doing
    anything at runtime.
    
    Another thing I had considered was using typeof(*p), but obviously
    we can't just declare a typeof(*p) variable either, since that may
    end up being void. Declaring a variable as typeof(*p)* gets around
    that, and still checks that typeof(*p) is valid, so do that. This
    type construction can't be done for _________p1 because that will
    actually be used and causes sparse address space warnings, so keep
    a separate unused variable for it.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kbuild-all@01.org
    Fixes: 112dc0c8069e ("locking/barriers: Suppress sparse warnings in lockless_dereference()")
    Link: http://lkml.kernel.org/r/1472192160-4049-1-git-send-email-johannes@sipsolutions.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 436aa4e42221..668569844d37 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -527,13 +527,14 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * object's lifetime is managed by something other than RCU.  That
  * "something other" might be reference counting or simple immortality.
  *
- * The seemingly unused size_t variable is to validate @p is indeed a pointer
- * type by making sure it can be dereferenced.
+ * The seemingly unused variable ___typecheck_p validates that @p is
+ * indeed a pointer type by using a pointer to typeof(*p) as the type.
+ * Taking a pointer to typeof(*p) again is needed in case p is void *.
  */
 #define lockless_dereference(p) \
 ({ \
 	typeof(p) _________p1 = READ_ONCE(p); \
-	size_t __maybe_unused __size_of_ptr = sizeof(*(p)); \
+	typeof(*(p)) *___typecheck_p __maybe_unused; \
 	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
 	(_________p1); \
 })

commit 112dc0c8069e5554e0ad29c58228f1e6ca49e13d
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Thu Aug 11 11:50:22 2016 +0200

    locking/barriers: Suppress sparse warnings in lockless_dereference()
    
    After Peter's commit:
    
      331b6d8c7afc ("locking/barriers: Validate lockless_dereference() is used on a pointer type")
    
    ... we get a lot of sparse warnings (one for every rcu_dereference, and more)
    since the expression here is assigning to the wrong address space.
    
    Instead of validating that 'p' is a pointer this way, instead make
    it fail compilation when it's not by using sizeof(*(p)). This will
    not cause any sparse warnings (tested, likely since the address
    space is irrelevant for sizeof), and will fail compilation when
    'p' isn't a pointer type.
    
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniel Vetter <daniel.vetter@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 331b6d8c7afc ("locking/barriers: Validate lockless_dereference() is used on a pointer type")
    Link: http://lkml.kernel.org/r/1470909022-687-2-git-send-email-johannes@sipsolutions.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1bb954842725..436aa4e42221 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -527,13 +527,13 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * object's lifetime is managed by something other than RCU.  That
  * "something other" might be reference counting or simple immortality.
  *
- * The seemingly unused void * variable is to validate @p is indeed a pointer
- * type. All pointer types silently cast to void *.
+ * The seemingly unused size_t variable is to validate @p is indeed a pointer
+ * type by making sure it can be dereferenced.
  */
 #define lockless_dereference(p) \
 ({ \
 	typeof(p) _________p1 = READ_ONCE(p); \
-	__maybe_unused const void * const _________p2 = _________p1; \
+	size_t __maybe_unused __size_of_ptr = sizeof(*(p)); \
 	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
 	(_________p1); \
 })

commit f0c98ebc57c2d5e535bc4f9167f35650d2ba3c90
Merge: d94ba9e7d8d5 0606263f24f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 28 17:22:07 2016 -0700

    Merge tag 'libnvdimm-for-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
    
     - Replace pcommit with ADR / directed-flushing.
    
       The pcommit instruction, which has not shipped on any product, is
       deprecated.  Instead, the requirement is that platforms implement
       either ADR, or provide one or more flush addresses per nvdimm.
    
       ADR (Asynchronous DRAM Refresh) flushes data in posted write buffers
       to the memory controller on a power-fail event.
    
       Flush addresses are defined in ACPI 6.x as an NVDIMM Firmware
       Interface Table (NFIT) sub-structure: "Flush Hint Address Structure".
       A flush hint is an mmio address that when written and fenced assures
       that all previous posted writes targeting a given dimm have been
       flushed to media.
    
     - On-demand ARS (address range scrub).
    
       Linux uses the results of the ACPI ARS commands to track bad blocks
       in pmem devices.  When latent errors are detected we re-scrub the
       media to refresh the bad block list, userspace can also request a
       re-scrub at any time.
    
     - Support for the Microsoft DSM (device specific method) command
       format.
    
     - Support for EDK2/OVMF virtual disk device memory ranges.
    
     - Various fixes and cleanups across the subsystem.
    
    * tag 'libnvdimm-for-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (41 commits)
      libnvdimm-btt: Delete an unnecessary check before the function call "__nd_device_register"
      nfit: do an ARS scrub on hitting a latent media error
      nfit: move to nfit/ sub-directory
      nfit, libnvdimm: allow an ARS scrub to be triggered on demand
      libnvdimm: register nvdimm_bus devices with an nd_bus driver
      pmem: clarify a debug print in pmem_clear_poison
      x86/insn: remove pcommit
      Revert "KVM: x86: add pcommit support"
      nfit, tools/testing/nvdimm/: unify shutdown paths
      libnvdimm: move ->module to struct nvdimm_bus_descriptor
      nfit: cleanup acpi_nfit_init calling convention
      nfit: fix _FIT evaluation memory leak + use after free
      tools/testing/nvdimm: add manufacturing_{date|location} dimm properties
      tools/testing/nvdimm: add virtual ramdisk range
      acpi, nfit: treat virtual ramdisk SPA as pmem region
      pmem: kill __pmem address space
      pmem: kill wmb_pmem()
      libnvdimm, pmem: use nvdimm_flush() for namespace I/O writes
      fs/dax: remove wmb_pmem()
      libnvdimm, pmem: flush posted-write queues on shutdown
      ...

commit 7a9eb20666317794d0279843fbd091af93907780
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jun 3 18:06:47 2016 -0700

    pmem: kill __pmem address space
    
    The __pmem address space was meant to annotate codepaths that touch
    persistent memory and need to coordinate a call to wmb_pmem().  Now that
    wmb_pmem() is gone, there is little need to keep this annotation.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 793c0829e3a3..b966974938ed 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -17,7 +17,6 @@
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 # define __percpu	__attribute__((noderef, address_space(3)))
-# define __pmem		__attribute__((noderef, address_space(5)))
 #ifdef CONFIG_SPARSE_RCU_POINTER
 # define __rcu		__attribute__((noderef, address_space(4)))
 #else /* CONFIG_SPARSE_RCU_POINTER */
@@ -45,7 +44,6 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __cond_lock(x,c) (c)
 # define __percpu
 # define __rcu
-# define __pmem
 # define __private
 # define ACCESS_PRIVATE(p, member) ((p)->member)
 #endif /* __CHECKER__ */

commit 7cb45c0fe9858f92cc264f6bf9d2f6a0e7d3b249
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 1 19:23:54 2016 +0200

    locking/barriers: Move smp_cond_load_acquire() to asm-generic/barrier.h
    
    Since all asm/barrier.h should/must include asm-generic/barrier.h the
    latter is a good place for generic infrastructure like this.
    
    This also allows archs to override the new smp_acquire__after_ctrl_dep().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 59a7004fc7dd..2e853b679a5d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -304,43 +304,6 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	__u.__val;					\
 })
 
-/**
- * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
- *
- * A control dependency provides a LOAD->STORE order, the additional RMB
- * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
- * aka. (load)-ACQUIRE.
- *
- * Architectures that do not do load speculation can have this be barrier().
- */
-#define smp_acquire__after_ctrl_dep()		smp_rmb()
-
-/**
- * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
- * @ptr: pointer to the variable to wait on
- * @cond: boolean expression to wait for
- *
- * Equivalent to using smp_load_acquire() on the condition variable but employs
- * the control dependency of the wait to reduce the barrier on many platforms.
- *
- * Due to C lacking lambda expressions we load the value of *ptr into a
- * pre-named variable @VAL to be used in @cond.
- */
-#ifndef smp_cond_load_acquire
-#define smp_cond_load_acquire(ptr, cond_expr) ({		\
-	typeof(ptr) __PTR = (ptr);				\
-	typeof(*ptr) VAL;					\
-	for (;;) {						\
-		VAL = READ_ONCE(*__PTR);			\
-		if (cond_expr)					\
-			break;					\
-		cpu_relax();					\
-	}							\
-	smp_acquire__after_ctrl_dep();				\
-	VAL;							\
-})
-#endif
-
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit 33ac279677dcc2441cb93d8cb9cf7a74df62814d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 24 13:17:12 2016 +0200

    locking/barriers: Introduce smp_acquire__after_ctrl_dep()
    
    Introduce smp_acquire__after_ctrl_dep(), this construct is not
    uncommon, but the lack of this barrier is.
    
    Use it to better express smp_rmb() uses in WRITE_ONCE(), the IPC
    semaphore code and the qspinlock code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 2bcaedc0f032..59a7004fc7dd 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -304,6 +304,17 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	__u.__val;					\
 })
 
+/**
+ * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
+ *
+ * A control dependency provides a LOAD->STORE order, the additional RMB
+ * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
+ * aka. (load)-ACQUIRE.
+ *
+ * Architectures that do not do load speculation can have this be barrier().
+ */
+#define smp_acquire__after_ctrl_dep()		smp_rmb()
+
 /**
  * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
  * @ptr: pointer to the variable to wait on
@@ -314,10 +325,6 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  *
  * Due to C lacking lambda expressions we load the value of *ptr into a
  * pre-named variable @VAL to be used in @cond.
- *
- * The control dependency provides a LOAD->STORE order, the additional RMB
- * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
- * aka. ACQUIRE.
  */
 #ifndef smp_cond_load_acquire
 #define smp_cond_load_acquire(ptr, cond_expr) ({		\
@@ -329,7 +336,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 			break;					\
 		cpu_relax();					\
 	}							\
-	smp_rmb(); /* ctrl + rmb := acquire */			\
+	smp_acquire__after_ctrl_dep();				\
 	VAL;							\
 })
 #endif

commit 1f03e8d2919270bd6ef64f39a45ce8df8a9f012a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 4 10:57:12 2016 +0200

    locking/barriers: Replace smp_cond_acquire() with smp_cond_load_acquire()
    
    This new form allows using hardware assisted waiting.
    
    Some hardware (ARM64 and x86) allow monitoring an address for changes,
    so by providing a pointer we can use this to replace the cpu_relax()
    with hardware optimized methods in the future.
    
    Requested-by: Will Deacon <will.deacon@arm.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06f27fd9d760..2bcaedc0f032 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -305,21 +305,34 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 })
 
 /**
- * smp_cond_acquire() - Spin wait for cond with ACQUIRE ordering
+ * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
+ * @ptr: pointer to the variable to wait on
  * @cond: boolean expression to wait for
  *
  * Equivalent to using smp_load_acquire() on the condition variable but employs
  * the control dependency of the wait to reduce the barrier on many platforms.
  *
+ * Due to C lacking lambda expressions we load the value of *ptr into a
+ * pre-named variable @VAL to be used in @cond.
+ *
  * The control dependency provides a LOAD->STORE order, the additional RMB
  * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
  * aka. ACQUIRE.
  */
-#define smp_cond_acquire(cond)	do {		\
-	while (!(cond))				\
-		cpu_relax();			\
-	smp_rmb(); /* ctrl + rmb := acquire */	\
-} while (0)
+#ifndef smp_cond_load_acquire
+#define smp_cond_load_acquire(ptr, cond_expr) ({		\
+	typeof(ptr) __PTR = (ptr);				\
+	typeof(*ptr) VAL;					\
+	for (;;) {						\
+		VAL = READ_ONCE(*__PTR);			\
+		if (cond_expr)					\
+			break;					\
+		cpu_relax();					\
+	}							\
+	smp_rmb(); /* ctrl + rmb := acquire */			\
+	VAL;							\
+})
+#endif
 
 #endif /* __KERNEL__ */
 

commit 331b6d8c7afc2e5b900b9dcd850c265e1ba8d8e7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sun May 22 12:48:27 2016 +0200

    locking/barriers: Validate lockless_dereference() is used on a pointer type
    
    Use the type to validate the argument @p is indeed a pointer type.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160522104827.GP3193@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 793c0829e3a3..06f27fd9d760 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -545,10 +545,14 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * Similar to rcu_dereference(), but for situations where the pointed-to
  * object's lifetime is managed by something other than RCU.  That
  * "something other" might be reference counting or simple immortality.
+ *
+ * The seemingly unused void * variable is to validate @p is indeed a pointer
+ * type. All pointer types silently cast to void *.
  */
 #define lockless_dereference(p) \
 ({ \
 	typeof(p) _________p1 = READ_ONCE(p); \
+	__maybe_unused const void * const _________p2 = _________p1; \
 	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
 	(_________p1); \
 })

commit d64e85d3e1c59c3664b9ec1183052ec4641ea1e2
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Thu May 19 17:10:52 2016 -0700

    compiler.h: add support for malloc attribute
    
    gcc as far back as at least 3.04 documents the function attribute
    __malloc__.  Add a shorthand for attaching that to a function
    declaration.  This was also suggested by Andi Kleen way back in 2002
    [1], but didn't get applied, perhaps because gcc at that time generated
    the exact same code with and without this attribute.
    
    This attribute tells the compiler that the return value (if non-NULL)
    can be assumed not to alias any other valid pointers at the time of the
    call.
    
    Please note that the documentation for a range of gcc versions (starting
    from around 4.7) contained a somewhat confusing and self-contradicting
    text:
    
      The malloc attribute is used to tell the compiler that a function may
      be treated as if any non-NULL pointer it returns cannot alias any other
      pointer valid when the function returns and *that the memory has
      undefined content*.  [...] Standard functions with this property include
      malloc and *calloc*.
    
    (emphasis mine). The intended meaning has later been clarified [2]:
    
      This tells the compiler that a function is malloc-like, i.e., that the
      pointer P returned by the function cannot alias any other pointer valid
      when the function returns, and moreover no pointers to valid objects
      occur in any storage addressed by P.
    
    What this means is that we can apply the attribute to kmalloc and
    friends, and it is ok for the returned memory to have well-defined
    contents (__GFP_ZERO).  But it is not ok to apply it to kmemdup(), nor
    to other functions which both allocate and possibly initialize the
    memory with existing pointers.  So unless someone is doing something
    pretty perverted kstrdup() should also be a fine candidate.
    
    [1] http://thread.gmane.org/gmane.linux.kernel/57172
    [2] https://gcc.gnu.org/bugzilla/show_bug.cgi?id=56955
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index b5ff9881bef8..793c0829e3a3 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -357,6 +357,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 #define __deprecated_for_modules
 #endif
 
+#ifndef __malloc
+#define __malloc
+#endif
+
 /*
  * Allow us to avoid 'defined but not used' warnings on functions and data,
  * as well as force them to be emitted to the assembly file.

commit 8bc6782fe20bd2584c73a35c47329c9fd0a8d34c
Merge: e23604edac2a 3500efae4410
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 15 09:00:12 2016 +0100

    Merge commit 'fixes.2015.02.23a' into core/rcu
    
     Conflicts:
            kernel/rcu/tree.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 39a1142dbba04d2e08259bd10a369465c932126b
Merge: 65d8fc777f6d fc77dbd34c5c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 29 09:55:22 2016 +0100

    Merge tag 'v4.5-rc6' into locking/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ad315455d396a1cbcb2f9fdd687b7e1b26b789e7
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Tue Dec 29 12:18:46 2015 +0800

    sparse: Add __private to privatize members of structs
    
    In C programming language, we don't have a easy way to privatize a
    member of a structure. However in kernel, sometimes there is a need to
    privatize a member in case of potential bugs or misuses.
    
    Fortunately, the noderef attribute of sparse is a way to privatize a
    member, as by defining a member as noderef, the address-of operator on
    the member will produce a noderef pointer to that member, and if anyone
    wants to dereference that kind of pointers to read or modify the member,
    sparse will yell.
    
    Based on this, __private modifier and related operation ACCESS_PRIVATE()
    are introduced, which could help detect undesigned public uses of
    private members of structs. Here is an example of sparse's output if it
    detect an undersigned public use:
    
    | kernel/rcu/tree.c:4453:25: warning: incorrect type in argument 1 (different modifiers)
    | kernel/rcu/tree.c:4453:25:    expected struct raw_spinlock [usertype] *lock
    | kernel/rcu/tree.c:4453:25:    got struct raw_spinlock [noderef] *<noident>
    
    Also, this patch improves compiler.h a little bit by adding comments for
    "#else" and "#endif".
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 00b042c49ccd..c845356952bb 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -20,12 +20,14 @@
 # define __pmem		__attribute__((noderef, address_space(5)))
 #ifdef CONFIG_SPARSE_RCU_POINTER
 # define __rcu		__attribute__((noderef, address_space(4)))
-#else
+#else /* CONFIG_SPARSE_RCU_POINTER */
 # define __rcu
-#endif
+#endif /* CONFIG_SPARSE_RCU_POINTER */
+# define __private	__attribute__((noderef))
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
-#else
+# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))
+#else /* __CHECKER__ */
 # define __user
 # define __kernel
 # define __safe
@@ -44,7 +46,9 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __percpu
 # define __rcu
 # define __pmem
-#endif
+# define __private
+# define ACCESS_PRIVATE(p, member) ((p)->member)
+#endif /* __CHECKER__ */
 
 /* Indirect macros required for expanded argument pasting, eg. __LINE__. */
 #define ___PASTE(a,b) a##b

commit b33c8ff4431a343561e2319f17c14286f2aa52e2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Feb 12 22:26:42 2016 +0100

    tracing: Fix freak link error caused by branch tracer
    
    In my randconfig tests, I came across a bug that involves several
    components:
    
    * gcc-4.9 through at least 5.3
    * CONFIG_GCOV_PROFILE_ALL enabling -fprofile-arcs for all files
    * CONFIG_PROFILE_ALL_BRANCHES overriding every if()
    * The optimized implementation of do_div() that tries to
      replace a library call with an division by multiplication
    * code in drivers/media/dvb-frontends/zl10353.c doing
    
            u32 adc_clock = 450560; /* 45.056 MHz */
            if (state->config.adc_clock)
                    adc_clock = state->config.adc_clock;
            do_div(value, adc_clock);
    
    In this case, gcc fails to determine whether the divisor
    in do_div() is __builtin_constant_p(). In particular, it
    concludes that __builtin_constant_p(adc_clock) is false, while
    __builtin_constant_p(!!adc_clock) is true.
    
    That in turn throws off the logic in do_div() that also uses
    __builtin_constant_p(), and instead of picking either the
    constant- optimized division, and the code in ilog2() that uses
    __builtin_constant_p() to figure out whether it knows the answer at
    compile time. The result is a link error from failing to find
    multiple symbols that should never have been called based on
    the __builtin_constant_p():
    
    dvb-frontends/zl10353.c:138: undefined reference to `____ilog2_NaN'
    dvb-frontends/zl10353.c:138: undefined reference to `__aeabi_uldivmod'
    ERROR: "____ilog2_NaN" [drivers/media/dvb-frontends/zl10353.ko] undefined!
    ERROR: "__aeabi_uldivmod" [drivers/media/dvb-frontends/zl10353.ko] undefined!
    
    This patch avoids the problem by changing __trace_if() to check
    whether the condition is known at compile-time to be nonzero, rather
    than checking whether it is actually a constant.
    
    I see this one link error in roughly one out of 1600 randconfig builds
    on ARM, and the patch fixes all known instances.
    
    Link: http://lkml.kernel.org/r/1455312410-1058841-1-git-send-email-arnd@arndb.de
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Fixes: ab3c9c686e22 ("branch tracer, intel-iommu: fix build with CONFIG_BRANCH_TRACER=y")
    Cc: stable@vger.kernel.org # v2.6.30+
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 00b042c49ccd..48f5aab117ae 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -144,7 +144,7 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
  */
 #define if(cond, ...) __trace_if( (cond , ## __VA_ARGS__) )
 #define __trace_if(cond) \
-	if (__builtin_constant_p((cond)) ? !!(cond) :			\
+	if (__builtin_constant_p(!!(cond)) ? !!(cond) :			\
 	({								\
 		int ______r;						\
 		static struct ftrace_branch_data			\

commit fed0764fafd8e2e629a033c0f7df4106b0dcb7f0
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Jan 25 16:33:20 2016 -0500

    locking/atomics: Update comment about READ_ONCE() and structures
    
    The comment is out of data. Also point out the performance drawback
    of the barrier();__builtin_memcpy(); barrier() followed by another
    copy from stack (__u) to lvalue;
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1453757600-11441-1-git-send-email-konrad.wilk@oracle.com
    [ Made it a bit more readable. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 00b042c49ccd..4291592b6433 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -263,8 +263,9 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * In contrast to ACCESS_ONCE these two macros will also work on aggregate
  * data types like structs or unions. If the size of the accessed data
  * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
- * READ_ONCE() and WRITE_ONCE()  will fall back to memcpy and print a
- * compile-time warning.
+ * READ_ONCE() and WRITE_ONCE() will fall back to memcpy(). There's at
+ * least two memcpy()s: one for the __builtin_memcpy() and then one for
+ * the macro doing the copy of variable - '__u' allocated on the stack.
  *
  * Their two major use cases are: (1) Mediating communication between
  * process-level code and irq/NMI handlers, all running on the same CPU,

commit b3e0b1b6d841a4b2f64fc09ea728913da8218424
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 16 14:39:38 2015 +0200

    locking, sched: Introduce smp_cond_acquire() and use it
    
    Introduce smp_cond_acquire() which combines a control dependency and a
    read barrier to form acquire semantics.
    
    This primitive has two benefits:
    
     - it documents control dependencies,
     - its typically cheaper than using smp_load_acquire() in a loop.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 4dac1036594f..00b042c49ccd 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -299,6 +299,23 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	__u.__val;					\
 })
 
+/**
+ * smp_cond_acquire() - Spin wait for cond with ACQUIRE ordering
+ * @cond: boolean expression to wait for
+ *
+ * Equivalent to using smp_load_acquire() on the condition variable but employs
+ * the control dependency of the wait to reduce the barrier on many platforms.
+ *
+ * The control dependency provides a LOAD->STORE order, the additional RMB
+ * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
+ * aka. ACQUIRE.
+ */
+#define smp_cond_acquire(cond)	do {		\
+	while (!(cond))				\
+		cpu_relax();			\
+	smp_rmb(); /* ctrl + rmb := acquire */	\
+} while (0)
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit a744fd17b5233360681ce03e43804406745b680b
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Thu Nov 5 18:45:02 2015 -0800

    compiler.h: add support for function attribute assume_aligned
    
    gcc 4.9 added the function attribute assume_aligned, indicating to the
    caller that the returned pointer may be assumed to have a certain minimal
    alignment.  This is useful if, for example, the return value is passed to
    memset().  Add a shorthand macro for that.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 52a459ff75f4..4dac1036594f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -417,6 +417,14 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 #define __visible
 #endif
 
+/*
+ * Assume alignment of return value.
+ */
+#ifndef __assume_aligned
+#define __assume_aligned(a, ...)
+#endif
+
+
 /* Are two types/vars the same type (ignoring qualifiers)? */
 #ifndef __same_type
 # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))

commit e627078a0cbdc0c391efeb5a2c4eb287328fd633
Merge: 14c79092909a b38feccd663b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 11:31:31 2015 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "There is only one new feature in this pull for the 4.4 merge window,
      most of it is small enhancements, cleanup and bug fixes:
    
       - Add the s390 backend for the software dirty bit tracking.  This
         adds two new pgtable functions pte_clear_soft_dirty and
         pmd_clear_soft_dirty which is why there is a hit to
         arch/x86/include/asm/pgtable.h in this pull request.
    
       - A series of cleanup patches for the AP bus, this includes the
         removal of the support for two outdated crypto cards (PCICC and
         PCICA).
    
       - The irq handling / signaling on buffer full in the runtime
         instrumentation code is dropped.
    
       - Some micro optimizations: remove unnecessary memory barriers for a
         couple of functions: [smb_]rmb, [smb_]wmb, atomics, bitops, and for
         spin_unlock.  Use the builtin bswap if available and make
         test_and_set_bit_lock more cache friendly.
    
       - Statistics and a tracepoint for the diagnose calls to the
         hypervisor.
    
       - The CPU measurement facility support to sample KVM guests is
         improved.
    
       - The vector instructions are now always enabled for user space
         processes if the hardware has the vector facility.  This simplifies
         the FPU handling code.  The fpu-internal.h header is split into fpu
         internals, api and types just like x86.
    
       - Cleanup and improvements for the common I/O layer.
    
       - Rework udelay to solve a problem with kprobe.  udelay has busy loop
         semantics but still uses an idle processor state for the wait"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (66 commits)
      s390: remove runtime instrumentation interrupts
      s390/cio: de-duplicate subchannel validation
      s390/css: unneeded initialization in for_each_subchannel
      s390/Kconfig: use builtin bswap
      s390/dasd: fix disconnected device with valid path mask
      s390/dasd: fix invalid PAV assignment after suspend/resume
      s390/dasd: fix double free in dasd_eckd_read_conf
      s390/kernel: fix ptrace peek/poke for floating point registers
      s390/cio: move ccw_device_stlck functions
      s390/cio: move ccw_device_call_handler
      s390/topology: reduce per_cpu() invocations
      s390/nmi: reduce size of percpu variable
      s390/nmi: fix terminology
      s390/nmi: remove casts
      s390/nmi: remove pointless error strings
      s390: don't store registers on disabled wait anymore
      s390: get rid of __set_psw_mask()
      s390/fpu: split fpu-internal.h into fpu internals, api, and type headers
      s390/dasd: fix list_del corruption after lcu changes
      s390/spinlock: remove unneeded serializations at unlock
      ...

commit 105ff3cbf225036b75a6a46c96d1ddce8e7bdc66
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 17:22:17 2015 -0800

    atomic: remove all traces of READ_ONCE_CTRL() and atomic*_read_ctrl()
    
    This seems to be a mis-reading of how alpha memory ordering works, and
    is not backed up by the alpha architecture manual.  The helper functions
    don't do anything special on any other architectures, and the arguments
    that support them being safe on other architectures also argue that they
    are safe on alpha.
    
    Basically, the "control dependency" is between a previous read and a
    subsequent write that is dependent on the value read.  Even if the
    subsequent write is actually done speculatively, there is no way that
    such a speculative write could be made visible to other cpu's until it
    has been committed, which requires validating the speculation.
    
    Note that most weakely ordered architectures (very much including alpha)
    do not guarantee any ordering relationship between two loads that depend
    on each other on a control dependency:
    
        read A
        if (val == 1)
            read B
    
    because the conditional may be predicted, and the "read B" may be
    speculatively moved up to before reading the value A.  So we require the
    user to insert a smp_rmb() between the two accesses to be correct:
    
        read A;
        if (A == 1)
            smp_rmb()
            read B
    
    Alpha is further special in that it can break that ordering even if the
    *address* of B depends on the read of A, because the cacheline that is
    read later may be stale unless you have a memory barrier in between the
    pointer read and the read of the value behind a pointer:
    
        read ptr
        read offset(ptr)
    
    whereas all other weakly ordered architectures guarantee that the data
    dependency (as opposed to just a control dependency) will order the two
    accesses.  As a result, alpha needs a "smp_read_barrier_depends()" in
    between those two reads for them to be ordered.
    
    The coontrol dependency that "READ_ONCE_CTRL()" and "atomic_read_ctrl()"
    had was a control dependency to a subsequent *write*, however, and
    nobody can finalize such a subsequent write without having actually done
    the read.  And were you to write such a value to a "stale" cacheline
    (the way the unordered reads came to be), that would seem to lose the
    write entirely.
    
    So the things that make alpha able to re-order reads even more
    aggressively than other weak architectures do not seem to be relevant
    for a subsequent write.  Alpha memory ordering may be strange, but
    there's no real indication that it is *that* strange.
    
    Also, the alpha architecture reference manual very explicitly talks
    about the definition of "Dependence Constraints" in section 5.6.1.7,
    where a preceding read dominates a subsequent write.
    
    Such a dependence constraint admittedly does not impose a BEFORE (alpha
    architecture term for globally visible ordering), but it does guarantee
    that there can be no "causal loop".  I don't see how you could avoid
    such a loop if another cpu could see the stored value and then impact
    the value of the first read.  Put another way: the read and the write
    could not be seen as being out of order wrt other cpus.
    
    So I do not see how these "x_ctrl()" functions can currently be necessary.
    
    I may have to eat my words at some point, but in the absense of clear
    proof that alpha actually needs this, or indeed even an explanation of
    how alpha could _possibly_ need it, I do not believe these functions are
    called for.
    
    And if it turns out that alpha really _does_ need a barrier for this
    case, that barrier still should not be "smp_read_barrier_depends()".
    We'd have to make up some new speciality barrier just for alpha, along
    with the documentation for why it really is necessary.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul E McKenney <paulmck@us.ibm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 3d7810341b57..fe817432190c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -299,22 +299,6 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	__u.__val;					\
 })
 
-/**
- * READ_ONCE_CTRL - Read a value heading a control dependency
- * @x: The value to be read, heading the control dependency
- *
- * Control dependencies are tricky.  See Documentation/memory-barriers.txt
- * for important information on how to use them.  Note that in many cases,
- * use of smp_load_acquire() will be much simpler.  Control dependencies
- * should be avoided except on the hottest of hotpaths.
- */
-#define READ_ONCE_CTRL(x) \
-({ \
-	typeof(x) __val = READ_ONCE(x); \
-	smp_read_barrier_depends(); /* Enforce control dependency. */ \
-	__val; \
-})
-
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit d976441f44bc5d48635d081d277aa76556ffbf8b
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Mon Oct 19 11:37:17 2015 +0300

    compiler, atomics, kasan: Provide READ_ONCE_NOCHECK()
    
    Some code may perform racy by design memory reads. This could be
    harmless, yet such code may produce KASAN warnings.
    
    To hide such accesses from KASAN this patch introduces
    READ_ONCE_NOCHECK() macro. KASAN will not check the memory
    accessed by READ_ONCE_NOCHECK(). The KernelThreadSanitizer
    (KTSAN) is going to ignore it as well.
    
    This patch creates __read_once_size_nocheck() a clone of
    __read_once_size(). The only difference between them is
    'no_sanitized_address' attribute appended to '*_nocheck'
    function. This attribute tells the compiler that instrumentation
    of memory accesses should not be applied to that function. We
    declare it as static '__maybe_unsed' because GCC is not capable
    to inline such function:
    https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
    
    With KASAN=n READ_ONCE_NOCHECK() is just a clone of READ_ONCE().
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wolfram Gloger <wmglo@dent.med.uni-muenchen.de>
    Cc: kasan-dev <kasan-dev@googlegroups.com>
    Link: http://lkml.kernel.org/r/1445243838-17763-2-git-send-email-aryabinin@virtuozzo.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c836eb2dc44d..3d7810341b57 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -198,19 +198,45 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 
 #include <uapi/linux/types.h>
 
-static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
+#define __READ_ONCE_SIZE						\
+({									\
+	switch (size) {							\
+	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;		\
+	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;		\
+	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;		\
+	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;		\
+	default:							\
+		barrier();						\
+		__builtin_memcpy((void *)res, (const void *)p, size);	\
+		barrier();						\
+	}								\
+})
+
+static __always_inline
+void __read_once_size(const volatile void *p, void *res, int size)
 {
-	switch (size) {
-	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
-	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;
-	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;
-	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;
-	default:
-		barrier();
-		__builtin_memcpy((void *)res, (const void *)p, size);
-		barrier();
-	}
+	__READ_ONCE_SIZE;
+}
+
+#ifdef CONFIG_KASAN
+/*
+ * This function is not 'inline' because __no_sanitize_address confilcts
+ * with inlining. Attempt to inline it may cause a build failure.
+ * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
+ * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
+ */
+static __no_sanitize_address __maybe_unused
+void __read_once_size_nocheck(const volatile void *p, void *res, int size)
+{
+	__READ_ONCE_SIZE;
+}
+#else
+static __always_inline
+void __read_once_size_nocheck(const volatile void *p, void *res, int size)
+{
+	__READ_ONCE_SIZE;
 }
+#endif
 
 static __always_inline void __write_once_size(volatile void *p, void *res, int size)
 {
@@ -248,8 +274,22 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * required ordering.
  */
 
-#define READ_ONCE(x) \
-	({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
+#define __READ_ONCE(x, check)						\
+({									\
+	union { typeof(x) __val; char __c[1]; } __u;			\
+	if (check)							\
+		__read_once_size(&(x), __u.__c, sizeof(x));		\
+	else								\
+		__read_once_size_nocheck(&(x), __u.__c, sizeof(x));	\
+	__u.__val;							\
+})
+#define READ_ONCE(x) __READ_ONCE(x, 1)
+
+/*
+ * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
+ * to hide memory access from KASAN.
+ */
+#define READ_ONCE_NOCHECK(x) __READ_ONCE(x, 0)
 
 #define WRITE_ONCE(x, val) \
 ({							\

commit 0c5a69f432ba1e586ac6ae5e4311c2f1cbd051fa
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Oct 7 10:54:36 2015 +0200

    s390/compiler.h Fix sparse vs. hotpatch
    
    sparse does not understand the s390 specific hotpatch attribute and
    floods the log with messages like
    include/uapi/linux/swab.h:92:8: error: attribute 'hotpatch': unknown attribute
    
    Let's just dont use it, if __CHECKER__ is defined.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c836eb2dc44d..449cb674c7fa 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -56,7 +56,7 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 #include <linux/compiler-gcc.h>
 #endif
 
-#ifdef CC_USING_HOTPATCH
+#if defined(CC_USING_HOTPATCH) && !defined(__CHECKER__)
 #define notrace __attribute__((hotpatch(0,0)))
 #else
 #define notrace __attribute__((no_instrument_function))

commit ba33034fffc1189d95301bd865f1c799256e72a2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Aug 4 09:55:48 2015 +0200

    locking, compiler.h: Cast away attributes in the WRITE_ONCE() magic
    
    The kernel build bot showed a new warning triggered by commit:
    
      76695af20c01 ("locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire()")
    
    because Sparse does not like WRITE_ONCE() accessing elements
    from the (sparse) RCU address space:
    
      fs/afs/inode.c:448:9: sparse: incorrect type in initializer (different address spaces)
      fs/afs/inode.c:448:9:    expected struct afs_permits *__val
      fs/afs/inode.c:448:9:    got void [noderef] <asn:4>*<noident>
    
    Solution is to force cast away the sparse attributes for the initializer
    of the union in WRITE_ONCE().
    
    (And as this now gets too long, also split the macro into multiple lines.)
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438674948-38310-2-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e08a6ae7c0a4..c836eb2dc44d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -252,7 +252,12 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
 
 #define WRITE_ONCE(x, val) \
-	({ union { typeof(x) __val; char __c[1]; } __u = { .__val = (val) }; __write_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
+({							\
+	union { typeof(x) __val; char __c[1]; } __u =	\
+		{ .__val = (__force typeof(x)) (val) }; \
+	__write_once_size(&(x), __u.__c, sizeof(x));	\
+	__u.__val;					\
+})
 
 /**
  * READ_ONCE_CTRL - Read a value heading a control dependency

commit 59c3cb553f5fc4ed6868eeaae6ffd8e1daf6d93e
Merge: e49251988b10 f0f2c072cf53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 11 20:44:31 2015 -0700

    Merge branch 'libnvdimm-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm
    
    Pull libnvdimm fixes from Dan Williams:
     "1) Fixes for a handful of smatch reports (Thanks Dan C.!) and minor
         bug fixes (patches 1-6)
    
      2) Correctness fixes to the BLK-mode nvdimm driver (patches 7-10).
    
         Granted these are slightly large for a -rc update.  They have been
         out for review in one form or another since the end of May and were
         deferred from the merge window while we settled on the "PMEM API"
         for the PMEM-mode nvdimm driver (ie memremap_pmem, memcpy_to_pmem,
         and wmb_pmem).
    
         Now that those apis are merged we implement them in the BLK driver
         to guarantee that mmio aperture moves stay ordered with respect to
         incoming read/write requests, and that writes are flushed through
         those mmio-windows and platform-buffers to be persistent on media.
    
      These pass the sub-system unit tests with the updates to
      tools/testing/nvdimm, and have received a successful build-report from
      the kbuild robot (468 configs).
    
      With acks from Rafael for the touches to drivers/acpi/"
    
    * 'libnvdimm-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm:
      nfit: add support for NVDIMM "latch" flag
      nfit: update block I/O path to use PMEM API
      tools/testing/nvdimm: add mock acpi_nfit_flush_address entries to nfit_test
      tools/testing/nvdimm: fix return code for unimplemented commands
      tools/testing/nvdimm: mock ioremap_wt
      pmem: add maintainer for include/linux/pmem.h
      nfit: fix smatch "use after null check" report
      nvdimm: Fix return value of nvdimm_bus_init() if class_create() fails
      libnvdimm: smatch cleanups in __nd_ioctl
      sparse: fix misplaced __pmem definition

commit 02201e3f1b46aed7c6348f406b7b40de80ba6de3
Merge: 0890a264794f 20bdc2cfdbc4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 1 10:49:25 2015 -0700

    Merge tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux
    
    Pull module updates from Rusty Russell:
     "Main excitement here is Peter Zijlstra's lockless rbtree optimization
      to speed module address lookup.  He found some abusers of the module
      lock doing that too.
    
      A little bit of parameter work here too; including Dan Streetman's
      breaking up the big param mutex so writing a parameter can load
      another module (yeah, really).  Unfortunately that broke the usual
      suspects, !CONFIG_MODULES and !CONFIG_SYSFS, so those fixes were
      appended too"
    
    * tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux: (26 commits)
      modules: only use mod->param_lock if CONFIG_MODULES
      param: fix module param locks when !CONFIG_SYSFS.
      rcu: merge fix for Convert ACCESS_ONCE() to READ_ONCE() and WRITE_ONCE()
      module: add per-module param_lock
      module: make perm const
      params: suppress unused variable error, warn once just in case code changes.
      modules: clarify CONFIG_MODULE_COMPRESS help, suggest 'N'.
      kernel/module.c: avoid ifdefs for sig_enforce declaration
      kernel/workqueue.c: remove ifdefs over wq_power_efficient
      kernel/params.c: export param_ops_bool_enable_only
      kernel/params.c: generalize bool_enable_only
      kernel/module.c: use generic module param operaters for sig_enforce
      kernel/params: constify struct kernel_param_ops uses
      sysfs: tightened sysfs permission checks
      module: Rework module_addr_{min,max}
      module: Use __module_address() for module_address_lookup()
      module: Make the mod_tree stuff conditional on PERF_EVENTS || TRACING
      module: Optimize __module_address() using a latched RB-tree
      rbtree: Implement generic latch_tree
      seqlock: Introduce raw_read_seqcount_latch()
      ...

commit 31f02455455d405320e2f749696bef4e02903b35
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jun 30 12:07:17 2015 -0400

    sparse: fix misplaced __pmem definition
    
    Move the definition of __pmem outside of CONFIG_SPARSE_RCU_POINTER to fix:
    
    drivers/nvdimm/pmem.c:198:17: sparse: too many arguments for function __builtin_expect
    drivers/nvdimm/pmem.c:36:33: sparse: expected ; at end of declaration
    drivers/nvdimm/pmem.c:48:21: sparse: void declaration
    
    ...due to __pmem failing to be defined in some configurations when
    CONFIG_SPARSE_RCU_POINTER=y.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 26fc8bc77f85..d8fbd500330e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -17,11 +17,11 @@
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 # define __percpu	__attribute__((noderef, address_space(3)))
+# define __pmem		__attribute__((noderef, address_space(5)))
 #ifdef CONFIG_SPARSE_RCU_POINTER
 # define __rcu		__attribute__((noderef, address_space(4)))
 #else
 # define __rcu
-# define __pmem		__attribute__((noderef, address_space(5)))
 #endif
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);

commit 88793e5c774ec69351ef6b5200bb59f532e41bca
Merge: 1bc5e157ed2b 61031952f4c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 29 10:34:42 2015 -0700

    Merge tag 'libnvdimm-for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm
    
    Pull libnvdimm subsystem from Dan Williams:
     "The libnvdimm sub-system introduces, in addition to the
      libnvdimm-core, 4 drivers / enabling modules:
    
      NFIT:
        Instantiates an "nvdimm bus" with the core and registers memory
        devices (NVDIMMs) enumerated by the ACPI 6.0 NFIT (NVDIMM Firmware
        Interface table).
    
        After registering NVDIMMs the NFIT driver then registers "region"
        devices.  A libnvdimm-region defines an access mode and the
        boundaries of persistent memory media.  A region may span multiple
        NVDIMMs that are interleaved by the hardware memory controller.  In
        turn, a libnvdimm-region can be carved into a "namespace" device and
        bound to the PMEM or BLK driver which will attach a Linux block
        device (disk) interface to the memory.
    
      PMEM:
        Initially merged in v4.1 this driver for contiguous spans of
        persistent memory address ranges is re-worked to drive
        PMEM-namespaces emitted by the libnvdimm-core.
    
        In this update the PMEM driver, on x86, gains the ability to assert
        that writes to persistent memory have been flushed all the way
        through the caches and buffers in the platform to persistent media.
        See memcpy_to_pmem() and wmb_pmem().
    
      BLK:
        This new driver enables access to persistent memory media through
        "Block Data Windows" as defined by the NFIT.  The primary difference
        of this driver to PMEM is that only a small window of persistent
        memory is mapped into system address space at any given point in
        time.
    
        Per-NVDIMM windows are reprogrammed at run time, per-I/O, to access
        different portions of the media.  BLK-mode, by definition, does not
        support DAX.
    
      BTT:
        This is a library, optionally consumed by either PMEM or BLK, that
        converts a byte-accessible namespace into a disk with atomic sector
        update semantics (prevents sector tearing on crash or power loss).
    
        The sinister aspect of sector tearing is that most applications do
        not know they have a atomic sector dependency.  At least today's
        disk's rarely ever tear sectors and if they do one almost certainly
        gets a CRC error on access.  NVDIMMs will always tear and always
        silently.  Until an application is audited to be robust in the
        presence of sector-tearing the usage of BTT is recommended.
    
      Thanks to: Ross Zwisler, Jeff Moyer, Vishal Verma, Christoph Hellwig,
      Ingo Molnar, Neil Brown, Boaz Harrosh, Robert Elliott, Matthew Wilcox,
      Andy Rudoff, Linda Knippers, Toshi Kani, Nicholas Moulin, Rafael
      Wysocki, and Bob Moore"
    
    * tag 'libnvdimm-for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm: (33 commits)
      arch, x86: pmem api for ensuring durability of persistent memory updates
      libnvdimm: Add sysfs numa_node to NVDIMM devices
      libnvdimm: Set numa_node to NVDIMM devices
      acpi: Add acpi_map_pxm_to_online_node()
      libnvdimm, nfit: handle unarmed dimms, mark namespaces read-only
      pmem: flag pmem block devices as non-rotational
      libnvdimm: enable iostat
      pmem: make_request cleanups
      libnvdimm, pmem: fix up max_hw_sectors
      libnvdimm, blk: add support for blk integrity
      libnvdimm, btt: add support for blk integrity
      fs/block_dev.c: skip rw_page if bdev has integrity
      libnvdimm: Non-Volatile Devices
      tools/testing/nvdimm: libnvdimm unit test infrastructure
      libnvdimm, nfit, nd_blk: driver for BLK-mode access persistent memory
      nd_btt: atomic sector updates
      libnvdimm: infrastructure for btt devices
      libnvdimm: write blk label set
      libnvdimm: write pmem label set
      libnvdimm: blk labels and namespace instantiation
      ...

commit 61031952f4c89dba1065f7a5b9419badb112554c
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Thu Jun 25 03:08:39 2015 -0400

    arch, x86: pmem api for ensuring durability of persistent memory updates
    
    Based on an original patch by Ross Zwisler [1].
    
    Writes to persistent memory have the potential to be posted to cpu
    cache, cpu write buffers, and platform write buffers (memory controller)
    before being committed to persistent media.  Provide apis,
    memcpy_to_pmem(), wmb_pmem(), and memremap_pmem(), to write data to
    pmem and assert that it is durable in PMEM (a persistent linear address
    range).  A '__pmem' attribute is added so sparse can track proper usage
    of pointers to pmem.
    
    This continues the status quo of pmem being x86 only for 4.2, but
    reworks to ioremap, and wider implementation of memremap() will enable
    other archs in 4.3.
    
    [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-May/000932.html
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    [djbw: various reworks]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 867722591be2..9a528d945498 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -21,6 +21,7 @@
 # define __rcu		__attribute__((noderef, address_space(4)))
 #else
 # define __rcu
+# define __pmem		__attribute__((noderef, address_space(5)))
 #endif
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
@@ -42,6 +43,7 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __cond_lock(x,c) (c)
 # define __percpu
 # define __rcu
+# define __pmem
 #endif
 
 /* Indirect macros required for expanded argument pasting, eg. __LINE__. */

commit 38183b9c31cf21d8996d6eee2e3a14508b20c418
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu May 28 17:20:58 2015 +1000

    rcu: merge fix for Convert ACCESS_ONCE() to READ_ONCE() and WRITE_ONCE()
    
    This mirrors the change introduced by 7d0ae8086b8 of same title
    in Linus' tree; it's not obvious as a merge resolution since we moved
    the function.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index eae42c21d5fd..52bdec710ed7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -467,7 +467,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  */
 #define lockless_dereference(p) \
 ({ \
-	typeof(p) _________p1 = ACCESS_ONCE(p); \
+	typeof(p) _________p1 = READ_ONCE(p); \
 	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
 	(_________p1); \
 })

commit 1bf7067c6e173dc10411704db48338ed69c05565
Merge: fc934d40178a 68722101ec3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 14:54:22 2015 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes are:
    
       - 'qspinlock' support, enabled on x86: queued spinlocks - these are
         now the spinlock variant used by x86 as they outperform ticket
         spinlocks in every category.  (Waiman Long)
    
       - 'pvqspinlock' support on x86: paravirtualized variant of queued
         spinlocks.  (Waiman Long, Peter Zijlstra)
    
       - 'qrwlock' support, enabled on x86: queued rwlocks.  Similar to
         queued spinlocks, they are now the variant used by x86:
    
           CONFIG_ARCH_USE_QUEUED_SPINLOCKS=y
           CONFIG_QUEUED_SPINLOCKS=y
           CONFIG_ARCH_USE_QUEUED_RWLOCKS=y
           CONFIG_QUEUED_RWLOCKS=y
    
       - various lockdep fixlets
    
       - various locking primitives cleanups, further WRITE_ONCE()
         propagation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      locking/lockdep: Remove hard coded array size dependency
      locking/qrwlock: Don't contend with readers when setting _QW_WAITING
      lockdep: Do not break user-visible string
      locking/arch: Rename set_mb() to smp_store_mb()
      locking/arch: Add WRITE_ONCE() to set_mb()
      rtmutex: Warn if trylock is called from hard/softirq context
      arch: Remove __ARCH_HAVE_CMPXCHG
      locking/rtmutex: Drop usage of __HAVE_ARCH_CMPXCHG
      locking/qrwlock: Rename QUEUE_RWLOCK to QUEUED_RWLOCKS
      locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS
      locking/pvqspinlock: Replace xchg() by the more descriptive set_mb()
      locking/pvqspinlock, x86: Enable PV qspinlock for Xen
      locking/pvqspinlock, x86: Enable PV qspinlock for KVM
      locking/pvqspinlock, x86: Implement the paravirt qspinlock call patching
      locking/pvqspinlock: Implement simple paravirt support for the qspinlock
      locking/qspinlock: Revert to test-and-set on hypervisors
      locking/qspinlock: Use a simple write to grab the lock
      locking/qspinlock: Optimize for smaller NR_CPUS
      locking/qspinlock: Extract out code snippets for the next patch
      locking/qspinlock: Add pending bit
      ...

commit 0a04b0166929405cd833c1cc40f99e862b965ddc
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 27 11:09:36 2015 +0930

    rcu: Move lockless_dereference() out of rcupdate.h
    
    I want to use lockless_dereference() from seqlock.h, which would mean
    including rcupdate.h from it, however rcupdate.h already includes
    seqlock.h.
    
    Avoid this by moving lockless_dereference() into compiler.h. This is
    somewhat tricky since it uses smp_read_barrier_depends() which isn't
    available there, but its a CPP macro so we can get away with it.
    
    The alternative would be moving it into asm/barrier.h, but that would
    be updating each arch (I can do if people feel that is more
    appropriate).
    
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 867722591be2..eae42c21d5fd 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -457,6 +457,21 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	(volatile typeof(x) *)&(x); })
 #define ACCESS_ONCE(x) (*__ACCESS_ONCE(x))
 
+/**
+ * lockless_dereference() - safely load a pointer for later dereference
+ * @p: The pointer to load
+ *
+ * Similar to rcu_dereference(), but for situations where the pointed-to
+ * object's lifetime is managed by something other than RCU.  That
+ * "something other" might be reference counting or simple immortality.
+ */
+#define lockless_dereference(p) \
+({ \
+	typeof(p) _________p1 = ACCESS_ONCE(p); \
+	smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
+	(_________p1); \
+})
+
 /* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */
 #ifdef CONFIG_KPROBES
 # define __kprobes	__attribute__((__section__(".kprobes.text")))

commit 5af4692a75daf08dddc93dbb4cd2a1b3d3b617af
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Apr 25 12:48:29 2015 -0700

    smp: Make control dependencies work on Alpha, improve documentation
    
    The current formulation of control dependencies fails on DEC Alpha,
    which does not respect dependencies of any kind unless an explicit
    memory barrier is provided.  This means that the current fomulation of
    control dependencies fails on Alpha.  This commit therefore creates a
    READ_ONCE_CTRL() that has the same overhead on non-Alpha systems, but
    causes Alpha to produce the needed ordering.  This commit also applies
    READ_ONCE_CTRL() to the one known use of control dependencies.
    
    Use of READ_ONCE_CTRL() also has the beneficial effect of adding a bit
    of self-documentation to control dependencies.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 867722591be2..5d66777914db 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -252,6 +252,22 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 #define WRITE_ONCE(x, val) \
 	({ typeof(x) __val = (val); __write_once_size(&(x), &__val, sizeof(__val)); __val; })
 
+/**
+ * READ_ONCE_CTRL - Read a value heading a control dependency
+ * @x: The value to be read, heading the control dependency
+ *
+ * Control dependencies are tricky.  See Documentation/memory-barriers.txt
+ * for important information on how to use them.  Note that in many cases,
+ * use of smp_load_acquire() will be much simpler.  Control dependencies
+ * should be avoided except on the hottest of hotpaths.
+ */
+#define READ_ONCE_CTRL(x) \
+({ \
+	typeof(x) __val = READ_ONCE(x); \
+	smp_read_barrier_depends(); /* Enforce control dependency. */ \
+	__val; \
+})
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit ab3f02fc237211f0583c1e7ba3bf504747be9b8d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:52:27 2015 +0200

    locking/arch: Add WRITE_ONCE() to set_mb()
    
    Since we assume set_mb() to result in a single store followed by a
    full memory barrier, employ WRITE_ONCE().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index a7c0941d10da..03e227ba481c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -250,7 +250,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
 
 #define WRITE_ONCE(x, val) \
-	({ typeof(x) __val = (val); __write_once_size(&(x), &__val, sizeof(__val)); __val; })
+	({ union { typeof(x) __val; char __c[1]; } __u = { .__val = (val) }; __write_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
 
 #endif /* __KERNEL__ */
 

commit 663fdcbee0a656cdaef934e7f50e6c2670373bc9
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Thu Apr 30 17:27:21 2015 +0530

    kernel: Replace reference to ASSIGN_ONCE() with WRITE_ONCE() in comment
    
    Looks like commit :
    
     43239cbe79fc ("kernel: Change ASSIGN_ONCE(val, x) to WRITE_ONCE(x, val)")
    
    left behind a reference to ASSIGN_ONCE(). Update this to WRITE_ONCE().
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: borntraeger@de.ibm.com
    Cc: dave@stgolabs.net
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20150430115721.22278.94082.stgit@preeti.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 867722591be2..a7c0941d10da 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -450,7 +450,7 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
  *
- * If possible use READ_ONCE/ASSIGN_ONCE instead.
+ * If possible use READ_ONCE()/WRITE_ONCE() instead.
  */
 #define __ACCESS_ONCE(x) ({ \
 	 __maybe_unused typeof(x) __var = (__force typeof(x)) 0; \

commit 7829fb09a2b4268b30dd9bc782fa5ebee278b137
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Apr 30 04:13:52 2015 +0200

    lib: make memzero_explicit more robust against dead store elimination
    
    In commit 0b053c951829 ("lib: memzero_explicit: use barrier instead
    of OPTIMIZER_HIDE_VAR"), we made memzero_explicit() more robust in
    case LTO would decide to inline memzero_explicit() and eventually
    find out it could be elimiated as dead store.
    
    While using barrier() works well for the case of gcc, recent efforts
    from LLVMLinux people suggest to use llvm as an alternative to gcc,
    and there, Stephan found in a simple stand-alone user space example
    that llvm could nevertheless optimize and thus elimitate the memset().
    A similar issue has been observed in the referenced llvm bug report,
    which is regarded as not-a-bug.
    
    Based on some experiments, icc is a bit special on its own, while it
    doesn't seem to eliminate the memset(), it could do so with an own
    implementation, and then result in similar findings as with llvm.
    
    The fix in this patch now works for all three compilers (also tested
    with more aggressive optimization levels). Arguably, in the current
    kernel tree it's more of a theoretical issue, but imho, it's better
    to be pedantic about it.
    
    It's clearly visible with gcc/llvm though, with the below code: if we
    would have used barrier() only here, llvm would have omitted clearing,
    not so with barrier_data() variant:
    
      static inline void memzero_explicit(void *s, size_t count)
      {
        memset(s, 0, count);
        barrier_data(s);
      }
    
      int main(void)
      {
        char buff[20];
        memzero_explicit(buff, sizeof(buff));
        return 0;
      }
    
      $ gcc -O2 test.c
      $ gdb a.out
      (gdb) disassemble main
      Dump of assembler code for function main:
       0x0000000000400400  <+0>: lea   -0x28(%rsp),%rax
       0x0000000000400405  <+5>: movq  $0x0,-0x28(%rsp)
       0x000000000040040e <+14>: movq  $0x0,-0x20(%rsp)
       0x0000000000400417 <+23>: movl  $0x0,-0x18(%rsp)
       0x000000000040041f <+31>: xor   %eax,%eax
       0x0000000000400421 <+33>: retq
      End of assembler dump.
    
      $ clang -O2 test.c
      $ gdb a.out
      (gdb) disassemble main
      Dump of assembler code for function main:
       0x00000000004004f0  <+0>: xorps  %xmm0,%xmm0
       0x00000000004004f3  <+3>: movaps %xmm0,-0x18(%rsp)
       0x00000000004004f8  <+8>: movl   $0x0,-0x8(%rsp)
       0x0000000000400500 <+16>: lea    -0x18(%rsp),%rax
       0x0000000000400505 <+21>: xor    %eax,%eax
       0x0000000000400507 <+23>: retq
      End of assembler dump.
    
    As gcc, clang, but also icc defines __GNUC__, it's sufficient to define
    this in compiler-gcc.h only to be picked up. For a fallback or otherwise
    unsupported compiler, we define it as a barrier. Similarly, for ecc which
    does not support gcc inline asm.
    
    Reference: https://llvm.org/bugs/show_bug.cgi?id=15495
    Reported-by: Stephan Mueller <smueller@chronox.de>
    Tested-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Stephan Mueller <smueller@chronox.de>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: mancha security <mancha1@zoho.com>
    Cc: Mark Charlebois <charlebm@gmail.com>
    Cc: Behan Webster <behanw@converseincode.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 0e41ca0e5927..867722591be2 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -169,6 +169,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define barrier() __memory_barrier()
 #endif
 
+#ifndef barrier_data
+# define barrier_data(ptr) barrier()
+#endif
+
 /* Unreachable code */
 #ifndef unreachable
 # define unreachable() do { } while (1)

commit 7bd3e239d6c6d1cad276e8f130b386df4234dcd7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 26 17:45:37 2015 +0100

    locking: Remove atomicy checks from {READ,WRITE}_ONCE
    
    The fact that volatile allows for atomic load/stores is a special case
    not a requirement for {READ,WRITE}_ONCE(). Their primary purpose is to
    force the compiler to emit load/stores _once_.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1b45e4a0519b..0e41ca0e5927 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -192,29 +192,16 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 
 #include <uapi/linux/types.h>
 
-static __always_inline void data_access_exceeds_word_size(void)
-#ifdef __compiletime_warning
-__compiletime_warning("data access exceeds word size and won't be atomic")
-#endif
-;
-
-static __always_inline void data_access_exceeds_word_size(void)
-{
-}
-
 static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
 {
 	switch (size) {
 	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
 	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;
 	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;
-#ifdef CONFIG_64BIT
 	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;
-#endif
 	default:
 		barrier();
 		__builtin_memcpy((void *)res, (const void *)p, size);
-		data_access_exceeds_word_size();
 		barrier();
 	}
 }
@@ -225,13 +212,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
 	case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
 	case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
-#ifdef CONFIG_64BIT
 	case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
-#endif
 	default:
 		barrier();
 		__builtin_memcpy((void *)p, (const void *)res, size);
-		data_access_exceeds_word_size();
 		barrier();
 	}
 }

commit dd36929720f40f17685e841ae0d4c581c165ea60
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 20 15:46:31 2015 -0800

    kernel: make READ_ONCE() valid on const arguments
    
    The use of READ_ONCE() causes lots of warnings witht he pending paravirt
    spinlock fixes, because those ends up having passing a member to a
    'const' structure to READ_ONCE().
    
    There should certainly be nothing wrong with using READ_ONCE() with a
    const source, but the helper function __read_once_size() would cause
    warnings because it would drop the 'const' qualifier, but also because
    the destination would be marked 'const' too due to the use of 'typeof'.
    
    Use a union of types in READ_ONCE() to avoid this issue.
    
    Also make sure to use parenthesis around the macro arguments to avoid
    possible operator precedence issues.
    
    Tested-by: Ingo Molnar <mingo@kernel.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d1ec10a940ff..1b45e4a0519b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -202,7 +202,7 @@ static __always_inline void data_access_exceeds_word_size(void)
 {
 }
 
-static __always_inline void __read_once_size(volatile void *p, void *res, int size)
+static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
 {
 	switch (size) {
 	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
@@ -259,10 +259,10 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
  */
 
 #define READ_ONCE(x) \
-	({ typeof(x) __val; __read_once_size(&x, &__val, sizeof(__val)); __val; })
+	({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
 
 #define WRITE_ONCE(x, val) \
-	({ typeof(x) __val; __val = val; __write_once_size(&x, &__val, sizeof(__val)); __val; })
+	({ typeof(x) __val = (val); __write_once_size(&(x), &__val, sizeof(__val)); __val; })
 
 #endif /* __KERNEL__ */
 

commit c833e17e276bd5d5f174aa924c4f102754ebc2be
Merge: fee5429e028c c5b19946eb76
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 14 10:54:28 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/borntraeger/linux
    
    Pull ACCESS_ONCE() rule tightening from Christian Borntraeger:
     "Tighten rules for ACCESS_ONCE
    
      This series tightens the rules for ACCESS_ONCE to only work on scalar
      types.  It also contains the necessary fixups as indicated by build
      bots of linux-next.  Now everything is in place to prevent new
      non-scalar users of ACCESS_ONCE and we can continue to convert code to
      READ_ONCE/WRITE_ONCE"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/borntraeger/linux:
      kernel: Fix sparse warning for ACCESS_ONCE
      next: sh: Fix compile error
      kernel: tighten rules for ACCESS ONCE
      mm/gup: Replace ACCESS_ONCE with READ_ONCE
      x86/spinlock: Leftover conversion ACCESS_ONCE->READ_ONCE
      x86/xen/p2m: Replace ACCESS_ONCE with READ_ONCE
      ppc/hugetlbfs: Replace ACCESS_ONCE with READ_ONCE
      ppc/kvm: Replace ACCESS_ONCE with READ_ONCE

commit b3d6524ff7956c5a898d51a18eaecb62a60a2b84
Merge: 07f80d41cf24 6a039eab53c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 17:42:32 2015 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
    
     - The remaining patches for the z13 machine support: kernel build
       option for z13, the cache synonym avoidance, SMT support,
       compare-and-delay for spinloops and the CES5S crypto adapater.
    
     - The ftrace support for function tracing with the gcc hotpatch option.
       This touches common code Makefiles, Steven is ok with the changes.
    
     - The hypfs file system gets an extension to access diagnose 0x0c data
       in user space for performance analysis for Linux running under z/VM.
    
     - The iucv hvc console gets wildcard spport for the user id filtering.
    
     - The cacheinfo code is converted to use the generic infrastructure.
    
     - Cleanup and bug fixes.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (42 commits)
      s390/process: free vx save area when releasing tasks
      s390/hypfs: Eliminate hypfs interval
      s390/hypfs: Add diagnose 0c support
      s390/cacheinfo: don't use smp_processor_id() in preemptible context
      s390/zcrypt: fixed domain scanning problem (again)
      s390/smp: increase maximum value of NR_CPUS to 512
      s390/jump label: use different nop instruction
      s390/jump label: add sanity checks
      s390/mm: correct missing space when reporting user process faults
      s390/dasd: cleanup profiling
      s390/dasd: add locking for global_profile access
      s390/ftrace: hotpatch support for function tracing
      ftrace: let notrace function attribute disable hotpatching if necessary
      ftrace: allow architectures to specify ftrace compile options
      s390: reintroduce diag 44 calls for cpu_relax()
      s390/zcrypt: Add support for new crypto express (CEX5S) adapter.
      s390/zcrypt: Number of supported ap domains is not retrievable.
      s390/spinlock: add compare-and-delay to lock wait loops
      s390/tape: remove redundant if statement
      s390/hvc_iucv: add simple wildcard matches to the iucv allow filter
      ...

commit 61f552141c9c0e88b3fdc7046265781ffd8fa68a
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sun Jan 18 16:45:42 2015 +0100

    ftrace: let notrace function attribute disable hotpatching if necessary
    
    gcc supports an s390 specific function attribute called "hotpatch".
    It can be used to specify the number of halfwords that shall be added before
    and after a function and which shall be filled with nops for runtime patching.
    
    s390 will use the hotpatch attribute for function tracing, therefore make
    sure that the notrace function attribute either disables the mcount call
    or in case of hotpatch nop generation.
    
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d5ad7b1118fc..1ef679f4b88e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -54,7 +54,11 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 #include <linux/compiler-gcc.h>
 #endif
 
+#ifdef CC_USING_HOTPATCH
+#define notrace __attribute__((hotpatch(0,0)))
+#else
 #define notrace __attribute__((no_instrument_function))
+#endif
 
 /* Intel compiler defines __GNUC__. So we will overwrite implementations
  * coming from above header files here

commit f49028292c13b958fdf4f36c8cc8119d0dde187b
Merge: eef8f4c2acac 78e691f4ae2d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 21 06:12:21 2015 +0100

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
      - Documentation updates.
    
      - Miscellaneous fixes.
    
      - Preemptible-RCU fixes, including fixing an old bug in the
        interaction of RCU priority boosting and CPU hotplug.
    
      - SRCU updates.
    
      - RCU CPU stall-warning updates.
    
      - RCU torture-test updates.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c5b19946eb76c67566aae6a84bf2b10ad59295ea
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Jan 12 12:13:39 2015 +0100

    kernel: Fix sparse warning for ACCESS_ONCE
    
    Commit 927609d622a3 ("kernel: tighten rules for ACCESS ONCE") results in
    sparse warnings like "Using plain integer as NULL pointer" - Let's add a
    type cast to the dummy assignment.
    To avoid warnings lik "sparse: warning: cast to restricted __hc32" we also
    use __force on that cast.
    
    Fixes: 927609d622a3 ("kernel: tighten rules for ACCESS ONCE")
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 5e186bf6f6c1..7bebf0563d18 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -461,7 +461,7 @@ static __always_inline void __assign_once_size(volatile void *p, void *res, int
  * If possible use READ_ONCE/ASSIGN_ONCE instead.
  */
 #define __ACCESS_ONCE(x) ({ \
-	 __maybe_unused typeof(x) __var = 0; \
+	 __maybe_unused typeof(x) __var = (__force typeof(x)) 0; \
 	(volatile typeof(x) *)&(x); })
 #define ACCESS_ONCE(x) (*__ACCESS_ONCE(x))
 

commit 927609d622a3773995f84bc03b4564f873cf0e22
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Nov 25 10:16:39 2014 +0100

    kernel: tighten rules for ACCESS ONCE
    
    Now that all non-scalar users of ACCESS_ONCE have been converted
    to READ_ONCE or ASSIGN once, lets tighten ACCESS_ONCE to only
    work on scalar types.
    This variant was proposed by Alexei Starovoitov.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index a1c81f80978e..5e186bf6f6c1 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -447,12 +447,23 @@ static __always_inline void __assign_once_size(volatile void *p, void *res, int
  * to make the compiler aware of ordering is to put the two invocations of
  * ACCESS_ONCE() in different C statements.
  *
- * This macro does absolutely -nothing- to prevent the CPU from reordering,
- * merging, or refetching absolutely anything at any time.  Its main intended
- * use is to mediate communication between process-level code and irq/NMI
- * handlers, all running on the same CPU.
+ * ACCESS_ONCE will only work on scalar types. For union types, ACCESS_ONCE
+ * on a union member will work as long as the size of the member matches the
+ * size of the union and the size is smaller than word size.
+ *
+ * The major use cases of ACCESS_ONCE used to be (1) Mediating communication
+ * between process-level code and irq/NMI handlers, all running on the same CPU,
+ * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
+ * mutilate accesses that either do not require ordering or that interact
+ * with an explicit memory barrier or atomic instruction that provides the
+ * required ordering.
+ *
+ * If possible use READ_ONCE/ASSIGN_ONCE instead.
  */
-#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
+#define __ACCESS_ONCE(x) ({ \
+	 __maybe_unused typeof(x) __var = 0; \
+	(volatile typeof(x) *)&(x); })
+#define ACCESS_ONCE(x) (*__ACCESS_ONCE(x))
 
 /* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */
 #ifdef CONFIG_KPROBES

commit 43239cbe79fc369f5d2160bd7f69e28b5c50a58c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jan 13 10:46:42 2015 +0100

    kernel: Change ASSIGN_ONCE(val, x) to WRITE_ONCE(x, val)
    
    Feedback has shown that WRITE_ONCE(x, val) is easier to use than
    ASSIGN_ONCE(val,x).
    There are no in-tree users yet, so lets change it for 3.19.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Davidlohr Bueso <dave@stgolabs.net>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index a1c81f80978e..33063f872ee3 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -215,7 +215,7 @@ static __always_inline void __read_once_size(volatile void *p, void *res, int si
 	}
 }
 
-static __always_inline void __assign_once_size(volatile void *p, void *res, int size)
+static __always_inline void __write_once_size(volatile void *p, void *res, int size)
 {
 	switch (size) {
 	case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
@@ -235,15 +235,15 @@ static __always_inline void __assign_once_size(volatile void *p, void *res, int
 /*
  * Prevent the compiler from merging or refetching reads or writes. The
  * compiler is also forbidden from reordering successive instances of
- * READ_ONCE, ASSIGN_ONCE and ACCESS_ONCE (see below), but only when the
+ * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
  * compiler is aware of some particular ordering.  One way to make the
  * compiler aware of ordering is to put the two invocations of READ_ONCE,
- * ASSIGN_ONCE or ACCESS_ONCE() in different C statements.
+ * WRITE_ONCE or ACCESS_ONCE() in different C statements.
  *
  * In contrast to ACCESS_ONCE these two macros will also work on aggregate
  * data types like structs or unions. If the size of the accessed data
  * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
- * READ_ONCE() and ASSIGN_ONCE()  will fall back to memcpy and print a
+ * READ_ONCE() and WRITE_ONCE()  will fall back to memcpy and print a
  * compile-time warning.
  *
  * Their two major use cases are: (1) Mediating communication between
@@ -257,8 +257,8 @@ static __always_inline void __assign_once_size(volatile void *p, void *res, int
 #define READ_ONCE(x) \
 	({ typeof(x) __val; __read_once_size(&x, &__val, sizeof(__val)); __val; })
 
-#define ASSIGN_ONCE(val, x) \
-	({ typeof(x) __val; __val = val; __assign_once_size(&x, &__val, sizeof(__val)); __val; })
+#define WRITE_ONCE(x, val) \
+	({ typeof(x) __val; __val = val; __write_once_size(&x, &__val, sizeof(__val)); __val; })
 
 #endif /* __KERNEL__ */
 

commit 536fa402221f09633e7c5801b327055ab716a363
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 5 11:14:48 2014 -0700

    compiler: Allow 1- and 2-byte smp_load_acquire() and smp_store_release()
    
    CPUs without single-byte and double-byte loads and stores place some
    "interesting" requirements on concurrent code.  For example (adapted
    from Peter Hurley's test code), suppose we have the following structure:
    
            struct foo {
                    spinlock_t lock1;
                    spinlock_t lock2;
                    char a; /* Protected by lock1. */
                    char b; /* Protected by lock2. */
            };
            struct foo *foop;
    
    Of course, it is common (and good) practice to place data protected
    by different locks in separate cache lines.  However, if the locks are
    rarely acquired (for example, only in rare error cases), and there are
    a great many instances of the data structure, then memory footprint can
    trump false-sharing concerns, so that it can be better to place them in
    the same cache cache line as above.
    
    But if the CPU does not support single-byte loads and stores, a store
    to foop->a will do a non-atomic read-modify-write operation on foop->b,
    which will come as a nasty surprise to someone holding foop->lock2.  So we
    now require CPUs to support single-byte and double-byte loads and stores.
    Therefore, this commit adjusts the definition of __native_word() to allow
    these sizes to be used by smp_load_acquire() and smp_store_release().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index a1c81f80978e..49811cdddaa5 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -385,7 +385,7 @@ static __always_inline void __assign_once_size(volatile void *p, void *res, int
 
 /* Is this type a native word size -- useful for atomic operations */
 #ifndef __native_word
-# define __native_word(t) (sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
+# define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
 #endif
 
 /* Compile time object size, -1 for unknown */

commit 230fa253df6352af12ad0a16128760b5cb3f92df
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Nov 25 10:01:16 2014 +0100

    kernel: Provide READ_ONCE and ASSIGN_ONCE
    
    ACCESS_ONCE does not work reliably on non-scalar types. For
    example gcc 4.6 and 4.7 might remove the volatile tag for such
    accesses during the SRA (scalar replacement of aggregates) step
    https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58145)
    
    Let's provide READ_ONCE/ASSIGN_ONCE that will do all accesses via
    scalar types as suggested by Linus Torvalds. Accesses larger than
    the machines word size cannot be guaranteed to be atomic. These
    macros will use memcpy and emit a build warning.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d5ad7b1118fc..a1c81f80978e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -186,6 +186,80 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)
 #endif
 
+#include <uapi/linux/types.h>
+
+static __always_inline void data_access_exceeds_word_size(void)
+#ifdef __compiletime_warning
+__compiletime_warning("data access exceeds word size and won't be atomic")
+#endif
+;
+
+static __always_inline void data_access_exceeds_word_size(void)
+{
+}
+
+static __always_inline void __read_once_size(volatile void *p, void *res, int size)
+{
+	switch (size) {
+	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
+	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;
+	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;
+#ifdef CONFIG_64BIT
+	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;
+#endif
+	default:
+		barrier();
+		__builtin_memcpy((void *)res, (const void *)p, size);
+		data_access_exceeds_word_size();
+		barrier();
+	}
+}
+
+static __always_inline void __assign_once_size(volatile void *p, void *res, int size)
+{
+	switch (size) {
+	case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
+	case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
+	case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
+#ifdef CONFIG_64BIT
+	case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
+#endif
+	default:
+		barrier();
+		__builtin_memcpy((void *)p, (const void *)res, size);
+		data_access_exceeds_word_size();
+		barrier();
+	}
+}
+
+/*
+ * Prevent the compiler from merging or refetching reads or writes. The
+ * compiler is also forbidden from reordering successive instances of
+ * READ_ONCE, ASSIGN_ONCE and ACCESS_ONCE (see below), but only when the
+ * compiler is aware of some particular ordering.  One way to make the
+ * compiler aware of ordering is to put the two invocations of READ_ONCE,
+ * ASSIGN_ONCE or ACCESS_ONCE() in different C statements.
+ *
+ * In contrast to ACCESS_ONCE these two macros will also work on aggregate
+ * data types like structs or unions. If the size of the accessed data
+ * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
+ * READ_ONCE() and ASSIGN_ONCE()  will fall back to memcpy and print a
+ * compile-time warning.
+ *
+ * Their two major use cases are: (1) Mediating communication between
+ * process-level code and irq/NMI handlers, all running on the same CPU,
+ * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
+ * mutilate accesses that either do not require ordering or that interact
+ * with an explicit memory barrier or atomic instruction that provides the
+ * required ordering.
+ */
+
+#define READ_ONCE(x) \
+	({ typeof(x) __val; __read_once_size(&x, &__val, sizeof(__val)); __val; })
+
+#define ASSIGN_ONCE(val, x) \
+	({ typeof(x) __val; __val = val; __assign_once_size(&x, &__val, sizeof(__val)); __val; })
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit 3737a12761636ebde0f09ef49daebb8eed18cc8a
Merge: c29deef32e36 82b897782d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:18:49 2014 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more perf updates from Ingo Molnar:
     "A second round of perf updates:
    
       - wide reaching kprobes sanitization and robustization, with the hope
         of fixing all 'probe this function crashes the kernel' bugs, by
         Masami Hiramatsu.
    
       - uprobes updates from Oleg Nesterov: tmpfs support, corner case
         fixes and robustization work.
    
       - perf tooling updates and fixes from Jiri Olsa, Namhyung Ki, Arnaldo
         et al:
            * Add support to accumulate hist periods (Namhyung Kim)
            * various fixes, refactorings and enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      perf: Differentiate exec() and non-exec() comm events
      perf: Fix perf_event_comm() vs. exec() assumption
      uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates
      perf/documentation: Add description for conditional branch filter
      perf/x86: Add conditional branch filtering support
      perf/tool: Add conditional branch filter 'cond' to perf record
      perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
      uprobes: Teach copy_insn() to support tmpfs
      uprobes: Shift ->readpage check from __copy_insn() to uprobe_register()
      perf/x86: Use common PMU interrupt disabled code
      perf/ARM: Use common PMU interrupt disabled code
      perf: Disable sampled events if no PMU interrupt
      perf: Fix use after free in perf_remove_from_context()
      perf tools: Fix 'make help' message error
      perf record: Fix poll return value propagation
      perf tools: Move elide bool into perf_hpp_fmt struct
      perf tools: Remove elide setup for SORT_MODE__MEMORY mode
      perf tools: Fix "==" into "=" in ui_browser__warning assignment
      perf tools: Allow overriding sysfs and proc finding with env var
      perf tools: Consider header files outside perf directory in tags target
      ...

commit 2c0d259e0e580dd95dd5d2d5aa4926169228d4a0
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jun 4 16:11:16 2014 -0700

    compiler.h: avoid sparse errors in __compiletime_error_fallback()
    
    Usually, BUG_ON and friends aren't even evaluated in sparse, but recently
    compiletime_assert_atomic_type() was added, and that now results in a
    sparse warning every time it is used.
    
    The reason turns out to be the temporary variable, after it sparse no
    longer considers the value to be a constant, and results in a warning and
    an error.  The error is the more annoying part of this as it suppresses
    any further warnings in the same file, hiding other problems.
    
    Unfortunately the condition cannot be simply expanded out to avoid the
    temporary variable since it breaks compiletime_assert on old versions of
    GCC such as GCC 4.2.4 which the latest metag compiler is based on.
    
    Therefore #ifndef __CHECKER__ out the __compiletime_error_fallback which
    uses the potentially negative size array to trigger a conditional compiler
    error, so that sparse doesn't see it.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Johannes Berg <johannes.berg@intel.com>
    Cc: Daniel Santos <daniel.santos@pobox.com>
    Cc: Luciano Coelho <luciano.coelho@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ee7239ea1583..64fdfe1cfcf0 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -323,9 +323,18 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #endif
 #ifndef __compiletime_error
 # define __compiletime_error(message)
-# define __compiletime_error_fallback(condition) \
+/*
+ * Sparse complains of variable sized arrays due to the temporary variable in
+ * __compiletime_assert. Unfortunately we can't just expand it out to make
+ * sparse see a constant array size without breaking compiletime_assert on old
+ * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
+ */
+# ifndef __CHECKER__
+#  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
-#else
+# endif
+#endif
+#ifndef __compiletime_error_fallback
 # define __compiletime_error_fallback(condition) do { } while (0)
 #endif
 

commit 376e242429bf8539ef39a080ac113c8799840b13
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:05 2014 +0900

    kprobes: Introduce NOKPROBE_SYMBOL() macro to maintain kprobes blacklist
    
    Introduce NOKPROBE_SYMBOL() macro which builds a kprobes
    blacklist at kernel build time.
    
    The usage of this macro is similar to EXPORT_SYMBOL(),
    placed after the function definition:
    
      NOKPROBE_SYMBOL(function);
    
    Since this macro will inhibit inlining of static/inline
    functions, this patch also introduces a nokprobe_inline macro
    for static/inline functions. In this case, we must use
    NOKPROBE_SYMBOL() for the inline function caller.
    
    When CONFIG_KPROBES=y, the macro stores the given function
    address in the "_kprobe_blacklist" section.
    
    Since the data structures are not fully initialized by the
    macro (because there is no "size" information),  those
    are re-initialized at boot time by using kallsyms.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Link: http://lkml.kernel.org/r/20140417081705.26341.96719.stgit@ltc230.yrl.intra.hitachi.co.jp
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jan-Simon Möller <dl9pf@gmx.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-sparse@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ee7239ea1583..0300c0f5c88b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -374,7 +374,9 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 /* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */
 #ifdef CONFIG_KPROBES
 # define __kprobes	__attribute__((__section__(".kprobes.text")))
+# define nokprobe_inline	__always_inline
 #else
 # define __kprobes
+# define nokprobe_inline	inline
 #endif
 #endif /* __LINUX_COMPILER_H */

commit 565cbdc2fec92ef2ae75995e06e69172ed2edecd
Author: Mark Charlebois <charlebm@gmail.com>
Date:   Tue Nov 20 22:13:10 2012 +0100

    LLVMLinux: Add support for clang to compiler.h and new compiler-clang.h
    
    Add a compiler-clang.h file to add specific macros needed for compiling the
    kernel with clang.
    
    Initially the only override required is the macro for silencing the
    compiler for a purposefully uninintialized variable.
    
    Author: Mark Charlebois <charlebm@gmail.com>
    Signed-off-by: Mark Charlebois <charlebm@gmail.com>
    Signed-off-by: Behan Webster <behanw@converseincode.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 2472740d7ab2..ee7239ea1583 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -63,6 +63,13 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # include <linux/compiler-intel.h>
 #endif
 
+/* Clang compiler defines __GNUC__. So we will overwrite implementations
+ * coming from above header files here
+ */
+#ifdef __clang__
+#include <linux/compiler-clang.h>
+#endif
+
 /*
  * Generic compiler-dependent macros required for kernel
  * build go below this comment. Actual compiler/compiler version

commit 13c789a6b219aa23f917466c7e630566106b14c2
Merge: 6dd9158ae857 79ba451d66ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 23 18:11:00 2014 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto update from Herbert Xu:
     "Here is the crypto update for 3.14:
    
       - Improved crypto_memneq helper
       - Use cyprto_memneq in arch-specific crypto code
       - Replaced orphaned DCP driver with Freescale MXS DCP driver
       - Added AVX/AVX2 version of AESNI-GCM encode and decode
       - Added AMD Cryptographic Coprocessor (CCP) driver
       - Misc fixes"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (41 commits)
      crypto: aesni - fix build on x86 (32bit)
      crypto: mxs - Fix sparse non static symbol warning
      crypto: ccp - CCP device enabled/disabled changes
      crypto: ccp - Cleanup hash invocation calls
      crypto: ccp - Change data length declarations to u64
      crypto: ccp - Check for caller result area before using it
      crypto: ccp - Cleanup scatterlist usage
      crypto: ccp - Apply appropriate gfp_t type to memory allocations
      crypto: drivers - Sort drivers/crypto/Makefile
      ARM: mxs: dts: Enable DCP for MXS
      crypto: mxs - Add Freescale MXS DCP driver
      crypto: mxs - Remove the old DCP driver
      crypto: ahash - Fully restore ahash request before completing
      crypto: aesni - fix build on x86 (32bit)
      crypto: talitos - Remove redundant dev_set_drvdata
      crypto: ccp - Remove redundant dev_set_drvdata
      crypto: crypto4xx - Remove redundant dev_set_drvdata
      crypto: caam - simplify and harden key parsing
      crypto: omap-sham - Fix Polling mode for larger blocks
      crypto: tcrypt - Added speed tests for AEAD crypto alogrithms in tcrypt test suite
      ...

commit 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()
    
    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.
    
    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.
    
    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.
    
    [Changelog by PaulMck]
    
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 92669cd182a6..fe7a686dfd8d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -298,6 +298,11 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
 #endif
 
+/* Is this type a native word size -- useful for atomic operations */
+#ifndef __native_word
+# define __native_word(t) (sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
+#endif
+
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1
@@ -337,6 +342,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #define compiletime_assert(condition, msg) \
 	_compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
 
+#define compiletime_assert_atomic_type(t)				\
+	compiletime_assert(__native_word(t),				\
+		"Need native word sized stores/loads for atomicity.")
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit fe8c8a126806fea4465c43d62a1f9d273a572bf5
Author: Cesar Eduardo Barros <cesarb@cesarb.eti.br>
Date:   Mon Nov 25 22:00:41 2013 -0200

    crypto: more robust crypto_memneq
    
    Disabling compiler optimizations can be fragile, since a new
    optimization could be added to -O0 or -Os that breaks the assumptions
    the code is making.
    
    Instead of disabling compiler optimizations, use a dummy inline assembly
    (based on RELOC_HIDE) to block the problematic kinds of optimization,
    while still allowing other optimizations to be applied to the code.
    
    The dummy inline assembly is added after every OR, and has the
    accumulator variable as its input and output. The compiler is forced to
    assume that the dummy inline assembly could both depend on the
    accumulator variable and change the accumulator variable, so it is
    forced to compute the value correctly before the inline assembly, and
    cannot assume anything about its value after the inline assembly.
    
    This change should be enough to make crypto_memneq work correctly (with
    data-independent timing) even if it is inlined at its call sites. That
    can be done later in a followup patch.
    
    Compile-tested on x86_64.
    
    Signed-off-by: Cesar Eduardo Barros <cesarb@cesarb.eti.br>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 92669cd182a6..a2329c5e6206 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -170,6 +170,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
     (typeof(ptr)) (__ptr + (off)); })
 #endif
 
+#ifndef OPTIMIZER_HIDE_VAR
+#define OPTIMIZER_HIDE_VAR(var) barrier()
+#endif
+
 /* Not-quite-unique ID. */
 #ifndef __UNIQUE_ID
 # define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)

commit 324670b620ab1ed00ba160e435686bd2ae4a59ce
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 4 19:40:50 2013 +0900

    kprobes: Move __kprobes definition into compiler.h
    
    Currently, __kprobes is defined in linux/kprobes.h which
    is too big to be included in small or basic headers
    that want to make use of this simple attribute.
    
    So move __kprobes definition into linux/compiler.h
    in which other compiler attributes are defined.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Timo Juhani Lindfors <timo.lindfors@iki.fi>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Nadia Yvette Chambers <nyc@holomorphy.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20130404104049.21071.20908.stgit@mhiramat-M0-7522
    [ Improved the attribute explanation a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 10b8f23fab0f..92669cd182a6 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -351,4 +351,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
  */
 #define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
 
+/* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */
+#ifdef CONFIG_KPROBES
+# define __kprobes	__attribute__((__section__(".kprobes.text")))
+#else
+# define __kprobes
+#endif
 #endif /* __LINUX_COMPILER_H */

commit 9a8ab1c39970a4938a72d94e6fd13be88a797590
Author: Daniel Santos <daniel.santos@pobox.com>
Date:   Thu Feb 21 16:41:55 2013 -0800

    bug.h, compiler.h: introduce compiletime_assert & BUILD_BUG_ON_MSG
    
    Introduce compiletime_assert to compiler.h, which moves the details of
    how to break a build and emit an error message for a specific compiler
    to the headers where these details should be.  Following in the
    tradition of the POSIX assert macro, compiletime_assert creates a
    build-time error when the supplied condition is *false*.
    
    Next, we add BUILD_BUG_ON_MSG to bug.h which simply wraps
    compiletime_assert, inverting the logic, so that it fails when the
    condition is *true*, consistent with the language "build bug on." This
    macro allows you to specify the error message you want emitted when the
    supplied condition is true.
    
    Finally, we remove all other code from bug.h that mucks with these
    details (BUILD_BUG & BUILD_BUG_ON), and have them all call
    BUILD_BUG_ON_MSG.  This not only reduces source code bloat, but also
    prevents the possibility of code being changed for one macro and not for
    the other (which was previously the case for BUILD_BUG and
    BUILD_BUG_ON).
    
    Since __compiletime_error_fallback is now only used in compiler.h, I'm
    considering it a private macro and removing the double negation that's
    now extraneous.
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Signed-off-by: Daniel Santos <daniel.santos@pobox.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 423bb6bd660f..10b8f23fab0f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -308,11 +308,35 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #ifndef __compiletime_error
 # define __compiletime_error(message)
 # define __compiletime_error_fallback(condition) \
-	do { ((void)sizeof(char[1 - 2*!!(condition)])); } while (0)
+	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 #else
 # define __compiletime_error_fallback(condition) do { } while (0)
 #endif
 
+#define __compiletime_assert(condition, msg, prefix, suffix)		\
+	do {								\
+		bool __cond = !(condition);				\
+		extern void prefix ## suffix(void) __compiletime_error(msg); \
+		if (__cond)						\
+			prefix ## suffix();				\
+		__compiletime_error_fallback(__cond);			\
+	} while (0)
+
+#define _compiletime_assert(condition, msg, prefix, suffix) \
+	__compiletime_assert(condition, msg, prefix, suffix)
+
+/**
+ * compiletime_assert - break build and emit msg if condition is false
+ * @condition: a compile-time constant condition to check
+ * @msg:       a message to emit if condition is false
+ *
+ * In tradition of POSIX assert, this macro will break the build if the
+ * supplied condition is *false*, emitting the supplied error message if the
+ * compiler has support to do so.
+ */
+#define compiletime_assert(condition, msg) \
+	_compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit c361d3e54364d19bb5e803d6e766e94674da7b0e
Author: Daniel Santos <daniel.santos@pobox.com>
Date:   Thu Feb 21 16:41:54 2013 -0800

    compiler.h, bug.h: prevent double error messages with BUILD_BUG{,_ON}
    
    Prior to the introduction of __attribute__((error("msg"))) in gcc 4.3,
    creating compile-time errors required a little trickery.
    BUILD_BUG{,_ON} uses this attribute when available to generate
    compile-time errors, but also uses the negative-sized array trick for
    older compilers, resulting in two error messages in some cases.  The
    reason it's "some" cases is that as of gcc 4.4, the negative-sized array
    will not create an error in some situations, like inline functions.
    
    This patch replaces the negative-sized array code with the new
    __compiletime_error_fallback() macro which expands to the same thing
    unless the the error attribute is available, in which case it expands to
    do{}while(0), resulting in exactly one compile-time error on all
    versions of gcc.
    
    Note that we are not changing the negative-sized array code for the
    unoptimized version of BUILD_BUG_ON, since it has the potential to catch
    problems that would be disabled in later versions of gcc were
    __compiletime_error_fallback used.  The reason is that that an
    unoptimized build can't always remove calls to an error-attributed
    function call (like we are using) that should effectively become dead
    code if it were optimized.  However, using a negative-sized array with a
    similar value will not result in an false-positive (error).  The only
    caveat being that it will also fail to catch valid conditions, which we
    should be expecting in an unoptimized build anyway.
    
    Signed-off-by: Daniel Santos <daniel.santos@pobox.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 4c638be76093..423bb6bd660f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -307,7 +307,12 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #endif
 #ifndef __compiletime_error
 # define __compiletime_error(message)
+# define __compiletime_error_fallback(condition) \
+	do { ((void)sizeof(char[1 - 2*!!(condition)])); } while (0)
+#else
+# define __compiletime_error_fallback(condition) do { } while (0)
 #endif
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit 6ae8d04871f84d853673e9e9f3f713e77a2fe145
Author: Daniel Santos <daniel.santos@pobox.com>
Date:   Thu Feb 21 16:41:42 2013 -0800

    compiler{,-gcc4}.h, bug.h: Remove duplicate macros
    
    __linktime_error() does the same thing as __compiletime_error() and is
    only used in bug.h.  Since the macro defines a function attribute that
    will cause a failure at compile-time (not link-time), it makes more sense
    to keep __compiletime_error(), which is also neatly mated with
    __compiletime_warning().
    
    Signed-off-by: Daniel Santos <daniel.santos@pobox.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Borislav Petkov <bp@alien8.de>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index dd852b73b286..4c638be76093 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -308,9 +308,6 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #ifndef __compiletime_error
 # define __compiletime_error(message)
 #endif
-#ifndef __linktime_error
-# define __linktime_error(message)
-#endif
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit 7a684c452e2589f3ddd7e2d466b4f747d3715ad9
Merge: 7f2de8171ddf e10e1774efbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 19 07:55:08 2012 -0800

    Merge tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux
    
    Pull module update from Rusty Russell:
     "Nothing all that exciting; a new module-from-fd syscall for those who
      want to verify the source of the module (ChromeOS) and/or use standard
      IMA on it or other security hooks."
    
    * tag 'modules-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux:
      MODSIGN: Fix kbuild output when using default extra_certificates
      MODSIGN: Avoid using .incbin in C source
      modules: don't hand 0 to vmalloc.
      module: Remove a extra null character at the top of module->strtab.
      ASN.1: Use the ASN1_LONG_TAG and ASN1_INDEFINITE_LENGTH constants
      ASN.1: Define indefinite length marker constant
      moduleparam: use __UNIQUE_ID()
      __UNIQUE_ID()
      MODSIGN: Add modules_sign make target
      powerpc: add finit_module syscall.
      ima: support new kernel module syscall
      add finit_module syscall to asm-generic
      ARM: add finit_module syscall to ARM
      security: introduce kernel_module_from_file hook
      module: add flags arg to sys_finit_module()
      module: add syscall to load module from fd

commit 8529091e8e2ae25e0f4003086f619765ff255e4b
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Mon Dec 17 16:03:24 2012 -0800

    linux/compiler.h: add __must_hold macro for functions called with a lock held
    
    linux/compiler.h has macros to denote functions that acquire or release
    locks, but not to denote functions called with a lock held that return
    with the lock still held.  Add a __must_hold macro to cover that case.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Reported-by: Ed Cashin <ecashin@coraid.com>
    Tested-by: Ed Cashin <ecashin@coraid.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f430e4162f41..b121554f1fe2 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -10,6 +10,7 @@
 # define __force	__attribute__((force))
 # define __nocast	__attribute__((nocast))
 # define __iomem	__attribute__((noderef, address_space(2)))
+# define __must_hold(x)	__attribute__((context(x,1,1)))
 # define __acquires(x)	__attribute__((context(x,0,1)))
 # define __releases(x)	__attribute__((context(x,1,0)))
 # define __acquire(x)	__context__(x,1)
@@ -33,6 +34,7 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __chk_user_ptr(x) (void)0
 # define __chk_io_ptr(x) (void)0
 # define __builtin_warning(x, y...) (1)
+# define __must_hold(x)
 # define __acquires(x)
 # define __releases(x)
 # define __acquire(x) (void)0

commit 6f33d58794ef4cef4b2c706029810f9688bd3026
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Nov 22 12:30:25 2012 +1030

    __UNIQUE_ID()
    
    Jan Beulich points out __COUNTER__ (gcc 4.3 and above), so let's use
    that to create unique ids.  This is better than __LINE__ which we use
    today, so provide a wrapper.
    
    Stanislaw Gruszka <sgruszka@redhat.com> reported that some module parameters
    start with a digit, so we need to prepend when we for the unique id.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Jan Beulich <jbeulich@suse.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f430e4162f41..5f45335e1ac7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -42,6 +42,10 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __rcu
 #endif
 
+/* Indirect macros required for expanded argument pasting, eg. __LINE__. */
+#define ___PASTE(a,b) a##b
+#define __PASTE(a,b) ___PASTE(a,b)
+
 #ifdef __KERNEL__
 
 #ifdef __GNUC__
@@ -164,6 +168,11 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
     (typeof(ptr)) (__ptr + (off)); })
 #endif
 
+/* Not-quite-unique ID. */
+#ifndef __UNIQUE_ID
+# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */

commit 9a858dc7cebce01a7bb616bebb85087fa2b40871
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Sep 17 14:09:15 2012 -0700

    compiler.h: add __visible
    
    gcc 4.6+ has support for a externally_visible attribute that prevents the
    optimizer from optimizing unused symbols away.  Add a __visible macro to
    use it with that compiler version or later.
    
    This is used (at least) by the "Link Time Optimization" patchset.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 923d093c9cea..f430e4162f41 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -278,6 +278,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __section(S) __attribute__ ((__section__(#S)))
 #endif
 
+#ifndef __visible
+#define __visible
+#endif
+
 /* Are two types/vars the same type (ignoring qualifiers)? */
 #ifndef __same_type
 # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))

commit e6be0c9e7e9f68b99e5a5ba609c455bae4b141d5
Author: Alexander Stein <alexander.stein@systec-electronic.com>
Date:   Thu Feb 23 13:42:30 2012 +0100

    compiler.h: Fix typo
    
    Signed-off-by: Alexander Stein <alexander.stein@systec-electronic.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 4a243546d142..923d093c9cea 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -236,7 +236,7 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 
 /*
  * Rather then using noinline to prevent stack consumption, use
- * noinline_for_stack instead.  For documentaiton reasons.
+ * noinline_for_stack instead.  For documentation reasons.
  */
 #define noinline_for_stack noinline
 

commit 1399ff86f2a2bbacbbe68fa00c5f8c752b344723
Author: David Daney <david.daney@cavium.com>
Date:   Tue Jan 10 15:07:25 2012 -0800

    kernel.h: add BUILD_BUG() macro
    
    We can place this in definitions that we expect the compiler to remove by
    dead code elimination.  If this assertion fails, we get a nice error
    message at build time.
    
    The GCC function attribute error("message") was added in version 4.3, so
    we define a new macro __linktime_error(message) to expand to this for
    GCC-4.3 and later.  This will give us an error diagnostic from the
    compiler on the line that fails.  For other compilers
    __linktime_error(message) expands to nothing, and we have to be content
    with a link time error, but at least we will still get a build error.
    
    BUILD_BUG() expands to the undefined function __build_bug_failed() and
    will fail at link time if the compiler ever emits code for it.  On GCC-4.3
    and later, attribute((error())) is used so that the failure will be noted
    at compile time instead.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: DM <dm.n9107@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 320d6c94ff84..4a243546d142 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -293,7 +293,9 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #ifndef __compiletime_error
 # define __compiletime_error(message)
 #endif
-
+#ifndef __linktime_error
+# define __linktime_error(message)
+#endif
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit ca5ecddfa8fcbd948c95530e7e817cee9fb43a3d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 28 14:39:09 2010 -0700

    rcu: define __rcu address space modifier for sparse
    
    This commit provides definitions for the __rcu annotation defined earlier.
    This annotation permits sparse to check for correct use of RCU-protected
    pointers.  If a pointer that is annotated with __rcu is accessed
    directly (as opposed to via rcu_dereference(), rcu_assign_pointer(),
    or one of their variants), sparse can be made to complain.  To enable
    such complaints, use the new default-disabled CONFIG_SPARSE_RCU_POINTER
    kernel configuration option.  Please note that these sparse complaints are
    intended to be a debugging aid, -not- a code-style-enforcement mechanism.
    
    There are special rcu_dereference_protected() and rcu_access_pointer()
    accessors for use when RCU read-side protection is not required, for
    example, when no other CPU has access to the data structure in question
    or while the current CPU hold the update-side lock.
    
    This patch also updates a number of docbook comments that were showing
    their age.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christopher Li <sparse@chrisli.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c1a62c56a660..320d6c94ff84 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -16,7 +16,11 @@
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 # define __percpu	__attribute__((noderef, address_space(3)))
+#ifdef CONFIG_SPARSE_RCU_POINTER
+# define __rcu		__attribute__((noderef, address_space(4)))
+#else
 # define __rcu
+#endif
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
 #else

commit 71d1d5c722db9ae3b3f9c08ef7ddcd7759fbb1e0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 11 16:13:14 2010 -0700

    rcu: add __rcu API for later sparse checking
    
    This commit defines an __rcu API, but provides only vacuous definitions
    for it.  This breaks dependencies among most of the subsequent patches,
    allowing them to reach mainline asynchronously via whatever trees are
    appropriate.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index a5a472b10746..c1a62c56a660 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -16,6 +16,7 @@
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 # define __percpu	__attribute__((noderef, address_space(3)))
+# define __rcu
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
 #else
@@ -34,6 +35,7 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __release(x) (void)0
 # define __cond_lock(x,c) (c)
 # define __percpu
+# define __rcu
 #endif
 
 #ifdef __KERNEL__

commit 32032df6c2f6c9c6b2ada2ce42322231824f70c2
Merge: 22b737f4c751 c5974b835a90
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 5 09:17:33 2010 +0900

    Merge branch 'master' into percpu
    
    Conflicts:
            arch/powerpc/platforms/pseries/hvCall.S
            include/linux/percpu.h

commit ef26b1691d11e17af205a4ff9c91458d931d11db
Merge: a77d2e081bbb 7cff7ce94a7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 15:32:03 2009 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      include/linux/compiler-gcc4.h: Fix build bug - gcc-4.0.2 doesn't understand __builtin_object_size
      x86/alternatives: No need for alternatives-asm.h to re-invent stuff already in asm.h
      x86/alternatives: Check replacementlen <= instrlen at build time
      x86, 64-bit: Set data segments to null after switching to 64-bit mode
      x86: Clean up the loadsegment() macro
      x86: Optimize loadsegment()
      x86: Add missing might_fault() checks to copy_{to,from}_user()
      x86-64: __copy_from_user_inatomic() adjustments
      x86: Remove unused thread_return label from switch_to()
      x86, 64-bit: Fix bstep_iret jump
      x86: Don't use the strict copy checks when branch profiling is in use
      x86, 64-bit: Move K8 B step iret fixup to fault entry asm
      x86: Generate cmpxchg build failures
      x86: Add a Kconfig option to turn the copy_from_user warnings into errors
      x86: Turn the copy_from_user check into an (optional) compile time warning
      x86: Use __builtin_memset and __builtin_memcpy for memset/memcpy
      x86: Use __builtin_object_size() to validate the buffer size for copy_from_user()

commit 96fa2b508d2d3fe040cf4ef2fffb955f0a537ea1
Merge: 7a797cdcca2b b8007ef74222
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 09:53:36 2009 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (40 commits)
      tracing: Separate raw syscall from syscall tracer
      ring-buffer-benchmark: Add parameters to set produce/consumer priorities
      tracing, function tracer: Clean up strstrip() usage
      ring-buffer benchmark: Run producer/consumer threads at nice +19
      tracing: Remove the stale include/trace/power.h
      tracing: Only print objcopy version warning once from recordmcount
      tracing: Prevent build warning: 'ftrace_graph_buf' defined but not used
      ring-buffer: Move access to commit_page up into function used
      tracing: do not disable interrupts for trace_clock_local
      ring-buffer: Add multiple iterations between benchmark timestamps
      kprobes: Sanitize struct kretprobe_instance allocations
      tracing: Fix to use __always_unused attribute
      compiler: Introduce __always_unused
      tracing: Exit with error if a weak function is used in recordmcount.pl
      tracing: Move conditional into update_funcs() in recordmcount.pl
      tracing: Add regex for weak functions in recordmcount.pl
      tracing: Move mcount section search to front of loop in recordmcount.pl
      tracing: Fix objcopy revision check in recordmcount.pl
      tracing: Check absolute path of input file in recordmcount.pl
      tracing: Correct the check for number of arguments in recordmcount.pl
      ...

commit 38938c879eb0c39edf85d5164aa0cffe2874304c
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Fri Dec 4 17:44:50 2009 -0800

    Add support for GCC-4.5's __builtin_unreachable() to compiler.h (v2)
    
    Starting with version 4.5, GCC has a new built-in function
    __builtin_unreachable() that can be used in places like the kernel's
    BUG() where inline assembly is used to transfer control flow.  This
    eliminated the need for an endless loop in these places.
    
    The patch adds a new macro 'unreachable()' that will expand to either
    __builtin_unreachable() or an endless loop depending on the compiler
    version.
    
    Change from v1: Simplify unreachable() for non-GCC 4.5 case.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 04fb5135b4e1..59f208926d13 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -144,6 +144,11 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define barrier() __memory_barrier()
 #endif
 
+/* Unreachable code */
+#ifndef unreachable
+# define unreachable() do { } while (1)
+#endif
+
 #ifndef RELOC_HIDE
 # define RELOC_HIDE(ptr, off)					\
   ({ unsigned long __ptr;					\

commit 7b2a35132ad0a70902dcd2844c27ed64cda0ce9b
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Nov 2 08:50:52 2009 +0800

    compiler: Introduce __always_unused
    
    I wrote some code which is used as compile-time checker, and the
    code should be elided after compile.
    
    So I need to annotate the code as "always unused", compared to
    "maybe unused".
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <4AEE2CEC.8040206@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 04fb5135b4e1..7947f4f6fa51 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -213,6 +213,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __maybe_unused		/* unimplemented */
 #endif
 
+#ifndef __always_unused
+# define __always_unused	/* unimplemented */
+#endif
+
 #ifndef noinline
 #define noinline
 #endif

commit e0fdb0e050eae331046385643618f12452aa7e73
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Oct 29 22:34:15 2009 +0900

    percpu: add __percpu for sparse.
    
    We have to make __kernel "__attribute__((address_space(0)))" so we can
    cast to it.
    
    tj: * put_cpu_var() update.
    
        * Annotations added to dynamic allocator interface.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 04fb5135b4e1..abba8045c6ef 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -5,7 +5,7 @@
 
 #ifdef __CHECKER__
 # define __user		__attribute__((noderef, address_space(1)))
-# define __kernel	/* default address space */
+# define __kernel	__attribute__((address_space(0)))
 # define __safe		__attribute__((safe))
 # define __force	__attribute__((force))
 # define __nocast	__attribute__((nocast))
@@ -15,6 +15,7 @@
 # define __acquire(x)	__context__(x,1)
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
+# define __percpu	__attribute__((noderef, address_space(3)))
 extern void __chk_user_ptr(const volatile void __user *);
 extern void __chk_io_ptr(const volatile void __iomem *);
 #else
@@ -32,6 +33,7 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __acquire(x) (void)0
 # define __release(x) (void)0
 # define __cond_lock(x,c) (c)
+# define __percpu
 #endif
 
 #ifdef __KERNEL__

commit 63312b6a6faae3f2e5577f2b001e3b504f10a2aa
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Fri Oct 2 07:50:50 2009 -0700

    x86: Add a Kconfig option to turn the copy_from_user warnings into errors
    
    For automated testing it is useful to have the option to turn
    the warnings on copy_from_user() etc checks into errors:
    
     In function ‘copy_from_user’,
         inlined from ‘fd_copyin’ at drivers/block/floppy.c:3080,
         inlined from ‘fd_ioctl’ at drivers/block/floppy.c:3503:
       linux/arch/x86/include/asm/uaccess_32.h:213:
      error: call to ‘copy_from_user_overflow’ declared with attribute error:
      copy_from_user buffer size is not provably correct
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20091002075050.4e9f7641@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 950356311f12..88fd4b673cb4 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -273,6 +273,9 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #ifndef __compiletime_warning
 # define __compiletime_warning(message)
 #endif
+#ifndef __compiletime_error
+# define __compiletime_error(message)
+#endif
 
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler

commit 4a3127693001c61a21d1ce680db6340623f52e93
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Wed Sep 30 13:05:23 2009 +0200

    x86: Turn the copy_from_user check into an (optional) compile time warning
    
    A previous patch added the buffer size check to copy_from_user().
    
    One of the things learned from analyzing the result of the previous
    patch is that in general, gcc is really good at proving that the
    code contains sufficient security checks to not need to do a
    runtime check. But that for those cases where gcc could not prove
    this, there was a relatively high percentage of real security
    issues.
    
    This patch turns the case of "gcc cannot prove" into a compile time
    warning, as long as a sufficiently new gcc is in use that supports
    this. The objective is that these warnings will trigger developers
    checking new cases out before a security hole enters a linux kernel
    release.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Cc: Jan Beulich <jbeulich@novell.com>
    LKML-Reference: <20090930130523.348ae6c4@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8e54108688f9..950356311f12 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -270,6 +270,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1
 #endif
+#ifndef __compiletime_warning
+# define __compiletime_warning(message)
+#endif
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit 9f0cf4adb6aa0bfccf675c938124e68f7f06349d
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sat Sep 26 14:33:01 2009 +0200

    x86: Use __builtin_object_size() to validate the buffer size for copy_from_user()
    
    gcc (4.x) supports the __builtin_object_size() builtin, which
    reports the size of an object that a pointer point to, when known
    at compile time. If the buffer size is not known at compile time, a
    constant -1 is returned.
    
    This patch uses this feature to add a sanity check to
    copy_from_user(); if the target buffer is known to be smaller than
    the copy size, the copy is aborted and a WARNing is emitted in
    memory debug mode.
    
    These extra checks compile away when the object size is not known,
    or if both the buffer size and the copy length are constants.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    LKML-Reference: <20090926143301.2c396b94@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 04fb5135b4e1..8e54108688f9 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -266,6 +266,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
 #endif
 
+/* Compile time object size, -1 for unknown */
+#ifndef __compiletime_object_size
+# define __compiletime_object_size(obj) -1
+#endif
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit d2c123c27db841c6c11a63de9c144823d2b1ba76
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Jun 12 21:46:56 2009 -0600

    module_param: add __same_type convenience wrapper for __builtin_types_compatible_p
    
    Impact: new API
    
    __builtin_types_compatible_p() is a little awkward to use: it takes two
    types rather than types or variables, and it's just damn long.
    
    (typeof(type) == type, so this works on types as well as vars).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 37bcb50a4d7c..04fb5135b4e1 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -261,6 +261,11 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # define __section(S) __attribute__ ((__section__(#S)))
 #endif
 
+/* Are two types/vars the same type (ignoring qualifiers)? */
+#ifndef __same_type
+# define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))
+#endif
+
 /*
  * Prevent the compiler from merging or refetching accesses.  The compiler
  * is also forbidden from reordering successive instances of ACCESS_ONCE(),

commit ab3c9c686e22ab264269337ce7b75d9760211198
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 07:59:41 2009 -0700

    branch tracer, intel-iommu: fix build with CONFIG_BRANCH_TRACER=y
    
    Fix the branch tracer barfing on comma statements within if ()
    statements.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8872ad6dd89b..37bcb50a4d7c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -115,7 +115,9 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
  * "Define 'is'", Bill Clinton
  * "Define 'if'", Steven Rostedt
  */
-#define if(cond) if (__builtin_constant_p((cond)) ? !!(cond) :		\
+#define if(cond, ...) __trace_if( (cond , ## __VA_ARGS__) )
+#define __trace_if(cond) \
+	if (__builtin_constant_p((cond)) ? !!(cond) :			\
 	({								\
 		int ______r;						\
 		static struct ftrace_branch_data			\

commit d9ad8bc0ca823705413f75b50c442a88cc518b35
Author: Bart Van Assche <bart.vanassche@gmail.com>
Date:   Sun Apr 5 16:20:02 2009 +0200

    branch tracer: Fix for enabling branch profiling makes sparse unusable
    
    One of the changes between kernels 2.6.28 and 2.6.29 is that a branch profiler
    has been added for if() statements. Unfortunately this patch makes the sparse
    output unusable with CONFIG_TRACE_BRANCH_PROFILING=y: when branch profiling is
    enabled, sparse prints so much false positives that the real issues are no
    longer visible. This behavior can be reproduced as follows:
    * enable CONFIG_TRACE_BRANCH_PROFILING, e.g. by running make allyesconfig or
      make allmodconfig.
    * run make C=2
    
    Result: a huge number of the following sparse warnings.
    ...
    include/linux/cpumask.h:547:2: warning: symbol '______r' shadows an earlier one
    include/linux/cpumask.h:547:2: originally declared here
    ...
    
    The patch below fixes this by disabling branch profiling while analyzing the
    kernel code with sparse.
    
    See also:
    * http://lkml.org/lkml/2008/11/21/18
    * http://bugzilla.kernel.org/show_bug.cgi?id=12925
    
    Signed-off-by: Bart Van Assche <bart.vanassche@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven Rostedt <srostedt@redhat.com>
    LKML-Reference: <200904051620.02311.bart.vanassche@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 6faa7e549de4..8872ad6dd89b 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -76,7 +76,8 @@ struct ftrace_branch_data {
  * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
  * to disable branch tracing on a per file basis.
  */
-#if defined(CONFIG_TRACE_BRANCH_PROFILING) && !defined(DISABLE_BRANCH_PROFILING)
+#if defined(CONFIG_TRACE_BRANCH_PROFILING) \
+    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)
 void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 
 #define likely_notrace(x)	__builtin_expect(!!(x), 1)

commit 97e7e4f391cac2b00417b581b432533d245d4fd0
Author: Witold Baryluk <baryluk@smp.if.uj.edu.pl>
Date:   Tue Mar 17 21:15:44 2009 +0100

    tracing: optimization of branch tracer
    
    Impact: better performance for if branch tracer
    
    Use an array to count the hit and misses of a conditional instead
    of using another conditional. This cuts down on saturation of branch
    predictions and increases performance of modern pipelined architectures.
    
    Signed-off-by: Witold Baryluk <baryluk@smp.if.uj.edu.pl>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d95da1020f1c..6faa7e549de4 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -68,6 +68,7 @@ struct ftrace_branch_data {
 			unsigned long miss;
 			unsigned long hit;
 		};
+		unsigned long miss_hit[2];
 	};
 };
 
@@ -125,10 +126,7 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 				.line = __LINE__,			\
 			};						\
 		______r = !!(cond);					\
-		if (______r)						\
-			______f.hit++;					\
-		else							\
-			______f.miss++;					\
+		______f.miss_hit[______r]++;					\
 		______r;						\
 	}))
 #endif /* CONFIG_PROFILE_ALL_BRANCHES */

commit f153b82121b0366fe0e5f9553545cce237335175
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 09:23:03 2009 -0800

    Sanitize gcc version header includes
    
     - include the gcc version-dependent header files from the generic gcc
       header file, rather than the other way around (iow: don't make the
       non-gcc header file have to know about gcc versions)
    
     - don't include compiler-gcc4.h for gcc 5 (for whenever it gets
       released).  That's just confusing and made us do odd things in the
       gcc4 header file (testing that we really had version 4!)
    
     - generate the name from the __GNUC__ version directly, rather than
       having a mess of #if conditionals.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ea7c6be354b7..d95da1020f1c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -36,12 +36,8 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 
 #ifdef __KERNEL__
 
-#if __GNUC__ >= 4
-# include <linux/compiler-gcc4.h>
-#elif __GNUC__ == 3 && __GNUC_MINOR__ >= 2
-# include <linux/compiler-gcc3.h>
-#else
-# error Sorry, your compiler is too old/not recognized.
+#ifdef __GNUC__
+#include <linux/compiler-gcc.h>
 #endif
 
 #define notrace __attribute__((no_instrument_function))

commit 2bcd521a684cc94befbe2ce7d5b613c841b0d304
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 21 01:30:54 2008 -0500

    trace: profile all if conditionals
    
    Impact: feature to profile if statements
    
    This patch adds a branch profiler for all if () statements.
    The results will be found in:
    
      /debugfs/tracing/profile_branch
    
    For example:
    
       miss      hit    %        Function                  File              Line
     ------- ---------  -        --------                  ----              ----
           0        1 100 x86_64_start_reservations      head64.c             127
           0        1 100 copy_bootdata                  head64.c             69
           1        0   0 x86_64_start_kernel            head64.c             111
          32        0   0 set_intr_gate                  desc.h               319
           1        0   0 reserve_ebda_region            head.c               51
           1        0   0 reserve_ebda_region            head.c               47
           0        1 100 reserve_ebda_region            head.c               42
           0        0   X maxcpus                        main.c               165
    
    Miss means the branch was not taken. Hit means the branch was taken.
    The percent is the percentage the branch was taken.
    
    This adds a significant amount of overhead and should only be used
    by those analyzing their system.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 0628a2013fae..ea7c6be354b7 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -63,8 +63,16 @@ struct ftrace_branch_data {
 	const char *func;
 	const char *file;
 	unsigned line;
-	unsigned long correct;
-	unsigned long incorrect;
+	union {
+		struct {
+			unsigned long correct;
+			unsigned long incorrect;
+		};
+		struct {
+			unsigned long miss;
+			unsigned long hit;
+		};
+	};
 };
 
 /*
@@ -103,6 +111,32 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 # ifndef unlikely
 #  define unlikely(x)	(__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 0))
 # endif
+
+#ifdef CONFIG_PROFILE_ALL_BRANCHES
+/*
+ * "Define 'is'", Bill Clinton
+ * "Define 'if'", Steven Rostedt
+ */
+#define if(cond) if (__builtin_constant_p((cond)) ? !!(cond) :		\
+	({								\
+		int ______r;						\
+		static struct ftrace_branch_data			\
+			__attribute__((__aligned__(4)))			\
+			__attribute__((section("_ftrace_branch")))	\
+			______f = {					\
+				.func = __func__,			\
+				.file = __FILE__,			\
+				.line = __LINE__,			\
+			};						\
+		______r = !!(cond);					\
+		if (______r)						\
+			______f.hit++;					\
+		else							\
+			______f.miss++;					\
+		______r;						\
+	}))
+#endif /* CONFIG_PROFILE_ALL_BRANCHES */
+
 #else
 # define likely(x)	__builtin_expect(!!(x), 1)
 # define unlikely(x)	__builtin_expect(!!(x), 0)

commit 45b797492a0758e64dff74e9db70e1f65e0603a5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 21 00:40:40 2008 -0500

    trace: consolidate unlikely and likely profiler
    
    Impact: clean up to make one profiler of like and unlikely tracer
    
    The likely and unlikely profiler prints out the file and line numbers
    of the annotated branches that it is profiling. It shows the number
    of times it was correct or incorrect in its guess. Having two
    different files or sections for that matter to tell us if it was a
    likely or unlikely is pretty pointless. We really only care if
    it was correct or not.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c25e525121f0..0628a2013fae 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -77,32 +77,18 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 #define likely_notrace(x)	__builtin_expect(!!(x), 1)
 #define unlikely_notrace(x)	__builtin_expect(!!(x), 0)
 
-#define likely_check(x) ({						\
+#define __branch_check__(x, expect) ({					\
 			int ______r;					\
 			static struct ftrace_branch_data		\
 				__attribute__((__aligned__(4)))		\
-				__attribute__((section("_ftrace_likely"))) \
+				__attribute__((section("_ftrace_annotated_branch"))) \
 				______f = {				\
 				.func = __func__,			\
 				.file = __FILE__,			\
 				.line = __LINE__,			\
 			};						\
 			______r = likely_notrace(x);			\
-			ftrace_likely_update(&______f, ______r, 1);	\
-			______r;					\
-		})
-#define unlikely_check(x) ({						\
-			int ______r;					\
-			static struct ftrace_branch_data		\
-				__attribute__((__aligned__(4)))		\
-				__attribute__((section("_ftrace_unlikely"))) \
-				______f = {				\
-				.func = __func__,			\
-				.file = __FILE__,			\
-				.line = __LINE__,			\
-			};						\
-			______r = unlikely_notrace(x);			\
-			ftrace_likely_update(&______f, ______r, 0);	\
+			ftrace_likely_update(&______f, ______r, expect); \
 			______r;					\
 		})
 
@@ -112,10 +98,10 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
  * written by Daniel Walker.
  */
 # ifndef likely
-#  define likely(x)	(__builtin_constant_p(x) ? !!(x) : likely_check(x))
+#  define likely(x)	(__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 1))
 # endif
 # ifndef unlikely
-#  define unlikely(x)	(__builtin_constant_p(x) ? !!(x) : unlikely_check(x))
+#  define unlikely(x)	(__builtin_constant_p(x) ? !!(x) : __branch_check__(x, 0))
 # endif
 #else
 # define likely(x)	__builtin_expect(!!(x), 1)

commit 42f565e116e0408b5ddc21a33c4a4d41fd572420
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 20 23:57:47 2008 -0500

    trace: remove extra assign in branch check
    
    Impact: clean up of branch check
    
    The unlikely/likely profiler does an extra assign of the f.line.
    This is not needed since it is already calculated at compile time.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c7d804a7a4d6..c25e525121f0 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -87,7 +87,6 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 				.file = __FILE__,			\
 				.line = __LINE__,			\
 			};						\
-			______f.line = __LINE__;			\
 			______r = likely_notrace(x);			\
 			ftrace_likely_update(&______f, ______r, 1);	\
 			______r;					\
@@ -102,7 +101,6 @@ void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 				.file = __FILE__,			\
 				.line = __LINE__,			\
 			};						\
-			______f.line = __LINE__;			\
 			______r = unlikely_notrace(x);			\
 			ftrace_likely_update(&______f, ______r, 0);	\
 			______r;					\

commit 2ed84eeb8808cf3c9f039213ca137ffd7d753f0e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 12 15:24:24 2008 -0500

    trace: rename unlikely profiler to branch profiler
    
    Impact: name change of unlikely tracer and profiler
    
    Ingo Molnar suggested changing the config from UNLIKELY_PROFILE
    to BRANCH_PROFILING. I never did like the "unlikely" name so I
    went one step farther, and renamed all the unlikely configurations
    to a "BRANCH" variant.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 63b7d9089d6e..c7d804a7a4d6 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -59,26 +59,27 @@ extern void __chk_io_ptr(const volatile void __iomem *);
  * specific implementations come from the above header files
  */
 
-/*
- * Note: DISABLE_UNLIKELY_PROFILE can be used by special lowlevel code
- * to disable branch tracing on a per file basis.
- */
-#if defined(CONFIG_TRACE_UNLIKELY_PROFILE) && !defined(DISABLE_UNLIKELY_PROFILE)
-struct ftrace_likely_data {
+struct ftrace_branch_data {
 	const char *func;
 	const char *file;
 	unsigned line;
 	unsigned long correct;
 	unsigned long incorrect;
 };
-void ftrace_likely_update(struct ftrace_likely_data *f, int val, int expect);
+
+/*
+ * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
+ * to disable branch tracing on a per file basis.
+ */
+#if defined(CONFIG_TRACE_BRANCH_PROFILING) && !defined(DISABLE_BRANCH_PROFILING)
+void ftrace_likely_update(struct ftrace_branch_data *f, int val, int expect);
 
 #define likely_notrace(x)	__builtin_expect(!!(x), 1)
 #define unlikely_notrace(x)	__builtin_expect(!!(x), 0)
 
 #define likely_check(x) ({						\
 			int ______r;					\
-			static struct ftrace_likely_data		\
+			static struct ftrace_branch_data		\
 				__attribute__((__aligned__(4)))		\
 				__attribute__((section("_ftrace_likely"))) \
 				______f = {				\
@@ -93,7 +94,7 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val, int expect);
 		})
 #define unlikely_check(x) ({						\
 			int ______r;					\
-			static struct ftrace_likely_data		\
+			static struct ftrace_branch_data		\
 				__attribute__((__aligned__(4)))		\
 				__attribute__((section("_ftrace_unlikely"))) \
 				______f = {				\

commit 2b7d0390a6d6d595f43ea3806639664afe5b9ebe
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 12 13:17:38 2008 +0100

    tracing: branch tracer, fix vdso crash
    
    Impact: fix bootup crash
    
    the branch tracer missed arch/x86/vdso/vclock_gettime.c from
    disabling tracing, which caused such bootup crashes:
    
      [  201.840097] init[1]: segfault at 7fffed3fe7c0 ip 00007fffed3fea2e sp 000077
    
    also clean up the ugly ifdefs in arch/x86/kernel/vsyscall_64.c by
    creating DISABLE_UNLIKELY_PROFILE facility for code to turn off
    instrumentation on a per file basis.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 935e30cfaf3c..63b7d9089d6e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -59,7 +59,11 @@ extern void __chk_io_ptr(const volatile void __iomem *);
  * specific implementations come from the above header files
  */
 
-#ifdef CONFIG_TRACE_UNLIKELY_PROFILE
+/*
+ * Note: DISABLE_UNLIKELY_PROFILE can be used by special lowlevel code
+ * to disable branch tracing on a per file basis.
+ */
+#if defined(CONFIG_TRACE_UNLIKELY_PROFILE) && !defined(DISABLE_UNLIKELY_PROFILE)
 struct ftrace_likely_data {
 	const char *func;
 	const char *file;

commit 1f0d69a9fc815db82f15722bf05227190b1d714d
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 12 00:14:39 2008 -0500

    tracing: profile likely and unlikely annotations
    
    Impact: new unlikely/likely profiler
    
    Andrew Morton recently suggested having an in-kernel way to profile
    likely and unlikely macros. This patch achieves that goal.
    
    When configured, every(*) likely and unlikely macro gets a counter attached
    to it. When the condition is hit, the hit and misses of that condition
    are recorded. These numbers can later be retrieved by:
    
      /debugfs/tracing/profile_likely    - All likely markers
      /debugfs/tracing/profile_unlikely  - All unlikely markers.
    
    # cat /debug/tracing/profile_unlikely | head
     correct incorrect  %        Function                  File              Line
     ------- ---------  -        --------                  ----              ----
        2167        0   0 do_arch_prctl                  process_64.c         832
           0        0   0 do_arch_prctl                  process_64.c         804
        2670        0   0 IS_ERR                         err.h                34
       71230     5693   7 __switch_to                    process_64.c         673
       76919        0   0 __switch_to                    process_64.c         639
       43184    33743  43 __switch_to                    process_64.c         624
       12740    64181  83 __switch_to                    process_64.c         594
       12740    64174  83 __switch_to                    process_64.c         590
    
    # cat /debug/tracing/profile_unlikely | \
      awk '{ if ($3 > 25) print $0; }' |head -20
       44963    35259  43 __switch_to                    process_64.c         624
       12762    67454  84 __switch_to                    process_64.c         594
       12762    67447  84 __switch_to                    process_64.c         590
        1478      595  28 syscall_get_error              syscall.h            51
           0     2821 100 syscall_trace_leave            ptrace.c             1567
           0        1 100 native_smp_prepare_cpus        smpboot.c            1237
       86338   265881  75 calc_delta_fair                sched_fair.c         408
      210410   108540  34 calc_delta_mine                sched.c              1267
           0    54550 100 sched_info_queued              sched_stats.h        222
       51899    66435  56 pick_next_task_fair            sched_fair.c         1422
           6       10  62 yield_task_fair                sched_fair.c         982
        7325     2692  26 rt_policy                      sched.c              144
           0     1270 100 pre_schedule_rt                sched_rt.c           1261
        1268    48073  97 pick_next_task_rt              sched_rt.c           884
           0    45181 100 sched_info_dequeued            sched_stats.h        177
           0       15 100 sched_move_task                sched.c              8700
           0       15 100 sched_move_task                sched.c              8690
       53167    33217  38 schedule                       sched.c              4457
           0    80208 100 sched_info_switch              sched_stats.h        270
       30585    49631  61 context_switch                 sched.c              2619
    
    # cat /debug/tracing/profile_likely | awk '{ if ($3 > 25) print $0; }'
       39900    36577  47 pick_next_task                 sched.c              4397
       20824    15233  42 switch_mm                      mmu_context_64.h     18
           0        7 100 __cancel_work_timer            workqueue.c          560
         617    66484  99 clocksource_adjust             timekeeping.c        456
           0   346340 100 audit_syscall_exit             auditsc.c            1570
          38   347350  99 audit_get_context              auditsc.c            732
           0   345244 100 audit_syscall_entry            auditsc.c            1541
          38     1017  96 audit_free                     auditsc.c            1446
           0     1090 100 audit_alloc                    auditsc.c            862
        2618     1090  29 audit_alloc                    auditsc.c            858
           0        6 100 move_masked_irq                migration.c          9
           1      198  99 probe_sched_wakeup             trace_sched_switch.c 58
           2        2  50 probe_wakeup                   trace_sched_wakeup.c 227
           0        2 100 probe_wakeup_sched_switch      trace_sched_wakeup.c 144
        4514     2090  31 __grab_cache_page              filemap.c            2149
       12882   228786  94 mapping_unevictable            pagemap.h            50
           4       11  73 __flush_cpu_slab               slub.c               1466
      627757   330451  34 slab_free                      slub.c               1731
        2959    61245  95 dentry_lru_del_init            dcache.c             153
         946     1217  56 load_elf_binary                binfmt_elf.c         904
         102       82  44 disk_put_part                  genhd.h              206
           1        1  50 dst_gc_task                    dst.c                82
           0       19 100 tcp_mss_split_point            tcp_output.c         1126
    
    As you can see by the above, there's a bit of work to do in rethinking
    the use of some unlikelys and likelys. Note: the unlikely case had 71 hits
    that were more than 25%.
    
    Note:  After submitting my first version of this patch, Andrew Morton
      showed me a version written by Daniel Walker, where I picked up
      the following ideas from:
    
      1)  Using __builtin_constant_p to avoid profiling fixed values.
      2)  Using __FILE__ instead of instruction pointers.
      3)  Using the preprocessor to stop all profiling of likely
           annotations from vsyscall_64.c.
    
    Thanks to Andrew Morton, Arjan van de Ven, Theodore Tso and Ingo Molnar
    for their feed back on this patch.
    
    (*) Not ever unlikely is recorded, those that are used by vsyscalls
     (a few of them) had to have profiling disabled.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Theodore Tso <tytso@mit.edu>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 98115d9d04da..935e30cfaf3c 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -59,8 +59,65 @@ extern void __chk_io_ptr(const volatile void __iomem *);
  * specific implementations come from the above header files
  */
 
-#define likely(x)	__builtin_expect(!!(x), 1)
-#define unlikely(x)	__builtin_expect(!!(x), 0)
+#ifdef CONFIG_TRACE_UNLIKELY_PROFILE
+struct ftrace_likely_data {
+	const char *func;
+	const char *file;
+	unsigned line;
+	unsigned long correct;
+	unsigned long incorrect;
+};
+void ftrace_likely_update(struct ftrace_likely_data *f, int val, int expect);
+
+#define likely_notrace(x)	__builtin_expect(!!(x), 1)
+#define unlikely_notrace(x)	__builtin_expect(!!(x), 0)
+
+#define likely_check(x) ({						\
+			int ______r;					\
+			static struct ftrace_likely_data		\
+				__attribute__((__aligned__(4)))		\
+				__attribute__((section("_ftrace_likely"))) \
+				______f = {				\
+				.func = __func__,			\
+				.file = __FILE__,			\
+				.line = __LINE__,			\
+			};						\
+			______f.line = __LINE__;			\
+			______r = likely_notrace(x);			\
+			ftrace_likely_update(&______f, ______r, 1);	\
+			______r;					\
+		})
+#define unlikely_check(x) ({						\
+			int ______r;					\
+			static struct ftrace_likely_data		\
+				__attribute__((__aligned__(4)))		\
+				__attribute__((section("_ftrace_unlikely"))) \
+				______f = {				\
+				.func = __func__,			\
+				.file = __FILE__,			\
+				.line = __LINE__,			\
+			};						\
+			______f.line = __LINE__;			\
+			______r = unlikely_notrace(x);			\
+			ftrace_likely_update(&______f, ______r, 0);	\
+			______r;					\
+		})
+
+/*
+ * Using __builtin_constant_p(x) to ignore cases where the return
+ * value is always the same.  This idea is taken from a similar patch
+ * written by Daniel Walker.
+ */
+# ifndef likely
+#  define likely(x)	(__builtin_constant_p(x) ? !!(x) : likely_check(x))
+# endif
+# ifndef unlikely
+#  define unlikely(x)	(__builtin_constant_p(x) ? !!(x) : unlikely_check(x))
+# endif
+#else
+# define likely(x)	__builtin_expect(!!(x), 1)
+# define unlikely(x)	__builtin_expect(!!(x), 0)
+#endif
 
 /* Optimization barrier */
 #ifndef barrier

commit 28614889bcb2558a47d02d52394b7fd9795a9547
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Aug 14 22:47:18 2008 -0400

    ftrace: move notrace to compiler.h
    
    The notrace define belongs in compiler.h so that it can be used in
    init.h
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8322141ee480..98115d9d04da 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -44,6 +44,8 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # error Sorry, your compiler is too old/not recognized.
 #endif
 
+#define notrace __attribute__((no_instrument_function))
+
 /* Intel compiler defines __GNUC__. So we will overwrite implementations
  * coming from above header files here
  */

commit ded00a56e99555c3f4000ef3eebfd5fe0d574565
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Aug 17 12:50:36 2008 -0700

    rcu: remove redundant ACCESS_ONCE definition from rcupreempt.c
    
    Remove the redundant definition of ACCESS_ONCE() from rcupreempt.c in
    favor of the one in compiler.h.  Also merge the comment header from
    rcupreempt.c's definition into that in compiler.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c8bd2daf95ec..8322141ee480 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -190,7 +190,9 @@ extern void __chk_io_ptr(const volatile void __iomem *);
  * ACCESS_ONCE() in different C statements.
  *
  * This macro does absolutely -nothing- to prevent the CPU from reordering,
- * merging, or refetching absolutely anything at any time.
+ * merging, or refetching absolutely anything at any time.  Its main intended
+ * use is to mediate communication between process-level code and irq/NMI
+ * handlers, all running on the same CPU.
  */
 #define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
 

commit 9c3cdc1f83a6e07092392ff4aba6466517dbd1d0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 10 19:51:16 2008 -0700

    Move ACCESS_ONCE() to <linux/compiler.h>
    
    It actually makes much more sense there, and we do tend to need it for
    non-RCU usage too.  Moving it to <linux/compiler.h> will allow some
    other cases that have open-coded the same logic to use the same helper
    function that RCU has used.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index dcae0c8d97e6..c8bd2daf95ec 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -182,4 +182,16 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __section(S) __attribute__ ((__section__(#S)))
 #endif
 
+/*
+ * Prevent the compiler from merging or refetching accesses.  The compiler
+ * is also forbidden from reordering successive instances of ACCESS_ONCE(),
+ * but only when the compiler is aware of some particular ordering.  One way
+ * to make the compiler aware of ordering is to put the two invocations of
+ * ACCESS_ONCE() in different C statements.
+ *
+ * This macro does absolutely -nothing- to prevent the CPU from reordering,
+ * merging, or refetching absolutely anything at any time.
+ */
+#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
+
 #endif /* __LINUX_COMPILER_H */

commit 735c4fb916e9f83a9350aeb2680d77d01ea75094
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Mar 4 14:28:40 2008 -0800

    add noinline_for_stack
    
    People are adding `noinline' in various places to prevent excess stack
    consumption due to gcc inlining.  But once this is done, it is quite unobvious
    why the `noinline' is present in the code.  We can comment each and every
    site, or we can use noinline_for_stack.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d0e17e1657dc..dcae0c8d97e6 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -138,6 +138,12 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 #define noinline
 #endif
 
+/*
+ * Rather then using noinline to prevent stack consumption, use
+ * noinline_for_stack instead.  For documentaiton reasons.
+ */
+#define noinline_for_stack noinline
+
 #ifndef __always_inline
 #define __always_inline inline
 #endif

commit 3ff6eecca4e5c49a5d1dd8b58ea0e20102ce08f0
Author: Adrian Bunk <bunk@kernel.org>
Date:   Thu Jan 24 22:16:20 2008 +0100

    remove __attribute_used__
    
    Remove the deprecated __attribute_used__.
    
    [Introduce __section in a few places to silence checkpatch /sam]
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e0114a61268f..d0e17e1657dc 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -126,10 +126,6 @@ extern void __chk_io_ptr(const volatile void __iomem *);
  * Mark functions that are referenced only in inline assembly as __used so
  * the code is emitted even though it appears to be unreferenced.
  */
-#ifndef __attribute_used__
-# define __attribute_used__	/* deprecated */
-#endif
-
 #ifndef __used
 # define __used			/* unimplemented */
 #endif

commit f3fe866d59d707c7a2bba0b23add078e19edb3dc
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jan 20 18:54:48 2008 +0100

    compiler.h: introduce __section()
    
    Add a new helper: __section() that makes a section definition
    much shorter and more readable.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c68b67b86ef1..e0114a61268f 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -175,4 +175,9 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 #define __cold
 #endif
 
+/* Simple shorthand for a section definition */
+#ifndef __section
+# define __section(S) __attribute__ ((__section__(#S)))
+#endif
+
 #endif /* __LINUX_COMPILER_H */

commit de48844398f81cfdf087d56e12c920d620dae8d5
Author: Jeff Garzik <jeff@garzik.org>
Date:   Thu Oct 25 04:06:13 2007 -0400

    Permit silencing of __deprecated warnings.
    
    The __deprecated marker is quite useful in highlighting the remnants of
    old APIs that want removing.
    
    However, it is quite normal for one or more years to pass, before the
    (usually ancient, bitrotten) code in question is either updated or
    deleted.
    
    Thus, like __must_check, add a Kconfig option that permits the silencing
    of this compiler warning.
    
    This change mimics the ifdef-ery and Kconfig defaults of MUST_CHECK as
    closely as possible.
    
    Signed-off-by: Jeff Garzik <jgarzik@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index c811c8b979ac..c68b67b86ef1 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -101,6 +101,12 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 #undef __must_check
 #define __must_check
 #endif
+#ifndef CONFIG_ENABLE_WARN_DEPRECATED
+#undef __deprecated
+#undef __deprecated_for_modules
+#define __deprecated
+#define __deprecated_for_modules
+#endif
 
 /*
  * Allow us to avoid 'defined but not used' warnings on functions and data,

commit e8c44319c691dfb4a0b039b095204c040df9b01a
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Oct 18 03:07:07 2007 -0700

    Replace __attribute_pure__ with __pure
    
    To be consistent with the use of attributes in the rest of the kernel
    replace all use of __attribute_pure__ with __pure and delete the definition
    of __attribute_pure__.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Acked-by: Mauro Carvalho Chehab <mchehab@infradead.org>
    Cc: Bryan Wu <bryan.wu@analog.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 86f9a3a6137d..c811c8b979ac 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -132,20 +132,6 @@ extern void __chk_io_ptr(const volatile void __iomem *);
 # define __maybe_unused		/* unimplemented */
 #endif
 
-/*
- * From the GCC manual:
- *
- * Many functions have no effects except the return value and their
- * return value depends only on the parameters and/or global
- * variables.  Such a function can be subject to common subexpression
- * elimination and loop optimization just as an arithmetic operator
- * would be.
- * [...]
- */
-#ifndef __attribute_pure__
-# define __attribute_pure__	/* unimplemented */
-#endif
-
 #ifndef noinline
 #define noinline
 #endif

commit c47ffe3d3d841986108a8316f6e01792cb45d0d2
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Jul 26 17:35:29 2007 +0100

    make __chk_{user,io}_ptr() accept pointers to volatile
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 12a1291855e2..86f9a3a6137d 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -15,8 +15,8 @@
 # define __acquire(x)	__context__(x,1)
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
-extern void __chk_user_ptr(const void __user *);
-extern void __chk_io_ptr(const void __iomem *);
+extern void __chk_user_ptr(const volatile void __user *);
+extern void __chk_io_ptr(const volatile void __iomem *);
 #else
 # define __user
 # define __kernel

commit a586df067afe0580bb02b7a6312ca2afe49bba03
Author: Andi Kleen <ak@suse.de>
Date:   Sat Jul 21 17:10:00 2007 +0200

    x86: Support __attribute__((__cold__)) in gcc 4.3
    
    gcc 4.3 supports a new __attribute__((__cold__)) to mark functions cold. Any
    path directly leading to a call of this function will be unlikely. And gcc
    will try to generate smaller code for the function itself.
    
    Please use with care. The code generation advantage isn't large and in most
    cases it is not worth uglifying code with this.
    
    This patch marks some common error functions like panic(), printk()
    as cold.  This will longer term make many unlikely()s unnecessary, although
    we can keep them for now for older compilers.
    
    BUG is not marked cold because there is currently no way to tell
    gcc to mark a inline function told.
    
    Also all __init and __exit functions are marked cold. With a non -Os
    build this will tell the compiler to generate slightly smaller code
    for them. I think it currently only uses less alignments for labels,
    but that might change in the future.
    
    One disadvantage over *likely() is that they cannot be easily instrumented
    to verify them.
    
    Another drawback is that only the latest gcc 4.3 snapshots support this.
    Unfortunately we cannot detect this using the preprocessor. This means older
    snapshots will fail now. I don't think that's a problem because they are
    unreleased compilers that nobody should be using.
    
    gcc also has a __hot__ attribute, but I don't see any sense in using
    this in the kernel right now. But someday I hope gcc will be able
    to use more aggressive optimizing for hot functions even in -Os,
    if that happens it should be added.
    
    Includes compile fix from Thomas Gleixner.
    
    Cc: Jan Hubicka <jh@suse.cz>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8287a72bb6a9..12a1291855e2 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -174,4 +174,13 @@ extern void __chk_io_ptr(const void __iomem *);
 # define __attribute_const__	/* unimplemented */
 #endif
 
+/*
+ * Tell gcc if a function is cold. The compiler will assume any path
+ * directly leading to the call is unlikely.
+ */
+
+#ifndef __cold
+#define __cold
+#endif
+
 #endif /* __LINUX_COMPILER_H */

commit 21124a82bb82e100369846ff2044dd5ea65fc934
Author: Andi Kleen <ak@suse.de>
Date:   Mon May 21 14:31:46 2007 +0200

    x86_64: Support gcc 5 properly
    
    The ifdef tests were broken.  Assume it acts like gcc 4
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 498c35920762..8287a72bb6a9 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -36,9 +36,7 @@ extern void __chk_io_ptr(const void __iomem *);
 
 #ifdef __KERNEL__
 
-#if __GNUC__ > 4
-#error no compiler-gcc.h file for this gcc version
-#elif __GNUC__ == 4
+#if __GNUC__ >= 4
 # include <linux/compiler-gcc4.h>
 #elif __GNUC__ == 3 && __GNUC_MINOR__ >= 2
 # include <linux/compiler-gcc3.h>

commit 0d7ebbbc6eaa5539f78ab20ed6ff1725a4e332ef
Author: David Rientjes <rientjes@google.com>
Date:   Wed May 9 02:35:27 2007 -0700

    compiler: introduce __used and __maybe_unused
    
    __used is defined to be __attribute__((unused)) for all pre-3.3 gcc
    compilers to suppress warnings for unused functions because perhaps they
    are referenced only in inline assembly.  It is defined to be
    __attribute__((used)) for gcc 3.3 and later so that the code is still
    emitted for such functions.
    
    __maybe_unused is defined to be __attribute__((unused)) for both function
    and variable use if it could possibly be unreferenced due to the evaluation
    of preprocessor macros.  Function prototypes shall be marked with
    __maybe_unused if the actual definition of the function is dependant on
    preprocessor macros.
    
    No update to compiler-intel.h is necessary because ICC supports both
    __attribute__((used)) and __attribute__((unused)) as specified by the gcc
    manual.
    
    __attribute_used__ is deprecated and will be removed once all current
    code is converted to using __used.
    
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 3b6949b41745..498c35920762 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -108,15 +108,30 @@ extern void __chk_io_ptr(const void __iomem *);
  * Allow us to avoid 'defined but not used' warnings on functions and data,
  * as well as force them to be emitted to the assembly file.
  *
- * As of gcc 3.3, static functions that are not marked with attribute((used))
- * may be elided from the assembly file.  As of gcc 3.3, static data not so
+ * As of gcc 3.4, static functions that are not marked with attribute((used))
+ * may be elided from the assembly file.  As of gcc 3.4, static data not so
  * marked will not be elided, but this may change in a future gcc version.
  *
+ * NOTE: Because distributions shipped with a backported unit-at-a-time
+ * compiler in gcc 3.3, we must define __used to be __attribute__((used))
+ * for gcc >=3.3 instead of 3.4.
+ *
  * In prior versions of gcc, such functions and data would be emitted, but
  * would be warned about except with attribute((unused)).
+ *
+ * Mark functions that are referenced only in inline assembly as __used so
+ * the code is emitted even though it appears to be unreferenced.
  */
 #ifndef __attribute_used__
-# define __attribute_used__	/* unimplemented */
+# define __attribute_used__	/* deprecated */
+#endif
+
+#ifndef __used
+# define __used			/* unimplemented */
+#endif
+
+#ifndef __maybe_unused
+# define __maybe_unused		/* unimplemented */
 #endif
 
 /*

commit 04a395233089ed160ef87a6c2155e5dedc6f7d15
Author: Russ Cox <rsc@swtch.com>
Date:   Mon Mar 26 11:23:56 2007 -0400

    [PATCH] Add const to pointer qualifiers for __chk_user_ptr and __chk_io_ptr.
    
    Change prototypes for __chk_user_ptr and __chk_io_ptr to take const
    void* instead of void*, so that code can pass "const void *" to them.
    
    (Right now sparse does not warn about passing const void* to void*
    functions, but that is a separate bug that I believe Josh is working on,
    and once sparse does check this, the changed prototypes will be
    necessary.)
    
    Signed-off-by: Russ Cox <rsc@swtch.com>
    Signed-off-by: Josh Triplett <josh@freedesktop.org>
    Acked-by: Christopher Li <sparse@chrisli.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index aca66984aafd..3b6949b41745 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -15,8 +15,8 @@
 # define __acquire(x)	__context__(x,1)
 # define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
-extern void __chk_user_ptr(void __user *);
-extern void __chk_io_ptr(void __iomem *);
+extern void __chk_user_ptr(const void __user *);
+extern void __chk_io_ptr(const void __iomem *);
 #else
 # define __user
 # define __kernel

commit 53569ab7851fd564427f7529b17162cba9a28407
Author: Alistair John Strachan <s0348365@sms.ed.ac.uk>
Date:   Tue Dec 12 19:28:50 2006 +0100

    include/linux/compiler.h: reject gcc 3 < gcc 3.2
    
    The kernel doesn't compile with GCC <3.2, do not allow it to succeed if GCC
    3.0.x or 3.1.x are used.
    
    Signed-off-by: Alistair John Strachan <s0348365@sms.ed.ac.uk>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 538423d4a865..aca66984aafd 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -40,7 +40,7 @@ extern void __chk_io_ptr(void __iomem *);
 #error no compiler-gcc.h file for this gcc version
 #elif __GNUC__ == 4
 # include <linux/compiler-gcc4.h>
-#elif __GNUC__ == 3
+#elif __GNUC__ == 3 && __GNUC_MINOR__ >= 2
 # include <linux/compiler-gcc3.h>
 #else
 # error Sorry, your compiler is too old/not recognized.

commit c902e0a0102f1095eec4b3511c13c84ca2bc4577
Author: Josh Triplett <josht@us.ibm.com>
Date:   Sat Sep 30 23:28:21 2006 -0700

    [PATCH] Pass sparse the lock expression given to lock annotations
    
    The lock annotation macros __acquires, __releases, __acquire, and __release
    all currently throw away the lock expression passed as an argument.  Now
    that sparse can parse __context__ and __attribute__((context)) with a
    context expression, pass the lock expression down to sparse as the context
    expression.  This requires a version of sparse from GIT commit
    37475a6c1c3e66219e68d912d5eb833f4098fd72 or later.
    
    Signed-off-by: Josh Triplett <josh@freedesktop.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 0780de440220..538423d4a865 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -10,10 +10,10 @@
 # define __force	__attribute__((force))
 # define __nocast	__attribute__((nocast))
 # define __iomem	__attribute__((noderef, address_space(2)))
-# define __acquires(x)	__attribute__((context(0,1)))
-# define __releases(x)	__attribute__((context(1,0)))
-# define __acquire(x)	__context__(1)
-# define __release(x)	__context__(-1)
+# define __acquires(x)	__attribute__((context(x,0,1)))
+# define __releases(x)	__attribute__((context(x,1,0)))
+# define __acquire(x)	__context__(x,1)
+# define __release(x)	__context__(x,-1)
 # define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 extern void __chk_user_ptr(void __user *);
 extern void __chk_io_ptr(void __iomem *);

commit dcc8e559ee5ae03fa6bdb8611d76d86d0083e793
Author: Josh Triplett <josht@us.ibm.com>
Date:   Fri Sep 29 02:01:03 2006 -0700

    [PATCH] Pass a lock expression to __cond_lock, like __acquire and __release
    
    Currently, __acquire and __release take a lock expression, but __cond_lock
    takes only a condition, not the lock acquired if the expression evaluates
    to true.  Change __cond_lock to accept a lock expression, and change all
    the callers to pass in a lock expression.
    
    Signed-off-by: Josh Triplett <josh@freedesktop.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 060b96112ec6..0780de440220 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -14,7 +14,7 @@
 # define __releases(x)	__attribute__((context(1,0)))
 # define __acquire(x)	__context__(1)
 # define __release(x)	__context__(-1)
-# define __cond_lock(x)	((x) ? ({ __context__(1); 1; }) : 0)
+# define __cond_lock(x,c)	((c) ? ({ __acquire(x); 1; }) : 0)
 extern void __chk_user_ptr(void __user *);
 extern void __chk_io_ptr(void __iomem *);
 #else
@@ -31,7 +31,7 @@ extern void __chk_io_ptr(void __iomem *);
 # define __releases(x)
 # define __acquire(x) (void)0
 # define __release(x) (void)0
-# define __cond_lock(x) (x)
+# define __cond_lock(x,c) (c)
 #endif
 
 #ifdef __KERNEL__

commit cebc04ba9aeb3a646cc746300421fc0e5aa4f253
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Aug 14 22:43:18 2006 -0700

    add CONFIG_ENABLE_MUST_CHECK
    
    Those 1500 warnings can be a bit of a pain.  Add a config option to shut them
    up.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 9b4f11094937..060b96112ec6 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,6 +99,11 @@ extern void __chk_io_ptr(void __iomem *);
 #define __must_check
 #endif
 
+#ifndef CONFIG_ENABLE_MUST_CHECK
+#undef __must_check
+#define __must_check
+#endif
+
 /*
  * Allow us to avoid 'defined but not used' warnings on functions and data,
  * as well as force them to be emitted to the assembly file.

commit 423bc7b22bdeb73efeabfcf91d8a459ac33088f1
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Thu May 4 00:41:02 2006 +0100

    Restore __attribute_const__ to user-visibility in linux/compiler.h...for now
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1234be9024a2..9b4f11094937 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -128,6 +128,16 @@ extern void __chk_io_ptr(void __iomem *);
 # define __attribute_pure__	/* unimplemented */
 #endif
 
+#ifndef noinline
+#define noinline
+#endif
+
+#ifndef __always_inline
+#define __always_inline inline
+#endif
+
+#endif /* __KERNEL__ */
+
 /*
  * From the GCC manual:
  *
@@ -146,13 +156,4 @@ extern void __chk_io_ptr(void __iomem *);
 # define __attribute_const__	/* unimplemented */
 #endif
 
-#ifndef noinline
-#define noinline
-#endif
-
-#ifndef __always_inline
-#define __always_inline inline
-#endif
-
-#endif /* __KERNEL__ */
 #endif /* __LINUX_COMPILER_H */

commit 4f79c3ffc6e04623711e86cf9a0e09e4aad8cb36
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Tue May 2 10:41:25 2006 +0100

    Guard some of linux/compiler.h with #ifdef __KERNEL__
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index f23d3c6fc2c0..1234be9024a2 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -78,6 +78,7 @@ extern void __chk_io_ptr(void __iomem *);
 
 #endif /* __ASSEMBLY__ */
 
+#ifdef __KERNEL__
 /*
  * Allow us to mark functions as 'deprecated' and have gcc emit a nice
  * warning for each use, in hopes of speeding the functions removal.
@@ -153,4 +154,5 @@ extern void __chk_io_ptr(void __iomem *);
 #define __always_inline inline
 #endif
 
+#endif /* __KERNEL__ */
 #endif /* __LINUX_COMPILER_H */

commit fd285bb54d8a3e99810090ae88cfe8ed77d1da25
Author: Andrew Morton <akpm@osdl.org>
Date:   Sun Jan 8 01:04:07 2006 -0800

    [PATCH] Abandon gcc-2.95.x
    
    There's one scsi driver which doesn't compile due to weird __VA_ARGS__ tricks
    and the rather useful scsi/sd.c is currently getting an ICE.  None of the new
    SAS code compiles, due to extensive use of anonymous unions.  The V4L guys are
    very good at exploiting the gcc-2.95.x macro expansion bug (_why_ does each
    driver need to implement its own debug macros?) and various people keep on
    sneaking in anonymous unions, which are rather nice.
    
    Plus anonymous unions are rather useful.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index d7378215b851..f23d3c6fc2c0 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -42,8 +42,6 @@ extern void __chk_io_ptr(void __iomem *);
 # include <linux/compiler-gcc4.h>
 #elif __GNUC__ == 3
 # include <linux/compiler-gcc3.h>
-#elif __GNUC__ == 2
-# include <linux/compiler-gcc2.h>
 #else
 # error Sorry, your compiler is too old/not recognized.
 #endif

commit 512345be2549308b8ae8e85a3ff7f6d56a38e5f6
Author: Paul E. McKenney <paulmck@us.ibm.com>
Date:   Sun May 1 08:59:03 2005 -0700

    [PATCH] Add deprecated_for_modules
    
    Add a deprecated_for_modules macro that allows symbols to be deprecated only
    when used by modules, as suggested by Andrew Morton some months back.
    
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 487725cf0d0d..d7378215b851 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -90,6 +90,12 @@ extern void __chk_io_ptr(void __iomem *);
 # define __deprecated		/* unimplemented */
 #endif
 
+#ifdef MODULE
+#define __deprecated_for_modules __deprecated
+#else
+#define __deprecated_for_modules
+#endif
+
 #ifndef __must_check
 #define __must_check
 #endif

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/compiler.h b/include/linux/compiler.h
new file mode 100644
index 000000000000..487725cf0d0d
--- /dev/null
+++ b/include/linux/compiler.h
@@ -0,0 +1,152 @@
+#ifndef __LINUX_COMPILER_H
+#define __LINUX_COMPILER_H
+
+#ifndef __ASSEMBLY__
+
+#ifdef __CHECKER__
+# define __user		__attribute__((noderef, address_space(1)))
+# define __kernel	/* default address space */
+# define __safe		__attribute__((safe))
+# define __force	__attribute__((force))
+# define __nocast	__attribute__((nocast))
+# define __iomem	__attribute__((noderef, address_space(2)))
+# define __acquires(x)	__attribute__((context(0,1)))
+# define __releases(x)	__attribute__((context(1,0)))
+# define __acquire(x)	__context__(1)
+# define __release(x)	__context__(-1)
+# define __cond_lock(x)	((x) ? ({ __context__(1); 1; }) : 0)
+extern void __chk_user_ptr(void __user *);
+extern void __chk_io_ptr(void __iomem *);
+#else
+# define __user
+# define __kernel
+# define __safe
+# define __force
+# define __nocast
+# define __iomem
+# define __chk_user_ptr(x) (void)0
+# define __chk_io_ptr(x) (void)0
+# define __builtin_warning(x, y...) (1)
+# define __acquires(x)
+# define __releases(x)
+# define __acquire(x) (void)0
+# define __release(x) (void)0
+# define __cond_lock(x) (x)
+#endif
+
+#ifdef __KERNEL__
+
+#if __GNUC__ > 4
+#error no compiler-gcc.h file for this gcc version
+#elif __GNUC__ == 4
+# include <linux/compiler-gcc4.h>
+#elif __GNUC__ == 3
+# include <linux/compiler-gcc3.h>
+#elif __GNUC__ == 2
+# include <linux/compiler-gcc2.h>
+#else
+# error Sorry, your compiler is too old/not recognized.
+#endif
+
+/* Intel compiler defines __GNUC__. So we will overwrite implementations
+ * coming from above header files here
+ */
+#ifdef __INTEL_COMPILER
+# include <linux/compiler-intel.h>
+#endif
+
+/*
+ * Generic compiler-dependent macros required for kernel
+ * build go below this comment. Actual compiler/compiler version
+ * specific implementations come from the above header files
+ */
+
+#define likely(x)	__builtin_expect(!!(x), 1)
+#define unlikely(x)	__builtin_expect(!!(x), 0)
+
+/* Optimization barrier */
+#ifndef barrier
+# define barrier() __memory_barrier()
+#endif
+
+#ifndef RELOC_HIDE
+# define RELOC_HIDE(ptr, off)					\
+  ({ unsigned long __ptr;					\
+     __ptr = (unsigned long) (ptr);				\
+    (typeof(ptr)) (__ptr + (off)); })
+#endif
+
+#endif /* __KERNEL__ */
+
+#endif /* __ASSEMBLY__ */
+
+/*
+ * Allow us to mark functions as 'deprecated' and have gcc emit a nice
+ * warning for each use, in hopes of speeding the functions removal.
+ * Usage is:
+ * 		int __deprecated foo(void)
+ */
+#ifndef __deprecated
+# define __deprecated		/* unimplemented */
+#endif
+
+#ifndef __must_check
+#define __must_check
+#endif
+
+/*
+ * Allow us to avoid 'defined but not used' warnings on functions and data,
+ * as well as force them to be emitted to the assembly file.
+ *
+ * As of gcc 3.3, static functions that are not marked with attribute((used))
+ * may be elided from the assembly file.  As of gcc 3.3, static data not so
+ * marked will not be elided, but this may change in a future gcc version.
+ *
+ * In prior versions of gcc, such functions and data would be emitted, but
+ * would be warned about except with attribute((unused)).
+ */
+#ifndef __attribute_used__
+# define __attribute_used__	/* unimplemented */
+#endif
+
+/*
+ * From the GCC manual:
+ *
+ * Many functions have no effects except the return value and their
+ * return value depends only on the parameters and/or global
+ * variables.  Such a function can be subject to common subexpression
+ * elimination and loop optimization just as an arithmetic operator
+ * would be.
+ * [...]
+ */
+#ifndef __attribute_pure__
+# define __attribute_pure__	/* unimplemented */
+#endif
+
+/*
+ * From the GCC manual:
+ *
+ * Many functions do not examine any values except their arguments,
+ * and have no effects except the return value.  Basically this is
+ * just slightly more strict class than the `pure' attribute above,
+ * since function is not allowed to read global memory.
+ *
+ * Note that a function that has pointer arguments and examines the
+ * data pointed to must _not_ be declared `const'.  Likewise, a
+ * function that calls a non-`const' function usually must not be
+ * `const'.  It does not make sense for a `const' function to return
+ * `void'.
+ */
+#ifndef __attribute_const__
+# define __attribute_const__	/* unimplemented */
+#endif
+
+#ifndef noinline
+#define noinline
+#endif
+
+#ifndef __always_inline
+#define __always_inline inline
+#endif
+
+#endif /* __LINUX_COMPILER_H */
