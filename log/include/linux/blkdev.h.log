commit bfe373f608cf81b7626dfeb904001b0e867c5110
Author: Hou Tao <houtao1@huawei.com>
Date:   Tue Apr 28 09:54:56 2020 +0800

    blk-mq-debugfs: update blk_queue_flag_name[] accordingly for new flags
    
    Else there may be magic numbers in /sys/kernel/debug/block/*/state.
    
    Signed-off-by: Hou Tao <houtao1@huawei.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8fd900998b4e..57241417ff2f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -590,6 +590,7 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
+/* Keep blk_queue_flag_name[] in sync with the definitions below */
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */

commit dc35ada4251f183137ee3a524543c9329d7a4fa2
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 15:41:23 2020 +0200

    block: fix a warning when blkdev.h is included for !CONFIG_BLOCK builds
    
    disk_start_io_acct and disk_end_io_acct need at least a struct gendisk
    forward declaration, but for weird historic reasons much of blkdev.h
    is stubbed out for CONFIG_BLOCK=n.  Fix this by stubbing more out for
    now, but eventually this header will need a massive cleanup.
    
    Fixes: 956d510ee78 ("block: add disk/bio-based accounting helpers")
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6f7ff0fa8fcf..8fd900998b4e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1892,12 +1892,12 @@ static inline void blk_wake_io_task(struct task_struct *waiter)
 		wake_up_process(waiter);
 }
 
+#ifdef CONFIG_BLOCK
 unsigned long disk_start_io_acct(struct gendisk *disk, unsigned int sectors,
 		unsigned int op);
 void disk_end_io_acct(struct gendisk *disk, unsigned int op,
 		unsigned long start_time);
 
-#ifdef CONFIG_BLOCK
 /**
  * bio_start_io_acct - start I/O accounting for bio based drivers
  * @bio:	bio to start account for

commit 956d510ee78caebc83c0eaeb892db5b239a36a06
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 27 07:24:04 2020 +0200

    block: add disk/bio-based accounting helpers
    
    Add two new helpers to simplify I/O accounting for bio based drivers.
    Currently these drivers use the generic_start_io_acct and
    generic_end_io_acct helpers which have very cumbersome calling
    conventions, don't actually return the time they started accounting,
    and try to deal with accounting for partitions, which can't happen
    for bio based drivers.  The new helpers will be used to subsequently
    replace uses of the old helpers.
    
    The main API is the bio based wrappes in blkdev.h, but for zram
    which wants to account rw_page based I/O lower level routines are
    provided as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7d10f4e63232..6f7ff0fa8fcf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1892,4 +1892,32 @@ static inline void blk_wake_io_task(struct task_struct *waiter)
 		wake_up_process(waiter);
 }
 
+unsigned long disk_start_io_acct(struct gendisk *disk, unsigned int sectors,
+		unsigned int op);
+void disk_end_io_acct(struct gendisk *disk, unsigned int op,
+		unsigned long start_time);
+
+#ifdef CONFIG_BLOCK
+/**
+ * bio_start_io_acct - start I/O accounting for bio based drivers
+ * @bio:	bio to start account for
+ *
+ * Returns the start time that should be passed back to bio_end_io_acct().
+ */
+static inline unsigned long bio_start_io_acct(struct bio *bio)
+{
+	return disk_start_io_acct(bio->bi_disk, bio_sectors(bio), bio_op(bio));
+}
+
+/**
+ * bio_end_io_acct - end I/O accounting for bio based drivers
+ * @bio:	bio to end account for
+ * @start:	start time returned by bio_start_io_acct()
+ */
+static inline void bio_end_io_acct(struct bio *bio, unsigned long start_time)
+{
+	return disk_end_io_acct(bio->bi_disk, bio_op(bio), start_time);
+}
+#endif /* CONFIG_BLOCK */
+
 #endif

commit 9398554fb3979852512ff4f1405e759889b45c16
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 13 14:36:00 2020 +0200

    block: remove the error_sector argument to blkdev_issue_flush
    
    The argument isn't used by any caller, and drivers don't fill out
    bi_sector for flush requests either.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2b33166b9daf..7d10f4e63232 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1233,7 +1233,7 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 
 extern void blk_io_schedule(void);
 
-extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
+int blkdev_issue_flush(struct block_device *, gfp_t);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 
@@ -1872,8 +1872,7 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 	return false;
 }
 
-static inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
-				     sector_t *error_sector)
+static inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask)
 {
 	return 0;
 }

commit d145dc23030bbf2de3a8ca5e0c29c2e568f69737
Author: Satya Tangirala <satyat@google.com>
Date:   Thu May 14 00:37:19 2020 +0000

    block: Make blk-integrity preclude hardware inline encryption
    
    Whenever a device supports blk-integrity, make the kernel pretend that
    the device doesn't support inline encryption (essentially by setting the
    keyslot manager in the request queue to NULL).
    
    There's no hardware currently that supports both integrity and inline
    encryption. However, it seems possible that there will be such hardware
    in the near future (like the NVMe key per I/O support that might support
    both inline encryption and PI).
    
    But properly integrating both features is not trivial, and without
    real hardware that implements both, it is difficult to tell if it will
    be done correctly by the majority of hardware that support both.
    So it seems best not to support both features together right now, and
    to decide what to do at probe time.
    
    Signed-off-by: Satya Tangirala <satyat@google.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 52a9f456cadf..2b33166b9daf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1583,6 +1583,12 @@ struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
 	return blk_get_integrity(bdev->bd_disk);
 }
 
+static inline bool
+blk_integrity_queue_supports_integrity(struct request_queue *q)
+{
+	return q->integrity.profile;
+}
+
 static inline bool blk_integrity_rq(struct request *rq)
 {
 	return rq->cmd_flags & REQ_INTEGRITY;
@@ -1663,6 +1669,11 @@ static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
 {
 	return NULL;
 }
+static inline bool
+blk_integrity_queue_supports_integrity(struct request_queue *q)
+{
+	return false;
+}
 static inline int blk_integrity_compare(struct gendisk *a, struct gendisk *b)
 {
 	return 0;
@@ -1714,6 +1725,25 @@ static inline struct bio_vec *rq_integrity_vec(struct request *rq)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
+#ifdef CONFIG_BLK_INLINE_ENCRYPTION
+
+bool blk_ksm_register(struct blk_keyslot_manager *ksm, struct request_queue *q);
+
+void blk_ksm_unregister(struct request_queue *q);
+
+#else /* CONFIG_BLK_INLINE_ENCRYPTION */
+
+static inline bool blk_ksm_register(struct blk_keyslot_manager *ksm,
+				    struct request_queue *q)
+{
+	return true;
+}
+
+static inline void blk_ksm_unregister(struct request_queue *q) { }
+
+#endif /* CONFIG_BLK_INLINE_ENCRYPTION */
+
+
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);

commit a892c8d52c02284076fbbacae6692aa5c5807d11
Author: Satya Tangirala <satyat@google.com>
Date:   Thu May 14 00:37:18 2020 +0000

    block: Inline encryption support for blk-mq
    
    We must have some way of letting a storage device driver know what
    encryption context it should use for en/decrypting a request. However,
    it's the upper layers (like the filesystem/fscrypt) that know about and
    manages encryption contexts. As such, when the upper layer submits a bio
    to the block layer, and this bio eventually reaches a device driver with
    support for inline encryption, the device driver will need to have been
    told the encryption context for that bio.
    
    We want to communicate the encryption context from the upper layer to the
    storage device along with the bio, when the bio is submitted to the block
    layer. To do this, we add a struct bio_crypt_ctx to struct bio, which can
    represent an encryption context (note that we can't use the bi_private
    field in struct bio to do this because that field does not function to pass
    information across layers in the storage stack). We also introduce various
    functions to manipulate the bio_crypt_ctx and make the bio/request merging
    logic aware of the bio_crypt_ctx.
    
    We also make changes to blk-mq to make it handle bios with encryption
    contexts. blk-mq can merge many bios into the same request. These bios need
    to have contiguous data unit numbers (the necessary changes to blk-merge
    are also made to ensure this) - as such, it suffices to keep the data unit
    number of just the first bio, since that's all a storage driver needs to
    infer the data unit number to use for each data block in each bio in a
    request. blk-mq keeps track of the encryption context to be used for all
    the bios in a request with the request's rq_crypt_ctx. When the first bio
    is added to an empty request, blk-mq will program the encryption context
    of that bio into the request_queue's keyslot manager, and store the
    returned keyslot in the request's rq_crypt_ctx. All the functions to
    operate on encryption contexts are in blk-crypto.c.
    
    Upper layers only need to call bio_crypt_set_ctx with the encryption key,
    algorithm and data_unit_num; they don't have to worry about getting a
    keyslot for each encryption context, as blk-mq/blk-crypto handles that.
    Blk-crypto also makes it possible for request-based layered devices like
    dm-rq to make use of inline encryption hardware by cloning the
    rq_crypt_ctx and programming a keyslot in the new request_queue when
    necessary.
    
    Note that any user of the block layer can submit bios with an
    encryption context, such as filesystems, device-mapper targets, etc.
    
    Signed-off-by: Satya Tangirala <satyat@google.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 354e44eebef9..52a9f456cadf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -222,6 +222,11 @@ struct request {
 	unsigned short nr_integrity_segments;
 #endif
 
+#ifdef CONFIG_BLK_INLINE_ENCRYPTION
+	struct bio_crypt_ctx *crypt_ctx;
+	struct blk_ksm_keyslot *crypt_keyslot;
+#endif
+
 	unsigned short write_hint;
 	unsigned short ioprio;
 

commit 1b2628397058ebce7277480960b29c788138de90
Author: Satya Tangirala <satyat@google.com>
Date:   Thu May 14 00:37:17 2020 +0000

    block: Keyslot Manager for Inline Encryption
    
    Inline Encryption hardware allows software to specify an encryption context
    (an encryption key, crypto algorithm, data unit num, data unit size) along
    with a data transfer request to a storage device, and the inline encryption
    hardware will use that context to en/decrypt the data. The inline
    encryption hardware is part of the storage device, and it conceptually sits
    on the data path between system memory and the storage device.
    
    Inline Encryption hardware implementations often function around the
    concept of "keyslots". These implementations often have a limited number
    of "keyslots", each of which can hold a key (we say that a key can be
    "programmed" into a keyslot). Requests made to the storage device may have
    a keyslot and a data unit number associated with them, and the inline
    encryption hardware will en/decrypt the data in the requests using the key
    programmed into that associated keyslot and the data unit number specified
    with the request.
    
    As keyslots are limited, and programming keys may be expensive in many
    implementations, and multiple requests may use exactly the same encryption
    contexts, we introduce a Keyslot Manager to efficiently manage keyslots.
    
    We also introduce a blk_crypto_key, which will represent the key that's
    programmed into keyslots managed by keyslot managers. The keyslot manager
    also functions as the interface that upper layers will use to program keys
    into inline encryption hardware. For more information on the Keyslot
    Manager, refer to documentation found in block/keyslot-manager.c and
    linux/keyslot-manager.h.
    
    Co-developed-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Satya Tangirala <satyat@google.com>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f9e4b21b051b..354e44eebef9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -43,6 +43,7 @@ struct pr_ops;
 struct rq_qos;
 struct blk_queue_stats;
 struct blk_stat_callback;
+struct blk_keyslot_manager;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -468,6 +469,11 @@ struct request_queue {
 	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 
+#ifdef CONFIG_BLK_INLINE_ENCRYPTION
+	/* Inline crypto capabilities */
+	struct blk_keyslot_manager *ksm;
+#endif
+
 	unsigned int		rq_timeout;
 	int			poll_nsec;
 

commit 71ac860af824ce9ebbbe8de20044e777c0fc33b9
Author: Ming Lei <ming.lei@redhat.com>
Date:   Thu May 14 16:45:09 2020 +0800

    block: move blk_io_schedule() out of header file
    
    blk_io_schedule() isn't called from performance sensitive code path, and
    it is easier to maintain by exporting it as symbol.
    
    Also blk_io_schedule() is only called by CONFIG_BLOCK code, so it is safe
    to do this way. Meantime fixes build failure when CONFIG_BLOCK is off.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Fixes: e6249cdd46e4 ("block: add blk_io_schedule() for avoiding task hung in sync dio")
    Reported-by: Satya Tangirala <satyat@google.com>
    Tested-by: Satya Tangirala <satyat@google.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5360696d85ff..f9e4b21b051b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -27,7 +27,6 @@
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
 #include <linux/blkzoned.h>
-#include <linux/sched/sysctl.h>
 
 struct module;
 struct scsi_ioctl_command;
@@ -1221,6 +1220,8 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 		 !list_empty(&plug->cb_list));
 }
 
+extern void blk_io_schedule(void);
+
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
@@ -1851,15 +1852,4 @@ static inline void blk_wake_io_task(struct task_struct *waiter)
 		wake_up_process(waiter);
 }
 
-static inline void blk_io_schedule(void)
-{
-	/* Prevent hang_check timer from firing at us during very long I/O */
-	unsigned long timeout = sysctl_hung_task_timeout_secs * HZ / 2;
-
-	if (timeout)
-		io_schedule_timeout(timeout);
-	else
-		io_schedule();
-}
-
 #endif

commit e732671aa5f67232cf760666a15242dead003362
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Tue May 12 17:55:49 2020 +0900

    block: Modify revalidate zones
    
    Modify the interface of blk_revalidate_disk_zones() to add an optional
    driver callback function that a driver can use to extend processing
    done during zone revalidation. The callback, if defined, is executed
    with the device request queue frozen, after all zones have been
    inspected.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 73f4f4f1df92..5360696d85ff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -358,7 +358,8 @@ unsigned int blkdev_nr_zones(struct gendisk *disk);
 extern int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 			    sector_t sectors, sector_t nr_sectors,
 			    gfp_t gfp_mask);
-extern int blk_revalidate_disk_zones(struct gendisk *disk);
+int blk_revalidate_disk_zones(struct gendisk *disk,
+			      void (*update_driver_data)(struct gendisk *disk));
 
 extern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 				     unsigned int cmd, unsigned long arg);

commit 1392d37018d4f68c5bb2c98dae9a018b73926865
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Tue May 12 17:55:48 2020 +0900

    block: introduce blk_req_zone_write_trylock
    
    Introduce blk_req_zone_write_trylock(), which either grabs the write-lock
    for a sequential zone or returns false, if the zone is already locked.
    
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5647c78bb876..73f4f4f1df92 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1738,6 +1738,7 @@ extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 
 #ifdef CONFIG_BLK_DEV_ZONED
 bool blk_req_needs_zone_write_lock(struct request *rq);
+bool blk_req_zone_write_trylock(struct request *rq);
 void __blk_req_zone_write_lock(struct request *rq);
 void __blk_req_zone_write_unlock(struct request *rq);
 

commit 0512a75b98f847c2e9a4b664013424e603e202f7
Author: Keith Busch <kbusch@kernel.org>
Date:   Tue May 12 17:55:47 2020 +0900

    block: Introduce REQ_OP_ZONE_APPEND
    
    Define REQ_OP_ZONE_APPEND to append-write sectors to a zone of a zoned
    block device. This is a no-merge write operation.
    
    A zone append write BIO must:
    * Target a zoned block device
    * Have a sector position indicating the start sector of the target zone
    * The target zone must be a sequential write zone
    * The BIO must not cross a zone boundary
    * The BIO size must not be split to ensure that a single range of LBAs
      is written with a single command.
    
    Implement these checks in generic_make_request_checks() using the
    helper function blk_check_zone_append(). To avoid write append BIO
    splitting, introduce the new max_zone_append_sectors queue limit
    attribute and ensure that a BIO size is always lower than this limit.
    Export this new limit through sysfs and check these limits in bio_full().
    
    Also when a LLDD can't dispatch a request to a specific zone, it
    will return BLK_STS_ZONE_RESOURCE indicating this request needs to
    be delayed, e.g.  because the zone it will be dispatched to is still
    write-locked. If this happens set the request aside in a local list
    to continue trying dispatching requests such as READ requests or a
    WRITE/ZONE_APPEND requests targetting other zones. This way we can
    still keep a high queue depth without starving other requests even if
    one request can't be served due to zone write-locking.
    
    Finally, make sure that the bio sector position indicates the actual
    write position as indicated by the device on completion.
    
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    [ jth: added zone-append specific add_page and merge_page helpers ]
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d736acf7f564..5647c78bb876 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -332,6 +332,7 @@ struct queue_limits {
 	unsigned int		max_hw_discard_sectors;
 	unsigned int		max_write_same_sectors;
 	unsigned int		max_write_zeroes_sectors;
+	unsigned int		max_zone_append_sectors;
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;
 
@@ -750,6 +751,9 @@ static inline bool rq_mergeable(struct request *rq)
 	if (req_op(rq) == REQ_OP_WRITE_ZEROES)
 		return false;
 
+	if (req_op(rq) == REQ_OP_ZONE_APPEND)
+		return false;
+
 	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
 		return false;
 	if (rq->rq_flags & RQF_NOMERGE_FLAGS)
@@ -1084,6 +1088,8 @@ extern void blk_queue_max_write_same_sectors(struct request_queue *q,
 extern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned int);
+extern void blk_queue_max_zone_append_sectors(struct request_queue *q,
+		unsigned int max_zone_append_sectors);
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
@@ -1301,6 +1307,11 @@ static inline unsigned int queue_max_segment_size(const struct request_queue *q)
 	return q->limits.max_segment_size;
 }
 
+static inline unsigned int queue_max_zone_append_sectors(const struct request_queue *q)
+{
+	return q->limits.max_zone_append_sectors;
+}
+
 static inline unsigned queue_logical_block_size(const struct request_queue *q)
 {
 	int retval = 512;

commit 02992df822e7e36685593aad10721a5a9f8d3402
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Tue May 12 17:55:45 2020 +0900

    block: provide fallbacks for blk_queue_zone_is_seq and blk_queue_zone_no
    
    blk_queue_zone_is_seq() and blk_queue_zone_no() have not been called with
    CONFIG_BLK_DEV_ZONED disabled until now.
    
    The introduction of REQ_OP_ZONE_APPEND will change this, so we need to
    provide noop fallbacks for the !CONFIG_BLK_DEV_ZONED case.
    
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 222eb5f32279..d736acf7f564 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -722,6 +722,16 @@ static inline unsigned int blk_queue_nr_zones(struct request_queue *q)
 {
 	return 0;
 }
+static inline bool blk_queue_zone_is_seq(struct request_queue *q,
+					 sector_t sector)
+{
+	return false;
+}
+static inline unsigned int blk_queue_zone_no(struct request_queue *q,
+					     sector_t sector)
+{
+	return 0;
+}
 #endif /* CONFIG_BLK_DEV_ZONED */
 
 static inline bool rq_is_sync(struct request *rq)

commit e6249cdd46e43a7d3bdb8cce5fe24565d6c11e94
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun May 3 09:54:22 2020 +0800

    block: add blk_io_schedule() for avoiding task hung in sync dio
    
    Sync dio could be big, or may take long time in discard or in case of
    IO failure.
    
    We have prevented task hung in submit_bio_wait() and blk_execute_rq(),
    so apply the same trick for prevent task hung from happening in sync dio.
    
    Add helper of blk_io_schedule() and use io_schedule_timeout() to prevent
    task hung warning.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Cc: Salman Qazi <sqazi@google.com>
    Cc: Jesse Barnes <jsbarnes@google.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: Hannes Reinecke <hare@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f00bd4042295..222eb5f32279 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -27,6 +27,7 @@
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
 #include <linux/blkzoned.h>
+#include <linux/sched/sysctl.h>
 
 struct module;
 struct scsi_ioctl_command;
@@ -1827,4 +1828,15 @@ static inline void blk_wake_io_task(struct task_struct *waiter)
 		wake_up_process(waiter);
 }
 
+static inline void blk_io_schedule(void)
+{
+	/* Prevent hang_check timer from firing at us during very long I/O */
+	unsigned long timeout = sysctl_hung_task_timeout_secs * HZ / 2;
+
+	if (timeout)
+		io_schedule_timeout(timeout);
+	else
+		io_schedule();
+}
+
 #endif

commit bdf8710d69f82ee6fd41b0166300c3306898b3c1
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 14 09:42:25 2020 +0200

    block: move dma_pad handling from blk_rq_map_sg into the callers
    
    There are only two callers of blk_rq_map_sg/__blk_rq_map_sg that set
    the dma_pad value in the queue.  Move the handling into those callers
    instead of burdening the common code, and move the ->extra_len field
    from struct request to struct scsi_cmnd.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8e4726bce498..f00bd4042295 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -224,8 +224,6 @@ struct request {
 	unsigned short write_hint;
 	unsigned short ioprio;
 
-	unsigned int extra_len;	/* length of alignment and padding */
-
 	enum mq_rq_state state;
 	refcount_t ref;
 

commit cc97923a5bccc776851c242b61015faf288d5c22
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 14 09:42:24 2020 +0200

    block: move dma drain handling to scsi
    
    Don't burden the common block code with with specifics of the libata DMA
    draining mechanism.  Instead move most of the code to the scsi midlayer.
    
    That also means the nr_phys_segments adjustments in the blk-mq fast path
    can go away entirely, given that SCSI never looks at nr_phys_segments
    after mapping the request to a scatterlist.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 496dc9491026..8e4726bce498 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -288,7 +288,6 @@ struct blk_queue_ctx;
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
 
 struct bio_vec;
-typedef int (dma_drain_needed_fn)(struct request *);
 
 enum blk_eh_timer_return {
 	BLK_EH_DONE,		/* drivers has completed the command */
@@ -397,7 +396,6 @@ struct request_queue {
 	struct rq_qos		*rq_qos;
 
 	make_request_fn		*make_request_fn;
-	dma_drain_needed_fn	*dma_drain_needed;
 
 	const struct blk_mq_ops	*mq_ops;
 
@@ -467,8 +465,6 @@ struct request_queue {
 	 */
 	unsigned long		nr_requests;	/* Max # of requests */
 
-	unsigned int		dma_drain_size;
-	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 
@@ -1097,9 +1093,6 @@ extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
 			      sector_t offset);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
 extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
-extern int blk_queue_dma_drain(struct request_queue *q,
-			       dma_drain_needed_fn *dma_drain_needed,
-			       void *buf, unsigned int size);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_dma_alignment(struct request_queue *, int);

commit 89de1504d53b59b12bfff227328ee3e63dd3a112
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 14 09:42:22 2020 +0200

    block: provide a blk_rq_map_sg variant that returns the last element
    
    To be able to move some of the special purpose hacks in blk_rq_map_sg
    into the callers we need a variant that returns the last mapped
    S/G list element to the caller.  Add that variant as __blk_rq_map_sg
    and make blk_rq_map_sg a trivial inline wrapper around it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 76da162b6ae9..496dc9491026 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1136,7 +1136,15 @@ static inline unsigned short blk_rq_nr_discard_segments(struct request *rq)
 	return max_t(unsigned short, rq->nr_phys_segments, 1);
 }
 
-extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
+int __blk_rq_map_sg(struct request_queue *q, struct request *rq,
+		struct scatterlist *sglist, struct scatterlist **last_sg);
+static inline int blk_rq_map_sg(struct request_queue *q, struct request *rq,
+		struct scatterlist *sglist)
+{
+	struct scatterlist *last_sg = NULL;
+
+	return __blk_rq_map_sg(q, rq, sglist, &last_sg);
+}
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
 

commit e64a0e16928415648d53d721b3d6fc3635eddf92
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 14 09:42:21 2020 +0200

    block: remove RQF_COPY_USER
    
    The RQF_COPY_USER is set for bio where the passthrough request mapping
    helpers decided that bounce buffering is required.  It is then used to
    pad scatterlist for drivers that required it.  But given that
    non-passthrough requests are per definition aligned, and directly mapped
    pass-through request must be aligned it is not actually required at all.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 32868fbedc9e..76da162b6ae9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -82,8 +82,6 @@ typedef __u32 __bitwise req_flags_t;
 /* set for "ide_preempt" requests and also for requests for which the SCSI
    "quiesce" state must be ignored. */
 #define RQF_PREEMPT		((__force req_flags_t)(1 << 8))
-/* contains copies of user pages */
-#define RQF_COPY_USER		((__force req_flags_t)(1 << 9))
 /* vaguely specified driver internal error.  Ignored by the block layer */
 #define RQF_FAILED		((__force req_flags_t)(1 << 10))
 /* don't warn about errors */

commit 1592614838cb52f4313ceff64894e2ca78591498
Merge: 10f36b1e80a9 766c3297d7e1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 11:43:51 2020 -0700

    Merge tag 'for-5.7/drivers-2020-03-29' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
    
     - floppy driver cleanup series from Willy
    
     - NVMe updates and fixes (Various)
    
     - null_blk trace improvements (Chaitanya)
    
     - bcache fixes (Coly)
    
     - md fixes (via Song)
    
     - loop block size change optimizations (Martijn)
    
     - scnprintf() use (Takashi)
    
    * tag 'for-5.7/drivers-2020-03-29' of git://git.kernel.dk/linux-block: (81 commits)
      null_blk: add trace in null_blk_zoned.c
      null_blk: add tracepoint helpers for zoned mode
      block: add a zone condition debug helper
      nvme: cleanup namespace identifier reporting in nvme_init_ns_head
      nvme: rename __nvme_find_ns_head to nvme_find_ns_head
      nvme: refactor nvme_identify_ns_descs error handling
      nvme-tcp: Add warning on state change failure at nvme_tcp_setup_ctrl
      nvme-rdma: Add warning on state change failure at nvme_rdma_setup_ctrl
      nvme: Fix controller creation races with teardown flow
      nvme: Make nvme_uninit_ctrl symmetric to nvme_init_ctrl
      nvme: Fix ctrl use-after-free during sysfs deletion
      nvme-pci: Re-order nvme_pci_free_ctrl
      nvme: Remove unused return code from nvme_delete_ctrl_sync
      nvme: Use nvme_state_terminal helper
      nvme: release ida resources
      nvme: Add compat_ioctl handler for NVME_IOCTL_SUBMIT_IO
      nvmet-tcp: optimize tcp stack TX when data digest is used
      nvme-fabrics: Use scnprintf() for avoiding potential buffer overflow
      nvme-multipath: do not reset on unknown status
      nvmet-rdma: allocate RW ctxs according to mdts
      ...

commit 02694e86356dcf72d39329e52630234ad687e206
Author: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date:   Wed Mar 25 10:49:54 2020 -0700

    block: add a zone condition debug helper
    
    Add a helper to stringify the zone conditions. We use this helper in the
    next patch to track zone conditions in tracepoints.
    
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f629d40c645c..a5acf17e7d76 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -952,6 +952,10 @@ static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
 }
 
 #ifdef CONFIG_BLK_DEV_ZONED
+
+/* Helper to convert BLK_ZONE_ZONE_XXX to its string format XXX */
+const char *blk_zone_cond_str(enum blk_zone_cond zone_cond);
+
 static inline unsigned int blk_rq_zone_no(struct request *rq)
 {
 	return blk_queue_zone_no(rq->q, blk_rq_pos(rq));

commit 3d745ea5b095a3985129e162900b7e6c22518a9d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:30:11 2020 +0100

    block: simplify queue allocation
    
    Current make_request based drivers use either blk_alloc_queue_node or
    blk_alloc_queue to allocate a queue, and then set up the make_request_fn
    function pointer and a few parameters using the blk_queue_make_request
    helper.  Simplify this by passing the make_request pointer to
    blk_alloc_queue, and while at it merge the _node variant into the main
    helper by always passing a node_id, and remove the superfluous gfp_mask
    parameter.  A lower-level __blk_alloc_queue is kept for the blk-mq case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e8defd718d62..3f27ff08483e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1063,7 +1063,6 @@ extern void blk_abort_request(struct request *);
  * Access functions for manipulating queue properties
  */
 extern void blk_cleanup_queue(struct request_queue *);
-extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
@@ -1140,8 +1139,7 @@ extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
 
 bool __must_check blk_get_queue(struct request_queue *);
-struct request_queue *blk_alloc_queue(gfp_t);
-struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id);
+struct request_queue *blk_alloc_queue(make_request_fn make_request, int node_id);
 extern void blk_put_queue(struct request_queue *);
 extern void blk_set_queue_dying(struct request_queue *);
 

commit 348e114bbd4dce430eae70f01a04c8fc259b4cf1
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:07:17 2020 +0100

    block: move the ->devnode callback to struct block_device_operations
    
    There really isn't any good reason to stash a method directly into
    struct gendisk.  Move it together with the other block device
    operations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 53a1325efbc3..e8defd718d62 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1697,6 +1697,7 @@ struct block_device_operations {
 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 	int (*report_zones)(struct gendisk *, sector_t sector,
 			unsigned int nr_zones, report_zones_cb cb, void *data);
+	char *(*devnode)(struct gendisk *disk, umode_t *mode);
 	struct module *owner;
 	const struct pr_ops *pr_ops;
 };

commit 1a9fba3a77a5b39d1c9e1611758303f2649474e9
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Mar 24 08:25:18 2020 +0100

    block: unexport read_dev_sector and put_dev_sector
    
    read_dev_sector and put_dev_sector are now only used by the partition
    parsing code.  Remove the export for read_dev_sector and merge it into
    the only caller.  Clean the mess up a bit by using goto labels and
    the SECTOR_SHIFT constant.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f629d40c645c..53a1325efbc3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1484,15 +1484,6 @@ static inline unsigned int block_size(struct block_device *bdev)
 	return bdev->bd_block_size;
 }
 
-typedef struct {struct page *v;} Sector;
-
-unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);
-
-static inline void put_dev_sector(Sector p)
-{
-	put_page(p.v);
-}
-
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 

commit e959e5405f34aa92d71d0dd162b969c21742061d
Author: Daniel Wagner <dwagner@suse.de>
Date:   Mon Mar 2 14:24:08 2020 +0100

    block: Remove used kblockd_schedule_work_on()
    
    Commit ee63cfa7fc19 ("block: add kblockd_schedule_work_on()")
    introduced the helper in 2016. Remove it because since then no caller
    was added.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Daniel Wagner <dwagner@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 10455b2bbbb4..f629d40c645c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1494,7 +1494,6 @@ static inline void put_dev_sector(Sector p)
 }
 
 int kblockd_schedule_work(struct work_struct *work);
-int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \

commit c780e86dd48ef6467a1146cf7d0fe1e05a635039
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 6 15:28:12 2020 +0100

    blktrace: Protect q->blk_trace with RCU
    
    KASAN is reporting that __blk_add_trace() has a use-after-free issue
    when accessing q->blk_trace. Indeed the switching of block tracing (and
    thus eventual freeing of q->blk_trace) is completely unsynchronized with
    the currently running tracing and thus it can happen that the blk_trace
    structure is being freed just while __blk_add_trace() works on it.
    Protect accesses to q->blk_trace by RCU during tracing and make sure we
    wait for the end of RCU grace period when shutting down tracing. Luckily
    that is rare enough event that we can afford that. Note that postponing
    the freeing of blk_trace to an RCU callback should better be avoided as
    it could have unexpected user visible side-effects as debugfs files
    would be still existing for a short while block tracing has been shut
    down.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=205711
    CC: stable@vger.kernel.org
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reported-by: Tristan Madani <tristmd@gmail.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 053ea4b51988..10455b2bbbb4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -524,7 +524,7 @@ struct request_queue {
 	unsigned int		sg_reserved_size;
 	int			node;
 #ifdef CONFIG_BLK_DEV_IO_TRACE
-	struct blk_trace	*blk_trace;
+	struct blk_trace __rcu	*blk_trace;
 	struct mutex		blk_trace_mutex;
 #endif
 	/*

commit 33c84e89abe4a92ab699c33029bd54269d574782
Merge: e9f8ca0ae7b7 11e673206f21
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 29 18:16:16 2020 -0800

    Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI updates from James Bottomley:
     "This series is slightly unusual because it includes Arnd's compat
      ioctl tree here:
    
        1c46a2cf2dbd Merge tag 'block-ioctl-cleanup-5.6' into 5.6/scsi-queue
    
      Excluding Arnd's changes, this is mostly an update of the usual
      drivers: megaraid_sas, mpt3sas, qla2xxx, ufs, lpfc, hisi_sas.
    
      There are a couple of core and base updates around error propagation
      and atomicity in the attribute container base we use for the SCSI
      transport classes.
    
      The rest is minor changes and updates"
    
    * tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (149 commits)
      scsi: hisi_sas: Rename hisi_sas_cq.pci_irq_mask
      scsi: hisi_sas: Add prints for v3 hw interrupt converge and automatic affinity
      scsi: hisi_sas: Modify the file permissions of trigger_dump to write only
      scsi: hisi_sas: Replace magic number when handle channel interrupt
      scsi: hisi_sas: replace spin_lock_irqsave/spin_unlock_restore with spin_lock/spin_unlock
      scsi: hisi_sas: use threaded irq to process CQ interrupts
      scsi: ufs: Use UFS device indicated maximum LU number
      scsi: ufs: Add max_lu_supported in struct ufs_dev_info
      scsi: ufs: Delete is_init_prefetch from struct ufs_hba
      scsi: ufs: Inline two functions into their callers
      scsi: ufs: Move ufshcd_get_max_pwr_mode() to ufshcd_device_params_init()
      scsi: ufs: Split ufshcd_probe_hba() based on its called flow
      scsi: ufs: Delete struct ufs_dev_desc
      scsi: ufs: Fix ufshcd_probe_hba() reture value in case ufshcd_scsi_add_wlus() fails
      scsi: ufs-mediatek: enable low-power mode for hibern8 state
      scsi: ufs: export some functions for vendor usage
      scsi: ufs-mediatek: add dbg_register_dump implementation
      scsi: qla2xxx: Fix a NULL pointer dereference in an error path
      scsi: qla1280: Make checking for 64bit support consistent
      scsi: megaraid_sas: Update driver version to 07.713.01.00-rc1
      ...

commit ad6bf88a6c19a39fb3b0045d78ea880325dfcf15
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Wed Jan 15 08:35:25 2020 -0500

    block: fix an integer overflow in logical block size
    
    Logical block size has type unsigned short. That means that it can be at
    most 32768. However, there are architectures that can run with 64k pages
    (for example arm64) and on these architectures, it may be possible to
    create block devices with 64k block size.
    
    For exmaple (run this on an architecture with 64k pages):
    
    Mount will fail with this error because it tries to read the superblock using 2-sector
    access:
      device-mapper: writecache: I/O is not aligned, sector 2, size 1024, block size 65536
      EXT4-fs (dm-0): unable to read superblock
    
    This patch changes the logical block size from unsigned short to unsigned
    int to avoid the overflow.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 47eb22a3b7f9..4c636c42ad68 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -328,6 +328,7 @@ struct queue_limits {
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
 	unsigned int		physical_block_size;
+	unsigned int		logical_block_size;
 	unsigned int		alignment_offset;
 	unsigned int		io_min;
 	unsigned int		io_opt;
@@ -338,7 +339,6 @@ struct queue_limits {
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;
 
-	unsigned short		logical_block_size;
 	unsigned short		max_segments;
 	unsigned short		max_integrity_segments;
 	unsigned short		max_discard_segments;
@@ -1077,7 +1077,7 @@ extern void blk_queue_max_write_same_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
 extern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
-extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
+extern void blk_queue_logical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
@@ -1291,7 +1291,7 @@ static inline unsigned int queue_max_segment_size(const struct request_queue *q)
 	return q->limits.max_segment_size;
 }
 
-static inline unsigned short queue_logical_block_size(const struct request_queue *q)
+static inline unsigned queue_logical_block_size(const struct request_queue *q)
 {
 	int retval = 512;
 
@@ -1301,7 +1301,7 @@ static inline unsigned short queue_logical_block_size(const struct request_queue
 	return retval;
 }
 
-static inline unsigned short bdev_logical_block_size(struct block_device *bdev)
+static inline unsigned int bdev_logical_block_size(struct block_device *bdev)
 {
 	return queue_logical_block_size(bdev_get_queue(bdev));
 }

commit ee6a129dffe10ae30b45b5a744e7c0a056291d13
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Nov 28 15:48:10 2019 +0100

    compat_ioctl: block: add blkdev_compat_ptr_ioctl
    
    A lot of block drivers need only a trivial .compat_ioctl callback.
    
    Add a helper function that can be set as the callback pointer
    to only convert the argument using the compat_ptr() conversion
    and otherwise assume all input and output data is compatible,
    or handled using in_compat_syscall() checks.
    
    This mirrors the compat_ptr_ioctl() helper function used in
    character devices.
    
    Reviewed-by: Ben Hutchings <ben.hutchings@codethink.co.uk>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 47eb22a3b7f9..3e0408618da7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1711,6 +1711,13 @@ struct block_device_operations {
 	const struct pr_ops *pr_ops;
 };
 
+#ifdef CONFIG_COMPAT
+extern int blkdev_compat_ptr_ioctl(struct block_device *, fmode_t,
+				      unsigned int, unsigned long);
+#else
+#define blkdev_compat_ptr_ioctl NULL
+#endif
+
 extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 				 unsigned long);
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);

commit ae58954d8734c44298f55ed71e683ea944994fab
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 3 10:39:07 2019 +0100

    block: don't handle bio based drivers in blk_revalidate_disk_zones
    
    bio based drivers only need to update q->nr_zones.  Do that manually
    instead of overloading blk_revalidate_disk_zones to keep that function
    simpler for the next round of changes that will rely even more on the
    request based functionality.
    
    Reviewed-by: Javier González <javier@javigon.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 503c4d4c5884..47eb22a3b7f9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -375,11 +375,6 @@ static inline unsigned int blkdev_nr_zones(struct gendisk *disk)
 	return 0;
 }
 
-static inline int blk_revalidate_disk_zones(struct gendisk *disk)
-{
-	return 0;
-}
-
 static inline int blkdev_report_zones_ioctl(struct block_device *bdev,
 					    fmode_t mode, unsigned int cmd,
 					    unsigned long arg)

commit f216fdd77b5654f8c4f6fac6020d6aabc58878ef
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 3 10:39:05 2019 +0100

    block: replace seq_zones_bitmap with conv_zones_bitmap
    
    Invert the meaning of seq_zones_bitmap by keeping a bitmap of
    conventional zones.  This allows not having a bitmap for devices
    that do not have conventional zones.
    
    Reviewed-by: Javier González <javier@javigon.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c5852de402b6..503c4d4c5884 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -503,9 +503,9 @@ struct request_queue {
 	/*
 	 * Zoned block device information for request dispatch control.
 	 * nr_zones is the total number of zones of the device. This is always
-	 * 0 for regular block devices. seq_zones_bitmap is a bitmap of nr_zones
-	 * bits which indicates if a zone is conventional (bit clear) or
-	 * sequential (bit set). seq_zones_wlock is a bitmap of nr_zones
+	 * 0 for regular block devices. conv_zones_bitmap is a bitmap of nr_zones
+	 * bits which indicates if a zone is conventional (bit set) or
+	 * sequential (bit clear). seq_zones_wlock is a bitmap of nr_zones
 	 * bits which indicates if a zone is write locked, that is, if a write
 	 * request targeting the zone was dispatched. All three fields are
 	 * initialized by the low level device driver (e.g. scsi/sd.c).
@@ -518,7 +518,7 @@ struct request_queue {
 	 * blk_mq_unfreeze_queue().
 	 */
 	unsigned int		nr_zones;
-	unsigned long		*seq_zones_bitmap;
+	unsigned long		*conv_zones_bitmap;
 	unsigned long		*seq_zones_wlock;
 #endif /* CONFIG_BLK_DEV_ZONED */
 
@@ -723,9 +723,11 @@ static inline unsigned int blk_queue_zone_no(struct request_queue *q,
 static inline bool blk_queue_zone_is_seq(struct request_queue *q,
 					 sector_t sector)
 {
-	if (!blk_queue_is_zoned(q) || !q->seq_zones_bitmap)
+	if (!blk_queue_is_zoned(q))
 		return false;
-	return test_bit(blk_queue_zone_no(q, sector), q->seq_zones_bitmap);
+	if (!q->conv_zones_bitmap)
+		return true;
+	return !test_bit(blk_queue_zone_no(q, sector), q->conv_zones_bitmap);
 }
 #else /* CONFIG_BLK_DEV_ZONED */
 static inline unsigned int blk_queue_nr_zones(struct request_queue *q)

commit 9b38bb4b1e6de47b379afaad2c707df639bb4dc7
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 3 10:39:04 2019 +0100

    block: simplify blkdev_nr_zones
    
    Simplify the arguments to blkdev_nr_zones by passing a gendisk instead
    of the block_device and capacity.  This also removes the need for
    __blkdev_nr_zones as all callers are outside the fast path and can
    deal with the additional branch.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6012e2592628..c5852de402b6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -357,8 +357,7 @@ typedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,
 #define BLK_ALL_ZONES  ((unsigned int)-1)
 int blkdev_report_zones(struct block_device *bdev, sector_t sector,
 			unsigned int nr_zones, report_zones_cb cb, void *data);
-
-extern unsigned int blkdev_nr_zones(struct block_device *bdev);
+unsigned int blkdev_nr_zones(struct gendisk *disk);
 extern int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 			    sector_t sectors, sector_t nr_sectors,
 			    gfp_t gfp_mask);
@@ -371,7 +370,7 @@ extern int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
 
 #else /* CONFIG_BLK_DEV_ZONED */
 
-static inline unsigned int blkdev_nr_zones(struct block_device *bdev)
+static inline unsigned int blkdev_nr_zones(struct gendisk *disk)
 {
 	return 0;
 }

commit 0da522107e5d9c000a4871d52e570912aa1225a2
Merge: ad0b314e0030 142b2ac82e31
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 1 13:46:15 2019 -0800

    Merge tag 'compat-ioctl-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground
    
    Pull removal of most of fs/compat_ioctl.c from Arnd Bergmann:
     "As part of the cleanup of some remaining y2038 issues, I came to
      fs/compat_ioctl.c, which still has a couple of commands that need
      support for time64_t.
    
      In completely unrelated work, I spent time on cleaning up parts of
      this file in the past, moving things out into drivers instead.
    
      After Al Viro reviewed an earlier version of this series and did a lot
      more of that cleanup, I decided to try to completely eliminate the
      rest of it and move it all into drivers.
    
      This series incorporates some of Al's work and many patches of my own,
      but in the end stops short of actually removing the last part, which
      is the scsi ioctl handlers. I have patches for those as well, but they
      need more testing or possibly a rewrite"
    
    * tag 'compat-ioctl-5.5' of git://git.kernel.org:/pub/scm/linux/kernel/git/arnd/playground: (42 commits)
      scsi: sd: enable compat ioctls for sed-opal
      pktcdvd: add compat_ioctl handler
      compat_ioctl: move SG_GET_REQUEST_TABLE handling
      compat_ioctl: ppp: move simple commands into ppp_generic.c
      compat_ioctl: handle PPPIOCGIDLE for 64-bit time_t
      compat_ioctl: move PPPIOCSCOMPRESS to ppp_generic
      compat_ioctl: unify copy-in of ppp filters
      tty: handle compat PPP ioctls
      compat_ioctl: move SIOCOUTQ out of compat_ioctl.c
      compat_ioctl: handle SIOCOUTQNSD
      af_unix: add compat_ioctl support
      compat_ioctl: reimplement SG_IO handling
      compat_ioctl: move WDIOC handling into wdt drivers
      fs: compat_ioctl: move FITRIM emulation into file systems
      gfs2: add compat_ioctl support
      compat_ioctl: remove unused convert_in_user macro
      compat_ioctl: remove last RAID handling code
      compat_ioctl: remove /dev/raw ioctl translation
      compat_ioctl: remove PCI ioctl translation
      compat_ioctl: remove joystick ioctl translation
      ...

commit d41003513e61dd9d4974cb441d30b63650b85654
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Nov 11 11:39:30 2019 +0900

    block: rework zone reporting
    
    Avoid the need to allocate a potentially large array of struct blk_zone
    in the block layer by switching the ->report_zones method interface to
    a callback model. Now the caller simply supplies a callback that is
    executed on each reported zone, and private data for it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Shin'ichiro Kawasaki <shinichiro.kawasaki@wdc.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6a4f7abbdcf7..397bb9bc230b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -349,17 +349,16 @@ struct queue_limits {
 	enum blk_zoned_model	zoned;
 };
 
+typedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,
+			       void *data);
+
 #ifdef CONFIG_BLK_DEV_ZONED
 
-/*
- * Maximum number of zones to report with a single report zones command.
- */
-#define BLK_ZONED_REPORT_MAX_ZONES	8192U
+#define BLK_ALL_ZONES  ((unsigned int)-1)
+int blkdev_report_zones(struct block_device *bdev, sector_t sector,
+			unsigned int nr_zones, report_zones_cb cb, void *data);
 
 extern unsigned int blkdev_nr_zones(struct block_device *bdev);
-extern int blkdev_report_zones(struct block_device *bdev,
-			       sector_t sector, struct blk_zone *zones,
-			       unsigned int *nr_zones);
 extern int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
 			    sector_t sectors, sector_t nr_sectors,
 			    gfp_t gfp_mask);
@@ -1709,7 +1708,7 @@ struct block_device_operations {
 	/* this callback is with swap_lock and sometimes page table lock held */
 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 	int (*report_zones)(struct gendisk *, sector_t sector,
-			    struct blk_zone *zones, unsigned int *nr_zones);
+			unsigned int nr_zones, report_zones_cb cb, void *data);
 	struct module *owner;
 	const struct pr_ops *pr_ops;
 };

commit e876df1fe0ad1b191284ee6ed2db7960bd322d00
Author: Ajay Joshi <ajay.joshi@wdc.com>
Date:   Sun Oct 27 23:05:46 2019 +0900

    block: add zone open, close and finish ioctl support
    
    Introduce three new ioctl commands BLKOPENZONE, BLKCLOSEZONE and
    BLKFINISHZONE to allow applications to control the condition of zones
    on a zoned block device through the execution of the REQ_OP_ZONE_OPEN,
    REQ_OP_ZONE_CLOSE and REQ_OP_ZONE_FINISH operations.
    
    Contains contributions from Matias Bjorling, Hans Holmberg,
    Dmitry Fomichev, Keith Busch, Damien Le Moal and Christoph Hellwig.
    
    Reviewed-by: Javier González <javier@javigon.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ajay Joshi <ajay.joshi@wdc.com>
    Signed-off-by: Matias Bjorling <matias.bjorling@wdc.com>
    Signed-off-by: Hans Holmberg <hans.holmberg@wdc.com>
    Signed-off-by: Dmitry Fomichev <dmitry.fomichev@wdc.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9cfafff86f66..6a4f7abbdcf7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -367,8 +367,8 @@ extern int blk_revalidate_disk_zones(struct gendisk *disk);
 
 extern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 				     unsigned int cmd, unsigned long arg);
-extern int blkdev_reset_zones_ioctl(struct block_device *bdev, fmode_t mode,
-				    unsigned int cmd, unsigned long arg);
+extern int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,
+				  unsigned int cmd, unsigned long arg);
 
 #else /* CONFIG_BLK_DEV_ZONED */
 
@@ -389,9 +389,9 @@ static inline int blkdev_report_zones_ioctl(struct block_device *bdev,
 	return -ENOTTY;
 }
 
-static inline int blkdev_reset_zones_ioctl(struct block_device *bdev,
-					   fmode_t mode, unsigned int cmd,
-					   unsigned long arg)
+static inline int blkdev_zone_mgmt_ioctl(struct block_device *bdev,
+					 fmode_t mode, unsigned int cmd,
+					 unsigned long arg)
 {
 	return -ENOTTY;
 }

commit 6c1b1da58f8c7a697a88ae35afeba196fc7b701e
Author: Ajay Joshi <ajay.joshi@wdc.com>
Date:   Sun Oct 27 23:05:45 2019 +0900

    block: add zone open, close and finish operations
    
    Zoned block devices (ZBC and ZAC devices) allow an explicit control
    over the condition (state) of zones. The operations allowed are:
    * Open a zone: Transition to open condition to indicate that a zone will
      actively be written
    * Close a zone: Transition to closed condition to release the drive
      resources used for writing to a zone
    * Finish a zone: Transition an open or closed zone to the full
      condition to prevent write operations
    
    To enable this control for in-kernel zoned block device users, define
    the new request operations REQ_OP_ZONE_OPEN, REQ_OP_ZONE_CLOSE
    and REQ_OP_ZONE_FINISH as well as the generic function
    blkdev_zone_mgmt() for submitting these operations on a range of zones.
    This results in blkdev_reset_zones() removal and replacement with this
    new zone magement function. Users of blkdev_reset_zones() (f2fs and
    dm-zoned) are updated accordingly.
    
    Contains contributions from Matias Bjorling, Hans Holmberg,
    Dmitry Fomichev, Keith Busch, Damien Le Moal and Christoph Hellwig.
    
    Reviewed-by: Javier González <javier@javigon.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ajay Joshi <ajay.joshi@wdc.com>
    Signed-off-by: Matias Bjorling <matias.bjorling@wdc.com>
    Signed-off-by: Hans Holmberg <hans.holmberg@wdc.com>
    Signed-off-by: Dmitry Fomichev <dmitry.fomichev@wdc.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4051acb92a1..9cfafff86f66 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -360,8 +360,9 @@ extern unsigned int blkdev_nr_zones(struct block_device *bdev);
 extern int blkdev_report_zones(struct block_device *bdev,
 			       sector_t sector, struct blk_zone *zones,
 			       unsigned int *nr_zones);
-extern int blkdev_reset_zones(struct block_device *bdev, sector_t sectors,
-			      sector_t nr_sectors, gfp_t gfp_mask);
+extern int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,
+			    sector_t sectors, sector_t nr_sectors,
+			    gfp_t gfp_mask);
 extern int blk_revalidate_disk_zones(struct gendisk *disk);
 
 extern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,

commit 98aaaec4a150c39219a8aaa68c3adc6eed443ea8
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Mar 14 17:45:18 2019 +0100

    compat_ioctl: reimplement SG_IO handling
    
    There are two code locations that implement the SG_IO ioctl: the old
    sg.c driver, and the generic scsi_ioctl helper that is in turn used by
    multiple drivers.
    
    To eradicate the old compat_ioctl conversion handler for the SG_IO
    command, I implement a readable pair of put_sg_io_hdr() /get_sg_io_hdr()
    helper functions that can be used for both compat and native mode,
    and then I call this from both drivers.
    
    For the iovec handling, there is already a compat_import_iovec() function
    that can simply be called in place of import_iovec().
    
    To avoid having to pass the compat/native state through multiple
    indirections, I mark the SG_IO command itself as compatible in
    fs/compat_ioctl.c and use in_compat_syscall() to figure out where
    we are called from.
    
    As a side-effect of this, the sg.c driver now also accepts the 32-bit
    sg_io_hdr format in compat mode using the read/write interface, not
    just ioctl. This should improve compatiblity with old 32-bit binaries,
    but it would break if any application intentionally passes the 64-bit
    data structure in compat mode here.
    
    Steffen Maier helped debug an issue in an earlier version of this patch.
    
    Cc: Steffen Maier <maier@linux.ibm.com>
    Cc: linux-scsi@vger.kernel.org
    Cc: Doug Gilbert <dgilbert@interlog.com>
    Cc: "James E.J. Bottomley" <jejb@linux.ibm.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3ea78b0c91c..2c8cd22b176b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -870,6 +870,8 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			  unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
+extern int get_sg_io_hdr(struct sg_io_hdr *hdr, const void __user *argp);
+extern int put_sg_io_hdr(const struct sg_io_hdr *hdr, void __user *argp);
 
 extern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);
 extern void blk_queue_exit(struct request_queue *q);

commit 9566256518de0520c964bdf23140eac324b136af
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Sep 30 16:00:42 2019 -0700

    block: Remove request_queue.nr_queues
    
    Commit 897bb0c7f1ea ("blk-mq: Use proper cpumask iterator"; v4.6)
    removed the last use of request_queue.nr_queues from outside
    blk_mq_init_allocate_queue(). Remove this member variable to make
    struct request_queue smaller. This patch does not change any
    functionality.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3ea78b0c91c..d4051acb92a1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -411,7 +411,6 @@ struct request_queue {
 
 	/* sw queues */
 	struct blk_mq_ctx __percpu	*queue_ctx;
-	unsigned int		nr_queues;
 
 	unsigned int		queue_depth;
 

commit 2e959dd87a9f58f1ad824d830e06388c9e328239
Merge: 9c9fa97a8edb d46fe2cb2dce
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 24 16:31:50 2019 -0700

    Merge tag 'for-5.4/post-2019-09-24' of git://git.kernel.dk/linux-block
    
    Pull more block updates from Jens Axboe:
     "Some later additions that weren't quite done for the first pull
      request, and also a few fixes that have arrived since.
    
      This contains:
    
       - Kill silly pktcdvd warning on attempting to register a non-scsi
         passthrough device (me)
    
       - Use symbolic constants for the block t10 protection types, and
         switch to handling it in core rather than in the drivers (Max)
    
       - libahci platform missing node put fix (Nishka)
    
       - Small series of fixes for BFQ (Paolo)
    
       - Fix possible nbd crash (Xiubo)"
    
    * tag 'for-5.4/post-2019-09-24' of git://git.kernel.dk/linux-block:
      block: drop device references in bsg_queue_rq()
      block: t10-pi: fix -Wswitch warning
      pktcdvd: remove warning on attempting to register non-passthrough dev
      ata: libahci_platform: Add of_node_put() before loop exit
      nbd: fix possible page fault for nbd disk
      nbd: rename the runtime flags as NBD_RT_ prefixed
      block, bfq: push up injection only after setting service time
      block, bfq: increase update frequency of inject limit
      block, bfq: reduce upper bound for inject limit to max_rq_in_driver+1
      block, bfq: update inject limit only after injection occurred
      block: centralize PI remapping logic to the block layer
      block: use symbolic constants for t10_pi type

commit 671df189537883f36cf9c7d4f9495bfac0f86627
Merge: c9fe5630dae1 c7d9eccb3c1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 19 13:27:23 2019 -0700

    Merge tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - add dma-mapping and block layer helpers to take care of IOMMU merging
       for mmc plus subsequent fixups (Yoshihiro Shimoda)
    
     - rework handling of the pgprot bits for remapping (me)
    
     - take care of the dma direct infrastructure for swiotlb-xen (me)
    
     - improve the dma noncoherent remapping infrastructure (me)
    
     - better defaults for ->mmap, ->get_sgtable and ->get_required_mask
       (me)
    
     - cleanup mmaping of coherent DMA allocations (me)
    
     - various misc cleanups (Andy Shevchenko, me)
    
    * tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping: (41 commits)
      mmc: renesas_sdhi_internal_dmac: Add MMC_CAP2_MERGE_CAPABLE
      mmc: queue: Fix bigger segments usage
      arm64: use asm-generic/dma-mapping.h
      swiotlb-xen: merge xen_unmap_single into xen_swiotlb_unmap_page
      swiotlb-xen: simplify cache maintainance
      swiotlb-xen: use the same foreign page check everywhere
      swiotlb-xen: remove xen_swiotlb_dma_mmap and xen_swiotlb_dma_get_sgtable
      xen: remove the exports for xen_{create,destroy}_contiguous_region
      xen/arm: remove xen_dma_ops
      xen/arm: simplify dma_cache_maint
      xen/arm: use dev_is_dma_coherent
      xen/arm: consolidate page-coherent.h
      xen/arm: use dma-noncoherent.h calls for xen-swiotlb cache maintainance
      arm: remove wrappers for the generic dma remap helpers
      dma-mapping: introduce a dma_common_find_pages helper
      dma-mapping: always use VM_DMA_COHERENT for generic DMA remap
      vmalloc: lift the arm flag for coherent mappings to common code
      dma-mapping: provide a better default ->get_required_mask
      dma-mapping: remove the dma_declare_coherent_memory export
      remoteproc: don't allow modular build
      ...

commit 54d4e6ab91eb24b47a58403d8561206e916f0242
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Mon Sep 16 18:44:29 2019 +0300

    block: centralize PI remapping logic to the block layer
    
    Currently t10_pi_prepare/t10_pi_complete functions are called during the
    NVMe and SCSi layers command preparetion/completion, but their actual
    place should be the block layer since T10-PI is a general data integrity
    feature that is used by block storage protocols. Introduce .prepare_fn
    and .complete_fn callbacks within the integrity profile that each type
    can implement according to its needs.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Suggested-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    
    Fixed to not call queue integrity functions if BLK_DEV_INTEGRITY
    isn't defined in the config.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3094f2d513b2..6032bb740cf4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1522,10 +1522,14 @@ struct blk_integrity_iter {
 };
 
 typedef blk_status_t (integrity_processing_fn) (struct blk_integrity_iter *);
+typedef void (integrity_prepare_fn) (struct request *);
+typedef void (integrity_complete_fn) (struct request *, unsigned int);
 
 struct blk_integrity_profile {
 	integrity_processing_fn		*generate_fn;
 	integrity_processing_fn		*verify_fn;
+	integrity_prepare_fn		*prepare_fn;
+	integrity_complete_fn		*complete_fn;
 	const char			*name;
 };
 

commit 3d24430694077313c75c6b89f618db09943621e4
Author: Hou Tao <houtao1@huawei.com>
Date:   Tue May 21 15:59:03 2019 +0800

    block: make rq sector size accessible for block stats
    
    Currently rq->data_len will be decreased by partial completion or
    zeroed by completion, so when blk_stat_add() is invoked, data_len
    will be zero and there will never be samples in poll_cb because
    blk_mq_poll_stats_bkt() will return -1 if data_len is zero.
    
    We could move blk_stat_add() back to __blk_mq_complete_request(),
    but that would make the effort of trying to call ktime_get_ns()
    once in vain. Instead we can reuse throtl_size field, and use
    it for both block stats and block throttle, and adjust the
    logic in blk_mq_poll_stats_bkt() accordingly.
    
    Fixes: 4bc6339a583c ("block: move blk_stat_add() to __blk_mq_end_request()")
    Tested-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Hou Tao <houtao1@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b196124e3240..3094f2d513b2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -206,9 +206,12 @@ struct request {
 #ifdef CONFIG_BLK_WBT
 	unsigned short wbt_flags;
 #endif
-#ifdef CONFIG_BLK_DEV_THROTTLING_LOW
-	unsigned short throtl_size;
-#endif
+	/*
+	 * rq sectors used for blk stats. It has the same value
+	 * with blk_rq_sectors(rq), except that it never be zeroed
+	 * by completion.
+	 */
+	unsigned short stats_sectors;
 
 	/*
 	 * Number of scatter-gather DMA addr+len pairs after
@@ -917,6 +920,7 @@ static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
  * blk_rq_err_bytes()		: bytes left till the next error boundary
  * blk_rq_sectors()		: sectors left in the entire request
  * blk_rq_cur_sectors()		: sectors left in the current segment
+ * blk_rq_stats_sectors()	: sectors of the entire request used for stats
  */
 static inline sector_t blk_rq_pos(const struct request *rq)
 {
@@ -945,6 +949,11 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> SECTOR_SHIFT;
 }
 
+static inline unsigned int blk_rq_stats_sectors(const struct request *rq)
+{
+	return rq->stats_sectors;
+}
+
 #ifdef CONFIG_BLK_DEV_ZONED
 static inline unsigned int blk_rq_zone_no(struct request *rq)
 {

commit 68c43f133a754c7bf5cb1018bb16dc0821cc43a1
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Thu Sep 5 18:51:31 2019 +0900

    block: Introduce elevator features
    
    Introduce the definition of elevator features through the
    elevator_features flags in the elevator_type structure. Each flag can
    represent a feature supported by an elevator. The first feature defined
    by this patch is support for zoned block device sequential write
    constraint with the flag ELEVATOR_F_ZBD_SEQ_WRITE, which is implemented
    by the mq-deadline elevator using zone write locking.
    
    Other possible features are IO priorities, write hints, latency targets
    or single-LUN dual-actuator disks (for which the elevator could maintain
    one LBA ordered list per actuator).
    
    The required_elevator_features field is also added to the request_queue
    structure to allow a device driver to specify elevator feature flags
    that an elevator must support for the correct operation of the device
    (e.g. device drivers for zoned block devices can have the
    ELEVATOR_F_ZBD_SEQ_WRITE flag as a required feature).
    The helper function blk_queue_required_elevator_features() is
    defined for setting this new field.
    
    With these two new fields in place, the elevator functions
    elevator_match() and elevator_find() are modified to allow a user to set
    only an elevator with a set of features that satisfies the device
    required features. Elevators not matching the device requirements are
    not shown in the device sysfs queue/scheduler file to prevent their use.
    
    The "none" elevator can always be selected as before.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d0ad21e4771b..b196124e3240 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -496,6 +496,8 @@ struct request_queue {
 
 	struct queue_limits	limits;
 
+	unsigned int		required_elevator_features;
+
 #ifdef CONFIG_BLK_DEV_ZONED
 	/*
 	 * Zoned block device information for request dispatch control.
@@ -1097,6 +1099,8 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
+extern void blk_queue_required_elevator_features(struct request_queue *q,
+						 unsigned int features);
 
 /*
  * Number of physical segments as sent to the device.

commit 45147fb522bb459e79bdcb7504ee7ec8cfd4c12c
Author: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
Date:   Wed Aug 28 21:35:42 2019 +0900

    block: add a helper function to merge the segments
    
    This patch adds a helper function whether a queue can merge
    the segments by the DMA MAP layer (e.g. via IOMMU).
    
    Signed-off-by: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Simon Horman <horms+renesas@verge.net.au
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375dafb1c..f6d55e2490dc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1085,6 +1085,8 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
+extern bool blk_queue_can_use_dma_map_merging(struct request_queue *q,
+					      struct device *dev);
 
 /*
  * Number of physical segments as sent to the device.

commit 6f816b4b746c2241540e537682d30d8e9997d674
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Aug 28 15:05:57 2019 -0700

    blk-mq: add optional request->alloc_time_ns
    
    There are currently two start time timestamps - start_time_ns and
    io_start_time_ns.  The former marks the request allocation and and the
    second issue-to-device time.  The planned io.weight controller needs
    to measure the total time bios take to execute after it leaves rq_qos
    including the time spent waiting for request to become available,
    which can easily dominate on saturated devices.
    
    This patch adds request->alloc_time_ns which records when the request
    allocation attempt started.  As it isn't used for the usual stats,
    make it optional behind CONFIG_BLK_RQ_ALLOC_TIME and
    QUEUE_FLAG_RQ_ALLOC_TIME so that it can be compiled out when there are
    no users and it's active only on queues which need it even when
    compiled in.
    
    v2: s/pre_start_time/alloc_time/ and add CONFIG_BLK_RQ_ALLOC_TIME
        gating as suggested by Jens.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ac790178787..d0ad21e4771b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -194,7 +194,11 @@ struct request {
 
 	struct gendisk *rq_disk;
 	struct hd_struct *part;
-	/* Time that I/O was submitted to the kernel. */
+#ifdef CONFIG_BLK_RQ_ALLOC_TIME
+	/* Time that the first bio started allocating this request. */
+	u64 alloc_time_ns;
+#endif
+	/* Time that this request was allocated for this IO. */
 	u64 start_time_ns;
 	/* Time that I/O was submitted to the device. */
 	u64 io_start_time_ns;
@@ -609,6 +613,7 @@ struct request_queue {
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
+#define QUEUE_FLAG_RQ_ALLOC_TIME 27	/* record rq->alloc_time_ns */
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))
@@ -637,6 +642,12 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 	test_bit(QUEUE_FLAG_SCSI_PASSTHROUGH, &(q)->queue_flags)
 #define blk_queue_pci_p2pdma(q)	\
 	test_bit(QUEUE_FLAG_PCI_P2PDMA, &(q)->queue_flags)
+#ifdef CONFIG_BLK_RQ_ALLOC_TIME
+#define blk_queue_rq_alloc_time(q)	\
+	test_bit(QUEUE_FLAG_RQ_ALLOC_TIME, &(q)->queue_flags)
+#else
+#define blk_queue_rq_alloc_time(q)	false
+#endif
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \

commit cecf5d87ff2035127bb5a9ee054d0023a4a7cad3
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Aug 27 19:01:48 2019 +0800

    block: split .sysfs_lock into two locks
    
    The kernfs built-in lock of 'kn->count' is held in sysfs .show/.store
    path. Meantime, inside block's .show/.store callback, q->sysfs_lock is
    required.
    
    However, when mq & iosched kobjects are removed via
    blk_mq_unregister_dev() & elv_unregister_queue(), q->sysfs_lock is held
    too. This way causes AB-BA lock because the kernfs built-in lock of
    'kn-count' is required inside kobject_del() too, see the lockdep warning[1].
    
    On the other hand, it isn't necessary to acquire q->sysfs_lock for
    both blk_mq_unregister_dev() & elv_unregister_queue() because
    clearing REGISTERED flag prevents storing to 'queue/scheduler'
    from being happened. Also sysfs write(store) is exclusive, so no
    necessary to hold the lock for elv_unregister_queue() when it is
    called in switching elevator path.
    
    So split .sysfs_lock into two: one is still named as .sysfs_lock for
    covering sync .store, the other one is named as .sysfs_dir_lock
    for covering kobjects and related status change.
    
    sysfs itself can handle the race between add/remove kobjects and
    showing/storing attributes under kobjects. For switching scheduler
    via storing to 'queue/scheduler', we use the queue flag of
    QUEUE_FLAG_REGISTERED with .sysfs_lock for avoiding the race, then
    we can avoid to hold .sysfs_lock during removing/adding kobjects.
    
    [1]  lockdep warning
        ======================================================
        WARNING: possible circular locking dependency detected
        5.3.0-rc3-00044-g73277fc75ea0 #1380 Not tainted
        ------------------------------------------------------
        rmmod/777 is trying to acquire lock:
        00000000ac50e981 (kn->count#202){++++}, at: kernfs_remove_by_name_ns+0x59/0x72
    
        but task is already holding lock:
        00000000fb16ae21 (&q->sysfs_lock){+.+.}, at: blk_unregister_queue+0x78/0x10b
    
        which lock already depends on the new lock.
    
        the existing dependency chain (in reverse order) is:
    
        -> #1 (&q->sysfs_lock){+.+.}:
               __lock_acquire+0x95f/0xa2f
               lock_acquire+0x1b4/0x1e8
               __mutex_lock+0x14a/0xa9b
               blk_mq_hw_sysfs_show+0x63/0xb6
               sysfs_kf_seq_show+0x11f/0x196
               seq_read+0x2cd/0x5f2
               vfs_read+0xc7/0x18c
               ksys_read+0xc4/0x13e
               do_syscall_64+0xa7/0x295
               entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
        -> #0 (kn->count#202){++++}:
               check_prev_add+0x5d2/0xc45
               validate_chain+0xed3/0xf94
               __lock_acquire+0x95f/0xa2f
               lock_acquire+0x1b4/0x1e8
               __kernfs_remove+0x237/0x40b
               kernfs_remove_by_name_ns+0x59/0x72
               remove_files+0x61/0x96
               sysfs_remove_group+0x81/0xa4
               sysfs_remove_groups+0x3b/0x44
               kobject_del+0x44/0x94
               blk_mq_unregister_dev+0x83/0xdd
               blk_unregister_queue+0xa0/0x10b
               del_gendisk+0x259/0x3fa
               null_del_dev+0x8b/0x1c3 [null_blk]
               null_exit+0x5c/0x95 [null_blk]
               __se_sys_delete_module+0x204/0x337
               do_syscall_64+0xa7/0x295
               entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
        other info that might help us debug this:
    
         Possible unsafe locking scenario:
    
               CPU0                    CPU1
               ----                    ----
          lock(&q->sysfs_lock);
                                       lock(kn->count#202);
                                       lock(&q->sysfs_lock);
          lock(kn->count#202);
    
         *** DEADLOCK ***
    
        2 locks held by rmmod/777:
         #0: 00000000e69bd9de (&lock){+.+.}, at: null_exit+0x2e/0x95 [null_blk]
         #1: 00000000fb16ae21 (&q->sysfs_lock){+.+.}, at: blk_unregister_queue+0x78/0x10b
    
        stack backtrace:
        CPU: 0 PID: 777 Comm: rmmod Not tainted 5.3.0-rc3-00044-g73277fc75ea0 #1380
        Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS ?-20180724_192412-buildhw-07.phx4
        Call Trace:
         dump_stack+0x9a/0xe6
         check_noncircular+0x207/0x251
         ? print_circular_bug+0x32a/0x32a
         ? find_usage_backwards+0x84/0xb0
         check_prev_add+0x5d2/0xc45
         validate_chain+0xed3/0xf94
         ? check_prev_add+0xc45/0xc45
         ? mark_lock+0x11b/0x804
         ? check_usage_forwards+0x1ca/0x1ca
         __lock_acquire+0x95f/0xa2f
         lock_acquire+0x1b4/0x1e8
         ? kernfs_remove_by_name_ns+0x59/0x72
         __kernfs_remove+0x237/0x40b
         ? kernfs_remove_by_name_ns+0x59/0x72
         ? kernfs_next_descendant_post+0x7d/0x7d
         ? strlen+0x10/0x23
         ? strcmp+0x22/0x44
         kernfs_remove_by_name_ns+0x59/0x72
         remove_files+0x61/0x96
         sysfs_remove_group+0x81/0xa4
         sysfs_remove_groups+0x3b/0x44
         kobject_del+0x44/0x94
         blk_mq_unregister_dev+0x83/0xdd
         blk_unregister_queue+0xa0/0x10b
         del_gendisk+0x259/0x3fa
         ? disk_events_poll_msecs_store+0x12b/0x12b
         ? check_flags+0x1ea/0x204
         ? mark_held_locks+0x1f/0x7a
         null_del_dev+0x8b/0x1c3 [null_blk]
         null_exit+0x5c/0x95 [null_blk]
         __se_sys_delete_module+0x204/0x337
         ? free_module+0x39f/0x39f
         ? blkcg_maybe_throttle_current+0x8a/0x718
         ? rwlock_bug+0x62/0x62
         ? __blkcg_punt_bio_submit+0xd0/0xd0
         ? trace_hardirqs_on_thunk+0x1a/0x20
         ? mark_held_locks+0x1f/0x7a
         ? do_syscall_64+0x4c/0x295
         do_syscall_64+0xa7/0x295
         entry_SYSCALL_64_after_hwframe+0x49/0xbe
        RIP: 0033:0x7fb696cdbe6b
        Code: 73 01 c3 48 8b 0d 1d 20 0c 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 008
        RSP: 002b:00007ffec9588788 EFLAGS: 00000206 ORIG_RAX: 00000000000000b0
        RAX: ffffffffffffffda RBX: 0000559e589137c0 RCX: 00007fb696cdbe6b
        RDX: 000000000000000a RSI: 0000000000000800 RDI: 0000559e58913828
        RBP: 0000000000000000 R08: 00007ffec9587701 R09: 0000000000000000
        R10: 00007fb696d4eae0 R11: 0000000000000206 R12: 00007ffec95889b0
        R13: 00007ffec95896b3 R14: 0000559e58913260 R15: 0000559e589137c0
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d5077f3fdfd6..1ac790178787 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -535,6 +535,7 @@ struct request_queue {
 	struct delayed_work	requeue_work;
 
 	struct mutex		sysfs_lock;
+	struct mutex		sysfs_dir_lock;
 
 	/*
 	 * for reusing dead hctx instance in case of updating

commit 58c898ba370e68d39470cd0d932b524682c1f9be
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Aug 27 19:01:47 2019 +0800

    block: add helper for checking if queue is registered
    
    There are 4 users which check if queue is registered, so add one helper
    to check it.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4798bb25f1ee..d5077f3fdfd6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -643,6 +643,7 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
+#define blk_queue_registered(q)	test_bit(QUEUE_FLAG_REGISTERED, &(q)->queue_flags)
 
 extern void blk_set_pm_only(struct request_queue *q);
 extern void blk_clear_pm_only(struct request_queue *q);

commit 988721db93b2f5e6477cb0ea0b64ba9bcfd67778
Author: Junxiao Bi <junxiao.bi@oracle.com>
Date:   Fri Aug 16 14:12:33 2019 -0700

    block: remove struct request_queue queue_head
    
    The dispatch list is not used any more, as the legacy block IO stack
    has been removed.
    
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 167bf879f072..4798bb25f1ee 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -391,10 +391,6 @@ static inline int blkdev_reset_zones_ioctl(struct block_device *bdev,
 #endif /* CONFIG_BLK_DEV_ZONED */
 
 struct request_queue {
-	/*
-	 * Together with queue_head for cacheline sharing
-	 */
-	struct list_head	queue_head;
 	struct request		*last_merge;
 	struct elevator_queue	*elevator;
 

commit e84e8f0663956f45c747df5629046794cff93893
Author: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date:   Thu Aug 1 10:26:35 2019 -0700

    block: add req op to reset all zones and flag
    
    This patch introduces a new request operation REQ_OP_ZONE_RESET_ALL.
    This is useful for the applications like mkfs where it needs to reset
    all the zones present on the underlying block device. As part for this
    patch we also introduce new QUEUE_FLAG_ZONE_RESETALL which indicates the
    queue zone reset all capability and corresponding helper macro.
    
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 96a29a72fd4a..167bf879f072 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -611,6 +611,7 @@ struct request_queue {
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
+#define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))
@@ -630,6 +631,8 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
 #define blk_queue_add_random(q)	test_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
+#define blk_queue_zone_resetall(q)	\
+	test_bit(QUEUE_FLAG_ZONE_RESETALL, &(q)->queue_flags)
 #define blk_queue_secure_erase(q) \
 	(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))
 #define blk_queue_dax(q)	test_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)

commit af2c68fe94e8c0a628519b60ba070c5cf6526a99
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Aug 1 15:50:40 2019 -0700

    block: Declare several function pointer arguments 'const'
    
    Make it clear to the compiler and also to humans that the functions
    that query request queue properties do not modify any member of the
    request_queue data structure.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375dafb1c..96a29a72fd4a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1232,42 +1232,42 @@ enum blk_default_limits {
 	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
 };
 
-static inline unsigned long queue_segment_boundary(struct request_queue *q)
+static inline unsigned long queue_segment_boundary(const struct request_queue *q)
 {
 	return q->limits.seg_boundary_mask;
 }
 
-static inline unsigned long queue_virt_boundary(struct request_queue *q)
+static inline unsigned long queue_virt_boundary(const struct request_queue *q)
 {
 	return q->limits.virt_boundary_mask;
 }
 
-static inline unsigned int queue_max_sectors(struct request_queue *q)
+static inline unsigned int queue_max_sectors(const struct request_queue *q)
 {
 	return q->limits.max_sectors;
 }
 
-static inline unsigned int queue_max_hw_sectors(struct request_queue *q)
+static inline unsigned int queue_max_hw_sectors(const struct request_queue *q)
 {
 	return q->limits.max_hw_sectors;
 }
 
-static inline unsigned short queue_max_segments(struct request_queue *q)
+static inline unsigned short queue_max_segments(const struct request_queue *q)
 {
 	return q->limits.max_segments;
 }
 
-static inline unsigned short queue_max_discard_segments(struct request_queue *q)
+static inline unsigned short queue_max_discard_segments(const struct request_queue *q)
 {
 	return q->limits.max_discard_segments;
 }
 
-static inline unsigned int queue_max_segment_size(struct request_queue *q)
+static inline unsigned int queue_max_segment_size(const struct request_queue *q)
 {
 	return q->limits.max_segment_size;
 }
 
-static inline unsigned short queue_logical_block_size(struct request_queue *q)
+static inline unsigned short queue_logical_block_size(const struct request_queue *q)
 {
 	int retval = 512;
 
@@ -1282,7 +1282,7 @@ static inline unsigned short bdev_logical_block_size(struct block_device *bdev)
 	return queue_logical_block_size(bdev_get_queue(bdev));
 }
 
-static inline unsigned int queue_physical_block_size(struct request_queue *q)
+static inline unsigned int queue_physical_block_size(const struct request_queue *q)
 {
 	return q->limits.physical_block_size;
 }
@@ -1292,7 +1292,7 @@ static inline unsigned int bdev_physical_block_size(struct block_device *bdev)
 	return queue_physical_block_size(bdev_get_queue(bdev));
 }
 
-static inline unsigned int queue_io_min(struct request_queue *q)
+static inline unsigned int queue_io_min(const struct request_queue *q)
 {
 	return q->limits.io_min;
 }
@@ -1302,7 +1302,7 @@ static inline int bdev_io_min(struct block_device *bdev)
 	return queue_io_min(bdev_get_queue(bdev));
 }
 
-static inline unsigned int queue_io_opt(struct request_queue *q)
+static inline unsigned int queue_io_opt(const struct request_queue *q)
 {
 	return q->limits.io_opt;
 }
@@ -1312,7 +1312,7 @@ static inline int bdev_io_opt(struct block_device *bdev)
 	return queue_io_opt(bdev_get_queue(bdev));
 }
 
-static inline int queue_alignment_offset(struct request_queue *q)
+static inline int queue_alignment_offset(const struct request_queue *q)
 {
 	if (q->limits.misaligned)
 		return -1;
@@ -1342,7 +1342,7 @@ static inline int bdev_alignment_offset(struct block_device *bdev)
 	return q->limits.alignment_offset;
 }
 
-static inline int queue_discard_alignment(struct request_queue *q)
+static inline int queue_discard_alignment(const struct request_queue *q)
 {
 	if (q->limits.discard_misaligned)
 		return -1;
@@ -1432,7 +1432,7 @@ static inline sector_t bdev_zone_sectors(struct block_device *bdev)
 	return 0;
 }
 
-static inline int queue_dma_alignment(struct request_queue *q)
+static inline int queue_dma_alignment(const struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;
 }
@@ -1543,7 +1543,7 @@ static inline void blk_queue_max_integrity_segments(struct request_queue *q,
 }
 
 static inline unsigned short
-queue_max_integrity_segments(struct request_queue *q)
+queue_max_integrity_segments(const struct request_queue *q)
 {
 	return q->limits.max_integrity_segments;
 }
@@ -1626,7 +1626,7 @@ static inline void blk_queue_max_integrity_segments(struct request_queue *q,
 						    unsigned int segs)
 {
 }
-static inline unsigned short queue_max_integrity_segments(struct request_queue *q)
+static inline unsigned short queue_max_integrity_segments(const struct request_queue *q)
 {
 	return 0;
 }

commit 26202928fafad8bda8b478edb7e62c885be623d7
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Mon Jul 1 14:09:18 2019 +0900

    block: Limit zone array allocation size
    
    Limit the size of the struct blk_zone array used in
    blk_revalidate_disk_zones() to avoid memory allocation failures leading
    to disk revalidation failure. Also further reduce the likelyhood of
    such failures by using kvcalloc() (that is vmalloc()) instead of
    allocating contiguous pages with alloc_pages().
    
    Fixes: 515ce6061312 ("scsi: sd_zbc: Fix sd_zbc_report_zones() buffer allocation")
    Fixes: e76239a3748c ("block: add a report_zones method")
    Cc: stable@vger.kernel.org
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 05036e3e3458..1ef375dafb1c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -344,6 +344,11 @@ struct queue_limits {
 
 #ifdef CONFIG_BLK_DEV_ZONED
 
+/*
+ * Maximum number of zones to report with a single report zones command.
+ */
+#define BLK_ZONED_REPORT_MAX_ZONES	8192U
+
 extern unsigned int blkdev_nr_zones(struct block_device *bdev);
 extern int blkdev_report_zones(struct block_device *bdev,
 			       sector_t sector, struct blk_zone *zones,

commit bd976e52725965ddcceb9abecbcc7ca46863665c
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Mon Jul 1 14:09:16 2019 +0900

    block: Kill gfp_t argument of blkdev_report_zones()
    
    Only GFP_KERNEL and GFP_NOIO are used with blkdev_report_zones(). In
    preparation of using vmalloc() for large report buffer and zone array
    allocations used by this function, remove its "gfp_t gfp_mask" argument
    and rely on the caller context to use memalloc_noio_save/restore() where
    necessary (block layer zone revalidation and dm-zoned I/O error path).
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 259bd7ad8312..05036e3e3458 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -347,7 +347,7 @@ struct queue_limits {
 extern unsigned int blkdev_nr_zones(struct block_device *bdev);
 extern int blkdev_report_zones(struct block_device *bdev,
 			       sector_t sector, struct blk_zone *zones,
-			       unsigned int *nr_zones, gfp_t gfp_mask);
+			       unsigned int *nr_zones);
 extern int blkdev_reset_zones(struct block_device *bdev, sector_t sectors,
 			      sector_t nr_sectors, gfp_t gfp_mask);
 extern int blk_revalidate_disk_zones(struct gendisk *disk);
@@ -1673,8 +1673,7 @@ struct block_device_operations {
 	/* this callback is with swap_lock and sometimes page table lock held */
 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 	int (*report_zones)(struct gendisk *, sector_t sector,
-			    struct blk_zone *zones, unsigned int *nr_zones,
-			    gfp_t gfp_mask);
+			    struct blk_zone *zones, unsigned int *nr_zones);
 	struct module *owner;
 	const struct pr_ops *pr_ops;
 };

commit 113ab72ed4794c193509a97d7c6d32a6886e1682
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Wed Jul 10 13:53:10 2019 +0900

    block: Fix potential overflow in blk_report_zones()
    
    For large values of the number of zones reported and/or large zone
    sizes, the sector increment calculated with
    
    blk_queue_zone_sectors(q) * n
    
    in blk_report_zones() loop can overflow the unsigned int type used for
    the calculation as both "n" and blk_queue_zone_sectors() value are
    unsigned int. E.g. for a device with 256 MB zones (524288 sectors),
    overflow happens with 8192 or more zones reported.
    
    Changing the return type of blk_queue_zone_sectors() to sector_t, fixes
    this problem and avoids overflow problem for all other callers of this
    helper too. The same change is also applied to the bdev_zone_sectors()
    helper.
    
    Fixes: e76239a3748c ("block: add a report_zones method")
    Cc: stable@vger.kernel.org
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0c482371c8b3..259bd7ad8312 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -681,7 +681,7 @@ static inline bool blk_queue_is_zoned(struct request_queue *q)
 	}
 }
 
-static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
+static inline sector_t blk_queue_zone_sectors(struct request_queue *q)
 {
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
@@ -1418,7 +1418,7 @@ static inline bool bdev_is_zoned(struct block_device *bdev)
 	return false;
 }
 
-static inline unsigned int bdev_zone_sectors(struct block_device *bdev)
+static inline sector_t bdev_zone_sectors(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);
 

commit e47bc4eda953928644109101d07c9c95dc29a458
Author: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date:   Thu Jun 20 10:59:16 2019 -0700

    block: add centralize REQ_OP_XXX to string helper
    
    In order to centralize the REQ_OP_XXX to string conversion which can be
    used in the block layer and different places in the kernel like f2fs,
    this patch adds a new helper function along with an array similar to the
    one present in the blk-mq-debugfs.c.
    
    We keep this helper functionality centralize under blk-core.c instead of
    blk-mq-debugfs.c since blk-core.c is configured using CONFIG_BLOCK and
    it will not be dependent on blk-mq-debugfs.c which is configured using
    CONFIG_BLK_DEBUG_FS.
    
    Next patch adjusts the code in the blk-mq-debugfs.c with newly
    introduced helper.
    
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d5d3bb45dfb6..0c482371c8b3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -865,6 +865,9 @@ extern void blk_execute_rq(struct request_queue *, struct gendisk *,
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 
+/* Helper to convert REQ_OP_XXX to its string format XXX */
+extern const char *blk_op_str(unsigned int op);
+
 int blk_status_to_errno(blk_status_t status);
 blk_status_t errno_to_blk_status(int errno);
 

commit 14ccb66b3f585b2bc21e7256c96090abed5a512c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 6 12:29:01 2019 +0200

    block: remove the bi_phys_segments field in struct bio
    
    We only need the number of segments in the blk-mq submission path.
    Remove the field from struct bio, and return it from a variant of
    blk_queue_split instead of that it can passed as an argument to
    those functions that need the value.
    
    This also means we stop recounting segments except for cloning
    and partial segments.
    
    To keep the number of arguments in this how path down remove
    pointless struct request_queue arguments from any of the functions
    that had it and grew a nr_segs argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2d4dfe82767a..d5d3bb45dfb6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -841,7 +841,6 @@ extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern int blk_rq_append_bio(struct request *rq, struct bio **bio);
 extern void blk_queue_split(struct request_queue *, struct bio **);
-extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
 extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
 			      unsigned int, void __user *);

commit f924cddebc900f7cb10d5538d69523e558fa681c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 6 12:29:00 2019 +0200

    block: remove blk_init_request_from_bio
    
    lightnvm should have never used this function, as it is sending
    passthrough requests, so switch it to blk_rq_append_bio like all the
    other passthrough request users.  Inline blk_init_request_from_bio into
    the only remaining caller.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Minwoo Im <minwoo.im.dev@gmail.com>
    Reviewed-by: Javier González <javier@javigon.com>
    Reviewed-by: Matias Bjørling <mb@lightnvm.io>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ad49a775c54f..2d4dfe82767a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -828,7 +828,6 @@ extern void blk_unregister_queue(struct gendisk *disk);
 extern blk_qc_t generic_make_request(struct bio *bio);
 extern blk_qc_t direct_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
-extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
 				       blk_mq_req_flags_t flags);

commit 3a211b71529fdd0a89095b18fb19155db0c8fb5d
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Thu May 23 18:43:11 2019 +0300

    blk-core: Remove blk_end_request*() declarations
    
    Commit a1ce35fa49852db60fc6e268 ("block: remove dead elevator code")
    deleted blk_end_request() and friends, but some declaration are still
    left. Purge them.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 90e6914bea0c..ad49a775c54f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1026,21 +1026,9 @@ void blk_steal_bios(struct bio_list *list, struct request *rq);
  *
  * blk_update_request() completes given number of bytes and updates
  * the request without completing it.
- *
- * blk_end_request() and friends.  __blk_end_request() must be called
- * with the request queue spinlock acquired.
- *
- * Several drivers define their own end_request and call
- * blk_end_request() for parts of the original function.
- * This prevents code duplication in drivers.
  */
 extern bool blk_update_request(struct request *rq, blk_status_t error,
 			       unsigned int nr_bytes);
-extern void blk_end_request_all(struct request *rq, blk_status_t error);
-extern bool __blk_end_request(struct request *rq, blk_status_t error,
-			      unsigned int nr_bytes);
-extern void __blk_end_request_all(struct request *rq, blk_status_t error);
-extern bool __blk_end_request_cur(struct request *rq, blk_status_t error);
 
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);

commit 2f578aaf51624aa6fcff041fc7dc5c2d4dfa447f
Author: Minwoo Im <minwoo.im.dev@gmail.com>
Date:   Sun Jun 9 05:15:51 2019 +0900

    block: move tag field position in struct request
    
    __data_len and __sector are internal fields which should not be accessed
    directly in driver-level like the comment above it. But, tag field can
    be accessed by driver level directly so that we need to make the comment
    right by moving it to some other place.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-block@vger.kernel.org
    Signed-off-by: Minwoo Im <minwoo.im.dev@gmail.com>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 592669bcc536..90e6914bea0c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -137,11 +137,11 @@ struct request {
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 
+	int tag;
 	int internal_tag;
 
 	/* the following two fields are internal, NEVER access directly */
 	unsigned int __data_len;	/* total data len */
-	int tag;
 	sector_t __sector;		/* sector cursor */
 
 	struct bio *bio;

commit 7996a8b5511a72465b0b286763c2d8f412b8874a
Author: Bob Liu <bob.liu@oracle.com>
Date:   Tue May 21 11:25:55 2019 +0800

    blk-mq: fix hang caused by freeze/unfreeze sequence
    
    The following is a description of a hang in blk_mq_freeze_queue_wait().
    The hang happens on attempt to freeze a queue while another task does
    queue unfreeze.
    
    The root cause is an incorrect sequence of percpu_ref_resurrect() and
    percpu_ref_kill() and as a result those two can be swapped:
    
     CPU#0                         CPU#1
     ----------------              -----------------
     q1 = blk_mq_init_queue(shared_tags)
    
                                    q2 = blk_mq_init_queue(shared_tags):
                                      blk_mq_add_queue_tag_set(shared_tags):
                                        blk_mq_update_tag_set_depth(shared_tags):
                                         list_for_each_entry()
                                          blk_mq_freeze_queue(q1)
                                           > percpu_ref_kill()
                                           > blk_mq_freeze_queue_wait()
    
     blk_cleanup_queue(q1)
      blk_mq_freeze_queue(q1)
       > percpu_ref_kill()
                     ^^^^^^ freeze_depth can't guarantee the order
    
                                          blk_mq_unfreeze_queue()
                                            > percpu_ref_resurrect()
    
       > blk_mq_freeze_queue_wait()
                     ^^^^^^ Hang here!!!!
    
    This wrong sequence raises kernel warning:
    percpu_ref_kill_and_confirm called more than once on blk_queue_usage_counter_release!
    WARNING: CPU: 0 PID: 11854 at lib/percpu-refcount.c:336 percpu_ref_kill_and_confirm+0x99/0xb0
    
    But the most unpleasant effect is a hang of a blk_mq_freeze_queue_wait(),
    which waits for a zero of a q_usage_counter, which never happens
    because percpu-ref was reinited (instead of being killed) and stays in
    PERCPU state forever.
    
    How to reproduce:
     - "insmod null_blk.ko shared_tags=1 nr_devices=0 queue_mode=2"
     - cpu0: python Script.py 0; taskset the corresponding process running on cpu0
     - cpu1: python Script.py 1; taskset the corresponding process running on cpu1
    
     Script.py:
     ------
     #!/usr/bin/python3
    
    import os
    import sys
    
    while True:
        on = "echo 1 > /sys/kernel/config/nullb/%s/power" % sys.argv[1]
        off = "echo 0 > /sys/kernel/config/nullb/%s/power" % sys.argv[1]
        os.system(on)
        os.system(off)
    ------
    
    This bug was first reported and fixed by Roman, previous discussion:
    [1] Message id: 1443287365-4244-7-git-send-email-akinobu.mita@gmail.com
    [2] Message id: 1443563240-29306-6-git-send-email-tj@kernel.org
    [3] https://patchwork.kernel.org/patch/9268199/
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Roman Pen <roman.penyaev@profitbricks.com>
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1aafeb923e7b..592669bcc536 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -542,7 +542,7 @@ struct request_queue {
 	struct list_head	unused_hctx_list;
 	spinlock_t		unused_hctx_lock;
 
-	atomic_t		mq_freeze_depth;
+	int			mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
 	struct bsg_class_device bsg_dev;
@@ -554,6 +554,11 @@ struct request_queue {
 #endif
 	struct rcu_head		rcu_head;
 	wait_queue_head_t	mq_freeze_wq;
+	/*
+	 * Protect concurrent access to q_usage_counter by
+	 * percpu_ref_kill() and percpu_ref_reinit().
+	 */
+	struct mutex		mq_freeze_lock;
 	struct percpu_ref	q_usage_counter;
 
 	struct blk_mq_tag_set	*tag_set;

commit 2f8f1336a48bd5186de3476da0a3e2ec06d0533a
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Apr 30 09:52:27 2019 +0800

    blk-mq: always free hctx after request queue is freed
    
    In normal queue cleanup path, hctx is released after request queue
    is freed, see blk_mq_release().
    
    However, in __blk_mq_update_nr_hw_queues(), hctx may be freed because
    of hw queues shrinking. This way is easy to cause use-after-free,
    because: one implicit rule is that it is safe to call almost all block
    layer APIs if the request queue is alive; and one hctx may be retrieved
    by one API, then the hctx can be freed by blk_mq_update_nr_hw_queues();
    finally use-after-free is triggered.
    
    Fixes this issue by always freeing hctx after releasing request queue.
    If some hctxs are removed in blk_mq_update_nr_hw_queues(), introduce
    a per-queue list to hold them, then try to resuse these hctxs if numa
    node is matched.
    
    Cc: Dongli Zhang <dongli.zhang@oracle.com>
    Cc: James Smart <james.smart@broadcom.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: linux-scsi@vger.kernel.org,
    Cc: Martin K . Petersen <martin.petersen@oracle.com>,
    Cc: Christoph Hellwig <hch@lst.de>,
    Cc: James E . J . Bottomley <jejb@linux.vnet.ibm.com>,
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: James Smart <james.smart@broadcom.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bd3e3f09bfa0..1aafeb923e7b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -535,6 +535,13 @@ struct request_queue {
 
 	struct mutex		sysfs_lock;
 
+	/*
+	 * for reusing dead hctx instance in case of updating
+	 * nr_hw_queues
+	 */
+	struct list_head	unused_hctx_list;
+	spinlock_t		unused_hctx_lock;
+
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)

commit 12adb7a013e318de553ccee4a006a718667972b3
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 30 13:56:16 2019 -0400

    block: remove the unused blk_queue_dma_pad function
    
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 99aa98f60b9e..bd3e3f09bfa0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1069,7 +1069,6 @@ extern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,
 extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
 			      sector_t offset);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
-extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
 extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
 extern int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,

commit 5c61ee2cd5860e41c8ab98837761ffaa93eb4dfe
Merge: cdf3e3deb747 085b7755808a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Apr 22 09:47:36 2019 -0600

    Merge tag 'v5.1-rc6' into for-5.2/block
    
    Pull in v5.1-rc6 to resolve two conflicts. One is in BFQ, in just a
    comment, and is trivial. The other one is a conflict due to a later fix
    in the bio multi-page work, and needs a bit more care.
    
    * tag 'v5.1-rc6': (770 commits)
      Linux 5.1-rc6
      block: make sure that bvec length can't be overflow
      block: kill all_q_node in request_queue
      x86/cpu/intel: Lower the "ENERGY_PERF_BIAS: Set to normal" message's log priority
      coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping
      mm/kmemleak.c: fix unused-function warning
      init: initialize jump labels before command line option parsing
      kernel/watchdog_hld.c: hard lockup message should end with a newline
      kcov: improve CONFIG_ARCH_HAS_KCOV help text
      mm: fix inactive list balancing between NUMA nodes and cgroups
      mm/hotplug: treat CMA pages as unmovable
      proc: fixup proc-pid-vm test
      proc: fix map_files test on F29
      mm/vmstat.c: fix /proc/vmstat format for CONFIG_DEBUG_TLBFLUSH=y CONFIG_SMP=n
      mm/memory_hotplug: do not unlock after failing to take the device_hotplug_lock
      mm: swapoff: shmem_unuse() stop eviction without igrab()
      mm: swapoff: take notice of completion sooner
      mm: swapoff: remove too limiting SWAP_UNUSE_MAX_TRIES
      mm: swapoff: shmem_find_swap_entries() filter out other types
      slab: store tagged freelist for off-slab slabmgmt
      ...
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit b40fabc05ea047f6af5933d26a5483873340b0d4
Author: Hou Tao <houtao1@huawei.com>
Date:   Fri Apr 19 10:31:27 2019 +0800

    block: kill all_q_node in request_queue
    
    all_q_node has not been used since commit 4b855ad37194 ("blk-mq: Create
    hctx for each present CPU"), so remove it.
    
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Hou Tao <houtao1@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c58a3b2bf00..317ab30d2904 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -548,7 +548,6 @@ struct request_queue {
 	struct rcu_head		rcu_head;
 	wait_queue_head_t	mq_freeze_wq;
 	struct percpu_ref	q_usage_counter;
-	struct list_head	all_q_node;
 
 	struct blk_mq_tag_set	*tag_set;
 	struct list_head	tag_set_list;

commit 3ab3a0313cb8c50391d74e40fd46a3408d8e4de9
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Mar 3 08:40:36 2019 -0700

    block: add dma_map_bvec helper
    
    Provide a nice little shortcut for mapping a single bvec.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 74469a4dc0a1..4b85dc066264 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -644,6 +644,10 @@ static inline bool blk_account_rq(struct request *rq)
 #define rq_dma_dir(rq) \
 	(op_is_write(req_op(rq)) ? DMA_TO_DEVICE : DMA_FROM_DEVICE)
 
+#define dma_map_bvec(dev, bv, dir, attrs) \
+	dma_map_page_attrs(dev, (bv)->bv_page, (bv)->bv_offset, (bv)->bv_len, \
+	(dir), (attrs))
+
 static inline bool queue_is_mq(struct request_queue *q)
 {
 	return q->mq_ops;

commit 9d9de535f385a8b3ba0e88ca0abf386c5704bbfc
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Mar 3 08:18:30 2019 -0700

    block: add a rq_dma_dir helper
    
    In a lot of places we want to know the DMA direction for a given
    struct request.  Add a little helper to make it a littler easier.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3a13fbe13e08..74469a4dc0a1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -641,6 +641,9 @@ static inline bool blk_account_rq(struct request *rq)
 
 #define rq_data_dir(rq)		(op_is_write(req_op(rq)) ? WRITE : READ)
 
+#define rq_dma_dir(rq) \
+	(op_is_write(req_op(rq)) ? DMA_TO_DEVICE : DMA_FROM_DEVICE)
+
 static inline bool queue_is_mq(struct request_queue *q)
 {
 	return q->mq_ops;

commit 2a876f5e25e8ec9fa5777d36e5695ee33dd63f6f
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Mar 3 08:38:29 2019 -0700

    block: add a rq_integrity_vec helper
    
    This provides a nice little shortcut to get the integrity data for
    drivers like NVMe that only support a single integrity segment.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 84ce76f92d83..3a13fbe13e08 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1559,6 +1559,17 @@ static inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,
 	return bio_integrity_intervals(bi, sectors) * bi->tuple_size;
 }
 
+/*
+ * Return the first bvec that contains integrity data.  Only drivers that are
+ * limited to a single integrity segment should use this helper.
+ */
+static inline struct bio_vec *rq_integrity_vec(struct request *rq)
+{
+	if (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))
+		return NULL;
+	return rq->bio->bi_integrity->bip_vec;
+}
+
 #else /* CONFIG_BLK_DEV_INTEGRITY */
 
 struct bio;
@@ -1633,6 +1644,11 @@ static inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,
 	return 0;
 }
 
+static inline struct bio_vec *rq_integrity_vec(struct request *rq)
+{
+	return NULL;
+}
+
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
 struct block_device_operations {

commit 3aef3cae4342c1d8137a1c0782cbb66f1be3943c
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Mar 3 09:14:01 2019 -0700

    block: add a req_bvec helper
    
    Return the currently active bvec segment, potentially spanning multiple
    pages.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c58a3b2bf00..84ce76f92d83 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -932,6 +932,17 @@ static inline unsigned int blk_rq_payload_bytes(struct request *rq)
 	return blk_rq_bytes(rq);
 }
 
+/*
+ * Return the first full biovec in the request.  The caller needs to check that
+ * there are any bvecs before calling this helper.
+ */
+static inline struct bio_vec req_bvec(struct request *rq)
+{
+	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
+		return rq->special_vec;
+	return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
+}
+
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {

commit 29ece8b4354f8c5eaee798a3d8a1b356efee426f
Author: Yufen Yu <yuyufen@huawei.com>
Date:   Mon Mar 18 22:44:41 2019 +0800

    block: add BLK_MQ_POLL_CLASSIC for hybrid poll and return EINVAL for unexpected value
    
    For q->poll_nsec == -1, means doing classic poll, not hybrid poll.
    We introduce a new flag BLK_MQ_POLL_CLASSIC to replace -1, which
    may make code much easier to read.
    
    Additionally, since val is an int obtained with kstrtoint(), val can be
    a negative value other than -1, so return -EINVAL for that case.
    
    Thanks to Damien Le Moal for some good suggestion.
    
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Yufen Yu <yuyufen@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0de92b29f589..5c58a3b2bf00 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -50,6 +50,9 @@ struct blk_stat_callback;
 /* Must be consistent with blk_mq_poll_stats_bkt() */
 #define BLK_MQ_POLL_STATS_BKTS 16
 
+/* Doing classic polling */
+#define BLK_MQ_POLL_CLASSIC -1
+
 /*
  * Maximum number of blkcg policies allowed to be registered concurrently.
  * Defined here to simplify include dependency.

commit 92fff53b7191cae566be9ca6752069426c7f8241
Merge: a50243b1ddcd 26af1a368e40
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 9 16:53:47 2019 -0800

    Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI updates from James Bottomley:
     "This is mostly update of the usual drivers: arcmsr, qla2xxx, lpfc,
      hisi_sas, target/iscsi and target/core.
    
      Additionally Christoph refactored gdth as part of the dma changes. The
      major mid-layer change this time is the removal of bidi commands and
      with them the whole of the osd/exofs driver and filesystem. This is a
      major simplification for block and mq in particular"
    
    * tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (240 commits)
      scsi: cxgb4i: validate tcp sequence number only if chip version <= T5
      scsi: cxgb4i: get pf number from lldi->pf
      scsi: core: replace GFP_ATOMIC with GFP_KERNEL in scsi_scan.c
      scsi: mpt3sas: Add missing breaks in switch statements
      scsi: aacraid: Fix missing break in switch statement
      scsi: kill command serial number
      scsi: csiostor: drop serial_number usage
      scsi: mvumi: use request tag instead of serial_number
      scsi: dpt_i2o: remove serial number usage
      scsi: st: osst: Remove negative constant left-shifts
      scsi: ufs-bsg: Allow reading descriptors
      scsi: ufs: Allow reading descriptor via raw upiu
      scsi: ufs-bsg: Change the calling convention for write descriptor
      scsi: ufs: Remove unused device quirks
      Revert "scsi: ufs: disable vccq if it's not needed by UFS device"
      scsi: megaraid_sas: Remove a bunch of set but not used variables
      scsi: clean obsolete return values of eh_timed_out
      scsi: sd: Optimal I/O size should be a multiple of physical block size
      scsi: MAINTAINERS: SCSI initiator and target tweaks
      scsi: fcoe: make use of fip_mode enum complete
      ...

commit 2705c93742e91730d335838025d75d8043861174
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Feb 15 19:13:23 2019 +0800

    block: kill QUEUE_FLAG_NO_SG_MERGE
    
    Since bdced438acd83ad83a6c ("block: setup bi_phys_segments after splitting"),
    physical segment number is mainly figured out in blk_queue_split() for
    fast path, and the flag of BIO_SEG_VALID is set there too.
    
    Now only blk_recount_segments() and blk_recalc_rq_segments() use this
    flag.
    
    Basically blk_recount_segments() is bypassed in fast path given BIO_SEG_VALID
    is set in blk_queue_split().
    
    For another user of blk_recalc_rq_segments():
    
    - run in partial completion branch of blk_update_request, which is an unusual case
    
    - run in blk_cloned_rq_check_limits(), still not a big problem if the flag is killed
    since dm-rq is the only user.
    
    Multi-page bvec is enabled now, not doing S/G merging is rather pointless with the
    current setup of the I/O path, as it isn't going to save you a significant amount
    of cycles.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b6292d469ea4..faed9d9eb84c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -588,7 +588,6 @@ struct request_queue {
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
-#define QUEUE_FLAG_NO_SG_MERGE	15	/* don't attempt to merge SG segments*/
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */

commit d18d91740ad22e9d7998884c4d80523d0ba95ddf
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Feb 15 19:13:11 2019 +0800

    block: introduce bio_for_each_bvec() and rq_for_each_bvec()
    
    bio_for_each_bvec() is used for iterating over multi-page bvec for bio
    split & merge code.
    
    rq_for_each_bvec() can be used for drivers which may handle the
    multi-page bvec directly, so far loop is one perfect use case.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3603270cb82d..b6292d469ea4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -792,6 +792,10 @@ struct req_iterator {
 	__rq_for_each_bio(_iter.bio, _rq)			\
 		bio_for_each_segment(bvl, _iter.bio, _iter.iter)
 
+#define rq_for_each_bvec(bvl, _rq, _iter)			\
+	__rq_for_each_bio(_iter.bio, _rq)			\
+		bio_for_each_bvec(bvl, _iter.bio, _iter.iter)
+
 #define rq_iter_last(bvec, _iter)				\
 		(_iter.bio->bi_next == NULL &&			\
 		 bio_iter_last(bvec, _iter.iter))

commit eca7abf31abba2acac445ec6a1d3f94cf0cab918
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Feb 9 15:42:07 2019 -0700

    block: queue flag cleanup
    
    We have QUEUE_FLAG_DEFAULT defined, but it's not used anymore since
    the legacy IO stack is gone. Kill it.
    
    Sanitize the queue flags in general, they use spaces (for some
    reason), and the space is pretty sparse. With the flags renumbered,
    we can more clearly see how many we have available.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 24ccab51085f..3603270cb82d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -572,37 +572,33 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
-#define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
-#define QUEUE_FLAG_DYING	2	/* queue being torn down */
-#define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
-#define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
-#define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
-#define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
-#define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
-#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
-#define QUEUE_FLAG_IO_STAT     10	/* do disk/partitions IO accounting */
-#define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
-#define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
-#define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
-#define QUEUE_FLAG_SECERASE    14	/* supports secure erase */
-#define QUEUE_FLAG_SAME_FORCE  15	/* force complete on same CPU */
-#define QUEUE_FLAG_DEAD        16	/* queue tear-down finished */
-#define QUEUE_FLAG_INIT_DONE   17	/* queue is initialized */
-#define QUEUE_FLAG_NO_SG_MERGE 18	/* don't attempt to merge SG segments*/
-#define QUEUE_FLAG_POLL	       19	/* IO polling enabled if set */
-#define QUEUE_FLAG_WC	       20	/* Write back caching */
-#define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
-#define QUEUE_FLAG_DAX         23	/* device supports DAX */
-#define QUEUE_FLAG_STATS       24	/* track IO start and completion times */
-#define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
-#define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
-#define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
-#define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
-#define QUEUE_FLAG_PCI_P2PDMA  29	/* device supports PCI p2p requests */
-
-#define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
-				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
-				 (1 << QUEUE_FLAG_ADD_RANDOM))
+#define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
+#define QUEUE_FLAG_DYING	1	/* queue being torn down */
+#define QUEUE_FLAG_BIDI		2	/* queue supports bidi requests */
+#define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
+#define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+#define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
+#define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
+#define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
+#define QUEUE_FLAG_IO_STAT	7	/* do disk/partitions IO accounting */
+#define QUEUE_FLAG_DISCARD	8	/* supports DISCARD */
+#define QUEUE_FLAG_NOXMERGES	9	/* No extended merges */
+#define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
+#define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
+#define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
+#define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
+#define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+#define QUEUE_FLAG_NO_SG_MERGE	15	/* don't attempt to merge SG segments*/
+#define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+#define QUEUE_FLAG_WC		17	/* Write back caching */
+#define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
+#define QUEUE_FLAG_DAX		19	/* device supports DAX */
+#define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+#define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
+#define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
+#define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+#define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
+#define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))

commit d11a3998985b351aaab6bbdc23bc884bd5e815c8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Feb 9 15:40:24 2019 -0700

    block: kill QUEUE_FLAG_FLUSH_NQ
    
    We have various helpers for setting/clearing this flag, and also
    a helper to check if the queue supports queueable flushes or not.
    But nobody uses them anymore, kill it with fire.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 338604dff7d0..24ccab51085f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -592,7 +592,6 @@ struct request_queue {
 #define QUEUE_FLAG_POLL	       19	/* IO polling enabled if set */
 #define QUEUE_FLAG_WC	       20	/* Write back caching */
 #define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
-#define QUEUE_FLAG_FLUSH_NQ    22	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         23	/* device supports DAX */
 #define QUEUE_FLAG_STATS       24	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
@@ -1069,7 +1068,6 @@ extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
-extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
 
 /*
@@ -1446,11 +1444,6 @@ static inline unsigned int block_size(struct block_device *bdev)
 	return bdev->bd_block_size;
 }
 
-static inline bool queue_flush_queueable(struct request_queue *q)
-{
-	return !test_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);
-}
-
 typedef struct {struct page *v;} Sector;
 
 unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);

commit 8b3238cabd50e2715b6544e724e74685209b190a
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 08:01:10 2018 -0800

    scsi: block: remove bidi support
    
    Unused now, and another field in struct request bites the dust.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fd1450d53f1c..21beb456b97a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -234,9 +234,6 @@ struct request {
 	 */
 	rq_end_io_fn *end_io;
 	void *end_io_data;
-
-	/* for bidi */
-	struct request *next_rq;
 };
 
 static inline bool blk_op_is_scsi(unsigned int op)
@@ -572,7 +569,6 @@ struct request_queue {
 
 #define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 #define QUEUE_FLAG_DYING	2	/* queue being torn down */
-#define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
@@ -644,8 +640,6 @@ static inline bool blk_account_rq(struct request *rq)
 	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
 }
 
-#define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
-
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
 #define rq_data_dir(rq)		(op_is_write(req_op(rq)) ? WRITE : READ)

commit 69ed175c195595c73901e18366cb0ebeaeb68b8a
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 9 19:35:11 2018 +0100

    scsi: block: remove req->special
    
    No users left.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 338604dff7d0..fd1450d53f1c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -216,8 +216,6 @@ struct request {
 	unsigned short write_hint;
 	unsigned short ioprio;
 
-	void *special;		/* opaque pointer available for LLD use */
-
 	unsigned int extra_len;	/* length of alignment and padding */
 
 	enum mq_rq_state state;

commit 938edb8a31b976c9a92eb0cd4ff481e93f76c1f1
Merge: af7ddd8a627c da7903092b88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 28 14:48:06 2018 -0800

    Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI updates from James Bottomley:
     "This is mostly update of the usual drivers: smarpqi, lpfc, qedi,
      megaraid_sas, libsas, zfcp, mpt3sas, hisi_sas.
    
      Additionally, we have a pile of annotation, unused variable and minor
      updates.
    
      The big API change is the updates for Christoph's DMA rework which
      include removing the DISABLE_CLUSTERING flag.
    
      And finally there are a couple of target tree updates"
    
    * tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (259 commits)
      scsi: isci: request: mark expected switch fall-through
      scsi: isci: remote_node_context: mark expected switch fall-throughs
      scsi: isci: remote_device: Mark expected switch fall-throughs
      scsi: isci: phy: Mark expected switch fall-through
      scsi: iscsi: Capture iscsi debug messages using tracepoints
      scsi: myrb: Mark expected switch fall-throughs
      scsi: megaraid: fix out-of-bound array accesses
      scsi: mpt3sas: mpt3sas_scsih: Mark expected switch fall-through
      scsi: fcoe: remove set but not used variable 'port'
      scsi: smartpqi: call pqi_free_interrupts() in pqi_shutdown()
      scsi: smartpqi: fix build warnings
      scsi: smartpqi: update driver version
      scsi: smartpqi: add ofa support
      scsi: smartpqi: increase fw status register read timeout
      scsi: smartpqi: bump driver version
      scsi: smartpqi: add smp_utils support
      scsi: smartpqi: correct lun reset issues
      scsi: smartpqi: correct volume status
      scsi: smartpqi: do not offline disks for transient did no connect conditions
      scsi: smartpqi: allow for larger raid maps
      ...

commit 38417468d4f05cfed62fca8f407d2df0cbe3fcc8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 13 16:17:10 2018 +0100

    scsi: block: remove the cluster flag
    
    Now that the the SCSI layer replaced the use of the cluster flag with
    segment size limits and the DMA boundary we can remove the cluster flag
    from the block layer.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4293dc1cd160..653ae90eec0b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -389,7 +389,6 @@ struct queue_limits {
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
-	unsigned char		cluster;
 	unsigned char		raid_partial_stripes_expensive;
 	enum blk_zoned_model	zoned;
 };
@@ -785,11 +784,6 @@ static inline bool queue_is_rq_based(struct request_queue *q)
 	return q->request_fn || q->mq_ops;
 }
 
-static inline unsigned int blk_queue_cluster(struct request_queue *q)
-{
-	return q->limits.cluster;
-}
-
 static inline enum blk_zoned_model
 blk_queue_zoned_model(struct request_queue *q)
 {

commit cc56694f132a8f5fa9334e3afe990de8c3378866
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Dec 17 09:46:00 2018 +0800

    blk-mq-debugfs: support rq_qos
    
    blk-mq-debugfs has been proved as very helpful for debug some
    tough issues, such as IO hang.
    
    We have seen blk-wbt related IO hang several times, even inside
    Red Hat BZ, there is such report not sovled yet, so this patch
    adds support debugfs on rq_qos.
    
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 81f1b105946b..45552e6eae1e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -560,6 +560,7 @@ struct request_queue {
 #ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;
 	struct dentry		*sched_debugfs_dir;
+	struct dentry		*rqos_debugfs_dir;
 #endif
 
 	bool			mq_sysfs_init_done;

commit 6e0de61107f03c3222550d9b548cd331d31d82d1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 5 06:50:40 2018 -0700

    blk-mq: remove QUEUE_FLAG_POLL from default MQ flags
    
    We only support polling if we have poll queues now, but the flag is
    being set by default. Remove the default QUEUE_FLAG_POLL setting, we'll
    set it in blk_mq_init_allocated_queue() if we have poll queues available
    for this device.
    
    Fixes: 6544d229bf43 ("block: enable polling by default if a poll map is initalized")
    Reported-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0b3874bdbc6a..81f1b105946b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -606,8 +606,7 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
-				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
-				 (1 << QUEUE_FLAG_POLL))
+				 (1 << QUEUE_FLAG_SAME_COMP))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);

commit 529262d56dbebe6a26df5d2fd24cc0e1bc8579e5
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 2 17:46:26 2018 +0100

    block: remove ->poll_fn
    
    This was intended to support users like nvme multipath, but is just
    getting in the way and adding another indirect call.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 08d940f85fa0..0b3874bdbc6a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -283,7 +283,6 @@ static inline unsigned short req_get_ioprio(struct request *req)
 struct blk_queue_ctx;
 
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
-typedef int (poll_q_fn) (struct request_queue *q, blk_qc_t, bool spin);
 
 struct bio_vec;
 typedef int (dma_drain_needed_fn)(struct request *);
@@ -401,7 +400,6 @@ struct request_queue {
 	struct rq_qos		*rq_qos;
 
 	make_request_fn		*make_request_fn;
-	poll_q_fn		*poll_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 
 	const struct blk_mq_ops	*mq_ops;

commit ce5b009cff1961137127edf91f44effd0eec8ffd
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 27 17:13:56 2018 -0700

    block: improve logic around when to sort a plug list
    
    Only do it if we have requests for multiple queues in the same
    plug.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 02732cae6080..08d940f85fa0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1131,6 +1131,7 @@ struct blk_plug {
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
 	unsigned short rq_count;
+	bool multiple_queues;
 };
 #define BLK_MAX_REQUEST_COUNT 16
 #define BLK_PLUG_FLUSH_SIZE (128 * 1024)

commit 5f0ed774ed2914decfd397569fface997532e94d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 23 22:04:33 2018 -0700

    block: sum requests in the plug structure
    
    This isn't exactly the same as the previous count, as it includes
    requests for all devices. But that really doesn't matter, if we have
    more than the threshold (16) queued up, flush it. It's not worth it
    to have an expensive list loop for this.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e3c0a8ec16a7..02732cae6080 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1130,6 +1130,7 @@ extern void blk_set_queue_dying(struct request_queue *);
 struct blk_plug {
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
+	unsigned short rq_count;
 };
 #define BLK_MAX_REQUEST_COUNT 16
 #define BLK_PLUG_FLUSH_SIZE (128 * 1024)

commit 0a1b8b87d064a47fad9ec475316002da28559207
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 26 08:24:43 2018 -0700

    block: make blk_poll() take a parameter on whether to spin or not
    
    blk_poll() has always kept spinning until it found an IO. This is
    fine for SYNC polling, since we need to find one request we have
    pending, but in preparation for ASYNC polling it can be beneficial
    to just check if we have any entries available or not.
    
    Existing callers are converted to pass in 'spin == true', to retain
    the old behavior.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3015e9b5ae3..e3c0a8ec16a7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -283,7 +283,7 @@ static inline unsigned short req_get_ioprio(struct request *req)
 struct blk_queue_ctx;
 
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
-typedef int (poll_q_fn) (struct request_queue *q, blk_qc_t);
+typedef int (poll_q_fn) (struct request_queue *q, blk_qc_t, bool spin);
 
 struct bio_vec;
 typedef int (dma_drain_needed_fn)(struct request *);
@@ -867,7 +867,7 @@ extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 int blk_status_to_errno(blk_status_t status);
 blk_status_t errno_to_blk_status(int errno);
 
-int blk_poll(struct request_queue *q, blk_qc_t cookie);
+int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {

commit 1052b8ac5282daf35df331edcbdb645839d17e6a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 26 08:21:49 2018 -0700

    blk-mq: when polling for IO, look for any completion
    
    If we want to support async IO polling, then we have to allow finding
    completions that aren't just for the one we are looking for. Always pass
    in -1 to the mq_ops->poll() helper, and have that return how many events
    were found in this poll loop.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9b53db06ad08..f3015e9b5ae3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -867,7 +867,7 @@ extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 int blk_status_to_errno(blk_status_t status);
 blk_status_t errno_to_blk_status(int errno);
 
-bool blk_poll(struct request_queue *q, blk_qc_t cookie);
+int blk_poll(struct request_queue *q, blk_qc_t cookie);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {

commit 1db4909e76f64a85f4aaa187f0f683f5c85a471d
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Nov 20 09:44:35 2018 +0800

    blk-mq: not embed .mq_kobj and ctx->kobj into queue instance
    
    Even though .mq_kobj, ctx->kobj and q->kobj share same lifetime
    from block layer's view, actually they don't because userspace may
    grab one kobject anytime via sysfs.
    
    This patch fixes the issue by the following approach:
    
    1) introduce 'struct blk_mq_ctxs' for holding .mq_kobj and managing
    all ctxs
    
    2) free all allocated ctxs and the 'blk_mq_ctxs' instance in release
    handler of .mq_kobj
    
    3) grab one ref of .mq_kobj before initializing each ctx->kobj, so that
    .mq_kobj is always released after all ctxs are freed.
    
    This patch fixes kernel panic issue during booting when DEBUG_KOBJECT_RELEASE
    is enabled.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e97c0a3b2262..9b53db06ad08 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -456,7 +456,7 @@ struct request_queue {
 	/*
 	 * mq queue kobject
 	 */
-	struct kobject mq_kobj;
+	struct kobject *mq_kobj;
 
 #ifdef  CONFIG_BLK_DEV_INTEGRITY
 	struct blk_integrity integrity;

commit 85f4d4b65fdd67f1d6dc9eeb1d91923cef07eb6a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 6 13:30:55 2018 -0700

    block: have ->poll_fn() return number of entries polled
    
    We currently only really support sync poll, ie poll with 1 IO in flight.
    This prepares us for supporting async poll.
    
    Note that the returned value isn't necessarily 100% accurate. If poll
    races with IRQ completion, we assume that the fact that the task is now
    runnable means we found at least one entry. In reality it could be more
    than 1, or not even 1. This is fine, the caller will just need to take
    this into account.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ad6eafc43f2..e97c0a3b2262 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -283,7 +283,7 @@ static inline unsigned short req_get_ioprio(struct request *req)
 struct blk_queue_ctx;
 
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
-typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
+typedef int (poll_q_fn) (struct request_queue *q, blk_qc_t);
 
 struct bio_vec;
 typedef int (dma_drain_needed_fn)(struct request *);

commit 2b78eae147a13ab2ca7caa121dd3fca2eecf8613
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 16 09:10:01 2018 +0100

    block: remove the rq_alloc_data request_queue field
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 91c44f7a7f62..1ad6eafc43f2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -567,7 +567,6 @@ struct request_queue {
 	bool			mq_sysfs_init_done;
 
 	size_t			cmd_size;
-	void			*rq_alloc_data;
 
 	struct work_struct	release_work;
 

commit 0619317ff8baa2da9238191ad5167ed3618c16d9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 13 21:16:54 2018 -0700

    block: add polled wakeup task helper
    
    If we're polling for IO on a device that doesn't use interrupts, then
    IO completion loop (and wake of task) is done by submitting task itself.
    If that is the case, then we don't need to enter the wake_up_process()
    function, we can simply mark ourselves as TASK_RUNNING.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 41aaa05e42c1..91c44f7a7f62 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1772,4 +1772,17 @@ static inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 
 #endif /* CONFIG_BLOCK */
 
+static inline void blk_wake_io_task(struct task_struct *waiter)
+{
+	/*
+	 * If we're polling, the task itself is doing the completions. For
+	 * that case, we don't need to signal a wakeup, it's enough to just
+	 * mark us as RUNNING.
+	 */
+	if (waiter == current)
+		__set_current_state(TASK_RUNNING);
+	else
+		wake_up_process(waiter);
+}
+
 #endif

commit 344e9ffcbd1898e1dc04085564a6e05c30ea8199
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 15 12:22:51 2018 -0700

    block: add queue_is_mq() helper
    
    Various spots check for q->mq_ops being non-NULL, but provide
    a helper to do this instead.
    
    Where the ->mq_ops != NULL check is redundant, remove it.
    
    Since mq == rq-based now that legacy is gone, get rid of the
    queue_is_rq_based() and just use queue_is_mq() everywhere.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1d185f1fc333..41aaa05e42c1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -656,11 +656,7 @@ static inline bool blk_account_rq(struct request *rq)
 
 #define rq_data_dir(rq)		(op_is_write(req_op(rq)) ? WRITE : READ)
 
-/*
- * Driver can handle struct request, if it either has an old style
- * request_fn defined, or is blk-mq based.
- */
-static inline bool queue_is_rq_based(struct request_queue *q)
+static inline bool queue_is_mq(struct request_queue *q)
 {
 	return q->mq_ops;
 }

commit 0d945c1f966b2bcb67bb12be749da0a7fb00201b
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 15 12:17:28 2018 -0700

    block: remove the queue_lock indirection
    
    With the legacy request path gone there is no good reason to keep
    queue_lock as a pointer, we can always use the embedded lock now.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    
    Fixed floppy and blk-cgroup missing conversions and half done edits.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c4a3a660e3f0..1d185f1fc333 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -446,13 +446,7 @@ struct request_queue {
 	 */
 	gfp_t			bounce_gfp;
 
-	/*
-	 * protects queue structures from reentrancy. ->__queue_lock should
-	 * _never_ be used directly, it is queue private. always use
-	 * ->queue_lock.
-	 */
-	spinlock_t		__queue_lock;
-	spinlock_t		*queue_lock;
+	spinlock_t		queue_lock;
 
 	/*
 	 * queue kobject

commit 6d46964230d182c4b6097379738849a809d791dc
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:18 2018 +0100

    block: remove the lock argument to blk_alloc_queue_node
    
    With the legacy request path gone there is no real need to override the
    queue_lock.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 30d8e0fbd104..c4a3a660e3f0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1122,8 +1122,7 @@ extern long nr_blockdev_pages(void);
 
 bool __must_check blk_get_queue(struct request_queue *);
 struct request_queue *blk_alloc_queue(gfp_t);
-struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
-					   spinlock_t *lock);
+struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id);
 extern void blk_put_queue(struct request_queue *);
 extern void blk_set_queue_dying(struct request_queue *);
 

commit 57d74df90783f6a6b3e79dfdd2a567ce5db3b790
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:07 2018 +0100

    block: use atomic bitops for ->queue_flags
    
    ->queue_flags is generally not set or cleared in the fast path, and also
    generally set or cleared one flag at a time.  Make use of the normal
    atomic bitops for it so that we don't need to take the queue_lock,
    which is otherwise mostly unused in the core block layer now.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 60507ab7b358..30d8e0fbd104 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -621,7 +621,6 @@ struct request_queue {
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
-bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)

commit 079076b3416e78ba2bb3ce38e05e320c388c3120
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:05 2018 +0100

    block: remove deadline __deadline manipulation helpers
    
    No users left since the removal of the legacy request interface, we can
    remove all the magic bit stealing now and make it a normal field.
    
    But use WRITE_ONCE/READ_ONCE on the new deadline field, given that we
    don't seem to have any mechanism to guarantee a new value actually
    gets seen by other threads.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dd1e53fd4acf..60507ab7b358 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -224,9 +224,7 @@ struct request {
 	refcount_t ref;
 
 	unsigned int timeout;
-
-	/* access through blk_rq_set_deadline, blk_rq_deadline */
-	unsigned long __deadline;
+	unsigned long deadline;
 
 	union {
 		struct __call_single_data csd;

commit 8f4236d9008b0973a8281256ccfde6913cdec6cb
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:04 2018 +0100

    block: remove QUEUE_FLAG_BYPASS and ->bypass
    
    Unused since the removal of the legacy request code.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c961329be96b..dd1e53fd4acf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -548,7 +548,6 @@ struct request_queue {
 
 	struct mutex		sysfs_lock;
 
-	int			bypass_depth;
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -586,7 +585,6 @@ struct request_queue {
 
 #define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 #define QUEUE_FLAG_DYING	2	/* queue being torn down */
-#define QUEUE_FLAG_BYPASS	3	/* act as dumb FIFO queue */
 #define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
@@ -630,7 +628,6 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
-#define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\

commit 7ff4f8035695984c513598e2d49c8277d5d234ca
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Nov 14 15:22:49 2018 -0700

    block: remove dead queue members
    
    No more users of ->in_flight[] or ->nr_sorted, get rid of them.
    
    Fixes: a1ce35fa4985 ("block: remove dead elevator code")
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e67ad2dd025e..c961329be96b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -486,9 +486,6 @@ struct request_queue {
 	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 
-	unsigned int		nr_sorted;
-	unsigned int		in_flight[2];
-
 	unsigned int		rq_timeout;
 	int			poll_nsec;
 

commit 0e17e06cbf7ede285ab74bab44d888b40c21f828
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 9 14:42:41 2018 +0100

    block: remove the BLKPREP_* values.
    
    Unused now.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dc2a6f625ecb..e67ad2dd025e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -776,16 +776,6 @@ static inline unsigned int blk_queue_depth(struct request_queue *q)
 	return q->nr_requests;
 }
 
-/*
- * q->prep_rq_fn return values
- */
-enum {
-	BLKPREP_OK,		/* serve it */
-	BLKPREP_KILL,		/* fatal error, kill, return -EIO */
-	BLKPREP_DEFER,		/* leave on queue */
-	BLKPREP_INVALID,	/* invalid command, kill, return -EREMOTEIO */
-};
-
 extern unsigned long blk_max_low_pfn, blk_max_pfn;
 
 /*

commit 9d037ad707ed6069fbea4e38e6ee37e027b13f1d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 9 19:37:44 2018 +0100

    block: remove req->timeout_list
    
    Unused now that the legacy request path is gone.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9b1f470cc784..dc2a6f625ecb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -228,8 +228,6 @@ struct request {
 	/* access through blk_rq_set_deadline, blk_rq_deadline */
 	unsigned long __deadline;
 
-	struct list_head timeout_list;
-
 	union {
 		struct __call_single_data csd;
 		u64 fifo_time;

commit ea4f995ee8b8f0578b3319949f2edd5d812fdb0a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 15:06:13 2018 -0600

    blk-mq: cache request hardware queue mapping
    
    We call blk_mq_map_queue() a lot, at least two times for each
    request per IO, sometimes more. Since we now have an indirect
    call as well in that function. cache the mapping so we don't
    have to re-call blk_mq_map_queue() for the same request
    multiple times.
    
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2ae7465d68ab..9b1f470cc784 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -129,6 +129,7 @@ enum mq_rq_state {
 struct request {
 	struct request_queue *q;
 	struct blk_mq_ctx *mq_ctx;
+	struct blk_mq_hw_ctx *mq_hctx;
 
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;

commit a8908939af569ce2419f43fd56eeaf003bc3d85d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 16 14:23:06 2018 -0600

    blk-mq: kill q->mq_map
    
    It's just a pointer to set->mq_map, use that instead. Move the
    assignment a bit earlier, so we always know it's valid.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c8fa4d3d7fee..2ae7465d68ab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -409,8 +409,6 @@ struct request_queue {
 
 	const struct blk_mq_ops	*mq_ops;
 
-	unsigned int		*mq_map;
-
 	/* sw queues */
 	struct blk_mq_ctx __percpu	*queue_ctx;
 	unsigned int		nr_queues;

commit 9cf2bab6307659b940da65d16dcc8f82c69f3a97
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 31 17:01:22 2018 -0600

    block: kill request ->cpu member
    
    This was used for completion placement for the legacy path,
    but for mq we have rq->mq_ctx->cpu for that. Add a helper
    to get the request CPU assignment, as the mq_ctx type is
    private to blk-mq.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4104844d6bb..c8fa4d3d7fee 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -130,7 +130,6 @@ struct request {
 	struct request_queue *q;
 	struct blk_mq_ctx *mq_ctx;
 
-	int cpu;
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 
@@ -669,7 +668,6 @@ static inline bool blk_account_rq(struct request *rq)
 	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
 }
 
-#define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)

commit c7bb9ad1744ea14e61e5fff99ee5282709b0c9d9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 31 09:43:30 2018 -0600

    block: get rid of q->softirq_done_fn()
    
    With the legacy path gone, all we do is funnel it through the
    mq_ops->complete() operation.
    
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c675e2b5af62..d4104844d6bb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -290,7 +290,6 @@ typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
 
 struct bio_vec;
-typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 
 enum blk_eh_timer_return {
@@ -407,7 +406,6 @@ struct request_queue {
 
 	make_request_fn		*make_request_fn;
 	poll_q_fn		*poll_fn;
-	softirq_done_fn		*softirq_done_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 
 	const struct blk_mq_ops	*mq_ops;
@@ -1113,7 +1111,6 @@ extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
-extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);

commit 7d692330e7cd581ccfee982334bf06b236cb999a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 24 10:48:12 2018 -0600

    block: get rid of blk_queued_rq()
    
    No point in hiding what this does, just open code it in the
    one spot where we are still using it.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e0c661a95c39..c675e2b5af62 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -673,8 +673,6 @@ static inline bool blk_account_rq(struct request *rq)
 
 #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
-/* rq->queuelist of dequeued request must be list_empty() */
-#define blk_queued_rq(rq)	(!list_empty(&(rq)->queuelist))
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 

commit db6d995235606191fa9db0c717e9d843200b71ea
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 2 08:46:15 2018 -0600

    block: remove request_list code
    
    It's now dead code, nobody uses it.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 877a3d235c45..e0c661a95c39 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -58,22 +58,6 @@ struct blk_stat_callback;
 
 typedef void (rq_end_io_fn)(struct request *, blk_status_t);
 
-struct request_list {
-	struct request_queue	*q;	/* the queue this rl belongs to */
-#ifdef CONFIG_BLK_CGROUP
-	struct blkcg_gq		*blkg;	/* blkg this request pool belongs to */
-#endif
-	/*
-	 * count[], starved[], and wait[] are indexed by
-	 * BLK_RW_SYNC/BLK_RW_ASYNC
-	 */
-	int			count[2];
-	int			starved[2];
-	mempool_t		*rq_pool;
-	wait_queue_head_t	wait[2];
-	unsigned int		flags;
-};
-
 /*
  * request flags */
 typedef __u32 __bitwise req_flags_t;
@@ -259,10 +243,6 @@ struct request {
 
 	/* for bidi */
 	struct request *next_rq;
-
-#ifdef CONFIG_BLK_CGROUP
-	struct request_list *rl;		/* rl this rq is alloced from */
-#endif
 };
 
 static inline bool blk_op_is_scsi(unsigned int op)
@@ -312,8 +292,6 @@ typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
 struct bio_vec;
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
-typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
-typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
 enum blk_eh_timer_return {
 	BLK_EH_DONE,		/* drivers has completed the command */
@@ -427,22 +405,10 @@ struct request_queue {
 	struct blk_queue_stats	*stats;
 	struct rq_qos		*rq_qos;
 
-	/*
-	 * If blkcg is not used, @q->root_rl serves all requests.  If blkcg
-	 * is used, root blkg allocates from @q->root_rl and all other
-	 * blkgs from their own blkg->rl.  Which one to use should be
-	 * determined using bio_request_list().
-	 */
-	struct request_list	root_rl;
-
 	make_request_fn		*make_request_fn;
 	poll_q_fn		*poll_fn;
 	softirq_done_fn		*softirq_done_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
-	/* Called just after a request is allocated */
-	init_rq_fn		*init_rq_fn;
-	/* Called just before a request is freed */
-	exit_rq_fn		*exit_rq_fn;
 
 	const struct blk_mq_ops	*mq_ops;
 

commit 1028e4b335665290dc563d5272f3c6b84e7fd66e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 09:47:17 2018 -0600

    bsg: move bsg-lib parts outside of request queue
    
    Get rid of the special bsg job fn and timeout handler, move them
    into a private bsg_set instead.
    
    Mostly from Christoph, with fixes for error handling and cleanups.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0364fc53f5c8..877a3d235c45 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -312,7 +312,6 @@ typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
 struct bio_vec;
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
-typedef int (bsg_job_fn) (struct bsg_job *);
 typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
 typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
@@ -321,8 +320,6 @@ enum blk_eh_timer_return {
 	BLK_EH_RESET_TIMER,	/* reset timer and try again */
 };
 
-typedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);
-
 enum blk_queue_state {
 	Queue_down,
 	Queue_up,
@@ -598,8 +595,6 @@ struct request_queue {
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
-	bsg_job_fn		*bsg_job_fn;
-	rq_timed_out_fn		*bsg_job_timeout_fn;
 	struct bsg_class_device bsg_dev;
 #endif
 

commit 4316b79e4321d4140164e42f228778e5bc66c84f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:25:07 2018 -0600

    block: kill legacy parts of timeout handling
    
    The only user of legacy timing now is BSG, which is invoked
    from the mq timeout handler. Kill the legacy code, and rename
    the q->rq_timed_out_fn to q->bsg_job_timeout_fn.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c502a7f40e84..0364fc53f5c8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -441,7 +441,6 @@ struct request_queue {
 	make_request_fn		*make_request_fn;
 	poll_q_fn		*poll_fn;
 	softirq_done_fn		*softirq_done_fn;
-	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	/* Called just after a request is allocated */
 	init_rq_fn		*init_rq_fn;
@@ -541,7 +540,6 @@ struct request_queue {
 
 	struct timer_list	timeout;
 	struct work_struct	timeout_work;
-	struct list_head	timeout_list;
 
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
@@ -601,6 +599,7 @@ struct request_queue {
 
 #if defined(CONFIG_BLK_DEV_BSG)
 	bsg_job_fn		*bsg_job_fn;
+	rq_timed_out_fn		*bsg_job_timeout_fn;
 	struct bsg_class_device bsg_dev;
 #endif
 
@@ -1156,7 +1155,6 @@ extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
-extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);

commit 92bc5a24844ada9b010f03c49a493e3edeadaa54
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 24 13:52:28 2018 -0600

    block: remove __blk_put_request()
    
    Now there's no difference between blk_put_request() and
    __blk_put_request() anymore, get rid of the underscore version and
    convert the few callers.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a9f6db8abcda..c502a7f40e84 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -906,7 +906,6 @@ extern blk_qc_t direct_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
-extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
 				       blk_mq_req_flags_t flags);
 extern int blk_lld_busy(struct request_queue *q);

commit a1ce35fa49852db60fc6e268038530be533c5b15
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:23:51 2018 -0600

    block: remove dead elevator code
    
    This removes a bunch of core and elevator related code. On the core
    front, we remove anything related to queue running, draining,
    initialization, plugging, and congestions. We also kill anything
    related to request allocation, merging, retrieval, and completion.
    
    Remove any checking for single queue IO schedulers, as they no
    longer exist. This means we can also delete a bunch of code related
    to request issue, adding, completion, etc - and all the SQ related
    ops and helpers.
    
    Also kill the load_default_modules(), as all that did was provide
    for a way to load the default single queue elevator.
    
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8afe3331777e..a9f6db8abcda 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -58,9 +58,6 @@ struct blk_stat_callback;
 
 typedef void (rq_end_io_fn)(struct request *, blk_status_t);
 
-#define BLK_RL_SYNCFULL		(1U << 0)
-#define BLK_RL_ASYNCFULL	(1U << 1)
-
 struct request_list {
 	struct request_queue	*q;	/* the queue this rl belongs to */
 #ifdef CONFIG_BLK_CGROUP
@@ -309,11 +306,8 @@ static inline unsigned short req_get_ioprio(struct request *req)
 
 struct blk_queue_ctx;
 
-typedef void (request_fn_proc) (struct request_queue *q);
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
-typedef int (prep_rq_fn) (struct request_queue *, struct request *);
-typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 
 struct bio_vec;
 typedef void (softirq_done_fn)(struct request *);
@@ -432,8 +426,6 @@ struct request_queue {
 	struct list_head	queue_head;
 	struct request		*last_merge;
 	struct elevator_queue	*elevator;
-	int			nr_rqs[2];	/* # allocated [a]sync rqs */
-	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
 	struct blk_queue_stats	*stats;
 	struct rq_qos		*rq_qos;
@@ -446,11 +438,8 @@ struct request_queue {
 	 */
 	struct request_list	root_rl;
 
-	request_fn_proc		*request_fn;
 	make_request_fn		*make_request_fn;
 	poll_q_fn		*poll_fn;
-	prep_rq_fn		*prep_rq_fn;
-	unprep_rq_fn		*unprep_rq_fn;
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
@@ -458,8 +447,6 @@ struct request_queue {
 	init_rq_fn		*init_rq_fn;
 	/* Called just before a request is freed */
 	exit_rq_fn		*exit_rq_fn;
-	/* Called from inside blk_get_request() */
-	void (*initialize_rq_fn)(struct request *rq);
 
 	const struct blk_mq_ops	*mq_ops;
 
@@ -475,17 +462,6 @@ struct request_queue {
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
 	unsigned int		nr_hw_queues;
 
-	/*
-	 * Dispatch queue sorting
-	 */
-	sector_t		end_sector;
-	struct request		*boundary_rq;
-
-	/*
-	 * Delayed queue handling
-	 */
-	struct delayed_work	delay_work;
-
 	struct backing_dev_info	*backing_dev_info;
 
 	/*
@@ -548,9 +524,6 @@ struct request_queue {
 	 * queue settings
 	 */
 	unsigned long		nr_requests;	/* Max # of requests */
-	unsigned int		nr_congestion_on;
-	unsigned int		nr_congestion_off;
-	unsigned int		nr_batching;
 
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
@@ -560,13 +533,6 @@ struct request_queue {
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
 
-	/*
-	 * Number of active block driver functions for which blk_drain_queue()
-	 * must wait. Must be incremented around functions that unlock the
-	 * queue_lock internally, e.g. scsi_request_fn().
-	 */
-	unsigned int		request_fn_active;
-
 	unsigned int		rq_timeout;
 	int			poll_nsec;
 
@@ -740,11 +706,6 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 extern void blk_set_pm_only(struct request_queue *q);
 extern void blk_clear_pm_only(struct request_queue *q);
 
-static inline int queue_in_flight(struct request_queue *q)
-{
-	return q->in_flight[0] + q->in_flight[1];
-}
-
 static inline bool blk_account_rq(struct request *rq)
 {
 	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
@@ -765,7 +726,7 @@ static inline bool blk_account_rq(struct request *rq)
  */
 static inline bool queue_is_rq_based(struct request_queue *q)
 {
-	return q->request_fn || q->mq_ops;
+	return q->mq_ops;
 }
 
 static inline unsigned int blk_queue_cluster(struct request_queue *q)
@@ -828,27 +789,6 @@ static inline bool rq_is_sync(struct request *rq)
 	return op_is_sync(rq->cmd_flags);
 }
 
-static inline bool blk_rl_full(struct request_list *rl, bool sync)
-{
-	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
-
-	return rl->flags & flag;
-}
-
-static inline void blk_set_rl_full(struct request_list *rl, bool sync)
-{
-	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
-
-	rl->flags |= flag;
-}
-
-static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
-{
-	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
-
-	rl->flags &= ~flag;
-}
-
 static inline bool rq_mergeable(struct request *rq)
 {
 	if (blk_rq_is_passthrough(rq))
@@ -969,7 +909,6 @@ extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
 				       blk_mq_req_flags_t flags);
-extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 			     struct bio_set *bs, gfp_t gfp_mask,
@@ -979,7 +918,6 @@ extern void blk_rq_unprep_clone(struct request *rq);
 extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern int blk_rq_append_bio(struct request *rq, struct bio **bio);
-extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_queue_split(struct request_queue *, struct bio **);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
@@ -992,15 +930,7 @@ extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 
 extern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);
 extern void blk_queue_exit(struct request_queue *q);
-extern void blk_start_queue(struct request_queue *q);
-extern void blk_start_queue_async(struct request_queue *q);
-extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
-extern void __blk_stop_queue(struct request_queue *q);
-extern void __blk_run_queue(struct request_queue *q);
-extern void __blk_run_queue_uncond(struct request_queue *q);
-extern void blk_run_queue(struct request_queue *);
-extern void blk_run_queue_async(struct request_queue *q);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,
 			   gfp_t);
@@ -1155,13 +1085,6 @@ static inline unsigned int blk_rq_count_bios(struct request *rq)
 	return nr_bios;
 }
 
-/*
- * Request issue related functions.
- */
-extern struct request *blk_peek_request(struct request_queue *q);
-extern void blk_start_request(struct request *rq);
-extern struct request *blk_fetch_request(struct request_queue *q);
-
 void blk_steal_bios(struct bio_list *list, struct request *rq);
 
 /*
@@ -1179,9 +1102,6 @@ void blk_steal_bios(struct bio_list *list, struct request *rq);
  */
 extern bool blk_update_request(struct request *rq, blk_status_t error,
 			       unsigned int nr_bytes);
-extern void blk_finish_request(struct request *rq, blk_status_t error);
-extern bool blk_end_request(struct request *rq, blk_status_t error,
-			    unsigned int nr_bytes);
 extern void blk_end_request_all(struct request *rq, blk_status_t error);
 extern bool __blk_end_request(struct request *rq, blk_status_t error,
 			      unsigned int nr_bytes);
@@ -1190,15 +1110,10 @@ extern bool __blk_end_request_cur(struct request *rq, blk_status_t error);
 
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
-extern void blk_unprep_request(struct request *);
 
 /*
  * Access functions for manipulating queue properties
  */
-extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
-					spinlock_t *lock, int node_id);
-extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
-extern int blk_init_allocated_queue(struct request_queue *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
@@ -1239,8 +1154,6 @@ extern int blk_queue_dma_drain(struct request_queue *q,
 			       void *buf, unsigned int size);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
-extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
-extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
@@ -1298,7 +1211,6 @@ extern void blk_set_queue_dying(struct request_queue *);
  * schedule() where blk_schedule_flush_plug() is called.
  */
 struct blk_plug {
-	struct list_head list; /* requests */
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
 };
@@ -1339,8 +1251,7 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 	struct blk_plug *plug = tsk->plug;
 
 	return plug &&
-		(!list_empty(&plug->list) ||
-		 !list_empty(&plug->mq_list) ||
+		 (!list_empty(&plug->mq_list) ||
 		 !list_empty(&plug->cb_list));
 }
 

commit 7ca01926463a15f5d2681458643b2453930b873a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 24 03:39:36 2018 -0600

    block: remove legacy rq tagging
    
    It's now unused, kill it.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6baea6563364..8afe3331777e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -85,8 +85,6 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_SORTED		((__force req_flags_t)(1 << 0))
 /* drive already may have started this one */
 #define RQF_STARTED		((__force req_flags_t)(1 << 1))
-/* uses tagged queueing */
-#define RQF_QUEUED		((__force req_flags_t)(1 << 2))
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
@@ -336,15 +334,6 @@ enum blk_queue_state {
 	Queue_up,
 };
 
-struct blk_queue_tag {
-	struct request **tag_index;	/* map of busy tags */
-	unsigned long *tag_map;		/* bit map of free/busy tags */
-	int max_depth;			/* what we will send to device */
-	int real_max_depth;		/* what the array can hold */
-	atomic_t refcnt;		/* map can be shared */
-	int alloc_policy;		/* tag allocation policy */
-	int next_tag;			/* next tag */
-};
 #define BLK_TAG_ALLOC_FIFO 0 /* allocate starting from 0 */
 #define BLK_TAG_ALLOC_RR 1 /* allocate starting from last allocated tag */
 
@@ -568,8 +557,6 @@ struct request_queue {
 	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 
-	struct blk_queue_tag	*queue_tags;
-
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
 
@@ -680,7 +667,6 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
-#define QUEUE_FLAG_QUEUED	0	/* uses generic tag queueing */
 #define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 #define QUEUE_FLAG_DYING	2	/* queue being torn down */
 #define QUEUE_FLAG_BYPASS	3	/* act as dumb FIFO queue */
@@ -724,7 +710,6 @@ void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 
-#define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
@@ -1359,26 +1344,6 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 		 !list_empty(&plug->cb_list));
 }
 
-/*
- * tag stuff
- */
-extern int blk_queue_start_tag(struct request_queue *, struct request *);
-extern struct request *blk_queue_find_tag(struct request_queue *, int);
-extern void blk_queue_end_tag(struct request_queue *, struct request *);
-extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *, int);
-extern void blk_queue_free_tags(struct request_queue *);
-extern int blk_queue_resize_tags(struct request_queue *, int);
-extern struct blk_queue_tag *blk_init_tags(int, int);
-extern void blk_free_tags(struct blk_queue_tag *);
-
-static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
-						int tag)
-{
-	if (unlikely(bqt == NULL || tag >= bqt->real_max_depth))
-		return NULL;
-	return bqt->tag_index[tag];
-}
-
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);

commit 771a93c489bf486b957c7399f89ee06d43ba2d93
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 22 05:12:32 2018 -0600

    block: remove blk_complete_request()
    
    It's now unused.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e867733b761d..6baea6563364 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1203,7 +1203,6 @@ extern bool __blk_end_request(struct request *rq, blk_status_t error,
 extern void __blk_end_request_all(struct request *rq, blk_status_t error);
 extern bool __blk_end_request_cur(struct request *rq, blk_status_t error);
 
-extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
 extern void blk_unprep_request(struct request *);

commit c6f2882691e8fd128083abdcc3c5aa5b410c2367
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 29 10:22:19 2018 -0600

    block: remove q->lld_busy_fn()
    
    Nobody is using the legacy path for blk_lld_busy() anymore, remove
    it.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4293dc1cd160..e867733b761d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -320,7 +320,6 @@ typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 struct bio_vec;
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
-typedef int (lld_busy_fn) (struct request_queue *q);
 typedef int (bsg_job_fn) (struct bsg_job *);
 typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
 typedef void (exit_rq_fn)(struct request_queue *, struct request *);
@@ -466,7 +465,6 @@ struct request_queue {
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
-	lld_busy_fn		*lld_busy_fn;
 	/* Called just after a request is allocated */
 	init_rq_fn		*init_rq_fn;
 	/* Called just before a request is freed */
@@ -1255,7 +1253,6 @@ extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
 extern int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size);
-extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);

commit bf5054569653c491ece544cc7ee333ae53b47121
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Fri Oct 12 19:08:50 2018 +0900

    block: Introduce blk_revalidate_disk_zones()
    
    Drivers exposing zoned block devices have to initialize and maintain
    correctness (i.e. revalidate) of the device zone bitmaps attached to
    the device request queue (seq_zones_bitmap and seq_zones_wlock).
    
    To simplify coding this, introduce a generic helper function
    blk_revalidate_disk_zones() suitable for most (and likely all) cases.
    This new function always update the seq_zones_bitmap and seq_zones_wlock
    bitmaps as well as the queue nr_zones field when called for a disk
    using a request based queue. For a disk using a BIO based queue, only
    the number of zones is updated since these queues do not have
    schedulers and so do not need the zone bitmaps.
    
    With this change, the zone bitmap initialization code in sd_zbc.c can be
    replaced with a call to this function in sd_zbc_read_zones(), which is
    called from the disk revalidate block operation method.
    
    A call to blk_revalidate_disk_zones() is also added to the null_blk
    driver for devices created with the zoned mode enabled.
    
    Finally, to ensure that zoned devices created with dm-linear or
    dm-flakey expose the correct number of zones through sysfs, a call to
    blk_revalidate_disk_zones() is added to dm_table_set_restrictions().
    
    The zone bitmaps allocated and initialized with
    blk_revalidate_disk_zones() are freed automatically from
    __blk_release_queue() using the block internal function
    blk_queue_free_zone_bitmaps().
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 51fe6472ce02..4293dc1cd160 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -402,6 +402,7 @@ extern int blkdev_report_zones(struct block_device *bdev,
 			       unsigned int *nr_zones, gfp_t gfp_mask);
 extern int blkdev_reset_zones(struct block_device *bdev, sector_t sectors,
 			      sector_t nr_sectors, gfp_t gfp_mask);
+extern int blk_revalidate_disk_zones(struct gendisk *disk);
 
 extern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
 				     unsigned int cmd, unsigned long arg);
@@ -414,6 +415,12 @@ static inline unsigned int blkdev_nr_zones(struct block_device *bdev)
 {
 	return 0;
 }
+
+static inline int blk_revalidate_disk_zones(struct gendisk *disk)
+{
+	return 0;
+}
+
 static inline int blkdev_report_zones_ioctl(struct block_device *bdev,
 					    fmode_t mode, unsigned int cmd,
 					    unsigned long arg)

commit e76239a3748c90a8b0e197f8f4544a8ce52f126e
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 12 19:08:49 2018 +0900

    block: add a report_zones method
    
    Dispatching a report zones command through the request queue is a major
    pain due to the command reply payload rewriting necessary. Given that
    blkdev_report_zones() is executing everything synchronously, implement
    report zones as a block device file operation instead, allowing major
    simplification of the code in many places.
    
    sd, null-blk, dm-linear and dm-flakey being the only block device
    drivers supporting exposing zoned block devices, these drivers are
    modified to provide the device side implementation of the
    report_zones() block device file operation.
    
    For device mappers, a new report_zones() target type operation is
    defined so that the upper block layer calls blkdev_report_zones() can
    be propagated down to the underlying devices of the dm targets.
    Implementation for this new operation is added to the dm-linear and
    dm-flakey targets.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    [Damien]
    * Changed method block_device argument to gendisk
    * Various bug fixes and improvements
    * Added support for null_blk, dm-linear and dm-flakey.
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6bb845f9601a..51fe6472ce02 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -396,11 +396,6 @@ struct queue_limits {
 
 #ifdef CONFIG_BLK_DEV_ZONED
 
-struct blk_zone_report_hdr {
-	unsigned int	nr_zones;
-	u8		padding[60];
-};
-
 extern unsigned int blkdev_nr_zones(struct block_device *bdev);
 extern int blkdev_report_zones(struct block_device *bdev,
 			       sector_t sector, struct blk_zone *zones,
@@ -1867,6 +1862,9 @@ struct block_device_operations {
 	int (*getgeo)(struct block_device *, struct hd_geometry *);
 	/* this callback is with swap_lock and sometimes page table lock held */
 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
+	int (*report_zones)(struct gendisk *, sector_t sector,
+			    struct blk_zone *zones, unsigned int *nr_zones,
+			    gfp_t gfp_mask);
 	struct module *owner;
 	const struct pr_ops *pr_ops;
 };

commit 965b652e901886ea54f93c60027b5be76328d958
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Fri Oct 12 19:08:48 2018 +0900

    block: Expose queue nr_zones in sysfs
    
    Expose through sysfs the nr_zones field of struct request_queue.
    Exposing this value helps in debugging disk issues as well as
    facilitating scripts based use of the disk (e.g. blktests).
    
    For zoned block devices, the nr_zones field indicates the total number
    of zones of the device calculated using the known disk capacity and
    zone size. This number of zones is always 0 for regular block devices.
    
    Since nr_zones is defined conditionally with CONFIG_BLK_DEV_ZONED,
    introduce the blk_queue_nr_zones() function to return the correct value
    for any device, regardless if CONFIG_BLK_DEV_ZONED is set.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ca5fdc1b7745..6bb845f9601a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -811,6 +811,11 @@ static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
 }
 
 #ifdef CONFIG_BLK_DEV_ZONED
+static inline unsigned int blk_queue_nr_zones(struct request_queue *q)
+{
+	return blk_queue_is_zoned(q) ? q->nr_zones : 0;
+}
+
 static inline unsigned int blk_queue_zone_no(struct request_queue *q,
 					     sector_t sector)
 {
@@ -826,6 +831,11 @@ static inline bool blk_queue_zone_is_seq(struct request_queue *q,
 		return false;
 	return test_bit(blk_queue_zone_no(q, sector), q->seq_zones_bitmap);
 }
+#else /* CONFIG_BLK_DEV_ZONED */
+static inline unsigned int blk_queue_nr_zones(struct request_queue *q)
+{
+	return 0;
+}
 #endif /* CONFIG_BLK_DEV_ZONED */
 
 static inline bool rq_is_sync(struct request *rq)

commit a91e138022bc29b5d2bbc56b41de3e0db6261e28
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Fri Oct 12 19:08:43 2018 +0900

    block: Introduce blkdev_nr_zones() helper
    
    Introduce the blkdev_nr_zones() helper function to get the total
    number of zones of a zoned block device. This number is always 0 for a
    regular block device (q->limits.zoned == BLK_ZONED_NONE case).
    
    Replace hard-coded number of zones calculation in dmz_get_zoned_device()
    with a call to this helper.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7d423721b327..ca5fdc1b7745 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -401,6 +401,7 @@ struct blk_zone_report_hdr {
 	u8		padding[60];
 };
 
+extern unsigned int blkdev_nr_zones(struct block_device *bdev);
 extern int blkdev_report_zones(struct block_device *bdev,
 			       sector_t sector, struct blk_zone *zones,
 			       unsigned int *nr_zones, gfp_t gfp_mask);
@@ -414,6 +415,10 @@ extern int blkdev_reset_zones_ioctl(struct block_device *bdev, fmode_t mode,
 
 #else /* CONFIG_BLK_DEV_ZONED */
 
+static inline unsigned int blkdev_nr_zones(struct block_device *bdev)
+{
+	return 0;
+}
 static inline int blkdev_report_zones_ioctl(struct block_device *bdev,
 					    fmode_t mode, unsigned int cmd,
 					    unsigned long arg)

commit bd6bf7c10484f026505814b690104cdef27ed460
Merge: a41efc2a0f68 663569db6476
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 06:50:48 2018 -0700

    Merge tag 'pci-v4.20-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci
    
    Pull PCI updates from Bjorn Helgaas:
    
     - Fix ASPM link_state teardown on removal (Lukas Wunner)
    
     - Fix misleading _OSC ASPM message (Sinan Kaya)
    
     - Make _OSC optional for PCI (Sinan Kaya)
    
     - Don't initialize ASPM link state when ACPI_FADT_NO_ASPM is set
       (Patrick Talbert)
    
     - Remove x86 and arm64 node-local allocation for host bridge structures
       (Punit Agrawal)
    
     - Pay attention to device-specific _PXM node values (Jonathan Cameron)
    
     - Support new Immediate Readiness bit (Felipe Balbi)
    
     - Differentiate between pciehp surprise and safe removal (Lukas Wunner)
    
     - Remove unnecessary pciehp includes (Lukas Wunner)
    
     - Drop pciehp hotplug_slot_ops wrappers (Lukas Wunner)
    
     - Tolerate PCIe Slot Presence Detect being hardwired to zero to
       workaround broken hardware, e.g., the Wilocity switch/wireless device
       (Lukas Wunner)
    
     - Unify pciehp controller & slot structs (Lukas Wunner)
    
     - Constify hotplug_slot_ops (Lukas Wunner)
    
     - Drop hotplug_slot_info (Lukas Wunner)
    
     - Embed hotplug_slot struct into users instead of allocating it
       separately (Lukas Wunner)
    
     - Initialize PCIe port service drivers directly instead of relying on
       initcall ordering (Keith Busch)
    
     - Restore PCI config state after a slot reset (Keith Busch)
    
     - Save/restore DPC config state along with other PCI config state
       (Keith Busch)
    
     - Reference count devices during AER handling to avoid race issue with
       concurrent hot removal (Keith Busch)
    
     - If an Upstream Port reports ERR_FATAL, don't try to read the Port's
       config space because it is probably unreachable (Keith Busch)
    
     - During error handling, use slot-specific reset instead of secondary
       bus reset to avoid link up/down issues on hotplug ports (Keith Busch)
    
     - Restore previous AER/DPC handling that does not remove and
       re-enumerate devices on ERR_FATAL (Keith Busch)
    
     - Notify all drivers that may be affected by error recovery resets
       (Keith Busch)
    
     - Always generate error recovery uevents, even if a driver doesn't have
       error callbacks (Keith Busch)
    
     - Make PCIe link active reporting detection generic (Keith Busch)
    
     - Support D3cold in PCIe hierarchies during system sleep and runtime,
       including hotplug and Thunderbolt ports (Mika Westerberg)
    
     - Handle hpmemsize/hpiosize kernel parameters uniformly, whether slots
       are empty or occupied (Jon Derrick)
    
     - Remove duplicated include from pci/pcie/err.c and unused variable
       from cpqphp (YueHaibing)
    
     - Remove driver pci_cleanup_aer_uncorrect_error_status() calls (Oza
       Pawandeep)
    
     - Uninline PCI bus accessors for better ftracing (Keith Busch)
    
     - Remove unused AER Root Port .error_resume method (Keith Busch)
    
     - Use kfifo in AER instead of a local version (Keith Busch)
    
     - Use threaded IRQ in AER bottom half (Keith Busch)
    
     - Use managed resources in AER core (Keith Busch)
    
     - Reuse pcie_port_find_device() for AER injection (Keith Busch)
    
     - Abstract AER interrupt handling to disconnect error injection (Keith
       Busch)
    
     - Refactor AER injection callbacks to simplify future improvments
       (Keith Busch)
    
     - Remove unused Netronome NFP32xx Device IDs (Jakub Kicinski)
    
     - Use bitmap_zalloc() for dma_alias_mask (Andy Shevchenko)
    
     - Add switch fall-through annotations (Gustavo A. R. Silva)
    
     - Remove unused Switchtec quirk variable (Joshua Abraham)
    
     - Fix pci.c kernel-doc warning (Randy Dunlap)
    
     - Remove trivial PCI wrappers for DMA APIs (Christoph Hellwig)
    
     - Add Intel GPU device IDs to spurious interrupt quirk (Bin Meng)
    
     - Run Switchtec DMA aliasing quirk only on NTB endpoints to avoid
       useless dmesg errors (Logan Gunthorpe)
    
     - Update Switchtec NTB documentation (Wesley Yung)
    
     - Remove redundant "default n" from Kconfig (Bartlomiej Zolnierkiewicz)
    
     - Avoid panic when drivers enable MSI/MSI-X twice (Tonghao Zhang)
    
     - Add PCI support for peer-to-peer DMA (Logan Gunthorpe)
    
     - Add sysfs group for PCI peer-to-peer memory statistics (Logan
       Gunthorpe)
    
     - Add PCI peer-to-peer DMA scatterlist mapping interface (Logan
       Gunthorpe)
    
     - Add PCI configfs/sysfs helpers for use by peer-to-peer users (Logan
       Gunthorpe)
    
     - Add PCI peer-to-peer DMA driver writer's documentation (Logan
       Gunthorpe)
    
     - Add block layer flag to indicate driver support for PCI peer-to-peer
       DMA (Logan Gunthorpe)
    
     - Map Infiniband scatterlists for peer-to-peer DMA if they contain P2P
       memory (Logan Gunthorpe)
    
     - Register nvme-pci CMB buffer as PCI peer-to-peer memory (Logan
       Gunthorpe)
    
     - Add nvme-pci support for PCI peer-to-peer memory in requests (Logan
       Gunthorpe)
    
     - Use PCI peer-to-peer memory in nvme (Stephen Bates, Steve Wise,
       Christoph Hellwig, Logan Gunthorpe)
    
     - Cache VF config space size to optimize enumeration of many VFs
       (KarimAllah Ahmed)
    
     - Remove unnecessary <linux/pci-ats.h> include (Bjorn Helgaas)
    
     - Fix VMD AERSID quirk Device ID matching (Jon Derrick)
    
     - Fix Cadence PHY handling during probe (Alan Douglas)
    
     - Signal Cadence Endpoint interrupts via AXI region 0 instead of last
       region (Alan Douglas)
    
     - Write Cadence Endpoint MSI interrupts with 32 bits of data (Alan
       Douglas)
    
     - Remove redundant controller tests for "device_type == pci" (Rob
       Herring)
    
     - Document R-Car E3 (R8A77990) bindings (Tho Vu)
    
     - Add device tree support for R-Car r8a7744 (Biju Das)
    
     - Drop unused mvebu PCIe capability code (Thomas Petazzoni)
    
     - Add shared PCI bridge emulation code (Thomas Petazzoni)
    
     - Convert mvebu to use shared PCI bridge emulation (Thomas Petazzoni)
    
     - Add aardvark Root Port emulation (Thomas Petazzoni)
    
     - Support 100MHz/200MHz refclocks for i.MX6 (Lucas Stach)
    
     - Add initial power management for i.MX7 (Leonard Crestez)
    
     - Add PME_Turn_Off support for i.MX7 (Leonard Crestez)
    
     - Fix qcom runtime power management error handling (Bjorn Andersson)
    
     - Update TI dra7xx unaligned access errata workaround for host mode as
       well as endpoint mode (Vignesh R)
    
     - Fix kirin section mismatch warning (Nathan Chancellor)
    
     - Remove iproc PAXC slot check to allow VF support (Jitendra Bhivare)
    
     - Quirk Keystone K2G to limit MRRS to 256 (Kishon Vijay Abraham I)
    
     - Update Keystone to use MRRS quirk for host bridge instead of open
       coding (Kishon Vijay Abraham I)
    
     - Refactor Keystone link establishment (Kishon Vijay Abraham I)
    
     - Simplify and speed up Keystone link training (Kishon Vijay Abraham I)
    
     - Remove unused Keystone host_init argument (Kishon Vijay Abraham I)
    
     - Merge Keystone driver files into one (Kishon Vijay Abraham I)
    
     - Remove redundant Keystone platform_set_drvdata() (Kishon Vijay
       Abraham I)
    
     - Rename Keystone functions for uniformity (Kishon Vijay Abraham I)
    
     - Add Keystone device control module DT binding (Kishon Vijay Abraham
       I)
    
     - Use SYSCON API to get Keystone control module device IDs (Kishon
       Vijay Abraham I)
    
     - Clean up Keystone PHY handling (Kishon Vijay Abraham I)
    
     - Use runtime PM APIs to enable Keystone clock (Kishon Vijay Abraham I)
    
     - Clean up Keystone config space access checks (Kishon Vijay Abraham I)
    
     - Get Keystone outbound window count from DT (Kishon Vijay Abraham I)
    
     - Clean up Keystone outbound window configuration (Kishon Vijay Abraham
       I)
    
     - Clean up Keystone DBI setup (Kishon Vijay Abraham I)
    
     - Clean up Keystone ks_pcie_link_up() (Kishon Vijay Abraham I)
    
     - Fix Keystone IRQ status checking (Kishon Vijay Abraham I)
    
     - Add debug messages for all Keystone errors (Kishon Vijay Abraham I)
    
     - Clean up Keystone includes and macros (Kishon Vijay Abraham I)
    
     - Fix Mediatek unchecked return value from devm_pci_remap_iospace()
       (Gustavo A. R. Silva)
    
     - Fix Mediatek endpoint/port matching logic (Honghui Zhang)
    
     - Change Mediatek Root Port Class Code to PCI_CLASS_BRIDGE_PCI (Honghui
       Zhang)
    
     - Remove redundant Mediatek PM domain check (Honghui Zhang)
    
     - Convert Mediatek to pci_host_probe() (Honghui Zhang)
    
     - Fix Mediatek MSI enablement (Honghui Zhang)
    
     - Add Mediatek system PM support for MT2712 and MT7622 (Honghui Zhang)
    
     - Add Mediatek loadable module support (Honghui Zhang)
    
     - Detach VMD resources after stopping root bus to prevent orphan
       resources (Jon Derrick)
    
     - Convert pcitest build process to that used by other tools (iio, perf,
       etc) (Gustavo Pimentel)
    
    * tag 'pci-v4.20-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci: (140 commits)
      PCI/AER: Refactor error injection fallbacks
      PCI/AER: Abstract AER interrupt handling
      PCI/AER: Reuse existing pcie_port_find_device() interface
      PCI/AER: Use managed resource allocations
      PCI: pcie: Remove redundant 'default n' from Kconfig
      PCI: aardvark: Implement emulated root PCI bridge config space
      PCI: mvebu: Convert to PCI emulated bridge config space
      PCI: mvebu: Drop unused PCI express capability code
      PCI: Introduce PCI bridge emulated config space common logic
      PCI: vmd: Detach resources after stopping root bus
      nvmet: Optionally use PCI P2P memory
      nvmet: Introduce helper functions to allocate and free request SGLs
      nvme-pci: Add support for P2P memory in requests
      nvme-pci: Use PCI p2pmem subsystem to manage the CMB
      IB/core: Ensure we map P2P memory correctly in rdma_rw_ctx_[init|destroy]()
      block: Add PCI P2P flag for request queue
      PCI/P2PDMA: Add P2P DMA driver writer's documentation
      docs-rst: Add a new directory for PCI documentation
      PCI/P2PDMA: Introduce configfs/sysfs enable attribute helpers
      PCI/P2PDMA: Add PCI p2pmem DMA mappings to adjust the bus offset
      ...

commit 49d92c0dd64ae769c2d67fe27ac260ae31259ba6
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Thu Oct 4 15:27:41 2018 -0600

    block: Add PCI P2P flag for request queue
    
    Add QUEUE_FLAG_PCI_P2P, meaning a driver's request queue supports targeting
    P2P memory.  This will be used by P2P providers and orchestrators (in
    subsequent patches) to ensure block devices can support P2P memory before
    submitting P2P-backed pages to submit_bio().
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6980014357d4..c32f7171899b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -699,6 +699,7 @@ struct request_queue {
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 #define QUEUE_FLAG_PREEMPT_ONLY	29	/* only process REQ_PREEMPT requests */
+#define QUEUE_FLAG_PCI_P2PDMA  30	/* device supports PCI p2p requests */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
@@ -731,6 +732,8 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 #define blk_queue_dax(q)	test_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)
 #define blk_queue_scsi_passthrough(q)	\
 	test_bit(QUEUE_FLAG_SCSI_PASSTHROUGH, &(q)->queue_flags)
+#define blk_queue_pci_p2pdma(q)	\
+	test_bit(QUEUE_FLAG_PCI_P2PDMA, &(q)->queue_flags)
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \

commit 4822e902f9bdffaea2817471365e000966f0d1a1
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Thu Oct 11 10:07:06 2018 +0300

    block: describe difference between flags IO_STAT and STATS
    
    This adds reasonable comments, but they definitely needs better names.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dee46c20701b..61207560e826 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -108,7 +108,7 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_QUIET		((__force req_flags_t)(1 << 11))
 /* elevator private data attached */
 #define RQF_ELVPRIV		((__force req_flags_t)(1 << 12))
-/* account I/O stat */
+/* account into disk and partition IO statistics */
 #define RQF_IO_STAT		((__force req_flags_t)(1 << 13))
 /* request came from our alloc pool */
 #define RQF_ALLOCED		((__force req_flags_t)(1 << 14))
@@ -116,7 +116,7 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_PM			((__force req_flags_t)(1 << 15))
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
-/* IO stats tracking on */
+/* track IO completion time */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
@@ -685,7 +685,7 @@ struct request_queue {
 #define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
 #define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
-#define QUEUE_FLAG_IO_STAT     10	/* do IO stats */
+#define QUEUE_FLAG_IO_STAT     10	/* do disk/partitions IO accounting */
 #define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
 #define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
@@ -699,7 +699,7 @@ struct request_queue {
 #define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
 #define QUEUE_FLAG_FLUSH_NQ    22	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         23	/* device supports DAX */
-#define QUEUE_FLAG_STATS       24	/* track rq completion times */
+#define QUEUE_FLAG_STATS       24	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */

commit c0aac682fa6590cb660cb083dbc09f55e799d2d2
Merge: 451bb7c33197 17b57b1883c1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 1 08:58:57 2018 -0600

    Merge tag 'v4.19-rc6' into for-4.20/block
    
    Merge -rc6 in, for two reasons:
    
    1) Resolve a trivial conflict in the blk-mq-tag.c documentation
    2) A few important regression fixes went into upstream directly, so
       they aren't in the 4.20 branch.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    * tag 'v4.19-rc6': (780 commits)
      Linux 4.19-rc6
      MAINTAINERS: fix reference to moved drivers/{misc => auxdisplay}/panel.c
      cpufreq: qcom-kryo: Fix section annotations
      perf/core: Add sanity check to deal with pinned event failure
      xen/blkfront: correct purging of persistent grants
      Revert "xen/blkfront: When purging persistent grants, keep them in the buffer"
      selftests/powerpc: Fix Makefiles for headers_install change
      blk-mq: I/O and timer unplugs are inverted in blktrace
      dax: Fix deadlock in dax_lock_mapping_entry()
      x86/boot: Fix kexec booting failure in the SEV bit detection code
      bcache: add separate workqueue for journal_write to avoid deadlock
      drm/amd/display: Fix Edid emulation for linux
      drm/amd/display: Fix Vega10 lightup on S3 resume
      drm/amdgpu: Fix vce work queue was not cancelled when suspend
      Revert "drm/panel: Add device_link from panel device to DRM device"
      xen/blkfront: When purging persistent grants, keep them in the buffer
      clocksource/drivers/timer-atmel-pit: Properly handle error cases
      block: fix deadline elevator drain for zoned block devices
      ACPI / hotplug / PCI: Don't scan for non-hotplug bridges if slot is not bridge
      drm/syncobj: Don't leak fences when WAIT_FOR_SUBMIT is set
      ...
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit cd84a62e0078dce09f4ed349bec84f86c9d54b30
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Sep 26 14:01:04 2018 -0700

    block, scsi: Change the preempt-only flag into a counter
    
    The RQF_PREEMPT flag is used for three purposes:
    - In the SCSI core, for making sure that power management requests
      are executed even if a device is in the "quiesced" state.
    - For domain validation by SCSI drivers that use the parallel port.
    - In the IDE driver, for IDE preempt requests.
    Rename "preempt-only" into "pm-only" because the primary purpose of
    this mode is power management. Since the power management core may
    but does not have to resume a runtime suspended device before
    performing system-wide suspend and since a later patch will set
    "pm-only" mode as long as a block device is runtime suspended, make
    it possible to set "pm-only" mode from more than one context. Since
    with this change scsi_device_quiesce() is no longer idempotent, make
    that function return early if it is called for a quiesced queue.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Jianchao Wang <jianchao.w.wang@oracle.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cd863511dedb..13bb54f26736 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -504,6 +504,12 @@ struct request_queue {
 	 * various queue flags, see QUEUE_* below
 	 */
 	unsigned long		queue_flags;
+	/*
+	 * Number of contexts that have called blk_set_pm_only(). If this
+	 * counter is above zero then only RQF_PM and RQF_PREEMPT requests are
+	 * processed.
+	 */
+	atomic_t		pm_only;
 
 	/*
 	 * ida allocated id for this queue.  Used to index queues from
@@ -698,7 +704,6 @@ struct request_queue {
 #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
-#define QUEUE_FLAG_PREEMPT_ONLY	29	/* only process REQ_PREEMPT requests */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
@@ -736,12 +741,11 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
-#define blk_queue_preempt_only(q)				\
-	test_bit(QUEUE_FLAG_PREEMPT_ONLY, &(q)->queue_flags)
+#define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
 
-extern int blk_set_preempt_only(struct request_queue *q);
-extern void blk_clear_preempt_only(struct request_queue *q);
+extern void blk_set_pm_only(struct request_queue *q);
+extern void blk_clear_pm_only(struct request_queue *q);
 
 static inline int queue_in_flight(struct request_queue *q)
 {

commit bca6b067b0b269a7b8ba129e2a918309ca8b4a55
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Sep 26 14:01:03 2018 -0700

    block: Move power management code into a new source file
    
    Move the code for runtime power management from blk-core.c into the
    new source file blk-pm.c. Move the corresponding declarations from
    <linux/blkdev.h> into <linux/blk-pm.h>. For CONFIG_PM=n, leave out
    the declarations of the functions that are not used in that mode.
    This patch not only reduces the number of #ifdefs in the block layer
    core code but also reduces the size of header file <linux/blkdev.h>
    and hence should help to reduce the build time of the Linux kernel
    if CONFIG_PM is not defined.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Jianchao Wang <jianchao.w.wang@oracle.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1d5e14139795..cd863511dedb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1280,29 +1280,6 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
 extern void blk_put_queue(struct request_queue *);
 extern void blk_set_queue_dying(struct request_queue *);
 
-/*
- * block layer runtime pm functions
- */
-#ifdef CONFIG_PM
-extern void blk_pm_runtime_init(struct request_queue *q, struct device *dev);
-extern int blk_pre_runtime_suspend(struct request_queue *q);
-extern void blk_post_runtime_suspend(struct request_queue *q, int err);
-extern void blk_pre_runtime_resume(struct request_queue *q);
-extern void blk_post_runtime_resume(struct request_queue *q, int err);
-extern void blk_set_runtime_active(struct request_queue *q);
-#else
-static inline void blk_pm_runtime_init(struct request_queue *q,
-	struct device *dev) {}
-static inline int blk_pre_runtime_suspend(struct request_queue *q)
-{
-	return -ENOSYS;
-}
-static inline void blk_post_runtime_suspend(struct request_queue *q, int err) {}
-static inline void blk_pre_runtime_resume(struct request_queue *q) {}
-static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
-static inline void blk_set_runtime_active(struct request_queue *q) {}
-#endif
-
 /*
  * blk_plug permits building a queue of related requests by holding the I/O
  * fragments for a short period. This allows merging of sequential requests

commit 27ca1d4ed04ea29dc77b47190a3cc82697023e76
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 24 09:43:49 2018 +0200

    block: move req_gap_back_merge to blk.h
    
    No need to expose these helpers outside the block layer.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b7e676bb01bc..1d5e14139795 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1676,25 +1676,6 @@ static inline void put_dev_sector(Sector p)
 	put_page(p.v);
 }
 
-static inline bool __bvec_gap_to_prev(struct request_queue *q,
-				struct bio_vec *bprv, unsigned int offset)
-{
-	return offset ||
-		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
-}
-
-/*
- * Check if adding a bio_vec after bprv with offset would create a gap in
- * the SG list. Most drivers don't care about this, but some do.
- */
-static inline bool bvec_gap_to_prev(struct request_queue *q,
-				struct bio_vec *bprv, unsigned int offset)
-{
-	if (!queue_virt_boundary(q))
-		return false;
-	return __bvec_gap_to_prev(q, bprv, offset);
-}
-
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);

commit e9907009cbfc0c93d987d5a8fdf3d6c3c7b89717
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 24 09:43:48 2018 +0200

    block: move req_gap_{back,front}_merge to blk-merge.c
    
    Keep it close to the actual users instead of exposing the function to all
    drivers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bc534c857344..b7e676bb01bc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1695,75 +1695,6 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 	return __bvec_gap_to_prev(q, bprv, offset);
 }
 
-/*
- * Check if the two bvecs from two bios can be merged to one segment.
- * If yes, no need to check gap between the two bios since the 1st bio
- * and the 1st bvec in the 2nd bio can be handled in one segment.
- */
-static inline bool bios_segs_mergeable(struct request_queue *q,
-		struct bio *prev, struct bio_vec *prev_last_bv,
-		struct bio_vec *next_first_bv)
-{
-	if (!BIOVEC_PHYS_MERGEABLE(prev_last_bv, next_first_bv))
-		return false;
-	if (!BIOVEC_SEG_BOUNDARY(q, prev_last_bv, next_first_bv))
-		return false;
-	if (prev->bi_seg_back_size + next_first_bv->bv_len >
-			queue_max_segment_size(q))
-		return false;
-	return true;
-}
-
-static inline bool bio_will_gap(struct request_queue *q,
-				struct request *prev_rq,
-				struct bio *prev,
-				struct bio *next)
-{
-	if (bio_has_data(prev) && queue_virt_boundary(q)) {
-		struct bio_vec pb, nb;
-
-		/*
-		 * don't merge if the 1st bio starts with non-zero
-		 * offset, otherwise it is quite difficult to respect
-		 * sg gap limit. We work hard to merge a huge number of small
-		 * single bios in case of mkfs.
-		 */
-		if (prev_rq)
-			bio_get_first_bvec(prev_rq->bio, &pb);
-		else
-			bio_get_first_bvec(prev, &pb);
-		if (pb.bv_offset)
-			return true;
-
-		/*
-		 * We don't need to worry about the situation that the
-		 * merged segment ends in unaligned virt boundary:
-		 *
-		 * - if 'pb' ends aligned, the merged segment ends aligned
-		 * - if 'pb' ends unaligned, the next bio must include
-		 *   one single bvec of 'nb', otherwise the 'nb' can't
-		 *   merge with 'pb'
-		 */
-		bio_get_last_bvec(prev, &pb);
-		bio_get_first_bvec(next, &nb);
-
-		if (!bios_segs_mergeable(q, prev, &pb, &nb))
-			return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
-	}
-
-	return false;
-}
-
-static inline bool req_gap_back_merge(struct request *req, struct bio *bio)
-{
-	return bio_will_gap(req->q, req, req->biotail, bio);
-}
-
-static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
-{
-	return bio_will_gap(req->q, NULL, bio, req->bio);
-}
-
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);

commit 43b729bfe9cf30ad11499a66e3b7bd300c716d44
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 24 09:43:47 2018 +0200

    block: move integrity_req_gap_{back,front}_merge to blk.h
    
    No need to expose these to drivers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d6869e0e2b64..bc534c857344 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1843,26 +1843,6 @@ queue_max_integrity_segments(struct request_queue *q)
 	return q->limits.max_integrity_segments;
 }
 
-static inline bool integrity_req_gap_back_merge(struct request *req,
-						struct bio *next)
-{
-	struct bio_integrity_payload *bip = bio_integrity(req->bio);
-	struct bio_integrity_payload *bip_next = bio_integrity(next);
-
-	return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
-				bip_next->bip_vec[0].bv_offset);
-}
-
-static inline bool integrity_req_gap_front_merge(struct request *req,
-						 struct bio *bio)
-{
-	struct bio_integrity_payload *bip = bio_integrity(bio);
-	struct bio_integrity_payload *bip_next = bio_integrity(req->bio);
-
-	return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
-				bip_next->bip_vec[0].bv_offset);
-}
-
 /**
  * bio_integrity_intervals - Return number of integrity intervals for a bio
  * @bi:		blk_integrity profile for device
@@ -1947,17 +1927,6 @@ static inline bool blk_integrity_merge_bio(struct request_queue *rq,
 	return true;
 }
 
-static inline bool integrity_req_gap_back_merge(struct request *req,
-						struct bio *next)
-{
-	return false;
-}
-static inline bool integrity_req_gap_front_merge(struct request *req,
-						 struct bio *bio)
-{
-	return false;
-}
-
 static inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,
 						   unsigned int sectors)
 {

commit 01c5f85aebaaddfd7e6051fb2ec80c1d4b463554
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Sep 11 10:59:53 2018 -0600

    blk-cgroup: increase number of supported policies
    
    After merging the iolatency policy, we potentially now have 4 policies
    being registered, but only support 3. This causes one of them to fail
    loading. Takashi reports that BFQ no longer works for him, because it
    fails to load due to policy registration failure.
    
    Bump to 5 policies, and also add a warning for when we have exceeded
    the global amount. If we have to touch this again, we should switch
    to a dynamic scheme instead.
    
    Reported-by: Takashi Iwai <tiwai@suse.de>
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Tested-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d6869e0e2b64..6980014357d4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -54,7 +54,7 @@ struct blk_stat_callback;
  * Maximum number of blkcg policies allowed to be registered concurrently.
  * Defined here to simplify include dependency.
  */
-#define BLKCG_MAX_POLS		3
+#define BLKCG_MAX_POLS		5
 
 typedef void (rq_end_io_fn)(struct request *, blk_status_t);
 

commit b1f4267cc5448d20ae0c515a74141e74365e78a3
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Aug 9 07:47:28 2018 -0700

    block: Remove two superfluous #include directives
    
    Commit 12f5b9314545 ("blk-mq: Remove generation seqeunce") removed the
    only seqcount_t and u64_stats_sync instances from <linux/blkdev.h> but
    did not remove the corresponding #include directives. Since these
    include directives are no longer needed, remove them.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Jianchao Wang <jianchao.w.wang@oracle.com>
    Cc: Hannes Reinecke <hare@suse.com>,
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 050d599f5ea9..d6869e0e2b64 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -27,8 +27,6 @@
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
 #include <linux/blkzoned.h>
-#include <linux/seqlock.h>
-#include <linux/u64_stats_sync.h>
 
 struct module;
 struct scsi_ioctl_command;

commit 359f642700f2ff05d9c94cd9216c97af7b8e9553
Author: Greg Edwards <gedwards@ddn.com>
Date:   Wed Jul 25 10:22:58 2018 -0400

    block: move bio_integrity_{intervals,bytes} into blkdev.h
    
    This allows bio_integrity_bytes() to be called from drivers instead of
    open coding it.
    
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Edwards <gedwards@ddn.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 331a6cb8805f..050d599f5ea9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1865,6 +1865,28 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
 				bip_next->bip_vec[0].bv_offset);
 }
 
+/**
+ * bio_integrity_intervals - Return number of integrity intervals for a bio
+ * @bi:		blk_integrity profile for device
+ * @sectors:	Size of the bio in 512-byte sectors
+ *
+ * Description: The block layer calculates everything in 512 byte
+ * sectors but integrity metadata is done in terms of the data integrity
+ * interval size of the storage device.  Convert the block layer sectors
+ * to the appropriate number of integrity intervals.
+ */
+static inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,
+						   unsigned int sectors)
+{
+	return sectors >> (bi->interval_exp - 9);
+}
+
+static inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,
+					       unsigned int sectors)
+{
+	return bio_integrity_intervals(bi, sectors) * bi->tuple_size;
+}
+
 #else /* CONFIG_BLK_DEV_INTEGRITY */
 
 struct bio;
@@ -1938,6 +1960,18 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
 	return false;
 }
 
+static inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,
+						   unsigned int sectors)
+{
+	return 0;
+}
+
+static inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,
+					       unsigned int sectors)
+{
+	return 0;
+}
+
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
 struct block_device_operations {

commit 3f289dcb4b265416a57ca79cf4a324060bb09060
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jul 18 04:47:36 2018 -0700

    block: make bdev_ops->rw_page() take a REQ_OP instead of bool
    
    c11f0c0b5bb9 ("block/mm: make bdev_ops->rw_page() take a bool for
    read/write") replaced @op with boolean @is_write, which limited the
    amount of information going into ->rw_page() and more importantly
    page_endio(), which removed the need to expose block internals to mm.
    
    Unfortunately, we want to track discards separately and @is_write
    isn't enough information.  This patch updates bdev_ops->rw_page() to
    take REQ_OP instead but leaves page_endio() to take bool @is_write.
    This allows the block part of operations to have enough information
    while not leaking it to mm.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Mike Christie <mchristi@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1939ed95f936..331a6cb8805f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1943,7 +1943,7 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
-	int (*rw_page)(struct block_device *, sector_t, struct page *, bool);
+	int (*rw_page)(struct block_device *, sector_t, struct page *, unsigned int);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	unsigned int (*check_events) (struct gendisk *disk,

commit 05814a10370b3252fe2b0898b6adac3cdd531096
Author: Vladimir Zapolskiy <vz@mleia.com>
Date:   Fri Jul 13 17:07:26 2018 +0300

    block: remove blkdev_entry_to_request() macro
    
    Remove blkdev_entry_to_request() macro, which remained unused through
    the observable history, also note that it repeats list_entry_rq() macro
    verbatim.
    
    Signed-off-by: Vladimir Zapolskiy <vz@mleia.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 137759862f07..1939ed95f936 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1436,8 +1436,6 @@ enum blk_default_limits {
 	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
 };
 
-#define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
-
 static inline unsigned long queue_segment_boundary(struct request_queue *q)
 {
 	return q->limits.seg_boundary_mask;

commit a79050434b45959f397042080fd1d70ffa9bd9df
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Jul 3 09:32:35 2018 -0600

    blk-rq-qos: refactor out common elements of blk-wbt
    
    blkcg-qos is going to do essentially what wbt does, only on a cgroup
    basis.  Break out the common code that will be shared between blkcg-qos
    and wbt into blk-rq-qos.* so they can both utilize the same
    infrastructure.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9d05646d5059..137759862f07 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -42,7 +42,7 @@ struct bsg_job;
 struct blkcg_gq;
 struct blk_flush_queue;
 struct pr_ops;
-struct rq_wb;
+struct rq_qos;
 struct blk_queue_stats;
 struct blk_stat_callback;
 
@@ -443,7 +443,7 @@ struct request_queue {
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
 	struct blk_queue_stats	*stats;
-	struct rq_wb		*rq_wb;
+	struct rq_qos		*rq_qos;
 
 	/*
 	 * If blkcg is not used, @q->root_rl serves all requests.  If blkcg

commit 97889f9ac24f8d2fc8e703ea7f80c162bab10d4d
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Jun 25 19:31:48 2018 +0800

    blk-mq: remove synchronize_rcu() from blk_mq_del_queue_tag_set()
    
    We have to remove synchronize_rcu() from blk_queue_cleanup(),
    otherwise long delay can be caused during lun probe. For removing
    it, we have to avoid to iterate the set->tag_list in IO path, eg,
    blk_mq_sched_restart().
    
    This patch reverts 5b79413946d (Revert "blk-mq: don't handle
    TAG_SHARED in restart"). Given we have fixed enough IO hang issue,
    and there isn't any reason to restart all queues in one tags any more,
    see the following reasons:
    
    1) blk-mq core can deal with shared-tags case well via blk_mq_get_driver_tag(),
    which can wake up queues waiting for driver tag.
    
    2) SCSI is a bit special because it may return BLK_STS_RESOURCE if queue,
    target or host is ready, but SCSI built-in restart can cover all these well,
    see scsi_end_request(), queue will be rerun after any request initiated from
    this host/target is completed.
    
    In my test on scsi_debug(8 luns), this patch may improve IOPS by 20% ~ 30%
    when running I/O on these 8 luns concurrently.
    
    Fixes: 705cda97ee3a ("blk-mq: Make it safe to use RCU to iterate over blk_mq_tag_set.tag_list")
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: linux-scsi@vger.kernel.org
    Reported-by: Andrew Jones <drjones@redhat.com>
    Tested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ca5a8b046894..9d05646d5059 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -442,8 +442,6 @@ struct request_queue {
 	int			nr_rqs[2];	/* # allocated [a]sync rqs */
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
-	atomic_t		shared_hctx_restart;
-
 	struct blk_queue_stats	*stats;
 	struct rq_wb		*rq_wb;
 

commit 6a5ac9846508ad7d1d23881d9d5add35f2e6ae71
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jun 15 14:55:21 2018 -0700

    block: Make struct request_queue smaller for CONFIG_BLK_DEV_ZONED=n
    
    Exclude zoned block device members from struct request_queue for
    CONFIG_BLK_DEV_ZONED == n. Avoid breaking the build by only building
    the code that uses these struct request_queue members if
    CONFIG_BLK_DEV_ZONED != n.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Cc: Matias Bjorling <mb@lightnvm.io>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 905daa7c647e..ca5a8b046894 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -592,6 +592,7 @@ struct request_queue {
 
 	struct queue_limits	limits;
 
+#ifdef CONFIG_BLK_DEV_ZONED
 	/*
 	 * Zoned block device information for request dispatch control.
 	 * nr_zones is the total number of zones of the device. This is always
@@ -612,6 +613,7 @@ struct request_queue {
 	unsigned int		nr_zones;
 	unsigned long		*seq_zones_bitmap;
 	unsigned long		*seq_zones_wlock;
+#endif /* CONFIG_BLK_DEV_ZONED */
 
 	/*
 	 * sg stuff
@@ -800,6 +802,7 @@ static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
 
+#ifdef CONFIG_BLK_DEV_ZONED
 static inline unsigned int blk_queue_zone_no(struct request_queue *q,
 					     sector_t sector)
 {
@@ -815,6 +818,7 @@ static inline bool blk_queue_zone_is_seq(struct request_queue *q,
 		return false;
 	return test_bit(blk_queue_zone_no(q, sector), q->seq_zones_bitmap);
 }
+#endif /* CONFIG_BLK_DEV_ZONED */
 
 static inline bool rq_is_sync(struct request *rq)
 {
@@ -1065,6 +1069,7 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> SECTOR_SHIFT;
 }
 
+#ifdef CONFIG_BLK_DEV_ZONED
 static inline unsigned int blk_rq_zone_no(struct request *rq)
 {
 	return blk_queue_zone_no(rq->q, blk_rq_pos(rq));
@@ -1074,6 +1079,7 @@ static inline unsigned int blk_rq_zone_is_seq(struct request *rq)
 {
 	return blk_queue_zone_is_seq(rq->q, blk_rq_pos(rq));
 }
+#endif /* CONFIG_BLK_DEV_ZONED */
 
 /*
  * Some commands like WRITE SAME have a payload or data transfer size which

commit 7c8542b7982264226cf94102950343185869b584
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jun 15 14:55:20 2018 -0700

    block: Inline blk_queue_nr_zones()
    
    Since the implementation of blk_queue_nr_zones() is trivial and since
    it only has a single caller, inline this function.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Cc: Matias Bjorling <mb@lightnvm.io>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49a400afb146..905daa7c647e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -800,11 +800,6 @@ static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
 
-static inline unsigned int blk_queue_nr_zones(struct request_queue *q)
-{
-	return q->nr_zones;
-}
-
 static inline unsigned int blk_queue_zone_no(struct request_queue *q,
 					     sector_t sector)
 {

commit 6b1d83d274486615cc341e410467a98fd9c27c0a
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jun 15 14:55:19 2018 -0700

    block: Remove bdev_nr_zones()
    
    Remove this function since it has no callers. This function was
    introduced in commit 6cc77e9cb080 ("block: introduce zoned block
    devices zone write locking").
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Matias Bjorling <mb@lightnvm.io>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 79226ca8f80f..49a400afb146 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1639,15 +1639,6 @@ static inline unsigned int bdev_zone_sectors(struct block_device *bdev)
 	return 0;
 }
 
-static inline unsigned int bdev_nr_zones(struct block_device *bdev)
-{
-	struct request_queue *q = bdev_get_queue(bdev);
-
-	if (q)
-		return blk_queue_nr_zones(q);
-	return 0;
-}
-
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 15bfd21fbc5d35834b9ea383dc458a1f0c9e3434
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue Jun 26 09:14:58 2018 -0600

    block: Fix transfer when chunk sectors exceeds max
    
    A device may have boundary restrictions where the number of sectors
    between boundaries exceeds its max transfer size. In this case, we need
    to cap the max size to the smaller of the two limits.
    
    Reported-by: Jitendra Bhivare <jitendra.bhivare@broadcom.com>
    Tested-by: Jitendra Bhivare <jitendra.bhivare@broadcom.com>
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9154570edf29..79226ca8f80f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1119,8 +1119,8 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 	if (!q->limits.chunk_sectors)
 		return q->limits.max_sectors;
 
-	return q->limits.chunk_sectors -
-			(offset & (q->limits.chunk_sectors - 1));
+	return min(q->limits.max_sectors, (unsigned int)(q->limits.chunk_sectors -
+			(offset & (q->limits.chunk_sectors - 1))));
 }
 
 static inline unsigned int blk_rq_get_max_sectors(struct request *rq,

commit be7f99c536c5aeebad29082b7d8dce32077fea14
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 15 13:55:07 2018 +0200

    block: remov blk_queue_invalidate_tags
    
    This function is entirely unused, so remove it and the tag_queue_busy
    member of struct request_queue.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fa6f11751430..9154570edf29 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -562,7 +562,6 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	struct blk_queue_tag	*queue_tags;
-	struct list_head	tag_busy_list;
 
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
@@ -1375,7 +1374,6 @@ extern void blk_queue_end_tag(struct request_queue *, struct request *);
 extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *, int);
 extern void blk_queue_free_tags(struct request_queue *);
 extern int blk_queue_resize_tags(struct request_queue *, int);
-extern void blk_queue_invalidate_tags(struct request_queue *);
 extern struct blk_queue_tag *blk_init_tags(int, int);
 extern void blk_free_tags(struct blk_queue_tag *);
 

commit da661267398869a553b7f67d739d360aaa1361b6
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 14 13:58:45 2018 +0200

    blk-mq: don't time out requests again that are in the timeout handler
    
    We can currently call the timeout handler again on a request that has
    already been handed over to the timeout handler.  Prevent that with a new
    flag.
    
    Fixes: 12f5b931 ("blk-mq: Remove generation seqeunce")
    Reported-by: Andrew Randrianasulu <randrianasulu@gmail.com>
    Tested-by: Andrew Randrianasulu <randrianasulu@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bca3a92eb55f..fa6f11751430 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -127,6 +127,8 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
+/* ->timeout has been called, don't expire again */
+#define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \

commit 338aa96d5661048b3c0cafc6d91876025603cacf
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sun May 20 18:25:47 2018 -0400

    block: convert bounce, q->bio_split to bioset_init()/mempool_init()
    
    Convert the core block functionality to embedded bio sets.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4efd9af62e25..bca3a92eb55f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -652,7 +652,7 @@ struct request_queue {
 
 	struct blk_mq_tag_set	*tag_set;
 	struct list_head	tag_set_list;
-	struct bio_set		*bio_split;
+	struct bio_set		bio_split;
 
 #ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;

commit 0b7576d8eb4c5634276ada359bfdb72e69eebc25
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue May 29 08:47:57 2018 -0600

    block: move ->timeout request member
    
    After the recent timeout handling changes, we have two holes in
    the struct. Move the timeout near the deadline, killing both,
    and moving related members closer together. On my config on
    x86-64, this shrinks struct request from 312 to 304 bytes.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9f06b29adaa4..4efd9af62e25 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -237,8 +237,6 @@ struct request {
 	unsigned short write_hint;
 	unsigned short ioprio;
 
-	unsigned int timeout;
-
 	void *special;		/* opaque pointer available for LLD use */
 
 	unsigned int extra_len;	/* length of alignment and padding */
@@ -246,6 +244,8 @@ struct request {
 	enum mq_rq_state state;
 	refcount_t ref;
 
+	unsigned int timeout;
+
 	/* access through blk_rq_set_deadline, blk_rq_deadline */
 	unsigned long __deadline;
 

commit 88b0cfad288831a86ffd79b6d00b689489598186
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 29 15:52:38 2018 +0200

    block: document the blk_eh_timer_return values
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d838a89639f2..9f06b29adaa4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -326,8 +326,8 @@ typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
 typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
 enum blk_eh_timer_return {
-	BLK_EH_DONE,
-	BLK_EH_RESET_TIMER,
+	BLK_EH_DONE,		/* drivers has completed the command */
+	BLK_EH_RESET_TIMER,	/* reset timer and try again */
 };
 
 typedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);

commit f6e7d48a780a95a599d7e325f45e9ac208ac6f13
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 29 15:52:37 2018 +0200

    block: remove BLK_EH_HANDLED
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f82e05df905b..d838a89639f2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -327,7 +327,6 @@ typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
 enum blk_eh_timer_return {
 	BLK_EH_DONE,
-	BLK_EH_HANDLED,
 	BLK_EH_RESET_TIMER,
 };
 

commit 6600593cbd9340b3d4fcde8e58d17653732620c4
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 29 15:52:29 2018 +0200

    block: rename BLK_EH_NOT_HANDLED to BLK_EH_DONE
    
    The BLK_EH_NOT_HANDLED implies nothing happen, but very often that
    is not what is happening - instead the driver already completed the
    command.  Fix the symbolic name to reflect that a little better.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a1c05e85a443..f82e05df905b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -326,7 +326,7 @@ typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
 typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
 enum blk_eh_timer_return {
-	BLK_EH_NOT_HANDLED,
+	BLK_EH_DONE,
 	BLK_EH_HANDLED,
 	BLK_EH_RESET_TIMER,
 };

commit 12f5b93145450c750f315657ef239a314811aeeb
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue May 29 15:52:28 2018 +0200

    blk-mq: Remove generation seqeunce
    
    This patch simplifies the timeout handling by relying on the request
    reference counting to ensure the iterator is operating on an inflight
    and truly timed out request. Since the reference counting prevents the
    tag from being reallocated, the block layer no longer needs to prevent
    drivers from completing their requests while the timeout handler is
    operating on it: a driver completing a request is allowed to proceed to
    the next state without additional syncronization with the block layer.
    
    This also removes any need for generation sequence numbers since the
    request lifetime is prevented from being reallocated as a new sequence
    while timeout handling is operating on it.
    
    To enables this a refcount is added to struct request so that request
    users can be sure they're operating on the same request without it
    changing while they're processing it.  The request's tag won't be
    released for reuse until both the timeout handler and the completion
    are done with it.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    [hch: slight cleanups, added back submission side hctx lock, use cmpxchg
     for completions]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3999719f828..a1c05e85a443 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -125,15 +125,22 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_SPECIAL_PAYLOAD	((__force req_flags_t)(1 << 18))
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
-/* timeout is expired */
-#define RQF_MQ_TIMEOUT_EXPIRED	((__force req_flags_t)(1 << 20))
 /* already slept for hybrid poll */
-#define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 21))
+#define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \
 	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
 
+/*
+ * Request state for blk-mq.
+ */
+enum mq_rq_state {
+	MQ_RQ_IDLE		= 0,
+	MQ_RQ_IN_FLIGHT		= 1,
+	MQ_RQ_COMPLETE		= 2,
+};
+
 /*
  * Try to put the fields that are referenced together in the same cacheline.
  *
@@ -236,26 +243,8 @@ struct request {
 
 	unsigned int extra_len;	/* length of alignment and padding */
 
-	/*
-	 * On blk-mq, the lower bits of ->gstate (generation number and
-	 * state) carry the MQ_RQ_* state value and the upper bits the
-	 * generation number which is monotonically incremented and used to
-	 * distinguish the reuse instances.
-	 *
-	 * ->gstate_seq allows updates to ->gstate and other fields
-	 * (currently ->deadline) during request start to be read
-	 * atomically from the timeout path, so that it can operate on a
-	 * coherent set of information.
-	 */
-	seqcount_t gstate_seq;
-	u64 gstate;
-
-	/*
-	 * ->aborted_gstate is used by the timeout to claim a specific
-	 * recycle instance of this request.  See blk_mq_timeout_work().
-	 */
-	struct u64_stats_sync aborted_gstate_sync;
-	u64 aborted_gstate;
+	enum mq_rq_state state;
+	refcount_t ref;
 
 	/* access through blk_rq_set_deadline, blk_rq_deadline */
 	unsigned long __deadline;

commit ff005a066240efb73ae29a2bb9269ae726bc2eae
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 9 09:54:05 2018 +0200

    block: sanitize blk_get_request calling conventions
    
    Switch everyone to blk_get_request_flags, and then rename
    blk_get_request_flags to blk_get_request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e42d510daf3c..f3999719f828 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -976,11 +976,8 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
-extern struct request *blk_get_request_flags(struct request_queue *,
-					     unsigned int op,
-					     blk_mq_req_flags_t flags);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
-				       gfp_t gfp_mask);
+				       blk_mq_req_flags_t flags);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,

commit 522a777566f5669606a1227bf13f3fb40963780b
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed May 9 02:08:53 2018 -0700

    block: consolidate struct request timestamp fields
    
    Currently, struct request has four timestamp fields:
    
    - A start time, set at get_request time, in jiffies, used for iostats
    - An I/O start time, set at start_request time, in ktime nanoseconds,
      used for blk-stats (i.e., wbt, kyber, hybrid polling)
    - Another start time and another I/O start time, used for cfq and bfq
    
    These can all be consolidated into one start time and one I/O start
    time, both in ktime nanoseconds, shaving off up to 16 bytes from struct
    request depending on the kernel config.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9ef412666df1..e42d510daf3c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -205,7 +205,8 @@ struct request {
 
 	struct gendisk *rq_disk;
 	struct hd_struct *part;
-	unsigned long start_time;
+	/* Time that I/O was submitted to the kernel. */
+	u64 start_time_ns;
 	/* Time that I/O was submitted to the device. */
 	u64 io_start_time_ns;
 
@@ -277,8 +278,6 @@ struct request {
 
 #ifdef CONFIG_BLK_CGROUP
 	struct request_list *rl;		/* rl this rq is alloced from */
-	unsigned long long cgroup_start_time_ns;
-	unsigned long long cgroup_io_start_time_ns;    /* when passed to hardware */
 #endif
 };
 
@@ -1798,39 +1797,6 @@ int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
-#ifdef CONFIG_BLK_CGROUP
-static inline void set_start_time_ns(struct request *req)
-{
-	req->cgroup_start_time_ns = ktime_get_ns();
-}
-
-static inline void set_io_start_time_ns(struct request *req)
-{
-	req->cgroup_io_start_time_ns = ktime_get_ns();
-}
-
-static inline u64 rq_start_time_ns(struct request *req)
-{
-	return req->cgroup_start_time_ns;
-}
-
-static inline u64 rq_io_start_time_ns(struct request *req)
-{
-	return req->cgroup_io_start_time_ns;
-}
-#else
-static inline void set_start_time_ns(struct request *req) {}
-static inline void set_io_start_time_ns(struct request *req) {}
-static inline u64 rq_start_time_ns(struct request *req)
-{
-	return 0;
-}
-static inline u64 rq_io_start_time_ns(struct request *req)
-{
-	return 0;
-}
-#endif
-
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \

commit 84c7afcebed913c93d50f116b046b7f0d8ec0cdc
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed May 9 02:08:51 2018 -0700

    block: use ktime_get_ns() instead of sched_clock() for cfq and bfq
    
    cfq and bfq have some internal fields that use sched_clock() which can
    trivially use ktime_get_ns() instead. Their timestamp fields in struct
    request can also use ktime_get_ns(), which resolves the 8 year old
    comment added by commit 28f4197e5d47 ("block: disable preemption before
    using sched_clock()").
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f2c2fc011e6b..9ef412666df1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1799,42 +1799,33 @@ int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
-/*
- * This should not be using sched_clock(). A real patch is in progress
- * to fix this up, until that is in place we need to disable preemption
- * around sched_clock() in this function and set_io_start_time_ns().
- */
 static inline void set_start_time_ns(struct request *req)
 {
-	preempt_disable();
-	req->cgroup_start_time_ns = sched_clock();
-	preempt_enable();
+	req->cgroup_start_time_ns = ktime_get_ns();
 }
 
 static inline void set_io_start_time_ns(struct request *req)
 {
-	preempt_disable();
-	req->cgroup_io_start_time_ns = sched_clock();
-	preempt_enable();
+	req->cgroup_io_start_time_ns = ktime_get_ns();
 }
 
-static inline uint64_t rq_start_time_ns(struct request *req)
+static inline u64 rq_start_time_ns(struct request *req)
 {
 	return req->cgroup_start_time_ns;
 }
 
-static inline uint64_t rq_io_start_time_ns(struct request *req)
+static inline u64 rq_io_start_time_ns(struct request *req)
 {
 	return req->cgroup_io_start_time_ns;
 }
 #else
 static inline void set_start_time_ns(struct request *req) {}
 static inline void set_io_start_time_ns(struct request *req) {}
-static inline uint64_t rq_start_time_ns(struct request *req)
+static inline u64 rq_start_time_ns(struct request *req)
 {
 	return 0;
 }
-static inline uint64_t rq_io_start_time_ns(struct request *req)
+static inline u64 rq_io_start_time_ns(struct request *req)
 {
 	return 0;
 }

commit 544ccc8dc904db55d4576c27a1eb66a888ffacea
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed May 9 02:08:50 2018 -0700

    block: get rid of struct blk_issue_stat
    
    struct blk_issue_stat squashes three things into one u64:
    
    - The time the driver started working on a request
    - The original size of the request (for the io.low controller)
    - Flags for writeback throttling
    
    It turns out that on x86_64, we have a 4 byte hole in struct request
    which we can fill with the non-timestamp fields from blk_issue_stat,
    simplifying things quite a bit.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c4eee043191..f2c2fc011e6b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -206,8 +206,18 @@ struct request {
 	struct gendisk *rq_disk;
 	struct hd_struct *part;
 	unsigned long start_time;
-	struct blk_issue_stat issue_stat;
-	/* Number of scatter-gather DMA addr+len pairs after
+	/* Time that I/O was submitted to the device. */
+	u64 io_start_time_ns;
+
+#ifdef CONFIG_BLK_WBT
+	unsigned short wbt_flags;
+#endif
+#ifdef CONFIG_BLK_DEV_THROTTLING_LOW
+	unsigned short throtl_size;
+#endif
+
+	/*
+	 * Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
 	 */
 	unsigned short nr_phys_segments;
@@ -267,8 +277,8 @@ struct request {
 
 #ifdef CONFIG_BLK_CGROUP
 	struct request_list *rl;		/* rl this rq is alloced from */
-	unsigned long long start_time_ns;
-	unsigned long long io_start_time_ns;    /* when passed to hardware */
+	unsigned long long cgroup_start_time_ns;
+	unsigned long long cgroup_io_start_time_ns;    /* when passed to hardware */
 #endif
 };
 
@@ -1797,25 +1807,25 @@ int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned lo
 static inline void set_start_time_ns(struct request *req)
 {
 	preempt_disable();
-	req->start_time_ns = sched_clock();
+	req->cgroup_start_time_ns = sched_clock();
 	preempt_enable();
 }
 
 static inline void set_io_start_time_ns(struct request *req)
 {
 	preempt_disable();
-	req->io_start_time_ns = sched_clock();
+	req->cgroup_io_start_time_ns = sched_clock();
 	preempt_enable();
 }
 
 static inline uint64_t rq_start_time_ns(struct request *req)
 {
-        return req->start_time_ns;
+	return req->cgroup_start_time_ns;
 }
 
 static inline uint64_t rq_io_start_time_ns(struct request *req)
 {
-        return req->io_start_time_ns;
+	return req->cgroup_io_start_time_ns;
 }
 #else
 static inline void set_start_time_ns(struct request *req) {}

commit 3442097b765c3e89fe3afe755054e6683b140f6d
Merge: 8fba70b0850a 8e1ceafe50ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 25 21:13:40 2018 -0700

    Merge tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI fixes from James Bottomley:
     "Eight bug fixes, one spelling update and one tracepoint addition.
    
      The most serious is probably the mptsas write same fix because it
      means anyone using these controllers sees errors when modern
      filesystems try to issue discards"
    
    * tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:
      scsi: target: fix crash with iscsi target and dvd
      scsi: sd_zbc: Avoid that resetting a zone fails sporadically
      scsi: sd: Defer spinning up drive while SANITIZE is in progress
      scsi: megaraid_sas: Do not log an error if FW successfully initializes.
      scsi: ufs: add trace event for ufs upiu
      scsi: core: remove reference to scsi_show_extd_sense()
      scsi: mptsas: Disable WRITE SAME
      scsi: fnic: fix spelling mistake in fnic stats "Abord" -> "Abort"
      scsi: scsi_debug: IMMED related delay adjustments
      scsi: iscsi: respond to netlink with unicast when appropriate

commit ccce20fc7968d546fb1e8e147bf5cdc8afc4278a
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Mon Apr 16 18:04:41 2018 -0700

    scsi: sd_zbc: Avoid that resetting a zone fails sporadically
    
    Since SCSI scanning occurs asynchronously, since sd_revalidate_disk() is
    called from sd_probe_async() and since sd_revalidate_disk() calls
    sd_zbc_read_zones() it can happen that sd_zbc_read_zones() is called
    concurrently with blkdev_report_zones() and/or blkdev_reset_zones().  That can
    cause these functions to fail with -EIO because sd_zbc_read_zones() e.g. sets
    q->nr_zones to zero before restoring it to the actual value, even if no drive
    characteristics have changed.  Avoid that this can happen by making the
    following changes:
    
    - Protect the code that updates zone information with blk_queue_enter()
      and blk_queue_exit().
    - Modify sd_zbc_setup_seq_zones_bitmap() and sd_zbc_setup() such that
      these functions do not modify struct scsi_disk before all zone
      information has been obtained.
    
    Note: since commit 055f6e18e08f ("block: Make q_usage_counter also track
    legacy requests"; kernel v4.15) the request queue freezing mechanism also
    affects legacy request queues.
    
    Fixes: 89d947561077 ("sd: Implement support for ZBC devices")
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Damien Le Moal <damien.lemoal@wdc.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: stable@vger.kernel.org # v4.16
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9af3e0f430bc..21e21f273a21 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -605,6 +605,11 @@ struct request_queue {
 	 * initialized by the low level device driver (e.g. scsi/sd.c).
 	 * Stacking drivers (device mappers) may or may not initialize
 	 * these fields.
+	 *
+	 * Reads of this information must be protected with blk_queue_enter() /
+	 * blk_queue_exit(). Modifying this information is only allowed while
+	 * no requests are being processed. See also blk_mq_freeze_queue() and
+	 * blk_mq_unfreeze_queue().
 	 */
 	unsigned int		nr_zones;
 	unsigned long		*seq_zones_bitmap;

commit 0ce9144471de9ee09306ca0127e7cd27521ccc3f
Author: Dave Chinner <dchinner@redhat.com>
Date:   Wed Apr 18 14:08:27 2018 +1000

    block: add blk_queue_fua() helper function
    
    So we can check FUA support status from the iomap direct IO code.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9af3e0f430bc..c362aadfe036 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -737,6 +737,7 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_preempt_only(q)				\
 	test_bit(QUEUE_FLAG_PREEMPT_ONLY, &(q)->queue_flags)
+#define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
 
 extern int blk_set_preempt_only(struct request_queue *q);
 extern void blk_clear_preempt_only(struct request_queue *q);

commit 233bde21aa43516baa013ef7ac33f3427056db3e
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 14 15:48:06 2018 -0700

    block: Move SECTOR_SIZE and SECTOR_SHIFT definitions into <linux/blkdev.h>
    
    It happens often while I'm preparing a patch for a block driver that
    I'm wondering: is a definition of SECTOR_SIZE and/or SECTOR_SHIFT
    available for this driver? Do I have to introduce definitions of these
    constants before I can use these constants? To avoid this confusion,
    move the existing definitions of SECTOR_SIZE and SECTOR_SHIFT into the
    <linux/blkdev.h> header file such that these become available for all
    block drivers. Make the SECTOR_SIZE definition in the uapi msdos_fs.h
    header file conditional to avoid that including that header file after
    <linux/blkdev.h> causes the compiler to complain about a SECTOR_SIZE
    redefinition.
    
    Note: the SECTOR_SIZE / SECTOR_SHIFT / SECTOR_BITS definitions have
    not been removed from uapi header files nor from NAND drivers in
    which these constants are used for another purpose than converting
    block layer offsets and sizes into a number of sectors.
    
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Reviewed-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 19eaf8d89368..9af3e0f430bc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1021,6 +1021,19 @@ static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 	return bdev->bd_disk->queue;	/* this is never NULL */
 }
 
+/*
+ * The basic unit of block I/O is a sector. It is used in a number of contexts
+ * in Linux (blk, bio, genhd). The size of one sector is 512 = 2**9
+ * bytes. Variables of type sector_t represent an offset or size that is a
+ * multiple of 512 bytes. Hence these two constants.
+ */
+#ifndef SECTOR_SHIFT
+#define SECTOR_SHIFT 9
+#endif
+#ifndef SECTOR_SIZE
+#define SECTOR_SIZE (1 << SECTOR_SHIFT)
+#endif
+
 /*
  * blk_rq_pos()			: the current sector
  * blk_rq_bytes()		: bytes left in the entire request
@@ -1048,12 +1061,12 @@ extern unsigned int blk_rq_err_bytes(const struct request *rq);
 
 static inline unsigned int blk_rq_sectors(const struct request *rq)
 {
-	return blk_rq_bytes(rq) >> 9;
+	return blk_rq_bytes(rq) >> SECTOR_SHIFT;
 }
 
 static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 {
-	return blk_rq_cur_bytes(rq) >> 9;
+	return blk_rq_cur_bytes(rq) >> SECTOR_SHIFT;
 }
 
 static inline unsigned int blk_rq_zone_no(struct request *rq)
@@ -1083,7 +1096,8 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {
 	if (unlikely(op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE))
-		return min(q->limits.max_discard_sectors, UINT_MAX >> 9);
+		return min(q->limits.max_discard_sectors,
+			   UINT_MAX >> SECTOR_SHIFT);
 
 	if (unlikely(op == REQ_OP_WRITE_SAME))
 		return q->limits.max_write_same_sectors;
@@ -1395,16 +1409,21 @@ extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {
-	return blkdev_issue_discard(sb->s_bdev, block << (sb->s_blocksize_bits - 9),
-				    nr_blocks << (sb->s_blocksize_bits - 9),
+	return blkdev_issue_discard(sb->s_bdev,
+				    block << (sb->s_blocksize_bits -
+					      SECTOR_SHIFT),
+				    nr_blocks << (sb->s_blocksize_bits -
+						  SECTOR_SHIFT),
 				    gfp_mask, flags);
 }
 static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask)
 {
 	return blkdev_issue_zeroout(sb->s_bdev,
-				    block << (sb->s_blocksize_bits - 9),
-				    nr_blocks << (sb->s_blocksize_bits - 9),
+				    block << (sb->s_blocksize_bits -
+					      SECTOR_SHIFT),
+				    nr_blocks << (sb->s_blocksize_bits -
+						  SECTOR_SHIFT),
 				    gfp_mask, 0);
 }
 
@@ -1511,7 +1530,8 @@ static inline int queue_alignment_offset(struct request_queue *q)
 static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)
 {
 	unsigned int granularity = max(lim->physical_block_size, lim->io_min);
-	unsigned int alignment = sector_div(sector, granularity >> 9) << 9;
+	unsigned int alignment = sector_div(sector, granularity >> SECTOR_SHIFT)
+		<< SECTOR_SHIFT;
 
 	return (granularity + lim->alignment_offset - alignment) % granularity;
 }
@@ -1545,8 +1565,8 @@ static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector
 		return 0;
 
 	/* Why are these in bytes, not sectors? */
-	alignment = lim->discard_alignment >> 9;
-	granularity = lim->discard_granularity >> 9;
+	alignment = lim->discard_alignment >> SECTOR_SHIFT;
+	granularity = lim->discard_granularity >> SECTOR_SHIFT;
 	if (!granularity)
 		return 0;
 
@@ -1557,7 +1577,7 @@ static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector
 	offset = (granularity + alignment - offset) % granularity;
 
 	/* Turn it back into bytes, gaah */
-	return offset << 9;
+	return offset << SECTOR_SHIFT;
 }
 
 static inline int bdev_discard_alignment(struct block_device *bdev)

commit 8a0ac14b8da9b86cfbe7aace40c8d485ed5c5b97
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:12 2018 -0800

    block: Move the queue_flag_*() functions from a public into a private header file
    
    This patch helps to avoid that new code gets introduced in block drivers
    that manipulates queue flags without holding the queue lock when that
    lock should be held.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 888c9b25cb8f..19eaf8d89368 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -712,75 +712,6 @@ void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 
-/*
- * @q->queue_lock is set while a queue is being initialized. Since we know
- * that no other threads access the queue object before @q->queue_lock has
- * been set, it is safe to manipulate queue flags without holding the
- * queue_lock if @q->queue_lock == NULL. See also blk_alloc_queue_node() and
- * blk_init_allocated_queue().
- */
-static inline void queue_lockdep_assert_held(struct request_queue *q)
-{
-	if (q->queue_lock)
-		lockdep_assert_held(q->queue_lock);
-}
-
-static inline void queue_flag_set_unlocked(unsigned int flag,
-					   struct request_queue *q)
-{
-	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
-	    kref_read(&q->kobj.kref))
-		lockdep_assert_held(q->queue_lock);
-	__set_bit(flag, &q->queue_flags);
-}
-
-static inline void queue_flag_clear_unlocked(unsigned int flag,
-					     struct request_queue *q)
-{
-	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
-	    kref_read(&q->kobj.kref))
-		lockdep_assert_held(q->queue_lock);
-	__clear_bit(flag, &q->queue_flags);
-}
-
-static inline int queue_flag_test_and_clear(unsigned int flag,
-					    struct request_queue *q)
-{
-	queue_lockdep_assert_held(q);
-
-	if (test_bit(flag, &q->queue_flags)) {
-		__clear_bit(flag, &q->queue_flags);
-		return 1;
-	}
-
-	return 0;
-}
-
-static inline int queue_flag_test_and_set(unsigned int flag,
-					  struct request_queue *q)
-{
-	queue_lockdep_assert_held(q);
-
-	if (!test_bit(flag, &q->queue_flags)) {
-		__set_bit(flag, &q->queue_flags);
-		return 0;
-	}
-
-	return 1;
-}
-
-static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
-{
-	queue_lockdep_assert_held(q);
-	__set_bit(flag, &q->queue_flags);
-}
-
-static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
-{
-	queue_lockdep_assert_held(q);
-	__clear_bit(flag, &q->queue_flags);
-}
-
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)

commit 1db2008b79a32db2ad41338c6c74c4735cf74f6d
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:11 2018 -0800

    block: Complain if queue_flag_(set|clear)_unlocked() is abused
    
    Since it is not safe to use queue_flag_(set|clear)_unlocked()
    without holding the queue lock after the sysfs entries for a
    queue have been created, complain if this happens.
    
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f84b3c7887b1..888c9b25cb8f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -728,12 +728,18 @@ static inline void queue_lockdep_assert_held(struct request_queue *q)
 static inline void queue_flag_set_unlocked(unsigned int flag,
 					   struct request_queue *q)
 {
+	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
+	    kref_read(&q->kobj.kref))
+		lockdep_assert_held(q->queue_lock);
 	__set_bit(flag, &q->queue_flags);
 }
 
 static inline void queue_flag_clear_unlocked(unsigned int flag,
 					     struct request_queue *q)
 {
+	if (test_bit(QUEUE_FLAG_INIT_DONE, &q->queue_flags) &&
+	    kref_read(&q->kobj.kref))
+		lockdep_assert_held(q->queue_lock);
 	__clear_bit(flag, &q->queue_flags);
 }
 

commit 8814ce8a0f680599a837af18aefdec774e5c7b97
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:04 2018 -0800

    block: Introduce blk_queue_flag_{set,clear,test_and_{set,clear}}()
    
    Introduce functions that modify the queue flags and that protect
    these modifications with the request queue lock. Except for moving
    one wake_up_all() call from inside to outside a critical section,
    this patch does not change any functionality.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c351aaec3ca7..f84b3c7887b1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -707,6 +707,11 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_POLL))
 
+void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
+void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
+bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
+bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
+
 /*
  * @q->queue_lock is set while a queue is being initialized. Since we know
  * that no other threads access the queue object before @q->queue_lock has

commit 66f91322f39cd18a01524264464c2ff4c98c936e
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:02 2018 -0800

    block: Reorder the queue flag manipulation function definitions
    
    Move the definition of queue_flag_clear_unlocked() up and move the
    definition of queue_in_flight() down such that all queue flag
    manipulation function definitions become contiguous.
    
    This patch does not change any functionality.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 667a9b0053d9..c351aaec3ca7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -726,6 +726,12 @@ static inline void queue_flag_set_unlocked(unsigned int flag,
 	__set_bit(flag, &q->queue_flags);
 }
 
+static inline void queue_flag_clear_unlocked(unsigned int flag,
+					     struct request_queue *q)
+{
+	__clear_bit(flag, &q->queue_flags);
+}
+
 static inline int queue_flag_test_and_clear(unsigned int flag,
 					    struct request_queue *q)
 {
@@ -758,17 +764,6 @@ static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 	__set_bit(flag, &q->queue_flags);
 }
 
-static inline void queue_flag_clear_unlocked(unsigned int flag,
-					     struct request_queue *q)
-{
-	__clear_bit(flag, &q->queue_flags);
-}
-
-static inline int queue_in_flight(struct request_queue *q)
-{
-	return q->in_flight[0] + q->in_flight[1];
-}
-
 static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 {
 	queue_lockdep_assert_held(q);
@@ -804,6 +799,11 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 extern int blk_set_preempt_only(struct request_queue *q);
 extern void blk_clear_preempt_only(struct request_queue *q);
 
+static inline int queue_in_flight(struct request_queue *q)
+{
+	return q->in_flight[0] + q->in_flight[1];
+}
+
 static inline bool blk_account_rq(struct request *rq)
 {
 	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);

commit 5ee0524ba137fe928a88b440d014e3c8451fb32c
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Feb 28 10:15:31 2018 -0800

    block: Add 'lock' as third argument to blk_alloc_queue_node()
    
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Philipp Reisner <philipp.reisner@linbit.com>
    Cc: Ulf Hansson <ulf.hansson@linaro.org>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ed63f3b69c12..667a9b0053d9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1321,7 +1321,8 @@ extern long nr_blockdev_pages(void);
 
 bool __must_check blk_get_queue(struct request_queue *);
 struct request_queue *blk_alloc_queue(gfp_t);
-struct request_queue *blk_alloc_queue_node(gfp_t, int);
+struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
+					   spinlock_t *lock);
 extern void blk_put_queue(struct request_queue *);
 extern void blk_set_queue_dying(struct request_queue *);
 

commit 096392e0714d3a520366ba467e215edf7280acff
Author: Minwoo Im <minwoo.im.dev@gmail.com>
Date:   Thu Feb 15 23:53:17 2018 +0900

    block: fix a typo in comment of BLK_MQ_POLL_STATS_BKTS
    
    Update comment typo _consisitent_ to _consistent_ from following commit.
    commit 0206319fdfee ("blk-mq: Fix poll_stat for new size-based bucketing.")
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Minwoo Im <minwoo.im.dev@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4f3df807cf8f..ed63f3b69c12 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -49,7 +49,7 @@ struct blk_stat_callback;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
-/* Must be consisitent with blk_mq_poll_stats_bkt() */
+/* Must be consistent with blk_mq_poll_stats_bkt() */
 #define BLK_MQ_POLL_STATS_BKTS 16
 
 /*

commit 0a4b6e2f80aad46fb55a5cf7b1664c0aef030ee0
Merge: 9697e9da8429 796baeeef85a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 11:51:49 2018 -0800

    Merge branch 'for-4.16/block' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "This is the main pull request for block IO related changes for the
      4.16 kernel. Nothing major in this pull request, but a good amount of
      improvements and fixes all over the map. This contains:
    
       - BFQ improvements, fixes, and cleanups from Angelo, Chiara, and
         Paolo.
    
       - Support for SMR zones for deadline and mq-deadline from Damien and
         Christoph.
    
       - Set of fixes for bcache by way of Michael Lyle, including fixes
         from himself, Kent, Rui, Tang, and Coly.
    
       - Series from Matias for lightnvm with fixes from Hans Holmberg,
         Javier, and Matias. Mostly centered around pblk, and the removing
         rrpc 1.2 in preparation for supporting 2.0.
    
       - A couple of NVMe pull requests from Christoph. Nothing major in
         here, just fixes and cleanups, and support for command tracing from
         Johannes.
    
       - Support for blk-throttle for tracking reads and writes separately.
         From Joseph Qi. A few cleanups/fixes also for blk-throttle from
         Weiping.
    
       - Series from Mike Snitzer that enables dm to register its queue more
         logically, something that's alwways been problematic on dm since
         it's a stacked device.
    
       - Series from Ming cleaning up some of the bio accessor use, in
         preparation for supporting multipage bvecs.
    
       - Various fixes from Ming closing up holes around queue mapping and
         quiescing.
    
       - BSD partition fix from Richard Narron, fixing a problem where we
         can't mount newer (10/11) FreeBSD partitions.
    
       - Series from Tejun reworking blk-mq timeout handling. The previous
         scheme relied on atomic bits, but it had races where we would think
         a request had timed out if it to reused at the wrong time.
    
       - null_blk now supports faking timeouts, to enable us to better
         exercise and test that functionality separately. From me.
    
       - Kill the separate atomic poll bit in the request struct. After
         this, we don't use the atomic bits on blk-mq anymore at all. From
         me.
    
       - sgl_alloc/free helpers from Bart.
    
       - Heavily contended tag case scalability improvement from me.
    
       - Various little fixes and cleanups from Arnd, Bart, Corentin,
         Douglas, Eryu, Goldwyn, and myself"
    
    * 'for-4.16/block' of git://git.kernel.dk/linux-block: (186 commits)
      block: remove smart1,2.h
      nvme: add tracepoint for nvme_complete_rq
      nvme: add tracepoint for nvme_setup_cmd
      nvme-pci: introduce RECONNECTING state to mark initializing procedure
      nvme-rdma: remove redundant boolean for inline_data
      nvme: don't free uuid pointer before printing it
      nvme-pci: Suspend queues after deleting them
      bsg: use pr_debug instead of hand crafted macros
      blk-mq-debugfs: don't allow write on attributes with seq_operations set
      nvme-pci: Fix queue double allocations
      block: Set BIO_TRACE_COMPLETION on new bio during split
      blk-throttle: use queue_is_rq_based
      block: Remove kblockd_schedule_delayed_work{,_on}()
      blk-mq: Avoid that blk_mq_delay_run_hw_queue() introduces unintended delays
      blk-mq: Rename blk_mq_request_direct_issue() into blk_mq_request_issue_directly()
      lib/scatterlist: Fix chaining support in sgl_alloc_order()
      blk-throttle: track read and write request individually
      block: add bdev_read_only() checks to common helpers
      block: fail op_is_write() requests to read-only partitions
      blk-throttle: export io_serviced_recursive, io_service_bytes_recursive
      ...

commit f5ced52aaa5494c1feb9f80252cb2a2cde0dace8
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jan 19 08:58:56 2018 -0800

    block: Remove kblockd_schedule_delayed_work{,_on}()
    
    The previous patch removed all users of these two functions. Hence
    also remove the functions themselves.
    
    Reviewed-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 71a9371c8182..afc43fb63c16 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1800,8 +1800,6 @@ static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
 
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_work_on(int cpu, struct work_struct *work);
-int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
-int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP

commit 7c3fb70f0341f9d924818e648906774921f4bcb3
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 10 11:46:39 2018 -0700

    block: rearrange a few request fields for better cache layout
    
    Move completion related items (like the call single data) near the
    end of the struct, instead of mixing them in with the initial
    queueing related fields.
    
    Move queuelist below the bio structures. Then we have all
    queueing related bits in the first cache line.
    
    This yields a 1.5-2% increase in IOPS for a null_blk test, both for
    sync and for high thread count access. Sync test goes form 975K to
    992K, 32-thread case from 20.8M to 21.2M IOPS.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4b2f7bb18d6..71a9371c8182 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -141,12 +141,6 @@ typedef __u32 __bitwise req_flags_t;
  * especially blk_mq_rq_ctx_init() to take care of the added fields.
  */
 struct request {
-	struct list_head queuelist;
-	union {
-		call_single_data_t csd;
-		u64 fifo_time;
-	};
-
 	struct request_queue *q;
 	struct blk_mq_ctx *mq_ctx;
 
@@ -164,6 +158,8 @@ struct request {
 	struct bio *bio;
 	struct bio *biotail;
 
+	struct list_head queuelist;
+
 	/*
 	 * The hash is used inside the scheduler, and killed once the
 	 * request reaches the dispatch list. The ipi_list is only used
@@ -211,19 +207,16 @@ struct request {
 	struct hd_struct *part;
 	unsigned long start_time;
 	struct blk_issue_stat issue_stat;
-#ifdef CONFIG_BLK_CGROUP
-	struct request_list *rl;		/* rl this rq is alloced from */
-	unsigned long long start_time_ns;
-	unsigned long long io_start_time_ns;    /* when passed to hardware */
-#endif
 	/* Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
 	 */
 	unsigned short nr_phys_segments;
+
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 	unsigned short nr_integrity_segments;
 #endif
 
+	unsigned short write_hint;
 	unsigned short ioprio;
 
 	unsigned int timeout;
@@ -232,8 +225,6 @@ struct request {
 
 	unsigned int extra_len;	/* length of alignment and padding */
 
-	unsigned short write_hint;
-
 	/*
 	 * On blk-mq, the lower bits of ->gstate (generation number and
 	 * state) carry the MQ_RQ_* state value and the upper bits the
@@ -260,6 +251,11 @@ struct request {
 
 	struct list_head timeout_list;
 
+	union {
+		call_single_data_t csd;
+		u64 fifo_time;
+	};
+
 	/*
 	 * completion callback.
 	 */
@@ -268,6 +264,12 @@ struct request {
 
 	/* for bidi */
 	struct request *next_rq;
+
+#ifdef CONFIG_BLK_CGROUP
+	struct request_list *rl;		/* rl this rq is alloced from */
+	unsigned long long start_time_ns;
+	unsigned long long io_start_time_ns;    /* when passed to hardware */
+#endif
 };
 
 static inline bool blk_rq_is_scsi(struct request *rq)

commit e14575b3d457f5806d79b85886ef94d9c29e3b2a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 10 11:34:25 2018 -0700

    block: convert REQ_ATOM_COMPLETE to stealing rq->__deadline bit
    
    We only have one atomic flag left. Instead of using an entire
    unsigned long for that, steal the bottom bit of the deadline
    field that we already reserved.
    
    Remove ->atomic_flags, since it's now unused.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aa6698cf483c..d4b2f7bb18d6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -156,8 +156,6 @@ struct request {
 
 	int internal_tag;
 
-	unsigned long atomic_flags;
-
 	/* the following two fields are internal, NEVER access directly */
 	unsigned int __data_len;	/* total data len */
 	int tag;

commit 0a72e7f44964b9ada3e5c15820372e9cb119bf80
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jan 9 14:23:42 2018 -0700

    block: add accessors for setting/querying request deadline
    
    We reduce the resolution of request expiry, but since we're already
    using jiffies for this where resolution depends on the kernel
    configuration and since the timeout resolution is coarse anyway,
    that should be fine.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba31674d8581..aa6698cf483c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -257,7 +257,9 @@ struct request {
 	struct u64_stats_sync aborted_gstate_sync;
 	u64 aborted_gstate;
 
-	unsigned long deadline;
+	/* access through blk_rq_set_deadline, blk_rq_deadline */
+	unsigned long __deadline;
+
 	struct list_head timeout_list;
 
 	/*

commit 76a86f9d027b342b8759a4b2f9f7fe046e284220
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 10 11:30:56 2018 -0700

    block: remove REQ_ATOM_POLL_SLEPT
    
    We don't need this to be an atomic flag, it can be a regular
    flag. We either end up on the same CPU for the polling, in which
    case the state is sane, or we did the sleep which would imply
    the needed barrier to ensure we see the right state.
    
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 007a7cf1f262..ba31674d8581 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -127,6 +127,8 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* timeout is expired */
 #define RQF_MQ_TIMEOUT_EXPIRED	((__force req_flags_t)(1 << 20))
+/* already slept for hybrid poll */
+#define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \

commit 634f9e4631a88025d3b90c1884e9a1b6a13d01d2
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:51 2018 -0800

    blk-mq: remove REQ_ATOM_COMPLETE usages from blk-mq
    
    After the recent updates to use generation number and state based
    synchronization, blk-mq no longer depends on REQ_ATOM_COMPLETE except
    to avoid firing the same timeout multiple times.
    
    Remove all REQ_ATOM_COMPLETE usages and use a new rq_flags flag
    RQF_MQ_TIMEOUT_EXPIRED to avoid firing the same timeout multiple
    times.  This removes atomic bitops from hot paths too.
    
    v2: Removed blk_clear_rq_complete() from blk_mq_rq_timed_out().
    
    v3: Added RQF_MQ_TIMEOUT_EXPIRED flag.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ae563d01b29d..007a7cf1f262 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -125,6 +125,8 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_SPECIAL_PAYLOAD	((__force req_flags_t)(1 << 18))
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
+/* timeout is expired */
+#define RQF_MQ_TIMEOUT_EXPIRED	((__force req_flags_t)(1 << 20))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \

commit 1d9bd5161ba32db5665a617edc8b0723880f543e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 9 08:29:48 2018 -0800

    blk-mq: replace timeout synchronization with a RCU and generation based scheme
    
    Currently, blk-mq timeout path synchronizes against the usual
    issue/completion path using a complex scheme involving atomic
    bitflags, REQ_ATOM_*, memory barriers and subtle memory coherence
    rules.  Unfortunately, it contains quite a few holes.
    
    There's a complex dancing around REQ_ATOM_STARTED and
    REQ_ATOM_COMPLETE between issue/completion and timeout paths; however,
    they don't have a synchronization point across request recycle
    instances and it isn't clear what the barriers add.
    blk_mq_check_expired() can easily read STARTED from N-2'th iteration,
    deadline from N-1'th, blk_mark_rq_complete() against Nth instance.
    
    In fact, it's pretty easy to make blk_mq_check_expired() terminate a
    later instance of a request.  If we induce 5 sec delay before
    time_after_eq() test in blk_mq_check_expired(), shorten the timeout to
    2s, and issue back-to-back large IOs, blk-mq starts timing out
    requests spuriously pretty quickly.  Nothing actually timed out.  It
    just made the call on a recycle instance of a request and then
    terminated a later instance long after the original instance finished.
    The scenario isn't theoretical either.
    
    This patch replaces the broken synchronization mechanism with a RCU
    and generation number based one.
    
    1. Each request has a u64 generation + state value, which can be
       updated only by the request owner.  Whenever a request becomes
       in-flight, the generation number gets bumped up too.  This provides
       the basis for the timeout path to distinguish different recycle
       instances of the request.
    
       Also, marking a request in-flight and setting its deadline are
       protected with a seqcount so that the timeout path can fetch both
       values coherently.
    
    2. The timeout path fetches the generation, state and deadline.  If
       the verdict is timeout, it records the generation into a dedicated
       request abortion field and does RCU wait.
    
    3. The completion path is also protected by RCU (from the previous
       patch) and checks whether the current generation number and state
       match the abortion field.  If so, it skips completion.
    
    4. The timeout path, after RCU wait, scans requests again and
       terminates the ones whose generation and state still match the ones
       requested for abortion.
    
       By now, the timeout path knows that either the generation number
       and state changed if it lost the race or the completion will yield
       to it and can safely timeout the request.
    
    While it's more lines of code, it's conceptually simpler, doesn't
    depend on direct use of subtle memory ordering or coherence, and
    hopefully doesn't terminate the wrong instance.
    
    While this change makes REQ_ATOM_COMPLETE synchronization unnecessary
    between issue/complete and timeout paths, REQ_ATOM_COMPLETE isn't
    removed yet as it's still used in other places.  Future patches will
    move all state tracking to the new mechanism and remove all bitops in
    the hot paths.
    
    Note that this patch adds a comment explaining a race condition in
    BLK_EH_RESET_TIMER path.  The race has always been there and this
    patch doesn't change it.  It's just documenting the existing race.
    
    v2: - Fixed BLK_EH_RESET_TIMER handling as pointed out by Jianchao.
        - s/request->gstate_seqc/request->gstate_seq/ as suggested by Peter.
        - READ_ONCE() added in blk_mq_rq_update_state() as suggested by Peter.
    
    v3: - Fixed possible extended seqcount / u64_stats_sync read looping
          spotted by Peter.
        - MQ_RQ_IDLE was incorrectly being set in complete_request instead
          of free_request.  Fixed.
    
    v4: - Rebased on top of hctx_lock() refactoring patch.
        - Added comment explaining the use of hctx_lock() in completion path.
    
    v5: - Added comments requested by Bart.
        - Note the addition of BLK_EH_RESET_TIMER race condition in the
          commit message.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <Bart.VanAssche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 46e606f5b44b..ae563d01b29d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -27,6 +27,8 @@
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
 #include <linux/blkzoned.h>
+#include <linux/seqlock.h>
+#include <linux/u64_stats_sync.h>
 
 struct module;
 struct scsi_ioctl_command;
@@ -230,6 +232,27 @@ struct request {
 
 	unsigned short write_hint;
 
+	/*
+	 * On blk-mq, the lower bits of ->gstate (generation number and
+	 * state) carry the MQ_RQ_* state value and the upper bits the
+	 * generation number which is monotonically incremented and used to
+	 * distinguish the reuse instances.
+	 *
+	 * ->gstate_seq allows updates to ->gstate and other fields
+	 * (currently ->deadline) during request start to be read
+	 * atomically from the timeout path, so that it can operate on a
+	 * coherent set of information.
+	 */
+	seqcount_t gstate_seq;
+	u64 gstate;
+
+	/*
+	 * ->aborted_gstate is used by the timeout to claim a specific
+	 * recycle instance of this request.  See blk_mq_timeout_work().
+	 */
+	struct u64_stats_sync aborted_gstate_sync;
+	u64 aborted_gstate;
+
 	unsigned long deadline;
 	struct list_head timeout_list;
 

commit 6cc77e9cb08041627fe1d32ac3a743249deb8167
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 21 15:43:38 2017 +0900

    block: introduce zoned block devices zone write locking
    
    Components relying only on the request_queue structure for accessing
    block devices (e.g. I/O schedulers) have a limited knowledged of the
    device characteristics. In particular, the device capacity cannot be
    easily discovered, which for a zoned block device also result in the
    inability to easily know the number of zones of the device (the zone
    size is indicated by the chunk_sectors field of the queue limits).
    
    Introduce the nr_zones field to the request_queue structure to simplify
    access to this information. Also, add the bitmap seq_zone_bitmap which
    indicates which zones of the device are sequential zones (write
    preferred or write required) and the bitmap seq_zones_wlock which
    indicates if a zone is write locked, that is, if a write request
    targeting a zone was dispatched to the device. These fields are
    initialized by the low level block device driver (sd.c for ZBC/ZAC
    disks). They are not initialized by stacking drivers (device mappers)
    handling zoned block devices (e.g. dm-linear).
    
    Using this, I/O schedulers can introduce zone write locking to control
    request dispatching to a zoned block device and avoid write request
    reordering by limiting to at most a single write request per zone
    outside of the scheduler at any time.
    
    Based on previous patches from Damien Le Moal.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    [Damien]
    * Fixed comments and identation in blkdev.h
    * Changed helper functions
    * Fixed this commit message
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8089ca17db9a..46e606f5b44b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -121,6 +121,8 @@ typedef __u32 __bitwise req_flags_t;
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
 #define RQF_SPECIAL_PAYLOAD	((__force req_flags_t)(1 << 18))
+/* The per-zone write lock is held for this request */
+#define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \
@@ -546,6 +548,22 @@ struct request_queue {
 
 	struct queue_limits	limits;
 
+	/*
+	 * Zoned block device information for request dispatch control.
+	 * nr_zones is the total number of zones of the device. This is always
+	 * 0 for regular block devices. seq_zones_bitmap is a bitmap of nr_zones
+	 * bits which indicates if a zone is conventional (bit clear) or
+	 * sequential (bit set). seq_zones_wlock is a bitmap of nr_zones
+	 * bits which indicates if a zone is write locked, that is, if a write
+	 * request targeting the zone was dispatched. All three fields are
+	 * initialized by the low level device driver (e.g. scsi/sd.c).
+	 * Stacking drivers (device mappers) may or may not initialize
+	 * these fields.
+	 */
+	unsigned int		nr_zones;
+	unsigned long		*seq_zones_bitmap;
+	unsigned long		*seq_zones_wlock;
+
 	/*
 	 * sg stuff
 	 */
@@ -790,6 +808,27 @@ static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
 
+static inline unsigned int blk_queue_nr_zones(struct request_queue *q)
+{
+	return q->nr_zones;
+}
+
+static inline unsigned int blk_queue_zone_no(struct request_queue *q,
+					     sector_t sector)
+{
+	if (!blk_queue_is_zoned(q))
+		return 0;
+	return sector >> ilog2(q->limits.chunk_sectors);
+}
+
+static inline bool blk_queue_zone_is_seq(struct request_queue *q,
+					 sector_t sector)
+{
+	if (!blk_queue_is_zoned(q) || !q->seq_zones_bitmap)
+		return false;
+	return test_bit(blk_queue_zone_no(q, sector), q->seq_zones_bitmap);
+}
+
 static inline bool rq_is_sync(struct request *rq)
 {
 	return op_is_sync(rq->cmd_flags);
@@ -1029,6 +1068,16 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
+static inline unsigned int blk_rq_zone_no(struct request *rq)
+{
+	return blk_queue_zone_no(rq->q, blk_rq_pos(rq));
+}
+
+static inline unsigned int blk_rq_zone_is_seq(struct request *rq)
+{
+	return blk_queue_zone_is_seq(rq->q, blk_rq_pos(rq));
+}
+
 /*
  * Some commands like WRITE SAME have a payload or data transfer size which
  * is different from the size of the request.  Any driver that supports such
@@ -1578,7 +1627,15 @@ static inline unsigned int bdev_zone_sectors(struct block_device *bdev)
 
 	if (q)
 		return blk_queue_zone_sectors(q);
+	return 0;
+}
 
+static inline unsigned int bdev_nr_zones(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return blk_queue_nr_zones(q);
 	return 0;
 }
 
@@ -1954,6 +2011,60 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
+
+#ifdef CONFIG_BLK_DEV_ZONED
+bool blk_req_needs_zone_write_lock(struct request *rq);
+void __blk_req_zone_write_lock(struct request *rq);
+void __blk_req_zone_write_unlock(struct request *rq);
+
+static inline void blk_req_zone_write_lock(struct request *rq)
+{
+	if (blk_req_needs_zone_write_lock(rq))
+		__blk_req_zone_write_lock(rq);
+}
+
+static inline void blk_req_zone_write_unlock(struct request *rq)
+{
+	if (rq->rq_flags & RQF_ZONE_WRITE_LOCKED)
+		__blk_req_zone_write_unlock(rq);
+}
+
+static inline bool blk_req_zone_is_write_locked(struct request *rq)
+{
+	return rq->q->seq_zones_wlock &&
+		test_bit(blk_rq_zone_no(rq), rq->q->seq_zones_wlock);
+}
+
+static inline bool blk_req_can_dispatch_to_zone(struct request *rq)
+{
+	if (!blk_req_needs_zone_write_lock(rq))
+		return true;
+	return !blk_req_zone_is_write_locked(rq);
+}
+#else
+static inline bool blk_req_needs_zone_write_lock(struct request *rq)
+{
+	return false;
+}
+
+static inline void blk_req_zone_write_lock(struct request *rq)
+{
+}
+
+static inline void blk_req_zone_write_unlock(struct request *rq)
+{
+}
+static inline bool blk_req_zone_is_write_locked(struct request *rq)
+{
+	return false;
+}
+
+static inline bool blk_req_can_dispatch_to_zone(struct request *rq)
+{
+	return true;
+}
+#endif /* CONFIG_BLK_DEV_ZONED */
+
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 4ccafe032005e9b96acbef2e389a4de5b1254add
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 20 13:13:58 2017 -0700

    block: unalign call_single_data in struct request
    
    A previous change blindly added massive alignment to the
    call_single_data structure in struct request. This ballooned it in size
    from 296 to 320 bytes on my setup, for no valid reason at all.
    
    Use the unaligned struct __call_single_data variant instead.
    
    Fixes: 966a967116e69 ("smp: Avoid using two cache lines for struct call_single_data")
    Cc: stable@vger.kernel.org # v4.14
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 100d0df38026..0ce8a372d506 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -135,7 +135,7 @@ typedef __u32 __bitwise req_flags_t;
 struct request {
 	struct list_head queuelist;
 	union {
-		call_single_data_t csd;
+		struct __call_single_data csd;
 		u64 fifo_time;
 	};
 

commit 0abc2a10389f0c9070f76ca906c7382788036b93
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Dec 18 15:40:44 2017 +0800

    block: fix blk_rq_append_bio
    
    Commit caa4b02476e3(blk-map: call blk_queue_bounce from blk_rq_append_bio)
    moves blk_queue_bounce() into blk_rq_append_bio(), but don't consider
    the fact that the bounced bio becomes invisible to caller since the
    parameter type is 'struct bio *'. Make it a pointer to a pointer to
    a bio, so the caller sees the right bio also after a bounce.
    
    Fixes: caa4b02476e3 ("blk-map: call blk_queue_bounce from blk_rq_append_bio")
    Cc: Christoph Hellwig <hch@lst.de>
    Reported-by: Michele Ballabio <barra_cuda@katamail.com>
    (handling failure of blk_rq_append_bio(), only call bio_get() after
    blk_rq_append_bio() returns OK)
    Tested-by: Michele Ballabio <barra_cuda@katamail.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index abd06f540863..100d0df38026 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -965,7 +965,7 @@ extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 extern void blk_rq_unprep_clone(struct request *rq);
 extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
-extern int blk_rq_append_bio(struct request *rq, struct bio *bio);
+extern int blk_rq_append_bio(struct request *rq, struct bio **bio);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_queue_split(struct request_queue *, struct bio **);
 extern void blk_recount_segments(struct request_queue *, struct bio *);

commit 14cb0dc6479dc5ebc63b3a459a5d89a2f1b39fed
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Dec 18 15:40:43 2017 +0800

    block: don't let passthrough IO go into .make_request_fn()
    
    Commit a8821f3f3("block: Improvements to bounce-buffer handling") tries
    to make sure that the bio to .make_request_fn won't exceed BIO_MAX_PAGES,
    but ignores that passthrough I/O can use blk_queue_bounce() too.
    Especially, passthrough IO may not be sector-aligned, and the check
    of 'sectors < bio_sectors(*bio_orig)' inside __blk_queue_bounce() may
    become true even though the max bvec number doesn't exceed BIO_MAX_PAGES,
    then cause the bio splitted, and the original passthrough bio is submited
    to generic_make_request().
    
    This patch fixes this issue by checking if the bio is passthrough IO,
    and use bio_kmalloc() to allocate the cloned passthrough bio.
    
    Cc: NeilBrown <neilb@suse.com>
    Fixes: a8821f3f3("block: Improvements to bounce-buffer handling")
    Tested-by: Michele Ballabio <barra_cuda@katamail.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8089ca17db9a..abd06f540863 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -241,14 +241,24 @@ struct request {
 	struct request *next_rq;
 };
 
+static inline bool blk_op_is_scsi(unsigned int op)
+{
+	return op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;
+}
+
+static inline bool blk_op_is_private(unsigned int op)
+{
+	return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
+}
+
 static inline bool blk_rq_is_scsi(struct request *rq)
 {
-	return req_op(rq) == REQ_OP_SCSI_IN || req_op(rq) == REQ_OP_SCSI_OUT;
+	return blk_op_is_scsi(req_op(rq));
 }
 
 static inline bool blk_rq_is_private(struct request *rq)
 {
-	return req_op(rq) == REQ_OP_DRV_IN || req_op(rq) == REQ_OP_DRV_OUT;
+	return blk_op_is_private(req_op(rq));
 }
 
 static inline bool blk_rq_is_passthrough(struct request *rq)
@@ -256,6 +266,13 @@ static inline bool blk_rq_is_passthrough(struct request *rq)
 	return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
 }
 
+static inline bool bio_is_passthrough(struct bio *bio)
+{
+	unsigned op = bio_op(bio);
+
+	return blk_op_is_scsi(op) || blk_op_is_private(op);
+}
+
 static inline unsigned short req_get_ioprio(struct request *req)
 {
 	return req->ioprio;

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit 9a95e4ef709533efac4aafcb8bddf73f96db50ed
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:59 2017 -0800

    block, nvme: Introduce blk_mq_req_flags_t
    
    Several block layer and NVMe core functions accept a combination
    of BLK_MQ_REQ_* flags through the 'flags' argument but there is
    no verification at compile time whether the right type of block
    layer flags is passed. Make it possible for sparse to verify this.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: linux-nvme@lists.infradead.org
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 402c9d536ae1..e80ea1d31343 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -935,7 +935,7 @@ extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request_flags(struct request_queue *,
 					     unsigned int op,
-					     unsigned int flags);
+					     blk_mq_req_flags_t flags);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
 				       gfp_t gfp_mask);
 extern void blk_requeue_request(struct request_queue *, struct request *);
@@ -959,7 +959,7 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern int blk_queue_enter(struct request_queue *q, unsigned int flags);
+extern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);
 extern void blk_queue_exit(struct request_queue *q);
 extern void blk_start_queue(struct request_queue *q);
 extern void blk_start_queue_async(struct request_queue *q);

commit 3a0a529971ec4e2d933e9c7798db101dfb6b1aec
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:58 2017 -0800

    block, scsi: Make SCSI quiesce and resume work reliably
    
    The contexts from which a SCSI device can be quiesced or resumed are:
    * Writing into /sys/class/scsi_device/*/device/state.
    * SCSI parallel (SPI) domain validation.
    * The SCSI device power management methods. See also scsi_bus_pm_ops.
    
    It is essential during suspend and resume that neither the filesystem
    state nor the filesystem metadata in RAM changes. This is why while
    the hibernation image is being written or restored that SCSI devices
    are quiesced. The SCSI core quiesces devices through scsi_device_quiesce()
    and scsi_device_resume(). In the SDEV_QUIESCE state execution of
    non-preempt requests is deferred. This is realized by returning
    BLKPREP_DEFER from inside scsi_prep_state_check() for quiesced SCSI
    devices. Avoid that a full queue prevents power management requests
    to be submitted by deferring allocation of non-preempt requests for
    devices in the quiesced state. This patch has been tested by running
    the following commands and by verifying that after each resume the
    fio job was still running:
    
    for ((i=0; i<10; i++)); do
      (
        cd /sys/block/md0/md &&
        while true; do
          [ "$(<sync_action)" = "idle" ] && echo check > sync_action
          sleep 1
        done
      ) &
      pids=($!)
      for d in /sys/class/block/sd*[a-z]; do
        bdev=${d#/sys/class/block/}
        hcil=$(readlink "$d/device")
        hcil=${hcil#../../../}
        echo 4 > "$d/queue/nr_requests"
        echo 1 > "/sys/class/scsi_device/$hcil/device/queue_depth"
        fio --name="$bdev" --filename="/dev/$bdev" --buffered=0 --bs=512 \
          --rw=randread --ioengine=libaio --numjobs=4 --iodepth=16       \
          --iodepth_batch=1 --thread --loops=$((2**31)) &
        pids+=($!)
      done
      sleep 1
      echo "$(date) Hibernating ..." >>hibernate-test-log.txt
      systemctl hibernate
      sleep 10
      kill "${pids[@]}"
      echo idle > /sys/block/md0/md/sync_action
      wait
      echo "$(date) Done." >>hibernate-test-log.txt
    done
    
    Reported-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    References: "I/O hangs after resuming from suspend-to-ram" (https://marc.info/?l=linux-block&m=150340235201348).
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Martin Steigerwald <martin@lichtvoll.de>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2147e2381a22..402c9d536ae1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -959,7 +959,7 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern int blk_queue_enter(struct request_queue *q, bool nowait);
+extern int blk_queue_enter(struct request_queue *q, unsigned int flags);
 extern void blk_queue_exit(struct request_queue *q);
 extern void blk_start_queue(struct request_queue *q);
 extern void blk_start_queue_async(struct request_queue *q);

commit c9254f2ddb19387ea9714a57ea48463c20333b92
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:57 2017 -0800

    block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
    
    This flag will be used in the next patch to let the block layer
    core know whether or not a SCSI request queue has been quiesced.
    A quiesced SCSI queue namely only processes RQF_PREEMPT requests.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Martin Steigerwald <martin@lichtvoll.de>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1af5ddd0f631..2147e2381a22 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -632,6 +632,7 @@ struct request_queue {
 #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
+#define QUEUE_FLAG_PREEMPT_ONLY	29	/* only process REQ_PREEMPT requests */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
@@ -732,6 +733,11 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+#define blk_queue_preempt_only(q)				\
+	test_bit(QUEUE_FLAG_PREEMPT_ONLY, &(q)->queue_flags)
+
+extern int blk_set_preempt_only(struct request_queue *q);
+extern void blk_clear_preempt_only(struct request_queue *q);
 
 static inline bool blk_account_rq(struct request *rq)
 {

commit 6a15674d1e90917f1723a814e2e8c949000440f7
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Thu Nov 9 10:49:54 2017 -0800

    block: Introduce blk_get_request_flags()
    
    A side effect of this patch is that the GFP mask that is passed to
    several allocation functions in the legacy block layer is changed
    from GFP_KERNEL into __GFP_DIRECT_RECLAIM.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Tested-by: Martin Steigerwald <martin@lichtvoll.de>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1437ef4d8037..1af5ddd0f631 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -927,6 +927,9 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
+extern struct request *blk_get_request_flags(struct request_queue *,
+					     unsigned int op,
+					     unsigned int flags);
 extern struct request *blk_get_request(struct request_queue *, unsigned int op,
 				       gfp_t gfp_mask);
 extern void blk_requeue_request(struct request_queue *, struct request *);

commit f00c4d80ffdac9e3a64947ebd57489f3232d5a74
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 5 10:36:31 2017 +0300

    block: pass full fmode_t to blk_verify_command
    
    Use the obvious calling convention.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 225617dd0a3f..1437ef4d8037 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1360,7 +1360,7 @@ static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
 				    gfp_mask, 0);
 }
 
-extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
+extern int blk_verify_command(unsigned char *cmd, fmode_t mode);
 
 enum blk_default_limits {
 	BLK_MAX_SEGMENTS	= 128,

commit ea435e1b9392a33deceaea2a16ebaa3397bead93
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 2 21:29:54 2017 +0300

    block: add a poll_fn callback to struct request_queue
    
    That we we can also poll non blk-mq queues.  Mostly needed for
    the NVMe multipath code, but could also be useful elsewhere.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fddda6a1f9b5..225617dd0a3f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -266,6 +266,7 @@ struct blk_queue_ctx;
 
 typedef void (request_fn_proc) (struct request_queue *q);
 typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
+typedef bool (poll_q_fn) (struct request_queue *q, blk_qc_t);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 
@@ -408,6 +409,7 @@ struct request_queue {
 
 	request_fn_proc		*request_fn;
 	make_request_fn		*make_request_fn;
+	poll_q_fn		*poll_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unprep_rq_fn		*unprep_rq_fn;
 	softirq_done_fn		*softirq_done_fn;
@@ -975,7 +977,7 @@ extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 int blk_status_to_errno(blk_status_t status);
 blk_status_t errno_to_blk_status(int errno);
 
-bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie);
+bool blk_poll(struct request_queue *q, blk_qc_t cookie);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {

commit ef71de8b15d891b27b8c983a9a8972b11cb4576a
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 2 21:29:51 2017 +0300

    block: add a blk_steal_bios helper
    
    This helpers allows to bounce steal the uncompleted bios from a request so
    that they can be reissued on another path.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index eda3d25c0f68..fddda6a1f9b5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1094,6 +1094,8 @@ extern struct request *blk_peek_request(struct request_queue *q);
 extern void blk_start_request(struct request *rq);
 extern struct request *blk_fetch_request(struct request_queue *q);
 
+void blk_steal_bios(struct bio_list *list, struct request *rq);
+
 /*
  * Request completion related functions.
  *

commit f421e1d9ade4e1b88183e54425cf50e390d16a7f
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 2 21:29:50 2017 +0300

    block: provide a direct_make_request helper
    
    This helper allows reinserting a bio into a new queue without much
    overhead, but requires all queue limits to be the same for the upper
    and lower queues, and it does not provide any recursion preventions.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Javier González <javier@cnexlabs.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 72637028f3c9..eda3d25c0f68 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -920,6 +920,7 @@ static inline void rq_flush_dcache_pages(struct request *rq)
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern blk_qc_t generic_make_request(struct bio *bio);
+extern blk_qc_t direct_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 02fa42d24b52..8da66379f7ea 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_BLKDEV_H
 #define _LINUX_BLKDEV_H
 

commit 7f66721a7d5bf6251f9c49aada9200c61ddcecc5
Author: Rakesh Pandit <rakesh@tuxera.com>
Date:   Thu Oct 12 19:58:10 2017 +0300

    fs/block_dev: remove vfs_msg() interface
    
    Replaced by pr_err usage in commit ef51042472f5 ("block, dax: move
    "select DAX" from BLOCK to FS_DAX")
    
    Signed-off-by: Rakesh Pandit <rakesh@tuxera.com>
    Acked-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9fb71fc7d0e8..72637028f3c9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -917,17 +917,6 @@ static inline void rq_flush_dcache_pages(struct request *rq)
 }
 #endif
 
-#ifdef CONFIG_PRINTK
-#define vfs_msg(sb, level, fmt, ...)				\
-	__vfs_msg(sb, level, fmt, ##__VA_ARGS__)
-#else
-#define vfs_msg(sb, level, fmt, ...)				\
-do {								\
-	no_printk(fmt, ##__VA_ARGS__);				\
-	__vfs_msg(sb, "", " ");					\
-} while (0)
-#endif
-
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern blk_qc_t generic_make_request(struct bio *bio);

commit 5fdee2127faa77c9c91862ad5e001dfab7013e92
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 5 21:22:52 2017 +0200

    block: remove QUEUE_FLAG_STACKABLE
    
    We already have a queue_is_rq_based helper to check if a request_queue
    is request based, so we can remove the flag for it.
    
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 02fa42d24b52..9fb71fc7d0e8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -609,7 +609,6 @@ struct request_queue {
 #define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
-#define QUEUE_FLAG_STACKABLE	8	/* supports request stacking */
 #define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     10	/* do IO stats */
@@ -633,12 +632,10 @@ struct request_queue {
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
-				 (1 << QUEUE_FLAG_STACKABLE)	|	\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
-				 (1 << QUEUE_FLAG_STACKABLE)	|	\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_POLL))
 
@@ -722,8 +719,6 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
 #define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
 #define blk_queue_add_random(q)	test_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)
-#define blk_queue_stackable(q)	\
-	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
 #define blk_queue_secure_erase(q) \
 	(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))

commit 5acb3cc2c2e9d3020a4fee43763c6463767f1572
Author: Waiman Long <longman@redhat.com>
Date:   Wed Sep 20 13:12:20 2017 -0600

    blktrace: Fix potential deadlock between delete & sysfs ops
    
    The lockdep code had reported the following unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(s_active#228);
                                   lock(&bdev->bd_mutex/1);
                                   lock(s_active#228);
      lock(&bdev->bd_mutex);
    
     *** DEADLOCK ***
    
    The deadlock may happen when one task (CPU1) is trying to delete a
    partition in a block device and another task (CPU0) is accessing
    tracing sysfs file (e.g. /sys/block/dm-1/trace/act_mask) in that
    partition.
    
    The s_active isn't an actual lock. It is a reference count (kn->count)
    on the sysfs (kernfs) file. Removal of a sysfs file, however, require
    a wait until all the references are gone. The reference count is
    treated like a rwsem using lockdep instrumentation code.
    
    The fact that a thread is in the sysfs callback method or in the
    ioctl call means there is a reference to the opended sysfs or device
    file. That should prevent the underlying block structure from being
    removed.
    
    Instead of using bd_mutex in the block_device structure, a new
    blk_trace_mutex is now added to the request_queue structure to protect
    access to the blk_trace structure.
    
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    
    Fix typo in patch subject line, and prune a comment detailing how
    the code used to work.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 460294bb0fa5..02fa42d24b52 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -551,6 +551,7 @@ struct request_queue {
 	int			node;
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	struct blk_trace	*blk_trace;
+	struct mutex		blk_trace_mutex;
 #endif
 	/*
 	 * for flush operations

commit a0725ab0c7536076d5477264420ef420ebb64501
Merge: 3ee31b89d9b1 ef13ecbc134d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 7 11:59:42 2017 -0700

    Merge branch 'for-4.14/block' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
     "This is the first pull request for 4.14, containing most of the code
      changes. It's a quiet series this round, which I think we needed after
      the churn of the last few series. This contains:
    
       - Fix for a registration race in loop, from Anton Volkov.
    
       - Overflow complaint fix from Arnd for DAC960.
    
       - Series of drbd changes from the usual suspects.
    
       - Conversion of the stec/skd driver to blk-mq. From Bart.
    
       - A few BFQ improvements/fixes from Paolo.
    
       - CFQ improvement from Ritesh, allowing idling for group idle.
    
       - A few fixes found by Dan's smatch, courtesy of Dan.
    
       - A warning fixup for a race between changing the IO scheduler and
         device remova. From David Jeffery.
    
       - A few nbd fixes from Josef.
    
       - Support for cgroup info in blktrace, from Shaohua.
    
       - Also from Shaohua, new features in the null_blk driver to allow it
         to actually hold data, among other things.
    
       - Various corner cases and error handling fixes from Weiping Zhang.
    
       - Improvements to the IO stats tracking for blk-mq from me. Can
         drastically improve performance for fast devices and/or big
         machines.
    
       - Series from Christoph removing bi_bdev as being needed for IO
         submission, in preparation for nvme multipathing code.
    
       - Series from Bart, including various cleanups and fixes for switch
         fall through case complaints"
    
    * 'for-4.14/block' of git://git.kernel.dk/linux-block: (162 commits)
      kernfs: checking for IS_ERR() instead of NULL
      drbd: remove BIOSET_NEED_RESCUER flag from drbd_{md_,}io_bio_set
      drbd: Fix allyesconfig build, fix recent commit
      drbd: switch from kmalloc() to kmalloc_array()
      drbd: abort drbd_start_resync if there is no connection
      drbd: move global variables to drbd namespace and make some static
      drbd: rename "usermode_helper" to "drbd_usermode_helper"
      drbd: fix race between handshake and admin disconnect/down
      drbd: fix potential deadlock when trying to detach during handshake
      drbd: A single dot should be put into a sequence.
      drbd: fix rmmod cleanup, remove _all_ debugfs entries
      drbd: Use setup_timer() instead of init_timer() to simplify the code.
      drbd: fix potential get_ldev/put_ldev refcount imbalance during attach
      drbd: new disk-option disable-write-same
      drbd: Fix resource role for newly created resources in events2
      drbd: mark symbols static where possible
      drbd: Send P_NEG_ACK upon write error in protocol != C
      drbd: add explicit plugging when submitting batches
      drbd: change list_for_each_safe to while(list_first_entry_or_null)
      drbd: introduce drbd_recv_header_maybe_unplug
      ...

commit edc2988c548db05e33b921fed15821010bc74895
Merge: d82fed752942 81a84ad3cb57
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Sep 4 11:01:18 2017 +0200

    Merge branch 'linus' into locking/core, to fix up conflicts
    
     Conflicts:
            mm/page_alloc.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 966a967116e699762dbf4af7f9e0d1955c25aa37
Author: Ying Huang <ying.huang@intel.com>
Date:   Tue Aug 8 12:30:00 2017 +0800

    smp: Avoid using two cache lines for struct call_single_data
    
    struct call_single_data is used in IPIs to transfer information between
    CPUs.  Its size is bigger than sizeof(unsigned long) and less than
    cache line size.  Currently it is not allocated with any explicit alignment
    requirements.  This makes it possible for allocated call_single_data to
    cross two cache lines, which results in double the number of the cache lines
    that need to be transferred among CPUs.
    
    This can be fixed by requiring call_single_data to be aligned with the
    size of call_single_data. Currently the size of call_single_data is the
    power of 2.  If we add new fields to call_single_data, we may need to
    add padding to make sure the size of new definition is the power of 2
    as well.
    
    Fortunately, this is enforced by GCC, which will report bad sizes.
    
    To set alignment requirements of call_single_data to the size of
    call_single_data, a struct definition and a typedef is used.
    
    To test the effect of the patch, I used the vm-scalability multiple
    thread swap test case (swap-w-seq-mt).  The test will create multiple
    threads and each thread will eat memory until all RAM and part of swap
    is used, so that huge number of IPIs are triggered when unmapping
    memory.  In the test, the throughput of memory writing improves ~5%
    compared with misaligned call_single_data, because of faster IPIs.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    [ Add call_single_data_t and align with size of call_single_data. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25f6a0cb27d3..006fa09a641e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -134,7 +134,7 @@ typedef __u32 __bitwise req_flags_t;
 struct request {
 	struct list_head queuelist;
 	union {
-		struct call_single_data csd;
+		call_single_data_t csd;
 		u64 fifo_time;
 	};
 

commit 50b4d485528d1dbe0bd249f2073140e3444f4a7b
Author: Benjamin Block <bblock@linux.vnet.ibm.com>
Date:   Thu Aug 24 01:57:56 2017 +0200

    bsg-lib: fix kernel panic resulting from missing allocation of reply-buffer
    
    Since we split the scsi_request out of struct request bsg fails to
    provide a reply-buffer for the drivers. This was done via the pointer
    for sense-data, that is not preallocated anymore.
    
    Failing to allocate/assign it results in illegal dereferences because
    LLDs use this pointer unquestioned.
    
    An example panic on s390x, using the zFCP driver, looks like this (I had
    debugging on, otherwise NULL-pointer dereferences wouldn't even panic on
    s390x):
    
    Unable to handle kernel pointer dereference in virtual kernel address space
    Failing address: 6b6b6b6b6b6b6000 TEID: 6b6b6b6b6b6b6403
    Fault in home space mode while using kernel ASCE.
    AS:0000000001590007 R3:0000000000000024
    Oops: 0038 ilc:2 [#1] PREEMPT SMP DEBUG_PAGEALLOC
    Modules linked in: <Long List>
    CPU: 2 PID: 0 Comm: swapper/2 Not tainted 4.12.0-bsg-regression+ #3
    Hardware name: IBM 2964 N96 702 (z/VM 6.4.0)
    task: 0000000065cb0100 task.stack: 0000000065cb4000
    Krnl PSW : 0704e00180000000 000003ff801e4156 (zfcp_fc_ct_els_job_handler+0x16/0x58 [zfcp])
               R:0 T:1 IO:1 EX:1 Key:0 M:1 W:0 P:0 AS:3 CC:2 PM:0 RI:0 EA:3
    Krnl GPRS: 0000000000000001 000000005fa9d0d0 000000005fa9d078 0000000000e16866
               000003ff00000290 6b6b6b6b6b6b6b6b 0000000059f78f00 000000000000000f
               00000000593a0958 00000000593a0958 0000000060d88800 000000005ddd4c38
               0000000058b50100 07000000659cba08 000003ff801e8556 00000000659cb9a8
    Krnl Code: 000003ff801e4146: e31020500004        lg      %r1,80(%r2)
               000003ff801e414c: 58402040           l       %r4,64(%r2)
              #000003ff801e4150: e35020200004       lg      %r5,32(%r2)
              >000003ff801e4156: 50405004           st      %r4,4(%r5)
               000003ff801e415a: e54c50080000       mvhi    8(%r5),0
               000003ff801e4160: e33010280012       lt      %r3,40(%r1)
               000003ff801e4166: a718fffb           lhi     %r1,-5
               000003ff801e416a: 1803               lr      %r0,%r3
    Call Trace:
    ([<000003ff801e8556>] zfcp_fsf_req_complete+0x726/0x768 [zfcp])
     [<000003ff801ea82a>] zfcp_fsf_reqid_check+0x102/0x180 [zfcp]
     [<000003ff801eb980>] zfcp_qdio_int_resp+0x230/0x278 [zfcp]
     [<00000000009b91b6>] qdio_kick_handler+0x2ae/0x2c8
     [<00000000009b9e3e>] __tiqdio_inbound_processing+0x406/0xc10
     [<00000000001684c2>] tasklet_action+0x15a/0x1d8
     [<0000000000bd28ec>] __do_softirq+0x3ec/0x848
     [<00000000001675a4>] irq_exit+0x74/0xf8
     [<000000000010dd6a>] do_IRQ+0xba/0xf0
     [<0000000000bd19e8>] io_int_handler+0x104/0x2d4
     [<00000000001033b6>] enabled_wait+0xb6/0x188
    ([<000000000010339e>] enabled_wait+0x9e/0x188)
     [<000000000010396a>] arch_cpu_idle+0x32/0x50
     [<0000000000bd0112>] default_idle_call+0x52/0x68
     [<00000000001cd0fa>] do_idle+0x102/0x188
     [<00000000001cd41e>] cpu_startup_entry+0x3e/0x48
     [<0000000000118c64>] smp_start_secondary+0x11c/0x130
     [<0000000000bd2016>] restart_int_handler+0x62/0x78
     [<0000000000000000>]           (null)
    INFO: lockdep is turned off.
    Last Breaking-Event-Address:
     [<000003ff801e41d6>] zfcp_fc_ct_job_handler+0x3e/0x48 [zfcp]
    
    Kernel panic - not syncing: Fatal exception in interrupt
    
    This patch moves bsg-lib to allocate and setup struct bsg_job ahead of
    time, including the allocation of a buffer for the reply-data.
    
    This means, struct bsg_job is not allocated separately anymore, but as part
    of struct request allocation - similar to struct scsi_cmd. Reflect this in
    the function names that used to handle creation/destruction of struct
    bsg_job.
    
    Reported-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Fixes: 82ed4db499b8 ("block: split scsi_request out of struct request")
    Cc: <stable@vger.kernel.org> #4.11+
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25f6a0cb27d3..2a5d52fa90f5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -568,7 +568,6 @@ struct request_queue {
 
 #if defined(CONFIG_BLK_DEV_BSG)
 	bsg_job_fn		*bsg_job_fn;
-	int			bsg_job_size;
 	struct bsg_class_device bsg_dev;
 #endif
 

commit e743eb1ecd5564b5ae0a4a76c1566f748a358839
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Aug 10 08:25:38 2017 -0600

    block: remove unused syncfull/asyncfull queue flags
    
    We haven't used these in years, but somehow the definitions still
    remained. Kill them, and renumber the QUEUE_FLAG_ space. We had
    a hole in the beginning of the space, too.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25f6a0cb27d3..f45f157b2910 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -601,38 +601,36 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
-#define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
-#define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
-#define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
-#define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
-#define QUEUE_FLAG_DYING	5	/* queue being torn down */
-#define QUEUE_FLAG_BYPASS	6	/* act as dumb FIFO queue */
-#define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
-#define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
-#define QUEUE_FLAG_SAME_COMP	9	/* complete on same CPU-group */
-#define QUEUE_FLAG_FAIL_IO     10	/* fake timeout */
-#define QUEUE_FLAG_STACKABLE   11	/* supports request stacking */
-#define QUEUE_FLAG_NONROT      12	/* non-rotational device (SSD) */
+#define QUEUE_FLAG_QUEUED	0	/* uses generic tag queueing */
+#define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
+#define QUEUE_FLAG_DYING	2	/* queue being torn down */
+#define QUEUE_FLAG_BYPASS	3	/* act as dumb FIFO queue */
+#define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
+#define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
+#define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
+#define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
+#define QUEUE_FLAG_STACKABLE	8	/* supports request stacking */
+#define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
-#define QUEUE_FLAG_IO_STAT     13	/* do IO stats */
-#define QUEUE_FLAG_DISCARD     14	/* supports DISCARD */
-#define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
-#define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
-#define QUEUE_FLAG_SECERASE    17	/* supports secure erase */
-#define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
-#define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
-#define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
-#define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
-#define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
-#define QUEUE_FLAG_WC	       23	/* Write back caching */
-#define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
-#define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
-#define QUEUE_FLAG_DAX         26	/* device supports DAX */
-#define QUEUE_FLAG_STATS       27	/* track rq completion times */
-#define QUEUE_FLAG_POLL_STATS  28	/* collecting stats for hybrid polling */
-#define QUEUE_FLAG_REGISTERED  29	/* queue has been registered to a disk */
-#define QUEUE_FLAG_SCSI_PASSTHROUGH 30	/* queue supports SCSI commands */
-#define QUEUE_FLAG_QUIESCED    31	/* queue has been quiesced */
+#define QUEUE_FLAG_IO_STAT     10	/* do IO stats */
+#define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
+#define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
+#define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
+#define QUEUE_FLAG_SECERASE    14	/* supports secure erase */
+#define QUEUE_FLAG_SAME_FORCE  15	/* force complete on same CPU */
+#define QUEUE_FLAG_DEAD        16	/* queue tear-down finished */
+#define QUEUE_FLAG_INIT_DONE   17	/* queue is initialized */
+#define QUEUE_FLAG_NO_SG_MERGE 18	/* don't attempt to merge SG segments*/
+#define QUEUE_FLAG_POLL	       19	/* IO polling enabled if set */
+#define QUEUE_FLAG_WC	       20	/* Write back caching */
+#define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
+#define QUEUE_FLAG_FLUSH_NQ    22	/* flush not queueuable */
+#define QUEUE_FLAG_DAX         23	/* device supports DAX */
+#define QUEUE_FLAG_STATS       24	/* track rq completion times */
+#define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
+#define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
+#define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
+#define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 1c4bc3ab9a064d98cdf6de6b44f89d5c3757fa32
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 19 09:26:22 2017 +0200

    block: remove the queue_bounce_pfn helper
    
    Only used inside the bounce code, and opencoding it makes it more obvious
    what is going on.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e7eef48c97c9..25f6a0cb27d3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1385,11 +1385,6 @@ enum blk_default_limits {
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
-static inline unsigned long queue_bounce_pfn(struct request_queue *q)
-{
-	return q->limits.bounce_pfn;
-}
-
 static inline unsigned long queue_segment_boundary(struct request_queue *q)
 {
 	return q->limits.seg_boundary_mask;

commit 3bce016a4c5975e4279bfb3cbd6d0332b856cc72
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 19 09:26:21 2017 +0200

    block: move bounce declarations to block/blk.h
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e1e289ab66b9..e7eef48c97c9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -884,19 +884,6 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
 #define BLK_DEFAULT_SG_TIMEOUT	(60 * HZ)
 #define BLK_MIN_SG_TIMEOUT	(7 * HZ)
 
-#ifdef CONFIG_BOUNCE
-extern int init_emergency_isa_pool(void);
-extern void blk_queue_bounce(struct request_queue *q, struct bio **bio);
-#else
-static inline int init_emergency_isa_pool(void)
-{
-	return 0;
-}
-static inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)
-{
-}
-#endif /* CONFIG_MMU */
-
 struct rq_map_data {
 	struct page **pages;
 	int page_order;

commit f793dfd3f39a3dc50468b06498606b3a906f42f1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Jun 26 08:15:27 2017 -0600

    blk-mq: expose write hints through debugfs
    
    Useful to verify that things are working the way they should.
    Reading the file will return number of kb written with each
    write hint. Writing the file will reset the statistics. No care
    is taken to ensure that we don't race on updates.
    
    Drivers will write to q->write_hints[] if they handle a given
    write hint.
    
    Reviewed-by: Andreas Dilger <adilger@dilger.ca>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0eebd3bcfd85..e1e289ab66b9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -596,6 +596,9 @@ struct request_queue {
 	void			*rq_alloc_data;
 
 	struct work_struct	release_work;
+
+#define BLK_MAX_WRITE_HINTS	5
+	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit cb6934f8ea1a595902ca37e250e0917d4dd7b2a7
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jun 27 09:22:02 2017 -0600

    block: add support for write hints in a bio
    
    No functional changes in this patch, we just use up some holes
    in the bio and request structures to define a write hint that
    we psas down the stack.
    
    Ensure that we don't merge requests that have different life time
    hints assigned to them, and that we inherit the write hint when
    cloning a bio.
    
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bf2157141d53..0eebd3bcfd85 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -225,6 +225,8 @@ struct request {
 
 	unsigned int extra_len;	/* length of alignment and padding */
 
+	unsigned short write_hint;
+
 	unsigned long deadline;
 	struct list_head timeout_list;
 

commit f95a0d6a95b12a79b7492da7ab687ae4cd741124
Merge: a9590fe148c0 8e8320c9315c
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Jun 22 21:55:24 2017 -0600

    Merge commit '8e8320c9315c' into for-4.13/block
    
    Pull in the fix for shared tags, as it conflicts with the pending
    changes in for-4.13/block. We already pulled in v4.12-rc5 to solve
    other conflicts or get fixes that went into 4.12, so not a lot
    of changes in this merge.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 8e8320c9315c47a6a090188720ccff32a6a6ba18
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jun 20 17:56:13 2017 -0600

    blk-mq: fix performance regression with shared tags
    
    If we have shared tags enabled, then every IO completion will trigger
    a full loop of every queue belonging to a tag set, and every hardware
    queue for each of those queues, even if nothing needs to be done.
    This causes a massive performance regression if you have a lot of
    shared devices.
    
    Instead of doing this huge full scan on every IO, add an atomic
    counter to the main queue that tracks how many hardware queues have
    been marked as needing a restart. With that, we can avoid looking for
    restartable queues, if we don't have to.
    
    Max reports that this restores performance. Before this patch, 4K
    IOPS was limited to 22-23K IOPS. With the patch, we are running at
    950-970K IOPS.
    
    Fixes: 6d8c6c0f97ad ("blk-mq: Restart a single queue if tag sets are shared")
    Reported-by: Max Gurtovoy <maxg@mellanox.com>
    Tested-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Tested-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b74a3edcb3da..1ddd36bd2173 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -391,6 +391,8 @@ struct request_queue {
 	int			nr_rqs[2];	/* # allocated [a]sync rqs */
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
+	atomic_t		shared_hctx_restart;
+
 	struct blk_queue_stats	*stats;
 	struct rq_wb		*rq_wb;
 

commit 9e0c829906b9aa1e7ad84689f2bcd56457bdb417
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:44 2017 -0700

    block: Add a comment above queue_lockdep_assert_held()
    
    Add a comment above the queue_lockdep_assert_held() macro that
    explains the purpose of the q->queue_lock test.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9a36164487d0..3e60e7a654bd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -635,6 +635,13 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_POLL))
 
+/*
+ * @q->queue_lock is set while a queue is being initialized. Since we know
+ * that no other threads access the queue object before @q->queue_lock has
+ * been set, it is safe to manipulate queue flags without holding the
+ * queue_lock if @q->queue_lock == NULL. See also blk_alloc_queue_node() and
+ * blk_init_allocated_queue().
+ */
 static inline void queue_lockdep_assert_held(struct request_queue *q)
 {
 	if (q->queue_lock)

commit d280bab305431c1836423f3cd6a5ff0e35a601ef
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:40 2017 -0700

    block: Introduce request_queue.initialize_rq_fn()
    
    Several block drivers need to initialize the driver-private request
    data after having called blk_get_request() and before .prep_rq_fn()
    is called, e.g. when submitting a REQ_OP_SCSI_* request. Avoid that
    that initialization code has to be repeated after every
    blk_get_request() call by adding new callback functions to struct
    request_queue and to struct blk_mq_ops.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e21dd893ee86..9a36164487d0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -410,8 +410,12 @@ struct request_queue {
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
+	/* Called just after a request is allocated */
 	init_rq_fn		*init_rq_fn;
+	/* Called just before a request is freed */
 	exit_rq_fn		*exit_rq_fn;
+	/* Called from inside blk_get_request() */
+	void (*initialize_rq_fn)(struct request *rq);
 
 	const struct blk_mq_ops	*mq_ops;
 

commit cd6ce1482fd9e691bb68c660fa918c90f6b1bc25
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jun 20 11:15:39 2017 -0700

    block: Make request operation type argument declarations consistent
    
    Instead of declaring the second argument of blk_*_get_request()
    as int and passing it to functions that expect an unsigned int,
    declare that second argument as unsigned int. Also because of
    consistency, rename that second argument from 'rw' into 'op'.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0deed7274a7f..e21dd893ee86 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -935,7 +935,8 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
-extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
+extern struct request *blk_get_request(struct request_queue *, unsigned int op,
+				       gfp_t gfp_mask);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,

commit efbeccdb59d666b9c77d505af01097cc0a9d102b
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 19 09:24:41 2017 +0200

    block: stop using bio_data() in blk_write_same_mergeable
    
    While the Write Same page currently always is in low-level it is just
    as easy and safer to just compare the page and offset directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 22cfba64ce81..0deed7274a7f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -815,7 +815,8 @@ static inline bool rq_mergeable(struct request *rq)
 
 static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
 {
-	if (bio_data(a) == bio_data(b))
+	if (bio_page(a) == bio_page(b) &&
+	    bio_offset(a) == bio_offset(b))
 		return true;
 
 	return false;

commit f4560ffe8cec1361b1021d81aca6a4173f8e7c87
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Jun 18 14:24:27 2017 -0600

    blk-mq: use QUEUE_FLAG_QUIESCED to quiesce queue
    
    It is required that no dispatch can happen any more once
    blk_mq_quiesce_queue() returns, and we don't have such requirement
    on APIs of stopping queue.
    
    But blk_mq_quiesce_queue() still may not block/drain dispatch in the
    the case of BLK_MQ_S_START_ON_RUN, so use the new introduced flag of
    QUEUE_FLAG_QUIESCED and evaluate it inside RCU read-side critical
    sections for fixing this issue.
    
    Also blk_mq_quiesce_queue() is implemented via stopping queue, which
    limits its uses, and easy to cause race, because any queue restart in
    other paths may break blk_mq_quiesce_queue(). With the introduced
    flag of QUEUE_FLAG_QUIESCED, we don't need to depend on stopping queue
    for quiescing any more.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8423f6baf818..22cfba64ce81 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -619,6 +619,7 @@ struct request_queue {
 #define QUEUE_FLAG_POLL_STATS  28	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED  29	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 30	/* queue supports SCSI commands */
+#define QUEUE_FLAG_QUIESCED    31	/* queue has been quiesced */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -715,6 +716,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+#define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 
 static inline bool blk_account_rq(struct request *rq)
 {

commit 97e0120990f4a7037f72c0e115e5c7f514025738
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jun 6 23:22:01 2017 +0800

    blk-mq: move blk_mq_quiesce_queue() into include/linux/blk-mq.h
    
    We usually put blk_mq_*() into include/linux/blk-mq.h, so
    move this API into there.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 670df402bc51..8423f6baf818 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -965,7 +965,6 @@ extern void __blk_run_queue(struct request_queue *q);
 extern void __blk_run_queue_uncond(struct request_queue *q);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_run_queue_async(struct request_queue *q);
-extern void blk_mq_quiesce_queue(struct request_queue *q);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,
 			   gfp_t);

commit af67c31fba3b879b241536a48df703a2eee18ebf
Author: NeilBrown <neilb@suse.com>
Date:   Sun Jun 18 14:38:57 2017 +1000

    blk: remove bio_set arg from blk_queue_split()
    
    blk_queue_split() is always called with the last arg being q->bio_split,
    where 'q' is the first arg.
    
    Also blk_queue_split() sometimes uses the passed-in 'bs' and sometimes uses
    q->bio_split.
    
    This is inconsistent and unnecessary.  Remove the last arg and always use
    q->bio_split inside blk_queue_split()
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Credit-to: Javier González <jg@lightnvm.io> (Noticed that lightnvm was missed)
    Reviewed-by: Javier González <javier@cnexlabs.com>
    Tested-by: Javier González <javier@cnexlabs.com>
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 76b6df862a12..670df402bc51 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -944,8 +944,7 @@ extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern int blk_rq_append_bio(struct request *rq, struct bio *bio);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
-extern void blk_queue_split(struct request_queue *, struct bio **,
-			    struct bio_set *);
+extern void blk_queue_split(struct request_queue *, struct bio **);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
 extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,

commit dc9edc44de6cd7cc8cc7f5b36c1adb221eda3207
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Jun 14 13:27:50 2017 -0600

    block: Fix a blk_exit_rl() regression
    
    Avoid that the following complaint is reported:
    
     BUG: sleeping function called from invalid context at kernel/workqueue.c:2790
     in_atomic(): 1, irqs_disabled(): 0, pid: 41, name: rcuop/3
     1 lock held by rcuop/3/41:
      #0:  (rcu_callback){......}, at: [<ffffffff8111f9a2>] rcu_nocb_kthread+0x282/0x500
     Call Trace:
      dump_stack+0x86/0xcf
      ___might_sleep+0x174/0x260
      __might_sleep+0x4a/0x80
      flush_work+0x7e/0x2e0
      __cancel_work_timer+0x143/0x1c0
      cancel_work_sync+0x10/0x20
      blk_throtl_exit+0x25/0x60
      blkcg_exit_queue+0x35/0x40
      blk_release_queue+0x42/0x130
      kobject_put+0xa9/0x190
    
    This happens since we invoke callbacks that need to block from the
    queue release handler. Fix this by pushing the final release to
    a workqueue.
    
    Reported-by: Ross Zwisler <zwisler@gmail.com>
    Fixes: commit b425e5049258 ("block: Avoid that blk_exit_rl() triggers a use-after-free")
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Tested-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    
    Updated changelog
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ab92c4ea138b..b74a3edcb3da 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -586,6 +586,8 @@ struct request_queue {
 
 	size_t			cmd_size;
 	void			*rq_alloc_data;
+
+	struct work_struct	release_work;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 4e4cbee93d56137ebff722be022cae5f70ef84fb
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:06 2017 +0200

    block: switch bios to blk_status_t
    
    Replace bi_error with a new bi_status to allow for a clear conversion.
    Note that device mapper overloaded bi_error with a private value, which
    we'll have to keep arround at least for now and thus propagate to a
    proper blk_status_t value.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2a8871638453..76b6df862a12 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1782,7 +1782,7 @@ struct blk_integrity_iter {
 	const char		*disk_name;
 };
 
-typedef int (integrity_processing_fn) (struct blk_integrity_iter *);
+typedef blk_status_t (integrity_processing_fn) (struct blk_integrity_iter *);
 
 struct blk_integrity_profile {
 	integrity_processing_fn		*generate_fn;

commit 2a842acab109f40f0d7d10b38e9ca88390628996
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:04 2017 +0200

    block: introduce new block status code type
    
    Currently we use nornal Linux errno values in the block layer, and while
    we accept any error a few have overloaded magic meanings.  This patch
    instead introduces a new  blk_status_t value that holds block layer specific
    status codes and explicitly explains their meaning.  Helpers to convert from
    and to the previous special meanings are provided for now, but I suspect
    we want to get rid of them in the long run - those drivers that have a
    errno input (e.g. networking) usually get errnos that don't know about
    the special block layer overloads, and similarly returning them to userspace
    will usually return somethings that strictly speaking isn't correct
    for file system operations, but that's left as an exercise for later.
    
    For now the set of errors is a very limited set that closely corresponds
    to the previous overloaded errno values, but there is some low hanging
    fruite to improve it.
    
    blk_status_t (ab)uses the sparse __bitwise annotations to allow for sparse
    typechecking, so that we can easily catch places passing the wrong values.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 019f18c65098..2a8871638453 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -55,7 +55,7 @@ struct blk_stat_callback;
  */
 #define BLKCG_MAX_POLS		3
 
-typedef void (rq_end_io_fn)(struct request *, int);
+typedef void (rq_end_io_fn)(struct request *, blk_status_t);
 
 #define BLK_RL_SYNCFULL		(1U << 0)
 #define BLK_RL_ASYNCFULL	(1U << 1)
@@ -940,7 +940,7 @@ extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 			     int (*bio_ctr)(struct bio *, struct bio *, void *),
 			     void *data);
 extern void blk_rq_unprep_clone(struct request *rq);
-extern int blk_insert_cloned_request(struct request_queue *q,
+extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern int blk_rq_append_bio(struct request *rq, struct bio *bio);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
@@ -980,6 +980,9 @@ extern void blk_execute_rq(struct request_queue *, struct gendisk *,
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 
+int blk_status_to_errno(blk_status_t status);
+blk_status_t errno_to_blk_status(int errno);
+
 bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
@@ -1112,16 +1115,16 @@ extern struct request *blk_fetch_request(struct request_queue *q);
  * blk_end_request() for parts of the original function.
  * This prevents code duplication in drivers.
  */
-extern bool blk_update_request(struct request *rq, int error,
+extern bool blk_update_request(struct request *rq, blk_status_t error,
 			       unsigned int nr_bytes);
-extern void blk_finish_request(struct request *rq, int error);
-extern bool blk_end_request(struct request *rq, int error,
+extern void blk_finish_request(struct request *rq, blk_status_t error);
+extern bool blk_end_request(struct request *rq, blk_status_t error,
 			    unsigned int nr_bytes);
-extern void blk_end_request_all(struct request *rq, int error);
-extern bool __blk_end_request(struct request *rq, int error,
+extern void blk_end_request_all(struct request *rq, blk_status_t error);
+extern bool __blk_end_request(struct request *rq, blk_status_t error,
 			      unsigned int nr_bytes);
-extern void __blk_end_request_all(struct request *rq, int error);
-extern bool __blk_end_request_cur(struct request *rq, int error);
+extern void __blk_end_request_all(struct request *rq, blk_status_t error);
+extern bool __blk_end_request_cur(struct request *rq, blk_status_t error);
 
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);

commit 9efc160f4bbd69b17b48edec53067537d04e62b7
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed May 31 14:43:46 2017 -0700

    block: Introduce queue flag QUEUE_FLAG_SCSI_PASSTHROUGH
    
    From the context where a SCSI command is submitted it is not always
    possible to figure out whether or not the queue the command is
    submitted to has struct scsi_request as the first member of its
    private data. Hence introduce the flag QUEUE_FLAG_SCSI_PASSTHROUGH.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Don Brace <don.brace@microsemi.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ab92c4ea138b..019f18c65098 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -618,6 +618,7 @@ struct request_queue {
 #define QUEUE_FLAG_STATS       27	/* track rq completion times */
 #define QUEUE_FLAG_POLL_STATS  28	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED  29	/* queue has been registered to a disk */
+#define QUEUE_FLAG_SCSI_PASSTHROUGH 30	/* queue supports SCSI commands */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -708,6 +709,8 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_secure_erase(q) \
 	(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))
 #define blk_queue_dax(q)	test_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)
+#define blk_queue_scsi_passthrough(q)	\
+	test_bit(QUEUE_FLAG_SCSI_PASSTHROUGH, &(q)->queue_flags)
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \

commit 0fcc3ab23d7395f58e8ab0834e7913e2e4314a83
Merge: deac8429d62c e84b83b9ee21
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 15:43:10 2017 -0700

    Merge branch 'libnvdimm-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm fixes from Dan Williams:
     "Incremental fixes and a small feature addition on top of the main
      libnvdimm 4.12 pull request:
    
       - Geert noticed that tinyconfig was bloated by BLOCK selecting DAX.
         The size regression is fixed by moving all dax helpers into the
         dax-core and only specifying "select DAX" for FS_DAX and
         dax-capable drivers. He also asked for clarification of the
         NR_DEV_DAX config option which, on closer look, does not need to be
         a config option at all. Mike also throws in a DEV_DAX_PMEM fixup
         for good measure.
    
       - Ben's attention to detail on -stable patch submissions caught a
         case where the recent fixes to arch_copy_from_iter_pmem() missed a
         condition where we strand dirty data in the cache. This is tagged
         for -stable and will also be included in the rework of the pmem api
         to a proposed {memcpy,copy_user}_flushcache() interface for 4.13.
    
       - Vishal adds a feature that missed the initial pull due to pending
         review feedback. It allows the kernel to clear media errors when
         initializing a BTT (atomic sector update driver) instance on a pmem
         namespace.
    
       - Ross noticed that the dax_device + dax_operations conversion broke
         __dax_zero_page_range(). The nvdimm unit tests fail to check this
         path, but xfstests immediately trips over it. No excuse for missing
         this before submitting the 4.12 pull request.
    
      These all pass the nvdimm unit tests and an xfstests spot check. The
      set has received a build success notification from the kbuild robot"
    
    * 'libnvdimm-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm:
      filesystem-dax: fix broken __dax_zero_page_range() conversion
      libnvdimm, btt: ensure that initializing metadata clears poison
      libnvdimm: add an atomic vs process context flag to rw_bytes
      x86, pmem: Fix cache flushing for iovec write < 8 bytes
      device-dax: kill NR_DEV_DAX
      block, dax: move "select DAX" from BLOCK to FS_DAX
      device-dax: Tell kbuild DEV_DAX_PMEM depends on DEV_DAX

commit ef51042472f55b325fd7f2b26a2e29fd89757234
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon May 8 10:55:27 2017 -0700

    block, dax: move "select DAX" from BLOCK to FS_DAX
    
    For configurations that do not enable DAX filesystems or drivers, do not
    require the DAX core to be built.
    
    Given that the 'direct_access' method has been removed from
    'block_device_operations', we can also go ahead and remove the
    block-related dax helper functions from fs/block_dev.c to
    drivers/dax/super.c. This keeps dax details out of the block layer and
    lets the DAX core be built as a module in the FS_DAX=n case.
    
    Filesystems need to include dax.h to call bdev_dax_supported().
    
    Cc: linux-xfs@vger.kernel.org
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: "Darrick J. Wong" <darrick.wong@oracle.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Reviewed-by: Jan Kara <jack@suse.com>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 848f87eb1905..e4d9899755a7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1940,8 +1940,6 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
-extern int bdev_dax_supported(struct super_block *, int);
-int bdev_dax_pgoff(struct block_device *, sector_t, size_t, pgoff_t *pgoff);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 044f1daaaaf7c86bc4fcf433848b7baae236946b
Merge: d557d1b58b35 daaadb3e9453
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 6 11:25:08 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Pull block fixes and updates from Jens Axboe:
     "Some fixes and followup features/changes that should go in, in this
      merge window. This contains:
    
       - Two fixes for lightnvm from Javier, fixing problems in the new code
         merge previously in this merge window.
    
       - A fix from Jan for the backing device changes, fixing an issue in
         NFS that causes a failure to mount on certain setups.
    
       - A change from Christoph, cleaning up the blk-mq init and exit
         request paths.
    
       - Remove elevator_change(), which is now unused. From Bart.
    
       - A fix for queue operation invocation on a dead queue, from Bart.
    
       - A series fixing up mtip32xx for blk-mq scheduling, removing a
         bandaid we previously had in place for this. From me.
    
       - A regression fix for this series, fixing a case where we wait on
         workqueue flushing from an invalid (non-blocking) context. From me.
    
       - A fix/optimization from Ming, ensuring that we don't both quiesce
         and freeze a queue at the same time.
    
       - A fix from Peter on lock ordering for CPU hotplug. Not a real
         problem right now, but will be once the CPU hotplug rework goes in.
    
       - A series from Omar, cleaning up out blk-mq debugfs support, and
         adding support for exporting info from schedulers in debugfs as
         well. This is really useful in debugging stalls or livelocks. From
         Omar"
    
    * 'for-linus' of git://git.kernel.dk/linux-block: (28 commits)
      mq-deadline: add debugfs attributes
      kyber: add debugfs attributes
      blk-mq-debugfs: allow schedulers to register debugfs attributes
      blk-mq: untangle debugfs and sysfs
      blk-mq: move debugfs declarations to a separate header file
      blk-mq: Do not invoke queue operations on a dead queue
      blk-mq-debugfs: get rid of a bunch of boilerplate
      blk-mq-debugfs: rename hw queue directories from <n> to hctx<n>
      blk-mq-debugfs: don't open code strstrip()
      blk-mq-debugfs: error on long write to queue "state" file
      blk-mq-debugfs: clean up flag definitions
      blk-mq-debugfs: separate flags with |
      nfs: Fix bdi handling for cloned superblocks
      block/mq: Cure cpu hotplug lock inversion
      lightnvm: fix bad back free on error path
      lightnvm: create cmd before allocating request
      blk-mq: don't use sync workqueue flushing from drivers
      mtip32xx: convert internal commands to regular block infrastructure
      mtip32xx: cleanup internal tag assumptions
      block: don't call blk_mq_quiesce_queue() after queue is frozen
      ...

commit 53ef7d0e208fa38c3f63d287e0c3ab174f1e1235
Merge: c6a677c6f37b 736163671bcb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 18:49:20 2017 -0700

    Merge tag 'libnvdimm-for-4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "The bulk of this has been in multiple -next releases. There were a few
      late breaking fixes and small features that got added in the last
      couple days, but the whole set has received a build success
      notification from the kbuild robot.
    
      Change summary:
    
       - Region media error reporting: A libnvdimm region device is the
         parent to one or more namespaces. To date, media errors have been
         reported via the "badblocks" attribute attached to pmem block
         devices for namespaces in "raw" or "memory" mode. Given that
         namespaces can be in "device-dax" or "btt-sector" mode this new
         interface reports media errors generically, i.e. independent of
         namespace modes or state.
    
         This subsequently allows userspace tooling to craft "ACPI 6.1
         Section 9.20.7.6 Function Index 4 - Clear Uncorrectable Error"
         requests and submit them via the ioctl path for NVDIMM root bus
         devices.
    
       - Introduce 'struct dax_device' and 'struct dax_operations': Prompted
         by a request from Linus and feedback from Christoph this allows for
         dax capable drivers to publish their own custom dax operations.
         This fixes the broken assumption that all dax operations are
         related to a persistent memory device, and makes it easier for
         other architectures and platforms to add customized persistent
         memory support.
    
       - 'libnvdimm' core updates: A new "deep_flush" sysfs attribute is
         available for storage appliance applications to manually trigger
         memory controllers to drain write-pending buffers that would
         otherwise be flushed automatically by the platform ADR
         (asynchronous-DRAM-refresh) mechanism at a power loss event.
         Support for "locked" DIMMs is included to prevent namespaces from
         surfacing when the namespace label data area is locked. Finally,
         fixes for various reported deadlocks and crashes, also tagged for
         -stable.
    
       - ACPI / nfit driver updates: General updates of the nfit driver to
         add DSM command overrides, ACPI 6.1 health state flags support, DSM
         payload debug available by default, and various fixes.
    
      Acknowledgements that came after the branch was pushed:
    
       - commmit 565851c972b5 "device-dax: fix sysfs attribute deadlock":
         Tested-by: Yi Zhang <yizhan@redhat.com>
    
       - commit 23f498448362 "libnvdimm: rework region badblocks clearing"
         Tested-by: Toshi Kani <toshi.kani@hpe.com>"
    
    * tag 'libnvdimm-for-4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (52 commits)
      libnvdimm, pfn: fix 'npfns' vs section alignment
      libnvdimm: handle locked label storage areas
      libnvdimm: convert NDD_ flags to use bitops, introduce NDD_LOCKED
      brd: fix uninitialized use of brd->dax_dev
      block, dax: use correct format string in bdev_dax_supported
      device-dax: fix sysfs attribute deadlock
      libnvdimm: restore "libnvdimm: band aid btt vs clear poison locking"
      libnvdimm: fix nvdimm_bus_lock() vs device_lock() ordering
      libnvdimm: rework region badblocks clearing
      acpi, nfit: kill ACPI_NFIT_DEBUG
      libnvdimm: fix clear length of nvdimm_forget_poison()
      libnvdimm, pmem: fix a NULL pointer BUG in nd_pmem_notify
      libnvdimm, region: sysfs trigger for nvdimm_flush()
      libnvdimm: fix phys_addr for nvdimm_clear_poison
      x86, dax, pmem: remove indirection around memcpy_from_pmem()
      block: remove block_device_operations ->direct_access()
      block, dax: convert bdev_dax_supported() to dax_direct_access()
      filesystem-dax: convert to dax_direct_access()
      Revert "block: use DAX for partition table reads"
      ext2, ext4, xfs: retrieve dax_device for iomap operations
      ...

commit d332ce091813d11a46144354baa72b755833392f
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu May 4 08:24:40 2017 -0600

    blk-mq-debugfs: allow schedulers to register debugfs attributes
    
    This provides the infrastructure for schedulers to expose their internal
    state through debugfs. We add a list of queue attributes and a list of
    hctx attributes to struct elevator_type and wire them up when switching
    schedulers.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    
    Add missing seq_file.h header in blk-mq-debugfs.h
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b49a79a29e58..80ae958717a1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -579,6 +579,7 @@ struct request_queue {
 
 #ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;
+	struct dentry		*sched_debugfs_dir;
 #endif
 
 	bool			mq_sysfs_init_done;

commit 9c1051aacde828073dbbab5e8e59c0fc802efa9a
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu May 4 08:17:21 2017 -0600

    blk-mq: untangle debugfs and sysfs
    
    Originally, I tied debugfs registration/unregistration together with
    sysfs. There's no reason to do this, and it's getting in the way of
    letting schedulers define their own debugfs attributes. Instead, tie the
    debugfs registration to the lifetime of the structures themselves.
    
    The saner lifetimes mean we can also get rid of the extra mq directory
    and move everything one level up. I.e., nvme0n1/mq/hctx0/tags is now
    just nvme0n1/hctx0/tags.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 83d28623645f..b49a79a29e58 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -579,7 +579,6 @@ struct request_queue {
 
 #ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;
-	struct dentry		*mq_debugfs_dir;
 #endif
 
 	bool			mq_sysfs_init_done;

commit 694752922b12bd318aa80191bd9d8c3dcfb39055
Merge: a351e9b9fc24 9438b3e080be
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 10:39:57 2017 -0700

    Merge branch 'for-4.12/block' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
    
     - Add BFQ IO scheduler under the new blk-mq scheduling framework. BFQ
       was initially a fork of CFQ, but subsequently changed to implement
       fairness based on B-WF2Q+, a modified variant of WF2Q. BFQ is meant
       to be used on desktop type single drives, providing good fairness.
       From Paolo.
    
     - Add Kyber IO scheduler. This is a full multiqueue aware scheduler,
       using a scalable token based algorithm that throttles IO based on
       live completion IO stats, similary to blk-wbt. From Omar.
    
     - A series from Jan, moving users to separately allocated backing
       devices. This continues the work of separating backing device life
       times, solving various problems with hot removal.
    
     - A series of updates for lightnvm, mostly from Javier. Includes a
       'pblk' target that exposes an open channel SSD as a physical block
       device.
    
     - A series of fixes and improvements for nbd from Josef.
    
     - A series from Omar, removing queue sharing between devices on mostly
       legacy drivers. This helps us clean up other bits, if we know that a
       queue only has a single device backing. This has been overdue for
       more than a decade.
    
     - Fixes for the blk-stats, and improvements to unify the stats and user
       windows. This both improves blk-wbt, and enables other users to
       register a need to receive IO stats for a device. From Omar.
    
     - blk-throttle improvements from Shaohua. This provides a scalable
       framework for implementing scalable priotization - particularly for
       blk-mq, but applicable to any type of block device. The interface is
       marked experimental for now.
    
     - Bucketized IO stats for IO polling from Stephen Bates. This improves
       efficiency of polled workloads in the presence of mixed block size
       IO.
    
     - A few fixes for opal, from Scott.
    
     - A few pulls for NVMe, including a lot of fixes for NVMe-over-fabrics.
       From a variety of folks, mostly Sagi and James Smart.
    
     - A series from Bart, improving our exposed info and capabilities from
       the blk-mq debugfs support.
    
     - A series from Christoph, cleaning up how handle WRITE_ZEROES.
    
     - A series from Christoph, cleaning up the block layer handling of how
       we track errors in a request. On top of being a nice cleanup, it also
       shrinks the size of struct request a bit.
    
     - Removal of mg_disk and hd (sorry Linus) by Christoph. The former was
       never used by platforms, and the latter has outlived it's usefulness.
    
     - Various little bug fixes and cleanups from a wide variety of folks.
    
    * 'for-4.12/block' of git://git.kernel.dk/linux-block: (329 commits)
      block: hide badblocks attribute by default
      blk-mq: unify hctx delay_work and run_work
      block: add kblock_mod_delayed_work_on()
      blk-mq: unify hctx delayed_run_work and run_work
      nbd: fix use after free on module unload
      MAINTAINERS: bfq: Add Paolo as maintainer for the BFQ I/O scheduler
      blk-mq-sched: alloate reserved tags out of normal pool
      mtip32xx: use runtime tag to initialize command header
      scsi: Implement blk_mq_ops.show_rq()
      blk-mq: Add blk_mq_ops.show_rq()
      blk-mq: Show operation, cmd_flags and rq_flags names
      blk-mq: Make blk_flags_show() callers append a newline character
      blk-mq: Move the "state" debugfs attribute one level down
      blk-mq: Unregister debugfs attributes earlier
      blk-mq: Only unregister hctxs for which registration succeeded
      blk-mq-debugfs: Rename functions for registering and unregistering the mq directory
      blk-mq: Let blk_mq_debugfs_register() look up the queue name
      blk-mq: Register <dev>/queue/mq after having registered <dev>/queue
      ide-pm: always pass 0 error to ide_complete_rq in ide_do_devset
      ide-pm: always pass 0 error to __blk_end_request_all
      ..

commit 818cd1cbaa7b00bbc35452a76bebc681a65f1912
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Apr 10 09:54:55 2017 -0600

    block: add kblock_mod_delayed_work_on()
    
    This modifies (or adds, if not currently pending) an existing
    delayed work item.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6c247861cb66..d098c66b3ab0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1685,6 +1685,7 @@ int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
 int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
+int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*

commit d4b29fd78ea6fc2be219be3af1a992149b4ff0f6
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 27 17:22:03 2017 -0800

    block: remove block_device_operations ->direct_access()
    
    Now that all the producers and consumers of dax interfaces have been
    converted to using dax_operations on a dax_device, remove the block
    device direct_access enabling.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 612c497d1461..848f87eb1905 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1916,28 +1916,12 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
-/**
- * struct blk_dax_ctl - control and output parameters for ->direct_access
- * @sector: (input) offset relative to a block_device
- * @addr: (output) kernel virtual address for @sector populated by driver
- * @pfn: (output) page frame number for @addr populated by driver
- * @size: (input) number of bytes requested
- */
-struct blk_dax_ctl {
-	sector_t sector;
-	void *addr;
-	long size;
-	pfn_t pfn;
-};
-
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
 	int (*rw_page)(struct block_device *, sector_t, struct page *, bool);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
-	long (*direct_access)(struct block_device *, sector_t, void **, pfn_t *,
-			long);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */
@@ -1956,7 +1940,6 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
-extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
 extern int bdev_dax_supported(struct super_block *, int);
 int bdev_dax_pgoff(struct block_device *, sector_t, size_t, pgoff_t *pgoff);
 #else /* CONFIG_BLOCK */

commit 0206319fdfee7c36b97aa6c0561bab206132f813
Author: Stephen Bates <sbates@raithlin.com>
Date:   Thu Apr 20 16:59:11 2017 -0600

    blk-mq: Fix poll_stat for new size-based bucketing.
    
    Fixes an issue where the size of the poll_stat array in request_queue
    does not match the size expected by the new size based bucketing for
    IO completion polling.
    
    Fixes: 720b8ccc4500 ("blk-mq: Add a polling specific stats function")
    Signed-off-by: Stephen Bates <sbates@raithlin.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6c4ab0d4a160..6c247861cb66 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -46,6 +46,9 @@ struct blk_stat_callback;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
+/* Must be consisitent with blk_mq_poll_stats_bkt() */
+#define BLK_MQ_POLL_STATS_BKTS 16
+
 /*
  * Maximum number of blkcg policies allowed to be registered concurrently.
  * Defined here to simplify include dependency.
@@ -517,7 +520,7 @@ struct request_queue {
 	int			poll_nsec;
 
 	struct blk_stat_callback	*poll_cb;
-	struct blk_rq_stat	poll_stat[2];
+	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
 	struct work_struct	timeout_work;

commit b0686260fecaa924d8eff2ace94bee70506bc308
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jan 26 20:37:35 2017 -0800

    dax: introduce dax_direct_access()
    
    Replace bdev_direct_access() with dax_direct_access() that uses
    dax_device and dax_operations instead of a block_device and
    block_device_operations for dax. Once all consumers of the old api have
    been converted bdev_direct_access() will be deleted.
    
    Given that block device partitioning decisions can cause dax page
    alignment constraints to be violated this also introduces the
    bdev_dax_pgoff() helper. It handles calculating a logical pgoff relative
    to the dax_device and also checks for page alignment.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f72708399b83..612c497d1461 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1958,6 +1958,7 @@ extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
 extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
 extern int bdev_dax_supported(struct super_block *, int);
+int bdev_dax_pgoff(struct block_device *, sector_t, size_t, pgoff_t *pgoff);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit d8f07aee3f2fd959878bf614d4e984900018eb9e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jan 26 23:30:05 2017 -0800

    block: kill bdev_dax_capable()
    
    This is leftover dead code that has since been replaced by
    bdev_dax_supported().
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5a7da607ca04..f72708399b83 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1958,7 +1958,6 @@ extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
 extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
 extern int bdev_dax_supported(struct super_block *, int);
-extern bool bdev_dax_capable(struct block_device *);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit caf7df12272118e0274c8353bcfeaf60c7743a47
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:03:16 2017 +0200

    block: remove the errors field from struct request
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a3dcee624de3..6c4ab0d4a160 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -220,8 +220,6 @@ struct request {
 
 	void *special;		/* opaque pointer available for LLD use */
 
-	int errors;
-
 	unsigned int extra_len;	/* length of alignment and padding */
 
 	unsigned long deadline;

commit e26738e037f34aedfe05e412f442833f44f4a6e5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:03:11 2017 +0200

    block: add a error_count field to struct request
    
    This is for the legacy floppy and ataflop drivers that currently abuse
    ->errors for this purpose.  It's stashed away in a union to not grow
    the struct size, the other fields are either used by modern drivers
    for different purposes or the I/O scheduler before queing the I/O
    to drivers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e2064ed3c703..a3dcee624de3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -175,6 +175,7 @@ struct request {
 		struct rb_node rb_node;	/* sort/lookup */
 		struct bio_vec special_vec;
 		void *completion_data;
+		int error_count; /* for legacy drivers, don't use */
 	};
 
 	/*

commit b7819b9259185dcdcc81eb32182a4dc13d695738
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 20 16:02:55 2017 +0200

    block: remove the blk_execute_rq return value
    
    The function only returns -EIO if rq->errors is non-zero, which is not
    very useful and lets a large number of callers ignore the return value.
    
    Just let the callers figure out their error themselves.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 51c9e391798e..e2064ed3c703 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -970,7 +970,7 @@ extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, uns
 extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
 			       struct rq_map_data *, const struct iov_iter *,
 			       gfp_t);
-extern int blk_execute_rq(struct request_queue *, struct gendisk *,
+extern void blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);

commit 0be0dee64eacd950f8e4b6c45adb5a92392eaaaf
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 19 14:01:27 2017 -0700

    block: Inline blk_rq_set_prio()
    
    Since only a single caller remains, inline blk_rq_set_prio(). Initialize
    req->ioprio even if no I/O priority has been set in the bio nor in the
    I/O context.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Adam Manzanares <adam.manzanares@wdc.com>
    Tested-by: Adam Manzanares <adam.manzanares@wdc.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Matias Bjørling <m@bjorling.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3470375952a1..51c9e391798e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1087,20 +1087,6 @@ static inline unsigned int blk_rq_count_bios(struct request *rq)
 	return nr_bios;
 }
 
-/*
- * blk_rq_set_prio - associate a request with prio from ioc
- * @rq: request of interest
- * @ioc: target iocontext
- *
- * Assocate request prio with ioc prio so request based drivers
- * can leverage priority information.
- */
-static inline void blk_rq_set_prio(struct request *rq, struct io_context *ioc)
-{
-	if (ioc)
-		rq->ioprio = ioc->ioprio;
-}
-
 /*
  * Request issue related functions.
  */

commit da8d7f079b868ceab830309f80efc69d350576f3
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Apr 19 14:01:24 2017 -0700

    block: Export blk_init_request_from_bio()
    
    Export this function such that it becomes available to block
    drivers.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Matias Bjørling <m@bjorling.me>
    Cc: Adam Manzanares <adam.manzanares@wdc.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5b52b3d7818c..3470375952a1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -924,6 +924,7 @@ extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern blk_qc_t generic_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
+extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);

commit fa1a15c08e23cb89c5837915b1989909bce47456
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 12 12:13:58 2017 +0200

    block: remove blk_end_request_cur
    
    This function is not used anywhere in the kernel.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cca704c80b01..5b52b3d7818c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1126,7 +1126,6 @@ extern void blk_finish_request(struct request *rq, int error);
 extern bool blk_end_request(struct request *rq, int error,
 			    unsigned int nr_bytes);
 extern void blk_end_request_all(struct request *rq, int error);
-extern bool blk_end_request_cur(struct request *rq, int error);
 extern bool __blk_end_request(struct request *rq, int error,
 			      unsigned int nr_bytes);
 extern void __blk_end_request_all(struct request *rq, int error);

commit 314fe91b4a99949bb720501ba74d2228093bbf47
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 12 12:13:57 2017 +0200

    block: remove blk_end_request_err and __blk_end_request_err
    
    Both functions are entirely unused.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fe9c512cc6fa..cca704c80b01 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1127,12 +1127,10 @@ extern bool blk_end_request(struct request *rq, int error,
 			    unsigned int nr_bytes);
 extern void blk_end_request_all(struct request *rq, int error);
 extern bool blk_end_request_cur(struct request *rq, int error);
-extern bool blk_end_request_err(struct request *rq, int error);
 extern bool __blk_end_request(struct request *rq, int error,
 			      unsigned int nr_bytes);
 extern void __blk_end_request_all(struct request *rq, int error);
 extern bool __blk_end_request_cur(struct request *rq, int error);
-extern bool __blk_end_request_err(struct request *rq, int error);
 
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);

commit e21b7a0b988772e82e7147e1c659a5afe2ae003c
Author: Arianna Avanzini <avanzini.arianna@gmail.com>
Date:   Wed Apr 12 18:23:08 2017 +0200

    block, bfq: add full hierarchical scheduling and cgroups support
    
    Add complete support for full hierarchical scheduling, with a cgroups
    interface. Full hierarchical scheduling is implemented through the
    'entity' abstraction: both bfq_queues, i.e., the internal BFQ queues
    associated with processes, and groups are represented in general by
    entities. Given the bfq_queues associated with the processes belonging
    to a given group, the entities representing these queues are sons of
    the entity representing the group. At higher levels, if a group, say
    G, contains other groups, then the entity representing G is the parent
    entity of the entities representing the groups in G.
    
    Hierarchical scheduling is performed as follows: if the timestamps of
    a leaf entity (i.e., of a bfq_queue) change, and such a change lets
    the entity become the next-to-serve entity for its parent entity, then
    the timestamps of the parent entity are recomputed as a function of
    the budget of its new next-to-serve leaf entity. If the parent entity
    belongs, in its turn, to a group, and its new timestamps let it become
    the next-to-serve for its parent entity, then the timestamps of the
    latter parent entity are recomputed as well, and so on. When a new
    bfq_queue must be set in service, the reverse path is followed: the
    next-to-serve highest-level entity is chosen, then its next-to-serve
    child entity, and so on, until the next-to-serve leaf entity is
    reached, and the bfq_queue that this entity represents is set in
    service.
    
    Writeback is accounted for on a per-group basis, i.e., for each group,
    the async I/O requests of the processes of the group are enqueued in a
    distinct bfq_queue, and the entity associated with this queue is a
    child of the entity associated with the group.
    
    Weights can be assigned explicitly to groups and processes through the
    cgroups interface, differently from what happens, for single
    processes, if the cgroups interface is not used (as explained in the
    description of the previous patch). In particular, since each node has
    a full scheduler, each group can be assigned its own weight.
    
    Signed-off-by: Fabio Checconi <fchecconi@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ec993573e0a8..fe9c512cc6fa 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -50,7 +50,7 @@ struct blk_stat_callback;
  * Maximum number of blkcg policies allowed to be registered concurrently.
  * Defined here to simplify include dependency.
  */
-#define BLKCG_MAX_POLS		2
+#define BLKCG_MAX_POLS		3
 
 typedef void (rq_end_io_fn)(struct request *, int);
 

commit 5a8d75a1b8c99bdc926ba69b7b7dbe4fae81a5af
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Apr 14 13:58:29 2017 -0600

    block: fix bio_will_gap() for first bvec with offset
    
    Commit 729204ef49ec("block: relax check on sg gap") allows us to merge
    bios, if both are physically contiguous.  This change can merge a huge
    number of small bios, through mkfs for example, mkfs.ntfs running time
    can be decreased to ~1/10.
    
    But if one rq starts with a non-aligned buffer (the 1st bvec's bv_offset
    is non-zero) and if we allow the merge, it is quite difficult to respect
    sg gap limit, especially the max segment size, or we risk having an
    unaligned virtual boundary.  This patch tries to avoid the issue by
    disallowing a merge, if the req starts with an unaligned buffer.
    
    Also add comments to explain why the merged segment can't end in
    unaligned virt boundary.
    
    Fixes: 729204ef49ec ("block: relax check on sg gap")
    Tested-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    
    Rewrote parts of the commit message and comments.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7548f332121a..01a696b0a4d3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1672,12 +1672,36 @@ static inline bool bios_segs_mergeable(struct request_queue *q,
 	return true;
 }
 
-static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
-			 struct bio *next)
+static inline bool bio_will_gap(struct request_queue *q,
+				struct request *prev_rq,
+				struct bio *prev,
+				struct bio *next)
 {
 	if (bio_has_data(prev) && queue_virt_boundary(q)) {
 		struct bio_vec pb, nb;
 
+		/*
+		 * don't merge if the 1st bio starts with non-zero
+		 * offset, otherwise it is quite difficult to respect
+		 * sg gap limit. We work hard to merge a huge number of small
+		 * single bios in case of mkfs.
+		 */
+		if (prev_rq)
+			bio_get_first_bvec(prev_rq->bio, &pb);
+		else
+			bio_get_first_bvec(prev, &pb);
+		if (pb.bv_offset)
+			return true;
+
+		/*
+		 * We don't need to worry about the situation that the
+		 * merged segment ends in unaligned virt boundary:
+		 *
+		 * - if 'pb' ends aligned, the merged segment ends aligned
+		 * - if 'pb' ends unaligned, the next bio must include
+		 *   one single bvec of 'nb', otherwise the 'nb' can't
+		 *   merge with 'pb'
+		 */
 		bio_get_last_bvec(prev, &pb);
 		bio_get_first_bvec(next, &nb);
 
@@ -1690,12 +1714,12 @@ static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
 
 static inline bool req_gap_back_merge(struct request *req, struct bio *bio)
 {
-	return bio_will_gap(req->q, req->biotail, bio);
+	return bio_will_gap(req->q, req, req->biotail, bio);
 }
 
 static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
 {
-	return bio_will_gap(req->q, bio, req->bio);
+	return bio_will_gap(req->q, NULL, bio, req->bio);
 }
 
 int kblockd_schedule_work(struct work_struct *work);

commit 48920ff2a5a940cd07d12cc79e4a2c75f1185aee
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 5 19:21:23 2017 +0200

    block: remove the discard_zeroes_data flag
    
    Now that we use the proper REQ_OP_WRITE_ZEROES operation everywhere we can
    kill this hack.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 21a30f011674..ec993573e0a8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -339,7 +339,6 @@ struct queue_limits {
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
 	unsigned char		cluster;
-	unsigned char		discard_zeroes_data;
 	unsigned char		raid_partial_stripes_expensive;
 	enum blk_zoned_model	zoned;
 };
@@ -1341,7 +1340,6 @@ extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 
 #define BLKDEV_DISCARD_SECURE	(1 << 0)	/* issue a secure erase */
-#define BLKDEV_DISCARD_ZERO	(1 << 1)	/* must reliably zero data */
 
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
@@ -1541,19 +1539,6 @@ static inline int bdev_discard_alignment(struct block_device *bdev)
 	return q->limits.discard_alignment;
 }
 
-static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)
-{
-	if (q->limits.max_discard_sectors && q->limits.discard_zeroes_data == 1)
-		return 1;
-
-	return 0;
-}
-
-static inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)
-{
-	return queue_discard_zeroes_data(bdev_get_queue(bdev));
-}
-
 static inline unsigned int bdev_write_same(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);

commit cb365b9675fda026caba4cb5df83292cb7c0811a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 5 19:21:10 2017 +0200

    block: add a new BLKDEV_ZERO_NOFALLBACK flag
    
    This avoids fallbacks to explicit zeroing in (__)blkdev_issue_zeroout if
    the caller doesn't want them.
    
    Also clean up the convoluted check for the return condition that this
    new flag is added to.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bd60f4401c9d..21a30f011674 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1350,6 +1350,7 @@ extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		struct bio **biop);
 
 #define BLKDEV_ZERO_NOUNMAP	(1 << 0)  /* do not free blocks */
+#define BLKDEV_ZERO_NOFALLBACK	(1 << 1)  /* don't write explicit zeroes */
 
 extern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct bio **biop,

commit ee472d835c264a4cb77f8cf878603e1e40f3559e
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 5 19:21:08 2017 +0200

    block: add a flags argument to (__)blkdev_issue_zeroout
    
    Turn the existing discard flag into a new BLKDEV_ZERO_UNMAP flag with
    similar semantics, but without referring to diѕcard.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d76bebbc632e..bd60f4401c9d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1336,23 +1336,27 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 	return bqt->tag_index[tag];
 }
 
+extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
+extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
+		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 
 #define BLKDEV_DISCARD_SECURE	(1 << 0)	/* issue a secure erase */
 #define BLKDEV_DISCARD_ZERO	(1 << 1)	/* must reliably zero data */
 
-extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, int flags,
 		struct bio **biop);
-extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
-		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
+
+#define BLKDEV_ZERO_NOUNMAP	(1 << 0)  /* do not free blocks */
+
 extern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct bio **biop,
-		bool discard);
+		unsigned flags);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
-		sector_t nr_sects, gfp_t gfp_mask, bool discard);
+		sector_t nr_sects, gfp_t gfp_mask, unsigned flags);
+
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {
@@ -1366,7 +1370,7 @@ static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
 	return blkdev_issue_zeroout(sb->s_bdev,
 				    block << (sb->s_blocksize_bits - 9),
 				    nr_blocks << (sb->s_blocksize_bits - 9),
-				    gfp_mask, true);
+				    gfp_mask, 0);
 }
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);

commit 65f619d2535197d97067eeeef75a40f25b552e69
Merge: fbbaf700e7b1 6d8c6c0f97ad
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Apr 7 12:45:20 2017 -0600

    Merge branch 'for-linus' into for-4.12/block
    
    We've added a considerable amount of fixes for stalls and issues
    with the blk-mq scheduling in the 4.11 series since forking
    off the for-4.12/block branch. We need to do improvements on
    top of that for 4.12, so pull in the previous fixes to make
    our lives easier going forward.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 6d8c6c0f97ad8a3517c42b179c1dc8e77397d0e2
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Apr 7 12:40:09 2017 -0600

    blk-mq: Restart a single queue if tag sets are shared
    
    To improve scalability, if hardware queues are shared, restart
    a single hardware queue in round-robin fashion. Rename
    blk_mq_sched_restart_queues() to reflect the new semantics.
    Remove blk_mq_sched_mark_restart_queue() because this function
    has no callers. Remove flag QUEUE_FLAG_RESTART because this
    patch removes the code that uses this flag.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5a7da607ca04..7548f332121a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -610,7 +610,6 @@ struct request_queue {
 #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         26	/* device supports DAX */
 #define QUEUE_FLAG_STATS       27	/* track rq completion times */
-#define QUEUE_FLAG_RESTART     28	/* queue needs restart at completion */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 1dd5198b2df913aec9b77c14529f9ff1b6d33e30
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 5 12:16:38 2017 -0600

    block: move timeout field in struct request to pack better
    
    After commit 64c7f1d1572c, we went from 1 to 2 holes in my
    test setup. If we move the timeout field a bit, we remove
    both of those holes and shrink struct request by 8 bytes.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ce6f9a6534c9..3cf241b0814d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -215,6 +215,8 @@ struct request {
 
 	unsigned short ioprio;
 
+	unsigned int timeout;
+
 	void *special;		/* opaque pointer available for LLD use */
 
 	int errors;
@@ -223,7 +225,6 @@ struct request {
 
 	unsigned long deadline;
 	struct list_head timeout_list;
-	unsigned int timeout;
 
 	/*
 	 * completion callback.

commit 64c7f1d1572cacadfc0a4ca5a937aeffa486de58
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 5 19:18:12 2017 +0200

    block, scsi: move the retries field to struct scsi_request
    
    Instead of bloating the generic struct request with it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a2dc6b390d48..ce6f9a6534c9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -224,7 +224,6 @@ struct request {
 	unsigned long deadline;
 	struct list_head timeout_list;
 	unsigned int timeout;
-	int retries;
 
 	/*
 	 * completion callback.

commit 334335d2f7a077a5ff561d86b0ad43bedd83ca05
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Mar 28 16:12:15 2017 -0700

    block: warn if sharing request queue across gendisks
    
    Now that the remaining drivers have been converted to one request queue
    per gendisk, let's warn if a request queue gets registered more than
    once. This will catch future drivers which might do it inadvertently or
    any old drivers that I may have missed.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1a7dc42a8918..a2dc6b390d48 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -617,6 +617,7 @@ struct request_queue {
 #define QUEUE_FLAG_STATS       27	/* track rq completion times */
 #define QUEUE_FLAG_RESTART     28	/* queue needs restart at completion */
 #define QUEUE_FLAG_POLL_STATS  29	/* collecting stats for hybrid polling */
+#define QUEUE_FLAG_REGISTERED  30	/* queue has been registered to a disk */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 34dbad5d26e2f4b88e60f0e9ad03f99480802812
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Mar 21 08:56:08 2017 -0700

    blk-stat: convert to callback-based statistics reporting
    
    Currently, statistics are gathered in ~0.13s windows, and users grab the
    statistics whenever they need them. This is not ideal for both in-tree
    users:
    
    1. Writeback throttling wants its own dynamically sized window of
       statistics. Since the blk-stats statistics are reset after every
       window and the wbt windows don't line up with the blk-stats windows,
       wbt doesn't see every I/O.
    2. Polling currently grabs the statistics on every I/O. Again, depending
       on how the window lines up, we may miss some I/Os. It's also
       unnecessary overhead to get the statistics on every I/O; the hybrid
       polling heuristic would be just as happy with the statistics from the
       previous full window.
    
    This reworks the blk-stats infrastructure to be callback-based: users
    register a callback that they want called at a given time with all of
    the statistics from the window during which the callback was active.
    Users can dynamically bucketize the statistics. wbt and polling both
    currently use read vs. write, but polling can be extended to further
    subdivide based on request size.
    
    The callbacks are kept on an RCU list, and each callback has percpu
    stats buffers. There will only be a few users, so the overhead on the
    I/O completion side is low. The stats flushing is also simplified
    considerably: since the timer function is responsible for clearing the
    statistics, we don't have to worry about stale statistics.
    
    wbt is a trivial conversion. After the conversion, the windowing problem
    mentioned above is fixed.
    
    For polling, we register an extra callback that caches the previous
    window's statistics in the struct request_queue for the hybrid polling
    heuristic to use.
    
    Since we no longer have a single stats buffer for the request queue,
    this also removes the sysfs and debugfs stats entries. To replace those,
    we add a debugfs entry for the poll statistics.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5a7da607ca04..1a7dc42a8918 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -40,6 +40,8 @@ struct blkcg_gq;
 struct blk_flush_queue;
 struct pr_ops;
 struct rq_wb;
+struct blk_queue_stats;
+struct blk_stat_callback;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -388,6 +390,7 @@ struct request_queue {
 	int			nr_rqs[2];	/* # allocated [a]sync rqs */
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
+	struct blk_queue_stats	*stats;
 	struct rq_wb		*rq_wb;
 
 	/*
@@ -505,8 +508,6 @@ struct request_queue {
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
 
-	struct blk_rq_stat	rq_stats[2];
-
 	/*
 	 * Number of active block driver functions for which blk_drain_queue()
 	 * must wait. Must be incremented around functions that unlock the
@@ -516,6 +517,10 @@ struct request_queue {
 
 	unsigned int		rq_timeout;
 	int			poll_nsec;
+
+	struct blk_stat_callback	*poll_cb;
+	struct blk_rq_stat	poll_stat[2];
+
 	struct timer_list	timeout;
 	struct work_struct	timeout_work;
 	struct list_head	timeout_list;
@@ -611,6 +616,7 @@ struct request_queue {
 #define QUEUE_FLAG_DAX         26	/* device supports DAX */
 #define QUEUE_FLAG_STATS       27	/* track rq completion times */
 #define QUEUE_FLAG_RESTART     28	/* queue needs restart at completion */
+#define QUEUE_FLAG_POLL_STATS  29	/* collecting stats for hybrid polling */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit c01228db4ba965986511a5b28c478bddd7e2726e
Author: Jan Kara <jack@suse.cz>
Date:   Wed Mar 8 17:48:34 2017 +0100

    Revert "scsi, block: fix duplicate bdi name registration crashes"
    
    This reverts commit 0dba1314d4f81115dce711292ec7981d17231064. It causes
    leaking of device numbers for SCSI when SCSI registers multiple gendisks
    for one request_queue in succession. It can be easily reproduced using
    Omar's script [1] on kernel with CONFIG_DEBUG_TEST_DRIVER_REMOVE.
    Furthermore the protection provided by this commit is not needed anymore
    as the problem it was fixing got also fixed by commit 165a5e22fafb
    "block: Move bdi_unregister() to del_gendisk()".
    
    [1]: http://marc.info/?l=linux-block&m=148554717109098&w=2
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 796016e63c1d..5a7da607ca04 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -435,7 +435,6 @@ struct request_queue {
 	struct delayed_work	delay_work;
 
 	struct backing_dev_info	*backing_dev_info;
-	struct disk_devt	*disk_devt;
 
 	/*
 	 * The queue owner gets to use this for whatever they like.

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aecca0e7d9ca..796016e63c1d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -2,6 +2,7 @@
 #define _LINUX_BLKDEV_H
 
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 
 #ifdef CONFIG_BLOCK
 

commit 1e739730c5b9ea80a2f25e9cf6e1025d47e3d8ed
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Feb 8 14:46:49 2017 +0100

    block: optionally merge discontiguous discard bios into a single request
    
    Add a new merge strategy that merges discard bios into a request until the
    maximum number of discard ranges (or the maximum discard size) is reached
    from the plug merging code.  I/O scheduler merging is not wired up yet
    but might also be useful, although not for fast devices like NVMe which
    are the only user for now.
    
    Note that for now we don't support limiting the size of each discard range,
    but if needed that can be added later.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e0bac14347e6..aecca0e7d9ca 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -331,6 +331,7 @@ struct queue_limits {
 	unsigned short		logical_block_size;
 	unsigned short		max_segments;
 	unsigned short		max_integrity_segments;
+	unsigned short		max_discard_segments;
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
@@ -1146,6 +1147,8 @@ extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
+extern void blk_queue_max_discard_segments(struct request_queue *,
+		unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
@@ -1189,6 +1192,15 @@ extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
 
+/*
+ * Number of physical segments as sent to the device.
+ *
+ * Normally this is the number of discontiguous data segments sent by the
+ * submitter.  But for data-less command like discard we might have no
+ * actual data segments submitted, but the driver might have to add it's
+ * own special payload.  In that case we still return 1 here so that this
+ * special payload will be mapped.
+ */
 static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
 {
 	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
@@ -1196,6 +1208,15 @@ static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
 	return rq->nr_phys_segments;
 }
 
+/*
+ * Number of discard segments (or ranges) the driver needs to fill in.
+ * Each discard bio merged into a request is counted as one segment.
+ */
+static inline unsigned short blk_rq_nr_discard_segments(struct request *rq)
+{
+	return max_t(unsigned short, rq->nr_phys_segments, 1);
+}
+
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
@@ -1384,6 +1405,11 @@ static inline unsigned short queue_max_segments(struct request_queue *q)
 	return q->limits.max_segments;
 }
 
+static inline unsigned short queue_max_discard_segments(struct request_queue *q)
+{
+	return q->limits.max_discard_segments;
+}
+
 static inline unsigned int queue_max_segment_size(struct request_queue *q)
 {
 	return q->limits.max_segment_size;

commit 03796c149a99e14506db9de3adba710c26f83ba9
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jan 31 14:53:18 2017 -0800

    block: fix debugfs config conditional in struct request_queue
    
    The debugfs dentries are only used for CONFIG_BLK_DEBUG_FS, so make them
    conditional on that instead of CONFIG_DEBUG_FS.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f84fbe55d3b3..e0bac14347e6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -570,7 +570,7 @@ struct request_queue {
 	struct list_head	tag_set_list;
 	struct bio_set		*bio_split;
 
-#ifdef CONFIG_DEBUG_FS
+#ifdef CONFIG_BLK_DEBUG_FS
 	struct dentry		*debugfs_dir;
 	struct dentry		*mq_debugfs_dir;
 #endif

commit 0dba1314d4f81115dce711292ec7981d17231064
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Feb 1 14:05:23 2017 -0800

    scsi, block: fix duplicate bdi name registration crashes
    
    Warnings of the following form occur because scsi reuses a devt number
    while the block layer still has it referenced as the name of the bdi
    [1]:
    
     WARNING: CPU: 1 PID: 93 at fs/sysfs/dir.c:31 sysfs_warn_dup+0x62/0x80
     sysfs: cannot create duplicate filename '/devices/virtual/bdi/8:192'
     [..]
     Call Trace:
      dump_stack+0x86/0xc3
      __warn+0xcb/0xf0
      warn_slowpath_fmt+0x5f/0x80
      ? kernfs_path_from_node+0x4f/0x60
      sysfs_warn_dup+0x62/0x80
      sysfs_create_dir_ns+0x77/0x90
      kobject_add_internal+0xb2/0x350
      kobject_add+0x75/0xd0
      device_add+0x15a/0x650
      device_create_groups_vargs+0xe0/0xf0
      device_create_vargs+0x1c/0x20
      bdi_register+0x90/0x240
      ? lockdep_init_map+0x57/0x200
      bdi_register_owner+0x36/0x60
      device_add_disk+0x1bb/0x4e0
      ? __pm_runtime_use_autosuspend+0x5c/0x70
      sd_probe_async+0x10d/0x1c0
      async_run_entry_fn+0x39/0x170
    
    This is a brute-force fix to pass the devt release information from
    sd_probe() to the locations where we register the bdi,
    device_add_disk(), and unregister the bdi, blk_cleanup_queue().
    
    Thanks to Omar for the quick reproducer script [2]. This patch survives
    where an unmodified kernel fails in a few seconds.
    
    [1]: https://marc.info/?l=linux-scsi&m=147116857810716&w=4
    [2]: http://marc.info/?l=linux-block&m=148554717109098&w=2
    
    Cc: James Bottomley <James.Bottomley@hansenpartnership.com>
    Cc: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Jan Kara <jack@suse.cz>
    Reported-by: Omar Sandoval <osandov@osandov.com>
    Tested-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b31137e2afd0..f84fbe55d3b3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -433,6 +433,7 @@ struct request_queue {
 	struct delayed_work	delay_work;
 
 	struct backing_dev_info	*backing_dev_info;
+	struct disk_devt	*disk_devt;
 
 	/*
 	 * The queue owner gets to use this for whatever they like.

commit efa7c9f97e3ef624e9a398bf69c15f58eea9f0e8
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 2 15:56:53 2017 +0100

    block: Get rid of blk_get_backing_dev_info()
    
    blk_get_backing_dev_info() is now a simple dereference. Remove that
    function and simplify some code around that.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e77c1039fd0e..b31137e2afd0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1187,7 +1187,6 @@ extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
-extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
 static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
 {

commit d03f6cdc1fc422accb734c7c07a661a0018d8631
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 2 15:56:51 2017 +0100

    block: Dynamically allocate and refcount backing_dev_info
    
    Instead of storing backing_dev_info inside struct request_queue,
    allocate it dynamically, reference count it, and free it when the last
    reference is dropped. Currently only request_queue holds the reference
    but in the following patch we add other users referencing
    backing_dev_info.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a75e42de34ab..e77c1039fd0e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -433,7 +433,6 @@ struct request_queue {
 	struct delayed_work	delay_work;
 
 	struct backing_dev_info	*backing_dev_info;
-	struct backing_dev_info	_backing_dev_info;
 
 	/*
 	 * The queue owner gets to use this for whatever they like.

commit dc3b17cc8bf21307c7e076e7c778d5db756f7871
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 2 15:56:50 2017 +0100

    block: Use pointer to backing_dev_info from request_queue
    
    We will want to have struct backing_dev_info allocated separately from
    struct request_queue. As the first step add pointer to backing_dev_info
    to request_queue and convert all users touching it. No functional
    changes in this patch.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 11f7a8e86a89..a75e42de34ab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -432,7 +432,8 @@ struct request_queue {
 	 */
 	struct delayed_work	delay_work;
 
-	struct backing_dev_info	backing_dev_info;
+	struct backing_dev_info	*backing_dev_info;
+	struct backing_dev_info	_backing_dev_info;
 
 	/*
 	 * The queue owner gets to use this for whatever they like.

commit d486f1f204382557b5fbcb3ddbb5845cd4ba4e2c
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jan 31 12:34:41 2017 -0700

    block: move internal_tag to same cache line as tag
    
    Since we removed cmd_type, we now have a hole in the struct. Move
    the internal_tag member to the same cacheline as tag, since we
    use them at the same time.
    
    This doesn't fix the hole, just moves it elsewhere.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1e947e725528..11f7a8e86a89 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -138,6 +138,9 @@ struct request {
 	int cpu;
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
+
+	int internal_tag;
+
 	unsigned long atomic_flags;
 
 	/* the following two fields are internal, NEVER access directly */
@@ -209,8 +212,6 @@ struct request {
 
 	unsigned short ioprio;
 
-	int internal_tag;
-
 	void *special;		/* opaque pointer available for LLD use */
 
 	int errors;

commit aebf526b53aea164508730427597d45f3e06b376
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 31 16:57:31 2017 +0100

    block: fold cmd_type into the REQ_OP_ space
    
    Instead of keeping two levels of indirection for requests types, fold it
    all into the operations.  The little caveat here is that previously
    cmd_type only applied to struct request, while the request and bio op
    fields were set to plain REQ_OP_READ/WRITE even for passthrough
    operations.
    
    Instead this patch adds new REQ_OP_* for SCSI passthrough and driver
    private requests, althought it has to add two for each so that we
    can communicate the data in/out nature of the request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7121be081517..1e947e725528 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -70,15 +70,6 @@ struct request_list {
 	unsigned int		flags;
 };
 
-/*
- * request command types
- */
-enum rq_cmd_type_bits {
-	REQ_TYPE_FS		= 1,	/* fs request */
-	REQ_TYPE_BLOCK_PC,		/* scsi command */
-	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
-};
-
 /*
  * request flags */
 typedef __u32 __bitwise req_flags_t;
@@ -145,7 +136,6 @@ struct request {
 	struct blk_mq_ctx *mq_ctx;
 
 	int cpu;
-	unsigned cmd_type;
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 	unsigned long atomic_flags;
@@ -242,9 +232,19 @@ struct request {
 	struct request *next_rq;
 };
 
+static inline bool blk_rq_is_scsi(struct request *rq)
+{
+	return req_op(rq) == REQ_OP_SCSI_IN || req_op(rq) == REQ_OP_SCSI_OUT;
+}
+
+static inline bool blk_rq_is_private(struct request *rq)
+{
+	return req_op(rq) == REQ_OP_DRV_IN || req_op(rq) == REQ_OP_DRV_OUT;
+}
+
 static inline bool blk_rq_is_passthrough(struct request *rq)
 {
-	return rq->cmd_type != REQ_TYPE_FS;
+	return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
 }
 
 static inline unsigned short req_get_ioprio(struct request *req)

commit 57292b58ddb58689e8c3b4c6eadbef10d9ca44dd
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 31 16:57:29 2017 +0100

    block: introduce blk_rq_is_passthrough
    
    This can be used to check for fs vs non-fs requests and basically
    removes all knowledge of BLOCK_PC specific from the block layer,
    as well as preparing for removing the cmd_type field in struct request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e4c5f284fe2d..7121be081517 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -242,6 +242,11 @@ struct request {
 	struct request *next_rq;
 };
 
+static inline bool blk_rq_is_passthrough(struct request *rq)
+{
+	return rq->cmd_type != REQ_TYPE_FS;
+}
+
 static inline unsigned short req_get_ioprio(struct request *req)
 {
 	return req->ioprio;
@@ -698,9 +703,10 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
 
-#define blk_account_rq(rq) \
-	(((rq)->rq_flags & RQF_STARTED) && \
-	 ((rq)->cmd_type == REQ_TYPE_FS))
+static inline bool blk_account_rq(struct request *rq)
+{
+	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
+}
 
 #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
@@ -775,7 +781,7 @@ static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
 
 static inline bool rq_mergeable(struct request *rq)
 {
-	if (rq->cmd_type != REQ_TYPE_FS)
+	if (blk_rq_is_passthrough(rq))
 		return false;
 
 	if (req_op(rq) == REQ_OP_FLUSH)
@@ -1049,7 +1055,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 {
 	struct request_queue *q = rq->q;
 
-	if (unlikely(rq->cmd_type != REQ_TYPE_FS))
+	if (blk_rq_is_passthrough(rq))
 		return q->limits.max_hw_sectors;
 
 	if (!q->limits.chunk_sectors ||

commit 82ed4db499b8598f16f8871261bff088d6b0597f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 27 09:46:29 2017 +0100

    block: split scsi_request out of struct request
    
    And require all drivers that want to support BLOCK_PC to allocate it
    as the first thing of their private data.  To support this the legacy
    IDE and BSG code is switched to set cmd_size on their queues to let
    the block layer allocate the additional space.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 461b7cf6af1d..e4c5f284fe2d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -128,8 +128,6 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_NOMERGE_FLAGS \
 	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
 
-#define BLK_MAX_CDB	16
-
 /*
  * Try to put the fields that are referenced together in the same cacheline.
  *
@@ -227,17 +225,7 @@ struct request {
 
 	int errors;
 
-	/*
-	 * when request is used as a packet command carrier
-	 */
-	unsigned char __cmd[BLK_MAX_CDB];
-	unsigned char *cmd;
-	unsigned short cmd_len;
-
 	unsigned int extra_len;	/* length of alignment and padding */
-	unsigned int sense_len;
-	unsigned int resid_len;	/* residual count */
-	void *sense;
 
 	unsigned long deadline;
 	struct list_head timeout_list;
@@ -925,7 +913,6 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
-extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,

commit 6d247d7f71d1fa4b66a5f4da7b1daa21510d529b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 27 09:51:45 2017 -0700

    block: allow specifying size for extra command data
    
    This mirrors the blk-mq capabilities to allocate extra drivers-specific
    data behind struct request by setting a cmd_size field, as well as having
    a constructor / destructor for it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6b1efc5760ea..461b7cf6af1d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -273,6 +273,8 @@ typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 typedef int (lld_busy_fn) (struct request_queue *q);
 typedef int (bsg_job_fn) (struct bsg_job *);
+typedef int (init_rq_fn)(struct request_queue *, struct request *, gfp_t);
+typedef void (exit_rq_fn)(struct request_queue *, struct request *);
 
 enum blk_eh_timer_return {
 	BLK_EH_NOT_HANDLED,
@@ -408,6 +410,8 @@ struct request_queue {
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
+	init_rq_fn		*init_rq_fn;
+	exit_rq_fn		*exit_rq_fn;
 
 	const struct blk_mq_ops	*mq_ops;
 
@@ -577,6 +581,9 @@ struct request_queue {
 #endif
 
 	bool			mq_sysfs_init_done;
+
+	size_t			cmd_size;
+	void			*rq_alloc_data;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 5ea708d15a928f7a479987704203616d3274c03b
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 3 14:52:44 2017 +0300

    block: simplify blk_init_allocated_queue
    
    Return an errno value instead of the passed in queue so that the callers
    don't have to keep track of two queues, and move the assignment of the
    request_fn and lock to the caller as passing them as argument doesn't
    simplify anything.  While we're at it also remove two pointless NULL
    assignments, given that the request structure is zeroed on allocation.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 05675b1dfd20..6b1efc5760ea 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1137,8 +1137,7 @@ extern void blk_unprep_request(struct request *);
 extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
 					spinlock_t *lock, int node_id);
 extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
-extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
-						      request_fn_proc *, spinlock_t *);
+extern int blk_init_allocated_queue(struct request_queue *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);

commit f924ba70c1b12706c6679d793202e8f4c125f7ae
Merge: 7a308bb3016f 400f73b23f45
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jan 27 15:08:31 2017 -0700

    Merge branch 'for-4.11/block' into for-4.11/rq-refactor
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 50e1dab86aa2c10cbca2f754aae9542169403141
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jan 26 14:42:34 2017 -0700

    blk-mq-sched: fix starvation for multiple hardware queues and shared tags
    
    If we have both multiple hardware queues and shared tag map between
    devices, we need to ensure that we propagate the hardware queue
    restart bit higher up. This is because we can get into a situation
    where we don't have any IO pending on a hardware queue, yet we fail
    getting a tag to start new IO. If that happens, it's not enough to
    mark the hardware queue as needing a restart, we need to bubble
    that up to the higher level queue as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Hannes Reinecke <hare@suse.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0ee283f3cffe..883b8abe4305 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -607,6 +607,7 @@ struct request_queue {
 #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         26	/* device supports DAX */
 #define QUEUE_FLAG_STATS       27	/* track rq completion times */
+#define QUEUE_FLAG_RESTART     28	/* queue needs restart at completion */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 07e4fead45e6e1932f0b960655ab554b6aab6a08
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 25 08:06:40 2017 -0800

    blk-mq: create debugfs directory tree
    
    In preparation for putting blk-mq debugging information in debugfs,
    create a directory tree mirroring the one in sysfs:
    
        # tree -d /sys/kernel/debug/block
        /sys/kernel/debug/block
        |-- nvme0n1
        |   `-- mq
        |       |-- 0
        |       |   `-- cpu0
        |       |-- 1
        |       |   `-- cpu1
        |       |-- 2
        |       |   `-- cpu2
        |       `-- 3
        |           `-- cpu3
        `-- vda
            `-- mq
                `-- 0
                    |-- cpu0
                    |-- cpu1
                    |-- cpu2
                    `-- cpu3
    
    Also add the scaffolding for the actual files that will go in here,
    either under the hardware queue or software queue directories.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25564857f5f8..0ee283f3cffe 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -571,6 +571,11 @@ struct request_queue {
 	struct list_head	tag_set_list;
 	struct bio_set		*bio_split;
 
+#ifdef CONFIG_DEBUG_FS
+	struct dentry		*debugfs_dir;
+	struct dentry		*mq_debugfs_dir;
+#endif
+
 	bool			mq_sysfs_init_done;
 };
 

commit bd166ef183c263c5ced656d49ef19c7da4adc774
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jan 17 06:03:22 2017 -0700

    blk-mq-sched: add framework for MQ capable IO schedulers
    
    This adds a set of hooks that intercepts the blk-mq path of
    allocating/inserting/issuing/completing requests, allowing
    us to develop a scheduler within that framework.
    
    We reuse the existing elevator scheduler API on the registration
    side, but augment that with the scheduler flagging support for
    the blk-mq interfce, and with a separate set of ops hooks for MQ
    devices.
    
    We split driver and scheduler tags, so we can run the scheduling
    independently of device queue depth.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2e99d659b0f1..25564857f5f8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -154,6 +154,7 @@ struct request {
 
 	/* the following two fields are internal, NEVER access directly */
 	unsigned int __data_len;	/* total data len */
+	int tag;
 	sector_t __sector;		/* sector cursor */
 
 	struct bio *bio;
@@ -220,9 +221,10 @@ struct request {
 
 	unsigned short ioprio;
 
+	int internal_tag;
+
 	void *special;		/* opaque pointer available for LLD use */
 
-	int tag;
 	int errors;
 
 	/*

commit 2e3258ecfaebace1ceffaa14e0ea94775d54f46f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 13 12:29:10 2017 +0100

    block: add blk_rq_payload_bytes
    
    Add a helper to calculate the actual data transfer size for special
    payload requests.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ff3d774f2751..1ca8e8fd1078 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1000,6 +1000,19 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
+/*
+ * Some commands like WRITE SAME have a payload or data transfer size which
+ * is different from the size of the request.  Any driver that supports such
+ * commands using the RQF_SPECIAL_PAYLOAD flag needs to use this helper to
+ * calculate the data transfer size.
+ */
+static inline unsigned int blk_rq_payload_bytes(struct request *rq)
+{
+	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
+		return rq->special_vec.bv_len;
+	return blk_rq_bytes(rq);
+}
+
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {

commit f99e86485cc32cd16e5cc97f9bb0474f28608d84
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Thu Jan 12 07:58:32 2017 -0700

    block: Rename blk_queue_zone_size and bdev_zone_size
    
    All block device data fields and functions returning a number of 512B
    sectors are by convention named xxx_sectors while names in the form
    xxx_size are generally used for a number of bytes. The blk_queue_zone_size
    and bdev_zone_size functions were not following this convention so rename
    them.
    
    No functional change is introduced by this patch.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    
    Collapsed the two patches, they were nonsensically split and broke
    bisection.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 83695641bd5e..ff3d774f2751 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -739,7 +739,7 @@ static inline bool blk_queue_is_zoned(struct request_queue *q)
 	}
 }
 
-static inline unsigned int blk_queue_zone_size(struct request_queue *q)
+static inline unsigned int blk_queue_zone_sectors(struct request_queue *q)
 {
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
@@ -1536,12 +1536,12 @@ static inline bool bdev_is_zoned(struct block_device *bdev)
 	return false;
 }
 
-static inline unsigned int bdev_zone_size(struct block_device *bdev)
+static inline unsigned int bdev_zone_sectors(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);
 
 	if (q)
-		return blk_queue_zone_size(q);
+		return blk_queue_zone_sectors(q);
 
 	return 0;
 }

commit f8a5b12247fe18f7fed801ad262a7ab190e1f848
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Dec 13 09:24:51 2016 -0700

    blk-mq: make mq_ops a const pointer
    
    We never change it, make that clear.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b20da8dfa7ec..2e99d659b0f1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -407,7 +407,7 @@ struct request_queue {
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
 
-	struct blk_mq_ops	*mq_ops;
+	const struct blk_mq_ops	*mq_ops;
 
 	unsigned int		*mq_map;
 

commit 729204ef49ec00b788ce23deb9eb922a5769f55d
Author: Ming Lei <ming.lei@canonical.com>
Date:   Sat Dec 17 18:49:09 2016 +0800

    block: relax check on sg gap
    
    If the last bvec of the 1st bio and the 1st bvec of the next
    bio are physically contigious, and the latter can be merged
    to last segment of the 1st bio, we should think they don't
    violate sg gap(or virt boundary) limit.
    
    Both Vitaly and Dexuan reported lots of unmergeable small bios
    are observed when running mkfs on Hyper-V virtual storage, and
    performance becomes quite low. This patch fixes that performance
    issue.
    
    The same issue should exist on NVMe, since it sets virt boundary too.
    
    Reported-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reported-by: Dexuan Cui <decui@microsoft.com>
    Tested-by: Dexuan Cui <decui@microsoft.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 83695641bd5e..b20da8dfa7ec 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1607,6 +1607,25 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 	return __bvec_gap_to_prev(q, bprv, offset);
 }
 
+/*
+ * Check if the two bvecs from two bios can be merged to one segment.
+ * If yes, no need to check gap between the two bios since the 1st bio
+ * and the 1st bvec in the 2nd bio can be handled in one segment.
+ */
+static inline bool bios_segs_mergeable(struct request_queue *q,
+		struct bio *prev, struct bio_vec *prev_last_bv,
+		struct bio_vec *next_first_bv)
+{
+	if (!BIOVEC_PHYS_MERGEABLE(prev_last_bv, next_first_bv))
+		return false;
+	if (!BIOVEC_SEG_BOUNDARY(q, prev_last_bv, next_first_bv))
+		return false;
+	if (prev->bi_seg_back_size + next_first_bv->bv_len >
+			queue_max_segment_size(q))
+		return false;
+	return true;
+}
+
 static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
 			 struct bio *next)
 {
@@ -1616,7 +1635,8 @@ static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
 		bio_get_last_bvec(prev, &pb);
 		bio_get_first_bvec(next, &nb);
 
-		return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
+		if (!bios_segs_mergeable(q, prev, &pb, &nb))
+			return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
 	}
 
 	return false;

commit e8465447d2f3366069115f7453153561ac9a1220
Author: Ritesh Harjani <riteshh@codeaurora.org>
Date:   Fri Dec 16 10:11:56 2016 +0530

    block: Remove unused member (busy) from struct blk_queue_tag
    
    Signed-off-by: Ritesh Harjani <riteshh@codeaurora.org>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 286b2a264383..83695641bd5e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -288,7 +288,6 @@ enum blk_queue_state {
 struct blk_queue_tag {
 	struct request **tag_index;	/* map of busy tags */
 	unsigned long *tag_map;		/* bit map of free/busy tags */
-	int busy;			/* current depth */
 	int max_depth;			/* what we will send to device */
 	int real_max_depth;		/* what the array can hold */
 	atomic_t refcnt;		/* map can be shared */

commit b92e09bb5bf4db65aeb8ca0094fdd5142ed54451
Merge: c11a6cfb0103 aecec8b60422
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 13:26:24 2016 -0800

    Merge branch 'for-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/libata
    
    Pull libata updates from Tejun Heo:
    
     - Adam added opt-in ATA command priority support.
    
     - There are machines which hide multiple nvme devices behind an ahci
       BAR. Dan Williams proposed a solution to force-switch the mode but
       deemed too hackishd. People are gonna discuss the proper way to
       handle the situation in nvme standard meetings. For now, detect and
       warn about the situation.
    
     - Low level driver specific changes.
    
    Christoph Hellwig pipes in about the hidden nvme warning:
     "I wish that was the case. We've pretty much agreed that we'll want to
      implement it as a virtual PCIe root bridge, similar to Intels other
      'innovation' VMD that we work around that way.
    
      But Intel management has apparently decided that they don't want to
      spend more cycles on this now that Lenovo has an optional BIOS that
      doesn't force this broken mode anymore, and no one outside of Intel
      has enough information to implement something like this.
    
      So for now I guess this warning is it, until Intel reconsideres and
      spends resources on fixing up the damage their Chipset people caused"
    
    * 'for-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/libata:
      ahci: warn about remapped NVMe devices
      ahci-remap.h: add ahci remapping definitions
      nvme: move NVMe class code to pci_ids.h
      pata: imx: support controller modes up to PIO4
      pata: imx: add support of setting timings for PIO modes
      pata: imx: set controller PIO mode with .set_piomode callback
      pata: imx: sort headers out
      ata: set ncq_prio_enabled iff device has support
      ata: ATA Command Priority Disabled By Default
      ata: Enabling ATA Command Priorities
      block: Add iocontext priority to request
      ahci: qoriq: added ls1046a platform support

commit f9d03f96b988002027d4b28ea1b7a24729a4c9b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 8 15:20:32 2016 -0700

    block: improve handling of the magic discard payload
    
    Instead of allocating a single unused biovec for discard requests, send
    them down without any payload.  Instead we allow the driver to add a
    "special" payload using a biovec embedded into struct request (unioned
    over other fields never used while in the driver), and overloading
    the number of segments for this case.
    
    This has a couple of advantages:
    
     - we don't have to allocate the bio_vec
     - the amount of special casing for discard requests in the block
       layer is significantly reduced
     - using this same scheme for other request types is trivial,
       which will be important for implementing the new WRITE_ZEROES
       op on devices where it actually requires a payload (e.g. SCSI)
     - we can get rid of playing games with the request length, as
       we'll never touch it and completions will work just fine
     - it will allow us to support ranged discard operations in the
       future by merging non-contiguous discard bios into a single
       request
     - last but not least it removes a lot of code
    
    This patch is the common base for my WIP series for ranges discards and to
    remove discard_zeroes_data in favor of always using REQ_OP_WRITE_ZEROES,
    so it would be good to get it in quickly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ebeef2b79c5a..c5393766909d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -120,10 +120,13 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
 /* IO stats tracking on */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
+/* Look at ->special_vec for the actual data payload instead of the
+   bio chain. */
+#define RQF_SPECIAL_PAYLOAD	((__force req_flags_t)(1 << 18))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \
-	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ)
+	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
 
 #define BLK_MAX_CDB	16
 
@@ -175,6 +178,7 @@ struct request {
 	 */
 	union {
 		struct rb_node rb_node;	/* sort/lookup */
+		struct bio_vec special_vec;
 		void *completion_data;
 	};
 
@@ -909,8 +913,6 @@ extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
-extern void blk_add_request_payload(struct request *rq, struct page *page,
-		int offset, unsigned int len);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 			     struct bio_set *bs, gfp_t gfp_mask,
@@ -1153,6 +1155,13 @@ extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
+static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
+{
+	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
+		return 1;
+	return rq->nr_phys_segments;
+}
+
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);

commit a6f0788ec2881ac14e97ff7fa6a78a807f87b5ba
Author: Chaitanya Kulkarni <chaitanya.kulkarni@hgst.com>
Date:   Wed Nov 30 12:28:59 2016 -0800

    block: add support for REQ_OP_WRITE_ZEROES
    
    This adds a new block layer operation to zero out a range of
    LBAs. This allows to implement zeroing for devices that don't use
    either discard with a predictable zero pattern or WRITE SAME of zeroes.
    The prominent example of that is NVMe with the Write Zeroes command,
    but in the future, this should also help with improving the way
    zeroing discards work. For this operation, suitable entry is exported in
    sysfs which indicate the number of maximum bytes allowed in one
    write zeroes operation by the device.
    
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@hgst.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7e9d8a0895be..ebeef2b79c5a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -323,6 +323,7 @@ struct queue_limits {
 	unsigned int		max_discard_sectors;
 	unsigned int		max_hw_discard_sectors;
 	unsigned int		max_write_same_sectors;
+	unsigned int		max_write_zeroes_sectors;
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;
 
@@ -774,6 +775,9 @@ static inline bool rq_mergeable(struct request *rq)
 	if (req_op(rq) == REQ_OP_FLUSH)
 		return false;
 
+	if (req_op(rq) == REQ_OP_WRITE_ZEROES)
+		return false;
+
 	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
 		return false;
 	if (rq->rq_flags & RQF_NOMERGE_FLAGS)
@@ -1004,6 +1008,9 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	if (unlikely(op == REQ_OP_WRITE_SAME))
 		return q->limits.max_write_same_sectors;
 
+	if (unlikely(op == REQ_OP_WRITE_ZEROES))
+		return q->limits.max_write_zeroes_sectors;
+
 	return q->limits.max_sectors;
 }
 
@@ -1107,6 +1114,8 @@ extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
 extern void blk_queue_max_write_same_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
+extern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
+		unsigned int max_write_same_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_alignment_offset(struct request_queue *q,
@@ -1475,6 +1484,16 @@ static inline unsigned int bdev_write_same(struct block_device *bdev)
 	return 0;
 }
 
+static inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return q->limits.max_write_zeroes_sectors;
+
+	return 0;
+}
+
 static inline enum blk_zoned_model bdev_zoned_model(struct block_device *bdev)
 {
 	struct request_queue *q = bdev_get_queue(bdev);

commit e73c23ff736e1ea371dfa419d7bf8e77ee53044a
Author: Chaitanya Kulkarni <chaitanya.kulkarni@hgst.com>
Date:   Wed Nov 30 12:28:58 2016 -0800

    block: add async variant of blkdev_issue_zeroout
    
    Similar to __blkdev_issue_discard this variant allows submitting
    the final bio asynchronously and chaining multiple ranges
    into a single completion.
    
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@hgst.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 541fdd8787a5..7e9d8a0895be 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1269,6 +1269,9 @@ extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		struct bio **biop);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
+extern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
+		sector_t nr_sects, gfp_t gfp_mask, struct bio **biop,
+		bool discard);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, bool discard);
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,

commit 9a05e7541c39680d28ecf91892338e074738d5fd
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Fri Nov 18 15:16:06 2016 +0100

    block: Change extern inline to static inline
    
    With compilers which follow the C99 standard (like modern versions of
    gcc and clang), "extern inline" does the opposite thing from older
    versions of gcc (emits code for an externally linkable version of the
    inline function).
    
    "static inline" does the intended behavior in all cases instead.
    
    Description taken from commit 6d91857d4826 ("staging, rtl8192e,
    LLVMLinux: Change extern inline to static inline").
    
    This also fixes the following GCC warning when building with CONFIG_PM
    disabled:
    
      ./include/linux/blkdev.h:1143:20: warning: no previous prototype for 'blk_set_runtime_active' [-Wmissing-prototypes]
    
    Fixes: d07ab6d11477 ("block: Add blk_set_runtime_active()")
    Reviewed-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 85699bc90a51..541fdd8787a5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1174,7 +1174,7 @@ static inline int blk_pre_runtime_suspend(struct request_queue *q)
 static inline void blk_post_runtime_suspend(struct request_queue *q, int err) {}
 static inline void blk_pre_runtime_resume(struct request_queue *q) {}
 static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
-extern inline void blk_set_runtime_active(struct request_queue *q) {}
+static inline void blk_set_runtime_active(struct request_queue *q) {}
 #endif
 
 /*

commit 64f1c21e86f7fe63337b5c23c129de3ec506431d
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Nov 14 13:03:03 2016 -0700

    blk-mq: make the polling code adaptive
    
    The previous commit introduced the hybrid sleep/poll mode. Take
    that one step further, and use the completion latencies to
    automatically sleep for half the mean completion time. This is
    a good approximation.
    
    This changes the 'io_poll_delay' sysfs file a bit to expose the
    various options. Depending on the value, the polling code will
    behave differently:
    
    -1      Never enter hybrid sleep mode
     0      Use half of the completion mean for the sleep delay
    >0      Use this specific value as the sleep delay
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Tested-By: Stephen Bates <sbates@raithlin.com>
    Reviewed-By: Stephen Bates <sbates@raithlin.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 37ed4ea705c8..85699bc90a51 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -509,7 +509,7 @@ struct request_queue {
 	unsigned int		request_fn_active;
 
 	unsigned int		rq_timeout;
-	unsigned int		poll_nsec;
+	int			poll_nsec;
 	struct timer_list	timeout;
 	struct work_struct	timeout_work;
 	struct list_head	timeout_list;

commit 06426adf072bca62ac31ea396ff2159a34f276c2
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Nov 14 13:01:59 2016 -0700

    blk-mq: implement hybrid poll mode for sync O_DIRECT
    
    This patch enables a hybrid polling mode. Instead of polling after IO
    submission, we can induce an artificial delay, and then poll after that.
    For example, if the IO is presumed to complete in 8 usecs from now, we
    can sleep for 4 usecs, wake up, and then do our polling. This still puts
    a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
    after the IO has completed, it'll happen before. With this hybrid
    scheme, we can achieve big latency reductions while still using the same
    (or less) amount of CPU.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Tested-By: Stephen Bates <sbates@raithlin.com>
    Reviewed-By: Stephen Bates <sbates@raithlin.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bab18ee5810d..37ed4ea705c8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -509,6 +509,7 @@ struct request_queue {
 	unsigned int		request_fn_active;
 
 	unsigned int		rq_timeout;
+	unsigned int		poll_nsec;
 	struct timer_list	timeout;
 	struct work_struct	timeout_work;
 	struct list_head	timeout_list;

commit bbd7bb7017d5c2b1e75f3818b4ce88fa58bb0eab
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Nov 4 09:34:34 2016 -0600

    block: move poll code to blk-mq
    
    The poll code is blk-mq specific, let's move it to blk-mq.c. This
    is a prep patch for improving the polling code.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 15da9e430f90..bab18ee5810d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -952,7 +952,7 @@ extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 
-bool blk_poll(struct request_queue *q, blk_qc_t cookie);
+bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {

commit 87760e5eef359788047d6fd54fc12eec74ce0d27
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Nov 9 12:38:14 2016 -0700

    block: hook up writeback throttling
    
    Enable throttling of buffered writeback to make it a lot
    more smooth, and has way less impact on other system activity.
    Background writeback should be, by definition, background
    activity. The fact that we flush huge bundles of it at the time
    means that it potentially has heavy impacts on foreground workloads,
    which isn't ideal. We can't easily limit the sizes of writes that
    we do, since that would impact file system layout in the presence
    of delayed allocation. So just throttle back buffered writeback,
    unless someone is waiting for it.
    
    The algorithm for when to throttle takes its inspiration in the
    CoDel networking scheduling algorithm. Like CoDel, blk-wb monitors
    the minimum latencies of requests over a window of time. In that
    window of time, if the minimum latency of any request exceeds a
    given target, then a scale count is incremented and the queue depth
    is shrunk. The next monitoring window is shrunk accordingly. Unlike
    CoDel, if we hit a window that exhibits good behavior, then we
    simply increment the scale count and re-calculate the limits for that
    scale value. This prevents us from oscillating between a
    close-to-ideal value and max all the time, instead remaining in the
    windows where we get good behavior.
    
    Unlike CoDel, blk-wb allows the scale count to to negative. This
    happens if we primarily have writes going on. Unlike positive
    scale counts, this doesn't change the size of the monitoring window.
    When the heavy writers finish, blk-bw quickly snaps back to it's
    stable state of a zero scale count.
    
    The patch registers a sysfs entry, 'wb_lat_usec'. This sets the latency
    target to me met. It defaults to 2 msec for non-rotational storage, and
    75 msec for rotational storage. Setting this value to '0' disables
    blk-wb. Generally, a user would not have to touch this setting.
    
    We don't enable WBT on devices that are managed with CFQ, and have
    a non-root block cgroup attached. If we have a proportional share setup
    on this particular disk, then the wbt throttling will interfere with
    that. We don't have a strong need for wbt for that case, since we will
    rely on CFQ doing that for us.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 303723a2e5b8..15da9e430f90 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -38,6 +38,7 @@ struct bsg_job;
 struct blkcg_gq;
 struct blk_flush_queue;
 struct pr_ops;
+struct rq_wb;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -383,6 +384,8 @@ struct request_queue {
 	int			nr_rqs[2];	/* # allocated [a]sync rqs */
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
+	struct rq_wb		*rq_wb;
+
 	/*
 	 * If blkcg is not used, @q->root_rl serves all requests.  If blkcg
 	 * is used, root blkg allocates from @q->root_rl and all other

commit cf43e6be865a582ba66ee4747ae27a0513f6bba1
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Nov 7 21:32:37 2016 -0700

    block: add scalable completion tracking of requests
    
    For legacy block, we simply track them in the request queue. For
    blk-mq, we track them on a per-sw queue basis, which we can then
    sum up through the hardware queues and finally to a per device
    state.
    
    The stats are tracked in, roughly, 0.1s interval windows.
    
    Add sysfs files to display the stats.
    
    The feature is off by default, to avoid any extra overhead. In-kernel
    users of it can turn it on by setting QUEUE_FLAG_STATS in the queue
    flags. We currently don't turn it on if someone just reads any of
    the stats files, that is something we could add as well.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d364be6e6959..303723a2e5b8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -117,6 +117,8 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_PM			((__force req_flags_t)(1 << 15))
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
+/* IO stats tracking on */
+#define RQF_STATS		((__force req_flags_t)(1 << 17))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \
@@ -197,6 +199,7 @@ struct request {
 	struct gendisk *rq_disk;
 	struct hd_struct *part;
 	unsigned long start_time;
+	struct blk_issue_stat issue_stat;
 #ifdef CONFIG_BLK_CGROUP
 	struct request_list *rl;		/* rl this rq is alloced from */
 	unsigned long long start_time_ns;
@@ -492,6 +495,9 @@ struct request_queue {
 
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
+
+	struct blk_rq_stat	rq_stats[2];
+
 	/*
 	 * Number of active block driver functions for which blk_drain_queue()
 	 * must wait. Must be incremented around functions that unlock the
@@ -585,6 +591,7 @@ struct request_queue {
 #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
 #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         26	/* device supports DAX */
+#define QUEUE_FLAG_STATS       27	/* track rq completion times */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit d278d4a8892f13b6a9eb6102b356402f0e062324
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Mar 30 10:21:08 2016 -0600

    block: add code to track actual device queue depth
    
    For blk-mq, ->nr_requests does track queue depth, at least at init
    time. But for the older queue paths, it's simply a soft setting.
    On top of that, it's generally larger than the hardware setting
    on purpose, to allow backup of requests for merging.
    
    Fill a hole in struct request with a 'queue_depth' member, that
    drivers can call to more closely inform the block layer of the
    real queue depth.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Jan Kara <jack@suse.cz>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9189a2d5c392..d364be6e6959 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -405,6 +405,8 @@ struct request_queue {
 	struct blk_mq_ctx __percpu	*queue_ctx;
 	unsigned int		nr_queues;
 
+	unsigned int		queue_depth;
+
 	/* hw dispatch queues */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
 	unsigned int		nr_hw_queues;
@@ -777,6 +779,14 @@ static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
 	return false;
 }
 
+static inline unsigned int blk_queue_depth(struct request_queue *q)
+{
+	if (q->queue_depth)
+		return q->queue_depth;
+
+	return q->nr_requests;
+}
+
 /*
  * q->prep_rq_fn return values
  */
@@ -1094,6 +1104,7 @@ extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
 extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
 extern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);
 extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
+extern void blk_set_queue_depth(struct request_queue *q, unsigned int depth);
 extern void blk_set_default_limits(struct queue_limits *lim);
 extern void blk_set_stacking_limits(struct queue_limits *lim);
 extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,

commit 50d24c34403c62ad29e8b6db559d491bae20b4b7
Author: Shaohua Li <shli@fb.com>
Date:   Thu Nov 3 17:03:53 2016 -0700

    block: immediately dispatch big size request
    
    Currently block plug holds up to 16 non-mergeable requests. This makes
    sense if the request size is small, eg, reduce lock contention. But if
    request size is big enough, we don't need to worry about lock
    contention. Holding such request makes no sense and it lows the disk
    utilization.
    
    In practice, this improves 10% throughput for my raid5 sequential write
    workload.
    
    The size (128k) is arbitrary right now, but it makes sure lock
    contention is small. This probably could be more intelligent, eg, check
    average request size holded. Since this is mainly for sequential IO,
    probably not worthy.
    
    V2: check the last request instead of the first request, so as long as
    there is one big size request we flush the plug.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 13d893a69b46..9189a2d5c392 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1173,6 +1173,7 @@ struct blk_plug {
 	struct list_head cb_list; /* md requires an unplug callback */
 };
 #define BLK_MAX_REQUEST_COUNT 16
+#define BLK_PLUG_FLUSH_SIZE (128 * 1024)
 
 struct blk_plug_cb;
 typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);

commit 6a83e74d214a47a1371cd2e6a783264fcba7d428
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Wed Nov 2 10:09:51 2016 -0600

    blk-mq: Introduce blk_mq_quiesce_queue()
    
    blk_mq_quiesce_queue() waits until ongoing .queue_rq() invocations
    have finished. This function does *not* wait until all outstanding
    requests have finished (this means invocation of request.end_io()).
    The algorithm used by blk_mq_quiesce_queue() is as follows:
    * Hold either an RCU read lock or an SRCU read lock around
      .queue_rq() calls. The former is used if .queue_rq() does not
      block and the latter if .queue_rq() may block.
    * blk_mq_quiesce_queue() first calls blk_mq_stop_hw_queues()
      followed by synchronize_srcu() or synchronize_rcu(). The latter
      call waits for .queue_rq() invocations that started before
      blk_mq_quiesce_queue() was called.
    * The blk_mq_hctx_stopped() calls that control whether or not
      .queue_rq() will be called are called with the (S)RCU read lock
      held. This is necessary to avoid race conditions against
      blk_mq_quiesce_queue().
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8396da2bb698..13d893a69b46 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -918,6 +918,7 @@ extern void __blk_run_queue(struct request_queue *q);
 extern void __blk_run_queue_uncond(struct request_queue *q);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_run_queue_async(struct request_queue *q);
+extern void blk_mq_quiesce_queue(struct request_queue *q);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,
 			   gfp_t);

commit ef295ecf090d3e86e5b742fc6ab34f1122a43773
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 28 08:48:16 2016 -0600

    block: better op and flags encoding
    
    Now that we don't need the common flags to overflow outside the range
    of a 32-bit type we can encode them the same way for both the bio and
    request fields.  This in addition allows us to place the operation
    first (and make some room for more ops while we're at it) and to
    stop having to shift around the operation values.
    
    In addition this allows passing around only one value in the block layer
    instead of two (and eventuall also in the file systems, but we can do
    that later) and thus clean up a lot of code.
    
    Last but not least this allows decreasing the size of the cmd_flags
    field in struct request to 32-bits.  Various functions passing this
    value could also be updated, but I'd like to avoid the churn for now.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b4415feac679..8396da2bb698 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -142,7 +142,7 @@ struct request {
 
 	int cpu;
 	unsigned cmd_type;
-	u64 cmd_flags;
+	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 	unsigned long atomic_flags;
 
@@ -244,20 +244,6 @@ struct request {
 	struct request *next_rq;
 };
 
-#define REQ_OP_SHIFT (8 * sizeof(u64) - REQ_OP_BITS)
-#define req_op(req)  ((req)->cmd_flags >> REQ_OP_SHIFT)
-
-#define req_set_op(req, op) do {				\
-	WARN_ON(op >= (1 << REQ_OP_BITS));			\
-	(req)->cmd_flags &= ((1ULL << REQ_OP_SHIFT) - 1);	\
-	(req)->cmd_flags |= ((u64) (op) << REQ_OP_SHIFT);	\
-} while (0)
-
-#define req_set_op_attrs(req, op, flags) do {	\
-	req_set_op(req, op);			\
-	(req)->cmd_flags |= flags;		\
-} while (0)
-
 static inline unsigned short req_get_ioprio(struct request *req)
 {
 	return req->ioprio;
@@ -741,17 +727,9 @@ static inline unsigned int blk_queue_zone_size(struct request_queue *q)
 	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
 }
 
-/*
- * We regard a request as sync, if either a read or a sync write
- */
-static inline bool rw_is_sync(int op, unsigned int rw_flags)
-{
-	return op == REQ_OP_READ || (rw_flags & REQ_SYNC);
-}
-
 static inline bool rq_is_sync(struct request *rq)
 {
-	return rw_is_sync(req_op(rq), rq->cmd_flags);
+	return op_is_sync(rq->cmd_flags);
 }
 
 static inline bool blk_rl_full(struct request_list *rl, bool sync)

commit e806402130c9c494e22c73ae9ead4e79d2a5811c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 20 15:12:13 2016 +0200

    block: split out request-only flags into a new namespace
    
    A lot of the REQ_* flags are only used on struct requests, and only of
    use to the block layer and a few drivers that dig into struct request
    internals.
    
    This patch adds a new req_flags_t rq_flags field to struct request for
    them, and thus dramatically shrinks the number of common requests.  It
    also removes the unfortunate situation where we have to fit the fields
    from the same enum into 32 bits for struct bio and 64 bits for
    struct request.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 90097dd8b8ed..b4415feac679 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -78,6 +78,50 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
 };
 
+/*
+ * request flags */
+typedef __u32 __bitwise req_flags_t;
+
+/* elevator knows about this request */
+#define RQF_SORTED		((__force req_flags_t)(1 << 0))
+/* drive already may have started this one */
+#define RQF_STARTED		((__force req_flags_t)(1 << 1))
+/* uses tagged queueing */
+#define RQF_QUEUED		((__force req_flags_t)(1 << 2))
+/* may not be passed by ioscheduler */
+#define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
+/* request for flush sequence */
+#define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
+/* merge of different types, fail separately */
+#define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
+/* track inflight for MQ */
+#define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
+/* don't call prep for this one */
+#define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
+/* set for "ide_preempt" requests and also for requests for which the SCSI
+   "quiesce" state must be ignored. */
+#define RQF_PREEMPT		((__force req_flags_t)(1 << 8))
+/* contains copies of user pages */
+#define RQF_COPY_USER		((__force req_flags_t)(1 << 9))
+/* vaguely specified driver internal error.  Ignored by the block layer */
+#define RQF_FAILED		((__force req_flags_t)(1 << 10))
+/* don't warn about errors */
+#define RQF_QUIET		((__force req_flags_t)(1 << 11))
+/* elevator private data attached */
+#define RQF_ELVPRIV		((__force req_flags_t)(1 << 12))
+/* account I/O stat */
+#define RQF_IO_STAT		((__force req_flags_t)(1 << 13))
+/* request came from our alloc pool */
+#define RQF_ALLOCED		((__force req_flags_t)(1 << 14))
+/* runtime pm request */
+#define RQF_PM			((__force req_flags_t)(1 << 15))
+/* on IO scheduler merge hash */
+#define RQF_HASHED		((__force req_flags_t)(1 << 16))
+
+/* flags that prevent us from merging requests: */
+#define RQF_NOMERGE_FLAGS \
+	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ)
+
 #define BLK_MAX_CDB	16
 
 /*
@@ -99,6 +143,7 @@ struct request {
 	int cpu;
 	unsigned cmd_type;
 	u64 cmd_flags;
+	req_flags_t rq_flags;
 	unsigned long atomic_flags;
 
 	/* the following two fields are internal, NEVER access directly */
@@ -648,7 +693,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 			     REQ_FAILFAST_DRIVER))
 
 #define blk_account_rq(rq) \
-	(((rq)->cmd_flags & REQ_STARTED) && \
+	(((rq)->rq_flags & RQF_STARTED) && \
 	 ((rq)->cmd_type == REQ_TYPE_FS))
 
 #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
@@ -740,6 +785,8 @@ static inline bool rq_mergeable(struct request *rq)
 
 	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
 		return false;
+	if (rq->rq_flags & RQF_NOMERGE_FLAGS)
+		return false;
 
 	return true;
 }

commit 5dc8b362a2374d007bc0db649b7ab6a79dd32bda
Author: Adam Manzanares <adam.manzanares@hgst.com>
Date:   Mon Oct 17 11:27:28 2016 -0700

    block: Add iocontext priority to request
    
    Patch adds an association between iocontext ioprio and the ioprio of a
    request. This is done to enable request based drivers the ability to
    act on priority information stored in the request. An example being
    ATA devices that support command priorities. If the ATA driver discovers
    that the device supports command priorities and the request has valid
    priority information indicating the request is high priority, then a high
    priority command can be sent to the device. This should improve tail
    latencies for high priority IO on any device that queues requests
    internally and can make use of the priority information stored in the
    request.
    
    The ioprio of the request is set in blk_rq_set_prio which takes the
    request and the ioc as arguments. If the ioc is valid in blk_rq_set_prio
    then the iopriority of the request is set as the iopriority of the ioc.
    In init_request_from_bio a check is made to see if the ioprio of the bio
    is valid and if so then the request prio comes from the bio.
    
    Signed-off-by: Adam Manzananares <adam.manzanares@wdc.com>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c47c358ba052..9a0ceaa1b7e6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -933,6 +933,20 @@ static inline unsigned int blk_rq_count_bios(struct request *rq)
 	return nr_bios;
 }
 
+/*
+ * blk_rq_set_prio - associate a request with prio from ioc
+ * @rq: request of interest
+ * @ioc: target iocontext
+ *
+ * Assocate request prio with ioc prio so request based drivers
+ * can leverage priority information.
+ */
+static inline void blk_rq_set_prio(struct request *rq, struct io_context *ioc)
+{
+	if (ioc)
+		rq->ioprio = ioc->ioprio;
+}
+
 /*
  * Request issue related functions.
  */

commit 3ed05a987e0f63b21e634101e0b460d32f3581c3
Author: Shaun Tancheff <shaun@tancheff.com>
Date:   Tue Oct 18 15:40:35 2016 +0900

    blk-zoned: implement ioctls
    
    Adds the new BLKREPORTZONE and BLKRESETZONE ioctls for respectively
    obtaining the zone configuration of a zoned block device and resetting
    the write pointer of sequential zones of a zoned block device.
    
    The BLKREPORTZONE ioctl maps directly to a single call of the function
    blkdev_report_zones. The zone information result is passed as an array
    of struct blk_zone identical to the structure used internally for
    processing the REQ_OP_ZONE_REPORT operation.  The BLKRESETZONE ioctl
    maps to a single call of the blkdev_reset_zones function.
    
    Signed-off-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@hgst.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 252043f7cd2c..90097dd8b8ed 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -316,6 +316,27 @@ extern int blkdev_report_zones(struct block_device *bdev,
 extern int blkdev_reset_zones(struct block_device *bdev, sector_t sectors,
 			      sector_t nr_sectors, gfp_t gfp_mask);
 
+extern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,
+				     unsigned int cmd, unsigned long arg);
+extern int blkdev_reset_zones_ioctl(struct block_device *bdev, fmode_t mode,
+				    unsigned int cmd, unsigned long arg);
+
+#else /* CONFIG_BLK_DEV_ZONED */
+
+static inline int blkdev_report_zones_ioctl(struct block_device *bdev,
+					    fmode_t mode, unsigned int cmd,
+					    unsigned long arg)
+{
+	return -ENOTTY;
+}
+
+static inline int blkdev_reset_zones_ioctl(struct block_device *bdev,
+					   fmode_t mode, unsigned int cmd,
+					   unsigned long arg)
+{
+	return -ENOTTY;
+}
+
 #endif /* CONFIG_BLK_DEV_ZONED */
 
 struct request_queue {

commit 6a0cb1bc106fc07ce0443303bcdb7f7da5131e5c
Author: Hannes Reinecke <hare@suse.de>
Date:   Tue Oct 18 15:40:33 2016 +0900

    block: Implement support for zoned block devices
    
    Implement zoned block device zone information reporting and reset.
    Zone information are reported as struct blk_zone. This implementation
    does not differentiate between host-aware and host-managed device
    models and is valid for both. Two functions are provided:
    blkdev_report_zones for discovering the zone configuration of a
    zoned block device, and blkdev_reset_zones for resetting the write
    pointer of sequential zones. The helper function blk_queue_zone_size
    and bdev_zone_size are also provided for, as the name suggest,
    obtaining the zone size (in 512B sectors) of the zones of the device.
    
    Signed-off-by: Hannes Reinecke <hare@suse.de>
    
    [Damien: * Removed the zone cache
             * Implement report zones operation based on earlier proposal
               by Shaun Tancheff <shaun.tancheff@seagate.com>]
    Signed-off-by: Damien Le Moal <damien.lemoal@hgst.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Tested-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f19e16bb43d1..252043f7cd2c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -24,6 +24,7 @@
 #include <linux/rcupdate.h>
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
+#include <linux/blkzoned.h>
 
 struct module;
 struct scsi_ioctl_command;
@@ -302,6 +303,21 @@ struct queue_limits {
 	enum blk_zoned_model	zoned;
 };
 
+#ifdef CONFIG_BLK_DEV_ZONED
+
+struct blk_zone_report_hdr {
+	unsigned int	nr_zones;
+	u8		padding[60];
+};
+
+extern int blkdev_report_zones(struct block_device *bdev,
+			       sector_t sector, struct blk_zone *zones,
+			       unsigned int *nr_zones, gfp_t gfp_mask);
+extern int blkdev_reset_zones(struct block_device *bdev, sector_t sectors,
+			      sector_t nr_sectors, gfp_t gfp_mask);
+
+#endif /* CONFIG_BLK_DEV_ZONED */
+
 struct request_queue {
 	/*
 	 * Together with queue_head for cacheline sharing
@@ -654,6 +670,11 @@ static inline bool blk_queue_is_zoned(struct request_queue *q)
 	}
 }
 
+static inline unsigned int blk_queue_zone_size(struct request_queue *q)
+{
+	return blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;
+}
+
 /*
  * We regard a request as sync, if either a read or a sync write
  */
@@ -1401,6 +1422,16 @@ static inline bool bdev_is_zoned(struct block_device *bdev)
 	return false;
 }
 
+static inline unsigned int bdev_zone_size(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return blk_queue_zone_size(q);
+
+	return 0;
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 797476b88bde2a6001f9552f383f147e58c1a330
Author: Damien Le Moal <damien.lemoal@hgst.com>
Date:   Tue Oct 18 15:40:29 2016 +0900

    block: Add 'zoned' queue limit
    
    Add the zoned queue limit to indicate the zoning model of a block device.
    Defined values are 0 (BLK_ZONED_NONE) for regular block devices,
    1 (BLK_ZONED_HA) for host-aware zone block devices and 2 (BLK_ZONED_HM)
    for host-managed zone block devices. The standards defined drive managed
    model is not defined here since these block devices do not provide any
    command for accessing zone information. Drive managed model devices will
    be reported as BLK_ZONED_NONE.
    
    The helper functions blk_queue_zoned_model and bdev_zoned_model return
    the zoned limit and the functions blk_queue_is_zoned and bdev_is_zoned
    return a boolean for callers to test if a block device is zoned.
    
    The zoned attribute is also exported as a string to applications via
    sysfs. BLK_ZONED_NONE shows as "none", BLK_ZONED_HA as "host-aware" and
    BLK_ZONED_HM as "host-managed".
    
    Signed-off-by: Damien Le Moal <damien.lemoal@hgst.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Tested-by: Shaun Tancheff <shaun.tancheff@seagate.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c47c358ba052..f19e16bb43d1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -261,6 +261,15 @@ struct blk_queue_tag {
 #define BLK_SCSI_MAX_CMDS	(256)
 #define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
 
+/*
+ * Zoned block device models (zoned limit).
+ */
+enum blk_zoned_model {
+	BLK_ZONED_NONE,	/* Regular block device */
+	BLK_ZONED_HA,	/* Host-aware zoned block device */
+	BLK_ZONED_HM,	/* Host-managed zoned block device */
+};
+
 struct queue_limits {
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
@@ -290,6 +299,7 @@ struct queue_limits {
 	unsigned char		cluster;
 	unsigned char		discard_zeroes_data;
 	unsigned char		raid_partial_stripes_expensive;
+	enum blk_zoned_model	zoned;
 };
 
 struct request_queue {
@@ -627,6 +637,23 @@ static inline unsigned int blk_queue_cluster(struct request_queue *q)
 	return q->limits.cluster;
 }
 
+static inline enum blk_zoned_model
+blk_queue_zoned_model(struct request_queue *q)
+{
+	return q->limits.zoned;
+}
+
+static inline bool blk_queue_is_zoned(struct request_queue *q)
+{
+	switch (blk_queue_zoned_model(q)) {
+	case BLK_ZONED_HA:
+	case BLK_ZONED_HM:
+		return true;
+	default:
+		return false;
+	}
+}
+
 /*
  * We regard a request as sync, if either a read or a sync write
  */
@@ -1354,6 +1381,26 @@ static inline unsigned int bdev_write_same(struct block_device *bdev)
 	return 0;
 }
 
+static inline enum blk_zoned_model bdev_zoned_model(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return blk_queue_zoned_model(q);
+
+	return BLK_ZONED_NONE;
+}
+
+static inline bool bdev_is_zoned(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return blk_queue_is_zoned(q);
+
+	return false;
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 2849450ad39d2e699fda2d5c6f41e05d87fd7004
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Sep 14 13:28:30 2016 -0400

    blk-mq: introduce blk_mq_delay_kick_requeue_list()
    
    blk_mq_delay_kick_requeue_list() provides the ability to kick the
    q->requeue_list after a specified time.  To do this the request_queue's
    'requeue_work' member was changed to a delayed_work.
    
    blk_mq_delay_kick_requeue_list() allows DM to defer processing requeued
    requests while it doesn't make sense to immediately requeue them
    (e.g. when all paths in a DM multipath have failed).
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 69aae720f4ef..c47c358ba052 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -449,7 +449,7 @@ struct request_queue {
 
 	struct list_head	requeue_list;
 	spinlock_t		requeue_lock;
-	struct work_struct	requeue_work;
+	struct delayed_work	requeue_work;
 
 	struct mutex		sysfs_lock;
 

commit ee63cfa7fc197b63669623721b8009cce5b0659b
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Aug 24 15:52:48 2016 -0600

    block: add kblockd_schedule_work_on()
    
    Add a helper to schedule a regular struct work on a particular CPU.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e79055c8b577..69aae720f4ef 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1440,8 +1440,8 @@ static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
 	return bio_will_gap(req->q, bio, req->bio);
 }
 
-struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
+int kblockd_schedule_work_on(int cpu, struct work_struct *work);
 int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
 int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 

commit 7afafc8a44bf0ab841b17d450b02aedb3a138985
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Aug 16 10:59:35 2016 +0300

    block: Fix secure erase
    
    Commit 288dab8a35a0 ("block: add a separate operation type for secure
    erase") split REQ_OP_SECURE_ERASE from REQ_OP_DISCARD without considering
    all the places REQ_OP_DISCARD was being used to mean either. Fix those.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Fixes: 288dab8a35a0 ("block: add a separate operation type for secure erase")
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c210b6a7bcf..e79055c8b577 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -882,7 +882,7 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     int op)
 {
-	if (unlikely(op == REQ_OP_DISCARD))
+	if (unlikely(op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE))
 		return min(q->limits.max_discard_sectors, UINT_MAX >> 9);
 
 	if (unlikely(op == REQ_OP_WRITE_SAME))
@@ -913,7 +913,9 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 	if (unlikely(rq->cmd_type != REQ_TYPE_FS))
 		return q->limits.max_hw_sectors;
 
-	if (!q->limits.chunk_sectors || (req_op(rq) == REQ_OP_DISCARD))
+	if (!q->limits.chunk_sectors ||
+	    req_op(rq) == REQ_OP_DISCARD ||
+	    req_op(rq) == REQ_OP_SECURE_ERASE)
 		return blk_queue_get_max_sectors(q, req_op(rq));
 
 	return min(blk_max_size_offset(q, offset),

commit c11f0c0b5bb949673e4fc16c742f0316ae4ced20
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Aug 5 08:11:04 2016 -0600

    block/mm: make bdev_ops->rw_page() take a bool for read/write
    
    Commit abf545484d31 changed it from an 'rw' flags type to the
    newer ops based interface, but now we're effectively leaking
    some bdev internals to the rest of the kernel. Since we only
    care about whether it's a read or a write at that level, just
    pass in a bool 'is_write' parameter instead.
    
    Then we can also move op_is_write() and friends back under
    CONFIG_BLOCK protection.
    
    Reviewed-by: Mike Christie <mchristi@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ccd68c0d01de..2c210b6a7bcf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1672,7 +1672,7 @@ struct blk_dax_ctl {
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
-	int (*rw_page)(struct block_device *, sector_t, struct page *, int op);
+	int (*rw_page)(struct block_device *, sector_t, struct page *, bool);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	long (*direct_access)(struct block_device *, sector_t, void **, pfn_t *,

commit abf545484d31b68777a85c5c8f5b4bcde08283eb
Author: Mike Christie <mchristi@redhat.com>
Date:   Thu Aug 4 14:23:34 2016 -0600

    mm/block: convert rw_page users to bio op use
    
    The rw_page users were not converted to use bio/req ops. As a result
    bdev_write_page is not passing down REQ_OP_WRITE and the IOs will
    be sent down as reads.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Fixes: 4e1b2d52a80d ("block, fs, drivers: remove REQ_OP compat defs and related code")
    
    Modified by me to:
    
    1) Drop op_flags passing into ->rw_page(), as we don't use it.
    2) Make op_is_write() and friends safe to use for !CONFIG_BLOCK
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index de7935961c27..ccd68c0d01de 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1672,7 +1672,7 @@ struct blk_dax_ctl {
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
-	int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
+	int (*rw_page)(struct block_device *, sector_t, struct page *, int op);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	long (*direct_access)(struct block_device *, sector_t, void **, pfn_t *,

commit 6d25ec147e3a71858bed5439c92accd7f739a0a3
Author: John Pittman <jpittman@redhat.com>
Date:   Mon Aug 1 16:35:53 2016 -0400

    Include: blkdev: Removed duplicate 'struct request;' declaration.
    
    In include/linux/blkdev.h duplicate declarations of the request
    struct exist.  Cleaned up by removing the second, unneeded
    declaration.
    
    Signed-off-by: John Pittman <jpittman@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index adf33079771e..de7935961c27 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -47,7 +47,6 @@ struct pr_ops;
  */
 #define BLKCG_MAX_POLS		2
 
-struct request;
 typedef void (rq_end_io_fn)(struct request *, int);
 
 #define BLK_RL_SYNCFULL		(1U << 0)

commit f0c98ebc57c2d5e535bc4f9167f35650d2ba3c90
Merge: d94ba9e7d8d5 0606263f24f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 28 17:22:07 2016 -0700

    Merge tag 'libnvdimm-for-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
    
     - Replace pcommit with ADR / directed-flushing.
    
       The pcommit instruction, which has not shipped on any product, is
       deprecated.  Instead, the requirement is that platforms implement
       either ADR, or provide one or more flush addresses per nvdimm.
    
       ADR (Asynchronous DRAM Refresh) flushes data in posted write buffers
       to the memory controller on a power-fail event.
    
       Flush addresses are defined in ACPI 6.x as an NVDIMM Firmware
       Interface Table (NFIT) sub-structure: "Flush Hint Address Structure".
       A flush hint is an mmio address that when written and fenced assures
       that all previous posted writes targeting a given dimm have been
       flushed to media.
    
     - On-demand ARS (address range scrub).
    
       Linux uses the results of the ACPI ARS commands to track bad blocks
       in pmem devices.  When latent errors are detected we re-scrub the
       media to refresh the bad block list, userspace can also request a
       re-scrub at any time.
    
     - Support for the Microsoft DSM (device specific method) command
       format.
    
     - Support for EDK2/OVMF virtual disk device memory ranges.
    
     - Various fixes and cleanups across the subsystem.
    
    * tag 'libnvdimm-for-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (41 commits)
      libnvdimm-btt: Delete an unnecessary check before the function call "__nd_device_register"
      nfit: do an ARS scrub on hitting a latent media error
      nfit: move to nfit/ sub-directory
      nfit, libnvdimm: allow an ARS scrub to be triggered on demand
      libnvdimm: register nvdimm_bus devices with an nd_bus driver
      pmem: clarify a debug print in pmem_clear_poison
      x86/insn: remove pcommit
      Revert "KVM: x86: add pcommit support"
      nfit, tools/testing/nvdimm/: unify shutdown paths
      libnvdimm: move ->module to struct nvdimm_bus_descriptor
      nfit: cleanup acpi_nfit_init calling convention
      nfit: fix _FIT evaluation memory leak + use after free
      tools/testing/nvdimm: add manufacturing_{date|location} dimm properties
      tools/testing/nvdimm: add virtual ramdisk range
      acpi, nfit: treat virtual ramdisk SPA as pmem region
      pmem: kill __pmem address space
      pmem: kill wmb_pmem()
      libnvdimm, pmem: use nvdimm_flush() for namespace I/O writes
      fs/dax: remove wmb_pmem()
      libnvdimm, pmem: flush posted-write queues on shutdown
      ...

commit 3fc9d690936fb2e20e180710965ba2cc3a0881f8
Merge: d05d7f40791c 13880f5b57ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 26 15:37:51 2016 -0700

    Merge branch 'for-4.8/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "This branch also contains core changes.  I've come to the conclusion
      that from 4.9 and forward, I'll be doing just a single branch.  We
      often have dependencies between core and drivers, and it's hard to
      always split them up appropriately without pulling core into drivers
      when that happens.
    
      That said, this contains:
    
       - separate secure erase type for the core block layer, from
         Christoph.
    
       - set of discard fixes, from Christoph.
    
       - bio shrinking fixes from Christoph, as a followup up to the
         op/flags change in the core branch.
    
       - map and append request fixes from Christoph.
    
       - NVMeF (NVMe over Fabrics) code from Christoph.  This is pretty
         exciting!
    
       - nvme-loop fixes from Arnd.
    
       - removal of ->driverfs_dev from Dan, after providing a
         device_add_disk() helper.
    
       - bcache fixes from Bhaktipriya and Yijing.
    
       - cdrom subchannel read fix from Vchannaiah.
    
       - set of lightnvm updates from Wenwei, Matias, Johannes, and Javier.
    
       - set of drbd updates and fixes from Fabian, Lars, and Philipp.
    
       - mg_disk error path fix from Bart.
    
       - user notification for failed device add for loop, from Minfei.
    
       - NVMe in general:
            + NVMe delay quirk from Guilherme.
            + SR-IOV support and command retry limits from Keith.
            + fix for memory-less NUMA node from Masayoshi.
            + use UINT_MAX for discard sectors, from Minfei.
            + cancel IO fixes from Ming.
            + don't allocate unused major, from Neil.
            + error code fixup from Dan.
            + use constants for PSDT/FUSE from James.
            + variable init fix from Jay.
            + fabrics fixes from Ming, Sagi, and Wei.
            + various fixes"
    
    * 'for-4.8/drivers' of git://git.kernel.dk/linux-block: (115 commits)
      nvme/pci: Provide SR-IOV support
      nvme: initialize variable before logical OR'ing it
      block: unexport various bio mapping helpers
      scsi/osd: open code blk_make_request
      target: stop using blk_make_request
      block: simplify and export blk_rq_append_bio
      block: ensure bios return from blk_get_request are properly initialized
      virtio_blk: use blk_rq_map_kern
      memstick: don't allow REQ_TYPE_BLOCK_PC requests
      block: shrink bio size again
      block: simplify and cleanup bvec pool handling
      block: get rid of bio_rw and READA
      block: don't ignore -EOPNOTSUPP blkdev_issue_write_same
      block: introduce BLKDEV_DISCARD_ZERO to fix zeroout
      NVMe: don't allocate unused nvme_major
      nvme: avoid crashes when node 0 is memoryless node.
      nvme: Limit command retries
      loop: Make user notify for adding loop device failed
      nvme-loop: fix nvme-loop Kconfig dependencies
      nvmet: fix return value check in nvmet_subsys_alloc()
      ...

commit 17007f3994cdb4643355c73f54f0adad006cf59e
Author: Damien Le Moal <damien.lemoal@hgst.com>
Date:   Wed Jul 20 21:40:47 2016 -0600

    block: Fix front merge check
    
    For a front merge, the maximum number of sectors of the
    request must be checked against the front merge BIO sector,
    not the current sector of the request.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@hgst.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9d84c98b5c79..48f05d768a53 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -922,7 +922,8 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 			(offset & (q->limits.chunk_sectors - 1));
 }
 
-static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
+static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
+						  sector_t offset)
 {
 	struct request_queue *q = rq->q;
 
@@ -932,7 +933,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	if (!q->limits.chunk_sectors || (req_op(rq) == REQ_OP_DISCARD))
 		return blk_queue_get_max_sectors(q, req_op(rq));
 
-	return min(blk_max_size_offset(q, blk_rq_pos(rq)),
+	return min(blk_max_size_offset(q, offset),
 			blk_queue_get_max_sectors(q, req_op(rq)));
 }
 

commit 163d4baaebe39c0e56d9c08597eab7b3ae0bf334
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Thu Jun 23 17:05:50 2016 -0400

    block: add QUEUE_FLAG_DAX for devices to advertise their DAX support
    
    Currently, presence of direct_access() in block_device_operations
    indicates support of DAX on its block device.  Because
    block_device_operations is instantiated with 'const', this DAX
    capablity may not be enabled conditinally.
    
    In preparation for supporting DAX to device-mapper devices, add
    QUEUE_FLAG_DAX to request_queue flags to advertise their DAX
    support.  This will allow to set the DAX capability based on how
    mapped device is composed.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: <linux-s390@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d116d3b52c73..9d84c98b5c79 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -505,6 +505,7 @@ struct request_queue {
 #define QUEUE_FLAG_WC	       23	/* Write back caching */
 #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
 #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
+#define QUEUE_FLAG_DAX         26	/* device supports DAX */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -594,6 +595,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
 #define blk_queue_secdiscard(q)	(blk_queue_discard(q) && \
 	test_bit(QUEUE_FLAG_SECDISCARD, &(q)->queue_flags))
+#define blk_queue_dax(q)	test_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \

commit 4613c5f1df92f3cb5a8f89c7dfefc37402c16bd8
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 19 11:31:53 2016 +0200

    scsi/osd: open code blk_make_request
    
    I wish the OSD code could simply use blk_rq_map_* helpers like
    everyone else, but the complex nature of deciding if we have
    DATA IN and/or DATA OUT buffers might make this impossible
    (at least for a mere human like me).
    
    But using blk_rq_append_bio at least allows sharing the setup code
    between request with or without dat a buffers, and given that this
    is the last user of blk_make_request it allows getting rid of that
    somewhat awkward interface.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Boaz Harrosh <ooo@electrozaur.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 706c8bf61c84..0cdea75a6614 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -788,8 +788,6 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
-extern struct request *blk_make_request(struct request_queue *, struct bio *,
-					gfp_t);
 extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,

commit 98d61d5b1a65a9df7cb3d9605f5d37d3dbbb4b5e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 19 11:31:51 2016 +0200

    block: simplify and export blk_rq_append_bio
    
    The target SCSI passthrough backend is much better served with the low-level
    blk_rq_append_bio construct then the helpers built on top of it, so export it.
    
    Also use the opportunity to remove the pointless request_queue argument and
    make the code flow a little more readable.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 156455cb07ad..706c8bf61c84 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -802,6 +802,7 @@ extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 extern void blk_rq_unprep_clone(struct request *rq);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
+extern int blk_rq_append_bio(struct request *rq, struct bio *bio);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_queue_split(struct request_queue *, struct bio **,
 			    struct bio_set *);

commit e950fdf71c9b4a6b63b58fed78956a96cc907402
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 19 11:23:33 2016 +0200

    block: introduce BLKDEV_DISCARD_ZERO to fix zeroout
    
    Currently blkdev_issue_zeroout cascades down from discards (if the driver
    guarantees that discards zero data), to WRITE SAME and then to a loop
    writing zeroes.  Unfortunately we ignore run-time EOPNOTSUPP errors in the
    block layer blkdev_issue_discard helper to work around DM volumes that
    may have mixed discard support underneath.
    
    This patch intoroduces a new BLKDEV_DISCARD_ZERO flag to
    blkdev_issue_discard that indicates we are called for zeroing operation.
    This allows both to ignore the EOPNOTSUPP hack and actually consolidating
    the discard_zeroes_data check into the function.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 53fee6123893..156455cb07ad 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1137,7 +1137,9 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 	return bqt->tag_index[tag];
 }
 
-#define BLKDEV_DISCARD_SECURE  0x01    /* secure discard */
+
+#define BLKDEV_DISCARD_SECURE	(1 << 0)	/* issue a secure erase */
+#define BLKDEV_DISCARD_ZERO	(1 << 1)	/* must reliably zero data */
 
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,

commit 7a9eb20666317794d0279843fbd091af93907780
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jun 3 18:06:47 2016 -0700

    pmem: kill __pmem address space
    
    The __pmem address space was meant to annotate codepaths that touch
    persistent memory and need to coordinate a call to wmb_pmem().  Now that
    wmb_pmem() is gone, there is little need to keep this annotation.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3d9cf326574f..fde908b2836b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1659,7 +1659,7 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
  */
 struct blk_dax_ctl {
 	sector_t sector;
-	void __pmem *addr;
+	void *addr;
 	long size;
 	pfn_t pfn;
 };
@@ -1670,8 +1670,8 @@ struct block_device_operations {
 	int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
-	long (*direct_access)(struct block_device *, sector_t, void __pmem **,
-			pfn_t *, long);
+	long (*direct_access)(struct block_device *, sector_t, void **, pfn_t *,
+			long);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */

commit 9828c2c6c1048c61034a8b94e6376aeff6d2284f
Author: Jan Kara <jack@suse.cz>
Date:   Tue Jun 28 09:03:59 2016 +0200

    block: Convert fifo_time from ulong to u64
    
    Currently rq->fifo_time is unsigned long but CFQ stores nanosecond
    timestamp in it which would overflow on 32-bit archs. Convert it to u64
    to avoid the overflow. Since the rq->fifo_time is unioned with struct
    call_single_data(), this does not change the size of struct request in
    any way.
    
    We have to slightly fixup block/deadline-iosched.c so that comparison
    happens in the right types.
    
    Fixes: 9a7f38c42c2b92391d9dabaf9f51df7cfe5608e4
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9746d223494c..d116d3b52c73 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -90,7 +90,7 @@ struct request {
 	struct list_head queuelist;
 	union {
 		struct call_single_data csd;
-		unsigned long fifo_time;
+		u64 fifo_time;
 	};
 
 	struct request_queue *q;

commit 288dab8a35a0bde426a09870943c8d3ee3a50dab
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 9 16:00:36 2016 +0200

    block: add a separate operation type for secure erase
    
    Instead of overloading the discard support with the REQ_SECURE flag.
    Use the opportunity to rename the queue flag as well, and remove the
    dead checks for this flag in the RAID 1 and RAID 10 drivers that don't
    claim support for secure erase.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0c9f8793c87e..53fee6123893 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -497,7 +497,7 @@ struct request_queue {
 #define QUEUE_FLAG_DISCARD     14	/* supports DISCARD */
 #define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
-#define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
+#define QUEUE_FLAG_SECERASE    17	/* supports secure erase */
 #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
@@ -593,8 +593,8 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
-#define blk_queue_secdiscard(q)	(blk_queue_discard(q) && \
-	test_bit(QUEUE_FLAG_SECDISCARD, &(q)->queue_flags))
+#define blk_queue_secure_erase(q) \
+	(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
@@ -675,21 +675,6 @@ static inline bool rq_mergeable(struct request *rq)
 	return true;
 }
 
-static inline bool blk_check_merge_flags(unsigned int flags1, unsigned int op1,
-					 unsigned int flags2, unsigned int op2)
-{
-	if ((op1 == REQ_OP_DISCARD) != (op2 == REQ_OP_DISCARD))
-		return false;
-
-	if ((flags1 & REQ_SECURE) != (flags2 & REQ_SECURE))
-		return false;
-
-	if ((op1 == REQ_OP_WRITE_SAME) != (op2 == REQ_OP_WRITE_SAME))
-		return false;
-
-	return true;
-}
-
 static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
 {
 	if (bio_data(a) == bio_data(b))
@@ -1158,7 +1143,7 @@ extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
-		sector_t nr_sects, gfp_t gfp_mask, int op_flags,
+		sector_t nr_sects, gfp_t gfp_mask, int flags,
 		struct bio **biop);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);

commit ca93e45347e61cd85e5d71961f6c94fad143593d
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 9 16:00:35 2016 +0200

    block: better packing for struct request
    
    Keep the 32-bit CPU and cmd_type flags together to avoid holes on 64-bit
    architectures.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0c9f8793c87e..9746d223494c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -96,12 +96,11 @@ struct request {
 	struct request_queue *q;
 	struct blk_mq_ctx *mq_ctx;
 
-	u64 cmd_flags;
+	int cpu;
 	unsigned cmd_type;
+	u64 cmd_flags;
 	unsigned long atomic_flags;
 
-	int cpu;
-
 	/* the following two fields are internal, NEVER access directly */
 	unsigned int __data_len;	/* total data len */
 	sector_t __sector;		/* sector cursor */

commit 3a5e02ced11e22ecd9da3d6710afe15bcfee1d10
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:32:23 2016 -0500

    block, drivers: add REQ_OP_FLUSH operation
    
    This adds a REQ_OP_FLUSH operation that is sent to request_fn
    based drivers by the block layer's flush code, instead of
    sending requests with the request->cmd_flags REQ_FLUSH bit set.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 78ae3dbf2de1..0c9f8793c87e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -666,6 +666,9 @@ static inline bool rq_mergeable(struct request *rq)
 	if (rq->cmd_type != REQ_TYPE_FS)
 		return false;
 
+	if (req_op(rq) == REQ_OP_FLUSH)
+		return false;
+
 	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
 		return false;
 

commit 4e1b2d52a80d79296a5d899d73249748dea71a53
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:32:22 2016 -0500

    block, fs, drivers: remove REQ_OP compat defs and related code
    
    This patch drops the compat definition of req_op where it matches
    the rq_flag_bits definitions, and drops the related old and compat
    code that allowed users to set either the op or flags for the operation.
    
    We also then store the operation in the bi_rw/cmd_flags field similar
    to how we used to store the bio ioprio where it sat in the upper bits
    of the field.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4937c056fcbd..78ae3dbf2de1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -200,8 +200,15 @@ struct request {
 	struct request *next_rq;
 };
 
-#define req_op(req)		(op_from_rq_bits((req)->cmd_flags))
-#define req_set_op(req, op)	((req)->cmd_flags |= op)
+#define REQ_OP_SHIFT (8 * sizeof(u64) - REQ_OP_BITS)
+#define req_op(req)  ((req)->cmd_flags >> REQ_OP_SHIFT)
+
+#define req_set_op(req, op) do {				\
+	WARN_ON(op >= (1 << REQ_OP_BITS));			\
+	(req)->cmd_flags &= ((1ULL << REQ_OP_SHIFT) - 1);	\
+	(req)->cmd_flags |= ((u64) (op) << REQ_OP_SHIFT);	\
+} while (0)
+
 #define req_set_op_attrs(req, op, flags) do {	\
 	req_set_op(req, op);			\
 	(req)->cmd_flags |= flags;		\
@@ -604,8 +611,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
-#define rq_data_dir(rq) \
-	(op_is_write(op_from_rq_bits(rq->cmd_flags)) ? WRITE : READ)
+#define rq_data_dir(rq)		(op_is_write(req_op(rq)) ? WRITE : READ)
 
 /*
  * Driver can handle struct request, if it either has an old style

commit d9d8c5c489f4969667a05727e9c2c4f78cffef1a
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:32:16 2016 -0500

    block: convert is_sync helpers to use REQ_OPs.
    
    This patch converts the is_sync helpers to use separate variables
    for the operation and flags.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25f01ff19780..4937c056fcbd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -624,14 +624,14 @@ static inline unsigned int blk_queue_cluster(struct request_queue *q)
 /*
  * We regard a request as sync, if either a read or a sync write
  */
-static inline bool rw_is_sync(unsigned int rw_flags)
+static inline bool rw_is_sync(int op, unsigned int rw_flags)
 {
-	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
+	return op == REQ_OP_READ || (rw_flags & REQ_SYNC);
 }
 
 static inline bool rq_is_sync(struct request *rq)
 {
-	return rw_is_sync(rq->cmd_flags);
+	return rw_is_sync(req_op(rq), rq->cmd_flags);
 }
 
 static inline bool blk_rl_full(struct request_list *rl, bool sync)

commit 8fe0d473f5477e9916d3ac581a226acfe83142be
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:32:15 2016 -0500

    block: convert merge/insert code to check for REQ_OPs.
    
    This patch converts the block layer merging code to use separate variables
    for the operation and flags, and to check req_op for the REQ_OP.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8c78aca080af..25f01ff19780 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -666,16 +666,16 @@ static inline bool rq_mergeable(struct request *rq)
 	return true;
 }
 
-static inline bool blk_check_merge_flags(unsigned int flags1,
-					 unsigned int flags2)
+static inline bool blk_check_merge_flags(unsigned int flags1, unsigned int op1,
+					 unsigned int flags2, unsigned int op2)
 {
-	if ((flags1 & REQ_DISCARD) != (flags2 & REQ_DISCARD))
+	if ((op1 == REQ_OP_DISCARD) != (op2 == REQ_OP_DISCARD))
 		return false;
 
 	if ((flags1 & REQ_SECURE) != (flags2 & REQ_SECURE))
 		return false;
 
-	if ((flags1 & REQ_WRITE_SAME) != (flags2 & REQ_WRITE_SAME))
+	if ((op1 == REQ_OP_WRITE_SAME) != (op2 == REQ_OP_WRITE_SAME))
 		return false;
 
 	return true;
@@ -887,12 +887,12 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 }
 
 static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
-						     unsigned int cmd_flags)
+						     int op)
 {
-	if (unlikely(cmd_flags & REQ_DISCARD))
+	if (unlikely(op == REQ_OP_DISCARD))
 		return min(q->limits.max_discard_sectors, UINT_MAX >> 9);
 
-	if (unlikely(cmd_flags & REQ_WRITE_SAME))
+	if (unlikely(op == REQ_OP_WRITE_SAME))
 		return q->limits.max_write_same_sectors;
 
 	return q->limits.max_sectors;
@@ -919,11 +919,11 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	if (unlikely(rq->cmd_type != REQ_TYPE_FS))
 		return q->limits.max_hw_sectors;
 
-	if (!q->limits.chunk_sectors || (rq->cmd_flags & REQ_DISCARD))
-		return blk_queue_get_max_sectors(q, rq->cmd_flags);
+	if (!q->limits.chunk_sectors || (req_op(rq) == REQ_OP_DISCARD))
+		return blk_queue_get_max_sectors(q, req_op(rq));
 
 	return min(blk_max_size_offset(q, blk_rq_pos(rq)),
-			blk_queue_get_max_sectors(q, rq->cmd_flags));
+			blk_queue_get_max_sectors(q, req_op(rq)));
 }
 
 static inline unsigned int blk_rq_count_bios(struct request *rq)

commit 469e3216e20a3946a292ff0414ab86de408d465e
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:49 2016 -0500

    block discard: use bio set op accessor
    
    This converts the block issue discard helper and users to use
    the bio_set_op_attrs accessor and only pass in the operation flags
    like REQ_SEQURE.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49c2dbcad583..8c78aca080af 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1149,7 +1149,8 @@ extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
-		sector_t nr_sects, gfp_t gfp_mask, int type, struct bio **biop);
+		sector_t nr_sects, gfp_t gfp_mask, int op_flags,
+		struct bio **biop);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,

commit f21508211d2b16e65821abd171378fa6ece126fe
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:42 2016 -0500

    block: add REQ_OP definitions and helpers
    
    The following patches separate the operation (WRITE, READ, DISCARD,
    etc) from the rq_flag_bits flags. This patch adds definitions for
    request/bio operations (REQ_OPs) and adds request/bio accessors to
    get/set the op.
    
    In this patch the REQ_OPs match the REQ rq_flag_bits ones
    for compat reasons while all the code is converted to use the
    op accessors in the set. In the last patches the op will become a
    number and the accessors and helpers in this patch will be dropped
    or updated.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3d9cf326574f..49c2dbcad583 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -200,6 +200,13 @@ struct request {
 	struct request *next_rq;
 };
 
+#define req_op(req)		(op_from_rq_bits((req)->cmd_flags))
+#define req_set_op(req, op)	((req)->cmd_flags |= op)
+#define req_set_op_attrs(req, op, flags) do {	\
+	req_set_op(req, op);			\
+	(req)->cmd_flags |= flags;		\
+} while (0)
+
 static inline unsigned short req_get_ioprio(struct request *req)
 {
 	return req->ioprio;
@@ -597,7 +604,8 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
-#define rq_data_dir(rq)		((int)((rq)->cmd_flags & 1))
+#define rq_data_dir(rq) \
+	(op_is_write(op_from_rq_bits(rq->cmd_flags)) ? WRITE : READ)
 
 /*
  * Driver can handle struct request, if it either has an old style

commit 315227f6da389f3a560f27f7777080857278e1b4
Merge: a10c38a4f385 40543f62cbdc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 26 19:34:26 2016 -0700

    Merge tag 'dax-misc-for-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull misc DAX updates from Vishal Verma:
     "DAX error handling for 4.7
    
       - Until now, dax has been disabled if media errors were found on any
         device.  This enables the use of DAX in the presence of these
         errors by making all sector-aligned zeroing go through the driver.
    
       - The driver (already) has the ability to clear errors on writes that
         are sent through the block layer using 'DSMs' defined in ACPI 6.1.
    
      Other misc changes:
    
       - When mounting DAX filesystems, check to make sure the partition is
         page aligned.  This is a requirement for DAX, and previously, we
         allowed such unaligned mounts to succeed, but subsequent
         reads/writes would fail.
    
       - Misc/cleanup fixes from Jan that remove unused code from DAX
         related to zeroing, writeback, and some size checks"
    
    * tag 'dax-misc-for-4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm:
      dax: fix a comment in dax_zero_page_range and dax_truncate_page
      dax: for truncate/hole-punch, do zeroing through the driver if possible
      dax: export a low-level __dax_zero_page_range helper
      dax: use sb_issue_zerout instead of calling dax_clear_sectors
      dax: enable dax in the presence of known media errors (badblocks)
      dax: fallback from pmd to pte on error
      block: Update blkdev_dax_capable() for consistency
      xfs: Add alignment check for DAX mount
      ext2: Add alignment check for DAX mount
      ext4: Add alignment check for DAX mount
      block: Add bdev_dax_supported() for dax mount checks
      block: Add vfs_msg() interface
      dax: Remove redundant inode size checks
      dax: Remove pointless writeback from dax_do_io()
      dax: Remove zeroing from dax_io()
      dax: Remove dead zeroing code from fault handlers
      ext2: Avoid DAX zeroing to corrupt data
      ext2: Fix block zeroing in ext2_get_blocks() for DAX
      dax: Remove complete_unwritten argument
      DAX: move RADIX_DAX_ definitions to dax.c

commit 0a70bd43053331d99881211e1d09f32de531432f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Feb 24 14:02:11 2016 -0800

    dax: enable dax in the presence of known media errors (badblocks)
    
    1/ If a mapping overlaps a bad sector fail the request.
    
    2/ Do not opportunistically report more dax-capable capacity than is
       requested when errors present.
    
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    [vishal: fix a conflict with system RAM collision patches]
    [vishal: add a 'size' parameter to ->direct_access]
    [vishal: fix a conflict with DAX alignment check patches]
    Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 27cbefe8c985..cf7c13c2c38d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1668,7 +1668,7 @@ struct block_device_operations {
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	long (*direct_access)(struct block_device *, sector_t, void __pmem **,
-			pfn_t *);
+			pfn_t *, long);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */

commit 24b9f0cf00c8e8df29a4ddfec8c139ad62753113
Merge: a4d1dbed0e27 116f7d4a21fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 17 16:03:32 2016 -0700

    Merge branch 'for-4.7/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "On top of the core pull request, this is the drivers pull request for
      this merge window.  This contains:
    
       - Switch drivers to the new write back cache API, and kill off the
         flush flags.  From me.
    
       - Kill the discard support for the STEC pci-e flash driver.  It's
         trivially broken, and apparently unmaintained, so it's safer to
         just remove it.  From Jeff Moyer.
    
       - A set of lightnvm updates from the usual suspects (Matias/Javier,
         and Simon), and fixes from Arnd, Jeff Mahoney, Sagi, and Wenwei
         Tao.
    
       - A set of updates for NVMe:
    
            - Turn the controller state management into a proper state
              machine.  From Christoph.
    
            - Shuffling of code in preparation for NVMe-over-fabrics, also
              from Christoph.
    
            - Cleanup of the command prep part from Ming Lin.
    
            - Rewrite of the discard support from Ming Lin.
    
            - Deadlock fix for namespace removal from Ming Lin.
    
            - Use the now exported blk-mq tag helper for IO termination.
              From Sagi.
    
            - Various little fixes from Christoph, Guilherme, Keith, Ming
              Lin, Wang Sheng-Hui.
    
       - Convert mtip32xx to use the now exported blk-mq tag iter function,
         from Keith"
    
    * 'for-4.7/drivers' of git://git.kernel.dk/linux-block: (74 commits)
      lightnvm: reserved space calculation incorrect
      lightnvm: rename nr_pages to nr_ppas on nvm_rq
      lightnvm: add is_cached entry to struct ppa_addr
      lightnvm: expose gennvm_mark_blk to targets
      lightnvm: remove mgt targets on mgt removal
      lightnvm: pass dma address to hardware rather than pointer
      lightnvm: do not assume sequential lun alloc.
      nvme/lightnvm: Log using the ctrl named device
      lightnvm: rename dma helper functions
      lightnvm: enable metadata to be sent to device
      lightnvm: do not free unused metadata on rrpc
      lightnvm: fix out of bound ppa lun id on bb tbl
      lightnvm: refactor set_bb_tbl for accepting ppa list
      lightnvm: move responsibility for bad blk mgmt to target
      lightnvm: make nvm_set_rqd_ppalist() aware of vblks
      lightnvm: remove struct factory_blks
      lightnvm: refactor device ops->get_bb_tbl()
      lightnvm: introduce nvm_for_each_lun_ppa() macro
      lightnvm: refactor dev->online_target to global nvm_targets
      lightnvm: rename nvm_targets to nvm_tgt_type
      ...

commit a8078b1fc616da6112eb95f0063cd34531d4ccf0
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Tue May 10 10:23:57 2016 -0600

    block: Update blkdev_dax_capable() for consistency
    
    blkdev_dax_capable() is similar to bdev_dax_supported(), but needs
    to remain as a separate interface for checking dax capability of
    a raw block device.
    
    Rename and relocate blkdev_dax_capable() to keep them maintained
    consistently, and call bdev_direct_access() for the dax capability
    check.
    
    There is no change in the behavior.
    
    Link: https://lkml.org/lkml/2016/5/9/950
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Boaz Harrosh <boaz@plexistor.com>
    Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 71231a55debd..27cbefe8c985 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1689,6 +1689,7 @@ extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
 extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
 extern int bdev_dax_supported(struct super_block *, int);
+extern bool bdev_dax_capable(struct block_device *);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 2d96afc8f70ef86c66a0b5d80c24a27d6dd13df3
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Tue May 10 10:23:53 2016 -0600

    block: Add bdev_dax_supported() for dax mount checks
    
    DAX imposes additional requirements to a device.  Add
    bdev_dax_supported() which performs all the precondition checks
    necessary for filesystem to mount the device with dax option.
    
    Also add a new check to verify if a partition is aligned by 4KB.
    When a partition is unaligned, any dax read/write access fails,
    except for metadata update.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Boaz Harrosh <boaz@plexistor.com>
    Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 78c48ab22f46..71231a55debd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1688,6 +1688,7 @@ extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
 extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
+extern int bdev_dax_supported(struct super_block *, int);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 2af3a8159cd204fc8437ed2f75863f0fb930f0d0
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Tue May 10 10:23:52 2016 -0600

    block: Add vfs_msg() interface
    
    In preparation of moving DAX capability checks to the block layer
    from filesystem code, add a VFS message interface that aligns with
    filesystem's message format.
    
    For instance, a vfs_msg() message followed by XFS messages in case
    of a dax mount error may look like:
    
      VFS (pmem0p1): error: unaligned partition for dax
      XFS (pmem0p1): DAX unsupported by block device. Turning off DAX.
      XFS (pmem0p1): Mounting V5 Filesystem
       :
    
    vfs_msg() is largely based on ext4_msg().
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Boaz Harrosh <boaz@plexistor.com>
    Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 669e419d6234..78c48ab22f46 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -767,6 +767,17 @@ static inline void rq_flush_dcache_pages(struct request *rq)
 }
 #endif
 
+#ifdef CONFIG_PRINTK
+#define vfs_msg(sb, level, fmt, ...)				\
+	__vfs_msg(sb, level, fmt, ##__VA_ARGS__)
+#else
+#define vfs_msg(sb, level, fmt, ...)				\
+do {								\
+	no_printk(fmt, ##__VA_ARGS__);				\
+	__vfs_msg(sb, "", " ");					\
+} while (0)
+#endif
+
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern blk_qc_t generic_make_request(struct bio *bio);

commit 38f252553300ee1d3346a5273e95fe1dd60ca50a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Apr 16 14:55:28 2016 -0400

    block: add __blkdev_issue_discard
    
    This is a version of blkdev_issue_discard which doesn't wait for
    the I/O to complete, but instead allows the caller to submit
    the final bio and/or chain it to others.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Sagi Grimberg <sagig@grimberg.me>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba72687c5654..b79131acf6c0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1131,6 +1131,8 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
+extern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
+		sector_t nr_sects, gfp_t gfp_mask, int type, struct bio **biop);
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,

commit c888a8f95ae5b1067855235b3b71c1ebccf504f5
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 13 13:33:19 2016 -0600

    block: kill off q->flush_flags
    
    Now that we converted everything to the newer block write cache
    interface, kill off the queue flush_flags and queueable flush
    entries.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3f232fa505d..57c085917da6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -433,8 +433,6 @@ struct request_queue {
 	/*
 	 * for flush operations
 	 */
-	unsigned int		flush_flags;
-	unsigned int		flush_not_queueable:1;
 	struct blk_flush_queue	*fq;
 
 	struct list_head	requeue_list;
@@ -493,6 +491,7 @@ struct request_queue {
 #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
 #define QUEUE_FLAG_WC	       23	/* Write back caching */
 #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
+#define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -1365,7 +1364,7 @@ static inline unsigned int block_size(struct block_device *bdev)
 
 static inline bool queue_flush_queueable(struct request_queue *q)
 {
-	return !q->flush_not_queueable;
+	return !test_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);
 }
 
 typedef struct {struct page *v;} Sector;

commit 2245f6de6c68b225986229a2de78c240536f7f38
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Mar 30 10:19:30 2016 -0600

    block: kill blk_queue_flush()
    
    We don't have any drivers left using it, so kill it off. Update
    documentation to use the newer blk_queue_write_cache().
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba72687c5654..f3f232fa505d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1009,7 +1009,6 @@ extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
-extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);

commit 93e9d8e836cb1a9a58b33eb6643bf061c6119ef2
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 12 12:32:46 2016 -0600

    block: add ability to flag write back caching on a device
    
    Add an internal helper and flag for setting whether a queue has
    write back caching, or write through (or none). Add a sysfs file
    to show this as well, and make it changeable from user space.
    
    This will replace the (awkward) blk_queue_flush() interface that
    drivers currently use to inform the block layer of write cache state
    and capabilities.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bbaa76757018..ba72687c5654 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -491,6 +491,8 @@ struct request_queue {
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
 #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
+#define QUEUE_FLAG_WC	       23	/* Write back caching */
+#define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -1009,6 +1011,7 @@ extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
+extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);

commit 37e58237a16b94fcd2c2d1b7e9c6e1ca661c231b
Author: Ming Lin <ming.l@ssi.samsung.com>
Date:   Tue Mar 22 00:24:44 2016 -0700

    block: add offset in blk_add_request_payload()
    
    We could kmalloc() the payload, so need the offset in page.
    
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 669e419d6234..bbaa76757018 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -779,7 +779,7 @@ extern struct request *blk_make_request(struct request_queue *, struct bio *,
 extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,
-		unsigned int len);
+		int offset, unsigned int len);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 			     struct bio_set *bs, gfp_t gfp_mask,

commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:47 2016 +0300

    mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros
    
    PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
    ago with promise that one day it will be possible to implement page
    cache with bigger chunks than PAGE_SIZE.
    
    This promise never materialized.  And unlikely will.
    
    We have many places where PAGE_CACHE_SIZE assumed to be equal to
    PAGE_SIZE.  And it's constant source of confusion on whether
    PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
    especially on the border between fs and mm.
    
    Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
    breakage to be doable.
    
    Let's stop pretending that pages in page cache are special.  They are
    not.
    
    The changes are pretty straight-forward:
    
     - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};
    
     - page_cache_get() -> get_page();
    
     - page_cache_release() -> put_page();
    
    This patch contains automated changes generated with coccinelle using
    script below.  For some reason, coccinelle doesn't patch header files.
    I've called spatch for them manually.
    
    The only adjustment after coccinelle is revert of changes to
    PAGE_CAHCE_ALIGN definition: we are going to drop it later.
    
    There are few places in the code where coccinelle didn't reach.  I'll
    fix them manually in a separate patch.  Comments and documentation also
    will be addressed with the separate patch.
    
    virtual patch
    
    @@
    expression E;
    @@
    - E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    expression E;
    @@
    - E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    @@
    - PAGE_CACHE_SHIFT
    + PAGE_SHIFT
    
    @@
    @@
    - PAGE_CACHE_SIZE
    + PAGE_SIZE
    
    @@
    @@
    - PAGE_CACHE_MASK
    + PAGE_MASK
    
    @@
    expression E;
    @@
    - PAGE_CACHE_ALIGN(E)
    + PAGE_ALIGN(E)
    
    @@
    expression E;
    @@
    - page_cache_get(E)
    + get_page(E)
    
    @@
    expression E;
    @@
    - page_cache_release(E)
    + put_page(E)
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7e5d7e018bea..669e419d6234 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1372,7 +1372,7 @@ unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);
 
 static inline void put_dev_sector(Sector p)
 {
-	page_cache_release(p.v);
+	put_page(p.v);
 }
 
 static inline bool __bvec_gap_to_prev(struct request_queue *q,

commit 3c2de27d793bf55167804fc47954711e94f27be7
Merge: 51b3eae8dbe5 8b23a8ce1094
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 18:52:29 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
    
     - Preparations of parallel lookups (the remaining main obstacle is the
       need to move security_d_instantiate(); once that becomes safe, the
       rest will be a matter of rather short series local to fs/*.c
    
     - preadv2/pwritev2 series from Christoph
    
     - assorted fixes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (32 commits)
      splice: handle zero nr_pages in splice_to_pipe()
      vfs: show_vfsstat: do not ignore errors from show_devname method
      dcache.c: new helper: __d_add()
      don't bother with __d_instantiate(dentry, NULL)
      untangle fsnotify_d_instantiate() a bit
      uninline d_add()
      replace d_add_unique() with saner primitive
      quota: use lookup_one_len_unlocked()
      cifs_get_root(): use lookup_one_len_unlocked()
      nfs_lookup: don't bother with d_instantiate(dentry, NULL)
      kill dentry_unhash()
      ceph_fill_trace(): don't bother with d_instantiate(dn, NULL)
      autofs4: don't bother with d_instantiate(dentry, NULL) in ->lookup()
      configfs: move d_rehash() into configfs_create() for regular files
      ceph: don't bother with d_rehash() in splice_dentry()
      namei: teach lookup_slow() to skip revalidate
      namei: massage lookup_slow() to be usable by lookup_one_len_unlocked()
      lookup_one_len_unlocked(): use lookup_dcache()
      namei: simplify invalidation logics in lookup_dcache()
      namei: change calling conventions for lookup_{fast,slow} and follow_managed()
      ...

commit fcab86add71623e3963d7565c0d61bb9d99aea7c
Merge: ef504fa591aa 8134233e8d34
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 18 20:06:46 2016 -0700

    Merge branch 'for-4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/libata
    
    Pull libata updates from Tejun Heo:
    
     - ahci grew runtime power management support so that the controller can
       be turned off if no devices are attached.
    
     - sata_via isn't dead yet.  It got hotplug support and more refined
       workaround for certain WD drives.
    
     - Misc cleanups.  There's a merge from for-4.5-fixes to avoid confusing
       conflicts in ahci PCI ID table.
    
    * 'for-4.6' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/libata:
      ata: ahci_xgene: dereferencing uninitialized pointer in probe
      AHCI: Remove obsolete Intel Lewisburg SATA RAID device IDs
      ata: sata_rcar: Use ARCH_RENESAS
      sata_via: Implement hotplug for VT6421
      sata_via: Apply WD workaround only when needed on VT6421
      ahci: Add runtime PM support for the host controller
      ahci: Add functions to manage runtime PM of AHCI ports
      ahci: Convert driver to use modern PM hooks
      ahci: Cache host controller version
      scsi: Drop runtime PM usage count after host is added
      scsi: Set request queue runtime PM status back to active on resume
      block: Add blk_set_runtime_active()
      ata: ahci_mvebu: add support for Armada 3700 variant
      libata: fix unbalanced spin_lock_irqsave/spin_unlock_irq() in ata_scsi_park_show()
      libata: support AHCI on OCTEON platform

commit 8b23a8ce1094f25a85826e25217c5b9779a4f5aa
Merge: f93812846f31 ed782b5a70a0 d6785d915214 8e0b60b96ba0
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Mar 18 16:07:38 2016 -0400

    Merge branches 'work.lookups', 'work.misc' and 'work.preadv2' into for-next

commit 8e0b60b96ba06d826a2b26e23b1986853a4e5291
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Mar 3 16:04:03 2016 +0100

    blk-mq: enable polling support by default
    
    Now that applications need to explicitly ask for polling we can enable it
    by default in blk-mq drivers.  Note that this will only have an affect
    on driver that supply a poll function, which currently only includes nvme.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Stephen Bates <stephen.bates@pmcs.com>
    Tested-by: Stephen Bates <stephen.bates@pmcs.com>
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 29189aeace19..ff33a1ab58c9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -499,7 +499,8 @@ struct request_queue {
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
-				 (1 << QUEUE_FLAG_SAME_COMP))
+				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
+				 (1 << QUEUE_FLAG_POLL))
 
 static inline void queue_lockdep_assert_held(struct request_queue *q)
 {

commit f21018427cb007a0894c36ad702990ab639cbbb4
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Mar 3 14:43:45 2016 -0700

    block: fix blk_rq_get_max_sectors for driver private requests
    
    Driver private request types should not get the artifical cap for the
    FS requests.  This is important to use the full device capabilities
    for internal command or NVMe pass through commands.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reported-by: Jeff Lien <Jeff.Lien@hgst.com>
    Tested-by: Jeff Lien <Jeff.Lien@hgst.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    
    Updated by me to use an explicit check for the one command type that
    does support extended checking, instead of relying on the ordering
    of the enum command values - as suggested by Keith.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d7f6bca707ef..413c84fbc4ed 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -895,7 +895,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 
-	if (unlikely(rq->cmd_type == REQ_TYPE_BLOCK_PC))
+	if (unlikely(rq->cmd_type != REQ_TYPE_FS))
 		return q->limits.max_hw_sectors;
 
 	if (!q->limits.chunk_sectors || (rq->cmd_flags & REQ_DISCARD))

commit 25e71a99f10e444cd00bb2ebccb11e1c9fb672b1
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Feb 26 23:40:52 2016 +0800

    block: get the 1st and last bvec via helpers
    
    This patch applies the two introduced helpers to
    figure out the 1st and last bvec, and fixes the
    original way after bio splitting.
    
    Cc: stable@vger.kernel.org
    Reported-by: Sagi Grimberg <sagig@dev.mellanox.co.il>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cd06a41581b3..d7f6bca707ef 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1394,11 +1394,16 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
 			 struct bio *next)
 {
-	if (!bio_has_data(prev) || !queue_virt_boundary(q))
-		return false;
+	if (bio_has_data(prev) && queue_virt_boundary(q)) {
+		struct bio_vec pb, nb;
+
+		bio_get_last_bvec(prev, &pb);
+		bio_get_first_bvec(next, &nb);
 
-	return __bvec_gap_to_prev(q, &prev->bi_io_vec[prev->bi_vcnt - 1],
-				  next->bi_io_vec[0].bv_offset);
+		return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
+	}
+
+	return false;
 }
 
 static inline bool req_gap_back_merge(struct request *req, struct bio *bio)

commit e0af29171aa8912e1ca95023b75ef336cd70d661
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Feb 26 23:40:51 2016 +0800

    block: check virt boundary in bio_will_gap()
    
    In the following patch, the way for figuring out
    the last bvec will be changed with a bit cost introduced,
    so return immediately if the queue doesn't have virt
    boundary limit. Actually most of devices have not
    this limit.
    
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4571ef1a12a9..cd06a41581b3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1372,6 +1372,13 @@ static inline void put_dev_sector(Sector p)
 	page_cache_release(p.v);
 }
 
+static inline bool __bvec_gap_to_prev(struct request_queue *q,
+				struct bio_vec *bprv, unsigned int offset)
+{
+	return offset ||
+		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
+}
+
 /*
  * Check if adding a bio_vec after bprv with offset would create a gap in
  * the SG list. Most drivers don't care about this, but some do.
@@ -1381,18 +1388,17 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 {
 	if (!queue_virt_boundary(q))
 		return false;
-	return offset ||
-		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
+	return __bvec_gap_to_prev(q, bprv, offset);
 }
 
 static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
 			 struct bio *next)
 {
-	if (!bio_has_data(prev))
+	if (!bio_has_data(prev) || !queue_virt_boundary(q))
 		return false;
 
-	return bvec_gap_to_prev(q, &prev->bi_io_vec[prev->bi_vcnt - 1],
-				next->bi_io_vec[0].bv_offset);
+	return __bvec_gap_to_prev(q, &prev->bi_io_vec[prev->bi_vcnt - 1],
+				  next->bi_io_vec[0].bv_offset);
 }
 
 static inline bool req_gap_back_merge(struct request *req, struct bio *bio)

commit d07ab6d114774d7fcb53c57d7474aef459713451
Author: Mika Westerberg <mika.westerberg@linux.intel.com>
Date:   Thu Feb 18 10:54:11 2016 +0200

    block: Add blk_set_runtime_active()
    
    If block device is left runtime suspended during system suspend, resume
    hook of the driver typically corrects runtime PM status of the device back
    to "active" after it is resumed. However, this is not enough as queue's
    runtime PM status is still "suspended". As long as it is in this state
    blk_pm_peek_request() returns NULL and thus prevents new requests to be
    processed.
    
    Add new function blk_set_runtime_active() that can be used to force the
    queue status back to "active" as needed.
    
    Signed-off-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 29189aeace19..8a77c47f8d21 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1026,6 +1026,7 @@ extern int blk_pre_runtime_suspend(struct request_queue *q);
 extern void blk_post_runtime_suspend(struct request_queue *q, int err);
 extern void blk_pre_runtime_resume(struct request_queue *q);
 extern void blk_post_runtime_resume(struct request_queue *q, int err);
+extern void blk_set_runtime_active(struct request_queue *q);
 #else
 static inline void blk_pm_runtime_init(struct request_queue *q,
 	struct device *dev) {}
@@ -1036,6 +1037,7 @@ static inline int blk_pre_runtime_suspend(struct request_queue *q)
 static inline void blk_post_runtime_suspend(struct request_queue *q, int err) {}
 static inline void blk_pre_runtime_resume(struct request_queue *q) {}
 static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
+extern inline void blk_set_runtime_active(struct request_queue *q) {}
 #endif
 
 /*

commit 12ffbbe94d8c0186daccc2e61d5ff87b15aa7bc6
Merge: 6344a5cd3e8a 0fb5b1fb30fb
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Feb 4 21:37:52 2016 -0800

    Merge remote-tracking branch 'mkp-scsi/4.5/scsi-fixes' into fixes

commit 0fb5b1fb30fba3671dd5b1489d78e93e08d62e4e
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Thu Feb 4 00:52:12 2016 -0500

    block/sd: Return -EREMOTEIO when WRITE SAME and DISCARD are disabled
    
    When a storage device rejects a WRITE SAME command we will disable write
    same functionality for the device and return -EREMOTEIO to the block
    layer. -EREMOTEIO will in turn prevent DM from retrying the I/O and/or
    failing the path.
    
    Yiwen Jiang discovered a small race where WRITE SAME requests issued
    simultaneously would cause -EIO to be returned. This happened because
    any requests being prepared after WRITE SAME had been disabled for the
    device caused us to return BLKPREP_KILL. The latter caused the block
    layer to return -EIO upon completion.
    
    To overcome this we introduce BLKPREP_INVALID which indicates that this
    is an invalid request for the device. blk_peek_request() is modified to
    return -EREMOTEIO in that case.
    
    Reported-by: Yiwen Jiang <jiangyiwen@huawei.com>
    Suggested-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Hannes Reinicke <hare@suse.de>
    Reviewed-by: Ewan Milne <emilne@redhat.com>
    Reviewed-by: Yiwen Jiang <jiangyiwen@huawei.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d372ea87ead5..a9b643a0647f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -681,9 +681,12 @@ static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
 /*
  * q->prep_rq_fn return values
  */
-#define BLKPREP_OK		0	/* serve it */
-#define BLKPREP_KILL		1	/* fatal error, kill */
-#define BLKPREP_DEFER		2	/* leave on queue */
+enum {
+	BLKPREP_OK,		/* serve it */
+	BLKPREP_KILL,		/* fatal error, kill, return -EIO */
+	BLKPREP_DEFER,		/* leave on queue */
+	BLKPREP_INVALID,	/* invalid command, kill, return -EREMOTEIO */
+};
 
 extern unsigned long blk_max_low_pfn, blk_max_pfn;
 

commit 3e1e21c7bfcfa9bf06c07f48a13faca2f62b3339
Merge: 0a13daedf7ff a9cf8284b451
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 21 19:58:02 2016 -0800

    Merge branch 'for-4.5/nvme' of git://git.kernel.dk/linux-block
    
    Pull NVMe updates from Jens Axboe:
     "Last branch for this series is the nvme changes.  It's in a separate
      branch to avoid splitting too much between core and NVMe changes,
      since NVMe is still helping drive some blk-mq changes.  That said, not
      a huge amount of core changes in here.  The grunt of the work is the
      continued split of the code"
    
    * 'for-4.5/nvme' of git://git.kernel.dk/linux-block: (67 commits)
      uapi: update install list after nvme.h rename
      NVMe: Export NVMe attributes to sysfs group
      NVMe: Shutdown controller only for power-off
      NVMe: IO queue deletion re-write
      NVMe: Remove queue freezing on resets
      NVMe: Use a retryable error code on reset
      NVMe: Fix admin queue ring wrap
      nvme: make SG_IO support optional
      nvme: fixes for NVME_IOCTL_IO_CMD on the char device
      nvme: synchronize access to ctrl->namespaces
      nvme: Move nvme_freeze/unfreeze_queues to nvme core
      PCI/AER: include header file
      NVMe: Export namespace attributes to sysfs
      NVMe: Add pci error handlers
      block: remove REQ_NO_TIMEOUT flag
      nvme: merge iod and cmd_info
      nvme: meta_sg doesn't have to be an array
      nvme: properly free resources for cancelled command
      nvme: simplify completion handling
      nvme: special case AEN requests
      ...

commit 7c24d9f3b27b198c3c4dfc8327a25fb077a96219
Merge: 99e38df89223 ed8a9d2c816e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 19 15:03:34 2016 -0800

    Merge branch 'for-4.5/core' of git://git.kernel.dk/linux-block
    
    Pull core block updates from Jens Axboe:
     "We don't have a lot of core changes this time around, it's mostly in
      drivers, which will come in a subsequent pull.
    
      The cores changes include:
    
       - blk-mq
            - Prep patch from Christoph, changing blk_mq_alloc_request() to
              take flags instead of just using gfp_t for sleep/nosleep.
            - Doc patch from me, clarifying the difference between legacy
              and blk-mq for timer usage.
            - Fixes from Raghavendra for memory-less numa nodes, and a reuse
              of CPU masks.
    
       - Cleanup from Geliang Tang, using offset_in_page() instead of open
         coding it.
    
       - From Ilya, rename request_queue slab to it reflects what it holds,
         and a fix for proper use of bdgrab/put.
    
       - A real fix for the split across stripe boundaries from Keith.  We
         yanked a broken version of this from 4.4-rc final, this one works.
    
       - From Mike Krinkin, emit a trace message when we split.
    
       - From Wei Tang, two small cleanups, not explicitly clearing memory
         that is already cleared"
    
    * 'for-4.5/core' of git://git.kernel.dk/linux-block:
      block: use bd{grab,put}() instead of open-coding
      block: split bios to max possible length
      block: add call to split trace point
      blk-mq: Avoid memoryless numa node encoded in hctx numa_node
      blk-mq: Reuse hardware context cpumask for tags
      blk-mq: add a flags parameter to blk_mq_alloc_request
      Revert "blk-flush: Queue through IO scheduler when flush not required"
      block: clarify blk_add_timer() use case for blk-mq
      bio: use offset_in_page macro
      block: do not initialise statics to 0 or NULL
      block: do not initialise globals to 0 or NULL
      block: rename request_queue slab cache

commit 34c0fd540e79fb49ef9ce864dae1058cca265780
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:14 2016 -0800

    mm, dax, pmem: introduce pfn_t
    
    For the purpose of communicating the optional presence of a 'struct
    page' for the pfn returned from ->direct_access(), introduce a type that
    encapsulates a page-frame-number plus flags.  These flags contain the
    historical "page_link" encoding for a scatterlist entry, but can also
    denote "device memory".  Where "device memory" is a set of pfns that are
    not part of the kernel's linear mapping by default, but are accessed via
    the same memory controller as ram.
    
    The motivation for this new type is large capacity persistent memory
    that needs struct page entries in the 'memmap' to support 3rd party DMA
    (i.e.  O_DIRECT I/O with a persistent memory source/target).  However,
    we also need it in support of maintaining a list of mapped inodes which
    need to be unmapped at driver teardown or freeze_bdev() time.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 88821fa26f19..bfb64d672e19 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -15,6 +15,7 @@
 #include <linux/backing-dev-defs.h>
 #include <linux/wait.h>
 #include <linux/mempool.h>
+#include <linux/pfn.h>
 #include <linux/bio.h>
 #include <linux/stringify.h>
 #include <linux/gfp.h>
@@ -1628,7 +1629,7 @@ struct blk_dax_ctl {
 	sector_t sector;
 	void __pmem *addr;
 	long size;
-	unsigned long pfn;
+	pfn_t pfn;
 };
 
 struct block_device_operations {
@@ -1638,7 +1639,7 @@ struct block_device_operations {
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	long (*direct_access)(struct block_device *, sector_t, void __pmem **,
-			unsigned long *pfn);
+			pfn_t *);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */

commit b2e0d1625e193b40cbbd45b799f82d54d34e015c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:55:59 2016 -0800

    dax: fix lifetime of in-kernel dax mappings with dax_map_atomic()
    
    The DAX implementation needs to protect new calls to ->direct_access()
    and usage of its return value against the driver for the underlying
    block device being disabled.  Use blk_queue_enter()/blk_queue_exit() to
    hold off blk_cleanup_queue() from proceeding, or otherwise fail new
    mapping requests if the request_queue is being torn down.
    
    This also introduces blk_dax_ctl to simplify the interface from fs/dax.c
    through dax_map_atomic() to bdev_direct_access().
    
    [willy@linux.intel.com: fix read() of a hole]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jan Kara <jack@suse.com>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c70e3588a48c..88821fa26f19 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1617,6 +1617,20 @@ static inline bool integrity_req_gap_front_merge(struct request *req,
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
+/**
+ * struct blk_dax_ctl - control and output parameters for ->direct_access
+ * @sector: (input) offset relative to a block_device
+ * @addr: (output) kernel virtual address for @sector populated by driver
+ * @pfn: (output) page frame number for @addr populated by driver
+ * @size: (input) number of bytes requested
+ */
+struct blk_dax_ctl {
+	sector_t sector;
+	void __pmem *addr;
+	long size;
+	unsigned long pfn;
+};
+
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
@@ -1643,8 +1657,7 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
-extern long bdev_direct_access(struct block_device *, sector_t,
-		void __pmem **addr, unsigned long *pfn, long size);
+extern long bdev_direct_access(struct block_device *, struct blk_dax_ctl *);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 21491412f2ec6f13d4104de734dec0ba659d092e
Author: Jens Axboe <axboe@fb.com>
Date:   Mon Dec 28 13:01:22 2015 -0700

    block: add blk_start_queue_async()
    
    We currently only have an inline/sync helper to restart a stopped
    queue. If drivers need an async version, they have to roll their
    own. Add a generic helper instead.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0169ba2e2e64..c70e3588a48c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -797,6 +797,7 @@ extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int blk_queue_enter(struct request_queue *q, gfp_t gfp);
 extern void blk_queue_exit(struct request_queue *q);
 extern void blk_start_queue(struct request_queue *q);
+extern void blk_start_queue_async(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);

commit 287922eb0b186e2a5bf54fdd04b734c25c90035c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Oct 30 20:57:30 2015 +0800

    block: defer timeouts to a workqueue
    
    Timer context is not very useful for drivers to perform any meaningful abort
    action from.  So instead of calling the driver from this useless context
    defer it to a workqueue as soon as possible.
    
    Note that while a delayed_work item would seem the right thing here I didn't
    dare to use it due to the magic in blk_add_timer that pokes deep into timer
    internals.  But maybe this encourages Tejun to add a sensible API for that to
    the workqueue API and we'll all be fine in the end :)
    
    Contains a major update from Keith Bush:
    
    "This patch removes synchronizing the timeout work so that the timer can
     start a freeze on its own queue. The timer enters the queue, so timer
     context can only start a freeze, but not wait for frozen."
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e711f294934c..221dc3bac49f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -407,6 +407,7 @@ struct request_queue {
 
 	unsigned int		rq_timeout;
 	struct timer_list	timeout;
+	struct work_struct	timeout_work;
 	struct list_head	timeout_list;
 
 	struct list_head	icq_list;

commit 19190f5ea9917645c770c85f5119bacdcaf4261a
Merge: a2dbb7b56f2c be9e2f775f5a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 6 08:02:25 2015 -0800

    Merge tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI fixes from James Bottomley:
     "This is quite a bumper crop of fixes: three from Arnd correcting
      various build issues in some configurations, a lock recursion in
      qla2xxx.  Two potentially exploitable issues in hpsa and mvsas, a
      potential null deref in st, a revert of a bdi registration fix that
      turned out to cause even more problems, a set of fixes to allow people
      who only defined MPT2SAS to still work after the mpt2/mpt3sas merger
      and a couple of fixes for issues turned up by the hyper-v storvsc
      driver"
    
    * tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:
      mpt3sas: fix Kconfig dependency problem for mpt2sas back compatibility
      Revert "scsi: Fix a bdi reregistration race"
      mpt3sas: Add dummy Kconfig option for backwards compatibility
      Fix a memory leak in scsi_host_dev_release()
      block/sd: Fix device-imposed transfer length limits
      scsi_debug: fix prevent_allow+verify regressions
      MAINTAINERS: Add myself as co-maintainer of the SCSI subsystem.
      sd: Make discard granularity match logical block size when LBPRZ=1
      scsi: hpsa: select CONFIG_SCSI_SAS_ATTR
      scsi: advansys needs ISA dma api for ISA support
      scsi_sysfs: protect against double execution of __scsi_remove_device()
      st: fix potential null pointer dereference.
      scsi: report 'INQUIRY result too short' once per host
      advansys: fix big-endian builds
      qla2xxx: Fix rwlock recursion
      hpsa: logical vs bitwise AND typo
      mvsas: don't allow negative timeouts
      mpt3sas: Fix use sas_is_tlr_enabled API before enabling MPI2_SCSIIO_CONTROL_TLR_ON flag

commit be9e2f775f5a3147205c552534c3abf0f9374a13
Merge: e619e6cbecb7 3ddda3e4c82d
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Dec 3 09:32:33 2015 -0800

    Merge branch 'mkp-fixes' into fixes

commit 6f3b0e8bcf3cbb87a7459b3ed018d31d918df3f8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 26 09:13:05 2015 +0100

    blk-mq: add a flags parameter to blk_mq_alloc_request
    
    We already have the reserved flag, and a nowait flag awkwardly encoded as
    a gfp_t.  Add a real flags argument to make the scheme more extensible and
    allow for a nicer calling convention.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c0d2b7927c1f..e711f294934c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -794,7 +794,7 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern int blk_queue_enter(struct request_queue *q, gfp_t gfp);
+extern int blk_queue_enter(struct request_queue *q, bool nowait);
 extern void blk_queue_exit(struct request_queue *q);
 extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);

commit bf4e6b4e757488dee1b6a581f49c7ac34cd217f8
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Nov 26 08:46:57 2015 +0100

    block: Always check queue limits for cloned requests
    
    When a cloned request is retried on other queues it always needs
    to be checked against the queue limits of that queue.
    Otherwise the calculations for nr_phys_segments might be wrong,
    leading to a crash in scsi_init_sgtable().
    
    To clarify this the patch renames blk_rq_check_limits()
    to blk_cloned_rq_check_limits() and removes the symbol
    export, as the new function should only be used for
    cloned requests and never exported.
    
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Ewan Milne <emilne@redhat.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Hannes Reinecke <hare@suse.de>
    Fixes: e2a60da74 ("block: Clean up special command handling logic")
    Cc: stable@vger.kernel.org # 3.7+
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c0d2b7927c1f..c06f8eaa42ff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -773,7 +773,6 @@ extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);
-extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 			     struct bio_set *bs, gfp_t gfp_mask,

commit ca369d51b3e1649be4a72addd6d6a168cfb3f537
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Nov 13 16:46:48 2015 -0500

    block/sd: Fix device-imposed transfer length limits
    
    Commit 4f258a46346c ("sd: Fix maximum I/O size for BLOCK_PC requests")
    had the unfortunate side-effect of removing an implicit clamp to
    BLK_DEF_MAX_SECTORS for REQ_TYPE_FS requests in the block layer
    code. This caused problems for some SMR drives.
    
    Debugging this issue revealed a few problems with the existing
    infrastructure since the block layer didn't know how to deal with
    device-imposed limits, only limits set by the I/O controller.
    
     - Introduce a new queue limit, max_dev_sectors, which is used by the
       ULD to signal the maximum sectors for a REQ_TYPE_FS request.
    
     - Ensure that max_dev_sectors is correctly stacked and taken into
       account when overriding max_sectors through sysfs.
    
     - Rework sd_read_block_limits() so it saves the max_xfer and opt_xfer
       values for later processing.
    
     - In sd_revalidate() set the queue's max_dev_sectors based on the
       MAXIMUM TRANSFER LENGTH value in the Block Limits VPD. If this value
       is not reported, fall back to a cap based on the CDB TRANSFER LENGTH
       field size.
    
     - In sd_revalidate(), use OPTIMAL TRANSFER LENGTH from the Block Limits
       VPD--if reported and sane--to signal the preferred device transfer
       size for FS requests. Otherwise use BLK_DEF_MAX_SECTORS.
    
     - blk_limits_max_hw_sectors() is no longer used and can be removed.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=93581
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: sweeneygj@gmx.com
    Tested-by: Arzeets <anatol.pomozov@gmail.com>
    Tested-by: David Eisner <david.eisner@oriel.oxon.org>
    Tested-by: Mario Kicherer <dev@kicherer.org>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 38a5ff772a37..9dacb745fa96 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -253,6 +253,7 @@ struct queue_limits {
 	unsigned long		virt_boundary_mask;
 
 	unsigned int		max_hw_sectors;
+	unsigned int		max_dev_sectors;
 	unsigned int		chunk_sectors;
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
@@ -948,7 +949,6 @@ extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
-extern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);

commit 2e6edc95382cc36423aff18a237173ad62d5ab52
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Nov 19 13:29:28 2015 -0800

    block: protect rw_page against device teardown
    
    Fix use after free crashes like the following:
    
     general protection fault: 0000 [#1] SMP
     Call Trace:
      [<ffffffffa0050216>] ? pmem_do_bvec.isra.12+0xa6/0xf0 [nd_pmem]
      [<ffffffffa0050ba2>] pmem_rw_page+0x42/0x80 [nd_pmem]
      [<ffffffff8128fd90>] bdev_read_page+0x50/0x60
      [<ffffffff812972f0>] do_mpage_readpage+0x510/0x770
      [<ffffffff8128fd20>] ? I_BDEV+0x20/0x20
      [<ffffffff811d86dc>] ? lru_cache_add+0x1c/0x50
      [<ffffffff81297657>] mpage_readpages+0x107/0x170
      [<ffffffff8128fd20>] ? I_BDEV+0x20/0x20
      [<ffffffff8128fd20>] ? I_BDEV+0x20/0x20
      [<ffffffff8129058d>] blkdev_readpages+0x1d/0x20
      [<ffffffff811d615f>] __do_page_cache_readahead+0x28f/0x310
      [<ffffffff811d6039>] ? __do_page_cache_readahead+0x169/0x310
      [<ffffffff811c5abd>] ? pagecache_get_page+0x2d/0x1d0
      [<ffffffff811c76f6>] filemap_fault+0x396/0x530
      [<ffffffff811f816e>] __do_fault+0x4e/0xf0
      [<ffffffff811fce7d>] handle_mm_fault+0x11bd/0x1b50
    
    Cc: <stable@vger.kernel.org>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Reported-by: kbuild test robot <lkp@intel.com>
    Acked-by: Matthew Wilcox <willy@linux.intel.com>
    [willy: symmetry fixups]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3fe27f8d91f0..c0d2b7927c1f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -794,6 +794,8 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
+extern int blk_queue_enter(struct request_queue *q, gfp_t gfp);
+extern void blk_queue_exit(struct request_queue *q);
 extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);

commit 05229beeddf7e75e2e616ddaad4b70e7fca9528d
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Nov 5 10:44:55 2015 -0700

    block: add block polling support
    
    Add basic support for polling for specific IO to complete. This uses
    the cookie that blk-mq passes back, which enables the block layer
    to pass this cookie to the driver to spin for a specific request.
    
    This will be combined with request latency tracking, so we can make
    qualified decisions about when to poll and when not to. For now, for
    benchmark purposes, we add a sysfs file that controls whether polling
    is enabled or not.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5ee0f5243025..3fe27f8d91f0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -487,6 +487,7 @@ struct request_queue {
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+#define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -814,6 +815,8 @@ extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 
+bool blk_poll(struct request_queue *q, blk_qc_t cookie);
+
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {
 	return bdev->bd_disk->queue;	/* this is never NULL */

commit dece16353ef47d8d33f5302bc158072a9d65e26f
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Nov 5 10:41:16 2015 -0700

    block: change ->make_request_fn() and users to return a queue cookie
    
    No functional changes in this patch, but it prepares us for returning
    a more useful cookie related to the IO that was queued up.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Keith Busch <keith.busch@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d045ca8487af..5ee0f5243025 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -209,7 +209,7 @@ static inline unsigned short req_get_ioprio(struct request *req)
 struct blk_queue_ctx;
 
 typedef void (request_fn_proc) (struct request_queue *q);
-typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);
+typedef blk_qc_t (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 
@@ -761,7 +761,7 @@ static inline void rq_flush_dcache_pages(struct request *rq)
 
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
-extern void generic_make_request(struct bio *bio);
+extern blk_qc_t generic_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);

commit ccf21b69a83afaee4d5499e0d03eacf23946e08c
Merge: 527d1529e38b 73fcf4e20ebd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 21:01:27 2015 -0800

    Merge branch 'for-4.4/reservations' of git://git.kernel.dk/linux-block
    
    Pull block reservation support from Jens Axboe:
     "This adds support for persistent reservations, both at the core level,
      as well as for sd and NVMe"
    
    [ Background from the docs: "Persistent Reservations allow restricting
      access to block devices to specific initiators in a shared storage
      setup.  All implementations are expected to ensure the reservations
      survive a power loss and cover all connections in a multi path
      environment" ]
    
    * 'for-4.4/reservations' of git://git.kernel.dk/linux-block:
      NVMe: Precedence error in nvme_pr_clear()
      nvme: add missing endianess annotations in nvme_pr_command
      NVMe: Add persistent reservation ops
      sd: implement the Persistent Reservation API
      block: add an API for Persistent Reservations
      block: cleanup blkdev_ioctl

commit bbd3e064362e5057cc4799ba2e4d68c7593e490b
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 15 14:10:48 2015 +0200

    block: add an API for Persistent Reservations
    
    This commits adds a driver API and ioctls for controlling Persistent
    Reservations s/genericly/generically/ at the block layer.  Persistent
    Reservations are supported by SCSI and NVMe and allow controlling who gets
    access to a device in a shared storage setup.
    
    Note that we add a pr_ops structure to struct block_device_operations
    instead of adding the members directly to avoid bloating all instances
    of devices that will never support Persistent Reservations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 19c2e947d4d1..fe25da05e823 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -35,6 +35,7 @@ struct sg_io_hdr;
 struct bsg_job;
 struct blkcg_gq;
 struct blk_flush_queue;
+struct pr_ops;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -1633,6 +1634,7 @@ struct block_device_operations {
 	/* this callback is with swap_lock and sometimes page table lock held */
 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 	struct module *owner;
+	const struct pr_ops *pr_ops;
 };
 
 extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,

commit ac6fc48c9fb7d3220ec4e0be0c29bb314ea75f9f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Oct 21 13:20:18 2015 -0400

    block: move blk_integrity to request_queue
    
    A trace like the following proceeds a crash in bio_integrity_process()
    when it goes to use an already freed blk_integrity profile.
    
     BUG: unable to handle kernel paging request at ffff8800d31b10d8
     IP: [<ffff8800d31b10d8>] 0xffff8800d31b10d8
     PGD 2f65067 PUD 21fffd067 PMD 80000000d30001e3
     Oops: 0011 [#1] SMP
     Dumping ftrace buffer:
     ---------------------------------
        ndctl-2222    2.... 44526245us : disk_release: pmem1s
     systemd--2223    4.... 44573945us : bio_integrity_endio: pmem1s
        <...>-409     4.... 44574005us : bio_integrity_process: pmem1s
     ---------------------------------
    [..]
      Call Trace:
      [<ffffffff8144e0f9>] ? bio_integrity_process+0x159/0x2d0
      [<ffffffff8144e4f6>] bio_integrity_verify_fn+0x36/0x60
      [<ffffffff810bd2dc>] process_one_work+0x1cc/0x4e0
    
    Given that a request_queue is pinned while i/o is in flight and that a
    gendisk is allowed to have a shorter lifetime, move blk_integrity to
    request_queue to satisfy requests arriving after the gendisk has been
    torn down.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    [martin: fix the CONFIG_BLK_DEV_INTEGRITY=n case]
    Tested-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3e0465257d68..cf57884db4b7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -369,6 +369,10 @@ struct request_queue {
 	 */
 	struct kobject mq_kobj;
 
+#ifdef  CONFIG_BLK_DEV_INTEGRITY
+	struct blk_integrity integrity;
+#endif	/* CONFIG_BLK_DEV_INTEGRITY */
+
 #ifdef CONFIG_PM
 	struct device		*dev;
 	int			rpm_status;
@@ -1481,7 +1485,7 @@ extern bool blk_integrity_merge_bio(struct request_queue *, struct request *,
 
 static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
 {
-	struct blk_integrity *bi = &disk->integrity;
+	struct blk_integrity *bi = &disk->queue->integrity;
 
 	if (!bi->profile)
 		return NULL;

commit 3ef28e83ab15799742e55fd13243a5f678b04242
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Oct 21 13:20:12 2015 -0400

    block: generic request_queue reference counting
    
    Allow pmem, and other synchronous/bio-based block drivers, to fallback
    on a per-cpu reference count managed by the core for tracking queue
    live/dead state.
    
    The existing per-cpu reference count for the blk_mq case is promoted to
    be used in all block i/o scenarios.  This involves initializing it by
    default, waiting for it to drop to zero at exit, and holding a live
    reference over the invocation of q->make_request_fn() in
    generic_make_request().  The blk_mq code continues to take its own
    reference per blk_mq request and retains the ability to freeze the
    queue, but the check that the queue is frozen is moved to
    generic_make_request().
    
    This fixes crash signatures like the following:
    
     BUG: unable to handle kernel paging request at ffff880140000000
     [..]
     Call Trace:
      [<ffffffff8145e8bf>] ? copy_user_handle_tail+0x5f/0x70
      [<ffffffffa004e1e0>] pmem_do_bvec.isra.11+0x70/0xf0 [nd_pmem]
      [<ffffffffa004e331>] pmem_make_request+0xd1/0x200 [nd_pmem]
      [<ffffffff811c3162>] ? mempool_alloc+0x72/0x1a0
      [<ffffffff8141f8b6>] generic_make_request+0xd6/0x110
      [<ffffffff8141f966>] submit_bio+0x76/0x170
      [<ffffffff81286dff>] submit_bh_wbc+0x12f/0x160
      [<ffffffff81286e62>] submit_bh+0x12/0x20
      [<ffffffff813395bd>] jbd2_write_superblock+0x8d/0x170
      [<ffffffff8133974d>] jbd2_mark_journal_empty+0x5d/0x90
      [<ffffffff813399cb>] jbd2_journal_destroy+0x24b/0x270
      [<ffffffff810bc4ca>] ? put_pwq_unlocked+0x2a/0x30
      [<ffffffff810bc6f5>] ? destroy_workqueue+0x225/0x250
      [<ffffffff81303494>] ext4_put_super+0x64/0x360
      [<ffffffff8124ab1a>] generic_shutdown_super+0x6a/0xf0
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 60669c20190f..3e0465257d68 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -450,7 +450,7 @@ struct request_queue {
 #endif
 	struct rcu_head		rcu_head;
 	wait_queue_head_t	mq_freeze_wq;
-	struct percpu_ref	mq_usage_counter;
+	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
 	struct blk_mq_tag_set	*tag_set;

commit 25520d55cdb6ee289abc68f553d364d22478ff54
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 21 13:19:49 2015 -0400

    block: Inline blk_integrity in struct gendisk
    
    Up until now the_integrity profile has been dynamically allocated and
    attached to struct gendisk after the disk has been made active.
    
    This causes problems because NVMe devices need to register the profile
    prior to the partition table being read due to a mandatory metadata
    buffer requirement. In addition, DM goes through hoops to deal with
    preallocating, but not initializing integrity profiles.
    
    Since the integrity profile is small (4 bytes + a pointer), Christoph
    suggested moving it to struct gendisk proper. This requires several
    changes:
    
     - Moving the blk_integrity definition to genhd.h.
    
     - Inlining blk_integrity in struct gendisk.
    
     - Removing the dynamic allocation code.
    
     - Adding helper functions which allow gendisk to set up and tear down
       the integrity sysfs dir when a disk is added/deleted.
    
     - Adding a blk_integrity_revalidate() callback for updating the stable
       pages bdi setting.
    
     - The calls that depend on whether a device has an integrity profile or
       not now key off of the bi->profile pointer.
    
     - Simplifying the integrity support routines in DM (Mike Snitzer).
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reported-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4f1968f15e30..60669c20190f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1468,16 +1468,7 @@ struct blk_integrity_profile {
 	const char			*name;
 };
 
-struct blk_integrity {
-	struct blk_integrity_profile	*profile;
-	unsigned char			flags;
-	unsigned char			tuple_size;
-	unsigned char			interval_exp;
-	unsigned char			tag_size;
-};
-
-extern bool blk_integrity_is_initialized(struct gendisk *);
-extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
+extern void blk_integrity_register(struct gendisk *, struct blk_integrity *);
 extern void blk_integrity_unregister(struct gendisk *);
 extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
 extern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,
@@ -1488,15 +1479,20 @@ extern bool blk_integrity_merge_rq(struct request_queue *, struct request *,
 extern bool blk_integrity_merge_bio(struct request_queue *, struct request *,
 				    struct bio *);
 
-static inline
-struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
+static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
 {
-	return bdev->bd_disk->integrity;
+	struct blk_integrity *bi = &disk->integrity;
+
+	if (!bi->profile)
+		return NULL;
+
+	return bi;
 }
 
-static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
+static inline
+struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
 {
-	return disk->integrity;
+	return blk_get_integrity(bdev->bd_disk);
 }
 
 static inline bool blk_integrity_rq(struct request *rq)
@@ -1570,10 +1566,9 @@ static inline int blk_integrity_compare(struct gendisk *a, struct gendisk *b)
 {
 	return 0;
 }
-static inline int blk_integrity_register(struct gendisk *d,
+static inline void blk_integrity_register(struct gendisk *d,
 					 struct blk_integrity *b)
 {
-	return 0;
 }
 static inline void blk_integrity_unregister(struct gendisk *d)
 {
@@ -1598,10 +1593,7 @@ static inline bool blk_integrity_merge_bio(struct request_queue *rq,
 {
 	return true;
 }
-static inline bool blk_integrity_is_initialized(struct gendisk *g)
-{
-	return 0;
-}
+
 static inline bool integrity_req_gap_back_merge(struct request *req,
 						struct bio *next)
 {

commit a48f041d91bf1aee599fa2adb53b780ed20c2ee5
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 21 13:19:38 2015 -0400

    block: Reduce the size of struct blk_integrity
    
    The per-device properties in the blk_integrity structure were previously
    unsigned short. However, most of the values fit inside a char. The only
    exception is the data interval size and we can work around that by
    storing it as a power of two.
    
    This cuts the size of the dynamic portion of blk_integrity in half.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reported-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f36c6476f1c7..4f1968f15e30 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1470,10 +1470,10 @@ struct blk_integrity_profile {
 
 struct blk_integrity {
 	struct blk_integrity_profile	*profile;
-	unsigned short			flags;
-	unsigned short			tuple_size;
-	unsigned short			interval;
-	unsigned short			tag_size;
+	unsigned char			flags;
+	unsigned char			tuple_size;
+	unsigned char			interval_exp;
+	unsigned char			tag_size;
 };
 
 extern bool blk_integrity_is_initialized(struct gendisk *);

commit 0f8087ecdeac921fc4920f1328f55c15080bc6aa
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 21 13:19:33 2015 -0400

    block: Consolidate static integrity profile properties
    
    We previously made a complete copy of a device's data integrity profile
    even though several of the fields inside the blk_integrity struct are
    pointers to fixed template entries in t10-pi.c.
    
    Split the static and per-device portions so that we can reference the
    template directly.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reported-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 830f9c07d4bb..f36c6476f1c7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1462,16 +1462,18 @@ struct blk_integrity_iter {
 
 typedef int (integrity_processing_fn) (struct blk_integrity_iter *);
 
-struct blk_integrity {
-	integrity_processing_fn	*generate_fn;
-	integrity_processing_fn	*verify_fn;
-
-	unsigned short		flags;
-	unsigned short		tuple_size;
-	unsigned short		interval;
-	unsigned short		tag_size;
+struct blk_integrity_profile {
+	integrity_processing_fn		*generate_fn;
+	integrity_processing_fn		*verify_fn;
+	const char			*name;
+};
 
-	const char		*name;
+struct blk_integrity {
+	struct blk_integrity_profile	*profile;
+	unsigned short			flags;
+	unsigned short			tuple_size;
+	unsigned short			interval;
+	unsigned short			tag_size;
 };
 
 extern bool blk_integrity_is_initialized(struct gendisk *);

commit aff34e192e4eeacfb8b5ffc68e10a240f2c0c6d7
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 21 13:19:27 2015 -0400

    block: Move integrity kobject to struct gendisk
    
    The integrity kobject purely exists to support the integrity
    subdirectory in sysfs and doesn't really have anything to do with the
    blk_integrity data structure. Move the kobject to struct gendisk where
    it belongs.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reported-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 19c2e947d4d1..830f9c07d4bb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1472,8 +1472,6 @@ struct blk_integrity {
 	unsigned short		tag_size;
 
 	const char		*name;
-
-	struct kobject		kobj;
 };
 
 extern bool blk_integrity_is_initialized(struct gendisk *);

commit 4593fdbe7a2f44d5e64c627c715dd0bcec9bdf14
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:20 2015 +0900

    blk-mq: fix sysfs registration/unregistration race
    
    There is a race between cpu hotplug handling and adding/deleting
    gendisk for blk-mq, where both are trying to register and unregister
    the same sysfs entries.
    
    null_add_dev
        --> blk_mq_init_queue
            --> blk_mq_init_allocated_queue
                --> add to 'all_q_list' (*)
        --> add_disk
            --> blk_register_queue
                --> blk_mq_register_disk (++)
    
    null_del_dev
        --> del_gendisk
            --> blk_unregister_queue
                --> blk_mq_unregister_disk (--)
        --> blk_cleanup_queue
            --> blk_mq_free_queue
                --> del from 'all_q_list' (*)
    
    blk_mq_queue_reinit
        --> blk_mq_sysfs_unregister (-)
        --> blk_mq_sysfs_register (+)
    
    While the request queue is added to 'all_q_list' (*),
    blk_mq_queue_reinit() can be called for the queue anytime by CPU
    hotplug callback.  But blk_mq_sysfs_unregister (-) and
    blk_mq_sysfs_register (+) in blk_mq_queue_reinit must not be called
    before blk_mq_register_disk (++) and after blk_mq_unregister_disk (--)
    is finished.  Because '/sys/block/*/mq/' is not exists.
    
    There has already been BLK_MQ_F_SYSFS_UP flag in hctx->flags which can
    be used to track these sysfs stuff, but it is only fixing this issue
    partially.
    
    In order to fix it completely, we just need per-queue flag instead of
    per-hctx flag with appropriate locking.  So this introduces
    q->mq_sysfs_init_done which is properly protected with all_q_mutex.
    
    Also, we need to ensure that blk_mq_map_swqueue() is called with
    all_q_mutex is held.  Since hctx->nr_ctx is reset temporarily and
    updated in blk_mq_map_swqueue(), so we should avoid
    blk_mq_register_hctx() seeing the temporary hctx->nr_ctx value
    in CPU hotplug handling or adding/deleting gendisk .
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 99da9ebc7377..19c2e947d4d1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -456,6 +456,8 @@ struct request_queue {
 	struct blk_mq_tag_set	*tag_set;
 	struct list_head	tag_set_list;
 	struct bio_set		*bio_split;
+
+	bool			mq_sysfs_init_done;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 133bb59585140747fd3938002670cb395f40dc76
Merge: 590dca3a7187 994518799930
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 19 18:57:09 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "This is a bit bigger than it should be, but I could (did) not want to
      send it off last week due to both wanting extra testing, and expecting
      a fix for the bounce regression as well.  In any case, this contains:
    
       - Fix for the blk-merge.c compilation warning on gcc 5.x from me.
    
       - A set of back/front SG gap merge fixes, from me and from Sagi.
         This ensures that we honor SG gapping for integrity payloads as
         well.
    
       - Two small fixes for null_blk from Matias, fixing a leak and a
         capacity propagation issue.
    
       - A blkcg fix from Tejun, fixing a NULL dereference.
    
       - A fast clone optimization from Ming, fixing a performance
         regression since the arbitrarily sized bio's were introduced.
    
       - Also from Ming, a regression fix for bouncing IOs"
    
    * 'for-linus' of git://git.kernel.dk/linux-block:
      block: fix bounce_end_io
      block: blk-merge: fast-clone bio when splitting rw bios
      block: blkg_destroy_all() should clear q->root_blkg and ->root_rl.blkg
      block: Copy a user iovec if it includes gaps
      block: Refuse adding appending a gapped integrity page to a bio
      block: Refuse request/bio merges with gaps in the integrity payload
      block: Check for gaps on front and back merges
      null_blk: fix wrong capacity when bs is not 512 bytes
      null_blk: fix memory leak on cleanup
      block: fix bogus compiler warnings in blk-merge.c

commit 10fbd36e362a0f367e34a7cd876a81295d8fc5ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 27 15:32:15 2015 -0700

    blk: rq_data_dir() should not return a boolean
    
    rq_data_dir() returns either READ or WRITE (0 == READ, 1 == WRITE), not
    a boolean value.
    
    Now, admittedly the "!= 0" doesn't really change the value (0 stays as
    zero, 1 stays as one), but it's not only redundant, it confuses gcc, and
    causes gcc to warn about the construct
    
        switch (rq_data_dir(req)) {
            case READ:
                ...
            case WRITE:
                ...
    
    that we have in a few drivers.
    
    Now, the gcc warning is silly and stupid (it seems to warn not about the
    switch value having a different type from the case statements, but about
    _any_ boolean switch value), but in this case the code itself is silly
    and stupid too, so let's just change it, and get rid of warnings like
    this:
    
      drivers/block/hd.c: In function ‘hd_request’:
      drivers/block/hd.c:630:11: warning: switch condition has boolean value [-Wswitch-bool]
         switch (rq_data_dir(req)) {
    
    The odd '!= 0' came in when "cmd_flags" got turned into a "u64" in
    commit 5953316dbf90 ("block: make rq->cmd_flags be 64-bit") and is
    presumably because the old code (that just did a logical 'and' with 1)
    would then end up making the type of rq_data_dir() be u64 too.
    
    But if we want to retain the old regular integer type, let's just cast
    the result to 'int' rather than use that rather odd '!= 0'.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 708923b9b623..38a5ff772a37 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -584,7 +584,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
-#define rq_data_dir(rq)		(((rq)->cmd_flags & 1) != 0)
+#define rq_data_dir(rq)		((int)((rq)->cmd_flags & 1))
 
 /*
  * Driver can handle struct request, if it either has an old style

commit 7f39add3b08cbbdb99abe50e6d7c342e6800d684
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Fri Sep 11 09:03:04 2015 -0600

    block: Refuse request/bio merges with gaps in the integrity payload
    
    If a driver sets the block queue virtual boundary mask, it means that
    it cannot handle gaps so we must not allow those in the integrity
    payload as well.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    
    Fixed up by me to have duplicate integrity merge functions, depending
    on whether block integrity is enabled or not. Fixes a compilations
    issue with CONFIG_BLK_DEV_INTEGRITY unset.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2ff94def041e..1aac7316a4b5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1514,6 +1514,26 @@ queue_max_integrity_segments(struct request_queue *q)
 	return q->limits.max_integrity_segments;
 }
 
+static inline bool integrity_req_gap_back_merge(struct request *req,
+						struct bio *next)
+{
+	struct bio_integrity_payload *bip = bio_integrity(req->bio);
+	struct bio_integrity_payload *bip_next = bio_integrity(next);
+
+	return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+				bip_next->bip_vec[0].bv_offset);
+}
+
+static inline bool integrity_req_gap_front_merge(struct request *req,
+						 struct bio *bio)
+{
+	struct bio_integrity_payload *bip = bio_integrity(bio);
+	struct bio_integrity_payload *bip_next = bio_integrity(req->bio);
+
+	return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+				bip_next->bip_vec[0].bv_offset);
+}
+
 #else /* CONFIG_BLK_DEV_INTEGRITY */
 
 struct bio;
@@ -1580,6 +1600,16 @@ static inline bool blk_integrity_is_initialized(struct gendisk *g)
 {
 	return 0;
 }
+static inline bool integrity_req_gap_back_merge(struct request *req,
+						struct bio *next)
+{
+	return false;
+}
+static inline bool integrity_req_gap_front_merge(struct request *req,
+						 struct bio *bio)
+{
+	return false;
+}
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 

commit 12f03ee606914317e7e6a0815e53a48205c31dae
Merge: d9241b22b58e 004f1afbe199
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 14:35:59 2015 -0700

    Merge tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This update has successfully completed a 0day-kbuild run and has
      appeared in a linux-next release.  The changes outside of the typical
      drivers/nvdimm/ and drivers/acpi/nfit.[ch] paths are related to the
      removal of IORESOURCE_CACHEABLE, the introduction of memremap(), and
      the introduction of ZONE_DEVICE + devm_memremap_pages().
    
      Summary:
    
       - Introduce ZONE_DEVICE and devm_memremap_pages() as a generic
         mechanism for adding device-driver-discovered memory regions to the
         kernel's direct map.
    
         This facility is used by the pmem driver to enable pfn_to_page()
         operations on the page frames returned by DAX ('direct_access' in
         'struct block_device_operations').
    
         For now, the 'memmap' allocation for these "device" pages comes
         from "System RAM".  Support for allocating the memmap from device
         memory will arrive in a later kernel.
    
       - Introduce memremap() to replace usages of ioremap_cache() and
         ioremap_wt().  memremap() drops the __iomem annotation for these
         mappings to memory that do not have i/o side effects.  The
         replacement of ioremap_cache() with memremap() is limited to the
         pmem driver to ease merging the api change in v4.3.
    
         Completion of the conversion is targeted for v4.4.
    
       - Similar to the usage of memcpy_to_pmem() + wmb_pmem() in the pmem
         driver, update the VFS DAX implementation and PMEM api to provide
         persistence guarantees for kernel operations on a DAX mapping.
    
       - Convert the ACPI NFIT 'BLK' driver to map the block apertures as
         cacheable to improve performance.
    
       - Miscellaneous updates and fixes to libnvdimm including support for
         issuing "address range scrub" commands, clarifying the optimal
         'sector size' of pmem devices, a clarification of the usage of the
         ACPI '_STA' (status) property for DIMM devices, and other minor
         fixes"
    
    * tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (34 commits)
      libnvdimm, pmem: direct map legacy pmem by default
      libnvdimm, pmem: 'struct page' for pmem
      libnvdimm, pfn: 'struct page' provider infrastructure
      x86, pmem: clarify that ARCH_HAS_PMEM_API implies PMEM mapped WB
      add devm_memremap_pages
      mm: ZONE_DEVICE for "device memory"
      mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
      dax: drop size parameter to ->direct_access()
      nd_blk: change aperture mapping from WC to WB
      nvdimm: change to use generic kvfree()
      pmem, dax: have direct_access use __pmem annotation
      dax: update I/O path to do proper PMEM flushing
      pmem: add copy_from_iter_pmem() and clear_pmem()
      pmem, x86: clean up conditional pmem includes
      pmem: remove layer when calling arch_has_wmb_pmem()
      pmem, x86: move x86 PMEM API to new pmem.h header
      libnvdimm, e820: make CONFIG_X86_PMEM_LEGACY a tristate option
      pmem: switch to devm_ allocations
      devres: add devm_memremap
      libnvdimm, btt: write and validate parent_uuid
      ...

commit 5e7c4274a70aa2d6f485996d0ca1dad52d0039ca
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Sep 3 19:28:20 2015 +0300

    block: Check for gaps on front and back merges
    
    We are checking for gaps to previous bio_vec, which can
    only detect back merges gaps. Moreover, at the point where
    we check for a gap, we don't know if we will attempt a back
    or a front merge. Thus, check for gap to prev in a back merge
    attempt and check for a gap to next in a front merge attempt.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    [sagig: Minor rename change]
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a622f270f09e..2ff94def041e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1368,6 +1368,26 @@ static inline bool bvec_gap_to_prev(struct request_queue *q,
 		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
 }
 
+static inline bool bio_will_gap(struct request_queue *q, struct bio *prev,
+			 struct bio *next)
+{
+	if (!bio_has_data(prev))
+		return false;
+
+	return bvec_gap_to_prev(q, &prev->bi_io_vec[prev->bi_vcnt - 1],
+				next->bi_io_vec[0].bv_offset);
+}
+
+static inline bool req_gap_back_merge(struct request *req, struct bio *bio)
+{
+	return bio_will_gap(req->q, req->biotail, bio);
+}
+
+static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
+{
+	return bio_will_gap(req->q, bio, req->bio);
+}
+
 struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);

commit cb389b9c0e00c30c9daf20287f7d91e2466edbb1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Aug 7 17:41:00 2015 -0400

    dax: drop size parameter to ->direct_access()
    
    None of the implementations currently use it.  The common
    bdev_direct_access() entry point handles all the size checks before
    calling ->direct_access().
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c401ecdff9cb..c22064f326b2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1556,7 +1556,7 @@ struct block_device_operations {
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	long (*direct_access)(struct block_device *, sector_t, void __pmem **,
-			unsigned long *pfn, long size);
+			unsigned long *pfn);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */

commit e2e05394e4a3420dab96f728df4531893494e15d
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Tue Aug 18 13:55:41 2015 -0600

    pmem, dax: have direct_access use __pmem annotation
    
    Update the annotation for the kaddr pointer returned by direct_access()
    so that it is a __pmem pointer.  This is consistent with the PMEM driver
    and with how this direct_access() pointer is used in the DAX code.
    
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4068c17d0df..c401ecdff9cb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1555,8 +1555,8 @@ struct block_device_operations {
 	int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
-	long (*direct_access)(struct block_device *, sector_t,
-					void **, unsigned long *pfn, long size);
+	long (*direct_access)(struct block_device *, sector_t, void __pmem **,
+			unsigned long *pfn, long size);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */
@@ -1574,8 +1574,8 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
-extern long bdev_direct_access(struct block_device *, sector_t, void **addr,
-						unsigned long *pfn, long size);
+extern long bdev_direct_access(struct block_device *, sector_t,
+		void __pmem **addr, unsigned long *pfn, long size);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit 03100aada96f0645bbcb89aea24c01f02d0ef1fa
Author: Keith Busch <keith.busch@intel.com>
Date:   Wed Aug 19 14:24:05 2015 -0700

    block: Replace SG_GAPS with new queue limits mask
    
    The SG_GAPS queue flag caused checks for bio vector alignment against
    PAGE_SIZE, but the device may have different constraints. This patch
    adds a queue limits so a driver with such constraints can set to allow
    requests that would have been unnecessarily split. The new gaps check
    takes the request_queue as a parameter to simplify the logic around
    invoking this function.
    
    This new limit makes the queue flag redundant, so removing it and
    all usage. Device-mappers will inherit the correct settings through
    blk_stack_limits().
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e427debc7008..a622f270f09e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -250,6 +250,7 @@ struct blk_queue_tag {
 struct queue_limits {
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
+	unsigned long		virt_boundary_mask;
 
 	unsigned int		max_hw_sectors;
 	unsigned int		chunk_sectors;
@@ -479,7 +480,6 @@ struct request_queue {
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
-#define QUEUE_FLAG_SG_GAPS     22	/* queue doesn't support SG gaps */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -981,6 +981,7 @@ extern int blk_queue_dma_drain(struct request_queue *q,
 			       void *buf, unsigned int size);
 extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
+extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
@@ -1149,6 +1150,11 @@ static inline unsigned long queue_segment_boundary(struct request_queue *q)
 	return q->limits.seg_boundary_mask;
 }
 
+static inline unsigned long queue_virt_boundary(struct request_queue *q)
+{
+	return q->limits.virt_boundary_mask;
+}
+
 static inline unsigned int queue_max_sectors(struct request_queue *q)
 {
 	return q->limits.max_sectors;
@@ -1349,6 +1355,19 @@ static inline void put_dev_sector(Sector p)
 	page_cache_release(p.v);
 }
 
+/*
+ * Check if adding a bio_vec after bprv with offset would create a gap in
+ * the SG list. Most drivers don't care about this, but some do.
+ */
+static inline bool bvec_gap_to_prev(struct request_queue *q,
+				struct bio_vec *bprv, unsigned int offset)
+{
+	if (!queue_virt_boundary(q))
+		return false;
+	return offset ||
+		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
+}
+
 struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);

commit d2be537c3ba3568acd79cd178327b842e60d035e
Author: Jeff Moyer <jmoyer@redhat.com>
Date:   Thu Aug 13 14:57:57 2015 -0400

    block: bump BLK_DEF_MAX_SECTORS to 2560
    
    A value of 2560 (1280k) will accommodate a 10-data-disk stripe
    write with chunk size 128k.  In the testing I've done using
    iozone, fio, and aio-stress across a number of different storage
    devices, a value of 1280 does not show a big performance
    difference from 512, but will hopefully help software RAID
    setups using SATA disks, as reported by Christoph.
    
    NOTE: drivers/block/aoe/aoeblk.c sets its own max_hw_sectors_kb to
    BLK_DEF_MAX_SECTORS.  So, this patch essentially changes aeoblk to
    Use a larger maximum sector size, and I did not test this.
    
    Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9f1e3f8eeed7..e427debc7008 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1132,7 +1132,7 @@ extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 enum blk_default_limits {
 	BLK_MAX_SEGMENTS	= 128,
 	BLK_SAFE_MAX_SECTORS	= 255,
-	BLK_DEF_MAX_SECTORS	= 1024,
+	BLK_DEF_MAX_SECTORS	= 2560,
 	BLK_MAX_SEGMENT_SIZE	= 65536,
 	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
 };

commit 30e2bc08b2bb7c069246feee78f7ed4006e130fe
Author: Jeff Moyer <jmoyer@redhat.com>
Date:   Thu Aug 13 14:57:56 2015 -0400

    Revert "block: remove artifical max_hw_sectors cap"
    
    This reverts commit 34b48db66e08ca1c1bc07cf305d672ac940268dc.
    That commit caused performance regressions for streaming I/O
    workloads on a number of different storage devices, from
    SATA disks to external RAID arrays.  It also managed to
    trip up some buggy firmware in at least one drive, causing
    data corruption.
    
    The next patch will bump the default max_sectors_kb value to
    1280, which will accommodate a 10-data-disk stripe write
    with chunk size 128k.  In the testing I've done using iozone,
    fio, and aio-stress, a value of 1280 does not show a big
    performance difference from 512.  This will hopefully still
    help the software RAID setup that Christoph saw the original
    performance gains with while still not regressing other
    storage configurations.
    
    Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a1feff54aeab..9f1e3f8eeed7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1132,6 +1132,7 @@ extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 enum blk_default_limits {
 	BLK_MAX_SEGMENTS	= 128,
 	BLK_SAFE_MAX_SECTORS	= 255,
+	BLK_DEF_MAX_SECTORS	= 1024,
 	BLK_MAX_SEGMENT_SIZE	= 65536,
 	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
 };

commit 8ae126660fddbeebb9251a174e6fa45b6ad8f932
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Mon Apr 27 23:48:34 2015 -0700

    block: kill merge_bvec_fn() completely
    
    As generic_make_request() is now able to handle arbitrarily sized bios,
    it's no longer necessary for each individual block driver to define its
    own ->merge_bvec_fn() callback. Remove every invocation completely.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: drbd-user@lists.linbit.com
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Yehuda Sadeh <yehuda@inktank.com>
    Cc: Sage Weil <sage@inktank.com>
    Cc: Alex Elder <elder@kernel.org>
    Cc: ceph-devel@vger.kernel.org
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: dm-devel@redhat.com
    Cc: Neil Brown <neilb@suse.de>
    Cc: linux-raid@vger.kernel.org
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Acked-by: NeilBrown <neilb@suse.de> (for the 'md' bits)
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    [dpark: also remove ->merge_bvec_fn() in dm-thin as well as
     dm-era-target, and resolve merge conflicts]
    Signed-off-by: Dongsu Park <dpark@posteo.net>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ca778d9c7d81..a1feff54aeab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -213,14 +213,6 @@ typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 
 struct bio_vec;
-struct bvec_merge_data {
-	struct block_device *bi_bdev;
-	sector_t bi_sector;
-	unsigned bi_size;
-	unsigned long bi_rw;
-};
-typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
-			     struct bio_vec *);
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 typedef int (lld_busy_fn) (struct request_queue *q);
@@ -306,7 +298,6 @@ struct request_queue {
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unprep_rq_fn		*unprep_rq_fn;
-	merge_bvec_fn		*merge_bvec_fn;
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
@@ -992,7 +983,6 @@ extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
-extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);

commit 54efd50bfd873e2dbf784e0b21a8027ba4299a3e
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Thu Apr 23 22:37:18 2015 -0700

    block: make generic_make_request handle arbitrarily sized bios
    
    The way the block layer is currently written, it goes to great lengths
    to avoid having to split bios; upper layer code (such as bio_add_page())
    checks what the underlying device can handle and tries to always create
    bios that don't need to be split.
    
    But this approach becomes unwieldy and eventually breaks down with
    stacked devices and devices with dynamic limits, and it adds a lot of
    complexity. If the block layer could split bios as needed, we could
    eliminate a lot of complexity elsewhere - particularly in stacked
    drivers. Code that creates bios can then create whatever size bios are
    convenient, and more importantly stacked drivers don't have to deal with
    both their own bio size limitations and the limitations of the
    (potentially multiple) devices underneath them.  In the future this will
    let us delete merge_bvec_fn and a bunch of other code.
    
    We do this by adding calls to blk_queue_split() to the various
    make_request functions that need it - a few can already handle arbitrary
    size bios. Note that we add the call _after_ any call to
    blk_queue_bounce(); this means that blk_queue_split() and
    blk_recalc_rq_segments() don't need to be concerned with bouncing
    affecting segment merging.
    
    Some make_request_fn() callbacks were simple enough to audit and verify
    they don't need blk_queue_split() calls. The skipped ones are:
    
     * nfhd_make_request (arch/m68k/emu/nfblock.c)
     * axon_ram_make_request (arch/powerpc/sysdev/axonram.c)
     * simdisk_make_request (arch/xtensa/platforms/iss/simdisk.c)
     * brd_make_request (ramdisk - drivers/block/brd.c)
     * mtip_submit_request (drivers/block/mtip32xx/mtip32xx.c)
     * loop_make_request
     * null_queue_bio
     * bcache's make_request fns
    
    Some others are almost certainly safe to remove now, but will be left
    for future patches.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ming Lei <ming.lei@canonical.com>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: dm-devel@redhat.com
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: drbd-user@lists.linbit.com
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Jim Paris <jim@jtan.com>
    Cc: Philip Kelleher <pjk1939@linux.vnet.ibm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Oleg Drokin <oleg.drokin@intel.com>
    Cc: Andreas Dilger <andreas.dilger@intel.com>
    Acked-by: NeilBrown <neilb@suse.de> (for the 'md/md.c' bits)
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    [dpark: skip more mq-based drivers, resolve merge conflicts, etc.]
    Signed-off-by: Dongsu Park <dpark@posteo.net>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 243f29e779ec..ca778d9c7d81 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -463,6 +463,7 @@ struct request_queue {
 
 	struct blk_mq_tag_set	*tag_set;
 	struct list_head	tag_set_list;
+	struct bio_set		*bio_split;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
@@ -783,6 +784,8 @@ extern void blk_rq_unprep_clone(struct request *rq);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
+extern void blk_queue_split(struct request_queue *, struct bio **,
+			    struct bio_set *);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
 extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,

commit 0034af036554c39eefd14d835a8ec3496ac46712
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jul 16 09:14:26 2015 -0600

    block: make /sys/block/<dev>/queue/discard_max_bytes writeable
    
    Lots of devices support huge discard sizes these days. Depending
    on how the device handles them internally, huge discards can
    introduce massive latencies (hundreds of msec) on the device side.
    
    We have a sysfs file, discard_max_bytes, that advertises the max
    hardware supported discard size. Make this writeable, and split
    the settings into a soft and hard limit. This can be set from
    'discard_granularity' and up to the hardware limit.
    
    Add a new sysfs file, 'discard_max_hw_bytes', that shows the hw
    set limit.
    
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4068c17d0df..243f29e779ec 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -268,6 +268,7 @@ struct queue_limits {
 	unsigned int		io_min;
 	unsigned int		io_opt;
 	unsigned int		max_discard_sectors;
+	unsigned int		max_hw_discard_sectors;
 	unsigned int		max_write_same_sectors;
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;

commit 22165fa79814e71e7a5974b3c37a5028ed16c8f9
Merge: a2f54be94f4c b5451e456840
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 12:35:01 2015 -0700

    Merge tag 'dm-4.2-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm
    
    Pull device mapper fixes from Mike Snitzer:
     "Apologies for not pressing this request-based DM partial completion
      issue further, it was an oversight on my part.  We'll have to get it
      fixed up properly and revisit for a future release.
    
       - Revert block and DM core changes the removed request-based DM's
         ability to handle partial request completions -- otherwise with the
         current SCSI LLDs these changes could lead to silent data
         corruption.
    
       - Fix two DM version bumps that were missing from the initial 4.2 DM
         pull request (enabled userspace lvm2 to know certain changes have
         been made)"
    
    * tag 'dm-4.2-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm:
      dm cache policy smq: fix "default" version to be 1.4.0
      dm: bump the ioctl version to 4.32.0
      Revert "block, dm: don't copy bios for request clones"
      Revert "dm: do not allocate any mempools for blk-mq request-based DM"

commit 78d8e58a086b214dddf1fd463e20a7e1d82d7866
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Jun 26 10:01:13 2015 -0400

    Revert "block, dm: don't copy bios for request clones"
    
    This reverts commit 5f1b670d0bef508a5554d92525f5f6d00d640b38.
    
    Justification for revert as reported in this dm-devel post:
    https://www.redhat.com/archives/dm-devel/2015-June/msg00160.html
    
    this change should not be pushed to mainline yet.
    
    Firstly, Christoph has a newer version of the patch that fixes silent
    data corruption problem:
      https://www.redhat.com/archives/dm-devel/2015-May/msg00229.html
    
    And the new version still depends on LLDDs to always complete requests
    to the end when error happens, while block API doesn't enforce such a
    requirement. If the assumption is ever broken, the inconsistency between
    request and bio (e.g. rq->__sector and rq->bio) will cause silent data
    corruption:
      https://www.redhat.com/archives/dm-devel/2015-June/msg00022.html
    
    Reported-by: Junichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 776d2ee43ba6..41c0fb573dff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -775,7 +775,11 @@ extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
 extern int blk_lld_busy(struct request_queue *q);
-extern void blk_rq_prep_clone(struct request *rq, struct request *rq_src);
+extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
+			     struct bio_set *bs, gfp_t gfp_mask,
+			     int (*bio_ctr)(struct bio *, struct bio *, void *),
+			     void *data);
+extern void blk_rq_unprep_clone(struct request *rq);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_delay_queue(struct request_queue *, unsigned long);

commit e4bc13adfd016fc1036838170288b5680d1a98b0
Merge: ad90fb97515b 3e1534cf4a2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 25 16:00:17 2015 -0700

    Merge branch 'for-4.2/writeback' of git://git.kernel.dk/linux-block
    
    Pull cgroup writeback support from Jens Axboe:
     "This is the big pull request for adding cgroup writeback support.
    
      This code has been in development for a long time, and it has been
      simmering in for-next for a good chunk of this cycle too.  This is one
      of those problems that has been talked about for at least half a
      decade, finally there's a solution and code to go with it.
    
      Also see last weeks writeup on LWN:
    
            http://lwn.net/Articles/648292/"
    
    * 'for-4.2/writeback' of git://git.kernel.dk/linux-block: (85 commits)
      writeback, blkio: add documentation for cgroup writeback support
      vfs, writeback: replace FS_CGROUP_WRITEBACK with SB_I_CGROUPWB
      writeback: do foreign inode detection iff cgroup writeback is enabled
      v9fs: fix error handling in v9fs_session_init()
      bdi: fix wrong error return value in cgwb_create()
      buffer: remove unusued 'ret' variable
      writeback: disassociate inodes from dying bdi_writebacks
      writeback: implement foreign cgroup inode bdi_writeback switching
      writeback: add lockdep annotation to inode_to_wb()
      writeback: use unlocked_inode_to_wb transaction in inode_congested()
      writeback: implement unlocked_inode_to_wb transaction and use it for stat updates
      writeback: implement [locked_]inode_to_wb_and_lock_list()
      writeback: implement foreign cgroup inode detection
      writeback: make writeback_control track the inode being written back
      writeback: relocate wb[_try]_get(), wb_put(), inode_{attach|detach}_wb()
      mm: vmscan: disable memcg direct reclaim stalling if cgroup writeback support is in use
      writeback: implement memcg writeback domain based throttling
      writeback: reset wb_domain->dirty_limit[_tstmp] when memcg domain size changes
      writeback: implement memcg wb_domain
      writeback: update wb_over_bg_thresh() to use wb_domain aware operations
      ...

commit ad90fb97515b732bc27a0109baa10af636c3c8cd
Merge: 6a398a3ef404 c546d5db75b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 25 15:22:36 2015 -0700

    Merge branch 'for-4.2/sg' of git://git.kernel.dk/linux-block
    
    Pull asm/scatterlist.h removal from Jens Axboe:
     "We don't have any specific arch scatterlist anymore, since parisc
      finally switched over.  Kill the include"
    
    * 'for-4.2/sg' of git://git.kernel.dk/linux-block:
      remove scatterlist.h generation from arch Kbuild files
      remove <asm/scatterlist.h>

commit bfffa1cc9db8a950dd4b1a09999f8a20e69a6652
Merge: cc8a0a943948 ae994ea97247
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 25 14:29:53 2015 -0700

    Merge branch 'for-4.2/core' of git://git.kernel.dk/linux-block
    
    Pull core block IO update from Jens Axboe:
     "Nothing really major in here, mostly a collection of smaller
      optimizations and cleanups, mixed with various fixes.  In more detail,
      this contains:
    
       - Addition of policy specific data to blkcg for block cgroups.  From
         Arianna Avanzini.
    
       - Various cleanups around command types from Christoph.
    
       - Cleanup of the suspend block I/O path from Christoph.
    
       - Plugging updates from Shaohua and Jeff Moyer, for blk-mq.
    
       - Eliminating atomic inc/dec of both remaining IO count and reference
         count in a bio.  From me.
    
       - Fixes for SG gap and chunk size support for data-less (discards)
         IO, so we can merge these better.  From me.
    
       - Small restructuring of blk-mq shared tag support, freeing drivers
         from iterating hardware queues.  From Keith Busch.
    
       - A few cfq-iosched tweaks, from Tahsin Erdogan and me.  Makes the
         IOPS mode the default for non-rotational storage"
    
    * 'for-4.2/core' of git://git.kernel.dk/linux-block: (35 commits)
      cfq-iosched: fix other locations where blkcg_to_cfqgd() can return NULL
      cfq-iosched: fix sysfs oops when attempting to read unconfigured weights
      cfq-iosched: move group scheduling functions under ifdef
      cfq-iosched: fix the setting of IOPS mode on SSDs
      blktrace: Add blktrace.c to BLOCK LAYER in MAINTAINERS file
      block, cgroup: implement policy-specific per-blkcg data
      block: Make CFQ default to IOPS mode on SSDs
      block: add blk_set_queue_dying() to blkdev.h
      blk-mq: Shared tag enhancements
      block: don't honor chunk sizes for data-less IO
      block: only honor SG gap prevention for merges that contain data
      block: fix returnvar.cocci warnings
      block, dm: don't copy bios for request clones
      block: remove management of bi_remaining when restoring original bi_end_io
      block: replace trylock with mutex_lock in blkdev_reread_part()
      block: export blkdev_reread_part() and __blkdev_reread_part()
      suspend: simplify block I/O handling
      block: collapse bio bit space
      block: remove unused BIO_RW_BLOCK and BIO_EOF flags
      block: remove BIO_EOPNOTSUPP
      ...

commit 3f21c265cd5f7ae867cc0e86a1f6d5093f1963cc
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jun 5 10:57:37 2015 -0600

    block: add blk_set_queue_dying() to blkdev.h
    
    We export this function and NVMe wants to use it, but for some reason
    it was never added to the block header. Do that.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ccaa9aecd593..a31380c35918 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1024,6 +1024,7 @@ bool __must_check blk_get_queue(struct request_queue *);
 struct request_queue *blk_alloc_queue(gfp_t);
 struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);
+extern void blk_set_queue_dying(struct request_queue *);
 
 /*
  * block layer runtime pm functions

commit d40f75a06dd675808eed385d490ba9468200b23f
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:42 2015 -0400

    writeback, blkcg: restructure blk_{set|clear}_queue_congested()
    
    blk_{set|clear}_queue_congested() take @q and set or clear,
    respectively, the congestion state of its bdi's root wb.  Because bdi
    used to be able to handle congestion state only on the root wb, the
    callers of those functions tested whether the congestion is on the
    root blkcg and skipped if not.
    
    This is cumbersome and makes implementation of per cgroup
    bdi_writeback congestion state propagation difficult.  This patch
    renames blk_{set|clear}_queue_congested() to
    blk_{set|clear}_congested(), and makes them take request_list instead
    of request_queue and test whether the specified request_list is the
    root one before updating bdi_writeback congestion state.  This makes
    the tests in the callers unnecessary and simplifies them.
    
    As there are no external users of these functions, the definitions are
    moved from include/linux/blkdev.h to block/blk-core.c.
    
    This patch doesn't introduce any noticeable behavior difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 60d2726a6b62..ab4a27852f1b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -790,25 +790,6 @@ extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 
 extern void blk_queue_bio(struct request_queue *q, struct bio *bio);
 
-/*
- * A queue has just exitted congestion.  Note this in the global counter of
- * congested queues, and wake up anyone who was waiting for requests to be
- * put back.
- */
-static inline void blk_clear_queue_congested(struct request_queue *q, int sync)
-{
-	clear_bdi_congested(&q->backing_dev_info, sync);
-}
-
-/*
- * A queue has just entered congestion.  Flag that in the queue's VM-visible
- * state flags and increment the global gounter of congested queues.
- */
-static inline void blk_set_queue_congested(struct request_queue *q, int sync)
-{
-	set_bdi_congested(&q->backing_dev_info, sync);
-}
-
 extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);

commit 66114cad64bf76a155fec1f0fff0de771cf909d5
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:32 2015 -0400

    writeback: separate out include/linux/backing-dev-defs.h
    
    With the planned cgroup writeback support, backing-dev related
    declarations will be more widely used across block and cgroup;
    unfortunately, including backing-dev.h from include/linux/blkdev.h
    makes cyclic include dependency quite likely.
    
    This patch separates out backing-dev-defs.h which only has the
    essential definitions and updates blkdev.h to include it.  c files
    which need access to more backing-dev details now include
    backing-dev.h directly.  This takes backing-dev.h off the common
    include dependency chain making it a lot easier to use it across block
    and cgroup.
    
    v2: fs/fat build failure fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ccaa9aecd593..60d2726a6b62 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -12,7 +12,7 @@
 #include <linux/timer.h>
 #include <linux/workqueue.h>
 #include <linux/pagemap.h>
-#include <linux/backing-dev.h>
+#include <linux/backing-dev-defs.h>
 #include <linux/wait.h>
 #include <linux/mempool.h>
 #include <linux/bio.h>

commit e548ca4ee4595f65b262661d166310ad8a149bec
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 29 13:11:32 2015 -0600

    block: don't honor chunk sizes for data-less IO
    
    We don't need to honor chunk sizes for IO that doesn't carry any
    data.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9ded80da2c16..ccaa9aecd593 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -903,7 +903,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	if (unlikely(rq->cmd_type == REQ_TYPE_BLOCK_PC))
 		return q->limits.max_hw_sectors;
 
-	if (!q->limits.chunk_sectors)
+	if (!q->limits.chunk_sectors || (rq->cmd_flags & REQ_DISCARD))
 		return blk_queue_get_max_sectors(q, rq->cmd_flags);
 
 	return min(blk_max_size_offset(q, blk_rq_pos(rq)),

commit 183f7802e73e26206558864d1b67e64382257277
Merge: 1c220c69ce0d f6454b049d81
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri May 29 14:17:16 2015 -0400

    Merge remote-tracking branch 'jens/for-4.2/core' into dm-4.2

commit 5f1b670d0bef508a5554d92525f5f6d00d640b38
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri May 22 09:14:04 2015 -0400

    block, dm: don't copy bios for request clones
    
    Currently dm-multipath has to clone the bios for every request sent
    to the lower devices, which wastes cpu cycles and ties down memory.
    
    This patch instead adds a new REQ_CLONE flag that instructs req_bio_endio
    to not complete bios attached to a request, which we set on clone
    requests similar to bios in a flush sequence.  With this change I/O
    errors on a path failure only get propagated to dm-multipath, which
    can then either resubmit the I/O or complete the bios on the original
    request.
    
    I've done some basic testing of this on a Linux target with ALUA support,
    and it survives path failures during I/O nicely.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bc917956a6d0..9ded80da2c16 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -775,11 +775,7 @@ extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
 extern int blk_lld_busy(struct request_queue *q);
-extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
-			     struct bio_set *bs, gfp_t gfp_mask,
-			     int (*bio_ctr)(struct bio *, struct bio *, void *),
-			     void *data);
-extern void blk_rq_unprep_clone(struct request *rq);
+extern void blk_rq_prep_clone(struct request *rq, struct request *rq_src);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_delay_queue(struct request_queue *, unsigned long);

commit 4ecd4fef3a074c8bb43c391a57742c422469ebbd
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 7 09:38:13 2015 +0200

    block: use an atomic_t for mq_freeze_depth
    
    lockdep gets unhappy about the not disabling irqs when using the queue_lock
    around it.  Instead of trying to fix that up just switch to an atomic_t
    and get rid of the lock.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2da818a48097..bc917956a6d0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -444,7 +444,7 @@ struct request_queue {
 	struct mutex		sysfs_lock;
 
 	int			bypass_depth;
-	int			mq_freeze_depth;
+	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
 	bsg_job_fn		*bsg_job_fn;

commit 336b7e1f230912cd8df2497be8dd7be4647d8fc8
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Mon May 11 14:06:32 2015 -0400

    block: remove export for blk_queue_bio
    
    With commit ff36ab345 ("dm: remove request-based logic from
    make_request_fn wrapper") DM no longer calls blk_queue_bio() directly,
    so remove its export.  Doing so required a forward declaration in
    blk-core.c.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7f9a516f24de..5d93a6645e88 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -821,8 +821,6 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern void blk_queue_bio(struct request_queue *q, struct bio *bio);
-
 /*
  * A queue has just exitted congestion.  Note this in the global counter of
  * congested queues, and wake up anyone who was waiting for requests to be

commit a7928c1578c550bd6f4dec62d65132e6db226c57
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:20 2015 +0200

    block: move PM request support to IDE
    
    This removes the request types and hacks from the block code and into the
    old IDE driver.  There is a small amunt of code duplication due to this,
    but it's not too bad.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c2829ba5e738..2da818a48097 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -30,7 +30,6 @@ struct scsi_ioctl_command;
 
 struct request_queue;
 struct elevator_queue;
-struct request_pm_state;
 struct blk_trace;
 struct request;
 struct sg_io_hdr;
@@ -75,8 +74,6 @@ struct request_list {
 enum rq_cmd_type_bits {
 	REQ_TYPE_FS		= 1,	/* fs request */
 	REQ_TYPE_BLOCK_PC,		/* scsi command */
-	REQ_TYPE_PM_SUSPEND,		/* suspend request */
-	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
 };
 
@@ -207,19 +204,6 @@ static inline unsigned short req_get_ioprio(struct request *req)
 	return req->ioprio;
 }
 
-/*
- * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME
- * requests. Some step values could eventually be made generic.
- */
-struct request_pm_state
-{
-	/* PM state machine step value, currently driver specific */
-	int	pm_step;
-	/* requested PM state value (S1, S2, S3, S4, ...) */
-	u32	pm_state;
-	void*	data;		/* for driver use */
-};
-
 #include <linux/elevator.h>
 
 struct blk_queue_ctx;
@@ -601,10 +585,6 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 	(((rq)->cmd_flags & REQ_STARTED) && \
 	 ((rq)->cmd_type == REQ_TYPE_FS))
 
-#define blk_pm_request(rq)	\
-	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \
-	 (rq)->cmd_type == REQ_TYPE_PM_RESUME)
-
 #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 /* rq->queuelist of dequeued request must be list_empty() */
@@ -838,6 +818,7 @@ extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
 extern void __blk_run_queue(struct request_queue *q);
+extern void __blk_run_queue_uncond(struct request_queue *q);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_run_queue_async(struct request_queue *q);
 extern int blk_rq_map_user(struct request_queue *, struct request *,

commit ac7cdff00a33d48d27217560fa3b16d802e5f535
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:19 2015 +0200

    block: remove REQ_TYPE_PM_SHUTDOWN
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6076b9e18dcb..c2829ba5e738 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -77,7 +77,6 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_BLOCK_PC,		/* scsi command */
 	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 	REQ_TYPE_PM_RESUME,		/* resume request */
-	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
 	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
 };
 

commit b0b93b48a30e809240ddd7449a6ad60a5ddf7b4d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:18 2015 +0200

    block: move REQ_TYPE_SENSE to the ide driver
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9cb4d80a4987..6076b9e18dcb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -75,7 +75,6 @@ struct request_list {
 enum rq_cmd_type_bits {
 	REQ_TYPE_FS		= 1,	/* fs request */
 	REQ_TYPE_BLOCK_PC,		/* scsi command */
-	REQ_TYPE_SENSE,			/* sense request */
 	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */

commit b42171ef7d938a66fa52e66a3d911ed63770b5ca
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:17 2015 +0200

    block: move REQ_TYPE_ATA_TASKFILE and REQ_TYPE_ATA_PC to ide.h
    
    These values are only used by the IDE driver, so move them into it
    by allowing drivers to take cmd_type values after the first private
    one.  Note that we have to turn cmd_type into a plain unsigned integer
    so that gcc doesn't complain about mismatching enum types.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 98c90272443b..9cb4d80a4987 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -79,14 +79,7 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
-	REQ_TYPE_DRV_PRIV,		/* driver defined type */
-	/*
-	 * for ATA/ATAPI devices. this really doesn't belong here, ide should
-	 * use REQ_TYPE_DRV_PRIV and use rq->cmd[0] with the range of driver
-	 * private REQ_LB opcodes to differentiate what type of request this is
-	 */
-	REQ_TYPE_ATA_TASKFILE,
-	REQ_TYPE_ATA_PC,
+	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
 };
 
 #define BLK_MAX_CDB	16
@@ -108,7 +101,7 @@ struct request {
 	struct blk_mq_ctx *mq_ctx;
 
 	u64 cmd_flags;
-	enum rq_cmd_type_bits cmd_type;
+	unsigned cmd_type;
 	unsigned long atomic_flags;
 
 	int cpu;

commit 4f8c9510ba71bb54477841bebb90154ef140860f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 17 22:37:16 2015 +0200

    block: rename REQ_TYPE_SPECIAL to REQ_TYPE_DRV_PRIV
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7f9a516f24de..98c90272443b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -79,10 +79,10 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
-	REQ_TYPE_SPECIAL,		/* driver defined type */
+	REQ_TYPE_DRV_PRIV,		/* driver defined type */
 	/*
 	 * for ATA/ATAPI devices. this really doesn't belong here, ide should
-	 * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver
+	 * use REQ_TYPE_DRV_PRIV and use rq->cmd[0] with the range of driver
 	 * private REQ_LB opcodes to differentiate what type of request this is
 	 */
 	REQ_TYPE_ATA_TASKFILE,

commit 84be456f883c4685680fba8e5154b5f72e92957e
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri May 1 12:46:15 2015 +0200

    remove <asm/scatterlist.h>
    
    We don't have any arch specific scatterlist now that parisc switched over
    to the generic one.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7f9a516f24de..504af1e65ce1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -22,8 +22,7 @@
 #include <linux/smp.h>
 #include <linux/rcupdate.h>
 #include <linux/percpu-refcount.h>
-
-#include <asm/scatterlist.h>
+#include <linux/scatterlist.h>
 
 struct module;
 struct scsi_ioctl_command;

commit d427e3c82ef4fc5fbb22c0cef0b040e6767b1028
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Feb 11 14:07:50 2015 +0100

    block: remove unused function blk_bio_map_sg
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bf4ef666d191..7f9a516f24de 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1049,8 +1049,6 @@ extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
-extern int blk_bio_map_sg(struct request_queue *q, struct bio *bio,
-			  struct scatterlist *sglist);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
 

commit 26e49cfc7e988a76bf1e55cef0d9e438e5489180
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Sun Jan 18 16:16:31 2015 +0100

    block: pass iov_iter to the BLOCK_PC mapping functions
    
    Make use of a new interface provided by iov_iter, backed by
    scatter-gather list of iovec, instead of the old interface based on
    sg_iovec. Also use iov_iter_advance() instead of manual iteration.
    
    This commit should contain only literal replacements, without
    functional changes.
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Doug Gilbert <dgilbert@interlog.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    [dpark: add more description in commit message]
    Signed-off-by: Dongsu Park <dongsu.park@profitbricks.com>
    [hch: fixed to do a deep clone of the iov_iter, and to properly use
          the iov_iter direction]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 13e16401a7ce..bf4ef666d191 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -855,8 +855,8 @@ extern int blk_rq_map_user(struct request_queue *, struct request *,
 extern int blk_rq_unmap_user(struct bio *);
 extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
-			       struct rq_map_data *, const struct sg_iovec *,
-			       int, unsigned int, gfp_t);
+			       struct rq_map_data *, const struct iov_iter *,
+			       gfp_t);
 extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,

commit ad9cf3bbd18a94806314741ac8092c3422f5aebe
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue Dec 16 12:54:25 2014 -0500

    block: mark blk-mq devices as stackable
    
    Commit 4ee5eaf4 ("block: add a queue flag for request stacking support")
    introduced the concept of "STACKABLE" and blk-mq devices fit the
    definition in that they establish q->request_fn.  So establish
    QUEUE_FLAG_STACKABLE in QUEUE_FLAG_MQ_DEFAULT.
    
    While not strictly needed (DM _could_ just check for q->mq_ops to assume
    the device is request-based), request-based DM support for blk-mq devices
    benefits from the ability to consistently check for QUEUE_FLAG_STACKABLE
    before allowing a device to be stacked into a request-based DM table.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6f388fd1c11c..13e16401a7ce 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -520,6 +520,7 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
+				 (1 << QUEUE_FLAG_STACKABLE)	|	\
 				 (1 << QUEUE_FLAG_SAME_COMP))
 
 static inline void queue_lockdep_assert_held(struct request_queue *q)

commit ee1b6f7aff94019c09e73837054979063f722046
Author: Shaohua Li <shli@fb.com>
Date:   Thu Jan 15 17:32:25 2015 -0800

    block: support different tag allocation policy
    
    The libata tag allocation is using a round-robin policy. Next patch will
    make libata use block generic tag allocation, so let's add a policy to
    tag allocation.
    
    Currently two policies: FIFO (default) and round-robin.
    
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4c4b732d7556..6f388fd1c11c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -272,7 +272,11 @@ struct blk_queue_tag {
 	int max_depth;			/* what we will send to device */
 	int real_max_depth;		/* what the array can hold */
 	atomic_t refcnt;		/* map can be shared */
+	int alloc_policy;		/* tag allocation policy */
+	int next_tag;			/* next tag */
 };
+#define BLK_TAG_ALLOC_FIFO 0 /* allocate starting from 0 */
+#define BLK_TAG_ALLOC_RR 1 /* allocate starting from last allocated tag */
 
 #define BLK_SCSI_MAX_CMDS	(256)
 #define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
@@ -1139,11 +1143,11 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 extern int blk_queue_start_tag(struct request_queue *, struct request *);
 extern struct request *blk_queue_find_tag(struct request_queue *, int);
 extern void blk_queue_end_tag(struct request_queue *, struct request *);
-extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *);
+extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *, int);
 extern void blk_queue_free_tags(struct request_queue *);
 extern int blk_queue_resize_tags(struct request_queue *, int);
 extern void blk_queue_invalidate_tags(struct request_queue *);
-extern struct blk_queue_tag *blk_init_tags(int);
+extern struct blk_queue_tag *blk_init_tags(int, int);
 extern void blk_free_tags(struct blk_queue_tag *);
 
 static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,

commit d93ba7a5a97c9f315bacdcdb8de4e5f368e7b396
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Jan 20 20:06:30 2015 -0500

    block: Add discard flag to blkdev_issue_zeroout() function
    
    blkdev_issue_discard() will zero a given block range. This is done by
    way of explicit writing, thus provisioning or allocating the blocks on
    disk.
    
    There are use cases where the desired behavior is to zero the blocks but
    unprovision them if possible. The blocks must deterministically contain
    zeroes when they are subsequently read back.
    
    This patch adds a flag to blkdev_issue_zeroout() that provides this
    variant. If the discard flag is set and a block device guarantees
    discard_zeroes_data we will use REQ_DISCARD to clear the block range. If
    the device does not support discard_zeroes_data or if the discard
    request fails we will fall back to first REQ_WRITE_SAME and then a
    regular REQ_WRITE.
    
    Also update the callers of blkdev_issue_zero() to reflect the new flag
    and make sb_issue_zeroout() prefer the discard approach.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e9086be6d9a0..4c4b732d7556 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1162,7 +1162,7 @@ extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
-			sector_t nr_sects, gfp_t gfp_mask);
+		sector_t nr_sects, gfp_t gfp_mask, bool discard);
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {
@@ -1176,7 +1176,7 @@ static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
 	return blkdev_issue_zeroout(sb->s_bdev,
 				    block << (sb->s_blocksize_bits - 9),
 				    nr_blocks << (sb->s_blocksize_bits - 9),
-				    gfp_mask);
+				    gfp_mask, true);
 }
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);

commit dd22f551ac0ad366f92f601835f6623b83adc331
Author: Matthew Wilcox <matthew.r.wilcox@intel.com>
Date:   Wed Jan 7 18:05:34 2015 +0200

    block: Change direct_access calling convention
    
    In order to support accesses to larger chunks of memory, pass in a
    'size' parameter (counted in bytes), and return the amount available at
    that address.
    
    Add a new helper function, bdev_direct_access(), to handle common
    functionality including partition handling, checking the length requested
    is positive, checking for the sector being page-aligned, and checking
    the length of the request does not pass the end of the partition.
    
    Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Boaz Harrosh <boaz@plexistor.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 92f4b4b288dd..e9086be6d9a0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1601,8 +1601,8 @@ struct block_device_operations {
 	int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
-	int (*direct_access) (struct block_device *, sector_t,
-						void **, unsigned long *);
+	long (*direct_access)(struct block_device *, sector_t,
+					void **, unsigned long *pfn, long size);
 	unsigned int (*check_events) (struct gendisk *disk,
 				      unsigned int clearing);
 	/* ->media_changed() is DEPRECATED, use ->check_events() instead */
@@ -1620,6 +1620,8 @@ extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 extern int bdev_read_page(struct block_device *, sector_t, struct page *);
 extern int bdev_write_page(struct block_device *, sector_t, struct page *,
 						struct writeback_control *);
+extern long bdev_direct_access(struct block_device *, sector_t, void **addr,
+						unsigned long *pfn, long size);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit caf292ae5bb9d57198ce001d8b762f7abae3a94d
Merge: 8f4385d590d4 fcbf6a087a7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 13 14:14:23 2014 -0800

    Merge branch 'for-3.19/core' of git://git.kernel.dk/linux-block
    
    Pull block driver core update from Jens Axboe:
     "This is the pull request for the core block IO changes for 3.19.  Not
      a huge round this time, mostly lots of little good fixes:
    
       - Fix a bug in sysfs blktrace interface causing a NULL pointer
         dereference, when enabled/disabled through that API.  From Arianna
         Avanzini.
    
       - Various updates/fixes/improvements for blk-mq:
    
            - A set of updates from Bart, mostly fixing buts in the tag
              handling.
    
            - Cleanup/code consolidation from Christoph.
    
            - Extend queue_rq API to be able to handle batching issues of IO
              requests. NVMe will utilize this shortly. From me.
    
            - A few tag and request handling updates from me.
    
            - Cleanup of the preempt handling for running queues from Paolo.
    
            - Prevent running of unmapped hardware queues from Ming Lei.
    
            - Move the kdump memory limiting check to be in the correct
              location, from Shaohua.
    
            - Initialize all software queues at init time from Takashi. This
              prevents a kobject warning when CPUs are brought online that
              weren't online when a queue was registered.
    
       - Single writeback fix for I_DIRTY clearing from Tejun.  Queued with
         the core IO changes, since it's just a single fix.
    
       - Version X of the __bio_add_page() segment addition retry from
         Maurizio.  Hope the Xth time is the charm.
    
       - Documentation fixup for IO scheduler merging from Jan.
    
       - Introduce (and use) generic IO stat accounting helpers for non-rq
         drivers, from Gu Zheng.
    
       - Kill off artificial limiting of max sectors in a request from
         Christoph"
    
    * 'for-3.19/core' of git://git.kernel.dk/linux-block: (26 commits)
      bio: modify __bio_add_page() to accept pages that don't start a new segment
      blk-mq: Fix uninitialized kobject at CPU hotplugging
      blktrace: don't let the sysfs interface remove trace from running list
      blk-mq: Use all available hardware queues
      blk-mq: Micro-optimize bt_get()
      blk-mq: Fix a race between bt_clear_tag() and bt_get()
      blk-mq: Avoid that __bt_get_word() wraps multiple times
      blk-mq: Fix a use-after-free
      blk-mq: prevent unmapped hw queue from being scheduled
      blk-mq: re-check for available tags after running the hardware queue
      blk-mq: fix hang in bt_get()
      blk-mq: move the kdump check to blk_mq_alloc_tag_set
      blk-mq: cleanup tag free handling
      blk-mq: use 'nr_cpu_ids' as highest CPU ID count for hwq <-> cpu map
      blk: introduce generic io stat accounting help function
      blk-mq: handle the single queue case in blk_mq_hctx_next_cpu
      genhd: check for int overflow in disk_expand_part_tbl()
      blk-mq: add blk_mq_free_hctx_request()
      blk-mq: export blk_mq_free_request()
      blk-mq: use get_cpu/put_cpu instead of preempt_disable/preempt_enable
      ...

commit 92a578b064d0227a3a7fbbdb9e29dbab7f8d400e
Merge: c75059c46293 e3d857e1ae78
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 21:17:00 2014 -0800

    Merge tag 'pm+acpi-3.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management updates from Rafael Wysocki:
     "This time we have some more new material than we used to have during
      the last couple of development cycles.
    
      The most important part of it to me is the introduction of a unified
      interface for accessing device properties provided by platform
      firmware.  It works with Device Trees and ACPI in a uniform way and
      drivers using it need not worry about where the properties come from
      as long as the platform firmware (either DT or ACPI) makes them
      available.  It covers both devices and "bare" device node objects
      without struct device representation as that turns out to be necessary
      in some cases.  This has been in the works for quite a few months (and
      development cycles) and has been approved by all of the relevant
      maintainers.
    
      On top of that, some drivers are switched over to the new interface
      (at25, leds-gpio, gpio_keys_polled) and some additional changes are
      made to the core GPIO subsystem to allow device drivers to manipulate
      GPIOs in the "canonical" way on platforms that provide GPIO
      information in their ACPI tables, but don't assign names to GPIO lines
      (in which case the driver needs to do that on the basis of what it
      knows about the device in question).  That also has been approved by
      the GPIO core maintainers and the rfkill driver is now going to use
      it.
    
      Second is support for hardware P-states in the intel_pstate driver.
      It uses CPUID to detect whether or not the feature is supported by the
      processor in which case it will be enabled by default.  However, it
      can be disabled entirely from the kernel command line if necessary.
    
      Next is support for a platform firmware interface based on ACPI
      operation regions used by the PMIC (Power Management Integrated
      Circuit) chips on the Intel Baytrail-T and Baytrail-T-CR platforms.
      That interface is used for manipulating power resources and for
      thermal management: sensor temperature reporting, trip point setting
      and so on.
    
      Also the ACPI core is now going to support the _DEP configuration
      information in a limited way.  Basically, _DEP it supposed to reflect
      off-the-hierarchy dependencies between devices which may be very
      indirect, like when AML for one device accesses locations in an
      operation region handled by another device's driver (usually, the
      device depended on this way is a serial bus or GPIO controller).  The
      support added this time is sufficient to make the ACPI battery driver
      work on Asus T100A, but it is general enough to be able to cover some
      other use cases in the future.
    
      Finally, we have a new cpufreq driver for the Loongson1B processor.
    
      In addition to the above, there are fixes and cleanups all over the
      place as usual and a traditional ACPICA update to a recent upstream
      release.
    
      As far as the fixes go, the ACPI LPSS (Low-power Subsystem) driver for
      Intel platforms should be able to handle power management of the DMA
      engine correctly, the cpufreq-dt driver should interact with the
      thermal subsystem in a better way and the ACPI backlight driver should
      handle some more corner cases, among other things.
    
      On top of the ACPICA update there are fixes for race conditions in the
      ACPICA's interrupt handling code which might lead to some random and
      strange looking failures on some systems.
    
      In the cleanups department the most visible part is the series of
      commits targeted at getting rid of the CONFIG_PM_RUNTIME configuration
      option.  That was triggered by a discussion regarding the generic
      power domains code during which we realized that trying to support
      certain combinations of PM config options was painful and not really
      worth it, because nobody would use them in production anyway.  For
      this reason, we decided to make CONFIG_PM_SLEEP select
      CONFIG_PM_RUNTIME and that lead to the conclusion that the latter
      became redundant and CONFIG_PM could be used instead of it.  The
      material here makes that replacement in a major part of the tree, but
      there will be at least one more batch of that in the second part of
      the merge window.
    
      Specifics:
    
       - Support for retrieving device properties information from ACPI _DSD
         device configuration objects and a unified device properties
         interface for device drivers (and subsystems) on top of that.  As
         stated above, this works with Device Trees and ACPI and allows
         device drivers to be written in a platform firmware (DT or ACPI)
         agnostic way.  The at25, leds-gpio and gpio_keys_polled drivers are
         now going to use this new interface and the GPIO subsystem is
         additionally modified to allow device drivers to assign names to
         GPIO resources returned by ACPI _CRS objects (in case _DSD is not
         present or does not provide the expected data).  The changes in
         this set are mostly from Mika Westerberg, Rafael J Wysocki, Aaron
         Lu, and Darren Hart with some fixes from others (Fabio Estevam,
         Geert Uytterhoeven).
    
       - Support for Hardware Managed Performance States (HWP) as described
         in Volume 3, section 14.4, of the Intel SDM in the intel_pstate
         driver.  CPUID is used to detect whether or not the feature is
         supported by the processor.  If supported, it will be enabled
         automatically unless the intel_pstate=no_hwp switch is present in
         the kernel command line.  From Dirk Brandewie.
    
       - New Intel Broadwell-H ID for intel_pstate (Dirk Brandewie).
    
       - Support for firmware interface based on ACPI operation regions used
         by the PMIC chips on the Intel Baytrail-T and Baytrail-T-CR
         platforms for power resource control and thermal management (Aaron
         Lu).
    
       - Limited support for retrieving off-the-hierarchy dependencies
         between devices from ACPI _DEP device configuration objects and
         deferred probing support for the ACPI battery driver based on the
         _DEP information to make that driver work on Asus T100A (Lan
         Tianyu).
    
       - New cpufreq driver for the Loongson1B processor (Kelvin Cheung).
    
       - ACPICA update to upstream revision 20141107 which only affects
         tools (Bob Moore).
    
       - Fixes for race conditions in the ACPICA's interrupt handling code
         and in the ACPI code related to system suspend and resume (Lv Zheng
         and Rafael J Wysocki).
    
       - ACPI core fix for an RCU-related issue in the ioremap() regions
         management code that slowed down significantly after CPUs had been
         allowed to enter idle states even if they'd had RCU callbakcs
         queued and triggered some problems in certain proprietary graphics
         driver (and elsewhere).  The fix replaces synchronize_rcu() in that
         code with synchronize_rcu_expedited() which makes the issue go
         away.  From Konstantin Khlebnikov.
    
       - ACPI LPSS (Low-Power Subsystem) driver fix to handle power
         management of the DMA engine included into the LPSS correctly.  The
         problem is that the DMA engine doesn't have ACPI PM support of its
         own and it simply is turned off when the last LPSS device having
         ACPI PM support goes into D3cold.  To work around that, the PM
         domain used by the ACPI LPSS driver is redesigned so at least one
         device with ACPI PM support will be on as long as the DMA engine is
         in use.  From Andy Shevchenko.
    
       - ACPI backlight driver fix to avoid using it on "Win8-compatible"
         systems where it doesn't work and where it was used by default by
         mistake (Aaron Lu).
    
       - Assorted minor ACPI core fixes and cleanups from Tomasz Nowicki,
         Sudeep Holla, Huang Rui, Hanjun Guo, Fabian Frederick, and Ashwin
         Chaugule (mostly related to the upcoming ARM64 support).
    
       - Intel RAPL (Running Average Power Limit) power capping driver fixes
         and improvements including new processor IDs (Jacob Pan).
    
       - Generic power domains modification to power up domains after
         attaching devices to them to meet the expectations of device
         drivers and bus types assuming devices to be accessible at probe
         time (Ulf Hansson).
    
       - Preliminary support for controlling device clocks from the generic
         power domains core code and modifications of the ARM/shmobile
         platform to use that feature (Ulf Hansson).
    
       - Assorted minor fixes and cleanups of the generic power domains core
         code (Ulf Hansson, Geert Uytterhoeven).
    
       - Assorted minor fixes and cleanups of the device clocks control code
         in the PM core (Geert Uytterhoeven, Grygorii Strashko).
    
       - Consolidation of device power management Kconfig options by making
         CONFIG_PM_SLEEP select CONFIG_PM_RUNTIME and removing the latter
         which is now redundant (Rafael J Wysocki and Kevin Hilman).  That
         is the first batch of the changes needed for this purpose.
    
       - Core device runtime power management support code cleanup related
         to the execution of callbacks (Andrzej Hajda).
    
       - cpuidle ARM support improvements (Lorenzo Pieralisi).
    
       - cpuidle cleanup related to the CPUIDLE_FLAG_TIME_VALID flag and a
         new MAINTAINERS entry for ARM Exynos cpuidle (Daniel Lezcano and
         Bartlomiej Zolnierkiewicz).
    
       - New cpufreq driver callback (->ready) to be executed when the
         cpufreq core is ready to use a given policy object and cpufreq-dt
         driver modification to use that callback for cooling device
         registration (Viresh Kumar).
    
       - cpufreq core fixes and cleanups (Viresh Kumar, Vince Hsu, James
         Geboski, Tomeu Vizoso).
    
       - Assorted fixes and cleanups in the cpufreq-pcc, intel_pstate,
         cpufreq-dt, pxa2xx cpufreq drivers (Lenny Szubowicz, Ethan Zhao,
         Stefan Wahren, Petr Cvek).
    
       - OPP (Operating Performance Points) framework modification to allow
         OPPs to be removed too and update of a few cpufreq drivers
         (cpufreq-dt, exynos5440, imx6q, cpufreq) to remove OPPs (added
         during initialization) on driver removal (Viresh Kumar).
    
       - Hibernation core fixes and cleanups (Tina Ruchandani and Markus
         Elfring).
    
       - PM Kconfig fix related to CPU power management (Pankaj Dubey).
    
       - cpupower tool fix (Prarit Bhargava)"
    
    * tag 'pm+acpi-3.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (120 commits)
      i2c-omap / PM: Drop CONFIG_PM_RUNTIME from i2c-omap.c
      dmaengine / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      tools: cpupower: fix return checks for sysfs_get_idlestate_count()
      drivers: sh / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      e1000e / igb / PM: Eliminate CONFIG_PM_RUNTIME
      MMC / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      MFD / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      misc / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      media / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      input / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      leds: leds-gpio: Fix multiple instances registration without 'label' property
      iio / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      hsi / OMAP / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      i2c-hid / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      drm / exynos / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      gpio / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      hwrandom / exynos / PM: Use CONFIG_PM in #ifdef
      block / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
      USB / PM: Drop CONFIG_PM_RUNTIME from the USB core
      PM: Merge the SET*_RUNTIME_PM_OPS() macros
      ...

commit 47fafbc701fecf112cfa580f61db229b9f68aba4
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Dec 4 01:00:23 2014 +0100

    block / PM: Replace CONFIG_PM_RUNTIME with CONFIG_PM
    
    After commit b2b49ccbdd54 (PM: Kconfig: Set PM_RUNTIME if PM_SLEEP is
    selected) PM_RUNTIME is always set if PM is set, so #ifdef blocks
    depending on CONFIG_PM_RUNTIME may now be changed to depend on
    CONFIG_PM.
    
    Replace CONFIG_PM_RUNTIME with CONFIG_PM in the block device core.
    
    Reviewed-by: Aaron Lu <aaron.lu@intel.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aac0f9ea952a..534dc402c54f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -398,7 +398,7 @@ struct request_queue {
 	 */
 	struct kobject mq_kobj;
 
-#ifdef CONFIG_PM_RUNTIME
+#ifdef CONFIG_PM
 	struct device		*dev;
 	int			rpm_status;
 	unsigned int		nr_pending;
@@ -1057,7 +1057,7 @@ extern void blk_put_queue(struct request_queue *);
 /*
  * block layer runtime pm functions
  */
-#ifdef CONFIG_PM_RUNTIME
+#ifdef CONFIG_PM
 extern void blk_pm_runtime_init(struct request_queue *q, struct device *dev);
 extern int blk_pre_runtime_suspend(struct request_queue *q);
 extern void blk_post_runtime_suspend(struct request_queue *q, int err);

commit 125c99bc8b6b108d251169a86324a7ed3c6f3cce
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Nov 3 12:47:47 2014 +0100

    scsi: add new scsi-command flag for tagged commands
    
    Currently scsi piggy backs on the block layer to define the concept
    of a tagged command.  But we want to be able to have block-level host-wide
    tags assigned even for untagged commands like the initial INQUIRY, so add
    a new SCSI-level flag for commands that are tagged at the scsi level, so
    that even commands without that set can have tags assigned to them.  Note
    that this alredy is the case for the blk-mq code path, and this just lets
    the old path catch up with it.
    
    We also set this flag based upon sdev->simple_tags instead of the block
    queue flag, so that it is entirely independent of the block layer tagging,
    and thus always correct even if a driver doesn't use block level tagging
    yet.
    
    Also remove the old blk_rq_tagged; it was only used by SCSI drivers, and
    removing it forces them to look for the proper replacement.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Mike Christie <michaelc@cs.wisc.edu>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aac0f9ea952a..6d76b8b4aa2b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1136,7 +1136,6 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 /*
  * tag stuff
  */
-#define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
 extern int blk_queue_start_tag(struct request_queue *, struct request *);
 extern struct request *blk_queue_find_tag(struct request_queue *, int);
 extern void blk_queue_end_tag(struct request_queue *, struct request *);

commit ad2be3796ff4c834e5c102d5455e76ee07dcf71d
Merge: 12267166c582 ff150a76ae78
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 2 14:39:35 2014 -0800

    Merge tag 'scsi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI fixes from James Bottomley:
     "This is a set of six patches consisting of:
       - two MAINTAINER updates
       - two scsi-mq fixs for the old parallel interface (not every request
         is tagged and we need to set the right flags to populate the SPI
         tag message)
       - a fix for a memory leak in scatterlist traversal caused by a
         preallocation update in 3.17
       - an ipv6 fix for cxgbi"
    
    [ The scatterlist fix also came in separately through the block layer tree ]
    
    * tag 'scsi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:
      MAINTAINERS: ufs - remove self
      MAINTAINERS: change hpsa and cciss maintainer
      libcxgbi : support ipv6 address host_param
      scsi: set REQ_QUEUE for the blk-mq case
      Revert "block: all blk-mq requests are tagged"
      lib/scatterlist: fix memory leak with scsi-mq

commit cb1a5ab6ece7a37da4ac98ee26b0475b7c3ea79e
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Oct 28 20:27:43 2014 -0600

    block: Fix merge logic when CONFIG_BLK_DEV_INTEGRITY is not defined
    
    Commit 4eaf99beadce switched to returning bool and as a result reversed
    the logic of the integrity merge checks.  However, the empty stubs used
    when the block integrity code is compiled out were still returning
    0. Make these stubs return "true".
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reported-by: Michael L. Semon <mlsemon35@gmail.com>
    Tested-by: Michael L. Semon <mlsemon35@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0207a78a8d82..6cbee8395f60 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1583,13 +1583,13 @@ static inline bool blk_integrity_merge_rq(struct request_queue *rq,
 					  struct request *r1,
 					  struct request *r2)
 {
-	return 0;
+	return true;
 }
 static inline bool blk_integrity_merge_bio(struct request_queue *rq,
 					   struct request *r,
 					   struct bio *b)
 {
-	return 0;
+	return true;
 }
 static inline bool blk_integrity_is_initialized(struct gendisk *g)
 {

commit e999dbc254044e8d2a5818d92d205f65bae28f37
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Oct 19 17:13:57 2014 +0200

    Revert "block: all blk-mq requests are tagged"
    
    This reverts commit fb3ccb5da71273e7f0d50b50bc879e50cedd60e7.
    
    SCSI-2/SPI actually needs the tagged/untagged flag in the request to
    work properly.  Revert this patch and add a follow on to set it in
    the right place.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Reported-by: Meelis Roos <mroos@linux.ee>
    Tested-by: Meelis Roos <mroos@linux.ee>
    Cc: stable@vger.kernel.org

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0207a78a8d82..51d0dc2259cf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1136,8 +1136,7 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 /*
  * tag stuff
  */
-#define blk_rq_tagged(rq) \
-	((rq)->mq_ctx || ((rq)->cmd_flags & REQ_QUEUED))
+#define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
 extern int blk_queue_start_tag(struct request_queue *, struct request *);
 extern struct request *blk_queue_find_tag(struct request_queue *, int);
 extern void blk_queue_end_tag(struct request_queue *, struct request *);

commit 34b48db66e08ca1c1bc07cf305d672ac940268dc
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 6 16:08:05 2014 -0700

    block: remove artifical max_hw_sectors cap
    
    Set max_sectors to the value the drivers provides as hardware limit by
    default.  Linux had proper I/O throttling for a long time and doesn't
    rely on a artifically small maximum I/O size anymore.  By not limiting
    the I/O size by default we remove an annoying tuning step required for
    most Linux installation.
    
    Note that both the user, and if absolutely required the driver can still
    impose a limit for FS requests below max_hw_sectors_kb.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0207a78a8d82..74d14dba6fb7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1186,7 +1186,6 @@ extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 enum blk_default_limits {
 	BLK_MAX_SEGMENTS	= 128,
 	BLK_SAFE_MAX_SECTORS	= 255,
-	BLK_DEF_MAX_SECTORS	= 1024,
 	BLK_MAX_SEGMENT_SIZE	= 65536,
 	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
 };

commit d3dc366bbaf07c125561e90d6da4bb147741101a
Merge: 511c41d9e666 e19a8a0ad2d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 18 11:53:51 2014 -0700

    Merge branch 'for-3.18/core' of git://git.kernel.dk/linux-block
    
    Pull core block layer changes from Jens Axboe:
     "This is the core block IO pull request for 3.18.  Apart from the new
      and improved flush machinery for blk-mq, this is all mostly bug fixes
      and cleanups.
    
       - blk-mq timeout updates and fixes from Christoph.
    
       - Removal of REQ_END, also from Christoph.  We pass it through the
         ->queue_rq() hook for blk-mq instead, freeing up one of the request
         bits.  The space was overly tight on 32-bit, so Martin also killed
         REQ_KERNEL since it's no longer used.
    
       - blk integrity updates and fixes from Martin and Gu Zheng.
    
       - Update to the flush machinery for blk-mq from Ming Lei.  Now we
         have a per hardware context flush request, which both cleans up the
         code should scale better for flush intensive workloads on blk-mq.
    
       - Improve the error printing, from Rob Elliott.
    
       - Backing device improvements and cleanups from Tejun.
    
       - Fixup of a misplaced rq_complete() tracepoint from Hannes.
    
       - Make blk_get_request() return error pointers, fixing up issues
         where we NULL deref when a device goes bad or missing.  From Joe
         Lawrence.
    
       - Prep work for drastically reducing the memory consumption of dm
         devices from Junichi Nomura.  This allows creating clone bio sets
         without preallocating a lot of memory.
    
       - Fix a blk-mq hang on certain combinations of queue depths and
         hardware queues from me.
    
       - Limit memory consumption for blk-mq devices for crash dump
         scenarios and drivers that use crazy high depths (certain SCSI
         shared tag setups).  We now just use a single queue and limited
         depth for that"
    
    * 'for-3.18/core' of git://git.kernel.dk/linux-block: (58 commits)
      block: Remove REQ_KERNEL
      blk-mq: allocate cpumask on the home node
      bio-integrity: remove the needless fail handle of bip_slab creating
      block: include func name in __get_request prints
      block: make blk_update_request print prefix match ratelimited prefix
      blk-merge: don't compute bi_phys_segments from bi_vcnt for cloned bio
      block: fix alignment_offset math that assumes io_min is a power-of-2
      blk-mq: Make bt_clear_tag() easier to read
      blk-mq: fix potential hang if rolling wakeup depth is too high
      block: add bioset_create_nobvec()
      block: use bio_clone_fast() in blk_rq_prep_clone()
      block: misplaced rq_complete tracepoint
      sd: Honor block layer integrity handling flags
      block: Replace strnicmp with strncasecmp
      block: Add T10 Protection Information functions
      block: Don't merge requests if integrity flags differ
      block: Integrity checksum flag
      block: Relocate bio integrity flags
      block: Add a disk flag to block integrity profile
      block: Add prefix to block integrity profile flags
      ...

commit 61a04e5b306ab9d6a30f78e86f1f140d7c888304
Author: Michele Curti <michele.curti@gmail.com>
Date:   Thu Oct 9 15:30:17 2014 -0700

    include/linux/blkdev.h: use NULL instead of zero
    
    Quite useless but it shuts up some warnings.
    
    Signed-off-by: Michele Curti <michele.curti@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 518b46555b80..87be398166d3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1564,7 +1564,7 @@ static inline int blk_rq_map_integrity_sg(struct request_queue *q,
 }
 static inline struct blk_integrity *bdev_get_integrity(struct block_device *b)
 {
-	return 0;
+	return NULL;
 }
 static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
 {

commit b8839b8c55f3fdd60dc36abcda7e0266aff7985c
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Oct 8 18:26:13 2014 -0400

    block: fix alignment_offset math that assumes io_min is a power-of-2
    
    The math in both blk_stack_limits() and queue_limit_alignment_offset()
    assume that a block device's io_min (aka minimum_io_size) is always a
    power-of-2.  Fix the math such that it works for non-power-of-2 io_min.
    
    This issue (of alignment_offset != 0) became apparent when testing
    dm-thinp with a thinp blocksize that matches a RAID6 stripesize of
    1280K.  Commit fdfb4c8c1 ("dm thin: set minimum_io_size to pool's data
    block size") unlocked the potential for alignment_offset != 0 due to
    the dm-thin-pool's io_min possibly being a non-power-of-2.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@vger.kernel.org
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 038b40f84c7a..554639249fd6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1279,10 +1279,9 @@ static inline int queue_alignment_offset(struct request_queue *q)
 static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)
 {
 	unsigned int granularity = max(lim->physical_block_size, lim->io_min);
-	unsigned int alignment = (sector << 9) & (granularity - 1);
+	unsigned int alignment = sector_div(sector, granularity >> 9) << 9;
 
-	return (granularity + lim->alignment_offset - alignment)
-		& (granularity - 1);
+	return (granularity + lim->alignment_offset - alignment) % granularity;
 }
 
 static inline int bdev_alignment_offset(struct block_device *bdev)

commit 4eaf99beadcefbf126fa05e66fb40fca999e09fd
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:20:06 2014 -0400

    block: Don't merge requests if integrity flags differ
    
    We'd occasionally merge requests with conflicting integrity flags.
    Introduce a merge helper which checks that the requests have compatible
    integrity payloads.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 773df190a4ee..038b40f84c7a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1497,10 +1497,10 @@ extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
 extern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,
 				   struct scatterlist *);
 extern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);
-extern int blk_integrity_merge_rq(struct request_queue *, struct request *,
-				  struct request *);
-extern int blk_integrity_merge_bio(struct request_queue *, struct request *,
-				   struct bio *);
+extern bool blk_integrity_merge_rq(struct request_queue *, struct request *,
+				   struct request *);
+extern bool blk_integrity_merge_bio(struct request_queue *, struct request *,
+				    struct bio *);
 
 static inline
 struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
@@ -1580,15 +1580,15 @@ static inline unsigned short queue_max_integrity_segments(struct request_queue *
 {
 	return 0;
 }
-static inline int blk_integrity_merge_rq(struct request_queue *rq,
-					 struct request *r1,
-					 struct request *r2)
+static inline bool blk_integrity_merge_rq(struct request_queue *rq,
+					  struct request *r1,
+					  struct request *r2)
 {
 	return 0;
 }
-static inline int blk_integrity_merge_bio(struct request_queue *rq,
-					  struct request *r,
-					  struct bio *b)
+static inline bool blk_integrity_merge_bio(struct request_queue *rq,
+					   struct request *r,
+					   struct bio *b)
 {
 	return 0;
 }

commit aae7df50190a640e51bfe11c93f94741ac82ff0b
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:20:05 2014 -0400

    block: Integrity checksum flag
    
    Make the choice of checksum a per-I/O property by introducing a flag
    that can be inspected by the SCSI layer. There are several reasons for
    this:
    
     1. It allows us to switch choice of checksum without unloading and
        reloading the HBA driver.
    
     2. During error recovery we need to be able to tell the HBA that
        checksums read from disk should not be verified and converted to IP
        checksums.
    
     3. For error injection purposes we need to be able to write a bad guard
        tag to storage. Since the storage device only supports T10 CRC we
        need to be able to disable IP checksum conversion on the HBA.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4600fc63e3fc..773df190a4ee 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1462,6 +1462,7 @@ enum blk_integrity_flags {
 	BLK_INTEGRITY_VERIFY		= 1 << 0,
 	BLK_INTEGRITY_GENERATE		= 1 << 1,
 	BLK_INTEGRITY_DEVICE_CAPABLE	= 1 << 2,
+	BLK_INTEGRITY_IP_CHECKSUM	= 1 << 3,
 };
 
 struct blk_integrity_iter {

commit 3aec2f41a8baeb70aa77556a4e4dcec7d9d70b4d
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:20:03 2014 -0400

    block: Add a disk flag to block integrity profile
    
    So far we have relied on the app tag size to determine whether a disk
    has been formatted with T10 protection information or not. However, not
    all target devices provide application tag storage.
    
    Add a flag to the block integrity profile that indicates whether the
    disk has been formatted with protection information.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@dev.mellanox.co.il>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cf92eb031ae9..4600fc63e3fc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1461,6 +1461,7 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 enum blk_integrity_flags {
 	BLK_INTEGRITY_VERIFY		= 1 << 0,
 	BLK_INTEGRITY_GENERATE		= 1 << 1,
+	BLK_INTEGRITY_DEVICE_CAPABLE	= 1 << 2,
 };
 
 struct blk_integrity_iter {

commit 8288f496eb1b1905c425e92eaf1abbb29119217b
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:20:02 2014 -0400

    block: Add prefix to block integrity profile flags
    
    Add a BLK_ prefix to the integrity profile flags. Also rename the flags
    to be more consistent with the generate/verify terminology in the rest
    of the integrity code.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 24c1e055b8a7..cf92eb031ae9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1458,8 +1458,10 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 
-#define INTEGRITY_FLAG_READ	2	/* verify data integrity on read */
-#define INTEGRITY_FLAG_WRITE	4	/* generate data integrity on write */
+enum blk_integrity_flags {
+	BLK_INTEGRITY_VERIFY		= 1 << 0,
+	BLK_INTEGRITY_GENERATE		= 1 << 1,
+};
 
 struct blk_integrity_iter {
 	void			*prot_buf;

commit 1859308853b19c4daf4afaab910d3d52ac1ec2ff
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:20:01 2014 -0400

    block: Clean up the code used to generate and verify integrity metadata
    
    Instead of the "operate" parameter we pass in a seed value and a pointer
    to a function that can be used to process the integrity metadata. The
    generation function is changed to have a return value to fit into this
    scheme.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d364c42dbf17..24c1e055b8a7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1461,7 +1461,7 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 #define INTEGRITY_FLAG_READ	2	/* verify data integrity on read */
 #define INTEGRITY_FLAG_WRITE	4	/* generate data integrity on write */
 
-struct blk_integrity_exchg {
+struct blk_integrity_iter {
 	void			*prot_buf;
 	void			*data_buf;
 	sector_t		seed;
@@ -1470,12 +1470,11 @@ struct blk_integrity_exchg {
 	const char		*disk_name;
 };
 
-typedef void (integrity_gen_fn) (struct blk_integrity_exchg *);
-typedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);
+typedef int (integrity_processing_fn) (struct blk_integrity_iter *);
 
 struct blk_integrity {
-	integrity_gen_fn	*generate_fn;
-	integrity_vrfy_fn	*verify_fn;
+	integrity_processing_fn	*generate_fn;
+	integrity_processing_fn	*verify_fn;
 
 	unsigned short		flags;
 	unsigned short		tuple_size;

commit 3be91c4a3d090bd700bd6ee5bf457c1bbf189a4f
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:19:59 2014 -0400

    block: Deprecate the use of the term sector in the context of block integrity
    
    The protection interval is not necessarily tied to the logical block
    size of a block device. Stop using the terms "sector" and "sectors".
    
    Going forward we will use the term "seed" to describe the initial
    reference tag value for a given I/O. "Interval" will be used to describe
    the portion of the data buffer that a given piece of protection
    information is associated with.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0bf5d79d9ba0..d364c42dbf17 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1464,9 +1464,9 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 struct blk_integrity_exchg {
 	void			*prot_buf;
 	void			*data_buf;
-	sector_t		sector;
+	sector_t		seed;
 	unsigned int		data_size;
-	unsigned short		sector_size;
+	unsigned short		interval;
 	const char		*disk_name;
 };
 
@@ -1479,7 +1479,7 @@ struct blk_integrity {
 
 	unsigned short		flags;
 	unsigned short		tuple_size;
-	unsigned short		sector_size;
+	unsigned short		interval;
 	unsigned short		tag_size;
 
 	const char		*name;

commit 8492b68bc4025e7bce1d57761bd7c047efda2f81
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:19:57 2014 -0400

    block: Remove integrity tagging functions
    
    None of the filesystems appear interested in using the integrity tagging
    feature. Potentially because very few storage devices actually permit
    using the application tag space.
    
    Remove the tagging functions.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7fcb2caef559..0bf5d79d9ba0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1472,14 +1472,10 @@ struct blk_integrity_exchg {
 
 typedef void (integrity_gen_fn) (struct blk_integrity_exchg *);
 typedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);
-typedef void (integrity_set_tag_fn) (void *, void *, unsigned int);
-typedef void (integrity_get_tag_fn) (void *, void *, unsigned int);
 
 struct blk_integrity {
 	integrity_gen_fn	*generate_fn;
 	integrity_vrfy_fn	*verify_fn;
-	integrity_set_tag_fn	*set_tag_fn;
-	integrity_get_tag_fn	*get_tag_fn;
 
 	unsigned short		flags;
 	unsigned short		tuple_size;

commit 180b2f95dd331010a9930a65c8a18d6d81b94dc1
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 26 19:19:56 2014 -0400

    block: Replace bi_integrity with bi_special
    
    For commands like REQ_COPY we need a way to pass extra information along
    with each bio. Like integrity metadata this information must be
    available at the bottom of the stack so bi_private does not suffice.
    
    Rename the existing bi_integrity field to bi_special and make it a union
    so we can have different bio extensions for each class of command.
    
    We previously used bi_integrity != NULL as a way to identify whether a
    bio had integrity metadata or not. Introduce a REQ_INTEGRITY to be the
    indicator now that bi_special can contain different things.
    
    In addition, bio_integrity(bio) will now return a pointer to the
    integrity payload (when applicable).
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49f3461e4272..7fcb2caef559 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1514,12 +1514,9 @@ static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
 	return disk->integrity;
 }
 
-static inline int blk_integrity_rq(struct request *rq)
+static inline bool blk_integrity_rq(struct request *rq)
 {
-	if (rq->bio == NULL)
-		return 0;
-
-	return bio_integrity(rq->bio);
+	return rq->cmd_flags & REQ_INTEGRITY;
 }
 
 static inline void blk_queue_max_integrity_segments(struct request_queue *q,

commit 7c94e1c157a227837b04f02f5edeff8301410ba2
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Sep 25 23:23:43 2014 +0800

    block: introduce blk_flush_queue to drive flush machinery
    
    This patch introduces 'struct blk_flush_queue' and puts all
    flush machinery related fields into this structure, so that
    
            - flush implementation details aren't exposed to driver
            - it is easy to convert to per dispatch-queue flush machinery
    
    This patch is basically a mechanical replacement.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e267bf0db559..49f3461e4272 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -36,6 +36,7 @@ struct request;
 struct sg_io_hdr;
 struct bsg_job;
 struct blkcg_gq;
+struct blk_flush_queue;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -455,14 +456,7 @@ struct request_queue {
 	 */
 	unsigned int		flush_flags;
 	unsigned int		flush_not_queueable:1;
-	unsigned int		flush_queue_delayed:1;
-	unsigned int		flush_pending_idx:1;
-	unsigned int		flush_running_idx:1;
-	unsigned long		flush_pending_since;
-	struct list_head	flush_queue[2];
-	struct list_head	flush_data_in_flight;
-	struct request		*flush_rq;
-	spinlock_t		mq_flush_lock;
+	struct blk_flush_queue	*fq;
 
 	struct list_head	requeue_list;
 	spinlock_t		requeue_lock;

commit ff9ea323816dc1c8ac7144afd4eab3ac97704430
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 8 08:03:56 2014 +0900

    block, bdi: an active gendisk always has a request_queue associated with it
    
    bdev_get_queue() returns the request_queue associated with the
    specified block_device.  blk_get_backing_dev_info() makes use of
    bdev_get_queue() to determine the associated bdi given a block_device.
    
    All the callers of bdev_get_queue() including
    blk_get_backing_dev_info() assume that bdev_get_queue() may return
    NULL and implement NULL handling; however, bdev_get_queue() requires
    the passed in block_device is opened and attached to its gendisk.
    Because an active gendisk always has a valid request_queue associated
    with it, bdev_get_queue() can never return NULL and neither can
    blk_get_backing_dev_info().
    
    Make it clear that neither of the two functions can return NULL and
    remove NULL handling from all the callers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 518b46555b80..e267bf0db559 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -865,7 +865,7 @@ extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {
-	return bdev->bd_disk->queue;
+	return bdev->bd_disk->queue;	/* this is never NULL */
 }
 
 /*

commit add703fda981b9719d37f371498b9f129acbd997
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 1 10:34:38 2014 -0600

    blk-mq: use percpu_ref for mq usage count
    
    Currently, blk-mq uses a percpu_counter to keep track of how many
    usages are in flight.  The percpu_counter is drained while freezing to
    ensure that no usage is left in-flight after freezing is complete.
    blk_mq_queue_enter/exit() and blk_mq_[un]freeze_queue() implement this
    per-cpu gating mechanism.
    
    This type of code has relatively high chance of subtle bugs which are
    extremely difficult to trigger and it's way too hairy to be open coded
    in blk-mq.  percpu_ref can serve the same purpose after the recent
    changes.  This patch replaces the open-coded per-cpu usage counting
    and draining mechanism with percpu_ref.
    
    blk_mq_queue_enter() performs tryget_live on the ref and exit()
    performs put.  blk_mq_freeze_queue() kills the ref and waits until the
    reference count reaches zero.  blk_mq_unfreeze_queue() revives the ref
    and wakes up the waiters.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Nicholas A. Bellinger <nab@linux-iscsi.org>
    Cc: Kent Overstreet <kmo@daterainc.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c8f344ff74fe..518b46555b80 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -21,6 +21,7 @@
 #include <linux/bsg.h>
 #include <linux/smp.h>
 #include <linux/rcupdate.h>
+#include <linux/percpu-refcount.h>
 
 #include <asm/scatterlist.h>
 
@@ -484,7 +485,7 @@ struct request_queue {
 #endif
 	struct rcu_head		rcu_head;
 	wait_queue_head_t	mq_freeze_wq;
-	struct percpu_counter	mq_usage_counter;
+	struct percpu_ref	mq_usage_counter;
 	struct list_head	all_q_node;
 
 	struct blk_mq_tag_set	*tag_set;

commit 780db2071ac4d167ee4154ad9c96088f1bba044b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 1 10:31:13 2014 -0600

    blk-mq: decouble blk-mq freezing from generic bypassing
    
    blk_mq freezing is entangled with generic bypassing which bypasses
    blkcg and io scheduler and lets IO requests fall through the block
    layer to the drivers in FIFO order.  This allows forward progress on
    IOs with the advanced features disabled so that those features can be
    configured or altered without worrying about stalling IO which may
    lead to deadlock through memory allocation.
    
    However, generic bypassing doesn't quite fit blk-mq.  blk-mq currently
    doesn't make use of blkcg or ioscheds and it maps bypssing to
    freezing, which blocks request processing and drains all the in-flight
    ones.  This causes problems as bypassing assumes that request
    processing is online.  blk-mq works around this by conditionally
    allowing request processing for the problem case - during queue
    initialization.
    
    Another weirdity is that except for during queue cleanup, bypassing
    started on the generic side prevents blk-mq from processing new
    requests but doesn't drain the in-flight ones.  This shouldn't break
    anything but again highlights that something isn't quite right here.
    
    The root cause is conflating blk-mq freezing and generic bypassing
    which are two different mechanisms.  The only intersecting purpose
    that they serve is during queue cleanup.  Let's properly separate
    blk-mq freezing from generic bypassing and simply use it where
    necessary.
    
    * request_queue->mq_freeze_depth is added and
      blk_mq_[un]freeze_queue() now operate on this counter instead of
      ->bypass_depth.  The replacement for QUEUE_FLAG_BYPASS isn't added
      but the counter is tested directly.  This will be further updated by
      later changes.
    
    * blk_mq_drain_queue() is dropped and "__" prefix is dropped from
      blk_mq_freeze_queue().  Queue cleanup path now calls
      blk_mq_freeze_queue() directly.
    
    * blk_queue_enter()'s fast path condition is simplified to simply
      check @q->mq_freeze_depth.  Previously, the condition was
    
            !blk_queue_dying(q) &&
                (!blk_queue_bypass(q) || !blk_queue_init_done(q))
    
      mq_freeze_depth is incremented right after dying is set and
      blk_queue_init_done() exception isn't necessary as blk-mq doesn't
      start frozen, which only leaves the blk_queue_bypass() test which
      can be replaced by @q->mq_freeze_depth test.
    
    This change simplifies the code and reduces confusion in the area.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Nicholas A. Bellinger <nab@linux-iscsi.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8699bcf5f099..c8f344ff74fe 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -470,6 +470,7 @@ struct request_queue {
 	struct mutex		sysfs_lock;
 
 	int			bypass_depth;
+	int			mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
 	bsg_job_fn		*bsg_job_fn;

commit 66cb45aa41315d1d9972cada354fbdf7870d7714
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jun 24 16:22:24 2014 -0600

    block: add support for limiting gaps in SG lists
    
    Another restriction inherited for NVMe - those devices don't support
    SG lists that have "gaps" in them. Gaps refers to cases where the
    previous SG entry doesn't end on a page boundary. For NVMe, all SG
    entries must start at offset 0 (except the first) and end on a page
    boundary (except the last).
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 713f8b62b435..8699bcf5f099 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -512,6 +512,7 @@ struct request_queue {
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+#define QUEUE_FLAG_SG_GAPS     22	/* queue doesn't support SG gaps */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 736ed4de766d4f0e8e6142dd4f9d73ef61835ed9
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jun 17 22:09:29 2014 -0700

    block: blk_max_size_offset() should check ->max_sectors
    
    Commit 762380ad9322 inadvertently changed a check for max_sectors
    to max_hw_sectors. Revert that part, so we still compare against
    max_sectors.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 31e11051f1ba..713f8b62b435 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -920,7 +920,7 @@ static inline unsigned int blk_max_size_offset(struct request_queue *q,
 					       sector_t offset)
 {
 	if (!q->limits.chunk_sectors)
-		return q->limits.max_hw_sectors;
+		return q->limits.max_sectors;
 
 	return q->limits.chunk_sectors -
 			(offset & (q->limits.chunk_sectors - 1));

commit f27b087b81b70513b8c61ec20596c868f7b93474
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Jun 6 07:57:37 2014 -0600

    block: add blk_rq_set_block_pc()
    
    With the optimizations around not clearing the full request at alloc
    time, we are leaving some of the needed init for REQ_TYPE_BLOCK_PC
    up to the user allocating the request.
    
    Add a blk_rq_set_block_pc() that sets the command type to
    REQ_TYPE_BLOCK_PC, and properly initializes the members associated
    with this type of request. Update callers to use this function instead
    of manipulating rq->cmd_type directly.
    
    Includes fixes from Christoph Hellwig <hch@lst.de> for my half-assed
    attempt.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dc2c703f05fd..31e11051f1ba 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -796,6 +796,7 @@ extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern struct request *blk_make_request(struct request_queue *, struct bio *,
 					gfp_t);
+extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);

commit 762380ad9322951cea4ce9d24864265f9c66a916
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Jun 5 13:38:39 2014 -0600

    block: add notion of a chunk size for request merging
    
    Some drivers have different limits on what size a request should
    optimally be, depending on the offset of the request. Similar to
    dividing a device into chunks. Add a setting that allows the driver
    to inform the block layer of such a chunk size. The block layer will
    then prevent merging across the chunks.
    
    This is needed to optimally support NVMe with a non-zero stripe size.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3cd426e971db..dc2c703f05fd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -280,6 +280,7 @@ struct queue_limits {
 	unsigned long		seg_boundary_mask;
 
 	unsigned int		max_hw_sectors;
+	unsigned int		chunk_sectors;
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
 	unsigned int		physical_block_size;
@@ -910,6 +911,20 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	return q->limits.max_sectors;
 }
 
+/*
+ * Return maximum size of a request at given offset. Only valid for
+ * file system requests.
+ */
+static inline unsigned int blk_max_size_offset(struct request_queue *q,
+					       sector_t offset)
+{
+	if (!q->limits.chunk_sectors)
+		return q->limits.max_hw_sectors;
+
+	return q->limits.chunk_sectors -
+			(offset & (q->limits.chunk_sectors - 1));
+}
+
 static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -917,7 +932,11 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	if (unlikely(rq->cmd_type == REQ_TYPE_BLOCK_PC))
 		return q->limits.max_hw_sectors;
 
-	return blk_queue_get_max_sectors(q, rq->cmd_flags);
+	if (!q->limits.chunk_sectors)
+		return blk_queue_get_max_sectors(q, rq->cmd_flags);
+
+	return min(blk_max_size_offset(q, blk_rq_pos(rq)),
+			blk_queue_get_max_sectors(q, rq->cmd_flags));
 }
 
 static inline unsigned int blk_rq_count_bios(struct request *rq)
@@ -983,6 +1002,7 @@ extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
+extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,

commit 00170fdd0846df7cdb5ad421d3a340440f930b8f
Merge: d09cc3659db4 3ff6db3287e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 4 16:55:13 2014 -0700

    Merge branch 'akpm' (patchbomb from Andrew) into next
    
    Merge misc updates from Andrew Morton:
    
     - a few fixes for 3.16.  Cc'ed to stable so they'll get there somehow.
    
     - various misc fixes and cleanups
    
     - most of the ocfs2 queue.  Review is slow...
    
     - most of MM.  The MM queue is pretty huge this time, but not much in
       the way of feature work.
    
     - some tweaks under kernel/
    
     - printk maintenance work
    
     - updates to lib/
    
     - checkpatch updates
    
     - tweaks to init/
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (276 commits)
      fs/autofs4/dev-ioctl.c: add __init to autofs_dev_ioctl_init
      fs/ncpfs/getopt.c: replace simple_strtoul by kstrtoul
      init/main.c: remove an ifdef
      kthreads: kill CLONE_KERNEL, change kernel_thread(kernel_init) to avoid CLONE_SIGHAND
      init/main.c: add initcall_blacklist kernel parameter
      init/main.c: don't use pr_debug()
      fs/binfmt_flat.c: make old_reloc() static
      fs/binfmt_elf.c: fix bool assignements
      fs/efs: convert printk(KERN_DEBUG to pr_debug
      fs/efs: add pr_fmt / use __func__
      fs/efs: convert printk to pr_foo()
      scripts/checkpatch.pl: device_initcall is not the only __initcall substitute
      checkpatch: check stable email address
      checkpatch: warn on unnecessary void function return statements
      checkpatch: prefer kstrto<foo> to sscanf(buf, "%<lhuidx>", &bar);
      checkpatch: add warning for kmalloc/kzalloc with multiply
      checkpatch: warn on #defines ending in semicolon
      checkpatch: make --strict a default for files in drivers/net and net/
      checkpatch: always warn on missing blank line after variable declaration block
      checkpatch: fix wildcard DT compatible string checking
      ...

commit 47a191fd38ebddb1bd1510ec2bc1085c578c8868
Author: Matthew Wilcox <matthew.r.wilcox@intel.com>
Date:   Wed Jun 4 16:07:46 2014 -0700

    fs/block_dev.c: add bdev_read_page() and bdev_write_page()
    
    A block device driver may choose to provide a rw_page operation.  These
    will be called when the filesystem is attempting to do page sized I/O to
    page cache pages (ie not for direct I/O).  This does preclude I/Os that
    are larger than page size, so this may only be a performance gain for
    some devices.
    
    Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
    Tested-by: Dheeraj Reddy <dheeraj.reddy@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 45cf6e537c83..2f3886e6cc78 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1588,6 +1588,7 @@ static inline bool blk_integrity_is_initialized(struct gendisk *g)
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	void (*release) (struct gendisk *, fmode_t);
+	int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*direct_access) (struct block_device *, sector_t,
@@ -1606,6 +1607,9 @@ struct block_device_operations {
 
 extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 				 unsigned long);
+extern int bdev_read_page(struct block_device *, sector_t, struct page *);
+extern int bdev_write_page(struct block_device *, sector_t, struct page *,
+						struct writeback_control *);
 #else /* CONFIG_BLOCK */
 
 struct block_device;

commit ac13a829f6adb674015ab399594c089990104af7
Author: Fabian Frederick <fabf@skynet.be>
Date:   Wed Jun 4 16:06:27 2014 -0700

    fs/libfs.c: add generic data flush to fsync
    
    Description by Jan Kara:
     "A lot of older filesystems don't properly flush volatile disk caches
      on fsync(2) which can lead to loss of fsynced data after power failure.
    
    This patch makes generic_file_fsync() issue proper cache flush to fix the
    problem.  Sysadmin can use /sys/devices/.../cache_type to tell the system
    it should not send the cache flush."
    
    [akpm@linux-foundation.org: nuke ifdef]
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Suggested-by: Jan Kara <jack@suse.cz>
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8aba35f46f87..45cf6e537c83 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1607,6 +1607,9 @@ struct block_device_operations {
 extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
 				 unsigned long);
 #else /* CONFIG_BLOCK */
+
+struct block_device;
+
 /*
  * stubs for when the block layer is configured out
  */
@@ -1642,6 +1645,12 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 	return false;
 }
 
+static inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
+				     sector_t *error_sector)
+{
+	return 0;
+}
+
 #endif /* CONFIG_BLOCK */
 
 #endif

commit e6cdb0929fe6726ba5203fc5529b74564d98a9e9
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Tue Jun 3 11:24:06 2014 +0800

    blk-mq: fix sparse warning on missed __percpu annotation
    
    'struct blk_mq_ctx' is  __percpu, so add the annotation
    and fix the sparse warning reported from Fengguang:
    
            [block:for-linus 2/3] block/blk-mq.h:75:16: sparse: incorrect
            type in initializer (different address spaces)
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8aba35f46f87..5c6f836afa1b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -335,7 +335,7 @@ struct request_queue {
 	unsigned int		*mq_map;
 
 	/* sw queues */
-	struct blk_mq_ctx	*queue_ctx;
+	struct blk_mq_ctx __percpu	*queue_ctx;
 	unsigned int		nr_queues;
 
 	/* hw dispatch queues */

commit f89ca166460e84620db73d4542f28d34c40a8917
Merge: 879466e6a53c 223023750082
Author: Jens Axboe <axboe@fb.com>
Date:   Fri May 30 08:11:50 2014 -0600

    Merge branch 'for-3.16/core' into for-3.16/drivers
    
    Pulled in for the blk_mq_tag_to_rq() change, which impacts
    mtip32xx.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 05f1dd5315217398fc8d122bdee80f96a9f21274
Author: Jens Axboe <axboe@fb.com>
Date:   Thu May 29 09:53:32 2014 -0600

    block: add queue flag for disabling SG merging
    
    If devices are not SG starved, we waste a lot of time potentially
    collapsing SG segments. Enough that 1.5% of the CPU time goes
    to this, at only 400K IOPS. Add a queue flag, QUEUE_FLAG_NO_SG_MERGE,
    which just returns the number of vectors in a bio instead of looping
    over all segments and checking for collapsible ones.
    
    Add a BLK_MQ_F_SG_MERGE flag so that drivers can opt-in on the sg
    merging, if they so desire.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 098304576d51..695b9fd41efe 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -510,6 +510,7 @@ struct request_queue {
 #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
+#define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit 4d92a9beb39d80a7d8ff7c04ae12a10290105ae5
Author: Jens Axboe <axboe@fb.com>
Date:   Thu May 29 08:09:00 2014 -0600

    block: remove 'magic' from struct blk_plug
    
    I don't think we've ever caught any bugs with this, and there's the
    list poisoning for the plug lists to catch uninitialized cases.
    So remove the magic member and save 8 bytes in the struct.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 913f1c2d3be0..098304576d51 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1060,7 +1060,6 @@ static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
  * schedule() where blk_schedule_flush_plug() is called.
  */
 struct blk_plug {
-	unsigned long magic; /* detect uninitialized use-cases */
 	struct list_head list; /* requests */
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */

commit 6178976500ae61fa7b12ebb2d3de816dc9396388
Merge: 6314a108ec19 d852564f8c88
Author: Jens Axboe <axboe@fb.com>
Date:   Wed May 28 09:50:26 2014 -0600

    Merge branch 'for-3.16/core' into for-3.16/drivers
    
    mtip32xx uses blk_mq_alloc_reserved_request(), so pull in the
    core changes so we have a properly merged end result.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 6fca6a611c27f1f0d90fbe1cc3c229dbf8c09e48
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 28 08:08:02 2014 -0600

    blk-mq: add helper to insert requests from irq context
    
    Both the cache flush state machine and the SCSI midlayer want to submit
    requests from irq context, and the current per-request requeue_work
    unfortunately causes corruption due to sharing with the csd field for
    flushes.  Replace them with a per-request_queue list of requests to
    be requeued.
    
    Based on an earlier test by Ming Lei.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reported-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6bc011a09e82..913f1c2d3be0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,7 +99,6 @@ struct request {
 	struct list_head queuelist;
 	union {
 		struct call_single_data csd;
-		struct work_struct requeue_work;
 		unsigned long fifo_time;
 	};
 
@@ -463,6 +462,10 @@ struct request_queue {
 	struct request		*flush_rq;
 	spinlock_t		mq_flush_lock;
 
+	struct list_head	requeue_list;
+	spinlock_t		requeue_lock;
+	struct work_struct	requeue_work;
+
 	struct mutex		sysfs_lock;
 
 	int			bypass_depth;

commit 0d2602ca30e410e84e8bdf05c84ed5688e0a5a44
Author: Jens Axboe <axboe@fb.com>
Date:   Tue May 13 15:10:52 2014 -0600

    blk-mq: improve support for shared tags maps
    
    This adds support for active queue tracking, meaning that the
    blk-mq tagging maintains a count of active users of a tag set.
    This allows us to maintain a notion of fairness between users,
    so that we can distribute the tag depth evenly without starving
    some users while allowing others to try unfair deep queues.
    
    If sharing of a tag set is detected, each hardware queue will
    track the depth of its own queue. And if this exceeds the total
    depth divided by the number of active queues, the user is actively
    throttled down.
    
    The active queue count is done lazily to avoid bouncing that data
    between submitter and completer. Each hardware queue gets marked
    active when it allocates its first tag, and gets marked inactive
    when 1) the last tag is cleared, and 2) the queue timeout grace
    period has passed.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 94b27210641b..6bc011a09e82 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -481,6 +481,9 @@ struct request_queue {
 	wait_queue_head_t	mq_freeze_wq;
 	struct percpu_counter	mq_usage_counter;
 	struct list_head	all_q_node;
+
+	struct blk_mq_tag_set	*tag_set;
+	struct list_head	tag_set_list;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit af76e555e5e29e08eb8ac1f7878e23dbf0d6741f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 6 12:12:45 2014 +0200

    blk-mq: initialize struct request fields individually
    
    This allows us to avoid a non-atomic memset over ->atomic_flags as well
    as killing lots of duplicate initializations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 20b26d4e53a2..94b27210641b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -90,9 +90,10 @@ enum rq_cmd_type_bits {
 #define BLK_MAX_CDB	16
 
 /*
- * try to put the fields that are referenced together in the same cacheline.
- * if you modify this structure, be sure to check block/blk-core.c:blk_rq_init()
- * as well!
+ * Try to put the fields that are referenced together in the same cacheline.
+ *
+ * If you modify this structure, make sure to update blk_rq_init() and
+ * especially blk_mq_rq_ctx_init() to take care of the added fields.
  */
 struct request {
 	struct list_head queuelist;

commit 49fd524f95cb4cc699d435e0ebb08b1c6220da6d
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 16 10:57:18 2014 -0600

    bsg: update check for rq based driver for blk-mq
    
    bsg currently checks ->request_fn to check whether a queue can
    handle struct request. But with blk-mq, we don't have a request_fn
    yet are request based. Add a queue_is_rq_based() helper and use
    that in bsg, I'm guessing this is not the last place we need to
    update for this. Besides, it better explains what is being
    checked.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 20b26d4e53a2..74ee55fefcf0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -612,6 +612,15 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define rq_data_dir(rq)		(((rq)->cmd_flags & 1) != 0)
 
+/*
+ * Driver can handle struct request, if it either has an old style
+ * request_fn defined, or is blk-mq based.
+ */
+static inline bool queue_is_rq_based(struct request_queue *q)
+{
+	return q->request_fn || q->mq_ops;
+}
+
 static inline unsigned int blk_queue_cluster(struct request_queue *q)
 {
 	return q->limits.cluster;

commit 12120077b2612a243d158605640cd39266906667
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:59 2014 +0200

    block: export blk_finish_request
    
    This allows to mirror the blk-mq code flow for more a more readable I/O
    completion handler in SCSI.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 71288083a46f..20b26d4e53a2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -936,6 +936,7 @@ extern struct request *blk_fetch_request(struct request_queue *q);
  */
 extern bool blk_update_request(struct request *rq, int error,
 			       unsigned int nr_bytes);
+extern void blk_finish_request(struct request *rq, int error);
 extern bool blk_end_request(struct request *rq, int error,
 			    unsigned int nr_bytes);
 extern void blk_end_request_all(struct request *rq, int error);

commit f88a164b72bd51fe4c89e06ac9939f2afe39c7ed
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 16 09:44:58 2014 +0200

    blk-mq: rename mq_flush_work struct request member
    
    We will use this work_struct to requeue scsi commands from the
    completion handler as well, so give it a more generic name.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 95bb551273ab..71288083a46f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -98,7 +98,7 @@ struct request {
 	struct list_head queuelist;
 	union {
 		struct call_single_data csd;
-		struct work_struct mq_flush_work;
+		struct work_struct requeue_work;
 		unsigned long fifo_time;
 	};
 

commit fb3ccb5da71273e7f0d50b50bc879e50cedd60e7
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 14 10:30:12 2014 +0200

    block: all blk-mq requests are tagged
    
    Instead of setting the REQ_QUEUED flag on each of them just take it into
    account in the only macro checking it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index eb5e94803892..95bb551273ab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1101,7 +1101,8 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 /*
  * tag stuff
  */
-#define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
+#define blk_rq_tagged(rq) \
+	((rq)->mq_ctx || ((rq)->cmd_flags & REQ_QUEUED))
 extern int blk_queue_start_tag(struct request_queue *, struct request *);
 extern struct request *blk_queue_find_tag(struct request_queue *, int);
 extern void blk_queue_end_tag(struct request_queue *, struct request *);

commit b4f42e2831ff9b9fa19252265d7c8985d47eefb9
Author: Jens Axboe <axboe@fb.com>
Date:   Thu Apr 10 09:46:28 2014 -0600

    block: remove struct request buffer member
    
    This was used in the olden days, back when onions were proper
    yellow. Basically it mapped to the current buffer to be
    transferred. With highmem being added more than a decade ago,
    most drivers map pages out of a bio, and rq->buffer isn't
    pointing at anything valid.
    
    Convert old style drivers to just use bio_data().
    
    For the discard payload use case, just reference the page
    in the bio.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 86a8df13a5fe..eb5e94803892 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -178,7 +178,6 @@ struct request {
 	unsigned short ioprio;
 
 	void *special;		/* opaque pointer available for LLD use */
-	char *buffer;		/* kaddr of the current segment if available */
 
 	int tag;
 	int errors;

commit f89e0dd9d1a72fdf6b8958bcadfa6abf84f3cae0
Merge: 21f9fcd81593 c9eaa447e77e
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 15 14:02:24 2014 -0600

    Merge tag 'v3.15-rc1' into for-3.16/core
    
    We don't like this, but things have diverged with the blk-mq fixes
    in 3.15-rc1. So merge it in.

commit 5166701b368caea89d57b14bf41cf39e819dad51
Merge: 0a7418f5f569 a786c06d9f27
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 12 14:49:50 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
     "The first vfs pile, with deep apologies for being very late in this
      window.
    
      Assorted cleanups and fixes, plus a large preparatory part of iov_iter
      work.  There's a lot more of that, but it'll probably go into the next
      merge window - it *does* shape up nicely, removes a lot of
      boilerplate, gets rid of locking inconsistencie between aio_write and
      splice_write and I hope to get Kent's direct-io rewrite merged into
      the same queue, but some of the stuff after this point is having
      (mostly trivial) conflicts with the things already merged into
      mainline and with some I want more testing.
    
      This one passes LTP and xfstests without regressions, in addition to
      usual beating.  BTW, readahead02 in ltp syscalls testsuite has started
      giving failures since "mm/readahead.c: fix readahead failure for
      memoryless NUMA nodes and limit readahead pages" - might be a false
      positive, might be a real regression..."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (63 commits)
      missing bits of "splice: fix racy pipe->buffers uses"
      cifs: fix the race in cifs_writev()
      ceph_sync_{,direct_}write: fix an oops on ceph_osdc_new_request() failure
      kill generic_file_buffered_write()
      ocfs2_file_aio_write(): switch to generic_perform_write()
      ceph_aio_write(): switch to generic_perform_write()
      xfs_file_buffered_aio_write(): switch to generic_perform_write()
      export generic_perform_write(), start getting rid of generic_file_buffer_write()
      generic_file_direct_write(): get rid of ppos argument
      btrfs_file_aio_write(): get rid of ppos
      kill the 5th argument of generic_file_buffered_write()
      kill the 4th argument of __generic_file_aio_write()
      lustre: don't open-code kernel_recvmsg()
      ocfs2: don't open-code kernel_recvmsg()
      drbd: don't open-code kernel_recvmsg()
      constify blk_rq_map_user_iov() and friends
      lustre: switch to kernel_sendmsg()
      ocfs2: don't open-code kernel_sendmsg()
      take iov_iter stuff to mm/iov_iter.c
      process_vm_access: tidy up a bit
      ...

commit 360f92c2443073143467a0088daffec96a17910b
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Apr 9 20:27:01 2014 -0600

    block: fix regression with block enabled tagging
    
    Martin reported that his test system would not boot with
    current git, it oopsed with this:
    
    BUG: unable to handle kernel paging request at ffff88046c6c9e80
    IP: [<ffffffff812971e0>] blk_queue_start_tag+0x90/0x150
    PGD 1ddf067 PUD 1de2067 PMD 47fc7d067 PTE 800000046c6c9060
    Oops: 0002 [#1] SMP DEBUG_PAGEALLOC
    Modules linked in: sd_mod lpfc(+) scsi_transport_fc scsi_tgt oracleasm
    rpcsec_gss_krb5 ipv6 igb dca i2c_algo_bit i2c_core hwmon
    CPU: 3 PID: 87 Comm: kworker/u17:1 Not tainted 3.14.0+ #246
    Hardware name: Supermicro X9DRX+-F/X9DRX+-F, BIOS 3.00 07/09/2013
    Workqueue: events_unbound async_run_entry_fn
    task: ffff8802743c2150 ti: ffff880273d02000 task.ti: ffff880273d02000
    RIP: 0010:[<ffffffff812971e0>]  [<ffffffff812971e0>]
    blk_queue_start_tag+0x90/0x150
    RSP: 0018:ffff880273d03a58  EFLAGS: 00010092
    RAX: ffff88046c6c9e78 RBX: ffff880077208e78 RCX: 00000000fffc8da6
    RDX: 00000000fffc186d RSI: 0000000000000009 RDI: 00000000fffc8d9d
    RBP: ffff880273d03a88 R08: 0000000000000001 R09: ffff8800021c2410
    R10: 0000000000000005 R11: 0000000000015b30 R12: ffff88046c5bb8a0
    R13: ffff88046c5c0890 R14: 000000000000001e R15: 000000000000001e
    FS:  0000000000000000(0000) GS:ffff880277b00000(0000)
    knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: ffff88046c6c9e80 CR3: 00000000018f6000 CR4: 00000000000407e0
    Stack:
     ffff880273d03a98 ffff880474b18800 0000000000000000 ffff880474157000
     ffff88046c5c0890 ffff880077208e78 ffff880273d03ae8 ffffffff813b9e62
     ffff880200000010 ffff880474b18968 ffff880474b18848 ffff88046c5c0cd8
    Call Trace:
     [<ffffffff813b9e62>] scsi_request_fn+0xf2/0x510
     [<ffffffff81293167>] __blk_run_queue+0x37/0x50
     [<ffffffff8129ac43>] blk_execute_rq_nowait+0xb3/0x130
     [<ffffffff8129ad24>] blk_execute_rq+0x64/0xf0
     [<ffffffff8108d2b0>] ? bit_waitqueue+0xd0/0xd0
     [<ffffffff813bba35>] scsi_execute+0xe5/0x180
     [<ffffffff813bbe4a>] scsi_execute_req_flags+0x9a/0x110
     [<ffffffffa01b1304>] sd_spinup_disk+0x94/0x460 [sd_mod]
     [<ffffffff81160000>] ? __unmap_hugepage_range+0x200/0x2f0
     [<ffffffffa01b2b9a>] sd_revalidate_disk+0xaa/0x3f0 [sd_mod]
     [<ffffffffa01b2fb8>] sd_probe_async+0xd8/0x200 [sd_mod]
     [<ffffffff8107703f>] async_run_entry_fn+0x3f/0x140
     [<ffffffff8106a1c5>] process_one_work+0x175/0x410
     [<ffffffff8106b373>] worker_thread+0x123/0x400
     [<ffffffff8106b250>] ? manage_workers+0x160/0x160
     [<ffffffff8107104e>] kthread+0xce/0xf0
     [<ffffffff81070f80>] ? kthread_freezable_should_stop+0x70/0x70
     [<ffffffff815f0bac>] ret_from_fork+0x7c/0xb0
     [<ffffffff81070f80>] ? kthread_freezable_should_stop+0x70/0x70
    Code: 48 0f ab 11 72 db 48 81 4b 40 00 00 10 00 89 83 08 01 00 00 48 89
    df 49 8b 04 24 48 89 1c d0 e8 f7 a8 ff ff 49 8b 85 28 05 00 00 <48> 89
    58 08 48 89 03 49 8d 85 28 05 00 00 48 89 43 08 49 89 9d
    RIP  [<ffffffff812971e0>] blk_queue_start_tag+0x90/0x150
     RSP <ffff880273d03a58>
    CR2: ffff88046c6c9e80
    
    Martin bisected and found this to be the problem patch;
    
            commit 6d113398dcf4dfcd9787a4ead738b186f7b7ff0f
            Author: Jan Kara <jack@suse.cz>
            Date:   Mon Feb 24 16:39:54 2014 +0100
    
                block: Stop abusing rq->csd.list in blk-softirq
    
    and the problem was immediately apparent. The patch states that
    it is safe to reuse queuelist at completion time, since it is
    no longer used. However, that is not true if a device is using
    block enabled tagging. If that is the case, then the queuelist
    is reused to keep track of busy tags. If a device also ended
    up using softirq completions, we'd reuse ->queuelist for the
    IPI handling while block tagging was still using it. Boom.
    
    Fix this by adding a new ipi_list list head, and share the
    memory used with the request hash table. The hash table is
    never used after the request is moved to the dispatch list,
    which happens long before any potential completion of the
    request. Add a new request bit for this, so we don't have
    cases that check rq->hash while it could potentially have
    been reused for the IPI completion.
    
    Reported-by: Martin K. Petersen <martin.petersen@oracle.com>
    Tested-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1e1fa3f93d5f..99617cf7dd1a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -118,7 +118,18 @@ struct request {
 	struct bio *bio;
 	struct bio *biotail;
 
-	struct hlist_node hash;	/* merge hash */
+	/*
+	 * The hash is used inside the scheduler, and killed once the
+	 * request reaches the dispatch list. The ipi_list is only used
+	 * to queue the request for softirq completion, which is long
+	 * after the request has been unhashed (and even removed from
+	 * the dispatch list).
+	 */
+	union {
+		struct hlist_node hash;	/* merge hash */
+		struct list_head ipi_list;
+	};
+
 	/*
 	 * The rb_node is only used inside the io scheduler, requests
 	 * are pruned when moved to the dispatch queue. So let the

commit 8ab14595b6dffecea264dcca2d6d9eea7c59273a
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 8 09:17:40 2014 -0600

    block: add kblockd_schedule_delayed_work_on()
    
    Same function as kblockd_schedule_delayed_work(), but allow the
    caller to pass in a CPU that the work should be executed on. This
    just directly extends and maps into the workqueue API, and will
    be used to make the blk-mq mappings more strict.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2425945d36ab..5a31307c5ded 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1361,6 +1361,7 @@ static inline void put_dev_sector(Sector p)
 struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
 int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
+int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*

commit 59c3d45e487315e6e05a3f2310b61109f8e503e7
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 8 09:15:35 2014 -0600

    block: remove 'q' parameter from kblockd_schedule_*_work()
    
    The queue parameter is never used, just get rid of it.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1e1fa3f93d5f..2425945d36ab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1359,8 +1359,8 @@ static inline void put_dev_sector(Sector p)
 }
 
 struct work_struct;
-int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
-int kblockd_schedule_delayed_work(struct request_queue *q, struct delayed_work *dwork, unsigned long delay);
+int kblockd_schedule_work(struct work_struct *work);
+int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*

commit 86d564c84c38b1ec06d9f2120d6a7373dcaeff0c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Feb 8 20:42:52 2014 -0500

    constify blk_rq_map_user_iov() and friends
    
    sg_iovec array passed to it can be const
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4afa4f8f6090..a639fd8a6d7b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -823,8 +823,8 @@ extern int blk_rq_map_user(struct request_queue *, struct request *,
 extern int blk_rq_unmap_user(struct bio *);
 extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
-			       struct rq_map_data *, struct sg_iovec *, int,
-			       unsigned int, gfp_t);
+			       struct rq_map_data *, const struct sg_iovec *,
+			       int, unsigned int, gfp_t);
 extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,

commit 8b4922d3173d2eee7b43be8e5caec3ab7d30feff
Author: Jan Kara <jack@suse.cz>
Date:   Mon Feb 24 16:39:52 2014 +0100

    block: Stop abusing csd.list for fifo_time
    
    Block layer currently abuses rq->csd.list.next for storing fifo_time.
    That is a terrible hack and completely unnecessary as well. Union
    achieves the same space saving in a cleaner way.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jens Axboe <axboe@fb.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4afa4f8f6090..1e1fa3f93d5f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,6 +99,7 @@ struct request {
 	union {
 		struct call_single_data csd;
 		struct work_struct mq_flush_work;
+		unsigned long fifo_time;
 	};
 
 	struct request_queue *q;

commit 5e57dc81106b942786f5db8e7ab8788bb9319933
Merge: 0d25e3691186 c8123f8c9cb5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 14 10:45:18 2014 -0800

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Pull block IO fixes from Jens Axboe:
     "Second round of updates and fixes for 3.14-rc2.  Most of this stuff
      has been queued up for a while.  The notable exception is the blk-mq
      changes, which are naturally a bit more in flux still.
    
      The pull request contains:
    
       - Two bug fixes for the new immutable vecs, causing crashes with raid
         or swap.  From Kent.
    
       - Various blk-mq tweaks and fixes from Christoph.  A fix for
         integrity bio's from Nic.
    
       - A few bcache fixes from Kent and Darrick Wong.
    
       - xen-blk{front,back} fixes from David Vrabel, Matt Rushton, Nicolas
         Swenson, and Roger Pau Monne.
    
       - Fix for a vec miscount with integrity vectors from Martin.
    
       - Minor annotations or fixes from Masanari Iida and Rashika Kheria.
    
       - Tweak to null_blk to do more normal FIFO processing of requests
         from Shlomo Pongratz.
    
       - Elevator switching bypass fix from Tejun.
    
       - Softlockup in blkdev_issue_discard() fix when !CONFIG_PREEMPT from
         me"
    
    * 'for-linus' of git://git.kernel.dk/linux-block: (31 commits)
      block: add cond_resched() to potentially long running ioctl discard loop
      xen-blkback: init persistent_purge_work work_struct
      blk-mq: pair blk_mq_start_request / blk_mq_requeue_request
      blk-mq: dont assume rq->errors is set when returning an error from ->queue_rq
      block: Fix cloning of discard/write same bios
      block: Fix type mismatch in ssize_t_blk_mq_tag_sysfs_show
      blk-mq: rework flush sequencing logic
      null_blk: use blk_complete_request and blk_mq_complete_request
      virtio_blk: use blk_mq_complete_request
      blk-mq: rework I/O completions
      fs: Add prototype declaration to appropriate header file include/linux/bio.h
      fs: Mark function as static in fs/bio-integrity.c
      block/null_blk: Fix completion processing from LIFO to FIFO
      block: Explicitly handle discard/write same segments
      block: Fix nr_vecs for inline integrity vectors
      blk-mq: Add bio_integrity setup to blk_mq_make_request
      blk-mq: initialize sg_reserved_size
      blk-mq: handle dma_drain_size
      blk-mq: divert __blk_put_request for MQ ops
      blk-mq: support at_head inserations for blk_execute_rq
      ...

commit 18741986a4b1dc4b1f171634c4191abc3b0fa023
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Feb 10 09:29:00 2014 -0700

    blk-mq: rework flush sequencing logic
    
    Witch to using a preallocated flush_rq for blk-mq similar to what's done
    with the old request path.  This allows us to set up the request properly
    with a tag from the actually allowed range and ->rq_disk as needed by
    some drivers.  To make life easier we also switch to dynamic allocation
    of ->flush_rq for the old path.
    
    This effectively reverts most of
    
        "blk-mq: fix for flush deadlock"
    
    and
    
        "blk-mq: Don't reserve a tag for flush request"
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0375654adb28..b2d25ecbcbc1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -101,7 +101,7 @@ struct request {
 	};
 	union {
 		struct call_single_data csd;
-		struct work_struct mq_flush_data;
+		struct work_struct mq_flush_work;
 	};
 
 	struct request_queue *q;
@@ -451,13 +451,8 @@ struct request_queue {
 	unsigned long		flush_pending_since;
 	struct list_head	flush_queue[2];
 	struct list_head	flush_data_in_flight;
-	union {
-		struct request	flush_rq;
-		struct {
-			spinlock_t mq_flush_lock;
-			struct work_struct mq_flush_work;
-		};
-	};
+	struct request		*flush_rq;
+	spinlock_t		mq_flush_lock;
 
 	struct mutex		sysfs_lock;
 

commit 6897fc22ea01b562b55c6168592bcbd3ee62b006
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Jan 30 15:45:47 2014 -0800

    kernel: use lockless list for smp_call_function_single
    
    Make smp_call_function_single and friends more efficient by using a
    lockless list.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0375654adb28..8678c4322b44 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -95,10 +95,7 @@ enum rq_cmd_type_bits {
  * as well!
  */
 struct request {
-	union {
-		struct list_head queuelist;
-		struct llist_node ll_list;
-	};
+	struct list_head queuelist;
 	union {
 		struct call_single_data csd;
 		struct work_struct mq_flush_data;

commit c78afc6261b09f74abff8c0719b80692a4959768
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jul 11 22:39:53 2013 -0700

    bcache/md: Use raid stripe size
    
    Now that we've got code for raid5/6 stripe awareness, bcache just needs
    to know about the stripes and when writing partial stripes is expensive
    - we probably don't want to enable this optimization for raid1 or 10,
    even though they have stripes. So add a flag to queue_limits.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 02cb6f0ea71d..0375654adb28 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -291,6 +291,7 @@ struct queue_limits {
 	unsigned char		discard_misaligned;
 	unsigned char		cluster;
 	unsigned char		discard_zeroes_data;
+	unsigned char		raid_partial_stripes_expensive;
 };
 
 struct request_queue {

commit 4550dd6c6b062fc5e5b647296d55da22616123c3
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Aug 7 14:26:21 2013 -0700

    block: Immutable bio vecs
    
    This adds a mechanism by which we can advance a bio by an arbitrary
    number of bytes without modifying the biovec: bio->bi_iter.bi_bvec_done
    indicates the number of bytes completed in the current bvec.
    
    Various driver code still needs to be updated to not refer to the bvec
    directly before we can use this for interesting things, like efficient
    bio splitting.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: Paul Clements <Paul.Clements@steeleye.com>
    Cc: drbd-user@lists.linbit.com
    Cc: nbd-general@lists.sourceforge.net

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 337b92a54658..02cb6f0ea71d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -750,9 +750,9 @@ struct req_iterator {
 	__rq_for_each_bio(_iter.bio, _rq)			\
 		bio_for_each_segment(bvl, _iter.bio, _iter.iter)
 
-#define rq_iter_last(rq, _iter)					\
+#define rq_iter_last(bvec, _iter)				\
 		(_iter.bio->bi_next == NULL &&			\
-		 bio_iter_last(_iter.bio, _iter.iter))
+		 bio_iter_last(bvec, _iter.iter))
 
 #ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
 # error	"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform"

commit 7988613b0e5b2638caf6cd493cc78e9595eba19c
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Sat Nov 23 17:19:00 2013 -0800

    block: Convert bio_for_each_segment() to bvec_iter
    
    More prep work for immutable biovecs - with immutable bvecs drivers
    won't be able to use the biovec directly, they'll need to use helpers
    that take into account bio->bi_iter.bi_bvec_done.
    
    This updates callers for the new usage without changing the
    implementation yet.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Ed L. Cashin" <ecashin@coraid.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Paul Clements <Paul.Clements@steeleye.com>
    Cc: Jim Paris <jim@jtan.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Yehuda Sadeh <yehuda@inktank.com>
    Cc: Sage Weil <sage@inktank.com>
    Cc: Alex Elder <elder@inktank.com>
    Cc: ceph-devel@vger.kernel.org
    Cc: Joshua Morris <josh.h.morris@us.ibm.com>
    Cc: Philip Kelleher <pjk1939@linux.vnet.ibm.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: linux390@de.ibm.com
    Cc: Nagalakshmi Nandigama <Nagalakshmi.Nandigama@lsi.com>
    Cc: Sreekanth Reddy <Sreekanth.Reddy@lsi.com>
    Cc: support@lsi.com
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Guo Chao <yan@linux.vnet.ibm.com>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Quoc-Son Anh <quoc-sonx.anh@intel.com>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: "Darrick J. Wong" <darrick.wong@oracle.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: drbd-user@lists.linbit.com
    Cc: nbd-general@lists.sourceforge.net
    Cc: cbe-oss-dev@lists.ozlabs.org
    Cc: xen-devel@lists.xensource.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: linux-raid@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: DL-MPTFusionLinux@lsi.com
    Cc: linux-scsi@vger.kernel.org
    Cc: devel@driverdev.osuosl.org
    Cc: linux-fsdevel@vger.kernel.org
    Cc: cluster-devel@redhat.com
    Cc: linux-mm@kvack.org
    Acked-by: Geoff Levand <geoff@infradead.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1b135d49b279..337b92a54658 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -735,7 +735,7 @@ struct rq_map_data {
 };
 
 struct req_iterator {
-	int i;
+	struct bvec_iter iter;
 	struct bio *bio;
 };
 
@@ -748,10 +748,11 @@ struct req_iterator {
 
 #define rq_for_each_segment(bvl, _rq, _iter)			\
 	__rq_for_each_bio(_iter.bio, _rq)			\
-		bio_for_each_segment(bvl, _iter.bio, _iter.i)
+		bio_for_each_segment(bvl, _iter.bio, _iter.iter)
 
 #define rq_iter_last(rq, _iter)					\
-		(_iter.bio->bi_next == NULL && _iter.i == _iter.bio->bi_vcnt-1)
+		(_iter.bio->bi_next == NULL &&			\
+		 bio_iter_last(_iter.bio, _iter.iter))
 
 #ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
 # error	"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform"

commit 94eddfbeaafa3e8040a2c47d370dea0e58e76941
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 19 09:25:07 2013 -0700

    blk-mq: ensure that we set REQ_IO_STAT so diskstats work
    
    If disk stats are enabled on the queue, a request needs to
    be marked with REQ_IO_STAT for accounting to be active on
    that request. This fixes an issue with virtio-blk not
    showing up in /proc/diskstats after the conversion to
    blk-mq.
    
    Add QUEUE_FLAG_MQ_DEFAULT, setting stats and same cpu-group
    completion on by default.
    
    Reported-by: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f26ec20f6354..1b135d49b279 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -505,6 +505,9 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
+#define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
+				 (1 << QUEUE_FLAG_SAME_COMP))
+
 static inline void queue_lockdep_assert_held(struct request_queue *q)
 {
 	if (q->queue_lock)

commit 320ae51feed5c2f13664aa05a76bec198967e04d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 24 09:20:05 2013 +0100

    blk-mq: new multi-queue block IO queueing mechanism
    
    Linux currently has two models for block devices:
    
    - The classic request_fn based approach, where drivers use struct
      request units for IO. The block layer provides various helper
      functionalities to let drivers share code, things like tag
      management, timeout handling, queueing, etc.
    
    - The "stacked" approach, where a driver squeezes in between the
      block layer and IO submitter. Since this bypasses the IO stack,
      driver generally have to manage everything themselves.
    
    With drivers being written for new high IOPS devices, the classic
    request_fn based driver doesn't work well enough. The design dates
    back to when both SMP and high IOPS was rare. It has problems with
    scaling to bigger machines, and runs into scaling issues even on
    smaller machines when you have IOPS in the hundreds of thousands
    per device.
    
    The stacked approach is then most often selected as the model
    for the driver. But this means that everybody has to re-invent
    everything, and along with that we get all the problems again
    that the shared approach solved.
    
    This commit introduces blk-mq, block multi queue support. The
    design is centered around per-cpu queues for queueing IO, which
    then funnel down into x number of hardware submission queues.
    We might have a 1:1 mapping between the two, or it might be
    an N:M mapping. That all depends on what the hardware supports.
    
    blk-mq provides various helper functions, which include:
    
    - Scalable support for request tagging. Most devices need to
      be able to uniquely identify a request both in the driver and
      to the hardware. The tagging uses per-cpu caches for freed
      tags, to enable cache hot reuse.
    
    - Timeout handling without tracking request on a per-device
      basis. Basically the driver should be able to get a notification,
      if a request happens to fail.
    
    - Optional support for non 1:1 mappings between issue and
      submission queues. blk-mq can redirect IO completions to the
      desired location.
    
    - Support for per-request payloads. Drivers almost always need
      to associate a request structure with some driver private
      command structure. Drivers can tell blk-mq this at init time,
      and then any request handed to the driver will have the
      required size of memory associated with it.
    
    - Support for merging of IO, and plugging. The stacked model
      gets neither of these. Even for high IOPS devices, merging
      sequential IO reduces per-command overhead and thus
      increases bandwidth.
    
    For now, this is provided as a potential 3rd queueing model, with
    the hope being that, as it matures, it can replace both the classic
    and stacked model. That would get us back to having just 1 real
    model for block devices, leaving the stacked approach to dm/md
    devices (as it was originally intended).
    
    Contributions in this patch from the following people:
    
    Shaohua Li <shli@fusionio.com>
    Alexander Gordeev <agordeev@redhat.com>
    Christoph Hellwig <hch@infradead.org>
    Mike Christie <michaelc@cs.wisc.edu>
    Matias Bjorling <m@bjorling.me>
    Jeff Moyer <jmoyer@redhat.com>
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0a8da96274c3..f26ec20f6354 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -8,6 +8,7 @@
 #include <linux/major.h>
 #include <linux/genhd.h>
 #include <linux/list.h>
+#include <linux/llist.h>
 #include <linux/timer.h>
 #include <linux/workqueue.h>
 #include <linux/pagemap.h>
@@ -94,10 +95,17 @@ enum rq_cmd_type_bits {
  * as well!
  */
 struct request {
-	struct list_head queuelist;
-	struct call_single_data csd;
+	union {
+		struct list_head queuelist;
+		struct llist_node ll_list;
+	};
+	union {
+		struct call_single_data csd;
+		struct work_struct mq_flush_data;
+	};
 
 	struct request_queue *q;
+	struct blk_mq_ctx *mq_ctx;
 
 	u64 cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
@@ -213,6 +221,8 @@ struct request_pm_state
 
 #include <linux/elevator.h>
 
+struct blk_queue_ctx;
+
 typedef void (request_fn_proc) (struct request_queue *q);
 typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
@@ -311,6 +321,18 @@ struct request_queue {
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
 
+	struct blk_mq_ops	*mq_ops;
+
+	unsigned int		*mq_map;
+
+	/* sw queues */
+	struct blk_mq_ctx	*queue_ctx;
+	unsigned int		nr_queues;
+
+	/* hw dispatch queues */
+	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	unsigned int		nr_hw_queues;
+
 	/*
 	 * Dispatch queue sorting
 	 */
@@ -359,6 +381,11 @@ struct request_queue {
 	 */
 	struct kobject kobj;
 
+	/*
+	 * mq queue kobject
+	 */
+	struct kobject mq_kobj;
+
 #ifdef CONFIG_PM_RUNTIME
 	struct device		*dev;
 	int			rpm_status;
@@ -423,7 +450,13 @@ struct request_queue {
 	unsigned long		flush_pending_since;
 	struct list_head	flush_queue[2];
 	struct list_head	flush_data_in_flight;
-	struct request		flush_rq;
+	union {
+		struct request	flush_rq;
+		struct {
+			spinlock_t mq_flush_lock;
+			struct work_struct mq_flush_work;
+		};
+	};
 
 	struct mutex		sysfs_lock;
 
@@ -435,14 +468,14 @@ struct request_queue {
 	struct bsg_class_device bsg_dev;
 #endif
 
-#ifdef CONFIG_BLK_CGROUP
-	struct list_head	all_q_node;
-#endif
 #ifdef CONFIG_BLK_DEV_THROTTLING
 	/* Throttle data */
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	wait_queue_head_t	mq_freeze_wq;
+	struct percpu_counter	mq_usage_counter;
+	struct list_head	all_q_node;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
@@ -465,6 +498,7 @@ struct request_queue {
 #define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
 #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
+#define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -537,6 +571,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
+#define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
@@ -1011,6 +1046,7 @@ static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
 struct blk_plug {
 	unsigned long magic; /* detect uninitialized use-cases */
 	struct list_head list; /* requests */
+	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
 };
 #define BLK_MAX_REQUEST_COUNT 16
@@ -1048,7 +1084,10 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 {
 	struct blk_plug *plug = tsk->plug;
 
-	return plug && (!list_empty(&plug->list) || !list_empty(&plug->cb_list));
+	return plug &&
+		(!list_empty(&plug->list) ||
+		 !list_empty(&plug->mq_list) ||
+		 !list_empty(&plug->cb_list));
 }
 
 /*
@@ -1323,6 +1362,7 @@ static inline void put_dev_sector(Sector p)
 
 struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
+int kblockd_schedule_delayed_work(struct request_queue *q, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*

commit 71fe07d040626de7b72244bf6de889c2e0f5aea3
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Oct 4 06:49:11 2013 -0700

    block: remove request ref_count
    
    This reference count has been around since before git history, but the only
    place where it's used is in blk_execute_rq, and ther it is entirely useless
    as it is incremented before submitting the request and decremented in the
    end_io handler before waking up the submitter thread.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f5c7596c93dd..0a8da96274c3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -160,8 +160,6 @@ struct request {
 
 	unsigned short ioprio;
 
-	int ref_count;
-
 	void *special;		/* opaque pointer available for LLD use */
 	char *buffer;		/* kaddr of the current segment if available */
 

commit 5953316dbf90067ebdeca626c34488bc166b73a8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu May 23 12:25:08 2013 +0200

    block: make rq->cmd_flags be 64-bit
    
    We have officially run out of flags in a 32-bit space. Extend it
    to 64-bit even on 32-bit archs.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0e6f765aa1f5..f5c7596c93dd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,7 +99,7 @@ struct request {
 
 	struct request_queue *q;
 
-	unsigned int cmd_flags;
+	u64 cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
 	unsigned long atomic_flags;
 
@@ -570,7 +570,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
-#define rq_data_dir(rq)		((rq)->cmd_flags & 1)
+#define rq_data_dir(rq)		(((rq)->cmd_flags & 1) != 0)
 
 static inline unsigned int blk_queue_cluster(struct request_queue *q)
 {

commit 75afb352991ff1cd3cf5955bfe611de6d83a0c87
Author: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
Date:   Sat Sep 21 13:57:47 2013 -0600

    block: Add nr_bios to block_rq_remap tracepoint
    
    Adding the number of bios in a remapped request to 'block_rq_remap'
    tracepoint.
    
    Request remapper clones bios in a request to track the completion
    status of each bio. So the number of bios can be useful information
    for investigation.
    
    Related discussions:
      http://www.redhat.com/archives/dm-devel/2013-August/msg00084.html
      http://www.redhat.com/archives/dm-devel/2013-September/msg00024.html
    
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2fdb4a451b49..0e6f765aa1f5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -862,6 +862,17 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	return blk_queue_get_max_sectors(q, rq->cmd_flags);
 }
 
+static inline unsigned int blk_rq_count_bios(struct request *rq)
+{
+	unsigned int nr_bios = 0;
+	struct bio *bio;
+
+	__rq_for_each_bio(bio, rq)
+		nr_bios++;
+
+	return nr_bios;
+}
+
 /*
  * Request issue related functions.
  */

commit 4de13d7aa8f4d02f4dc99d4609575659f92b3c5a
Merge: 5af43c24ca59 b8d4a5bf6a04
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 8 10:13:35 2013 -0700

    Merge branch 'for-3.10/core' of git://git.kernel.dk/linux-block
    
    Pull block core updates from Jens Axboe:
    
     - Major bit is Kents prep work for immutable bio vecs.
    
     - Stable candidate fix for a scheduling-while-atomic in the queue
       bypass operation.
    
     - Fix for the hang on exceeded rq->datalen 32-bit unsigned when merging
       discard bios.
    
     - Tejuns changes to convert the writeback thread pool to the generic
       workqueue mechanism.
    
     - Runtime PM framework, SCSI patches exists on top of these in James'
       tree.
    
     - A few random fixes.
    
    * 'for-3.10/core' of git://git.kernel.dk/linux-block: (40 commits)
      relay: move remove_buf_file inside relay_close_buf
      partitions/efi.c: replace useless kzalloc's by kmalloc's
      fs/block_dev.c: fix iov_shorten() criteria in blkdev_aio_read()
      block: fix max discard sectors limit
      blkcg: fix "scheduling while atomic" in blk_queue_bypass_start
      Documentation: cfq-iosched: update documentation help for cfq tunables
      writeback: expose the bdi_wq workqueue
      writeback: replace custom worker pool implementation with unbound workqueue
      writeback: remove unused bdi_pending_list
      aoe: Fix unitialized var usage
      bio-integrity: Add explicit field for owner of bip_buf
      block: Add an explicit bio flag for bios that own their bvec
      block: Add bio_alloc_pages()
      block: Convert some code to bio_for_each_segment_all()
      block: Add bio_for_each_segment_all()
      bounce: Refactor __blk_queue_bounce to not use bi_io_vec
      raid1: use bio_copy_data()
      pktcdvd: Use bio_reset() in disabled code to kill bi_idx usage
      pktcdvd: use bio_copy_data()
      block: Add bio_copy_data()
      ...

commit db2a144bedd58b3dcf19950c2f476c58c9f39d18
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 5 21:52:57 2013 -0400

    block_device_operations->release() should return void
    
    The value passed is 0 in all but "it can never happen" cases (and those
    only in a couple of drivers) *and* it would've been lost on the way
    out anyway, even if something tried to pass something meaningful.
    Just don't bother.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 78feda9bbae2..e38cfe77f7f0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1484,7 +1484,7 @@ static inline bool blk_integrity_is_initialized(struct gendisk *g)
 
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
-	int (*release) (struct gendisk *, fmode_t);
+	void (*release) (struct gendisk *, fmode_t);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*direct_access) (struct block_device *, sector_t,

commit 871dd9286e25330c8a581e5dacfa8b1dfe1dd641
Author: James Bottomley <JBottomley@Parallels.com>
Date:   Wed Apr 24 08:52:50 2013 -0600

    block: fix max discard sectors limit
    
    linux-v3.8-rc1 and later support for plug for blkdev_issue_discard with
    commit 0cfbcafcae8b7364b5fa96c2b26ccde7a3a296a9
    (block: add plug for blkdev_issue_discard )
    
    For example,
    1) DISCARD rq-1 with size size 4GB
    2) DISCARD rq-2 with size size 1GB
    
    If these 2 discard requests get merged, final request size will be 5GB.
    
    In this case, request's __data_len field may overflow as it can store
    max 4GB(unsigned int).
    
    This issue was observed while doing mkfs.f2fs on 5GB SD card:
    https://lkml.org/lkml/2013/4/1/292
    
    Info: sector size = 512
    Info: total sectors = 11370496 (in 512bytes)
    Info: zone aligned segment0 blkaddr: 512
    [  257.789764] blk_update_request: bio idx 0 >= vcnt 0
    
    mkfs process gets stuck in D state and I see the following in the dmesg:
    
    [  257.789733] __end_that: dev mmcblk0: type=1, flags=122c8081
    [  257.789764]   sector 4194304, nr/cnr 2981888/4294959104
    [  257.789764]   bio df3840c0, biotail df3848c0, buffer   (null), len
    1526726656
    [  257.789764] blk_update_request: bio idx 0 >= vcnt 0
    [  257.794921] request botched: dev mmcblk0: type=1, flags=122c8081
    [  257.794921]   sector 4194304, nr/cnr 2981888/4294959104
    [  257.794921]   bio df3840c0, biotail df3848c0, buffer   (null), len
    1526726656
    
    This patch fixes this issue.
    
    Reported-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Tested-by: Max Filippov <jcmvbkbc@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 89d89c7162aa..6189bf26b53d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -844,7 +844,7 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 						     unsigned int cmd_flags)
 {
 	if (unlikely(cmd_flags & REQ_DISCARD))
-		return q->limits.max_discard_sectors;
+		return min(q->limits.max_discard_sectors, UINT_MAX >> 9);
 
 	if (unlikely(cmd_flags & REQ_WRITE_SAME))
 		return q->limits.max_write_same_sectors;

commit 6c9546675864f51506af69eca388e5d922942c56
Author: Lin Ming <ming.m.lin@intel.com>
Date:   Sat Mar 23 11:42:26 2013 +0800

    block: add runtime pm helpers
    
    Add runtime pm helper functions:
    
    void blk_pm_runtime_init(struct request_queue *q, struct device *dev)
      - Initialization function for drivers to call.
    
    int blk_pre_runtime_suspend(struct request_queue *q)
      - If any requests are in the queue, mark last busy and return -EBUSY.
        Otherwise set q->rpm_status to RPM_SUSPENDING and return 0.
    
    void blk_post_runtime_suspend(struct request_queue *q, int err)
      - If the suspend succeeded then set q->rpm_status to RPM_SUSPENDED.
        Otherwise set it to RPM_ACTIVE and mark last busy.
    
    void blk_pre_runtime_resume(struct request_queue *q)
      - Set q->rpm_status to RPM_RESUMING.
    
    void blk_post_runtime_resume(struct request_queue *q, int err)
      - If the resume succeeded then set q->rpm_status to RPM_ACTIVE
        and call __blk_run_queue, then mark last busy and autosuspend.
        Otherwise set q->rpm_status to RPM_SUSPENDED.
    
    The idea and API is designed by Alan Stern and described here:
    http://marc.info/?l=linux-scsi&m=133727953625963&w=2
    
    Signed-off-by: Lin Ming <ming.m.lin@intel.com>
    Signed-off-by: Aaron Lu <aaron.lu@intel.com>
    Acked-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 78feda9bbae2..89d89c7162aa 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -361,6 +361,12 @@ struct request_queue {
 	 */
 	struct kobject kobj;
 
+#ifdef CONFIG_PM_RUNTIME
+	struct device		*dev;
+	int			rpm_status;
+	unsigned int		nr_pending;
+#endif
+
 	/*
 	 * queue settings
 	 */
@@ -960,6 +966,27 @@ struct request_queue *blk_alloc_queue(gfp_t);
 struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);
 
+/*
+ * block layer runtime pm functions
+ */
+#ifdef CONFIG_PM_RUNTIME
+extern void blk_pm_runtime_init(struct request_queue *q, struct device *dev);
+extern int blk_pre_runtime_suspend(struct request_queue *q);
+extern void blk_post_runtime_suspend(struct request_queue *q, int err);
+extern void blk_pre_runtime_resume(struct request_queue *q);
+extern void blk_post_runtime_resume(struct request_queue *q, int err);
+#else
+static inline void blk_pm_runtime_init(struct request_queue *q,
+	struct device *dev) {}
+static inline int blk_pre_runtime_suspend(struct request_queue *q)
+{
+	return -ENOSYS;
+}
+static inline void blk_post_runtime_suspend(struct request_queue *q, int err) {}
+static inline void blk_pre_runtime_resume(struct request_queue *q) {}
+static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
+#endif
+
 /*
  * blk_plug permits building a queue of related requests by holding the I/O
  * fragments for a short period. This allows merging of sequential requests

commit ac9a19745196388ae5d828c0be7a1d6e472101f3
Merge: 422765c26389 43114018cb0b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Jan 11 19:53:53 2013 +0100

    Merge branch 'blkcg-cfq-hierarchy' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup into for-3.9/core
    
    Tejun writes:
    
    Hello, Jens.
    
    Please consider pulling from the following branch to receive cfq blkcg
    hierarchy support.  The branch is based on top of v3.8-rc2.
    
      git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git blkcg-cfq-hierarchy
    
    The patchset was reviewd in the following thread.
    
      http://thread.gmane.org/gmane.linux.kernel.cgroups/5571

commit 422765c2638924da10ff363b5eed77924911bdc7
Author: Jianpeng Ma <majianpeng@gmail.com>
Date:   Fri Jan 11 14:46:09 2013 +0100

    block: Remove should_sort judgement when flush blk_plug
    
    In commit 975927b942c932,it add blk_rq_pos to sort rq when flushing.
    Although this commit was used for the situation which blk_plug handled
    multi devices on the same time like md device.
    I think there must be some situations like this but only single
    device.
    So remove the should_sort judgement.
    Because the parameter should_sort is only for this purpose,it can delete
    should_sort from blk_plug.
    
    CC: Shaohua Li <shli@kernel.org>
    Signed-off-by: Jianpeng Ma <majianpeng@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f94bc83011ed..dbe74279f3d6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -974,7 +974,6 @@ struct blk_plug {
 	unsigned long magic; /* detect uninitialized use-cases */
 	struct list_head list; /* requests */
 	struct list_head cb_list; /* md requires an unplug callback */
-	unsigned int should_sort; /* list to be sorted before flushing? */
 };
 #define BLK_MAX_REQUEST_COUNT 16
 

commit 548bc8e1b38e48653a90f48f636f8d253504f8a2
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jan 9 08:05:13 2013 -0800

    block: RCU free request_queue
    
    RCU free request_queue so that blkcg_gq->q can be dereferenced under
    RCU lock.  This will be used to implement hierarchical stats.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f94bc83011ed..406343c43cda 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -19,6 +19,7 @@
 #include <linux/gfp.h>
 #include <linux/bsg.h>
 #include <linux/smp.h>
+#include <linux/rcupdate.h>
 
 #include <asm/scatterlist.h>
 
@@ -437,6 +438,7 @@ struct request_queue {
 	/* Throttle data */
 	struct throtl_data *td;
 #endif
+	struct rcu_head		rcu_head;
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 59771079c18c44e39106f0f30054025acafadb41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 19 07:18:35 2012 -0800

    blk: avoid divide-by-zero with zero discard granularity
    
    Commit 8dd2cb7e880d ("block: discard granularity might not be power of
    2") changed a couple of 'binary and' operations into modulus operations.
    Which turned the harmless case of a zero discard_granularity into a
    possible divide-by-zero.
    
    The code also had a much more subtle bug: it was doing the modulus of a
    value in bytes using 'sector_t'.  That was always conceptually wrong,
    but didn't actually matter back when the code assumed a power-of-two
    granularity: we only looked at the low bits anyway.
    
    But with potentially arbitrary sector numbers, using a 'sector_t' to
    express bytes is very very wrong: depending on configuration it limits
    the starting offset of the device to just 32 bits, and any overflow
    would result in a wrong value if the modulus wasn't a power-of-two.
    
    So re-write the code to not only protect against the divide-by-zero, but
    to do the starting sector arithmetic in sectors, and using the proper
    types.
    
    [ For any mathematicians out there: it also looks monumentally stupid to
      do the 'modulo granularity' operation *twice*, never mind having a "+
      granularity" in the second modulus op.
    
      But that's the easiest way to avoid negative values or overflow, and
      it is how the original code was done. ]
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Reported-by: Doug Anderson <dianders@chromium.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Shaohua Li <shli@fusionio.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index acb4f7bbbd32..f94bc83011ed 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1188,14 +1188,25 @@ static inline int queue_discard_alignment(struct request_queue *q)
 
 static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)
 {
-	sector_t alignment = sector << 9;
-	alignment = sector_div(alignment, lim->discard_granularity);
+	unsigned int alignment, granularity, offset;
 
 	if (!lim->max_discard_sectors)
 		return 0;
 
-	alignment = lim->discard_granularity + lim->discard_alignment - alignment;
-	return sector_div(alignment, lim->discard_granularity);
+	/* Why are these in bytes, not sectors? */
+	alignment = lim->discard_alignment >> 9;
+	granularity = lim->discard_granularity >> 9;
+	if (!granularity)
+		return 0;
+
+	/* Offset of the partition start in 'granularity' sectors */
+	offset = sector_div(sector, granularity);
+
+	/* And why do we do this modulus *again* in blkdev_issue_discard()? */
+	offset = (granularity + alignment - offset) % granularity;
+
+	/* Turn it back into bytes, gaah */
+	return offset << 9;
 }
 
 static inline int bdev_discard_alignment(struct block_device *bdev)

commit 8dd2cb7e880d2f77fba53b523c99133ad5054cfd
Author: Shaohua Li <shli@kernel.org>
Date:   Fri Dec 14 11:15:36 2012 +0800

    block: discard granularity might not be power of 2
    
    In MD raid case, discard granularity might not be power of 2, for example, a
    4-disk raid5 has 3*chunk_size discard granularity. Correct the calculation for
    such cases.
    
    Reported-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c9d233e727f2..acb4f7bbbd32 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1188,13 +1188,14 @@ static inline int queue_discard_alignment(struct request_queue *q)
 
 static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)
 {
-	unsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);
+	sector_t alignment = sector << 9;
+	alignment = sector_div(alignment, lim->discard_granularity);
 
 	if (!lim->max_discard_sectors)
 		return 0;
 
-	return (lim->discard_granularity + lim->discard_alignment - alignment)
-		& (lim->discard_granularity - 1);
+	alignment = lim->discard_granularity + lim->discard_alignment - alignment;
+	return sector_div(alignment, lim->discard_granularity);
 }
 
 static inline int bdev_discard_alignment(struct block_device *bdev)

commit 24faf6f604efe18236bded4303009fc252913bf0
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Nov 28 13:46:45 2012 +0100

    block: Make blk_cleanup_queue() wait until request_fn finished
    
    Some request_fn implementations, e.g. scsi_request_fn(), unlock
    the queue lock internally. This may result in multiple threads
    executing request_fn for the same queue simultaneously. Keep
    track of the number of active request_fn calls and make sure that
    blk_cleanup_queue() waits until all active request_fn invocations
    have finished. A block driver may start cleaning up resources
    needed by its request_fn as soon as blk_cleanup_queue() finished,
    so blk_cleanup_queue() must wait for all outstanding request_fn
    invocations to finish.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reported-by: Chanho Min <chanho.min@lge.com>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8bc46c250ca4..c9d233e727f2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -378,6 +378,12 @@ struct request_queue {
 
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
+	/*
+	 * Number of active block driver functions for which blk_drain_queue()
+	 * must wait. Must be incremented around functions that unlock the
+	 * queue_lock internally, e.g. scsi_request_fn().
+	 */
+	unsigned int		request_fn_active;
 
 	unsigned int		rq_timeout;
 	struct timer_list	timeout;

commit c246e80d86736312933646896c4157daf511dadc
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Dec 6 14:32:01 2012 +0100

    block: Avoid that request_fn is invoked on a dead queue
    
    A block driver may start cleaning up resources needed by its
    request_fn as soon as blk_cleanup_queue() finished, so request_fn
    must not be invoked after draining finished. This is important
    when blk_run_queue() is invoked without any requests in progress.
    As an example, if blk_drain_queue() and scsi_run_queue() run in
    parallel, blk_drain_queue() may have finished all requests after
    scsi_run_queue() has taken a SCSI device off the starved list but
    before that last function has had a chance to run the queue.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Chanho Min <chanho.min@lge.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aba8246afe72..8bc46c250ca4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -452,6 +452,7 @@ struct request_queue {
 #define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
 #define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
 #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
+#define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -522,6 +523,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
+#define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\

commit 3f3299d5c0268d6cc3f47b446e8aca436e4a5651
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Wed Nov 28 13:42:38 2012 +0100

    block: Rename queue dead flag
    
    QUEUE_FLAG_DEAD is used to indicate that queuing new requests must
    stop. After this flag has been set queue draining starts. However,
    during the queue draining phase it is still safe to invoke the
    queue's request_fn, so QUEUE_FLAG_DYING is a better name for this
    flag.
    
    This patch has been generated by running the following command
    over the kernel source tree:
    
    git grep -lEw 'blk_queue_dead|QUEUE_FLAG_DEAD' |
        xargs sed -i.tmp -e 's/blk_queue_dead/blk_queue_dying/g'      \
            -e 's/QUEUE_FLAG_DEAD/QUEUE_FLAG_DYING/g';                \
    sed -i.tmp -e "s/QUEUE_FLAG_DYING$(printf \\t)*5/QUEUE_FLAG_DYING$(printf \\t)5/g" \
        include/linux/blkdev.h;                                       \
    sed -i.tmp -e 's/ DEAD/ DYING/g' -e 's/dead queue/a dying queue/' \
        -e 's/Dead queue/A dying queue/' block/blk-core.c
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: James Bottomley <JBottomley@Parallels.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Chanho Min <chanho.min@lge.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1756001210d2..aba8246afe72 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -437,7 +437,7 @@ struct request_queue {
 #define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
 #define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
 #define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
-#define QUEUE_FLAG_DEAD		5	/* queue being torn down */
+#define QUEUE_FLAG_DYING	5	/* queue being torn down */
 #define QUEUE_FLAG_BYPASS	6	/* act as dumb FIFO queue */
 #define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
@@ -521,7 +521,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
-#define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+#define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\

commit 4363ac7c13a9a4b763c6e8d9fdbfc2468f3b8ca4
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Sep 18 12:19:27 2012 -0400

    block: Implement support for WRITE SAME
    
    The WRITE SAME command supported on some SCSI devices allows the same
    block to be efficiently replicated throughout a block range. Only a
    single logical block is transferred from the host and the storage device
    writes the same data to all blocks described by the I/O.
    
    This patch implements support for WRITE SAME in the block layer. The
    blkdev_issue_write_same() function can be used by filesystems and block
    drivers to replicate a buffer across a block range. This can be used to
    efficiently initialize software RAID devices, etc.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 90f7abe8f183..1756001210d2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -270,6 +270,7 @@ struct queue_limits {
 	unsigned int		io_min;
 	unsigned int		io_opt;
 	unsigned int		max_discard_sectors;
+	unsigned int		max_write_same_sectors;
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;
 
@@ -614,9 +615,20 @@ static inline bool blk_check_merge_flags(unsigned int flags1,
 	if ((flags1 & REQ_SECURE) != (flags2 & REQ_SECURE))
 		return false;
 
+	if ((flags1 & REQ_WRITE_SAME) != (flags2 & REQ_WRITE_SAME))
+		return false;
+
 	return true;
 }
 
+static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
+{
+	if (bio_data(a) == bio_data(b))
+		return true;
+
+	return false;
+}
+
 /*
  * q->prep_rq_fn return values
  */
@@ -818,6 +830,9 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 	if (unlikely(cmd_flags & REQ_DISCARD))
 		return q->limits.max_discard_sectors;
 
+	if (unlikely(cmd_flags & REQ_WRITE_SAME))
+		return q->limits.max_write_same_sectors;
+
 	return q->limits.max_sectors;
 }
 
@@ -886,6 +901,8 @@ extern void blk_queue_max_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
+extern void blk_queue_max_write_same_sectors(struct request_queue *q,
+		unsigned int max_write_same_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_alignment_offset(struct request_queue *q,
@@ -1016,6 +1033,8 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
+extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
+		sector_t nr_sects, gfp_t gfp_mask, struct page *page);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 			sector_t nr_sects, gfp_t gfp_mask);
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
@@ -1193,6 +1212,16 @@ static inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)
 	return queue_discard_zeroes_data(bdev_get_queue(bdev));
 }
 
+static inline unsigned int bdev_write_same(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q)
+		return q->limits.max_write_same_sectors;
+
+	return 0;
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit f31dc1cd490539e2b62a126bc4dc2495b165d772
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Sep 18 12:19:26 2012 -0400

    block: Consolidate command flag and queue limit checks for merges
    
     - blk_check_merge_flags() verifies that cmd_flags / bi_rw are
       compatible. This function is called for both req-req and req-bio
       merging.
    
     - blk_rq_get_max_sectors() and blk_queue_get_max_sectors() can be used
       to query the maximum sector count for a given request or queue. The
       calls will return the right value from the queue limits given the
       type of command (RW, discard, write same, etc.)
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3a6fea7460f1..90f7abe8f183 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -605,6 +605,18 @@ static inline bool rq_mergeable(struct request *rq)
 	return true;
 }
 
+static inline bool blk_check_merge_flags(unsigned int flags1,
+					 unsigned int flags2)
+{
+	if ((flags1 & REQ_DISCARD) != (flags2 & REQ_DISCARD))
+		return false;
+
+	if ((flags1 & REQ_SECURE) != (flags2 & REQ_SECURE))
+		return false;
+
+	return true;
+}
+
 /*
  * q->prep_rq_fn return values
  */
@@ -800,6 +812,25 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
+static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
+						     unsigned int cmd_flags)
+{
+	if (unlikely(cmd_flags & REQ_DISCARD))
+		return q->limits.max_discard_sectors;
+
+	return q->limits.max_sectors;
+}
+
+static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
+{
+	struct request_queue *q = rq->q;
+
+	if (unlikely(rq->cmd_type == REQ_TYPE_BLOCK_PC))
+		return q->limits.max_hw_sectors;
+
+	return blk_queue_get_max_sectors(q, rq->cmd_flags);
+}
+
 /*
  * Request issue related functions.
  */

commit e2a60da74fc8215c68509a89e9a69c66363153db
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Sep 18 12:19:25 2012 -0400

    block: Clean up special command handling logic
    
    Remove special-casing of non-rw fs style requests (discard). The nomerge
    flags are consolidated in blk_types.h, and rq_mergeable() and
    bio_mergeable() have been modified to use them.
    
    bio_is_rw() is used in place of bio_has_data() a few places. This is
    done to to distinguish true reads and writes from other fs type requests
    that carry a payload (e.g. write same).
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4a2ab7c85393..3a6fea7460f1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -540,8 +540,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define blk_account_rq(rq) \
 	(((rq)->cmd_flags & REQ_STARTED) && \
-	 ((rq)->cmd_type == REQ_TYPE_FS || \
-	  ((rq)->cmd_flags & REQ_DISCARD)))
+	 ((rq)->cmd_type == REQ_TYPE_FS))
 
 #define blk_pm_request(rq)	\
 	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \
@@ -595,17 +594,16 @@ static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
 	rl->flags &= ~flag;
 }
 
+static inline bool rq_mergeable(struct request *rq)
+{
+	if (rq->cmd_type != REQ_TYPE_FS)
+		return false;
 
-/*
- * mergeable request must not have _NOMERGE or _BARRIER bit set, nor may
- * it already be started by driver.
- */
-#define RQ_NOMERGE_FLAGS	\
-	(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA | REQ_DISCARD)
-#define rq_mergeable(rq)	\
-	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
-	 (((rq)->cmd_flags & REQ_DISCARD) || \
-	  (rq)->cmd_type == REQ_TYPE_FS))
+	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
+		return false;
+
+	return true;
+}
 
 /*
  * q->prep_rq_fn return values

commit 276f0f5d157bb4a816053f4f3a941dbcd4f76556
Author: Shaohua Li <shli@fusionio.com>
Date:   Thu Aug 9 15:20:23 2012 +0200

    block: disable discard request merge temporarily
    
    The SCSI discard request merge never worked, and looks no solution
    for in future, let's disable it temporarily.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dc632975d54f..4a2ab7c85393 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -601,7 +601,7 @@ static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
  * it already be started by driver.
  */
 #define RQ_NOMERGE_FLAGS	\
-	(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA)
+	(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA | REQ_DISCARD)
 #define rq_mergeable(rq)	\
 	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
 	 (((rq)->cmd_flags & REQ_DISCARD) || \

commit 85b9f66a41eb8ee3f1dfc95707412705463cdd97
Author: Asias He <asias@redhat.com>
Date:   Thu Aug 2 23:42:04 2012 +0200

    block: Add blk_bio_map_sg() helper
    
    Add a helper to map a bio to a scatterlist, modelled after
    blk_rq_map_sg.
    
    This helper is useful for any driver that wants to create
    a scatterlist from its ->make_request_fn method.
    
    Changes in v2:
     - Use __blk_segment_map_sg to avoid duplicated code
     - Add cocbook style function comment
    
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Asias He <asias@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 281516ae8b4e..dc632975d54f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -894,6 +894,8 @@ extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
+extern int blk_bio_map_sg(struct request_queue *q, struct bio *bio,
+			  struct scatterlist *sglist);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
 

commit c6e666345e1b79c62ba82339cc7d55a89cb73f88
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Aug 2 09:48:50 2012 +0200

    block: split discard into aligned requests
    
    When a disk has large discard_granularity and small max_discard_sectors,
    discards are not split with optimal alignment.  In the limit case of
    discard_granularity == max_discard_sectors, no request could be aligned
    correctly, so in fact you might end up with no discarded logical blocks
    at all.
    
    Another example that helps showing the condition in the patch is with
    discard_granularity == 64, max_discard_sectors == 128.  A request that is
    submitted for 256 sectors 2..257 will be split in two: 2..129, 130..257.
    However, only 2 aligned blocks out of 3 are included in the request;
    128..191 may be left intact and not discarded.  With this patch, the
    first request will be truncated to ensure good alignment of what's left,
    and the split will be 2..127, 128..255, 256..257.  The patch will also
    take into account the discard_alignment.
    
    At most one extra request will be introduced, because the first request
    will be reduced by at most granularity-1 sectors, and granularity
    must be less than max_discard_sectors.  Subsequent requests will run
    on round_down(max_discard_sectors, granularity) sectors, as in the
    current code.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Tested-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4e72a9d48232..281516ae8b4e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1139,6 +1139,16 @@ static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector
 		& (lim->discard_granularity - 1);
 }
 
+static inline int bdev_discard_alignment(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (bdev != bdev->bd_contains)
+		return bdev->bd_part->discard_alignment;
+
+	return q->limits.discard_alignment;
+}
+
 static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)
 {
 	if (q->limits.max_discard_sectors && q->limits.discard_zeroes_data == 1)

commit 74018dc3063a2c729fc73041c0a9f03aac995920
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 31 09:08:15 2012 +0200

    blk: pass from_schedule to non-request unplug functions.
    
    This will allow md/raid to know why the unplug was called,
    and will be able to act according - if !from_schedule it
    is safe to perform tasks which could themselves schedule.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 607ca228f47e..4e72a9d48232 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -923,7 +923,7 @@ struct blk_plug {
 #define BLK_MAX_REQUEST_COUNT 16
 
 struct blk_plug_cb;
-typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *);
+typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);
 struct blk_plug_cb {
 	struct list_head list;
 	blk_plug_cb_fn callback;

commit 9cbb17508808f8a6bdd83354b61e126ac4fa6fed
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 31 09:08:14 2012 +0200

    blk: centralize non-request unplug handling.
    
    Both md and umem has similar code for getting notified on an
    blk_finish_plug event.
    Centralize this code in block/ and allow each driver to
    provide its distinctive difference.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3816ce8a08fc..607ca228f47e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -922,11 +922,15 @@ struct blk_plug {
 };
 #define BLK_MAX_REQUEST_COUNT 16
 
+struct blk_plug_cb;
+typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *);
 struct blk_plug_cb {
 	struct list_head list;
-	void (*callback)(struct blk_plug_cb *);
+	blk_plug_cb_fn callback;
+	void *data;
 };
-
+extern struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug,
+					     void *data, int size);
 extern void blk_start_plug(struct blk_plug *);
 extern void blk_finish_plug(struct blk_plug *);
 extern void blk_flush_plug_list(struct blk_plug *, bool);

commit a051661ca6d134c18599498b185b667859d4339b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 26 15:05:44 2012 -0700

    blkcg: implement per-blkg request allocation
    
    Currently, request_queue has one request_list to allocate requests
    from regardless of blkcg of the IO being issued.  When the unified
    request pool is used up, cfq proportional IO limits become meaningless
    - whoever grabs the next request being freed wins the race regardless
    of the configured weights.
    
    This can be easily demonstrated by creating a blkio cgroup w/ very low
    weight, put a program which can issue a lot of random direct IOs there
    and running a sequential IO from a different cgroup.  As soon as the
    request pool is used up, the sequential IO bandwidth crashes.
    
    This patch implements per-blkg request_list.  Each blkg has its own
    request_list and any IO allocates its request from the matching blkg
    making blkcgs completely isolated in terms of request allocation.
    
    * Root blkcg uses the request_list embedded in each request_queue,
      which was renamed to @q->root_rl from @q->rq.  While making blkcg rl
      handling a bit harier, this enables avoiding most overhead for root
      blkcg.
    
    * Queue fullness is properly per request_list but bdi isn't blkcg
      aware yet, so congestion state currently just follows the root
      blkcg.  As writeback isn't aware of blkcg yet, this works okay for
      async congestion but readahead may get the wrong signals.  It's
      better than blkcg completely collapsing with shared request_list but
      needs to be improved with future changes.
    
    * After this change, each block cgroup gets a full request pool making
      resource consumption of each cgroup higher.  This makes allowing
      non-root users to create cgroups less desirable; however, note that
      allowing non-root users to directly manage cgroups is already
      severely broken regardless of this patch - each block cgroup
      consumes kernel memory and skews IO weight (IO weights are not
      hierarchical).
    
    v2: queue-sysfs.txt updated and patch description udpated as suggested
        by Vivek.
    
    v3: blk_get_rl() wasn't checking error return from
        blkg_lookup_create() and may cause oops on lookup failure.  Fix it
        by falling back to root_rl on blkg lookup failures.  This problem
        was spotted by Rakesh Iyer <rni@google.com>.
    
    v4: Updated to accomodate 458f27a982 "block: Avoid missed wakeup in
        request waitqueue".  blk_drain_queue() now wakes up waiters on all
        blkg->rl on the target queue.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f2385ee7c7b2..3816ce8a08fc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -51,7 +51,9 @@ typedef void (rq_end_io_fn)(struct request *, int);
 
 struct request_list {
 	struct request_queue	*q;	/* the queue this rl belongs to */
-
+#ifdef CONFIG_BLK_CGROUP
+	struct blkcg_gq		*blkg;	/* blkg this request pool belongs to */
+#endif
 	/*
 	 * count[], starved[], and wait[] are indexed by
 	 * BLK_RW_SYNC/BLK_RW_ASYNC
@@ -143,6 +145,7 @@ struct request {
 	struct hd_struct *part;
 	unsigned long start_time;
 #ifdef CONFIG_BLK_CGROUP
+	struct request_list *rl;		/* rl this rq is alloced from */
 	unsigned long long start_time_ns;
 	unsigned long long io_start_time_ns;    /* when passed to hardware */
 #endif
@@ -291,9 +294,12 @@ struct request_queue {
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
 	/*
-	 * the queue request freelist, one for reads and one for writes
+	 * If blkcg is not used, @q->root_rl serves all requests.  If blkcg
+	 * is used, root blkg allocates from @q->root_rl and all other
+	 * blkgs from their own blkg->rl.  Which one to use should be
+	 * determined using bio_request_list().
 	 */
-	struct request_list	rq;
+	struct request_list	root_rl;
 
 	request_fn_proc		*request_fn;
 	make_request_fn		*make_request_fn;

commit 5b788ce3e2acac9bf109743b1281d77347cf2101
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Jun 4 20:40:59 2012 -0700

    block: prepare for multiple request_lists
    
    Request allocation is about to be made per-blkg meaning that there'll
    be multiple request lists.
    
    * Make queue full state per request_list.  blk_*queue_full() functions
      are renamed to blk_*rl_full() and takes @rl instead of @q.
    
    * Rename blk_init_free_list() to blk_init_rl() and make it take @rl
      instead of @q.  Also add @gfp_mask parameter.
    
    * Add blk_exit_rl() instead of destroying rl directly from
      blk_release_queue().
    
    * Add request_list->q and make request alloc/free functions -
      blk_free_request(), [__]freed_request(), __get_request() - take @rl
      instead of @q.
    
    This patch doesn't introduce any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7e44ed93f84b..f2385ee7c7b2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -46,7 +46,12 @@ struct blkcg_gq;
 struct request;
 typedef void (rq_end_io_fn)(struct request *, int);
 
+#define BLK_RL_SYNCFULL		(1U << 0)
+#define BLK_RL_ASYNCFULL	(1U << 1)
+
 struct request_list {
+	struct request_queue	*q;	/* the queue this rl belongs to */
+
 	/*
 	 * count[], starved[], and wait[] are indexed by
 	 * BLK_RW_SYNC/BLK_RW_ASYNC
@@ -55,6 +60,7 @@ struct request_list {
 	int			starved[2];
 	mempool_t		*rq_pool;
 	wait_queue_head_t	wait[2];
+	unsigned int		flags;
 };
 
 /*
@@ -562,27 +568,25 @@ static inline bool rq_is_sync(struct request *rq)
 	return rw_is_sync(rq->cmd_flags);
 }
 
-static inline int blk_queue_full(struct request_queue *q, int sync)
+static inline bool blk_rl_full(struct request_list *rl, bool sync)
 {
-	if (sync)
-		return test_bit(QUEUE_FLAG_SYNCFULL, &q->queue_flags);
-	return test_bit(QUEUE_FLAG_ASYNCFULL, &q->queue_flags);
+	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
+
+	return rl->flags & flag;
 }
 
-static inline void blk_set_queue_full(struct request_queue *q, int sync)
+static inline void blk_set_rl_full(struct request_list *rl, bool sync)
 {
-	if (sync)
-		queue_flag_set(QUEUE_FLAG_SYNCFULL, q);
-	else
-		queue_flag_set(QUEUE_FLAG_ASYNCFULL, q);
+	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
+
+	rl->flags |= flag;
 }
 
-static inline void blk_clear_queue_full(struct request_queue *q, int sync)
+static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
 {
-	if (sync)
-		queue_flag_clear(QUEUE_FLAG_SYNCFULL, q);
-	else
-		queue_flag_clear(QUEUE_FLAG_ASYNCFULL, q);
+	unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
+
+	rl->flags &= ~flag;
 }
 
 

commit 8a5ecdd42862bf87ceab00bf2a15d7eabf58c02d
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Jun 4 20:40:58 2012 -0700

    block: add q->nr_rqs[] and move q->rq.elvpriv to q->nr_rqs_elvpriv
    
    Add q->nr_rqs[] which currently behaves the same as q->rq.count[] and
    move q->rq.elvpriv to q->nr_rqs_elvpriv.  blk_drain_queue() is updated
    to use q->nr_rqs[] instead of q->rq.count[].
    
    These counters separates queue-wide request statistics from the
    request list and allow implementation of per-queue request allocation.
    
    While at it, properly indent fields of struct request_list.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 07954b05b86c..7e44ed93f84b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -51,11 +51,10 @@ struct request_list {
 	 * count[], starved[], and wait[] are indexed by
 	 * BLK_RW_SYNC/BLK_RW_ASYNC
 	 */
-	int count[2];
-	int starved[2];
-	int elvpriv;
-	mempool_t *rq_pool;
-	wait_queue_head_t wait[2];
+	int			count[2];
+	int			starved[2];
+	mempool_t		*rq_pool;
+	wait_queue_head_t	wait[2];
 };
 
 /*
@@ -282,6 +281,8 @@ struct request_queue {
 	struct list_head	queue_head;
 	struct request		*last_merge;
 	struct elevator_queue	*elevator;
+	int			nr_rqs[2];	/* # allocated [a]sync rqs */
+	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
 
 	/*
 	 * the queue request freelist, one for reads and one for writes

commit 76aaa5101fffaef12b45b4c01ed0d0528f23dedf
Author: Asias He <asias@redhat.com>
Date:   Thu Jun 14 09:04:07 2012 +0200

    block: Drop dead function blk_abort_queue()
    
    This function was only used by btrfs code in btrfs_abort_devices()
    (seems in a wrong way).
    
    It was removed in commit d07eb9117050c9ed3f78296ebcc06128b52693be,
    So, Let's remove the dead code to avoid any confusion.
    
    Changes in v2: update commit log, btrfs_abort_devices() was removed
    already.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-kernel@vger.kernel.org
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: David Sterba <dave@jikos.cz>
    Signed-off-by: Asias He <asias@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba43f408baa3..07954b05b86c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -827,7 +827,6 @@ extern bool __blk_end_request_err(struct request *rq, int error);
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
-extern void blk_abort_queue(struct request_queue *);
 extern void blk_unprep_request(struct request *);
 
 /*

commit 0d167518e045cc8bb63f0a8a0a85ad4fa4e0044f
Merge: 2f83766d4b18 ff26eaadf4d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 30 08:52:42 2012 -0700

    Merge branch 'for-3.5/core' of git://git.kernel.dk/linux-block
    
    Merge block/IO core bits from Jens Axboe:
     "This is a bit bigger on the core side than usual, but that is purely
      because we decided to hold off on parts of Tejun's submission on 3.4
      to give it a bit more time to simmer.  As a consequence, it's seen a
      long cycle in for-next.
    
      It contains:
    
       - Bug fix from Dan, wrong locking type.
       - Relax splice gifting restriction from Eric.
       - A ton of updates from Tejun, primarily for blkcg.  This improves
         the code a lot, making the API nicer and cleaner, and also includes
         fixes for how we handle and tie policies and re-activate on
         switches.  The changes also include generic bug fixes.
       - A simple fix from Vivek, along with a fix for doing proper delayed
         allocation of the blkcg stats."
    
    Fix up annoying conflict just due to different merge resolution in
    Documentation/feature-removal-schedule.txt
    
    * 'for-3.5/core' of git://git.kernel.dk/linux-block: (92 commits)
      blkcg: tg_stats_alloc_lock is an irq lock
      vmsplice: relax alignement requirements for SPLICE_F_GIFT
      blkcg: use radix tree to index blkgs from blkcg
      blkcg: fix blkcg->css ref leak in __blkg_lookup_create()
      block: fix elvpriv allocation failure handling
      block: collapse blk_alloc_request() into get_request()
      blkcg: collapse blkcg_policy_ops into blkcg_policy
      blkcg: embed struct blkg_policy_data in policy specific data
      blkcg: mass rename of blkcg API
      blkcg: style cleanups for blk-cgroup.h
      blkcg: remove blkio_group->path[]
      blkcg: blkg_rwstat_read() was missing inline
      blkcg: shoot down blkgs if all policies are deactivated
      blkcg: drop stuff unused after per-queue policy activation update
      blkcg: implement per-queue policy activation
      blkcg: add request_queue->root_blkg
      blkcg: make request_queue bypassing on allocation
      blkcg: make sure blkg_lookup() returns %NULL if @q is bypassing
      blkcg: make blkg_conf_prep() take @pol and return with queue lock held
      blkcg: remove static policy ID enums
      ...

commit 85fd0bc95bc76d129db0d0114c40665745bbba07
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon May 14 08:29:23 2012 +0200

    Fix blkdev.h build errors when BLOCK=n
    
    I see builds failing with:
    
      CC [M]  drivers/mmc/host/dw_mmc.o
    In file included from drivers/mmc/host/dw_mmc.c:15:
    include/linux/blkdev.h:1404: warning: 'struct task_struct' declared inside parameter list
    include/linux/blkdev.h:1404: warning: its scope is only this definition or declaration, which is probably not what you want
    include/linux/blkdev.h:1408: warning: 'struct task_struct' declared inside parameter list
    include/linux/blkdev.h:1413: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'blk_needs_flush_plug'
    make[4]: *** [drivers/mmc/host/dw_mmc.o] Error 1
    
    This is because dw_mmc.c includes linux/blkdev.h as the very first file,
    and when CONFIG_BLOCK=n, blkdev.h omits all includes.
    
    As it requires linux/sched.h even when CONFIG_BLOCK=n, move this out of
    the #ifdef.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2aa24664a5b5..4d4ac24a263e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1,9 +1,10 @@
 #ifndef _LINUX_BLKDEV_H
 #define _LINUX_BLKDEV_H
 
+#include <linux/sched.h>
+
 #ifdef CONFIG_BLOCK
 
-#include <linux/sched.h>
 #include <linux/major.h>
 #include <linux/genhd.h>
 #include <linux/list.h>

commit 0b7877d4eea3f93e3dd941999522bbd8c538cb53
Merge: bd1a68b59c8e 69964ea4c7b6
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue May 1 14:29:55 2012 +0200

    Merge tag 'v3.4-rc5' into for-3.5/core
    
    The core branch is behind driver commits that we want to build
    on for 3.5, hence I'm pulling in a later -rc.
    
    Linux 3.4-rc5
    
    Conflicts:
            Documentation/feature-removal-schedule.txt
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 3c798398e393e5f9502dbab2b51e6c25e2e8f2ac
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Apr 16 13:57:25 2012 -0700

    blkcg: mass rename of blkcg API
    
    During the recent blkcg cleanup, most of blkcg API has changed to such
    extent that mass renaming wouldn't cause any noticeable pain.  Take
    the chance and cleanup the naming.
    
    * Rename blkio_cgroup to blkcg.
    
    * Drop blkio / blkiocg prefixes and consistently use blkcg.
    
    * Rename blkio_group to blkcg_gq, which is consistent with io_cq but
      keep the blkg prefix / variable name.
    
    * Rename policy method type and field names to signify they're dealing
      with policy data.
    
    * Rename blkio_policy_type to blkcg_policy.
    
    This patch doesn't cause any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 68720ab275d4..af33fb1adfee 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -31,7 +31,7 @@ struct blk_trace;
 struct request;
 struct sg_io_hdr;
 struct bsg_job;
-struct blkio_group;
+struct blkcg_gq;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -371,7 +371,7 @@ struct request_queue {
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
 	DECLARE_BITMAP		(blkcg_pols, BLKCG_MAX_POLS);
-	struct blkio_group	*root_blkg;
+	struct blkcg_gq		*root_blkg;
 	struct list_head	blkg_list;
 #endif
 

commit a2b1693bac45ea3fe3ba612fd22c45f17449f610
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Apr 13 13:11:33 2012 -0700

    blkcg: implement per-queue policy activation
    
    All blkcg policies were assumed to be enabled on all request_queues.
    Due to various implementation obstacles, during the recent blkcg core
    updates, this was temporarily implemented as shooting down all !root
    blkgs on elevator switch and policy [de]registration combined with
    half-broken in-place root blkg updates.  In addition to being buggy
    and racy, this meant losing all blkcg configurations across those
    events.
    
    Now that blkcg is cleaned up enough, this patch replaces the temporary
    implementation with proper per-queue policy activation.  Each blkcg
    policy should call the new blkcg_[de]activate_policy() to enable and
    disable the policy on a specific queue.  blkcg_activate_policy()
    allocates and installs policy data for the policy for all existing
    blkgs.  blkcg_deactivate_policy() does the reverse.  If a policy is
    not enabled for a given queue, blkg printing / config functions skip
    the respective blkg for the queue.
    
    blkcg_activate_policy() also takes care of root blkg creation, and
    cfq_init_queue() and blk_throtl_init() are updated accordingly.
    
    This replaces blkcg_bypass_{start|end}() and update_root_blkg_pd()
    unnecessary.  Dropped.
    
    v2: cfq_init_queue() was returning uninitialized @ret on root_group
        alloc failure if !CONFIG_CFQ_GROUP_IOSCHED.  Fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b01c377fd739..68720ab275d4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -370,6 +370,7 @@ struct request_queue {
 
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
+	DECLARE_BITMAP		(blkcg_pols, BLKCG_MAX_POLS);
 	struct blkio_group	*root_blkg;
 	struct list_head	blkg_list;
 #endif

commit 03d8e11142a893ad322285d3c8a08e88b570cda1
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Apr 13 13:11:32 2012 -0700

    blkcg: add request_queue->root_blkg
    
    With per-queue policy activation, root blkg creation will be moved to
    blkcg core.  Add q->root_blkg in preparation.  For blk-throtl, this
    replaces throtl_data->root_tg; however, cfq needs to keep
    cfqd->root_group for !CONFIG_CFQ_GROUP_IOSCHED.
    
    This is to prepare for per-queue policy activation and doesn't cause
    any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d2c69f8c188a..b01c377fd739 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -31,6 +31,7 @@ struct blk_trace;
 struct request;
 struct sg_io_hdr;
 struct bsg_job;
+struct blkio_group;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -369,6 +370,7 @@ struct request_queue {
 
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
+	struct blkio_group	*root_blkg;
 	struct list_head	blkg_list;
 #endif
 

commit 8bd435b30ecacb69bbb8b2d3e251f770b807c5b2
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Apr 13 13:11:28 2012 -0700

    blkcg: remove static policy ID enums
    
    Remove BLKIO_POLICY_* enums and let blkio_policy_register() allocate
    @pol->plid dynamically on registration.  The maximum number of blkcg
    policies which can be registered at the same time is defined by
    BLKCG_MAX_POLS constant added to include/linux/blkdev.h.
    
    Note that blkio_policy_register() now may fail.  Policy init functions
    updated accordingly and unnecessary ifdefs removed from cfq_init().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 33f1b29e53f4..d2c69f8c188a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -35,6 +35,12 @@ struct bsg_job;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
+/*
+ * Maximum number of blkcg policies allowed to be registered concurrently.
+ * Defined here to simplify include dependency.
+ */
+#define BLKCG_MAX_POLS		2
+
 struct request;
 typedef void (rq_end_io_fn)(struct request *, int);
 
@@ -363,7 +369,6 @@ struct request_queue {
 
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
-	/* XXX: array size hardcoded to avoid include dependency (temporary) */
 	struct list_head	blkg_list;
 #endif
 

commit 8bcb6c7d48eb341b1f49f814cdcbe05eb6f15680
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Mar 30 12:33:28 2012 +0200

    block: use lockdep_assert_held for queue locking
    
    Instead of an ugly open coded variant.
    
    Cc: axboe@kernel.dk
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 606cf339bb56..2aa24664a5b5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -426,14 +426,10 @@ struct request_queue {
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
-static inline int queue_is_locked(struct request_queue *q)
+static inline void queue_lockdep_assert_held(struct request_queue *q)
 {
-#ifdef CONFIG_SMP
-	spinlock_t *lock = q->queue_lock;
-	return lock && spin_is_locked(lock);
-#else
-	return 1;
-#endif
+	if (q->queue_lock)
+		lockdep_assert_held(q->queue_lock);
 }
 
 static inline void queue_flag_set_unlocked(unsigned int flag,
@@ -445,7 +441,7 @@ static inline void queue_flag_set_unlocked(unsigned int flag,
 static inline int queue_flag_test_and_clear(unsigned int flag,
 					    struct request_queue *q)
 {
-	WARN_ON_ONCE(!queue_is_locked(q));
+	queue_lockdep_assert_held(q);
 
 	if (test_bit(flag, &q->queue_flags)) {
 		__clear_bit(flag, &q->queue_flags);
@@ -458,7 +454,7 @@ static inline int queue_flag_test_and_clear(unsigned int flag,
 static inline int queue_flag_test_and_set(unsigned int flag,
 					  struct request_queue *q)
 {
-	WARN_ON_ONCE(!queue_is_locked(q));
+	queue_lockdep_assert_held(q);
 
 	if (!test_bit(flag, &q->queue_flags)) {
 		__set_bit(flag, &q->queue_flags);
@@ -470,7 +466,7 @@ static inline int queue_flag_test_and_set(unsigned int flag,
 
 static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 {
-	WARN_ON_ONCE(!queue_is_locked(q));
+	queue_lockdep_assert_held(q);
 	__set_bit(flag, &q->queue_flags);
 }
 
@@ -487,7 +483,7 @@ static inline int queue_in_flight(struct request_queue *q)
 
 static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 {
-	WARN_ON_ONCE(!queue_is_locked(q));
+	queue_lockdep_assert_held(q);
 	__clear_bit(flag, &q->queue_flags);
 }
 

commit c875f4d0250a1f070fa26087a73bdd8f54c48100
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:22 2012 -0800

    blkcg: drop unnecessary RCU locking
    
    Now that blkg additions / removals are always done under both q and
    blkcg locks, the only places RCU locking is necessary are
    blkg_lookup[_create]() for lookup w/o blkcg lock.  This patch drops
    unncessary RCU locking replacing it with plain blkcg locking as
    necessary.
    
    * blkiocg_pre_destroy() already perform proper locking and don't need
      RCU.  Dropped.
    
    * blkio_read_blkg_stats() now uses blkcg->lock instead of RCU read
      lock.  This isn't a hot path.
    
    * Now unnecessary synchronize_rcu() from queue exit paths removed.
      This makes q->nr_blkgs unnecessary.  Dropped.
    
    * RCU annotation on blkg->q removed.
    
    -v2: Vivek pointed out that blkg_lookup_create() still needs to be
         called under rcu_read_lock().  Updated.
    
    -v3: After the update, stats_lock locking in blkio_read_blkg_stats()
         shouldn't be using _irq variant as it otherwise ends up enabling
         irq while blkcg->lock is locked.  Fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b4d1d4bfc168..33f1b29e53f4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -365,7 +365,6 @@ struct request_queue {
 #ifdef CONFIG_BLK_CGROUP
 	/* XXX: array size hardcoded to avoid include dependency (temporary) */
 	struct list_head	blkg_list;
-	int			nr_blkgs;
 #endif
 
 	struct queue_limits	limits;

commit 03aa264ac15637b6f98374270bcdf31400965505
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:19 2012 -0800

    blkcg: let blkcg core manage per-queue blkg list and counter
    
    With the previous patch to move blkg list heads and counters to
    request_queue and blkg, logic to manage them in both policies are
    almost identical and can be moved to blkcg core.
    
    This patch moves blkg link logic into blkg_lookup_create(), implements
    common blkg unlink code in blkg_destroy(), and updates
    blkg_destory_all() so that it's policy specific and can skip root
    group.  The updated blkg_destroy_all() is now used to both clear queue
    for bypassing and elv switching, and release all blkgs on q exit.
    
    This patch introduces a race window where policy [de]registration may
    race against queue blkg clearing.  This can only be a problem on cfq
    unload and shouldn't be a real problem in practice (and we have many
    other places where this race already exists).  Future patches will
    remove these unlikely races.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f4e35edea70f..b4d1d4bfc168 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -364,8 +364,8 @@ struct request_queue {
 	struct list_head	icq_list;
 #ifdef CONFIG_BLK_CGROUP
 	/* XXX: array size hardcoded to avoid include dependency (temporary) */
-	struct list_head	blkg_list[2];
-	int			nr_blkgs[2];
+	struct list_head	blkg_list;
+	int			nr_blkgs;
 #endif
 
 	struct queue_limits	limits;

commit 4eef3049986e8397d5003916aed8cad6567a5e02
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:18 2012 -0800

    blkcg: move per-queue blkg list heads and counters to queue and blkg
    
    Currently, specific policy implementations are responsible for
    maintaining list and number of blkgs.  This duplicates code
    unnecessarily, and hinders factoring common code and providing blkcg
    API with better defined semantics.
    
    After this patch, request_queue hosts list heads and counters and blkg
    has list nodes for both policies.  This patch only relocates the
    necessary fields and the next patch will actually move management code
    into blkcg core.
    
    Note that request_queue->blkg_list[] and ->nr_blkgs[] are hardcoded to
    have 2 elements.  This is to avoid include dependency and will be
    removed by the next patch.
    
    This patch doesn't introduce any behavior change.
    
    -v2: Now unnecessary conditional on CONFIG_BLK_CGROUP_MODULE removed
         as pointed out by Vivek.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e8c0bbd06b9a..f4e35edea70f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -362,6 +362,11 @@ struct request_queue {
 	struct list_head	timeout_list;
 
 	struct list_head	icq_list;
+#ifdef CONFIG_BLK_CGROUP
+	/* XXX: array size hardcoded to avoid include dependency (temporary) */
+	struct list_head	blkg_list[2];
+	int			nr_blkgs[2];
+#endif
 
 	struct queue_limits	limits;
 

commit 923adde1be1df57cebd80c563058e503376645e8
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:13 2012 -0800

    blkcg: clear all request_queues on blkcg policy [un]registrations
    
    Keep track of all request_queues which have blkcg initialized and turn
    on bypass and invoke blkcg_clear_queue() on all before making changes
    to blkcg policies.
    
    This is to prepare for moving blkg management into blkcg core.  Note
    that this uses more brute force than necessary.  Finer grained shoot
    down will be implemented later and given that policy [un]registration
    almost never happens on running systems (blk-throtl can't be built as
    a module and cfq usually is the builtin default iosched), this
    shouldn't be a problem for the time being.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 315db1d91bc4..e8c0bbd06b9a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -397,6 +397,9 @@ struct request_queue {
 	struct bsg_class_device bsg_dev;
 #endif
 
+#ifdef CONFIG_BLK_CGROUP
+	struct list_head	all_q_node;
+#endif
 #ifdef CONFIG_BLK_DEV_THROTTLING
 	/* Throttle data */
 	struct throtl_data *td;

commit d732580b4eb31553c63744a47d590f770cafb8f0
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:14:58 2012 -0800

    block: implement blk_queue_bypass_start/end()
    
    Rename and extend elv_queisce_start/end() to
    blk_queue_bypass_start/end() which are exported and supports nesting
    via @q->bypass_depth.  Also add blk_queue_bypass() to test bypass
    state.
    
    This will be further extended and used for blkio_group management.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 606cf339bb56..315db1d91bc4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -389,6 +389,8 @@ struct request_queue {
 
 	struct mutex		sysfs_lock;
 
+	int			bypass_depth;
+
 #if defined(CONFIG_BLK_DEV_BSG)
 	bsg_job_fn		*bsg_job_fn;
 	int			bsg_job_size;
@@ -406,7 +408,7 @@ struct request_queue {
 #define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
 #define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
-#define QUEUE_FLAG_ELVSWITCH	6	/* don't use elevator, just do FIFO */
+#define QUEUE_FLAG_BYPASS	6	/* act as dumb FIFO queue */
 #define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	9	/* complete on same CPU-group */
@@ -494,6 +496,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+#define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)

commit 11a3122f6cf2d988a77eb8883d0fc49cd013a6d5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 7 07:51:30 2012 +0100

    block: strip out locking optimization in put_io_context()
    
    put_io_context() performed a complex trylock dancing to avoid
    deferring ioc release to workqueue.  It was also broken on UP because
    trylock was always assumed to succeed which resulted in unbalanced
    preemption count.
    
    While there are ways to fix the UP breakage, even the most
    pathological microbench (forced ioc allocation and tight fork/exit
    loop) fails to show any appreciable performance benefit of the
    optimization.  Strip it out.  If there turns out to be workloads which
    are affected by this change, simpler optimization from the discussion
    thread can be applied later.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <1328514611.21268.66.camel@sli10-conroe>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6c6a1f008065..606cf339bb56 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -399,9 +399,6 @@ struct request_queue {
 	/* Throttle data */
 	struct throtl_data *td;
 #endif
-#ifdef CONFIG_LOCKDEP
-	int			ioc_release_depth;
-#endif
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit b3c9dd182ed3bdcdaf0e42625a35924b0497afdc
Merge: 83c2f912b43c 5d381efb3d1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 15 12:24:45 2012 -0800

    Merge branch 'for-3.3/core' of git://git.kernel.dk/linux-block
    
    * 'for-3.3/core' of git://git.kernel.dk/linux-block: (37 commits)
      Revert "block: recursive merge requests"
      block: Stop using macro stubs for the bio data integrity calls
      blockdev: convert some macros to static inlines
      fs: remove unneeded plug in mpage_readpages()
      block: Add BLKROTATIONAL ioctl
      block: Introduce blk_set_stacking_limits function
      block: remove WARN_ON_ONCE() in exit_io_context()
      block: an exiting task should be allowed to create io_context
      block: ioc_cgroup_changed() needs to be exported
      block: recursive merge requests
      block, cfq: fix empty queue crash caused by request merge
      block, cfq: move icq creation and rq->elv.icq association to block core
      block, cfq: restructure io_cq creation path for io_context interface cleanup
      block, cfq: move io_cq exit/release to blk-ioc.c
      block, cfq: move icq cache management to block core
      block, cfq: move io_cq lookup to blk-ioc.c
      block, cfq: move cfqd->icq_list to request_queue and add request->elv.icq
      block, cfq: reorganize cfq_io_context into generic and cfq specific parts
      block: remove elevator_queue->ops
      block: reorder elevator switch sequence
      ...
    
    Fix up conflicts in:
     - block/blk-cgroup.c
            Switch from can_attach_task to can_attach
     - block/cfq-iosched.c
            conflict with now removed cic index changes (we now use q->id instead)

commit 0bfc96cb77224736dfa35c3c555d37b3646ef35e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 12 16:01:28 2012 +0100

    block: fail SCSI passthrough ioctls on partition devices
    
    Linux allows executing the SG_IO ioctl on a partition or LVM volume, and
    will pass the command to the underlying block device.  This is
    well-known, but it is also a large security problem when (via Unix
    permissions, ACLs, SELinux or a combination thereof) a program or user
    needs to be granted access only to part of the disk.
    
    This patch lets partitions forward a small set of harmless ioctls;
    others are logged with printk so that we can see which ioctls are
    actually sent.  In my tests only CDROM_GET_CAPABILITY actually occurred.
    Of course it was being sent to a (partition on a) hard disk, so it would
    have failed with ENOTTY and the patch isn't changing anything in
    practice.  Still, I'm treating it specially to avoid spamming the logs.
    
    In principle, this restriction should include programs running with
    CAP_SYS_RAWIO.  If for example I let a program access /dev/sda2 and
    /dev/sdb, it still should not be able to read/write outside the
    boundaries of /dev/sda2 independent of the capabilities.  However, for
    now programs with CAP_SYS_RAWIO will still be allowed to send the
    ioctls.  Their actions will still be logged.
    
    This patch does not affect the non-libata IDE driver.  That driver
    however already tests for bd != bd->bd_contains before issuing some
    ioctl; it could be restricted further to forbid these ioctls even for
    programs running with CAP_SYS_ADMIN/CAP_SYS_RAWIO.
    
    Cc: linux-scsi@vger.kernel.org
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: James Bottomley <JBottomley@parallels.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [ Make it also print the command name when warning - Linus ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ca7b869508c7..0ed1eb062313 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -675,6 +675,7 @@ extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
+extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
 extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
 			      unsigned int, void __user *);
 extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,

commit 577ebb374c78314ac4617242f509e2f5e7156649
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 12 16:01:27 2012 +0100

    block: add and use scsi_blk_cmd_ioctl
    
    Introduce a wrapper around scsi_cmd_ioctl that takes a block device.
    
    The function will then be enhanced to detect partition block devices
    and, in that case, subject the ioctls to whitelisting.
    
    Cc: linux-scsi@vger.kernel.org
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: James Bottomley <JBottomley@parallels.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 94acd8172b5b..ca7b869508c7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -675,6 +675,8 @@ extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
+extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
+			      unsigned int, void __user *);
 extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			  unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,

commit fd83240a60ecc59849420df3393e9e6d35c77683
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Jan 12 09:17:30 2012 +0100

    blockdev: convert some macros to static inlines
    
    We prefer to program in C rather than preprocessor and it fixes this
    warning when CONFIG_BLK_DEV_INTEGRITY is not set:
    
    drivers/md/dm-table.c: In function 'dm_table_set_integrity':
    drivers/md/dm-table.c:1285:3: warning: statement with no effect [-Wunused-value]
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index adc34133a56a..5cfb9b22627f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1298,19 +1298,70 @@ queue_max_integrity_segments(struct request_queue *q)
 
 #else /* CONFIG_BLK_DEV_INTEGRITY */
 
-#define blk_integrity_rq(rq)			(0)
-#define blk_rq_count_integrity_sg(a, b)		(0)
-#define blk_rq_map_integrity_sg(a, b, c)	(0)
-#define bdev_get_integrity(a)			(0)
-#define blk_get_integrity(a)			(0)
-#define blk_integrity_compare(a, b)		(0)
-#define blk_integrity_register(a, b)		(0)
-#define blk_integrity_unregister(a)		do { } while (0)
-#define blk_queue_max_integrity_segments(a, b)	do { } while (0)
-#define queue_max_integrity_segments(a)		(0)
-#define blk_integrity_merge_rq(a, b, c)		(0)
-#define blk_integrity_merge_bio(a, b, c)	(0)
-#define blk_integrity_is_initialized(a)		(0)
+struct bio;
+struct block_device;
+struct gendisk;
+struct blk_integrity;
+
+static inline int blk_integrity_rq(struct request *rq)
+{
+	return 0;
+}
+static inline int blk_rq_count_integrity_sg(struct request_queue *q,
+					    struct bio *b)
+{
+	return 0;
+}
+static inline int blk_rq_map_integrity_sg(struct request_queue *q,
+					  struct bio *b,
+					  struct scatterlist *s)
+{
+	return 0;
+}
+static inline struct blk_integrity *bdev_get_integrity(struct block_device *b)
+{
+	return 0;
+}
+static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
+{
+	return NULL;
+}
+static inline int blk_integrity_compare(struct gendisk *a, struct gendisk *b)
+{
+	return 0;
+}
+static inline int blk_integrity_register(struct gendisk *d,
+					 struct blk_integrity *b)
+{
+	return 0;
+}
+static inline void blk_integrity_unregister(struct gendisk *d)
+{
+}
+static inline void blk_queue_max_integrity_segments(struct request_queue *q,
+						    unsigned int segs)
+{
+}
+static inline unsigned short queue_max_integrity_segments(struct request_queue *q)
+{
+	return 0;
+}
+static inline int blk_integrity_merge_rq(struct request_queue *rq,
+					 struct request *r1,
+					 struct request *r2)
+{
+	return 0;
+}
+static inline int blk_integrity_merge_bio(struct request_queue *rq,
+					  struct request *r,
+					  struct bio *b)
+{
+	return 0;
+}
+static inline bool blk_integrity_is_initialized(struct gendisk *g)
+{
+	return 0;
+}
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 

commit b1bd055d397e09f99dcef9b138ed104ff1812fcb
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Jan 11 16:27:11 2012 +0100

    block: Introduce blk_set_stacking_limits function
    
    Stacking driver queue limits are typically bounded exclusively by the
    capabilities of the low level devices, not by the stacking driver
    itself.
    
    This patch introduces blk_set_stacking_limits() which has more liberal
    metrics than the default queue limits function. This allows us to
    inherit topology parameters from bottom devices without manually
    tweaking the default limits in each driver prior to calling the stacking
    function.
    
    Since there is now a clear distinction between stacking and low-level
    devices, blk_set_default_limits() has been modified to carry the more
    conservative values that we used to manually set in
    blk_queue_make_request().
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8bca04873f53..adc34133a56a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -844,6 +844,7 @@ extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
 extern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);
 extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
 extern void blk_set_default_limits(struct queue_limits *lim);
+extern void blk_set_stacking_limits(struct queue_limits *lim);
 extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 			    sector_t offset);
 extern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,

commit a612fddf0d8090f2877305c9168b6c1a34fb5d90
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:41 2011 +0100

    block, cfq: move cfqd->icq_list to request_queue and add request->elv.icq
    
    Most of icq management is about to be moved out of cfq into blk-ioc.
    This patch prepares for it.
    
    * Move cfqd->icq_list to request_queue->icq_list
    
    * Make request explicitly point to icq instead of through elevator
      private data.  ->elevator_private[3] is replaced with sub struct elv
      which contains icq pointer and priv[2].  cfq is updated accordingly.
    
    * Meaningless clearing of ->elevator_private[0] removed from
      elv_set_request().  At that point in code, the field was guaranteed
      to be %NULL anyway.
    
    This patch doesn't introduce any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 65c2f8c70089..8bca04873f53 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -111,10 +111,14 @@ struct request {
 	 * Three pointers are available for the IO schedulers, if they need
 	 * more they have to dynamically allocate it.  Flush requests are
 	 * never put on the IO scheduler. So let the flush fields share
-	 * space with the three elevator_private pointers.
+	 * space with the elevator data.
 	 */
 	union {
-		void *elevator_private[3];
+		struct {
+			struct io_cq		*icq;
+			void			*priv[2];
+		} elv;
+
 		struct {
 			unsigned int		seq;
 			struct list_head	list;
@@ -357,6 +361,8 @@ struct request_queue {
 	struct timer_list	timeout;
 	struct list_head	timeout_list;
 
+	struct list_head	icq_list;
+
 	struct queue_limits	limits;
 
 	/*

commit b2efa05265d62bc29f3a64400fad4b44340eedb8
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:39 2011 +0100

    block, cfq: unlink cfq_io_context's immediately
    
    cic is association between io_context and request_queue.  A cic is
    linked from both ioc and q and should be destroyed when either one
    goes away.  As ioc and q both have their own locks, locking becomes a
    bit complex - both orders work for removal from one but not from the
    other.
    
    Currently, cfq tries to circumvent this locking order issue with RCU.
    ioc->lock nests inside queue_lock but the radix tree and cic's are
    also protected by RCU allowing either side to walk their lists without
    grabbing lock.
    
    This rather unconventional use of RCU quickly devolves into extremely
    fragile convolution.  e.g. The following is from cfqd going away too
    soon after ioc and q exits raced.
    
     general protection fault: 0000 [#1] PREEMPT SMP
     CPU 2
     Modules linked in:
     [   88.503444]
     Pid: 599, comm: hexdump Not tainted 3.1.0-rc10-work+ #158 Bochs Bochs
     RIP: 0010:[<ffffffff81397628>]  [<ffffffff81397628>] cfq_exit_single_io_context+0x58/0xf0
     ...
     Call Trace:
      [<ffffffff81395a4a>] call_for_each_cic+0x5a/0x90
      [<ffffffff81395ab5>] cfq_exit_io_context+0x15/0x20
      [<ffffffff81389130>] exit_io_context+0x100/0x140
      [<ffffffff81098a29>] do_exit+0x579/0x850
      [<ffffffff81098d5b>] do_group_exit+0x5b/0xd0
      [<ffffffff81098de7>] sys_exit_group+0x17/0x20
      [<ffffffff81b02f2b>] system_call_fastpath+0x16/0x1b
    
    The only real hot path here is cic lookup during request
    initialization and avoiding extra locking requires very confined use
    of RCU.  This patch makes cic removal from both ioc and request_queue
    perform double-locking and unlink immediately.
    
    * From q side, the change is almost trivial as ioc->lock nests inside
      queue_lock.  It just needs to grab each ioc->lock as it walks
      cic_list and unlink it.
    
    * From ioc side, it's a bit more difficult because of inversed lock
      order.  ioc needs its lock to walk its cic_list but can't grab the
      matching queue_lock and needs to perform unlock-relock dancing.
    
      Unlinking is now wholly done from put_io_context() and fast path is
      optimized by using the queue_lock the caller already holds, which is
      by far the most common case.  If the ioc accessed multiple devices,
      it tries with trylock.  In unlikely cases of fast path failure, it
      falls back to full double-locking dance from workqueue.
    
    Double-locking isn't the prettiest thing in the world but it's *far*
    simpler and more understandable than RCU trick without adding any
    meaningful overhead.
    
    This still leaves a lot of now unnecessary RCU logics.  Future patches
    will trim them.
    
    -v2: Vivek pointed out that cic->q was being dereferenced after
         cic->release() was called.  Updated to use local variable @this_q
         instead.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d1b6f4ed1f96..65c2f8c70089 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -393,6 +393,9 @@ struct request_queue {
 	/* Throttle data */
 	struct throtl_data *td;
 #endif
+#ifdef CONFIG_LOCKDEP
+	int			ioc_release_depth;
+#endif
 };
 
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 09ac46c429464c919d04bb737b27edd84d944f02
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:38 2011 +0100

    block: misc updates to blk_get_queue()
    
    * blk_get_queue() is peculiar in that it returns 0 on success and 1 on
      failure instead of 0 / -errno or boolean.  Update it such that it
      returns %true on success and %false on failure.
    
    * Make sure the caller checks for the return value.
    
    * Separate out __blk_get_queue() which doesn't check whether @q is
      dead and put it in blk.h.  This will be used later.
    
    This patch doesn't introduce any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8c8dbc4738ea..d1b6f4ed1f96 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -865,7 +865,7 @@ extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatte
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
 
-int blk_get_queue(struct request_queue *);
+bool __must_check blk_get_queue(struct request_queue *);
 struct request_queue *blk_alloc_queue(gfp_t);
 struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);

commit a73f730d013ff2788389fd0c46ad3e5510f124e6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block, cfq: move cfqd->cic_index to q->id
    
    cfq allocates per-queue id using ida and uses it to index cic radix
    tree from io_context.  Move it to q->id and allocate on queue init and
    free on queue release.  This simplifies cfq a bit and will allow for
    further improvements of io context life-cycle management.
    
    This patch doesn't introduce any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 783f97c14d0a..8c8dbc4738ea 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -310,6 +310,12 @@ struct request_queue {
 	 */
 	unsigned long		queue_flags;
 
+	/*
+	 * ida allocated id for this queue.  Used to index queues from
+	 * ioctx.
+	 */
+	int			id;
+
 	/*
 	 * queue needs bounce pages for pages above this limit
 	 */

commit 34f6055c80285e4efb3f602a9119db75239744dc
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block: add blk_queue_dead()
    
    There are a number of QUEUE_FLAG_DEAD tests.  Add blk_queue_dead()
    macro and use it.
    
    This patch doesn't introduce any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8a6b51b13a1c..783f97c14d0a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -481,6 +481,7 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+#define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)

commit 1ba64edef6051d2ec79bb2fbd3a0c8f0df00ab55
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:37 2011 +0100

    block, sx8: kill blk_insert_request()
    
    The only user left for blk_insert_request() is sx8 and it can be
    trivially switched to use blk_execute_rq_nowait() - special requests
    aren't included in io stat and sx8 doesn't use block layer tagging.
    Switch sx8 and kill blk_insert_requeset().
    
    This patch doesn't introduce any functional difference.
    
    Only compile tested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jeff Garzik <jgarzik@pobox.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c7a6d3b5bc7b..8a6b51b13a1c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -660,7 +660,6 @@ extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern struct request *blk_make_request(struct request_queue *, struct bio *,
 					gfp_t);
-extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);

commit 5151412dd4338b273afdb107c3772528e9e67d92
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Wed Nov 23 10:59:13 2011 +0100

    block: initialize request_queue's numa node during
    
    struct request_queue is allocated with __GFP_ZERO so its "node" field is
    zero before initialization.  This causes an oops if node 0 is offline in
    the page allocator because its zonelists are not initialized.  From Dave
    Young's dmesg:
    
            SRAT: Node 1 PXM 2 0-d0000000
            SRAT: Node 1 PXM 2 100000000-330000000
            SRAT: Node 0 PXM 1 330000000-630000000
            Initmem setup node 1 0000000000000000-000000000affb000
            ...
            Built 1 zonelists in Node order, mobility grouping on.
            ...
            BUG: unable to handle kernel paging request at 0000000000001c08
            IP: [<ffffffff8111c355>] __alloc_pages_nodemask+0xb5/0x870
    
    and __alloc_pages_nodemask+0xb5 translates to a NULL pointer on
    zonelist->_zonerefs.
    
    The fix is to initialize q->node at the time of allocation so the correct
    node is passed to the slab allocator later.
    
    Since blk_init_allocated_queue_node() is no longer needed, merge it with
    blk_init_allocated_queue().
    
    [rientjes@google.com: changelog, initializing q->node]
    Cc: stable@vger.kernel.org [2.6.37+]
    Reported-by: Dave Young <dyoung@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Tested-by: Dave Young <dyoung@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c7a6d3b5bc7b..94acd8172b5b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -805,9 +805,6 @@ extern void blk_unprep_request(struct request *);
  */
 extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
 					spinlock_t *lock, int node_id);
-extern struct request_queue *blk_init_allocated_queue_node(struct request_queue *,
-							   request_fn_proc *,
-							   spinlock_t *, int node_id);
 extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
 						      request_fn_proc *, spinlock_t *);

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit de47725421ad5627a5c905f4e40bb844ebc06d29
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 13:46:22 2011 -0400

    include: replace linux/module.h with "struct module" wherever possible
    
    The <linux/module.h> pretty much brings in the kitchen sink along
    with it, so it should be avoided wherever reasonably possible in
    terms of being included from other commonly used <linux/something.h>
    files, as it results in a measureable increase on compile times.
    
    The worst culprit was probably device.h since it is used everywhere.
    This file also had an implicit dependency/usage of mutex.h which was
    masked by module.h, and is also fixed here at the same time.
    
    There are over a dozen other headers that simply declare the
    struct instead of pulling in the whole file, so follow their lead
    and simply make it a few more.
    
    Most of the implicit dependencies on module.h being present by
    these headers pulling it in have been now weeded out, so we can
    finally make this change with hopefully minimal breakage.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7fbaa9103344..d750a3a79299 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -14,7 +14,6 @@
 #include <linux/wait.h>
 #include <linux/mempool.h>
 #include <linux/bio.h>
-#include <linux/module.h>
 #include <linux/stringify.h>
 #include <linux/gfp.h>
 #include <linux/bsg.h>
@@ -22,6 +21,7 @@
 
 #include <asm/scatterlist.h>
 
+struct module;
 struct scsi_ioctl_command;
 
 struct request_queue;

commit bc9fcbf9cb8ec76d340da16fbf48a9a316e14c52
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Oct 19 14:31:18 2011 +0200

    block: move blk_throtl prototypes to block/blk.h
    
    blk_throtl interface is block internal and there's no reason to have
    them in linux/blkdev.h.  Move them to block/blk.h.
    
    This patch doesn't introduce any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0b68044e7abb..5267cd2f20dc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1197,20 +1197,6 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 }
 #endif
 
-#ifdef CONFIG_BLK_DEV_THROTTLING
-extern int blk_throtl_init(struct request_queue *q);
-extern void blk_throtl_exit(struct request_queue *q);
-extern int blk_throtl_bio(struct request_queue *q, struct bio **bio);
-#else /* CONFIG_BLK_DEV_THROTTLING */
-static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
-{
-	return 0;
-}
-
-static inline int blk_throtl_init(struct request_queue *q) { return 0; }
-static inline int blk_throtl_exit(struct request_queue *q) { return 0; }
-#endif /* CONFIG_BLK_DEV_THROTTLING */
-
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \

commit 5c04b426f2e8b46cfc7969a35b2631063a3c646c
Merge: 499337bb6511 899e3ee40496
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 19 14:30:42 2011 +0200

    Merge branch 'v3.1-rc10' into for-3.2/core
    
    Conflicts:
            block/blk-core.c
            include/linux/blkdev.h
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 75df713627f28f88b901b329c8857747545fd4ab
Author: Suresh Jayaraman <sjayaraman@suse.de>
Date:   Wed Sep 21 10:00:16 2011 +0200

    block: document blk-plug
    
    Thus spake Andrew Morton:
    
    "And I have the usual maintainability whine.  If someone comes up to
    vmscan.c and sees it calling blk_start_plug(), how are they supposed to
    work out why that call is there?  They go look at the blk_start_plug()
    definition and it is undocumented.  I think we can do better than this?"
    
    Adapted from the LWN article - http://lwn.net/Articles/438256/ by Jens
    Axboe and from an earlier attempt by Shaohua Li to document blk-plug.
    
    [akpm@linux-foundation.org: grammatical and spelling tweaks]
    Signed-off-by: Suresh Jayaraman <sjayaraman@suse.de>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Andrew Morton <akpm@google.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c712efdafc3f..1978655faa3b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -860,17 +860,23 @@ struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);
 
 /*
- * Note: Code in between changing the blk_plug list/cb_list or element of such
- * lists is preemptable, but such code can't do sleep (or be very careful),
- * otherwise data is corrupted. For details, please check schedule() where
- * blk_schedule_flush_plug() is called.
+ * blk_plug permits building a queue of related requests by holding the I/O
+ * fragments for a short period. This allows merging of sequential requests
+ * into single larger request. As the requests are moved from a per-task list to
+ * the device's request_queue in a batch, this results in improved scalability
+ * as the lock contention for request_queue lock is reduced.
+ *
+ * It is ok not to disable preemption when adding the request to the plug list
+ * or when attempting a merge, because blk_schedule_flush_list() will only flush
+ * the plug list when the task sleeps by itself. For details, please see
+ * schedule() where blk_schedule_flush_plug() is called.
  */
 struct blk_plug {
-	unsigned long magic;
-	struct list_head list;
-	struct list_head cb_list;
-	unsigned int should_sort;
-	unsigned int count;
+	unsigned long magic; /* detect uninitialized use-cases */
+	struct list_head list; /* requests */
+	struct list_head cb_list; /* md requires an unplug callback */
+	unsigned int should_sort; /* list to be sorted before flushing? */
+	unsigned int count; /* number of queued requests */
 };
 #define BLK_MAX_REQUEST_COUNT 16
 

commit 5a7bbad27a410350e64a2d7f5ec18fc73836c14f
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Sep 12 12:12:01 2011 +0200

    block: remove support for bio remapping from ->make_request
    
    There is very little benefit in allowing to let a ->make_request
    instance update the bios device and sector and loop around it in
    __generic_make_request when we can archive the same through calling
    generic_make_request from the driver and letting the loop in
    generic_make_request handle it.
    
    Note that various drivers got the return value from ->make_request and
    returned non-zero values for errors.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 085f95414c7f..c712efdafc3f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -193,7 +193,7 @@ struct request_pm_state
 #include <linux/elevator.h>
 
 typedef void (request_fn_proc) (struct request_queue *q);
-typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
+typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 
@@ -675,7 +675,7 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern int blk_queue_bio(struct request_queue *q, struct bio *bio);
+extern void blk_queue_bio(struct request_queue *q, struct bio *bio);
 
 /*
  * A queue has just exitted congestion.  Note this in the global counter of

commit c20e8de27fef9f59869c81c288ad6cf28200e00c
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Mon Sep 12 12:03:37 2011 +0200

    block: rename __make_request() to blk_queue_bio()
    
    Now that it's exported, lets put it in a more sane namespace.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e9c3d9b07630..085f95414c7f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -675,7 +675,7 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-extern int __make_request(struct request_queue *q, struct bio *bio);
+extern int blk_queue_bio(struct request_queue *q, struct bio *bio);
 
 /*
  * A queue has just exitted congestion.  Note this in the global counter of

commit 166e1f901b01872e8b70733a3f2e2c6980389cf8
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Sep 12 12:08:27 2011 +0200

    block: export __make_request
    
    Avoid the hacks need for request based device mappers currently by simply
    exporting the symbol instead of trying to get it through the back door.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0e67c45b3bc9..e9c3d9b07630 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -675,6 +675,8 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
+extern int __make_request(struct request_queue *q, struct bio *bio);
+
 /*
  * A queue has just exitted congestion.  Note this in the global counter of
  * congested queues, and wake up anyone who was waiting for requests to be

commit 56ebdaf2fa3c5276be201c5d1aff1490b682ecf2
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Wed Aug 24 16:04:34 2011 +0200

    block: simplify force plug flush code a little bit
    
    Cleaning up the code a little bit. attempt_plug_merge() traverses the plug
    list anyway, we can do the request counting there, so stack size is reduced
    a little bit.
    The motivation here is I suspect if we should count the requests for each
    queue (task could handle multiple disks in the meantime), but my test doesn't
    show it's worthy doing. If somebody proves we should do it, below change
    will make that more easier.
    
    Signed-off-by: Shaohua Li <shli@kernel.org>
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 84b15d54f8c2..7fbaa9103344 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -873,7 +873,6 @@ struct blk_plug {
 	struct list_head list;
 	struct list_head cb_list;
 	unsigned int should_sort;
-	unsigned int count;
 };
 #define BLK_MAX_REQUEST_COUNT 16
 

commit 4853abaae7e4a2af938115ce9071ef8684fb7af4
Author: Jeff Moyer <jmoyer@redhat.com>
Date:   Mon Aug 15 21:37:25 2011 +0200

    block: fix flush machinery for stacking drivers with differring flush flags
    
    Commit ae1b1539622fb46e51b4d13b3f9e5f4c713f86ae, block: reimplement
    FLUSH/FUA to support merge, introduced a performance regression when
    running any sort of fsyncing workload using dm-multipath and certain
    storage (in our case, an HP EVA).  The test I ran was fs_mark, and it
    dropped from ~800 files/sec on ext4 to ~100 files/sec.  It turns out
    that dm-multipath always advertised flush+fua support, and passed
    commands on down the stack, where those flags used to get stripped off.
    The above commit changed that behavior:
    
    static inline struct request *__elv_next_request(struct request_queue *q)
    {
            struct request *rq;
    
            while (1) {
    -               while (!list_empty(&q->queue_head)) {
    +               if (!list_empty(&q->queue_head)) {
                            rq = list_entry_rq(q->queue_head.next);
    -                       if (!(rq->cmd_flags & (REQ_FLUSH | REQ_FUA)) ||
    -                           (rq->cmd_flags & REQ_FLUSH_SEQ))
    -                               return rq;
    -                       rq = blk_do_flush(q, rq);
    -                       if (rq)
    -                               return rq;
    +                       return rq;
                    }
    
    Note that previously, a command would come in here, have
    REQ_FLUSH|REQ_FUA set, and then get handed off to blk_do_flush:
    
    struct request *blk_do_flush(struct request_queue *q, struct request *rq)
    {
            unsigned int fflags = q->flush_flags; /* may change, cache it */
            bool has_flush = fflags & REQ_FLUSH, has_fua = fflags & REQ_FUA;
            bool do_preflush = has_flush && (rq->cmd_flags & REQ_FLUSH);
            bool do_postflush = has_flush && !has_fua && (rq->cmd_flags &
            REQ_FUA);
            unsigned skip = 0;
    ...
            if (blk_rq_sectors(rq) && !do_preflush && !do_postflush) {
                    rq->cmd_flags &= ~REQ_FLUSH;
                    if (!has_fua)
                            rq->cmd_flags &= ~REQ_FUA;
                    return rq;
            }
    
    So, the flush machinery was bypassed in such cases (q->flush_flags == 0
    && rq->cmd_flags & (REQ_FLUSH|REQ_FUA)).
    
    Now, however, we don't get into the flush machinery at all.  Instead,
    __elv_next_request just hands a request with flush and fua bits set to
    the scsi_request_fn, even if the underlying request_queue does not
    support flush or fua.
    
    The agreed upon approach is to fix the flush machinery to allow
    stacking.  While this isn't used in practice (since there is only one
    request-based dm target, and that target will now reflect the flush
    flags of the underlying device), it does future-proof the solution, and
    make it function as designed.
    
    In order to make this work, I had to add a field to the struct request,
    inside the flush structure (to store the original req->end_io).  Shaohua
    had suggested overloading the union with rb_node and completion_data,
    but the completion data is used by device mapper and can also be used by
    other drivers.  So, I didn't see a way around the additional field.
    
    I tested this patch on an HP EVA with both ext4 and xfs, and it recovers
    the lost performance.  Comments and other testers, as always, are
    appreciated.
    
    Cheers,
    Jeff
    
    Signed-off-by: Jeff Moyer <jmoyer@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 847928546076..84b15d54f8c2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -118,6 +118,7 @@ struct request {
 		struct {
 			unsigned int		seq;
 			struct list_head	list;
+			rq_end_io_fn		*saved_end_io;
 		} flush;
 	};
 

commit aa387cc895672b00f807ad7c734a2defaf677712
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Sun Jul 31 22:05:09 2011 +0200

    block: add bsg helper library
    
    This moves the FC classes bsg code to the block layer and
    makes it a lib so that other classes like iscsi and SAS can use it.
    
    It is helpful because working with the request queue, bios,
    creating scatterlists, etc are a pain that the LLD does not
    have to worry about with normal IOs and should not have to
    worry about for bsg requests.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0e67c45b3bc9..847928546076 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -30,6 +30,7 @@ struct request_pm_state;
 struct blk_trace;
 struct request;
 struct sg_io_hdr;
+struct bsg_job;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -209,6 +210,7 @@ typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 typedef int (lld_busy_fn) (struct request_queue *q);
+typedef int (bsg_job_fn) (struct bsg_job *);
 
 enum blk_eh_timer_return {
 	BLK_EH_NOT_HANDLED,
@@ -375,6 +377,8 @@ struct request_queue {
 	struct mutex		sysfs_lock;
 
 #if defined(CONFIG_BLK_DEV_BSG)
+	bsg_job_fn		*bsg_job_fn;
+	int			bsg_job_size;
 	struct bsg_class_device bsg_dev;
 #endif
 

commit 5757a6d76cdf6dda2a492c09b985c015e86779b1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Jul 23 20:44:25 2011 +0200

    block: strict rq_affinity
    
    Some systems benefit from completions always being steered to the strict
    requester cpu rather than the looser "per-socket" steering that
    blk_cpu_to_group() attempts by default. This is because the first
    CPU in the group mask ends up being completely overloaded with work,
    while the others (including the original submitter) has power left
    to spare.
    
    Allow the strict mode to be set by writing '2' to the sysfs control
    file. This is identical to the scheme used for the nomerges file,
    where '2' is a more aggressive setting than just being turned on.
    
    echo 2 > /sys/block/<bdev>/queue/rq_affinity
    
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Roland Dreier <roland@purestorage.com>
    Tested-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c0cd9a2f22ef..0e67c45b3bc9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -392,7 +392,7 @@ struct request_queue {
 #define QUEUE_FLAG_ELVSWITCH	6	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
-#define QUEUE_FLAG_SAME_COMP	9	/* force complete on same CPU */
+#define QUEUE_FLAG_SAME_COMP	9	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO     10	/* fake timeout */
 #define QUEUE_FLAG_STACKABLE   11	/* supports request stacking */
 #define QUEUE_FLAG_NONROT      12	/* non-rotational device (SSD) */
@@ -402,6 +402,7 @@ struct request_queue {
 #define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
 #define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
+#define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\

commit d7b7630130e52361af66ce3b994696e2357ba7de
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Wed Jul 13 21:17:23 2011 +0200

    block: reorder request_queue to remove 64 bit alignment padding
    
    Reorder request_queue to remove 16 bytes of alignment padding in 64 bit
    builds.
    
    On my config this shrinks the size of this structure from 1608 to 1592
    bytes and therefore needs one fewer cachelines.
    
    Also trivially move the open bracket { to be on the same line as the
    structure name to make it easier to grep.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6dcea6885a5d..c0cd9a2f22ef 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -260,8 +260,7 @@ struct queue_limits {
 	unsigned char		discard_zeroes_data;
 };
 
-struct request_queue
-{
+struct request_queue {
 	/*
 	 * Together with queue_head for cacheline sharing
 	 */
@@ -304,14 +303,14 @@ struct request_queue
 	void			*queuedata;
 
 	/*
-	 * queue needs bounce pages for pages above this limit
+	 * various queue flags, see QUEUE_* below
 	 */
-	gfp_t			bounce_gfp;
+	unsigned long		queue_flags;
 
 	/*
-	 * various queue flags, see QUEUE_* below
+	 * queue needs bounce pages for pages above this limit
 	 */
-	unsigned long		queue_flags;
+	gfp_t			bounce_gfp;
 
 	/*
 	 * protects queue structures from reentrancy. ->__queue_lock should
@@ -334,8 +333,8 @@ struct request_queue
 	unsigned int		nr_congestion_off;
 	unsigned int		nr_batching;
 
-	void			*dma_drain_buffer;
 	unsigned int		dma_drain_size;
+	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 

commit 316cc67d5e03801a5ee4ac660a4dfe9e02aed475
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Fri Jul 8 08:19:21 2011 +0200

    block: document blk_plug list access
    
    I'm often confused why not disable preempt when changing blk_plug list. It
    would be better to add comments here in case others have the similar concerns.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 92edb9601242..6dcea6885a5d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -857,6 +857,12 @@ struct request_queue *blk_alloc_queue(gfp_t);
 struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);
 
+/*
+ * Note: Code in between changing the blk_plug list/cb_list or element of such
+ * lists is preemptable, but such code can't do sleep (or be very careful),
+ * otherwise data is corrupted. For details, please check schedule() where
+ * blk_schedule_flush_plug() is called.
+ */
 struct blk_plug {
 	unsigned long magic;
 	struct list_head list;

commit 55c022bbddb2c056b5dff1bd1b1758d31b6d64c9
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Fri Jul 8 08:19:20 2011 +0200

    block: avoid building too big plug list
    
    When I test fio script with big I/O depth, I found the total throughput drops
    compared to some relative small I/O depth. The reason is the thread accumulates
    big requests in its plug list and causes some delays (surely this depends
    on CPU speed).
    I thought we'd better have a threshold for requests. When a threshold reaches,
    this means there is no request merge and queue lock contention isn't severe
    when pushing per-task requests to queue, so the main advantages of blk plug
    don't exist. We can force a plug list flush in this case.
    With this, my test throughput actually increases and almost equals to small
    I/O depth. Another side effect is irq off time decreases in blk_flush_plug_list()
    for big I/O depth.
    The BLK_MAX_REQUEST_COUNT is choosen arbitarily, but 16 is efficiently to
    reduce lock contention to me. But I'm open here, 32 is ok in my test too.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 36f2e2b99ae3..92edb9601242 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -862,7 +862,10 @@ struct blk_plug {
 	struct list_head list;
 	struct list_head cb_list;
 	unsigned int should_sort;
+	unsigned int count;
 };
+#define BLK_MAX_REQUEST_COUNT 16
+
 struct blk_plug_cb {
 	struct list_head list;
 	void (*callback)(struct blk_plug_cb *);

commit 04bf7869ca0fd12009aee301cac2264a36df4d98
Merge: d2f31a5fd60d 7b28afe01ab6
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Fri Jul 1 16:17:13 2011 +0200

    Merge branch 'for-linus' into for-3.1/core
    
    Conflicts:
            block/blk-throttle.c
            block/cfq-iosched.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

commit 4d0d98b60eba726e0a4f3e6617628b070c444707
Author: Wanlong Gao <wanlong.gao@gmail.com>
Date:   Mon Jun 13 10:45:38 2011 +0200

    block:fix the comment error in blkdev.h
    
    There is not a function rq_init but blk_rq_init in block/blk-core.c.
    
    Signed-off-by: Wanlong Gao <wanlong.gao@gmail.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ae9091a68480..4ce6e68da2bd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -73,7 +73,7 @@ enum rq_cmd_type_bits {
 
 /*
  * try to put the fields that are referenced together in the same cacheline.
- * if you modify this structure, be sure to check block/blk-core.c:rq_init()
+ * if you modify this structure, be sure to check block/blk-core.c:blk_rq_init()
  * as well!
  */
 struct request {

commit ea9d6553b3b3044e7374774cc33bb1b2eee19dd3
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Tue May 31 13:45:53 2011 +0200

    block: remove unwanted semicolons
    
    Since those defined functions require additional semicolon
    from the caller, they could cause potential syntax errors
    when used in if-else statements.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ae9091a68480..1a23722e8878 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1282,8 +1282,8 @@ queue_max_integrity_segments(struct request_queue *q)
 #define blk_get_integrity(a)			(0)
 #define blk_integrity_compare(a, b)		(0)
 #define blk_integrity_register(a, b)		(0)
-#define blk_integrity_unregister(a)		do { } while (0);
-#define blk_queue_max_integrity_segments(a, b)	do { } while (0);
+#define blk_integrity_unregister(a)		do { } while (0)
+#define blk_queue_max_integrity_segments(a, b)	do { } while (0)
 #define queue_max_integrity_segments(a)		(0)
 #define blk_integrity_merge_rq(a, b, c)		(0)
 #define blk_integrity_merge_bio(a, b, c)	(0)

commit 698567f3fa790fea37509a54dea855302dd88331
Merge: d70d0711edd8 61c4f2c81c61
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Fri May 20 20:33:15 2011 +0200

    Merge commit 'v2.6.39' into for-2.6.40/core
    
    Since for-2.6.40/core was forked off the 2.6.39 devel tree, we've
    had churn in the core area that makes it difficult to handle
    patches for eg cfq or blk-throttle. Instead of requiring that they
    be based in older versions with bugs that have been fixed later
    in the rc cycle, merge in 2.6.39 final.
    
    Also fixes up conflicts in the below files.
    
    Conflicts:
            drivers/block/paride/pcd.c
            drivers/cdrom/viocd.c
            drivers/ide/ide-cd.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

commit a934a00a69e940b126b9bdbf83e630ef5fe43523
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed May 18 10:37:35 2011 +0200

    block: Fix discard topology stacking and reporting
    
    In some cases we would end up stacking discard_zeroes_data incorrectly.
    Fix this by enabling the feature by default for stacking drivers and
    clearing it for low-level drivers. Incorporating a device that does not
    support dzd will then cause the feature to be disabled in the stacking
    driver.
    
    Also ensure that the maximum discard value does not overflow when
    exported in sysfs and return 0 in the alignment and dzd fields for
    devices that don't support discard.
    
    Reported-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9f921bf4bf8c..520d8618ed76 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -257,7 +257,7 @@ struct queue_limits {
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
 	unsigned char		cluster;
-	signed char		discard_zeroes_data;
+	unsigned char		discard_zeroes_data;
 };
 
 struct request_queue
@@ -1069,13 +1069,16 @@ static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector
 {
 	unsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);
 
+	if (!lim->max_discard_sectors)
+		return 0;
+
 	return (lim->discard_granularity + lim->discard_alignment - alignment)
 		& (lim->discard_granularity - 1);
 }
 
 static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)
 {
-	if (q->limits.discard_zeroes_data == 1)
+	if (q->limits.max_discard_sectors && q->limits.discard_zeroes_data == 1)
 		return 1;
 
 	return 0;

commit 3ac0cc4508709d42ec9aa351086c7d38bfc0660c
Author: shaohua.li@intel.com <shaohua.li@intel.com>
Date:   Fri May 6 11:34:41 2011 -0600

    block: hold queue if flush is running for non-queueable flush drive
    
    In some drives, flush requests are non-queueable. When flush request is
    running, normal read/write requests can't run. If block layer dispatches
    such request, driver can't handle it and requeue it.  Tejun suggested we
    can hold the queue when flush is running. This can avoid unnecessary
    requeue.  Also this can improve performance. For example, we have
    request flush1, write1, flush 2. flush1 is dispatched, then queue is
    hold, write1 isn't inserted to queue. After flush1 is finished, flush2
    will be dispatched. Since disk cache is already clean, flush2 will be
    finished very soon, so looks like flush2 is folded to flush1.
    
    In my test, the queue holding completely solves a regression introduced by
    commit 53d63e6b0dfb95882ec0219ba6bbd50cde423794:
    
        block: make the flush insertion use the tail of the dispatch list
    
        It's not a preempt type request, in fact we have to insert it
        behind requests that do specify INSERT_FRONT.
    
    which causes about 20% regression running a sysbench fileio
    workload.
    
    Stable: 2.6.39 only
    
    Cc: stable@kernel.org
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8bd2a271b2d8..9f921bf4bf8c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -365,6 +365,7 @@ struct request_queue
 	 */
 	unsigned int		flush_flags;
 	unsigned int		flush_not_queueable:1;
+	unsigned int		flush_queue_delayed:1;
 	unsigned int		flush_pending_idx:1;
 	unsigned int		flush_running_idx:1;
 	unsigned long		flush_pending_since;

commit f3876930952390a31c3a7fd68dd621464a36eb80
Author: shaohua.li@intel.com <shaohua.li@intel.com>
Date:   Fri May 6 11:34:32 2011 -0600

    block: add a non-queueable flush flag
    
    flush request isn't queueable in some drives. Add a flag to let driver
    notify block layer about this. We can optimize flush performance with the
    knowledge.
    
    Stable: 2.6.39 only
    
    Cc: stable@kernel.org
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cbbfd98ad4a3..8bd2a271b2d8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -364,6 +364,7 @@ struct request_queue
 	 * for flush operations
 	 */
 	unsigned int		flush_flags;
+	unsigned int		flush_not_queueable:1;
 	unsigned int		flush_pending_idx:1;
 	unsigned int		flush_running_idx:1;
 	unsigned long		flush_pending_since;
@@ -843,6 +844,7 @@ extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
+extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
@@ -1111,6 +1113,11 @@ static inline unsigned int block_size(struct block_device *bdev)
 	return bdev->bd_block_size;
 }
 
+static inline bool queue_flush_queueable(struct request_queue *q)
+{
+	return !q->flush_not_queueable;
+}
+
 typedef struct {struct page *v;} Sector;
 
 unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);

commit c21e6beba8835d09bb80e34961430b13e60381c5
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Apr 19 13:32:46 2011 +0200

    block: get rid of QUEUE_FLAG_REENTER
    
    We are currently using this flag to check whether it's safe
    to call into ->request_fn(). If it is set, we punt to kblockd.
    But we get a lot of false positives and excessive punts to
    kblockd, which hurts performance.
    
    The only real abuser of this infrastructure is SCSI. So export
    the async queue run and convert SCSI over to use that. There's
    room for improvement in that SCSI need not always use the async
    call, but this fixes our performance issue and they can fix that
    up in due time.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cbbfd98ad4a3..2ad95fa1d130 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -388,20 +388,19 @@ struct request_queue
 #define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
 #define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
-#define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
-#define QUEUE_FLAG_ELVSWITCH	7	/* don't use elevator, just do FIFO */
-#define QUEUE_FLAG_BIDI		8	/* queue supports bidi requests */
-#define QUEUE_FLAG_NOMERGES     9	/* disable merge attempts */
-#define QUEUE_FLAG_SAME_COMP   10	/* force complete on same CPU */
-#define QUEUE_FLAG_FAIL_IO     11	/* fake timeout */
-#define QUEUE_FLAG_STACKABLE   12	/* supports request stacking */
-#define QUEUE_FLAG_NONROT      13	/* non-rotational device (SSD) */
+#define QUEUE_FLAG_ELVSWITCH	6	/* don't use elevator, just do FIFO */
+#define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
+#define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
+#define QUEUE_FLAG_SAME_COMP	9	/* force complete on same CPU */
+#define QUEUE_FLAG_FAIL_IO     10	/* fake timeout */
+#define QUEUE_FLAG_STACKABLE   11	/* supports request stacking */
+#define QUEUE_FLAG_NONROT      12	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
-#define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
-#define QUEUE_FLAG_DISCARD     16	/* supports DISCARD */
-#define QUEUE_FLAG_NOXMERGES   17	/* No extended merges */
-#define QUEUE_FLAG_ADD_RANDOM  18	/* Contributes to random pool */
-#define QUEUE_FLAG_SECDISCARD  19	/* supports SECDISCARD */
+#define QUEUE_FLAG_IO_STAT     13	/* do IO stats */
+#define QUEUE_FLAG_DISCARD     14	/* supports DISCARD */
+#define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
+#define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
+#define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@ -699,6 +698,7 @@ extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
 extern void __blk_run_queue(struct request_queue *q);
 extern void blk_run_queue(struct request_queue *);
+extern void blk_run_queue_async(struct request_queue *q);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,
 			   gfp_t);

commit 24ecfbe27f65563909b14492afda2f1c21f7c044
Author: Christoph Hellwig <hch@infradead.org>
Date:   Mon Apr 18 11:41:33 2011 +0200

    block: add blk_run_queue_async
    
    Instead of overloading __blk_run_queue to force an offload to kblockd
    add a new blk_run_queue_async helper to do it explicitly.  I've kept
    the blk_queue_stopped check for now, but I suspect it's not needed
    as the check we do when the workqueue items runs should be enough.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3448d89297e8..cbbfd98ad4a3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -697,7 +697,7 @@ extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
-extern void __blk_run_queue(struct request_queue *q, bool force_kblockd);
+extern void __blk_run_queue(struct request_queue *q);
 extern void blk_run_queue(struct request_queue *);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,

commit b4cb290e0a7d19235bd075c2ad4d60dbab0bac15
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Mon Apr 18 09:54:05 2011 +0200

    Revert "block: add callback function for unplug notification"
    
    MD can't use this since it really requires us to be able to
    keep more than a single piece of state for the unplug. Commit
    048c9374 added the required support for MD, so get rid of this
    now unused code.
    
    This reverts commit f75664570d8b75469cc468f23c2b27220984983b.
    
    Conflicts:
    
            block/blk-core.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3f7879391a7..3448d89297e8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -196,7 +196,6 @@ typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
-typedef void (unplugged_fn) (struct request_queue *);
 
 struct bio_vec;
 struct bvec_merge_data {
@@ -284,7 +283,6 @@ struct request_queue
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
-	unplugged_fn		*unplugged_fn;
 
 	/*
 	 * Dispatch queue sorting
@@ -843,7 +841,6 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
-extern void blk_queue_unplugged(struct request_queue *, unplugged_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);

commit 048c9374a749a27f16493cea033fa4a8ff492356
Author: NeilBrown <neilb@suse.de>
Date:   Mon Apr 18 09:52:22 2011 +0200

    block: Enhance new plugging support to support general callbacks
    
    md/raid requires an unplug callback, but as it does not uses
    requests the current code cannot provide one.
    
    So allow arbitrary callbacks to be attached to the blk_plug.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ec0357d8c4a5..f3f7879391a7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -860,8 +860,13 @@ extern void blk_put_queue(struct request_queue *);
 struct blk_plug {
 	unsigned long magic;
 	struct list_head list;
+	struct list_head cb_list;
 	unsigned int should_sort;
 };
+struct blk_plug_cb {
+	struct list_head list;
+	void (*callback)(struct blk_plug_cb *);
+};
 
 extern void blk_start_plug(struct blk_plug *);
 extern void blk_finish_plug(struct blk_plug *);
@@ -887,7 +892,7 @@ static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 {
 	struct blk_plug *plug = tsk->plug;
 
-	return plug && !list_empty(&plug->list);
+	return plug && (!list_empty(&plug->list) || !list_empty(&plug->cb_list));
 }
 
 /*

commit a237c1c5bc5dc5c76a21be922dca4826f3eca8ca
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Sat Apr 16 13:27:55 2011 +0200

    block: let io_schedule() flush the plug inline
    
    Linus correctly observes that the most important dispatch cases
    are now done from kblockd, this isn't ideal for latency reasons.
    The original reason for switching dispatches out-of-line was to
    avoid too deep a stack, so by _only_ letting the "accidental"
    flush directly in schedule() be guarded by offload to kblockd,
    we should be able to get the best of both worlds.
    
    So add a blk_schedule_flush_plug() that offloads to kblockd,
    and only use that from the schedule() path.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1c76506fcf11..ec0357d8c4a5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -871,6 +871,14 @@ static inline void blk_flush_plug(struct task_struct *tsk)
 {
 	struct blk_plug *plug = tsk->plug;
 
+	if (plug)
+		blk_flush_plug_list(plug, false);
+}
+
+static inline void blk_schedule_flush_plug(struct task_struct *tsk)
+{
+	struct blk_plug *plug = tsk->plug;
+
 	if (plug)
 		blk_flush_plug_list(plug, true);
 }
@@ -1317,6 +1325,11 @@ static inline void blk_flush_plug(struct task_struct *task)
 {
 }
 
+static inline void blk_schedule_flush_plug(struct task_struct *task)
+{
+}
+
+
 static inline bool blk_needs_flush_plug(struct task_struct *tsk)
 {
 	return false;

commit f6603783f9f099bf7a83b3f6c689bbbf74f0e96e
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Fri Apr 15 15:49:07 2011 +0200

    block: only force kblockd unplugging from the schedule() path
    
    For the explicit unplugging, we'd prefer to kick things off
    immediately and not pay the penalty of the latency to switch
    to kblockd. So let blk_finish_plug() do the run inline, while
    the implicit-on-schedule-out unplug will punt to kblockd.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ffe48ff318f9..1c76506fcf11 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -865,14 +865,14 @@ struct blk_plug {
 
 extern void blk_start_plug(struct blk_plug *);
 extern void blk_finish_plug(struct blk_plug *);
-extern void blk_flush_plug_list(struct blk_plug *);
+extern void blk_flush_plug_list(struct blk_plug *, bool);
 
 static inline void blk_flush_plug(struct task_struct *tsk)
 {
 	struct blk_plug *plug = tsk->plug;
 
 	if (plug)
-		blk_flush_plug_list(plug);
+		blk_flush_plug_list(plug, true);
 }
 
 static inline bool blk_needs_flush_plug(struct task_struct *tsk)

commit 88b996cd0652280cc9b9fc70008fda15f14175e1
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 15 15:20:10 2011 +0200

    block: cleanup the block plug helper functions
    
    It's a bit of a mess currently. task->plug is being cleared
    and reset in __blk_finish_plug(), and blk_finish_plug() is
    testing for a NULL plug which cannot happen even from schedule()
    anymore since it uses blk_needs_flush_plug() to determine
    whether to call into this function at all.
    
    So get rid of some of the cruft.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c07ffafac5d4..ffe48ff318f9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -865,14 +865,14 @@ struct blk_plug {
 
 extern void blk_start_plug(struct blk_plug *);
 extern void blk_finish_plug(struct blk_plug *);
-extern void __blk_flush_plug(struct task_struct *, struct blk_plug *);
+extern void blk_flush_plug_list(struct blk_plug *);
 
 static inline void blk_flush_plug(struct task_struct *tsk)
 {
 	struct blk_plug *plug = tsk->plug;
 
-	if (unlikely(plug))
-		__blk_flush_plug(tsk, plug);
+	if (plug)
+		blk_flush_plug_list(plug);
 }
 
 static inline bool blk_needs_flush_plug(struct task_struct *tsk)

commit f75664570d8b75469cc468f23c2b27220984983b
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Apr 12 10:17:31 2011 +0200

    block: add callback function for unplug notification
    
    MD would like to know when a queue is unplugged, so it can flush
    it's bitmap writes. Add such a callback.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 32176cc8e715..c07ffafac5d4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -196,6 +196,7 @@ typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
+typedef void (unplugged_fn) (struct request_queue *);
 
 struct bio_vec;
 struct bvec_merge_data {
@@ -283,6 +284,7 @@ struct request_queue
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
+	unplugged_fn		*unplugged_fn;
 
 	/*
 	 * Dispatch queue sorting
@@ -841,6 +843,7 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
+extern void blk_queue_unplugged(struct request_queue *, unplugged_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);

commit a63a5cf84dac7a23a57c800eea5734701e7d3c04
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Apr 1 21:02:31 2011 +0200

    dm: improve block integrity support
    
    The current block integrity (DIF/DIX) support in DM is verifying that
    all devices' integrity profiles match during DM device resume (which
    is past the point of no return).  To some degree that is unavoidable
    (stacked DM devices force this late checking).  But for most DM
    devices (which aren't stacking on other DM devices) the ideal time to
    verify all integrity profiles match is during table load.
    
    Introduce the notion of an "initialized" integrity profile: a profile
    that was blk_integrity_register()'d with a non-NULL 'blk_integrity'
    template.  Add blk_integrity_is_initialized() to allow checking if a
    profile was initialized.
    
    Update DM integrity support to:
    - check all devices with _initialized_ integrity profiles match
      during table load; uninitialized profiles (e.g. for underlying DM
      device(s) of a stacked DM device) are ignored.
    - disallow a table load that would result in an integrity profile that
      conflicts with a DM device's existing (in-use) integrity profile
    - avoid clearing an existing integrity profile
    - validate all integrity profiles match during resume; but if they
      don't all we can do is report the mismatch (during resume we're past
      the point of no return)
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 16a902f099ac..32176cc8e715 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1206,6 +1206,7 @@ struct blk_integrity {
 	struct kobject		kobj;
 };
 
+extern bool blk_integrity_is_initialized(struct gendisk *);
 extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
 extern void blk_integrity_unregister(struct gendisk *);
 extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
@@ -1262,6 +1263,7 @@ queue_max_integrity_segments(struct request_queue *q)
 #define queue_max_integrity_segments(a)		(0)
 #define blk_integrity_merge_rq(a, b, c)		(0)
 #define blk_integrity_merge_bio(a, b, c)	(0)
+#define blk_integrity_is_initialized(a)		(0)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 

commit 1f940bdfc0d03265d178d9dfd840d854819f797d
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Fri Mar 11 20:17:08 2011 +0100

    block: fixup plugging stubs for !CONFIG_BLOCK
    
    They used an older prototype, fix it up.
    
    Reported-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 91fa428fa2c1..16a902f099ac 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1297,15 +1297,18 @@ static inline long nr_blockdev_pages(void)
 	return 0;
 }
 
-static inline void blk_start_plug(struct list_head *list)
+struct blk_plug {
+};
+
+static inline void blk_start_plug(struct blk_plug *plug)
 {
 }
 
-static inline void blk_finish_plug(struct list_head *list)
+static inline void blk_finish_plug(struct blk_plug *plug)
 {
 }
 
-static inline void blk_flush_plug(struct task_struct *tsk)
+static inline void blk_flush_plug(struct task_struct *task)
 {
 }
 

commit 4c63f5646e405b5010cc9499419060bf2e838f5b
Merge: cafb0bfca1a7 69d60eb96ae8
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Mar 10 08:58:35 2011 +0100

    Merge branch 'for-2.6.39/stack-plug' into for-2.6.39/core
    
    Conflicts:
            block/blk-core.c
            block/blk-flush.c
            drivers/md/raid1.c
            drivers/md/raid10.c
            drivers/md/raid5.c
            fs/nilfs2/btnode.c
            fs/nilfs2/mdt.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

commit 7eaceaccab5f40bbfda044629a6298616aeaed50
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Mar 10 08:52:07 2011 +0100

    block: remove per-queue plugging
    
    Code has been converted over to the new explicit on-stack plugging,
    and delay users have been converted to use the new API for that.
    So lets kill off the old plugging along with aops->sync_page().
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5873037eeb91..64ab2a1bb167 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -196,7 +196,6 @@ typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
-typedef void (unplug_fn) (struct request_queue *);
 
 struct bio_vec;
 struct bvec_merge_data {
@@ -279,7 +278,6 @@ struct request_queue
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unprep_rq_fn		*unprep_rq_fn;
-	unplug_fn		*unplug_fn;
 	merge_bvec_fn		*merge_bvec_fn;
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
@@ -292,14 +290,6 @@ struct request_queue
 	sector_t		end_sector;
 	struct request		*boundary_rq;
 
-	/*
-	 * Auto-unplugging state
-	 */
-	struct timer_list	unplug_timer;
-	int			unplug_thresh;	/* After this many requests */
-	unsigned long		unplug_delay;	/* After this many jiffies */
-	struct work_struct	unplug_work;
-
 	/*
 	 * Delayed queue handling
 	 */
@@ -399,14 +389,13 @@ struct request_queue
 #define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
-#define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
-#define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
-#define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
-#define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
-#define QUEUE_FLAG_SAME_COMP   11	/* force complete on same CPU */
-#define QUEUE_FLAG_FAIL_IO     12	/* fake timeout */
-#define QUEUE_FLAG_STACKABLE   13	/* supports request stacking */
-#define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
+#define QUEUE_FLAG_ELVSWITCH	7	/* don't use elevator, just do FIFO */
+#define QUEUE_FLAG_BIDI		8	/* queue supports bidi requests */
+#define QUEUE_FLAG_NOMERGES     9	/* disable merge attempts */
+#define QUEUE_FLAG_SAME_COMP   10	/* force complete on same CPU */
+#define QUEUE_FLAG_FAIL_IO     11	/* fake timeout */
+#define QUEUE_FLAG_STACKABLE   12	/* supports request stacking */
+#define QUEUE_FLAG_NONROT      13	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
 #define QUEUE_FLAG_DISCARD     16	/* supports DISCARD */
@@ -484,7 +473,6 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 	__clear_bit(flag, &q->queue_flags);
 }
 
-#define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
@@ -679,9 +667,6 @@ extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 extern void blk_rq_unprep_clone(struct request *rq);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
-extern void blk_plug_device(struct request_queue *);
-extern void blk_plug_device_unlocked(struct request_queue *);
-extern int blk_remove_plug(struct request_queue *);
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
@@ -726,7 +711,6 @@ extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
-extern void blk_unplug(struct request_queue *q);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {
@@ -863,7 +847,6 @@ extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bd
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
-extern void generic_unplug_device(struct request_queue *);
 extern long nr_blockdev_pages(void);
 
 int blk_get_queue(struct request_queue *);

commit 73c101011926c5832e6e141682180c4debe2cf45
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Mar 8 13:19:51 2011 +0100

    block: initial patch for on-stack per-task plugging
    
    This patch adds support for creating a queuing context outside
    of the queue itself. This enables us to batch up pieces of IO
    before grabbing the block device queue lock and submitting them to
    the IO scheduler.
    
    The context is created on the stack of the process and assigned in
    the task structure, so that we can auto-unplug it if we hit a schedule
    event.
    
    The current queue plugging happens implicitly if IO is submitted to
    an empty device, yet callers have to remember to unplug that IO when
    they are going to wait for it. This is an ugly API and has caused bugs
    in the past. Additionally, it requires hacks in the vm (->sync_page()
    callback) to handle that logic. By switching to an explicit plugging
    scheme we make the API a lot nicer and can get rid of the ->sync_page()
    hack in the vm.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f55b2a8b6610..5873037eeb91 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -871,6 +871,31 @@ struct request_queue *blk_alloc_queue(gfp_t);
 struct request_queue *blk_alloc_queue_node(gfp_t, int);
 extern void blk_put_queue(struct request_queue *);
 
+struct blk_plug {
+	unsigned long magic;
+	struct list_head list;
+	unsigned int should_sort;
+};
+
+extern void blk_start_plug(struct blk_plug *);
+extern void blk_finish_plug(struct blk_plug *);
+extern void __blk_flush_plug(struct task_struct *, struct blk_plug *);
+
+static inline void blk_flush_plug(struct task_struct *tsk)
+{
+	struct blk_plug *plug = tsk->plug;
+
+	if (unlikely(plug))
+		__blk_flush_plug(tsk, plug);
+}
+
+static inline bool blk_needs_flush_plug(struct task_struct *tsk)
+{
+	struct blk_plug *plug = tsk->plug;
+
+	return plug && !list_empty(&plug->list);
+}
+
 /*
  * tag stuff
  */
@@ -1294,6 +1319,23 @@ static inline long nr_blockdev_pages(void)
 	return 0;
 }
 
+static inline void blk_start_plug(struct list_head *list)
+{
+}
+
+static inline void blk_finish_plug(struct list_head *list)
+{
+}
+
+static inline void blk_flush_plug(struct task_struct *tsk)
+{
+}
+
+static inline bool blk_needs_flush_plug(struct task_struct *tsk)
+{
+	return false;
+}
+
 #endif /* CONFIG_BLOCK */
 
 #endif

commit 3cca6dc1c81e2407928dc4c6105252146fd3924f
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Wed Mar 2 11:08:00 2011 -0500

    block: add API for delaying work/request_fn a little bit
    
    Currently we use plugging for that, but as plugging is going away,
    we need an alternative mechanism.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e3ee74fc5903..f55b2a8b6610 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -300,6 +300,11 @@ struct request_queue
 	unsigned long		unplug_delay;	/* After this many jiffies */
 	struct work_struct	unplug_work;
 
+	/*
+	 * Delayed queue handling
+	 */
+	struct delayed_work	delay_work;
+
 	struct backing_dev_info	backing_dev_info;
 
 	/*
@@ -677,6 +682,7 @@ extern int blk_insert_cloned_request(struct request_queue *q,
 extern void blk_plug_device(struct request_queue *);
 extern void blk_plug_device_unlocked(struct request_queue *);
 extern int blk_remove_plug(struct request_queue *);
+extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			  unsigned int, void __user *);

commit e83a46bbb1d4c03defd733a64b727632a40059ad
Merge: da527770007f fd51469fb68b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Mar 4 19:09:02 2011 +0100

    Merge branch 'for-linus' of ../linux-2.6-block into block-for-2.6.39/core
    
    This merge creates two set of conflicts.  One is simple context
    conflicts caused by removal of throtl_scheduled_delayed_work() in
    for-linus and removal of throtl_shutdown_timer_wq() in
    for-2.6.39/core.
    
    The other is caused by commit 255bb490c8 (block: blk-flush shouldn't
    call directly into q->request_fn() __blk_run_queue()) in for-linus
    crashing with FLUSH reimplementation in for-2.6.39/core.  The conflict
    isn't trivial but the resolution is straight-forward.
    
    * __blk_run_queue() calls in flush_end_io() and flush_data_end_io()
      should be called with @force_kblockd set to %true.
    
    * elv_insert() in blk_kick_flush() should use
      %ELEVATOR_INSERT_REQUEUE.
    
    Both changes are to avoid invoking ->request_fn() directly from
    request completion path and closely match the changes in the commit
    255bb490c8.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit da527770007fce8e4541947d47918248286da875
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Wed Mar 2 19:05:33 2011 -0500

    block: Move blk_throtl_exit() call to blk_cleanup_queue()
    
    Move blk_throtl_exit() in blk_cleanup_queue() as blk_throtl_exit() is
    written in such a way that it needs queue lock. In blk_release_queue()
    there is no gurantee that ->queue_lock is still around.
    
    Initially blk_throtl_exit() was in blk_cleanup_queue() but Ingo reported
    one problem.
    
      https://lkml.org/lkml/2010/10/23/86
    
      And a quick fix moved blk_throtl_exit() to blk_release_queue().
    
            commit 7ad58c028652753814054f4e3ac58f925e7343f4
            Author: Jens Axboe <jaxboe@fusionio.com>
            Date:   Sat Oct 23 20:40:26 2010 +0200
    
            block: fix use-after-free bug in blk throttle code
    
    This patch reverts above change and does not try to shutdown the
    throtl work in blk_sync_queue(). By avoiding call to
    throtl_shutdown_timer_wq() from blk_sync_queue(), we should also avoid
    the problem reported by Ingo.
    
    blk_sync_queue() seems to be used only by md driver and it seems to be
    using it to make sure q->unplug_fn is not called as md registers its
    own unplug functions and it is about to free up the data structures
    used by unplug_fn(). Block throttle does not call back into unplug_fn()
    or into md. So there is no need to cancel blk throttle work.
    
    In fact I think cancelling block throttle work is bad because it might
    happen that some bios are throttled and scheduled to be dispatched later
    with the help of pending work and if work is cancelled, these bios might
    never be dispatched.
    
    Block layer also uses blk_sync_queue() during blk_cleanup_queue() and
    blk_release_queue() time. That should be safe as we are also calling
    blk_throtl_exit() which should make sure all the throttling related
    data structures are cleaned up.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e3ee74fc5903..23fb92506c31 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1144,7 +1144,6 @@ extern int blk_throtl_init(struct request_queue *q);
 extern void blk_throtl_exit(struct request_queue *q);
 extern int blk_throtl_bio(struct request_queue *q, struct bio **bio);
 extern void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay);
-extern void throtl_shutdown_timer_wq(struct request_queue *q);
 #else /* CONFIG_BLK_DEV_THROTTLING */
 static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
 {
@@ -1154,7 +1153,6 @@ static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
 static inline int blk_throtl_init(struct request_queue *q) { return 0; }
 static inline int blk_throtl_exit(struct request_queue *q) { return 0; }
 static inline void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay) {}
-static inline void throtl_shutdown_timer_wq(struct request_queue *q) {}
 #endif /* CONFIG_BLK_DEV_THROTTLING */
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \

commit 1654e7411a1ad4999fe7890ef51d2a2bbb1fcf76
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 2 08:48:05 2011 -0500

    block: add @force_kblockd to __blk_run_queue()
    
    __blk_run_queue() automatically either calls q->request_fn() directly
    or schedules kblockd depending on whether the function is recursed.
    blk-flush implementation needs to be able to explicitly choose
    kblockd.  Add @force_kblockd.
    
    All the current users are converted to specify %false for the
    parameter and this patch doesn't introduce any behavior change.
    
    stable: This is prerequisite for fixing ide oops caused by the new
            blk-flush implementation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Jan Beulich <JBeulich@novell.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dd8cd0f47e3a..d5063e1b5555 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -699,7 +699,7 @@ extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
-extern void __blk_run_queue(struct request_queue *);
+extern void __blk_run_queue(struct request_queue *q, bool force_kblockd);
 extern void blk_run_queue(struct request_queue *);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,

commit 6fae9c25134baffbeeb20031479e7ff6f6d8eec0
Merge: c186794dbb46 f5412be59960
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Mar 1 15:04:39 2011 -0500

    Merge commit 'v2.6.38-rc6' into for-2.6.39/core
    
    Conflicts:
            block/cfq-iosched.c
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

commit 450adcbe518ab3a3953d8475309525d22de77cba
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Tue Mar 1 13:40:54 2011 -0500

    blk-throttle: Do not use kblockd workqueue for throtl work
    
    o Dominik Klein reported a system hang issue while doing some blkio
      throttling testing.
    
      https://lkml.org/lkml/2011/2/24/173
    
    o Some tracing revealed that CFQ was not dispatching any more jobs as
      queue unplug was not happening. And queue unplug was not happening
      because unplug work was not being called as there was one throttling
      work on same cpu which as not finished yet. And throttling work had not
      finished as it was tyring to dispatch a bio to CFQ but all the request
      descriptors were consume to it was put to sleep.
    
    o So basically it is a cyclic dependecny between CFQ unplug work and
      throtl dispatch work. Tejun suggested that use separate workqueue for
      such cases.
    
    o This patch uses a separate workqueue for throttle related work and
      does not rely on kblockd workqueue anymore.
    
    Cc: stable@kernel.org
    Reported-by: Dominik Klein <dk@in-telegence.net>
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4d18ff34670a..dd8cd0f47e3a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1088,7 +1088,6 @@ static inline void put_dev_sector(Sector p)
 
 struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
-int kblockd_schedule_delayed_work(struct request_queue *q, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*
@@ -1136,7 +1135,6 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 extern int blk_throtl_init(struct request_queue *q);
 extern void blk_throtl_exit(struct request_queue *q);
 extern int blk_throtl_bio(struct request_queue *q, struct bio **bio);
-extern void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay);
 extern void throtl_shutdown_timer_wq(struct request_queue *q);
 #else /* CONFIG_BLK_DEV_THROTTLING */
 static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
@@ -1146,7 +1144,6 @@ static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
 
 static inline int blk_throtl_init(struct request_queue *q) { return 0; }
 static inline int blk_throtl_exit(struct request_queue *q) { return 0; }
-static inline void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay) {}
 static inline void throtl_shutdown_timer_wq(struct request_queue *q) {}
 #endif /* CONFIG_BLK_DEV_THROTTLING */
 

commit c186794dbb466b45cf40f942f2d09d6d5b4b0e42
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Feb 11 11:08:00 2011 +0100

    block: share request flush fields with  elevator_private
    
    Flush requests are never put on the IO scheduler.  Convert request
    structure's elevator_private* into an array and have the flush fields
    share a union with it.
    
    Reclaim the space lost in 'struct request' by moving 'completion_data'
    back in the union with 'rb_node'.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Acked-by: Vivek Goyal <vgoyal@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6d7e9afd08c3..12bb426949e9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,25 +99,26 @@ struct request {
 	/*
 	 * The rb_node is only used inside the io scheduler, requests
 	 * are pruned when moved to the dispatch queue. So let the
-	 * flush fields share space with the rb_node.
+	 * completion_data share space with the rb_node.
 	 */
 	union {
 		struct rb_node rb_node;	/* sort/lookup */
-		struct {
-			unsigned int			seq;
-			struct list_head		list;
-		} flush;
+		void *completion_data;
 	};
 
-	void *completion_data;
-
 	/*
 	 * Three pointers are available for the IO schedulers, if they need
-	 * more they have to dynamically allocate it.
+	 * more they have to dynamically allocate it.  Flush requests are
+	 * never put on the IO scheduler. So let the flush fields share
+	 * space with the three elevator_private pointers.
 	 */
-	void *elevator_private;
-	void *elevator_private2;
-	void *elevator_private3;
+	union {
+		void *elevator_private[3];
+		struct {
+			unsigned int		seq;
+			struct list_head	list;
+		} flush;
+	};
 
 	struct gendisk *rq_disk;
 	unsigned long start_time;

commit ae1b1539622fb46e51b4d13b3f9e5f4c713f86ae
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 25 12:43:54 2011 +0100

    block: reimplement FLUSH/FUA to support merge
    
    The current FLUSH/FUA support has evolved from the implementation
    which had to perform queue draining.  As such, sequencing is done
    queue-wide one flush request after another.  However, with the
    draining requirement gone, there's no reason to keep the queue-wide
    sequential approach.
    
    This patch reimplements FLUSH/FUA support such that each FLUSH/FUA
    request is sequenced individually.  The actual FLUSH execution is
    double buffered and whenever a request wants to execute one for either
    PRE or POSTFLUSH, it queues on the pending queue.  Once certain
    conditions are met, a flush request is issued and on its completion
    all pending requests proceed to the next sequence.
    
    This allows arbitrary merging of different type of flushes.  How they
    are merged can be primarily controlled and tuned by adjusting the
    above said 'conditions' used to determine when to issue the next
    flush.
    
    This is inspired by Darrick's patches to merge multiple zero-data
    flushes which helps workloads with highly concurrent fsync requests.
    
    * As flush requests are never put on the IO scheduler, request fields
      used for flush share space with rq->rb_node.  rq->completion_data is
      moved out of the union.  This increases the request size by one
      pointer.
    
      As rq->elevator_private* are used only by the iosched too, it is
      possible to reduce the request size further.  However, to do that,
      we need to modify request allocation path such that iosched data is
      not allocated for flush requests.
    
    * FLUSH/FUA processing happens on insertion now instead of dispatch.
    
    - Comments updated as per Vivek and Mike.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "Darrick J. Wong" <djwong@us.ibm.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 36ab42c9bb99..6d7e9afd08c3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,13 +99,18 @@ struct request {
 	/*
 	 * The rb_node is only used inside the io scheduler, requests
 	 * are pruned when moved to the dispatch queue. So let the
-	 * completion_data share space with the rb_node.
+	 * flush fields share space with the rb_node.
 	 */
 	union {
 		struct rb_node rb_node;	/* sort/lookup */
-		void *completion_data;
+		struct {
+			unsigned int			seq;
+			struct list_head		list;
+		} flush;
 	};
 
+	void *completion_data;
+
 	/*
 	 * Three pointers are available for the IO schedulers, if they need
 	 * more they have to dynamically allocate it.
@@ -362,11 +367,12 @@ struct request_queue
 	 * for flush operations
 	 */
 	unsigned int		flush_flags;
-	unsigned int		flush_seq;
-	int			flush_err;
+	unsigned int		flush_pending_idx:1;
+	unsigned int		flush_running_idx:1;
+	unsigned long		flush_pending_since;
+	struct list_head	flush_queue[2];
+	struct list_head	flush_data_in_flight;
 	struct request		flush_rq;
-	struct request		*orig_flush_rq;
-	struct list_head	pending_flushes;
 
 	struct mutex		sysfs_lock;
 

commit 275220f0fcff1adf28a717076e00f575edf05fda
Merge: fe3c560b8a22 81c5e2ae33c4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 13 10:45:01 2011 -0800

    Merge branch 'for-2.6.38/core' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.38/core' of git://git.kernel.dk/linux-2.6-block: (43 commits)
      block: ensure that completion error gets properly traced
      blktrace: add missing probe argument to block_bio_complete
      block cfq: don't use atomic_t for cfq_group
      block cfq: don't use atomic_t for cfq_queue
      block: trace event block fix unassigned field
      block: add internal hd part table references
      block: fix accounting bug on cross partition merges
      kref: add kref_test_and_get
      bio-integrity: mark kintegrityd_wq highpri and CPU intensive
      block: make kblockd_workqueue smarter
      Revert "sd: implement sd_check_events()"
      block: Clean up exit_io_context() source code.
      Fix compile warnings due to missing removal of a 'ret' variable
      fs/block: type signature of major_to_index(int) to major_to_index(unsigned)
      block: convert !IS_ERR(p) && p to !IS_ERR_NOR_NULL(p)
      cfq-iosched: don't check cfqg in choose_service_tree()
      fs/splice: Pull buf->ops->confirm() from splice_from_pipe actors
      cdrom: export cdrom_check_events()
      sd: implement sd_check_events()
      sr: implement sr_check_events()
      ...

commit 81c5e2ae33c4b19e53966b427e33646bf6811830
Merge: 797a455d2c68 fcc57045d53e
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Jan 13 14:47:54 2011 +0100

    Merge branch 'for-2.6.38/event-handling' into for-2.6.38/core

commit 09e099d4bafea3b15be003d548bdf94b4b6e0e17
Author: Jerome Marchand <jmarchan@redhat.com>
Date:   Wed Jan 5 16:57:38 2011 +0100

    block: fix accounting bug on cross partition merges
    
    /proc/diskstats would display a strange output as follows.
    
    $ cat /proc/diskstats |grep sda
       8       0 sda 90524 7579 102154 20464 0 0 0 0 0 14096 20089
       8       1 sda1 19085 1352 21841 4209 0 0 0 0 4294967064 15689 4293424691
                                                    ~~~~~~~~~~
       8       2 sda2 71252 3624 74891 15950 0 0 0 0 232 23995 1562390
       8       3 sda3 54 487 2188 92 0 0 0 0 0 88 92
       8       4 sda4 4 0 8 0 0 0 0 0 0 0 0
       8       5 sda5 81 2027 2130 138 0 0 0 0 0 87 137
    
    Its reason is the wrong way of accounting hd_struct->in_flight. When a bio is
    merged into a request belongs to different partition by ELEVATOR_FRONT_MERGE.
    
    The detailed root cause is as follows.
    
    Assuming that there are two partition, sda1 and sda2.
    
    1. A request for sda2 is in request_queue. Hence sda1's hd_struct->in_flight
       is 0 and sda2's one is 1.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |          0
       sda2 |          1
       ---------------------------
    
    2. A bio belongs to sda1 is issued and is merged into the request mentioned on
       step1 by ELEVATOR_BACK_MERGE. The first sector of the request is changed
       from sda2 region to sda1 region. However the two partition's
       hd_struct->in_flight are not changed.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |          0
       sda2 |          1
       ---------------------------
    
    3. The request is finished and blk_account_io_done() is called. In this case,
       sda2's hd_struct->in_flight, not a sda1's one, is decremented.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |         -1
       sda2 |          1
       ---------------------------
    
    The patch fixes the problem by caching the partition lookup
    inside the request structure, hence making sure that the increment
    and decrement will always happen on the same partition struct. This
    also speeds up IO with accounting enabled, since it cuts down on
    the number of lookups we have to do.
    
    Also add a refcount to struct hd_struct to keep the partition in
    memory as long as users exist. We use kref_test_and_get() to ensure
    we don't add a reference to a partition which is going away.
    
    Signed-off-by: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aae86fd10c4f..482a7fd48831 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -115,6 +115,7 @@ struct request {
 	void *elevator_private3;
 
 	struct gendisk *rq_disk;
+	struct hd_struct *part;
 	unsigned long start_time;
 #ifdef CONFIG_BLK_CGROUP
 	unsigned long long start_time_ns;

commit 72d4cd9f38b5ed96b75df4c622be25e1c2648dd3
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Dec 17 08:34:20 2010 +0100

    block: max hardware sectors limit wrapper
    
    Implement blk_limits_max_hw_sectors() and make
    blk_queue_max_hw_sectors() a wrapper around it.
    
    DM needs this to avoid setting queue_limits' max_hw_sectors and
    max_sectors directly.  dm_set_device_limits() now leverages
    blk_limits_max_hw_sectors() logic to establish the appropriate
    max_hw_sectors minimum (PAGE_SIZE).  Fixes issue where DM was
    incorrectly setting max_sectors rather than max_hw_sectors (which
    caused dm_merge_bvec()'s max_hw_sectors check to be ineffective).
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@kernel.org
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 95aeeeb49e8b..36ab42c9bb99 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -808,6 +808,7 @@ extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
+extern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);

commit e692cb668fdd5a712c6ed2a2d6f2a36ee83997b4
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Dec 1 19:41:49 2010 +0100

    block: Deprecate QUEUE_FLAG_CLUSTER and use queue_limits instead
    
    When stacking devices, a request_queue is not always available. This
    forced us to have a no_cluster flag in the queue_limits that could be
    used as a carrier until the request_queue had been set up for a
    metadevice.
    
    There were several problems with that approach. First of all it was up
    to the stacking device to remember to set queue flag after stacking had
    completed. Also, the queue flag and the queue limits had to be kept in
    sync at all times. We got that wrong, which could lead to us issuing
    commands that went beyond the max scatterlist limit set by the driver.
    
    The proper fix is to avoid having two flags for tracking the same thing.
    We deprecate QUEUE_FLAG_CLUSTER and use the queue limit directly in the
    block layer merging functions. The queue_limit 'no_cluster' is turned
    into 'cluster' to avoid double negatives and to ease stacking.
    Clustering defaults to being enabled as before. The queue flag logic is
    removed from the stacking function, and explicitly setting the cluster
    flag is no longer necessary in DM and MD.
    
    Reported-by: Ed Lin <ed.lin@promise.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aae86fd10c4f..95aeeeb49e8b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -250,7 +250,7 @@ struct queue_limits {
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
-	unsigned char		no_cluster;
+	unsigned char		cluster;
 	signed char		discard_zeroes_data;
 };
 
@@ -380,7 +380,6 @@ struct request_queue
 #endif
 };
 
-#define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 #define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
 #define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
@@ -403,7 +402,6 @@ struct request_queue
 #define QUEUE_FLAG_SECDISCARD  19	/* supports SECDISCARD */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
-				 (1 << QUEUE_FLAG_CLUSTER) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
 				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 				 (1 << QUEUE_FLAG_ADD_RANDOM))
@@ -510,6 +508,11 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 
 #define rq_data_dir(rq)		((rq)->cmd_flags & 1)
 
+static inline unsigned int blk_queue_cluster(struct request_queue *q)
+{
+	return q->limits.cluster;
+}
+
 /*
  * We regard a request as sync, if either a read or a sync write
  */

commit 77ea887e433ad8389d416826936c110fa7910f80
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 8 20:57:37 2010 +0100

    implement in-kernel gendisk events handling
    
    Currently, media presence polling for removeable block devices is done
    from userland.  There are several issues with this.
    
    * Polling is done by periodically opening the device.  For SCSI
      devices, the command sequence generated by such action involves a
      few different commands including TEST_UNIT_READY.  This behavior,
      while perfectly legal, is different from Windows which only issues
      single command, GET_EVENT_STATUS_NOTIFICATION.  Unfortunately, some
      ATAPI devices lock up after being periodically queried such command
      sequences.
    
    * There is no reliable and unintrusive way for a userland program to
      tell whether the target device is safe for media presence polling.
      For example, polling for media presence during an on-going burning
      session can make it fail.  The polling program can avoid this by
      opening the device with O_EXCL but then it risks making a valid
      exclusive user of the device fail w/ -EBUSY.
    
    * Userland polling is unnecessarily heavy and in-kernel implementation
      is lighter and better coordinated (workqueue, timer slack).
    
    This patch implements framework for in-kernel disk event handling,
    which includes media presence polling.
    
    * bdops->check_events() is added, which supercedes ->media_changed().
      It should check whether there's any pending event and return if so.
      Currently, two events are defined - DISK_EVENT_MEDIA_CHANGE and
      DISK_EVENT_EJECT_REQUEST.  ->check_events() is guaranteed not to be
      called parallelly.
    
    * gendisk->events and ->async_events are added.  These should be
      initialized by block driver before passing the device to add_disk().
      The former contains the mask of all supported events and the latter
      the mask of all events which the device can report without polling.
      /sys/block/*/events[_async] export these to userland.
    
    * Kernel parameter block.events_dfl_poll_msecs controls the system
      polling interval (default is 0 which means disable) and
      /sys/block/*/events_poll_msecs control polling intervals for
      individual devices (default is -1 meaning use system setting).  Note
      that if a device can report all supported events asynchronously and
      its polling interval isn't explicitly set, the device won't be
      polled regardless of the system polling interval.
    
    * If a device is opened exclusively with write access, event checking
      is automatically disabled until all write exclusive accesses are
      released.
    
    * There are event 'clearing' events.  For example, both of currently
      defined events are cleared after the device has been successfully
      opened.  This information is passed to ->check_events() callback
      using @clearing argument as a hint.
    
    * Event checking is always performed from system_nrt_wq and timer
      slack is set to 25% for polling.
    
    * Nothing changes for drivers which implement ->media_changed() but
      not ->check_events().  Going forward, all drivers will be converted
      to ->check_events() and ->media_change() will be dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 83031bcf8366..05667e6989f1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1251,6 +1251,9 @@ struct block_device_operations {
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*direct_access) (struct block_device *, sector_t,
 						void **, unsigned long *);
+	unsigned int (*check_events) (struct gendisk *disk,
+				      unsigned int clearing);
+	/* ->media_changed() is DEPRECATED, use ->check_events() instead */
 	int (*media_changed) (struct gendisk *);
 	void (*unlock_native_capacity) (struct gendisk *);
 	int (*revalidate_disk) (struct gendisk *);

commit d2bf1b6723ed0eab378363649d15b7893bf14e91
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 8 20:57:36 2010 +0100

    block: move register_disk() and del_gendisk() to block/genhd.c
    
    There's no reason for register_disk() and del_gendisk() to be in
    fs/partitions/check.c.  Move both to genhd.c.  While at it, collapse
    unlink_gendisk(), which was artificially in a separate function due to
    genhd.c / check.c split, into del_gendisk().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aae86fd10c4f..83031bcf8366 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -643,7 +643,6 @@ static inline void rq_flush_dcache_pages(struct request *rq)
 
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
-extern void register_disk(struct gendisk *dev);
 extern void generic_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);

commit 02e031cbc843b010e72fcc05c76113c688b2860f
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 10 14:54:09 2010 +0100

    block: remove REQ_HARDBARRIER
    
    REQ_HARDBARRIER is dead now, so remove the leftovers.  What's left
    at this point is:
    
     - various checks inside the block layer.
     - sanity checks in bio based drivers.
     - now unused bio_empty_barrier helper.
     - Xen blockfront use of BLKIF_OP_WRITE_BARRIER - it's dead for a while,
       but Xen really needs to sort out it's barrier situaton.
     - setting of ordered tags in uas - dead code copied from old scsi
       drivers.
     - scsi different retry for barriers - it's dead and should have been
       removed when flushes were converted to FS requests.
     - blktrace handling of barriers - removed.  Someone who knows blktrace
       better should add support for REQ_FLUSH and REQ_FUA, though.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5027a599077d..aae86fd10c4f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -552,8 +552,7 @@ static inline void blk_clear_queue_full(struct request_queue *q, int sync)
  * it already be started by driver.
  */
 #define RQ_NOMERGE_FLAGS	\
-	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER | \
-	 REQ_FLUSH | REQ_FUA)
+	(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA)
 #define rq_mergeable(rq)	\
 	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
 	 (((rq)->cmd_flags & REQ_DISCARD) || \

commit a107e5a3a473a2ea62bd5af24e11b84adf1486ff
Merge: e3e1288e86a0 a269029d0e21
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Wed Oct 27 23:44:47 2010 -0400

    Merge branch 'next' into upstream-merge
    
    Conflicts:
            fs/ext4/inode.c
            fs/ext4/mballoc.c
            include/trace/events/ext4.h

commit e6fa0be699449d28a20e815bfe9ce26725ec4962
Author: Lukas Czerner <lczerner@redhat.com>
Date:   Wed Oct 27 21:30:04 2010 -0400

    Add helper function for blkdev_issue_zeroout (sb_issue_discard)
    
    This is done the same way as helper sb_issue_discard for
    blkdev_issue_discard.
    
    Signed-off-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c54906f678f..e5cb4d029689 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -941,6 +941,14 @@ static inline int sb_issue_discard(struct super_block *sb,
 	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_NOFS,
 				   BLKDEV_IFL_WAIT | BLKDEV_IFL_BARRIER);
 }
+static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
+		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
+{
+	return blkdev_issue_zeroout(sb->s_bdev,
+				    block << (sb->s_blocksize_bits - 9),
+				    nr_blocks << (sb->s_blocksize_bits - 9),
+				    gfp_mask, flags);
+}
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 

commit f253b86b4ad1b3220544e75880510fd455ebd23f
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Sun Oct 24 22:06:02 2010 +0200

    Revert "block: fix accounting bug on cross partition merges"
    
    This reverts commit 7681bfeeccff5efa9eb29bf09249a3c400b15327.
    
    Conflicts:
    
            include/linux/genhd.h
    
    It has numerous issues with the cleanup path and non-elevator
    devices. Revert it for now so we can come up with a clean
    version without rushing things.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 009b80e49f53..646b462d04df 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -115,7 +115,6 @@ struct request {
 	void *elevator_private3;
 
 	struct gendisk *rq_disk;
-	struct hd_struct *part;
 	unsigned long start_time;
 #ifdef CONFIG_BLK_CGROUP
 	unsigned long long start_time_ns;

commit a2887097f25cd38cadfc11d10769e2b349fb5eca
Merge: 8abfc6e7a45e 005a1d15f5a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 22 17:07:18 2010 -0700

    Merge branch 'for-2.6.37/barrier' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.37/barrier' of git://git.kernel.dk/linux-2.6-block: (46 commits)
      xen-blkfront: disable barrier/flush write support
      Added blk-lib.c and blk-barrier.c was renamed to blk-flush.c
      block: remove BLKDEV_IFL_WAIT
      aic7xxx_old: removed unused 'req' variable
      block: remove the BH_Eopnotsupp flag
      block: remove the BLKDEV_IFL_BARRIER flag
      block: remove the WRITE_BARRIER flag
      swap: do not send discards as barriers
      fat: do not send discards as barriers
      ext4: do not send discards as barriers
      jbd2: replace barriers with explicit flush / FUA usage
      jbd2: Modify ASYNC_COMMIT code to not rely on queue draining on barrier
      jbd: replace barriers with explicit flush / FUA usage
      nilfs2: replace barriers with explicit flush / FUA usage
      reiserfs: replace barriers with explicit flush / FUA usage
      gfs2: replace barriers with explicit flush / FUA usage
      btrfs: replace barriers with explicit flush / FUA usage
      xfs: replace barriers with explicit flush / FUA usage
      block: pass gfp_mask and flags to sb_issue_discard
      dm: convey that all flushes are processed as empty
      ...

commit 7681bfeeccff5efa9eb29bf09249a3c400b15327
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Tue Oct 19 09:05:00 2010 +0200

    block: fix accounting bug on cross partition merges
    
    /proc/diskstats would display a strange output as follows.
    
    $ cat /proc/diskstats |grep sda
       8       0 sda 90524 7579 102154 20464 0 0 0 0 0 14096 20089
       8       1 sda1 19085 1352 21841 4209 0 0 0 0 4294967064 15689 4293424691
                                                    ~~~~~~~~~~
       8       2 sda2 71252 3624 74891 15950 0 0 0 0 232 23995 1562390
       8       3 sda3 54 487 2188 92 0 0 0 0 0 88 92
       8       4 sda4 4 0 8 0 0 0 0 0 0 0 0
       8       5 sda5 81 2027 2130 138 0 0 0 0 0 87 137
    
    Its reason is the wrong way of accounting hd_struct->in_flight. When a bio is
    merged into a request belongs to different partition by ELEVATOR_FRONT_MERGE.
    
    The detailed root cause is as follows.
    
    Assuming that there are two partition, sda1 and sda2.
    
    1. A request for sda2 is in request_queue. Hence sda1's hd_struct->in_flight
       is 0 and sda2's one is 1.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |          0
       sda2 |          1
       ---------------------------
    
    2. A bio belongs to sda1 is issued and is merged into the request mentioned on
       step1 by ELEVATOR_BACK_MERGE. The first sector of the request is changed
       from sda2 region to sda1 region. However the two partition's
       hd_struct->in_flight are not changed.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |          0
       sda2 |          1
       ---------------------------
    
    3. The request is finished and blk_account_io_done() is called. In this case,
       sda2's hd_struct->in_flight, not a sda1's one, is decremented.
    
            | hd_struct->in_flight
       ---------------------------
       sda1 |         -1
       sda2 |          1
       ---------------------------
    
    The patch fixes the problem by caching the partition lookup
    inside the request structure, hence making sure that the increment
    and decrement will always happen on the same partition struct. This
    also speeds up IO with accounting enabled, since it cuts down on
    the number of lookups we have to do.
    
    When reloading partition tables, quiesce IO to ensure that no
    request references to the partition struct exists. When it is safe
    to free the partition table, the IO for that device is restarted
    again.
    
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8f3dd981b973..16f7f1be1acf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -115,6 +115,7 @@ struct request {
 	void *elevator_private3;
 
 	struct gendisk *rq_disk;
+	struct hd_struct *part;
 	unsigned long start_time;
 #ifdef CONFIG_BLK_CGROUP
 	unsigned long long start_time_ns;

commit 892b6f90db81cccb723d5d92f4fddc2d68b206e1
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 13 21:18:03 2010 +0200

    block: Ensure physical block size is unsigned int
    
    Physical block size was declared unsigned int to accomodate the maximum
    size reported by READ CAPACITY(16).  Make sure we use the right type in
    the related functions.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1341df5806df..8f3dd981b973 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -860,7 +860,7 @@ extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
-extern void blk_queue_physical_block_size(struct request_queue *, unsigned short);
+extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
 extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
 extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
@@ -1013,7 +1013,7 @@ static inline unsigned int queue_physical_block_size(struct request_queue *q)
 	return q->limits.physical_block_size;
 }
 
-static inline int bdev_physical_block_size(struct block_device *bdev)
+static inline unsigned int bdev_physical_block_size(struct block_device *bdev)
 {
 	return queue_physical_block_size(bdev_get_queue(bdev));
 }

commit dd3932eddf428571762596e17b65f5dc92ca361b
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 16 20:51:46 2010 +0200

    block: remove BLKDEV_IFL_WAIT
    
    All the blkdev_issue_* helpers can only sanely be used for synchronous
    caller.  To issue cache flushes or barriers asynchronously the caller needs
    to set up a bio by itself with a completion callback to move the asynchronous
    state machine ahead.  So drop the BLKDEV_IFL_WAIT flag that is always
    specified when calling blkdev_issue_* and also remove the now unused flags
    argument to blkdev_issue_flush and blkdev_issue_zeroout.  For
    blkdev_issue_discard we need to keep it for the secure discard flag, which
    gains a more descriptive name and loses the bitops vs flag confusion.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cfcb3a610605..accbd0e5c893 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -867,18 +867,14 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 		return NULL;
 	return bqt->tag_index[tag];
 }
-enum{
-	BLKDEV_WAIT,	/* wait for completion */
-	BLKDEV_SECURE,	/* secure discard */
-};
-#define BLKDEV_IFL_WAIT		(1 << BLKDEV_WAIT)
-#define BLKDEV_IFL_SECURE	(1 << BLKDEV_SECURE)
-extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *,
-			unsigned long);
+
+#define BLKDEV_DISCARD_SECURE  0x01    /* secure discard */
+
+extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
-			sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
+			sector_t nr_sects, gfp_t gfp_mask);
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {

commit e43473b7f223ec866f7db273697e76c337c390f9
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Wed Sep 15 17:06:35 2010 -0400

    blkio: Core implementation of throttle policy
    
    o Actual implementation of throttling policy in block layer. Currently it
      implements READ and WRITE bytes per second throttling logic. IOPS throttling
      comes in later patches.
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 780824edac16..1341df5806df 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -371,6 +371,11 @@ struct request_queue
 #if defined(CONFIG_BLK_DEV_BSG)
 	struct bsg_class_device bsg_dev;
 #endif
+
+#ifdef CONFIG_BLK_DEV_THROTTLING
+	/* Throttle data */
+	struct throtl_data *td;
+#endif
 };
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
@@ -1131,6 +1136,7 @@ static inline void put_dev_sector(Sector p)
 
 struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
+int kblockd_schedule_delayed_work(struct request_queue *q, struct delayed_work *dwork, unsigned long delay);
 
 #ifdef CONFIG_BLK_CGROUP
 /*
@@ -1174,6 +1180,24 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
 }
 #endif
 
+#ifdef CONFIG_BLK_DEV_THROTTLING
+extern int blk_throtl_init(struct request_queue *q);
+extern void blk_throtl_exit(struct request_queue *q);
+extern int blk_throtl_bio(struct request_queue *q, struct bio **bio);
+extern void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay);
+extern void throtl_shutdown_timer_wq(struct request_queue *q);
+#else /* CONFIG_BLK_DEV_THROTTLING */
+static inline int blk_throtl_bio(struct request_queue *q, struct bio **bio)
+{
+	return 0;
+}
+
+static inline int blk_throtl_init(struct request_queue *q) { return 0; }
+static inline int blk_throtl_exit(struct request_queue *q) { return 0; }
+static inline void throtl_schedule_delayed_work(struct request_queue *q, unsigned long delay) {}
+static inline void throtl_shutdown_timer_wq(struct request_queue *q) {}
+#endif /* CONFIG_BLK_DEV_THROTTLING */
+
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \

commit 144177991ca624841ddbd1e7edff958fc0f6d1fe
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Wed Sep 15 13:08:27 2010 +0200

    block: fix an address space warning in blk-map.c
    
    Change type of 2nd parameter of blk_rq_aligned() into unsigned long
    and remove unnecessary casting. Now we can call it with 'uaddr'
    instead of 'ubuf' in __blk_rq_map_user() so that it can remove
    following warnings from sparse:
    
     block/blk-map.c:57:31: warning: incorrect type in argument 2 (different address spaces)
     block/blk-map.c:57:31:    expected void *addr
     block/blk-map.c:57:31:    got void [noderef] <asn:1>*ubuf
    
    However blk_rq_map_kern() needs one more local variable to handle it.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7e661106270a..780824edac16 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1097,11 +1097,11 @@ static inline int queue_dma_alignment(struct request_queue *q)
 	return q ? q->dma_alignment : 511;
 }
 
-static inline int blk_rq_aligned(struct request_queue *q, void *addr,
+static inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,
 				 unsigned int len)
 {
 	unsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;
-	return !((unsigned long)addr & alignment) && !(len & alignment);
+	return !(addr & alignment) && !(len & alignment);
 }
 
 /* assumes size > 256 */

commit 13f05c8d8e98bbdce89158bfdb2e380940695a88
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 10 20:50:10 2010 +0200

    block/scsi: Provide a limit on the number of integrity segments
    
    Some controllers have a hardware limit on the number of protection
    information scatter-gather list segments they can handle.
    
    Introduce a max_integrity_segments limit in the block layer and provide
    a new scsi_host_template setting that allows HBA drivers to provide a
    value suitable for the hardware.
    
    Add support for honoring the integrity segment limit when merging both
    bios and requests.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@carl.home.kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c54906f678f..7e661106270a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -124,6 +124,9 @@ struct request {
 	 * physical address coalescing is performed.
 	 */
 	unsigned short nr_phys_segments;
+#if defined(CONFIG_BLK_DEV_INTEGRITY)
+	unsigned short nr_integrity_segments;
+#endif
 
 	unsigned short ioprio;
 
@@ -243,6 +246,7 @@ struct queue_limits {
 
 	unsigned short		logical_block_size;
 	unsigned short		max_segments;
+	unsigned short		max_integrity_segments;
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
@@ -1213,8 +1217,13 @@ struct blk_integrity {
 extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
 extern void blk_integrity_unregister(struct gendisk *);
 extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
-extern int blk_rq_map_integrity_sg(struct request *, struct scatterlist *);
-extern int blk_rq_count_integrity_sg(struct request *);
+extern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,
+				   struct scatterlist *);
+extern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);
+extern int blk_integrity_merge_rq(struct request_queue *, struct request *,
+				  struct request *);
+extern int blk_integrity_merge_bio(struct request_queue *, struct request *,
+				   struct bio *);
 
 static inline
 struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
@@ -1235,16 +1244,32 @@ static inline int blk_integrity_rq(struct request *rq)
 	return bio_integrity(rq->bio);
 }
 
+static inline void blk_queue_max_integrity_segments(struct request_queue *q,
+						    unsigned int segs)
+{
+	q->limits.max_integrity_segments = segs;
+}
+
+static inline unsigned short
+queue_max_integrity_segments(struct request_queue *q)
+{
+	return q->limits.max_integrity_segments;
+}
+
 #else /* CONFIG_BLK_DEV_INTEGRITY */
 
 #define blk_integrity_rq(rq)			(0)
-#define blk_rq_count_integrity_sg(a)		(0)
-#define blk_rq_map_integrity_sg(a, b)		(0)
+#define blk_rq_count_integrity_sg(a, b)		(0)
+#define blk_rq_map_integrity_sg(a, b, c)	(0)
 #define bdev_get_integrity(a)			(0)
 #define blk_get_integrity(a)			(0)
 #define blk_integrity_compare(a, b)		(0)
 #define blk_integrity_register(a, b)		(0)
 #define blk_integrity_unregister(a)		do { } while (0);
+#define blk_queue_max_integrity_segments(a, b)	do { } while (0);
+#define queue_max_integrity_segments(a)		(0)
+#define blk_integrity_merge_rq(a, b, c)		(0)
+#define blk_integrity_merge_bio(a, b, c)	(0)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 

commit 8c5553678237b7121355108e03c36086037d8975
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Aug 18 05:29:22 2010 -0400

    block: remove the BLKDEV_IFL_BARRIER flag
    
    Remove support for barriers on discards, which is unused now.  Also
    remove the DISCARD_NOBARRIER I/O type in favour of just setting the
    rw flags up locally in blkdev_issue_discard.
    
    tj: Also remove DISCARD_SECURE and use REQ_SECURE directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6b305eb4a343..cfcb3a610605 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -869,11 +869,9 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 }
 enum{
 	BLKDEV_WAIT,	/* wait for completion */
-	BLKDEV_BARRIER,	/* issue request with barrier */
 	BLKDEV_SECURE,	/* secure discard */
 };
 #define BLKDEV_IFL_WAIT		(1 << BLKDEV_WAIT)
-#define BLKDEV_IFL_BARRIER	(1 << BLKDEV_BARRIER)
 #define BLKDEV_IFL_SECURE	(1 << BLKDEV_SECURE)
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *,
 			unsigned long);

commit 2cf6d26a354ab6362e301b5a323832b02867df47
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Aug 18 05:29:10 2010 -0400

    block: pass gfp_mask and flags to sb_issue_discard
    
    We'll need to get rid of the BLKDEV_IFL_BARRIER flag, and to facilitate
    that and to make the interface less confusing pass all flags explicitly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8ef705f800ab..6b305eb4a343 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -881,13 +881,12 @@ extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 			sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
-static inline int sb_issue_discard(struct super_block *sb,
-				   sector_t block, sector_t nr_blocks)
+static inline int sb_issue_discard(struct super_block *sb, sector_t block,
+		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {
-	block <<= (sb->s_blocksize_bits - 9);
-	nr_blocks <<= (sb->s_blocksize_bits - 9);
-	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_NOFS,
-				   BLKDEV_IFL_WAIT | BLKDEV_IFL_BARRIER);
+	return blkdev_issue_discard(sb->s_bdev, block << (sb->s_blocksize_bits - 9),
+				    nr_blocks << (sb->s_blocksize_bits - 9),
+				    gfp_mask, flags);
 }
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);

commit 4fed947cb311e5aa51781d316cefca836352f6ce
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:17 2010 +0200

    block: implement REQ_FLUSH/FUA based interface for FLUSH/FUA requests
    
    Now that the backend conversion is complete, export sequenced
    FLUSH/FUA capability through REQ_FLUSH/FUA flags.  REQ_FLUSH means the
    device cache should be flushed before executing the request.  REQ_FUA
    means that the data in the request should be on non-volatile media on
    completion.
    
    Block layer will choose the correct way of implementing the semantics
    and execute it.  The request may be passed to the device directly if
    the device can handle it; otherwise, it will be sequenced using one or
    more proxy requests.  Devices will never see REQ_FLUSH and/or FUA
    which it doesn't support.
    
    Also, unlike the original REQ_HARDBARRIER, REQ_FLUSH/FUA requests are
    never failed with -EOPNOTSUPP.  If the underlying device doesn't
    support FLUSH/FUA, the block layer simply make those noop.  IOW, it no
    longer distinguishes between writeback cache which doesn't support
    cache flush and writethrough/no cache.  Devices which have WB cache
    w/o flush are very difficult to come by these days and there's nothing
    much we can do anyway, so it doesn't make sense to require everyone to
    implement -EOPNOTSUPP handling.  This will simplify filesystems and
    block drivers as they can drop -EOPNOTSUPP retry logic for barriers.
    
    * QUEUE_ORDERED_* are removed and QUEUE_FSEQ_* are moved into
      blk-flush.c.
    
    * REQ_FLUSH w/o data can also be directly passed to drivers without
      sequencing but some drivers assume that zero length requests don't
      have rq->bio which isn't true for these requests requiring the use
      of proxy requests.
    
    * REQ_COMMON_MASK now includes REQ_FLUSH | REQ_FUA so that they are
      copied from bio to request.
    
    * WRITE_BARRIER is marked deprecated and WRITE_FLUSH, WRITE_FUA and
      WRITE_FLUSH_FUA are added.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1cd83ec077db..8ef705f800ab 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -357,7 +357,6 @@ struct request_queue
 	/*
 	 * for flush operations
 	 */
-	unsigned int		ordered, next_ordered;
 	unsigned int		flush_flags;
 	unsigned int		flush_seq;
 	int			flush_err;
@@ -465,40 +464,6 @@ static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 	__clear_bit(flag, &q->queue_flags);
 }
 
-enum {
-	/*
-	 * Hardbarrier is supported with one of the following methods.
-	 *
-	 * NONE		: hardbarrier unsupported
-	 * DRAIN	: ordering by draining is enough
-	 * DRAIN_FLUSH	: ordering by draining w/ pre and post flushes
-	 * DRAIN_FUA	: ordering by draining w/ pre flush and FUA write
-	 */
-	QUEUE_ORDERED_DO_PREFLUSH	= 0x10,
-	QUEUE_ORDERED_DO_BAR		= 0x20,
-	QUEUE_ORDERED_DO_POSTFLUSH	= 0x40,
-	QUEUE_ORDERED_DO_FUA		= 0x80,
-
-	QUEUE_ORDERED_NONE		= 0x00,
-
-	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_DO_BAR,
-	QUEUE_ORDERED_DRAIN_FLUSH	= QUEUE_ORDERED_DRAIN |
-					  QUEUE_ORDERED_DO_PREFLUSH |
-					  QUEUE_ORDERED_DO_POSTFLUSH,
-	QUEUE_ORDERED_DRAIN_FUA		= QUEUE_ORDERED_DRAIN |
-					  QUEUE_ORDERED_DO_PREFLUSH |
-					  QUEUE_ORDERED_DO_FUA,
-
-	/*
-	 * FLUSH/FUA sequences.
-	 */
-	QUEUE_FSEQ_STARTED	= (1 << 0), /* flushing in progress */
-	QUEUE_FSEQ_PREFLUSH	= (1 << 1), /* pre-flushing in progress */
-	QUEUE_FSEQ_DATA		= (1 << 2), /* data write in progress */
-	QUEUE_FSEQ_POSTFLUSH	= (1 << 3), /* post-flushing in progress */
-	QUEUE_FSEQ_DONE		= (1 << 4),
-};
-
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
@@ -578,7 +543,8 @@ static inline void blk_clear_queue_full(struct request_queue *q, int sync)
  * it already be started by driver.
  */
 #define RQ_NOMERGE_FLAGS	\
-	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER)
+	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER | \
+	 REQ_FLUSH | REQ_FUA)
 #define rq_mergeable(rq)	\
 	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
 	 (((rq)->cmd_flags & REQ_DISCARD) || \

commit dd4c133f387c48f526022860ad70354637a80f4c
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    block: rename barrier/ordered to flush
    
    With ordering requirements dropped, barrier and ordered are misnomers.
    Now all block layer does is sequencing FLUSH and FUA.  Rename them to
    flush.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 20a3710a481b..1cd83ec077db 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -357,13 +357,13 @@ struct request_queue
 	/*
 	 * for flush operations
 	 */
+	unsigned int		ordered, next_ordered;
 	unsigned int		flush_flags;
-
-	unsigned int		ordered, next_ordered, ordseq;
-	int			orderr;
-	struct request		bar_rq;
-	struct request		*orig_bar_rq;
-	struct list_head	pending_barriers;
+	unsigned int		flush_seq;
+	int			flush_err;
+	struct request		flush_rq;
+	struct request		*orig_flush_rq;
+	struct list_head	pending_flushes;
 
 	struct mutex		sysfs_lock;
 
@@ -490,13 +490,13 @@ enum {
 					  QUEUE_ORDERED_DO_FUA,
 
 	/*
-	 * Ordered operation sequence
+	 * FLUSH/FUA sequences.
 	 */
-	QUEUE_ORDSEQ_STARTED	= (1 << 0), /* flushing in progress */
-	QUEUE_ORDSEQ_PREFLUSH	= (1 << 1), /* pre-flushing in progress */
-	QUEUE_ORDSEQ_BAR	= (1 << 2), /* barrier write in progress */
-	QUEUE_ORDSEQ_POSTFLUSH	= (1 << 3), /* post-flushing in progress */
-	QUEUE_ORDSEQ_DONE	= (1 << 4),
+	QUEUE_FSEQ_STARTED	= (1 << 0), /* flushing in progress */
+	QUEUE_FSEQ_PREFLUSH	= (1 << 1), /* pre-flushing in progress */
+	QUEUE_FSEQ_DATA		= (1 << 2), /* data write in progress */
+	QUEUE_FSEQ_POSTFLUSH	= (1 << 3), /* post-flushing in progress */
+	QUEUE_FSEQ_DONE		= (1 << 4),
 };
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)

commit 28e7d1845216538303bb95d679d8fd4de50e2f1a
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    block: drop barrier ordering by queue draining
    
    Filesystems will take all the responsibilities for ordering requests
    around commit writes and will only indicate how the commit writes
    themselves should be handled by block layers.  This patch drops
    barrier ordering by queue draining from block layer.  Ordering by
    draining implementation was somewhat invasive to request handling.
    List of notable changes follow.
    
    * Each queue has 1 bit color which is flipped on each barrier issue.
      This is used to track whether a given request is issued before the
      current barrier or not.  REQ_ORDERED_COLOR flag and coloring
      implementation in __elv_add_request() are removed.
    
    * Requests which shouldn't be processed yet for draining were stalled
      by returning -EAGAIN from blk_do_ordered() according to the test
      result between blk_ordered_req_seq() and blk_blk_ordered_cur_seq().
      This logic is removed.
    
    * Draining completion logic in elv_completed_request() removed.
    
    * All barrier sequence requests were queued to request queue and then
      trckled to lower layer according to progress and thus maintaining
      request orders during requeue was necessary.  This is replaced by
      queueing the next request in the barrier sequence only after the
      current one is complete from blk_ordered_complete_seq(), which
      removes the need for multiple proxy requests in struct request_queue
      and the request sorting logic in the ELEVATOR_INSERT_REQUEUE path of
      elv_insert().
    
    * As barriers no longer have ordering constraints, there's no need to
      dump the whole elevator onto the dispatch queue on each barrier.
      Insert barriers at the front instead.
    
    * If other barrier requests come to the front of the dispatch queue
      while one is already in progress, they are stored in
      q->pending_barriers and restored to dispatch queue one-by-one after
      each barrier completion from blk_ordered_complete_seq().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 996549d71923..20a3710a481b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -360,9 +360,10 @@ struct request_queue
 	unsigned int		flush_flags;
 
 	unsigned int		ordered, next_ordered, ordseq;
-	int			orderr, ordcolor;
-	struct request		pre_flush_rq, bar_rq, post_flush_rq;
+	int			orderr;
+	struct request		bar_rq;
 	struct request		*orig_bar_rq;
+	struct list_head	pending_barriers;
 
 	struct mutex		sysfs_lock;
 
@@ -491,12 +492,11 @@ enum {
 	/*
 	 * Ordered operation sequence
 	 */
-	QUEUE_ORDSEQ_STARTED	= 0x01,	/* flushing in progress */
-	QUEUE_ORDSEQ_DRAIN	= 0x02,	/* waiting for the queue to be drained */
-	QUEUE_ORDSEQ_PREFLUSH	= 0x04,	/* pre-flushing in progress */
-	QUEUE_ORDSEQ_BAR	= 0x08,	/* original barrier req in progress */
-	QUEUE_ORDSEQ_POSTFLUSH	= 0x10,	/* post-flushing in progress */
-	QUEUE_ORDSEQ_DONE	= 0x20,
+	QUEUE_ORDSEQ_STARTED	= (1 << 0), /* flushing in progress */
+	QUEUE_ORDSEQ_PREFLUSH	= (1 << 1), /* pre-flushing in progress */
+	QUEUE_ORDSEQ_BAR	= (1 << 2), /* barrier write in progress */
+	QUEUE_ORDSEQ_POSTFLUSH	= (1 << 3), /* post-flushing in progress */
+	QUEUE_ORDSEQ_DONE	= (1 << 4),
 };
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
@@ -869,9 +869,6 @@ extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern unsigned blk_ordered_cur_seq(struct request_queue *);
-extern unsigned blk_ordered_req_seq(struct request *);
-extern bool blk_ordered_complete_seq(struct request_queue *, unsigned, int);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);

commit dd831006d5be7f74c3fe7aef82380c51c3637960
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    block: misc cleanups in barrier code
    
    Make the following cleanups in preparation of barrier/flush update.
    
    * blk_do_ordered() declaration is moved from include/linux/blkdev.h to
      block/blk.h.
    
    * blk_do_ordered() now returns pointer to struct request, with %NULL
      meaning "try the next request" and ERR_PTR(-EAGAIN) "try again
      later".  The third case will be dropped with further changes.
    
    * In the initialization of proxy barrier request, data direction is
      already set by init_request_from_bio().  Drop unnecessary explicit
      REQ_WRITE setting and move init_request_from_bio() above REQ_FUA
      flag setting.
    
    * add_request() is collapsed into __make_request().
    
    These changes don't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e97911d4dec3..996549d71923 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -869,7 +869,6 @@ extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern bool blk_do_ordered(struct request_queue *, struct request **);
 extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);
 extern bool blk_ordered_complete_seq(struct request_queue *, unsigned, int);

commit 4913efe456c987057e5d36a3f0a55422a9072cae
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    block: deprecate barrier and replace blk_queue_ordered() with blk_queue_flush()
    
    Barrier is deemed too heavy and will soon be replaced by FLUSH/FUA
    requests.  Deprecate barrier.  All REQ_HARDBARRIERs are failed with
    -EOPNOTSUPP and blk_queue_ordered() is replaced with simpler
    blk_queue_flush().
    
    blk_queue_flush() takes combinations of REQ_FLUSH and FUA.  If a
    device has write cache and can flush it, it should set REQ_FLUSH.  If
    the device can handle FUA writes, it should also set REQ_FUA.
    
    All blk_queue_ordered() users are converted.
    
    * ORDERED_DRAIN is mapped to 0 which is the default value.
    * ORDERED_DRAIN_FLUSH is mapped to REQ_FLUSH.
    * ORDERED_DRAIN_FLUSH_FUA is mapped to REQ_FLUSH | REQ_FUA.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Alasdair G Kergon <agk@redhat.com>
    Cc: Pierre Ossman <drzeus@drzeus.cx>
    Cc: Stefan Weinhuber <wein@de.ibm.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7077bc0d6138..e97911d4dec3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -355,8 +355,10 @@ struct request_queue
 	struct blk_trace	*blk_trace;
 #endif
 	/*
-	 * reserved for flush operations
+	 * for flush operations
 	 */
+	unsigned int		flush_flags;
+
 	unsigned int		ordered, next_ordered, ordseq;
 	int			orderr, ordcolor;
 	struct request		pre_flush_rq, bar_rq, post_flush_rq;
@@ -865,8 +867,8 @@ extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
+extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern int blk_queue_ordered(struct request_queue *, unsigned);
 extern bool blk_do_ordered(struct request_queue *, struct request **);
 extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);

commit 6958f145459ca7ad9715024de97445addacb8510
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    block: kill QUEUE_ORDERED_BY_TAG
    
    Nobody is making meaningful use of ORDERED_BY_TAG now and queue
    draining for barrier requests will be removed soon which will render
    the advantage of tag ordering moot.  Kill ORDERED_BY_TAG.  The
    following users are affected.
    
    * brd: converted to ORDERED_DRAIN.
    * virtio_blk: ORDERED_TAG path was already marked deprecated.  Removed.
    * xen-blkfront: ORDERED_TAG case dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 015375c7d031..7077bc0d6138 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -470,12 +470,7 @@ enum {
 	 * DRAIN	: ordering by draining is enough
 	 * DRAIN_FLUSH	: ordering by draining w/ pre and post flushes
 	 * DRAIN_FUA	: ordering by draining w/ pre flush and FUA write
-	 * TAG		: ordering by tag is enough
-	 * TAG_FLUSH	: ordering by tag w/ pre and post flushes
-	 * TAG_FUA	: ordering by tag w/ pre flush and FUA write
 	 */
-	QUEUE_ORDERED_BY_DRAIN		= 0x01,
-	QUEUE_ORDERED_BY_TAG		= 0x02,
 	QUEUE_ORDERED_DO_PREFLUSH	= 0x10,
 	QUEUE_ORDERED_DO_BAR		= 0x20,
 	QUEUE_ORDERED_DO_POSTFLUSH	= 0x40,
@@ -483,8 +478,7 @@ enum {
 
 	QUEUE_ORDERED_NONE		= 0x00,
 
-	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_BY_DRAIN |
-					  QUEUE_ORDERED_DO_BAR,
+	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_DO_BAR,
 	QUEUE_ORDERED_DRAIN_FLUSH	= QUEUE_ORDERED_DRAIN |
 					  QUEUE_ORDERED_DO_PREFLUSH |
 					  QUEUE_ORDERED_DO_POSTFLUSH,
@@ -492,15 +486,6 @@ enum {
 					  QUEUE_ORDERED_DO_PREFLUSH |
 					  QUEUE_ORDERED_DO_FUA,
 
-	QUEUE_ORDERED_TAG		= QUEUE_ORDERED_BY_TAG |
-					  QUEUE_ORDERED_DO_BAR,
-	QUEUE_ORDERED_TAG_FLUSH		= QUEUE_ORDERED_TAG |
-					  QUEUE_ORDERED_DO_PREFLUSH |
-					  QUEUE_ORDERED_DO_POSTFLUSH,
-	QUEUE_ORDERED_TAG_FUA		= QUEUE_ORDERED_TAG |
-					  QUEUE_ORDERED_DO_PREFLUSH |
-					  QUEUE_ORDERED_DO_FUA,
-
 	/*
 	 * Ordered operation sequence
 	 */

commit 0da2f50944976e890ccc9436ab88c0da87788d02
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:16 2010 +0200

    ide: remove unnecessary blk_queue_flushing() test in do_ide_request()
    
    Unplugging from a request function doesn't really help much (it's
    already in the request_fn) and soon block layer will be updated to mix
    barrier sequence with other commands, so there's no need to treat
    queue flushing any differently.
    
    ide was the only user of blk_queue_flushing().  Remove it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c54906f678f..015375c7d031 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -521,7 +521,6 @@ enum {
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
 #define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
 #define blk_queue_add_random(q)	test_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)
-#define blk_queue_flushing(q)	((q)->ordseq)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)

commit 8d57a98ccd0b4489003473979da8f5a1363ba7a3
Author: Adrian Hunter <adrian.hunter@nokia.com>
Date:   Wed Aug 11 14:17:49 2010 -0700

    block: add secure discard
    
    Secure discard is the same as discard except that all copies of the
    discarded sectors (perhaps created by garbage collection) must also be
    erased.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@nokia.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Cc: Kyungmin Park <kmpark@infradead.org>
    Cc: Madhusudhan Chikkature <madhu.cr@ti.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ben Gardiner <bengardiner@nanometrics.ca>
    Cc: <linux-mmc@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 89c855c5655c..2c54906f678f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -389,6 +389,7 @@ struct request_queue
 #define QUEUE_FLAG_DISCARD     16	/* supports DISCARD */
 #define QUEUE_FLAG_NOXMERGES   17	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM  18	/* Contributes to random pool */
+#define QUEUE_FLAG_SECDISCARD  19	/* supports SECDISCARD */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
@@ -524,6 +525,8 @@ enum {
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
+#define blk_queue_secdiscard(q)	(blk_queue_discard(q) && \
+	test_bit(QUEUE_FLAG_SECDISCARD, &(q)->queue_flags))
 
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
@@ -918,10 +921,12 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 }
 enum{
 	BLKDEV_WAIT,	/* wait for completion */
-	BLKDEV_BARRIER,	/*issue request with barrier */
+	BLKDEV_BARRIER,	/* issue request with barrier */
+	BLKDEV_SECURE,	/* secure discard */
 };
 #define BLKDEV_IFL_WAIT		(1 << BLKDEV_WAIT)
 #define BLKDEV_IFL_BARRIER	(1 << BLKDEV_BARRIER)
+#define BLKDEV_IFL_SECURE	(1 << BLKDEV_SECURE)
 extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *,
 			unsigned long);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,

commit edca4a380584a65a16839bdee33ec82244f0f88e
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue Aug 3 12:54:51 2010 +0200

    block: disallow FS recursion from sb_issue_discard allocation
    
    Filesystems can call sb_issue_discard on a memory reclaim path
    (e.g. ext4 calls sb_issue_discard during journal commit).
    
    Use GFP_NOFS in sb_issue_discard to avoid recursing back into the FS.
    
    Reported-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a8b05fc80c6d..89c855c5655c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -933,7 +933,7 @@ static inline int sb_issue_discard(struct super_block *sb,
 {
 	block <<= (sb->s_blocksize_bits - 9);
 	nr_blocks <<= (sb->s_blocksize_bits - 9);
-	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL,
+	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_NOFS,
 				   BLKDEV_IFL_WAIT | BLKDEV_IFL_BARRIER);
 }
 

commit 8a6cfeb6deca3a8fefd639d898b0d163c0b5d368
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Jul 8 10:18:46 2010 +0200

    block: push down BKL into .locked_ioctl
    
    As a preparation for the removal of the big kernel
    lock in the block layer, this removes the BKL
    from the common ioctl handling code, moving it
    into every single driver still using it.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index baf5258f5985..a8b05fc80c6d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1246,7 +1246,6 @@ static inline int blk_integrity_rq(struct request *rq)
 struct block_device_operations {
 	int (*open) (struct block_device *, fmode_t);
 	int (*release) (struct gendisk *, fmode_t);
-	int (*locked_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*direct_access) (struct block_device *, sector_t,

commit a89f5c899db3c6be4bb426e4efb72ecee29a93b5
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Jul 6 09:03:18 2010 +0200

    block: remove unused REQ_TYPE_LINUX_BLOCK
    
    Nobody uses REQ_TYPE_LINUX_BLOCK (and its REQ_LB_OP_*).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jeff Garzik <jgarzik@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3a2c5d9a9288..baf5258f5985 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -60,7 +60,6 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
 	REQ_TYPE_SPECIAL,		/* driver defined type */
-	REQ_TYPE_LINUX_BLOCK,		/* generic block layer message */
 	/*
 	 * for ATA/ATAPI devices. this really doesn't belong here, ide should
 	 * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver
@@ -70,20 +69,6 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_ATA_PC,
 };
 
-/*
- * For request of type REQ_TYPE_LINUX_BLOCK, rq->cmd[0] is the opcode being
- * sent down (similar to how REQ_TYPE_BLOCK_PC means that ->cmd[] holds a
- * SCSI cdb.
- *
- * 0x00 -> 0x3f are driver private, to be used for whatever purpose they need,
- * typically to differentiate REQ_TYPE_SPECIAL requests.
- *
- */
-enum {
-	REQ_LB_OP_EJECT	= 0x40,		/* eject request */
-	REQ_LB_OP_FLUSH = 0x41,		/* flush request */
-};
-
 #define BLK_MAX_CDB	16
 
 /*

commit 00fff26539bfe3fad21c164fc4002d9ede056fb0
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Sat Jul 3 17:45:40 2010 +0900

    block: remove q->prepare_flush_fn completely
    
    This removes q->prepare_flush_fn completely (changes the
    blk_queue_ordered API).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6bba04c7ec48..3a2c5d9a9288 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -212,7 +212,6 @@ struct bvec_merge_data {
 };
 typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
 			     struct bio_vec *);
-typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 typedef int (lld_busy_fn) (struct request_queue *q);
@@ -286,7 +285,6 @@ struct request_queue
 	unprep_rq_fn		*unprep_rq_fn;
 	unplug_fn		*unplug_fn;
 	merge_bvec_fn		*merge_bvec_fn;
-	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
@@ -896,7 +894,7 @@ extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
+extern int blk_queue_ordered(struct request_queue *, unsigned);
 extern bool blk_do_ordered(struct request_queue *, struct request **);
 extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);

commit 28018c242a4ec7017bbbf81d2d3952f820a27118
Author: James Bottomley <James.Bottomley@suse.de>
Date:   Thu Jul 1 19:49:17 2010 +0900

    block: implement an unprep function corresponding directly to prep
    
    Reviewed-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 204fbe22354d..6bba04c7ec48 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -200,6 +200,7 @@ struct request_pm_state
 typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
+typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unplug_fn) (struct request_queue *);
 
 struct bio_vec;
@@ -282,6 +283,7 @@ struct request_queue
 	request_fn_proc		*request_fn;
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
+	unprep_rq_fn		*unprep_rq_fn;
 	unplug_fn		*unplug_fn;
 	merge_bvec_fn		*merge_bvec_fn;
 	prepare_flush_fn	*prepare_flush_fn;
@@ -841,6 +843,7 @@ extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
 extern void blk_abort_queue(struct request_queue *);
+extern void blk_unprep_request(struct request *);
 
 /*
  * Access functions for manipulating queue properties
@@ -885,6 +888,7 @@ extern int blk_queue_dma_drain(struct request_queue *q,
 extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
+extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
 extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);

commit 66ac0280197981f88774e74b60c8e5f9f07c1dba
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 18 16:59:42 2010 +0200

    block: don't allocate a payload for discard request
    
    Allocating a fixed payload for discard requests always was a horrible hack,
    and it's not coming to byte us when adding support for discard in DM/MD.
    
    So change the code to leave the allocation of a payload to the lowlevel
    driver.  Unfortunately that means we'll need another hack, which allows
    us to update the various block layer length fields indicating that we
    have a payload.  Instead of hiding this in sd.c, which we already partially
    do for UNMAP support add a documented helper in the core block layer for it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3fc0f5908619..204fbe22354d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -705,6 +705,8 @@ extern struct request *blk_make_request(struct request_queue *, struct bio *,
 					gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
+extern void blk_add_request_payload(struct request *rq, struct page *page,
+		unsigned int len);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
 extern int blk_lld_busy(struct request_queue *q);
 extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,

commit 7b6d91daee5cac6402186ff224c3af39d79f4a0e
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 7 18:20:39 2010 +0200

    block: unify flags for struct bio and struct request
    
    Remove the current bio flags and reuse the request flags for the bio, too.
    This allows to more easily trace the type of I/O from the filesystem
    down to the block driver.  There were two flags in the bio that were
    missing in the requests:  BIO_RW_UNPLUG and BIO_RW_AHEAD.  Also I've
    renamed two request flags that had a superflous RW in them.
    
    Note that the flags are in bio.h despite having the REQ_ name - as
    blkdev.h includes bio.h that is the only way to go for now.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3ecd28ef9ba4..3fc0f5908619 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -84,70 +84,6 @@ enum {
 	REQ_LB_OP_FLUSH = 0x41,		/* flush request */
 };
 
-/*
- * request type modified bits. first four bits match BIO_RW* bits, important
- */
-enum rq_flag_bits {
-	__REQ_RW,		/* not set, read. set, write */
-	__REQ_FAILFAST_DEV,	/* no driver retries of device errors */
-	__REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
-	__REQ_FAILFAST_DRIVER,	/* no driver retries of driver errors */
-	/* above flags must match BIO_RW_* */
-	__REQ_DISCARD,		/* request to discard sectors */
-	__REQ_SORTED,		/* elevator knows about this request */
-	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
-	__REQ_HARDBARRIER,	/* may not be passed by drive either */
-	__REQ_FUA,		/* forced unit access */
-	__REQ_NOMERGE,		/* don't touch this for merging */
-	__REQ_STARTED,		/* drive already may have started this one */
-	__REQ_DONTPREP,		/* don't call prep for this one */
-	__REQ_QUEUED,		/* uses queueing */
-	__REQ_ELVPRIV,		/* elevator private data attached */
-	__REQ_FAILED,		/* set if the request failed */
-	__REQ_QUIET,		/* don't worry about errors */
-	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
-	__REQ_ORDERED_COLOR,	/* is before or after barrier */
-	__REQ_RW_SYNC,		/* request is sync (sync write or read) */
-	__REQ_ALLOCED,		/* request came from our alloc pool */
-	__REQ_RW_META,		/* metadata io request */
-	__REQ_COPY_USER,	/* contains copies of user pages */
-	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
-	__REQ_NOIDLE,		/* Don't anticipate more IO after this one */
-	__REQ_IO_STAT,		/* account I/O stat */
-	__REQ_MIXED_MERGE,	/* merge of different types, fail separately */
-	__REQ_NR_BITS,		/* stops here */
-};
-
-#define REQ_RW		(1 << __REQ_RW)
-#define REQ_FAILFAST_DEV	(1 << __REQ_FAILFAST_DEV)
-#define REQ_FAILFAST_TRANSPORT	(1 << __REQ_FAILFAST_TRANSPORT)
-#define REQ_FAILFAST_DRIVER	(1 << __REQ_FAILFAST_DRIVER)
-#define REQ_DISCARD	(1 << __REQ_DISCARD)
-#define REQ_SORTED	(1 << __REQ_SORTED)
-#define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
-#define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
-#define REQ_FUA		(1 << __REQ_FUA)
-#define REQ_NOMERGE	(1 << __REQ_NOMERGE)
-#define REQ_STARTED	(1 << __REQ_STARTED)
-#define REQ_DONTPREP	(1 << __REQ_DONTPREP)
-#define REQ_QUEUED	(1 << __REQ_QUEUED)
-#define REQ_ELVPRIV	(1 << __REQ_ELVPRIV)
-#define REQ_FAILED	(1 << __REQ_FAILED)
-#define REQ_QUIET	(1 << __REQ_QUIET)
-#define REQ_PREEMPT	(1 << __REQ_PREEMPT)
-#define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
-#define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
-#define REQ_ALLOCED	(1 << __REQ_ALLOCED)
-#define REQ_RW_META	(1 << __REQ_RW_META)
-#define REQ_COPY_USER	(1 << __REQ_COPY_USER)
-#define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
-#define REQ_NOIDLE	(1 << __REQ_NOIDLE)
-#define REQ_IO_STAT	(1 << __REQ_IO_STAT)
-#define REQ_MIXED_MERGE	(1 << __REQ_MIXED_MERGE)
-
-#define REQ_FAILFAST_MASK	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | \
-				 REQ_FAILFAST_DRIVER)
-
 #define BLK_MAX_CDB	16
 
 /*
@@ -631,7 +567,7 @@ enum {
  */
 static inline bool rw_is_sync(unsigned int rw_flags)
 {
-	return !(rw_flags & REQ_RW) || (rw_flags & REQ_RW_SYNC);
+	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
 }
 
 static inline bool rq_is_sync(struct request *rq)

commit 33659ebbae262228eef4e0fe990f393d1f0ed941
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 7 18:17:56 2010 +0200

    block: remove wrappers for request type/flags
    
    Remove all the trivial wrappers for the cmd_type and cmd_flags fields in
    struct requests.  This allows much easier grepping for different request
    types instead of unwinding through macros.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d7ae241a9e55..3ecd28ef9ba4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -604,33 +604,20 @@ enum {
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
 
-#define blk_fs_request(rq)	((rq)->cmd_type == REQ_TYPE_FS)
-#define blk_pc_request(rq)	((rq)->cmd_type == REQ_TYPE_BLOCK_PC)
-#define blk_special_request(rq)	((rq)->cmd_type == REQ_TYPE_SPECIAL)
-#define blk_sense_request(rq)	((rq)->cmd_type == REQ_TYPE_SENSE)
-
-#define blk_failfast_dev(rq)	((rq)->cmd_flags & REQ_FAILFAST_DEV)
-#define blk_failfast_transport(rq) ((rq)->cmd_flags & REQ_FAILFAST_TRANSPORT)
-#define blk_failfast_driver(rq)	((rq)->cmd_flags & REQ_FAILFAST_DRIVER)
-#define blk_noretry_request(rq)	(blk_failfast_dev(rq) ||	\
-				 blk_failfast_transport(rq) ||	\
-				 blk_failfast_driver(rq))
-#define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
-#define blk_rq_io_stat(rq)	((rq)->cmd_flags & REQ_IO_STAT)
-#define blk_rq_quiet(rq)	((rq)->cmd_flags & REQ_QUIET)
-
-#define blk_account_rq(rq)	(blk_rq_started(rq) && (blk_fs_request(rq) || blk_discard_rq(rq))) 
-
-#define blk_pm_suspend_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND)
-#define blk_pm_resume_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_RESUME)
+#define blk_noretry_request(rq) \
+	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
+			     REQ_FAILFAST_DRIVER))
+
+#define blk_account_rq(rq) \
+	(((rq)->cmd_flags & REQ_STARTED) && \
+	 ((rq)->cmd_type == REQ_TYPE_FS || \
+	  ((rq)->cmd_flags & REQ_DISCARD)))
+
 #define blk_pm_request(rq)	\
-	(blk_pm_suspend_request(rq) || blk_pm_resume_request(rq))
+	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \
+	 (rq)->cmd_type == REQ_TYPE_PM_RESUME)
 
 #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
-#define blk_sorted_rq(rq)	((rq)->cmd_flags & REQ_SORTED)
-#define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
-#define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
-#define blk_discard_rq(rq)	((rq)->cmd_flags & REQ_DISCARD)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 /* rq->queuelist of dequeued request must be list_empty() */
 #define blk_queued_rq(rq)	(!list_empty(&(rq)->queuelist))
@@ -652,9 +639,6 @@ static inline bool rq_is_sync(struct request *rq)
 	return rw_is_sync(rq->cmd_flags);
 }
 
-#define rq_is_meta(rq)		((rq)->cmd_flags & REQ_RW_META)
-#define rq_noidle(rq)		((rq)->cmd_flags & REQ_NOIDLE)
-
 static inline int blk_queue_full(struct request_queue *q, int sync)
 {
 	if (sync)
@@ -687,7 +671,8 @@ static inline void blk_clear_queue_full(struct request_queue *q, int sync)
 	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER)
 #define rq_mergeable(rq)	\
 	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
-	 (blk_discard_rq(rq) || blk_fs_request((rq))))
+	 (((rq)->cmd_flags & REQ_DISCARD) || \
+	  (rq)->cmd_type == REQ_TYPE_FS))
 
 /*
  * q->prep_rq_fn return values

commit bfe172310e58225f0d07f9354b683abacbd6a0d8
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon May 31 15:59:03 2010 +0900

    block: kill ISA_DMA_THRESHOLD usage
    
    block uses ISA_DMA_THRESHOLD for BLK_BOUNCE_ISA. Only SCSI uses
    ISA_DMA_THRESHOLD for ancient drivers with non-zero
    unchecked_isa_dma. Nowadays drivers (and subsystems) use dma_mask
    properly instead of ISA_DMA_THRESHOLD.
    
    Documentation/scsi/scsi_mid_low_api.txt says:
    
    unchecked_isa_dma - 1=>only use bottom 16 MB of ram (ISA DMA addressing
                       restriction), 0=>can use full 32 bit (or better) DMA
                       address space
    
    So block simply uses DMA_BIT_MASK(24) for BLK_BOUNCE_ISA for SCSI.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: James Bottomley <James.Bottomley@suse.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b8224ea4a5de..d7ae241a9e55 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -712,7 +712,7 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
 #define BLK_BOUNCE_HIGH		-1ULL
 #endif
 #define BLK_BOUNCE_ANY		(-1ULL)
-#define BLK_BOUNCE_ISA		(ISA_DMA_THRESHOLD)
+#define BLK_BOUNCE_ISA		(DMA_BIT_MASK(24))
 
 /*
  * default timeout for SG_IO if none specified

commit e2e1a148bc45855816ae6b4692ce29d0020fa22e
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Wed Jun 9 10:42:09 2010 +0200

    block: add sysfs knob for turning off disk entropy contributions
    
    There are two reasons for doing this:
    
    - On SSD disks, the completion times aren't as random as they
      are for rotational drives. So it's questionable whether they
      should contribute to the random pool in the first place.
    
    - Calling add_disk_randomness() has a lot of overhead.
    
    This adds /sys/block/<dev>/queue/add_random that will allow you to
    switch off on a per-device basis. The default setting is on, so there
    should be no functional changes from this patch.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 09a840264d6f..b8224ea4a5de 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -467,11 +467,13 @@ struct request_queue
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
 #define QUEUE_FLAG_DISCARD     16	/* supports DISCARD */
 #define QUEUE_FLAG_NOXMERGES   17	/* No extended merges */
+#define QUEUE_FLAG_ADD_RANDOM  18	/* Contributes to random pool */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
 				 (1 << QUEUE_FLAG_STACKABLE)	|	\
-				 (1 << QUEUE_FLAG_SAME_COMP))
+				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
+				 (1 << QUEUE_FLAG_ADD_RANDOM))
 
 static inline int queue_is_locked(struct request_queue *q)
 {
@@ -596,6 +598,7 @@ enum {
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
 #define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
+#define blk_queue_add_random(q)	test_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)

commit 28f4197e5d4707311febeec8a0eb97cb5fd93c97
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Jun 1 12:23:18 2010 +0200

    block: disable preemption before using sched_clock()
    
    Commit 9195291e5f05e01d67f9a09c756b8aca8f009089 added calls to
    sched_clock() from preemptible code. sched_clock() is both the
    wrong interface AND cannot be called without preempt disabled.
    
    Apply a temporary fix to get rid of the warnings, a real patch
    is in the works.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8b7f5e0914ad..09a840264d6f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1211,14 +1211,23 @@ struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
 
 #ifdef CONFIG_BLK_CGROUP
+/*
+ * This should not be using sched_clock(). A real patch is in progress
+ * to fix this up, until that is in place we need to disable preemption
+ * around sched_clock() in this function and set_io_start_time_ns().
+ */
 static inline void set_start_time_ns(struct request *req)
 {
+	preempt_disable();
 	req->start_time_ns = sched_clock();
+	preempt_enable();
 }
 
 static inline void set_io_start_time_ns(struct request *req)
 {
+	preempt_disable();
 	req->io_start_time_ns = sched_clock();
+	preempt_enable();
 }
 
 static inline uint64_t rq_start_time_ns(struct request *req)

commit d79df0b1eda0099a22cbcece01ce5e7d222450de
Merge: 6e80e8ed5eb9 c8d1a126924f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 21 15:26:46 2010 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging-2.6: (577 commits)
      Staging: ramzswap: Handler for swap slot free callback
      swap: Add swap slot free callback to block_device_operations
      swap: Add flag to identify block swap devices
      Staging: vt6655: use ETH_FRAME_LEN macro instead of custom one
      Staging: vt6655: use ETH_DATA_LEN macro instead of custom one
      Staging: vt6655: use ETH_FCS_LEN macro instead of custom one
      Staging: vt6656: use ETH_HLEN macro instead of custom one
      Staging: comedi: quatech_daqp_cs.c Replace eos semaphore with a completion.
      Staging: dt3155v4l: remove private memory allocator
      Staging: crystalhd: Remove typedefs from driver
      Staging: winbond: Fix for pointer name format issue in mds.c
      Staging: vt6656: removed custom UCHAR/USHORT/UINT/ULONG/ULONGLONG typedefs
      Staging: vt6656: removed custom CHAR/SHORT/INT/LONG typedefs
      Staging: comedi: Altered the way printk is used in 8255.c
      staging: iio: adis16350 and similar IMU driver
      Staging: iio: max1363 Fix two bugs in single_channel_from_ring
      Staging: iio: adis16220 extract bin_attribute structures from state
      Staging: iio: adis16220 vibration sensor driver
      Staging: comedi: Kconfig dependancy fixes
      Staging: comedi: fix up build error from last Kconfig changes
      ...

commit c3e33e043f5e9c583aa59d5591a614b2a8243d3a
Author: Tejun Heo <tj@kernel.org>
Date:   Sat May 15 20:09:29 2010 +0200

    block,ide: simplify bdops->set_capacity() to ->unlock_native_capacity()
    
    bdops->set_capacity() is unnecessarily generic.  All that's required
    is a simple one way notification to lower level driver telling it to
    try to unlock native capacity.  There's no reason to pass in target
    capacity or return the new capacity.  The former is always the
    inherent native capacity and the latter can be handled via the usual
    device resize / revalidation path.  In fact, the current API is always
    used that way.
    
    Replace ->set_capacity() with ->unlock_native_capacity() which take
    only @disk and doesn't return anything.  IDE which is the only current
    user of the API is converted accordingly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 346fd4856733..be411c12ebbe 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1330,8 +1330,7 @@ struct block_device_operations {
 	int (*direct_access) (struct block_device *, sector_t,
 						void **, unsigned long *);
 	int (*media_changed) (struct gendisk *);
-	unsigned long long (*set_capacity) (struct gendisk *,
-						unsigned long long);
+	void (*unlock_native_capacity) (struct gendisk *);
 	int (*revalidate_disk) (struct gendisk *);
 	int (*getgeo)(struct block_device *, struct hd_geometry *);
 	struct module *owner;

commit b3a27d0529c6e5206f1b60f60263e3ecfd0d77cb
Author: Nitin Gupta <ngupta@vflare.org>
Date:   Mon May 17 11:02:43 2010 +0530

    swap: Add swap slot free callback to block_device_operations
    
    This callback is required when RAM based devices are used as swap disks.
    One such device is ramzswap which is used as compressed in-memory swap
    disk.  For such devices, we need a callback as soon as a swap slot is no
    longer used to allow freeing memory allocated for this slot.  Without this
    callback, stale data can quickly accumulate in memory defeating the whole
    purpose of such devices.
    
    Signed-off-by: Nitin Gupta <ngupta@vflare.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Nigel Cunningham <nigel@tuxonice.net>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6690e8bae7bb..413284a51ccc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1287,6 +1287,8 @@ struct block_device_operations {
 						unsigned long long);
 	int (*revalidate_disk) (struct gendisk *);
 	int (*getgeo)(struct block_device *, struct hd_geometry *);
+	/* this callback is with swap_lock and sometimes page table lock held */
+	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 	struct module *owner;
 };
 

commit 01effb0dc1451fad55925873ffbfb88fa4eadce0
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue May 11 08:57:42 2010 +0200

    block: allow initialization of previously allocated request_queue
    
    blk_init_queue() allocates the request_queue structure and then
    initializes it as needed (request_fn, elevator, etc).
    
    Split initialization out to blk_init_allocated_queue_node.
    Introduce blk_init_allocated_queue wrapper function to model existing
    blk_init_queue and blk_init_queue_node interfaces.
    
    Export elv_register_queue to allow a newly added elevator to be
    registered with sysfs.  Export elv_unregister_queue for symmetry.
    
    These changes allow DM to initialize a device's request_queue with more
    precision.  In particular, DM no longer unconditionally initializes a
    full request_queue (elevator et al).  It only does so for a
    request-based DM device.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3ac2bd2fc485..346fd4856733 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -921,7 +921,12 @@ extern void blk_abort_queue(struct request_queue *);
  */
 extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
 					spinlock_t *lock, int node_id);
+extern struct request_queue *blk_init_allocated_queue_node(struct request_queue *,
+							   request_fn_proc *,
+							   spinlock_t *, int node_id);
 extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
+extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
+						      request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);

commit 3f14d792f9a8fede64ce918dbb517f934497a4f8
Author: Dmitry Monakhov <dmonakhov@openvz.org>
Date:   Wed Apr 28 17:55:09 2010 +0400

    blkdev: add blkdev_issue_zeroout helper function
    
    - Add bio_batch helper primitive. This is rather generic primitive
      for submitting/waiting a complex request which consists of several
      bios.
    - blkdev_issue_zeroout() generate number of zero filed write bios.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 59b9aed0ee7d..3ac2bd2fc485 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1008,7 +1008,8 @@ extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *,
 			unsigned long);
 extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
-
+extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
+			sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 static inline int sb_issue_discard(struct super_block *sb,
 				   sector_t block, sector_t nr_blocks)
 {

commit fbd9b09a177a481eda256447c881f014f29034fe
Author: Dmitry Monakhov <dmonakhov@openvz.org>
Date:   Wed Apr 28 17:55:06 2010 +0400

    blkdev: generalize flags for blkdev_issue_fn functions
    
    The patch just convert all blkdev_issue_xxx function to common
    set of flags. Wait/allocation semantics preserved.
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5cf17a49ce38..59b9aed0ee7d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -998,12 +998,16 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 		return NULL;
 	return bqt->tag_index[tag];
 }
-
-extern int blkdev_issue_flush(struct block_device *, sector_t *);
-#define DISCARD_FL_WAIT		0x01	/* wait for completion */
-#define DISCARD_FL_BARRIER	0x02	/* issue DISCARD_BARRIER request */
-extern int blkdev_issue_discard(struct block_device *, sector_t sector,
-		sector_t nr_sects, gfp_t, int flags);
+enum{
+	BLKDEV_WAIT,	/* wait for completion */
+	BLKDEV_BARRIER,	/*issue request with barrier */
+};
+#define BLKDEV_IFL_WAIT		(1 << BLKDEV_WAIT)
+#define BLKDEV_IFL_BARRIER	(1 << BLKDEV_BARRIER)
+extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *,
+			unsigned long);
+extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
+		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
 
 static inline int sb_issue_discard(struct super_block *sb,
 				   sector_t block, sector_t nr_blocks)
@@ -1011,7 +1015,7 @@ static inline int sb_issue_discard(struct super_block *sb,
 	block <<= (sb->s_blocksize_bits - 9);
 	nr_blocks <<= (sb->s_blocksize_bits - 9);
 	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL,
-				    DISCARD_FL_BARRIER);
+				   BLKDEV_IFL_WAIT | BLKDEV_IFL_BARRIER);
 }
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);

commit 7f1dc8a2d2f45fc557b27fd56115338b1d34fc24
Author: Vivek Goyal <vgoyal@redhat.com>
Date:   Wed Apr 21 17:44:16 2010 +0200

    blkio: Fix blkio crash during rq stat update
    
    blkio + cfq was crashing even when two sequential readers were put in two
    separate cgroups (group_isolation=0).
    
    The reason being that cfqq can migrate across groups based on its being
    sync-noidle or not, it can happen that at request insertion time, cfqq
    belonged to one cfqg and at request dispatch time, it belonged to root
    group. In this case request stats per cgroup can go wrong and it also runs
    into BUG_ON().
    
    This patch implements rq stashing away a cfq group pointer and not relying
    on cfqq->cfqg pointer alone for rq stat accounting.
    
    [   65.163523] ------------[ cut here ]------------
    [   65.164301] kernel BUG at block/blk-cgroup.c:117!
    [   65.164301] invalid opcode: 0000 [#1] SMP
    [   65.164301] last sysfs file: /sys/devices/pci0000:00/0000:00:05.0/0000:60:00.1/host9/rport-9:0-0/target9:0:0/9:0:0:2/block/sde/stat
    [   65.164301] CPU 1
    [   65.164301] Modules linked in: dm_round_robin dm_multipath qla2xxx scsi_transport_fc dm_zero dm_mirror dm_region_hash dm_log dm_mod [last unloaded: scsi_wait_scan]
    [   65.164301]
    [   65.164301] Pid: 4505, comm: fio Not tainted 2.6.34-rc4-blk-for-35 #34 0A98h/HP xw8600 Workstation
    [   65.164301] RIP: 0010:[<ffffffff8121924f>]  [<ffffffff8121924f>] blkiocg_update_io_remove_stats+0x5b/0xaf
    [   65.164301] RSP: 0018:ffff8800ba5a79e8  EFLAGS: 00010046
    [   65.164301] RAX: 0000000000000096 RBX: ffff8800bb268d60 RCX: 0000000000000000
    [   65.164301] RDX: ffff8800bb268eb8 RSI: 0000000000000000 RDI: ffff8800bb268e00
    [   65.164301] RBP: ffff8800ba5a7a08 R08: 0000000000000064 R09: 0000000000000001
    [   65.164301] R10: 0000000000079640 R11: ffff8800a0bd5bf0 R12: ffff8800bab4af01
    [   65.164301] R13: ffff8800bab4af00 R14: ffff8800bb1d8928 R15: 0000000000000000
    [   65.164301] FS:  00007f18f75056f0(0000) GS:ffff880001e40000(0000) knlGS:0000000000000000
    [   65.164301] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   65.164301] CR2: 000000000040e7f0 CR3: 00000000ba52b000 CR4: 00000000000006e0
    [   65.164301] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [   65.164301] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    [   65.164301] Process fio (pid: 4505, threadinfo ffff8800ba5a6000, task ffff8800ba45ae80)
    [   65.164301] Stack:
    [   65.164301]  ffff8800ba5a7a08 ffff8800ba722540 ffff8800bab4af68 ffff8800bab4af68
    [   65.164301] <0> ffff8800ba5a7a38 ffffffff8121d814 ffff8800ba722540 ffff8800bab4af68
    [   65.164301] <0> ffff8800ba722540 ffff8800a08f6800 ffff8800ba5a7a68 ffffffff8121d8ca
    [   65.164301] Call Trace:
    [   65.164301]  [<ffffffff8121d814>] cfq_remove_request+0xe4/0x116
    [   65.164301]  [<ffffffff8121d8ca>] cfq_dispatch_insert+0x84/0xe1
    [   65.164301]  [<ffffffff8121e833>] cfq_dispatch_requests+0x767/0x8e8
    [   65.164301]  [<ffffffff8120e524>] ? submit_bio+0xc3/0xcc
    [   65.164301]  [<ffffffff810ad657>] ? sync_page_killable+0x0/0x35
    [   65.164301]  [<ffffffff8120ea8d>] blk_peek_request+0x191/0x1a7
    [   65.164301]  [<ffffffffa000109c>] ? dm_get_live_table+0x44/0x4f [dm_mod]
    [   65.164301]  [<ffffffffa0002799>] dm_request_fn+0x38/0x14c [dm_mod]
    [   65.164301]  [<ffffffff810ad657>] ? sync_page_killable+0x0/0x35
    [   65.164301]  [<ffffffff8120f600>] __generic_unplug_device+0x32/0x37
    [   65.164301]  [<ffffffff8120f8a0>] generic_unplug_device+0x2e/0x3c
    [   65.164301]  [<ffffffffa00011a6>] dm_unplug_all+0x42/0x5b [dm_mod]
    [   65.164301]  [<ffffffff8120b063>] blk_unplug+0x29/0x2d
    [   65.164301]  [<ffffffff8120b079>] blk_backing_dev_unplug+0x12/0x14
    [   65.164301]  [<ffffffff81108a82>] block_sync_page+0x35/0x39
    [   65.164301]  [<ffffffff810ad64e>] sync_page+0x41/0x4a
    [   65.164301]  [<ffffffff810ad665>] sync_page_killable+0xe/0x35
    [   65.164301]  [<ffffffff81589027>] __wait_on_bit_lock+0x46/0x8f
    [   65.164301]  [<ffffffff810ad52d>] __lock_page_killable+0x66/0x6d
    [   65.164301]  [<ffffffff81055fd4>] ? wake_bit_function+0x0/0x33
    [   65.164301]  [<ffffffff810ad560>] lock_page_killable+0x2c/0x2e
    [   65.164301]  [<ffffffff810aebfd>] generic_file_aio_read+0x361/0x4f0
    [   65.164301]  [<ffffffff810e906c>] do_sync_read+0xcb/0x108
    [   65.164301]  [<ffffffff811e32a3>] ? security_file_permission+0x16/0x18
    [   65.164301]  [<ffffffff810e96d3>] vfs_read+0xab/0x108
    [   65.164301]  [<ffffffff810e97f0>] sys_read+0x4a/0x6e
    [   65.164301]  [<ffffffff81002b5b>] system_call_fastpath+0x16/0x1b
    [   65.164301] Code: 00 74 1c 48 8b 8b 60 01 00 00 48 85 c9 75 04 0f 0b eb fe 48 ff c9 48 89 8b 60 01 00 00 eb 1a 48 8b 8b 58 01 00 00 48 85 c9 75 04 <0f> 0b eb fe 48 ff c9 48 89 8b 58 01 00 00 45 84 e4 74 16 48 8b
    [   65.164301] RIP  [<ffffffff8121924f>] blkiocg_update_io_remove_stats+0x5b/0xaf
    [   65.164301]  RSP <ffff8800ba5a79e8>
    [   65.164301] ---[ end trace 1b2b828753032e68 ]---
    
    Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d483c494672a..5cf17a49ce38 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -186,11 +186,12 @@ struct request {
 	};
 
 	/*
-	 * two pointers are available for the IO schedulers, if they need
+	 * Three pointers are available for the IO schedulers, if they need
 	 * more they have to dynamically allocate it.
 	 */
 	void *elevator_private;
 	void *elevator_private2;
+	void *elevator_private3;
 
 	struct gendisk *rq_disk;
 	unsigned long start_time;

commit 84c124da9ff50bd71fab9c939ee5b7cd8bef2bd9
Author: Divyesh Shah <dpshah@google.com>
Date:   Fri Apr 9 08:31:19 2010 +0200

    blkio: Changes to IO controller additional stats patches
    
    that include some minor fixes and addresses all comments.
    
    Changelog: (most based on Vivek Goyal's comments)
    o renamed blkiocg_reset_write to blkiocg_reset_stats
    o more clarification in the documentation on io_service_time and io_wait_time
    o Initialize blkg->stats_lock
    o rename io_add_stat to blkio_add_stat and declare it static
    o use bool for direction and sync
    o derive direction and sync info from existing rq methods
    o use 12 for major:minor string length
    o define io_service_time better to cover the NCQ case
    o add a separate reset_stats interface
    o make the indexed stats a 2d array to simplify macro and function pointer code
    o blkio.time now exports in jiffies as before
    o Added stats description in patch description and
      Documentation/cgroup/blkio-controller.txt
    o Prefix all stats functions with blkio and make them static as applicable
    o replace IO_TYPE_MAX with IO_TYPE_TOTAL
    o Moved #define constant to top of blk-cgroup.c
    o Pass dev_t around instead of char *
    o Add note to documentation file about resetting stats
    o use BLK_CGROUP_MODULE in addition to BLK_CGROUP config option in #ifdef
      statements
    o Avoid struct request specific knowledge in blk-cgroup. blk-cgroup.h now has
      rq_direction() and rq_sync() functions which are used by CFQ and when using
      io-controller at a higher level, bio_* functions can be added.
    
    Signed-off-by: Divyesh Shah<dpshah@google.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f3fff8bf85ee..d483c494672a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1209,9 +1209,27 @@ static inline void set_io_start_time_ns(struct request *req)
 {
 	req->io_start_time_ns = sched_clock();
 }
+
+static inline uint64_t rq_start_time_ns(struct request *req)
+{
+        return req->start_time_ns;
+}
+
+static inline uint64_t rq_io_start_time_ns(struct request *req)
+{
+        return req->io_start_time_ns;
+}
 #else
 static inline void set_start_time_ns(struct request *req) {}
 static inline void set_io_start_time_ns(struct request *req) {}
+static inline uint64_t rq_start_time_ns(struct request *req)
+{
+	return 0;
+}
+static inline uint64_t rq_io_start_time_ns(struct request *req)
+{
+	return 0;
+}
 #endif
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \

commit 9195291e5f05e01d67f9a09c756b8aca8f009089
Author: Divyesh Shah <dpshah@google.com>
Date:   Thu Apr 1 15:01:41 2010 -0700

    blkio: Increment the blkio cgroup stats for real now
    
    We also add start_time_ns and io_start_time_ns fields to struct request
    here to record the time when a request is created and when it is
    dispatched to device. We use ns uints here as ms and jiffies are
    not very useful for non-rotational media.
    
    Signed-off-by: Divyesh Shah<dpshah@google.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6690e8bae7bb..f3fff8bf85ee 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -194,7 +194,10 @@ struct request {
 
 	struct gendisk *rq_disk;
 	unsigned long start_time;
-
+#ifdef CONFIG_BLK_CGROUP
+	unsigned long long start_time_ns;
+	unsigned long long io_start_time_ns;    /* when passed to hardware */
+#endif
 	/* Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
 	 */
@@ -1196,6 +1199,21 @@ static inline void put_dev_sector(Sector p)
 struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
 
+#ifdef CONFIG_BLK_CGROUP
+static inline void set_start_time_ns(struct request *req)
+{
+	req->start_time_ns = sched_clock();
+}
+
+static inline void set_io_start_time_ns(struct request *req)
+{
+	req->io_start_time_ns = sched_clock();
+}
+#else
+static inline void set_start_time_ns(struct request *req) {}
+static inline void set_io_start_time_ns(struct request *req) {}
+#endif
+
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \

commit 181fdde3b4268cb7b4af76ba6337e7ec8accbb36
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Fri Mar 19 08:58:16 2010 +0100

    block: remove 16 bytes of padding from struct request on 64bits
    
    Remove alignment padding to shrink struct request from 336 to 320 bytes
    so needing one fewer cacheline and therefore removing 48 bytes from
    struct request_queue.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 41551c9341b6..6690e8bae7bb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -158,7 +158,6 @@ enum rq_flag_bits {
 struct request {
 	struct list_head queuelist;
 	struct call_single_data csd;
-	int cpu;
 
 	struct request_queue *q;
 
@@ -166,9 +165,11 @@ struct request {
 	enum rq_cmd_type_bits cmd_type;
 	unsigned long atomic_flags;
 
+	int cpu;
+
 	/* the following two fields are internal, NEVER access directly */
-	sector_t __sector;		/* sector cursor */
 	unsigned int __data_len;	/* total data len */
+	sector_t __sector;		/* sector cursor */
 
 	struct bio *bio;
 	struct bio *biotail;
@@ -201,20 +202,20 @@ struct request {
 
 	unsigned short ioprio;
 
+	int ref_count;
+
 	void *special;		/* opaque pointer available for LLD use */
 	char *buffer;		/* kaddr of the current segment if available */
 
 	int tag;
 	int errors;
 
-	int ref_count;
-
 	/*
 	 * when request is used as a packet command carrier
 	 */
-	unsigned short cmd_len;
 	unsigned char __cmd[BLK_MAX_CDB];
 	unsigned char *cmd;
+	unsigned short cmd_len;
 
 	unsigned int extra_len;	/* length of alignment and padding */
 	unsigned int sense_len;

commit ee714f2dd33e726346e34f5cda12543162f4753e
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Mar 10 00:48:32 2010 -0500

    block: Finalize conversion of block limits functions
    
    Remove compatibility wrappers and update remaining drivers.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ebd22dbed861..41551c9341b6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -921,26 +921,7 @@ extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
-
-/* Temporary compatibility wrapper */
-static inline void blk_queue_max_sectors(struct request_queue *q, unsigned int max)
-{
-	blk_queue_max_hw_sectors(q, max);
-}
-
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
-
-static inline void blk_queue_max_phys_segments(struct request_queue *q, unsigned short max)
-{
-	blk_queue_max_segments(q, max);
-}
-
-static inline void blk_queue_max_hw_segments(struct request_queue *q, unsigned short max)
-{
-	blk_queue_max_segments(q, max);
-}
-
-
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
@@ -1030,11 +1011,6 @@ static inline int sb_issue_discard(struct super_block *sb,
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 
-#define MAX_PHYS_SEGMENTS 128
-#define MAX_HW_SEGMENTS 128
-#define SAFE_MAX_SECTORS 255
-#define MAX_SEGMENT_SIZE	65536
-
 enum blk_default_limits {
 	BLK_MAX_SEGMENTS	= 128,
 	BLK_SAFE_MAX_SECTORS	= 255,

commit 8a78362c4eefc1deddbefe2c7f38aabbc2429d6b
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Feb 26 00:20:39 2010 -0500

    block: Consolidate phys_segment and hw_segment limits
    
    Except for SCSI no device drivers distinguish between physical and
    hardware segment limits.  Consolidate the two into a single segment
    limit.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 57bc48446e92..ebd22dbed861 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -316,8 +316,7 @@ struct queue_limits {
 	unsigned int		discard_alignment;
 
 	unsigned short		logical_block_size;
-	unsigned short		max_hw_segments;
-	unsigned short		max_phys_segments;
+	unsigned short		max_segments;
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
@@ -929,8 +928,19 @@ static inline void blk_queue_max_sectors(struct request_queue *q, unsigned int m
 	blk_queue_max_hw_sectors(q, max);
 }
 
-extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
-extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
+extern void blk_queue_max_segments(struct request_queue *, unsigned short);
+
+static inline void blk_queue_max_phys_segments(struct request_queue *q, unsigned short max)
+{
+	blk_queue_max_segments(q, max);
+}
+
+static inline void blk_queue_max_hw_segments(struct request_queue *q, unsigned short max)
+{
+	blk_queue_max_segments(q, max);
+}
+
+
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
@@ -1055,14 +1065,9 @@ static inline unsigned int queue_max_hw_sectors(struct request_queue *q)
 	return q->limits.max_hw_sectors;
 }
 
-static inline unsigned short queue_max_hw_segments(struct request_queue *q)
-{
-	return q->limits.max_hw_segments;
-}
-
-static inline unsigned short queue_max_phys_segments(struct request_queue *q)
+static inline unsigned short queue_max_segments(struct request_queue *q)
 {
-	return q->limits.max_phys_segments;
+	return q->limits.max_segments;
 }
 
 static inline unsigned int queue_max_segment_size(struct request_queue *q)

commit 086fa5ff0854c676ec333760f4c0154b3b242616
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Feb 26 00:20:38 2010 -0500

    block: Rename blk_queue_max_sectors to blk_queue_max_hw_sectors
    
    The block layer calling convention is blk_queue_<limit name>.
    blk_queue_max_sectors predates this practice, leading to some confusion.
    Rename the function to appropriately reflect that its intended use is to
    set max_hw_sectors.
    
    Also introduce a temporary wrapper for backwards compability.  This can
    be removed after the merge window is closed.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5d378627f446..57bc48446e92 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -921,7 +921,14 @@ extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
-extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
+extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
+
+/* Temporary compatibility wrapper */
+static inline void blk_queue_max_sectors(struct request_queue *q, unsigned int max)
+{
+	blk_queue_max_hw_sectors(q, max);
+}
+
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);

commit eb28d31bc97e6374d81f404da309401ffaed467b
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Feb 26 00:20:37 2010 -0500

    block: Add BLK_ prefix to definitions
    
    Add a BLK_ prefix to block layer constants.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c051cea2df07..5d378627f446 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1016,11 +1016,15 @@ extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128
 #define SAFE_MAX_SECTORS 255
-#define BLK_DEF_MAX_SECTORS 1024
-
 #define MAX_SEGMENT_SIZE	65536
 
-#define BLK_SEG_BOUNDARY_MASK	0xFFFFFFFFUL
+enum blk_default_limits {
+	BLK_MAX_SEGMENTS	= 128,
+	BLK_SAFE_MAX_SECTORS	= 255,
+	BLK_DEF_MAX_SECTORS	= 1024,
+	BLK_MAX_SEGMENT_SIZE	= 65536,
+	BLK_SEG_BOUNDARY_MASK	= 0xFFFFFFFFUL,
+};
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 

commit e751e76a5f7adeee7438e68b0965559ad2864d0d
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Feb 26 00:20:36 2010 -0500

    block: Remove unused accessor function
    
    blk_queue_max_hw_sectors is no longer called by any subsystem and can be
    removed.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2f17793048e7..c051cea2df07 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -922,7 +922,6 @@ extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
-extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);

commit 7f03292ee16e68be96aa88904afc6152f8d37736
Merge: c5ecc484c528 baac35c4155a
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Feb 25 08:48:05 2010 +0100

    Merge branch 'master' into for-2.6.34
    
    Conflicts:
            include/linux/blkdev.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

commit 79da0644a8e0838522828f106e4049639eea6baf
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Feb 23 08:40:43 2010 +0100

    Revert "block: improve queue_should_plug() by looking at IO depths"
    
    This reverts commit fb1e75389bd06fd5987e9cda1b4e0305c782f854.
    
    "Benjamin S." <sbenni@gmx.de> reports that the patch in question
    causes a big drop in sequential throughput for him, dropping from
    200MB/sec down to only 70MB/sec.
    
    Needs to be investigated more fully, for now lets just revert the
    offending commit.
    
    Conflicts:
    
            include/linux/blkdev.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c8018977efa..1896e868854f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -461,8 +461,7 @@ struct request_queue
 #define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
-#define QUEUE_FLAG_CQ	       16	/* hardware does queuing */
-#define QUEUE_FLAG_DISCARD     17	/* supports DISCARD */
+#define QUEUE_FLAG_DISCARD     16	/* supports DISCARD */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
@@ -586,7 +585,6 @@ enum {
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
-#define blk_queue_queuing(q)	test_bit(QUEUE_FLAG_CQ, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)

commit 488991e28e55b4fbca8067edf0259f69d1a6f92c
Author: Alan D. Brunelle <Alan.Brunelle@hp.com>
Date:   Fri Jan 29 09:04:08 2010 +0100

    block: Added in stricter no merge semantics for block I/O
    
    Updated 'nomerges' tunable to accept a value of '2' - indicating that _no_
    merges at all are to be attempted (not even the simple one-hit cache).
    
    The following table illustrates the additional benefit - 5 minute runs of
    a random I/O load were applied to a dozen devices on a 16-way x86_64 system.
    
    nomerges        Throughput      %System         Improvement (tput / %sys)
    --------        ------------    -----------     -------------------------
    0               12.45 MB/sec    0.669365609
    1               12.50 MB/sec    0.641519199     0.40% / 2.71%
    2               12.52 MB/sec    0.639849750     0.56% / 2.96%
    
    Signed-off-by: Alan D. Brunelle <alan.brunelle@hp.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ffb13ad35716..f71f5c58620c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -463,6 +463,7 @@ struct request_queue
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
 #define QUEUE_FLAG_CQ	       16	/* hardware does queuing */
 #define QUEUE_FLAG_DISCARD     17	/* supports DISCARD */
+#define QUEUE_FLAG_NOXMERGES   18	/* No extended merges */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
@@ -589,6 +590,8 @@ enum {
 #define blk_queue_queuing(q)	test_bit(QUEUE_FLAG_CQ, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+#define blk_queue_noxmerges(q)	\
+	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
 #define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)

commit e03a72e13648ac6277bf2bab6b8324d51f89c0fa
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Mon Jan 11 03:21:51 2010 -0500

    block: Stop using byte offsets
    
    All callers of the stacking functions use 512-byte sector units rather
    than byte offsets.  Simplify the code so the stacking functions take
    sectors when specifying data offsets.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c8018977efa..ffb13ad35716 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1112,18 +1112,13 @@ static inline int queue_alignment_offset(struct request_queue *q)
 	return q->limits.alignment_offset;
 }
 
-static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t offset)
+static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)
 {
 	unsigned int granularity = max(lim->physical_block_size, lim->io_min);
+	unsigned int alignment = (sector << 9) & (granularity - 1);
 
-	offset &= granularity - 1;
-	return (granularity + lim->alignment_offset - offset) & (granularity - 1);
-}
-
-static inline int queue_sector_alignment_offset(struct request_queue *q,
-						sector_t sector)
-{
-	return queue_limit_alignment_offset(&q->limits, sector << 9);
+	return (granularity + lim->alignment_offset - alignment)
+		& (granularity - 1);
 }
 
 static inline int bdev_alignment_offset(struct block_device *bdev)
@@ -1147,10 +1142,8 @@ static inline int queue_discard_alignment(struct request_queue *q)
 	return q->limits.discard_alignment;
 }
 
-static inline int queue_sector_discard_alignment(struct request_queue *q,
-						 sector_t sector)
+static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)
 {
-	struct queue_limits *lim = &q->limits;
 	unsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);
 
 	return (lim->discard_granularity + lim->discard_alignment - alignment)

commit 17be8c245054b9c7786545af3ba3ca4e54cd4ad9
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Mon Jan 11 03:21:49 2010 -0500

    block: bdev_stack_limits wrapper
    
    DM does not want to know about partition offsets.  Add a partition-aware
    wrapper that DM can use when stacking block devices.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Alasdair G Kergon <agk@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a41bcc8e140f..5c8018977efa 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -938,6 +938,8 @@ extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
 extern void blk_set_default_limits(struct queue_limits *lim);
 extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 			    sector_t offset);
+extern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,
+			    sector_t offset);
 extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
 			      sector_t offset);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);

commit dd3d145d49c5816b79acc6761ebbd842bc50b0ee
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Mon Jan 11 03:21:48 2010 -0500

    block: Fix discard alignment calculation and printing
    
    Discard alignment reporting for partitions was incorrect.  Update to
    match the algorithm used elsewhere.
    
    The alignment can be negative (misaligned).  Fix format string
    accordingly.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9b98173a8184..a41bcc8e140f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1148,8 +1148,11 @@ static inline int queue_discard_alignment(struct request_queue *q)
 static inline int queue_sector_discard_alignment(struct request_queue *q,
 						 sector_t sector)
 {
-	return ((sector << 9) - q->limits.discard_alignment)
-		& (q->limits.discard_granularity - 1);
+	struct queue_limits *lim = &q->limits;
+	unsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);
+
+	return (lim->discard_granularity + lim->discard_alignment - alignment)
+		& (lim->discard_granularity - 1);
 }
 
 static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)

commit 9bd3f98821a83041e77ee25158b80b535d02d7b4
Author: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
Date:   Wed Dec 30 08:41:07 2009 +0100

    block: blk_rq_err_sectors cleanup
    
    blk_rq_err_sectors() seems useless, get rid of it.
    
    Signed-off-by: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 59b832be3044..9b98173a8184 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -845,7 +845,6 @@ static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
  * blk_rq_err_bytes()		: bytes left till the next error boundary
  * blk_rq_sectors()		: sectors left in the entire request
  * blk_rq_cur_sectors()		: sectors left in the current segment
- * blk_rq_err_sectors()		: sectors left till the next error boundary
  */
 static inline sector_t blk_rq_pos(const struct request *rq)
 {
@@ -874,11 +873,6 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
-static inline unsigned int blk_rq_err_sectors(const struct request *rq)
-{
-	return blk_rq_err_bytes(rq) >> 9;
-}
-
 /*
  * Request issue related functions.
  */

commit 81744ee44ab2845c16ffd7d6f762f7b4a49a4750
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Dec 29 08:35:35 2009 +0100

    block: Fix incorrect alignment offset reporting and update documentation
    
    queue_sector_alignment_offset returned the wrong value which caused
    partitions to report an incorrect alignment_offset.  Since offset
    alignment calculation is needed several places it has been split into a
    separate helper function.  The topology stacking function has been
    updated accordingly.
    
    Furthermore, comments have been added to clarify how the stacking
    function works.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Tested-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 784a919aa0d0..59b832be3044 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1116,11 +1116,18 @@ static inline int queue_alignment_offset(struct request_queue *q)
 	return q->limits.alignment_offset;
 }
 
+static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t offset)
+{
+	unsigned int granularity = max(lim->physical_block_size, lim->io_min);
+
+	offset &= granularity - 1;
+	return (granularity + lim->alignment_offset - offset) & (granularity - 1);
+}
+
 static inline int queue_sector_alignment_offset(struct request_queue *q,
 						sector_t sector)
 {
-	return ((sector << 9) - q->limits.alignment_offset)
-		& (q->limits.io_min - 1);
+	return queue_limit_alignment_offset(&q->limits, sector << 9);
 }
 
 static inline int bdev_alignment_offset(struct block_device *bdev)

commit 98262f2762f0067375f83824d81ea929e37e6bfe
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Thu Dec 3 09:24:48 2009 +0100

    block: Allow devices to indicate whether discarded blocks are zeroed
    
    The discard ioctl is used by mkfs utilities to clear a block device
    prior to putting metadata down.  However, not all devices return zeroed
    blocks after a discard.  Some drives return stale data, potentially
    containing old superblocks.  It is therefore important to know whether
    discarded blocks are properly zeroed.
    
    Both ATA and SCSI drives have configuration bits that indicate whether
    zeroes are returned after a discard operation.  Implement a block level
    interface that allows this information to be bubbled up the stack and
    queried via a new block device ioctl.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e727f6c44c44..784a919aa0d0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -322,6 +322,7 @@ struct queue_limits {
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
 	unsigned char		no_cluster;
+	signed char		discard_zeroes_data;
 };
 
 struct request_queue
@@ -1150,6 +1151,19 @@ static inline int queue_sector_discard_alignment(struct request_queue *q,
 		& (q->limits.discard_granularity - 1);
 }
 
+static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)
+{
+	if (q->limits.discard_zeroes_data == 1)
+		return 1;
+
+	return 0;
+}
+
+static inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)
+{
+	return queue_discard_zeroes_data(bdev_get_queue(bdev));
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 2d4dc890b5c8fabd818a8586607e6843c4375e62
Author: Ilya Loginov <isloginov@gmail.com>
Date:   Thu Nov 26 09:16:19 2009 +0100

    block: add helpers to run flush_dcache_page() against a bio and a request's pages
    
    Mtdblock driver doesn't call flush_dcache_page for pages in request.  So,
    this causes problems on architectures where the icache doesn't fill from
    the dcache or with dcache aliases.  The patch fixes this.
    
    The ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE symbol was introduced to avoid
    pointless empty cache-thrashing loops on architectures for which
    flush_dcache_page() is a no-op.  Every architecture was provided with this
    flush pages on architectires where ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE is
    equal 1 or do nothing otherwise.
    
    See "fix mtd_blkdevs problem with caches on some architectures" discussion
    on LKML for more information.
    
    Signed-off-by: Ilya Loginov <isloginov@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Peter Horton <phorton@bitbox.co.uk>
    Cc: "Ed L. Cashin" <ecashin@coraid.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1cc02972fbe2..e727f6c44c44 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -752,6 +752,17 @@ struct req_iterator {
 #define rq_iter_last(rq, _iter)					\
 		(_iter.bio->bi_next == NULL && _iter.i == _iter.bio->bi_vcnt-1)
 
+#ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
+# error	"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform"
+#endif
+#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
+extern void rq_flush_dcache_pages(struct request *rq);
+#else
+static inline void rq_flush_dcache_pages(struct request *rq)
+{
+}
+#endif
+
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);

commit 86b37281411cf1e9bc0a6b5406c45edb7bd9ea5d
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Nov 10 11:50:21 2009 +0100

    block: Expose discard granularity
    
    While SSDs track block usage on a per-sector basis, RAID arrays often
    have allocation blocks that are bigger.  Allow the discard granularity
    and alignment to be set and teach the topology stacking logic how to
    handle them.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 39c601f783a0..1cc02972fbe2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -312,12 +312,15 @@ struct queue_limits {
 	unsigned int		io_min;
 	unsigned int		io_opt;
 	unsigned int		max_discard_sectors;
+	unsigned int		discard_granularity;
+	unsigned int		discard_alignment;
 
 	unsigned short		logical_block_size;
 	unsigned short		max_hw_segments;
 	unsigned short		max_phys_segments;
 
 	unsigned char		misaligned;
+	unsigned char		discard_misaligned;
 	unsigned char		no_cluster;
 };
 
@@ -1121,6 +1124,21 @@ static inline int bdev_alignment_offset(struct block_device *bdev)
 	return q->limits.alignment_offset;
 }
 
+static inline int queue_discard_alignment(struct request_queue *q)
+{
+	if (q->limits.discard_misaligned)
+		return -1;
+
+	return q->limits.discard_alignment;
+}
+
+static inline int queue_sector_discard_alignment(struct request_queue *q,
+						 sector_t sector)
+{
+	return ((sector << 9) - q->limits.discard_alignment)
+		& (q->limits.discard_granularity - 1);
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit b9d128f1088ea5245109dfc9bbceb128b6371a77
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Oct 29 13:59:26 2009 +0100

    block: move bdi/address_space unplug functions to backing-dev.h
    
    There's nothing block related about them, the backing device
    is used by things like NFS etc as well. This gets rid of the
    need to protect such calls by CONFIG_BLOCK.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 221cecd86bd3..39c601f783a0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -823,19 +823,6 @@ static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 	return bdev->bd_disk->queue;
 }
 
-static inline void blk_run_backing_dev(struct backing_dev_info *bdi,
-				       struct page *page)
-{
-	if (bdi && bdi->unplug_io_fn)
-		bdi->unplug_io_fn(bdi, page);
-}
-
-static inline void blk_run_address_space(struct address_space *mapping)
-{
-	if (mapping)
-		blk_run_backing_dev(mapping->backing_dev_info, NULL);
-}
-
 /*
  * blk_rq_pos()			: the current sector
  * blk_rq_bytes()		: bytes left in the entire request

commit 23e018a1b083ecb4b8bb2fb43d58e7c19b5d7959
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Oct 5 08:52:35 2009 +0200

    block: get rid of kblock_schedule_delayed_work()
    
    It was briefly introduced to allow CFQ to to delayed scheduling,
    but we ended up removing that feature again. So lets kill the
    function and export, and just switch CFQ back to the normal work
    schedule since it is now passing in a '0' delay from all call
    sites.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 25119041e034..221cecd86bd3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1172,11 +1172,7 @@ static inline void put_dev_sector(Sector p)
 }
 
 struct work_struct;
-struct delayed_work;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
-int kblockd_schedule_delayed_work(struct request_queue *q,
-					struct delayed_work *work,
-				 	unsigned long delay);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))

commit ac481c20ef8f6c6f2be75d581863f40c43874ef7
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Sat Oct 3 20:52:01 2009 +0200

    block: Topology ioctls
    
    Not all users of the topology information want to use libblkid.  Provide
    the topology information through bdev ioctls.
    
    Also clarify sector size comments for existing BLK ioctls.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a7323930d2ba..25119041e034 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1081,25 +1081,37 @@ static inline unsigned int queue_physical_block_size(struct request_queue *q)
 	return q->limits.physical_block_size;
 }
 
+static inline int bdev_physical_block_size(struct block_device *bdev)
+{
+	return queue_physical_block_size(bdev_get_queue(bdev));
+}
+
 static inline unsigned int queue_io_min(struct request_queue *q)
 {
 	return q->limits.io_min;
 }
 
+static inline int bdev_io_min(struct block_device *bdev)
+{
+	return queue_io_min(bdev_get_queue(bdev));
+}
+
 static inline unsigned int queue_io_opt(struct request_queue *q)
 {
 	return q->limits.io_opt;
 }
 
+static inline int bdev_io_opt(struct block_device *bdev)
+{
+	return queue_io_opt(bdev_get_queue(bdev));
+}
+
 static inline int queue_alignment_offset(struct request_queue *q)
 {
-	if (q && q->limits.misaligned)
+	if (q->limits.misaligned)
 		return -1;
 
-	if (q && q->limits.alignment_offset)
-		return q->limits.alignment_offset;
-
-	return 0;
+	return q->limits.alignment_offset;
 }
 
 static inline int queue_sector_alignment_offset(struct request_queue *q,
@@ -1109,6 +1121,19 @@ static inline int queue_sector_alignment_offset(struct request_queue *q,
 		& (q->limits.io_min - 1);
 }
 
+static inline int bdev_alignment_offset(struct block_device *bdev)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+
+	if (q->limits.misaligned)
+		return -1;
+
+	if (bdev != bdev->bd_contains)
+		return bdev->bd_part->alignment_offset;
+
+	return q->limits.alignment_offset;
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 8e2967555571659d2c8a70dd120710110ed7bba4
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sat Oct 3 16:26:03 2009 +0200

    cfq-iosched: implement slower async initiate and queue ramp up
    
    This slowly ramps up the async queue depth based on the time
    passed since the sync IO, and doesn't allow async at all until
    a sync slice period has passed.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1a03b715dfad..a7323930d2ba 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1147,7 +1147,11 @@ static inline void put_dev_sector(Sector p)
 }
 
 struct work_struct;
+struct delayed_work;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
+int kblockd_schedule_delayed_work(struct request_queue *q,
+					struct delayed_work *work,
+				 	unsigned long delay);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))

commit 67efc9258010da35b27b3854d0880c7e193004ed
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Sep 30 13:54:20 2009 +0200

    block: allow large discard requests
    
    Currently we set the bio size to the byte equivalent of the blocks to
    be trimmed when submitting the initial DISCARD ioctl.  That means it
    is subject to the max_hw_sectors limitation of the HBA which is
    much lower than the size of a DISCARD request we can support.
    Add a separate max_discard_sectors tunable to limit the size for discard
    requests.
    
    We limit the max discard request size in bytes to 32bit as that is the
    limit for bio->bi_size.  This could be much larger if we had a way to pass
    that information through the block layer.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f62d45e87618..1a03b715dfad 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -311,6 +311,7 @@ struct queue_limits {
 	unsigned int		alignment_offset;
 	unsigned int		io_min;
 	unsigned int		io_opt;
+	unsigned int		max_discard_sectors;
 
 	unsigned short		logical_block_size;
 	unsigned short		max_hw_segments;
@@ -928,6 +929,8 @@ extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
+extern void blk_queue_max_discard_sectors(struct request_queue *q,
+		unsigned int max_discard_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_alignment_offset(struct request_queue *q,

commit c15227de132f1295f3db6b7df9079956b1020fd8
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Sep 30 13:52:12 2009 +0200

    block: use normal I/O path for discard requests
    
    prepare_discard_fn() was being called in a place where memory allocation
    was effectively impossible.  This makes it inappropriate for all but
    the most trivial translations of Linux's DISCARD operation to the block
    command set.  Additionally adding a payload there makes the ownership
    of the bio backing unclear as it's now allocated by the device driver
    and not the submitter as usual.
    
    It is replaced with QUEUE_FLAG_DISCARD which is used to indicate whether
    the queue supports discard operations or not.  blkdev_issue_discard now
    allocates a one-page, sector-length payload which is the right thing
    for the common ATA and SCSI implementations.
    
    The mtd implementation of prepare_discard_fn() is replaced with simply
    checking for the request being a discard.
    
    Largely based on a previous patch from Matthew Wilcox <matthew@wil.cx>
    which did the prepare_discard_fn but not the different payload allocation
    yet.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e23a86cae5ac..f62d45e87618 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -82,7 +82,6 @@ enum rq_cmd_type_bits {
 enum {
 	REQ_LB_OP_EJECT	= 0x40,		/* eject request */
 	REQ_LB_OP_FLUSH = 0x41,		/* flush request */
-	REQ_LB_OP_DISCARD = 0x42,	/* discard sectors */
 };
 
 /*
@@ -261,7 +260,6 @@ typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unplug_fn) (struct request_queue *);
-typedef int (prepare_discard_fn) (struct request_queue *, struct request *);
 
 struct bio_vec;
 struct bvec_merge_data {
@@ -340,7 +338,6 @@ struct request_queue
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unplug_fn		*unplug_fn;
-	prepare_discard_fn	*prepare_discard_fn;
 	merge_bvec_fn		*merge_bvec_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
@@ -460,6 +457,7 @@ struct request_queue
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
 #define QUEUE_FLAG_CQ	       16	/* hardware does queuing */
+#define QUEUE_FLAG_DISCARD     17	/* supports DISCARD */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
@@ -591,6 +589,7 @@ enum {
 #define blk_queue_flushing(q)	((q)->ordseq)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
+#define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
 
 #define blk_fs_request(rq)	((rq)->cmd_type == REQ_TYPE_FS)
 #define blk_pc_request(rq)	((rq)->cmd_type == REQ_TYPE_BLOCK_PC)
@@ -955,7 +954,6 @@ extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
-extern void blk_queue_set_discard(struct request_queue *, prepare_discard_fn *);
 extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);

commit 746cd1e7e4a555ddaee53b19a46e05c9c61eaf09
Author: Christoph Hellwig <hch@infradead.org>
Date:   Sat Sep 12 07:35:43 2009 +0200

    block: use blkdev_issue_discard in blk_ioctl_discard
    
    blk_ioctl_discard duplicates large amounts of code from blkdev_issue_discard,
    the only difference between the two is that blkdev_issue_discard needs to
    send a barrier discard request and blk_ioctl_discard a non-barrier one,
    and blk_ioctl_discard needs to wait on the request.  To facilitates this
    add a flags argument to blkdev_issue_discard to control both aspects of the
    behaviour.  This will be very useful later on for using the waiting
    funcitonality for other callers.
    
    Based on an earlier patch from Matthew Wilcox <matthew@wil.cx>.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 86c2bdff3b89..e23a86cae5ac 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -998,15 +998,18 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 }
 
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
-extern int blkdev_issue_discard(struct block_device *,
-				sector_t sector, sector_t nr_sects, gfp_t);
+#define DISCARD_FL_WAIT		0x01	/* wait for completion */
+#define DISCARD_FL_BARRIER	0x02	/* issue DISCARD_BARRIER request */
+extern int blkdev_issue_discard(struct block_device *, sector_t sector,
+		sector_t nr_sects, gfp_t, int flags);
 
 static inline int sb_issue_discard(struct super_block *sb,
 				   sector_t block, sector_t nr_blocks)
 {
 	block <<= (sb->s_blocksize_bits - 9);
 	nr_blocks <<= (sb->s_blocksize_bits - 9);
-	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL);
+	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL,
+				    DISCARD_FL_BARRIER);
 }
 
 extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);

commit 3c5820c743479285ce2678fd3c12b1fd39fe998f
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Sep 11 21:54:52 2009 +0200

    block: Optimal I/O limit wrapper
    
    Implement blk_limits_io_opt() and make blk_queue_io_opt() a wrapper
    around it. DM needs this to avoid poking at the queue_limits directly.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8bf1a10e4d88..86c2bdff3b89 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -935,6 +935,7 @@ extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
 extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
 extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
+extern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);
 extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
 extern void blk_set_default_limits(struct queue_limits *lim);
 extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,

commit 01e97f6b897bf06ec83375d691f2f4d57f5b3a09
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Sep 3 20:06:47 2009 +0200

    block: enable rq CPU completion affinity by default
    
    Test results here look good, and on big OLTP runs it has also shown
    to significantly increase cycles attributed to the database and
    cause a performance boost.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 98b45633a27e..8bf1a10e4d88 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -463,7 +463,8 @@ struct request_queue
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
-				 (1 << QUEUE_FLAG_STACKABLE))
+				 (1 << QUEUE_FLAG_STACKABLE)	|	\
+				 (1 << QUEUE_FLAG_SAME_COMP))
 
 static inline int queue_is_locked(struct request_queue *q)
 {

commit fb1e75389bd06fd5987e9cda1b4e0305c782f854
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jul 30 08:18:24 2009 +0200

    block: improve queue_should_plug() by looking at IO depths
    
    Instead of just checking whether this device uses block layer
    tagging, we can improve the detection by looking at the maximum
    queue depth it has reached. If that crosses 4, then deem it a
    queuing device.
    
    This is important on high IOPS devices, since plugging hurts
    the performance there (it can be as much as 10-15% of the sys
    time).
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 88edb62def82..98b45633a27e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -459,6 +459,7 @@ struct request_queue
 #define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 #define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
+#define QUEUE_FLAG_CQ	       16	/* hardware does queuing */
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
@@ -581,6 +582,7 @@ enum {
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
+#define blk_queue_queuing(q)	test_bit(QUEUE_FLAG_CQ, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)

commit 1f98a13f623e0ef666690a18c1250335fc6d7ef1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Sep 11 14:32:04 2009 +0200

    bio: first step in sanitizing the bio->bi_rw flag testing
    
    Get rid of any functions that test for these bits and make callers
    use bio_rw_flagged() directly. Then it is at least directly apparent
    what variable and flag they check.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 650b6a9cb679..88edb62def82 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -86,7 +86,7 @@ enum {
 };
 
 /*
- * request type modified bits. first two bits match BIO_RW* bits, important
+ * request type modified bits. first four bits match BIO_RW* bits, important
  */
 enum rq_flag_bits {
 	__REQ_RW,		/* not set, read. set, write */

commit 80a761fd33cf812f771e212139157bf8f58d4b3f
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 3 17:48:17 2009 +0900

    block: implement mixed merge of different failfast requests
    
    Failfast has characteristics from other attributes.  When issuing,
    executing and successuflly completing requests, failfast doesn't make
    any difference.  It only affects how a request is handled on failure.
    Allowing requests with different failfast settings to be merged cause
    normal IOs to fail prematurely while not allowing has performance
    penalties as failfast is used for read aheads which are likely to be
    located near in-flight or to-be-issued normal IOs.
    
    This patch introduces the concept of 'mixed merge'.  A request is a
    mixed merge if it is merge of segments which require different
    handling on failure.  Currently the only mixable attributes are
    failfast ones (or lack thereof).
    
    When a bio with different failfast settings is added to an existing
    request or requests of different failfast settings are merged, the
    merged request is marked mixed.  Each bio carries failfast settings
    and the request always tracks failfast state of the first bio.  When
    the request fails, blk_rq_err_bytes() can be used to determine how
    many bytes can be safely failed without crossing into an area which
    requires further retrials.
    
    This allows request merging regardless of failfast settings while
    keeping the failure handling correct.
    
    This patch only implements mixed merge but doesn't enable it.  The
    next one will update SCSI to make use of mixed merge.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Niel Lambrechts <niel.lambrechts@gmail.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c3015736d814..650b6a9cb679 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -115,6 +115,7 @@ enum rq_flag_bits {
 	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
 	__REQ_NOIDLE,		/* Don't anticipate more IO after this one */
 	__REQ_IO_STAT,		/* account I/O stat */
+	__REQ_MIXED_MERGE,	/* merge of different types, fail separately */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -143,6 +144,7 @@ enum rq_flag_bits {
 #define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
 #define REQ_NOIDLE	(1 << __REQ_NOIDLE)
 #define REQ_IO_STAT	(1 << __REQ_IO_STAT)
+#define REQ_MIXED_MERGE	(1 << __REQ_MIXED_MERGE)
 
 #define REQ_FAILFAST_MASK	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | \
 				 REQ_FAILFAST_DRIVER)
@@ -832,11 +834,13 @@ static inline void blk_run_address_space(struct address_space *mapping)
 }
 
 /*
- * blk_rq_pos()		: the current sector
- * blk_rq_bytes()	: bytes left in the entire request
- * blk_rq_cur_bytes()	: bytes left in the current segment
- * blk_rq_sectors()	: sectors left in the entire request
- * blk_rq_cur_sectors()	: sectors left in the current segment
+ * blk_rq_pos()			: the current sector
+ * blk_rq_bytes()		: bytes left in the entire request
+ * blk_rq_cur_bytes()		: bytes left in the current segment
+ * blk_rq_err_bytes()		: bytes left till the next error boundary
+ * blk_rq_sectors()		: sectors left in the entire request
+ * blk_rq_cur_sectors()		: sectors left in the current segment
+ * blk_rq_err_sectors()		: sectors left till the next error boundary
  */
 static inline sector_t blk_rq_pos(const struct request *rq)
 {
@@ -853,6 +857,8 @@ static inline int blk_rq_cur_bytes(const struct request *rq)
 	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
 }
 
+extern unsigned int blk_rq_err_bytes(const struct request *rq);
+
 static inline unsigned int blk_rq_sectors(const struct request *rq)
 {
 	return blk_rq_bytes(rq) >> 9;
@@ -863,6 +869,11 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
+static inline unsigned int blk_rq_err_sectors(const struct request *rq)
+{
+	return blk_rq_err_bytes(rq) >> 9;
+}
+
 /*
  * Request issue related functions.
  */
@@ -889,10 +900,12 @@ extern bool blk_end_request(struct request *rq, int error,
 			    unsigned int nr_bytes);
 extern void blk_end_request_all(struct request *rq, int error);
 extern bool blk_end_request_cur(struct request *rq, int error);
+extern bool blk_end_request_err(struct request *rq, int error);
 extern bool __blk_end_request(struct request *rq, int error,
 			      unsigned int nr_bytes);
 extern void __blk_end_request_all(struct request *rq, int error);
 extern bool __blk_end_request_cur(struct request *rq, int error);
+extern bool __blk_end_request_err(struct request *rq, int error);
 
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);

commit a82afdfcb8c0df09776b6458af6b68fc58b2e87b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jul 3 17:48:16 2009 +0900

    block: use the same failfast bits for bio and request
    
    bio and request use the same set of failfast bits.  This patch makes
    the following changes to simplify things.
    
    * enumify BIO_RW* bits and reorder bits such that BIOS_RW_FAILFAST_*
      bits coincide with __REQ_FAILFAST_* bits.
    
    * The above pushes BIO_RW_AHEAD out of sync with __REQ_FAILFAST_DEV
      but the matching is useless anyway.  init_request_from_bio() is
      responsible for setting FAILFAST bits on FS requests and non-FS
      requests never use BIO_RW_AHEAD.  Drop the code and comment from
      blk_rq_bio_prep().
    
    * Define REQ_FAILFAST_MASK which is OR of all FAILFAST bits and
      simplify FAILFAST flags handling in init_request_from_bio().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 69103e053c92..c3015736d814 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -93,6 +93,7 @@ enum rq_flag_bits {
 	__REQ_FAILFAST_DEV,	/* no driver retries of device errors */
 	__REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
 	__REQ_FAILFAST_DRIVER,	/* no driver retries of driver errors */
+	/* above flags must match BIO_RW_* */
 	__REQ_DISCARD,		/* request to discard sectors */
 	__REQ_SORTED,		/* elevator knows about this request */
 	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
@@ -143,6 +144,9 @@ enum rq_flag_bits {
 #define REQ_NOIDLE	(1 << __REQ_NOIDLE)
 #define REQ_IO_STAT	(1 << __REQ_IO_STAT)
 
+#define REQ_FAILFAST_MASK	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | \
+				 REQ_FAILFAST_DRIVER)
+
 #define BLK_MAX_CDB	16
 
 /*

commit 7c958e32649e0c35801762878fb0b6da8c55a515
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri Jul 31 11:49:11 2009 -0400

    block: Add a wrapper for setting minimum request size without a queue
    
    Introduce blk_limits_io_min() and make blk_queue_io_min() call it.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e7cb5dbf6c26..69103e053c92 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -913,6 +913,7 @@ extern void blk_queue_logical_block_size(struct request_queue *, unsigned short)
 extern void blk_queue_physical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
+extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
 extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
 extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
 extern void blk_set_default_limits(struct queue_limits *lim);

commit 373c0a7ed3ea3b34efedb7c83ffb521adff7c894
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Sat Jul 11 10:06:54 2009 -0400

    Fix compile error due to congestion_wait() changes
    
    Move the definition of BLK_RW_ASYNC/BLK_RW_SYNC into linux/backing-dev.h
    so that it is available to all callers of set/clear_bdi_congested().
    
    This replaces commit 097041e576ee3a50d92dd643ee8ca65bf6a62e21 ("fuse:
    Fix build error"), which will be reverted.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Acked-by: Larry Finger <Larry.Finger@lwfinger.net>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Miklos Szeredi <miklos@szeredi.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0146e0fecf1a..e7cb5dbf6c26 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -70,11 +70,6 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_ATA_PC,
 };
 
-enum {
-	BLK_RW_ASYNC	= 0,
-	BLK_RW_SYNC	= 1,
-};
-
 /*
  * For request of type REQ_TYPE_LINUX_BLOCK, rq->cmd[0] is the opcode being
  * sent down (similar to how REQ_TYPE_BLOCK_PC means that ->cmd[] holds a

commit ecb554a846f8e9d2a58f6d6c118168a63ac065aa
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Jul 9 14:46:53 2009 +0200

    block: fix sg SG_DXFER_TO_FROM_DEV regression
    
    I overlooked SG_DXFER_TO_FROM_DEV support when I converted sg to use
    the block layer mapping API (2.6.28).
    
    Douglas Gilbert explained SG_DXFER_TO_FROM_DEV:
    
    http://www.spinics.net/lists/linux-scsi/msg37135.html
    
    =
    The semantics of SG_DXFER_TO_FROM_DEV were:
       - copy user space buffer to kernel (LLD) buffer
       - do SCSI command which is assumed to be of the DATA_IN
         (data from device) variety. This would overwrite
         some or all of the kernel buffer
       - copy kernel (LLD) buffer back to the user space.
    
    The idea was to detect short reads by filling the original
    user space buffer with some marker bytes ("0xec" it would
    seem in this report). The "resid" value is a better way
    of detecting short reads but that was only added this century
    and requires co-operation from the LLD.
    =
    
    This patch changes the block layer mapping API to support this
    semantics. This simply adds another field to struct rq_map_data and
    enables __bio_copy_iov() to copy data from user space even with READ
    requests.
    
    It's better to add the flags field and kills null_mapped and the new
    from_user fields in struct rq_map_data but that approach makes it
    difficult to send this patch to stable trees because st and osst
    drivers use struct rq_map_data (they were converted to use the block
    layer in 2.6.29 and 2.6.30). Well, I should clean up the block layer
    mapping API.
    
    zhou sf reported this regiression and tested this patch:
    
    http://www.spinics.net/lists/linux-scsi/msg37128.html
    http://www.spinics.net/lists/linux-scsi/msg37168.html
    
    Reported-by: zhou sf <sxzzsf@gmail.com>
    Tested-by: zhou sf <sxzzsf@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bb3d39978701..0146e0fecf1a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -723,6 +723,7 @@ struct rq_map_data {
 	int nr_entries;
 	unsigned long offset;
 	int null_mapped;
+	int from_user;
 };
 
 struct req_iterator {

commit 8aa7e847d834ed937a9ad37a0f2ad5b8584c1ab0
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jul 9 14:52:32 2009 +0200

    Fix congestion_wait() sync/async vs read/write confusion
    
    Commit 1faa16d22877f4839bd433547d770c676d1d964c accidentally broke
    the bdi congestion wait queue logic, causing us to wait on congestion
    for WRITE (== 1) when we really wanted BLK_RW_ASYNC (== 0) instead.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49ae07951d55..bb3d39978701 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -779,18 +779,18 @@ extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
  * congested queues, and wake up anyone who was waiting for requests to be
  * put back.
  */
-static inline void blk_clear_queue_congested(struct request_queue *q, int rw)
+static inline void blk_clear_queue_congested(struct request_queue *q, int sync)
 {
-	clear_bdi_congested(&q->backing_dev_info, rw);
+	clear_bdi_congested(&q->backing_dev_info, sync);
 }
 
 /*
  * A queue has just entered congestion.  Flag that in the queue's VM-visible
  * state flags and increment the global gounter of congested queues.
  */
-static inline void blk_set_queue_congested(struct request_queue *q, int rw)
+static inline void blk_set_queue_congested(struct request_queue *q, int sync)
 {
-	set_bdi_congested(&q->backing_dev_info, rw);
+	set_bdi_congested(&q->backing_dev_info, sync);
 }
 
 extern void blk_start_queue(struct request_queue *q);

commit 018e0446890661504783f92388ecce7138c1566d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 26 16:27:10 2009 +0200

    block: get rid of queue-private command filter
    
    The initial patches to support this through sysfs export were broken
    and have been if 0'ed out in any release. So lets just kill the code
    and reclaim some space in struct request_queue, if anyone would later
    like to fixup the sysfs bits, the git history can easily restore
    the removed bits.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8963d9149b5f..49ae07951d55 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -301,12 +301,6 @@ struct blk_queue_tag {
 #define BLK_SCSI_MAX_CMDS	(256)
 #define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
 
-struct blk_cmd_filter {
-	unsigned long read_ok[BLK_SCSI_CMD_PER_LONG];
-	unsigned long write_ok[BLK_SCSI_CMD_PER_LONG];
-	struct kobject kobj;
-};
-
 struct queue_limits {
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
@@ -445,7 +439,6 @@ struct request_queue
 #if defined(CONFIG_BLK_DEV_BSG)
 	struct bsg_class_device bsg_dev;
 #endif
-	struct blk_cmd_filter cmd_filter;
 };
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
@@ -998,13 +991,7 @@ static inline int sb_issue_discard(struct super_block *sb,
 	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL);
 }
 
-/*
-* command filter functions
-*/
-extern int blk_verify_command(struct blk_cmd_filter *filter,
-			      unsigned char *cmd, fmode_t has_write_perm);
-extern void blk_unregister_filter(struct gendisk *disk);
-extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
+extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
 
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128

commit e475bba2fdee9c3dbfe25f026f8fb8de69508ad2
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Tue Jun 16 08:23:52 2009 +0200

    block: Introduce helper to reset queue limits to default values
    
    DM reuses the request queue when swapping in a new device table
    Introduce blk_set_default_limits() which can be used to reset the the
    queue_limits prior to stacking devices.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Acked-by: Alasdair G Kergon <agk@redhat.com>
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0b1a6cae9de1..8963d9149b5f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -926,6 +926,7 @@ extern void blk_queue_alignment_offset(struct request_queue *q,
 				       unsigned int alignment);
 extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
 extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
+extern void blk_set_default_limits(struct queue_limits *lim);
 extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 			    sector_t offset);
 extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,

commit d614aec4752f8c61b2e7cb77806b6bd59aa50836
Merge: db8e7f10ed67 ad7c52d0988a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 12 09:29:42 2009 -0700

    Merge branch 'for-2.6.31' of git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6
    
    * 'for-2.6.31' of git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6: (29 commits)
      ide: re-implement ide_pci_init_one() on top of ide_pci_init_two()
      ide: unexport ide_find_dma_mode()
      ide: fix PowerMac bootup oops
      ide: skip probe if there are no devices on the port (v2)
      sl82c105: add printk() logging facility
      ide-tape: fix proc warning
      ide: add IDE_DFLAG_NIEN_QUIRK device flag
      ide: respect quirk_drives[] list on all controllers
      hpt366: enable all quirks for devices on quirk_drives[] list
      hpt366: sync quirk_drives[] list with pdc202xx_{new,old}.c
      ide: remove superfluous SELECT_MASK() call from do_rw_taskfile()
      ide: remove superfluous SELECT_MASK() call from ide_driveid_update()
      icside: remove superfluous ->maskproc method
      ide-tape: fix IDE_AFLAG_* atomic accesses
      ide-tape: change IDE_AFLAG_IGNORE_DSC non-atomically
      pdc202xx_old: kill resetproc() method
      pdc202xx_old: don't call pdc202xx_reset() on IRQ timeout
      pdc202xx_old: use ide_dma_test_irq()
      ide: preserve Host Protected Area by default (v2)
      ide-gd: implement block device ->set_capacity method (v2)
      ...

commit b0fd271d5fba0b2d00888363f3869e3f9b26caa9
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Thu Jun 11 13:10:16 2009 +0200

    block: add request clone interface (v2)
    
    This patch adds the following 2 interfaces for request-stacking drivers:
    
      - blk_rq_prep_clone(struct request *clone, struct request *orig,
                          struct bio_set *bs, gfp_t gfp_mask,
                          int (*bio_ctr)(struct bio *, struct bio*, void *),
                          void *data)
          * Clones bios in the original request to the clone request
            (bio_ctr is called for each cloned bios.)
          * Copies attributes of the original request to the clone request.
            The actual data parts (e.g. ->cmd, ->buffer, ->sense) are not
            copied.
    
      - blk_rq_unprep_clone(struct request *clone)
          * Frees cloned bios from the clone request.
    
    Request stacking drivers (e.g. request-based dm) need to make a clone
    request for a submitted request and dispatch it to other devices.
    
    To allocate request for the clone, request stacking drivers may not
    be able to use blk_get_request() because the allocation may be done
    in an irq-disabled context.
    So blk_rq_prep_clone() takes a request allocated by the caller
    as an argument.
    
    For each clone bio in the clone request, request stacking drivers
    should be able to set up their own completion handler.
    So blk_rq_prep_clone() takes a callback function which is called
    for each clone bio, and a pointer for private data which is passed
    to the callback.
    
    NOTE:
    blk_rq_prep_clone() doesn't copy any actual data of the original
    request.  Pages are shared between original bios and cloned bios.
    So caller must not complete the original request before the clone
    request.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5e740a135e73..ebdfde8fe556 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -765,6 +765,11 @@ extern void blk_insert_request(struct request_queue *, struct request *, int, vo
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
 extern int blk_lld_busy(struct request_queue *q);
+extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
+			     struct bio_set *bs, gfp_t gfp_mask,
+			     int (*bio_ctr)(struct bio *, struct bio *, void *),
+			     void *data);
+extern void blk_rq_unprep_clone(struct request *rq);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_plug_device(struct request_queue *);

commit 9df1bb9b516daeece159ab7fb262d01a0359247c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jun 9 06:22:57 2009 +0200

    Revert "block: Fix bounce limit setting in DM"
    
    This reverts commit a05c0205ba031c01bba33a21bf0a35920eb64833.
    
    DM doesn't need to access the bounce_pfn directly.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 989aa1790f48..5e740a135e73 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -910,7 +910,6 @@ extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
-extern void blk_queue_bounce_pfn(struct request_queue *, u64);
 extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);

commit db429e9ec0f9dee2d8e50c154f04f29f880fc9d6
Author: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
Date:   Sun Jun 7 13:52:52 2009 +0200

    partitions: add ->set_capacity block device method
    
    * Add ->set_capacity block device method and use it in rescan_partitions()
      to attempt enabling native capacity of the device upon detecting the
      partition which exceeds device capacity.
    
    * Add GENHD_FL_NATIVE_CAPACITY flag to try limit attempts of enabling
      native capacity during partition scan.
    
    Together with the consecutive patch implementing ->set_capacity method in
    ide-gd device driver this allows automatic disabling of Host Protected Area
    (HPA) if any partitions overlapping HPA are detected.
    
    Cc: Robert Hancock <hancockrwd@gmail.com>
    Cc: Frans Pop <elendil@planet.nl>
    Cc: "Andries E. Brouwer" <Andries.Brouwer@cwi.nl>
    Acked-by: Al Viro <viro@zeniv.linux.org.uk>
    Emphatically-Acked-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6f841fb1be30..a2d7298be351 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1106,6 +1106,8 @@ struct block_device_operations {
 	int (*direct_access) (struct block_device *, sector_t,
 						void **, unsigned long *);
 	int (*media_changed) (struct gendisk *);
+	unsigned long long (*set_capacity) (struct gendisk *,
+						unsigned long long);
 	int (*revalidate_disk) (struct gendisk *);
 	int (*getgeo)(struct block_device *, struct hd_geometry *);
 	struct module *owner;

commit a05c0205ba031c01bba33a21bf0a35920eb64833
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Jun 3 09:33:18 2009 +0200

    block: Fix bounce limit setting in DM
    
    blk_queue_bounce_limit() is more than a wrapper about the request queue
    limits.bounce_pfn variable.  Introduce blk_queue_bounce_pfn() which can
    be called by stacking drivers that wish to set the bounce limit
    explicitly.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5e740a135e73..989aa1790f48 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -910,6 +910,7 @@ extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
+extern void blk_queue_bounce_pfn(struct request_queue *, u64);
 extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);

commit c72758f33784e5e2a1a4bb9421ef3e6de8f9fcf3
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri May 22 17:17:53 2009 -0400

    block: Export I/O topology for block devices and partitions
    
    To support devices with physical block sizes bigger than 512 bytes we
    need to ensure proper alignment.  This patch adds support for exposing
    I/O topology characteristics as devices are stacked.
    
      logical_block_size is the smallest unit the device can address.
    
      physical_block_size indicates the smallest I/O the device can write
      without incurring a read-modify-write penalty.
    
      The io_min parameter is the smallest preferred I/O size reported by
      the device.  In many cases this is the same as the physical block
      size.  However, the io_min parameter can be scaled up when stacking
      (RAID5 chunk size > physical block size).
    
      The io_opt characteristic indicates the optimal I/O size reported by
      the device.  This is usually the stripe width for arrays.
    
      The alignment_offset parameter indicates the number of bytes the start
      of the device/partition is offset from the device's natural alignment.
      Partition tools and MD/DM utilities can use this to pad their offsets
      so filesystems start on proper boundaries.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b7bb6fdba12c..5e740a135e73 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -314,11 +314,16 @@ struct queue_limits {
 	unsigned int		max_hw_sectors;
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
+	unsigned int		physical_block_size;
+	unsigned int		alignment_offset;
+	unsigned int		io_min;
+	unsigned int		io_opt;
 
 	unsigned short		logical_block_size;
 	unsigned short		max_hw_segments;
 	unsigned short		max_phys_segments;
 
+	unsigned char		misaligned;
 	unsigned char		no_cluster;
 };
 
@@ -911,6 +916,15 @@ extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
+extern void blk_queue_physical_block_size(struct request_queue *, unsigned short);
+extern void blk_queue_alignment_offset(struct request_queue *q,
+				       unsigned int alignment);
+extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
+extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
+extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
+			    sector_t offset);
+extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
+			      sector_t offset);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
 extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
 extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
@@ -1047,6 +1061,39 @@ static inline unsigned short bdev_logical_block_size(struct block_device *bdev)
 	return queue_logical_block_size(bdev_get_queue(bdev));
 }
 
+static inline unsigned int queue_physical_block_size(struct request_queue *q)
+{
+	return q->limits.physical_block_size;
+}
+
+static inline unsigned int queue_io_min(struct request_queue *q)
+{
+	return q->limits.io_min;
+}
+
+static inline unsigned int queue_io_opt(struct request_queue *q)
+{
+	return q->limits.io_opt;
+}
+
+static inline int queue_alignment_offset(struct request_queue *q)
+{
+	if (q && q->limits.misaligned)
+		return -1;
+
+	if (q && q->limits.alignment_offset)
+		return q->limits.alignment_offset;
+
+	return 0;
+}
+
+static inline int queue_sector_alignment_offset(struct request_queue *q,
+						sector_t sector)
+{
+	return ((sector << 9) - q->limits.alignment_offset)
+		& (q->limits.io_min - 1);
+}
+
 static inline int queue_dma_alignment(struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;

commit 025146e13b63483add912706c101fb0fb6f015cc
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri May 22 17:17:51 2009 -0400

    block: Move queue limits to an embedded struct
    
    To accommodate stacking drivers that do not have an associated request
    queue we're moving the limits to a separate, embedded structure.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 29b48f7b4ba8..b7bb6fdba12c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -307,6 +307,21 @@ struct blk_cmd_filter {
 	struct kobject kobj;
 };
 
+struct queue_limits {
+	unsigned long		bounce_pfn;
+	unsigned long		seg_boundary_mask;
+
+	unsigned int		max_hw_sectors;
+	unsigned int		max_sectors;
+	unsigned int		max_segment_size;
+
+	unsigned short		logical_block_size;
+	unsigned short		max_hw_segments;
+	unsigned short		max_phys_segments;
+
+	unsigned char		no_cluster;
+};
+
 struct request_queue
 {
 	/*
@@ -358,7 +373,6 @@ struct request_queue
 	/*
 	 * queue needs bounce pages for pages above this limit
 	 */
-	unsigned long		bounce_pfn;
 	gfp_t			bounce_gfp;
 
 	/*
@@ -387,14 +401,6 @@ struct request_queue
 	unsigned int		nr_congestion_off;
 	unsigned int		nr_batching;
 
-	unsigned int		max_sectors;
-	unsigned int		max_hw_sectors;
-	unsigned short		max_phys_segments;
-	unsigned short		max_hw_segments;
-	unsigned short		logical_block_size;
-	unsigned int		max_segment_size;
-
-	unsigned long		seg_boundary_mask;
 	void			*dma_drain_buffer;
 	unsigned int		dma_drain_size;
 	unsigned int		dma_pad_mask;
@@ -410,6 +416,8 @@ struct request_queue
 	struct timer_list	timeout;
 	struct list_head	timeout_list;
 
+	struct queue_limits	limits;
+
 	/*
 	 * sg stuff
 	 */
@@ -991,45 +999,45 @@ extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 static inline unsigned long queue_bounce_pfn(struct request_queue *q)
 {
-	return q->bounce_pfn;
+	return q->limits.bounce_pfn;
 }
 
 static inline unsigned long queue_segment_boundary(struct request_queue *q)
 {
-	return q->seg_boundary_mask;
+	return q->limits.seg_boundary_mask;
 }
 
 static inline unsigned int queue_max_sectors(struct request_queue *q)
 {
-	return q->max_sectors;
+	return q->limits.max_sectors;
 }
 
 static inline unsigned int queue_max_hw_sectors(struct request_queue *q)
 {
-	return q->max_hw_sectors;
+	return q->limits.max_hw_sectors;
 }
 
 static inline unsigned short queue_max_hw_segments(struct request_queue *q)
 {
-	return q->max_hw_segments;
+	return q->limits.max_hw_segments;
 }
 
 static inline unsigned short queue_max_phys_segments(struct request_queue *q)
 {
-	return q->max_phys_segments;
+	return q->limits.max_phys_segments;
 }
 
 static inline unsigned int queue_max_segment_size(struct request_queue *q)
 {
-	return q->max_segment_size;
+	return q->limits.max_segment_size;
 }
 
 static inline unsigned short queue_logical_block_size(struct request_queue *q)
 {
 	int retval = 512;
 
-	if (q && q->logical_block_size)
-		retval = q->logical_block_size;
+	if (q && q->limits.logical_block_size)
+		retval = q->limits.logical_block_size;
 
 	return retval;
 }

commit ae03bf639a5027d27270123f5f6e3ee6a412781d
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri May 22 17:17:50 2009 -0400

    block: Use accessor functions for queue limits
    
    Convert all external users of queue limits to using wrapper functions
    instead of poking the request queue variables directly.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 872b78b7a101..29b48f7b4ba8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -898,6 +898,7 @@ extern void blk_cleanup_queue(struct request_queue *);
 extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
 extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
+extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
@@ -988,6 +989,41 @@ extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
+static inline unsigned long queue_bounce_pfn(struct request_queue *q)
+{
+	return q->bounce_pfn;
+}
+
+static inline unsigned long queue_segment_boundary(struct request_queue *q)
+{
+	return q->seg_boundary_mask;
+}
+
+static inline unsigned int queue_max_sectors(struct request_queue *q)
+{
+	return q->max_sectors;
+}
+
+static inline unsigned int queue_max_hw_sectors(struct request_queue *q)
+{
+	return q->max_hw_sectors;
+}
+
+static inline unsigned short queue_max_hw_segments(struct request_queue *q)
+{
+	return q->max_hw_segments;
+}
+
+static inline unsigned short queue_max_phys_segments(struct request_queue *q)
+{
+	return q->max_phys_segments;
+}
+
+static inline unsigned int queue_max_segment_size(struct request_queue *q)
+{
+	return q->max_segment_size;
+}
+
 static inline unsigned short queue_logical_block_size(struct request_queue *q)
 {
 	int retval = 512;

commit e1defc4ff0cf57aca6c5e3ff99fa503f5943c1f1
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Fri May 22 17:17:49 2009 -0400

    block: Do away with the notion of hardsect_size
    
    Until now we have had a 1:1 mapping between storage device physical
    block size and the logical block sized used when addressing the device.
    With SATA 4KB drives coming out that will no longer be the case.  The
    sector size will be 4KB but the logical block size will remain
    512-bytes.  Hence we need to distinguish between the physical block size
    and the logical ditto.
    
    This patch renames hardsect_size to logical_block_size.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 56ce53fce72e..872b78b7a101 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -391,7 +391,7 @@ struct request_queue
 	unsigned int		max_hw_sectors;
 	unsigned short		max_phys_segments;
 	unsigned short		max_hw_segments;
-	unsigned short		hardsect_size;
+	unsigned short		logical_block_size;
 	unsigned int		max_segment_size;
 
 	unsigned long		seg_boundary_mask;
@@ -901,7 +901,7 @@ extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
-extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
+extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
 extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
 extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
@@ -988,19 +988,19 @@ extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
-static inline int queue_hardsect_size(struct request_queue *q)
+static inline unsigned short queue_logical_block_size(struct request_queue *q)
 {
 	int retval = 512;
 
-	if (q && q->hardsect_size)
-		retval = q->hardsect_size;
+	if (q && q->logical_block_size)
+		retval = q->logical_block_size;
 
 	return retval;
 }
 
-static inline int bdev_hardsect_size(struct block_device *bdev)
+static inline unsigned short bdev_logical_block_size(struct block_device *bdev)
 {
-	return queue_hardsect_size(bdev_get_queue(bdev));
+	return queue_logical_block_size(bdev_get_queue(bdev));
 }
 
 static inline int queue_dma_alignment(struct request_queue *q)

commit e4b636366c00738b9609cda307014d71b1225b7f
Merge: b9ed7252d219 279e677faa77
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 22 20:25:34 2009 +0200

    Merge branch 'master' into for-2.6.31
    
    Conflicts:
            drivers/block/hd.c
            drivers/block/mg_disk.c
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

commit 0a7ae2ff0d29bb3b327edff4c8ab67b3834fa811
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed May 20 08:54:31 2009 +0200

    block: change the tag sync vs async restriction logic
    
    Make them fully share the tag space, but disallow async requests using
    the last any two slots.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 564445be7a6d..a967dd775dbd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -404,7 +404,7 @@ struct request_queue
 	struct list_head	tag_busy_list;
 
 	unsigned int		nr_sorted;
-	unsigned int		in_flight;
+	unsigned int		in_flight[2];
 
 	unsigned int		rq_timeout;
 	struct timer_list	timeout;
@@ -511,6 +511,11 @@ static inline void queue_flag_clear_unlocked(unsigned int flag,
 	__clear_bit(flag, &q->queue_flags);
 }
 
+static inline int queue_in_flight(struct request_queue *q)
+{
+	return q->in_flight[0] + q->in_flight[1];
+}
+
 static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 {
 	WARN_ON_ONCE(!queue_is_locked(q));

commit a411f4bbb89f1f08687b344064d6775bce1e4658
Author: Boaz Harrosh <bharrosh@panasas.com>
Date:   Sun May 17 19:00:01 2009 +0300

    block: Un-export blk_rq_append_bio
    
    OSD was the last in-tree user of blk_rq_append_bio(). Now
    that it is fixed blk_rq_append_bio is un-exported and
    is only used internally by block layer.
    
    Signed-off-by: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 88a83e112c95..564445be7a6d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -757,12 +757,6 @@ extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 
-/*
- * Temporary export, until SCSI gets fixed up.
- */
-extern int blk_rq_append_bio(struct request_queue *q, struct request *rq,
-			     struct bio *bio);
-
 /*
  * A queue has just exitted congestion.  Note this in the global counter of
  * congested queues, and wake up anyone who was waiting for requests to be

commit 79eb63e9e5875b84341a3a05f8e6ae9cdb4bb6f6
Author: Boaz Harrosh <bharrosh@panasas.com>
Date:   Sun May 17 18:57:15 2009 +0300

    block: Add blk_make_request(), takes bio, returns a request
    
    New block API:
    given a struct bio allocates a new request. This is the parallel of
    generic_make_request for BLOCK_PC commands users.
    
    The passed bio may be a chained-bio. The bio is bounced if needed
    inside the call to this member.
    
    This is in the effort of un-exporting blk_rq_append_bio().
    
    Signed-off-by: Boaz Harrosh <bharrosh@panasas.com>
    CC: Jeff Garzik <jeff@garzik.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f9d60a78c08a..88a83e112c95 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -740,6 +740,8 @@ extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
+extern struct request *blk_make_request(struct request_queue *, struct bio *,
+					gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);

commit b1f744937f1be3e6d3009382a755679133cf782d
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon May 11 17:56:09 2009 +0900

    block: move completion related functions back to blk-core.c
    
    Let's put the completion related functions back to block/blk-core.c
    where they have lived. We can also unexport blk_end_bidi_request() and
    __blk_end_bidi_request(), which nobody uses.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1069f4483c6e..f9d60a78c08a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -872,126 +872,14 @@ extern struct request *blk_fetch_request(struct request_queue *q);
  */
 extern bool blk_update_request(struct request *rq, int error,
 			       unsigned int nr_bytes);
-extern bool blk_end_bidi_request(struct request *rq, int error,
-				 unsigned int nr_bytes,
-				 unsigned int bidi_bytes);
-extern bool __blk_end_bidi_request(struct request *rq, int error,
-				   unsigned int nr_bytes,
-				   unsigned int bidi_bytes);
-
-/**
- * blk_end_request - Helper function for drivers to complete the request.
- * @rq:       the request being processed
- * @error:    %0 for success, < %0 for error
- * @nr_bytes: number of bytes to complete
- *
- * Description:
- *     Ends I/O on a number of bytes attached to @rq.
- *     If @rq has leftover, sets it up for the next range of segments.
- *
- * Return:
- *     %false - we are done with this request
- *     %true  - still buffers pending for this request
- **/
-static inline bool blk_end_request(struct request *rq, int error,
-				   unsigned int nr_bytes)
-{
-	return blk_end_bidi_request(rq, error, nr_bytes, 0);
-}
-
-/**
- * blk_end_request_all - Helper function for drives to finish the request.
- * @rq: the request to finish
- * @err: %0 for success, < %0 for error
- *
- * Description:
- *     Completely finish @rq.
- */
-static inline void blk_end_request_all(struct request *rq, int error)
-{
-	bool pending;
-	unsigned int bidi_bytes = 0;
-
-	if (unlikely(blk_bidi_rq(rq)))
-		bidi_bytes = blk_rq_bytes(rq->next_rq);
-
-	pending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
-	BUG_ON(pending);
-}
-
-/**
- * blk_end_request_cur - Helper function to finish the current request chunk.
- * @rq: the request to finish the current chunk for
- * @err: %0 for success, < %0 for error
- *
- * Description:
- *     Complete the current consecutively mapped chunk from @rq.
- *
- * Return:
- *     %false - we are done with this request
- *     %true  - still buffers pending for this request
- */
-static inline bool blk_end_request_cur(struct request *rq, int error)
-{
-	return blk_end_request(rq, error, blk_rq_cur_bytes(rq));
-}
-
-/**
- * __blk_end_request - Helper function for drivers to complete the request.
- * @rq:       the request being processed
- * @error:    %0 for success, < %0 for error
- * @nr_bytes: number of bytes to complete
- *
- * Description:
- *     Must be called with queue lock held unlike blk_end_request().
- *
- * Return:
- *     %false - we are done with this request
- *     %true  - still buffers pending for this request
- **/
-static inline bool __blk_end_request(struct request *rq, int error,
-				     unsigned int nr_bytes)
-{
-	return __blk_end_bidi_request(rq, error, nr_bytes, 0);
-}
-
-/**
- * __blk_end_request_all - Helper function for drives to finish the request.
- * @rq: the request to finish
- * @err: %0 for success, < %0 for error
- *
- * Description:
- *     Completely finish @rq.  Must be called with queue lock held.
- */
-static inline void __blk_end_request_all(struct request *rq, int error)
-{
-	bool pending;
-	unsigned int bidi_bytes = 0;
-
-	if (unlikely(blk_bidi_rq(rq)))
-		bidi_bytes = blk_rq_bytes(rq->next_rq);
-
-	pending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
-	BUG_ON(pending);
-}
-
-/**
- * __blk_end_request_cur - Helper function to finish the current request chunk.
- * @rq: the request to finish the current chunk for
- * @err: %0 for success, < %0 for error
- *
- * Description:
- *     Complete the current consecutively mapped chunk from @rq.  Must
- *     be called with queue lock held.
- *
- * Return:
- *     %false - we are done with this request
- *     %true  - still buffers pending for this request
- */
-static inline bool __blk_end_request_cur(struct request *rq, int error)
-{
-	return __blk_end_request(rq, error, blk_rq_cur_bytes(rq));
-}
+extern bool blk_end_request(struct request *rq, int error,
+			    unsigned int nr_bytes);
+extern void blk_end_request_all(struct request *rq, int error);
+extern bool blk_end_request_cur(struct request *rq, int error);
+extern bool __blk_end_request(struct request *rq, int error,
+			      unsigned int nr_bytes);
+extern void __blk_end_request_all(struct request *rq, int error);
+extern bool __blk_end_request_cur(struct request *rq, int error);
 
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);

commit 1822952ba2b9f22f79019d07ebbeca31dc14b718
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon May 11 17:56:07 2009 +0900

    block: let blk_end_request_all handle bidi requests
    
    blk_end_request_all() and __blk_end_request_all() should finish all
    bytes including bidi, by definition. That's what all bidi users need ,
    bidi requests must be complete as a whole (partial completion is
    impossible).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6e59d3b92ff2..1069f4483c6e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -910,8 +910,12 @@ static inline bool blk_end_request(struct request *rq, int error,
 static inline void blk_end_request_all(struct request *rq, int error)
 {
 	bool pending;
+	unsigned int bidi_bytes = 0;
 
-	pending = blk_end_request(rq, error, blk_rq_bytes(rq));
+	if (unlikely(blk_bidi_rq(rq)))
+		bidi_bytes = blk_rq_bytes(rq->next_rq);
+
+	pending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
 	BUG_ON(pending);
 }
 
@@ -962,8 +966,12 @@ static inline bool __blk_end_request(struct request *rq, int error,
 static inline void __blk_end_request_all(struct request *rq, int error)
 {
 	bool pending;
+	unsigned int bidi_bytes = 0;
+
+	if (unlikely(blk_bidi_rq(rq)))
+		bidi_bytes = blk_rq_bytes(rq->next_rq);
 
-	pending = __blk_end_request(rq, error, blk_rq_bytes(rq));
+	pending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
 	BUG_ON(pending);
 }
 

commit 9934c8c04561413609d2bc38c6b9f268cba774a4
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 8 11:54:16 2009 +0900

    block: implement and enforce request peek/start/fetch
    
    Till now block layer allowed two separate modes of request execution.
    A request is always acquired from the request queue via
    elv_next_request().  After that, drivers are free to either dequeue it
    or process it without dequeueing.  Dequeue allows elv_next_request()
    to return the next request so that multiple requests can be in flight.
    
    Executing requests without dequeueing has its merits mostly in
    allowing drivers for simpler devices which can't do sg to deal with
    segments only without considering request boundary.  However, the
    benefit this brings is dubious and declining while the cost of the API
    ambiguity is increasing.  Segment based drivers are usually for very
    old or limited devices and as converting to dequeueing model isn't
    difficult, it doesn't justify the API overhead it puts on block layer
    and its more modern users.
    
    Previous patches converted all block low level drivers to dequeueing
    model.  This patch completes the API transition by...
    
    * renaming elv_next_request() to blk_peek_request()
    
    * renaming blkdev_dequeue_request() to blk_start_request()
    
    * adding blk_fetch_request() which is combination of peek and start
    
    * disallowing completion of queued (not started) requests
    
    * applying new API to all LLDs
    
    Renamings are for consistency and to break out of tree code so that
    it's apparent that out of tree drivers need updating.
    
    [ Impact: block request issue API cleanup, no functional change ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Mike Miller <mike.miller@hp.com>
    Cc: unsik Kim <donari75@gmail.com>
    Cc: Paul Clements <paul.clements@steeleye.com>
    Cc: Tim Waugh <tim@cyberelk.net>
    Cc: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Laurent Vivier <Laurent@lvivier.info>
    Cc: Jeff Garzik <jgarzik@pobox.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Grant Likely <grant.likely@secretlab.ca>
    Cc: Adrian McMenamin <adrian@mcmen.demon.co.uk>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Cc: Borislav Petkov <petkovbb@googlemail.com>
    Cc: Sergei Shtylyov <sshtylyov@ru.mvista.com>
    Cc: Alex Dubov <oakad@yahoo.com>
    Cc: Pierre Ossman <drzeus@drzeus.cx>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Markus Lidel <Markus.Lidel@shadowconnect.com>
    Cc: Stefan Weinhuber <wein@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Pete Zaitcev <zaitcev@redhat.com>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c75580345700..6e59d3b92ff2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -818,8 +818,6 @@ static inline void blk_run_address_space(struct address_space *mapping)
 		blk_run_backing_dev(mapping->backing_dev_info, NULL);
 }
 
-extern void blkdev_dequeue_request(struct request *req);
-
 /*
  * blk_rq_pos()		: the current sector
  * blk_rq_bytes()	: bytes left in the entire request
@@ -852,6 +850,13 @@ static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 	return blk_rq_cur_bytes(rq) >> 9;
 }
 
+/*
+ * Request issue related functions.
+ */
+extern struct request *blk_peek_request(struct request_queue *q);
+extern void blk_start_request(struct request *rq);
+extern struct request *blk_fetch_request(struct request_queue *q);
+
 /*
  * Request completion related functions.
  *

commit a2dec7b36364a5cc564c4d76cf16d2e7d33f5c05
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 7 22:24:44 2009 +0900

    block: hide request sector and data_len
    
    Block low level drivers for some reason have been pretty good at
    abusing block layer API.  Especially struct request's fields tend to
    get violated in all possible ways.  Make it clear that low level
    drivers MUST NOT access or manipulate rq->sector and rq->data_len
    directly by prefixing them with double underscores.
    
    This change is also necessary to break build of out-of-tree codes
    which assume the previous block API where internal fields can be
    manipulated and rq->data_len carries residual count on completion.
    
    [ Impact: hide internal fields, block API change ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ce2bf5efa9ba..c75580345700 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -166,8 +166,9 @@ struct request {
 	enum rq_cmd_type_bits cmd_type;
 	unsigned long atomic_flags;
 
-	sector_t sector;	/* sector cursor */
-	unsigned int data_len;	/* total data len, don't access directly */
+	/* the following two fields are internal, NEVER access directly */
+	sector_t __sector;		/* sector cursor */
+	unsigned int __data_len;	/* total data len */
 
 	struct bio *bio;
 	struct bio *biotail;
@@ -828,12 +829,12 @@ extern void blkdev_dequeue_request(struct request *req);
  */
 static inline sector_t blk_rq_pos(const struct request *rq)
 {
-	return rq->sector;
+	return rq->__sector;
 }
 
 static inline unsigned int blk_rq_bytes(const struct request *rq)
 {
-	return rq->data_len;
+	return rq->__data_len;
 }
 
 static inline int blk_rq_cur_bytes(const struct request *rq)

commit 2e46e8b27aa57c6bd34b3102b40ee4d0144b4fab
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 7 22:24:41 2009 +0900

    block: drop request->hard_* and *nr_sectors
    
    struct request has had a few different ways to represent some
    properties of a request.  ->hard_* represent block layer's view of the
    request progress (completion cursor) and the ones without the prefix
    are supposed to represent the issue cursor and allowed to be updated
    as necessary by the low level drivers.  The thing is that as block
    layer supports partial completion, the two cursors really aren't
    necessary and only cause confusion.  In addition, manual management of
    request detail from low level drivers is cumbersome and error-prone at
    the very least.
    
    Another interesting duplicate fields are rq->[hard_]nr_sectors and
    rq->{hard_cur|current}_nr_sectors against rq->data_len and
    rq->bio->bi_size.  This is more convoluted than the hard_ case.
    
    rq->[hard_]nr_sectors are initialized for requests with bio but
    blk_rq_bytes() uses it only for !pc requests.  rq->data_len is
    initialized for all request but blk_rq_bytes() uses it only for pc
    requests.  This causes good amount of confusion throughout block layer
    and its drivers and determining the request length has been a bit of
    black magic which may or may not work depending on circumstances and
    what the specific LLD is actually doing.
    
    rq->{hard_cur|current}_nr_sectors represent the number of sectors in
    the contiguous data area at the front.  This is mainly used by drivers
    which transfers data by walking request segment-by-segment.  This
    value always equals rq->bio->bi_size >> 9.  However, data length for
    pc requests may not be multiple of 512 bytes and using this field
    becomes a bit confusing.
    
    In general, having multiple fields to represent the same property
    leads only to confusion and subtle bugs.  With recent block low level
    driver cleanups, no driver is accessing or manipulating these
    duplicate fields directly.  Drop all the duplicates.  Now rq->sector
    means the current sector, rq->data_len the current total length and
    rq->bio->bi_size the current segment length.  Everything else is
    defined in terms of these three and available only through accessors.
    
    * blk_recalc_rq_sectors() is collapsed into blk_update_request() and
      now handles pc and fs requests equally other than rq->sector update.
      This means that now pc requests can use partial completion too (no
      in-kernel user yet tho).
    
    * bio_cur_sectors() is replaced with bio_cur_bytes() as block layer
      now uses byte count as the primary data length.
    
    * blk_rq_pos() is now guranteed to be always correct.  In-block users
      converted.
    
    * blk_rq_bytes() is now guaranteed to be always valid as is
      blk_rq_sectors().  In-block users converted.
    
    * blk_rq_sectors() is now guaranteed to equal blk_rq_bytes() >> 9.
      More convenient one is used.
    
    * blk_rq_bytes() and blk_rq_cur_bytes() are now inlined and take const
      pointer to request.
    
    [ Impact: API cleanup, single way to represent one property of a request ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4e5f85598728..ce2bf5efa9ba 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -166,19 +166,8 @@ struct request {
 	enum rq_cmd_type_bits cmd_type;
 	unsigned long atomic_flags;
 
-	/* Maintain bio traversal state for part by part I/O submission.
-	 * hard_* are block layer internals, no driver should touch them!
-	 */
-
-	sector_t sector;		/* next sector to submit */
-	sector_t hard_sector;		/* next sector to complete */
-	unsigned long nr_sectors;	/* no. of sectors left to submit */
-	unsigned long hard_nr_sectors;	/* no. of sectors left to complete */
-	/* no. of sectors left to submit in the current segment */
-	unsigned int current_nr_sectors;
-
-	/* no. of sectors left to complete in the current segment */
-	unsigned int hard_cur_sectors;
+	sector_t sector;	/* sector cursor */
+	unsigned int data_len;	/* total data len, don't access directly */
 
 	struct bio *bio;
 	struct bio *biotail;
@@ -226,7 +215,6 @@ struct request {
 	unsigned char __cmd[BLK_MAX_CDB];
 	unsigned char *cmd;
 
-	unsigned int data_len;
 	unsigned int extra_len;	/* length of alignment and padding */
 	unsigned int sense_len;
 	unsigned int resid_len;	/* residual count */
@@ -840,20 +828,27 @@ extern void blkdev_dequeue_request(struct request *req);
  */
 static inline sector_t blk_rq_pos(const struct request *rq)
 {
-	return rq->hard_sector;
+	return rq->sector;
+}
+
+static inline unsigned int blk_rq_bytes(const struct request *rq)
+{
+	return rq->data_len;
 }
 
-extern unsigned int blk_rq_bytes(struct request *rq);
-extern unsigned int blk_rq_cur_bytes(struct request *rq);
+static inline int blk_rq_cur_bytes(const struct request *rq)
+{
+	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
+}
 
 static inline unsigned int blk_rq_sectors(const struct request *rq)
 {
-	return rq->hard_nr_sectors;
+	return blk_rq_bytes(rq) >> 9;
 }
 
 static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
 {
-	return rq->hard_cur_sectors;
+	return blk_rq_cur_bytes(rq) >> 9;
 }
 
 /*
@@ -928,7 +923,7 @@ static inline void blk_end_request_all(struct request *rq, int error)
  */
 static inline bool blk_end_request_cur(struct request *rq, int error)
 {
-	return blk_end_request(rq, error, rq->hard_cur_sectors << 9);
+	return blk_end_request(rq, error, blk_rq_cur_bytes(rq));
 }
 
 /**
@@ -981,7 +976,7 @@ static inline void __blk_end_request_all(struct request *rq, int error)
  */
 static inline bool __blk_end_request_cur(struct request *rq, int error)
 {
-	return __blk_end_request(rq, error, rq->hard_cur_sectors << 9);
+	return __blk_end_request(rq, error, blk_rq_cur_bytes(rq));
 }
 
 extern void blk_complete_request(struct request *);

commit 5b93629b4509c03ffa87a9316412fedf6f58cb37
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 7 22:24:38 2009 +0900

    block: implement blk_rq_pos/[cur_]sectors() and convert obvious ones
    
    Implement accessors - blk_rq_pos(), blk_rq_sectors() and
    blk_rq_cur_sectors() which return rq->hard_sector, rq->hard_nr_sectors
    and rq->hard_cur_sectors respectively and convert direct references of
    the said fields to the accessors.
    
    This is in preparation of request data length handling cleanup.
    
    Geert   : suggested adding const to struct request * parameter to accessors
    Sergei  : spotted error in patch description
    
    [ Impact: cleanup ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Acked-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Tested-by: Grant Likely <grant.likely@secretlab.ca>
    Acked-by: Grant Likely <grant.likely@secretlab.ca>
    Ackec-by: Sergei Shtylyov <sshtylyov@ru.mvista.com>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Cc: Borislav Petkov <petkovbb@googlemail.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6a967cad89fa..4e5f85598728 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -832,13 +832,30 @@ static inline void blk_run_address_space(struct address_space *mapping)
 extern void blkdev_dequeue_request(struct request *req);
 
 /*
- * blk_end_request() takes bytes instead of sectors as a complete size.
- * blk_rq_bytes() returns bytes left to complete in the entire request.
- * blk_rq_cur_bytes() returns bytes left to complete in the current segment.
+ * blk_rq_pos()		: the current sector
+ * blk_rq_bytes()	: bytes left in the entire request
+ * blk_rq_cur_bytes()	: bytes left in the current segment
+ * blk_rq_sectors()	: sectors left in the entire request
+ * blk_rq_cur_sectors()	: sectors left in the current segment
  */
+static inline sector_t blk_rq_pos(const struct request *rq)
+{
+	return rq->hard_sector;
+}
+
 extern unsigned int blk_rq_bytes(struct request *rq);
 extern unsigned int blk_rq_cur_bytes(struct request *rq);
 
+static inline unsigned int blk_rq_sectors(const struct request *rq)
+{
+	return rq->hard_nr_sectors;
+}
+
+static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
+{
+	return rq->hard_cur_sectors;
+}
+
 /*
  * Request completion related functions.
  *

commit c3a4d78c580de4edc9ef0f7c59812fb02ceb037f
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 7 22:24:37 2009 +0900

    block: add rq->resid_len
    
    rq->data_len served two purposes - the length of data buffer on issue
    and the residual count on completion.  This duality creates some
    headaches.
    
    First of all, block layer and low level drivers can't really determine
    what rq->data_len contains while a request is executing.  It could be
    the total request length or it coulde be anything else one of the
    lower layers is using to keep track of residual count.  This
    complicates things because blk_rq_bytes() and thus
    [__]blk_end_request_all() relies on rq->data_len for PC commands.
    Drivers which want to report residual count should first cache the
    total request length, update rq->data_len and then complete the
    request with the cached data length.
    
    Secondly, it makes requests default to reporting full residual count,
    ie. reporting that no data transfer occurred.  The residual count is
    an exception not the norm; however, the driver should clear
    rq->data_len to zero to signify the normal cases while leaving it
    alone means no data transfer occurred at all.  This reverse default
    behavior complicates code unnecessarily and renders block PC on some
    drivers (ide-tape/floppy) unuseable.
    
    This patch adds rq->resid_len which is used only for residual count.
    
    While at it, remove now unnecessasry blk_rq_bytes() caching in
    ide_pc_intr() as rq->data_len is not changed anymore.
    
    Boaz    : spotted missing conversion in osd
    Sergei  : spotted too early conversion to blk_rq_bytes() in ide-tape
    
    [ Impact: cleanup residual count handling, report 0 resid by default ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Cc: Borislav Petkov <petkovbb@googlemail.com>
    Cc: Sergei Shtylyov <sshtylyov@ru.mvista.com>
    Cc: Mike Miller <mike.miller@hp.com>
    Cc: Eric Moore <Eric.Moore@lsi.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Doug Gilbert <dgilbert@interlog.com>
    Cc: Mike Miller <mike.miller@hp.com>
    Cc: Eric Moore <Eric.Moore@lsi.com>
    Cc: Darrick J. Wong <djwong@us.ibm.com>
    Cc: Pete Zaitcev <zaitcev@redhat.com>
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3a5b1bd6582c..6a967cad89fa 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -229,6 +229,7 @@ struct request {
 	unsigned int data_len;
 	unsigned int extra_len;	/* length of alignment and padding */
 	unsigned int sense_len;
+	unsigned int resid_len;	/* residual count */
 	void *sense;
 
 	unsigned long deadline;

commit 7b39da786a63e82d9d440075eae9018b79154d2a
Merge: 2142babac999 96c16743973e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 2 16:48:32 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6:
      ide-cd: fix REQ_QUIET tests in cdrom_decode_status
    
    Fix up trivial conflicts in include/linux/blkdev.h

commit 96c16743973e8c1a7b9c655d10b7973408d6d1dd
Author: Borislav Petkov <petkovbb@gmail.com>
Date:   Thu Apr 30 18:24:34 2009 +0200

    ide-cd: fix REQ_QUIET tests in cdrom_decode_status
    
    Original patch (dfa4411cc3a690011cab90e9a536938795366cf9) was buggy.
    This is a more proper fix which introduces blk_rq_quiet() macro
    alleviating the need for dumb, too short caching variables.
    
    Thanks to Helge Deller and Bart for debugging this.
    
    Signed-off-by: Borislav Petkov <petkovbb@gmail.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Sergei Shtylyov <sshtylyov@ru.mvista.com>
    Reported-and-tested-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba54c834a590..6f841fb1be30 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -598,6 +598,7 @@ enum {
 				 blk_failfast_transport(rq) ||	\
 				 blk_failfast_driver(rq))
 #define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
+#define blk_rq_quiet(rq)	((rq)->cmd_flags & REQ_QUIET)
 
 #define blk_account_rq(rq)	(blk_rq_started(rq) && (blk_fs_request(rq) || blk_discard_rq(rq))) 
 

commit 9fd8d0e1bcb848257968d9a7d73ca4d890ea8bd1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 28 13:06:04 2009 +0900

    block: make blk_end_request_cur() return bool
    
    In the process of mindlessly copying [__]blk_end_request_all(),
    [__]blk_end_request_cur() ended up returning void even though they're
    partial completion functions.  Fix it.
    
    [ Impact: fix braindead API ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 12c545e2737c..3a5b1bd6582c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -903,10 +903,14 @@ static inline void blk_end_request_all(struct request *rq, int error)
  *
  * Description:
  *     Complete the current consecutively mapped chunk from @rq.
+ *
+ * Return:
+ *     %false - we are done with this request
+ *     %true  - still buffers pending for this request
  */
-static inline void blk_end_request_cur(struct request *rq, int error)
+static inline bool blk_end_request_cur(struct request *rq, int error)
 {
-	blk_end_request(rq, error, rq->hard_cur_sectors << 9);
+	return blk_end_request(rq, error, rq->hard_cur_sectors << 9);
 }
 
 /**
@@ -952,10 +956,14 @@ static inline void __blk_end_request_all(struct request *rq, int error)
  * Description:
  *     Complete the current consecutively mapped chunk from @rq.  Must
  *     be called with queue lock held.
+ *
+ * Return:
+ *     %false - we are done with this request
+ *     %true  - still buffers pending for this request
  */
-static inline void __blk_end_request_cur(struct request *rq, int error)
+static inline bool __blk_end_request_cur(struct request *rq, int error)
 {
-	__blk_end_request(rq, error, rq->hard_cur_sectors << 9);
+	return __blk_end_request(rq, error, rq->hard_cur_sectors << 9);
 }
 
 extern void blk_complete_request(struct request *);

commit 731ec497e5888c6792ad62613ae9be97eebcd7ca
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:20 2009 +0900

    block: kill rq->data
    
    Now that all block request data transfer is done via bio, rq->data
    isn't used.  Kill it.
    
    While at it, make the roles of rq->special and buffer clear.
    
    [ Impact: drop now unncessary field from struct request ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Boaz Harrosh <bharrosh@panasas.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cfeb3c2feb27..12c545e2737c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -211,8 +211,8 @@ struct request {
 
 	unsigned short ioprio;
 
-	void *special;
-	char *buffer;
+	void *special;		/* opaque pointer available for LLD use */
+	char *buffer;		/* kaddr of the current segment if available */
 
 	int tag;
 	int errors;
@@ -229,7 +229,6 @@ struct request {
 	unsigned int data_len;
 	unsigned int extra_len;	/* length of alignment and padding */
 	unsigned int sense_len;
-	void *data;
 	void *sense;
 
 	unsigned long deadline;

commit f06d9a2b52e246a66b606130cea3f0d7b7be17a7
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:19 2009 +0900

    block: replace end_request() with [__]blk_end_request_cur()
    
    end_request() has been kept around for backward compatibility;
    however, it's about time for it to go away.
    
    * There aren't too many users left.
    
    * Its use of @updtodate is pretty confusing.
    
    * In some cases, newer code ends up using mixture of end_request() and
      [__]blk_end_request[_all](), which is way too confusing.
    
    So, add [__]blk_end_request_cur() and replace end_request() with it.
    Most conversions are straightforward.  Noteworthy ones are...
    
    * paride/pcd: next_request() updated to take 0/-errno instead of 1/0.
    
    * paride/pf: pf_end_request() and next_request() updated to take
      0/-errno instead of 1/0.
    
    * xd: xd_readwrite() updated to return 0/-errno instead of 1/0.
    
    * mtd/mtd_blkdevs: blktrans_discard_request() updated to return
      0/-errno instead of 1/0.  Unnecessary local variable res
      initialization removed from mtd_blktrans_thread().
    
    [ Impact: cleanup ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Joerg Dorchain <joerg@dorchain.net>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Grant Likely <grant.likely@secretlab.ca>
    Acked-by: Laurent Vivier <Laurent@lvivier.info>
    Cc: Tim Waugh <tim@cyberelk.net>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Markus Lidel <Markus.Lidel@shadowconnect.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Pete Zaitcev <zaitcev@redhat.com>
    Cc: unsik Kim <donari75@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e33c8356b3da..cfeb3c2feb27 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -845,9 +845,8 @@ extern unsigned int blk_rq_cur_bytes(struct request *rq);
  * blk_update_request() completes given number of bytes and updates
  * the request without completing it.
  *
- * blk_end_request() and friends.  __blk_end_request() and
- * end_request() must be called with the request queue spinlock
- * acquired.
+ * blk_end_request() and friends.  __blk_end_request() must be called
+ * with the request queue spinlock acquired.
  *
  * Several drivers define their own end_request and call
  * blk_end_request() for parts of the original function.
@@ -898,6 +897,19 @@ static inline void blk_end_request_all(struct request *rq, int error)
 	BUG_ON(pending);
 }
 
+/**
+ * blk_end_request_cur - Helper function to finish the current request chunk.
+ * @rq: the request to finish the current chunk for
+ * @err: %0 for success, < %0 for error
+ *
+ * Description:
+ *     Complete the current consecutively mapped chunk from @rq.
+ */
+static inline void blk_end_request_cur(struct request *rq, int error)
+{
+	blk_end_request(rq, error, rq->hard_cur_sectors << 9);
+}
+
 /**
  * __blk_end_request - Helper function for drivers to complete the request.
  * @rq:       the request being processed
@@ -934,29 +946,17 @@ static inline void __blk_end_request_all(struct request *rq, int error)
 }
 
 /**
- * end_request - end I/O on the current segment of the request
- * @rq:		the request being processed
- * @uptodate:	error value or %0/%1 uptodate flag
+ * __blk_end_request_cur - Helper function to finish the current request chunk.
+ * @rq: the request to finish the current chunk for
+ * @err: %0 for success, < %0 for error
  *
  * Description:
- *     Ends I/O on the current segment of a request. If that is the only
- *     remaining segment, the request is also completed and freed.
- *
- *     This is a remnant of how older block drivers handled I/O completions.
- *     Modern drivers typically end I/O on the full request in one go, unless
- *     they have a residual value to account for. For that case this function
- *     isn't really useful, unless the residual just happens to be the
- *     full current segment. In other words, don't use this function in new
- *     code. Use blk_end_request() or __blk_end_request() to end a request.
- **/
-static inline void end_request(struct request *rq, int uptodate)
+ *     Complete the current consecutively mapped chunk from @rq.  Must
+ *     be called with queue lock held.
+ */
+static inline void __blk_end_request_cur(struct request *rq, int error)
 {
-	int error = 0;
-
-	if (uptodate <= 0)
-		error = uptodate ? uptodate : -EIO;
-
-	__blk_end_bidi_request(rq, error, rq->hard_cur_sectors << 9, 0);
+	__blk_end_request(rq, error, rq->hard_cur_sectors << 9);
 }
 
 extern void blk_complete_request(struct request *);

commit 40cbbb781d3eba5d6ac0860db078af490e5c7c6b
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:19 2009 +0900

    block: implement and use [__]blk_end_request_all()
    
    There are many [__]blk_end_request() call sites which call it with
    full request length and expect full completion.  Many of them ensure
    that the request actually completes by doing BUG_ON() the return
    value, which is awkward and error-prone.
    
    This patch adds [__]blk_end_request_all() which takes @rq and @error
    and fully completes the request.  BUG_ON() is added to to ensure that
    this actually happens.
    
    Most conversions are simple but there are a few noteworthy ones.
    
    * cdrom/viocd: viocd_end_request() replaced with direct calls to
      __blk_end_request_all().
    
    * s390/block/dasd: dasd_end_request() replaced with direct calls to
      __blk_end_request_all().
    
    * s390/char/tape_block: tapeblock_end_request() replaced with direct
      calls to blk_end_request_all().
    
    [ Impact: cleanup ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Mike Miller <mike.miller@hp.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jeff Garzik <jgarzik@pobox.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Alex Dubov <oakad@yahoo.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 501f6845cc73..e33c8356b3da 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -882,6 +882,22 @@ static inline bool blk_end_request(struct request *rq, int error,
 	return blk_end_bidi_request(rq, error, nr_bytes, 0);
 }
 
+/**
+ * blk_end_request_all - Helper function for drives to finish the request.
+ * @rq: the request to finish
+ * @err: %0 for success, < %0 for error
+ *
+ * Description:
+ *     Completely finish @rq.
+ */
+static inline void blk_end_request_all(struct request *rq, int error)
+{
+	bool pending;
+
+	pending = blk_end_request(rq, error, blk_rq_bytes(rq));
+	BUG_ON(pending);
+}
+
 /**
  * __blk_end_request - Helper function for drivers to complete the request.
  * @rq:       the request being processed
@@ -901,6 +917,22 @@ static inline bool __blk_end_request(struct request *rq, int error,
 	return __blk_end_bidi_request(rq, error, nr_bytes, 0);
 }
 
+/**
+ * __blk_end_request_all - Helper function for drives to finish the request.
+ * @rq: the request to finish
+ * @err: %0 for success, < %0 for error
+ *
+ * Description:
+ *     Completely finish @rq.  Must be called with queue lock held.
+ */
+static inline void __blk_end_request_all(struct request *rq, int error)
+{
+	bool pending;
+
+	pending = __blk_end_request(rq, error, blk_rq_bytes(rq));
+	BUG_ON(pending);
+}
+
 /**
  * end_request - end I/O on the current segment of the request
  * @rq:		the request being processed

commit 2e60e02297cf54e367567f2d85b2ca56b1c4a906
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:18 2009 +0900

    block: clean up request completion API
    
    Request completion has gone through several changes and became a bit
    messy over the time.  Clean it up.
    
    1. end_that_request_data() is a thin wrapper around
       end_that_request_data_first() which checks whether bio is NULL
       before doing anything and handles bidi completion.
       blk_update_request() is a thin wrapper around
       end_that_request_data() which clears nr_sectors on the last
       iteration but doesn't use the bidi completion.
    
       Clean it up by moving the initial bio NULL check and nr_sectors
       clearing on the last iteration into end_that_request_data() and
       renaming it to blk_update_request(), which makes blk_end_io() the
       only user of end_that_request_data().  Collapse
       end_that_request_data() into blk_end_io().
    
    2. There are four visible completion variants - blk_end_request(),
       __blk_end_request(), blk_end_bidi_request() and end_request().
       blk_end_request() and blk_end_bidi_request() uses blk_end_request()
       as the backend but __blk_end_request() and end_request() use
       separate implementation in __blk_end_request() due to different
       locking rules.
    
       blk_end_bidi_request() is identical to blk_end_io().  Collapse
       blk_end_io() into blk_end_bidi_request(), separate out request
       update into internal helper blk_update_bidi_request() and add
       __blk_end_bidi_request().  Redefine [__]blk_end_request() as thin
       inline wrappers around [__]blk_end_bidi_request().
    
    3. As the whole request issue/completion usages are about to be
       modified and audited, it's a good chance to convert completion
       functions return bool which better indicates the intended meaning
       of return values.
    
    4. The function name end_that_request_last() is from the days when it
       was a public interface and slighly confusing.  Give it a proper
       internal name - blk_finish_request().
    
    5. Add description explaning that blk_end_bidi_request() can be safely
       used for uni requests as suggested by Boaz Harrosh.
    
    The only visible behavior change is from #1.  nr_sectors counts are
    cleared after the final iteration no matter which function is used to
    complete the request.  I couldn't find any place where the code
    assumes those nr_sectors counters contain the values for the last
    segment and this change is good as it makes the API much more
    consistent as the end result is now same whether a request is
    completed using [__]blk_end_request() alone or in combination with
    blk_update_request().
    
    API further cleaned up per Christoph's suggestion.
    
    [ Impact: cleanup, rq->*nr_sectors always updated after req completion ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Christoph Hellwig <hch@infradead.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1fa9dcf9aa6a..501f6845cc73 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -840,27 +840,97 @@ extern unsigned int blk_rq_bytes(struct request *rq);
 extern unsigned int blk_rq_cur_bytes(struct request *rq);
 
 /*
- * blk_end_request() and friends.
- * __blk_end_request() and end_request() must be called with
- * the request queue spinlock acquired.
+ * Request completion related functions.
+ *
+ * blk_update_request() completes given number of bytes and updates
+ * the request without completing it.
+ *
+ * blk_end_request() and friends.  __blk_end_request() and
+ * end_request() must be called with the request queue spinlock
+ * acquired.
  *
  * Several drivers define their own end_request and call
  * blk_end_request() for parts of the original function.
  * This prevents code duplication in drivers.
  */
-extern int blk_end_request(struct request *rq, int error,
-				unsigned int nr_bytes);
-extern int __blk_end_request(struct request *rq, int error,
-				unsigned int nr_bytes);
-extern int blk_end_bidi_request(struct request *rq, int error,
-				unsigned int nr_bytes, unsigned int bidi_bytes);
-extern void end_request(struct request *, int);
+extern bool blk_update_request(struct request *rq, int error,
+			       unsigned int nr_bytes);
+extern bool blk_end_bidi_request(struct request *rq, int error,
+				 unsigned int nr_bytes,
+				 unsigned int bidi_bytes);
+extern bool __blk_end_bidi_request(struct request *rq, int error,
+				   unsigned int nr_bytes,
+				   unsigned int bidi_bytes);
+
+/**
+ * blk_end_request - Helper function for drivers to complete the request.
+ * @rq:       the request being processed
+ * @error:    %0 for success, < %0 for error
+ * @nr_bytes: number of bytes to complete
+ *
+ * Description:
+ *     Ends I/O on a number of bytes attached to @rq.
+ *     If @rq has leftover, sets it up for the next range of segments.
+ *
+ * Return:
+ *     %false - we are done with this request
+ *     %true  - still buffers pending for this request
+ **/
+static inline bool blk_end_request(struct request *rq, int error,
+				   unsigned int nr_bytes)
+{
+	return blk_end_bidi_request(rq, error, nr_bytes, 0);
+}
+
+/**
+ * __blk_end_request - Helper function for drivers to complete the request.
+ * @rq:       the request being processed
+ * @error:    %0 for success, < %0 for error
+ * @nr_bytes: number of bytes to complete
+ *
+ * Description:
+ *     Must be called with queue lock held unlike blk_end_request().
+ *
+ * Return:
+ *     %false - we are done with this request
+ *     %true  - still buffers pending for this request
+ **/
+static inline bool __blk_end_request(struct request *rq, int error,
+				     unsigned int nr_bytes)
+{
+	return __blk_end_bidi_request(rq, error, nr_bytes, 0);
+}
+
+/**
+ * end_request - end I/O on the current segment of the request
+ * @rq:		the request being processed
+ * @uptodate:	error value or %0/%1 uptodate flag
+ *
+ * Description:
+ *     Ends I/O on the current segment of a request. If that is the only
+ *     remaining segment, the request is also completed and freed.
+ *
+ *     This is a remnant of how older block drivers handled I/O completions.
+ *     Modern drivers typically end I/O on the full request in one go, unless
+ *     they have a residual value to account for. For that case this function
+ *     isn't really useful, unless the residual just happens to be the
+ *     full current segment. In other words, don't use this function in new
+ *     code. Use blk_end_request() or __blk_end_request() to end a request.
+ **/
+static inline void end_request(struct request *rq, int uptodate)
+{
+	int error = 0;
+
+	if (uptodate <= 0)
+		error = uptodate ? uptodate : -EIO;
+
+	__blk_end_bidi_request(rq, error, rq->hard_cur_sectors << 9, 0);
+}
+
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
 extern void blk_abort_queue(struct request_queue *);
-extern void blk_update_request(struct request *rq, int error,
-			       unsigned int nr_bytes);
 
 /*
  * Access functions for manipulating queue properties

commit 0b302d5aa7975006fa2ec3d66386610b9b36c669
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:18 2009 +0900

    block: kill blk_end_request_callback()
    
    With recent IDE updates, blk_end_request_callback() doesn't have any
    user now.  Kill it.
    
    [ Impact: removal of unused convoluted interface ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 156ffd9de967..1fa9dcf9aa6a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -855,9 +855,6 @@ extern int __blk_end_request(struct request *rq, int error,
 extern int blk_end_bidi_request(struct request *rq, int error,
 				unsigned int nr_bytes, unsigned int bidi_bytes);
 extern void end_request(struct request *, int);
-extern int blk_end_request_callback(struct request *rq, int error,
-				unsigned int nr_bytes,
-				int (drv_callback)(struct request *));
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);

commit 5efccd17ceb0fc43837a331297c2c407969d7201
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:18 2009 +0900

    block: reorder request completion functions
    
    Reorder request completion functions such that
    
    * All request completion functions are located together.
    
    * Functions which are used by only one caller is put right above the
      caller.
    
    * end_request() is put after other completion functions but before
      blk_update_request().
    
    This change is for completion function cleanup which will follow.
    
    [ Impact: cleanup, code reorganization ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 12e20de44b60..156ffd9de967 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -831,6 +831,14 @@ static inline void blk_run_address_space(struct address_space *mapping)
 
 extern void blkdev_dequeue_request(struct request *req);
 
+/*
+ * blk_end_request() takes bytes instead of sectors as a complete size.
+ * blk_rq_bytes() returns bytes left to complete in the entire request.
+ * blk_rq_cur_bytes() returns bytes left to complete in the current segment.
+ */
+extern unsigned int blk_rq_bytes(struct request *rq);
+extern unsigned int blk_rq_cur_bytes(struct request *rq);
+
 /*
  * blk_end_request() and friends.
  * __blk_end_request() and end_request() must be called with
@@ -857,14 +865,6 @@ extern void blk_abort_queue(struct request_queue *);
 extern void blk_update_request(struct request *rq, int error,
 			       unsigned int nr_bytes);
 
-/*
- * blk_end_request() takes bytes instead of sectors as a complete size.
- * blk_rq_bytes() returns bytes left to complete in the entire request.
- * blk_rq_cur_bytes() returns bytes left to complete in the current segment.
- */
-extern unsigned int blk_rq_bytes(struct request *rq);
-extern unsigned int blk_rq_cur_bytes(struct request *rq);
-
 /*
  * Access functions for manipulating queue properties
  */

commit a7f557923441186a3cdbabc54f1bcacf42b63bf5
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 23 11:05:17 2009 +0900

    block: kill blk_start_queueing()
    
    blk_start_queueing() is identical to __blk_run_queue() except that it
    doesn't check for recursion.  None of the current users depends on
    blk_start_queueing() running request_fn directly.  Replace usages of
    blk_start_queueing() with [__]blk_run_queue() and kill it.
    
    [ Impact: removal of mostly duplicate interface function ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2755d5c6da22..12e20de44b60 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -797,7 +797,6 @@ extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
 extern void __blk_run_queue(struct request_queue *);
 extern void blk_run_queue(struct request_queue *);
-extern void blk_start_queueing(struct request_queue *);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
 			   struct rq_map_data *, void __user *, unsigned long,
 			   gfp_t);

commit 42dad7647aec49b3ad20dd0cb832b232a6ae514f
Author: Jerome Marchand <jmarchan@redhat.com>
Date:   Wed Apr 22 14:01:49 2009 +0200

    block: simplify I/O stat accounting
    
    This simplifies I/O stat accounting switching code and separates it
    completely from I/O scheduler switch code.
    
    Requests are accounted according to the state of their request queue
    at the time of the request allocation. There is no need anymore to
    flush the request queue when switching I/O accounting state.
    
    Signed-off-by: Jerome Marchand <jmarchan@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ba54c834a590..2755d5c6da22 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -118,6 +118,7 @@ enum rq_flag_bits {
 	__REQ_COPY_USER,	/* contains copies of user pages */
 	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
 	__REQ_NOIDLE,		/* Don't anticipate more IO after this one */
+	__REQ_IO_STAT,		/* account I/O stat */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -145,6 +146,7 @@ enum rq_flag_bits {
 #define REQ_COPY_USER	(1 << __REQ_COPY_USER)
 #define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
 #define REQ_NOIDLE	(1 << __REQ_NOIDLE)
+#define REQ_IO_STAT	(1 << __REQ_IO_STAT)
 
 #define BLK_MAX_CDB	16
 
@@ -598,6 +600,7 @@ enum {
 				 blk_failfast_transport(rq) ||	\
 				 blk_failfast_driver(rq))
 #define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
+#define blk_rq_io_stat(rq)	((rq)->cmd_flags & REQ_IO_STAT)
 
 #define blk_account_rq(rq)	(blk_rq_started(rq) && (blk_fs_request(rq) || blk_discard_rq(rq))) 
 

commit 2385327725419a76cfbca7258abd95908b8ba9eb
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Apr 7 08:59:11 2009 +0200

    block: remove unused REQ_UNPLUG
    
    The request inherits the unplug flag from the bio, but it isn't actually
    used. The bio flag stops at __make_request(), which tells it to unplug
    after submission. Passing it on to the request doesn't make any sense.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e03660964e02..ba54c834a590 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -117,7 +117,6 @@ enum rq_flag_bits {
 	__REQ_RW_META,		/* metadata io request */
 	__REQ_COPY_USER,	/* contains copies of user pages */
 	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
-	__REQ_UNPLUG,		/* unplug queue on submission */
 	__REQ_NOIDLE,		/* Don't anticipate more IO after this one */
 	__REQ_NR_BITS,		/* stops here */
 };
@@ -145,7 +144,6 @@ enum rq_flag_bits {
 #define REQ_RW_META	(1 << __REQ_RW_META)
 #define REQ_COPY_USER	(1 << __REQ_COPY_USER)
 #define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
-#define REQ_UNPLUG	(1 << __REQ_UNPLUG)
 #define REQ_NOIDLE	(1 << __REQ_NOIDLE)
 
 #define BLK_MAX_CDB	16

commit aeb6fafb8fa53266d70ca7474fcda2bdaf96524a
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Apr 6 14:48:07 2009 +0200

    block: Add flag for telling the IO schedulers NOT to anticipate more IO
    
    By default, CFQ will anticipate more IO from a given io context if the
    previously completed IO was sync. This used to be fine, since the only
    sync IO was reads and O_DIRECT writes. But with more "normal" sync writes
    being used now, we don't want to anticipate for those.
    
    Add a bio/request flag that informs the IO scheduler that this is a sync
    request that we should not idle for. Introduce WRITE_ODIRECT specifically
    for O_DIRECT writes, and make sure that the other sync writes set this
    flag.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 67dae3bd881c..e03660964e02 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -118,6 +118,7 @@ enum rq_flag_bits {
 	__REQ_COPY_USER,	/* contains copies of user pages */
 	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
 	__REQ_UNPLUG,		/* unplug queue on submission */
+	__REQ_NOIDLE,		/* Don't anticipate more IO after this one */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -145,6 +146,7 @@ enum rq_flag_bits {
 #define REQ_COPY_USER	(1 << __REQ_COPY_USER)
 #define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
 #define REQ_UNPLUG	(1 << __REQ_UNPLUG)
+#define REQ_NOIDLE	(1 << __REQ_NOIDLE)
 
 #define BLK_MAX_CDB	16
 
@@ -633,6 +635,7 @@ static inline bool rq_is_sync(struct request *rq)
 }
 
 #define rq_is_meta(rq)		((rq)->cmd_flags & REQ_RW_META)
+#define rq_noidle(rq)		((rq)->cmd_flags & REQ_NOIDLE)
 
 static inline int blk_queue_full(struct request_queue *q, int sync)
 {

commit 1faa16d22877f4839bd433547d770c676d1d964c
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Apr 6 14:48:01 2009 +0200

    block: change the request allocation/congestion logic to be sync/async based
    
    This makes sure that we never wait on async IO for sync requests, instead
    of doing the split on writes vs reads.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 465d6babc847..67dae3bd881c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -38,6 +38,10 @@ struct request;
 typedef void (rq_end_io_fn)(struct request *, int);
 
 struct request_list {
+	/*
+	 * count[], starved[], and wait[] are indexed by
+	 * BLK_RW_SYNC/BLK_RW_ASYNC
+	 */
 	int count[2];
 	int starved[2];
 	int elvpriv;
@@ -66,6 +70,11 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_ATA_PC,
 };
 
+enum {
+	BLK_RW_ASYNC	= 0,
+	BLK_RW_SYNC	= 1,
+};
+
 /*
  * For request of type REQ_TYPE_LINUX_BLOCK, rq->cmd[0] is the opcode being
  * sent down (similar to how REQ_TYPE_BLOCK_PC means that ->cmd[] holds a
@@ -103,7 +112,7 @@ enum rq_flag_bits {
 	__REQ_QUIET,		/* don't worry about errors */
 	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
 	__REQ_ORDERED_COLOR,	/* is before or after barrier */
-	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
+	__REQ_RW_SYNC,		/* request is sync (sync write or read) */
 	__REQ_ALLOCED,		/* request came from our alloc pool */
 	__REQ_RW_META,		/* metadata io request */
 	__REQ_COPY_USER,	/* contains copies of user pages */
@@ -438,8 +447,8 @@ struct request_queue
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 #define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
-#define	QUEUE_FLAG_READFULL	3	/* read queue has been filled */
-#define QUEUE_FLAG_WRITEFULL	4	/* write queue has been filled */
+#define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
+#define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
@@ -611,32 +620,41 @@ enum {
 #define rq_data_dir(rq)		((rq)->cmd_flags & 1)
 
 /*
- * We regard a request as sync, if it's a READ or a SYNC write.
+ * We regard a request as sync, if either a read or a sync write
  */
-#define rq_is_sync(rq)		(rq_data_dir((rq)) == READ || (rq)->cmd_flags & REQ_RW_SYNC)
+static inline bool rw_is_sync(unsigned int rw_flags)
+{
+	return !(rw_flags & REQ_RW) || (rw_flags & REQ_RW_SYNC);
+}
+
+static inline bool rq_is_sync(struct request *rq)
+{
+	return rw_is_sync(rq->cmd_flags);
+}
+
 #define rq_is_meta(rq)		((rq)->cmd_flags & REQ_RW_META)
 
-static inline int blk_queue_full(struct request_queue *q, int rw)
+static inline int blk_queue_full(struct request_queue *q, int sync)
 {
-	if (rw == READ)
-		return test_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
-	return test_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+	if (sync)
+		return test_bit(QUEUE_FLAG_SYNCFULL, &q->queue_flags);
+	return test_bit(QUEUE_FLAG_ASYNCFULL, &q->queue_flags);
 }
 
-static inline void blk_set_queue_full(struct request_queue *q, int rw)
+static inline void blk_set_queue_full(struct request_queue *q, int sync)
 {
-	if (rw == READ)
-		queue_flag_set(QUEUE_FLAG_READFULL, q);
+	if (sync)
+		queue_flag_set(QUEUE_FLAG_SYNCFULL, q);
 	else
-		queue_flag_set(QUEUE_FLAG_WRITEFULL, q);
+		queue_flag_set(QUEUE_FLAG_ASYNCFULL, q);
 }
 
-static inline void blk_clear_queue_full(struct request_queue *q, int rw)
+static inline void blk_clear_queue_full(struct request_queue *q, int sync)
 {
-	if (rw == READ)
-		queue_flag_clear(QUEUE_FLAG_READFULL, q);
+	if (sync)
+		queue_flag_clear(QUEUE_FLAG_SYNCFULL, q);
 	else
-		queue_flag_clear(QUEUE_FLAG_WRITEFULL, q);
+		queue_flag_clear(QUEUE_FLAG_ASYNCFULL, q);
 }
 
 

commit 1e42807918d17e8c93bf14fbb74be84b141334c1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Feb 23 09:03:10 2009 +0100

    block: reduce stack footprint of blk_recount_segments()
    
    blk_recalc_rq_segments() requires a request structure passed in, which
    we don't have from blk_recount_segments(). So the latter allocates one on
    the stack, using > 400 bytes of stack for that. This can cause us to spill
    over one page of stack from ext4 at least:
    
     0)     4560     400   blk_recount_segments+0x43/0x62
     1)     4160      32   bio_phys_segments+0x1c/0x24
     2)     4128      32   blk_rq_bio_prep+0x2a/0xf9
     3)     4096      32   init_request_from_bio+0xf9/0xfe
     4)     4064     112   __make_request+0x33c/0x3f6
     5)     3952     144   generic_make_request+0x2d1/0x321
     6)     3808      64   submit_bio+0xb9/0xc3
     7)     3744      48   submit_bh+0xea/0x10e
     8)     3696     368   ext4_mb_init_cache+0x257/0xa6a [ext4]
     9)     3328     288   ext4_mb_regular_allocator+0x421/0xcd9 [ext4]
    10)     3040     160   ext4_mb_new_blocks+0x211/0x4b4 [ext4]
    11)     2880     336   ext4_ext_get_blocks+0xb61/0xd45 [ext4]
    12)     2544      96   ext4_get_blocks_wrap+0xf2/0x200 [ext4]
    13)     2448      80   ext4_da_get_block_write+0x6e/0x16b [ext4]
    14)     2368     352   mpage_da_map_blocks+0x7e/0x4b3 [ext4]
    15)     2016     352   ext4_da_writepages+0x2ce/0x43c [ext4]
    16)     1664      32   do_writepages+0x2d/0x3c
    17)     1632     144   __writeback_single_inode+0x162/0x2cd
    18)     1488      96   generic_sync_sb_inodes+0x1e3/0x32b
    19)     1392      16   sync_sb_inodes+0xe/0x10
    20)     1376      48   writeback_inodes+0x69/0xb3
    21)     1328     208   balance_dirty_pages_ratelimited_nr+0x187/0x2f9
    22)     1120     224   generic_file_buffered_write+0x1d4/0x2c4
    23)      896     176   __generic_file_aio_write_nolock+0x35f/0x393
    24)      720      80   generic_file_aio_write+0x6c/0xc8
    25)      640      80   ext4_file_write+0xa9/0x137 [ext4]
    26)      560     320   do_sync_write+0xf0/0x137
    27)      240      48   vfs_write+0xb3/0x13c
    28)      192      64   sys_write+0x4c/0x74
    29)      128     128   system_call_fastpath+0x16/0x1b
    
    Split the segment counting out into a __blk_recalc_rq_segments() helper
    to avoid allocating an onstack request just for checking the physical
    segment count.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index dcaa0fd84b02..465d6babc847 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -708,6 +708,8 @@ struct req_iterator {
 };
 
 /* This should not be used directly - use rq_for_each_segment */
+#define for_each_bio(_bio)		\
+	for (; _bio; _bio = _bio->bi_next)
 #define __rq_for_each_bio(_bio, rq)	\
 	if ((rq->bio))			\
 		for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)

commit 0648e10d71c8e510d80772c4cb4220c97e9c34c7
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Feb 2 08:43:48 2009 +0100

    block: fix inconsistent parenthesisation of QUEUE_FLAG_DEFAULT
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d08c4b8219a6..dcaa0fd84b02 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -455,7 +455,7 @@ struct request_queue
 
 #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_CLUSTER) |		\
-				  1 << QUEUE_FLAG_STACKABLE)
+				 (1 << QUEUE_FLAG_STACKABLE))
 
 static inline int queue_is_locked(struct request_queue *q)
 {

commit bc58ba9468d94d62c56ab9b47173583ec140b165
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jan 23 10:54:44 2009 +0100

    block: add sysfs file for controlling io stats accounting
    
    This allows us to turn off disk stat accounting completely, for the cases
    where the 0.5-1% reduction in system time is important.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 75426e4b8cdf..d08c4b8219a6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -451,6 +451,11 @@ struct request_queue
 #define QUEUE_FLAG_STACKABLE   13	/* supports request stacking */
 #define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
+#define QUEUE_FLAG_IO_STAT     15	/* do IO stats */
+
+#define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
+				 (1 << QUEUE_FLAG_CLUSTER) |		\
+				  1 << QUEUE_FLAG_STACKABLE)
 
 static inline int queue_is_locked(struct request_queue *q)
 {
@@ -567,6 +572,7 @@ enum {
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
+#define blk_queue_io_stat(q)	test_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)

commit 213d9417fec62ef4c3675621b9364a667954d4dd
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jan 6 09:16:05 2009 +0100

    block: seperate bio/request unplug and sync bits
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 044467ef7b11..75426e4b8cdf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -108,6 +108,7 @@ enum rq_flag_bits {
 	__REQ_RW_META,		/* metadata io request */
 	__REQ_COPY_USER,	/* contains copies of user pages */
 	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
+	__REQ_UNPLUG,		/* unplug queue on submission */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -134,6 +135,7 @@ enum rq_flag_bits {
 #define REQ_RW_META	(1 << __REQ_RW_META)
 #define REQ_COPY_USER	(1 << __REQ_COPY_USER)
 #define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
+#define REQ_UNPLUG	(1 << __REQ_UNPLUG)
 
 #define BLK_MAX_CDB	16
 

commit 97ae77a1cd332c7b011d71315c8faabce6840c72
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Dec 18 14:49:38 2008 +0900

    [SCSI] block: make blk_rq_map_user take a NULL user-space buffer for WRITE
    
    The commit 818827669d85b84241696ffef2de485db46b0b5e (block: make
    blk_rq_map_user take a NULL user-space buffer) extended
    blk_rq_map_user to accept a NULL user-space buffer with a READ
    command. It was necessary to convert sg to use the block layer mapping
    API.
    
    This patch extends blk_rq_map_user again for a WRITE command. It is
    necessary to convert st and osst drivers to use the block layer
    apping API.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 811e5342c452..044467ef7b11 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -691,6 +691,7 @@ struct rq_map_data {
 	int page_order;
 	int nr_entries;
 	unsigned long offset;
+	int null_mapped;
 };
 
 struct req_iterator {

commit 56c451f4b583ccdf80c9e676179c9cb49de86745
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Dec 18 14:49:37 2008 +0900

    [SCSI] block: fix the partial mappings with struct rq_map_data
    
    This fixes bio_copy_user_iov to properly handle the partial mappings
    with struct rq_map_data (which only sg uses for now but st and osst
    will shortly). It adds the offset member to struct rq_map_data and
    changes blk_rq_map_user to update it so that bio_copy_user_iov can add
    an appropriate page frame via bio_add_pc_page().
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7035cec583b6..811e5342c452 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -690,6 +690,7 @@ struct rq_map_data {
 	struct page **pages;
 	int page_order;
 	int nr_entries;
+	unsigned long offset;
 };
 
 struct req_iterator {

commit b374d18a4bfce705e4a99ae9f501b53e86ecb283
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 31 10:05:07 2008 +0100

    block: get rid of elevator_t typedef
    
    Just use struct elevator_queue everywhere instead.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 41bbadfd17f6..7035cec583b6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -26,7 +26,6 @@ struct scsi_ioctl_command;
 
 struct request_queue;
 struct elevator_queue;
-typedef struct elevator_queue elevator_t;
 struct request_pm_state;
 struct blk_trace;
 struct request;
@@ -313,7 +312,7 @@ struct request_queue
 	 */
 	struct list_head	queue_head;
 	struct request		*last_merge;
-	elevator_t		*elevator;
+	struct elevator_queue	*elevator;
 
 	/*
 	 * the queue request freelist, one for reads and one for writes

commit 58eea927d2de43dc6f03d1ba2c46e55854b31540
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Nov 28 13:32:06 2008 +0900

    block: simplify empty barrier implementation
    
    Empty barrier required special handling in __elv_next_request() to
    complete it without letting the low level driver see it.
    
    With previous changes, barrier code is now flexible enough to skip the
    BAR step using the same barrier sequence selection mechanism.  Drop
    the special handling and mask off q->ordered from start_ordered().
    
    Remove blk_empty_barrier() test which now has no user.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3c7078e0129d..41bbadfd17f6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -596,7 +596,6 @@ enum {
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
 #define blk_discard_rq(rq)	((rq)->cmd_flags & REQ_DISCARD)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
-#define blk_empty_barrier(rq)	(blk_barrier_rq(rq) && blk_fs_request(rq) && !(rq)->hard_nr_sectors)
 /* rq->queuelist of dequeued request must be list_empty() */
 #define blk_queued_rq(rq)	(!list_empty(&(rq)->queuelist))
 

commit 8f11b3e99a1136fcbb67316c3260f085299c0bff
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Nov 28 13:32:05 2008 +0900

    block: make barrier completion more robust
    
    Barrier completion had the following assumptions.
    
    * start_ordered() couldn't finish the whole sequence properly.  If all
      actions are to be skipped, q->ordseq is set correctly but the actual
      completion was never triggered thus hanging the barrier request.
    
    * Drain completion in elv_complete_request() assumed that there's
      always at least one request in the queue when drain completes.
    
    Both assumptions are true but these assumptions need to be removed to
    improve empty barrier implementation.  This patch makes the following
    changes.
    
    * Make start_ordered() use blk_ordered_complete_seq() to mark skipped
      steps complete and notify __elv_next_request() that it should fetch
      the next request if the whole barrier has completed inside
      start_ordered().
    
    * Make drain completion path in elv_complete_request() check whether
      the queue is empty.  Empty queue also indicates drain completion.
    
    * While at it, convert 0/1 return from blk_do_ordered() to false/true.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b044267009ed..3c7078e0129d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -866,10 +866,10 @@ extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
 extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
-extern int blk_do_ordered(struct request_queue *, struct request **);
+extern bool blk_do_ordered(struct request_queue *, struct request **);
 extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);
-extern void blk_ordered_complete_seq(struct request_queue *, unsigned, int);
+extern bool blk_ordered_complete_seq(struct request_queue *, unsigned, int);
 
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);

commit f671620e7d895af221bdfeda751d54fa55ed9546
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Nov 28 13:32:04 2008 +0900

    block: make every barrier action optional
    
    In all barrier sequences, the barrier write itself was always assumed
    to be issued and thus didn't have corresponding control flag.  This
    patch adds QUEUE_ORDERED_DO_BAR and unify action mask handling in
    start_ordered() such that any barrier action can be skipped.
    
    This patch doesn't introduce any visible behavior changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5c92b4432399..b044267009ed 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -526,12 +526,14 @@ enum {
 	QUEUE_ORDERED_BY_DRAIN		= 0x01,
 	QUEUE_ORDERED_BY_TAG		= 0x02,
 	QUEUE_ORDERED_DO_PREFLUSH	= 0x10,
+	QUEUE_ORDERED_DO_BAR		= 0x20,
 	QUEUE_ORDERED_DO_POSTFLUSH	= 0x40,
 	QUEUE_ORDERED_DO_FUA		= 0x80,
 
 	QUEUE_ORDERED_NONE		= 0x00,
 
-	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_BY_DRAIN,
+	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_BY_DRAIN |
+					  QUEUE_ORDERED_DO_BAR,
 	QUEUE_ORDERED_DRAIN_FLUSH	= QUEUE_ORDERED_DRAIN |
 					  QUEUE_ORDERED_DO_PREFLUSH |
 					  QUEUE_ORDERED_DO_POSTFLUSH,
@@ -539,7 +541,8 @@ enum {
 					  QUEUE_ORDERED_DO_PREFLUSH |
 					  QUEUE_ORDERED_DO_FUA,
 
-	QUEUE_ORDERED_TAG		= QUEUE_ORDERED_BY_TAG,
+	QUEUE_ORDERED_TAG		= QUEUE_ORDERED_BY_TAG |
+					  QUEUE_ORDERED_DO_BAR,
 	QUEUE_ORDERED_TAG_FLUSH		= QUEUE_ORDERED_TAG |
 					  QUEUE_ORDERED_DO_PREFLUSH |
 					  QUEUE_ORDERED_DO_POSTFLUSH,

commit 313e42999dbc0f234ca5909a236f78f082cb43b1
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Nov 28 13:32:02 2008 +0900

    block: reorganize QUEUE_ORDERED_* constants
    
    Separate out ordering type (drain,) and action masks (preflush,
    postflush, fua) from visible ordering mode selectors
    (QUEUE_ORDERED_*).  Ordering types are now named QUEUE_ORDERED_BY_*
    while action masks are named QUEUE_ORDERED_DO_*.
    
    This change is necessary to add QUEUE_ORDERED_DO_BAR and make it
    optional to improve empty barrier implementation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e9bb73ff1d64..5c92b4432399 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -523,22 +523,29 @@ enum {
 	 * TAG_FLUSH	: ordering by tag w/ pre and post flushes
 	 * TAG_FUA	: ordering by tag w/ pre flush and FUA write
 	 */
-	QUEUE_ORDERED_NONE	= 0x00,
-	QUEUE_ORDERED_DRAIN	= 0x01,
-	QUEUE_ORDERED_TAG	= 0x02,
-
-	QUEUE_ORDERED_PREFLUSH	= 0x10,
-	QUEUE_ORDERED_POSTFLUSH	= 0x20,
-	QUEUE_ORDERED_FUA	= 0x40,
-
-	QUEUE_ORDERED_DRAIN_FLUSH = QUEUE_ORDERED_DRAIN |
-			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_POSTFLUSH,
-	QUEUE_ORDERED_DRAIN_FUA	= QUEUE_ORDERED_DRAIN |
-			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_FUA,
-	QUEUE_ORDERED_TAG_FLUSH	= QUEUE_ORDERED_TAG |
-			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_POSTFLUSH,
-	QUEUE_ORDERED_TAG_FUA	= QUEUE_ORDERED_TAG |
-			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_FUA,
+	QUEUE_ORDERED_BY_DRAIN		= 0x01,
+	QUEUE_ORDERED_BY_TAG		= 0x02,
+	QUEUE_ORDERED_DO_PREFLUSH	= 0x10,
+	QUEUE_ORDERED_DO_POSTFLUSH	= 0x40,
+	QUEUE_ORDERED_DO_FUA		= 0x80,
+
+	QUEUE_ORDERED_NONE		= 0x00,
+
+	QUEUE_ORDERED_DRAIN		= QUEUE_ORDERED_BY_DRAIN,
+	QUEUE_ORDERED_DRAIN_FLUSH	= QUEUE_ORDERED_DRAIN |
+					  QUEUE_ORDERED_DO_PREFLUSH |
+					  QUEUE_ORDERED_DO_POSTFLUSH,
+	QUEUE_ORDERED_DRAIN_FUA		= QUEUE_ORDERED_DRAIN |
+					  QUEUE_ORDERED_DO_PREFLUSH |
+					  QUEUE_ORDERED_DO_FUA,
+
+	QUEUE_ORDERED_TAG		= QUEUE_ORDERED_BY_TAG,
+	QUEUE_ORDERED_TAG_FLUSH		= QUEUE_ORDERED_TAG |
+					  QUEUE_ORDERED_DO_PREFLUSH |
+					  QUEUE_ORDERED_DO_POSTFLUSH,
+	QUEUE_ORDERED_TAG_FUA		= QUEUE_ORDERED_TAG |
+					  QUEUE_ORDERED_DO_PREFLUSH |
+					  QUEUE_ORDERED_DO_FUA,
 
 	/*
 	 * Ordered operation sequence

commit 64d01dc9e1927e6535627d73f2336c75d1dd3fe2
Author: Cheng Renquan <crquan@gmail.com>
Date:   Wed Dec 3 12:41:39 2008 +0100

    block: use cancel_work_sync() instead of kblockd_flush_work()
    
    After many improvements on kblockd_flush_work, it is now identical to
    cancel_work_sync, so a direct call to cancel_work_sync is suggested.
    
    The only difference is that cancel_work_sync is a GPL symbol,
    so no non-GPL modules anymore.
    
    Signed-off-by: Cheng Renquan <crquan@gmail.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 482e9600f7a2..e9bb73ff1d64 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -978,7 +978,6 @@ static inline void put_dev_sector(Sector p)
 
 struct work_struct;
 int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
-void kblockd_flush_work(struct work_struct *work);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))

commit 88e740f1654bf28565edd528a060695c1f2b5ad7
Author: Fernando Luis Vázquez Cao <fernando@oss.ntt.co.jp>
Date:   Mon Oct 27 18:44:46 2008 +0900

    block: add queue flag for paravirt frontend drivers
    
    As is the case with SSD devices, we do not want to idle in AS/CFQ when
    the block device is a paravirt front-end driver. This patch adds a flag
    (QUEUE_FLAG_VIRT) which should be used by front-end drivers such as
    virtio_blk and xen-blkfront to indicate a paravirtualized device.
    
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 031a315c0509..482e9600f7a2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -449,6 +449,7 @@ struct request_queue
 #define QUEUE_FLAG_FAIL_IO     12	/* fake timeout */
 #define QUEUE_FLAG_STACKABLE   13	/* supports request stacking */
 #define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
+#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 
 static inline int queue_is_locked(struct request_queue *q)
 {

commit f2f1fa78a155524b849edf359e42a3001ea652c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 5 14:49:18 2008 -0800

    Enforce a minimum SG_IO timeout
    
    There's no point in having too short SG_IO timeouts, since if the
    command does end up timing out, we'll end up through the reset sequence
    that is several seconds long in order to abort the command that timed
    out.
    
    As a result, shorter timeouts than a few seconds simply do not make
    sense, as the recovery would be longer than the timeout itself.
    
    Add a BLK_MIN_SG_TIMEOUT to match the existign BLK_DEFAULT_SG_TIMEOUT.
    
    Suggested-by: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Cc: Jeff Garzik <jeff@garzik.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6dcd30d806cd..031a315c0509 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -662,6 +662,7 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
  * default timeout for SG_IO if none specified
  */
 #define BLK_DEFAULT_SG_TIMEOUT	(60 * HZ)
+#define BLK_MIN_SG_TIMEOUT	(7 * HZ)
 
 #ifdef CONFIG_BOUNCE
 extern int init_emergency_isa_pool(void);

commit 0e435ac26e3f951d83338ed3d4ab7dc0fe0055bc
Author: Milan Broz <mbroz@redhat.com>
Date:   Wed Dec 3 12:55:08 2008 +0100

    block: fix setting of max_segment_size and seg_boundary mask
    
    Fix setting of max_segment_size and seg_boundary mask for stacked md/dm
    devices.
    
    When stacking devices (LVM over MD over SCSI) some of the request queue
    parameters are not set up correctly in some cases by default, namely
    max_segment_size and and seg_boundary mask.
    
    If you create MD device over SCSI, these attributes are zeroed.
    
    Problem become when there is over this mapping next device-mapper mapping
    - queue attributes are set in DM this way:
    
    request_queue   max_segment_size  seg_boundary_mask
    SCSI                65536             0xffffffff
    MD RAID1                0                      0
    LVM                 65536                 -1 (64bit)
    
    Unfortunately bio_add_page (resp.  bio_phys_segments) calculates number of
    physical segments according to these parameters.
    
    During the generic_make_request() is segment cout recalculated and can
    increase bio->bi_phys_segments count over the allowed limit.  (After
    bio_clone() in stack operation.)
    
    Thi is specially problem in CCISS driver, where it produce OOPS here
    
        BUG_ON(creq->nr_phys_segments > MAXSGENTRIES);
    
    (MAXSEGENTRIES is 31 by default.)
    
    Sometimes even this command is enough to cause oops:
    
      dd iflag=direct if=/dev/<vg>/<lv> of=/dev/null bs=128000 count=10
    
    This command generates bios with 250 sectors, allocated in 32 4k-pages
    (last page uses only 1024 bytes).
    
    For LVM layer, it allocates bio with 31 segments (still OK for CCISS),
    unfortunatelly on lower layer it is recalculated to 32 segments and this
    violates CCISS restriction and triggers BUG_ON().
    
    The patch tries to fix it by:
    
     * initializing attributes above in queue request constructor
       blk_queue_make_request()
    
     * make sure that blk_queue_stack_limits() inherits setting
    
     (DM uses its own function to set the limits because it
     blk_queue_stack_limits() was introduced later.  It should probably switch
     to use generic stack limit function too.)
    
     * sets the default seg_boundary value in one place (blkdev.h)
    
     * use this mask as default in DM (instead of -1, which differs in 64bit)
    
    Bugs related to this:
    https://bugzilla.redhat.com/show_bug.cgi?id=471639
    http://bugzilla.kernel.org/show_bug.cgi?id=8672
    
    Signed-off-by: Milan Broz <mbroz@redhat.com>
    Reviewed-by: Alasdair G Kergon <agk@redhat.com>
    Cc: Neil Brown <neilb@suse.de>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Mike Miller <mike.miller@hp.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9cc7cc5fdce1..6dcd30d806cd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -918,6 +918,8 @@ extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define MAX_SEGMENT_SIZE	65536
 
+#define BLK_SEG_BOUNDARY_MASK	0xFFFFFFFFUL
+
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
 static inline int queue_hardsect_size(struct request_queue *q)

commit 53a08807c01989c6847bb135d8d43f61c5dfdda5
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 3 12:41:26 2008 +0100

    block: internal dequeue shouldn't start timer
    
    blkdev_dequeue_request() and elv_dequeue_request() are equivalent and
    both start the timeout timer.  Barrier code dequeues the original
    barrier request but doesn't passes the request itself to lower level
    driver, only broken down proxy requests; however, as the original
    barrier code goes through the same dequeue path and timeout timer is
    started on it.  If barrier sequence takes long enough, this timer
    expires but the low level driver has no idea about this request and
    oops follows.
    
    Timeout timer shouldn't have been started on the original barrier
    request as it never goes through actual IO.  This patch unexports
    elv_dequeue_request(), which has no external user anyway, and makes it
    operate on elevator proper w/o adding the timer and make
    blkdev_dequeue_request() call elv_dequeue_request() and add timer.
    Internal users which don't pass the request to driver - barrier code
    and end_that_request_last() - are converted to use
    elv_dequeue_request().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Mike Anderson <andmike@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a135256b272c..9cc7cc5fdce1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -786,6 +786,8 @@ static inline void blk_run_address_space(struct address_space *mapping)
 		blk_run_backing_dev(mapping->backing_dev_info, NULL);
 }
 
+extern void blkdev_dequeue_request(struct request *req);
+
 /*
  * blk_end_request() and friends.
  * __blk_end_request() and end_request() must be called with
@@ -820,11 +822,6 @@ extern void blk_update_request(struct request *rq, int error,
 extern unsigned int blk_rq_bytes(struct request *rq);
 extern unsigned int blk_rq_cur_bytes(struct request *rq);
 
-static inline void blkdev_dequeue_request(struct request *req)
-{
-	elv_dequeue_request(req->q, req);
-}
-
 /*
  * Access functions for manipulating queue properties
  */

commit 90b8f2824ce68dd87d304641a1d5a048dfff39f5
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 2 10:43:36 2008 -0500

    [PATCH] end of methods switch: remove the old ones
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b573186ff1a1..a135256b272c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1057,15 +1057,7 @@ static inline int blk_integrity_rq(struct request *rq)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
-struct file;
-struct inode;
-
 struct block_device_operations {
-	int (*__open) (struct inode *, struct file *);
-	int (*__release) (struct inode *, struct file *);
-	int (*__ioctl) (struct inode *, struct file *, unsigned, unsigned long);
-	long (*__unlocked_ioctl) (struct file *, unsigned, unsigned long);
-	long (*__compat_ioctl) (struct file *, unsigned, unsigned long);
 	int (*open) (struct block_device *, fmode_t);
 	int (*release) (struct gendisk *, fmode_t);
 	int (*locked_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);

commit d4430d62fa77208824a37fe6f85ab2831d274769
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 2 09:09:22 2008 -0500

    [PATCH] beginning of methods conversion
    
    To keep the size of changesets sane we split the switch by drivers;
    to keep the damn thing bisectable we do the following:
            1) rename the affected methods, add ones with correct
    prototypes, make (few) callers handle both.  That's this changeset.
            2) for each driver convert to new methods.  *ALL* drivers
    are converted in this series.
            3) kill the old (renamed) methods.
    
    Note that it _is_ a flagday; all in-tree drivers are converted and by the
    end of this series no trace of old methods remain.  The only reason why
    we do that this way is to keep the damn thing bisectable and allow per-driver
    debugging if anything goes wrong.
    
    New methods:
            open(bdev, mode)
            release(disk, mode)
            ioctl(bdev, mode, cmd, arg)             /* Called without BKL */
            compat_ioctl(bdev, mode, cmd, arg)
            locked_ioctl(bdev, mode, cmd, arg)      /* Called with BKL, legacy */
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2bad616b9949..b573186ff1a1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1061,11 +1061,16 @@ struct file;
 struct inode;
 
 struct block_device_operations {
-	int (*open) (struct inode *, struct file *);
-	int (*release) (struct inode *, struct file *);
-	int (*ioctl) (struct inode *, struct file *, unsigned, unsigned long);
-	long (*unlocked_ioctl) (struct file *, unsigned, unsigned long);
-	long (*compat_ioctl) (struct file *, unsigned, unsigned long);
+	int (*__open) (struct inode *, struct file *);
+	int (*__release) (struct inode *, struct file *);
+	int (*__ioctl) (struct inode *, struct file *, unsigned, unsigned long);
+	long (*__unlocked_ioctl) (struct file *, unsigned, unsigned long);
+	long (*__compat_ioctl) (struct file *, unsigned, unsigned long);
+	int (*open) (struct block_device *, fmode_t);
+	int (*release) (struct gendisk *, fmode_t);
+	int (*locked_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
+	int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
+	int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 	int (*direct_access) (struct block_device *, sector_t,
 						void **, unsigned long *);
 	int (*media_changed) (struct gendisk *);

commit 633a08b81206122469365b4c72eaeb71f04f2cb4
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Aug 29 20:34:12 2007 -0400

    [PATCH] introduce __blkdev_driver_ioctl()
    
    Analog of blkdev_driver_ioctl() with sane arguments.  For
    now uses fake struct file, by the end of the series it won't
    and blkdev_driver_ioctl() will become a wrapper around it.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 48ec8862a11a..2bad616b9949 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1074,6 +1074,8 @@ struct block_device_operations {
 	struct module *owner;
 };
 
+extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
+				 unsigned long);
 #else /* CONFIG_BLOCK */
 /*
  * stubs for when the block layer is configured out

commit 08f85851215100d0eebf026810955ee6ad456c38
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 8 13:26:20 2007 -0400

    [PATCH] move block_device_operations to blkdev.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 48f41b991adb..48ec8862a11a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1057,6 +1057,23 @@ static inline int blk_integrity_rq(struct request *rq)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
+struct file;
+struct inode;
+
+struct block_device_operations {
+	int (*open) (struct inode *, struct file *);
+	int (*release) (struct inode *, struct file *);
+	int (*ioctl) (struct inode *, struct file *, unsigned, unsigned long);
+	long (*unlocked_ioctl) (struct file *, unsigned, unsigned long);
+	long (*compat_ioctl) (struct file *, unsigned, unsigned long);
+	int (*direct_access) (struct block_device *, sector_t,
+						void **, unsigned long *);
+	int (*media_changed) (struct gendisk *);
+	int (*revalidate_disk) (struct gendisk *);
+	int (*getgeo)(struct block_device *, struct hd_geometry *);
+	struct module *owner;
+};
+
 #else /* CONFIG_BLOCK */
 /*
  * stubs for when the block layer is configured out

commit 74f3c8aff36ad6552ea609c8b20bfd588fa16f38
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Aug 27 15:38:10 2007 -0400

    [PATCH] switch scsi_cmd_ioctl() to passing fmode_t
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8945c30e9936..48f41b991adb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -717,8 +717,8 @@ extern void blk_plug_device(struct request_queue *);
 extern void blk_plug_device_unlocked(struct request_queue *);
 extern int blk_remove_plug(struct request_queue *);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
-extern int scsi_cmd_ioctl(struct file *, struct request_queue *,
-			  struct gendisk *, unsigned int, void __user *);
+extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
+			  unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
 			 struct scsi_ioctl_command __user *);
 

commit e915e872ed921d707bc32b3f2184d43abfa8c9e2
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Sep 2 17:16:41 2008 -0400

    [PATCH] switch sg_scsi_ioctl() to passing fmode_t
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a4413ec3cb3a..8945c30e9936 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -719,8 +719,8 @@ extern int blk_remove_plug(struct request_queue *);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct request_queue *,
 			  struct gendisk *, unsigned int, void __user *);
-extern int sg_scsi_ioctl(struct file *, struct request_queue *,
-		struct gendisk *, struct scsi_ioctl_command __user *);
+extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
+			 struct scsi_ioctl_command __user *);
 
 /*
  * Temporary export, until SCSI gets fixed up.

commit aeb5d727062a0238a2f96c9c380fbd2be4640c6f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Sep 2 15:28:45 2008 -0400

    [PATCH] introduce fmode_t, do annotations
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b4fe68fe3a57..a4413ec3cb3a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -910,7 +910,8 @@ static inline int sb_issue_discard(struct super_block *sb,
 * command filter functions
 */
 extern int blk_verify_command(struct blk_cmd_filter *filter,
-			      unsigned char *cmd, int has_write_perm);
+			      unsigned char *cmd, fmode_t has_write_perm);
+extern void blk_unregister_filter(struct gendisk *disk);
 extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define MAX_PHYS_SEGMENTS 128

commit c53dbf54863e7f3b0b8810dda2bdd0290006bdac
Merge: b73b636e8987 f73e2d13a16c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 17 09:29:55 2008 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-linus' of git://git.kernel.dk/linux-2.6-block:
      block: remove __generic_unplug_device() from exports
      block: move q->unplug_work initialization
      blktrace: pass zfcp driver data
      blktrace: add support for driver data
      block: fix current kernel-doc warnings
      block: only call ->request_fn when the queue is not stopped
      block: simplify string handling in elv_iosched_store()
      block: fix kernel-doc for blk_alloc_devt()
      block: fix nr_phys_segments miscalculation bug
      block: add partition attribute for partition number
      block: add BIG FAT WARNING to CONFIG_DEBUG_BLOCK_EXT_DEVT
      softirq: Add support for triggering softirq work on softirqs.

commit f73e2d13a16cc88c4faa4729967f92bfeec8a142
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 17 14:03:08 2008 +0200

    block: remove __generic_unplug_device() from exports
    
    The only out-of-core user is IDE, and that should be using
    blk_start_queueing() instead.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a92d9e4ea96e..8eed8b15f992 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -856,7 +856,6 @@ extern void blk_ordered_complete_seq(struct request_queue *, unsigned, int);
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern void generic_unplug_device(struct request_queue *);
-extern void __generic_unplug_device(struct request_queue *);
 extern long nr_blockdev_pages(void);
 
 int blk_get_queue(struct request_queue *);

commit 6000a368cd8e6da1caf101411bdb494cd6fb8b09
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Tue Aug 19 18:45:30 2008 -0500

    [SCSI] block: separate failfast into multiple bits.
    
    Multipath is best at handling transport errors. If it gets a device
    error then there is not much the multipath layer can do. It will just
    access the same device but from a different path.
    
    This patch breaks up failfast into device, transport and driver errors.
    The multipath layers (md and dm mutlipath) only ask the lower levels to
    fast fail transport errors. The user of failfast, read ahead, will ask
    to fast fail on all errors.
    
    Note that blk_noretry_request will return true if any failfast bit
    is set. This allows drivers that do not support the multipath failfast
    bits to continue to fail on any failfast error like before. Drivers
    like scsi that are able to fail fast specific errors can check
    for the specific fail fast type. In the next patch I will convert
    scsi.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a92d9e4ea96e..f3491d225268 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -87,7 +87,9 @@ enum {
  */
 enum rq_flag_bits {
 	__REQ_RW,		/* not set, read. set, write */
-	__REQ_FAILFAST,		/* no low level driver retries */
+	__REQ_FAILFAST_DEV,	/* no driver retries of device errors */
+	__REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
+	__REQ_FAILFAST_DRIVER,	/* no driver retries of driver errors */
 	__REQ_DISCARD,		/* request to discard sectors */
 	__REQ_SORTED,		/* elevator knows about this request */
 	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
@@ -111,8 +113,10 @@ enum rq_flag_bits {
 };
 
 #define REQ_RW		(1 << __REQ_RW)
+#define REQ_FAILFAST_DEV	(1 << __REQ_FAILFAST_DEV)
+#define REQ_FAILFAST_TRANSPORT	(1 << __REQ_FAILFAST_TRANSPORT)
+#define REQ_FAILFAST_DRIVER	(1 << __REQ_FAILFAST_DRIVER)
 #define REQ_DISCARD	(1 << __REQ_DISCARD)
-#define REQ_FAILFAST	(1 << __REQ_FAILFAST)
 #define REQ_SORTED	(1 << __REQ_SORTED)
 #define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
 #define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
@@ -560,7 +564,12 @@ enum {
 #define blk_special_request(rq)	((rq)->cmd_type == REQ_TYPE_SPECIAL)
 #define blk_sense_request(rq)	((rq)->cmd_type == REQ_TYPE_SENSE)
 
-#define blk_noretry_request(rq)	((rq)->cmd_flags & REQ_FAILFAST)
+#define blk_failfast_dev(rq)	((rq)->cmd_flags & REQ_FAILFAST_DEV)
+#define blk_failfast_transport(rq) ((rq)->cmd_flags & REQ_FAILFAST_TRANSPORT)
+#define blk_failfast_driver(rq)	((rq)->cmd_flags & REQ_FAILFAST_DRIVER)
+#define blk_noretry_request(rq)	(blk_failfast_dev(rq) ||	\
+				 blk_failfast_transport(rq) ||	\
+				 blk_failfast_driver(rq))
 #define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
 
 #define blk_account_rq(rq)	(blk_rq_started(rq) && (blk_fs_request(rq) || blk_discard_rq(rq))) 

commit b02739b01c5309d74a59859f2ce92c931d1f1955
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Thu Oct 2 18:47:49 2008 +0200

    block: gendisk integrity wrapper
    
    This is a wrapper for accessing a gendisk's integrity bits.  It allows
    the integrity support in MD to be compiled with BLK_DEV_INTEGRITY off.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 00d340b0f758..a92d9e4ea96e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1022,6 +1022,11 @@ struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
 	return bdev->bd_disk->integrity;
 }
 
+static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
+{
+	return disk->integrity;
+}
+
 static inline int blk_integrity_rq(struct request *rq)
 {
 	if (rq->bio == NULL)
@@ -1036,6 +1041,7 @@ static inline int blk_integrity_rq(struct request *rq)
 #define blk_rq_count_integrity_sg(a)		(0)
 #define blk_rq_map_integrity_sg(a, b)		(0)
 #define bdev_get_integrity(a)			(0)
+#define blk_get_integrity(a)			(0)
 #define blk_integrity_compare(a, b)		(0)
 #define blk_integrity_register(a, b)		(0)
 #define blk_integrity_unregister(a)		do { } while (0);

commit ad7fce93147d32ae53d25d9ea1a8ba31a239deee
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Oct 1 03:38:39 2008 -0400

    block: Switch blk_integrity_compare from bdev to gendisk
    
    The DM and MD integrity support now depends on being able to use
    gendisks instead of block_devices when comparing integrity profiles.
    Change function parameters accordingly.
    
    Also update comparison logic so that two NULL profiles are a valid
    configuration.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bc693f5c3886..00d340b0f758 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1012,7 +1012,7 @@ struct blk_integrity {
 
 extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
 extern void blk_integrity_unregister(struct gendisk *);
-extern int blk_integrity_compare(struct block_device *, struct block_device *);
+extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
 extern int blk_rq_map_integrity_sg(struct request *, struct scatterlist *);
 extern int blk_rq_count_integrity_sg(struct request *);
 

commit b04accc425d52ca59699290661e0dfd09b0feeeb
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Oct 2 12:53:22 2008 +0200

    block: revert part of d7533ad0e132f92e75c1b2eb7c26387b25a583c1
    
    We need bdev_get_integrity() to support the pending md/dm patches.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bfc18e497c7f..bc693f5c3886 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1016,6 +1016,12 @@ extern int blk_integrity_compare(struct block_device *, struct block_device *);
 extern int blk_rq_map_integrity_sg(struct request *, struct scatterlist *);
 extern int blk_rq_count_integrity_sg(struct request *);
 
+static inline
+struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
+{
+	return bdev->bd_disk->integrity;
+}
+
 static inline int blk_integrity_rq(struct request *rq)
 {
 	if (rq->bio == NULL)
@@ -1029,6 +1035,7 @@ static inline int blk_integrity_rq(struct request *rq)
 #define blk_integrity_rq(rq)			(0)
 #define blk_rq_count_integrity_sg(a)		(0)
 #define blk_rq_map_integrity_sg(a, b)		(0)
+#define bdev_get_integrity(a)			(0)
 #define blk_integrity_compare(a, b)		(0)
 #define blk_integrity_register(a, b)		(0)
 #define blk_integrity_unregister(a)		do { } while (0);

commit d00e29fd99dd63d1c51917604e35dee824ed567f
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Wed Oct 1 10:14:46 2008 -0400

    block: remove end_{queued|dequeued}_request()
    
    This patch removes end_queued_request() and end_dequeued_request(),
    which are no longer used.
    
    As a results, users of __end_request() became only end_request().
    So the actual code in __end_request() is moved to end_request()
    and __end_request() is removed.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9e0ee1a8254e..bfc18e497c7f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -793,8 +793,6 @@ extern int __blk_end_request(struct request *rq, int error,
 extern int blk_end_bidi_request(struct request *rq, int error,
 				unsigned int nr_bytes, unsigned int bidi_bytes);
 extern void end_request(struct request *, int);
-extern void end_queued_request(struct request *, int);
-extern void end_dequeued_request(struct request *, int);
 extern int blk_end_request_callback(struct request *rq, int error,
 				unsigned int nr_bytes,
 				int (drv_callback)(struct request *));

commit ef9e3facdf1fe1228721a7c295a76d1b7a0e57ec
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Wed Oct 1 16:12:15 2008 +0200

    block: add lld busy state exporting interface
    
    This patch adds an new interface, blk_lld_busy(), to check lld's
    busy state from the block layer.
    blk_lld_busy() calls down into low-level drivers for the checking
    if the drivers set q->lld_busy_fn() using blk_queue_lld_busy().
    
    This resolves a performance problem on request stacking devices below.
    
    Some drivers like scsi mid layer stop dispatching request when
    they detect busy state on its low-level device like host/target/device.
    It allows other requests to stay in the I/O scheduler's queue
    for a chance of merging.
    
    Request stacking drivers like request-based dm should follow
    the same logic.
    However, there is no generic interface for the stacked device
    to check if the underlying device(s) are busy.
    If the request stacking driver dispatches and submits requests to
    the busy underlying device, the requests will stay in
    the underlying device's queue without a chance of merging.
    This causes performance problem on burst I/O load.
    
    With this patch, busy state of the underlying device is exported
    via q->lld_busy_fn().  So the request stacking driver can check it
    and stop dispatching requests if busy.
    
    The underlying device driver must return the busy state appropriately:
        1: when the device driver can't process requests immediately.
        0: when the device driver can process requests immediately,
           including abnormal situations where the device driver needs
           to kill all requests.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0cf3e619fb21..9e0ee1a8254e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -269,6 +269,7 @@ typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
 typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
+typedef int (lld_busy_fn) (struct request_queue *q);
 
 enum blk_eh_timer_return {
 	BLK_EH_NOT_HANDLED,
@@ -325,6 +326,7 @@ struct request_queue
 	softirq_done_fn		*softirq_done_fn;
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
+	lld_busy_fn		*lld_busy_fn;
 
 	/*
 	 * Dispatch queue sorting
@@ -699,6 +701,7 @@ extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
+extern int blk_lld_busy(struct request_queue *q);
 extern int blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
 extern void blk_plug_device(struct request_queue *);
@@ -835,6 +838,7 @@ extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
 extern int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size);
+extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);

commit a68bbddba486020c9c74825ce90c4c1ec463e0e8
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Sep 24 13:03:33 2008 +0200

    block: add queue flag for SSD/non-rotational devices
    
    We don't want to idle in AS/CFQ if the device doesn't have a seek
    penalty. So add a QUEUE_FLAG_NONROT to indicate a non-rotational
    device, low level drivers should set this flag upon discovery of
    an SSD or similar device type.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 86f77ef127f4..0cf3e619fb21 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -442,6 +442,7 @@ struct request_queue
 #define QUEUE_FLAG_SAME_COMP   11	/* force complete on same CPU */
 #define QUEUE_FLAG_FAIL_IO     12	/* fake timeout */
 #define QUEUE_FLAG_STACKABLE   13	/* supports request stacking */
+#define QUEUE_FLAG_NONROT      14	/* non-rotational device (SSD) */
 
 static inline int queue_is_locked(struct request_queue *q)
 {
@@ -547,6 +548,7 @@ enum {
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+#define blk_queue_nonrot(q)	test_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
 #define blk_queue_stackable(q)	\
 	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)

commit 4ee5eaf4516a60f8ef64d3c246c64c6be0cf8c3a
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Thu Sep 18 10:46:13 2008 -0400

    block: add a queue flag for request stacking support
    
    This patch adds a queue flag to indicate the block device can be
    used for request stacking.
    
    Request stacking drivers need to stack their devices on top of
    only devices of which q->request_fn is functional.
    Since bio stacking drivers (e.g. md, loop) basically initialize
    their queue using blk_alloc_queue() and don't set q->request_fn,
    the check of (q->request_fn == NULL) looks enough for that purpose.
    
    However, dm will become both types of stacking driver (bio-based and
    request-based).  And dm will always set q->request_fn even if the dm
    device is bio-based of which q->request_fn is not functional actually.
    So we need something else to distinguish the type of the device.
    Adding a queue flag is a solution for that.
    
    The reason why dm always sets q->request_fn is to keep
    the compatibility of dm user-space tools.
    Currently, all dm user-space tools are using bio-based dm without
    specifying the type of the dm device they use.
    To use request-based dm without changing such tools, the kernel
    must decide the type of the dm device automatically.
    The automatic type decision can't be done at the device creation time
    and needs to be deferred until such tools load a mapping table,
    since the actual type is decided by dm target type included in
    the mapping table.
    
    So a dm device has to be initialized using blk_init_queue()
    so that we can load either type of table.
    Then, all queue stuffs are set (e.g. q->request_fn) and we have
    no element to distinguish that it is bio-based or request-based,
    even after a table is loaded and the type of the device is decided.
    
    By the way, some stuffs of the queue (e.g. request_list, elevator)
    are needless when the dm device is used as bio-based.
    But the memory size is not so large (about 20[KB] per queue on ia64),
    so I hope the memory loss can be acceptable for bio-based dm users.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 964c246bc271..86f77ef127f4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -441,6 +441,7 @@ struct request_queue
 #define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP   11	/* force complete on same CPU */
 #define QUEUE_FLAG_FAIL_IO     12	/* fake timeout */
+#define QUEUE_FLAG_STACKABLE   13	/* supports request stacking */
 
 static inline int queue_is_locked(struct request_queue *q)
 {
@@ -547,6 +548,8 @@ enum {
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
+#define blk_queue_stackable(q)	\
+	test_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)
 
 #define blk_fs_request(rq)	((rq)->cmd_type == REQ_TYPE_FS)
 #define blk_pc_request(rq)	((rq)->cmd_type == REQ_TYPE_BLOCK_PC)

commit 82124d60354846623a4b94af335717a5e142a074
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Thu Sep 18 10:45:38 2008 -0400

    block: add request submission interface
    
    This patch adds blk_insert_cloned_request(), a generic request
    submission interface for request stacking drivers.
    Request-based dm will use it to submit their clones to underlying
    devices.
    
    blk_rq_check_limits() is also added because it is possible that
    the lower queue has stronger limitations than the upper queue
    if multiple drivers are stacking at request-level.
    Not only for blk_insert_cloned_request()'s internal use, the function
    will be used by request-based dm when the queue limitation is
    modified (e.g. by replacing dm's table).
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e82a84c9f37a..964c246bc271 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -693,6 +693,9 @@ extern void __blk_put_request(struct request_queue *, struct request *);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
+extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
+extern int blk_insert_cloned_request(struct request_queue *q,
+				     struct request *rq);
 extern void blk_plug_device(struct request_queue *);
 extern void blk_plug_device_unlocked(struct request_queue *);
 extern int blk_remove_plug(struct request_queue *);

commit 32fab448e5e86694beade415e750363538ea5f49
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Thu Sep 18 10:45:09 2008 -0400

    block: add request update interface
    
    This patch adds blk_update_request(), which updates struct request
    with completing its data part, but doesn't complete the struct
    request itself.
    Though it looks like end_that_request_first() of older kernels,
    blk_update_request() should be used only by request stacking drivers.
    
    Request-based dm will use it in bio->bi_end_io callback to update
    the original request when a data part of a cloned request completes.
    Followings are additional background information of why request-based
    dm needs this interface.
    
      - Request stacking drivers can't use blk_end_request() directly from
        the lower driver's completion context (bio->bi_end_io or rq->end_io),
        because some device drivers (e.g. ide) may try to complete
        their request with queue lock held, and it may cause deadlock.
        See below for detailed description of possible deadlock:
        <http://marc.info/?l=linux-kernel&m=120311479108569&w=2>
    
      - To solve that, request-based dm offloads the completion of
        cloned struct request to softirq context (i.e. using
        blk_complete_request() from rq->end_io).
    
      - Though it is possible to use the same solution from bio->bi_end_io,
        it will delay the notification of bio completion to the original
        submitter.  Also, it will cause inefficient partial completion,
        because the lower driver can't perform the cloned request anymore
        and request-based dm needs to requeue and redispatch it to
        the lower driver again later.  That's not good.
    
      - So request-based dm needs blk_update_request() to perform the bio
        completion in the lower driver's completion context, which is more
        efficient.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e23b838825bd..e82a84c9f37a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -791,6 +791,8 @@ extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
 extern void blk_abort_queue(struct request_queue *);
+extern void blk_update_request(struct request *rq, int error,
+			       unsigned int nr_bytes);
 
 /*
  * blk_end_request() takes bytes instead of sectors as a complete size.

commit 9c02f2b02e29a2244e36c6e1f246080d8afc6cff
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Sep 18 09:31:53 2008 -0700

    block: cleanup some of the integrity stuff in blkdev.h
    
    Don't put functions that are only used in fs/bio-integrity.c in
    blkdev.h, it's much cleaner to just keep it in there. Also kill
    completely unused bdev_get_tag_size()
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e34999d14c16..e23b838825bd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1004,47 +1004,6 @@ extern int blk_integrity_compare(struct block_device *, struct block_device *);
 extern int blk_rq_map_integrity_sg(struct request *, struct scatterlist *);
 extern int blk_rq_count_integrity_sg(struct request *);
 
-static inline unsigned short blk_integrity_tuple_size(struct blk_integrity *bi)
-{
-	if (bi)
-		return bi->tuple_size;
-
-	return 0;
-}
-
-static inline struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
-{
-	return bdev->bd_disk->integrity;
-}
-
-static inline unsigned int bdev_get_tag_size(struct block_device *bdev)
-{
-	struct blk_integrity *bi = bdev_get_integrity(bdev);
-
-	if (bi)
-		return bi->tag_size;
-
-	return 0;
-}
-
-static inline int bdev_integrity_enabled(struct block_device *bdev, int rw)
-{
-	struct blk_integrity *bi = bdev_get_integrity(bdev);
-
-	if (bi == NULL)
-		return 0;
-
-	if (rw == READ && bi->verify_fn != NULL &&
-	    (bi->flags & INTEGRITY_FLAG_READ))
-		return 1;
-
-	if (rw == WRITE && bi->generate_fn != NULL &&
-	    (bi->flags & INTEGRITY_FLAG_WRITE))
-		return 1;
-
-	return 0;
-}
-
 static inline int blk_integrity_rq(struct request *rq)
 {
 	if (rq->bio == NULL)
@@ -1058,8 +1017,6 @@ static inline int blk_integrity_rq(struct request *rq)
 #define blk_integrity_rq(rq)			(0)
 #define blk_rq_count_integrity_sg(a)		(0)
 #define blk_rq_map_integrity_sg(a, b)		(0)
-#define bdev_get_integrity(a)			(0)
-#define bdev_get_tag_size(a)			(0)
 #define blk_integrity_compare(a, b)		(0)
 #define blk_integrity_register(a, b)		(0)
 #define blk_integrity_unregister(a)		do { } while (0);

commit 581d4e28d9195aa8b2231383dbabc288988d615e
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sun Sep 14 05:56:33 2008 -0700

    block: add fault injection mechanism for faking request timeouts
    
    Only works for the generic request timer handling. Allows one to
    sporadically ignore request completions, thus exercising the timeout
    handling.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b47767c72ce3..e34999d14c16 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -440,6 +440,7 @@ struct request_queue
 #define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP   11	/* force complete on same CPU */
+#define QUEUE_FLAG_FAIL_IO     12	/* fake timeout */
 
 static inline int queue_is_locked(struct request_queue *q)
 {

commit 3e6053d76dcbd92b2f9f4ad5ece9bce83149523e
Author: Hugh Dickins <hugh@veritas.com>
Date:   Thu Sep 11 10:57:55 2008 +0200

    block: adjust blkdev_issue_discard for swap
    
    Two mods to blkdev_issue_discard(), thinking ahead to its use on swap:
    
    1. Add gfp_mask argument, so swap allocation can use it where GFP_KERNEL
       might deadlock but GFP_NOIO is safe.
    
    2. Enlarge nr_sects argument from unsigned to sector_t: unsigned long is
       enough to cover a whole swap area, but sector_t suits any partition.
    
    Change sb_issue_discard()'s nr_blocks to sector_t too; but no need seen
    for a gfp_mask there, just pass GFP_KERNEL down to blkdev_issue_discard().
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 37781d6fe045..b47767c72ce3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -16,6 +16,7 @@
 #include <linux/bio.h>
 #include <linux/module.h>
 #include <linux/stringify.h>
+#include <linux/gfp.h>
 #include <linux/bsg.h>
 #include <linux/smp.h>
 
@@ -873,15 +874,15 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 }
 
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
-extern int blkdev_issue_discard(struct block_device *, sector_t sector,
-				unsigned nr_sects);
+extern int blkdev_issue_discard(struct block_device *,
+				sector_t sector, sector_t nr_sects, gfp_t);
 
 static inline int sb_issue_discard(struct super_block *sb,
-				   sector_t block, unsigned nr_blocks)
+				   sector_t block, sector_t nr_blocks)
 {
 	block <<= (sb->s_blocksize_bits - 9);
 	nr_blocks <<= (sb->s_blocksize_bits - 9);
-	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks);
+	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks, GFP_KERNEL);
 }
 
 /*

commit 11914a53d2ec2974a565311af327b8983d8c820d
Author: Mike Anderson <andmike@linux.vnet.ibm.com>
Date:   Sat Sep 13 20:31:27 2008 +0200

    block: Add interface to abort queued requests
    
    Signed-off-by: Mike Anderson <andmike@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 067f28b80072..37781d6fe045 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -788,6 +788,7 @@ extern int blk_end_request_callback(struct request *rq, int error,
 extern void blk_complete_request(struct request *);
 extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
+extern void blk_abort_queue(struct request_queue *);
 
 /*
  * blk_end_request() takes bytes instead of sectors as a complete size.

commit 242f9dcb8ba6f68fcd217a119a7648a4f69290e9
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sun Sep 14 05:55:09 2008 -0700

    block: unify request timeout handling
    
    Right now SCSI and others do their own command timeout handling.
    Move those bits to the block layer.
    
    Instead of having a timer per command, we try to be a bit more clever
    and simply have one per-queue. This avoids the overhead of having to
    tear down and setup a timer for each command, so it will result in a lot
    less timer fiddling.
    
    Signed-off-by: Mike Anderson <andmike@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9c2549260427..067f28b80072 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -147,6 +147,7 @@ struct request {
 
 	unsigned int cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
+	unsigned long atomic_flags;
 
 	/* Maintain bio traversal state for part by part I/O submission.
 	 * hard_* are block layer internals, no driver should touch them!
@@ -214,6 +215,8 @@ struct request {
 	void *data;
 	void *sense;
 
+	unsigned long deadline;
+	struct list_head timeout_list;
 	unsigned int timeout;
 	int retries;
 
@@ -266,6 +269,14 @@ typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 
+enum blk_eh_timer_return {
+	BLK_EH_NOT_HANDLED,
+	BLK_EH_HANDLED,
+	BLK_EH_RESET_TIMER,
+};
+
+typedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);
+
 enum blk_queue_state {
 	Queue_down,
 	Queue_up,
@@ -311,6 +322,7 @@ struct request_queue
 	merge_bvec_fn		*merge_bvec_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
+	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 
 	/*
@@ -386,6 +398,10 @@ struct request_queue
 	unsigned int		nr_sorted;
 	unsigned int		in_flight;
 
+	unsigned int		rq_timeout;
+	struct timer_list	timeout;
+	struct list_head	timeout_list;
+
 	/*
 	 * sg stuff
 	 */
@@ -770,6 +786,8 @@ extern int blk_end_request_callback(struct request *rq, int error,
 				unsigned int nr_bytes,
 				int (drv_callback)(struct request *));
 extern void blk_complete_request(struct request *);
+extern void __blk_complete_request(struct request *);
+extern void blk_abort_request(struct request *);
 
 /*
  * blk_end_request() takes bytes instead of sectors as a complete size.
@@ -811,6 +829,8 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern void blk_queue_set_discard(struct request_queue *, prepare_discard_fn *);
+extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
+extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
 extern int blk_do_ordered(struct request_queue *, struct request **);

commit 879040742cf09f2360a9ac41846288707e4e567c
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Aug 28 15:05:58 2008 +0900

    block: add blk_rq_aligned helper function
    
    This adds blk_rq_aligned helper function to see if alignment and
    padding requirement is satisfied for DMA transfer. This also converts
    blk_rq_map_kern and __blk_rq_map_user to use the helper function.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 358ac423ed2f..9c2549260427 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -899,6 +899,13 @@ static inline int queue_dma_alignment(struct request_queue *q)
 	return q ? q->dma_alignment : 511;
 }
 
+static inline int blk_rq_aligned(struct request_queue *q, void *addr,
+				 unsigned int len)
+{
+	unsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;
+	return !((unsigned long)addr & alignment) && !(len & alignment);
+}
+
 /* assumes size > 256 */
 static inline unsigned int blksize_bits(unsigned int size)
 {

commit 152e283fdfea0cd11e297d982378b55937842dde
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Aug 28 16:17:06 2008 +0900

    block: introduce struct rq_map_data to use reserved pages
    
    This patch introduces struct rq_map_data to enable bio_copy_use_iov()
    use reserved pages.
    
    Currently, bio_copy_user_iov allocates bounce pages but
    drivers/scsi/sg.c wants to allocate pages by itself and use
    them. struct rq_map_data can be used to pass allocated pages to
    bio_copy_user_iov.
    
    The current users of bio_copy_user_iov simply passes NULL (they don't
    want to use pre-allocated pages).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Douglas Gilbert <dougg@torque.net>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 00e388d0e221..358ac423ed2f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -642,6 +642,12 @@ static inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)
 }
 #endif /* CONFIG_MMU */
 
+struct rq_map_data {
+	struct page **pages;
+	int page_order;
+	int nr_entries;
+};
+
 struct req_iterator {
 	int i;
 	struct bio *bio;
@@ -711,11 +717,13 @@ extern void __blk_run_queue(struct request_queue *);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_start_queueing(struct request_queue *);
 extern int blk_rq_map_user(struct request_queue *, struct request *,
-			   void __user *, unsigned long, gfp_t);
+			   struct rq_map_data *, void __user *, unsigned long,
+			   gfp_t);
 extern int blk_rq_unmap_user(struct bio *);
 extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
-			       struct sg_iovec *, int, unsigned int, gfp_t);
+			       struct rq_map_data *, struct sg_iovec *, int,
+			       unsigned int, gfp_t);
 extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,

commit a3bce90edd8f6cafe3f63b1a943800792e830178
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Aug 28 16:17:05 2008 +0900

    block: add gfp_mask argument to blk_rq_map_user and blk_rq_map_user_iov
    
    Currently, blk_rq_map_user and blk_rq_map_user_iov always do
    GFP_KERNEL allocation.
    
    This adds gfp_mask argument to blk_rq_map_user and blk_rq_map_user_iov
    so sg can use it (sg always does GFP_ATOMIC allocation).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Douglas Gilbert <dougg@torque.net>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 12df8efeef19..00e388d0e221 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -710,11 +710,12 @@ extern void __blk_stop_queue(struct request_queue *q);
 extern void __blk_run_queue(struct request_queue *);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_start_queueing(struct request_queue *);
-extern int blk_rq_map_user(struct request_queue *, struct request *, void __user *, unsigned long);
+extern int blk_rq_map_user(struct request_queue *, struct request *,
+			   void __user *, unsigned long, gfp_t);
 extern int blk_rq_unmap_user(struct bio *);
 extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
-			       struct sg_iovec *, int, unsigned int);
+			       struct sg_iovec *, int, unsigned int, gfp_t);
 extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,

commit ab780f1ece0dc8d5e8e8e85435acc5e4747ccda3
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Aug 26 10:25:02 2008 +0200

    block: inherit CPU completion on bio->rq and rq->rq merges
    
    Somewhat incomplete, as we do allow merges of requests and bios
    that have different completion CPUs given. This is done on the
    assumption that a larger IO is still more beneficial than CPU
    locality.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 93204bf7b297..12df8efeef19 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -545,6 +545,7 @@ enum {
 #define blk_pm_request(rq)	\
 	(blk_pm_suspend_request(rq) || blk_pm_resume_request(rq))
 
+#define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
 #define blk_sorted_rq(rq)	((rq)->cmd_flags & REQ_SORTED)
 #define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)

commit c7c22e4d5c1fdebfac4dba76de7d0338c2b0d832
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Sat Sep 13 20:26:01 2008 +0200

    block: add support for IO CPU affinity
    
    This patch adds support for controlling the IO completion CPU of
    either all requests on a queue, or on a per-request basis. We export
    a sysfs variable (rq_affinity) which, if set, migrates completions
    of requests to the CPU that originally submitted it. A bio helper
    (bio_set_completion_cpu()) is also added, so that queuers can ask
    for completion on that specific CPU.
    
    In testing, this has been show to cut the system time by as much
    as 20-40% on synthetic workloads where CPU affinity is desired.
    
    This requires a little help from the architecture, so it'll only
    work as designed for archs that are using the new generic smp
    helper infrastructure.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 10aa46c8f170..93204bf7b297 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -17,6 +17,7 @@
 #include <linux/module.h>
 #include <linux/stringify.h>
 #include <linux/bsg.h>
+#include <linux/smp.h>
 
 #include <asm/scatterlist.h>
 
@@ -139,7 +140,8 @@ enum rq_flag_bits {
  */
 struct request {
 	struct list_head queuelist;
-	struct list_head donelist;
+	struct call_single_data csd;
+	int cpu;
 
 	struct request_queue *q;
 
@@ -420,6 +422,7 @@ struct request_queue
 #define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
+#define QUEUE_FLAG_SAME_COMP   11	/* force complete on same CPU */
 
 static inline int queue_is_locked(struct request_queue *q)
 {

commit 18887ad910e56066233a07fd3cfb2fa11338b782
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jul 28 13:08:45 2008 +0200

    block: make kblockd_schedule_work() take the queue as parameter
    
    Preparatory patch for checking queuing affinity.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1adb03827bd3..10aa46c8f170 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -912,7 +912,7 @@ static inline void put_dev_sector(Sector p)
 }
 
 struct work_struct;
-int kblockd_schedule_work(struct work_struct *work);
+int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);
 void kblockd_flush_work(struct work_struct *work);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \

commit 5df97b91b5d7ed426034fcc84cb6e7cf682b8838
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Fri Aug 15 10:20:02 2008 +0200

    drop vmerge accounting
    
    Remove hw_segments field from struct bio and struct request. Without virtual
    merge accounting they have no purpose.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 490ce458b031..1adb03827bd3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -189,13 +189,6 @@ struct request {
 	 */
 	unsigned short nr_phys_segments;
 
-	/* Number of scatter-gather addr+len pairs after
-	 * physical and DMA remapping hardware coalescing is performed.
-	 * This is the number of scatter-gather entries the driver
-	 * will actually have to deal with after DMA mapping is done.
-	 */
-	unsigned short nr_hw_segments;
-
 	unsigned short ioprio;
 
 	void *special;

commit 766ca4428d1239a970926856c447310c9c191af2
Author: Fernando Luis Vázquez Cao <fernando@oss.ntt.co.jp>
Date:   Thu Aug 14 09:59:13 2008 +0200

    virtio_blk: use a wrapper function to access io context information of IO requests
    
    struct request has an ioprio member but it is never updated because
    currently bios do not hold io context information. The implication of
    this is that virtio_blk ends up passing useless information to the
    backend driver.
    
    That said, some IO schedulers such as CFQ do store io context
    information in struct request, but use private members for that, which
    means that that information cannot be directly accessed in a IO
    scheduler-independent way.
    
    This patch adds a function to obtain the ioprio of a request. We should
    avoid accessing ioprio directly and use this function instead, so that
    its users do not have to care about future changes in block layer
    structures or what the currently active IO controller is.
    
    This patch does not introduce any functional changes but paves the way
    for future clean-ups and enhancements.
    
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f131776f029e..490ce458b031 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -232,6 +232,11 @@ struct request {
 	struct request *next_rq;
 };
 
+static inline unsigned short req_get_ioprio(struct request *req)
+{
+	return req->ioprio;
+}
+
 /*
  * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME
  * requests. Some step values could eventually be made generic.

commit 1a8e2bddd5c29008f311613e75925fecbf522c5b
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Wed Aug 13 12:35:09 2008 +0100

    Kill REQ_TYPE_FLUSH
    
    It was only used by ps3disk, and it should probably have been
    REQ_TYPE_LINUX_BLOCK + REQ_LB_OP_FLUSH.
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e9eb35c9bf26..f131776f029e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -54,7 +54,6 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 	REQ_TYPE_PM_RESUME,		/* resume request */
 	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
-	REQ_TYPE_FLUSH,			/* flush request */
 	REQ_TYPE_SPECIAL,		/* driver defined type */
 	REQ_TYPE_LINUX_BLOCK,		/* generic block layer message */
 	/*
@@ -76,11 +75,8 @@ enum rq_cmd_type_bits {
  *
  */
 enum {
-	/*
-	 * just examples for now
-	 */
 	REQ_LB_OP_EJECT	= 0x40,		/* eject request */
-	REQ_LB_OP_FLUSH = 0x41,		/* flush device */
+	REQ_LB_OP_FLUSH = 0x41,		/* flush request */
 	REQ_LB_OP_DISCARD = 0x42,	/* discard sectors */
 };
 

commit e17fc0a1ccf88f6d4dcb363729f3141b0958c325
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Sat Aug 9 16:42:20 2008 +0100

    Allow elevators to sort/merge discard requests
    
    But blkdev_issue_discard() still emits requests which are interpreted as
    soft barriers, because naïve callers might otherwise issue subsequent
    writes to those same sectors, which might cross on the queue (if they're
    reallocated quickly enough).
    
    Callers still _can_ issue non-barrier discard requests, but they have to
    take care of queue ordering for themselves.
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 727886d25c4e..e9eb35c9bf26 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -541,7 +541,7 @@ enum {
 #define blk_noretry_request(rq)	((rq)->cmd_flags & REQ_FAILFAST)
 #define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
 
-#define blk_account_rq(rq)	(blk_rq_started(rq) && blk_fs_request(rq))
+#define blk_account_rq(rq)	(blk_rq_started(rq) && (blk_fs_request(rq) || blk_discard_rq(rq))) 
 
 #define blk_pm_suspend_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND)
 #define blk_pm_resume_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_RESUME)
@@ -598,7 +598,8 @@ static inline void blk_clear_queue_full(struct request_queue *q, int rw)
 #define RQ_NOMERGE_FLAGS	\
 	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER)
 #define rq_mergeable(rq)	\
-	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && blk_fs_request((rq)))
+	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \
+	 (blk_discard_rq(rq) || blk_fs_request((rq))))
 
 /*
  * q->prep_rq_fn return values

commit eae9acd13a8d14b50c00a961fa959606f34bbd92
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Tue Aug 5 18:08:25 2008 +0100

    Support 'discard sectors' operation in translation layer support core
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 26ececbbebe2..727886d25c4e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -81,6 +81,7 @@ enum {
 	 */
 	REQ_LB_OP_EJECT	= 0x40,		/* eject request */
 	REQ_LB_OP_FLUSH = 0x41,		/* flush device */
+	REQ_LB_OP_DISCARD = 0x42,	/* discard sectors */
 };
 
 /*

commit fb2dce862d9f9a68e6b9374579056ec9eca02a63
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Tue Aug 5 18:01:53 2008 +0100

    Add 'discard' request handling
    
    Some block devices benefit from a hint that they can forget the contents
    of certain sectors. Add basic support for this to the block core, along
    with a 'blkdev_issue_discard()' helper function which issues such
    requests.
    
    The caller doesn't get to provide an end_io functio, since
    blkdev_issue_discard() will automatically split the request up into
    multiple bios if appropriate. Neither does the function wait for
    completion -- it's expected that callers won't care about when, or even
    _if_, the request completes. It's only a hint to the device anyway. By
    definition, the file system doesn't _care_ about these sectors any more.
    
    [With feedback from OGAWA Hirofumi <hirofumi@mail.parknet.co.jp> and
    Jens Axboe <jens.axboe@oracle.com]
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e0ba018f5e88..26ececbbebe2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -89,6 +89,7 @@ enum {
 enum rq_flag_bits {
 	__REQ_RW,		/* not set, read. set, write */
 	__REQ_FAILFAST,		/* no low level driver retries */
+	__REQ_DISCARD,		/* request to discard sectors */
 	__REQ_SORTED,		/* elevator knows about this request */
 	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
 	__REQ_HARDBARRIER,	/* may not be passed by drive either */
@@ -111,6 +112,7 @@ enum rq_flag_bits {
 };
 
 #define REQ_RW		(1 << __REQ_RW)
+#define REQ_DISCARD	(1 << __REQ_DISCARD)
 #define REQ_FAILFAST	(1 << __REQ_FAILFAST)
 #define REQ_SORTED	(1 << __REQ_SORTED)
 #define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
@@ -252,6 +254,7 @@ typedef void (request_fn_proc) (struct request_queue *q);
 typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
 typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unplug_fn) (struct request_queue *);
+typedef int (prepare_discard_fn) (struct request_queue *, struct request *);
 
 struct bio_vec;
 struct bvec_merge_data {
@@ -307,6 +310,7 @@ struct request_queue
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unplug_fn		*unplug_fn;
+	prepare_discard_fn	*prepare_discard_fn;
 	merge_bvec_fn		*merge_bvec_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
@@ -546,6 +550,7 @@ enum {
 #define blk_sorted_rq(rq)	((rq)->cmd_flags & REQ_SORTED)
 #define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
+#define blk_discard_rq(rq)	((rq)->cmd_flags & REQ_DISCARD)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 #define blk_empty_barrier(rq)	(blk_barrier_rq(rq) && blk_fs_request(rq) && !(rq)->hard_nr_sectors)
 /* rq->queuelist of dequeued request must be list_empty() */
@@ -796,6 +801,7 @@ extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
+extern void blk_queue_set_discard(struct request_queue *, prepare_discard_fn *);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
 extern int blk_do_ordered(struct request_queue *, struct request **);
@@ -837,6 +843,16 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 }
 
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
+extern int blkdev_issue_discard(struct block_device *, sector_t sector,
+				unsigned nr_sects);
+
+static inline int sb_issue_discard(struct super_block *sb,
+				   sector_t block, unsigned nr_blocks)
+{
+	block <<= (sb->s_blocksize_bits - 9);
+	nr_blocks <<= (sb->s_blocksize_bits - 9);
+	return blkdev_issue_discard(sb->s_bdev, block, nr_blocks);
+}
 
 /*
 * command filter functions

commit d628eaef310533767ce68664873869c2d7f78f09
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Sat Aug 9 16:22:17 2008 +0100

    Fix up comments about matching flags between bio and rq
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 53ea933cf60b..e0ba018f5e88 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -84,7 +84,7 @@ enum {
 };
 
 /*
- * request type modified bits. first three bits match BIO_RW* bits, important
+ * request type modified bits. first two bits match BIO_RW* bits, important
  */
 enum rq_flag_bits {
 	__REQ_RW,		/* not set, read. set, write */

commit 2dc75d3c3b49c64fd26b4832a7efb75546cb3fc5
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Sep 11 14:20:23 2008 +0200

    block: disable sysfs parts of the disk command filter
    
    We still have life time issues with the sysfs command filter kobject,
    so disable it for 2.6.27 release. We can revisit this and make it work
    properly for 2.6.28, for 2.6.27 release it's too risky.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 44710d7e7bff..53ea933cf60b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -843,8 +843,6 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 */
 extern int blk_verify_command(struct blk_cmd_filter *filter,
 			      unsigned char *cmd, int has_write_perm);
-extern int blk_register_filter(struct gendisk *disk);
-extern void blk_unregister_filter(struct gendisk *disk);
 extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define MAX_PHYS_SEGMENTS 128

commit 5168c47b4c294412f079dd3cc891e0276bb0479e
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Aug 26 09:03:17 2008 +0200

    block: remove blk_queue_tag_depth() and blk_queue_tag_queue()
    
    They are unused and ->busy doesn't exist anymore.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ab247d589ad6..44710d7e7bff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -817,8 +817,6 @@ extern void blk_put_queue(struct request_queue *);
 /*
  * tag stuff
  */
-#define blk_queue_tag_depth(q)		((q)->queue_tags->busy)
-#define blk_queue_tag_queue(q)		((q)->queue_tags->busy < (q)->queue_tags->max_depth)
 #define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
 extern int blk_queue_start_tag(struct request_queue *, struct request *);
 extern struct request *blk_queue_find_tag(struct request_queue *, int);

commit 4beab5c623fef4622f9a8593f85760ff10b5a3f7
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Sat Jul 26 18:03:25 2008 +0900

    block: rename blk_scsi_cmd_filter to blk_cmd_filter
    
    Technically, the cmd_filter would be applied to other protocols though
    it's unlikely to happen. Putting SCSI stuff to request_queue is kinda
    layer violation. So let's rename it.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d2d34e2774b2..ab247d589ad6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -283,7 +283,7 @@ struct blk_queue_tag {
 #define BLK_SCSI_MAX_CMDS	(256)
 #define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
 
-struct blk_scsi_cmd_filter {
+struct blk_cmd_filter {
 	unsigned long read_ok[BLK_SCSI_CMD_PER_LONG];
 	unsigned long write_ok[BLK_SCSI_CMD_PER_LONG];
 	struct kobject kobj;
@@ -407,7 +407,7 @@ struct request_queue
 #if defined(CONFIG_BLK_DEV_BSG)
 	struct bsg_class_device bsg_dev;
 #endif
-	struct blk_scsi_cmd_filter cmd_filter;
+	struct blk_cmd_filter cmd_filter;
 };
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
@@ -843,11 +843,11 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 /*
 * command filter functions
 */
-extern int blk_verify_command(struct blk_scsi_cmd_filter *filter,
+extern int blk_verify_command(struct blk_cmd_filter *filter,
 			      unsigned char *cmd, int has_write_perm);
 extern int blk_register_filter(struct gendisk *disk);
 extern void blk_unregister_filter(struct gendisk *disk);
-extern void blk_set_cmd_filter_defaults(struct blk_scsi_cmd_filter *filter);
+extern void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter);
 
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128

commit abf5439370491dd6fbb4fe1a7939680d2a9bc9d4
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Sat Aug 16 14:10:05 2008 +0900

    block: move cmdfilter from gendisk to request_queue
    
    cmd_filter works only for the block layer SG_IO with SCSI block
    devices. It breaks scsi/sg.c, bsg, and the block layer SG_IO with SCSI
    character devices (such as st). We hit a kernel crash with them.
    
    The problem is that cmd_filter code accesses to gendisk (having struct
    blk_scsi_cmd_filter) via inode->i_bdev->bd_disk. It works for only
    SCSI block device files. With character device files, inode->i_bdev
    leads you to struct cdev. inode->i_bdev->bd_disk->blk_scsi_cmd_filter
    isn't safe.
    
    SCSI ULDs don't expose gendisk; they keep it private. bsg needs to be
    independent on any protocols. We shouldn't change ULDs to expose their
    gendisk.
    
    This patch moves struct blk_scsi_cmd_filter from gendisk to
    request_queue, a common object, which eveyone can access to.
    
    The user interface doesn't change; users can change the filters via
    /sys/block/. gendisk has a pointer to request_queue so the cmd_filter
    code accesses to struct blk_scsi_cmd_filter.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e61f22be4d0e..d2d34e2774b2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -280,6 +280,15 @@ struct blk_queue_tag {
 	atomic_t refcnt;		/* map can be shared */
 };
 
+#define BLK_SCSI_MAX_CMDS	(256)
+#define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
+
+struct blk_scsi_cmd_filter {
+	unsigned long read_ok[BLK_SCSI_CMD_PER_LONG];
+	unsigned long write_ok[BLK_SCSI_CMD_PER_LONG];
+	struct kobject kobj;
+};
+
 struct request_queue
 {
 	/*
@@ -398,6 +407,7 @@ struct request_queue
 #if defined(CONFIG_BLK_DEV_BSG)
 	struct bsg_class_device bsg_dev;
 #endif
+	struct blk_scsi_cmd_filter cmd_filter;
 };
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
@@ -833,11 +843,11 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 /*
 * command filter functions
 */
-extern int blk_verify_command(struct file *file, unsigned char *cmd);
-extern int blk_cmd_filter_verify_command(struct blk_scsi_cmd_filter *filter,
-					 unsigned char *cmd, mode_t *f_mode);
+extern int blk_verify_command(struct blk_scsi_cmd_filter *filter,
+			      unsigned char *cmd, int has_write_perm);
 extern int blk_register_filter(struct gendisk *disk);
 extern void blk_unregister_filter(struct gendisk *disk);
+extern void blk_set_cmd_filter_defaults(struct blk_scsi_cmd_filter *filter);
 
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128

commit 6c5e0c4d518a37e1d5d794c14433e80284415079
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Aug 1 20:31:32 2008 +0200

    block: add a blk_plug_device_unlocked() that grabs the queue lock
    
    blk_plug_device() must be called with the queue lock held, so callers
    often just grab and release the lock for that purpose. Add a helper
    that does just that.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 88d68081a0f1..e61f22be4d0e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -655,6 +655,7 @@ extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
 extern void blk_plug_device(struct request_queue *);
+extern void blk_plug_device_unlocked(struct request_queue *);
 extern int blk_remove_plug(struct request_queue *);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct request_queue *,

commit d442cc44c0db56e84ef6aa244a88427d2efe06cd
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Wed Jul 16 16:09:06 2008 -0400

    block: Trivial fix for blk_integrity_rq()
    
    Fail integrity check gracefully when request does not have a bio
    attached (BLOCK_PC).
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 32a441b05fd5..88d68081a0f1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -985,6 +985,9 @@ static inline int bdev_integrity_enabled(struct block_device *bdev, int rw)
 
 static inline int blk_integrity_rq(struct request *rq)
 {
+	if (rq->bio == NULL)
+		return 0;
+
 	return bio_integrity(rq->bio);
 }
 

commit 98339cbd360b77c3167db287fd611468c2c44559
Merge: e4e0fadcd929 cbbc4e818de4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 15 11:15:36 2008 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/bart/ide-2.6: (80 commits)
      ide-floppy: fix unfortunate function naming
      ide-tape: unify idetape_create_read/write_cmd
      ide: add ide_pc_intr() helper
      ide-{floppy,scsi}: read Status Register before stopping DMA engine
      ide-scsi: add more debugging to idescsi_pc_intr()
      ide-scsi: use pc->callback
      ide-floppy: add more debugging to idefloppy_pc_intr()
      ide-tape: always log debug info in idetape_pc_intr() if debugging is enabled
      ide-tape: add ide_tape_io_buffers() helper
      ide-tape: factor out DSC handling from idetape_pc_intr()
      ide-{floppy,tape}: move checking of ->failed_pc to ->callback
      ide: add ide_issue_pc() helper
      ide: add PC_FLAG_DRQ_INTERRUPT pc flag
      ide-scsi: move idescsi_map_sg() call out from idescsi_issue_pc()
      ide: add ide_transfer_pc() helper
      ide-scsi: set drive->scsi flag for devices handled by the driver
      ide-{cd,floppy,tape}: remove checking for drive->scsi
      ide: add PC_FLAG_ZIP_DRIVE pc flag
      ide-tape: factor out waiting for good ireason from idetape_transfer_pc()
      ide-tape: set PC_FLAG_DMA_IN_PROGRESS flag in idetape_transfer_pc()
      ...

commit 681a561b7ec7fdcd8f35b68e44ac6d6c70aecc04
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Jul 15 21:21:45 2008 +0200

    block: unexport blk_end_sync_rq
    
    All the users of blk_end_sync_rq has gone (they are converted to use
    blk_execute_rq). This unexports blk_end_sync_rq.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Borislav Petkov <petkovbb@gmail.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d2a1b71e93c3..1171abd7eb17 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -623,7 +623,6 @@ extern void generic_make_request(struct bio *bio);
 extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
-extern void blk_end_sync_rq(struct request *rq, int error);
 extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
 extern void blk_requeue_request(struct request_queue *, struct request *);

commit 27f8221af406e43b529a5425bc99c9b1e9bdf521
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Jul 4 09:30:03 2008 +0200

    block: add blk_queue_update_dma_pad
    
    This adds blk_queue_update_dma_pad to prevent LLDs from overwriting
    the dma pad mask wrongly (we added blk_queue_update_dma_alignment due
    to the same reason).
    
    This also converts libata to use blk_queue_update_dma_pad instead of
    blk_queue_dma_pad.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e04c4ac8a7cf..1ffd8bfdc4c9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -776,6 +776,7 @@ extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
 extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
+extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
 extern int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size);

commit e48ec69005f02b70b7ecfde1bc39a599086d16ef
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jul 3 13:18:54 2008 +0200

    block: extend queue_flag bitops
    
    Add test_and_clear and test_and_set.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ff9d0bdf2a16..e04c4ac8a7cf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -428,6 +428,32 @@ static inline void queue_flag_set_unlocked(unsigned int flag,
 	__set_bit(flag, &q->queue_flags);
 }
 
+static inline int queue_flag_test_and_clear(unsigned int flag,
+					    struct request_queue *q)
+{
+	WARN_ON_ONCE(!queue_is_locked(q));
+
+	if (test_bit(flag, &q->queue_flags)) {
+		__clear_bit(flag, &q->queue_flags);
+		return 1;
+	}
+
+	return 0;
+}
+
+static inline int queue_flag_test_and_set(unsigned int flag,
+					  struct request_queue *q)
+{
+	WARN_ON_ONCE(!queue_is_locked(q));
+
+	if (!test_bit(flag, &q->queue_flags)) {
+		__set_bit(flag, &q->queue_flags);
+		return 0;
+	}
+
+	return 1;
+}
+
 static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 {
 	WARN_ON_ONCE(!queue_is_locked(q));

commit cc371e66e340f35eed8dc4651c7c18e754c7fb26
Author: Alasdair G Kergon <agk@redhat.com>
Date:   Thu Jul 3 09:53:43 2008 +0200

    Add bvec_merge_data to handle stacked devices and ->merge_bvec()
    
    When devices are stacked, one device's merge_bvec_fn may need to perform
    the mapping and then call one or more functions for its underlying devices.
    
    The following bio fields are used:
      bio->bi_sector
      bio->bi_bdev
      bio->bi_size
      bio->bi_rw  using bio_data_dir()
    
    This patch creates a new struct bvec_merge_data holding a copy of those
    fields to avoid having to change them directly in the struct bio when
    going down the stack only to have to change them back again on the way
    back up.  (And then when the bio gets mapped for real, the whole
    exercise gets repeated, but that's a problem for another day...)
    
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Milan Broz <mbroz@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7ab8acad5b6e..ff9d0bdf2a16 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -254,7 +254,14 @@ typedef int (prep_rq_fn) (struct request_queue *, struct request *);
 typedef void (unplug_fn) (struct request_queue *);
 
 struct bio_vec;
-typedef int (merge_bvec_fn) (struct request_queue *, struct bio *, struct bio_vec *);
+struct bvec_merge_data {
+	struct block_device *bi_bdev;
+	sector_t bi_sector;
+	unsigned bi_size;
+	unsigned long bi_rw;
+};
+typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
+			     struct bio_vec *);
 typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);

commit b24498d477a14680fc3bb3ad884fa9fa76a2d237
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 27 09:12:09 2008 +0200

    block: integrity flags can't use bit ops on unsigned short
    
    Just use normal open coded bit operations instead, they need not be
    atomic.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a842b776d099..7ab8acad5b6e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -870,8 +870,8 @@ void kblockd_flush_work(struct work_struct *work);
 
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 
-#define INTEGRITY_FLAG_READ	1	/* verify data integrity on read */
-#define INTEGRITY_FLAG_WRITE	2	/* generate data integrity on write */
+#define INTEGRITY_FLAG_READ	2	/* verify data integrity on read */
+#define INTEGRITY_FLAG_WRITE	4	/* generate data integrity on write */
 
 struct blk_integrity_exchg {
 	void			*prot_buf;
@@ -940,11 +940,11 @@ static inline int bdev_integrity_enabled(struct block_device *bdev, int rw)
 		return 0;
 
 	if (rw == READ && bi->verify_fn != NULL &&
-	    test_bit(INTEGRITY_FLAG_READ, &bi->flags))
+	    (bi->flags & INTEGRITY_FLAG_READ))
 		return 1;
 
 	if (rw == WRITE && bi->generate_fn != NULL &&
-	    test_bit(INTEGRITY_FLAG_WRITE, &bi->flags))
+	    (bi->flags & INTEGRITY_FLAG_WRITE))
 		return 1;
 
 	return 0;

commit 0b07de85a76e1346e675f0e98437378932473df7
Author: Adel Gadllah <adel.gadllah@gmail.com>
Date:   Thu Jun 26 13:48:27 2008 +0200

    allow userspace to modify scsi command filter on per device basis
    
    This patch exports the per-gendisk command filter to user space through
    sysfs, so it can be changed by the system administrator.
    All users of the old cmd filter have been converted to use the new one.
    
    Original patch from Peter Jones.
    
    Signed-off-by: Adel Gadllah <adel.gadllah@gmail.com>
    Signed-off-by: Peter Jones <pjones@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d3ae9ad97213..a842b776d099 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -671,7 +671,6 @@ extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
-extern int blk_verify_command(unsigned char *, int);
 extern void blk_unplug(struct request_queue *q);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
@@ -797,6 +796,15 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
+/*
+* command filter functions
+*/
+extern int blk_verify_command(struct file *file, unsigned char *cmd);
+extern int blk_cmd_filter_verify_command(struct blk_scsi_cmd_filter *filter,
+					 unsigned char *cmd, mode_t *f_mode);
+extern int blk_register_filter(struct gendisk *disk);
+extern void blk_unregister_filter(struct gendisk *disk);
+
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128
 #define SAFE_MAX_SECTORS 255

commit 6e2401ad6f33de15ff00f78b88159f00a14f3b35
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Wed Jun 18 10:15:02 2008 +0200

    block: integrity cleanups
    
    - No need to check for NULL bio, we'll get an immediate oops anyway.
    - Make bio_integrity() a proper function.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 443df75d2cde..d3ae9ad97213 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -860,7 +860,6 @@ void kblockd_flush_work(struct work_struct *work);
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-*")
 
-
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 
 #define INTEGRITY_FLAG_READ	1	/* verify data integrity on read */
@@ -945,8 +944,6 @@ static inline int bdev_integrity_enabled(struct block_device *bdev, int rw)
 
 static inline int blk_integrity_rq(struct request *rq)
 {
-	BUG_ON(rq->bio == NULL);
-
 	return bio_integrity(rq->bio);
 }
 
@@ -963,7 +960,6 @@ static inline int blk_integrity_rq(struct request *rq)
 
 #endif /* CONFIG_BLK_DEV_INTEGRITY */
 
-
 #else /* CONFIG_BLOCK */
 /*
  * stubs for when the block layer is configured out

commit da9cbc87395308a21465bd25441297bbba0477e1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jun 30 20:42:08 2008 +0200

    block: blkdev.h cleanup, move iocontext stuff to iocontext.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4a9ed45270ff..443df75d2cde 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -33,12 +33,6 @@ struct sg_io_hdr;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
-int put_io_context(struct io_context *ioc);
-void exit_io_context(void);
-struct io_context *get_io_context(gfp_t gfp_flags, int node);
-struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
-void copy_io_context(struct io_context **pdst, struct io_context **psrc);
-
 struct request;
 typedef void (rq_end_io_fn)(struct request *, int);
 
@@ -981,17 +975,6 @@ static inline long nr_blockdev_pages(void)
 	return 0;
 }
 
-static inline void exit_io_context(void)
-{
-}
-
-struct io_context;
-static inline int put_io_context(struct io_context *ioc)
-{
-	return 1;
-}
-
-
 #endif /* CONFIG_BLOCK */
 
 #endif

commit 7ba1ba12eeef0aa7113beb16410ef8b7c748e18b
Author: Martin K. Petersen <martin.petersen@oracle.com>
Date:   Mon Jun 30 20:04:41 2008 +0200

    block: Block layer data integrity support
    
    Some block devices support verifying the integrity of requests by way
    of checksums or other protection information that is submitted along
    with the I/O.
    
    This patch implements support for generating and verifying integrity
    metadata, as well as correctly merging, splitting and cloning bios and
    requests that have this extra information attached.
    
    See Documentation/block/data-integrity.txt for more information.
    
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6a3da6717135..4a9ed45270ff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -112,6 +112,7 @@ enum rq_flag_bits {
 	__REQ_ALLOCED,		/* request came from our alloc pool */
 	__REQ_RW_META,		/* metadata io request */
 	__REQ_COPY_USER,	/* contains copies of user pages */
+	__REQ_INTEGRITY,	/* integrity metadata has been remapped */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -134,6 +135,7 @@ enum rq_flag_bits {
 #define REQ_ALLOCED	(1 << __REQ_ALLOCED)
 #define REQ_RW_META	(1 << __REQ_RW_META)
 #define REQ_COPY_USER	(1 << __REQ_COPY_USER)
+#define REQ_INTEGRITY	(1 << __REQ_INTEGRITY)
 
 #define BLK_MAX_CDB	16
 
@@ -865,6 +867,109 @@ void kblockd_flush_work(struct work_struct *work);
 	MODULE_ALIAS("block-major-" __stringify(major) "-*")
 
 
+#if defined(CONFIG_BLK_DEV_INTEGRITY)
+
+#define INTEGRITY_FLAG_READ	1	/* verify data integrity on read */
+#define INTEGRITY_FLAG_WRITE	2	/* generate data integrity on write */
+
+struct blk_integrity_exchg {
+	void			*prot_buf;
+	void			*data_buf;
+	sector_t		sector;
+	unsigned int		data_size;
+	unsigned short		sector_size;
+	const char		*disk_name;
+};
+
+typedef void (integrity_gen_fn) (struct blk_integrity_exchg *);
+typedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);
+typedef void (integrity_set_tag_fn) (void *, void *, unsigned int);
+typedef void (integrity_get_tag_fn) (void *, void *, unsigned int);
+
+struct blk_integrity {
+	integrity_gen_fn	*generate_fn;
+	integrity_vrfy_fn	*verify_fn;
+	integrity_set_tag_fn	*set_tag_fn;
+	integrity_get_tag_fn	*get_tag_fn;
+
+	unsigned short		flags;
+	unsigned short		tuple_size;
+	unsigned short		sector_size;
+	unsigned short		tag_size;
+
+	const char		*name;
+
+	struct kobject		kobj;
+};
+
+extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
+extern void blk_integrity_unregister(struct gendisk *);
+extern int blk_integrity_compare(struct block_device *, struct block_device *);
+extern int blk_rq_map_integrity_sg(struct request *, struct scatterlist *);
+extern int blk_rq_count_integrity_sg(struct request *);
+
+static inline unsigned short blk_integrity_tuple_size(struct blk_integrity *bi)
+{
+	if (bi)
+		return bi->tuple_size;
+
+	return 0;
+}
+
+static inline struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
+{
+	return bdev->bd_disk->integrity;
+}
+
+static inline unsigned int bdev_get_tag_size(struct block_device *bdev)
+{
+	struct blk_integrity *bi = bdev_get_integrity(bdev);
+
+	if (bi)
+		return bi->tag_size;
+
+	return 0;
+}
+
+static inline int bdev_integrity_enabled(struct block_device *bdev, int rw)
+{
+	struct blk_integrity *bi = bdev_get_integrity(bdev);
+
+	if (bi == NULL)
+		return 0;
+
+	if (rw == READ && bi->verify_fn != NULL &&
+	    test_bit(INTEGRITY_FLAG_READ, &bi->flags))
+		return 1;
+
+	if (rw == WRITE && bi->generate_fn != NULL &&
+	    test_bit(INTEGRITY_FLAG_WRITE, &bi->flags))
+		return 1;
+
+	return 0;
+}
+
+static inline int blk_integrity_rq(struct request *rq)
+{
+	BUG_ON(rq->bio == NULL);
+
+	return bio_integrity(rq->bio);
+}
+
+#else /* CONFIG_BLK_DEV_INTEGRITY */
+
+#define blk_integrity_rq(rq)			(0)
+#define blk_rq_count_integrity_sg(a)		(0)
+#define blk_rq_map_integrity_sg(a, b)		(0)
+#define bdev_get_integrity(a)			(0)
+#define bdev_get_tag_size(a)			(0)
+#define blk_integrity_compare(a, b)		(0)
+#define blk_integrity_register(a, b)		(0)
+#define blk_integrity_unregister(a)		do { } while (0);
+
+#endif /* CONFIG_BLK_DEV_INTEGRITY */
+
+
 #else /* CONFIG_BLOCK */
 /*
  * stubs for when the block layer is configured out

commit 244b4d56f85bcd11b21ab0b94845a3dabeed5c10
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jun 12 20:12:36 2008 +0200

    block: kill request_queue_t
    
    Everything was moved to struct request_queue a few kernel revisions
    ago, maintaining the deprecated typedef to avoid breaking things.
    Now the time has come to get rid of that typedef.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d2a1b71e93c3..6a3da6717135 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -23,7 +23,6 @@
 struct scsi_ioctl_command;
 
 struct request_queue;
-typedef struct request_queue request_queue_t __deprecated;
 struct elevator_queue;
 typedef struct elevator_queue elevator_t;
 struct request_pm_state;

commit 7663c1e2792a9662b23dec6e19bfcd3d55360b8f
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Apr 29 21:31:27 2008 +0200

    Improve queue_is_locked()
    
    spin_is_locked() doesn't work on UP without spinlock debugging. Make it
    safer and just return 1 on UP, so we don't get false positives. The plan
    is to kill this debug function during the -rc cycle.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 95864b3ff298..d2a1b71e93c3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -412,8 +412,12 @@ struct request_queue
 
 static inline int queue_is_locked(struct request_queue *q)
 {
+#ifdef CONFIG_SMP
 	spinlock_t *lock = q->queue_lock;
 	return lock && spin_is_locked(lock);
+#else
+	return 1;
+#endif
 }
 
 static inline void queue_flag_set_unlocked(unsigned int flag,

commit 8f45c1a58a25c3a1a2f42521445e1e786c4c0b92
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 29 10:16:38 2008 -0700

    block: fix queue locking verification
    
    The new queue_flag_set/clear() functions verify that the queue is
    locked, but in doing so they will actually instead oops if the queue
    lock hasn't been initialized at all.
    
    So fix the lock debug test to consider the "no lock" case to be
    unlocked.  This way you get a nice WARN_ON_ONCE() instead of a fatal
    oops.
    
    Bug introduced by commit 75ad23bc0fcb4f992a5d06982bf0857ab1738e9e
    ("block: make queue flags non-atomic").
    
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c09696a90d6a..95864b3ff298 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -410,6 +410,12 @@ struct request_queue
 #define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
 
+static inline int queue_is_locked(struct request_queue *q)
+{
+	spinlock_t *lock = q->queue_lock;
+	return lock && spin_is_locked(lock);
+}
+
 static inline void queue_flag_set_unlocked(unsigned int flag,
 					   struct request_queue *q)
 {
@@ -418,7 +424,7 @@ static inline void queue_flag_set_unlocked(unsigned int flag,
 
 static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
 {
-	WARN_ON_ONCE(!spin_is_locked(q->queue_lock));
+	WARN_ON_ONCE(!queue_is_locked(q));
 	__set_bit(flag, &q->queue_flags);
 }
 
@@ -430,7 +436,7 @@ static inline void queue_flag_clear_unlocked(unsigned int flag,
 
 static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
 {
-	WARN_ON_ONCE(!spin_is_locked(q->queue_lock));
+	WARN_ON_ONCE(!queue_is_locked(q));
 	__clear_bit(flag, &q->queue_flags);
 }
 

commit ac9fafa1243640349aa481adf473db283a695766
Author: Alan D. Brunelle <Alan.Brunelle@hp.com>
Date:   Tue Apr 29 14:44:19 2008 +0200

    block: Skip I/O merges when disabled
    
    The block I/O + elevator + I/O scheduler code spend a lot of time trying
    to merge I/Os -- rightfully so under "normal" circumstances. However,
    if one were to know that the incoming I/O stream was /very/ random in
    nature, the cycles are wasted.
    
    This patch adds a per-request_queue tunable that (when set) disables
    merge attempts (beyond the simple one-hit cache check), thus freeing up
    a non-trivial amount of CPU cycles.
    
    Signed-off-by: Alan D. Brunelle <alan.brunelle@hp.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 08df1ea8bac4..c09696a90d6a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -408,6 +408,7 @@ struct request_queue
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
 #define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
+#define QUEUE_FLAG_NOMERGES    10	/* disable merge attempts */
 
 static inline void queue_flag_set_unlocked(unsigned int flag,
 					   struct request_queue *q)
@@ -476,6 +477,7 @@ enum {
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+#define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
 
 #define blk_fs_request(rq)	((rq)->cmd_type == REQ_TYPE_FS)

commit d7e3c3249ef23b4617393c69fe464765b4ff1645
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Apr 29 09:54:39 2008 +0200

    block: add large command support
    
    This patch changes rq->cmd from the static array to a pointer to
    support large commands.
    
    We rarely handle large commands. So for optimization, a struct request
    still has a static array for a command. rq_init sets rq->cmd pointer
    to the static array.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d17032c347c0..08df1ea8bac4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -215,8 +215,9 @@ struct request {
 	/*
 	 * when request is used as a packet command carrier
 	 */
-	unsigned int cmd_len;
-	unsigned char cmd[BLK_MAX_CDB];
+	unsigned short cmd_len;
+	unsigned char __cmd[BLK_MAX_CDB];
+	unsigned char *cmd;
 
 	unsigned int data_len;
 	unsigned int extra_len;	/* length of alignment and padding */

commit 2a4aa30c5f967eb6ae874c67fa6fceeee84815f9
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Apr 29 09:54:36 2008 +0200

    block: rename and export rq_init()
    
    This rename rq_init() blk_rq_init() and export it. Any path that hands
    the request to the block layer needs to call it to initialize the
    request.
    
    This is a preparation for large command support, which needs to
    initialize the request in a proper way (that is, just doing a memset()
    will not work).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8ca481cd7d73..d17032c347c0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -607,6 +607,7 @@ extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);
 extern void generic_make_request(struct bio *bio);
+extern void blk_rq_init(struct request_queue *q, struct request *rq);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(struct request_queue *, struct request *);
 extern void blk_end_sync_rq(struct request *rq, int error);

commit 75ad23bc0fcb4f992a5d06982bf0857ab1738e9e
Author: Nick Piggin <npiggin@suse.de>
Date:   Tue Apr 29 14:48:33 2008 +0200

    block: make queue flags non-atomic
    
    We can save some atomic ops in the IO path, if we clearly define
    the rules of how to modify the queue flags.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c5065e3d2ca9..8ca481cd7d73 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -408,6 +408,30 @@ struct request_queue
 #define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
 
+static inline void queue_flag_set_unlocked(unsigned int flag,
+					   struct request_queue *q)
+{
+	__set_bit(flag, &q->queue_flags);
+}
+
+static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
+{
+	WARN_ON_ONCE(!spin_is_locked(q->queue_lock));
+	__set_bit(flag, &q->queue_flags);
+}
+
+static inline void queue_flag_clear_unlocked(unsigned int flag,
+					     struct request_queue *q)
+{
+	__clear_bit(flag, &q->queue_flags);
+}
+
+static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
+{
+	WARN_ON_ONCE(!spin_is_locked(q->queue_lock));
+	__clear_bit(flag, &q->queue_flags);
+}
+
 enum {
 	/*
 	 * Hardbarrier is supported with one of the following methods.
@@ -496,17 +520,17 @@ static inline int blk_queue_full(struct request_queue *q, int rw)
 static inline void blk_set_queue_full(struct request_queue *q, int rw)
 {
 	if (rw == READ)
-		set_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
+		queue_flag_set(QUEUE_FLAG_READFULL, q);
 	else
-		set_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+		queue_flag_set(QUEUE_FLAG_WRITEFULL, q);
 }
 
 static inline void blk_clear_queue_full(struct request_queue *q, int rw)
 {
 	if (rw == READ)
-		clear_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
+		queue_flag_clear(QUEUE_FLAG_READFULL, q);
 	else
-		clear_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+		queue_flag_clear(QUEUE_FLAG_WRITEFULL, q);
 }
 
 
@@ -626,6 +650,7 @@ extern void blk_start_queue(struct request_queue *q);
 extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(struct request_queue *q);
+extern void __blk_run_queue(struct request_queue *);
 extern void blk_run_queue(struct request_queue *);
 extern void blk_start_queueing(struct request_queue *);
 extern int blk_rq_map_user(struct request_queue *, struct request *, void __user *, unsigned long);

commit 2472892a3ce17b177cc0d8099a6391949c75abf2
Author: Andi Kleen <andi@firstfloor.org>
Date:   Mon Apr 21 09:51:05 2008 +0200

    block: fix memory hotplug and bouncing in block layer
    
    Only noticed this while hacking something else, no test case.
    
    blk_max_low_pfn is initialized once at bootup by the block layer from
    max_low_pfn.  But max_low_pfn is not necessarily constant over the runtime of
    the system when you consider memory hotplug.  What could happen if that
    someone adds memory later the block layer wouldn't get updated and then start
    bouncing memory unnecessarily.
    
    Also on 64bit blk_max_low_pfn actually isn't needed because it just disables
    bouncing essentially and there is no highmem.  And nobody can pass pfns >
    max_low_pfn to the block layer, because those wouldn't have a struct page and
    I suspect block layer wouldn't be very happy without that.
    
    So set BLK_BOUNCE_HIGH to infinity (-1ULL) on 64bit.  That avoids the problem
    of having to update it on memory hotadd.
    
    On 32bit I kept the same behaviour because at least on i386
    memory hotadd only adds HIGHMEM, never lowmem.
    
    BLK_BOUNCE_ANY is always set to infinity on both 32 and 64bit.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b3a58adc4352..c5065e3d2ca9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -535,8 +535,13 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
  * BLK_BOUNCE_ANY	: don't bounce anything
  * BLK_BOUNCE_ISA	: bounce pages above ISA DMA boundary
  */
+
+#if BITS_PER_LONG == 32
 #define BLK_BOUNCE_HIGH		((u64)blk_max_low_pfn << PAGE_SHIFT)
-#define BLK_BOUNCE_ANY		((u64)blk_max_pfn << PAGE_SHIFT)
+#else
+#define BLK_BOUNCE_HIGH		-1ULL
+#endif
+#define BLK_BOUNCE_ANY		(-1ULL)
 #define BLK_BOUNCE_ISA		(ISA_DMA_THRESHOLD)
 
 /*

commit f18573abcc57844a7c3c12699d40eead8728cd8a
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Apr 11 12:56:52 2008 +0200

    block: move the padding adjustment to blk_rq_map_sg
    
    blk_rq_map_user adjusts bi_size of the last bio. It breaks the rule
    that req->data_len (the true data length) is equal to sum(bio). It
    broke the scsi command completion code.
    
    commit e97a294ef6938512b655b1abf17656cf2b26f709 was introduced to fix
    the above issue. However, the partial completion code doesn't work
    with it. The commit is also a layer violation (scsi mid-layer should
    not know about the block layer's padding).
    
    This patch moves the padding adjustment to blk_rq_map_sg (suggested by
    James). The padding works like the drain buffer. This patch breaks the
    rule that req->data_len is equal to sum(sg), however, the drain buffer
    already broke it. So this patch just restores the rule that
    req->data_len is equal to sub(bio) without breaking anything new.
    
    Now when a low level driver needs padding, blk_rq_map_user and
    blk_rq_map_user_iov guarantee there's enough room for padding.
    blk_rq_map_sg can safely extend the last entry of a scatter list.
    
    blk_rq_map_sg must extend the last entry of a scatter list only for a
    request that got through bio_copy_user_iov. This patches introduces
    new REQ_COPY_USER flag.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6f79d40dd3c0..b3a58adc4352 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -112,6 +112,7 @@ enum rq_flag_bits {
 	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
 	__REQ_ALLOCED,		/* request came from our alloc pool */
 	__REQ_RW_META,		/* metadata io request */
+	__REQ_COPY_USER,	/* contains copies of user pages */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -133,6 +134,7 @@ enum rq_flag_bits {
 #define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
 #define REQ_ALLOCED	(1 << __REQ_ALLOCED)
 #define REQ_RW_META	(1 << __REQ_RW_META)
+#define REQ_COPY_USER	(1 << __REQ_COPY_USER)
 
 #define BLK_MAX_CDB	16
 

commit e3790c7d42a545e8fe8b38b513613ca96687b670
Author: Tejun Heo <htejun@gmail.com>
Date:   Tue Mar 4 11:18:17 2008 +0100

    block: separate out padding from alignment
    
    Block layer alignment was used for two different purposes - memory
    alignment and padding.  This causes problems in lower layers because
    drivers which only require memory alignment ends up with adjusted
    rq->data_len.  Separate out padding such that padding occurs iff
    driver explicitly requests it.
    
    Tomo: restorethe code to update bio in blk_rq_map_user
          introduced by the commit 40b01b9bbdf51ae543a04744283bf2d56c4a6afa
          according to padding alignment.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b72526c13ca0..6f79d40dd3c0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -362,6 +362,7 @@ struct request_queue
 	unsigned long		seg_boundary_mask;
 	void			*dma_drain_buffer;
 	unsigned int		dma_drain_size;
+	unsigned int		dma_pad_mask;
 	unsigned int		dma_alignment;
 
 	struct blk_queue_tag	*queue_tags;
@@ -701,6 +702,7 @@ extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
+extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
 extern int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size);

commit 7a85f8896f4b4a4a0249563b92af9e3161a6b467
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Mar 4 11:17:11 2008 +0100

    block: restore the meaning of rq->data_len to the true data length
    
    The meaning of rq->data_len was changed to the length of an allocated
    buffer from the true data length. It breaks SG_IO friends and
    bsg. This patch restores the meaning of rq->data_len to the true data
    length and adds rq->extra_len to store an extended length (due to
    drain buffer and padding).
    
    This patch also removes the code to update bio in blk_rq_map_user
    introduced by the commit 40b01b9bbdf51ae543a04744283bf2d56c4a6afa.
    The commit adjusts bio according to memory alignment
    (queue_dma_alignment). However, memory alignment is NOT padding
    alignment. This adjustment also breaks SG_IO friends and bsg. Padding
    alignment needs to be fixed in a proper way (by a separate patch).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <axboe@carl.home.kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6fe67d1939c2..b72526c13ca0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -216,8 +216,8 @@ struct request {
 	unsigned int cmd_len;
 	unsigned char cmd[BLK_MAX_CDB];
 
-	unsigned int raw_data_len;
 	unsigned int data_len;
+	unsigned int extra_len;	/* length of alignment and padding */
 	unsigned int sense_len;
 	void *data;
 	void *sense;

commit 2fb98e8414c42cb14698833aac640b143b9ade4f
Author: Tejun Heo <htejun@gmail.com>
Date:   Tue Feb 19 11:36:53 2008 +0100

    block: implement request_queue->dma_drain_needed
    
    Draining shouldn't be done for commands where overflow may indicate
    data integrity issues.  Add dma_drain_needed callback to
    request_queue.  Drain buffer is appened iff this function returns
    non-zero.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f1fe9fbf1c0e..6fe67d1939c2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -259,6 +259,7 @@ struct bio_vec;
 typedef int (merge_bvec_fn) (struct request_queue *, struct bio *, struct bio_vec *);
 typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
+typedef int (dma_drain_needed_fn)(struct request *);
 
 enum blk_queue_state {
 	Queue_down,
@@ -295,6 +296,7 @@ struct request_queue
 	merge_bvec_fn		*merge_bvec_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
+	dma_drain_needed_fn	*dma_drain_needed;
 
 	/*
 	 * Dispatch queue sorting
@@ -699,8 +701,9 @@ extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
-extern int blk_queue_dma_drain(struct request_queue *q, void *buf,
-			       unsigned int size);
+extern int blk_queue_dma_drain(struct request_queue *q,
+			       dma_drain_needed_fn *dma_drain_needed,
+			       void *buf, unsigned int size);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);

commit 6b00769fe1502b4ad97bb327ef7ac971b208bfb5
Author: Tejun Heo <htejun@gmail.com>
Date:   Tue Feb 19 11:36:35 2008 +0100

    block: add request->raw_data_len
    
    With padding and draining moved into it, block layer now may extend
    requests as directed by queue parameters, so now a request has two
    sizes - the original request size and the extended size which matches
    the size of area pointed to by bios and later by sgs.  The latter size
    is what lower layers are primarily interested in when allocating,
    filling up DMA tables and setting up the controller.
    
    Both padding and draining extend the data area to accomodate
    controller characteristics.  As any controller which speaks SCSI can
    handle underflows, feeding larger data area is safe.
    
    So, this patch makes the primary data length field, request->data_len,
    indicate the size of full data area and add a separate length field,
    request->raw_data_len, for the unmodified request size.  The latter is
    used to report to higher layer (userland) and where the original
    request size should be fed to the controller or device.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e1888cc5b8ae..f1fe9fbf1c0e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -216,6 +216,7 @@ struct request {
 	unsigned int cmd_len;
 	unsigned char cmd[BLK_MAX_CDB];
 
+	unsigned int raw_data_len;
 	unsigned int data_len;
 	unsigned int sense_len;
 	void *data;

commit 63a7138671c50a6f2c27bbd1a308dc75967062a3
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Feb 8 12:41:03 2008 +0100

    block: fixup rq_init() a bit
    
    Rearrange fields in cache order and initialize some fields that
    we didn't previously init. Remove init of ->completion_data, it's
    part of a union with ->hash. Luckily clearing the rb node is the same
    as setting it to null!
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 90392a9d7a9c..e1888cc5b8ae 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -137,7 +137,9 @@ enum rq_flag_bits {
 #define BLK_MAX_CDB	16
 
 /*
- * try to put the fields that are referenced together in the same cacheline
+ * try to put the fields that are referenced together in the same cacheline.
+ * if you modify this structure, be sure to check block/blk-core.c:rq_init()
+ * as well!
  */
 struct request {
 	struct list_head queuelist;

commit 3bc217ffe6774e7971d6a7ce6350ce806ebab044
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Feb 1 11:34:49 2008 +0100

    block: kill swap_io_context()
    
    It blindly copies everything in the io_context, including the lock.
    That doesn't work so well for either lock ordering or lockdep.
    
    There seems zero point in swapping io contexts on a request to request
    merge, so the best point of action is to just remove it.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cf170391c474..90392a9d7a9c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -39,7 +39,6 @@ void exit_io_context(void);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
 struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
-void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
 
 struct request;
 typedef void (rq_end_io_fn)(struct request *, int);

commit 22b132102f1540dd40f3e41df88796829b685f1a
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 31 12:36:19 2008 +0100

    block: new end request handling interface should take unsigned byte counts
    
    No point in passing signed integers as the byte count, they can never
    be negative.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e18d4192f6e8..cf170391c474 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -655,15 +655,18 @@ static inline void blk_run_address_space(struct address_space *mapping)
  * blk_end_request() for parts of the original function.
  * This prevents code duplication in drivers.
  */
-extern int blk_end_request(struct request *rq, int error, int nr_bytes);
-extern int __blk_end_request(struct request *rq, int error, int nr_bytes);
-extern int blk_end_bidi_request(struct request *rq, int error, int nr_bytes,
-				int bidi_bytes);
+extern int blk_end_request(struct request *rq, int error,
+				unsigned int nr_bytes);
+extern int __blk_end_request(struct request *rq, int error,
+				unsigned int nr_bytes);
+extern int blk_end_bidi_request(struct request *rq, int error,
+				unsigned int nr_bytes, unsigned int bidi_bytes);
 extern void end_request(struct request *, int);
 extern void end_queued_request(struct request *, int);
 extern void end_dequeued_request(struct request *, int);
-extern int blk_end_request_callback(struct request *rq, int error, int nr_bytes,
-				    int (drv_callback)(struct request *));
+extern int blk_end_request_callback(struct request *rq, int error,
+				unsigned int nr_bytes,
+				int (drv_callback)(struct request *));
 extern void blk_complete_request(struct request *);
 
 /*

commit 023ccde109b995bb99862bf9c87efd006b1d1885
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jan 29 14:08:06 2008 +0100

    block: fix warning on compile with CONFIG_BLOCK
    
    struct io_context was not defined, just add an empty forward decl.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 71e7a847dffc..e18d4192f6e8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -825,6 +825,7 @@ static inline void exit_io_context(void)
 {
 }
 
+struct io_context;
 static inline int put_io_context(struct io_context *ioc)
 {
 	return 1;

commit 8d01eddf292dcd78b640418c80fb300532799cd4
Merge: f0f005206998 fa0ccd837e3d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 29 08:51:56 2008 +1100

    Merge branch 'for-2.6.25' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.25' of git://git.kernel.dk/linux-2.6-block:
      block: implement drain buffers
      __bio_clone: don't calculate hw/phys segment counts
      block: allow queue dma_alignment of zero
      blktrace: Add blktrace ioctls to SCSI generic devices

commit f0f0052069989b80d2a3e50c9cd2f2a650bc1aea
Merge: 68fbda7de07e a65b58663d98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 29 08:51:32 2008 +1100

    Merge branch 'blk-end-request' of git://git.kernel.dk/linux-2.6-block
    
    * 'blk-end-request' of git://git.kernel.dk/linux-2.6-block: (30 commits)
      blk_end_request: changing xsysace (take 4)
      blk_end_request: changing ub (take 4)
      blk_end_request: cleanup of request completion (take 4)
      blk_end_request: cleanup 'uptodate' related code (take 4)
      blk_end_request: remove/unexport end_that_request_* (take 4)
      blk_end_request: changing scsi (take 4)
      blk_end_request: add bidi completion interface (take 4)
      blk_end_request: changing ide-cd (take 4)
      blk_end_request: add callback feature (take 4)
      blk_end_request: changing ide normal caller (take 4)
      blk_end_request: changing cpqarray (take 4)
      blk_end_request: changing cciss (take 4)
      blk_end_request: changing ide-scsi (take 4)
      blk_end_request: changing s390 (take 4)
      blk_end_request: changing mmc (take 4)
      blk_end_request: changing i2o_block (take 4)
      blk_end_request: changing viocd (take 4)
      blk_end_request: changing xen-blkfront (take 4)
      blk_end_request: changing viodasd (take 4)
      blk_end_request: changing sx8 (take 4)
      ...

commit fa0ccd837e3dddb44c7db2f128a8bb7e4eabc21a
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Thu Jan 10 11:30:36 2008 -0600

    block: implement drain buffers
    
    These DMA drain buffer implementations in drivers are pretty horrible
    to do in terms of manipulating the scatterlist.  Plus they're being
    done at least in drivers/ide and drivers/ata, so we now have code
    duplication.
    
    The one use case for this, as I understand it is AHCI controllers doing
    PIO mode to mmc devices but translating this to DMA at the controller
    level.
    
    So, what about adding a callback to the block layer that permits the
    adding of the drain buffer for the problem devices.  The idea is that
    you'd do this in slave_configure after you find one of these devices.
    
    The beauty of doing it in the block layer is that it quietly adds the
    drain buffer to the end of the sg list, so it automatically gets mapped
    (and unmapped) without anything unusual having to be done to the
    scatterlist in driver/scsi or drivers/ata and without any alteration to
    the transfer length.
    
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c7a3ab575c24..e542c8fd9215 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -429,6 +429,8 @@ struct request_queue
 	unsigned int		max_segment_size;
 
 	unsigned long		seg_boundary_mask;
+	void			*dma_drain_buffer;
+	unsigned int		dma_drain_size;
 	unsigned int		dma_alignment;
 
 	struct blk_queue_tag	*queue_tags;
@@ -760,6 +762,8 @@ extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
 extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
+extern int blk_queue_dma_drain(struct request_queue *q, void *buf,
+			       unsigned int size);
 extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);

commit d38ecf935fcb10264a6bc190855d9595165e6eeb
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:53:35 2008 +0100

    io context sharing: preliminary support
    
    Detach task state from ioc, instead keep track of how many processes
    are accessing the ioc.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 510a18ba1ec5..2483a05231c7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -34,7 +34,7 @@ struct sg_io_hdr;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
-void put_io_context(struct io_context *ioc);
+int put_io_context(struct io_context *ioc);
 void exit_io_context(void);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
 struct io_context *alloc_io_context(gfp_t gfp_flags, int node);

commit fd0928df98b9578be8a786ac0cb78a47a5e17a20
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:52:45 2008 +0100

    ioprio: move io priority from task_struct to io_context
    
    This is where it belongs and then it doesn't take up space for a
    process that doesn't do IO.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49b7a4c31a6d..510a18ba1ec5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -34,83 +34,10 @@ struct sg_io_hdr;
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
 
-/*
- * This is the per-process anticipatory I/O scheduler state.
- */
-struct as_io_context {
-	spinlock_t lock;
-
-	void (*dtor)(struct as_io_context *aic); /* destructor */
-	void (*exit)(struct as_io_context *aic); /* called on task exit */
-
-	unsigned long state;
-	atomic_t nr_queued; /* queued reads & sync writes */
-	atomic_t nr_dispatched; /* number of requests gone to the drivers */
-
-	/* IO History tracking */
-	/* Thinktime */
-	unsigned long last_end_request;
-	unsigned long ttime_total;
-	unsigned long ttime_samples;
-	unsigned long ttime_mean;
-	/* Layout pattern */
-	unsigned int seek_samples;
-	sector_t last_request_pos;
-	u64 seek_total;
-	sector_t seek_mean;
-};
-
-struct cfq_queue;
-struct cfq_io_context {
-	struct rb_node rb_node;
-	void *key;
-
-	struct cfq_queue *cfqq[2];
-
-	struct io_context *ioc;
-
-	unsigned long last_end_request;
-	sector_t last_request_pos;
-
-	unsigned long ttime_total;
-	unsigned long ttime_samples;
-	unsigned long ttime_mean;
-
-	unsigned int seek_samples;
-	u64 seek_total;
-	sector_t seek_mean;
-
-	struct list_head queue_list;
-
-	void (*dtor)(struct io_context *); /* destructor */
-	void (*exit)(struct io_context *); /* called on task exit */
-};
-
-/*
- * This is the per-process I/O subsystem state.  It is refcounted and
- * kmalloc'ed. Currently all fields are modified in process io context
- * (apart from the atomic refcount), so require no locking.
- */
-struct io_context {
-	atomic_t refcount;
-	struct task_struct *task;
-
-	unsigned int ioprio_changed;
-
-	/*
-	 * For request batching
-	 */
-	unsigned long last_waited; /* Time last woken after wait for request */
-	int nr_batch_requests;     /* Number of requests left in the batch */
-
-	struct as_io_context *aic;
-	struct rb_root cic_root;
-	void *ioc_data;
-};
-
 void put_io_context(struct io_context *ioc);
 void exit_io_context(void);
 struct io_context *get_io_context(gfp_t gfp_flags, int node);
+struct io_context *alloc_io_context(gfp_t gfp_flags, int node);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
 
@@ -894,6 +821,12 @@ static inline void exit_io_context(void)
 {
 }
 
+static inline int put_io_context(struct io_context *ioc)
+{
+	return 1;
+}
+
+
 #endif /* CONFIG_BLOCK */
 
 #endif

commit 5450d3e1d68f10be087f0855d8bad5458b50ecbe
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:53:03 2007 -0500

    blk_end_request: cleanup 'uptodate' related code (take 4)
    
    This patch converts 'uptodate' arguments of no longer exported
    interfaces, end_that_request_first/last, to 'error', and removes
    internal conversions for it in blk_end_request interfaces.
    
    Also, this patch removes no longer needed end_io_error().
    
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0c39ac75bed4..0ea82d222046 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -745,14 +745,6 @@ extern void blk_complete_request(struct request *);
 extern unsigned int blk_rq_bytes(struct request *rq);
 extern unsigned int blk_rq_cur_bytes(struct request *rq);
 
-/*
- * end_that_request_first/chunk() takes an uptodate argument. we account
- * any value <= as an io error. 0 means -EIO for compatability reasons,
- * any other < 0 value is the direct error type. An uptodate value of
- * 1 indicates successful io completion
- */
-#define end_io_error(uptodate)	(unlikely((uptodate) <= 0))
-
 static inline void blkdev_dequeue_request(struct request *req)
 {
 	elv_dequeue_request(req->q, req);

commit 3bcddeac1c4c7e6fb90531b80f236b1a05dfe514
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:52:28 2007 -0500

    blk_end_request: remove/unexport end_that_request_* (take 4)
    
    This patch removes the following functions:
      o end_that_request_first()
      o end_that_request_chunk()
    and stops exporting the functions below:
      o end_that_request_last()
    
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 029b7097f9e5..0c39ac75bed4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -718,21 +718,18 @@ static inline void blk_run_address_space(struct address_space *mapping)
 }
 
 /*
- * end_request() and friends. Must be called with the request queue spinlock
- * acquired. All functions called within end_request() _must_be_ atomic.
+ * blk_end_request() and friends.
+ * __blk_end_request() and end_request() must be called with
+ * the request queue spinlock acquired.
  *
  * Several drivers define their own end_request and call
- * end_that_request_first() and end_that_request_last()
- * for parts of the original function. This prevents
- * code duplication in drivers.
+ * blk_end_request() for parts of the original function.
+ * This prevents code duplication in drivers.
  */
 extern int blk_end_request(struct request *rq, int error, int nr_bytes);
 extern int __blk_end_request(struct request *rq, int error, int nr_bytes);
 extern int blk_end_bidi_request(struct request *rq, int error, int nr_bytes,
 				int bidi_bytes);
-extern int end_that_request_first(struct request *, int, int);
-extern int end_that_request_chunk(struct request *, int, int);
-extern void end_that_request_last(struct request *, int);
 extern void end_request(struct request *, int);
 extern void end_queued_request(struct request *, int);
 extern void end_dequeued_request(struct request *, int);

commit e3a04fe34a3ec81ddeddb6c73fb7299716cffbb0
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:51:46 2007 -0500

    blk_end_request: add bidi completion interface (take 4)
    
    This patch adds a variant of the interface, blk_end_bidi_request(),
    which completes a bidi request.
    
    Bidi request must be completed as a whole, both rq and rq->next_rq
    at once.  So the interface has 2 arguments for completion size.
    
    As for ->end_io, only rq->end_io is called (rq->next_rq->end_io is not
    called).  So if special completion handling is needed, the handler
    must be set to rq->end_io.
    And the handler must take care of freeing next_rq too, since
    the interface doesn't care of it if rq->end_io is not NULL.
    
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 63fe7542b3fa..029b7097f9e5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -728,6 +728,8 @@ static inline void blk_run_address_space(struct address_space *mapping)
  */
 extern int blk_end_request(struct request *rq, int error, int nr_bytes);
 extern int __blk_end_request(struct request *rq, int error, int nr_bytes);
+extern int blk_end_bidi_request(struct request *rq, int error, int nr_bytes,
+				int bidi_bytes);
 extern int end_that_request_first(struct request *, int, int);
 extern int end_that_request_chunk(struct request *, int, int);
 extern void end_that_request_last(struct request *, int);

commit e19a3ab058fe91c8c54d43dc56dccf7eb386478e
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:51:02 2007 -0500

    blk_end_request: add callback feature (take 4)
    
    This patch adds a variant of the interface, blk_end_request_callback(),
    which has driver callback feature.
    
    Drivers may need to do special works between end_that_request_first()
    and end_that_request_last().
    For such drivers, blk_end_request_callback() allows it to pass
    a callback function which is called between end_that_request_first()
    and end_that_request_last().
    
    This interface is only for fallback of other blk_end_request interfaces.
    Drivers should avoid their tricky behaviors and use other interfaces
    as much as possible.
    
    Currently, only one driver, ide-cd, needs this interface.
    So this interface should/will be removed, after the driver removes
    such tricky behaviors.
    
    o ide-cd (cdrom_newpc_intr())
      In PIO mode, cdrom_newpc_intr() needs to defer end_that_request_last()
      until the device clears DRQ_STAT and raises an interrupt after
      end_that_request_first().
      So end_that_request_first() and end_that_request_last() are called
      separately in cdrom_newpc_intr().
    
      This means blk_end_request_callback() has to return without
      completing request even if no leftover in the request.
      To satisfy the requirement, callback function has return value
      so that drivers can tell blk_end_request_callback() to return
      without completing request.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aa2341df7932..63fe7542b3fa 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -734,6 +734,8 @@ extern void end_that_request_last(struct request *, int);
 extern void end_request(struct request *, int);
 extern void end_queued_request(struct request *, int);
 extern void end_dequeued_request(struct request *, int);
+extern int blk_end_request_callback(struct request *rq, int error, int nr_bytes,
+				    int (drv_callback)(struct request *));
 extern void blk_complete_request(struct request *);
 
 /*

commit 3b11313a6c2a42425bf06e92528bda6affd58dec
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:41:17 2007 -0500

    blk_end_request: add/export functions to get request size (take 4)
    
    This patch adds/exports functions to get the size of request in bytes.
    They are useful because blk_end_request interfaces take bytes
    as a completed I/O size instead of sectors.
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3b212f02db8d..aa2341df7932 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -736,6 +736,14 @@ extern void end_queued_request(struct request *, int);
 extern void end_dequeued_request(struct request *, int);
 extern void blk_complete_request(struct request *);
 
+/*
+ * blk_end_request() takes bytes instead of sectors as a complete size.
+ * blk_rq_bytes() returns bytes left to complete in the entire request.
+ * blk_rq_cur_bytes() returns bytes left to complete in the current segment.
+ */
+extern unsigned int blk_rq_bytes(struct request *rq);
+extern unsigned int blk_rq_cur_bytes(struct request *rq);
+
 /*
  * end_that_request_first/chunk() takes an uptodate argument. we account
  * any value <= as an io error. 0 means -EIO for compatability reasons,

commit 336cdb4003200a90f4fc52a4e9ccc2baa570fffb
Author: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
Date:   Tue Dec 11 17:40:30 2007 -0500

    blk_end_request: add new request completion interface (take 4)
    
    This patch adds 2 new interfaces for request completion:
      o blk_end_request()   : called without queue lock
      o __blk_end_request() : called with queue lock held
    
    blk_end_request takes 'error' as an argument instead of 'uptodate',
    which current end_that_request_* take.
    The meanings of values are below and the value is used when bio is
    completed.
        0 : success
      < 0 : error
    
    Some device drivers call some generic functions below between
    end_that_request_{first/chunk} and end_that_request_last().
      o add_disk_randomness()
      o blk_queue_end_tag()
      o blkdev_dequeue_request()
    These are called in the blk_end_request interfaces as a part of
    generic request completion.
    So all device drivers become to call above functions.
    To decide whether to call blkdev_dequeue_request(), blk_end_request
    uses list_empty(&rq->queuelist) (blk_queued_rq() macro is added for it).
    So drivers must re-initialize it using list_init() or so before calling
    blk_end_request if drivers use it for its specific purpose.
    (Currently, there is no driver which completes request without
     re-initializing the queuelist after used it.  So rq->queuelist
     can be used for the purpose above.)
    
    "Normal" drivers can be converted to use blk_end_request()
    in a standard way shown below.
    
     a) end_that_request_{chunk/first}
        spin_lock_irqsave()
        (add_disk_randomness(), blk_queue_end_tag(), blkdev_dequeue_request())
        end_that_request_last()
        spin_unlock_irqrestore()
        => blk_end_request()
    
     b) spin_lock_irqsave()
        end_that_request_{chunk/first}
        (add_disk_randomness(), blk_queue_end_tag(), blkdev_dequeue_request())
        end_that_request_last()
        spin_unlock_irqrestore()
        => spin_lock_irqsave()
           __blk_end_request()
           spin_unlock_irqsave()
    
     c) spin_lock_irqsave()
        (add_disk_randomness(), blk_queue_end_tag(), blkdev_dequeue_request())
        end_that_request_last()
        spin_unlock_irqrestore()
        => blk_end_request()   or   spin_lock_irqsave()
                                    __blk_end_request()
                                    spin_unlock_irqrestore()
    
    Signed-off-by: Kiyoshi Ueda <k-ueda@ct.jp.nec.com>
    Signed-off-by: Jun'ichi Nomura <j-nomura@ce.jp.nec.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49b7a4c31a6d..3b212f02db8d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -537,6 +537,8 @@ enum {
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 #define blk_empty_barrier(rq)	(blk_barrier_rq(rq) && blk_fs_request(rq) && !(rq)->hard_nr_sectors)
+/* rq->queuelist of dequeued request must be list_empty() */
+#define blk_queued_rq(rq)	(!list_empty(&(rq)->queuelist))
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
@@ -724,6 +726,8 @@ static inline void blk_run_address_space(struct address_space *mapping)
  * for parts of the original function. This prevents
  * code duplication in drivers.
  */
+extern int blk_end_request(struct request *rq, int error, int nr_bytes);
+extern int __blk_end_request(struct request *rq, int error, int nr_bytes);
 extern int end_that_request_first(struct request *, int, int);
 extern int end_that_request_chunk(struct request *, int, int);
 extern void end_that_request_last(struct request *, int);

commit 482eb689169948e9f4966fbae6be4d6bc0bfa818
Author: Pete Wyckoff <pw@osc.edu>
Date:   Tue Jan 1 10:23:02 2008 -0500

    block: allow queue dma_alignment of zero
    
    Let queue_dma_alignment return 0 if it was specifically set to 0.
    This permits devices with no particular alignment restrictions to
    use arbitrary user space buffers without copying.
    
    Signed-off-by: Pete Wyckoff <pw@osc.edu>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 49b7a4c31a6d..c7a3ab575c24 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -836,12 +836,7 @@ static inline int bdev_hardsect_size(struct block_device *bdev)
 
 static inline int queue_dma_alignment(struct request_queue *q)
 {
-	int retval = 511;
-
-	if (q && q->dma_alignment)
-		retval = q->dma_alignment;
-
-	return retval;
+	return q ? q->dma_alignment : 511;
 }
 
 /* assumes size > 256 */

commit 7267c3377443322588cddaf457cf106839a60463
Author: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
Date:   Sat Jan 26 20:13:13 2008 +0100

    ide: remove REQ_TYPE_ATA_CMD
    
    Based on the earlier work by Tejun Heo.
    
    All users are gone so we can finally remove it.
    
    Cc: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bd20a4e8663a..49b7a4c31a6d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -143,7 +143,6 @@ enum rq_cmd_type_bits {
 	 * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver
 	 * private REQ_LB opcodes to differentiate what type of request this is
 	 */
-	REQ_TYPE_ATA_CMD,
 	REQ_TYPE_ATA_TASKFILE,
 	REQ_TYPE_ATA_PC,
 };

commit 9b73e76f3cf63379dcf45fcd4f112f5812418d0a
Merge: 50d9a126240f 23c3e290fb9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 25 17:19:08 2008 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi-misc-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi-misc-2.6: (200 commits)
      [SCSI] usbstorage: use last_sector_bug flag universally
      [SCSI] libsas: abstract STP task status into a function
      [SCSI] ultrastor: clean up inline asm warnings
      [SCSI] aic7xxx: fix firmware build
      [SCSI] aacraid: fib context lock for management ioctls
      [SCSI] ch: remove forward declarations
      [SCSI] ch: fix device minor number management bug
      [SCSI] ch: handle class_device_create failure properly
      [SCSI] NCR5380: fix section mismatch
      [SCSI] sg: fix /proc/scsi/sg/devices when no SCSI devices
      [SCSI] IB/iSER: add logical unit reset support
      [SCSI] don't use __GFP_DMA for sense buffers if not required
      [SCSI] use dynamically allocated sense buffer
      [SCSI] scsi.h: add macro for enclosure bit of inquiry data
      [SCSI] sd: add fix for devices with last sector access problems
      [SCSI] fix pcmcia compile problem
      [SCSI] aacraid: add Voodoo Lite class of cards.
      [SCSI] aacraid: add new driver features flags
      [SCSI] qla2xxx: Update version number to 8.02.00-k7.
      [SCSI] qla2xxx: Issue correct MBC_INITIALIZE_FIRMWARE command.
      ...

commit 29ed2a5f8c4380959f18e9cbaff13bc61e09889c
Author: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>
Date:   Fri Jan 25 22:17:11 2008 +0100

    ide: remove REQ_TYPE_ATA_TASK
    
    Based on the earlier work by Tejun Heo.
    
    All users are gone so we can finally remove it.
    
    Cc: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <bzolnier@gmail.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d18ee67b40f8..40ee1706caa3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -144,7 +144,6 @@ enum rq_cmd_type_bits {
 	 * private REQ_LB opcodes to differentiate what type of request this is
 	 */
 	REQ_TYPE_ATA_CMD,
-	REQ_TYPE_ATA_TASK,
 	REQ_TYPE_ATA_TASKFILE,
 	REQ_TYPE_ATA_PC,
 };

commit 11c3e689f1c3a73e3af7b0ea767b1b0626da8033
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Mon Dec 31 16:37:00 2007 -0600

    [SCSI] block: Introduce new blk_queue_update_dma_alignment interface
    
    The purpose of this is to allow stacked alignment settings, with the
    ultimate queue alignment being set to the largest alignment requirement
    in the stack.
    
    The reason for this is so that the SCSI mid-layer can relax the default
    alignment requirements (which are basically causing a lot of superfluous
    copying to go on in the SG_IO interface) while allowing transports,
    devices or HBAs to add stricter limits if they need them.
    
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d18ee67b40f8..81e99e516302 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -766,6 +766,7 @@ extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
 extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(struct request_queue *, int);
+extern void blk_queue_update_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);

commit 2ad8b1ef11c98c5603580878aebf9f1bc74129e4
Author: Alan D. Brunelle <Alan.Brunelle@hp.com>
Date:   Wed Nov 7 14:26:56 2007 -0500

    Add UNPLUG traces to all appropriate places
    
    Added blk_unplug interface, allowing all invocations of unplugs to result
    in a generated blktrace UNPLUG.
    
    Signed-off-by: Alan D. Brunelle <Alan.Brunelle@hp.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8396db24d019..d18ee67b40f8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -697,6 +697,7 @@ extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 extern int blk_verify_command(unsigned char *, int);
+extern void blk_unplug(struct request_queue *q);
 
 static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {

commit 6eca9004dfcb274a502438a591df5b197690afb1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Oct 25 10:14:47 2007 +0200

    [BLOCK] Fix bad sharing of tag busy list on queues with shared tag maps
    
    For the locking to work, only the tag map and tag bit map may be shared
    (incidentally, I was just explaining this to Nick yesterday, but I
    apparently didn't review the code well enough myself). But we also share
    the busy list!  The busy_list must be queue private, or we need a
    block_queue_tag covering lock as well.
    
    So we have to move the busy_list to the queue. This'll work fine, and
    it'll actually also fix a problem with blk_queue_invalidate_tags() which
    will invalidate tags across all shared queues. This is a bit confusing,
    the low level driver should call it for each queue seperately since
    otherwise you cannot kill tags on just a single queue for eg a hard
    drive that stops responding. Since the function has no callers
    currently, it's not an issue.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bbf906a0b419..8396db24d019 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -341,7 +341,6 @@ enum blk_queue_state {
 struct blk_queue_tag {
 	struct request **tag_index;	/* map of busy tags */
 	unsigned long *tag_map;		/* bit map of free/busy tags */
-	struct list_head busy_list;	/* fifo list of busy tags */
 	int busy;			/* current depth */
 	int max_depth;			/* what we will send to device */
 	int real_max_depth;		/* what the array can hold */
@@ -435,6 +434,7 @@ struct request_queue
 	unsigned int		dma_alignment;
 
 	struct blk_queue_tag	*queue_tags;
+	struct list_head	tag_busy_list;
 
 	unsigned int		nr_sorted;
 	unsigned int		in_flight;

commit fd5d806266935179deda1502101624832eacd01f
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Oct 16 11:05:02 2007 +0200

    block: convert blkdev_issue_flush() to use empty barriers
    
    Then we can get rid of ->issue_flush_fn() and all the driver private
    implementations of that.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fb2ff749dc1f..bbf906a0b419 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -330,7 +330,6 @@ typedef void (unplug_fn) (struct request_queue *);
 
 struct bio_vec;
 typedef int (merge_bvec_fn) (struct request_queue *, struct bio *, struct bio_vec *);
-typedef int (issue_flush_fn) (struct request_queue *, struct gendisk *, sector_t *);
 typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 
@@ -368,7 +367,6 @@ struct request_queue
 	prep_rq_fn		*prep_rq_fn;
 	unplug_fn		*unplug_fn;
 	merge_bvec_fn		*merge_bvec_fn;
-	issue_flush_fn		*issue_flush_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
 
@@ -770,7 +768,6 @@ extern void blk_queue_dma_alignment(struct request_queue *, int);
 extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
-extern void blk_queue_issue_flush_fn(struct request_queue *, issue_flush_fn *);
 extern int blk_do_ordered(struct request_queue *, struct request **);
 extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);

commit bf2de6f5a4faf0197268f18d08969b003b87b6e8
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Sep 27 13:01:25 2007 +0200

    block: Initial support for data-less (or empty) barrier support
    
    This implements functionality to pass down or insert a barrier
    in a queue, without having data attached to it. The ->prepare_flush_fn()
    infrastructure from data barriers are reused to provide this
    functionality.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 610967992ddb..fb2ff749dc1f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -540,6 +540,7 @@ enum {
 #define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
 #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
+#define blk_empty_barrier(rq)	(blk_barrier_rq(rq) && blk_fs_request(rq) && !(rq)->hard_nr_sectors)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 

commit a0cd128542cd9c67f27458a08e989db486a293ce
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Sep 21 10:41:07 2007 +0200

    block: add end_queued_request() and end_dequeued_request() helpers
    
    We can use this helper in the elevator core for BLKPREP_KILL, and it'll
    also be useful for the empty barrier patch.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5ed888b04b29..610967992ddb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -729,7 +729,9 @@ static inline void blk_run_address_space(struct address_space *mapping)
 extern int end_that_request_first(struct request *, int, int);
 extern int end_that_request_chunk(struct request *, int, int);
 extern void end_that_request_last(struct request *, int);
-extern void end_request(struct request *req, int uptodate);
+extern void end_request(struct request *, int);
+extern void end_queued_request(struct request *, int);
+extern void end_dequeued_request(struct request *, int);
 extern void blk_complete_request(struct request *);
 
 /*

commit 2da96acde0318f121ed3f5993ae9324c856ecfd4
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Oct 12 12:40:38 2007 +0200

    [BLOCK] Move sector_div() from blkdev.h to kernel.h
    
    We need it even if CONFIG_BLOCK is disabled, so move it outside of
    the block layer include system.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 95be0ac57e76..5ed888b04b29 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -20,20 +20,6 @@
 
 #include <asm/scatterlist.h>
 
-#ifdef CONFIG_LBD
-# include <asm/div64.h>
-# define sector_div(a, b) do_div(a, b)
-#else
-# define sector_div(n, b)( \
-{ \
-	int _res; \
-	_res = (n) % (b); \
-	(n) /= (b); \
-	_res; \
-} \
-)
-#endif
-
 struct scsi_ioctl_command;
 
 struct request_queue;

commit d24517d793f21edab1a411da95f2c45cb88a84aa
Author: NeilBrown <neilb@suse.de>
Date:   Thu Sep 27 12:46:11 2007 +0200

    Remove flush_dry_bio_endio
    
    The entire function of flush_dry_bio_endio is to undo the effects
    of bio_endio (when called on a barrier request).  So remove the
    function and the call to bio_endio.
    
    This allows us to remove "bi_size" from "struct request_queue".
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    
    ### Diffstat output
     ./block/ll_rw_blk.c      |   39 ++-------------------------------------
     ./include/linux/blkdev.h |    1 -
     2 files changed, 2 insertions(+), 38 deletions(-)
    
    diff .prev/block/ll_rw_blk.c ./block/ll_rw_blk.c
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a0a998140447..95be0ac57e76 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -471,7 +471,6 @@ struct request_queue
 	int			orderr, ordcolor;
 	struct request		pre_flush_rq, bar_rq, post_flush_rq;
 	struct request		*orig_bar_rq;
-	unsigned int		bi_size;
 
 	struct mutex		sysfs_lock;
 

commit f5ff8422bbdd59f8c1f699df248e1b7a11073027
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Sep 21 09:19:54 2007 +0200

    Fix warnings with !CONFIG_BLOCK
    
    Hide everything in blkdev.h with CONFIG_BLOCK isn't set, and fixup
    the (few) files that fail to build because they were relying on blkdev.h
    pulling in extra includes for them.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 492ac946391e..a0a998140447 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1,6 +1,8 @@
 #ifndef _LINUX_BLKDEV_H
 #define _LINUX_BLKDEV_H
 
+#ifdef CONFIG_BLOCK
+
 #include <linux/sched.h>
 #include <linux/major.h>
 #include <linux/genhd.h>
@@ -32,8 +34,6 @@
 )
 #endif
 
-#ifdef CONFIG_BLOCK
-
 struct scsi_ioctl_command;
 
 struct request_queue;

commit 66846572bfb4ec62bcba260028cbbcbdb77bd636
Author: NeilBrown <neilb@suse.de>
Date:   Thu Aug 16 13:31:28 2007 +0200

    Stop exporting blk_rq_bio_prep
    
    blk_rq_bio_prep is exported for use in exactly
    one place.  That place can benefit from using
    the new blk_rq_append_bio instead.
    So
      - change dm-emc to call blk_rq_append_bio
      - stop exporting blk_rq_bio_prep, and
      - initialise rq_disk in blk_rq_bio_prep,
           as dm-emc needs it.
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    
    diff .prev/block/ll_rw_blk.c ./block/ll_rw_blk.c
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3021a5b1d392..492ac946391e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -823,7 +823,6 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 	return bqt->tag_index[tag];
 }
 
-extern void blk_rq_bio_prep(struct request_queue *, struct request *, struct bio *);
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
 #define MAX_PHYS_SEGMENTS 128

commit 3001ca77128273cc5634d79f5306ce2e5a14ec41
Author: NeilBrown <neilb@suse.de>
Date:   Thu Aug 16 13:31:27 2007 +0200

    New function blk_req_append_bio
    
    ll_back_merge_fn is currently exported to SCSI where is it used,
    together with blk_rq_bio_prep, in exactly the same way these
    functions are used in __blk_rq_map_user.
    
    So move the common code into a new function (blk_rq_append_bio), and
    don't export ll_back_merge_fn any longer.
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    
    diff .prev/block/ll_rw_blk.c ./block/ll_rw_blk.c
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a4b13b8a9d09..3021a5b1d392 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -675,8 +675,8 @@ extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 /*
  * Temporary export, until SCSI gets fixed up.
  */
-extern int ll_back_merge_fn(struct request_queue *, struct request *,
-		struct bio *);
+extern int blk_rq_append_bio(struct request_queue *q, struct request *rq,
+			     struct bio *bio);
 
 /*
  * A queue has just exitted congestion.  Note this in the global counter of

commit 5705f7021748a69d84d6567e68e8851dab551464
Author: NeilBrown <neilb@suse.de>
Date:   Tue Sep 25 12:35:59 2007 +0200

    Introduce rq_for_each_segment replacing rq_for_each_bio
    
    Every usage of rq_for_each_bio wraps a usage of
    bio_for_each_segment, so these can be combined into
    rq_for_each_segment.
    
    We define "struct req_iterator" to hold the 'bio' and 'index' that
    are needed for the double iteration.
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    
    Various compile fixes by me...
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b126c6f68e27..a4b13b8a9d09 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -637,10 +637,23 @@ static inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)
 }
 #endif /* CONFIG_MMU */
 
-#define rq_for_each_bio(_bio, rq)	\
+struct req_iterator {
+	int i;
+	struct bio *bio;
+};
+
+/* This should not be used directly - use rq_for_each_segment */
+#define __rq_for_each_bio(_bio, rq)	\
 	if ((rq->bio))			\
 		for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)
 
+#define rq_for_each_segment(bvl, _rq, _iter)			\
+	__rq_for_each_bio(_iter.bio, _rq)			\
+		bio_for_each_segment(bvl, _iter.bio, _iter.i)
+
+#define rq_iter_last(rq, _iter)					\
+		(_iter.bio->bi_next == NULL && _iter.i == _iter.bio->bi_vcnt-1)
+
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);

commit 4e97182a22ed5ca6a5cbc39275d4752d5a4369da
Author: Qi Yong <qiyong@mail.fc-cn.com>
Date:   Wed Jul 25 08:45:51 2007 +0200

    [patch] QUEUE_FLAG_READFULL QUEUE_FLAG_WRITEFULL comment fix
    
    The two comments were transposed.
    
    Signed-off-by: Qi Yong <qiyong@mail.fc-cn.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a1c96d9ee720..b126c6f68e27 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -483,8 +483,8 @@ struct request_queue
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 #define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
-#define	QUEUE_FLAG_READFULL	3	/* write queue has been filled */
-#define QUEUE_FLAG_WRITEFULL	4	/* read queue has been filled */
+#define	QUEUE_FLAG_READFULL	3	/* read queue has been filled */
+#define QUEUE_FLAG_WRITEFULL	4	/* write queue has been filled */
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */

commit 71f65e6bd7651610d2d6aeb3c12aab63667ace30
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jul 24 10:29:42 2007 +0200

    [BLOCK] Add request_queue_t and mark it deprecated
    
    Andrew thinks I should be nice and allow outside code to at least just
    compile, so add the request_queue_t typedef back and mark it deprecated.
    It'll warn people that this type is going away soonish.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4be37de02052..a1c96d9ee720 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -37,6 +37,7 @@
 struct scsi_ioctl_command;
 
 struct request_queue;
+typedef struct request_queue request_queue_t __deprecated;
 struct elevator_queue;
 typedef struct elevator_queue elevator_t;
 struct request_pm_state;

commit 165125e1e480f9510a5ffcfbfee4e3ee38c05f23
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jul 24 09:28:11 2007 +0200

    [BLOCK] Get rid of request_queue_t typedef
    
    Some of the code has been gradually transitioned to using the proper
    struct request_queue, but there's lots left. So do a full sweet of
    the kernel and get rid of this typedef and replace its uses with
    the proper type.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 695e34964cb7..4be37de02052 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -37,7 +37,6 @@
 struct scsi_ioctl_command;
 
 struct request_queue;
-typedef struct request_queue request_queue_t;
 struct elevator_queue;
 typedef struct elevator_queue elevator_t;
 struct request_pm_state;
@@ -233,7 +232,7 @@ struct request {
 	struct list_head queuelist;
 	struct list_head donelist;
 
-	request_queue_t *q;
+	struct request_queue *q;
 
 	unsigned int cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
@@ -337,15 +336,15 @@ struct request_pm_state
 
 #include <linux/elevator.h>
 
-typedef void (request_fn_proc) (request_queue_t *q);
-typedef int (make_request_fn) (request_queue_t *q, struct bio *bio);
-typedef int (prep_rq_fn) (request_queue_t *, struct request *);
-typedef void (unplug_fn) (request_queue_t *);
+typedef void (request_fn_proc) (struct request_queue *q);
+typedef int (make_request_fn) (struct request_queue *q, struct bio *bio);
+typedef int (prep_rq_fn) (struct request_queue *, struct request *);
+typedef void (unplug_fn) (struct request_queue *);
 
 struct bio_vec;
-typedef int (merge_bvec_fn) (request_queue_t *, struct bio *, struct bio_vec *);
-typedef int (issue_flush_fn) (request_queue_t *, struct gendisk *, sector_t *);
-typedef void (prepare_flush_fn) (request_queue_t *, struct request *);
+typedef int (merge_bvec_fn) (struct request_queue *, struct bio *, struct bio_vec *);
+typedef int (issue_flush_fn) (struct request_queue *, struct gendisk *, sector_t *);
+typedef void (prepare_flush_fn) (struct request_queue *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
 
 enum blk_queue_state {
@@ -626,13 +625,13 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
 
 #ifdef CONFIG_BOUNCE
 extern int init_emergency_isa_pool(void);
-extern void blk_queue_bounce(request_queue_t *q, struct bio **bio);
+extern void blk_queue_bounce(struct request_queue *q, struct bio **bio);
 #else
 static inline int init_emergency_isa_pool(void)
 {
 	return 0;
 }
-static inline void blk_queue_bounce(request_queue_t *q, struct bio **bio)
+static inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)
 {
 }
 #endif /* CONFIG_MMU */
@@ -646,14 +645,14 @@ extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);
 extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
-extern void __blk_put_request(request_queue_t *, struct request *);
+extern void __blk_put_request(struct request_queue *, struct request *);
 extern void blk_end_sync_rq(struct request *rq, int error);
-extern struct request *blk_get_request(request_queue_t *, int, gfp_t);
-extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
-extern void blk_requeue_request(request_queue_t *, struct request *);
-extern void blk_plug_device(request_queue_t *);
-extern int blk_remove_plug(request_queue_t *);
-extern void blk_recount_segments(request_queue_t *, struct bio *);
+extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
+extern void blk_insert_request(struct request_queue *, struct request *, int, void *);
+extern void blk_requeue_request(struct request_queue *, struct request *);
+extern void blk_plug_device(struct request_queue *);
+extern int blk_remove_plug(struct request_queue *);
+extern void blk_recount_segments(struct request_queue *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct request_queue *,
 			  struct gendisk *, unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct file *, struct request_queue *,
@@ -662,14 +661,15 @@ extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 /*
  * Temporary export, until SCSI gets fixed up.
  */
-extern int ll_back_merge_fn(request_queue_t *, struct request *, struct bio *);
+extern int ll_back_merge_fn(struct request_queue *, struct request *,
+		struct bio *);
 
 /*
  * A queue has just exitted congestion.  Note this in the global counter of
  * congested queues, and wake up anyone who was waiting for requests to be
  * put back.
  */
-static inline void blk_clear_queue_congested(request_queue_t *q, int rw)
+static inline void blk_clear_queue_congested(struct request_queue *q, int rw)
 {
 	clear_bdi_congested(&q->backing_dev_info, rw);
 }
@@ -678,29 +678,29 @@ static inline void blk_clear_queue_congested(request_queue_t *q, int rw)
  * A queue has just entered congestion.  Flag that in the queue's VM-visible
  * state flags and increment the global gounter of congested queues.
  */
-static inline void blk_set_queue_congested(request_queue_t *q, int rw)
+static inline void blk_set_queue_congested(struct request_queue *q, int rw)
 {
 	set_bdi_congested(&q->backing_dev_info, rw);
 }
 
-extern void blk_start_queue(request_queue_t *q);
-extern void blk_stop_queue(request_queue_t *q);
+extern void blk_start_queue(struct request_queue *q);
+extern void blk_stop_queue(struct request_queue *q);
 extern void blk_sync_queue(struct request_queue *q);
-extern void __blk_stop_queue(request_queue_t *q);
-extern void blk_run_queue(request_queue_t *);
-extern void blk_start_queueing(request_queue_t *);
-extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned long);
+extern void __blk_stop_queue(struct request_queue *q);
+extern void blk_run_queue(struct request_queue *);
+extern void blk_start_queueing(struct request_queue *);
+extern int blk_rq_map_user(struct request_queue *, struct request *, void __user *, unsigned long);
 extern int blk_rq_unmap_user(struct bio *);
-extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);
-extern int blk_rq_map_user_iov(request_queue_t *, struct request *,
+extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
+extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
 			       struct sg_iovec *, int, unsigned int);
-extern int blk_execute_rq(request_queue_t *, struct gendisk *,
+extern int blk_execute_rq(struct request_queue *, struct gendisk *,
 			  struct request *, int);
-extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
+extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
 extern int blk_verify_command(unsigned char *, int);
 
-static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
+static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 {
 	return bdev->bd_disk->queue;
 }
@@ -749,41 +749,41 @@ static inline void blkdev_dequeue_request(struct request *req)
 /*
  * Access functions for manipulating queue properties
  */
-extern request_queue_t *blk_init_queue_node(request_fn_proc *rfn,
+extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
 					spinlock_t *lock, int node_id);
-extern request_queue_t *blk_init_queue(request_fn_proc *, spinlock_t *);
-extern void blk_cleanup_queue(request_queue_t *);
-extern void blk_queue_make_request(request_queue_t *, make_request_fn *);
-extern void blk_queue_bounce_limit(request_queue_t *, u64);
-extern void blk_queue_max_sectors(request_queue_t *, unsigned int);
-extern void blk_queue_max_phys_segments(request_queue_t *, unsigned short);
-extern void blk_queue_max_hw_segments(request_queue_t *, unsigned short);
-extern void blk_queue_max_segment_size(request_queue_t *, unsigned int);
-extern void blk_queue_hardsect_size(request_queue_t *, unsigned short);
-extern void blk_queue_stack_limits(request_queue_t *t, request_queue_t *b);
-extern void blk_queue_segment_boundary(request_queue_t *, unsigned long);
-extern void blk_queue_prep_rq(request_queue_t *, prep_rq_fn *pfn);
-extern void blk_queue_merge_bvec(request_queue_t *, merge_bvec_fn *);
-extern void blk_queue_dma_alignment(request_queue_t *, int);
-extern void blk_queue_softirq_done(request_queue_t *, softirq_done_fn *);
+extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
+extern void blk_cleanup_queue(struct request_queue *);
+extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
+extern void blk_queue_bounce_limit(struct request_queue *, u64);
+extern void blk_queue_max_sectors(struct request_queue *, unsigned int);
+extern void blk_queue_max_phys_segments(struct request_queue *, unsigned short);
+extern void blk_queue_max_hw_segments(struct request_queue *, unsigned short);
+extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
+extern void blk_queue_hardsect_size(struct request_queue *, unsigned short);
+extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
+extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
+extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
+extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
+extern void blk_queue_dma_alignment(struct request_queue *, int);
+extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern int blk_queue_ordered(request_queue_t *, unsigned, prepare_flush_fn *);
-extern void blk_queue_issue_flush_fn(request_queue_t *, issue_flush_fn *);
-extern int blk_do_ordered(request_queue_t *, struct request **);
-extern unsigned blk_ordered_cur_seq(request_queue_t *);
+extern int blk_queue_ordered(struct request_queue *, unsigned, prepare_flush_fn *);
+extern void blk_queue_issue_flush_fn(struct request_queue *, issue_flush_fn *);
+extern int blk_do_ordered(struct request_queue *, struct request **);
+extern unsigned blk_ordered_cur_seq(struct request_queue *);
 extern unsigned blk_ordered_req_seq(struct request *);
-extern void blk_ordered_complete_seq(request_queue_t *, unsigned, int);
+extern void blk_ordered_complete_seq(struct request_queue *, unsigned, int);
 
-extern int blk_rq_map_sg(request_queue_t *, struct request *, struct scatterlist *);
+extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
-extern void generic_unplug_device(request_queue_t *);
-extern void __generic_unplug_device(request_queue_t *);
+extern void generic_unplug_device(struct request_queue *);
+extern void __generic_unplug_device(struct request_queue *);
 extern long nr_blockdev_pages(void);
 
-int blk_get_queue(request_queue_t *);
-request_queue_t *blk_alloc_queue(gfp_t);
-request_queue_t *blk_alloc_queue_node(gfp_t, int);
-extern void blk_put_queue(request_queue_t *);
+int blk_get_queue(struct request_queue *);
+struct request_queue *blk_alloc_queue(gfp_t);
+struct request_queue *blk_alloc_queue_node(gfp_t, int);
+extern void blk_put_queue(struct request_queue *);
 
 /*
  * tag stuff
@@ -791,13 +791,13 @@ extern void blk_put_queue(request_queue_t *);
 #define blk_queue_tag_depth(q)		((q)->queue_tags->busy)
 #define blk_queue_tag_queue(q)		((q)->queue_tags->busy < (q)->queue_tags->max_depth)
 #define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
-extern int blk_queue_start_tag(request_queue_t *, struct request *);
-extern struct request *blk_queue_find_tag(request_queue_t *, int);
-extern void blk_queue_end_tag(request_queue_t *, struct request *);
-extern int blk_queue_init_tags(request_queue_t *, int, struct blk_queue_tag *);
-extern void blk_queue_free_tags(request_queue_t *);
-extern int blk_queue_resize_tags(request_queue_t *, int);
-extern void blk_queue_invalidate_tags(request_queue_t *);
+extern int blk_queue_start_tag(struct request_queue *, struct request *);
+extern struct request *blk_queue_find_tag(struct request_queue *, int);
+extern void blk_queue_end_tag(struct request_queue *, struct request *);
+extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *);
+extern void blk_queue_free_tags(struct request_queue *);
+extern int blk_queue_resize_tags(struct request_queue *, int);
+extern void blk_queue_invalidate_tags(struct request_queue *);
 extern struct blk_queue_tag *blk_init_tags(int);
 extern void blk_free_tags(struct blk_queue_tag *);
 
@@ -809,7 +809,7 @@ static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 	return bqt->tag_index[tag];
 }
 
-extern void blk_rq_bio_prep(request_queue_t *, struct request *, struct bio *);
+extern void blk_rq_bio_prep(struct request_queue *, struct request *, struct bio *);
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
 #define MAX_PHYS_SEGMENTS 128
@@ -821,7 +821,7 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
-static inline int queue_hardsect_size(request_queue_t *q)
+static inline int queue_hardsect_size(struct request_queue *q)
 {
 	int retval = 512;
 
@@ -836,7 +836,7 @@ static inline int bdev_hardsect_size(struct block_device *bdev)
 	return queue_hardsect_size(bdev_get_queue(bdev));
 }
 
-static inline int queue_dma_alignment(request_queue_t *q)
+static inline int queue_dma_alignment(struct request_queue *q)
 {
 	int retval = 511;
 

commit 41e1703b9b88cf9b5e91cdd2f7dcded3ec3917cb
Author: FUJITA Tomonori <tomof@acm.org>
Date:   Sun Jul 22 10:06:50 2007 +0900

    [SCSI] bsg: unexport sg v3 helper functions
    
    blk_fill_sghdr_rq, blk_unmap_sghdr_rq, and blk_complete_sghdr_rq were
    exported for bsg, however bsg was changed to support only sg v4.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f78965fc6426..695e34964cb7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -698,11 +698,6 @@ extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
-extern int blk_fill_sghdr_rq(request_queue_t *, struct request *,
-		      struct sg_io_hdr *, int);
-extern int blk_unmap_sghdr_rq(struct request *, struct sg_io_hdr *);
-extern int blk_complete_sghdr_rq(struct request *, struct sg_io_hdr *,
-			  struct bio *);
 extern int blk_verify_command(unsigned char *, int);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)

commit 2a7326b5bbafac4c96bcdb944b2a773593030b96
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Jul 17 04:03:37 2007 -0700

    CONFIG_BOUNCE to avoid useless inclusion of bounce buffer logic
    
    The bounce buffer logic is included on systems that do not need it.  If a
    system does not have zones like ZONE_DMA and ZONE_HIGHMEM that can lead to
    the use of bounce buffers then there is no need to reserve memory pools etc
    etc.  This is true f.e.  for SGI Altix.
    
    Also nicifies the Makefile and gets rid of the tricky "and" there.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b32564a1e105..f78965fc6426 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -624,7 +624,7 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
  */
 #define BLK_DEFAULT_SG_TIMEOUT	(60 * HZ)
 
-#ifdef CONFIG_MMU
+#ifdef CONFIG_BOUNCE
 extern int init_emergency_isa_pool(void);
 extern void blk_queue_bounce(request_queue_t *q, struct bio **bio);
 #else

commit abae1fde63fcdd2a3abaa0d7930938d8326f83d2
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon Jul 16 08:52:14 2007 +0200

    add a struct request pointer to the request structure
    
    This adds a struct request pointer to the request structure for the
    second data phase (bidi for now). A request queue supporting bidi
    requests sets QUEUE_FLAG_BIDI. This prevents sending bidi requests to
    a non-bidi queue.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 24b474e05a44..b32564a1e105 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -317,6 +317,9 @@ struct request {
 	 */
 	rq_end_io_fn *end_io;
 	void *end_io_data;
+
+	/* for bidi */
+	struct request *next_rq;
 };
 
 /*
@@ -486,6 +489,7 @@ struct request_queue
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
 #define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
+#define QUEUE_FLAG_BIDI		9	/* queue supports bidi requests */
 
 enum {
 	/*
@@ -550,6 +554,7 @@ enum {
 #define blk_sorted_rq(rq)	((rq)->cmd_flags & REQ_SORTED)
 #define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
 #define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
+#define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 

commit d351af01b9307566135cb0f355ca65d0952c10b5
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon Jul 9 12:40:35 2007 +0200

    bsg: bind bsg to request_queue instead of gendisk
    
    This patch binds bsg devices to request_queue instead of gendisk. Any
    objects (like transport entities) can define own request_handler and
    create own bsg device.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2746632c2267..24b474e05a44 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -14,6 +14,7 @@
 #include <linux/bio.h>
 #include <linux/module.h>
 #include <linux/stringify.h>
+#include <linux/bsg.h>
 
 #include <asm/scatterlist.h>
 
@@ -470,6 +471,10 @@ struct request_queue
 	unsigned int		bi_size;
 
 	struct mutex		sysfs_lock;
+
+#if defined(CONFIG_BLK_DEV_BSG)
+	struct bsg_class_device bsg_dev;
+#endif
 };
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */

commit 45e79a3acdcf54113b3d7b23e9e64e6541dbfeb5
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon Jul 9 12:39:20 2007 +0200

    bsg: add a request_queue argument to scsi_cmd_ioctl()
    
    bsg uses scsi_cmd_ioctl() for some SCSI/sg ioctl
    commands. scsi_cmd_ioctl() gets a request queue from a gendisk
    arguement. This prevents bsg being bound to SCSI devices that don't
    have a gendisk (like OSD). This adds a request_queue argument to
    scsi_cmd_ioctl(). The SCSI/sg ioctl commands doesn't use a gendisk so
    it's safe for any SCSI devices to use scsi_cmd_ioctl().
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f6bc0d03ffad..2746632c2267 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -644,7 +644,8 @@ extern void blk_requeue_request(request_queue_t *, struct request *);
 extern void blk_plug_device(request_queue_t *);
 extern int blk_remove_plug(request_queue_t *);
 extern void blk_recount_segments(request_queue_t *, struct bio *);
-extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
+extern int scsi_cmd_ioctl(struct file *, struct request_queue *,
+			  struct gendisk *, unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 		struct gendisk *, struct scsi_ioctl_command __user *);
 

commit 337ad41deae1b56e56731246322a93251df86e79
Author: FUJITA Tomonori <tomof@acm.org>
Date:   Wed Dec 20 11:18:54 2006 +0100

    block: export blk_verify_command for SG v4
    
    blk_fill_sghdr_rq doesn't work for SG v4 so verify_command needed to
    be exported.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 53002d40efa2..f6bc0d03ffad 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -692,6 +692,7 @@ extern int blk_fill_sghdr_rq(request_queue_t *, struct request *,
 extern int blk_unmap_sghdr_rq(struct request *, struct sg_io_hdr *);
 extern int blk_complete_sghdr_rq(struct request *, struct sg_io_hdr *,
 			  struct bio *);
+extern int blk_verify_command(unsigned char *, int);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
 {

commit 3d6392cfbd7dc11f23058e3493683afab4ac13a3
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jul 9 12:38:05 2007 +0200

    bsg: support for full generic block layer SG v3
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fae138bd2207..53002d40efa2 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -41,6 +41,8 @@ struct elevator_queue;
 typedef struct elevator_queue elevator_t;
 struct request_pm_state;
 struct blk_trace;
+struct request;
+struct sg_io_hdr;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -607,6 +609,11 @@ extern unsigned long blk_max_low_pfn, blk_max_pfn;
 #define BLK_BOUNCE_ANY		((u64)blk_max_pfn << PAGE_SHIFT)
 #define BLK_BOUNCE_ISA		(ISA_DMA_THRESHOLD)
 
+/*
+ * default timeout for SG_IO if none specified
+ */
+#define BLK_DEFAULT_SG_TIMEOUT	(60 * HZ)
+
 #ifdef CONFIG_MMU
 extern int init_emergency_isa_pool(void);
 extern void blk_queue_bounce(request_queue_t *q, struct bio **bio);
@@ -680,6 +687,11 @@ extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
 				  struct request *, int, rq_end_io_fn *);
+extern int blk_fill_sghdr_rq(request_queue_t *, struct request *,
+		      struct sg_io_hdr *, int);
+extern int blk_unmap_sghdr_rq(struct request *, struct sg_io_hdr *);
+extern int blk_complete_sghdr_rq(struct request *, struct sg_io_hdr *,
+			  struct bio *);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
 {

commit 7deeed13170e634adc4552ff94588d6301a3da83
Author: Benjamin Gilbert <bgilbert@cs.cmu.edu>
Date:   Tue Jun 19 09:05:21 2007 +0200

    [TRIVIAL PATCH] Kill blk_congestion_wait() stub for !CONFIG_BLOCK
    
    blk_congestion_wait() doesn't exist anymore, but there's still a stub
    in blkdev.h for the !CONFIG_BLOCK case.  Kill it.
    
    Signed-off-by: Benjamin Gilbert <bgilbert@cs.cmu.edu>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index db5b00a792f5..fae138bd2207 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -868,11 +868,6 @@ void kblockd_flush_work(struct work_struct *work);
  */
 #define buffer_heads_over_limit 0
 
-static inline long blk_congestion_wait(int rw, long timeout)
-{
-	return io_schedule_timeout(timeout);
-}
-
 static inline long nr_blockdev_pages(void)
 {
 	return 0;

commit 19a75d83ffeab004cfcfac64024ad3997bac7220
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed May 9 02:33:56 2007 -0700

    kblockd: use flush_work
    
    Switch the kblockd flushing from a global flush to a more specific
    flush_work().
    
    (akpm: bypassed maintainers, sorry.  There are other patches which depend on
    this)
    
    Cc: "Maciej W. Rozycki" <macro@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jens Axboe <axboe@suse.de>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a686eabe22d6..db5b00a792f5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -854,7 +854,7 @@ static inline void put_dev_sector(Sector p)
 
 struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
-void kblockd_flush(void);
+void kblockd_flush_work(struct work_struct *work);
 
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))

commit 4e521c27eee33cebd618c26649e2c93803004647
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Apr 24 21:17:33 2007 +0200

    ll_rw_blk: add io_context private pointer
    
    To be used by as/cfq as they see fit.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 83dcd8c0e974..a686eabe22d6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -116,6 +116,7 @@ struct io_context {
 
 	struct as_io_context *aic;
 	struct rb_root cic_root;
+	void *ioc_data;
 };
 
 void put_io_context(struct io_context *ioc);

commit aaf1228ddfb44f04c87d1e7dfc5ccffdba74363d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jan 19 11:30:16 2007 +1100

    cfq-iosched: remove cfq_io_context last_queue
    
    It hasn't been used for a while, kill it off and remove the old
    if 0 code chunk.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 36a6eacefe20..83dcd8c0e974 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -82,7 +82,6 @@ struct cfq_io_context {
 
 	unsigned long last_end_request;
 	sector_t last_request_pos;
- 	unsigned long last_queue;
 
 	unsigned long ttime_total;
 	unsigned long ttime_samples;

commit 8e5cfc45e7527eb5c8a9a22d56a7b9227e7c0913
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Dec 19 11:12:46 2006 +0100

    [PATCH] Fixup blk_rq_unmap_user() API
    
    The blk_rq_unmap_user() API is not very nice. It expects the caller to
    know that rq->bio has to be reset to the original bio, and it will
    silently do nothing if that is not done. Instead make it explicit that
    we need to pass in the first bio, by expecting a bio argument.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0fa33017ec02..36a6eacefe20 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -672,7 +672,7 @@ extern void __blk_stop_queue(request_queue_t *q);
 extern void blk_run_queue(request_queue_t *);
 extern void blk_start_queueing(request_queue_t *);
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned long);
-extern int blk_rq_unmap_user(struct request *);
+extern int blk_rq_unmap_user(struct bio *);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(request_queue_t *, struct request *,
 			       struct sg_iovec *, int, unsigned int);

commit 1aa4f24fe96938cabe7a1e9da8bc3bfbd1dfe3fa
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Dec 19 08:33:11 2006 +0100

    [PATCH] Remove queue merging hooks
    
    We have full flexibility of merging parameters now, so we can remove the
    hooks that define back/front/request merge strategies. Nobody is using
    them anymore.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ea330d7b46c0..0fa33017ec02 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -331,10 +331,6 @@ struct request_pm_state
 
 #include <linux/elevator.h>
 
-typedef int (merge_request_fn) (request_queue_t *, struct request *,
-				struct bio *);
-typedef int (merge_requests_fn) (request_queue_t *, struct request *,
-				 struct request *);
 typedef void (request_fn_proc) (request_queue_t *q);
 typedef int (make_request_fn) (request_queue_t *q, struct bio *bio);
 typedef int (prep_rq_fn) (request_queue_t *, struct request *);
@@ -376,9 +372,6 @@ struct request_queue
 	struct request_list	rq;
 
 	request_fn_proc		*request_fn;
-	merge_request_fn	*back_merge_fn;
-	merge_request_fn	*front_merge_fn;
-	merge_requests_fn	*merge_requests_fn;
 	make_request_fn		*make_request_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unplug_fn		*unplug_fn;
@@ -648,6 +641,11 @@ extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __
 extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 		struct gendisk *, struct scsi_ioctl_command __user *);
 
+/*
+ * Temporary export, until SCSI gets fixed up.
+ */
+extern int ll_back_merge_fn(request_queue_t *, struct request *, struct bio *);
+
 /*
  * A queue has just exitted congestion.  Note this in the global counter of
  * congested queues, and wake up anyone who was waiting for requests to be

commit 2b02a1792000900ead4638f4d42fcdd742062cfa
Author: Boaz Harrosh <bharrosh@panasas.com>
Date:   Tue Dec 5 10:19:14 2006 +0100

    [PATCH] remove blk_queue_activity_fn
    
    While working on bidi support at struct request level
    I have found that blk_queue_activity_fn is actually never used.
    The only user is in ide-probe.c with this code:
    
            /* enable led activity for disk drives only */
            if (drive->media == ide_disk && hwif->led_act)
                    blk_queue_activity_fn(q, hwif->led_act, drive);
    
    And led_act is never initialized anywhere.
    (Looking back at older kernels it was used in the PPC arch, but was removed around 2.6.18)
    Unless it is all for future use off course.
    (this patch is against linux-2.6-block.git as off 2006/12/4)
    
    Signed-off-by: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e1c7286165ff..ea330d7b46c0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -342,7 +342,6 @@ typedef void (unplug_fn) (request_queue_t *);
 
 struct bio_vec;
 typedef int (merge_bvec_fn) (request_queue_t *, struct bio *, struct bio_vec *);
-typedef void (activity_fn) (void *data, int rw);
 typedef int (issue_flush_fn) (request_queue_t *, struct gendisk *, sector_t *);
 typedef void (prepare_flush_fn) (request_queue_t *, struct request *);
 typedef void (softirq_done_fn)(struct request *);
@@ -384,7 +383,6 @@ struct request_queue
 	prep_rq_fn		*prep_rq_fn;
 	unplug_fn		*unplug_fn;
 	merge_bvec_fn		*merge_bvec_fn;
-	activity_fn		*activity_fn;
 	issue_flush_fn		*issue_flush_fn;
 	prepare_flush_fn	*prepare_flush_fn;
 	softirq_done_fn		*softirq_done_fn;
@@ -411,8 +409,6 @@ struct request_queue
 	 */
 	void			*queuedata;
 
-	void			*activity_data;
-
 	/*
 	 * queue needs bounce pages for pages above this limit
 	 */
@@ -677,7 +673,6 @@ extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(request_queue_t *q);
 extern void blk_run_queue(request_queue_t *);
 extern void blk_start_queueing(request_queue_t *);
-extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned long);
 extern int blk_rq_unmap_user(struct request *);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);

commit 0e75f9063f5c55fb0b0b546a7c356f8ec186825e
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Fri Dec 1 10:40:55 2006 +0100

    [PATCH] block: support larger block pc requests
    
    This patch modifies blk_rq_map/unmap_user() and the cdrom and scsi_ioctl.c
    users so that it supports requests larger than bio by chaining them together.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7bfcde2d5578..e1c7286165ff 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -678,10 +678,11 @@ extern void __blk_stop_queue(request_queue_t *q);
 extern void blk_run_queue(request_queue_t *);
 extern void blk_start_queueing(request_queue_t *);
 extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
-extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
-extern int blk_rq_unmap_user(struct bio *, unsigned int);
+extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned long);
+extern int blk_rq_unmap_user(struct request *);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);
-extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
+extern int blk_rq_map_user_iov(request_queue_t *, struct request *,
+			       struct sg_iovec *, int, unsigned int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,

commit 3fcfab16c5b86eaa3db3a9a31adba550c5b67141
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Oct 19 23:28:16 2006 -0700

    [PATCH] separate bdi congestion functions from queue congestion functions
    
    Separate out the concept of "queue congestion" from "backing-dev congestion".
    Congestion is a backing-dev concept, not a queue concept.
    
    The blk_* congestion functions are retained, as wrappers around the core
    backing-dev congestion functions.
    
    This proper layering is needed so that NFS can cleanly use the congestion
    functions, and so that CONFIG_BLOCK=n actually links.
    
    Cc: "Thomas Maier" <balagi@justmail.de>
    Cc: "Jens Axboe" <jens.axboe@oracle.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Peter Osterlund <petero2@telia.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9575e3a5ff2a..7bfcde2d5578 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -651,8 +651,26 @@ extern void blk_recount_segments(request_queue_t *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 		struct gendisk *, struct scsi_ioctl_command __user *);
-extern void blk_clear_queue_congested(request_queue_t *q, int rw);
-extern void blk_set_queue_congested(request_queue_t *q, int rw);
+
+/*
+ * A queue has just exitted congestion.  Note this in the global counter of
+ * congested queues, and wake up anyone who was waiting for requests to be
+ * put back.
+ */
+static inline void blk_clear_queue_congested(request_queue_t *q, int rw)
+{
+	clear_bdi_congested(&q->backing_dev_info, rw);
+}
+
+/*
+ * A queue has just entered congestion.  Flag that in the queue's VM-visible
+ * state flags and increment the global gounter of congested queues.
+ */
+static inline void blk_set_queue_congested(request_queue_t *q, int rw)
+{
+	set_bdi_congested(&q->backing_dev_info, rw);
+}
+
 extern void blk_start_queue(request_queue_t *q);
 extern void blk_stop_queue(request_queue_t *q);
 extern void blk_sync_queue(struct request_queue *q);
@@ -767,10 +785,8 @@ extern int blk_queue_init_tags(request_queue_t *, int, struct blk_queue_tag *);
 extern void blk_queue_free_tags(request_queue_t *);
 extern int blk_queue_resize_tags(request_queue_t *, int);
 extern void blk_queue_invalidate_tags(request_queue_t *);
-extern long blk_congestion_wait(int rw, long timeout);
 extern struct blk_queue_tag *blk_init_tags(int);
 extern void blk_free_tags(struct blk_queue_tag *);
-extern void blk_congestion_end(int rw);
 
 static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
 						int tag)

commit 79e2de4bc53d7ca2a8eedee49e4a92479b4b530e
Author: Thomas Maier <balagi@justmail.de>
Date:   Thu Oct 19 23:28:15 2006 -0700

    [PATCH] export clear_queue_congested and set_queue_congested
    
    Export the clear_queue_congested() and set_queue_congested() functions
    located in ll_rw_blk.c
    
    The functions are renamed to blk_clear_queue_congested() and
    blk_set_queue_congested().
    
    (needed in the pktcdvd driver's bio write congestion control)
    
    Signed-off-by: Thomas Maier <balagi@justmail.de>
    Cc: Peter Osterlund <petero2@telia.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d370d2cfe138..9575e3a5ff2a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -651,6 +651,8 @@ extern void blk_recount_segments(request_queue_t *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
 extern int sg_scsi_ioctl(struct file *, struct request_queue *,
 		struct gendisk *, struct scsi_ioctl_command __user *);
+extern void blk_clear_queue_congested(request_queue_t *q, int rw);
+extern void blk_set_queue_congested(request_queue_t *q, int rw);
 extern void blk_start_queue(request_queue_t *q);
 extern void blk_stop_queue(request_queue_t *q);
 extern void blk_sync_queue(struct request_queue *q);

commit cea2885a2e989d1dc19af1fc991717b33b7d1456
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Oct 12 15:08:45 2006 +0200

    [PATCH] ide-cd: fix breakage with internally queued commands
    
    We still need to maintain a private PC style command, since it
    isn't completely unified with REQ_TYPE_BLOCK_PC yet.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 26f7856ff812..d370d2cfe138 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -157,6 +157,7 @@ enum rq_cmd_type_bits {
 	REQ_TYPE_ATA_CMD,
 	REQ_TYPE_ATA_TASK,
 	REQ_TYPE_ATA_TASKFILE,
+	REQ_TYPE_ATA_PC,
 };
 
 /*

commit f583f4924d669d36de677e0cc2422ee95203d444
Author: David C Somayajulu <david.somayajulu@qlogic.com>
Date:   Wed Oct 4 08:27:25 2006 +0200

    [PATCH] helper function for retrieving scsi_cmd given host based block layer tag
    
    This was necessitated by the need for a function to get back
    to a scsi_cmnd, when an hba the posts its (corresponding) completion
    interrupt with a block layer tag as its reference.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: David Somayajulu <david.somayajulu@qlogic.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1d79b8d4ca6d..26f7856ff812 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -769,6 +769,14 @@ extern struct blk_queue_tag *blk_init_tags(int);
 extern void blk_free_tags(struct blk_queue_tag *);
 extern void blk_congestion_end(int rw);
 
+static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
+						int tag)
+{
+	if (unlikely(bqt == NULL || tag >= bqt->real_max_depth))
+		return NULL;
+	return bqt->tag_index[tag];
+}
+
 extern void blk_rq_bio_prep(request_queue_t *, struct request *, struct bio *);
 extern int blkdev_issue_flush(struct block_device *, sector_t *);
 

commit bcfd8d36151e531e1c6c731f1fbf792509a1c494
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Aug 31 12:56:06 2006 +0200

    [PATCH] CONFIG_BLOCK: blk_congestion_wait() fix
    
    Don't just do nothing: it'll cause busywaits all over writeback and page
    reclaim.
    
    For now, take a fixed-length nap.  Will improve when NFS starts waking up
    throttled processes.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3e36107d342a..1d79b8d4ca6d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1,6 +1,7 @@
 #ifndef _LINUX_BLKDEV_H
 #define _LINUX_BLKDEV_H
 
+#include <linux/sched.h>
 #include <linux/major.h>
 #include <linux/genhd.h>
 #include <linux/list.h>
@@ -848,7 +849,7 @@ void kblockd_flush(void);
 
 static inline long blk_congestion_wait(int rw, long timeout)
 {
-	return timeout;
+	return io_schedule_timeout(timeout);
 }
 
 static inline long nr_blockdev_pages(void)
@@ -856,7 +857,9 @@ static inline long nr_blockdev_pages(void)
 	return 0;
 }
 
-static inline void exit_io_context(void) {}
+static inline void exit_io_context(void)
+{
+}
 
 #endif /* CONFIG_BLOCK */
 

commit 9361401eb7619c033e2394e4f9f6d410d6719ac7
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 30 20:45:40 2006 +0200

    [PATCH] BLOCK: Make it possible to disable the block layer [try #6]
    
    Make it possible to disable the block layer.  Not all embedded devices require
    it, some can make do with just JFFS2, NFS, ramfs, etc - none of which require
    the block layer to be present.
    
    This patch does the following:
    
     (*) Introduces CONFIG_BLOCK to disable the block layer, buffering and blockdev
         support.
    
     (*) Adds dependencies on CONFIG_BLOCK to any configuration item that controls
         an item that uses the block layer.  This includes:
    
         (*) Block I/O tracing.
    
         (*) Disk partition code.
    
         (*) All filesystems that are block based, eg: Ext3, ReiserFS, ISOFS.
    
         (*) The SCSI layer.  As far as I can tell, even SCSI chardevs use the
             block layer to do scheduling.  Some drivers that use SCSI facilities -
             such as USB storage - end up disabled indirectly from this.
    
         (*) Various block-based device drivers, such as IDE and the old CDROM
             drivers.
    
         (*) MTD blockdev handling and FTL.
    
         (*) JFFS - which uses set_bdev_super(), something it could avoid doing by
             taking a leaf out of JFFS2's book.
    
     (*) Makes most of the contents of linux/blkdev.h, linux/buffer_head.h and
         linux/elevator.h contingent on CONFIG_BLOCK being set.  sector_div() is,
         however, still used in places, and so is still available.
    
     (*) Also made contingent are the contents of linux/mpage.h, linux/genhd.h and
         parts of linux/fs.h.
    
     (*) Makes a number of files in fs/ contingent on CONFIG_BLOCK.
    
     (*) Makes mm/bounce.c (bounce buffering) contingent on CONFIG_BLOCK.
    
     (*) set_page_dirty() doesn't call __set_page_dirty_buffers() if CONFIG_BLOCK
         is not enabled.
    
     (*) fs/no-block.c is created to hold out-of-line stubs and things that are
         required when CONFIG_BLOCK is not set:
    
         (*) Default blockdev file operations (to give error ENODEV on opening).
    
     (*) Makes some /proc changes:
    
         (*) /proc/devices does not list any blockdevs.
    
         (*) /proc/diskstats and /proc/partitions are contingent on CONFIG_BLOCK.
    
     (*) Makes some compat ioctl handling contingent on CONFIG_BLOCK.
    
     (*) If CONFIG_BLOCK is not defined, makes sys_quotactl() return -ENODEV if
         given command other than Q_SYNC or if a special device is specified.
    
     (*) In init/do_mounts.c, no reference is made to the blockdev routines if
         CONFIG_BLOCK is not defined.  This does not prohibit NFS roots or JFFS2.
    
     (*) The bdflush, ioprio_set and ioprio_get syscalls can now be absent (return
         error ENOSYS by way of cond_syscall if so).
    
     (*) The seclvl_bd_claim() and seclvl_bd_release() security calls do nothing if
         CONFIG_BLOCK is not set, since they can't then happen.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c01a90998a7..3e36107d342a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -16,6 +16,22 @@
 
 #include <asm/scatterlist.h>
 
+#ifdef CONFIG_LBD
+# include <asm/div64.h>
+# define sector_div(a, b) do_div(a, b)
+#else
+# define sector_div(n, b)( \
+{ \
+	int _res; \
+	_res = (n) % (b); \
+	(n) /= (b); \
+	_res; \
+} \
+)
+#endif
+
+#ifdef CONFIG_BLOCK
+
 struct scsi_ioctl_command;
 
 struct request_queue;
@@ -818,24 +834,30 @@ struct work_struct;
 int kblockd_schedule_work(struct work_struct *work);
 void kblockd_flush(void);
 
-#ifdef CONFIG_LBD
-# include <asm/div64.h>
-# define sector_div(a, b) do_div(a, b)
-#else
-# define sector_div(n, b)( \
-{ \
-	int _res; \
-	_res = (n) % (b); \
-	(n) /= (b); \
-	_res; \
-} \
-)
-#endif 
-
 #define MODULE_ALIAS_BLOCKDEV(major,minor) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
 #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \
 	MODULE_ALIAS("block-major-" __stringify(major) "-*")
 
 
+#else /* CONFIG_BLOCK */
+/*
+ * stubs for when the block layer is configured out
+ */
+#define buffer_heads_over_limit 0
+
+static inline long blk_congestion_wait(int rw, long timeout)
+{
+	return timeout;
+}
+
+static inline long nr_blockdev_pages(void)
+{
+	return 0;
+}
+
+static inline void exit_io_context(void) {}
+
+#endif /* CONFIG_BLOCK */
+
 #endif

commit 5404bc7a87b9949cf61e0174b21f80e73239ab25
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Aug 10 09:01:02 2006 +0200

    [PATCH] Allow file systems to differentiate between data and meta reads
    
    We can use this information for making more intelligent priority
    decisions, and it will also be useful for blktrace.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 5d8e288db163..2c01a90998a7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -180,6 +180,7 @@ enum rq_flag_bits {
 	__REQ_ORDERED_COLOR,	/* is before or after barrier */
 	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
 	__REQ_ALLOCED,		/* request came from our alloc pool */
+	__REQ_RW_META,		/* metadata io request */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -200,6 +201,7 @@ enum rq_flag_bits {
 #define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
 #define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
 #define REQ_ALLOCED	(1 << __REQ_ALLOCED)
+#define REQ_RW_META	(1 << __REQ_RW_META)
 
 #define BLK_MAX_CDB	16
 
@@ -543,6 +545,7 @@ enum {
  * We regard a request as sync, if it's a READ or a SYNC write.
  */
 #define rq_is_sync(rq)		(rq_data_dir((rq)) == READ || (rq)->cmd_flags & REQ_RW_SYNC)
+#define rq_is_meta(rq)		((rq)->cmd_flags & REQ_RW_META)
 
 static inline int blk_queue_full(struct request_queue *q, int rw)
 {

commit dc72ef4ae35c2016fb594bcc85ce871376682174
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Jul 20 14:54:05 2006 +0200

    [PATCH] Add blk_start_queueing() helper
    
    CFQ implements this on its own now, but it's really block layer
    knowledge. Tells a device queue to start dispatching requests to
    the driver, taking care to unplug if needed. Also fixes the issue
    where as/cfq will invoke a stopped queue, which we really don't
    want.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6609371c303e..5d8e288db163 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -635,6 +635,7 @@ extern void blk_stop_queue(request_queue_t *q);
 extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(request_queue_t *q);
 extern void blk_run_queue(request_queue_t *);
+extern void blk_start_queueing(request_queue_t *);
 extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
 extern int blk_rq_unmap_user(struct bio *, unsigned int);

commit b5deef901282628d88c784f4c9d2f0583ec3b355
Author: Jens Axboe <axboe@suse.de>
Date:   Wed Jul 19 23:39:40 2006 +0200

    [PATCH] Make sure all block/io scheduler setups are node aware
    
    Some were kmalloc_node(), some were still kmalloc(). Change them all to
    kmalloc_node().
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 593386162f47..6609371c303e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -104,8 +104,7 @@ struct io_context {
 
 void put_io_context(struct io_context *ioc);
 void exit_io_context(void);
-struct io_context *current_io_context(gfp_t gfp_flags);
-struct io_context *get_io_context(gfp_t gfp_flags);
+struct io_context *get_io_context(gfp_t gfp_flags, int node);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
 

commit a3b05e8f58c95dfccbf2c824d0c68e5990571f24
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Jul 28 09:36:46 2006 +0200

    [PATCH] Kill various deprecated/unused block layer defines/functions
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 79cb9fa8034a..593386162f47 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -578,12 +578,6 @@ static inline void blk_clear_queue_full(struct request_queue *q, int rw)
 #define rq_mergeable(rq)	\
 	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && blk_fs_request((rq)))
 
-/*
- * noop, requests are automagically marked as active/inactive by I/O
- * scheduler -- see elv_next_request
- */
-#define blk_queue_headactive(q, head_active)
-
 /*
  * q->prep_rq_fn return values
  */
@@ -621,11 +615,6 @@ static inline void blk_queue_bounce(request_queue_t *q, struct bio **bio)
 	if ((rq->bio))			\
 		for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)
 
-struct sec_size {
-	unsigned block_size;
-	unsigned block_size_bits;
-};
-
 extern int blk_register_queue(struct gendisk *disk);
 extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);
@@ -690,16 +679,6 @@ extern void end_that_request_last(struct request *, int);
 extern void end_request(struct request *req, int uptodate);
 extern void blk_complete_request(struct request *);
 
-static inline int rq_all_done(struct request *rq, unsigned int nr_bytes)
-{
-	if (blk_fs_request(rq))
-		return (nr_bytes >= (rq->hard_nr_sectors << 9));
-	else if (blk_pc_request(rq))
-		return nr_bytes >= rq->data_len;
-
-	return 0;
-}
-
 /*
  * end_that_request_first/chunk() takes an uptodate argument. we account
  * any value <= as an io error. 0 means -EIO for compatability reasons,
@@ -807,14 +786,6 @@ static inline int queue_dma_alignment(request_queue_t *q)
 	return retval;
 }
 
-static inline int bdev_dma_aligment(struct block_device *bdev)
-{
-	return queue_dma_alignment(bdev_get_queue(bdev));
-}
-
-#define blk_finished_io(nsects)	do { } while (0)
-#define blk_started_io(nsects)	do { } while (0)
-
 /* assumes size > 256 */
 static inline unsigned int blksize_bits(unsigned int size)
 {

commit fc46379daf90dce57bf765c81d3b39f55150aac2
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Aug 29 09:05:44 2006 +0200

    [PATCH] cfq-iosched: kill cfq_exit_lock
    
    cfq_exit_lock is protecting two things now:
    
    - The per-ioc rbtree of cfq_io_contexts
    
    - The per-cfqd linked list of cfq_io_contexts
    
    The per-cfqd linked list can be protected by the queue lock, as it is (by
    definition) per cfqd as the queue lock is.
    
    The per-ioc rbtree is mainly used and updated by the process itself only.
    The only outside use is the io priority changing. If we move the
    priority changing to not browsing the rbtree, we can remove any locking
    from the rbtree updates and lookup completely. Let the sys_ioprio syscall
    just mark processes as having the iopriority changed and lazily update
    the private cfq io contexts the next time io is queued, and we can
    remove this locking as well.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a1e288069e2e..79cb9fa8034a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -90,7 +90,7 @@ struct io_context {
 	atomic_t refcount;
 	struct task_struct *task;
 
-	int (*set_ioprio)(struct io_context *, unsigned int);
+	unsigned int ioprio_changed;
 
 	/*
 	 * For request batching

commit e6a1c874a064e7d07f24986aba7cd537b7f4a25d
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Aug 10 09:00:21 2006 +0200

    [PATCH] struct request: shrink and optimize some more
    
    Move some members around and unionize completion_data and rb_node since
    they cannot ever be used at the same time.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8a3e309e0842..a1e288069e2e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -211,6 +211,8 @@ struct request {
 	struct list_head queuelist;
 	struct list_head donelist;
 
+	request_queue_t *q;
+
 	unsigned int cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
 
@@ -219,12 +221,12 @@ struct request {
 	 */
 
 	sector_t sector;		/* next sector to submit */
+	sector_t hard_sector;		/* next sector to complete */
 	unsigned long nr_sectors;	/* no. of sectors left to submit */
+	unsigned long hard_nr_sectors;	/* no. of sectors left to complete */
 	/* no. of sectors left to submit in the current segment */
 	unsigned int current_nr_sectors;
 
-	sector_t hard_sector;		/* next sector to complete */
-	unsigned long hard_nr_sectors;	/* no. of sectors left to complete */
 	/* no. of sectors left to complete in the current segment */
 	unsigned int hard_cur_sectors;
 
@@ -232,7 +234,15 @@ struct request {
 	struct bio *biotail;
 
 	struct hlist_node hash;	/* merge hash */
-	struct rb_node rb_node;	/* sort/lookup */
+	/*
+	 * The rb_node is only used inside the io scheduler, requests
+	 * are pruned when moved to the dispatch queue. So let the
+	 * completion_data share space with the rb_node.
+	 */
+	union {
+		struct rb_node rb_node;	/* sort/lookup */
+		void *completion_data;
+	};
 
 	/*
 	 * two pointers are available for the IO schedulers, if they need
@@ -241,8 +251,6 @@ struct request {
 	void *elevator_private;
 	void *elevator_private2;
 
-	void *completion_data;
-
 	struct gendisk *rq_disk;
 	unsigned long start_time;
 
@@ -260,8 +268,6 @@ struct request {
 
 	unsigned short ioprio;
 
-	request_queue_t *q;
-
 	void *special;
 	char *buffer;
 

commit cdd6026217c0e4cda2efce1bdc318661bef1f66f
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Jul 28 09:32:07 2006 +0200

    [PATCH] Remove ->rq_status from struct request
    
    After Christophs SCSI change, the only usage left is RQ_ACTIVE
    and RQ_INACTIVE. The block layer sets RQ_INACTIVE right before freeing
    the request, so any check for RQ_INACTIVE in a driver is a bug and
    indicates use-after-free.
    
    So kill/clean the remaining users, straight forward.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d4c1dd046e27..8a3e309e0842 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -243,8 +243,6 @@ struct request {
 
 	void *completion_data;
 
-	int rq_status;	/* should split this into a few status bits */
-	int errors;
 	struct gendisk *rq_disk;
 	unsigned long start_time;
 
@@ -262,14 +260,16 @@ struct request {
 
 	unsigned short ioprio;
 
-	int tag;
-
-	int ref_count;
 	request_queue_t *q;
 
 	void *special;
 	char *buffer;
 
+	int tag;
+	int errors;
+
+	int ref_count;
+
 	/*
 	 * when request is used as a packet command carrier
 	 */
@@ -456,9 +456,6 @@ struct request_queue
 	struct mutex		sysfs_lock;
 };
 
-#define RQ_INACTIVE		(-1)
-#define RQ_ACTIVE		1
-
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 #define QUEUE_FLAG_STOPPED	2	/* queue is stopped */

commit 49171e5c6f414d49a061b5c1c84967c2eb569822
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Aug 10 08:59:11 2006 +0200

    [PATCH] Remove struct request_list from struct request
    
    It is always identical to &q->rq, and we only use it for detecting
    whether this request came out of our mempool or not. So replace it
    with an additional ->flags bit flag.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 604f23189097..d4c1dd046e27 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -180,6 +180,7 @@ enum rq_flag_bits {
 	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
 	__REQ_ORDERED_COLOR,	/* is before or after barrier */
 	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
+	__REQ_ALLOCED,		/* request came from our alloc pool */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -199,6 +200,7 @@ enum rq_flag_bits {
 #define REQ_PREEMPT	(1 << __REQ_PREEMPT)
 #define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
 #define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
+#define REQ_ALLOCED	(1 << __REQ_ALLOCED)
 
 #define BLK_MAX_CDB	16
 
@@ -264,7 +266,6 @@ struct request {
 
 	int ref_count;
 	request_queue_t *q;
-	struct request_list *rl;
 
 	void *special;
 	char *buffer;

commit c00895ab2f08df7044e58ee01c38bf0a661ea0eb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Sep 30 20:29:12 2006 +0200

    [PATCH] Remove ->waiting member from struct request
    
    As the comments indicates in blkdev.h, we can fold it into ->end_io_data
    usage as that is really what ->waiting is. Fixup the users of
    blk_end_sync_rq().
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d2dc17151f6c..604f23189097 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -266,7 +266,6 @@ struct request {
 	request_queue_t *q;
 	struct request_list *rl;
 
-	struct completion *waiting;
 	void *special;
 	char *buffer;
 
@@ -285,7 +284,7 @@ struct request {
 	int retries;
 
 	/*
-	 * completion callback. end_io_data should be folded in with waiting
+	 * completion callback.
 	 */
 	rq_end_io_fn *end_io;
 	void *end_io_data;

commit ff7d145fd911266ae42af7552edc32681c01addb
Author: Jens Axboe <axboe@suse.de>
Date:   Wed Jul 12 14:04:37 2006 +0200

    [PATCH] Add one more pointer to struct request for IO scheduler usage
    
    Then we have enough room in the request to get rid of the dynamic
    allocations in CFQ/AS.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 55ef6efe3eb5..d2dc17151f6c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -232,7 +232,13 @@ struct request {
 	struct hlist_node hash;	/* merge hash */
 	struct rb_node rb_node;	/* sort/lookup */
 
+	/*
+	 * two pointers are available for the IO schedulers, if they need
+	 * more they have to dynamically allocate it.
+	 */
 	void *elevator_private;
+	void *elevator_private2;
+
 	void *completion_data;
 
 	int rq_status;	/* should split this into a few status bits */

commit 9e2585a8a23f3a42f815b2a638725d85a921cd65
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Jul 28 09:26:13 2006 +0200

    [PATCH] as-iosched: remove arq->is_sync member
    
    We can track this in struct request.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Nick Piggin <npiggin@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a905c4934a55..55ef6efe3eb5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -531,6 +531,11 @@ enum {
 
 #define rq_data_dir(rq)		((rq)->cmd_flags & 1)
 
+/*
+ * We regard a request as sync, if it's a READ or a SYNC write.
+ */
+#define rq_is_sync(rq)		(rq_data_dir((rq)) == READ || (rq)->cmd_flags & REQ_RW_SYNC)
+
 static inline int blk_queue_full(struct request_queue *q, int rw)
 {
 	if (rw == READ)

commit 2e662b65f05d550b6799ed6bfa9963b82279e6b7
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Jul 13 11:55:04 2006 +0200

    [PATCH] elevator: abstract out the rbtree sort handling
    
    The rbtree sort/lookup/reposition logic is mostly duplicated in
    cfq/deadline/as, so move it to the elevator core. The io schedulers
    still provide the actual rb root, as we don't want to impose any sort
    of specific handling on the schedulers.
    
    Introduce the helpers and rb_node in struct request to help migrate the
    IO schedulers.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8f5486964671..a905c4934a55 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -230,6 +230,7 @@ struct request {
 	struct bio *biotail;
 
 	struct hlist_node hash;	/* merge hash */
+	struct rb_node rb_node;	/* sort/lookup */
 
 	void *elevator_private;
 	void *completion_data;

commit 9817064b68fef7e4580c6df1ea597e106b9ff88b
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Jul 28 09:23:08 2006 +0200

    [PATCH] elevator: move the backmerging logic into the elevator core
    
    Right now, every IO scheduler implements its own backmerging (except for
    noop, which does no merging). That results in duplicated code for
    essentially the same operation, which is never a good thing. This patch
    moves the backmerging out of the io schedulers and into the elevator
    core. We save 1.6kb of text and as a bonus get backmerging for noop as
    well. Win-win!
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b2a412cf468f..8f5486964671 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -229,6 +229,8 @@ struct request {
 	struct bio *bio;
 	struct bio *biotail;
 
+	struct hlist_node hash;	/* merge hash */
+
 	void *elevator_private;
 	void *completion_data;
 
@@ -696,21 +698,6 @@ static inline void blkdev_dequeue_request(struct request *req)
 	elv_dequeue_request(req->q, req);
 }
 
-/*
- * This should be in elevator.h, but that requires pulling in rq and q
- */
-static inline void elv_dispatch_add_tail(struct request_queue *q,
-					 struct request *rq)
-{
-	if (q->last_merge == rq)
-		q->last_merge = NULL;
-	q->nr_sorted--;
-
-	q->end_sector = rq_end_sector(rq);
-	q->boundary_rq = rq;
-	list_add_tail(&rq->queuelist, &q->queue_head);
-}
-
 /*
  * Access functions for manipulating queue properties
  */

commit 4aff5e2333c9a1609662f2091f55c3f6fffdad36
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Aug 10 08:44:47 2006 +0200

    [PATCH] Split struct request ->flags into two parts
    
    Right now ->flags is a bit of a mess: some are request types, and
    others are just modifiers. Clean this up by splitting it into
    ->cmd_type and ->cmd_flags. This allows introduction of generic
    Linux block message types, useful for sending generic Linux commands
    to block devices.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index cfde8b3ee919..b2a412cf468f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -120,6 +120,86 @@ struct request_list {
 	wait_queue_head_t wait[2];
 };
 
+/*
+ * request command types
+ */
+enum rq_cmd_type_bits {
+	REQ_TYPE_FS		= 1,	/* fs request */
+	REQ_TYPE_BLOCK_PC,		/* scsi command */
+	REQ_TYPE_SENSE,			/* sense request */
+	REQ_TYPE_PM_SUSPEND,		/* suspend request */
+	REQ_TYPE_PM_RESUME,		/* resume request */
+	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
+	REQ_TYPE_FLUSH,			/* flush request */
+	REQ_TYPE_SPECIAL,		/* driver defined type */
+	REQ_TYPE_LINUX_BLOCK,		/* generic block layer message */
+	/*
+	 * for ATA/ATAPI devices. this really doesn't belong here, ide should
+	 * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver
+	 * private REQ_LB opcodes to differentiate what type of request this is
+	 */
+	REQ_TYPE_ATA_CMD,
+	REQ_TYPE_ATA_TASK,
+	REQ_TYPE_ATA_TASKFILE,
+};
+
+/*
+ * For request of type REQ_TYPE_LINUX_BLOCK, rq->cmd[0] is the opcode being
+ * sent down (similar to how REQ_TYPE_BLOCK_PC means that ->cmd[] holds a
+ * SCSI cdb.
+ *
+ * 0x00 -> 0x3f are driver private, to be used for whatever purpose they need,
+ * typically to differentiate REQ_TYPE_SPECIAL requests.
+ *
+ */
+enum {
+	/*
+	 * just examples for now
+	 */
+	REQ_LB_OP_EJECT	= 0x40,		/* eject request */
+	REQ_LB_OP_FLUSH = 0x41,		/* flush device */
+};
+
+/*
+ * request type modified bits. first three bits match BIO_RW* bits, important
+ */
+enum rq_flag_bits {
+	__REQ_RW,		/* not set, read. set, write */
+	__REQ_FAILFAST,		/* no low level driver retries */
+	__REQ_SORTED,		/* elevator knows about this request */
+	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
+	__REQ_HARDBARRIER,	/* may not be passed by drive either */
+	__REQ_FUA,		/* forced unit access */
+	__REQ_NOMERGE,		/* don't touch this for merging */
+	__REQ_STARTED,		/* drive already may have started this one */
+	__REQ_DONTPREP,		/* don't call prep for this one */
+	__REQ_QUEUED,		/* uses queueing */
+	__REQ_ELVPRIV,		/* elevator private data attached */
+	__REQ_FAILED,		/* set if the request failed */
+	__REQ_QUIET,		/* don't worry about errors */
+	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
+	__REQ_ORDERED_COLOR,	/* is before or after barrier */
+	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
+	__REQ_NR_BITS,		/* stops here */
+};
+
+#define REQ_RW		(1 << __REQ_RW)
+#define REQ_FAILFAST	(1 << __REQ_FAILFAST)
+#define REQ_SORTED	(1 << __REQ_SORTED)
+#define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
+#define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
+#define REQ_FUA		(1 << __REQ_FUA)
+#define REQ_NOMERGE	(1 << __REQ_NOMERGE)
+#define REQ_STARTED	(1 << __REQ_STARTED)
+#define REQ_DONTPREP	(1 << __REQ_DONTPREP)
+#define REQ_QUEUED	(1 << __REQ_QUEUED)
+#define REQ_ELVPRIV	(1 << __REQ_ELVPRIV)
+#define REQ_FAILED	(1 << __REQ_FAILED)
+#define REQ_QUIET	(1 << __REQ_QUIET)
+#define REQ_PREEMPT	(1 << __REQ_PREEMPT)
+#define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
+#define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
+
 #define BLK_MAX_CDB	16
 
 /*
@@ -129,7 +209,8 @@ struct request {
 	struct list_head queuelist;
 	struct list_head donelist;
 
-	unsigned long flags;		/* see REQ_ bits below */
+	unsigned int cmd_flags;
+	enum rq_cmd_type_bits cmd_type;
 
 	/* Maintain bio traversal state for part by part I/O submission.
 	 * hard_* are block layer internals, no driver should touch them!
@@ -202,73 +283,7 @@ struct request {
 };
 
 /*
- * first three bits match BIO_RW* bits, important
- */
-enum rq_flag_bits {
-	__REQ_RW,		/* not set, read. set, write */
-	__REQ_FAILFAST,		/* no low level driver retries */
-	__REQ_SORTED,		/* elevator knows about this request */
-	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
-	__REQ_HARDBARRIER,	/* may not be passed by drive either */
-	__REQ_FUA,		/* forced unit access */
-	__REQ_CMD,		/* is a regular fs rw request */
-	__REQ_NOMERGE,		/* don't touch this for merging */
-	__REQ_STARTED,		/* drive already may have started this one */
-	__REQ_DONTPREP,		/* don't call prep for this one */
-	__REQ_QUEUED,		/* uses queueing */
-	__REQ_ELVPRIV,		/* elevator private data attached */
-	/*
-	 * for ATA/ATAPI devices
-	 */
-	__REQ_PC,		/* packet command (special) */
-	__REQ_BLOCK_PC,		/* queued down pc from block layer */
-	__REQ_SENSE,		/* sense retrival */
-
-	__REQ_FAILED,		/* set if the request failed */
-	__REQ_QUIET,		/* don't worry about errors */
-	__REQ_SPECIAL,		/* driver suplied command */
-	__REQ_DRIVE_CMD,
-	__REQ_DRIVE_TASK,
-	__REQ_DRIVE_TASKFILE,
-	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
-	__REQ_PM_SUSPEND,	/* suspend request */
-	__REQ_PM_RESUME,	/* resume request */
-	__REQ_PM_SHUTDOWN,	/* shutdown request */
-	__REQ_ORDERED_COLOR,	/* is before or after barrier */
-	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
-	__REQ_NR_BITS,		/* stops here */
-};
-
-#define REQ_RW		(1 << __REQ_RW)
-#define REQ_FAILFAST	(1 << __REQ_FAILFAST)
-#define REQ_SORTED	(1 << __REQ_SORTED)
-#define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
-#define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
-#define REQ_FUA		(1 << __REQ_FUA)
-#define REQ_CMD		(1 << __REQ_CMD)
-#define REQ_NOMERGE	(1 << __REQ_NOMERGE)
-#define REQ_STARTED	(1 << __REQ_STARTED)
-#define REQ_DONTPREP	(1 << __REQ_DONTPREP)
-#define REQ_QUEUED	(1 << __REQ_QUEUED)
-#define REQ_ELVPRIV	(1 << __REQ_ELVPRIV)
-#define REQ_PC		(1 << __REQ_PC)
-#define REQ_BLOCK_PC	(1 << __REQ_BLOCK_PC)
-#define REQ_SENSE	(1 << __REQ_SENSE)
-#define REQ_FAILED	(1 << __REQ_FAILED)
-#define REQ_QUIET	(1 << __REQ_QUIET)
-#define REQ_SPECIAL	(1 << __REQ_SPECIAL)
-#define REQ_DRIVE_CMD	(1 << __REQ_DRIVE_CMD)
-#define REQ_DRIVE_TASK	(1 << __REQ_DRIVE_TASK)
-#define REQ_DRIVE_TASKFILE	(1 << __REQ_DRIVE_TASKFILE)
-#define REQ_PREEMPT	(1 << __REQ_PREEMPT)
-#define REQ_PM_SUSPEND	(1 << __REQ_PM_SUSPEND)
-#define REQ_PM_RESUME	(1 << __REQ_PM_RESUME)
-#define REQ_PM_SHUTDOWN	(1 << __REQ_PM_SHUTDOWN)
-#define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
-#define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
-
-/*
- * State information carried for REQ_PM_SUSPEND and REQ_PM_RESUME
+ * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME
  * requests. Some step values could eventually be made generic.
  */
 struct request_pm_state
@@ -490,25 +505,28 @@ enum {
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
 #define blk_queue_flushing(q)	((q)->ordseq)
 
-#define blk_fs_request(rq)	((rq)->flags & REQ_CMD)
-#define blk_pc_request(rq)	((rq)->flags & REQ_BLOCK_PC)
-#define blk_noretry_request(rq)	((rq)->flags & REQ_FAILFAST)
-#define blk_rq_started(rq)	((rq)->flags & REQ_STARTED)
+#define blk_fs_request(rq)	((rq)->cmd_type == REQ_TYPE_FS)
+#define blk_pc_request(rq)	((rq)->cmd_type == REQ_TYPE_BLOCK_PC)
+#define blk_special_request(rq)	((rq)->cmd_type == REQ_TYPE_SPECIAL)
+#define blk_sense_request(rq)	((rq)->cmd_type == REQ_TYPE_SENSE)
+
+#define blk_noretry_request(rq)	((rq)->cmd_flags & REQ_FAILFAST)
+#define blk_rq_started(rq)	((rq)->cmd_flags & REQ_STARTED)
 
 #define blk_account_rq(rq)	(blk_rq_started(rq) && blk_fs_request(rq))
 
-#define blk_pm_suspend_request(rq)	((rq)->flags & REQ_PM_SUSPEND)
-#define blk_pm_resume_request(rq)	((rq)->flags & REQ_PM_RESUME)
+#define blk_pm_suspend_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND)
+#define blk_pm_resume_request(rq)	((rq)->cmd_type == REQ_TYPE_PM_RESUME)
 #define blk_pm_request(rq)	\
-	((rq)->flags & (REQ_PM_SUSPEND | REQ_PM_RESUME))
+	(blk_pm_suspend_request(rq) || blk_pm_resume_request(rq))
 
-#define blk_sorted_rq(rq)	((rq)->flags & REQ_SORTED)
-#define blk_barrier_rq(rq)	((rq)->flags & REQ_HARDBARRIER)
-#define blk_fua_rq(rq)		((rq)->flags & REQ_FUA)
+#define blk_sorted_rq(rq)	((rq)->cmd_flags & REQ_SORTED)
+#define blk_barrier_rq(rq)	((rq)->cmd_flags & REQ_HARDBARRIER)
+#define blk_fua_rq(rq)		((rq)->cmd_flags & REQ_FUA)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
-#define rq_data_dir(rq)		((rq)->flags & 1)
+#define rq_data_dir(rq)		((rq)->cmd_flags & 1)
 
 static inline int blk_queue_full(struct request_queue *q, int rw)
 {
@@ -541,7 +559,7 @@ static inline void blk_clear_queue_full(struct request_queue *q, int rw)
 #define RQ_NOMERGE_FLAGS	\
 	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER)
 #define rq_mergeable(rq)	\
-	(!((rq)->flags & RQ_NOMERGE_FLAGS) && blk_fs_request((rq)))
+	(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && blk_fs_request((rq)))
 
 /*
  * noop, requests are automagically marked as active/inactive by I/O
@@ -737,7 +755,7 @@ extern void blk_put_queue(request_queue_t *);
  */
 #define blk_queue_tag_depth(q)		((q)->queue_tags->busy)
 #define blk_queue_tag_queue(q)		((q)->queue_tags->busy < (q)->queue_tags->max_depth)
-#define blk_rq_tagged(rq)		((rq)->flags & REQ_QUEUED)
+#define blk_rq_tagged(rq)		((rq)->cmd_flags & REQ_QUEUED)
 extern int blk_queue_start_tag(request_queue_t *, struct request *);
 extern struct request *blk_queue_find_tag(request_queue_t *, int);
 extern void blk_queue_end_tag(request_queue_t *, struct request *);

commit 6c5c934153513dc72e2d6464f39e8ef1f27c0a3e
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 29 01:59:40 2006 -0700

    [PATCH] ifdef blktrace debugging fields
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c773ee545ebd..cfde8b3ee919 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -417,9 +417,9 @@ struct request_queue
 	unsigned int		sg_timeout;
 	unsigned int		sg_reserved_size;
 	int			node;
-
+#ifdef CONFIG_BLK_DEV_IO_TRACE
 	struct blk_trace	*blk_trace;
-
+#endif
 	/*
 	 * reserved for flush operations
 	 */

commit 1aedf2ccc60fade26c46fae12e28664d0da3f199
Merge: dfdc58ba354a 1ab9dd0902df
Author: James Bottomley <jejb@sparkweed.localdomain>
Date:   Sat Sep 23 21:03:52 2006 -0500

    Merge mulgrave-w:git/linux-2.6
    
    Conflicts:
    
            include/linux/blkdev.h
    
    Trivial merge to incorporate tag prototypes.

commit 275a082fe9308e710324e26ccb5363c53d8fd45f
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Aug 22 20:06:24 2006 -0400

    Add a real API for dealing with blk_congestion_wait()
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aafe82788b4e..96c9040c00a8 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -746,6 +746,7 @@ extern void blk_queue_free_tags(request_queue_t *);
 extern int blk_queue_resize_tags(request_queue_t *, int);
 extern void blk_queue_invalidate_tags(request_queue_t *);
 extern long blk_congestion_wait(int rw, long timeout);
+extern void blk_congestion_end(int rw);
 
 extern void blk_rq_bio_prep(request_queue_t *, struct request *, struct bio *);
 extern int blkdev_issue_flush(struct block_device *, sector_t *);

commit 492dfb489658dfe4a755fa29dd0e34e9c8bd8fb8
Author: James Bottomley <James.Bottomley@steeleye.com>
Date:   Wed Aug 30 15:48:45 2006 -0400

    [SCSI] block: add support for shared tag maps
    
    The current block queue implementation already contains most of the
    machinery for shared tag maps.  The only remaining pieces are a way to
    allocate and destroy a tag map independently of the queues (so that
    the maps can be managed on the life cycle of the overseeing entity)
    
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aafe82788b4e..427b0d61be6c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -746,6 +746,8 @@ extern void blk_queue_free_tags(request_queue_t *);
 extern int blk_queue_resize_tags(request_queue_t *, int);
 extern void blk_queue_invalidate_tags(request_queue_t *);
 extern long blk_congestion_wait(int rw, long timeout);
+extern struct blk_queue_tag *blk_init_tags(int);
+extern void blk_free_tags(struct blk_queue_tag *);
 
 extern void blk_rq_bio_prep(request_queue_t *, struct request *, struct bio *);
 extern int blkdev_issue_flush(struct block_device *, sector_t *);

commit 8f34ee75decb80007ba77bba5a7384eadff4866d
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Jun 13 09:02:34 2006 +0200

    [PATCH] Rearrange a few struct request members
    
    This saves 8 bytes of data in 64-bit archs.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 371c0ce5f630..aafe82788b4e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -151,11 +151,9 @@ struct request {
 	void *elevator_private;
 	void *completion_data;
 
-	unsigned short ioprio;
-
 	int rq_status;	/* should split this into a few status bits */
-	struct gendisk *rq_disk;
 	int errors;
+	struct gendisk *rq_disk;
 	unsigned long start_time;
 
 	/* Number of scatter-gather DMA addr+len pairs after
@@ -170,8 +168,9 @@ struct request {
 	 */
 	unsigned short nr_hw_segments;
 
+	unsigned short ioprio;
+
 	int tag;
-	char *buffer;
 
 	int ref_count;
 	request_queue_t *q;
@@ -179,6 +178,7 @@ struct request {
 
 	struct completion *waiting;
 	void *special;
+	char *buffer;
 
 	/*
 	 * when request is used as a packet command carrier
@@ -187,9 +187,8 @@ struct request {
 	unsigned char cmd[BLK_MAX_CDB];
 
 	unsigned int data_len;
-	void *data;
-
 	unsigned int sense_len;
+	void *data;
 	void *sense;
 
 	unsigned int timeout;

commit ad3caddaa1708e506f20b8e25a4a8ae586fc7d5b
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Jun 13 08:46:57 2006 +0200

    [PATCH] Get rid of struct request request_pm_state member
    
    The IDE power management can just use the ->end_io_data member to store
    it's data.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 482a21d67627..371c0ce5f630 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -195,11 +195,6 @@ struct request {
 	unsigned int timeout;
 	int retries;
 
-	/*
-	 * For Power Management requests
-	 */
-	struct request_pm_state *pm;
-
 	/*
 	 * completion callback. end_io_data should be folded in with waiting
 	 */

commit b31dc66a54ad986b6b73bdc49c8efc17cbad1833
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Jun 13 08:26:10 2006 +0200

    [PATCH] Kill PF_SYNCWRITE flag
    
    A process flag to indicate whether we are doing sync io is incredibly
    ugly. It also causes performance problems when one does a lot of async
    io and then proceeds to sync it. Part of the io will go out as async,
    and the other part as sync. This causes a disconnect between the
    previously submitted io and the synced io. For io schedulers such as CFQ,
    this will cause us lost merges and suboptimal behaviour in scheduling.
    
    Remove PF_SYNCWRITE completely from the fsync/msync paths, and let
    the O_DIRECT path just directly indicate that the writes are sync
    by using WRITE_SYNC instead.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 3457e7b97363..482a21d67627 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -241,6 +241,7 @@ enum rq_flag_bits {
 	__REQ_PM_RESUME,	/* resume request */
 	__REQ_PM_SHUTDOWN,	/* shutdown request */
 	__REQ_ORDERED_COLOR,	/* is before or after barrier */
+	__REQ_RW_SYNC,		/* request is sync (O_DIRECT) */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -270,6 +271,7 @@ enum rq_flag_bits {
 #define REQ_PM_RESUME	(1 << __REQ_PM_RESUME)
 #define REQ_PM_SHUTDOWN	(1 << __REQ_PM_SHUTDOWN)
 #define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
+#define REQ_RW_SYNC	(1 << __REQ_RW_SYNC)
 
 /*
  * State information carried for REQ_PM_SUSPEND and REQ_PM_RESUME

commit 28e4b224955cbe30275b2a7842e729023a4f4b03
Merge: 22ae813b85df 67d59dfdeb21
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Wed Jun 21 11:18:25 2006 -0700

    Merge master.kernel.org:/pub/scm/linux/kernel/git/jejb/scsi-misc-2.6
    
    * master.kernel.org:/pub/scm/linux/kernel/git/jejb/scsi-misc-2.6: (85 commits)
      [SCSI] 53c700: remove reliance on deprecated cmnd fields
      [SCSI] hptiop: don't use cmnd->bufflen
      [SCSI] hptiop: HighPoint RocketRAID 3xxx controller driver
      [SCSI] aacraid: small misc. cleanups
      [SCSI] aacraid: Update supported product information
      [SCSI] aacraid: Fix return code interpretation
      [SCSI] scsi_transport_sas: fix panic in sas_free_rphy
      [SCSI] remove RQ_SCSI_* flags
      [SCSI] remove scsi_request infrastructure
      [SCSI] mptfusion: change driver revision to 3.03.10
      [SCSI] mptfc: abort of board reset leaves port dead requiring reboot
      [SCSI] mptfc: fix fibre channel infinite request/response loop
      [SCSI] mptfc: set fibre channel fw target missing timers to one second
      [SCSI] mptfusion: move fc event/reset handling to mptfc
      [SCSI] spi transport: don't allow dt to be set on SE or HVD buses
      [SCSI] aic7xxx: expose the bus setting to sysfs
      [SCSI] scsi: remove Documentation/scsi/cpqfc.txt
      [SCSI] drivers/scsi: Use ARRAY_SIZE macro
      [SCSI] Remove last page_address from dc395x.c
      [SCSI] hptiop: HighPoint RocketRAID 3xxx controller driver
      ...
    
    Fixed up conflicts in drivers/message/fusion/mptbase.c manually (due to
    the sparc interrupt cleanups)

commit 8d7feac3c7504425aaf61dc7d804685a6b89ee43
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 10 18:37:19 2006 +0200

    [SCSI] remove RQ_SCSI_* flags
    
    The RQ_SCSI_* flags are a vestiage of a long past history.  The EH code
    still sets them but we never make use of that information.  The other
    users is pluto.c which never had a chance to work but needs to be kept
    compiling to keep Davem happy, so copy over the definition there.
    
    We could probably get rid of RQ_ACTIVE/RQ_INACTIVE aswell with some
    work, there's only two more or less bogus looking uses in ubd and scsi.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 59e1259b1c40..c889c459fd1b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -439,9 +439,6 @@ struct request_queue
 
 #define RQ_INACTIVE		(-1)
 #define RQ_ACTIVE		1
-#define RQ_SCSI_BUSY		0xffff
-#define RQ_SCSI_DONE		0xfffe
-#define RQ_SCSI_DISCONNECTING	0xffe0
 
 #define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
 #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 59e1259b1c40..5d327313a9f7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1,7 +1,6 @@
 #ifndef _LINUX_BLKDEV_H
 #define _LINUX_BLKDEV_H
 
-#include <linux/config.h>
 #include <linux/major.h>
 #include <linux/genhd.h>
 #include <linux/list.h>

commit 21b2f0c803adaf00fce1b606c50b49ae8b106773
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 22 17:52:04 2006 +0100

    [SCSI] unify SCSI_IOCTL_SEND_COMMAND implementations
    
    We currently have two implementations of this obsolete ioctl, one in
    the block layer and one in the scsi code.  Both of them have drawbacks.
    
    This patch kills the scsi layer version after updating the block version
    with the missing bits:
    
     - argument checking
     - use scatterlist I/O
     - set number of retries based on the submitted command
    
    This is the last user of non-S/G I/O except for the gdth driver, so
    getting this in ASAP and through the scsi tree would be nie to kill
    the non-S/G I/O path.  Jens, what do you think about adding a check
    for non-S/G I/O in the midlayer?
    
    Thanks to  Or Gerlitz for testing this patch.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d0cac8b58de7..59e1259b1c40 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -17,6 +17,8 @@
 
 #include <asm/scatterlist.h>
 
+struct scsi_ioctl_command;
+
 struct request_queue;
 typedef struct request_queue request_queue_t;
 struct elevator_queue;
@@ -611,6 +613,8 @@ extern void blk_plug_device(request_queue_t *);
 extern int blk_remove_plug(request_queue_t *);
 extern void blk_recount_segments(request_queue_t *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
+extern int sg_scsi_ioctl(struct file *, struct request_queue *,
+		struct gendisk *, struct scsi_ioctl_command __user *);
 extern void blk_start_queue(request_queue_t *q);
 extern void blk_stop_queue(request_queue_t *q);
 extern void blk_sync_queue(struct request_queue *q);

commit 206dc69b31ca05baac68c75b8ed2ba7dd857d273
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Mar 28 13:03:44 2006 +0200

    [BLOCK] cfq-iosched: seek and async performance fixes
    
    Detect whether a given process is seeky and if so disable (mostly) the
    idle window if it is. We still allow just a little idle time, just enough
    to allow that process to submit a new request. That is needed to maintain
    fairness across priority groups.
    
    In some cases, we could setup several async queues. This is not optimal
    from a performance POV, since we want all async io in one queue to perform
    good sorting on it. It also impacted sync queues, as async io got too much
    slice time.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ed0ffa673568..d0cac8b58de7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -63,11 +63,17 @@ struct cfq_io_context {
 	struct io_context *ioc;
 
 	unsigned long last_end_request;
-	unsigned long last_queue;
+	sector_t last_request_pos;
+ 	unsigned long last_queue;
+
 	unsigned long ttime_total;
 	unsigned long ttime_samples;
 	unsigned long ttime_mean;
 
+	unsigned int seek_samples;
+	u64 seek_total;
+	sector_t seek_mean;
+
 	struct list_head queue_list;
 
 	void (*dtor)(struct io_context *); /* destructor */

commit e2d74ac0664c89757bde8fb18c98cd7bf53da61c
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Mar 28 08:59:01 2006 +0200

    [PATCH] [BLOCK] cfq-iosched: change cfq io context linking from list to tree
    
    On setups with many disks, we spend a considerable amount of time
    looking up the process-disk mapping on each queue of io. Testing with
    a NULL based block driver, this costs 40-50% reduction in throughput
    for 1000 disks.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index c179966f1a2f..ed0ffa673568 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -55,13 +55,11 @@ struct as_io_context {
 
 struct cfq_queue;
 struct cfq_io_context {
-	/*
-	 * circular list of cfq_io_contexts belonging to a process io context
-	 */
-	struct list_head list;
-	struct cfq_queue *cfqq[2];
+	struct rb_node rb_node;
 	void *key;
 
+	struct cfq_queue *cfqq[2];
+
 	struct io_context *ioc;
 
 	unsigned long last_end_request;
@@ -72,8 +70,8 @@ struct cfq_io_context {
 
 	struct list_head queue_list;
 
-	void (*dtor)(struct cfq_io_context *);
-	void (*exit)(struct cfq_io_context *);
+	void (*dtor)(struct io_context *); /* destructor */
+	void (*exit)(struct io_context *); /* called on task exit */
 };
 
 /*
@@ -94,7 +92,7 @@ struct io_context {
 	int nr_batch_requests;     /* Number of requests left in the batch */
 
 	struct as_io_context *aic;
-	struct cfq_io_context *cic;
+	struct rb_root cic_root;
 };
 
 void put_io_context(struct io_context *ioc);

commit 2056a782f8e7e65fd4bfd027506b4ce1c5e9ccd4
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Mar 23 20:00:26 2006 +0100

    [PATCH] Block queue IO tracing support (blktrace) as of 2006-03-23
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 56bb6a4e15f3..c179966f1a2f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -22,6 +22,7 @@ typedef struct request_queue request_queue_t;
 struct elevator_queue;
 typedef struct elevator_queue elevator_t;
 struct request_pm_state;
+struct blk_trace;
 
 #define BLKDEV_MIN_RQ	4
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
@@ -416,6 +417,8 @@ struct request_queue
 	unsigned int		sg_reserved_size;
 	int			node;
 
+	struct blk_trace	*blk_trace;
+
 	/*
 	 * reserved for flush operations
 	 */

commit 483f4afc421435b7cfe5e88f74eea0b73a476d75
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 18 18:34:37 2006 -0500

    [PATCH] fix sysfs interaction and lifetime rules handling for queues

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 80518f703538..56bb6a4e15f3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -406,8 +406,6 @@ struct request_queue
 
 	struct blk_queue_tag	*queue_tags;
 
-	atomic_t		refcnt;
-
 	unsigned int		nr_sorted;
 	unsigned int		in_flight;
 
@@ -426,6 +424,8 @@ struct request_queue
 	struct request		pre_flush_rq, bar_rq, post_flush_rq;
 	struct request		*orig_bar_rq;
 	unsigned int		bi_size;
+
+	struct mutex		sysfs_lock;
 };
 
 #define RQ_INACTIVE		(-1)
@@ -727,7 +727,7 @@ extern long nr_blockdev_pages(void);
 int blk_get_queue(request_queue_t *);
 request_queue_t *blk_alloc_queue(gfp_t);
 request_queue_t *blk_alloc_queue_node(gfp_t, int);
-#define blk_put_queue(q) blk_cleanup_queue((q))
+extern void blk_put_queue(request_queue_t *);
 
 /*
  * tag stuff

commit d9ff41879364cfca7c15abc20ae398e35de3f883
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 18 13:51:22 2006 -0500

    [PATCH] make cfq_exit_queue() prune the cfq_io_context for that queue
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index e19cb631084e..80518f703538 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -69,6 +69,8 @@ struct cfq_io_context {
 	unsigned long ttime_samples;
 	unsigned long ttime_mean;
 
+	struct list_head queue_list;
+
 	void (*dtor)(struct cfq_io_context *);
 	void (*exit)(struct cfq_io_context *);
 };

commit 12a057321529df2fb650ac5f34dfd7abcca667df
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Mar 18 13:38:01 2006 -0500

    [PATCH] keep sync and async cfq_queue separate
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 860e7a485a5f..e19cb631084e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -58,7 +58,7 @@ struct cfq_io_context {
 	 * circular list of cfq_io_contexts belonging to a process io context
 	 */
 	struct list_head list;
-	struct cfq_queue *cfqq;
+	struct cfq_queue *cfqq[2];
 	void *key;
 
 	struct io_context *ioc;

commit 2cb2e147a6d20bffd1d6b7a79be7301560f751c3
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Jan 17 09:04:32 2006 +0100

    [BLOCK] ll_rw_blk: make max_sectors and max_hw_sectors unsigned ints
    
    IDE lba48 can support full 64k request size, which overflows the
    max_hw_sectors variable.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 02a585faa62c..860e7a485a5f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -392,8 +392,8 @@ struct request_queue
 	unsigned int		nr_congestion_off;
 	unsigned int		nr_batching;
 
-	unsigned short		max_sectors;
-	unsigned short		max_hw_sectors;
+	unsigned int		max_sectors;
+	unsigned int		max_hw_sectors;
 	unsigned short		max_phys_segments;
 	unsigned short		max_hw_segments;
 	unsigned short		hardsect_size;
@@ -697,7 +697,7 @@ extern request_queue_t *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(request_queue_t *);
 extern void blk_queue_make_request(request_queue_t *, make_request_fn *);
 extern void blk_queue_bounce_limit(request_queue_t *, u64);
-extern void blk_queue_max_sectors(request_queue_t *, unsigned short);
+extern void blk_queue_max_sectors(request_queue_t *, unsigned int);
 extern void blk_queue_max_phys_segments(request_queue_t *, unsigned short);
 extern void blk_queue_max_hw_segments(request_queue_t *, unsigned short);
 extern void blk_queue_max_segment_size(request_queue_t *, unsigned int);

commit e2688f00dc0ceb9d9867434dffbd080411fc23b0
Merge: 356cebea1123 a9925a06ea52
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Mon Jan 9 09:26:40 2006 -0800

    Merge branch 'blk-softirq' of git://brick.kernel.dk/data/git/linux-2.6-block
    
    Manual merge for trivial #include changes

commit ff856bad67cb65cb4dc4ef88b808804fc4265782
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jan 9 16:02:34 2006 +0100

    [BLOCK] ll_rw_blk: Enable out-of-order request completions through softirq
    
    Request completion can be a quite heavy process, since it needs to
    iterate through the entire request and complete the bio's it holds.
    This patch adds blk_complete_request() which moves this processing
    into a dedicated block softirq.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fb0985377421..804cc4ec9533 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -118,9 +118,9 @@ struct request_list {
  * try to put the fields that are referenced together in the same cacheline
  */
 struct request {
-	struct list_head queuelist; /* looking for ->queue? you must _not_
-				     * access it directly, use
-				     * blkdev_dequeue_request! */
+	struct list_head queuelist;
+	struct list_head donelist;
+
 	unsigned long flags;		/* see REQ_ bits below */
 
 	/* Maintain bio traversal state for part by part I/O submission.
@@ -141,6 +141,7 @@ struct request {
 	struct bio *biotail;
 
 	void *elevator_private;
+	void *completion_data;
 
 	unsigned short ioprio;
 
@@ -291,6 +292,7 @@ typedef int (merge_bvec_fn) (request_queue_t *, struct bio *, struct bio_vec *);
 typedef void (activity_fn) (void *data, int rw);
 typedef int (issue_flush_fn) (request_queue_t *, struct gendisk *, sector_t *);
 typedef void (prepare_flush_fn) (request_queue_t *, struct request *);
+typedef void (softirq_done_fn)(struct request *);
 
 enum blk_queue_state {
 	Queue_down,
@@ -332,6 +334,7 @@ struct request_queue
 	activity_fn		*activity_fn;
 	issue_flush_fn		*issue_flush_fn;
 	prepare_flush_fn	*prepare_flush_fn;
+	softirq_done_fn		*softirq_done_fn;
 
 	/*
 	 * Dispatch queue sorting
@@ -646,6 +649,17 @@ extern int end_that_request_first(struct request *, int, int);
 extern int end_that_request_chunk(struct request *, int, int);
 extern void end_that_request_last(struct request *, int);
 extern void end_request(struct request *req, int uptodate);
+extern void blk_complete_request(struct request *);
+
+static inline int rq_all_done(struct request *rq, unsigned int nr_bytes)
+{
+	if (blk_fs_request(rq))
+		return (nr_bytes >= (rq->hard_nr_sectors << 9));
+	else if (blk_pc_request(rq))
+		return nr_bytes >= rq->data_len;
+
+	return 0;
+}
 
 /*
  * end_that_request_first/chunk() takes an uptodate argument. we account
@@ -694,6 +708,7 @@ extern void blk_queue_segment_boundary(request_queue_t *, unsigned long);
 extern void blk_queue_prep_rq(request_queue_t *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(request_queue_t *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(request_queue_t *, int);
+extern void blk_queue_softirq_done(request_queue_t *, softirq_done_fn *);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern int blk_queue_ordered(request_queue_t *, unsigned, prepare_flush_fn *);
 extern void blk_queue_issue_flush_fn(request_queue_t *, issue_flush_fn *);

commit 356cebea1123804e4aa85b43ab39bbd0ac8e667c
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jan 9 15:30:20 2006 +0100

    [BLOCK] Kill blk_attempt_remerge()
    
    It's a broken interface, it's done way too late. And apparently it triggers
    slab problems in recent kernels as well (most likely after the generic dispatch
    code was merged). So kill it, ide-cd is the only user of it.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fb0985377421..96b233991685 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -592,7 +592,6 @@ extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(request_queue_t *, struct request *);
 extern void blk_end_sync_rq(struct request *rq, int error);
-extern void blk_attempt_remerge(request_queue_t *, struct request *);
 extern struct request *blk_get_request(request_queue_t *, int, gfp_t);
 extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
 extern void blk_requeue_request(request_queue_t *, struct request *);

commit 15fc858a0067c800f410a24551a7b461978abf0b
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Jan 6 10:00:50 2006 +0100

    [BLOCK] Correct blk_execute_rq_nowait() prototype

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 15db0f112d0a..fb0985377421 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -613,8 +613,7 @@ extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_io
 extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
-				  struct request *, int,
-				  void (*done)(struct request *));
+				  struct request *, int, rq_end_io_fn *);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
 {

commit 797e7dbbee0a91fa1349192f18ad5c454997d876
Author: Tejun Heo <htejun@gmail.com>
Date:   Fri Jan 6 09:51:03 2006 +0100

    [BLOCK] reimplement handling of barrier request
    
    Reimplement handling of barrier requests.
    
    * Flexible handling to deal with various capabilities of
      target devices.
    * Retry support for falling back.
    * Tagged queues which don't support ordered tag can do ordered.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a0ce8c585165..15db0f112d0a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -207,6 +207,7 @@ enum rq_flag_bits {
 	__REQ_SORTED,		/* elevator knows about this request */
 	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
 	__REQ_HARDBARRIER,	/* may not be passed by drive either */
+	__REQ_FUA,		/* forced unit access */
 	__REQ_CMD,		/* is a regular fs rw request */
 	__REQ_NOMERGE,		/* don't touch this for merging */
 	__REQ_STARTED,		/* drive already may have started this one */
@@ -230,9 +231,7 @@ enum rq_flag_bits {
 	__REQ_PM_SUSPEND,	/* suspend request */
 	__REQ_PM_RESUME,	/* resume request */
 	__REQ_PM_SHUTDOWN,	/* shutdown request */
-	__REQ_BAR_PREFLUSH,	/* barrier pre-flush done */
-	__REQ_BAR_POSTFLUSH,	/* barrier post-flush */
-	__REQ_BAR_FLUSH,	/* rq is the flush request */
+	__REQ_ORDERED_COLOR,	/* is before or after barrier */
 	__REQ_NR_BITS,		/* stops here */
 };
 
@@ -241,6 +240,7 @@ enum rq_flag_bits {
 #define REQ_SORTED	(1 << __REQ_SORTED)
 #define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
 #define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
+#define REQ_FUA		(1 << __REQ_FUA)
 #define REQ_CMD		(1 << __REQ_CMD)
 #define REQ_NOMERGE	(1 << __REQ_NOMERGE)
 #define REQ_STARTED	(1 << __REQ_STARTED)
@@ -260,9 +260,7 @@ enum rq_flag_bits {
 #define REQ_PM_SUSPEND	(1 << __REQ_PM_SUSPEND)
 #define REQ_PM_RESUME	(1 << __REQ_PM_RESUME)
 #define REQ_PM_SHUTDOWN	(1 << __REQ_PM_SHUTDOWN)
-#define REQ_BAR_PREFLUSH	(1 << __REQ_BAR_PREFLUSH)
-#define REQ_BAR_POSTFLUSH	(1 << __REQ_BAR_POSTFLUSH)
-#define REQ_BAR_FLUSH	(1 << __REQ_BAR_FLUSH)
+#define REQ_ORDERED_COLOR	(1 << __REQ_ORDERED_COLOR)
 
 /*
  * State information carried for REQ_PM_SUSPEND and REQ_PM_RESUME
@@ -292,8 +290,7 @@ struct bio_vec;
 typedef int (merge_bvec_fn) (request_queue_t *, struct bio *, struct bio_vec *);
 typedef void (activity_fn) (void *data, int rw);
 typedef int (issue_flush_fn) (request_queue_t *, struct gendisk *, sector_t *);
-typedef int (prepare_flush_fn) (request_queue_t *, struct request *);
-typedef void (end_flush_fn) (request_queue_t *, struct request *);
+typedef void (prepare_flush_fn) (request_queue_t *, struct request *);
 
 enum blk_queue_state {
 	Queue_down,
@@ -335,7 +332,6 @@ struct request_queue
 	activity_fn		*activity_fn;
 	issue_flush_fn		*issue_flush_fn;
 	prepare_flush_fn	*prepare_flush_fn;
-	end_flush_fn		*end_flush_fn;
 
 	/*
 	 * Dispatch queue sorting
@@ -420,14 +416,11 @@ struct request_queue
 	/*
 	 * reserved for flush operations
 	 */
-	struct request		*flush_rq;
-	unsigned char		ordered;
-};
-
-enum {
-	QUEUE_ORDERED_NONE,
-	QUEUE_ORDERED_TAG,
-	QUEUE_ORDERED_FLUSH,
+	unsigned int		ordered, next_ordered, ordseq;
+	int			orderr, ordcolor;
+	struct request		pre_flush_rq, bar_rq, post_flush_rq;
+	struct request		*orig_bar_rq;
+	unsigned int		bi_size;
 };
 
 #define RQ_INACTIVE		(-1)
@@ -445,12 +438,51 @@ enum {
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
 #define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
-#define QUEUE_FLAG_FLUSH	9	/* doing barrier flush sequence */
+
+enum {
+	/*
+	 * Hardbarrier is supported with one of the following methods.
+	 *
+	 * NONE		: hardbarrier unsupported
+	 * DRAIN	: ordering by draining is enough
+	 * DRAIN_FLUSH	: ordering by draining w/ pre and post flushes
+	 * DRAIN_FUA	: ordering by draining w/ pre flush and FUA write
+	 * TAG		: ordering by tag is enough
+	 * TAG_FLUSH	: ordering by tag w/ pre and post flushes
+	 * TAG_FUA	: ordering by tag w/ pre flush and FUA write
+	 */
+	QUEUE_ORDERED_NONE	= 0x00,
+	QUEUE_ORDERED_DRAIN	= 0x01,
+	QUEUE_ORDERED_TAG	= 0x02,
+
+	QUEUE_ORDERED_PREFLUSH	= 0x10,
+	QUEUE_ORDERED_POSTFLUSH	= 0x20,
+	QUEUE_ORDERED_FUA	= 0x40,
+
+	QUEUE_ORDERED_DRAIN_FLUSH = QUEUE_ORDERED_DRAIN |
+			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_POSTFLUSH,
+	QUEUE_ORDERED_DRAIN_FUA	= QUEUE_ORDERED_DRAIN |
+			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_FUA,
+	QUEUE_ORDERED_TAG_FLUSH	= QUEUE_ORDERED_TAG |
+			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_POSTFLUSH,
+	QUEUE_ORDERED_TAG_FUA	= QUEUE_ORDERED_TAG |
+			QUEUE_ORDERED_PREFLUSH | QUEUE_ORDERED_FUA,
+
+	/*
+	 * Ordered operation sequence
+	 */
+	QUEUE_ORDSEQ_STARTED	= 0x01,	/* flushing in progress */
+	QUEUE_ORDSEQ_DRAIN	= 0x02,	/* waiting for the queue to be drained */
+	QUEUE_ORDSEQ_PREFLUSH	= 0x04,	/* pre-flushing in progress */
+	QUEUE_ORDSEQ_BAR	= 0x08,	/* original barrier req in progress */
+	QUEUE_ORDSEQ_POSTFLUSH	= 0x10,	/* post-flushing in progress */
+	QUEUE_ORDSEQ_DONE	= 0x20,
+};
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
 #define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
-#define blk_queue_flushing(q)	test_bit(QUEUE_FLAG_FLUSH, &(q)->queue_flags)
+#define blk_queue_flushing(q)	((q)->ordseq)
 
 #define blk_fs_request(rq)	((rq)->flags & REQ_CMD)
 #define blk_pc_request(rq)	((rq)->flags & REQ_BLOCK_PC)
@@ -466,8 +498,7 @@ enum {
 
 #define blk_sorted_rq(rq)	((rq)->flags & REQ_SORTED)
 #define blk_barrier_rq(rq)	((rq)->flags & REQ_HARDBARRIER)
-#define blk_barrier_preflush(rq)	((rq)->flags & REQ_BAR_PREFLUSH)
-#define blk_barrier_postflush(rq)	((rq)->flags & REQ_BAR_POSTFLUSH)
+#define blk_fua_rq(rq)		((rq)->flags & REQ_FUA)
 
 #define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
 
@@ -665,11 +696,12 @@ extern void blk_queue_prep_rq(request_queue_t *, prep_rq_fn *pfn);
 extern void blk_queue_merge_bvec(request_queue_t *, merge_bvec_fn *);
 extern void blk_queue_dma_alignment(request_queue_t *, int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
-extern void blk_queue_ordered(request_queue_t *, int);
+extern int blk_queue_ordered(request_queue_t *, unsigned, prepare_flush_fn *);
 extern void blk_queue_issue_flush_fn(request_queue_t *, issue_flush_fn *);
-extern struct request *blk_start_pre_flush(request_queue_t *,struct request *);
-extern int blk_complete_barrier_rq(request_queue_t *, struct request *, int);
-extern int blk_complete_barrier_rq_locked(request_queue_t *, struct request *, int);
+extern int blk_do_ordered(request_queue_t *, struct request **);
+extern unsigned blk_ordered_cur_seq(request_queue_t *);
+extern unsigned blk_ordered_req_seq(struct request *);
+extern void blk_ordered_complete_seq(request_queue_t *, unsigned, int);
 
 extern int blk_rq_map_sg(request_queue_t *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);

commit 8ffdc6550c47f75ca4e6c9f30a2a89063e035cf2
Author: Tejun Heo <htejun@gmail.com>
Date:   Fri Jan 6 09:49:03 2006 +0100

    [BLOCK] add @uptodate to end_that_request_last() and @error to rq_end_io_fn()
    
    add @uptodate argument to end_that_request_last() and @error
    to rq_end_io_fn().  there's no generic way to pass error code
    to request completion function, making generic error handling
    of non-fs request difficult (rq->errors is driver-specific and
    each driver uses it differently).  this patch adds @uptodate
    to end_that_request_last() and @error to rq_end_io_fn().
    
    for fs requests, this doesn't really matter, so just using the
    same uptodate argument used in the last call to
    end_that_request_first() should suffice.  imho, this can also
    help the generic command-carrying request jens is working on.
    
    Signed-off-by: tejun heo <htejun@gmail.com>
    Signed-Off-By: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a18500d196e1..a0ce8c585165 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -102,7 +102,7 @@ void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
 
 struct request;
-typedef void (rq_end_io_fn)(struct request *);
+typedef void (rq_end_io_fn)(struct request *, int);
 
 struct request_list {
 	int count[2];
@@ -560,7 +560,7 @@ extern void register_disk(struct gendisk *dev);
 extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void __blk_put_request(request_queue_t *, struct request *);
-extern void blk_end_sync_rq(struct request *rq);
+extern void blk_end_sync_rq(struct request *rq, int error);
 extern void blk_attempt_remerge(request_queue_t *, struct request *);
 extern struct request *blk_get_request(request_queue_t *, int, gfp_t);
 extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
@@ -614,7 +614,7 @@ static inline void blk_run_address_space(struct address_space *mapping)
  */
 extern int end_that_request_first(struct request *, int, int);
 extern int end_that_request_chunk(struct request *, int, int);
-extern void end_that_request_last(struct request *);
+extern void end_that_request_last(struct request *, int);
 extern void end_request(struct request *req, int uptodate);
 
 /*

commit defd94b75409b983f94548ea2f52ff5787ddb848
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Mon Dec 5 02:37:06 2005 -0600

    [SCSI] seperate max_sectors from max_hw_sectors
    
    - export __blk_put_request and blk_execute_rq_nowait
    needed for async REQ_BLOCK_PC requests
    - seperate max_hw_sectors and max_sectors for block/scsi_ioctl.c and
    SG_IO bio.c helpers per Jens's last comments. Since block/scsi_ioctl.c SG_IO was
    already testing against max_sectors and SCSI-ml was setting max_sectors and
    max_hw_sectors to the same value this does not change any scsi SG_IO behavior. It only
    prepares ll_rw_blk.c, scsi_ioctl.c and bio.c for when SCSI-ml begins to set
    a valid max_hw_sectors for all LLDs. Today if a LLD does not set it
    SCSI-ml sets it to a safe default and some LLDs set it to a artificial low
    value to overcome memory and feedback issues.
    
    Note: Since we now cap max_sectors to BLK_DEF_MAX_SECTORS, which is 1024,
    drivers that used to call blk_queue_max_sectors with a large value of
    max_sectors will now see the fs requests capped to BLK_DEF_MAX_SECTORS.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 509e9a03a328..a18500d196e1 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -702,7 +702,8 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
 #define MAX_PHYS_SEGMENTS 128
 #define MAX_HW_SEGMENTS 128
-#define MAX_SECTORS 255
+#define SAFE_MAX_SECTORS 255
+#define BLK_DEF_MAX_SECTORS 1024
 
 #define MAX_SEGMENT_SIZE	65536
 

commit 17e01f216b611fc46956dcd9063aec4de75991e3
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Fri Nov 11 05:31:37 2005 -0600

    [SCSI] add retries field to request for REQ_BLOCK_PC use
    
    For tape we need to control the retries. This patch adds a retries
    counter on the request for REQ_BLOCK_PC commands originating from
    scsi_execute* to use. REQ_BLOCK_PC commands comming from the block
    layer SG_IO path continue to use the retires set in the ULD init_command.
    (scsi_execute* does not set the gendisk so we do not execute
    the init_command in that path).
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 9a68716dcf75..509e9a03a328 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -184,6 +184,7 @@ struct request {
 	void *sense;
 
 	unsigned int timeout;
+	int retries;
 
 	/*
 	 * For Power Management requests

commit 6e39b69e7ea9205c5f80aeac3ef999ab8fb1a4cc
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Fri Nov 11 05:30:24 2005 -0600

    [SCSI] export blk layer functions needed for blk_execute_rq_nowait
    
    To send async requests we need these two functions exported.
    
    Signed-off-by: Mike Christie <michaelc@cs.wisc.edu>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a33a31e71bbc..9a68716dcf75 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -558,6 +558,7 @@ extern void blk_unregister_queue(struct gendisk *disk);
 extern void register_disk(struct gendisk *dev);
 extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
+extern void __blk_put_request(request_queue_t *, struct request *);
 extern void blk_end_sync_rq(struct request *rq);
 extern void blk_attempt_remerge(request_queue_t *, struct request *);
 extern struct request *blk_get_request(request_queue_t *, int, gfp_t);
@@ -579,6 +580,10 @@ extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned
 extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
+extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
+				  struct request *, int,
+				  void (*done)(struct request *));
+
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
 {
 	return bdev->bd_disk->queue;

commit 15853af9f07673680439b224519c692f1352b959
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Nov 10 08:52:05 2005 +0100

    [BLOCK] Implement elv_drain_elevator for improved switch error detection
    
    This patch adds request_queue->nr_sorted which keeps the number of
    requests in the iosched and implement elv_drain_elevator which
    performs forced dispatching.  elv_drain_elevator checks whether
    iosched actually dispatches all requests it has and prints error
    message if it doesn't.  As buggy forced dispatching can result in
    wrong barrier operations, I think this extra check is worthwhile.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 025a7f084dbd..a33a31e71bbc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -406,6 +406,7 @@ struct request_queue
 
 	atomic_t		refcnt;
 
+	unsigned int		nr_sorted;
 	unsigned int		in_flight;
 
 	/*
@@ -631,6 +632,7 @@ static inline void elv_dispatch_add_tail(struct request_queue *q,
 {
 	if (q->last_merge == rq)
 		q->last_merge = NULL;
+	q->nr_sorted--;
 
 	q->end_sector = rq_end_sector(rq);
 	q->boundary_rq = rq;

commit 5dd962494f76fb3ef1196cd420b5d6260d7a3766
Merge: 28d721e24c88 772eca782535
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Fri Oct 28 08:56:34 2005 -0700

    Merge branch 'elevator-switch' of git://brick.kernel.dk/data/git/linux-2.6-block
    
    Manual fixup for trivial "gfp_t" changes.

commit 28d721e24c88496ff8e9c4a0959bdc1415c0658e
Merge: 0ee40c662843 cb19833dccb3
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Fri Oct 28 08:53:49 2005 -0700

    Merge branch 'generic-dispatch' of git://brick.kernel.dk/data/git/linux-2.6-block

commit 8267e268e0914ac9371d07f711fcf20cc572993c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 21 03:20:53 2005 -0400

    [PATCH] gfp_t: block layer core
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index efdc9b5bc05c..1afbdb2d752c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -96,8 +96,8 @@ struct io_context {
 
 void put_io_context(struct io_context *ioc);
 void exit_io_context(void);
-struct io_context *current_io_context(int gfp_flags);
-struct io_context *get_io_context(int gfp_flags);
+struct io_context *current_io_context(gfp_t gfp_flags);
+struct io_context *get_io_context(gfp_t gfp_flags);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
 
@@ -354,7 +354,7 @@ struct request_queue
 	 * queue needs bounce pages for pages above this limit
 	 */
 	unsigned long		bounce_pfn;
-	unsigned int		bounce_gfp;
+	gfp_t			bounce_gfp;
 
 	/*
 	 * various queue flags, see QUEUE_* below
@@ -550,7 +550,7 @@ extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void blk_end_sync_rq(struct request *rq);
 extern void blk_attempt_remerge(request_queue_t *, struct request *);
-extern struct request *blk_get_request(request_queue_t *, int, int);
+extern struct request *blk_get_request(request_queue_t *, int, gfp_t);
 extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
 extern void blk_requeue_request(request_queue_t *, struct request *);
 extern void blk_plug_device(request_queue_t *);
@@ -565,7 +565,7 @@ extern void blk_run_queue(request_queue_t *);
 extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
 extern int blk_rq_unmap_user(struct bio *, unsigned int);
-extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, unsigned int);
+extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);
 extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
@@ -654,8 +654,8 @@ extern void blk_wait_queue_drained(request_queue_t *, int);
 extern void blk_finish_queue_drain(request_queue_t *);
 
 int blk_get_queue(request_queue_t *);
-request_queue_t *blk_alloc_queue(int gfp_mask);
-request_queue_t *blk_alloc_queue_node(int,int);
+request_queue_t *blk_alloc_queue(gfp_t);
+request_queue_t *blk_alloc_queue_node(gfp_t, int);
 #define blk_put_queue(q) blk_cleanup_queue((q))
 
 /*

commit 64521d1a3ba7cc7fe4ab5640da83d88144f05340
Author: Jens Axboe <axboe@suse.de>
Date:   Fri Oct 28 08:30:39 2005 +0200

    [BLOCK] elevator switch fixes/cleanup
    
    - 100msec sleep is a little excessive, lots of requests can complete
      in that timeframe. Use 10msec instead.
    - Rename QUEUE_FLAG_BYPASS to QUEUE_FLAG_ELVSWITCH to indicate what
      is going on.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6186d5e2110f..7d9d14d6d8f9 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -442,7 +442,7 @@ enum {
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
-#define QUEUE_FLAG_BYPASS	8	/* don't use elevator, just do FIFO */
+#define QUEUE_FLAG_ELVSWITCH	8	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_FLUSH	9	/* doing barrier flush sequence */
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)

commit cb98fc8bb9c141009e2bda99c0db39d387e142cf
Author: Tejun Heo <htejun@gmail.com>
Date:   Fri Oct 28 08:29:39 2005 +0200

    [BLOCK] Reimplement elevator switch
    
     This patch reimplements elevator switch.  This patch assumes generic
    dispatch queue patchset is applied.
    
     * Each request is tagged with REQ_ELVPRIV flag if it has its elevator
       private data set.
     * Requests which doesn't have REQ_ELVPRIV flag set never enter
       iosched.  They are always directly back inserted to dispatch queue.
       Of course, elevator_put_req_fn is called only for requests which
       have its REQ_ELVPRIV set.
     * Request queue maintains the current number of requests which have
       its elevator data set (elevator_set_req_fn called) in
       q->rq->elvpriv.
     * If a request queue has QUEUE_FLAG_BYPASS set, elevator private data
       is not allocated for new requests.
    
     To switch to another iosched, we set QUEUE_FLAG_BYPASS and wait until
    elvpriv goes to zero; then, we attach the new iosched and clears
    QUEUE_FLAG_BYPASS.  New implementation is much simpler and main code
    paths are less cluttered, IMHO.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 159dbcd2eb59..6186d5e2110f 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -107,9 +107,9 @@ typedef void (rq_end_io_fn)(struct request *);
 struct request_list {
 	int count[2];
 	int starved[2];
+	int elvpriv;
 	mempool_t *rq_pool;
 	wait_queue_head_t wait[2];
-	wait_queue_head_t drain;
 };
 
 #define BLK_MAX_CDB	16
@@ -211,6 +211,7 @@ enum rq_flag_bits {
 	__REQ_STARTED,		/* drive already may have started this one */
 	__REQ_DONTPREP,		/* don't call prep for this one */
 	__REQ_QUEUED,		/* uses queueing */
+	__REQ_ELVPRIV,		/* elevator private data attached */
 	/*
 	 * for ATA/ATAPI devices
 	 */
@@ -244,6 +245,7 @@ enum rq_flag_bits {
 #define REQ_STARTED	(1 << __REQ_STARTED)
 #define REQ_DONTPREP	(1 << __REQ_DONTPREP)
 #define REQ_QUEUED	(1 << __REQ_QUEUED)
+#define REQ_ELVPRIV	(1 << __REQ_ELVPRIV)
 #define REQ_PC		(1 << __REQ_PC)
 #define REQ_BLOCK_PC	(1 << __REQ_BLOCK_PC)
 #define REQ_SENSE	(1 << __REQ_SENSE)
@@ -413,8 +415,6 @@ struct request_queue
 	unsigned int		sg_reserved_size;
 	int			node;
 
-	struct list_head	drain_list;
-
 	/*
 	 * reserved for flush operations
 	 */
@@ -442,7 +442,7 @@ enum {
 #define QUEUE_FLAG_DEAD		5	/* queue being torn down */
 #define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
 #define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
-#define QUEUE_FLAG_DRAIN	8	/* draining queue for sched switch */
+#define QUEUE_FLAG_BYPASS	8	/* don't use elevator, just do FIFO */
 #define QUEUE_FLAG_FLUSH	9	/* doing barrier flush sequence */
 
 #define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
@@ -668,8 +668,6 @@ extern void blk_dump_rq_flags(struct request *, char *);
 extern void generic_unplug_device(request_queue_t *);
 extern void __generic_unplug_device(request_queue_t *);
 extern long nr_blockdev_pages(void);
-extern void blk_wait_queue_drained(request_queue_t *, int);
-extern void blk_finish_queue_drain(request_queue_t *);
 
 int blk_get_queue(request_queue_t *);
 request_queue_t *blk_alloc_queue(int gfp_mask);

commit cb19833dccb32f97cacbfff834b53523915f13f6
Author: Tejun Heo <htejun@gmail.com>
Date:   Mon Oct 24 08:35:58 2005 +0200

    [BLOCK] kill generic max_back_kb handling
    
    This patch kills max_back_kb handling from elv_dispatch_sort() and
    kills max_back_kb field from struct request_queue.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bd55b1df8997..159dbcd2eb59 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -339,7 +339,6 @@ struct request_queue
 	 */
 	sector_t		end_sector;
 	struct request		*boundary_rq;
-	unsigned int		max_back_kb;
 
 	/*
 	 * Auto-unplugging state

commit 06b86245c052963029bfd9020ca1f08ceb66f85a
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Oct 20 16:46:23 2005 +0200

    [PATCH] 03/05 move last_merge handlin into generic elevator code
    
    Currently, both generic elevator code and specific ioscheds
    participate in the management and usage of last_merge.  This
    and the following patches move last_merge handling into
    generic elevator code.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index a2e496f285a4..bd55b1df8997 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -630,6 +630,9 @@ static inline void blkdev_dequeue_request(struct request *req)
 static inline void elv_dispatch_add_tail(struct request_queue *q,
 					 struct request *rq)
 {
+	if (q->last_merge == rq)
+		q->last_merge = NULL;
+
 	q->end_sector = rq_end_sector(rq);
 	q->boundary_rq = rq;
 	list_add_tail(&rq->queuelist, &q->queue_head);

commit 1b47f531e244e339b648dfff071c086f936e49e1
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Oct 20 16:37:00 2005 +0200

    [PATCH] generic dispatch fixes
    
    - Split elv_dispatch_insert() into two functions
    - Rename rq_last_sector() to rq_end_sector()
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 2c7b9154927a..a2e496f285a4 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -337,7 +337,7 @@ struct request_queue
 	/*
 	 * Dispatch queue sorting
 	 */
-	sector_t		last_sector;
+	sector_t		end_sector;
 	struct request		*boundary_rq;
 	unsigned int		max_back_kb;
 
@@ -624,6 +624,17 @@ static inline void blkdev_dequeue_request(struct request *req)
 	elv_dequeue_request(req->q, req);
 }
 
+/*
+ * This should be in elevator.h, but that requires pulling in rq and q
+ */
+static inline void elv_dispatch_add_tail(struct request_queue *q,
+					 struct request *rq)
+{
+	q->end_sector = rq_end_sector(rq);
+	q->boundary_rq = rq;
+	list_add_tail(&rq->queuelist, &q->queue_head);
+}
+
 /*
  * Access functions for manipulating queue properties
  */

commit 8922e16cf6269e668123acb1ae1fdc62b7a3a4fc
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Oct 20 16:23:44 2005 +0200

    [PATCH] 01/05 Implement generic dispatch queue
    
    Implements generic dispatch queue which can replace all
    dispatch queues implemented by each iosched.  This reduces
    code duplication, eases enforcing semantics over dispatch
    queue, and simplifies specific ioscheds.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index efdc9b5bc05c..2c7b9154927a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -203,6 +203,7 @@ struct request {
 enum rq_flag_bits {
 	__REQ_RW,		/* not set, read. set, write */
 	__REQ_FAILFAST,		/* no low level driver retries */
+	__REQ_SORTED,		/* elevator knows about this request */
 	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
 	__REQ_HARDBARRIER,	/* may not be passed by drive either */
 	__REQ_CMD,		/* is a regular fs rw request */
@@ -235,6 +236,7 @@ enum rq_flag_bits {
 
 #define REQ_RW		(1 << __REQ_RW)
 #define REQ_FAILFAST	(1 << __REQ_FAILFAST)
+#define REQ_SORTED	(1 << __REQ_SORTED)
 #define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
 #define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
 #define REQ_CMD		(1 << __REQ_CMD)
@@ -332,6 +334,13 @@ struct request_queue
 	prepare_flush_fn	*prepare_flush_fn;
 	end_flush_fn		*end_flush_fn;
 
+	/*
+	 * Dispatch queue sorting
+	 */
+	sector_t		last_sector;
+	struct request		*boundary_rq;
+	unsigned int		max_back_kb;
+
 	/*
 	 * Auto-unplugging state
 	 */
@@ -454,6 +463,7 @@ enum {
 #define blk_pm_request(rq)	\
 	((rq)->flags & (REQ_PM_SUSPEND | REQ_PM_RESUME))
 
+#define blk_sorted_rq(rq)	((rq)->flags & REQ_SORTED)
 #define blk_barrier_rq(rq)	((rq)->flags & REQ_HARDBARRIER)
 #define blk_barrier_preflush(rq)	((rq)->flags & REQ_BAR_PREFLUSH)
 #define blk_barrier_postflush(rq)	((rq)->flags & REQ_BAR_POSTFLUSH)
@@ -611,12 +621,7 @@ extern void end_request(struct request *req, int uptodate);
 
 static inline void blkdev_dequeue_request(struct request *req)
 {
-	BUG_ON(list_empty(&req->queuelist));
-
-	list_del_init(&req->queuelist);
-
-	if (req->rl)
-		elv_remove_request(req->q, req);
+	elv_dequeue_request(req->q, req);
 }
 
 /*

commit 2befb9e36dc52d715c3a67a9dbad36ac1edc376f
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sat Sep 10 00:27:17 2005 -0700

    [PATCH] include/linux/blkdev.h: "extern inline" -> "static inline"
    
    "extern inline" doesn't make much sense.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index aefa26fbae8a..efdc9b5bc05c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -728,7 +728,7 @@ static inline unsigned int blksize_bits(unsigned int size)
 	return bits;
 }
 
-extern inline unsigned int block_size(struct block_device *bdev)
+static inline unsigned int block_size(struct block_device *bdev)
 {
 	return bdev->bd_block_size;
 }

commit 31151ba2cef171344beac254e65bd7e00138bb0d
Merge: 3d52acb34247 73747aed04d3
Author: James Bottomley <jejb@titanic.(none)>
Date:   Sun Aug 28 10:43:07 2005 -0500

    fix mismerge in ll_rw_blk.c

commit ba02508248e90a9d696aebd18b48a3290235b53c
Author: Tejun Heo <htejun@gmail.com>
Date:   Fri Aug 5 13:28:11 2005 -0700

    [PATCH] blk: fix tag shrinking (revive real_max_size)
    
    My patch in commit fa72b903f75e4f0f0b2c2feed093005167da4023 incorrectly
    removed blk_queue_tag->real_max_depth.
    
    The original resize implementation was incorrect in the following
    points.
    
     * actual allocation size of tag_index was shorter than real_max_size,
       but assumed to be of the same size, possibly causing memory access
       beyond the allocated area.
     * bits in tag_map between max_deptn and real_max_depth were
       initialized to 1's, making the tags permanently reserved.
    
    In an attempt to fix above two bugs, I had removed allocation optimization
    in init_tag_map and real_max_size.  Tag map/index were allocated and freed
    immediately during resize.
    
    Unfortunately, I wasn't considering that tag map/index can be resized
    dynamically with tags beyond new_depth active.  This led to accessing
    freed area after shrinking tags and led to the following bug reporting
    thread on linux-scsi.
    
       http://marc.theaimsgroup.com/?l=linux-scsi&m=112319898111885&w=2
    
    To fix the problem, I've revived real_max_depth without allocation
    optimization in init_tag_map, and Andrew Vasquez confirmed that the
    problem was fixed.  As Jens is not going to be available for a week, he
    asked me to make sure that this patch reaches you.
    
       http://marc.theaimsgroup.com/?l=linux-scsi&m=112325778530886&w=2
    
    Also, a comment was added to make sure that real_max_size is needed for
    dynamic shrinking.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0881b5cdee3d..19bd8e7e11bf 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -301,6 +301,7 @@ struct blk_queue_tag {
 	struct list_head busy_list;	/* fifo list of busy tags */
 	int busy;			/* current depth */
 	int max_depth;			/* what we will send to device */
+	int real_max_depth;		/* what the array can hold */
 	atomic_t refcnt;		/* map can be shared */
 };
 

commit fb3cc4320e1fd87143683b540e459a2e20fdc9bb
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Tue Jun 28 20:45:15 2005 -0700

    [PATCH] blk: light iocontext ops
    
    get_io_context needlessly turned off interrupts and checked for racing io
    context creations.  Both of which aren't needed, because the io context can
    only be created while in process context of the current process.
    
    Also, split the function in 2.  A light version, current_io_context does not
    elevate the reference count specifically, but can be used when in process
    context, because the process holds a reference itself.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 21a8674cd149..0881b5cdee3d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -96,6 +96,7 @@ struct io_context {
 
 void put_io_context(struct io_context *ioc);
 void exit_io_context(void);
+struct io_context *current_io_context(int gfp_flags);
 struct io_context *get_io_context(int gfp_flags);
 void copy_io_context(struct io_context **pdst, struct io_context **psrc);
 void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index b54a0348a890..21a8674cd149 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -54,16 +54,23 @@ struct as_io_context {
 
 struct cfq_queue;
 struct cfq_io_context {
-	void (*dtor)(struct cfq_io_context *);
-	void (*exit)(struct cfq_io_context *);
-
-	struct io_context *ioc;
-
 	/*
 	 * circular list of cfq_io_contexts belonging to a process io context
 	 */
 	struct list_head list;
 	struct cfq_queue *cfqq;
+	void *key;
+
+	struct io_context *ioc;
+
+	unsigned long last_end_request;
+	unsigned long last_queue;
+	unsigned long ttime_total;
+	unsigned long ttime_samples;
+	unsigned long ttime_mean;
+
+	void (*dtor)(struct cfq_io_context *);
+	void (*exit)(struct cfq_io_context *);
 };
 
 /*
@@ -73,7 +80,9 @@ struct cfq_io_context {
  */
 struct io_context {
 	atomic_t refcount;
-	pid_t pid;
+	struct task_struct *task;
+
+	int (*set_ioprio)(struct io_context *, unsigned int);
 
 	/*
 	 * For request batching
@@ -81,8 +90,6 @@ struct io_context {
 	unsigned long last_waited; /* Time last woken after wait for request */
 	int nr_batch_requests;     /* Number of requests left in the batch */
 
-	spinlock_t lock;
-
 	struct as_io_context *aic;
 	struct cfq_io_context *cic;
 };
@@ -134,6 +141,8 @@ struct request {
 
 	void *elevator_private;
 
+	unsigned short ioprio;
+
 	int rq_status;	/* should split this into a few status bits */
 	struct gendisk *rq_disk;
 	int errors;

commit 93d17d3d84b7147e8f07aeeb15ec01aa92c6b564
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sat Jun 25 14:59:10 2005 -0700

    [PATCH] drivers/block/ll_rw_blk.c: cleanups
    
    This patch contains the following cleanups:
    - make needlessly global code static
    - remove the following unused global functions:
      - blkdev_scsi_issue_flush_fn
      - __blk_attempt_remerge
    - remove the following unused EXPORT_SYMBOL's:
      - blk_phys_contig_segment
      - blk_hw_contig_segment
      - blkdev_scsi_issue_flush_fn
      - __blk_attempt_remerge
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 60272141ff19..b54a0348a890 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -539,15 +539,12 @@ extern void generic_make_request(struct bio *bio);
 extern void blk_put_request(struct request *);
 extern void blk_end_sync_rq(struct request *rq);
 extern void blk_attempt_remerge(request_queue_t *, struct request *);
-extern void __blk_attempt_remerge(request_queue_t *, struct request *);
 extern struct request *blk_get_request(request_queue_t *, int, int);
 extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
 extern void blk_requeue_request(request_queue_t *, struct request *);
 extern void blk_plug_device(request_queue_t *);
 extern int blk_remove_plug(request_queue_t *);
 extern void blk_recount_segments(request_queue_t *, struct bio *);
-extern int blk_phys_contig_segment(request_queue_t *q, struct bio *, struct bio *);
-extern int blk_hw_contig_segment(request_queue_t *q, struct bio *, struct bio *);
 extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
 extern void blk_start_queue(request_queue_t *q);
 extern void blk_stop_queue(request_queue_t *q);
@@ -631,7 +628,6 @@ extern void blk_queue_dma_alignment(request_queue_t *, int);
 extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
 extern void blk_queue_ordered(request_queue_t *, int);
 extern void blk_queue_issue_flush_fn(request_queue_t *, issue_flush_fn *);
-extern int blkdev_scsi_issue_flush_fn(request_queue_t *, struct gendisk *, sector_t *);
 extern struct request *blk_start_pre_flush(request_queue_t *,struct request *);
 extern int blk_complete_barrier_rq(request_queue_t *, struct request *, int);
 extern int blk_complete_barrier_rq_locked(request_queue_t *, struct request *, int);
@@ -675,8 +671,6 @@ extern int blkdev_issue_flush(struct block_device *, sector_t *);
 
 #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
 
-extern void drive_stat_acct(struct request *, int, int);
-
 static inline int queue_hardsect_size(request_queue_t *q)
 {
 	int retval = 512;

commit f7d37d028dfba90b1b747f8ac685bf0959aeda8b
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Jun 23 00:08:50 2005 -0700

    [PATCH] blk: remove BLK_TAGS_{PER_LONG|MASK}
    
    Replace BLK_TAGS_PER_LONG with BITS_PER_LONG and remove unused BLK_TAGS_MASK.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 8d7e2f4151d0..60272141ff19 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -285,9 +285,6 @@ enum blk_queue_state {
 	Queue_up,
 };
 
-#define BLK_TAGS_PER_LONG	(sizeof(unsigned long) * 8)
-#define BLK_TAGS_MASK		(BLK_TAGS_PER_LONG - 1)
-
 struct blk_queue_tag {
 	struct request **tag_index;	/* map of busy tags */
 	unsigned long *tag_map;		/* bit map of free/busy tags */

commit fa72b903f75e4f0f0b2c2feed093005167da4023
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Jun 23 00:08:49 2005 -0700

    [PATCH] blk: remove blk_queue_tag->real_max_depth optimization
    
    blk_queue_tag->real_max_depth was used to optimize out unnecessary
    allocations/frees on tag resize.  However, the whole thing was very broken -
    tag_map was never allocated to real_max_depth resulting in access beyond the
    end of the map, bits in [max_depth..real_max_depth] were set when initializing
    a map and copied when resizing resulting in pre-occupied tags.
    
    As the gain of the optimization is very small, well, almost nill, remove the
    whole thing.
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 235c3414d268..8d7e2f4151d0 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -294,7 +294,6 @@ struct blk_queue_tag {
 	struct list_head busy_list;	/* fifo list of busy tags */
 	int busy;			/* current depth */
 	int max_depth;			/* what we will send to device */
-	int real_max_depth;		/* what the array can hold */
 	atomic_t refcnt;		/* map can be shared */
 };
 

commit 1946089a109251655c5438d92c539bd2930e71ea
Author: Christoph Lameter <christoph@lameter.com>
Date:   Thu Jun 23 00:08:19 2005 -0700

    [PATCH] NUMA aware block device control structure allocation
    
    Patch to allocate the control structures for for ide devices on the node of
    the device itself (for NUMA systems).  The patch depends on the Slab API
    change patch by Manfred and me (in mm) and the pcidev_to_node patch that I
    posted today.
    
    Does some realignment too.
    
    Signed-off-by: Justin M. Forbes <jmforbes@linuxtx.org>
    Signed-off-by: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Pravin Shelar <pravin@calsoftinc.com>
    Signed-off-by: Shobhit Dayal <shobhit@calsoftinc.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4a99b76c5a33..235c3414d268 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -396,6 +396,7 @@ struct request_queue
 	 */
 	unsigned int		sg_timeout;
 	unsigned int		sg_reserved_size;
+	int			node;
 
 	struct list_head	drain_list;
 
@@ -615,6 +616,8 @@ static inline void blkdev_dequeue_request(struct request *req)
 /*
  * Access functions for manipulating queue properties
  */
+extern request_queue_t *blk_init_queue_node(request_fn_proc *rfn,
+					spinlock_t *lock, int node_id);
 extern request_queue_t *blk_init_queue(request_fn_proc *, spinlock_t *);
 extern void blk_cleanup_queue(request_queue_t *);
 extern void blk_queue_make_request(request_queue_t *, make_request_fn *);
@@ -646,7 +649,8 @@ extern void blk_wait_queue_drained(request_queue_t *, int);
 extern void blk_finish_queue_drain(request_queue_t *);
 
 int blk_get_queue(request_queue_t *);
-request_queue_t *blk_alloc_queue(int);
+request_queue_t *blk_alloc_queue(int gfp_mask);
+request_queue_t *blk_alloc_queue_node(int,int);
 #define blk_put_queue(q) blk_cleanup_queue((q))
 
 /*

commit 994ca9a19616f0d4161a9e825f0835925d522426
Author: James Bottomley <James.Bottomley@SteelEye.com>
Date:   Mon Jun 20 14:11:09 2005 +0200

    [PATCH] update blk_execute_rq to take an at_head parameter
    
    Original From: Mike Christie <michaelc@cs.wisc.edu>
    
    Modified to split out block changes (this patch) and SCSI pieces.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0430ea3e5f2e..a48dc12c6699 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -562,8 +562,8 @@ extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, u
 extern int blk_rq_unmap_user(struct bio *, unsigned int);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, unsigned int);
 extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
-extern int blk_execute_rq(request_queue_t *, struct gendisk *, struct request *);
-
+extern int blk_execute_rq(request_queue_t *, struct gendisk *,
+			  struct request *, int);
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
 {
 	return bdev->bd_disk->queue;

commit f1970baf6d74e03bd32072ab453f2fc01bc1b8d3
Author: James Bottomley <James.Bottomley@SteelEye.com>
Date:   Mon Jun 20 14:06:52 2005 +0200

    [PATCH] Add scatter-gather support for the block layer SG_IO
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fc0dce078616..0430ea3e5f2e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -561,6 +561,7 @@ extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
 extern int blk_rq_unmap_user(struct bio *, unsigned int);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, unsigned int);
+extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *, struct request *);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)

commit dd1cab95f356f1395278633565f198463cf6bd24
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 20 14:06:01 2005 +0200

    [PATCH] Cleanup blk_rq_map_* interfaces
    
    Change the blk_rq_map_user() and blk_rq_map_kern() interface to require
    a previously allocated request to be passed in. This is both more efficient
    for multiple iterations of mapping data to the same request, and it is also
    a much nicer API.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 67339bc5f6bc..fc0dce078616 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -558,10 +558,9 @@ extern void blk_sync_queue(struct request_queue *q);
 extern void __blk_stop_queue(request_queue_t *q);
 extern void blk_run_queue(request_queue_t *);
 extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
-extern struct request *blk_rq_map_user(request_queue_t *, int, void __user *, unsigned int);
-extern int blk_rq_unmap_user(struct request *, struct bio *, unsigned int);
-extern struct request *blk_rq_map_kern(request_queue_t *, int, void *,
-					unsigned int, unsigned int);
+extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
+extern int blk_rq_unmap_user(struct bio *, unsigned int);
+extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, unsigned int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *, struct request *);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)

commit df46b9a44ceb5af2ea2351ce8e28ae7bd840b00f
Author: Mike Christie <michaelc@cs.wisc.edu>
Date:   Mon Jun 20 14:04:44 2005 +0200

    [PATCH] Add blk_rq_map_kern()
    
    Add blk_rq_map_kern which takes a kernel buffer and maps it into
    a request and bio. This can be used by the dm hw_handlers, old
    sg_scsi_ioctl, and one day scsi special requests so all requests
    comming into scsi will have bios. All requests having bios
    should allow scsi to use scatter lists for all IO and allow it
    to use block layer functions.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 4a99b76c5a33..67339bc5f6bc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -560,6 +560,8 @@ extern void blk_run_queue(request_queue_t *);
 extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
 extern struct request *blk_rq_map_user(request_queue_t *, int, void __user *, unsigned int);
 extern int blk_rq_unmap_user(struct request *, struct bio *, unsigned int);
+extern struct request *blk_rq_map_kern(request_queue_t *, int, void *,
+					unsigned int, unsigned int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *, struct request *);
 
 static inline request_queue_t *bdev_get_queue(struct block_device *bdev)

commit 867d1191fca388a79e4bb500dd85a9e871c96b99
Author: Tejun Heo <htejun@gmail.com>
Date:   Sun Apr 24 02:06:05 2005 -0500

    [SCSI] remove requeue feature from blk_insert_request()
    
    blk_insert_request() has a unobivous feature of requeuing a
    request setting REQ_SPECIAL|REQ_SOFTBARRIER.  SCSI midlayer
    was the only user and as previous patches removed the usage,
    remove the feature from blk_insert_request().  Only special
    requests should be queued with blk_insert_request().  All
    requeueing should go through blk_requeue_request().
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index ef1afc178c0a..4a99b76c5a33 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -544,7 +544,7 @@ extern void blk_end_sync_rq(struct request *rq);
 extern void blk_attempt_remerge(request_queue_t *, struct request *);
 extern void __blk_attempt_remerge(request_queue_t *, struct request *);
 extern struct request *blk_get_request(request_queue_t *, int, int);
-extern void blk_insert_request(request_queue_t *, struct request *, int, void *, int);
+extern void blk_insert_request(request_queue_t *, struct request *, int, void *);
 extern void blk_requeue_request(request_queue_t *, struct request *);
 extern void blk_plug_device(request_queue_t *);
 extern int blk_remove_plug(request_queue_t *);

commit 152587deb8903c0edf483a5b889f975bc6bea7e0
Author:  <axboe@suse.de>
Date:   Tue Apr 12 16:22:06 2005 -0500

    [PATCH] fix NMI lockup with CFQ scheduler
    
    The current problem seen is that the queue lock is actually in the
    SCSI device structure, so when that structure is freed on device
    release, we go boom if the queue tries to access the lock again.
    
    The fix here is to move the lock from the scsi_device to the queue.
    
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 70ac2860a605..ef1afc178c0a 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -355,8 +355,11 @@ struct request_queue
 	unsigned long		queue_flags;
 
 	/*
-	 * protects queue structures from reentrancy
+	 * protects queue structures from reentrancy. ->__queue_lock should
+	 * _never_ be used directly, it is queue private. always use
+	 * ->queue_lock.
 	 */
+	spinlock_t		__queue_lock;
 	spinlock_t		*queue_lock;
 
 	/*

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
new file mode 100644
index 000000000000..70ac2860a605
--- /dev/null
+++ b/include/linux/blkdev.h
@@ -0,0 +1,759 @@
+#ifndef _LINUX_BLKDEV_H
+#define _LINUX_BLKDEV_H
+
+#include <linux/config.h>
+#include <linux/major.h>
+#include <linux/genhd.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/backing-dev.h>
+#include <linux/wait.h>
+#include <linux/mempool.h>
+#include <linux/bio.h>
+#include <linux/module.h>
+#include <linux/stringify.h>
+
+#include <asm/scatterlist.h>
+
+struct request_queue;
+typedef struct request_queue request_queue_t;
+struct elevator_queue;
+typedef struct elevator_queue elevator_t;
+struct request_pm_state;
+
+#define BLKDEV_MIN_RQ	4
+#define BLKDEV_MAX_RQ	128	/* Default maximum */
+
+/*
+ * This is the per-process anticipatory I/O scheduler state.
+ */
+struct as_io_context {
+	spinlock_t lock;
+
+	void (*dtor)(struct as_io_context *aic); /* destructor */
+	void (*exit)(struct as_io_context *aic); /* called on task exit */
+
+	unsigned long state;
+	atomic_t nr_queued; /* queued reads & sync writes */
+	atomic_t nr_dispatched; /* number of requests gone to the drivers */
+
+	/* IO History tracking */
+	/* Thinktime */
+	unsigned long last_end_request;
+	unsigned long ttime_total;
+	unsigned long ttime_samples;
+	unsigned long ttime_mean;
+	/* Layout pattern */
+	unsigned int seek_samples;
+	sector_t last_request_pos;
+	u64 seek_total;
+	sector_t seek_mean;
+};
+
+struct cfq_queue;
+struct cfq_io_context {
+	void (*dtor)(struct cfq_io_context *);
+	void (*exit)(struct cfq_io_context *);
+
+	struct io_context *ioc;
+
+	/*
+	 * circular list of cfq_io_contexts belonging to a process io context
+	 */
+	struct list_head list;
+	struct cfq_queue *cfqq;
+};
+
+/*
+ * This is the per-process I/O subsystem state.  It is refcounted and
+ * kmalloc'ed. Currently all fields are modified in process io context
+ * (apart from the atomic refcount), so require no locking.
+ */
+struct io_context {
+	atomic_t refcount;
+	pid_t pid;
+
+	/*
+	 * For request batching
+	 */
+	unsigned long last_waited; /* Time last woken after wait for request */
+	int nr_batch_requests;     /* Number of requests left in the batch */
+
+	spinlock_t lock;
+
+	struct as_io_context *aic;
+	struct cfq_io_context *cic;
+};
+
+void put_io_context(struct io_context *ioc);
+void exit_io_context(void);
+struct io_context *get_io_context(int gfp_flags);
+void copy_io_context(struct io_context **pdst, struct io_context **psrc);
+void swap_io_context(struct io_context **ioc1, struct io_context **ioc2);
+
+struct request;
+typedef void (rq_end_io_fn)(struct request *);
+
+struct request_list {
+	int count[2];
+	int starved[2];
+	mempool_t *rq_pool;
+	wait_queue_head_t wait[2];
+	wait_queue_head_t drain;
+};
+
+#define BLK_MAX_CDB	16
+
+/*
+ * try to put the fields that are referenced together in the same cacheline
+ */
+struct request {
+	struct list_head queuelist; /* looking for ->queue? you must _not_
+				     * access it directly, use
+				     * blkdev_dequeue_request! */
+	unsigned long flags;		/* see REQ_ bits below */
+
+	/* Maintain bio traversal state for part by part I/O submission.
+	 * hard_* are block layer internals, no driver should touch them!
+	 */
+
+	sector_t sector;		/* next sector to submit */
+	unsigned long nr_sectors;	/* no. of sectors left to submit */
+	/* no. of sectors left to submit in the current segment */
+	unsigned int current_nr_sectors;
+
+	sector_t hard_sector;		/* next sector to complete */
+	unsigned long hard_nr_sectors;	/* no. of sectors left to complete */
+	/* no. of sectors left to complete in the current segment */
+	unsigned int hard_cur_sectors;
+
+	struct bio *bio;
+	struct bio *biotail;
+
+	void *elevator_private;
+
+	int rq_status;	/* should split this into a few status bits */
+	struct gendisk *rq_disk;
+	int errors;
+	unsigned long start_time;
+
+	/* Number of scatter-gather DMA addr+len pairs after
+	 * physical address coalescing is performed.
+	 */
+	unsigned short nr_phys_segments;
+
+	/* Number of scatter-gather addr+len pairs after
+	 * physical and DMA remapping hardware coalescing is performed.
+	 * This is the number of scatter-gather entries the driver
+	 * will actually have to deal with after DMA mapping is done.
+	 */
+	unsigned short nr_hw_segments;
+
+	int tag;
+	char *buffer;
+
+	int ref_count;
+	request_queue_t *q;
+	struct request_list *rl;
+
+	struct completion *waiting;
+	void *special;
+
+	/*
+	 * when request is used as a packet command carrier
+	 */
+	unsigned int cmd_len;
+	unsigned char cmd[BLK_MAX_CDB];
+
+	unsigned int data_len;
+	void *data;
+
+	unsigned int sense_len;
+	void *sense;
+
+	unsigned int timeout;
+
+	/*
+	 * For Power Management requests
+	 */
+	struct request_pm_state *pm;
+
+	/*
+	 * completion callback. end_io_data should be folded in with waiting
+	 */
+	rq_end_io_fn *end_io;
+	void *end_io_data;
+};
+
+/*
+ * first three bits match BIO_RW* bits, important
+ */
+enum rq_flag_bits {
+	__REQ_RW,		/* not set, read. set, write */
+	__REQ_FAILFAST,		/* no low level driver retries */
+	__REQ_SOFTBARRIER,	/* may not be passed by ioscheduler */
+	__REQ_HARDBARRIER,	/* may not be passed by drive either */
+	__REQ_CMD,		/* is a regular fs rw request */
+	__REQ_NOMERGE,		/* don't touch this for merging */
+	__REQ_STARTED,		/* drive already may have started this one */
+	__REQ_DONTPREP,		/* don't call prep for this one */
+	__REQ_QUEUED,		/* uses queueing */
+	/*
+	 * for ATA/ATAPI devices
+	 */
+	__REQ_PC,		/* packet command (special) */
+	__REQ_BLOCK_PC,		/* queued down pc from block layer */
+	__REQ_SENSE,		/* sense retrival */
+
+	__REQ_FAILED,		/* set if the request failed */
+	__REQ_QUIET,		/* don't worry about errors */
+	__REQ_SPECIAL,		/* driver suplied command */
+	__REQ_DRIVE_CMD,
+	__REQ_DRIVE_TASK,
+	__REQ_DRIVE_TASKFILE,
+	__REQ_PREEMPT,		/* set for "ide_preempt" requests */
+	__REQ_PM_SUSPEND,	/* suspend request */
+	__REQ_PM_RESUME,	/* resume request */
+	__REQ_PM_SHUTDOWN,	/* shutdown request */
+	__REQ_BAR_PREFLUSH,	/* barrier pre-flush done */
+	__REQ_BAR_POSTFLUSH,	/* barrier post-flush */
+	__REQ_BAR_FLUSH,	/* rq is the flush request */
+	__REQ_NR_BITS,		/* stops here */
+};
+
+#define REQ_RW		(1 << __REQ_RW)
+#define REQ_FAILFAST	(1 << __REQ_FAILFAST)
+#define REQ_SOFTBARRIER	(1 << __REQ_SOFTBARRIER)
+#define REQ_HARDBARRIER	(1 << __REQ_HARDBARRIER)
+#define REQ_CMD		(1 << __REQ_CMD)
+#define REQ_NOMERGE	(1 << __REQ_NOMERGE)
+#define REQ_STARTED	(1 << __REQ_STARTED)
+#define REQ_DONTPREP	(1 << __REQ_DONTPREP)
+#define REQ_QUEUED	(1 << __REQ_QUEUED)
+#define REQ_PC		(1 << __REQ_PC)
+#define REQ_BLOCK_PC	(1 << __REQ_BLOCK_PC)
+#define REQ_SENSE	(1 << __REQ_SENSE)
+#define REQ_FAILED	(1 << __REQ_FAILED)
+#define REQ_QUIET	(1 << __REQ_QUIET)
+#define REQ_SPECIAL	(1 << __REQ_SPECIAL)
+#define REQ_DRIVE_CMD	(1 << __REQ_DRIVE_CMD)
+#define REQ_DRIVE_TASK	(1 << __REQ_DRIVE_TASK)
+#define REQ_DRIVE_TASKFILE	(1 << __REQ_DRIVE_TASKFILE)
+#define REQ_PREEMPT	(1 << __REQ_PREEMPT)
+#define REQ_PM_SUSPEND	(1 << __REQ_PM_SUSPEND)
+#define REQ_PM_RESUME	(1 << __REQ_PM_RESUME)
+#define REQ_PM_SHUTDOWN	(1 << __REQ_PM_SHUTDOWN)
+#define REQ_BAR_PREFLUSH	(1 << __REQ_BAR_PREFLUSH)
+#define REQ_BAR_POSTFLUSH	(1 << __REQ_BAR_POSTFLUSH)
+#define REQ_BAR_FLUSH	(1 << __REQ_BAR_FLUSH)
+
+/*
+ * State information carried for REQ_PM_SUSPEND and REQ_PM_RESUME
+ * requests. Some step values could eventually be made generic.
+ */
+struct request_pm_state
+{
+	/* PM state machine step value, currently driver specific */
+	int	pm_step;
+	/* requested PM state value (S1, S2, S3, S4, ...) */
+	u32	pm_state;
+	void*	data;		/* for driver use */
+};
+
+#include <linux/elevator.h>
+
+typedef int (merge_request_fn) (request_queue_t *, struct request *,
+				struct bio *);
+typedef int (merge_requests_fn) (request_queue_t *, struct request *,
+				 struct request *);
+typedef void (request_fn_proc) (request_queue_t *q);
+typedef int (make_request_fn) (request_queue_t *q, struct bio *bio);
+typedef int (prep_rq_fn) (request_queue_t *, struct request *);
+typedef void (unplug_fn) (request_queue_t *);
+
+struct bio_vec;
+typedef int (merge_bvec_fn) (request_queue_t *, struct bio *, struct bio_vec *);
+typedef void (activity_fn) (void *data, int rw);
+typedef int (issue_flush_fn) (request_queue_t *, struct gendisk *, sector_t *);
+typedef int (prepare_flush_fn) (request_queue_t *, struct request *);
+typedef void (end_flush_fn) (request_queue_t *, struct request *);
+
+enum blk_queue_state {
+	Queue_down,
+	Queue_up,
+};
+
+#define BLK_TAGS_PER_LONG	(sizeof(unsigned long) * 8)
+#define BLK_TAGS_MASK		(BLK_TAGS_PER_LONG - 1)
+
+struct blk_queue_tag {
+	struct request **tag_index;	/* map of busy tags */
+	unsigned long *tag_map;		/* bit map of free/busy tags */
+	struct list_head busy_list;	/* fifo list of busy tags */
+	int busy;			/* current depth */
+	int max_depth;			/* what we will send to device */
+	int real_max_depth;		/* what the array can hold */
+	atomic_t refcnt;		/* map can be shared */
+};
+
+struct request_queue
+{
+	/*
+	 * Together with queue_head for cacheline sharing
+	 */
+	struct list_head	queue_head;
+	struct request		*last_merge;
+	elevator_t		*elevator;
+
+	/*
+	 * the queue request freelist, one for reads and one for writes
+	 */
+	struct request_list	rq;
+
+	request_fn_proc		*request_fn;
+	merge_request_fn	*back_merge_fn;
+	merge_request_fn	*front_merge_fn;
+	merge_requests_fn	*merge_requests_fn;
+	make_request_fn		*make_request_fn;
+	prep_rq_fn		*prep_rq_fn;
+	unplug_fn		*unplug_fn;
+	merge_bvec_fn		*merge_bvec_fn;
+	activity_fn		*activity_fn;
+	issue_flush_fn		*issue_flush_fn;
+	prepare_flush_fn	*prepare_flush_fn;
+	end_flush_fn		*end_flush_fn;
+
+	/*
+	 * Auto-unplugging state
+	 */
+	struct timer_list	unplug_timer;
+	int			unplug_thresh;	/* After this many requests */
+	unsigned long		unplug_delay;	/* After this many jiffies */
+	struct work_struct	unplug_work;
+
+	struct backing_dev_info	backing_dev_info;
+
+	/*
+	 * The queue owner gets to use this for whatever they like.
+	 * ll_rw_blk doesn't touch it.
+	 */
+	void			*queuedata;
+
+	void			*activity_data;
+
+	/*
+	 * queue needs bounce pages for pages above this limit
+	 */
+	unsigned long		bounce_pfn;
+	unsigned int		bounce_gfp;
+
+	/*
+	 * various queue flags, see QUEUE_* below
+	 */
+	unsigned long		queue_flags;
+
+	/*
+	 * protects queue structures from reentrancy
+	 */
+	spinlock_t		*queue_lock;
+
+	/*
+	 * queue kobject
+	 */
+	struct kobject kobj;
+
+	/*
+	 * queue settings
+	 */
+	unsigned long		nr_requests;	/* Max # of requests */
+	unsigned int		nr_congestion_on;
+	unsigned int		nr_congestion_off;
+	unsigned int		nr_batching;
+
+	unsigned short		max_sectors;
+	unsigned short		max_hw_sectors;
+	unsigned short		max_phys_segments;
+	unsigned short		max_hw_segments;
+	unsigned short		hardsect_size;
+	unsigned int		max_segment_size;
+
+	unsigned long		seg_boundary_mask;
+	unsigned int		dma_alignment;
+
+	struct blk_queue_tag	*queue_tags;
+
+	atomic_t		refcnt;
+
+	unsigned int		in_flight;
+
+	/*
+	 * sg stuff
+	 */
+	unsigned int		sg_timeout;
+	unsigned int		sg_reserved_size;
+
+	struct list_head	drain_list;
+
+	/*
+	 * reserved for flush operations
+	 */
+	struct request		*flush_rq;
+	unsigned char		ordered;
+};
+
+enum {
+	QUEUE_ORDERED_NONE,
+	QUEUE_ORDERED_TAG,
+	QUEUE_ORDERED_FLUSH,
+};
+
+#define RQ_INACTIVE		(-1)
+#define RQ_ACTIVE		1
+#define RQ_SCSI_BUSY		0xffff
+#define RQ_SCSI_DONE		0xfffe
+#define RQ_SCSI_DISCONNECTING	0xffe0
+
+#define QUEUE_FLAG_CLUSTER	0	/* cluster several segments into 1 */
+#define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
+#define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
+#define	QUEUE_FLAG_READFULL	3	/* write queue has been filled */
+#define QUEUE_FLAG_WRITEFULL	4	/* read queue has been filled */
+#define QUEUE_FLAG_DEAD		5	/* queue being torn down */
+#define QUEUE_FLAG_REENTER	6	/* Re-entrancy avoidance */
+#define QUEUE_FLAG_PLUGGED	7	/* queue is plugged */
+#define QUEUE_FLAG_DRAIN	8	/* draining queue for sched switch */
+#define QUEUE_FLAG_FLUSH	9	/* doing barrier flush sequence */
+
+#define blk_queue_plugged(q)	test_bit(QUEUE_FLAG_PLUGGED, &(q)->queue_flags)
+#define blk_queue_tagged(q)	test_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)
+#define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+#define blk_queue_flushing(q)	test_bit(QUEUE_FLAG_FLUSH, &(q)->queue_flags)
+
+#define blk_fs_request(rq)	((rq)->flags & REQ_CMD)
+#define blk_pc_request(rq)	((rq)->flags & REQ_BLOCK_PC)
+#define blk_noretry_request(rq)	((rq)->flags & REQ_FAILFAST)
+#define blk_rq_started(rq)	((rq)->flags & REQ_STARTED)
+
+#define blk_account_rq(rq)	(blk_rq_started(rq) && blk_fs_request(rq))
+
+#define blk_pm_suspend_request(rq)	((rq)->flags & REQ_PM_SUSPEND)
+#define blk_pm_resume_request(rq)	((rq)->flags & REQ_PM_RESUME)
+#define blk_pm_request(rq)	\
+	((rq)->flags & (REQ_PM_SUSPEND | REQ_PM_RESUME))
+
+#define blk_barrier_rq(rq)	((rq)->flags & REQ_HARDBARRIER)
+#define blk_barrier_preflush(rq)	((rq)->flags & REQ_BAR_PREFLUSH)
+#define blk_barrier_postflush(rq)	((rq)->flags & REQ_BAR_POSTFLUSH)
+
+#define list_entry_rq(ptr)	list_entry((ptr), struct request, queuelist)
+
+#define rq_data_dir(rq)		((rq)->flags & 1)
+
+static inline int blk_queue_full(struct request_queue *q, int rw)
+{
+	if (rw == READ)
+		return test_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
+	return test_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+}
+
+static inline void blk_set_queue_full(struct request_queue *q, int rw)
+{
+	if (rw == READ)
+		set_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
+	else
+		set_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+}
+
+static inline void blk_clear_queue_full(struct request_queue *q, int rw)
+{
+	if (rw == READ)
+		clear_bit(QUEUE_FLAG_READFULL, &q->queue_flags);
+	else
+		clear_bit(QUEUE_FLAG_WRITEFULL, &q->queue_flags);
+}
+
+
+/*
+ * mergeable request must not have _NOMERGE or _BARRIER bit set, nor may
+ * it already be started by driver.
+ */
+#define RQ_NOMERGE_FLAGS	\
+	(REQ_NOMERGE | REQ_STARTED | REQ_HARDBARRIER | REQ_SOFTBARRIER)
+#define rq_mergeable(rq)	\
+	(!((rq)->flags & RQ_NOMERGE_FLAGS) && blk_fs_request((rq)))
+
+/*
+ * noop, requests are automagically marked as active/inactive by I/O
+ * scheduler -- see elv_next_request
+ */
+#define blk_queue_headactive(q, head_active)
+
+/*
+ * q->prep_rq_fn return values
+ */
+#define BLKPREP_OK		0	/* serve it */
+#define BLKPREP_KILL		1	/* fatal error, kill */
+#define BLKPREP_DEFER		2	/* leave on queue */
+
+extern unsigned long blk_max_low_pfn, blk_max_pfn;
+
+/*
+ * standard bounce addresses:
+ *
+ * BLK_BOUNCE_HIGH	: bounce all highmem pages
+ * BLK_BOUNCE_ANY	: don't bounce anything
+ * BLK_BOUNCE_ISA	: bounce pages above ISA DMA boundary
+ */
+#define BLK_BOUNCE_HIGH		((u64)blk_max_low_pfn << PAGE_SHIFT)
+#define BLK_BOUNCE_ANY		((u64)blk_max_pfn << PAGE_SHIFT)
+#define BLK_BOUNCE_ISA		(ISA_DMA_THRESHOLD)
+
+#ifdef CONFIG_MMU
+extern int init_emergency_isa_pool(void);
+extern void blk_queue_bounce(request_queue_t *q, struct bio **bio);
+#else
+static inline int init_emergency_isa_pool(void)
+{
+	return 0;
+}
+static inline void blk_queue_bounce(request_queue_t *q, struct bio **bio)
+{
+}
+#endif /* CONFIG_MMU */
+
+#define rq_for_each_bio(_bio, rq)	\
+	if ((rq->bio))			\
+		for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)
+
+struct sec_size {
+	unsigned block_size;
+	unsigned block_size_bits;
+};
+
+extern int blk_register_queue(struct gendisk *disk);
+extern void blk_unregister_queue(struct gendisk *disk);
+extern void register_disk(struct gendisk *dev);
+extern void generic_make_request(struct bio *bio);
+extern void blk_put_request(struct request *);
+extern void blk_end_sync_rq(struct request *rq);
+extern void blk_attempt_remerge(request_queue_t *, struct request *);
+extern void __blk_attempt_remerge(request_queue_t *, struct request *);
+extern struct request *blk_get_request(request_queue_t *, int, int);
+extern void blk_insert_request(request_queue_t *, struct request *, int, void *, int);
+extern void blk_requeue_request(request_queue_t *, struct request *);
+extern void blk_plug_device(request_queue_t *);
+extern int blk_remove_plug(request_queue_t *);
+extern void blk_recount_segments(request_queue_t *, struct bio *);
+extern int blk_phys_contig_segment(request_queue_t *q, struct bio *, struct bio *);
+extern int blk_hw_contig_segment(request_queue_t *q, struct bio *, struct bio *);
+extern int scsi_cmd_ioctl(struct file *, struct gendisk *, unsigned int, void __user *);
+extern void blk_start_queue(request_queue_t *q);
+extern void blk_stop_queue(request_queue_t *q);
+extern void blk_sync_queue(struct request_queue *q);
+extern void __blk_stop_queue(request_queue_t *q);
+extern void blk_run_queue(request_queue_t *);
+extern void blk_queue_activity_fn(request_queue_t *, activity_fn *, void *);
+extern struct request *blk_rq_map_user(request_queue_t *, int, void __user *, unsigned int);
+extern int blk_rq_unmap_user(struct request *, struct bio *, unsigned int);
+extern int blk_execute_rq(request_queue_t *, struct gendisk *, struct request *);
+
+static inline request_queue_t *bdev_get_queue(struct block_device *bdev)
+{
+	return bdev->bd_disk->queue;
+}
+
+static inline void blk_run_backing_dev(struct backing_dev_info *bdi,
+				       struct page *page)
+{
+	if (bdi && bdi->unplug_io_fn)
+		bdi->unplug_io_fn(bdi, page);
+}
+
+static inline void blk_run_address_space(struct address_space *mapping)
+{
+	if (mapping)
+		blk_run_backing_dev(mapping->backing_dev_info, NULL);
+}
+
+/*
+ * end_request() and friends. Must be called with the request queue spinlock
+ * acquired. All functions called within end_request() _must_be_ atomic.
+ *
+ * Several drivers define their own end_request and call
+ * end_that_request_first() and end_that_request_last()
+ * for parts of the original function. This prevents
+ * code duplication in drivers.
+ */
+extern int end_that_request_first(struct request *, int, int);
+extern int end_that_request_chunk(struct request *, int, int);
+extern void end_that_request_last(struct request *);
+extern void end_request(struct request *req, int uptodate);
+
+/*
+ * end_that_request_first/chunk() takes an uptodate argument. we account
+ * any value <= as an io error. 0 means -EIO for compatability reasons,
+ * any other < 0 value is the direct error type. An uptodate value of
+ * 1 indicates successful io completion
+ */
+#define end_io_error(uptodate)	(unlikely((uptodate) <= 0))
+
+static inline void blkdev_dequeue_request(struct request *req)
+{
+	BUG_ON(list_empty(&req->queuelist));
+
+	list_del_init(&req->queuelist);
+
+	if (req->rl)
+		elv_remove_request(req->q, req);
+}
+
+/*
+ * Access functions for manipulating queue properties
+ */
+extern request_queue_t *blk_init_queue(request_fn_proc *, spinlock_t *);
+extern void blk_cleanup_queue(request_queue_t *);
+extern void blk_queue_make_request(request_queue_t *, make_request_fn *);
+extern void blk_queue_bounce_limit(request_queue_t *, u64);
+extern void blk_queue_max_sectors(request_queue_t *, unsigned short);
+extern void blk_queue_max_phys_segments(request_queue_t *, unsigned short);
+extern void blk_queue_max_hw_segments(request_queue_t *, unsigned short);
+extern void blk_queue_max_segment_size(request_queue_t *, unsigned int);
+extern void blk_queue_hardsect_size(request_queue_t *, unsigned short);
+extern void blk_queue_stack_limits(request_queue_t *t, request_queue_t *b);
+extern void blk_queue_segment_boundary(request_queue_t *, unsigned long);
+extern void blk_queue_prep_rq(request_queue_t *, prep_rq_fn *pfn);
+extern void blk_queue_merge_bvec(request_queue_t *, merge_bvec_fn *);
+extern void blk_queue_dma_alignment(request_queue_t *, int);
+extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
+extern void blk_queue_ordered(request_queue_t *, int);
+extern void blk_queue_issue_flush_fn(request_queue_t *, issue_flush_fn *);
+extern int blkdev_scsi_issue_flush_fn(request_queue_t *, struct gendisk *, sector_t *);
+extern struct request *blk_start_pre_flush(request_queue_t *,struct request *);
+extern int blk_complete_barrier_rq(request_queue_t *, struct request *, int);
+extern int blk_complete_barrier_rq_locked(request_queue_t *, struct request *, int);
+
+extern int blk_rq_map_sg(request_queue_t *, struct request *, struct scatterlist *);
+extern void blk_dump_rq_flags(struct request *, char *);
+extern void generic_unplug_device(request_queue_t *);
+extern void __generic_unplug_device(request_queue_t *);
+extern long nr_blockdev_pages(void);
+extern void blk_wait_queue_drained(request_queue_t *, int);
+extern void blk_finish_queue_drain(request_queue_t *);
+
+int blk_get_queue(request_queue_t *);
+request_queue_t *blk_alloc_queue(int);
+#define blk_put_queue(q) blk_cleanup_queue((q))
+
+/*
+ * tag stuff
+ */
+#define blk_queue_tag_depth(q)		((q)->queue_tags->busy)
+#define blk_queue_tag_queue(q)		((q)->queue_tags->busy < (q)->queue_tags->max_depth)
+#define blk_rq_tagged(rq)		((rq)->flags & REQ_QUEUED)
+extern int blk_queue_start_tag(request_queue_t *, struct request *);
+extern struct request *blk_queue_find_tag(request_queue_t *, int);
+extern void blk_queue_end_tag(request_queue_t *, struct request *);
+extern int blk_queue_init_tags(request_queue_t *, int, struct blk_queue_tag *);
+extern void blk_queue_free_tags(request_queue_t *);
+extern int blk_queue_resize_tags(request_queue_t *, int);
+extern void blk_queue_invalidate_tags(request_queue_t *);
+extern long blk_congestion_wait(int rw, long timeout);
+
+extern void blk_rq_bio_prep(request_queue_t *, struct request *, struct bio *);
+extern int blkdev_issue_flush(struct block_device *, sector_t *);
+
+#define MAX_PHYS_SEGMENTS 128
+#define MAX_HW_SEGMENTS 128
+#define MAX_SECTORS 255
+
+#define MAX_SEGMENT_SIZE	65536
+
+#define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
+
+extern void drive_stat_acct(struct request *, int, int);
+
+static inline int queue_hardsect_size(request_queue_t *q)
+{
+	int retval = 512;
+
+	if (q && q->hardsect_size)
+		retval = q->hardsect_size;
+
+	return retval;
+}
+
+static inline int bdev_hardsect_size(struct block_device *bdev)
+{
+	return queue_hardsect_size(bdev_get_queue(bdev));
+}
+
+static inline int queue_dma_alignment(request_queue_t *q)
+{
+	int retval = 511;
+
+	if (q && q->dma_alignment)
+		retval = q->dma_alignment;
+
+	return retval;
+}
+
+static inline int bdev_dma_aligment(struct block_device *bdev)
+{
+	return queue_dma_alignment(bdev_get_queue(bdev));
+}
+
+#define blk_finished_io(nsects)	do { } while (0)
+#define blk_started_io(nsects)	do { } while (0)
+
+/* assumes size > 256 */
+static inline unsigned int blksize_bits(unsigned int size)
+{
+	unsigned int bits = 8;
+	do {
+		bits++;
+		size >>= 1;
+	} while (size > 256);
+	return bits;
+}
+
+extern inline unsigned int block_size(struct block_device *bdev)
+{
+	return bdev->bd_block_size;
+}
+
+typedef struct {struct page *v;} Sector;
+
+unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);
+
+static inline void put_dev_sector(Sector p)
+{
+	page_cache_release(p.v);
+}
+
+struct work_struct;
+int kblockd_schedule_work(struct work_struct *work);
+void kblockd_flush(void);
+
+#ifdef CONFIG_LBD
+# include <asm/div64.h>
+# define sector_div(a, b) do_div(a, b)
+#else
+# define sector_div(n, b)( \
+{ \
+	int _res; \
+	_res = (n) % (b); \
+	(n) /= (b); \
+	_res; \
+} \
+)
+#endif 
+
+#define MODULE_ALIAS_BLOCKDEV(major,minor) \
+	MODULE_ALIAS("block-major-" __stringify(major) "-" __stringify(minor))
+#define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \
+	MODULE_ALIAS("block-major-" __stringify(major) "-*")
+
+
+#endif
