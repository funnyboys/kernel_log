commit f5ad3991493c69d203d42b94d32349b54c58a3f1
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Feb 13 13:54:37 2019 -0800

    srcu: Remove cleanup_srcu_struct_quiesced()
    
    The cleanup_srcu_struct_quiesced() function was added because NVME
    used WQ_MEM_RECLAIM workqueues and SRCU did not, which meant that
    NVME workqueues waiting on SRCU workqueues could result in deadlocks
    during low-memory conditions.  However, SRCU now also has WQ_MEM_RECLAIM
    workqueues, so there is no longer a potential for deadlock.  Furthermore,
    it turns out to be extremely hard to use cleanup_srcu_struct_quiesced()
    correctly due to the fact that SRCU callback invocation accesses the
    srcu_struct structure's per-CPU data area just after callbacks are
    invoked.  Therefore, the usual practice of using srcu_barrier() to wait
    for callbacks to be invoked before invoking cleanup_srcu_struct_quiesced()
    fails because SRCU's callback-invocation workqueue handler might be
    delayed, which can result in cleanup_srcu_struct_quiesced() being invoked
    (and thus freeing the per-CPU data) before the SRCU's callback-invocation
    workqueue handler is finished using that per-CPU data.  Nor is this a
    theoretical problem: KASAN emitted use-after-free warnings because of
    this problem on actual runs.
    
    In short, NVME can now safely invoke cleanup_srcu_struct(), which
    avoids the use-after-free scenario.  And cleanup_srcu_struct_quiesced()
    is quite difficult to use safely.  This commit therefore removes
    cleanup_srcu_struct_quiesced(), switching its sole user back to
    cleanup_srcu_struct().  This effectively reverts the following pair
    of commits:
    
    f7194ac32ca2 ("srcu: Add cleanup_srcu_struct_quiesced()")
    4317228ad9b8 ("nvme: Avoid flush dependency in delete controller flow")
    
    Reported-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Tested-by: Bart Van Assche <bvanassche@acm.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index c495b2d51569..e432cc92c73d 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -56,45 +56,11 @@ struct srcu_struct { };
 
 void call_srcu(struct srcu_struct *ssp, struct rcu_head *head,
 		void (*func)(struct rcu_head *head));
-void _cleanup_srcu_struct(struct srcu_struct *ssp, bool quiesced);
+void cleanup_srcu_struct(struct srcu_struct *ssp);
 int __srcu_read_lock(struct srcu_struct *ssp) __acquires(ssp);
 void __srcu_read_unlock(struct srcu_struct *ssp, int idx) __releases(ssp);
 void synchronize_srcu(struct srcu_struct *ssp);
 
-/**
- * cleanup_srcu_struct - deconstruct a sleep-RCU structure
- * @ssp: structure to clean up.
- *
- * Must invoke this after you are finished using a given srcu_struct that
- * was initialized via init_srcu_struct(), else you leak memory.
- */
-static inline void cleanup_srcu_struct(struct srcu_struct *ssp)
-{
-	_cleanup_srcu_struct(ssp, false);
-}
-
-/**
- * cleanup_srcu_struct_quiesced - deconstruct a quiesced sleep-RCU structure
- * @ssp: structure to clean up.
- *
- * Must invoke this after you are finished using a given srcu_struct that
- * was initialized via init_srcu_struct(), else you leak memory.  Also,
- * all grace-period processing must have completed.
- *
- * "Completed" means that the last synchronize_srcu() and
- * synchronize_srcu_expedited() calls must have returned before the call
- * to cleanup_srcu_struct_quiesced().  It also means that the callback
- * from the last call_srcu() must have been invoked before the call to
- * cleanup_srcu_struct_quiesced(), but you can use srcu_barrier() to help
- * with this last.  Violating these rules will get you a WARN_ON() splat
- * (with high probability, anyway), and will also cause the srcu_struct
- * to be leaked.
- */
-static inline void cleanup_srcu_struct_quiesced(struct srcu_struct *ssp)
-{
-	_cleanup_srcu_struct(ssp, true);
-}
-
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 /**

commit e7ffb4eb9a6d89678e7f62461737899f88dab64e
Merge: 87d1779dc48f 423a86a610ca 6706dae90d85 5a4eb3cb2012 e81baf4cb19a 6684880a8b16
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sat Feb 9 08:47:52 2019 -0800

    Merge branches 'doc.2019.01.26a', 'fixes.2019.01.26a', 'sil.2019.01.26a', 'spdx.2019.02.09a', 'srcu.2019.01.26a' and 'torture.2019.01.26a' into HEAD
    
    doc.2019.01.26a:  Documentation updates.
    fixes.2019.01.26a:  Miscellaneous fixes.
    sil.2019.01.26a:  Removal of a few more spin_is_locked() instances.
    spdx.2019.02.09a:  Add SPDX identifiers to RCU files
    srcu.2019.01.26a:  SRCU updates.
    torture.2019.01.26a: Torture-test updates.

commit 8c366db05b1f27fac01a7dbf9e4904d499bd5d55
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Jan 17 10:39:22 2019 -0800

    linux/srcu: Convert to SPDX license identifier
    
    Replace the license boiler plate with a SPDX license identifier.
    While in the area, update an email address.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Update ,h SPDX format per Joe Perches. ]
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index c614375cd264..0d5fed02df16 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -1,24 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
 /*
  * Sleepable Read-Copy Update mechanism for mutual exclusion
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
  * Copyright (C) IBM Corporation, 2006
  * Copyright (C) Fujitsu, 2012
  *
- * Author: Paul McKenney <paulmck@us.ibm.com>
+ * Author: Paul McKenney <paulmck@linux.ibm.com>
  *	   Lai Jiangshan <laijs@cn.fujitsu.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -

commit c8ca1aa774b20f182733d1661f3b6aa3105338e7
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 10:06:46 2018 -0800

    srcu: Check for invalid idx argument in srcu_read_unlock()
    
    The current SRCU implementation has an idx argument of zero or one,
    and never anything else.  This commit therefore adds a WARN_ON_ONCE()
    to complain if this restriction is violated.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index c614375cd264..33cf83b9bda8 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -223,6 +223,7 @@ srcu_read_lock_notrace(struct srcu_struct *ssp) __acquires(ssp)
 static inline void srcu_read_unlock(struct srcu_struct *ssp, int idx)
 	__releases(ssp)
 {
+	WARN_ON_ONCE(idx & ~0x1);
 	rcu_lock_release(&(ssp)->dep_map);
 	__srcu_read_unlock(ssp, idx);
 }

commit aacb5d91ab1bfbb0e8123da59a2e333d52ba7f60
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sun Oct 28 10:32:51 2018 -0700

    srcu: Use "ssp" instead of "sp" for srcu_struct pointer
    
    In RCU, the distinction between "rsp", "rnp", and "rdp" has served well
    for a great many years, but in SRCU, "sp" vs. "sdp" has proven confusing.
    This commit therefore renames SRCU's "sp" pointers to "ssp", so that there
    is "ssp" for srcu_struct pointer, "snp" for srcu_node pointer, and "sdp"
    for srcu_data pointer.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index ebd5f1511690..c614375cd264 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -38,20 +38,20 @@ struct srcu_struct;
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
-int __init_srcu_struct(struct srcu_struct *sp, const char *name,
+int __init_srcu_struct(struct srcu_struct *ssp, const char *name,
 		       struct lock_class_key *key);
 
-#define init_srcu_struct(sp) \
+#define init_srcu_struct(ssp) \
 ({ \
 	static struct lock_class_key __srcu_key; \
 	\
-	__init_srcu_struct((sp), #sp, &__srcu_key); \
+	__init_srcu_struct((ssp), #ssp, &__srcu_key); \
 })
 
 #define __SRCU_DEP_MAP_INIT(srcu_name)	.dep_map = { .name = #srcu_name },
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
-int init_srcu_struct(struct srcu_struct *sp);
+int init_srcu_struct(struct srcu_struct *ssp);
 
 #define __SRCU_DEP_MAP_INIT(srcu_name)
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
@@ -67,28 +67,28 @@ int init_srcu_struct(struct srcu_struct *sp);
 struct srcu_struct { };
 #endif
 
-void call_srcu(struct srcu_struct *sp, struct rcu_head *head,
+void call_srcu(struct srcu_struct *ssp, struct rcu_head *head,
 		void (*func)(struct rcu_head *head));
-void _cleanup_srcu_struct(struct srcu_struct *sp, bool quiesced);
-int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
-void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
-void synchronize_srcu(struct srcu_struct *sp);
+void _cleanup_srcu_struct(struct srcu_struct *ssp, bool quiesced);
+int __srcu_read_lock(struct srcu_struct *ssp) __acquires(ssp);
+void __srcu_read_unlock(struct srcu_struct *ssp, int idx) __releases(ssp);
+void synchronize_srcu(struct srcu_struct *ssp);
 
 /**
  * cleanup_srcu_struct - deconstruct a sleep-RCU structure
- * @sp: structure to clean up.
+ * @ssp: structure to clean up.
  *
  * Must invoke this after you are finished using a given srcu_struct that
  * was initialized via init_srcu_struct(), else you leak memory.
  */
-static inline void cleanup_srcu_struct(struct srcu_struct *sp)
+static inline void cleanup_srcu_struct(struct srcu_struct *ssp)
 {
-	_cleanup_srcu_struct(sp, false);
+	_cleanup_srcu_struct(ssp, false);
 }
 
 /**
  * cleanup_srcu_struct_quiesced - deconstruct a quiesced sleep-RCU structure
- * @sp: structure to clean up.
+ * @ssp: structure to clean up.
  *
  * Must invoke this after you are finished using a given srcu_struct that
  * was initialized via init_srcu_struct(), else you leak memory.  Also,
@@ -103,16 +103,16 @@ static inline void cleanup_srcu_struct(struct srcu_struct *sp)
  * (with high probability, anyway), and will also cause the srcu_struct
  * to be leaked.
  */
-static inline void cleanup_srcu_struct_quiesced(struct srcu_struct *sp)
+static inline void cleanup_srcu_struct_quiesced(struct srcu_struct *ssp)
 {
-	_cleanup_srcu_struct(sp, true);
+	_cleanup_srcu_struct(ssp, true);
 }
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 /**
  * srcu_read_lock_held - might we be in SRCU read-side critical section?
- * @sp: The srcu_struct structure to check
+ * @ssp: The srcu_struct structure to check
  *
  * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an SRCU
  * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,
@@ -126,16 +126,16 @@ static inline void cleanup_srcu_struct_quiesced(struct srcu_struct *sp)
  * relies on normal RCU, it can be called from the CPU which
  * is in the idle loop from an RCU point of view or offline.
  */
-static inline int srcu_read_lock_held(const struct srcu_struct *sp)
+static inline int srcu_read_lock_held(const struct srcu_struct *ssp)
 {
 	if (!debug_lockdep_rcu_enabled())
 		return 1;
-	return lock_is_held(&sp->dep_map);
+	return lock_is_held(&ssp->dep_map);
 }
 
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
-static inline int srcu_read_lock_held(const struct srcu_struct *sp)
+static inline int srcu_read_lock_held(const struct srcu_struct *ssp)
 {
 	return 1;
 }
@@ -145,7 +145,7 @@ static inline int srcu_read_lock_held(const struct srcu_struct *sp)
 /**
  * srcu_dereference_check - fetch SRCU-protected pointer for later dereferencing
  * @p: the pointer to fetch and protect for later dereferencing
- * @sp: pointer to the srcu_struct, which is used to check that we
+ * @ssp: pointer to the srcu_struct, which is used to check that we
  *	really are in an SRCU read-side critical section.
  * @c: condition to check for update-side use
  *
@@ -154,32 +154,32 @@ static inline int srcu_read_lock_held(const struct srcu_struct *sp)
  * to 1.  The @c argument will normally be a logical expression containing
  * lockdep_is_held() calls.
  */
-#define srcu_dereference_check(p, sp, c) \
-	__rcu_dereference_check((p), (c) || srcu_read_lock_held(sp), __rcu)
+#define srcu_dereference_check(p, ssp, c) \
+	__rcu_dereference_check((p), (c) || srcu_read_lock_held(ssp), __rcu)
 
 /**
  * srcu_dereference - fetch SRCU-protected pointer for later dereferencing
  * @p: the pointer to fetch and protect for later dereferencing
- * @sp: pointer to the srcu_struct, which is used to check that we
+ * @ssp: pointer to the srcu_struct, which is used to check that we
  *	really are in an SRCU read-side critical section.
  *
  * Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
  * is enabled, invoking this outside of an RCU read-side critical
  * section will result in an RCU-lockdep splat.
  */
-#define srcu_dereference(p, sp) srcu_dereference_check((p), (sp), 0)
+#define srcu_dereference(p, ssp) srcu_dereference_check((p), (ssp), 0)
 
 /**
  * srcu_dereference_notrace - no tracing and no lockdep calls from here
  * @p: the pointer to fetch and protect for later dereferencing
- * @sp: pointer to the srcu_struct, which is used to check that we
+ * @ssp: pointer to the srcu_struct, which is used to check that we
  *	really are in an SRCU read-side critical section.
  */
-#define srcu_dereference_notrace(p, sp) srcu_dereference_check((p), (sp), 1)
+#define srcu_dereference_notrace(p, ssp) srcu_dereference_check((p), (ssp), 1)
 
 /**
  * srcu_read_lock - register a new reader for an SRCU-protected structure.
- * @sp: srcu_struct in which to register the new reader.
+ * @ssp: srcu_struct in which to register the new reader.
  *
  * Enter an SRCU read-side critical section.  Note that SRCU read-side
  * critical sections may be nested.  However, it is illegal to
@@ -194,44 +194,44 @@ static inline int srcu_read_lock_held(const struct srcu_struct *sp)
  * srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
  * was invoked in process context.
  */
-static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
+static inline int srcu_read_lock(struct srcu_struct *ssp) __acquires(ssp)
 {
 	int retval;
 
-	retval = __srcu_read_lock(sp);
-	rcu_lock_acquire(&(sp)->dep_map);
+	retval = __srcu_read_lock(ssp);
+	rcu_lock_acquire(&(ssp)->dep_map);
 	return retval;
 }
 
 /* Used by tracing, cannot be traced and cannot invoke lockdep. */
 static inline notrace int
-srcu_read_lock_notrace(struct srcu_struct *sp) __acquires(sp)
+srcu_read_lock_notrace(struct srcu_struct *ssp) __acquires(ssp)
 {
 	int retval;
 
-	retval = __srcu_read_lock(sp);
+	retval = __srcu_read_lock(ssp);
 	return retval;
 }
 
 /**
  * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
- * @sp: srcu_struct in which to unregister the old reader.
+ * @ssp: srcu_struct in which to unregister the old reader.
  * @idx: return value from corresponding srcu_read_lock().
  *
  * Exit an SRCU read-side critical section.
  */
-static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
-	__releases(sp)
+static inline void srcu_read_unlock(struct srcu_struct *ssp, int idx)
+	__releases(ssp)
 {
-	rcu_lock_release(&(sp)->dep_map);
-	__srcu_read_unlock(sp, idx);
+	rcu_lock_release(&(ssp)->dep_map);
+	__srcu_read_unlock(ssp, idx);
 }
 
 /* Used by tracing, cannot be traced and cannot call lockdep. */
 static inline notrace void
-srcu_read_unlock_notrace(struct srcu_struct *sp, int idx) __releases(sp)
+srcu_read_unlock_notrace(struct srcu_struct *ssp, int idx) __releases(ssp)
 {
-	__srcu_read_unlock(sp, idx);
+	__srcu_read_unlock(ssp, idx);
 }
 
 /**

commit f3e763c3e544b73ae5c4a3842cedb9ff6ca37715
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Mon Sep 3 12:45:45 2018 -0700

    srcu: Fix kernel-doc missing notation
    
    Fix kernel-doc warnings for missing parameter descriptions:
    
    ../include/linux/srcu.h:175: warning: Function parameter or member 'p' not described in 'srcu_dereference_notrace'
    ../include/linux/srcu.h:175: warning: Function parameter or member 'sp' not described in 'srcu_dereference_notrace'
    
    Fixes: 0b764a6e4e19d ("srcu: Add notrace variant of srcu_dereference")
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 67135d4a8a30..ebd5f1511690 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -171,6 +171,9 @@ static inline int srcu_read_lock_held(const struct srcu_struct *sp)
 
 /**
  * srcu_dereference_notrace - no tracing and no lockdep calls from here
+ * @p: the pointer to fetch and protect for later dereferencing
+ * @sp: pointer to the srcu_struct, which is used to check that we
+ *	really are in an SRCU read-side critical section.
  */
 #define srcu_dereference_notrace(p, sp) srcu_dereference_check((p), (sp), 1)
 

commit 0b764a6e4e19dc254caca636002eaa47b1d0b0af
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Thu Jun 28 11:21:44 2018 -0700

    srcu: Add notrace variant of srcu_dereference
    
    In the last patch in this series, we are making lockdep register hooks
    onto the irq_{disable,enable} tracepoints. These tracepoints use the
    _rcuidle tracepoint variant. In this series we switch the _rcuidle
    tracepoint callers to use SRCU instead of sched-RCU. Inorder to
    dereference the pointer to the probe functions, we could call
    srcu_dereference, however this API will call back into lockdep to check
    if the lock is held *before* the lockdep probe hooks have a chance to
    run and annotate the IRQ enabled/disabled state.
    
    For this reason we need a notrace variant of srcu_dereference since
    otherwise we get lockdep splats. This patch adds the needed
    srcu_dereference_notrace variant.
    
    Link: http://lkml.kernel.org/r/20180628182149.226164-3-joel@joelfernandes.org
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 3e72a291c401..67135d4a8a30 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -169,6 +169,11 @@ static inline int srcu_read_lock_held(const struct srcu_struct *sp)
  */
 #define srcu_dereference(p, sp) srcu_dereference_check((p), (sp), 0)
 
+/**
+ * srcu_dereference_notrace - no tracing and no lockdep calls from here
+ */
+#define srcu_dereference_notrace(p, sp) srcu_dereference_check((p), (sp), 1)
+
 /**
  * srcu_read_lock - register a new reader for an SRCU-protected structure.
  * @sp: srcu_struct in which to register the new reader.

commit 1f45a4db3657c5ecafb7d3331536c987e6d18311
Author: Paul McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 11:21:43 2018 -0700

    srcu: Add notrace variants of srcu_read_{lock,unlock}
    
    This is needed for a future tracepoint patch that uses srcu, and to make
    sure it doesn't call into lockdep.
    
    tracepoint code already calls notrace variants for rcu_read_lock_sched
    so this patch does the same for srcu which will be used in a later
    patch. Keeps it consistent with rcu-sched.
    
    [Joel: Added commit message]
    Link: http://lkml.kernel.org/r/20180628182149.226164-2-joel@joelfernandes.org
    
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 91494d7e8e41..3e72a291c401 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -195,6 +195,16 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 	return retval;
 }
 
+/* Used by tracing, cannot be traced and cannot invoke lockdep. */
+static inline notrace int
+srcu_read_lock_notrace(struct srcu_struct *sp) __acquires(sp)
+{
+	int retval;
+
+	retval = __srcu_read_lock(sp);
+	return retval;
+}
+
 /**
  * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
  * @sp: srcu_struct in which to unregister the old reader.
@@ -209,6 +219,13 @@ static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__srcu_read_unlock(sp, idx);
 }
 
+/* Used by tracing, cannot be traced and cannot call lockdep. */
+static inline notrace void
+srcu_read_unlock_notrace(struct srcu_struct *sp, int idx) __releases(sp)
+{
+	__srcu_read_unlock(sp, idx);
+}
+
 /**
  * smp_mb__after_srcu_read_unlock - ensure full ordering after srcu_read_unlock
  *

commit f7194ac32ca241d28765a98e42a7fe13debc85a7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 5 17:19:17 2018 -0700

    srcu: Add cleanup_srcu_struct_quiesced()
    
    The current cleanup_srcu_struct() flushes work, which prevents it
    from being invoked from some workqueue contexts, as well as from
    atomic (non-blocking) contexts.  This patch therefore introduced a
    cleanup_srcu_struct_quiesced(), which can be invoked only after all
    activity on the specified srcu_struct has completed.  This restriction
    allows cleanup_srcu_struct_quiesced() to be invoked from workqueue
    contexts as well as from atomic contexts.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nitzan Carmi <nitzanc@mellanox.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 33c1c698df09..91494d7e8e41 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -69,11 +69,45 @@ struct srcu_struct { };
 
 void call_srcu(struct srcu_struct *sp, struct rcu_head *head,
 		void (*func)(struct rcu_head *head));
-void cleanup_srcu_struct(struct srcu_struct *sp);
+void _cleanup_srcu_struct(struct srcu_struct *sp, bool quiesced);
 int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
 
+/**
+ * cleanup_srcu_struct - deconstruct a sleep-RCU structure
+ * @sp: structure to clean up.
+ *
+ * Must invoke this after you are finished using a given srcu_struct that
+ * was initialized via init_srcu_struct(), else you leak memory.
+ */
+static inline void cleanup_srcu_struct(struct srcu_struct *sp)
+{
+	_cleanup_srcu_struct(sp, false);
+}
+
+/**
+ * cleanup_srcu_struct_quiesced - deconstruct a quiesced sleep-RCU structure
+ * @sp: structure to clean up.
+ *
+ * Must invoke this after you are finished using a given srcu_struct that
+ * was initialized via init_srcu_struct(), else you leak memory.  Also,
+ * all grace-period processing must have completed.
+ *
+ * "Completed" means that the last synchronize_srcu() and
+ * synchronize_srcu_expedited() calls must have returned before the call
+ * to cleanup_srcu_struct_quiesced().  It also means that the callback
+ * from the last call_srcu() must have been invoked before the call to
+ * cleanup_srcu_struct_quiesced(), but you can use srcu_barrier() to help
+ * with this last.  Violating these rules will get you a WARN_ON() splat
+ * (with high probability, anyway), and will also cause the srcu_struct
+ * to be leaked.
+ */
+static inline void cleanup_srcu_struct_quiesced(struct srcu_struct *sp)
+{
+	_cleanup_srcu_struct(sp, true);
+}
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 /**

commit 05b93801a23c21a6f355f4c492c51715d6ccc96d
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Wed Jan 17 07:14:14 2018 -0800

    lockdep: Convert some users to const
    
    These users of lockdep_is_held() either wanted lockdep_is_held to
    take a const pointer, or would benefit from providing a const pointer.
    
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: https://lkml.kernel.org/r/20180117151414.23686-4-willy@infradead.org

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 62be8966e837..33c1c698df09 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -92,7 +92,7 @@ void synchronize_srcu(struct srcu_struct *sp);
  * relies on normal RCU, it can be called from the CPU which
  * is in the idle loop from an RCU point of view or offline.
  */
-static inline int srcu_read_lock_held(struct srcu_struct *sp)
+static inline int srcu_read_lock_held(const struct srcu_struct *sp)
 {
 	if (!debug_lockdep_rcu_enabled())
 		return 1;
@@ -101,7 +101,7 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
 
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
-static inline int srcu_read_lock_held(struct srcu_struct *sp)
+static inline int srcu_read_lock_held(const struct srcu_struct *sp)
 {
 	return 1;
 }

commit 27fdb35fe99011d86bcc54f62fe84712c53f4d05
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Oct 19 14:26:21 2017 -0700

    doc: Fix various RCU docbook comment-header problems
    
    Because many of RCU's files have not been included into docbook, a
    number of errors have accumulated.  This commit fixes them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 39af9bc0f653..62be8966e837 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -78,6 +78,7 @@ void synchronize_srcu(struct srcu_struct *sp);
 
 /**
  * srcu_read_lock_held - might we be in SRCU read-side critical section?
+ * @sp: The srcu_struct structure to check
  *
  * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an SRCU
  * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,

commit bd8cc5a062f41e334596edbe823e2fa0adddd1b7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 15 14:57:01 2017 -0700

    srcu: Remove Classic SRCU
    
    Classic SRCU was only ever intended to be a fallback in case of issues
    with Tree/Tiny SRCU, and the latter two are doing quite well in testing.
    This commit therefore removes Classic SRCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 5f509018e6b5..39af9bc0f653 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -60,8 +60,6 @@ int init_srcu_struct(struct srcu_struct *sp);
 #include <linux/srcutiny.h>
 #elif defined(CONFIG_TREE_SRCU)
 #include <linux/srcutree.h>
-#elif defined(CONFIG_CLASSIC_SRCU)
-#include <linux/srcuclassic.h>
 #elif defined(CONFIG_SRCU)
 #error "Unknown SRCU implementation specified to kernel configuration"
 #else

commit 5a0465e17a18c467b712a816985b7b8dd8d10c16
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 4 11:31:04 2017 -0700

    srcu: Shrink srcu.h by moving docbook and private function
    
    The call_srcu() docbook entry is currently in include/linux/srcu.h,
    which causes needless processing for each include point.  This commit
    therefore moves this entry to kernel/rcu/srcutree.c, which the compiler
    reads only once.  In addition, the srcu_batches_completed() function is
    used only within RCU and its torture-test suites.  This commit therefore
    also moves this function's declaration from include/linux/srcutiny.h,
    include/linux/srcutree.h, and include/linux/srcuclassic.h to
    kernel/rcu/rcu.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index ea356d800675..5f509018e6b5 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -65,32 +65,12 @@ int init_srcu_struct(struct srcu_struct *sp);
 #elif defined(CONFIG_SRCU)
 #error "Unknown SRCU implementation specified to kernel configuration"
 #else
-
 /* Dummy definition for things like notifiers.  Actual use gets link error. */
 struct srcu_struct { };
-
 #endif
 
-/**
- * call_srcu() - Queue a callback for invocation after an SRCU grace period
- * @sp: srcu_struct in queue the callback
- * @head: structure to be used for queueing the SRCU callback.
- * @func: function to be invoked after the SRCU grace period
- *
- * The callback function will be invoked some time after a full SRCU
- * grace period elapses, in other words after all pre-existing SRCU
- * read-side critical sections have completed.  However, the callback
- * function might well execute concurrently with other SRCU read-side
- * critical sections that started after call_srcu() was invoked.  SRCU
- * read-side critical sections are delimited by srcu_read_lock() and
- * srcu_read_unlock(), and may be nested.
- *
- * The callback will be invoked from process context, but must nevertheless
- * be fast and must not block.
- */
 void call_srcu(struct srcu_struct *sp, struct rcu_head *head,
 		void (*func)(struct rcu_head *head));
-
 void cleanup_srcu_struct(struct srcu_struct *sp);
 int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);

commit 07f6e64bf2ab98cad0d9c595659209858e7bff83
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Apr 28 13:53:04 2017 -0700

    srcu: Make SRCU be once again optional
    
    Commit d160a727c40e ("srcu: Make SRCU be built by default") in response
    to build errors, which were caused by code that included srcu.h
    despite !SRCU.  However, srcutiny.o is almost 2K of code, which is not
    insignificant for those attempting to run the Linux kernel on IoT devices.
    This commit therefore makes SRCU be once again optional, and adjusts
    srcu.h to allow error-free inclusion in !SRCU kernel builds.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4c1d5f7e62c4..ea356d800675 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -62,8 +62,13 @@ int init_srcu_struct(struct srcu_struct *sp);
 #include <linux/srcutree.h>
 #elif defined(CONFIG_CLASSIC_SRCU)
 #include <linux/srcuclassic.h>
-#else
+#elif defined(CONFIG_SRCU)
 #error "Unknown SRCU implementation specified to kernel configuration"
+#else
+
+/* Dummy definition for things like notifiers.  Actual use gets link error. */
+struct srcu_struct { };
+
 #endif
 
 /**

commit 1123a6041654e8f889014659593bad4168e542c2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 31 14:03:11 2017 +0200

    srcu: Allow use of Classic SRCU from both process and interrupt context
    
    Linu Cherian reported a WARN in cleanup_srcu_struct() when shutting
    down a guest running iperf on a VFIO assigned device.  This happens
    because irqfd_wakeup() calls srcu_read_lock(&kvm->irq_srcu) in interrupt
    context, while a worker thread does the same inside kvm_set_irq().  If the
    interrupt happens while the worker thread is executing __srcu_read_lock(),
    updates to the Classic SRCU ->lock_count[] field or the Tree SRCU
    ->srcu_lock_count[] field can be lost.
    
    The docs say you are not supposed to call srcu_read_lock() and
    srcu_read_unlock() from irq context, but KVM interrupt injection happens
    from (host) interrupt context and it would be nice if SRCU supported the
    use case.  KVM is using SRCU here not really for the "sleepable" part,
    but rather due to its IPI-free fast detection of grace periods.  It is
    therefore not desirable to switch back to RCU, which would effectively
    revert commit 719d93cd5f5c ("kvm/irqchip: Speed up KVM_SET_GSI_ROUTING",
    2014-01-16).
    
    However, the docs are overly conservative.  You can have an SRCU instance
    only has users in irq context, and you can mix process and irq context
    as long as process context users disable interrupts.  In addition,
    __srcu_read_unlock() actually uses this_cpu_dec() on both Tree SRCU and
    Classic SRCU.  For those two implementations, only srcu_read_lock()
    is unsafe.
    
    When Classic SRCU's __srcu_read_unlock() was changed to use this_cpu_dec(),
    in commit 5a41344a3d83 ("srcu: Simplify __srcu_read_unlock() via
    this_cpu_dec()", 2012-11-29), __srcu_read_lock() did two increments.
    Therefore it kept __this_cpu_inc(), with preempt_disable/enable in
    the caller.  Tree SRCU however only does one increment, so on most
    architectures it is more efficient for __srcu_read_lock() to use
    this_cpu_inc(), and any performance differences appear to be down in
    the noise.
    
    Cc: stable@vger.kernel.org
    Fixes: 719d93cd5f5c ("kvm/irqchip: Speed up KVM_SET_GSI_ROUTING")
    Reported-by: Linu Cherian <linuc.decode@gmail.com>
    Suggested-by: Linu Cherian <linuc.decode@gmail.com>
    Cc: kvm@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 167ad8831aaf..4c1d5f7e62c4 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -172,9 +172,7 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 {
 	int retval;
 
-	preempt_disable();
 	retval = __srcu_read_lock(sp);
-	preempt_enable();
 	rcu_lock_acquire(&(sp)->dep_map);
 	return retval;
 }

commit dad81a2026841b5e2651aab58a7398c13cc05847
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Mar 25 17:23:44 2017 -0700

    srcu: Introduce CLASSIC_SRCU Kconfig option
    
    The TREE_SRCU rewrite is large and a bit on the non-simple side, so
    this commit helps reduce risk by allowing the old v4.11 SRCU algorithm
    to be selected using a new CLASSIC_SRCU Kconfig option that depends
    on RCU_EXPERT.  The default is to use the new TREE_SRCU and TINY_SRCU
    algorithms, in order to help get these the testing that they need.
    However, if your users do not require the update-side scalability that
    is to be provided by TREE_SRCU, select RCU_EXPERT and then CLASSIC_SRCU
    to revert back to the old classic SRCU algorithm.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 907f09b14eda..167ad8831aaf 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -60,6 +60,8 @@ int init_srcu_struct(struct srcu_struct *sp);
 #include <linux/srcutiny.h>
 #elif defined(CONFIG_TREE_SRCU)
 #include <linux/srcutree.h>
+#elif defined(CONFIG_CLASSIC_SRCU)
+#include <linux/srcuclassic.h>
 #else
 #error "Unknown SRCU implementation specified to kernel configuration"
 #endif

commit d8be81735aa89413b333de488251f0e64e2be591
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Mar 25 09:59:38 2017 -0700

    srcu: Create a tiny SRCU
    
    In response to automated complaints about modifications to SRCU
    increasing its size, this commit creates a tiny SRCU that is
    used in SMP=n && PREEMPT=n builds.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 73a1b6296224..907f09b14eda 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -34,28 +34,7 @@
 #include <linux/workqueue.h>
 #include <linux/rcu_segcblist.h>
 
-struct srcu_array {
-	unsigned long lock_count[2];
-	unsigned long unlock_count[2];
-};
-
-struct srcu_struct {
-	unsigned long completed;
-	unsigned long srcu_gp_seq;
-	atomic_t srcu_exp_cnt;
-	struct srcu_array __percpu *per_cpu_ref;
-	spinlock_t queue_lock; /* protect ->srcu_cblist */
-	struct rcu_segcblist srcu_cblist;
-	struct delayed_work work;
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	struct lockdep_map dep_map;
-#endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
-};
-
-/* Values for -> state variable. */
-#define SRCU_STATE_IDLE		0
-#define SRCU_STATE_SCAN1	1
-#define SRCU_STATE_SCAN2	2
+struct srcu_struct;
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
@@ -77,42 +56,13 @@ int init_srcu_struct(struct srcu_struct *sp);
 #define __SRCU_DEP_MAP_INIT(srcu_name)
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
-void process_srcu(struct work_struct *work);
-
-#define __SRCU_STRUCT_INIT(name)					\
-	{								\
-		.completed = -300,					\
-		.per_cpu_ref = &name##_srcu_array,			\
-		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
-		.srcu_cblist = RCU_SEGCBLIST_INITIALIZER(name.srcu_cblist),\
-		.work = __DELAYED_WORK_INITIALIZER(name.work, process_srcu, 0),\
-		__SRCU_DEP_MAP_INIT(name)				\
-	}
-
-/*
- * Define and initialize a srcu struct at build time.
- * Do -not- call init_srcu_struct() nor cleanup_srcu_struct() on it.
- *
- * Note that although DEFINE_STATIC_SRCU() hides the name from other
- * files, the per-CPU variable rules nevertheless require that the
- * chosen name be globally unique.  These rules also prohibit use of
- * DEFINE_STATIC_SRCU() within a function.  If these rules are too
- * restrictive, declare the srcu_struct manually.  For example, in
- * each file:
- *
- *	static struct srcu_struct my_srcu;
- *
- * Then, before the first use of each my_srcu, manually initialize it:
- *
- *	init_srcu_struct(&my_srcu);
- *
- * See include/linux/percpu-defs.h for the rules on per-CPU variables.
- */
-#define __DEFINE_SRCU(name, is_static)					\
-	static DEFINE_PER_CPU(struct srcu_array, name##_srcu_array);\
-	is_static struct srcu_struct name = __SRCU_STRUCT_INIT(name)
-#define DEFINE_SRCU(name)		__DEFINE_SRCU(name, /* not static */)
-#define DEFINE_STATIC_SRCU(name)	__DEFINE_SRCU(name, static)
+#ifdef CONFIG_TINY_SRCU
+#include <linux/srcutiny.h>
+#elif defined(CONFIG_TREE_SRCU)
+#include <linux/srcutree.h>
+#else
+#error "Unknown SRCU implementation specified to kernel configuration"
+#endif
 
 /**
  * call_srcu() - Queue a callback for invocation after an SRCU grace period
@@ -138,9 +88,6 @@ void cleanup_srcu_struct(struct srcu_struct *sp);
 int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
-void synchronize_srcu_expedited(struct srcu_struct *sp);
-unsigned long srcu_batches_completed(struct srcu_struct *sp);
-void srcu_barrier(struct srcu_struct *sp);
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 

commit f60d231a87c5c9f23f10e69996f396d46f5bf901
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Mar 24 13:46:33 2017 -0700

    srcu: Crude control of expedited grace periods
    
    SRCU's implementation of expedited grace periods has always assumed
    that the SRCU instance is idle when the expedited request arrives.
    This commit improves this a bit by maintaining a count of the number
    of outstanding expedited requests, thus allowing prior non-expedited
    grace periods accommodate these requests by shifting to expedited mode.
    However, any non-expedited wait already in progress will still wait for
    the full duration.
    
    Improved control of expedited grace periods is planned, but one step
    at a time.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index e7dbc01b61a1..73a1b6296224 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -42,6 +42,7 @@ struct srcu_array {
 struct srcu_struct {
 	unsigned long completed;
 	unsigned long srcu_gp_seq;
+	atomic_t srcu_exp_cnt;
 	struct srcu_array __percpu *per_cpu_ref;
 	spinlock_t queue_lock; /* protect ->srcu_cblist */
 	struct rcu_segcblist srcu_cblist;

commit 80a7956fe36c2ee40c6ff12c77926d267802b7c8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 22 15:26:18 2017 -0700

    srcu: Merge ->srcu_state into ->srcu_gp_seq
    
    Updating ->srcu_state and ->srcu_gp_seq will lead to extremely complex
    race conditions given multiple callback queues, so this commit takes
    advantage of the two-bit state now available in rcu_seq counters to
    store the state in the bottom two bits of ->srcu_gp_seq.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index ad154a7bc114..e7dbc01b61a1 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -43,8 +43,7 @@ struct srcu_struct {
 	unsigned long completed;
 	unsigned long srcu_gp_seq;
 	struct srcu_array __percpu *per_cpu_ref;
-	spinlock_t queue_lock; /* protect ->srcu_cblist, ->srcu_state */
-	int srcu_state;
+	spinlock_t queue_lock; /* protect ->srcu_cblist */
 	struct rcu_segcblist srcu_cblist;
 	struct delayed_work work;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -56,7 +55,6 @@ struct srcu_struct {
 #define SRCU_STATE_IDLE		0
 #define SRCU_STATE_SCAN1	1
 #define SRCU_STATE_SCAN2	2
-#define SRCU_STATE_DONE		3
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
@@ -85,7 +83,6 @@ void process_srcu(struct work_struct *work);
 		.completed = -300,					\
 		.per_cpu_ref = &name##_srcu_array,			\
 		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
-		.srcu_state = SRCU_STATE_IDLE,				\
 		.srcu_cblist = RCU_SEGCBLIST_INITIALIZER(name.srcu_cblist),\
 		.work = __DELAYED_WORK_INITIALIZER(name.work, process_srcu, 0),\
 		__SRCU_DEP_MAP_INIT(name)				\

commit 8660b7d8a545227fd9ee80508aa82528ea9947d7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Mar 13 16:48:18 2017 -0700

    srcu: Use rcu_segcblist to track SRCU callbacks
    
    This commit switches SRCU from custom-built callback queues to the new
    rcu_segcblist structure.  This change associates grace-period sequence
    numbers with groups of callbacks, which will be needed for efficient
    processing of per-CPU callbacks.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 047ac8c28a4e..ad154a7bc114 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -22,7 +22,7 @@
  *	   Lai Jiangshan <laijs@cn.fujitsu.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -
- * 		Documentation/RCU/ *.txt
+ *		Documentation/RCU/ *.txt
  *
  */
 
@@ -32,31 +32,20 @@
 #include <linux/mutex.h>
 #include <linux/rcupdate.h>
 #include <linux/workqueue.h>
+#include <linux/rcu_segcblist.h>
 
 struct srcu_array {
 	unsigned long lock_count[2];
 	unsigned long unlock_count[2];
 };
 
-struct rcu_batch {
-	struct rcu_head *head, **tail;
-};
-
-#define RCU_BATCH_INIT(name) { NULL, &(name.head) }
-
 struct srcu_struct {
 	unsigned long completed;
 	unsigned long srcu_gp_seq;
 	struct srcu_array __percpu *per_cpu_ref;
-	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
+	spinlock_t queue_lock; /* protect ->srcu_cblist, ->srcu_state */
 	int srcu_state;
-	/* callbacks just queued */
-	struct rcu_batch batch_queue;
-	/* callbacks try to do the first check_zero */
-	struct rcu_batch batch_check0;
-	/* callbacks done with the first check_zero and the flip */
-	struct rcu_batch batch_check1;
-	struct rcu_batch batch_done;
+	struct rcu_segcblist srcu_cblist;
 	struct delayed_work work;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map dep_map;
@@ -97,10 +86,7 @@ void process_srcu(struct work_struct *work);
 		.per_cpu_ref = &name##_srcu_array,			\
 		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
 		.srcu_state = SRCU_STATE_IDLE,				\
-		.batch_queue = RCU_BATCH_INIT(name.batch_queue),	\
-		.batch_check0 = RCU_BATCH_INIT(name.batch_check0),	\
-		.batch_check1 = RCU_BATCH_INIT(name.batch_check1),	\
-		.batch_done = RCU_BATCH_INIT(name.batch_done),		\
+		.srcu_cblist = RCU_SEGCBLIST_INITIALIZER(name.srcu_cblist),\
 		.work = __DELAYED_WORK_INITIALIZER(name.work, process_srcu, 0),\
 		__SRCU_DEP_MAP_INIT(name)				\
 	}

commit ac367c1c621b75689f6d5cd8301d364ba2c9f292
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Mar 11 07:14:06 2017 -0800

    srcu: Add grace-period sequence numbers
    
    This commit adds grace-period sequence numbers, which will be used to
    handle mid-boot grace periods and per-CPU callback lists.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index f149a685896c..047ac8c28a4e 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -46,6 +46,7 @@ struct rcu_batch {
 
 struct srcu_struct {
 	unsigned long completed;
+	unsigned long srcu_gp_seq;
 	struct srcu_array __percpu *per_cpu_ref;
 	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
 	int srcu_state;

commit c2a8ec0778b2ca0d360ba9b5cac7fcd5ddfe798f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Mar 10 15:31:55 2017 -0800

    srcu: Move to state-based grace-period sequencing
    
    The current SRCU grace-period processing might never reach the last
    portion of srcu_advance_batches().  This is OK given the current
    implementation, as the first portion, up to the try_check_zero()
    following the srcu_flip() is sufficient to drive grace periods forward.
    However, it has the unfortunate side-effect of making it impossible to
    determine when a given grace period has ended, and it will be necessary
    to efficiently trace ends of grace periods in order to efficiently handle
    per-CPU SRCU callback lists.
    
    This commit therefore adds states to the SRCU grace-period processing,
    so that the end of a given SRCU grace period is marked by the transition
    to the SRCU_STATE_DONE state.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index a598cf3ac70c..f149a685896c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -48,7 +48,7 @@ struct srcu_struct {
 	unsigned long completed;
 	struct srcu_array __percpu *per_cpu_ref;
 	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
-	bool running;
+	int srcu_state;
 	/* callbacks just queued */
 	struct rcu_batch batch_queue;
 	/* callbacks try to do the first check_zero */
@@ -62,6 +62,12 @@ struct srcu_struct {
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 };
 
+/* Values for -> state variable. */
+#define SRCU_STATE_IDLE		0
+#define SRCU_STATE_SCAN1	1
+#define SRCU_STATE_SCAN2	2
+#define SRCU_STATE_DONE		3
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 int __init_srcu_struct(struct srcu_struct *sp, const char *name,
@@ -89,7 +95,7 @@ void process_srcu(struct work_struct *work);
 		.completed = -300,					\
 		.per_cpu_ref = &name##_srcu_array,			\
 		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
-		.running = false,					\
+		.srcu_state = SRCU_STATE_IDLE,				\
 		.batch_queue = RCU_BATCH_INIT(name.batch_queue),	\
 		.batch_check0 = RCU_BATCH_INIT(name.batch_check0),	\
 		.batch_check1 = RCU_BATCH_INIT(name.batch_check1),	\

commit f2c4689640e9a34bc45c013032185ed4ce47e7ff
Author: Lance Roy <ldr709@gmail.com>
Date:   Mon Jan 23 13:35:18 2017 -0800

    srcu: Implement more-efficient reader counts
    
    SRCU uses two per-cpu counters: a nesting counter to count the number of
    active critical sections, and a sequence counter to ensure that the nesting
    counters don't change while they are being added together in
    srcu_readers_active_idx_check().
    
    This patch instead uses per-cpu lock and unlock counters. Because both
    counters only increase and srcu_readers_active_idx_check() reads the unlock
    counter before the lock counter, this achieves the same end without having
    to increment two different counters in srcu_read_lock(). This also saves a
    smp_mb() in srcu_readers_active_idx_check().
    
    Possible bug: There is no guarantee that the lock counter won't overflow
    during srcu_readers_active_idx_check(), as there are no memory barriers
    around srcu_flip() (see comment in srcu_readers_active_idx_check() for
    details). However, this problem was already present before this patch.
    
    Suggested-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Lance Roy <ldr709@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index dc8eb63c6568..a598cf3ac70c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -33,9 +33,9 @@
 #include <linux/rcupdate.h>
 #include <linux/workqueue.h>
 
-struct srcu_struct_array {
-	unsigned long c[2];
-	unsigned long seq[2];
+struct srcu_array {
+	unsigned long lock_count[2];
+	unsigned long unlock_count[2];
 };
 
 struct rcu_batch {
@@ -46,7 +46,7 @@ struct rcu_batch {
 
 struct srcu_struct {
 	unsigned long completed;
-	struct srcu_struct_array __percpu *per_cpu_ref;
+	struct srcu_array __percpu *per_cpu_ref;
 	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
 	bool running;
 	/* callbacks just queued */
@@ -118,7 +118,7 @@ void process_srcu(struct work_struct *work);
  * See include/linux/percpu-defs.h for the rules on per-CPU variables.
  */
 #define __DEFINE_SRCU(name, is_static)					\
-	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
+	static DEFINE_PER_CPU(struct srcu_array, name##_srcu_array);\
 	is_static struct srcu_struct name = __SRCU_STRUCT_INIT(name)
 #define DEFINE_SRCU(name)		__DEFINE_SRCU(name, /* not static */)
 #define DEFINE_STATIC_SRCU(name)	__DEFINE_SRCU(name, static)

commit 9de630c4f264dec48e61edd871cb98b8e1b58250
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 8 07:43:50 2016 -0800

    rcu: Document unique-name limitation for DEFINE_STATIC_SRCU()
    
    SRCU uses per-CPU variables, and DEFINE_STATIC_SRCU() uses a static
    per-CPU variable.  However, per-CPU variables have significant
    restrictions, for example, names of per-CPU variables must be globally
    unique, even if declared static.  These restrictions carry over to
    DEFINE_STATIC_SRCU(), and this commit therefore documents these
    restrictions.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Suggested-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index f5f80c5643ac..dc8eb63c6568 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -99,8 +99,23 @@ void process_srcu(struct work_struct *work);
 	}
 
 /*
- * define and init a srcu struct at build time.
- * dont't call init_srcu_struct() nor cleanup_srcu_struct() on it.
+ * Define and initialize a srcu struct at build time.
+ * Do -not- call init_srcu_struct() nor cleanup_srcu_struct() on it.
+ *
+ * Note that although DEFINE_STATIC_SRCU() hides the name from other
+ * files, the per-CPU variable rules nevertheless require that the
+ * chosen name be globally unique.  These rules also prohibit use of
+ * DEFINE_STATIC_SRCU() within a function.  If these rules are too
+ * restrictive, declare the srcu_struct manually.  For example, in
+ * each file:
+ *
+ *	static struct srcu_struct my_srcu;
+ *
+ * Then, before the first use of each my_srcu, manually initialize it:
+ *
+ *	init_srcu_struct(&my_srcu);
+ *
+ * See include/linux/percpu-defs.h for the rules on per-CPU variables.
  */
 #define __DEFINE_SRCU(name, is_static)					\
 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\

commit 49f5903b473c5f63f3b57856d1bd4593db0a2eef
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 1 00:42:57 2015 -0700

    rcu: Move preemption disabling out of __srcu_read_lock()
    
    Currently, __srcu_read_lock() cannot be invoked from restricted
    environments because it contains calls to preempt_disable() and
    preempt_enable(), both of which can invoke lockdep, which is a bad
    idea in some restricted execution modes.  This commit therefore moves
    the preempt_disable() and preempt_enable() from __srcu_read_lock()
    to srcu_read_lock().  It also inserts the preempt_disable() and
    preempt_enable() around the call to __srcu_read_lock() in do_exit().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index bdeb4567b71e..f5f80c5643ac 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -215,8 +215,11 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
  */
 static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 {
-	int retval = __srcu_read_lock(sp);
+	int retval;
 
+	preempt_disable();
+	retval = __srcu_read_lock(sp);
+	preempt_enable();
 	rcu_lock_acquire(&(sp)->dep_map);
 	return retval;
 }

commit b826565aaf8809df146666c03d1acbb7febbd13e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 2 11:46:33 2015 -0800

    rcu: Reverse rcu_dereference_check() conditions
    
    The rcu_dereference_check() family of primitives evaluates the RCU
    lockdep expression first, and only then evaluates the expression passed
    in.  This works fine normally, but can potentially fail in environments
    (such as NMI handlers) where lockdep cannot be invoked.  The problem is
    that even if the expression passed in is "1", the compiler would need to
    prove that the RCU lockdep expression (rcu_read_lock_held(), for example)
    is free of side effects in order to be able to elide it.  Given that
    rcu_read_lock_held() is sometimes separately compiled, the compiler cannot
    always use this optimization.
    
    This commit therefore reverse the order of evaluation, so that the
    expression passed in is evaluated first, and the RCU lockdep expression is
    evaluated only if the passed-in expression evaluated to false, courtesy
    of the C-language short-circuit boolean evaluation rules.  This compells
    the compiler to forego executing the RCU lockdep expression in cases
    where the passed-in expression evaluates to "1" at compile time, so that
    (for example) rcu_dereference_raw() can be guaranteed to execute safely
    within an NMI handler.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 9cfd9623fb03..bdeb4567b71e 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -182,7 +182,7 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
  * lockdep_is_held() calls.
  */
 #define srcu_dereference_check(p, sp, c) \
-	__rcu_dereference_check((p), srcu_read_lock_held(sp) || (c), __rcu)
+	__rcu_dereference_check((p), (c) || srcu_read_lock_held(sp), __rcu)
 
 /**
  * srcu_dereference - fetch SRCU-protected pointer for later dereferencing

commit 9735af5c78599703be633c057af3faee26482028
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Nov 26 10:42:50 2014 -0800

    rcu: Combine DEFINE_SRCU() and DEFINE_STATIC_SRCU()
    
    The DEFINE_SRCU() and DEFINE_STATIC_SRCU() definitions are quite
    similar, so this commit combines them, saving a bit of code and removing
    redundancy.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index ef923dd96249..9cfd9623fb03 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -102,13 +102,11 @@ void process_srcu(struct work_struct *work);
  * define and init a srcu struct at build time.
  * dont't call init_srcu_struct() nor cleanup_srcu_struct() on it.
  */
-#define DEFINE_SRCU(name)						\
+#define __DEFINE_SRCU(name, is_static)					\
 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
-	struct srcu_struct name = __SRCU_STRUCT_INIT(name);
-
-#define DEFINE_STATIC_SRCU(name)					\
-	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
-	static struct srcu_struct name = __SRCU_STRUCT_INIT(name);
+	is_static struct srcu_struct name = __SRCU_STRUCT_INIT(name)
+#define DEFINE_SRCU(name)		__DEFINE_SRCU(name, /* not static */)
+#define DEFINE_STATIC_SRCU(name)	__DEFINE_SRCU(name, static)
 
 /**
  * call_srcu() - Queue a callback for invocation after an SRCU grace period

commit a5c198f4f7da6cc48116ca239c59c9f44b753364
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 23 20:30:06 2014 -0800

    rcu: Expand SRCU ->completed to 64 bits
    
    When rcutorture used only the low-order 32 bits of the grace-period
    number, it was not a problem for SRCU to use a 32-bit completed field.
    However, rcutorture now uses the full 64 bits on 64-bit systems, so
    this commit converts SRCU's ->completed field to unsigned long so as to
    provide 64 bits on 64-bit systems.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index a2783cb5d275..ef923dd96249 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -45,7 +45,7 @@ struct rcu_batch {
 #define RCU_BATCH_INIT(name) { NULL, &(name.head) }
 
 struct srcu_struct {
-	unsigned completed;
+	unsigned long completed;
 	struct srcu_struct_array __percpu *per_cpu_ref;
 	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
 	bool running;
@@ -135,7 +135,7 @@ int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
 void synchronize_srcu_expedited(struct srcu_struct *sp);
-long srcu_batches_completed(struct srcu_struct *sp);
+unsigned long srcu_batches_completed(struct srcu_struct *sp);
 void srcu_barrier(struct srcu_struct *sp);
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC

commit 87de1cfdc55b16b794e245b07322340725149d62
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 3 10:02:52 2013 -0800

    rcu: Stop tracking FSF's postal address
    
    All of the RCU source files have the usual GPL header, which contains a
    long-obsolete postal address for FSF.  To avoid the need to track the
    FSF office's movements, this commit substitutes the URL where GPL may
    be found.
    
    Reported-by: Greg KH <gregkh@linuxfoundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 9b058eecd403..a2783cb5d275 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -12,8 +12,8 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
  *
  * Copyright (C) IBM Corporation, 2006
  * Copyright (C) Fujitsu, 2012

commit ce332f662deb545c8a4f3f58debcca26bb2e44b0
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Mon Nov 4 22:36:17 2013 +0200

    srcu: API for barrier after srcu read unlock
    
    srcu read lock/unlock include a full memory barrier
    but that's an implementation detail.
    Add an API for make memory fencing explicit for
    users that need this barrier, to make sure we
    can change it as needed without breaking all users.
    
    Acked-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index c114614ed172..9b058eecd403 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -237,4 +237,18 @@ static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__srcu_read_unlock(sp, idx);
 }
 
+/**
+ * smp_mb__after_srcu_read_unlock - ensure full ordering after srcu_read_unlock
+ *
+ * Converts the preceding srcu_read_unlock into a two-way memory barrier.
+ *
+ * Call this after srcu_read_unlock, to guarantee that all memory operations
+ * that occur after smp_mb__after_srcu_read_unlock will appear to happen after
+ * the preceding srcu_read_unlock.
+ */
+static inline void smp_mb__after_srcu_read_unlock(void)
+{
+	/* __srcu_read_unlock has smp_mb() internally so nothing to do here. */
+}
+
 #endif

commit 99f88919f8fa8a8b01b5306c59c9977b94604df8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 12 16:54:14 2013 -0700

    rcu: Remove srcu_read_lock_raw() and srcu_read_unlock_raw().
    
    These interfaces never did get used, so this commit removes them,
    their rcutorture tests, and documentation referencing them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 04f4121a23ae..c114614ed172 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -237,47 +237,4 @@ static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__srcu_read_unlock(sp, idx);
 }
 
-/**
- * srcu_read_lock_raw - register a new reader for an SRCU-protected structure.
- * @sp: srcu_struct in which to register the new reader.
- *
- * Enter an SRCU read-side critical section.  Similar to srcu_read_lock(),
- * but avoids the RCU-lockdep checking.  This means that it is legal to
- * use srcu_read_lock_raw() in one context, for example, in an exception
- * handler, and then have the matching srcu_read_unlock_raw() in another
- * context, for example in the task that took the exception.
- *
- * However, the entire SRCU read-side critical section must reside within a
- * single task.  For example, beware of using srcu_read_lock_raw() in
- * a device interrupt handler and srcu_read_unlock() in the interrupted
- * task:  This will not work if interrupts are threaded.
- */
-static inline int srcu_read_lock_raw(struct srcu_struct *sp)
-{
-	unsigned long flags;
-	int ret;
-
-	local_irq_save(flags);
-	ret =  __srcu_read_lock(sp);
-	local_irq_restore(flags);
-	return ret;
-}
-
-/**
- * srcu_read_unlock_raw - unregister reader from an SRCU-protected structure.
- * @sp: srcu_struct in which to unregister the old reader.
- * @idx: return value from corresponding srcu_read_lock_raw().
- *
- * Exit an SRCU read-side critical section without lockdep-RCU checking.
- * See srcu_read_lock_raw() for more details.
- */
-static inline void srcu_read_unlock_raw(struct srcu_struct *sp, int idx)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	__srcu_read_unlock(sp, idx);
-	local_irq_restore(flags);
-}
-
 #endif

commit 511a0868bed6694512348fc177cdfaf3fd97d0bb
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Nov 29 16:46:06 2012 +0800

    srcu: Remove checks preventing idle CPUs from calling srcu_read_lock()
    
    SRCU has its own statemachine and no longer relies on normal RCU.
    Its read-side critical section can now be used by an offline CPU, so this
    commit removes the check and the comments, reverting the SRCU portion
    of ff195cb6 (rcu: Warn when srcu_read_lock() is used in an extended
    quiescent state).
    
    It also makes the codes match the comments in whatisRCU.txt:
    
    g.      Do you need read-side critical sections that are respected
            even though they are in the middle of the idle loop, during
            user-mode execution, or on an offlined CPU?  If so, SRCU is the
            only choice that will work for you.
    
    [ paulmck: There is at least one remaining issue, namely use of lockdep
               with tracing enabled. ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index ae3ee39eb699..04f4121a23ae 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -151,25 +151,14 @@ void srcu_barrier(struct srcu_struct *sp);
  * Checks debug_lockdep_rcu_enabled() to prevent false positives during boot
  * and while lockdep is disabled.
  *
- * Note that if the CPU is in the idle loop from an RCU point of view
- * (ie: that we are in the section between rcu_idle_enter() and
- * rcu_idle_exit()) then srcu_read_lock_held() returns false even if
- * the CPU did an srcu_read_lock().  The reason for this is that RCU
- * ignores CPUs that are in such a section, considering these as in
- * extended quiescent state, so such a CPU is effectively never in an
- * RCU read-side critical section regardless of what RCU primitives it
- * invokes.  This state of affairs is required --- we need to keep an
- * RCU-free window in idle where the CPU may possibly enter into low
- * power mode. This way we can notice an extended quiescent state to
- * other CPUs that started a grace period. Otherwise we would delay any
- * grace period as long as we run in the idle task.
+ * Note that SRCU is based on its own statemachine and it doesn't
+ * relies on normal RCU, it can be called from the CPU which
+ * is in the idle loop from an RCU point of view or offline.
  */
 static inline int srcu_read_lock_held(struct srcu_struct *sp)
 {
 	if (!debug_lockdep_rcu_enabled())
 		return 1;
-	if (rcu_is_cpu_idle())
-		return 0;
 	return lock_is_held(&sp->dep_map);
 }
 
@@ -231,8 +220,6 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 	int retval = __srcu_read_lock(sp);
 
 	rcu_lock_acquire(&(sp)->dep_map);
-	rcu_lockdep_assert(!rcu_is_cpu_idle(),
-			   "srcu_read_lock() used illegally while idle");
 	return retval;
 }
 
@@ -246,8 +233,6 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__releases(sp)
 {
-	rcu_lockdep_assert(!rcu_is_cpu_idle(),
-			   "srcu_read_unlock() used illegally while idle");
 	rcu_lock_release(&(sp)->dep_map);
 	__srcu_read_unlock(sp, idx);
 }

commit 3bc97a782cc8c112f64a25143452b06206364cc8
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Nov 29 16:46:05 2012 +0800

    srcu: Remove checks preventing offline CPUs from calling srcu_read_lock()
    
    SRCU has its own statemachine and no longer relies on normal RCU.
    Its read-side critical section can now be used by an offline CPU, so this
    commit removes the check and the comments, reverting the SRCU portion
    of c0d6d01b (rcu: Check for illegal use of RCU from offlined CPUs).
    
    It also makes the code match the comments in whatisRCU.txt:
    
    g.      Do you need read-side critical sections that are respected
            even though they are in the middle of the idle loop, during
            user-mode execution, or on an offlined CPU?  If so, SRCU is the
            only choice that will work for you.
    
    [ paulmck: There is at least one remaining issue, namely use of lockdep
               with tracing enabled. ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 6eb691b08358..ae3ee39eb699 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -163,9 +163,6 @@ void srcu_barrier(struct srcu_struct *sp);
  * power mode. This way we can notice an extended quiescent state to
  * other CPUs that started a grace period. Otherwise we would delay any
  * grace period as long as we run in the idle task.
- *
- * Similarly, we avoid claiming an SRCU read lock held if the current
- * CPU is offline.
  */
 static inline int srcu_read_lock_held(struct srcu_struct *sp)
 {
@@ -173,8 +170,6 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
 		return 1;
 	if (rcu_is_cpu_idle())
 		return 0;
-	if (!rcu_lockdep_current_cpu_online())
-		return 0;
 	return lock_is_held(&sp->dep_map);
 }
 

commit 55c6659afaa6fd79a3b5a7c2b42bb87e0c11209d
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Sat Oct 13 01:14:16 2012 +0800

    srcu: Add DEFINE_SRCU()
    
    In old days, we had two different API sets for dynamic-allocated per-CPU
    data and DEFINE_PER_CPU()-defined per_cpu data, and because SRCU used
    dynamic-allocated per-CPU data, its srcu_struct structures cannot be
    declared statically.  This commit therefore introduces DEFINE_SRCU()
    and DEFINE_STATIC_SRCU() to allow statically declared SRCU structures,
    using the new static per-CPU interfaces.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Updated for __DELAYED_WORK_INITIALIZER() added argument,
               fixed whitespace issue. ]

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 5cce128f196c..6eb691b08358 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -42,6 +42,8 @@ struct rcu_batch {
 	struct rcu_head *head, **tail;
 };
 
+#define RCU_BATCH_INIT(name) { NULL, &(name.head) }
+
 struct srcu_struct {
 	unsigned completed;
 	struct srcu_struct_array __percpu *per_cpu_ref;
@@ -72,14 +74,42 @@ int __init_srcu_struct(struct srcu_struct *sp, const char *name,
 	__init_srcu_struct((sp), #sp, &__srcu_key); \
 })
 
+#define __SRCU_DEP_MAP_INIT(srcu_name)	.dep_map = { .name = #srcu_name },
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 int init_srcu_struct(struct srcu_struct *sp);
 
+#define __SRCU_DEP_MAP_INIT(srcu_name)
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 void process_srcu(struct work_struct *work);
 
+#define __SRCU_STRUCT_INIT(name)					\
+	{								\
+		.completed = -300,					\
+		.per_cpu_ref = &name##_srcu_array,			\
+		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
+		.running = false,					\
+		.batch_queue = RCU_BATCH_INIT(name.batch_queue),	\
+		.batch_check0 = RCU_BATCH_INIT(name.batch_check0),	\
+		.batch_check1 = RCU_BATCH_INIT(name.batch_check1),	\
+		.batch_done = RCU_BATCH_INIT(name.batch_done),		\
+		.work = __DELAYED_WORK_INITIALIZER(name.work, process_srcu, 0),\
+		__SRCU_DEP_MAP_INIT(name)				\
+	}
+
+/*
+ * define and init a srcu struct at build time.
+ * dont't call init_srcu_struct() nor cleanup_srcu_struct() on it.
+ */
+#define DEFINE_SRCU(name)						\
+	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
+	struct srcu_struct name = __SRCU_STRUCT_INIT(name);
+
+#define DEFINE_STATIC_SRCU(name)					\
+	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
+	static struct srcu_struct name = __SRCU_STRUCT_INIT(name);
+
 /**
  * call_srcu() - Queue a callback for invocation after an SRCU grace period
  * @sp: srcu_struct in queue the callback

commit f2ebfbc991044fd5b89d4529741d7500feb37fbd
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Sat Oct 13 01:14:15 2012 +0800

    srcu: Export process_srcu()
    
    Because process_srcu() will be used in DEFINE_SRCU(), which is a macro
    that could be expanded pretty much anywhere, it can no longer be static.
    Note that process_srcu() is still internal to srcu.h.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index a55ddb19053c..5cce128f196c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -78,6 +78,8 @@ int init_srcu_struct(struct srcu_struct *sp);
 
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
+void process_srcu(struct work_struct *work);
+
 /**
  * call_srcu() - Queue a callback for invocation after an SRCU grace period
  * @sp: srcu_struct in queue the callback

commit 4e87b2d7e887df3fe251dd7f702591a3acf369ca
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Sat Oct 13 01:14:14 2012 +0800

    srcu: Credit Lai Jiangshan with SRCU rewrite
    
    Lai Jiangshan rewrote SRCU, so this commit ensures that he gets his
    proper share of blame^Wcredit.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 55a5c52cbb25..a55ddb19053c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -16,8 +16,10 @@
  * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
  *
  * Copyright (C) IBM Corporation, 2006
+ * Copyright (C) Fujitsu, 2012
  *
  * Author: Paul McKenney <paulmck@us.ibm.com>
+ *	   Lai Jiangshan <laijs@cn.fujitsu.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -
  * 		Documentation/RCU/ *.txt

commit 931ea9d1a6e06a5e3af03aa4aaaa7c7fd90e163f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Mar 19 16:12:13 2012 +0800

    rcu: Implement per-domain single-threaded call_srcu() state machine
    
    This commit implements an SRCU state machine in support of call_srcu().
    The state machine is preemptible, light-weight, and single-threaded,
    minimizing synchronization overhead.  In particular, there is no longer
    any need for synchronize_srcu() to be guarded by a mutex.
    
    Expedited processing is handled, at least in the absence of concurrent
    grace-period operations on that same srcu_struct structure, by having
    the synchronize_srcu_expedited() thread take on the role of the
    workqueue thread for one iteration.
    
    There is a reasonable probability that a given SRCU callback will
    be invoked on the same CPU that registered it, however, there is no
    guarantee.  Concurrent SRCU grace-period primitives can cause callbacks
    to be executed elsewhere, even in absence of CPU-hotplug operations.
    
    Callbacks execute in process context, but under the influence of
    local_bh_disable(), so it is illegal to sleep in an SRCU callback
    function.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index e5ce80452b62..55a5c52cbb25 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -29,16 +29,30 @@
 
 #include <linux/mutex.h>
 #include <linux/rcupdate.h>
+#include <linux/workqueue.h>
 
 struct srcu_struct_array {
 	unsigned long c[2];
 	unsigned long seq[2];
 };
 
+struct rcu_batch {
+	struct rcu_head *head, **tail;
+};
+
 struct srcu_struct {
 	unsigned completed;
 	struct srcu_struct_array __percpu *per_cpu_ref;
-	struct mutex mutex;
+	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
+	bool running;
+	/* callbacks just queued */
+	struct rcu_batch batch_queue;
+	/* callbacks try to do the first check_zero */
+	struct rcu_batch batch_check0;
+	/* callbacks done with the first check_zero and the flip */
+	struct rcu_batch batch_check1;
+	struct rcu_batch batch_done;
+	struct delayed_work work;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map dep_map;
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
@@ -62,12 +76,33 @@ int init_srcu_struct(struct srcu_struct *sp);
 
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
+/**
+ * call_srcu() - Queue a callback for invocation after an SRCU grace period
+ * @sp: srcu_struct in queue the callback
+ * @head: structure to be used for queueing the SRCU callback.
+ * @func: function to be invoked after the SRCU grace period
+ *
+ * The callback function will be invoked some time after a full SRCU
+ * grace period elapses, in other words after all pre-existing SRCU
+ * read-side critical sections have completed.  However, the callback
+ * function might well execute concurrently with other SRCU read-side
+ * critical sections that started after call_srcu() was invoked.  SRCU
+ * read-side critical sections are delimited by srcu_read_lock() and
+ * srcu_read_unlock(), and may be nested.
+ *
+ * The callback will be invoked from process context, but must nevertheless
+ * be fast and must not block.
+ */
+void call_srcu(struct srcu_struct *sp, struct rcu_head *head,
+		void (*func)(struct rcu_head *head));
+
 void cleanup_srcu_struct(struct srcu_struct *sp);
 int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
 void synchronize_srcu_expedited(struct srcu_struct *sp);
 long srcu_batches_completed(struct srcu_struct *sp);
+void srcu_barrier(struct srcu_struct *sp);
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 

commit 966f58c2f6df826f385706673a9bb1edcfd3499a
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Mar 6 17:57:33 2012 +0800

    rcu: Remove unused srcu_barrier()
    
    The old srcu_barrier() macro is now unused.  This commit removes it so
    that it may be used for the SRCU flavor of rcu_barrier(), which will in
    turn be needed to allow the upcoming call_srcu() to be used from within
    modules.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 15354db3e865..e5ce80452b62 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -44,12 +44,6 @@ struct srcu_struct {
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 };
 
-#ifndef CONFIG_PREEMPT
-#define srcu_barrier() barrier()
-#else /* #ifndef CONFIG_PREEMPT */
-#define srcu_barrier()
-#endif /* #else #ifndef CONFIG_PREEMPT */
-
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 int __init_srcu_struct(struct srcu_struct *sp, const char *name,

commit b52ce066c55a6a53cf1f8d71308d74f908e31b99
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Feb 27 09:29:09 2012 -0800

    rcu: Implement a variant of Peter's SRCU algorithm
    
    This commit implements a variant of Peter's algorithm, which may be found
    at https://lkml.org/lkml/2012/2/1/119.
    
    o       Make the checking lock-free to enable parallel checking.
            Parallel checking is required when (1) the original checking
            task is preempted for a long time, (2) sychronize_srcu_expedited()
            starts during an ongoing SRCU grace period, or (3) we wish to
            avoid acquiring a lock.
    
    o       Since the checking is lock-free, we avoid a mutex in state machine
            for call_srcu().
    
    o       Remove the SRCU_REF_MASK and remove the coupling with the flipping.
            This might allow us to remove the preempt_disable() in future
            versions, though such removal will need great care because it
            rescinds the one-old-reader-per-CPU guarantee.
    
    o       Remove a smp_mb(), simplify the comments and make the smp_mb() pairs
            more intuitive.
    
    Inspired-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 5b49d41868c8..15354db3e865 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -32,18 +32,13 @@
 
 struct srcu_struct_array {
 	unsigned long c[2];
+	unsigned long seq[2];
 };
 
-/* Bit definitions for field ->c above and ->snap below. */
-#define SRCU_USAGE_BITS		1
-#define SRCU_REF_MASK		(ULONG_MAX >> SRCU_USAGE_BITS)
-#define SRCU_USAGE_COUNT	(SRCU_REF_MASK + 1)
-
 struct srcu_struct {
 	unsigned completed;
 	struct srcu_struct_array __percpu *per_cpu_ref;
 	struct mutex mutex;
-	unsigned long snap[NR_CPUS];
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map dep_map;
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */

commit 440253c17fc4ed41d778492a7fb44dc0d756eccc
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Feb 22 13:29:06 2012 -0800

    rcu: Increment upper bit only for srcu_read_lock()
    
    The purpose of the upper bit of SRCU's per-CPU counters is to guarantee
    that no reasonable series of srcu_read_lock() and srcu_read_unlock()
    operations can return the value of the counter to its original value.
    This guarantee is require only after the index has been switched to
    the other set of counters, so at most one srcu_read_lock() can affect
    a given CPU's counter.  The number of srcu_read_unlock() operations
    on a given counter is limited to the number of tasks in the system,
    which given the Linux kernel's current structure is limited to far less
    than 2^30 on 32-bit systems and far less than 2^62 on 64-bit systems.
    (Something about a limited number of bytes in the kernel's address space.)
    
    Therefore, if srcu_read_lock() increments the upper bits, then
    srcu_read_unlock() need not do so.  In this case, an srcu_read_lock() and
    an srcu_read_unlock() will flip the lower bit of the upper field of the
    counter.  An unreasonably large additional number of srcu_read_unlock()
    operations would be required to return the counter to its initial value,
    thus preserving the guarantee.
    
    This commit takes this approach, which further allows it to shrink
    the size of the upper field to one bit, making the number of
    srcu_read_unlock() operations required to return the counter to its
    initial value even more unreasonable than before.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index a478c8eb8479..5b49d41868c8 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -35,7 +35,7 @@ struct srcu_struct_array {
 };
 
 /* Bit definitions for field ->c above and ->snap below. */
-#define SRCU_USAGE_BITS		2
+#define SRCU_USAGE_BITS		1
 #define SRCU_REF_MASK		(ULONG_MAX >> SRCU_USAGE_BITS)
 #define SRCU_USAGE_COUNT	(SRCU_REF_MASK + 1)
 

commit cef50120b61c2af4ce34bc165e19cad66296f93d
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sun Feb 5 07:42:44 2012 -0800

    rcu: Direct algorithmic SRCU implementation
    
    The current implementation of synchronize_srcu_expedited() can cause
    severe OS jitter due to its use of synchronize_sched(), which in turn
    invokes try_stop_cpus(), which causes each CPU to be sent an IPI.
    This can result in severe performance degradation for real-time workloads
    and especially for short-interation-length HPC workloads.  Furthermore,
    because only one instance of try_stop_cpus() can be making forward progress
    at a given time, only one instance of synchronize_srcu_expedited() can
    make forward progress at a time, even if they are all operating on
    distinct srcu_struct structures.
    
    This commit, inspired by an earlier implementation by Peter Zijlstra
    (https://lkml.org/lkml/2012/1/31/211) and by further offline discussions,
    takes a strictly algorithmic bits-in-memory approach.  This has the
    disadvantage of requiring one explicit memory-barrier instruction in
    each of srcu_read_lock() and srcu_read_unlock(), but on the other hand
    completely dispenses with OS jitter and furthermore allows SRCU to be
    used freely by CPUs that RCU believes to be idle or offline.
    
    The update-side implementation handles the single read-side memory
    barrier by rechecking the per-CPU counters after summing them and
    by running through the update-side state machine twice.
    
    This implementation has passed moderate rcutorture testing on both
    x86 and Power.  Also updated to use this_cpu_ptr() instead of per_cpu_ptr(),
    as suggested by Peter Zijlstra.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index d3d5fa54f25e..a478c8eb8479 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -31,13 +31,19 @@
 #include <linux/rcupdate.h>
 
 struct srcu_struct_array {
-	int c[2];
+	unsigned long c[2];
 };
 
+/* Bit definitions for field ->c above and ->snap below. */
+#define SRCU_USAGE_BITS		2
+#define SRCU_REF_MASK		(ULONG_MAX >> SRCU_USAGE_BITS)
+#define SRCU_USAGE_COUNT	(SRCU_REF_MASK + 1)
+
 struct srcu_struct {
-	int completed;
+	unsigned completed;
 	struct srcu_struct_array __percpu *per_cpu_ref;
 	struct mutex mutex;
+	unsigned long snap[NR_CPUS];
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map dep_map;
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */

commit bde23c6892878e48f64de668660778991bc2fb56
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Feb 1 10:30:46 2012 -0800

    rcu: Convert WARN_ON_ONCE() in rcu_lock_acquire() to lockdep
    
    The WARN_ON_ONCE() in rcu_lock_acquire() results in infinite recursion
    on S390, and also doesn't print very much information.  Remove this.
    
    Updated patch to add lockdep-RCU assertions to RCU's read-side primitives.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 9a323728e60c..d3d5fa54f25e 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -172,6 +172,8 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 	int retval = __srcu_read_lock(sp);
 
 	rcu_lock_acquire(&(sp)->dep_map);
+	rcu_lockdep_assert(!rcu_is_cpu_idle(),
+			   "srcu_read_lock() used illegally while idle");
 	return retval;
 }
 
@@ -185,6 +187,8 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__releases(sp)
 {
+	rcu_lockdep_assert(!rcu_is_cpu_idle(),
+			   "srcu_read_unlock() used illegally while idle");
 	rcu_lock_release(&(sp)->dep_map);
 	__srcu_read_unlock(sp, idx);
 }

commit c0d6d01bffdce19fa19baad6cb8cc3eed7bfd6f5
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Jan 23 12:41:26 2012 -0800

    rcu: Check for illegal use of RCU from offlined CPUs
    
    Although it is legal to use RCU during early boot, it is anything
    but legal to use RCU at runtime from an offlined CPU.  After all, RCU
    explicitly ignores offlined CPUs.  This commit therefore adds checks
    for runtime use of RCU from offlined CPUs.
    
    These checks are not perfect, in particular, they can be subverted
    through use of things like rcu_dereference_raw().  Note that it is not
    possible to put checks in rcu_read_lock() and friends due to the fact
    that these primitives are used in code that might be used under either
    RCU or lock-based protection, which means that checking rcu_read_lock()
    gets you fat piles of false positives.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index e1b005918bbb..9a323728e60c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -99,15 +99,18 @@ long srcu_batches_completed(struct srcu_struct *sp);
  * power mode. This way we can notice an extended quiescent state to
  * other CPUs that started a grace period. Otherwise we would delay any
  * grace period as long as we run in the idle task.
+ *
+ * Similarly, we avoid claiming an SRCU read lock held if the current
+ * CPU is offline.
  */
 static inline int srcu_read_lock_held(struct srcu_struct *sp)
 {
-	if (rcu_is_cpu_idle())
-		return 0;
-
 	if (!debug_lockdep_rcu_enabled())
 		return 1;
-
+	if (rcu_is_cpu_idle())
+		return 0;
+	if (!rcu_lockdep_current_cpu_online())
+		return 0;
 	return lock_is_held(&sp->dep_map);
 }
 

commit 3842a0832a1d6eb0b31421f8810a813135967512
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Nov 28 10:42:42 2011 -0800

    rcu: Document same-context read-side constraints
    
    The intent is that a given RCU read-side critical section be confined
    to a single context.  For example, it is illegal to invoke rcu_read_lock()
    in an exception handler and then invoke rcu_read_unlock() from the
    context of the task that received the exception.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 1eb520cd1680..e1b005918bbb 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -158,6 +158,11 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
  * one way to indirectly wait on an SRCU grace period is to acquire
  * a mutex that is held elsewhere while calling synchronize_srcu() or
  * synchronize_srcu_expedited().
+ *
+ * Note that srcu_read_lock() and the matching srcu_read_unlock() must
+ * occur in the same context, for example, it is illegal to invoke
+ * srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
+ * was invoked in process context.
  */
 static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 {

commit 0c53dd8b31404c1d7fd15be8f065ebaec615a562
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Oct 9 15:13:11 2011 -0700

    rcu: Introduce raw SRCU read-side primitives
    
    The RCU implementations, including SRCU, are designed to be used in a
    lock-like fashion, so that the read-side lock and unlock primitives must
    execute in the same context for any given read-side critical section.
    This constraint is enforced by lockdep-RCU.  However, there is a need
    to enter an SRCU read-side critical section within the context of an
    exception and then exit in the context of the task that encountered the
    exception.  The cost of this capability is that the read-side operations
    incur the overhead of disabling interrupts.
    
    Note that although the current implementation allows a given read-side
    critical section to be entered by one task and then exited by another, all
    known possible implementations that allow this have scalability problems.
    Therefore, a given read-side critical section must be exited by the same
    task that entered it, though perhaps from an interrupt or exception
    handler running within that task's context.  But if you are thinking
    in terms of interrupt handlers, make sure that you have considered the
    possibility of threaded interrupt handlers.
    
    Credit goes to Peter Zijlstra for suggesting use of the existing _raw
    suffix to indicate disabling lockdep over the earlier "bulkref" names.
    
    Requested-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index d4b12443b2ef..1eb520cd1680 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -181,4 +181,47 @@ static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__srcu_read_unlock(sp, idx);
 }
 
+/**
+ * srcu_read_lock_raw - register a new reader for an SRCU-protected structure.
+ * @sp: srcu_struct in which to register the new reader.
+ *
+ * Enter an SRCU read-side critical section.  Similar to srcu_read_lock(),
+ * but avoids the RCU-lockdep checking.  This means that it is legal to
+ * use srcu_read_lock_raw() in one context, for example, in an exception
+ * handler, and then have the matching srcu_read_unlock_raw() in another
+ * context, for example in the task that took the exception.
+ *
+ * However, the entire SRCU read-side critical section must reside within a
+ * single task.  For example, beware of using srcu_read_lock_raw() in
+ * a device interrupt handler and srcu_read_unlock() in the interrupted
+ * task:  This will not work if interrupts are threaded.
+ */
+static inline int srcu_read_lock_raw(struct srcu_struct *sp)
+{
+	unsigned long flags;
+	int ret;
+
+	local_irq_save(flags);
+	ret =  __srcu_read_lock(sp);
+	local_irq_restore(flags);
+	return ret;
+}
+
+/**
+ * srcu_read_unlock_raw - unregister reader from an SRCU-protected structure.
+ * @sp: srcu_struct in which to unregister the old reader.
+ * @idx: return value from corresponding srcu_read_lock_raw().
+ *
+ * Exit an SRCU read-side critical section without lockdep-RCU checking.
+ * See srcu_read_lock_raw() for more details.
+ */
+static inline void srcu_read_unlock_raw(struct srcu_struct *sp, int idx)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	__srcu_read_unlock(sp, idx);
+	local_irq_restore(flags);
+}
+
 #endif

commit 867f236bd12f5091df6dc7cc75f94d7fd982d78a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Oct 7 18:22:05 2011 +0200

    rcu: Make srcu_read_lock_held() call common lockdep-enabled function
    
    A common debug_lockdep_rcu_enabled() function is used to check whether
    RCU lockdep splats should be reported, but srcu_read_lock() does not
    use it.  This commit therefore brings srcu_read_lock_held() up to date.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4e0a3d41dae3..d4b12443b2ef 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -84,6 +84,9 @@ long srcu_batches_completed(struct srcu_struct *sp);
  * this assumes we are in an SRCU read-side critical section unless it can
  * prove otherwise.
  *
+ * Checks debug_lockdep_rcu_enabled() to prevent false positives during boot
+ * and while lockdep is disabled.
+ *
  * Note that if the CPU is in the idle loop from an RCU point of view
  * (ie: that we are in the section between rcu_idle_enter() and
  * rcu_idle_exit()) then srcu_read_lock_held() returns false even if
@@ -102,7 +105,7 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
 	if (rcu_is_cpu_idle())
 		return 0;
 
-	if (!debug_locks)
+	if (!debug_lockdep_rcu_enabled())
 		return 1;
 
 	return lock_is_held(&sp->dep_map);

commit ff195cb69ba8d2af9b891be3a26db95fe1999d43
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Oct 7 18:22:04 2011 +0200

    rcu: Warn when srcu_read_lock() is used in an extended quiescent state
    
    Catch SRCU up to the other variants of RCU by making PROVE_RCU
    complain if either srcu_read_lock() or srcu_read_lock_held() are
    used from within RCU-idle mode.
    
    Frederic reworked this to allow for the new versions of his patches
    that check for extended quiescent states.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 58971e891f48..4e0a3d41dae3 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -28,6 +28,7 @@
 #define _LINUX_SRCU_H
 
 #include <linux/mutex.h>
+#include <linux/rcupdate.h>
 
 struct srcu_struct_array {
 	int c[2];
@@ -60,18 +61,10 @@ int __init_srcu_struct(struct srcu_struct *sp, const char *name,
 	__init_srcu_struct((sp), #sp, &__srcu_key); \
 })
 
-# define srcu_read_acquire(sp) \
-		lock_acquire(&(sp)->dep_map, 0, 0, 2, 1, NULL, _THIS_IP_)
-# define srcu_read_release(sp) \
-		lock_release(&(sp)->dep_map, 1, _THIS_IP_)
-
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 int init_srcu_struct(struct srcu_struct *sp);
 
-# define srcu_read_acquire(sp)  do { } while (0)
-# define srcu_read_release(sp)  do { } while (0)
-
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 void cleanup_srcu_struct(struct srcu_struct *sp);
@@ -90,12 +83,29 @@ long srcu_batches_completed(struct srcu_struct *sp);
  * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,
  * this assumes we are in an SRCU read-side critical section unless it can
  * prove otherwise.
+ *
+ * Note that if the CPU is in the idle loop from an RCU point of view
+ * (ie: that we are in the section between rcu_idle_enter() and
+ * rcu_idle_exit()) then srcu_read_lock_held() returns false even if
+ * the CPU did an srcu_read_lock().  The reason for this is that RCU
+ * ignores CPUs that are in such a section, considering these as in
+ * extended quiescent state, so such a CPU is effectively never in an
+ * RCU read-side critical section regardless of what RCU primitives it
+ * invokes.  This state of affairs is required --- we need to keep an
+ * RCU-free window in idle where the CPU may possibly enter into low
+ * power mode. This way we can notice an extended quiescent state to
+ * other CPUs that started a grace period. Otherwise we would delay any
+ * grace period as long as we run in the idle task.
  */
 static inline int srcu_read_lock_held(struct srcu_struct *sp)
 {
-	if (debug_locks)
-		return lock_is_held(&sp->dep_map);
-	return 1;
+	if (rcu_is_cpu_idle())
+		return 0;
+
+	if (!debug_locks)
+		return 1;
+
+	return lock_is_held(&sp->dep_map);
 }
 
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
@@ -150,7 +160,7 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 {
 	int retval = __srcu_read_lock(sp);
 
-	srcu_read_acquire(sp);
+	rcu_lock_acquire(&(sp)->dep_map);
 	return retval;
 }
 
@@ -164,7 +174,7 @@ static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
 	__releases(sp)
 {
-	srcu_read_release(sp);
+	rcu_lock_release(&(sp)->dep_map);
 	__srcu_read_unlock(sp, idx);
 }
 

commit 73d4da4d360136826b36f78f5cf72b29da82c8a6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 16 10:50:54 2010 -0700

    rcu: Upgrade srcu_read_lock() docbook about SRCU grace periods
    
    It is illegal to wait for an SRCU grace period while within the
    corresponding flavor of SRCU read-side critical section.  Therefore,
    this commit updates the srcu_read_lock() docbook accordingly.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 6f456a720ff0..58971e891f48 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -139,7 +139,12 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
  * @sp: srcu_struct in which to register the new reader.
  *
  * Enter an SRCU read-side critical section.  Note that SRCU read-side
- * critical sections may be nested.
+ * critical sections may be nested.  However, it is illegal to
+ * call anything that waits on an SRCU grace period for the same
+ * srcu_struct, whether directly or indirectly.  Please note that
+ * one way to indirectly wait on an SRCU grace period is to acquire
+ * a mutex that is held elsewhere while calling synchronize_srcu() or
+ * synchronize_srcu_expedited().
  */
 static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
 {

commit ca5ecddfa8fcbd948c95530e7e817cee9fb43a3d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 28 14:39:09 2010 -0700

    rcu: define __rcu address space modifier for sparse
    
    This commit provides definitions for the __rcu annotation defined earlier.
    This annotation permits sparse to check for correct use of RCU-protected
    pointers.  If a pointer that is annotated with __rcu is accessed
    directly (as opposed to via rcu_dereference(), rcu_assign_pointer(),
    or one of their variants), sparse can be made to complain.  To enable
    such complaints, use the new default-disabled CONFIG_SPARSE_RCU_POINTER
    kernel configuration option.  Please note that these sparse complaints are
    intended to be a debugging aid, -not- a code-style-enforcement mechanism.
    
    There are special rcu_dereference_protected() and rcu_access_pointer()
    accessors for use when RCU read-side protection is not required, for
    example, when no other CPU has access to the data structure in question
    or while the current CPU hold the update-side lock.
    
    This patch also updates a number of docbook comments that were showing
    their age.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christopher Li <sparse@chrisli.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4d5d2f546dbf..6f456a720ff0 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -108,12 +108,31 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 /**
- * srcu_dereference - fetch SRCU-protected pointer with checking
+ * srcu_dereference_check - fetch SRCU-protected pointer for later dereferencing
+ * @p: the pointer to fetch and protect for later dereferencing
+ * @sp: pointer to the srcu_struct, which is used to check that we
+ *	really are in an SRCU read-side critical section.
+ * @c: condition to check for update-side use
  *
- * Makes rcu_dereference_check() do the dirty work.
+ * If PROVE_RCU is enabled, invoking this outside of an RCU read-side
+ * critical section will result in an RCU-lockdep splat, unless @c evaluates
+ * to 1.  The @c argument will normally be a logical expression containing
+ * lockdep_is_held() calls.
  */
-#define srcu_dereference(p, sp) \
-		rcu_dereference_check(p, srcu_read_lock_held(sp))
+#define srcu_dereference_check(p, sp, c) \
+	__rcu_dereference_check((p), srcu_read_lock_held(sp) || (c), __rcu)
+
+/**
+ * srcu_dereference - fetch SRCU-protected pointer for later dereferencing
+ * @p: the pointer to fetch and protect for later dereferencing
+ * @sp: pointer to the srcu_struct, which is used to check that we
+ *	really are in an SRCU read-side critical section.
+ *
+ * Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
+ * is enabled, invoking this outside of an RCU read-side critical
+ * section will result in an RCU-lockdep splat.
+ */
+#define srcu_dereference(p, sp) srcu_dereference_check((p), (sp), 0)
 
 /**
  * srcu_read_lock - register a new reader for an SRCU-protected structure.

commit d14aada8e20bdf81ffd43f433b123972cf575b32
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 19 22:24:22 2010 -0700

    rcu: make SRCU usable in modules
    
    Add a #include for mutex.h to allow SRCU to be more easily used in
    kernel modules.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 9c01f1022428..4d5d2f546dbf 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -27,6 +27,8 @@
 #ifndef _LINUX_SRCU_H
 #define _LINUX_SRCU_H
 
+#include <linux/mutex.h>
+
 struct srcu_struct_array {
 	int c[2];
 };

commit d20200b591f59847ab6a5c23507084a7d29e23c5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 30 10:52:21 2010 -0700

    rcu: Fix bogus CONFIG_PROVE_LOCKING in comments to reflect reality.
    
    It is CONFIG_DEBUG_LOCK_ALLOC rather than CONFIG_PROVE_LOCKING, so fix it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4d5ecb222af9..9c01f1022428 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -84,8 +84,8 @@ long srcu_batches_completed(struct srcu_struct *sp);
 /**
  * srcu_read_lock_held - might we be in SRCU read-side critical section?
  *
- * If CONFIG_PROVE_LOCKING is selected and enabled, returns nonzero iff in
- * an SRCU read-side critical section.  In absence of CONFIG_PROVE_LOCKING,
+ * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an SRCU
+ * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,
  * this assumes we are in an SRCU read-side critical section unless it can
  * prove otherwise.
  */

commit 0a135ba14d71fb84c691a5386aff5049691fe6d7
Merge: 4850f524b2c4 a29d8b8e2d81
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 3 07:34:18 2010 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu:
      percpu: add __percpu sparse annotations to what's left
      percpu: add __percpu sparse annotations to fs
      percpu: add __percpu sparse annotations to core kernel subsystems
      local_t: Remove leftover local.h
      this_cpu: Remove pageset_notifier
      this_cpu: Page allocator conversion
      percpu, x86: Generic inc / dec percpu instructions
      local_t: Move local.h include to ringbuffer.c and ring_buffer_benchmark.c
      module: Use this_cpu_xx to dynamically allocate counters
      local_t: Remove cpu_local_xx macros
      percpu: refactor the code in pcpu_[de]populate_chunk()
      percpu: remove compile warnings caused by __verify_pcpu_ptr()
      percpu: make accessors check for percpu pointer in sparse
      percpu: add __percpu for sparse.
      percpu: make access macros universal
      percpu: remove per_cpu__ prefix.

commit c26d34a5858f96a564c45048bf5f09319d2abad1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 22 17:04:46 2010 -0800

    rcu: Add lockdep-enabled variants of rcu_dereference()
    
    Make rcu_dereference() check for being in an RCU read-side
    critical section, and create rcu_dereference_bh(),
    rcu_dereference_sched(), and srcu_dereference() to check for the
    other flavors of RCU.  Also create rcu_dereference_raw() to
    avoid checking, and make rcu_dereference_check() use
    rcu_dereference_raw().
    
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1266887105-1528-2-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index adbe1670b366..3084f80909cd 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -105,6 +105,14 @@ static inline int srcu_read_lock_held(struct srcu_struct *sp)
 
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
+/**
+ * srcu_dereference - fetch SRCU-protected pointer with checking
+ *
+ * Makes rcu_dereference_check() do the dirty work.
+ */
+#define srcu_dereference(p, sp) \
+		rcu_dereference_check(p, srcu_read_lock_held(sp))
+
 /**
  * srcu_read_lock - register a new reader for an SRCU-protected structure.
  * @sp: srcu_struct in which to register the new reader.

commit 632ee200130899252508c478ad0e808222573fbc
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 22 17:04:45 2010 -0800

    rcu: Introduce lockdep-based checking to RCU read-side primitives
    
    Inspection is proving insufficient to catch all RCU misuses,
    which is understandable given that rcu_dereference() might be
    protected by any of four different flavors of RCU (RCU, RCU-bh,
    RCU-sched, and SRCU), and might also/instead be protected by any
    of a number of locking primitives. It is therefore time to
    enlist the aid of lockdep.
    
    This set of patches is inspired by earlier work by Peter
    Zijlstra and Thomas Gleixner, and takes the following approach:
    
    o       Set up separate lockdep classes for RCU, RCU-bh, and RCU-sched.
    
    o       Set up separate lockdep classes for each instance of SRCU.
    
    o       Create primitives that check for being in an RCU read-side
            critical section.  These return exact answers if lockdep is
            fully enabled, but if unsure, report being in an RCU read-side
            critical section.  (We want to avoid false positives!)
            The primitives are:
    
            For RCU: rcu_read_lock_held(void)
    
            For RCU-bh: rcu_read_lock_bh_held(void)
    
            For RCU-sched: rcu_read_lock_sched_held(void)
    
            For SRCU: srcu_read_lock_held(struct srcu_struct *sp)
    
    o       Add rcu_dereference_check(), which takes a second argument
            in which one places a boolean expression based on the above
            primitives and/or lockdep_is_held().
    
    o       A new kernel configuration parameter, CONFIG_PROVE_RCU, enables
            rcu_dereference_check().  This depends on CONFIG_PROVE_LOCKING,
            and should be quite helpful during the transition period while
            CONFIG_PROVE_RCU-unaware patches are in flight.
    
    The existing rcu_dereference() primitive does no checking, but
    upcoming patches will change that.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1266887105-1528-1-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4765d97dcafb..adbe1670b366 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -35,6 +35,9 @@ struct srcu_struct {
 	int completed;
 	struct srcu_struct_array *per_cpu_ref;
 	struct mutex mutex;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 };
 
 #ifndef CONFIG_PREEMPT
@@ -43,12 +46,92 @@ struct srcu_struct {
 #define srcu_barrier()
 #endif /* #else #ifndef CONFIG_PREEMPT */
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+
+int __init_srcu_struct(struct srcu_struct *sp, const char *name,
+		       struct lock_class_key *key);
+
+#define init_srcu_struct(sp) \
+({ \
+	static struct lock_class_key __srcu_key; \
+	\
+	__init_srcu_struct((sp), #sp, &__srcu_key); \
+})
+
+# define srcu_read_acquire(sp) \
+		lock_acquire(&(sp)->dep_map, 0, 0, 2, 1, NULL, _THIS_IP_)
+# define srcu_read_release(sp) \
+		lock_release(&(sp)->dep_map, 1, _THIS_IP_)
+
+#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
 int init_srcu_struct(struct srcu_struct *sp);
+
+# define srcu_read_acquire(sp)  do { } while (0)
+# define srcu_read_release(sp)  do { } while (0)
+
+#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
 void cleanup_srcu_struct(struct srcu_struct *sp);
-int srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
-void srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
+int __srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
+void __srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
 void synchronize_srcu_expedited(struct srcu_struct *sp);
 long srcu_batches_completed(struct srcu_struct *sp);
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+
+/**
+ * srcu_read_lock_held - might we be in SRCU read-side critical section?
+ *
+ * If CONFIG_PROVE_LOCKING is selected and enabled, returns nonzero iff in
+ * an SRCU read-side critical section.  In absence of CONFIG_PROVE_LOCKING,
+ * this assumes we are in an SRCU read-side critical section unless it can
+ * prove otherwise.
+ */
+static inline int srcu_read_lock_held(struct srcu_struct *sp)
+{
+	if (debug_locks)
+		return lock_is_held(&sp->dep_map);
+	return 1;
+}
+
+#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
+static inline int srcu_read_lock_held(struct srcu_struct *sp)
+{
+	return 1;
+}
+
+#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
+/**
+ * srcu_read_lock - register a new reader for an SRCU-protected structure.
+ * @sp: srcu_struct in which to register the new reader.
+ *
+ * Enter an SRCU read-side critical section.  Note that SRCU read-side
+ * critical sections may be nested.
+ */
+static inline int srcu_read_lock(struct srcu_struct *sp) __acquires(sp)
+{
+	int retval = __srcu_read_lock(sp);
+
+	srcu_read_acquire(sp);
+	return retval;
+}
+
+/**
+ * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
+ * @sp: srcu_struct in which to unregister the old reader.
+ * @idx: return value from corresponding srcu_read_lock().
+ *
+ * Exit an SRCU read-side critical section.
+ */
+static inline void srcu_read_unlock(struct srcu_struct *sp, int idx)
+	__releases(sp)
+{
+	srcu_read_release(sp);
+	__srcu_read_unlock(sp, idx);
+}
+
 #endif

commit 43cf38eb5cea91245502df3fcee4dbfc1c74dd1c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 2 14:38:57 2010 +0900

    percpu: add __percpu sparse annotations to core kernel subsystems
    
    Add __percpu sparse annotations to core subsystems.
    
    These annotations are to make sparse consider percpu variables to be
    in a different address space and warn if accessed without going
    through percpu accessors.  This patch doesn't affect normal builds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-mm@kvack.org
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Eric Biederman <ebiederm@xmission.com>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 4765d97dcafb..41eedccc962c 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -33,7 +33,7 @@ struct srcu_struct_array {
 
 struct srcu_struct {
 	int completed;
-	struct srcu_struct_array *per_cpu_ref;
+	struct srcu_struct_array __percpu *per_cpu_ref;
 	struct mutex mutex;
 };
 

commit 0cd397d33608ae6c97d2ee6c8c43462b419b7e26
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Oct 25 19:03:51 2009 -0700

    rcu: Add synchronize_srcu_expedited()
    
    This patch creates a synchronize_srcu_expedited() that uses
    synchronize_sched_expedited() where synchronize_srcu()
    uses synchronize_sched().  The synchronize_srcu() and
    synchronize_srcu_expedited() functions become one-liners that
    pass synchronize_sched() or synchronize_sched_expedited(),
    repectively, to a new __synchronize_srcu() function.
    
    While in the file, move the EXPORT_SYMBOL_GPL()s to immediately
    follow the corresponding functions.
    
    Requested-by: Avi Kivity <avi@redhat.com>
    Tested-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Josh Triplett <josh@joshtriplett.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    Cc: avi@redhat.com
    LKML-Reference: <12565226354038-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index aca0eee53930..4765d97dcafb 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -48,6 +48,7 @@ void cleanup_srcu_struct(struct srcu_struct *sp);
 int srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
+void synchronize_srcu_expedited(struct srcu_struct *sp);
 long srcu_batches_completed(struct srcu_struct *sp);
 
 #endif

commit e6a92013ba458804161c0c5b6d134d82204dc233
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Wed Oct 4 02:17:05 2006 -0700

    [PATCH] SRCU: report out-of-memory errors
    
    Currently the init_srcu_struct() routine has no way to report out-of-memory
    errors.  This patch (as761) makes it return -ENOMEM when the per-cpu data
    allocation fails.
    
    The patch also makes srcu_init_notifier_head() report a BUG if a notifier
    head can't be initialized.  Perhaps it should return -ENOMEM instead, but
    in the most likely cases where this might occur I don't think any recovery
    is possible.  Notifier chains generally are not created dynamically.
    
    [akpm@osdl.org: avoid statement-with-side-effect in macro]
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 8a45367b5f3a..aca0eee53930 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -43,7 +43,7 @@ struct srcu_struct {
 #define srcu_barrier()
 #endif /* #else #ifndef CONFIG_PREEMPT */
 
-void init_srcu_struct(struct srcu_struct *sp);
+int init_srcu_struct(struct srcu_struct *sp);
 void cleanup_srcu_struct(struct srcu_struct *sp);
 int srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);

commit eabc069401bcf45bcc3f19e643017bf761780aa8
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Wed Oct 4 02:17:04 2006 -0700

    [PATCH] Add SRCU-based notifier chains
    
    This patch (as751) adds a new type of notifier chain, based on the SRCU
    (Sleepable Read-Copy Update) primitives recently added to the kernel.  An
    SRCU notifier chain is much like a blocking notifier chain, in that it must
    be called in process context and its callout routines are allowed to sleep.
     The difference is that the chain's links are protected by the SRCU
    mechanism rather than by an rw-semaphore, so calling the chain has
    extremely low overhead: no memory barriers and no cache-line bouncing.  On
    the other hand, unregistering from the chain is expensive and the chain
    head requires special runtime initialization (plus cleanup if it is to be
    deallocated).
    
    SRCU notifiers are appropriate for notifiers that will be called very
    frequently and for which unregistration occurs very seldom.  The proposed
    "task notifier" scheme qualifies, as may some of the network notifiers.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Acked-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index 947fdab2ddb0..8a45367b5f3a 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -24,6 +24,9 @@
  *
  */
 
+#ifndef _LINUX_SRCU_H
+#define _LINUX_SRCU_H
+
 struct srcu_struct_array {
 	int c[2];
 };
@@ -46,4 +49,5 @@ int srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
 void srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
 void synchronize_srcu(struct srcu_struct *sp);
 long srcu_batches_completed(struct srcu_struct *sp);
-void cleanup_srcu_struct(struct srcu_struct *sp);
+
+#endif

commit 621934ee7ed5b073c7fd638b347e632c53572761
Author: Paul E. McKenney <paulmck@us.ibm.com>
Date:   Wed Oct 4 02:17:02 2006 -0700

    [PATCH] srcu-3: RCU variant permitting read-side blocking
    
    Updated patch adding a variant of RCU that permits sleeping in read-side
    critical sections.  SRCU is as follows:
    
    o       Each use of SRCU creates its own srcu_struct, and each
            srcu_struct has its own set of grace periods.  This is
            critical, as it prevents one subsystem with a blocking
            reader from holding up SRCU grace periods for other
            subsystems.
    
    o       The SRCU primitives (srcu_read_lock(), srcu_read_unlock(),
            and synchronize_srcu()) all take a pointer to a srcu_struct.
    
    o       The SRCU primitives must be called from process context.
    
    o       srcu_read_lock() returns an int that must be passed to
            the matching srcu_read_unlock().  Realtime RCU avoids the
            need for this by storing the state in the task struct,
            but SRCU needs to allow a given code path to pass through
            multiple SRCU domains -- storing state in the task struct
            would therefore require either arbitrary space in the
            task struct or arbitrary limits on SRCU nesting.  So I
            kicked the state-storage problem up to the caller.
    
            Of course, it is not permitted to call synchronize_srcu()
            while in an SRCU read-side critical section.
    
    o       There is no call_srcu().  It would not be hard to implement
            one, but it seems like too easy a way to OOM the system.
            (Hey, we have enough trouble with call_rcu(), which does
            -not- permit readers to sleep!!!)  So, if you want it,
            please tell me why...
    
    [josht@us.ibm.com: sparse notation]
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Josh Triplett <josh@freedesktop.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/srcu.h b/include/linux/srcu.h
new file mode 100644
index 000000000000..947fdab2ddb0
--- /dev/null
+++ b/include/linux/srcu.h
@@ -0,0 +1,49 @@
+/*
+ * Sleepable Read-Copy Update mechanism for mutual exclusion
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) IBM Corporation, 2006
+ *
+ * Author: Paul McKenney <paulmck@us.ibm.com>
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 		Documentation/RCU/ *.txt
+ *
+ */
+
+struct srcu_struct_array {
+	int c[2];
+};
+
+struct srcu_struct {
+	int completed;
+	struct srcu_struct_array *per_cpu_ref;
+	struct mutex mutex;
+};
+
+#ifndef CONFIG_PREEMPT
+#define srcu_barrier() barrier()
+#else /* #ifndef CONFIG_PREEMPT */
+#define srcu_barrier()
+#endif /* #else #ifndef CONFIG_PREEMPT */
+
+void init_srcu_struct(struct srcu_struct *sp);
+void cleanup_srcu_struct(struct srcu_struct *sp);
+int srcu_read_lock(struct srcu_struct *sp) __acquires(sp);
+void srcu_read_unlock(struct srcu_struct *sp, int idx) __releases(sp);
+void synchronize_srcu(struct srcu_struct *sp);
+long srcu_batches_completed(struct srcu_struct *sp);
+void cleanup_srcu_struct(struct srcu_struct *sp);
