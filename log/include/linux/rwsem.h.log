commit de8f5e4f2dc1f032b46afda0a78cab5456974f89
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Mar 21 12:26:01 2020 +0100

    lockdep: Introduce wait-type checks
    
    Extend lockdep to validate lock wait-type context.
    
    The current wait-types are:
    
            LD_WAIT_FREE,           /* wait free, rcu etc.. */
            LD_WAIT_SPIN,           /* spin loops, raw_spinlock_t etc.. */
            LD_WAIT_CONFIG,         /* CONFIG_PREEMPT_LOCK, spinlock_t etc.. */
            LD_WAIT_SLEEP,          /* sleeping locks, mutex_t etc.. */
    
    Where lockdep validates that the current lock (the one being acquired)
    fits in the current wait-context (as generated by the held stack).
    
    This ensures that there is no attempt to acquire mutexes while holding
    spinlocks, to acquire spinlocks while holding raw_spinlocks and so on. In
    other words, its a more fancy might_sleep().
    
    Obviously RCU made the entire ordeal more complex than a simple single
    value test because RCU can be acquired in (pretty much) any context and
    while it presents a context to nested locks it is not the same as it
    got acquired in.
    
    Therefore its necessary to split the wait_type into two values, one
    representing the acquire (outer) and one representing the nested context
    (inner). For most 'normal' locks these two are the same.
    
    [ To make static initialization easier we have the rule that:
      .outer == INV means .outer == .inner; because INV == 0. ]
    
    It further means that its required to find the minimal .inner of the held
    stack to compare against the outer of the new lock; because while 'normal'
    RCU presents a CONFIG type to nested locks, if it is taken while already
    holding a SPIN type it obviously doesn't relax the rules.
    
    Below is an example output generated by the trivial test code:
    
      raw_spin_lock(&foo);
      spin_lock(&bar);
      spin_unlock(&bar);
      raw_spin_unlock(&foo);
    
     [ BUG: Invalid wait context ]
     -----------------------------
     swapper/0/1 is trying to lock:
     ffffc90000013f20 (&bar){....}-{3:3}, at: kernel_init+0xdb/0x187
     other info that might help us debug this:
     1 lock held by swapper/0/1:
      #0: ffffc90000013ee0 (&foo){+.+.}-{2:2}, at: kernel_init+0xd1/0x187
    
    The way to read it is to look at the new -{n,m} part in the lock
    description; -{3:3} for the attempted lock, and try and match that up to
    the held locks, which in this case is the one: -{2,2}.
    
    This tells that the acquiring lock requires a more relaxed environment than
    presented by the lock stack.
    
    Currently only the normal locks and RCU are converted, the rest of the
    lockdep users defaults to .inner = INV which is ignored. More conversions
    can be done when desired.
    
    The check for spinlock_t nesting is not enabled by default. It's a separate
    config option for now as there are known problems which are currently
    addressed. The config option allows to identify these problems and to
    verify that the solutions found are indeed solving them.
    
    The config switch will be removed and the checks will permanently enabled
    once the vast majority of issues has been addressed.
    
    [ bigeasy: Move LD_WAIT_FREE,â€¦ out of CONFIG_LOCKDEP to avoid compile
               failure with CONFIG_DEBUG_SPINLOCK + !CONFIG_LOCKDEP]
    [ tglx: Add the config option ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.427089655@linutronix.de

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8a418d9eeb7a..7e5b2a4eb560 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -65,7 +65,11 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 /* Common initializer macros and functions */
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define __RWSEM_DEP_MAP_INIT(lockname) , .dep_map = { .name = #lockname }
+# define __RWSEM_DEP_MAP_INIT(lockname)			\
+	, .dep_map = {					\
+		.name = #lockname,			\
+		.wait_type_inner = LD_WAIT_SLEEP,	\
+	}
 #else
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif

commit bcba67cd806800fa8e973ac49dbc7d2d8fb3e55e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Feb 4 09:34:37 2020 +0100

    locking/rwsem: Remove RWSEM_OWNER_UNKNOWN
    
    Remove the now unused RWSEM_OWNER_UNKNOWN hack. This hack breaks
    PREEMPT_RT and getting rid of it was the entire motivation for
    re-writing the percpu rwsem.
    
    The biggest problem is that it is fundamentally incompatible with any
    form of Priority Inheritance, any exclusively held lock must have a
    distinct owner.
    
    Requested-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Juri Lelli <juri.lelli@redhat.com>
    Link: https://lkml.kernel.org/r/20200204092228.GP14946@hirez.programming.kicks-ass.net

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 00d6054687dd..8a418d9eeb7a 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -53,12 +53,6 @@ struct rw_semaphore {
 #endif
 };
 
-/*
- * Setting all bits of the owner field except bit 0 will indicate
- * that the rwsem is writer-owned with an unknown owner.
- */
-#define RWSEM_OWNER_UNKNOWN	(-2L)
-
 /* In all implementations count != 0 means locked */
 static inline int rwsem_is_locked(struct rw_semaphore *sem)
 {

commit fce45cd41101f1a9620267146b21f09b3454d8db
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Jul 28 21:47:35 2019 -0700

    locking/rwsem: Check for operations on an uninitialized rwsem
    
    Currently rwsems is the only locking primitive that lacks this
    debug feature. Add it under CONFIG_DEBUG_RWSEMS and do the magic
    checking in the locking fastpath (trylock) operation such that
    we cover all cases. The unlocking part is pretty straightforward.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: mingo@kernel.org
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Link: https://lkml.kernel.org/r/20190729044735.9632-1-dave@stgolabs.net

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 9d9c663987d8..00d6054687dd 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -45,6 +45,9 @@ struct rw_semaphore {
 #endif
 	raw_spinlock_t wait_lock;
 	struct list_head wait_list;
+#ifdef CONFIG_DEBUG_RWSEMS
+	void *magic;
+#endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
 #endif
@@ -73,6 +76,12 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif
 
+#ifdef CONFIG_DEBUG_RWSEMS
+# define __DEBUG_RWSEM_INITIALIZER(lockname) , .magic = &lockname
+#else
+# define __DEBUG_RWSEM_INITIALIZER(lockname)
+#endif
+
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED
 #else
@@ -85,6 +94,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
 	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
 	  __RWSEM_OPT_INIT(name)				\
+	  __DEBUG_RWSEM_INITIALIZER(name)			\
 	  __RWSEM_DEP_MAP_INIT(name) }
 
 #define DECLARE_RWSEM(name) \

commit 387b14684f94483cbbb72843db406ec9a8d0d6d2
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Wed Apr 10 08:32:41 2019 -0300

    docs: locking: convert docs to ReST and rename to *.rst
    
    Convert the locking documents to ReST and add them to the
    kernel development book where it belongs.
    
    Most of the stuff here is just to make Sphinx to properly
    parse the text file, as they're already in good shape,
    not requiring massive changes in order to be parsed.
    
    The conversion is actually:
      - add blank lines and identation in order to identify paragraphs;
      - fix tables markups;
      - add some lists markups;
      - mark literal blocks;
      - adjust title markups.
    
    At its new index.rst, let's add a :orphan: while this is not linked to
    the main index.rst file, in order to avoid build warnings.
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Federico Vaga <federico.vaga@vaga.pv.it>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index e401358c4e7e..9d9c663987d8 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -160,7 +160,7 @@ extern void downgrade_write(struct rw_semaphore *sem);
  * static then another method for expressing nested locking is
  * the explicit definition of lock class keys and the use of
  * lockdep_set_class() at lock initialization time.
- * See Documentation/locking/lockdep-design.txt for more details.)
+ * See Documentation/locking/lockdep-design.rst for more details.)
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);

commit 94a9717b3c40e77a54e4afacd8f19a9a86bfeead
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:12 2019 -0400

    locking/rwsem: Make rwsem->owner an atomic_long_t
    
    The rwsem->owner contains not just the task structure pointer, it also
    holds some flags for storing the current state of the rwsem. Some of
    the flags may have to be atomically updated. To reflect the new reality,
    the owner is now changed to an atomic_long_t type.
    
    New helper functions are added to properly separate out the task
    structure pointer and the embedded flags.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-14-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index bb76e82398b2..e401358c4e7e 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -35,10 +35,11 @@
 struct rw_semaphore {
 	atomic_long_t count;
 	/*
-	 * Write owner or one of the read owners. Can be used as a
-	 * speculative check to see if the owner is running on the cpu.
+	 * Write owner or one of the read owners as well flags regarding
+	 * the current state of the rwsem. Can be used as a speculative
+	 * check to see if the write owner is running on the cpu.
 	 */
-	struct task_struct *owner;
+	atomic_long_t owner;
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 	struct optimistic_spin_queue osq; /* spinner MCS lock */
 #endif
@@ -53,7 +54,7 @@ struct rw_semaphore {
  * Setting all bits of the owner field except bit 0 will indicate
  * that the rwsem is writer-owned with an unknown owner.
  */
-#define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-2L)
+#define RWSEM_OWNER_UNKNOWN	(-2L)
 
 /* In all implementations count != 0 means locked */
 static inline int rwsem_is_locked(struct rw_semaphore *sem)
@@ -80,7 +81,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 
 #define __RWSEM_INITIALIZER(name)				\
 	{ __RWSEM_INIT_COUNT(name),				\
-	  .owner = NULL,					\
+	  .owner = ATOMIC_LONG_INIT(0),				\
 	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
 	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
 	  __RWSEM_OPT_INIT(name)				\

commit 02f1082b003a0cd48f48f12533d969cdbf1c2b63
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:10 2019 -0400

    locking/rwsem: Clarify usage of owner's nonspinaable bit
    
    Bit 1 of sem->owner (RWSEM_ANONYMOUSLY_OWNED) is used to designate an
    anonymous owner - readers or an anonymous writer. The setting of this
    anonymous bit is used as an indicator that optimistic spinning cannot
    be done on this rwsem.
    
    With the upcoming reader optimistic spinning patches, a reader-owned
    rwsem can be spinned on for a limit period of time. We still need
    this bit to indicate a rwsem is nonspinnable, but not setting this
    bit loses its meaning that the owner is known. So rename the bit
    to RWSEM_NONSPINNABLE to clarify its meaning.
    
    This patch also fixes a DEBUG_RWSEMS_WARN_ON() bug in __up_write().
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-12-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 148983e21d47..bb76e82398b2 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -50,7 +50,7 @@ struct rw_semaphore {
 };
 
 /*
- * Setting bit 1 of the owner field but not bit 0 will indicate
+ * Setting all bits of the owner field except bit 0 will indicate
  * that the rwsem is writer-owned with an unknown owner.
  */
 #define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-2L)

commit c71fd893f614f205dbc050d60299cc5496491c19
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:00 2019 -0400

    locking/rwsem: Make owner available even if !CONFIG_RWSEM_SPIN_ON_OWNER
    
    The owner field in the rw_semaphore structure is used primarily for
    optimistic spinning. However, identifying the rwsem owner can also be
    helpful in debugging as well as tracing locking related issues when
    analyzing crash dump. The owner field may also store state information
    that can be important to the operation of the rwsem.
    
    So the owner field is now made a permanent member of the rw_semaphore
    structure irrespective of CONFIG_RWSEM_SPIN_ON_OWNER.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-2-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 2ea18a3def04..148983e21d47 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -34,12 +34,12 @@
  */
 struct rw_semaphore {
 	atomic_long_t count;
-#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 	/*
-	 * Write owner. Used as a speculative check to see
-	 * if the owner is running on the cpu.
+	 * Write owner or one of the read owners. Can be used as a
+	 * speculative check to see if the owner is running on the cpu.
 	 */
 	struct task_struct *owner;
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 	struct optimistic_spin_queue osq; /* spinner MCS lock */
 #endif
 	raw_spinlock_t wait_lock;
@@ -73,13 +73,14 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 #endif
 
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
-#define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED, .owner = NULL
+#define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED
 #else
 #define __RWSEM_OPT_INIT(lockname)
 #endif
 
 #define __RWSEM_INITIALIZER(name)				\
 	{ __RWSEM_INIT_COUNT(name),				\
+	  .owner = NULL,					\
 	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
 	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
 	  __RWSEM_OPT_INIT(name)				\

commit 364f784f048c984721986db90c95ca8350213c91
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:20 2019 -0400

    locking/rwsem: Optimize rwsem structure for uncontended lock acquisition
    
    For an uncontended rwsem, count and owner are the only fields a task
    needs to touch when acquiring the rwsem. So they are put next to each
    other to increase the chance that they will share the same cacheline.
    
    On a ThunderX2 99xx (arm64) system with 32K L1 cache and 256K L2
    cache, a rwsem locking microbenchmark with one locking thread was
    run to write-lock and write-unlock an array of rwsems separated 2
    cachelines apart in a 1M byte memory block. The locking rates (kops/s)
    of the microbenchmark when the rwsems are at various "long" (8-byte)
    offsets from beginning of the cacheline before and after the patch were
    as follows:
    
      Cacheline Offset   Pre-patch    Post-patch
      ----------------   ---------    ----------
            0             17,449        16,588
            1             17,450        16,465
            2             17,450        16,460
            3             17,453        16,462
            4             14,867        16,471
            5             14,867        16,470
            6             14,853        16,464
            7             14,867        13,172
    
    Before the patch, the count and owner are 4 "long"s apart. After the
    patch, they are only 1 "long" apart.
    
    The rwsem data have to be loaded from the L3 cache for each access. It
    can be seen that the locking rates are more consistent after the patch
    than before. Note that for this particular system, the performance
    drop happens whenever the count and owner are at an odd multiples of
    "long"s apart. No performance drop was observed when only a single rwsem
    was used (hot cache). So the drop is likely just an idiosyncrasy of the
    cache architecture of this chip than an inherent problem with the patch.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-12-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index b44e533235c7..2ea18a3def04 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -20,21 +20,30 @@
 #include <linux/osq_lock.h>
 #endif
 
-struct rw_semaphore;
-
-/* All arch specific implementations share the same struct */
+/*
+ * For an uncontended rwsem, count and owner are the only fields a task
+ * needs to touch when acquiring the rwsem. So they are put next to each
+ * other to increase the chance that they will share the same cacheline.
+ *
+ * In a contended rwsem, the owner is likely the most frequently accessed
+ * field in the structure as the optimistic waiter that holds the osq lock
+ * will spin on owner. For an embedded rwsem, other hot fields in the
+ * containing structure should be moved further away from the rwsem to
+ * reduce the chance that they will share the same cacheline causing
+ * cacheline bouncing problem.
+ */
 struct rw_semaphore {
 	atomic_long_t count;
-	struct list_head wait_list;
-	raw_spinlock_t wait_lock;
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
-	struct optimistic_spin_queue osq; /* spinner MCS lock */
 	/*
 	 * Write owner. Used as a speculative check to see
 	 * if the owner is running on the cpu.
 	 */
 	struct task_struct *owner;
+	struct optimistic_spin_queue osq; /* spinner MCS lock */
 #endif
+	raw_spinlock_t wait_lock;
+	struct list_head wait_list;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
 #endif

commit 12a30a7fc142a123c61da9623bd824d95d36c12e
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:12 2019 -0400

    locking/rwsem: Move rwsem internal function declarations to rwsem-xadd.h
    
    We don't need to expose rwsem internal functions which are not supposed
    to be called directly from other kernel code.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-4-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 0fc41062c649..b44e533235c7 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -46,13 +46,6 @@ struct rw_semaphore {
  */
 #define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-2L)
 
-extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_down_read_failed_killable(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_down_write_failed_killable(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
-extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
-
 /* In all implementations count != 0 means locked */
 static inline int rwsem_is_locked(struct rw_semaphore *sem)
 {

commit 390a0c62c23cb026cd4664a66f6f45fed3a215f6
Author: Waiman Long <longman@redhat.com>
Date:   Fri Mar 22 10:30:07 2019 -0400

    locking/rwsem: Remove rwsem-spinlock.c & use rwsem-xadd.c for all archs
    
    Currently, we have two different implementation of rwsem:
    
     1) CONFIG_RWSEM_GENERIC_SPINLOCK (rwsem-spinlock.c)
     2) CONFIG_RWSEM_XCHGADD_ALGORITHM (rwsem-xadd.c)
    
    As we are going to use a single generic implementation for rwsem-xadd.c
    and no architecture-specific code will be needed, there is no point
    in keeping two different implementations of rwsem. In most cases, the
    performance of rwsem-spinlock.c will be worse. It also doesn't get all
    the performance tuning and optimizations that had been implemented in
    rwsem-xadd.c over the years.
    
    For simplication, we are going to remove rwsem-spinlock.c and make all
    architectures use a single implementation of rwsem - rwsem-xadd.c.
    
    All references to RWSEM_GENERIC_SPINLOCK and RWSEM_XCHGADD_ALGORITHM
    in the code are removed.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-riscv@lists.infradead.org
    Cc: linux-um@lists.infradead.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: nios2-dev@lists.rocketboards.org
    Cc: openrisc@lists.librecores.org
    Cc: uclinux-h8-devel@lists.sourceforge.jp
    Link: https://lkml.kernel.org/r/20190322143008.21313-3-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 6e56006b2cb6..0fc41062c649 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -22,10 +22,6 @@
 
 struct rw_semaphore;
 
-#ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
-#include <linux/rwsem-spinlock.h> /* use a generic implementation */
-#define __RWSEM_INIT_COUNT(name)	.count = RWSEM_UNLOCKED_VALUE
-#else
 /* All arch specific implementations share the same struct */
 struct rw_semaphore {
 	atomic_long_t count;
@@ -65,7 +61,6 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 
 #define RWSEM_UNLOCKED_VALUE		0L
 #define __RWSEM_INIT_COUNT(name)	.count = ATOMIC_LONG_INIT(RWSEM_UNLOCKED_VALUE)
-#endif
 
 /* Common initializer macros and functions */
 

commit 46ad0840b1584b92b5ff2cc3ed0b011dd6b8e0f1
Author: Waiman Long <longman@redhat.com>
Date:   Fri Mar 22 10:30:06 2019 -0400

    locking/rwsem: Remove arch specific rwsem files
    
    As the generic rwsem-xadd code is using the appropriate acquire and
    release versions of the atomic operations, the arch specific rwsem.h
    files will not be that much faster than the generic code as long as the
    atomic functions are properly implemented. So we can remove those arch
    specific rwsem.h and stop building asm/rwsem.h to reduce maintenance
    effort.
    
    Currently, only x86, alpha and ia64 have implemented architecture
    specific fast paths. I don't have access to alpha and ia64 systems for
    testing, but they are legacy systems that are not likely to be updated
    to the latest kernel anyway.
    
    By using a rwsem microbenchmark, the total locking rates on a 4-socket
    56-core 112-thread x86-64 system before and after the patch were as
    follows (mixed means equal # of read and write locks):
    
                          Before Patch              After Patch
       # of Threads  wlock   rlock   mixed     wlock   rlock   mixed
       ------------  -----   -----   -----     -----   -----   -----
            1        29,201  30,143  29,458    28,615  30,172  29,201
            2         6,807  13,299   1,171     7,725  15,025   1,804
            4         6,504  12,755   1,520     7,127  14,286   1,345
            8         6,762  13,412     764     6,826  13,652     726
           16         6,693  15,408     662     6,599  15,938     626
           32         6,145  15,286     496     5,549  15,487     511
           64         5,812  15,495      60     5,858  15,572      60
    
    There were some run-to-run variations for the multi-thread tests. For
    x86-64, using the generic C code fast path seems to be a little bit
    faster than the assembly version with low lock contention.  Looking at
    the assembly version of the fast paths, there are assembly to/from C
    code wrappers that save and restore all the callee-clobbered registers
    (7 registers on x86-64). The assembly generated from the generic C
    code doesn't need to do that. That may explain the slight performance
    gain here.
    
    The generic asm rwsem.h can also be merged into kernel/locking/rwsem.h
    with no code change as no other code other than those under
    kernel/locking needs to access the internal rwsem macros and functions.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-riscv@lists.infradead.org
    Cc: linux-um@lists.infradead.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: nios2-dev@lists.rocketboards.org
    Cc: openrisc@lists.librecores.org
    Cc: uclinux-h8-devel@lists.sourceforge.jp
    Link: https://lkml.kernel.org/r/20190322143008.21313-2-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 67dbb57508b1..6e56006b2cb6 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -57,15 +57,13 @@ extern struct rw_semaphore *rwsem_down_write_failed_killable(struct rw_semaphore
 extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
 extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
 
-/* Include the arch specific part */
-#include <asm/rwsem.h>
-
 /* In all implementations count != 0 means locked */
 static inline int rwsem_is_locked(struct rw_semaphore *sem)
 {
 	return atomic_long_read(&sem->count) != 0;
 }
 
+#define RWSEM_UNLOCKED_VALUE		0L
 #define __RWSEM_INIT_COUNT(name)	.count = ATOMIC_LONG_INIT(RWSEM_UNLOCKED_VALUE)
 #endif
 

commit 925b9cd1b89a94b7124d128c80dfc48f78a63098
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 6 16:18:34 2018 -0400

    locking/rwsem: Make owner store task pointer of last owning reader
    
    Currently, when a reader acquires a lock, it only sets the
    RWSEM_READER_OWNED bit in the owner field. The other bits are simply
    not used. When debugging hanging cases involving rwsems and readers,
    the owner value does not provide much useful information at all.
    
    This patch modifies the current behavior to always store the task_struct
    pointer of the last rwsem-acquiring reader in a reader-owned rwsem. This
    may be useful in debugging rwsem hanging cases especially if only one
    reader is involved. However, the task in the owner field may not the
    real owner or one of the real owners at all when the owner value is
    examined, for example, in a crash dump. So it is just an additional
    hint about the past history.
    
    If CONFIG_DEBUG_RWSEMS=y is enabled, the owner field will be checked at
    unlock time too to make sure the task pointer value is valid. That does
    have a slight performance cost and so is only enabled as part of that
    debug option.
    
    From the performance point of view, it is expected that the changes
    shouldn't have any noticeable performance impact. A rwsem microbenchmark
    (with 48 worker threads and 1:1 reader/writer ratio) was ran on a
    2-socket 24-core 48-thread Haswell system.  The locking rates on a
    4.19-rc1 based kernel were as follows:
    
      1) Unpatched kernel:                          543.3 kops/s
      2) Patched kernel:                            549.2 kops/s
      3) Patched kernel (CONFIG_DEBUG_RWSEMS on):   546.6 kops/s
    
    There was actually a slight increase in performance (1.1%) in this
    particular case. Maybe it was caused by the elimination of a branch or
    just a testing noise. Turning on the CONFIG_DEBUG_RWSEMS option also
    had less than the expected impact on performance.
    
    The least significant 2 bits of the owner value are now used to designate
    the rwsem is readers owned and the owners are anonymous.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1536265114-10842-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index ab93b6eae696..67dbb57508b1 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -45,10 +45,10 @@ struct rw_semaphore {
 };
 
 /*
- * Setting bit 0 of the owner field with other non-zero bits will indicate
+ * Setting bit 1 of the owner field but not bit 0 will indicate
  * that the rwsem is writer-owned with an unknown owner.
  */
-#define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-1L)
+#define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-2L)
 
 extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_read_failed_killable(struct rw_semaphore *sem);

commit 5a817641f68a6399a5fac8b7d2da67a73698ffed
Author: Waiman Long <longman@redhat.com>
Date:   Tue May 15 17:49:51 2018 -0400

    locking/percpu-rwsem: Annotate rwsem ownership transfer by setting RWSEM_OWNER_UNKNOWN
    
    The filesystem freezing code needs to transfer ownership of a rwsem
    embedded in a percpu-rwsem from the task that does the freezing to
    another one that does the thawing by calling percpu_rwsem_release()
    after freezing and percpu_rwsem_acquire() before thawing.
    
    However, the new rwsem debug code runs afoul with this scheme by warning
    that the task that releases the rwsem isn't the one that acquires it,
    as reported by Amir Goldstein:
    
      DEBUG_LOCKS_WARN_ON(sem->owner != get_current())
      WARNING: CPU: 1 PID: 1401 at /home/amir/build/src/linux/kernel/locking/rwsem.c:133 up_write+0x59/0x79
    
      Call Trace:
       percpu_up_write+0x1f/0x28
       thaw_super_locked+0xdf/0x120
       do_vfs_ioctl+0x270/0x5f1
       ksys_ioctl+0x52/0x71
       __x64_sys_ioctl+0x16/0x19
       do_syscall_64+0x5d/0x167
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    To work properly with the rwsem debug code, we need to annotate that the
    rwsem ownership is unknown during the tranfer period until a brave soul
    comes forward to acquire the ownership. During that period, optimistic
    spinning will be disabled.
    
    Reported-by: Amir Goldstein <amir73il@gmail.com>
    Tested-by: Amir Goldstein <amir73il@gmail.com>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Theodore Y. Ts'o <tytso@mit.edu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-fsdevel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1526420991-21213-3-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 56707d5ff6ad..ab93b6eae696 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -44,6 +44,12 @@ struct rw_semaphore {
 #endif
 };
 
+/*
+ * Setting bit 0 of the owner field with other non-zero bits will indicate
+ * that the rwsem is writer-owned with an unknown owner.
+ */
+#define RWSEM_OWNER_UNKNOWN	((struct task_struct *)-1L)
+
 extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_read_failed_killable(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 0ad7318ff299..dfa34d803439 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /* rwsem.h: R/W semaphores, public interface
  *
  * Written by David Howells (dhowells@redhat.com).

commit 76f8507f7a6442215df19de74f07eabca2462f1e
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri Sep 29 19:06:38 2017 +0300

    locking/rwsem: Add down_read_killable()
    
    Similar to down_read() and down_write_killable(),
    add killable version of down_read(), based on
    __down_read_killable() function, added in previous
    patches.
    
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: avagin@virtuozzo.com
    Cc: davem@davemloft.net
    Cc: fenghua.yu@intel.com
    Cc: gorcunov@virtuozzo.com
    Cc: heiko.carstens@de.ibm.com
    Cc: hpa@zytor.com
    Cc: ink@jurassic.park.msu.ru
    Cc: mattst88@gmail.com
    Cc: rientjes@google.com
    Cc: rth@twiddle.net
    Cc: schwidefsky@de.ibm.com
    Cc: tony.luck@intel.com
    Cc: viro@zeniv.linux.org.uk
    Link: http://lkml.kernel.org/r/150670119884.23930.2585570605960763239.stgit@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 0ad7318ff299..6ac8ee5f15dd 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -111,6 +111,7 @@ static inline int rwsem_is_contended(struct rw_semaphore *sem)
  * lock for reading
  */
 extern void down_read(struct rw_semaphore *sem);
+extern int __must_check down_read_killable(struct rw_semaphore *sem);
 
 /*
  * trylock for reading -- returns 1 if successful, 0 if contention

commit 83ced169d9a01f22eb39f1fcc1f89ad9d223238f
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Mon Jun 19 21:02:26 2017 +0300

    locking/rwsem-xadd: Add killable versions of rwsem_down_read_failed()
    
    Rename rwsem_down_read_failed() in __rwsem_down_read_failed_common()
    and teach it to abort waiting in case of pending signals and killable
    state argument passed.
    
    Note, that we shouldn't wake anybody up in EINTR path, as:
    
    We check for (waiter.task) under spinlock before we go to out_nolock
    path. Current task wasn't able to be woken up, so there are
    a writer, owning the sem, or a writer, which is the first waiter.
    In the both cases we shouldn't wake anybody. If there is a writer,
    owning the sem, and we were the only waiter, remove RWSEM_WAITING_BIAS,
    as there are no waiters anymore.
    
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: avagin@virtuozzo.com
    Cc: davem@davemloft.net
    Cc: fenghua.yu@intel.com
    Cc: gorcunov@virtuozzo.com
    Cc: heiko.carstens@de.ibm.com
    Cc: hpa@zytor.com
    Cc: ink@jurassic.park.msu.ru
    Cc: mattst88@gmail.com
    Cc: rth@twiddle.net
    Cc: schwidefsky@de.ibm.com
    Cc: tony.luck@intel.com
    Link: http://lkml.kernel.org/r/149789534632.9059.2901382369609922565.stgit@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index dd1d14250340..0ad7318ff299 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -44,6 +44,7 @@ struct rw_semaphore {
 };
 
 extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
+extern struct rw_semaphore *rwsem_down_read_failed_killable(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_write_failed_killable(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);

commit 8ee62b1870be8e630158701632a533d0378e15b8
Author: Jason Low <jason.low2@hpe.com>
Date:   Fri Jun 3 22:26:02 2016 -0700

    locking/rwsem: Convert sem->count to 'atomic_long_t'
    
    Convert the rwsem count variable to an atomic_long_t since we use it
    as an atomic variable. This also allows us to remove the
    rwsem_atomic_{add,update}() "abstraction" which would now be an unnecesary
    level of indirection. In follow up patches, we also remove the
    rwsem_atomic_{add,update}() definitions across the various architectures.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jason Low <jason.low2@hpe.com>
    [ Build warning fixes on various architectures. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Cc: Terry Rudd <terry.rudd@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Waiman Long <Waiman.Long@hpe.com>
    Link: http://lkml.kernel.org/r/1465017963-4839-2-git-send-email-jason.low2@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index d37fbb34d06f..dd1d14250340 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -23,10 +23,11 @@ struct rw_semaphore;
 
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
 #include <linux/rwsem-spinlock.h> /* use a generic implementation */
+#define __RWSEM_INIT_COUNT(name)	.count = RWSEM_UNLOCKED_VALUE
 #else
 /* All arch specific implementations share the same struct */
 struct rw_semaphore {
-	long count;
+	atomic_long_t count;
 	struct list_head wait_list;
 	raw_spinlock_t wait_lock;
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
@@ -54,9 +55,10 @@ extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
 /* In all implementations count != 0 means locked */
 static inline int rwsem_is_locked(struct rw_semaphore *sem)
 {
-	return sem->count != 0;
+	return atomic_long_read(&sem->count) != 0;
 }
 
+#define __RWSEM_INIT_COUNT(name)	.count = ATOMIC_LONG_INIT(RWSEM_UNLOCKED_VALUE)
 #endif
 
 /* Common initializer macros and functions */
@@ -74,7 +76,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 #endif
 
 #define __RWSEM_INITIALIZER(name)				\
-	{ .count = RWSEM_UNLOCKED_VALUE,			\
+	{ __RWSEM_INIT_COUNT(name),				\
 	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
 	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
 	  __RWSEM_OPT_INIT(name)				\

commit 887bddfa90c79957d61067cd54a10087be0c8b23
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu May 26 00:04:58 2016 -0400

    add down_write_killable_nested()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index d1c12d160ace..d37fbb34d06f 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -156,6 +156,7 @@ extern void downgrade_write(struct rw_semaphore *sem);
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);
+extern int down_write_killable_nested(struct rw_semaphore *sem, int subclass);
 extern void _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest_lock);
 
 # define down_write_nest_lock(sem, nest_lock)			\
@@ -176,6 +177,7 @@ extern void up_read_non_owner(struct rw_semaphore *sem);
 # define down_read_nested(sem, subclass)		down_read(sem)
 # define down_write_nest_lock(sem, nest_lock)	down_write(sem)
 # define down_write_nested(sem, subclass)	down_write(sem)
+# define down_write_killable_nested(sem, subclass)	down_write_killable(sem)
 # define down_read_non_owner(sem)		down_read(sem)
 # define up_read_non_owner(sem)			up_read(sem)
 #endif

commit 916633a403702549d37ea353e63a68e5b0dc27ad
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Apr 7 17:12:31 2016 +0200

    locking/rwsem: Provide down_write_killable()
    
    Now that all the architectures implement the necessary glue code
    we can introduce down_write_killable(). The only difference wrt. regular
    down_write() is that the slow path waits in TASK_KILLABLE state and the
    interruption by the fatal signal is reported as -EINTR to the caller.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Signed-off-by: Jason Low <jason.low2@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1460041951-22347-12-git-send-email-mhocko@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 7d7ae029dac5..d1c12d160ace 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -118,6 +118,7 @@ extern int down_read_trylock(struct rw_semaphore *sem);
  * lock for writing
  */
 extern void down_write(struct rw_semaphore *sem);
+extern int __must_check down_write_killable(struct rw_semaphore *sem);
 
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention

commit d47996082f52baa0ca8b48d26b3cbef5ede70a73
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Apr 7 17:12:26 2016 +0200

    locking/rwsem: Introduce basis for down_write_killable()
    
    Introduce a generic implementation necessary for down_write_killable().
    
    This is a trivial extension of the already existing down_write() call
    which can be interrupted by SIGKILL.  This patch doesn't provide
    down_write_killable() yet because arches have to provide the necessary
    pieces before.
    
    rwsem_down_write_failed() which is a generic slow path for the
    write lock is extended to take a task state and renamed to
    __rwsem_down_write_failed_common(). The return value is either a valid
    semaphore pointer or ERR_PTR(-EINTR).
    
    rwsem_down_write_failed_killable() is exported as a new way to wait for
    the lock and be killable.
    
    For rwsem-spinlock implementation the current __down_write() it updated
    in a similar way as __rwsem_down_write_failed_common() except it doesn't
    need new exports just visible __down_write_killable().
    
    Architectures which are not using the generic rwsem implementation are
    supposed to provide their __down_write_killable() implementation and
    use rwsem_down_write_failed_killable() for the slow path.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Signed-off-by: Jason Low <jason.low2@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1460041951-22347-7-git-send-email-mhocko@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8f498cdde280..7d7ae029dac5 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -14,6 +14,7 @@
 #include <linux/list.h>
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
+#include <linux/err.h>
 #ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #include <linux/osq_lock.h>
 #endif
@@ -43,6 +44,7 @@ struct rw_semaphore {
 
 extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
+extern struct rw_semaphore *rwsem_down_write_failed_killable(struct rw_semaphore *sem);
 extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
 extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
 

commit 214e0aed639ef40987bf6159fad303171a6de31e
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Wed Jul 30 13:41:55 2014 -0700

    locking/Documentation: Move locking related docs into Documentation/locking/
    
    Specifically:
      Documentation/locking/lockdep-design.txt
      Documentation/locking/lockstat.txt
      Documentation/locking/mutex-design.txt
      Documentation/locking/rt-mutex-design.txt
      Documentation/locking/rt-mutex.txt
      Documentation/locking/spinlocks.txt
      Documentation/locking/ww-mutex-design.txt
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: jason.low2@hp.com
    Cc: aswin@hp.com
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Lubomir Rintel <lkundrak@v3.sk>
    Cc: Masanari Iida <standby24x7@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: fengguang.wu@intel.com
    Link: http://lkml.kernel.org/r/1406752916-3341-6-git-send-email-davidlohr@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 035d3c57fc8a..8f498cdde280 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -149,7 +149,7 @@ extern void downgrade_write(struct rw_semaphore *sem);
  * static then another method for expressing nested locking is
  * the explicit definition of lock class keys and the use of
  * lockdep_set_class() at lock initialization time.
- * See Documentation/lockdep-design.txt for more details.)
+ * See Documentation/locking/lockdep-design.txt for more details.)
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);

commit 5db6c6fefb1ca0e81e3bd6dd8998bf51c453d823
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Fri Jul 11 14:00:06 2014 -0700

    locking/rwsem: Add CONFIG_RWSEM_SPIN_ON_OWNER
    
    Just like with mutexes (CONFIG_MUTEX_SPIN_ON_OWNER),
    encapsulate the dependencies for rwsem optimistic spinning.
    No logical changes here as it continues to depend on both
    SMP and the XADD algorithm variant.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Acked-by: Jason Low <jason.low2@hp.com>
    [ Also make it depend on ARCH_SUPPORTS_ATOMIC_RMW. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1405112406-13052-2-git-send-email-davidlohr@hp.com
    Cc: aswin@hp.com
    Cc: Chris Mason <clm@fb.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 716807f0eb2d..035d3c57fc8a 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -14,7 +14,9 @@
 #include <linux/list.h>
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #include <linux/osq_lock.h>
+#endif
 
 struct rw_semaphore;
 
@@ -26,7 +28,7 @@ struct rw_semaphore {
 	long count;
 	struct list_head wait_list;
 	raw_spinlock_t wait_lock;
-#ifdef CONFIG_SMP
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 	struct optimistic_spin_queue osq; /* spinner MCS lock */
 	/*
 	 * Write owner. Used as a speculative check to see
@@ -63,7 +65,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif
 
-#if defined(CONFIG_SMP) && !defined(CONFIG_RWSEM_GENERIC_SPINLOCK)
+#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
 #define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED, .owner = NULL
 #else
 #define __RWSEM_OPT_INIT(lockname)

commit ce069fc920e5734558b3d9cbef1ab06cf01ee793
Author: Jason Low <jason.low2@hp.com>
Date:   Mon Jul 14 10:27:52 2014 -0700

    locking/rwsem: Reduce the size of struct rw_semaphore
    
    Recent optimistic spinning additions to rwsem provide significant performance
    benefits on many workloads on large machines. The cost of it was increasing
    the size of the rwsem structure by up to 128 bits.
    
    However, now that the previous patches in this series bring the overhead of
    struct optimistic_spin_queue to 32 bits, this patch reorders some fields in
    struct rw_semaphore such that we can reduce the overhead of the rwsem structure
    by 64 bits (on 64 bit systems).
    
    The extra overhead required for rwsem optimistic spinning would now be up
    to 8 additional bytes instead of up to 16 bytes. Additionally, the size of
    rwsem would now be more in line with mutexes.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1405358872-3732-6-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 25cd9aa2f3d7..716807f0eb2d 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -24,15 +24,15 @@ struct rw_semaphore;
 /* All arch specific implementations share the same struct */
 struct rw_semaphore {
 	long count;
-	raw_spinlock_t wait_lock;
 	struct list_head wait_list;
+	raw_spinlock_t wait_lock;
 #ifdef CONFIG_SMP
+	struct optimistic_spin_queue osq; /* spinner MCS lock */
 	/*
 	 * Write owner. Used as a speculative check to see
 	 * if the owner is running on the cpu.
 	 */
 	struct task_struct *owner;
-	struct optimistic_spin_queue osq; /* spinner MCS lock */
 #endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
@@ -64,21 +64,18 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 #endif
 
 #if defined(CONFIG_SMP) && !defined(CONFIG_RWSEM_GENERIC_SPINLOCK)
-#define __RWSEM_INITIALIZER(name)			\
-	{ RWSEM_UNLOCKED_VALUE,				\
-	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
-	  LIST_HEAD_INIT((name).wait_list),		\
-	  NULL, /* owner */				\
-	  OSQ_LOCK_UNLOCKED /* osq */			\
-	  __RWSEM_DEP_MAP_INIT(name) }
+#define __RWSEM_OPT_INIT(lockname) , .osq = OSQ_LOCK_UNLOCKED, .owner = NULL
 #else
-#define __RWSEM_INITIALIZER(name)			\
-	{ RWSEM_UNLOCKED_VALUE,				\
-	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
-	  LIST_HEAD_INIT((name).wait_list)		\
-	  __RWSEM_DEP_MAP_INIT(name) }
+#define __RWSEM_OPT_INIT(lockname)
 #endif
 
+#define __RWSEM_INITIALIZER(name)				\
+	{ .count = RWSEM_UNLOCKED_VALUE,			\
+	  .wait_list = LIST_HEAD_INIT((name).wait_list),	\
+	  .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock)	\
+	  __RWSEM_OPT_INIT(name)				\
+	  __RWSEM_DEP_MAP_INIT(name) }
+
 #define DECLARE_RWSEM(name) \
 	struct rw_semaphore name = __RWSEM_INITIALIZER(name)
 

commit 4d9d951e6b5df85ccfca2c5bd8b4f5c71d256b65
Author: Jason Low <jason.low2@hp.com>
Date:   Mon Jul 14 10:27:50 2014 -0700

    locking/spinlocks/mcs: Introduce and use init macro and function for osq locks
    
    Currently, we initialize the osq lock by directly setting the lock's values. It
    would be preferable if we use an init macro to do the initialization like we do
    with other locks.
    
    This patch introduces and uses a macro and function for initializing the osq lock.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1405358872-3732-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 9fdcdd03507d..25cd9aa2f3d7 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -69,7 +69,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
 	  LIST_HEAD_INIT((name).wait_list),		\
 	  NULL, /* owner */				\
-	  { ATOMIC_INIT(OSQ_UNLOCKED_VAL) } /* osq */   \
+	  OSQ_LOCK_UNLOCKED /* osq */			\
 	  __RWSEM_DEP_MAP_INIT(name) }
 #else
 #define __RWSEM_INITIALIZER(name)			\

commit 90631822c5d307b5410500806e8ac3e63928aa3e
Author: Jason Low <jason.low2@hp.com>
Date:   Mon Jul 14 10:27:49 2014 -0700

    locking/spinlocks/mcs: Convert osq lock to atomic_t to reduce overhead
    
    The cancellable MCS spinlock is currently used to queue threads that are
    doing optimistic spinning. It uses per-cpu nodes, where a thread obtaining
    the lock would access and queue the local node corresponding to the CPU that
    it's running on. Currently, the cancellable MCS lock is implemented by using
    pointers to these nodes.
    
    In this patch, instead of operating on pointers to the per-cpu nodes, we
    store the CPU numbers in which the per-cpu nodes correspond to in atomic_t.
    A similar concept is used with the qspinlock.
    
    By operating on the CPU # of the nodes using atomic_t instead of pointers
    to those nodes, this can reduce the overhead of the cancellable MCS spinlock
    by 32 bits (on 64 bit systems).
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1405358872-3732-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index ba3f108ddea1..9fdcdd03507d 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -13,10 +13,9 @@
 #include <linux/kernel.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
-
 #include <linux/atomic.h>
+#include <linux/osq_lock.h>
 
-struct optimistic_spin_node;
 struct rw_semaphore;
 
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
@@ -33,7 +32,7 @@ struct rw_semaphore {
 	 * if the owner is running on the cpu.
 	 */
 	struct task_struct *owner;
-	struct optimistic_spin_node *osq; /* spinner MCS lock */
+	struct optimistic_spin_queue osq; /* spinner MCS lock */
 #endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
@@ -70,7 +69,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
 	  LIST_HEAD_INIT((name).wait_list),		\
 	  NULL, /* owner */				\
-	  NULL /* mcs lock */                           \
+	  { ATOMIC_INIT(OSQ_UNLOCKED_VAL) } /* osq */   \
 	  __RWSEM_DEP_MAP_INIT(name) }
 #else
 #define __RWSEM_INITIALIZER(name)			\

commit 046a619d8e9746fa4c0e29e8c6b78e16efc008a8
Author: Jason Low <jason.low2@hp.com>
Date:   Mon Jul 14 10:27:48 2014 -0700

    locking/spinlocks/mcs: Rename optimistic_spin_queue() to optimistic_spin_node()
    
    Currently, the per-cpu nodes structure for the cancellable MCS spinlock is
    named "optimistic_spin_queue". However, in a follow up patch in the series
    we will be introducing a new structure that serves as the new "handle" for
    the lock. It would make more sense if that structure is named
    "optimistic_spin_queue". Additionally, since the current use of the
    "optimistic_spin_queue" structure are  "nodes", it might be better if we
    rename them to "node" anyway.
    
    This preparatory patch renames all current "optimistic_spin_queue"
    to "optimistic_spin_node".
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1405358872-3732-2-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8d79708146aa..ba3f108ddea1 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -16,7 +16,7 @@
 
 #include <linux/atomic.h>
 
-struct optimistic_spin_queue;
+struct optimistic_spin_node;
 struct rw_semaphore;
 
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
@@ -33,7 +33,7 @@ struct rw_semaphore {
 	 * if the owner is running on the cpu.
 	 */
 	struct task_struct *owner;
-	struct optimistic_spin_queue *osq; /* spinner MCS lock */
+	struct optimistic_spin_node *osq; /* spinner MCS lock */
 #endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;

commit dbb5eafa23fdb7a00d64b04e23a3efc59d6474cb
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Mon May 19 17:27:57 2014 -0700

    locking/rwsem: Fix warnings for CONFIG_RWSEM_GENERIC_SPINLOCK
    
    Optimistic spinning is only used by the xadd variant
    of rw-semaphores. Make sure that we use the old version
    of the __RWSEM_INITIALIZER macro for systems that rely
    on the spinlock one, otherwise warnings can be triggered,
    such as the following reported on an arm box:
    
      ipc/ipcns_notifier.c:22:8: warning: excess elements in struct initializer [enabled by default]
      ipc/ipcns_notifier.c:22:8: warning: (near initialization for 'ipcns_chain.rwsem') [enabled by default]
      ipc/ipcns_notifier.c:22:8: warning: excess elements in struct initializer [enabled by default]
      ipc/ipcns_notifier.c:22:8: warning: (near initialization for 'ipcns_chain.rwsem') [enabled by default]
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1400545677.6399.10.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 3e108f154cb6..8d79708146aa 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -64,7 +64,7 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_RWSEM_GENERIC_SPINLOCK)
 #define __RWSEM_INITIALIZER(name)			\
 	{ RWSEM_UNLOCKED_VALUE,				\
 	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\

commit 4fc828e24cd9c385d3a44e1b499ec7fc70239d8a
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Fri May 2 11:24:15 2014 -0700

    locking/rwsem: Support optimistic spinning
    
    We have reached the point where our mutexes are quite fine tuned
    for a number of situations. This includes the use of heuristics
    and optimistic spinning, based on MCS locking techniques.
    
    Exclusive ownership of read-write semaphores are, conceptually,
    just about the same as mutexes, making them close cousins. To
    this end we need to make them both perform similarly, and
    right now, rwsems are simply not up to it. This was discovered
    by both reverting commit 4fc3f1d6 (mm/rmap, migration: Make
    rmap_walk_anon() and try_to_unmap_anon() more scalable) and
    similarly, converting some other mutexes (ie: i_mmap_mutex) to
    rwsems. This creates a situation where users have to choose
    between a rwsem and mutex taking into account this important
    performance difference. Specifically, biggest difference between
    both locks is when we fail to acquire a mutex in the fastpath,
    optimistic spinning comes in to play and we can avoid a large
    amount of unnecessary sleeping and overhead of moving tasks in
    and out of wait queue. Rwsems do not have such logic.
    
    This patch, based on the work from Tim Chen and I, adds support
    for write-side optimistic spinning when the lock is contended.
    It also includes support for the recently added cancelable MCS
    locking for adaptive spinning. Note that is is only applicable
    to the xadd method, and the spinlock rwsem variant remains intact.
    
    Allowing optimistic spinning before putting the writer on the wait
    queue reduces wait queue contention and provided greater chance
    for the rwsem to get acquired. With these changes, rwsem is on par
    with mutex. The performance benefits can be seen on a number of
    workloads. For instance, on a 8 socket, 80 core 64bit Westmere box,
    aim7 shows the following improvements in throughput:
    
     +--------------+---------------------+-----------------+
     |   Workload   | throughput-increase | number of users |
     +--------------+---------------------+-----------------+
     | alltests     | 20%                 | >1000           |
     | custom       | 27%, 60%            | 10-100, >1000   |
     | high_systime | 36%, 30%            | >100, >1000     |
     | shared       | 58%, 29%            | 10-100, >1000   |
     +--------------+---------------------+-----------------+
    
    There was also improvement on smaller systems, such as a quad-core
    x86-64 laptop running a 30Gb PostgreSQL (pgbench) workload for up
    to +60% in throughput for over 50 clients. Additionally, benefits
    were also noticed in exim (mail server) workloads. Furthermore, no
    performance regression have been seen at all.
    
    Based-on-work-from: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    [peterz: rej fixup due to comment patches, sched/rt.h header]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Cc: "Paul E.McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "Scott J Norton" <scott.norton@hp.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1399055055.6275.15.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 03f3b05e8ec1..3e108f154cb6 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -16,6 +16,7 @@
 
 #include <linux/atomic.h>
 
+struct optimistic_spin_queue;
 struct rw_semaphore;
 
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
@@ -23,9 +24,17 @@ struct rw_semaphore;
 #else
 /* All arch specific implementations share the same struct */
 struct rw_semaphore {
-	long			count;
-	raw_spinlock_t		wait_lock;
-	struct list_head	wait_list;
+	long count;
+	raw_spinlock_t wait_lock;
+	struct list_head wait_list;
+#ifdef CONFIG_SMP
+	/*
+	 * Write owner. Used as a speculative check to see
+	 * if the owner is running on the cpu.
+	 */
+	struct task_struct *owner;
+	struct optimistic_spin_queue *osq; /* spinner MCS lock */
+#endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
 #endif
@@ -55,11 +64,21 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif
 
+#ifdef CONFIG_SMP
+#define __RWSEM_INITIALIZER(name)			\
+	{ RWSEM_UNLOCKED_VALUE,				\
+	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
+	  LIST_HEAD_INIT((name).wait_list),		\
+	  NULL, /* owner */				\
+	  NULL /* mcs lock */                           \
+	  __RWSEM_DEP_MAP_INIT(name) }
+#else
 #define __RWSEM_INITIALIZER(name)			\
 	{ RWSEM_UNLOCKED_VALUE,				\
 	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
 	  LIST_HEAD_INIT((name).wait_list)		\
 	  __RWSEM_DEP_MAP_INIT(name) }
+#endif
 
 #define DECLARE_RWSEM(name) \
 	struct rw_semaphore name = __RWSEM_INITIALIZER(name)

commit 4a444b1f06d259ce938a47048840260f71a91c84
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Aug 30 10:05:22 2013 -0400

    rwsem: add rwsem_is_contended
    
    Btrfs needs a simple way to know if it needs to let go of it's read lock on a
    rwsem.  Introduce rwsem_is_contended to check to see if there are any waiters on
    this rwsem currently.  This is just a hueristic, it is meant to be light and not
    100% accurate and called by somebody already holding on to the rwsem in either
    read or write.  Thanks,
    
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 0616ffe45702..03f3b05e8ec1 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -74,6 +74,17 @@ do {								\
 	__init_rwsem((sem), #sem, &__key);			\
 } while (0)
 
+/*
+ * This is the same regardless of which rwsem implementation that is being used.
+ * It is just a heuristic meant to be called by somebody alreadying holding the
+ * rwsem to see if somebody from an incompatible type is wanting access to the
+ * lock.
+ */
+static inline int rwsem_is_contended(struct rw_semaphore *sem)
+{
+	return !list_empty(&sem->wait_list);
+}
+
 /*
  * lock for reading
  */

commit 84759c6d18c5144432781ddca037d929ee9db8a5
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Sep 21 21:43:05 2011 -0700

    Revert "rw_semaphore: remove up/down_read_non_owner"
    
    This reverts commit 11b80f459adaf91a712f95e7734a17655a36bf30.
    
    Bcache needs rw semaphores for cache coherency in writeback mode -
    writes have to take a read lock on a per cache device rw sem, and
    release it when the bio completes.
    
    But since this is for bios it's naturally not in the context of the
    process that originally took the lock.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    CC: Christoph Hellwig <hch@infradead.org>
    CC: David Howells <dhowells@redhat.com>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8da67d625e13..0616ffe45702 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -133,10 +133,20 @@ do {								\
 	_down_write_nest_lock(sem, &(nest_lock)->dep_map);	\
 } while (0);
 
+/*
+ * Take/release a lock when not the owner will release it.
+ *
+ * [ This API should be avoided as much as possible - the
+ *   proper abstraction for this case is completions. ]
+ */
+extern void down_read_non_owner(struct rw_semaphore *sem);
+extern void up_read_non_owner(struct rw_semaphore *sem);
 #else
 # define down_read_nested(sem, subclass)		down_read(sem)
 # define down_write_nest_lock(sem, nest_lock)	down_write(sem)
 # define down_write_nested(sem, subclass)	down_write(sem)
+# define down_read_non_owner(sem)		down_read(sem)
+# define up_read_non_owner(sem)			up_read(sem)
 #endif
 
 #endif /* _LINUX_RWSEM_H */

commit e65b9ad222c280c031bc8d3642cc38dd3026fe06
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Tue Jan 15 20:12:37 2013 +0100

    lockdep, rwsem: fix down_write_nest_lock() if !CONFIG_DEBUG_LOCK_ALLOC
    
    Commit 1b963c81b145 ("lockdep, rwsem: provide down_write_nest_lock()")
    contains a bug in a codepath when CONFIG_DEBUG_LOCK_ALLOC is disabled,
    which causes down_read() to be called instead of down_write() by mistake
    on such configurations.  Fix that.
    
    Reported-and-tested-by: Andrew Clayton <andrew@digital-domain.net>
    Reported-and-tested-by: Zlatko Calusic <zlatko.calusic@iskon.hr>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 413cc11e414a..8da67d625e13 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -135,7 +135,7 @@ do {								\
 
 #else
 # define down_read_nested(sem, subclass)		down_read(sem)
-# define down_write_nest_lock(sem, nest_lock)	down_read(sem)
+# define down_write_nest_lock(sem, nest_lock)	down_write(sem)
 # define down_write_nested(sem, subclass)	down_write(sem)
 #endif
 

commit 1b963c81b14509e330e0fe3218b645ece2738dc5
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Jan 11 14:31:56 2013 -0800

    lockdep, rwsem: provide down_write_nest_lock()
    
    down_write_nest_lock() provides a means to annotate locking scenario
    where an outer lock is guaranteed to serialize the order nested locks
    are being acquired.
    
    This is analogoue to already existing mutex_lock_nest_lock() and
    spin_lock_nest_lock().
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 54bd7cd7ecbd..413cc11e414a 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -125,8 +125,17 @@ extern void downgrade_write(struct rw_semaphore *sem);
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);
+extern void _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest_lock);
+
+# define down_write_nest_lock(sem, nest_lock)			\
+do {								\
+	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);	\
+	_down_write_nest_lock(sem, &(nest_lock)->dep_map);	\
+} while (0);
+
 #else
 # define down_read_nested(sem, subclass)		down_read(sem)
+# define down_write_nest_lock(sem, nest_lock)	down_read(sem)
 # define down_write_nested(sem, subclass)	down_write(sem)
 #endif
 

commit 9ffc93f203c18a70623f21950f1dd473c9ec48cd
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Remove all #inclusions of asm/system.h
    
    Remove all #inclusions of asm/system.h preparatory to splitting and killing
    it.  Performed with the following command:
    
    perl -p -i -e 's!^#\s*include\s*<asm/system[.]h>.*\n!!' `grep -Irl '^#\s*include\s*<asm/system[.]h>' *`
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 63d406554391..54bd7cd7ecbd 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -14,7 +14,6 @@
 #include <linux/list.h>
 #include <linux/spinlock.h>
 
-#include <asm/system.h>
 #include <linux/atomic.h>
 
 struct rw_semaphore;

commit ddb6c9b58a19edcfac93ac670b066c836ff729f1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 24 09:54:54 2010 +0100

    locking, rwsem: Annotate inner lock as raw
    
    There is no reason to allow the lock protecting rwsems (the
    ownerless variant) to be preemptible on -rt. Convert it to raw.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 6a6741440cb7..63d406554391 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -25,7 +25,7 @@ struct rw_semaphore;
 /* All arch specific implementations share the same struct */
 struct rw_semaphore {
 	long			count;
-	spinlock_t		wait_lock;
+	raw_spinlock_t		wait_lock;
 	struct list_head	wait_list;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
@@ -56,9 +56,11 @@ static inline int rwsem_is_locked(struct rw_semaphore *sem)
 # define __RWSEM_DEP_MAP_INIT(lockname)
 #endif
 
-#define __RWSEM_INITIALIZER(name) \
-	{ RWSEM_UNLOCKED_VALUE, __SPIN_LOCK_UNLOCKED(name.wait_lock),	\
-	  LIST_HEAD_INIT((name).wait_list) __RWSEM_DEP_MAP_INIT(name) }
+#define __RWSEM_INITIALIZER(name)			\
+	{ RWSEM_UNLOCKED_VALUE,				\
+	  __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),	\
+	  LIST_HEAD_INIT((name).wait_list)		\
+	  __RWSEM_DEP_MAP_INIT(name) }
 
 #define DECLARE_RWSEM(name) \
 	struct rw_semaphore name = __RWSEM_INITIALIZER(name)

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 77950dfa0a9e..6a6741440cb7 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -15,7 +15,7 @@
 #include <linux/spinlock.h>
 
 #include <asm/system.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 
 struct rw_semaphore;
 

commit 11b80f459adaf91a712f95e7734a17655a36bf30
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Jun 24 14:29:44 2011 -0400

    rw_semaphore: remove up/down_read_non_owner
    
    Now that the last users is gone these can be removed.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index a8afe9cd000c..77950dfa0a9e 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -124,19 +124,9 @@ extern void downgrade_write(struct rw_semaphore *sem);
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);
-/*
- * Take/release a lock when not the owner will release it.
- *
- * [ This API should be avoided as much as possible - the
- *   proper abstraction for this case is completions. ]
- */
-extern void down_read_non_owner(struct rw_semaphore *sem);
-extern void up_read_non_owner(struct rw_semaphore *sem);
 #else
 # define down_read_nested(sem, subclass)		down_read(sem)
 # define down_write_nested(sem, subclass)	down_write(sem)
-# define down_read_non_owner(sem)		down_read(sem)
-# define up_read_non_owner(sem)			up_read(sem)
 #endif
 
 #endif /* _LINUX_RWSEM_H */

commit d123375425d7df4b6081a631fc1203fceafa59b2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 21:32:01 2011 +0100

    rwsem: Remove redundant asmregparm annotation
    
    Peter Zijlstra pointed out, that the only user of asmregparm (x86) is
    compiling the kernel already with -mregparm=3. So the annotation of
    the rwsem functions is redundant. Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <alpine.LFD.2.00.1101262130450.31804@localhost6.localdomain6>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index f7c957f3165f..a8afe9cd000c 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -32,10 +32,10 @@ struct rw_semaphore {
 #endif
 };
 
-extern asmregparm struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
-extern asmregparm struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
-extern asmregparm struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
-extern asmregparm struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
+extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
+extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
+extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
+extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
 
 /* Include the arch specific part */
 #include <asm/rwsem.h>

commit aac72277fda6ef788bb8d5deaa502ce9b9b6e472
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 20:06:06 2011 +0000

    rwsem: Move duplicate function prototypes to linux/rwsem.h
    
    All architecture specific rwsem headers carry the same function
    prototypes. Just x86 adds asmregparm, which is an empty define on all
    other architectures. S390 has a stale rwsem_downgrade_write()
    prototype.
    
    Remove the duplicates and add the prototypes to linux/rwsem.h
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <20110126195833.970840140@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8b9c7e9f6910..f7c957f3165f 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -32,6 +32,11 @@ struct rw_semaphore {
 #endif
 };
 
+extern asmregparm struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
+extern asmregparm struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
+extern asmregparm struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
+extern asmregparm struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
+
 /* Include the arch specific part */
 #include <asm/rwsem.h>
 

commit 41e5887fa39ab272d9266a09cbefdef270e28b93
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 20:06:03 2011 +0000

    rwsem: Unify the duplicate rwsem_is_locked() inlines
    
    Instead of having the same implementation in each architecture, move
    it to linux/rwsem.h and remove the duplicates. It's unlikely that an
    arch will ever implement something different, but we can deal with
    that when it happens.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <20110126195833.876773757@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 8970c3e802e2..8b9c7e9f6910 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -34,6 +34,13 @@ struct rw_semaphore {
 
 /* Include the arch specific part */
 #include <asm/rwsem.h>
+
+/* In all implementations count != 0 means locked */
+static inline int rwsem_is_locked(struct rw_semaphore *sem)
+{
+	return sem->count != 0;
+}
+
 #endif
 
 /* Common initializer macros and functions */

commit 12249b34414dba7f386aadcf6be7ca36c6878300
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 20:06:00 2011 +0000

    rwsem: Move duplicate init macros and functions to linux/rwsem.h
    
    The rwsem initializers and related macros and functions are mostly the
    same. Some of them lack the lockdep initializer, but having it in
    place does not matter for architectures which do not support lockdep.
    
    powerpc, sparc, x86: No functional change
    
    sh, s390: Removes the duplicate init_rwsem (inline and #define)
    
    alpha, ia64, xtensa: Use the lockdep capable init function in
                         lib/rwsem.c which is just uninlining the init
                         function for the LOCKDEP=n case
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <20110126195833.771812729@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index e8be18edb664..8970c3e802e2 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -36,6 +36,31 @@ struct rw_semaphore {
 #include <asm/rwsem.h>
 #endif
 
+/* Common initializer macros and functions */
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __RWSEM_DEP_MAP_INIT(lockname) , .dep_map = { .name = #lockname }
+#else
+# define __RWSEM_DEP_MAP_INIT(lockname)
+#endif
+
+#define __RWSEM_INITIALIZER(name) \
+	{ RWSEM_UNLOCKED_VALUE, __SPIN_LOCK_UNLOCKED(name.wait_lock),	\
+	  LIST_HEAD_INIT((name).wait_list) __RWSEM_DEP_MAP_INIT(name) }
+
+#define DECLARE_RWSEM(name) \
+	struct rw_semaphore name = __RWSEM_INITIALIZER(name)
+
+extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
+			 struct lock_class_key *key);
+
+#define init_rwsem(sem)						\
+do {								\
+	static struct lock_class_key __key;			\
+								\
+	__init_rwsem((sem), #sem, &__key);			\
+} while (0)
+
 /*
  * lock for reading
  */

commit 1c8ed640d918290ddc1de5ada02ef6686a733c9f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 20:05:56 2011 +0000

    rwsem: Move duplicate struct rwsem declaration to linux/rwsem.h
    
    The difference between these declarations is the data type of the
    count member and the lack of lockdep in some architectures/
    
    long is equivivalent to signed long and the #ifdef guarded dep_map
    member does not hurt anyone.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <20110126195833.679641914@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 496296d12d62..e8be18edb664 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -22,7 +22,18 @@ struct rw_semaphore;
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
 #include <linux/rwsem-spinlock.h> /* use a generic implementation */
 #else
-#include <asm/rwsem.h> /* use an arch-specific implementation */
+/* All arch specific implementations share the same struct */
+struct rw_semaphore {
+	long			count;
+	spinlock_t		wait_lock;
+	struct list_head	wait_list;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+/* Include the arch specific part */
+#include <asm/rwsem.h>
 #endif
 
 /*

commit c16a87ce063f79e0ec7d25ce2950e1bc6db03c72
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 26 20:05:50 2011 +0000

    rwsem: Cleanup includes
    
    All rwsem implementations include the same headers. Include them from
    include/linux/rwsem.h
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    LKML-Reference: <20110126195833.483520950@linutronix.de>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index efd348fe8ca7..496296d12d62 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -11,6 +11,9 @@
 
 #include <linux/types.h>
 #include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+
 #include <asm/system.h>
 #include <asm/atomic.h>
 

commit 735643ee6cc5249bfac07fcad0946a5e7aff4423
Author: Robert P. J. Day <rpjday@crashcourse.ca>
Date:   Wed Apr 30 00:55:12 2008 -0700

    Remove "#ifdef __KERNEL__" checks from unexported headers
    
    Remove the "#ifdef __KERNEL__" tests from unexported header files in
    linux/include whose entire contents are wrapped in that preprocessor
    test.
    
    Signed-off-by: Robert P. J. Day <rpjday@crashcourse.ca>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 7b524b4109a0..efd348fe8ca7 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -9,8 +9,6 @@
 
 #include <linux/linkage.h>
 
-#ifdef __KERNEL__
-
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <asm/system.h>
@@ -90,5 +88,4 @@ extern void up_read_non_owner(struct rw_semaphore *sem);
 # define up_read_non_owner(sem)			up_read(sem)
 #endif
 
-#endif /* __KERNEL__ */
 #endif /* _LINUX_RWSEM_H */

commit 5fca80e8b4bf5d69b900115b14342133ce81d79e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 10 04:44:02 2006 -0700

    [PATCH] lockdep: add more rwsem.h documentation
    
    Add more documentation to rwsem.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 658afb37c3f5..7b524b4109a0 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -61,12 +61,25 @@ extern void downgrade_write(struct rw_semaphore *sem);
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 /*
- * nested locking:
+ * nested locking. NOTE: rwsems are not allowed to recurse
+ * (which occurs if the same task tries to acquire the same
+ * lock instance multiple times), but multiple locks of the
+ * same lock class might be taken, if the order of the locks
+ * is always the same. This ordering rule can be expressed
+ * to lockdep via the _nested() APIs, but enumerating the
+ * subclasses that are used. (If the nesting relationship is
+ * static then another method for expressing nested locking is
+ * the explicit definition of lock class keys and the use of
+ * lockdep_set_class() at lock initialization time.
+ * See Documentation/lockdep-design.txt for more details.)
  */
 extern void down_read_nested(struct rw_semaphore *sem, int subclass);
 extern void down_write_nested(struct rw_semaphore *sem, int subclass);
 /*
- * Take/release a lock when not the owner will release it:
+ * Take/release a lock when not the owner will release it.
+ *
+ * [ This API should be avoided as much as possible - the
+ *   proper abstraction for this case is completions. ]
  */
 extern void down_read_non_owner(struct rw_semaphore *sem);
 extern void up_read_non_owner(struct rw_semaphore *sem);

commit 4ea2176dfa714882e88180b474e4cbcd888b70af
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:53 2006 -0700

    [PATCH] lockdep: prove rwsem locking correctness
    
    Use the lock validator framework to prove rwsem locking correctness.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 93581534b915..658afb37c3f5 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -27,64 +27,55 @@ struct rw_semaphore;
 /*
  * lock for reading
  */
-static inline void down_read(struct rw_semaphore *sem)
-{
-	might_sleep();
-	__down_read(sem);
-}
+extern void down_read(struct rw_semaphore *sem);
 
 /*
  * trylock for reading -- returns 1 if successful, 0 if contention
  */
-static inline int down_read_trylock(struct rw_semaphore *sem)
-{
-	int ret;
-	ret = __down_read_trylock(sem);
-	return ret;
-}
+extern int down_read_trylock(struct rw_semaphore *sem);
 
 /*
  * lock for writing
  */
-static inline void down_write(struct rw_semaphore *sem)
-{
-	might_sleep();
-	__down_write(sem);
-}
+extern void down_write(struct rw_semaphore *sem);
 
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
-static inline int down_write_trylock(struct rw_semaphore *sem)
-{
-	int ret;
-	ret = __down_write_trylock(sem);
-	return ret;
-}
+extern int down_write_trylock(struct rw_semaphore *sem);
 
 /*
  * release a read lock
  */
-static inline void up_read(struct rw_semaphore *sem)
-{
-	__up_read(sem);
-}
+extern void up_read(struct rw_semaphore *sem);
 
 /*
  * release a write lock
  */
-static inline void up_write(struct rw_semaphore *sem)
-{
-	__up_write(sem);
-}
+extern void up_write(struct rw_semaphore *sem);
 
 /*
  * downgrade write lock to read lock
  */
-static inline void downgrade_write(struct rw_semaphore *sem)
-{
-	__downgrade_write(sem);
-}
+extern void downgrade_write(struct rw_semaphore *sem);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+/*
+ * nested locking:
+ */
+extern void down_read_nested(struct rw_semaphore *sem, int subclass);
+extern void down_write_nested(struct rw_semaphore *sem, int subclass);
+/*
+ * Take/release a lock when not the owner will release it:
+ */
+extern void down_read_non_owner(struct rw_semaphore *sem);
+extern void up_read_non_owner(struct rw_semaphore *sem);
+#else
+# define down_read_nested(sem, subclass)		down_read(sem)
+# define down_write_nested(sem, subclass)	down_write(sem)
+# define down_read_non_owner(sem)		down_read(sem)
+# define up_read_non_owner(sem)			up_read(sem)
+#endif
 
 #endif /* __KERNEL__ */
 #endif /* _LINUX_RWSEM_H */

commit c4e05116a2c4d8187127dbf77ab790aa57a47388
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:29 2006 -0700

    [PATCH] lockdep: clean up rwsems
    
    Clean up rwsems.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index f99fe90732ab..93581534b915 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -9,8 +9,6 @@
 
 #include <linux/linkage.h>
 
-#define RWSEM_DEBUG 0
-
 #ifdef __KERNEL__
 
 #include <linux/types.h>
@@ -26,23 +24,13 @@ struct rw_semaphore;
 #include <asm/rwsem.h> /* use an arch-specific implementation */
 #endif
 
-#ifndef rwsemtrace
-#if RWSEM_DEBUG
-extern void FASTCALL(rwsemtrace(struct rw_semaphore *sem, const char *str));
-#else
-#define rwsemtrace(SEM,FMT)
-#endif
-#endif
-
 /*
  * lock for reading
  */
 static inline void down_read(struct rw_semaphore *sem)
 {
 	might_sleep();
-	rwsemtrace(sem,"Entering down_read");
 	__down_read(sem);
-	rwsemtrace(sem,"Leaving down_read");
 }
 
 /*
@@ -51,9 +39,7 @@ static inline void down_read(struct rw_semaphore *sem)
 static inline int down_read_trylock(struct rw_semaphore *sem)
 {
 	int ret;
-	rwsemtrace(sem,"Entering down_read_trylock");
 	ret = __down_read_trylock(sem);
-	rwsemtrace(sem,"Leaving down_read_trylock");
 	return ret;
 }
 
@@ -63,9 +49,7 @@ static inline int down_read_trylock(struct rw_semaphore *sem)
 static inline void down_write(struct rw_semaphore *sem)
 {
 	might_sleep();
-	rwsemtrace(sem,"Entering down_write");
 	__down_write(sem);
-	rwsemtrace(sem,"Leaving down_write");
 }
 
 /*
@@ -74,9 +58,7 @@ static inline void down_write(struct rw_semaphore *sem)
 static inline int down_write_trylock(struct rw_semaphore *sem)
 {
 	int ret;
-	rwsemtrace(sem,"Entering down_write_trylock");
 	ret = __down_write_trylock(sem);
-	rwsemtrace(sem,"Leaving down_write_trylock");
 	return ret;
 }
 
@@ -85,9 +67,7 @@ static inline int down_write_trylock(struct rw_semaphore *sem)
  */
 static inline void up_read(struct rw_semaphore *sem)
 {
-	rwsemtrace(sem,"Entering up_read");
 	__up_read(sem);
-	rwsemtrace(sem,"Leaving up_read");
 }
 
 /*
@@ -95,9 +75,7 @@ static inline void up_read(struct rw_semaphore *sem)
  */
 static inline void up_write(struct rw_semaphore *sem)
 {
-	rwsemtrace(sem,"Entering up_write");
 	__up_write(sem);
-	rwsemtrace(sem,"Leaving up_write");
 }
 
 /*
@@ -105,9 +83,7 @@ static inline void up_write(struct rw_semaphore *sem)
  */
 static inline void downgrade_write(struct rw_semaphore *sem)
 {
-	rwsemtrace(sem,"Entering downgrade_write");
 	__downgrade_write(sem);
-	rwsemtrace(sem,"Leaving downgrade_write");
 }
 
 #endif /* __KERNEL__ */

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index bfb988885002..f99fe90732ab 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -13,7 +13,6 @@
 
 #ifdef __KERNEL__
 
-#include <linux/config.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <asm/system.h>

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
new file mode 100644
index 000000000000..bfb988885002
--- /dev/null
+++ b/include/linux/rwsem.h
@@ -0,0 +1,115 @@
+/* rwsem.h: R/W semaphores, public interface
+ *
+ * Written by David Howells (dhowells@redhat.com).
+ * Derived from asm-i386/semaphore.h
+ */
+
+#ifndef _LINUX_RWSEM_H
+#define _LINUX_RWSEM_H
+
+#include <linux/linkage.h>
+
+#define RWSEM_DEBUG 0
+
+#ifdef __KERNEL__
+
+#include <linux/config.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+
+struct rw_semaphore;
+
+#ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
+#include <linux/rwsem-spinlock.h> /* use a generic implementation */
+#else
+#include <asm/rwsem.h> /* use an arch-specific implementation */
+#endif
+
+#ifndef rwsemtrace
+#if RWSEM_DEBUG
+extern void FASTCALL(rwsemtrace(struct rw_semaphore *sem, const char *str));
+#else
+#define rwsemtrace(SEM,FMT)
+#endif
+#endif
+
+/*
+ * lock for reading
+ */
+static inline void down_read(struct rw_semaphore *sem)
+{
+	might_sleep();
+	rwsemtrace(sem,"Entering down_read");
+	__down_read(sem);
+	rwsemtrace(sem,"Leaving down_read");
+}
+
+/*
+ * trylock for reading -- returns 1 if successful, 0 if contention
+ */
+static inline int down_read_trylock(struct rw_semaphore *sem)
+{
+	int ret;
+	rwsemtrace(sem,"Entering down_read_trylock");
+	ret = __down_read_trylock(sem);
+	rwsemtrace(sem,"Leaving down_read_trylock");
+	return ret;
+}
+
+/*
+ * lock for writing
+ */
+static inline void down_write(struct rw_semaphore *sem)
+{
+	might_sleep();
+	rwsemtrace(sem,"Entering down_write");
+	__down_write(sem);
+	rwsemtrace(sem,"Leaving down_write");
+}
+
+/*
+ * trylock for writing -- returns 1 if successful, 0 if contention
+ */
+static inline int down_write_trylock(struct rw_semaphore *sem)
+{
+	int ret;
+	rwsemtrace(sem,"Entering down_write_trylock");
+	ret = __down_write_trylock(sem);
+	rwsemtrace(sem,"Leaving down_write_trylock");
+	return ret;
+}
+
+/*
+ * release a read lock
+ */
+static inline void up_read(struct rw_semaphore *sem)
+{
+	rwsemtrace(sem,"Entering up_read");
+	__up_read(sem);
+	rwsemtrace(sem,"Leaving up_read");
+}
+
+/*
+ * release a write lock
+ */
+static inline void up_write(struct rw_semaphore *sem)
+{
+	rwsemtrace(sem,"Entering up_write");
+	__up_write(sem);
+	rwsemtrace(sem,"Leaving up_write");
+}
+
+/*
+ * downgrade write lock to read lock
+ */
+static inline void downgrade_write(struct rw_semaphore *sem)
+{
+	rwsemtrace(sem,"Entering downgrade_write");
+	__downgrade_write(sem);
+	rwsemtrace(sem,"Leaving downgrade_write");
+}
+
+#endif /* __KERNEL__ */
+#endif /* _LINUX_RWSEM_H */
