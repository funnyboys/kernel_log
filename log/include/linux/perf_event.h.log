commit cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2
Merge: 2e63f6ce7ed2 065fcfd49763
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 16:27:18 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
    
     1) Allow setting bluetooth L2CAP modes via socket option, from Luiz
        Augusto von Dentz.
    
     2) Add GSO partial support to igc, from Sasha Neftin.
    
     3) Several cleanups and improvements to r8169 from Heiner Kallweit.
    
     4) Add IF_OPER_TESTING link state and use it when ethtool triggers a
        device self-test. From Andrew Lunn.
    
     5) Start moving away from custom driver versions, use the globally
        defined kernel version instead, from Leon Romanovsky.
    
     6) Support GRO vis gro_cells in DSA layer, from Alexander Lobakin.
    
     7) Allow hard IRQ deferral during NAPI, from Eric Dumazet.
    
     8) Add sriov and vf support to hinic, from Luo bin.
    
     9) Support Media Redundancy Protocol (MRP) in the bridging code, from
        Horatiu Vultur.
    
    10) Support netmap in the nft_nat code, from Pablo Neira Ayuso.
    
    11) Allow UDPv6 encapsulation of ESP in the ipsec code, from Sabrina
        Dubroca. Also add ipv6 support for espintcp.
    
    12) Lots of ReST conversions of the networking documentation, from Mauro
        Carvalho Chehab.
    
    13) Support configuration of ethtool rxnfc flows in bcmgenet driver,
        from Doug Berger.
    
    14) Allow to dump cgroup id and filter by it in inet_diag code, from
        Dmitry Yakunin.
    
    15) Add infrastructure to export netlink attribute policies to
        userspace, from Johannes Berg.
    
    16) Several optimizations to sch_fq scheduler, from Eric Dumazet.
    
    17) Fallback to the default qdisc if qdisc init fails because otherwise
        a packet scheduler init failure will make a device inoperative. From
        Jesper Dangaard Brouer.
    
    18) Several RISCV bpf jit optimizations, from Luke Nelson.
    
    19) Correct the return type of the ->ndo_start_xmit() method in several
        drivers, it's netdev_tx_t but many drivers were using
        'int'. From Yunjian Wang.
    
    20) Add an ethtool interface for PHY master/slave config, from Oleksij
        Rempel.
    
    21) Add BPF iterators, from Yonghang Song.
    
    22) Add cable test infrastructure, including ethool interfaces, from
        Andrew Lunn. Marvell PHY driver is the first to support this
        facility.
    
    23) Remove zero-length arrays all over, from Gustavo A. R. Silva.
    
    24) Calculate and maintain an explicit frame size in XDP, from Jesper
        Dangaard Brouer.
    
    25) Add CAP_BPF, from Alexei Starovoitov.
    
    26) Support terse dumps in the packet scheduler, from Vlad Buslov.
    
    27) Support XDP_TX bulking in dpaa2 driver, from Ioana Ciornei.
    
    28) Add devm_register_netdev(), from Bartosz Golaszewski.
    
    29) Minimize qdisc resets, from Cong Wang.
    
    30) Get rid of kernel_getsockopt and kernel_setsockopt in order to
        eliminate set_fs/get_fs calls. From Christoph Hellwig.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2517 commits)
      selftests: net: ip_defrag: ignore EPERM
      net_failover: fixed rollback in net_failover_open()
      Revert "tipc: Fix potential tipc_aead refcnt leak in tipc_crypto_rcv"
      Revert "tipc: Fix potential tipc_node refcnt leak in tipc_rcv"
      vmxnet3: allow rx flow hash ops only when rss is enabled
      hinic: add set_channels ethtool_ops support
      selftests/bpf: Add a default $(CXX) value
      tools/bpf: Don't use $(COMPILE.c)
      bpf, selftests: Use bpf_probe_read_kernel
      s390/bpf: Use bcr 0,%0 as tail call nop filler
      s390/bpf: Maintain 8-byte stack alignment
      selftests/bpf: Fix verifier test
      selftests/bpf: Fix sample_cnt shared between two threads
      bpf, selftests: Adapt cls_redirect to call csum_level helper
      bpf: Add csum_level helper for fixing up csum levels
      bpf: Fix up bpf_skb_adjust_room helper's skb csum setting
      sfc: add missing annotation for efx_ef10_try_update_nic_stats_vf()
      crypto/chtls: IPv6 support for inline TLS
      Crypto/chcr: Fixes a coccinile check error
      Crypto/chcr: Fixes compilations warnings
      ...

commit c50c75e9b87946499a62bffc021e95c87a1d57cd
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Mon May 11 15:12:27 2020 -0500

    perf/core: Replace zero-length array with flexible-array
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    sizeof(flexible-array-member) triggers a warning because flexible array
    members have incomplete type[1]. There are some instances of code in
    which the sizeof operator is being incorrectly/erroneously applied to
    zero-length arrays and the result is zero. Such instances may be hiding
    some bugs. So, this work (flexible-array member conversions) will also
    help to get completely rid of those sorts of issues.
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200511201227.GA14041@embeddedor

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 87e21681759c..d7b610c4eebd 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -61,7 +61,7 @@ struct perf_guest_info_callbacks {
 
 struct perf_callchain_entry {
 	__u64				nr;
-	__u64				ip[0]; /* /proc/sys/kernel/perf_event_max_stack */
+	__u64				ip[]; /* /proc/sys/kernel/perf_event_max_stack */
 };
 
 struct perf_callchain_entry_ctx {
@@ -113,7 +113,7 @@ struct perf_raw_record {
 struct perf_branch_stack {
 	__u64				nr;
 	__u64				hw_idx;
-	struct perf_branch_entry	entries[0];
+	struct perf_branch_entry	entries[];
 };
 
 struct task_struct;

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9c3e7619c929..347ea379622a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1280,15 +1280,12 @@ extern int sysctl_perf_cpu_time_max_percent;
 
 extern void perf_sample_event_took(u64 sample_len_ns);
 
-extern int perf_proc_update_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-extern int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-
+int perf_proc_update_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
+int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *lenp, loff_t *ppos);
 int perf_event_max_stack_handler(struct ctl_table *table, int write,
-				 void __user *buffer, size_t *lenp, loff_t *ppos);
+		void *buffer, size_t *lenp, loff_t *ppos);
 
 /* Access to perf_event_open(2) syscall. */
 #define PERF_SECURITY_OPEN		0

commit 18aa18566218d4a46d940049b835314d2b071cc2
Author: Alexey Budankov <alexey.budankov@linux.intel.com>
Date:   Thu Apr 2 11:46:24 2020 +0300

    perf/core: Open access to the core for CAP_PERFMON privileged process
    
    Open access to monitoring of kernel code, CPUs, tracepoints and
    namespaces data for a CAP_PERFMON privileged process. Providing the
    access under CAP_PERFMON capability singly, without the rest of
    CAP_SYS_ADMIN credentials, excludes chances to misuse the credentials
    and makes operation more secure.
    
    CAP_PERFMON implements the principle of least privilege for performance
    monitoring and observability operations (POSIX IEEE 1003.1e 2.2.2.39
    principle of least privilege: A security design principle that states
    that a process or program be granted only those privileges (e.g.,
    capabilities) necessary to accomplish its legitimate function, and only
    for the time that such privileges are actually required)
    
    For backward compatibility reasons the access to perf_events subsystem
    remains open for CAP_SYS_ADMIN privileged processes but CAP_SYS_ADMIN
    usage for secure perf_events monitoring is discouraged with respect to
    CAP_PERFMON capability.
    
    Signed-off-by: Alexey Budankov <alexey.budankov@linux.intel.com>
    Reviewed-by: James Morris <jamorris@linux.microsoft.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Igor Lubashev <ilubashe@akamai.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: linux-man@vger.kernel.org
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Serge Hallyn <serge@hallyn.com>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-security-module@vger.kernel.org
    Cc: selinux@vger.kernel.org
    Link: http://lore.kernel.org/lkml/471acaef-bb8a-5ce2-923f-90606b78eef9@linux.intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9c3e7619c929..87e21681759c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1305,7 +1305,7 @@ static inline int perf_is_paranoid(void)
 
 static inline int perf_allow_kernel(struct perf_event_attr *attr)
 {
-	if (sysctl_perf_event_paranoid > 1 && !capable(CAP_SYS_ADMIN))
+	if (sysctl_perf_event_paranoid > 1 && !perfmon_capable())
 		return -EACCES;
 
 	return security_perf_event_open(attr, PERF_SECURITY_KERNEL);
@@ -1313,7 +1313,7 @@ static inline int perf_allow_kernel(struct perf_event_attr *attr)
 
 static inline int perf_allow_cpu(struct perf_event_attr *attr)
 {
-	if (sysctl_perf_event_paranoid > 0 && !capable(CAP_SYS_ADMIN))
+	if (sysctl_perf_event_paranoid > 0 && !perfmon_capable())
 		return -EACCES;
 
 	return security_perf_event_open(attr, PERF_SECURITY_CPU);
@@ -1321,7 +1321,7 @@ static inline int perf_allow_cpu(struct perf_event_attr *attr)
 
 static inline int perf_allow_tracepoint(struct perf_event_attr *attr)
 {
-	if (sysctl_perf_event_paranoid > -1 && !capable(CAP_SYS_ADMIN))
+	if (sysctl_perf_event_paranoid > -1 && !perfmon_capable())
 		return -EPERM;
 
 	return security_perf_event_open(attr, PERF_SECURITY_TRACEPOINT);

commit 6546b19f95acc986807de981402bbac6b3a94b0f
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Mar 25 21:45:29 2020 +0900

    perf/core: Add PERF_SAMPLE_CGROUP feature
    
    The PERF_SAMPLE_CGROUP bit is to save (perf_event) cgroup information in
    the sample.  It will add a 64-bit id to identify current cgroup and it's
    the file handle in the cgroup file system.  Userspace should use this
    information with PERF_RECORD_CGROUP event to match which cgroup it
    belongs.
    
    I put it before PERF_SAMPLE_AUX for simplicity since it just needs a
    64-bit word.  But if we want bigger samples, I can work on that
    direction too.
    
    Committer testing:
    
      $ pahole perf_sample_data | grep -w cgroup -B5 -A5
            /* --- cacheline 4 boundary (256 bytes) was 56 bytes ago --- */
            struct perf_regs           regs_intr;            /*   312    16 */
            /* --- cacheline 5 boundary (320 bytes) was 8 bytes ago --- */
            u64                        stack_user_size;      /*   328     8 */
            u64                        phys_addr;            /*   336     8 */
            u64                        cgroup;               /*   344     8 */
    
            /* size: 384, cachelines: 6, members: 22 */
            /* padding: 32 */
      };
      $
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lore.kernel.org/lkml/20200325124536.2800725-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8768a39b5258..9c3e7619c929 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1020,6 +1020,7 @@ struct perf_sample_data {
 	u64				stack_user_size;
 
 	u64				phys_addr;
+	u64				cgroup;
 } ____cacheline_aligned;
 
 /* default value for data source */

commit 836196beb377e59e54ec9e04f7402076ef7a8bd8
Author: Ian Rogers <irogers@google.com>
Date:   Thu Feb 13 23:51:31 2020 -0800

    perf/core: Add per perf_cpu_context min_heap storage
    
    The storage required for visit_groups_merge's min heap needs to vary in
    order to support more iterators, such as when multiple nested cgroups'
    events are being visited. This change allows for 2 iterators and doesn't
    support growth.
    
    Based-on-work-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ian Rogers <irogers@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200214075133.181299-5-irogers@google.com

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 68e21e828893..8768a39b5258 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -862,6 +862,13 @@ struct perf_cpu_context {
 	int				sched_cb_usage;
 
 	int				online;
+	/*
+	 * Per-CPU storage for iterators used in visit_groups_merge. The default
+	 * storage is of size 2 to hold the CPU and any CPU event iterators.
+	 */
+	int				heap_size;
+	struct perf_event		**heap;
+	struct perf_event		*heap_default[2];
 };
 
 struct perf_output_handle {

commit bbfd5e4fab63703375eafaf241a0c696024a59e1
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Mon Jan 27 08:53:54 2020 -0800

    perf/core: Add new branch sample type for HW index of raw branch records
    
    The low level index is the index in the underlying hardware buffer of
    the most recently captured taken branch which is always saved in
    branch_entries[0]. It is very useful for reconstructing the call stack.
    For example, in Intel LBR call stack mode, the depth of reconstructed
    LBR call stack limits to the number of LBR registers. With the low level
    index information, perf tool may stitch the stacks of two samples. The
    reconstructed LBR call stack can break the HW limitation.
    
    Add a new branch sample type to retrieve low level index of raw branch
    records. The low level index is between -1 (unknown) and max depth which
    can be retrieved in /sys/devices/cpu/caps/branches.
    
    Only when the new branch sample type is set, the low level index
    information is dumped into the PERF_SAMPLE_BRANCH_STACK output.
    Perf tool should check the attr.branch_sample_type, and apply the
    corresponding format for PERF_SAMPLE_BRANCH_STACK samples.
    Otherwise, some user case may be broken. For example, users may parse a
    perf.data, which include the new branch sample type, with an old version
    perf tool (without the check). Users probably get incorrect information
    without any warning.
    
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200127165355.27495-2-kan.liang@linux.intel.com

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 547773f5894e..68e21e828893 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -93,14 +93,26 @@ struct perf_raw_record {
 /*
  * branch stack layout:
  *  nr: number of taken branches stored in entries[]
+ *  hw_idx: The low level index of raw branch records
+ *          for the most recent branch.
+ *          -1ULL means invalid/unknown.
  *
  * Note that nr can vary from sample to sample
  * branches (to, from) are stored from most recent
  * to least recent, i.e., entries[0] contains the most
  * recent branch.
+ * The entries[] is an abstraction of raw branch records,
+ * which may not be stored in age order in HW, e.g. Intel LBR.
+ * The hw_idx is to expose the low level index of raw
+ * branch record for the most recent branch aka entries[0].
+ * The hw_idx index is between -1 (unknown) and max depth,
+ * which can be retrieved in /sys/devices/cpu/caps/branches.
+ * For the architectures whose raw branch records are
+ * already stored in age order, the hw_idx should be 0.
  */
 struct perf_branch_stack {
 	__u64				nr;
+	__u64				hw_idx;
 	struct perf_branch_entry	entries[0];
 };
 

commit ca21b9b37059ee07176028de415cc4699db259cb
Merge: 2fbc23c73835 45f035748b2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 9 12:04:09 2020 -0800

    Merge tag 'perf-urgent-2020-02-09' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Thomas Gleixner:
     "A set of fixes and improvements for the perf subsystem:
    
      Kernel fixes:
    
       - Install cgroup events to the correct CPU context to prevent a
         potential list double add
    
       - Prevent an integer underflow in the perf mlock accounting
    
       - Add a missing prototype for arch_perf_update_userpage()
    
      Tooling:
    
       - Add a missing unlock in the error path of maps__insert() in perf
         maps.
    
       - Fix the build with the latest libbfd
    
       - Fix the perf parser so it does not delete parse event terms, which
         caused a regression for using perf with the ARM CoreSight as the
         sink configuration was missing due to the deletion.
    
       - Fix the double free in the perf CPU map merging test case
    
       - Add the missing ustring support for the perf probe command"
    
    * tag 'perf-urgent-2020-02-09' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf maps: Add missing unlock to maps__insert() error case
      perf probe: Add ustring support for perf probe command
      perf: Make perf able to build with latest libbfd
      perf test: Fix test case Merge cpu map
      perf parse: Copy string to perf_evsel_config_term
      perf parse: Refactor 'struct perf_evsel_config_term'
      kernel/events: Add a missing prototype for arch_perf_update_userpage()
      perf/cgroups: Install cgroup events to correct cpuctx
      perf/core: Fix mlock accounting in perf_mmap()

commit f1ec3a517b4352e78dbef6b1e591f43202ecb3fe
Author: Benjamin Thiel <b.thiel@posteo.de>
Date:   Thu Jan 9 14:13:51 2020 +0100

    kernel/events: Add a missing prototype for arch_perf_update_userpage()
    
    ... in order to fix a -Wmissing-prototype warning.
    
    No functional changes.
    
    Signed-off-by: Benjamin Thiel <b.thiel@posteo.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200109131351.9468-1-b.thiel@posteo.de

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6d4c22aee384..52928e089bc7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1544,4 +1544,8 @@ int perf_event_exit_cpu(unsigned int cpu);
 #define perf_event_exit_cpu	NULL
 #endif
 
+extern void __weak arch_perf_update_userpage(struct perf_event *event,
+					     struct perf_event_mmap_page *userpg,
+					     u64 now);
+
 #endif /* _LINUX_PERF_EVENT_H */

commit 56de4e8f9146680bcd048a29888f7438d5e58c55
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:21:30 2019 -0500

    perf: Make struct ring_buffer less ambiguous
    
    eBPF requires needing to know the size of the perf ring buffer structure.
    But it unfortunately has the same name as the generic ring buffer used by
    tracing and oprofile. To make it less ambiguous, rename the perf ring buffer
    structure to "perf_buffer".
    
    As other parts of the ring buffer code has "perf_" as the prefix, it only
    makes sense to give the ring buffer the "perf_" prefix as well.
    
    Link: https://lore.kernel.org/r/20191213153553.GE20583@krava
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6d4c22aee384..cf65763af0cb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -582,7 +582,7 @@ struct swevent_hlist {
 #define PERF_ATTACH_ITRACE	0x10
 
 struct perf_cgroup;
-struct ring_buffer;
+struct perf_buffer;
 
 struct pmu_event_list {
 	raw_spinlock_t		lock;
@@ -694,7 +694,7 @@ struct perf_event {
 	struct mutex			mmap_mutex;
 	atomic_t			mmap_count;
 
-	struct ring_buffer		*rb;
+	struct perf_buffer		*rb;
 	struct list_head		rb_entry;
 	unsigned long			rcu_batches;
 	int				rcu_pending;
@@ -854,7 +854,7 @@ struct perf_cpu_context {
 
 struct perf_output_handle {
 	struct perf_event		*event;
-	struct ring_buffer		*rb;
+	struct perf_buffer		*rb;
 	unsigned long			wakeup;
 	unsigned long			size;
 	u64				aux_flags;

commit 3f59dbcace56fae7e4ed303bab90f1bedadcfdf4
Merge: df28204bb0f2 ceb9e77324fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 15:04:47 2019 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "The main kernel side changes in this cycle were:
    
       - Various Intel-PT updates and optimizations (Alexander Shishkin)
    
       - Prohibit kprobes on Xen/KVM emulate prefixes (Masami Hiramatsu)
    
       - Add support for LSM and SELinux checks to control access to the
         perf syscall (Joel Fernandes)
    
       - Misc other changes, optimizations, fixes and cleanups - see the
         shortlog for details.
    
      There were numerous tooling changes as well - 254 non-merge commits.
      Here are the main changes - too many to list in detail:
    
       - Enhancements to core tooling infrastructure, perf.data, libperf,
         libtraceevent, event parsing, vendor events, Intel PT, callchains,
         BPF support and instruction decoding.
    
       - There were updates to the following tools:
    
            perf annotate
            perf diff
            perf inject
            perf kvm
            perf list
            perf maps
            perf parse
            perf probe
            perf record
            perf report
            perf script
            perf stat
            perf test
            perf trace
    
       - And a lot of other changes: please see the shortlog and Git log for
         more details"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (279 commits)
      perf parse: Fix potential memory leak when handling tracepoint errors
      perf probe: Fix spelling mistake "addrees" -> "address"
      libtraceevent: Fix memory leakage in copy_filter_type
      libtraceevent: Fix header installation
      perf intel-bts: Does not support AUX area sampling
      perf intel-pt: Add support for decoding AUX area samples
      perf intel-pt: Add support for recording AUX area samples
      perf pmu: When using default config, record which bits of config were changed by the user
      perf auxtrace: Add support for queuing AUX area samples
      perf session: Add facility to peek at all events
      perf auxtrace: Add support for dumping AUX area samples
      perf inject: Cut AUX area samples
      perf record: Add aux-sample-size config term
      perf record: Add support for AUX area sampling
      perf auxtrace: Add support for AUX area sample recording
      perf auxtrace: Move perf_evsel__find_pmu()
      perf record: Add a function to test for kernel support for AUX area sampling
      perf tools: Add kernel AUX area sampling definitions
      perf/core: Make the mlock accounting simple again
      perf report: Jump to symbol source view from total cycles view
      ...

commit 46f4f0aabc61bfd365e1eb3c8a6d766d1a49cf32
Merge: 14edff88315a b07a5c53d42a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 10:01:51 2019 +0100

    Merge branch 'kvm-tsx-ctrl' into HEAD
    
    Conflicts:
            arch/x86/kvm/vmx/vmx.c

commit 52ba4b0b99770e892f43da1238f437155acb8b58
Author: Like Xu <like.xu@linux.intel.com>
Date:   Sun Oct 27 18:52:39 2019 +0800

    perf/core: Provide a kernel-internal interface to pause perf_event
    
    Exporting perf_event_pause() as an external accessor for kernel users (such
    as KVM) who may do both disable perf_event and read count with just one
    time to hold perf_event_ctx_lock. Also the value could be reset optionally.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d601df36e671..e9768bfc76f6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1337,6 +1337,7 @@ extern void perf_event_disable_inatomic(struct perf_event *event);
 extern void perf_event_task_tick(void);
 extern int perf_event_account_interrupt(struct perf_event *event);
 extern int perf_event_period(struct perf_event *event, u64 value);
+extern u64 perf_event_pause(struct perf_event *event, bool reset);
 #else /* !CONFIG_PERF_EVENTS: */
 static inline void *
 perf_aux_output_begin(struct perf_output_handle *handle,
@@ -1420,6 +1421,10 @@ static inline int perf_event_period(struct perf_event *event, u64 value)
 {
 	return -EINVAL;
 }
+static inline u64 perf_event_pause(struct perf_event *event, bool reset)
+{
+	return 0;
+}
 #endif
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)

commit 3ca270fc9edb258d5bfa271bcf851614e9e6e7d4
Author: Like Xu <like.xu@linux.intel.com>
Date:   Sun Oct 27 18:52:38 2019 +0800

    perf/core: Provide a kernel-internal interface to recalibrate event period
    
    Currently, perf_event_period() is used by user tools via ioctl. Based on
    naming convention, exporting perf_event_period() for kernel users (such
    as KVM) who may recalibrate the event period for their assigned counter
    according to their requirements.
    
    The perf_event_period() is an external accessor, just like the
    perf_event_{en,dis}able() and should thus use perf_event_ctx_lock().
    
    Suggested-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Like Xu <like.xu@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61448c19a132..d601df36e671 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1336,6 +1336,7 @@ extern void perf_event_disable_local(struct perf_event *event);
 extern void perf_event_disable_inatomic(struct perf_event *event);
 extern void perf_event_task_tick(void);
 extern int perf_event_account_interrupt(struct perf_event *event);
+extern int perf_event_period(struct perf_event *event, u64 value);
 #else /* !CONFIG_PERF_EVENTS: */
 static inline void *
 perf_aux_output_begin(struct perf_output_handle *handle,
@@ -1415,6 +1416,10 @@ static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
 static inline int perf_event_release_kernel(struct perf_event *event)	{ return 0; }
+static inline int perf_event_period(struct perf_event *event, u64 value)
+{
+	return -EINVAL;
+}
 #endif
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)

commit a4faf00d994c40e64f656805ac375c65e324eefb
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Fri Oct 25 17:08:33 2019 +0300

    perf/aux: Allow using AUX data in perf samples
    
    AUX data can be used to annotate perf events such as performance counters
    or tracepoints/breakpoints by including it in sample records when
    PERF_SAMPLE_AUX flag is set. Such samples would be instrumental in debugging
    and profiling by providing, for example, a history of instruction flow
    leading up to the event's overflow.
    
    The implementation makes use of grouping an AUX event with all the events
    that wish to take samples of the AUX data, such that the former is the
    group leader. The samplees should also specify the desired size of the AUX
    sample via attr.aux_sample_size.
    
    AUX capable PMUs need to explicitly add support for sampling, because it
    relies on a new callback to take a snapshot of the buffer without touching
    the event states.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: adrian.hunter@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: https://lkml.kernel.org/r/20191025140835.53665-2-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 011dcbdbccc2..34c7c6910026 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -249,6 +249,8 @@ struct perf_event;
 #define PERF_PMU_CAP_NO_EXCLUDE			0x80
 #define PERF_PMU_CAP_AUX_OUTPUT			0x100
 
+struct perf_output_handle;
+
 /**
  * struct pmu - generic performance monitoring unit
  */
@@ -432,6 +434,19 @@ struct pmu {
 	 */
 	void (*free_aux)		(void *aux); /* optional */
 
+	/*
+	 * Take a snapshot of the AUX buffer without touching the event
+	 * state, so that preempting ->start()/->stop() callbacks does
+	 * not interfere with their logic. Called in PMI context.
+	 *
+	 * Returns the size of AUX data copied to the output handle.
+	 *
+	 * Optional.
+	 */
+	long (*snapshot_aux)		(struct perf_event *event,
+					 struct perf_output_handle *handle,
+					 unsigned long size);
+
 	/*
 	 * Validate address range filters: make sure the HW supports the
 	 * requested configuration and number of filters; return 0 if the
@@ -973,6 +988,7 @@ struct perf_sample_data {
 		u32	reserved;
 	}				cpu_entry;
 	struct perf_callchain_entry	*callchain;
+	u64				aux_size;
 
 	/*
 	 * regs_user may point to task_pt_regs or to regs_user_copy, depending
@@ -1362,6 +1378,9 @@ extern unsigned int perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
 extern unsigned int perf_output_skip(struct perf_output_handle *handle,
 				     unsigned int len);
+extern long perf_output_copy_aux(struct perf_output_handle *aux_handle,
+				 struct perf_output_handle *handle,
+				 unsigned long from, unsigned long to);
 extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
 extern u64 perf_swevent_set_period(struct perf_event *event);

commit fc1adfe306b71e094df636012f8c0fed971cad45
Author: Alexey Budankov <alexey.budankov@linux.intel.com>
Date:   Wed Oct 23 10:11:04 2019 +0300

    perf/core, perf/x86: Introduce swap_task_ctx() method at 'struct pmu'
    
    Declare swap_task_ctx() methods at the generic and x86 specific
    pmu types to bridge calls to platform specific PMU code on optimized
    context switch path between equivalent task perf event contexts.
    
    Signed-off-by: Alexey Budankov <alexey.budankov@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Ian Rogers <irogers@google.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: https://lkml.kernel.org/r/9a0aa84a-f062-9b64-3133-373658550c4b@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4f77b22d47be..011dcbdbccc2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -410,6 +410,15 @@ struct pmu {
 	 */
 	size_t				task_ctx_size;
 
+	/*
+	 * PMU specific parts of task perf event context (i.e. ctx->task_ctx_data)
+	 * can be synchronized using this function. See Intel LBR callstack support
+	 * implementation and Perf core context switch handling callbacks for usage
+	 * examples.
+	 */
+	void (*swap_task_ctx)		(struct perf_event_context *prev,
+					 struct perf_event_context *next);
+					/* optional */
 
 	/*
 	 * Set up pmu-private data structures for an AUX area

commit 65133033ee6ee34724ea3d82d5d1cfc6839ffdae
Merge: 27a0a90d6301 652521d460cb
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 28 12:38:26 2019 +0100

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 652521d460cbfa24ef27717b4b28acfac4281be6
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Thu Oct 24 14:29:04 2019 +0200

    perf/headers: Fix spelling s/EACCESS/EACCES/, s/privilidge/privilege/
    
    As per POSIX, the correct spelling of the error code is EACCES:
    
      include/uapi/asm-generic/errno-base.h:#define EACCES 13 /* Permission denied */
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: https://lkml.kernel.org/r/20191024122904.12463-1-geert+renesas@glider.be
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61448c19a132..68ccc5b1913b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -292,7 +292,7 @@ struct pmu {
 	 *  -EBUSY	-- @event is for this PMU but PMU temporarily unavailable
 	 *  -EINVAL	-- @event is for this PMU but @event is not valid
 	 *  -EOPNOTSUPP -- @event is for this PMU, @event is valid, but not supported
-	 *  -EACCESS	-- @event is for this PMU, @event is valid, but no privilidges
+	 *  -EACCES	-- @event is for this PMU, @event is valid, but no privileges
 	 *
 	 *  0		-- @event is for this PMU and valid
 	 *

commit da97e18458fb42d7c00fac5fd1c56a3896ec666e
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Mon Oct 14 13:03:08 2019 -0400

    perf_event: Add support for LSM and SELinux checks
    
    In current mainline, the degree of access to perf_event_open(2) system
    call depends on the perf_event_paranoid sysctl.  This has a number of
    limitations:
    
    1. The sysctl is only a single value. Many types of accesses are controlled
       based on the single value thus making the control very limited and
       coarse grained.
    2. The sysctl is global, so if the sysctl is changed, then that means
       all processes get access to perf_event_open(2) opening the door to
       security issues.
    
    This patch adds LSM and SELinux access checking which will be used in
    Android to access perf_event_open(2) for the purposes of attaching BPF
    programs to tracepoints, perf profiling and other operations from
    userspace. These operations are intended for production systems.
    
    5 new LSM hooks are added:
    1. perf_event_open: This controls access during the perf_event_open(2)
       syscall itself. The hook is called from all the places that the
       perf_event_paranoid sysctl is checked to keep it consistent with the
       systctl. The hook gets passed a 'type' argument which controls CPU,
       kernel and tracepoint accesses (in this context, CPU, kernel and
       tracepoint have the same semantics as the perf_event_paranoid sysctl).
       Additionally, I added an 'open' type which is similar to
       perf_event_paranoid sysctl == 3 patch carried in Android and several other
       distros but was rejected in mainline [1] in 2016.
    
    2. perf_event_alloc: This allocates a new security object for the event
       which stores the current SID within the event. It will be useful when
       the perf event's FD is passed through IPC to another process which may
       try to read the FD. Appropriate security checks will limit access.
    
    3. perf_event_free: Called when the event is closed.
    
    4. perf_event_read: Called from the read(2) and mmap(2) syscalls for the event.
    
    5. perf_event_write: Called from the ioctl(2) syscalls for the event.
    
    [1] https://lwn.net/Articles/696240/
    
    Since Peter had suggest LSM hooks in 2016 [1], I am adding his
    Suggested-by tag below.
    
    To use this patch, we set the perf_event_paranoid sysctl to -1 and then
    apply selinux checking as appropriate (default deny everything, and then
    add policy rules to give access to domains that need it). In the future
    we can remove the perf_event_paranoid sysctl altogether.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Co-developed-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: James Morris <jmorris@namei.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: rostedt@goodmis.org
    Cc: Yonghong Song <yhs@fb.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: jeffv@google.com
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: primiano@google.com
    Cc: Song Liu <songliubraving@fb.com>
    Cc: rsavitski@google.com
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Matthew Garrett <matthewgarrett@google.com>
    Link: https://lkml.kernel.org/r/20191014170308.70668-1-joel@joelfernandes.org

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61448c19a132..587ae4d002f5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -56,6 +56,7 @@ struct perf_guest_info_callbacks {
 #include <linux/perf_regs.h>
 #include <linux/cgroup.h>
 #include <linux/refcount.h>
+#include <linux/security.h>
 #include <asm/local.h>
 
 struct perf_callchain_entry {
@@ -721,6 +722,9 @@ struct perf_event {
 	struct perf_cgroup		*cgrp; /* cgroup event is attach to */
 #endif
 
+#ifdef CONFIG_SECURITY
+	void *security;
+#endif
 	struct list_head		sb_list;
 #endif /* CONFIG_PERF_EVENTS */
 };
@@ -1241,19 +1245,41 @@ extern int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
 int perf_event_max_stack_handler(struct ctl_table *table, int write,
 				 void __user *buffer, size_t *lenp, loff_t *ppos);
 
-static inline bool perf_paranoid_tracepoint_raw(void)
+/* Access to perf_event_open(2) syscall. */
+#define PERF_SECURITY_OPEN		0
+
+/* Finer grained perf_event_open(2) access control. */
+#define PERF_SECURITY_CPU		1
+#define PERF_SECURITY_KERNEL		2
+#define PERF_SECURITY_TRACEPOINT	3
+
+static inline int perf_is_paranoid(void)
 {
 	return sysctl_perf_event_paranoid > -1;
 }
 
-static inline bool perf_paranoid_cpu(void)
+static inline int perf_allow_kernel(struct perf_event_attr *attr)
 {
-	return sysctl_perf_event_paranoid > 0;
+	if (sysctl_perf_event_paranoid > 1 && !capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	return security_perf_event_open(attr, PERF_SECURITY_KERNEL);
 }
 
-static inline bool perf_paranoid_kernel(void)
+static inline int perf_allow_cpu(struct perf_event_attr *attr)
 {
-	return sysctl_perf_event_paranoid > 1;
+	if (sysctl_perf_event_paranoid > 0 && !capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	return security_perf_event_open(attr, PERF_SECURITY_CPU);
+}
+
+static inline int perf_allow_tracepoint(struct perf_event_attr *attr)
+{
+	if (sysctl_perf_event_paranoid > -1 && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return security_perf_event_open(attr, PERF_SECURITY_TRACEPOINT);
 }
 
 extern void perf_event_init(void);

commit ab43762ef010967e4ccd53627f70a2eecbeafefb
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Tue Aug 6 11:46:00 2019 +0300

    perf: Allow normal events to output AUX data
    
    In some cases, ordinary (non-AUX) events can generate data for AUX events.
    For example, PEBS events can come out as records in the Intel PT stream
    instead of their usual DS records, if configured to do so.
    
    One requirement for such events is to consistently schedule together, to
    ensure that the data from the "AUX output" events isn't lost while their
    corresponding AUX event is not scheduled. We use grouping to provide this
    guarantee: an "AUX output" event can be added to a group where an AUX event
    is a group leader, and provided that the former supports writing to the
    latter.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: kan.liang@linux.intel.com
    Link: https://lkml.kernel.org/r/20190806084606.4021-2-alexander.shishkin@linux.intel.com

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e8ad3c590a23..61448c19a132 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -246,6 +246,7 @@ struct perf_event;
 #define PERF_PMU_CAP_ITRACE			0x20
 #define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40
 #define PERF_PMU_CAP_NO_EXCLUDE			0x80
+#define PERF_PMU_CAP_AUX_OUTPUT			0x100
 
 /**
  * struct pmu - generic performance monitoring unit
@@ -446,6 +447,16 @@ struct pmu {
 	void (*addr_filters_sync)	(struct perf_event *event);
 					/* optional */
 
+	/*
+	 * Check if event can be used for aux_output purposes for
+	 * events of this PMU.
+	 *
+	 * Runs from perf_event_open(). Should return 0 for "no match"
+	 * or non-zero for "match".
+	 */
+	int (*aux_output_match)		(struct perf_event *event);
+					/* optional */
+
 	/*
 	 * Filter events for PMU-specific reasons.
 	 */
@@ -681,6 +692,9 @@ struct perf_event {
 	struct perf_addr_filter_range	*addr_filter_ranges;
 	unsigned long			addr_filters_gen;
 
+	/* for aux_output events */
+	struct perf_event		*aux_event;
+
 	void (*destroy)(struct perf_event *);
 	struct rcu_head			rcu_head;
 

commit 8a58ddae23796c733c5dfbd717538d89d036c5bd
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Mon Jul 1 14:07:55 2019 +0300

    perf/core: Fix exclusive events' grouping
    
    So far, we tried to disallow grouping exclusive events for the fear of
    complications they would cause with moving between contexts. Specifically,
    moving a software group to a hardware context would violate the exclusivity
    rules if both groups contain matching exclusive events.
    
    This attempt was, however, unsuccessful: the check that we have in the
    perf_event_open() syscall is both wrong (looks at wrong PMU) and
    insufficient (group leader may still be exclusive), as can be illustrated
    by running:
    
      $ perf record -e '{intel_pt//,cycles}' uname
      $ perf record -e '{cycles,intel_pt//}' uname
    
    ultimately successfully.
    
    Furthermore, we are completely free to trigger the exclusivity violation
    by:
    
       perf -e '{cycles,intel_pt//}' -e '{intel_pt//,instructions}'
    
    even though the helpful perf record will not allow that, the ABI will.
    
    The warning later in the perf_event_open() path will also not trigger, because
    it's also wrong.
    
    Fix all this by validating the original group before moving, getting rid
    of broken safeguards and placing a useful one to perf_install_in_context().
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: mathieu.poirier@linaro.org
    Cc: will.deacon@arm.com
    Fixes: bed5b25ad9c8a ("perf: Add a pmu capability for "exclusive" events")
    Link: https://lkml.kernel.org/r/20190701110755.24646-1-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 16e38c286d46..e8ad3c590a23 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1055,6 +1055,11 @@ static inline int in_software_context(struct perf_event *event)
 	return event->ctx->pmu->task_ctx_nr == perf_sw_context;
 }
 
+static inline int is_exclusive_pmu(struct pmu *pmu)
+{
+	return pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE;
+}
+
 extern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void ___perf_sw_event(u32, u64, struct pt_regs *, u64);

commit 552a031ba12a4236be107a5b082a399237758a5d
Merge: f584dd32edc5 0ecfebd2b524
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jul 8 18:04:41 2019 +0200

    Merge tag 'v5.2' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fd7d55172d1e2e501e6da0a5c1de25f06612dc2e
Author: Ian Rogers <irogers@google.com>
Date:   Sat Jun 1 01:27:22 2019 -0700

    perf/cgroups: Don't rotate events for cgroups unnecessarily
    
    Currently perf_rotate_context assumes that if the context's nr_events !=
    nr_active a rotation is necessary for perf event multiplexing. With
    cgroups, nr_events is the total count of events for all cgroups and
    nr_active will not include events in a cgroup other than the current
    task's. This makes rotation appear necessary for cgroups when it is not.
    
    Add a perf_event_context flag that is set when rotation is necessary.
    Clear the flag during sched_out and set it when a flexible sched_in
    fails due to resources.
    
    Signed-off-by: Ian Rogers <irogers@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: https://lkml.kernel.org/r/20190601082722.44543-1-irogers@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3dc01cf98e16..2ddae518dce6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -749,6 +749,11 @@ struct perf_event_context {
 	int				nr_stat;
 	int				nr_freq;
 	int				rotate_disable;
+	/*
+	 * Set when nr_events != nr_active, except tolerant to events not
+	 * necessary to be active due to scheduling constraints, such as cgroups.
+	 */
+	int				rotate_necessary;
 	refcount_t			refcount;
 	struct task_struct		*task;
 

commit e321d02db87af7840da29ef833a2a71fc0eab198
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Tue May 28 15:08:30 2019 -0700

    perf/x86: Disable extended registers for non-supported PMUs
    
    The perf fuzzer caused Skylake machine to crash:
    
    [ 9680.085831] Call Trace:
    [ 9680.088301]  <IRQ>
    [ 9680.090363]  perf_output_sample_regs+0x43/0xa0
    [ 9680.094928]  perf_output_sample+0x3aa/0x7a0
    [ 9680.099181]  perf_event_output_forward+0x53/0x80
    [ 9680.103917]  __perf_event_overflow+0x52/0xf0
    [ 9680.108266]  ? perf_trace_run_bpf_submit+0xc0/0xc0
    [ 9680.113108]  perf_swevent_hrtimer+0xe2/0x150
    [ 9680.117475]  ? check_preempt_wakeup+0x181/0x230
    [ 9680.122091]  ? check_preempt_curr+0x62/0x90
    [ 9680.126361]  ? ttwu_do_wakeup+0x19/0x140
    [ 9680.130355]  ? try_to_wake_up+0x54/0x460
    [ 9680.134366]  ? reweight_entity+0x15b/0x1a0
    [ 9680.138559]  ? __queue_work+0x103/0x3f0
    [ 9680.142472]  ? update_dl_rq_load_avg+0x1cd/0x270
    [ 9680.147194]  ? timerqueue_del+0x1e/0x40
    [ 9680.151092]  ? __remove_hrtimer+0x35/0x70
    [ 9680.155191]  __hrtimer_run_queues+0x100/0x280
    [ 9680.159658]  hrtimer_interrupt+0x100/0x220
    [ 9680.163835]  smp_apic_timer_interrupt+0x6a/0x140
    [ 9680.168555]  apic_timer_interrupt+0xf/0x20
    [ 9680.172756]  </IRQ>
    
    The XMM registers can only be collected by PEBS hardware events on the
    platforms with PEBS baseline support, e.g. Icelake, not software/probe
    events.
    
    Add capabilities flag PERF_PMU_CAP_EXTENDED_REGS to indicate the PMU
    which support extended registers. For X86, the extended registers are
    XMM registers.
    
    Add has_extended_regs() to check if extended registers are applied.
    
    The generic code define the mask of extended registers as 0 if arch
    headers haven't overridden it.
    
    Originally-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 878068ea270e ("perf/x86: Support outputting XMM registers")
    Link: https://lkml.kernel.org/r/1559081314-9714-1-git-send-email-kan.liang@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0ab99c7b652d..2bca72f3028b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -241,6 +241,7 @@ struct perf_event;
 #define PERF_PMU_CAP_NO_INTERRUPT		0x01
 #define PERF_PMU_CAP_NO_NMI			0x02
 #define PERF_PMU_CAP_AUX_NO_SG			0x04
+#define PERF_PMU_CAP_EXTENDED_REGS		0x08
 #define PERF_PMU_CAP_EXCLUSIVE			0x10
 #define PERF_PMU_CAP_ITRACE			0x20
 #define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40

commit f3a3a8257e5a1a5e67cbb1afdbc4c1c6a26f1b22
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun May 12 17:55:11 2019 +0200

    perf/core: Add attr_groups_update into struct pmu
    
    Adding attr_update attribute group into pmu, to allow
    having multiple attribute groups for same group name.
    
    This will allow us to update "events" or "format"
    directories with attributes that depend on various
    HW conditions.
    
    For example having group_format_extra group that updates
    "format" directory only if pmu version is 2 and higher:
    
      static umode_t
      exra_is_visible(struct kobject *kobj, struct attribute *attr, int i)
      {
             return x86_pmu.version >= 2 ? attr->mode : 0;
      }
    
      static struct attribute_group group_format_extra = {
             .name       = "format",
             .is_visible = exra_is_visible,
      };
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190512155518.21468-3-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0ab99c7b652d..3dc01cf98e16 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -255,6 +255,7 @@ struct pmu {
 	struct module			*module;
 	struct device			*dev;
 	const struct attribute_group	**attr_groups;
+	const struct attribute_group	**attr_update;
 	const char			*name;
 	int				type;
 

commit 0ef0fd351550130129bbdb77362488befd7b69d2
Merge: 4489da718309 c011d23ba046
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 17 10:33:30 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for SVE and Pointer Authentication in guests
       - PMU improvements
    
      POWER:
       - support for direct access to the POWER9 XIVE interrupt controller
       - memory and performance optimizations
    
      x86:
       - support for accessing memory not backed by struct page
       - fixes and refactoring
    
      Generic:
       - dirty page tracking improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (155 commits)
      kvm: fix compilation on aarch64
      Revert "KVM: nVMX: Expose RDPMC-exiting only when guest supports PMU"
      kvm: x86: Fix L1TF mitigation for shadow MMU
      KVM: nVMX: Disable intercept for FS/GS base MSRs in vmcs02 when possible
      KVM: PPC: Book3S: Remove useless checks in 'release' method of KVM device
      KVM: PPC: Book3S HV: XIVE: Fix spelling mistake "acessing" -> "accessing"
      KVM: PPC: Book3S HV: Make sure to load LPID for radix VCPUs
      kvm: nVMX: Set nested_run_pending in vmx_set_nested_state after checks complete
      tests: kvm: Add tests for KVM_SET_NESTED_STATE
      KVM: nVMX: KVM_SET_NESTED_STATE - Tear down old EVMCS state before setting new state
      tests: kvm: Add tests for KVM_CAP_MAX_VCPUS and KVM_CAP_MAX_CPU_ID
      tests: kvm: Add tests to .gitignore
      KVM: Introduce KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2
      KVM: Fix kvm_clear_dirty_log_protect off-by-(minus-)one
      KVM: Fix the bitmap range to copy during clear dirty
      KVM: arm64: Fix ptrauth ID register masking logic
      KVM: x86: use direct accessors for RIP and RSP
      KVM: VMX: Use accessors for GPRs outside of dedicated caching logic
      KVM: x86: Omit caching logic for always-available GPRs
      kvm, x86: Properly check whether a pfn is an MMIO or not
      ...

commit 90489a72fba9529c85e051067ecb41183b8e982e
Merge: 007dc78fea62 d15d356887e7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 14:16:36 2019 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "The main kernel changes were:
    
       - add support for Intel's "adaptive PEBS v4" - which embedds LBS data
         in PEBS records and can thus batch up and reduce the IRQ (NMI) rate
         significantly - reducing overhead and making call-graph profiling
         less intrusive.
    
       - add Intel CPU core and uncore support updates for Tremont, Icelake,
    
       - extend the x86 PMU constraints scheduler with 'constraint ranges'
         to better support Icelake hw constraints,
    
       - make x86 call-chain support work better with CONFIG_FRAME_POINTER=y
    
       - misc other changes
    
      Tooling changes:
    
       - updates to the main tools: 'perf record', 'perf trace', 'perf
         stat'
    
       - updated Intel and S/390 vendor events
    
       - libtraceevent updates
    
       - misc other updates and fixes"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (69 commits)
      perf/x86: Make perf callchains work without CONFIG_FRAME_POINTER
      watchdog: Fix typo in comment
      perf/x86/intel: Add Tremont core PMU support
      perf/x86/intel/uncore: Add Intel Icelake uncore support
      perf/x86/msr: Add Icelake support
      perf/x86/intel/rapl: Add Icelake support
      perf/x86/intel/cstate: Add Icelake support
      perf/x86/intel: Add Icelake support
      perf/x86: Support constraint ranges
      perf/x86/lbr: Avoid reading the LBRs when adaptive PEBS handles them
      perf/x86/intel: Support adaptive PEBS v4
      perf/x86/intel/ds: Extract code of event update in short period
      perf/x86/intel: Extract memory code PEBS parser for reuse
      perf/x86: Support outputting XMM registers
      perf/x86/intel: Force resched when TFA sysctl is modified
      perf/core: Add perf_pmu_resched() as global function
      perf/headers: Fix stale comment for struct perf_addr_filter
      perf/core: Make perf_swevent_init_cpu() static
      perf/x86: Add sanity checks to x86_schedule_events()
      perf/x86: Optimize x86_schedule_events()
      ...

commit 72e830f68428ab9ea9eca65d160795f4e02cecfc
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Fri May 3 11:55:36 2019 +0300

    perf/x86/intel/pt: Remove software double buffering PMU capability
    
    Now that all AUX allocations are high-order by default, the software
    double buffering PMU capability doesn't make sense any more, get rid
    of it. In case some PMUs choose to opt out, we can re-introduce it.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: adrian.hunter@intel.com
    Link: http://lkml.kernel.org/r/20190503085536.24119-3-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ef764f613..1f678f023850 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -240,7 +240,6 @@ struct perf_event;
 #define PERF_PMU_CAP_NO_INTERRUPT		0x01
 #define PERF_PMU_CAP_NO_NMI			0x02
 #define PERF_PMU_CAP_AUX_NO_SG			0x04
-#define PERF_PMU_CAP_AUX_SW_DOUBLEBUF		0x08
 #define PERF_PMU_CAP_EXCLUSIVE			0x10
 #define PERF_PMU_CAP_ITRACE			0x20
 #define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40

commit 8479e04e7d6b1974629a0f657afa8ec5f17d2e90
Author: Luwei Kang <luwei.kang@intel.com>
Date:   Mon Feb 18 19:26:07 2019 -0500

    KVM: x86: Inject PMI for KVM guest
    
    Inject a PMI for KVM guest when Intel PT working
    in Host-Guest mode and Guest ToPA entry memory buffer
    was completely filled.
    
    Signed-off-by: Luwei Kang <luwei.kang@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ef764f613..820c4ff31bc5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -30,6 +30,7 @@ struct perf_guest_info_callbacks {
 	int				(*is_in_guest)(void);
 	int				(*is_user_mode)(void);
 	unsigned long			(*get_guest_ip)(void);
+	void				(*handle_intel_pt_intr)(void);
 };
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT

commit d15d356887e770c5f2dcf963b52c7cb510c9e42d
Author: Kairui Song <kasong@redhat.com>
Date:   Tue Apr 23 00:26:52 2019 +0800

    perf/x86: Make perf callchains work without CONFIG_FRAME_POINTER
    
    Currently perf callchain doesn't work well with ORC unwinder
    when sampling from trace point. We'll get useless in kernel callchain
    like this:
    
    perf  6429 [000]    22.498450:             kmem:mm_page_alloc: page=0x176a17 pfn=1534487 order=0 migratetype=0 gfp_flags=GFP_KERNEL
        ffffffffbe23e32e __alloc_pages_nodemask+0x22e (/lib/modules/5.1.0-rc3+/build/vmlinux)
            7efdf7f7d3e8 __poll+0x18 (/usr/lib64/libc-2.28.so)
            5651468729c1 [unknown] (/usr/bin/perf)
            5651467ee82a main+0x69a (/usr/bin/perf)
            7efdf7eaf413 __libc_start_main+0xf3 (/usr/lib64/libc-2.28.so)
        5541f689495641d7 [unknown] ([unknown])
    
    The root cause is that, for trace point events, it doesn't provide a
    real snapshot of the hardware registers. Instead perf tries to get
    required caller's registers and compose a fake register snapshot
    which suppose to contain enough information for start a unwinding.
    However without CONFIG_FRAME_POINTER, if failed to get caller's BP as the
    frame pointer, so current frame pointer is returned instead. We get
    a invalid register combination which confuse the unwinder, and end the
    stacktrace early.
    
    So in such case just don't try dump BP, and let the unwinder start
    directly when the register is not a real snapshot. Use SP
    as the skip mark, unwinder will skip all the frames until it meet
    the frame of the trace point caller.
    
    Tested with frame pointer unwinder and ORC unwinder, this makes perf
    callchain get the full kernel space stacktrace again like this:
    
    perf  6503 [000]  1567.570191:             kmem:mm_page_alloc: page=0x16c904 pfn=1493252 order=0 migratetype=0 gfp_flags=GFP_KERNEL
        ffffffffb523e2ae __alloc_pages_nodemask+0x22e (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52383bd __get_free_pages+0xd (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52fd28a __pollwait+0x8a (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb521426f perf_poll+0x2f (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52fe3e2 do_sys_poll+0x252 (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb52ff027 __x64_sys_poll+0x37 (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb500418b do_syscall_64+0x5b (/lib/modules/5.1.0-rc3+/build/vmlinux)
        ffffffffb5a0008c entry_SYSCALL_64_after_hwframe+0x44 (/lib/modules/5.1.0-rc3+/build/vmlinux)
            7f71e92d03e8 __poll+0x18 (/usr/lib64/libc-2.28.so)
            55a22960d9c1 [unknown] (/usr/bin/perf)
            55a22958982a main+0x69a (/usr/bin/perf)
            7f71e9202413 __libc_start_main+0xf3 (/usr/lib64/libc-2.28.so)
        5541f689495641d7 [unknown] ([unknown])
    
    Co-developed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Kairui Song <kasong@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190422162652.15483-1-kasong@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f3864e1c5569..cf023db0e8a2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1058,12 +1058,18 @@ static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned lo
 #endif
 
 /*
- * Take a snapshot of the regs. Skip ip and frame pointer to
- * the nth caller. We only need a few of the regs:
+ * When generating a perf sample in-line, instead of from an interrupt /
+ * exception, we lack a pt_regs. This is typically used from software events
+ * like: SW_CONTEXT_SWITCHES, SW_MIGRATIONS and the tie-in with tracepoints.
+ *
+ * We typically don't need a full set, but (for x86) do require:
  * - ip for PERF_SAMPLE_IP
  * - cs for user_mode() tests
- * - bp for callchains
- * - eflags, for future purposes, just in case
+ * - sp for PERF_SAMPLE_CALLCHAIN
+ * - eflags for MISC bits and CALLCHAIN (see: perf_hw_regs())
+ *
+ * NOTE: assumes @regs is otherwise already 0 filled; this is important for
+ * things like PERF_SAMPLE_REGS_INTR.
  */
 static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 {

commit c68d224e5ed15605e651e2482c6ffd95915ddf58
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Apr 8 10:32:51 2019 -0700

    perf/core: Add perf_pmu_resched() as global function
    
    This patch add perf_pmu_resched() a global function that can be called
    to force rescheduling of events for a given PMU. The function locks
    both cpuctx and task_ctx internally. This will be used by a subsequent
    patch.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    [ Simplified the calling convention. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: kan.liang@intel.com
    Cc: nelson.dsouza@intel.com
    Cc: tonyj@suse.com
    Link: https://lkml.kernel.org/r/20190408173252.37932-2-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 085a95e2582a..f3864e1c5569 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -888,6 +888,9 @@ extern void perf_sched_cb_dec(struct pmu *pmu);
 extern void perf_sched_cb_inc(struct pmu *pmu);
 extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
+
+extern void perf_pmu_resched(struct pmu *pmu);
+
 extern int perf_event_refresh(struct perf_event *event, int refresh);
 extern void perf_event_update_userpage(struct perf_event *event);
 extern int perf_event_release_kernel(struct perf_event *event);

commit 1279e41d535e28cc3b56fa4a09e71a709641cae6
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Wed Apr 3 14:54:24 2019 +0800

    perf/headers: Fix stale comment for struct perf_addr_filter
    
    The @inode field has been removed after:
    
      9511bce9fe8e ("perf/core: Fix bad use of igrab()")
    
    Update the description.
    
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: https://lkml.kernel.org/r/1554274464-5739-1-git-send-email-zhangshaokun@hisilicon.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ef764f613..085a95e2582a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -464,7 +464,7 @@ enum perf_addr_filter_action_t {
 /**
  * struct perf_addr_filter - address range filter definition
  * @entry:	event's filter list linkage
- * @inode:	object file's inode for file-based filters
+ * @path:	object file's path for file-based filters
  * @offset:	filter range offset
  * @size:	filter range size (size==0 means single address trigger)
  * @action:	filter/start/stop

commit c978b9460fe1d4a1e1effa0abd6bd69b18a098a8
Merge: 0a1571243d3f de667cce7f4f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 28 08:29:50 2019 +0100

    Merge tag 'perf-core-for-mingo-5.1-20190225' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux into perf/core
    
    Pull perf/core improvements and fixes from Arnaldo Carvalho de Melo:
    
    perf annotate:
    
      Wei Li:
    
      - Fix getting source line failure
    
    perf script:
    
      Andi Kleen:
    
      - Handle missing fields with -F +...
    
    perf data:
    
      Jiri Olsa:
    
      - Prep work to support per-cpu files in a directory.
    
    Intel PT:
    
      Adrian Hunter:
    
      - Improve thread_stack__no_call_return()
    
      - Hide x86 retpolines in thread stacks.
    
      - exported SQL viewer refactorings, new 'top calls' report..
    
      Alexander Shishkin:
    
      - Copy parent's address filter offsets on clone
    
      - Fix address filters for vmas with non-zero offset. Applies to
        ARM's CoreSight as well.
    
    python scripts:
    
      Tony Jones:
    
      - Python3 support for several 'perf script' python scripts.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9ed8f1a6e7670aadd5aef30456a90b456ed1b185
Merge: 43f4e6279f05 7d762d69145a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 28 08:27:17 2019 +0100

    Merge branch 'linus' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c60f83b813e5b25ccd5de7e8c8925c31b3aebcc1
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Fri Feb 15 13:56:55 2019 +0200

    perf, pt, coresight: Fix address filters for vmas with non-zero offset
    
    Currently, the address range calculation for file-based filters works as
    long as the vma that maps the matching part of the object file starts
    from offset zero into the file (vm_pgoff==0). Otherwise, the resulting
    filter range would be off by vm_pgoff pages. Another related problem is
    that in case of a partially matching vma, that is, a vma that matches
    part of a filter region, the filter range size wouldn't be adjusted.
    
    Fix the arithmetics around address filter range calculations, taking
    into account vma offset, so that the entire calculation is done before
    the filter configuration is passed to the PMU drivers instead of having
    those drivers do the final bit of arithmetics.
    
    Based on the patch by Adrian Hunter <adrian.hunter.intel.com>.
    
    Reported-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Tested-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Fixes: 375637bc5249 ("perf/core: Introduce address range filtering")
    Link: http://lkml.kernel.org/r/20190215115655.63469-3-alexander.shishkin@linux.intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d9c3610e0e25..6ebc72f65017 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -490,6 +490,11 @@ struct perf_addr_filters_head {
 	unsigned int		nr_file_filters;
 };
 
+struct perf_addr_filter_range {
+	unsigned long		start;
+	unsigned long		size;
+};
+
 /**
  * enum perf_event_state - the states of an event:
  */
@@ -666,7 +671,7 @@ struct perf_event {
 	/* address range filters */
 	struct perf_addr_filters_head	addr_filters;
 	/* vma address array for file-based filders */
-	unsigned long			*addr_filters_offs;
+	struct perf_addr_filter_range	*addr_filter_ranges;
 	unsigned long			addr_filters_gen;
 
 	void (*destroy)(struct perf_event *);

commit 81ec3f3c4c4d78f2d3b6689c9816bfbdf7417dbb
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Feb 4 13:35:32 2019 +0100

    perf/x86: Add check_period PMU callback
    
    Vince (and later on Ravi) reported crashes in the BTS code during
    fuzzing with the following backtrace:
    
      general protection fault: 0000 [#1] SMP PTI
      ...
      RIP: 0010:perf_prepare_sample+0x8f/0x510
      ...
      Call Trace:
       <IRQ>
       ? intel_pmu_drain_bts_buffer+0x194/0x230
       intel_pmu_drain_bts_buffer+0x160/0x230
       ? tick_nohz_irq_exit+0x31/0x40
       ? smp_call_function_single_interrupt+0x48/0xe0
       ? call_function_single_interrupt+0xf/0x20
       ? call_function_single_interrupt+0xa/0x20
       ? x86_schedule_events+0x1a0/0x2f0
       ? x86_pmu_commit_txn+0xb4/0x100
       ? find_busiest_group+0x47/0x5d0
       ? perf_event_set_state.part.42+0x12/0x50
       ? perf_mux_hrtimer_restart+0x40/0xb0
       intel_pmu_disable_event+0xae/0x100
       ? intel_pmu_disable_event+0xae/0x100
       x86_pmu_stop+0x7a/0xb0
       x86_pmu_del+0x57/0x120
       event_sched_out.isra.101+0x83/0x180
       group_sched_out.part.103+0x57/0xe0
       ctx_sched_out+0x188/0x240
       ctx_resched+0xa8/0xd0
       __perf_event_enable+0x193/0x1e0
       event_function+0x8e/0xc0
       remote_function+0x41/0x50
       flush_smp_call_function_queue+0x68/0x100
       generic_smp_call_function_single_interrupt+0x13/0x30
       smp_call_function_single_interrupt+0x3e/0xe0
       call_function_single_interrupt+0xf/0x20
       </IRQ>
    
    The reason is that while event init code does several checks
    for BTS events and prevents several unwanted config bits for
    BTS event (like precise_ip), the PERF_EVENT_IOC_PERIOD allows
    to create BTS event without those checks being done.
    
    Following sequence will cause the crash:
    
    If we create an 'almost' BTS event with precise_ip and callchains,
    and it into a BTS event it will crash the perf_prepare_sample()
    function because precise_ip events are expected to come
    in with callchain data initialized, but that's not the
    case for intel_pmu_drain_bts_buffer() caller.
    
    Adding a check_period callback to be called before the period
    is changed via PERF_EVENT_IOC_PERIOD. It will deny the change
    if the event would become BTS. Plus adding also the limit_period
    check as well.
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20190204123532.GA4794@krava
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1d5c551a5add..e1a051724f7e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -447,6 +447,11 @@ struct pmu {
 	 * Filter events for PMU-specific reasons.
 	 */
 	int (*filter_match)		(struct perf_event *event); /* optional */
+
+	/*
+	 * Check period value for PERF_EVENT_IOC_PERIOD ioctl.
+	 */
+	int (*check_period)		(struct perf_event *event, u64 value); /* optional */
 };
 
 enum perf_addr_filter_action_t {

commit 840018668ce2d96783356204ff282d6c9b0e5f66
Author: Mathieu Poirier <mathieu.poirier@linaro.org>
Date:   Thu Jan 31 11:47:08 2019 -0700

    perf/aux: Make perf_event accessible to setup_aux()
    
    When pmu::setup_aux() is called the coresight PMU needs to know which
    sink to use for the session by looking up the information in the
    event's attr::config2 field.
    
    As such simply replace the cpu information by the complete perf_event
    structure and change all affected customers.
    
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Reviewed-by: Suzuki Poulouse <suzuki.poulose@arm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190131184714.20388-2-mathieu.poirier@linaro.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6cb5d483ab34..d9c3610e0e25 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -410,7 +410,7 @@ struct pmu {
 	/*
 	 * Set up pmu-private data structures for an AUX area
 	 */
-	void *(*setup_aux)		(int cpu, void **pages,
+	void *(*setup_aux)		(struct perf_event *event, void **pages,
 					 int nr_pages, bool overwrite);
 					/* optional */
 

commit 8c94abbbe1ba24961278055434504b7dc3595415
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Mon Jan 28 14:27:26 2019 +0200

    perf: Convert perf_event_context.refcount to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable perf_event_context.refcount is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts. Please check Documentation/core-api/refcount-vs-atomic.rst
    for more information.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the perf_event_context.refcount it might make a difference
    in following places:
    
     - get_ctx(), perf_event_ctx_lock_nested(), perf_lock_task_context()
       and __perf_event_ctx_lock_double(): increment in
       refcount_inc_not_zero() only guarantees control dependency
       on success vs. fully ordered atomic counterpart
     - put_ctx(): decrement in refcount_dec_and_test() provides
       RELEASE ordering and ACQUIRE ordering + control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: namhyung@kernel.org
    Link: https://lkml.kernel.org/r/1548678448-24458-2-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a79e59fc3b7d..6cb5d483ab34 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -54,6 +54,7 @@ struct perf_guest_info_callbacks {
 #include <linux/sysfs.h>
 #include <linux/perf_regs.h>
 #include <linux/cgroup.h>
+#include <linux/refcount.h>
 #include <asm/local.h>
 
 struct perf_callchain_entry {
@@ -737,7 +738,7 @@ struct perf_event_context {
 	int				nr_stat;
 	int				nr_freq;
 	int				rotate_disable;
-	atomic_t			refcount;
+	refcount_t			refcount;
 	struct task_struct		*task;
 
 	/*

commit 6ee52e2a3fe4ea35520720736e6791df1fb67106
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:15 2019 -0800

    perf, bpf: Introduce PERF_RECORD_BPF_EVENT
    
    For better performance analysis of BPF programs, this patch introduces
    PERF_RECORD_BPF_EVENT, a new perf_event_type that exposes BPF program
    load/unload information to user space.
    
    Each BPF program may contain up to BPF_MAX_SUBPROGS (256) sub programs.
    The following example shows kernel symbols for a BPF program with 7 sub
    programs:
    
        ffffffffa0257cf9 t bpf_prog_b07ccb89267cf242_F
        ffffffffa02592e1 t bpf_prog_2dcecc18072623fc_F
        ffffffffa025b0e9 t bpf_prog_bb7a405ebaec5d5c_F
        ffffffffa025dd2c t bpf_prog_a7540d4a39ec1fc7_F
        ffffffffa025fcca t bpf_prog_05762d4ade0e3737_F
        ffffffffa026108f t bpf_prog_db4bd11e35df90d4_F
        ffffffffa0263f00 t bpf_prog_89d64e4abf0f0126_F
        ffffffffa0257cf9 t bpf_prog_ae31629322c4b018__dummy_tracepoi
    
    When a bpf program is loaded, PERF_RECORD_KSYMBOL is generated for each
    of these sub programs. Therefore, PERF_RECORD_BPF_EVENT is not needed
    for simple profiling.
    
    For annotation, user space need to listen to PERF_RECORD_BPF_EVENT and
    gather more information about these (sub) programs via sys_bpf.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradeaed.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-4-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 136fe0495374..a79e59fc3b7d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1125,6 +1125,9 @@ extern void perf_event_mmap(struct vm_area_struct *vma);
 
 extern void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,
 			       bool unregister, const char *sym);
+extern void perf_event_bpf_event(struct bpf_prog *prog,
+				 enum perf_bpf_event_type type,
+				 u16 flags);
 
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
@@ -1350,6 +1353,9 @@ static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 typedef int (perf_ksymbol_get_name_f)(char *name, int name_len, void *data);
 static inline void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,
 				      bool unregister, const char *sym)	{ }
+static inline void perf_event_bpf_event(struct bpf_prog *prog,
+					enum perf_bpf_event_type type,
+					u16 flags)			{ }
 static inline void perf_event_exec(void)				{ }
 static inline void perf_event_comm(struct task_struct *tsk, bool exec)	{ }
 static inline void perf_event_namespaces(struct task_struct *tsk)	{ }

commit 76193a94522f1d4edf2447a536f3f796ce56343b
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:13 2019 -0800

    perf, bpf: Introduce PERF_RECORD_KSYMBOL
    
    For better performance analysis of dynamically JITed and loaded kernel
    functions, such as BPF programs, this patch introduces
    PERF_RECORD_KSYMBOL, a new perf_event_type that exposes kernel symbol
    register/unregister information to user space.
    
    The following data structure is used for PERF_RECORD_KSYMBOL.
    
        /*
         * struct {
         *      struct perf_event_header        header;
         *      u64                             addr;
         *      u32                             len;
         *      u16                             ksym_type;
         *      u16                             flags;
         *      char                            name[];
         *      struct sample_id                sample_id;
         * };
         */
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-2-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4eb88065a9b5..136fe0495374 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1122,6 +1122,10 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);
+
+extern void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,
+			       bool unregister, const char *sym);
+
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
@@ -1342,6 +1346,10 @@ static inline int perf_unregister_guest_info_callbacks
 (struct perf_guest_info_callbacks *callbacks)				{ return 0; }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
+
+typedef int (perf_ksymbol_get_name_f)(char *name, int name_len, void *data);
+static inline void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,
+				      bool unregister, const char *sym)	{ }
 static inline void perf_event_exec(void)				{ }
 static inline void perf_event_comm(struct task_struct *tsk, bool exec)	{ }
 static inline void perf_event_namespaces(struct task_struct *tsk)	{ }

commit 5620196951192f7cd2da0a04e7c0113f40bfc14e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Jan 11 13:20:20 2019 -0300

    perf: Make perf_event_output() propagate the output() return
    
    For the original mode of operation it isn't needed, since we report back
    errors via PERF_RECORD_LOST records in the ring buffer, but for use in
    bpf_perf_event_output() it is convenient to return the errors, basically
    -ENOSPC.
    
    Currently bpf_perf_event_output() returns an error indication, the last
    thing it does, which is to push it to the ring buffer is that can fail
    and if so, this failure won't be reported back to its users, fix it.
    
    Reported-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Tested-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: https://lkml.kernel.org/r/20190118150938.GN5823@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f8ec36197718..4eb88065a9b5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -978,9 +978,9 @@ extern void perf_event_output_forward(struct perf_event *event,
 extern void perf_event_output_backward(struct perf_event *event,
 				       struct perf_sample_data *data,
 				       struct pt_regs *regs);
-extern void perf_event_output(struct perf_event *event,
-			      struct perf_sample_data *data,
-			      struct pt_regs *regs);
+extern int perf_event_output(struct perf_event *event,
+			     struct perf_sample_data *data,
+			     struct pt_regs *regs);
 
 static inline bool
 is_default_overflow_handler(struct perf_event *event)

commit cf5c6c211b7e9eb4f4219f83671432c9ef257187
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Thu Jan 17 15:25:04 2019 +0800

    perf: Remove duplicated workqueue.h include from perf_event.h
    
    It is already included a little bit higher up in that file.
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20190117072504.14428-1-yuehaibing@huawei.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cec02dc63b51..f8ec36197718 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -53,7 +53,6 @@ struct perf_guest_info_callbacks {
 #include <linux/atomic.h>
 #include <linux/sysfs.h>
 #include <linux/perf_regs.h>
-#include <linux/workqueue.h>
 #include <linux/cgroup.h>
 #include <asm/local.h>
 

commit cc6795aeffea0a80d0baf9ad31ba926a6c42cef5
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Thu Jan 10 13:53:25 2019 +0000

    perf/core: Add PERF_PMU_CAP_NO_EXCLUDE for exclusion incapable PMUs
    
    Many PMU drivers do not have the capability to exclude counting events
    that occur in specific contexts such as idle, kernel, guest, etc. These
    drivers indicate this by returning an error in their event_init upon
    testing the events attribute flags. This approach is error prone and
    often inconsistent.
    
    Let's instead allow PMU drivers to advertise their inability to exclude
    based on context via a new capability: PERF_PMU_CAP_NO_EXCLUDE. This
    allows the perf core to reject requests for exclusion events where
    there is no support in the PMU.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Sascha Hauer <s.hauer@pengutronix.de>
    Cc: Shawn Guo <shawnguo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: robin.murphy@arm.com
    Cc: suzuki.poulose@arm.com
    Link: https://lkml.kernel.org/r/1547128414-50693-4-git-send-email-andrew.murray@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 54a78d22f0a6..cec02dc63b51 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -244,6 +244,7 @@ struct perf_event;
 #define PERF_PMU_CAP_EXCLUSIVE			0x10
 #define PERF_PMU_CAP_ITRACE			0x20
 #define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40
+#define PERF_PMU_CAP_NO_EXCLUDE			0x80
 
 /**
  * struct pmu - generic performance monitoring unit

commit 486efe9f8e30bac1e236f867df164f4966f3e207
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Thu Jan 10 13:53:24 2019 +0000

    perf/core: Add function to test for event exclusion flags
    
    Add a function that tests if any of the perf event exclusion flags
    are set on a given event.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Sascha Hauer <s.hauer@pengutronix.de>
    Cc: Shawn Guo <shawnguo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: robin.murphy@arm.com
    Cc: suzuki.poulose@arm.com
    Link: https://lkml.kernel.org/r/1547128414-50693-3-git-send-email-andrew.murray@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1d5c551a5add..54a78d22f0a6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1004,6 +1004,15 @@ perf_event__output_id_sample(struct perf_event *event,
 extern void
 perf_log_lost_samples(struct perf_event *event, u64 lost);
 
+static inline bool event_has_any_exclude_flag(struct perf_event *event)
+{
+	struct perf_event_attr *attr = &event->attr;
+
+	return attr->exclude_idle || attr->exclude_user ||
+	       attr->exclude_kernel || attr->exclude_hv ||
+	       attr->exclude_guest || attr->exclude_host;
+}
+
 static inline bool is_sampling_event(struct perf_event *event)
 {
 	return event->attr.sample_period != 0;

commit 43b9e4febc66b98d83cc1560196d56ac7fef3c32
Author: Mukesh Ojha <mojha@codeaurora.org>
Date:   Tue Nov 27 14:43:32 2018 +0530

    perf/core: Declare the __percpu attribute on non-deref types
    
    Sparse reports the current declaration of two perf percpu variables
    with this warning:
    
      warning: incorrect type in initializer (different address spaces)
             expected void const [noderef] <asn:3>*__vpp_verify
             got struct perf_cpu_context *<noident>
    
    While it's normally perfectly fine to place GCC attributes anywhere
    in the definition, this particular attribute is for a checking
    compiler's such as Sparse's benefit, which doesn't want __percpu
    on pointers.
    
    So reorder the attribute to come after the structure type, not after
    the pointer type.
    
    [ mingo: Rewrote the changelog. ]
    
    Signed-off-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1543310012-7967-1-git-send-email-mojha@codeaurora.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 53c500f0ca79..1d5c551a5add 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -262,8 +262,8 @@ struct pmu {
 	 */
 	int				capabilities;
 
-	int * __percpu			pmu_disable_count;
-	struct perf_cpu_context * __percpu pmu_cpu_context;
+	int __percpu			*pmu_disable_count;
+	struct perf_cpu_context __percpu *pmu_cpu_context;
 	atomic_t			exclusive_cnt; /* < 0: cpu; > 0: tsk */
 	int				task_ctx_nr;
 	int				hrtimer_interval_ms;

commit 93081caaaed6a40a4f6d9b7ba3f581a4bb1d4404
Merge: 788faab70d5a 7f635ff187ab
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jul 25 11:47:02 2018 +0200

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6cbc304f2f360f25cc8607817239d6f4a2fd3dc5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 10 15:48:41 2018 +0200

    perf/x86/intel: Fix unwind errors from PEBS entries (mk-II)
    
    Vince reported the perf_fuzzer giving various unwinder warnings and
    Josh reported:
    
    > Deja vu.  Most of these are related to perf PEBS, similar to the
    > following issue:
    >
    >   b8000586c90b ("perf/x86/intel: Cure bogus unwind from PEBS entries")
    >
    > This is basically the ORC version of that.  setup_pebs_sample_data() is
    > assembling a franken-pt_regs which ORC isn't happy about.  RIP is
    > inconsistent with some of the other registers (like RSP and RBP).
    
    And where the previous unwinder only needed BP,SP ORC also requires
    IP. But we cannot spoof IP because then the sample will get displaced,
    entirely negating the point of PEBS.
    
    So cure the whole thing differently by doing the unwind early; this
    does however require a means to communicate we did the unwind early.
    We (ab)use an unused sample_type bit for this, which we set on events
    that fill out the data->callchain before the normal
    perf_prepare_sample().
    
    Debugged-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1fa12887ec02..87f6db437e4a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1130,6 +1130,7 @@ extern void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct
 extern struct perf_callchain_entry *
 get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
 		   u32 max_stack, bool crosstask, bool add_mark);
+extern struct perf_callchain_entry *perf_callchain(struct perf_event *event, struct pt_regs *regs);
 extern int get_callchain_buffers(int max_stack);
 extern void put_callchain_buffers(void);
 

commit 788faab70d5a882693286b8d5022779559c79904
Author: Tobias Tefke <tobias.tefke@gmail.com>
Date:   Mon Jul 9 12:57:15 2018 +0200

    perf, tools: Use correct articles in comments
    
    Some of the comments in the perf events code use articles incorrectly,
    using 'a' for words beginning with a vowel sound, where 'an' should be
    used.
    
    Signed-off-by: Tobias Tefke <tobias.tefke@tutanota.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: alexander.shishkin@linux.intel.com
    Cc: jolsa@redhat.com
    Cc: namhyung@kernel.org
    Link: http://lkml.kernel.org/r/20180709105715.22938-1-tobias.tefke@tutanota.com
    [ Fix a few more perf related 'a event' typo fixes from all around the kernel and tooling tree. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1fa12887ec02..e6dd3a2f8ec4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -490,7 +490,7 @@ struct perf_addr_filters_head {
 };
 
 /**
- * enum perf_event_state - the states of a event
+ * enum perf_event_state - the states of an event:
  */
 enum perf_event_state {
 	PERF_EVENT_STATE_DEAD		= -4,

commit 1c8c5a9d38f607c0b6fd12c91cbe1a4418762a21
Merge: 285767604576 7170e6045a6a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 6 18:39:49 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add Maglev hashing scheduler to IPVS, from Inju Song.
    
     2) Lots of new TC subsystem tests from Roman Mashak.
    
     3) Add TCP zero copy receive and fix delayed acks and autotuning with
        SO_RCVLOWAT, from Eric Dumazet.
    
     4) Add XDP_REDIRECT support to mlx5 driver, from Jesper Dangaard
        Brouer.
    
     5) Add ttl inherit support to vxlan, from Hangbin Liu.
    
     6) Properly separate ipv6 routes into their logically independant
        components. fib6_info for the routing table, and fib6_nh for sets of
        nexthops, which thus can be shared. From David Ahern.
    
     7) Add bpf_xdp_adjust_tail helper, which can be used to generate ICMP
        messages from XDP programs. From Nikita V. Shirokov.
    
     8) Lots of long overdue cleanups to the r8169 driver, from Heiner
        Kallweit.
    
     9) Add BTF ("BPF Type Format"), from Martin KaFai Lau.
    
    10) Add traffic condition monitoring to iwlwifi, from Luca Coelho.
    
    11) Plumb extack down into fib_rules, from Roopa Prabhu.
    
    12) Add Flower classifier offload support to igb, from Vinicius Costa
        Gomes.
    
    13) Add UDP GSO support, from Willem de Bruijn.
    
    14) Add documentation for eBPF helpers, from Quentin Monnet.
    
    15) Add TLS tx offload to mlx5, from Ilya Lesokhin.
    
    16) Allow applications to be given the number of bytes available to read
        on a socket via a control message returned from recvmsg(), from
        Soheil Hassas Yeganeh.
    
    17) Add x86_32 eBPF JIT compiler, from Wang YanQing.
    
    18) Add AF_XDP sockets, with zerocopy support infrastructure as well.
        From Björn Töpel.
    
    19) Remove indirect load support from all of the BPF JITs and handle
        these operations in the verifier by translating them into native BPF
        instead. From Daniel Borkmann.
    
    20) Add GRO support to ipv6 gre tunnels, from Eran Ben Elisha.
    
    21) Allow XDP programs to do lookups in the main kernel routing tables
        for forwarding. From David Ahern.
    
    22) Allow drivers to store hardware state into an ELF section of kernel
        dump vmcore files, and use it in cxgb4. From Rahul Lakkireddy.
    
    23) Various RACK and loss detection improvements in TCP, from Yuchung
        Cheng.
    
    24) Add TCP SACK compression, from Eric Dumazet.
    
    25) Add User Mode Helper support and basic bpfilter infrastructure, from
        Alexei Starovoitov.
    
    26) Support ports and protocol values in RTM_GETROUTE, from Roopa
        Prabhu.
    
    27) Support bulking in ->ndo_xdp_xmit() API, from Jesper Dangaard
        Brouer.
    
    28) Add lots of forwarding selftests, from Petr Machata.
    
    29) Add generic network device failover driver, from Sridhar Samudrala.
    
    * ra.kernel.org:/pub/scm/linux/kernel/git/davem/net-next: (1959 commits)
      strparser: Add __strp_unpause and use it in ktls.
      rxrpc: Fix terminal retransmission connection ID to include the channel
      net: hns3: Optimize PF CMDQ interrupt switching process
      net: hns3: Fix for VF mailbox receiving unknown message
      net: hns3: Fix for VF mailbox cannot receiving PF response
      bnx2x: use the right constant
      Revert "net: sched: cls: Fix offloading when ingress dev is vxlan"
      net: dsa: b53: Fix for brcm tag issue in Cygnus SoC
      enic: fix UDP rss bits
      netdev-FAQ: clarify DaveM's position for stable backports
      rtnetlink: validate attributes in do_setlink()
      mlxsw: Add extack messages for port_{un, }split failures
      netdevsim: Add extack error message for devlink reload
      devlink: Add extack to reload and port_{un, }split operations
      net: metrics: add proper netlink validation
      ipmr: fix error path when ipmr_new_table fails
      ip6mr: only set ip6mr_table from setsockopt when ip6mr_new_table succeeds
      net: hns3: remove unused hclgevf_cfg_func_mta_filter
      netfilter: provide udp*_lib_lookup for nf_tproxy
      qed*: Utilize FW 8.37.2.0
      ...

commit 9511bce9fe8e5e6c0f923c09243a713eba560141
Author: Song Liu <songliubraving@fb.com>
Date:   Tue Apr 17 23:29:07 2018 -0700

    perf/core: Fix bad use of igrab()
    
    As Miklos reported and suggested:
    
     "This pattern repeats two times in trace_uprobe.c and in
      kernel/events/core.c as well:
    
          ret = kern_path(filename, LOOKUP_FOLLOW, &path);
          if (ret)
              goto fail_address_parse;
    
          inode = igrab(d_inode(path.dentry));
          path_put(&path);
    
      And it's wrong.  You can only hold a reference to the inode if you
      have an active ref to the superblock as well (which is normally
      through path.mnt) or holding s_umount.
    
      This way unmounting the containing filesystem while the tracepoint is
      active will give you the "VFS: Busy inodes after unmount..." message
      and a crash when the inode is finally put.
    
      Solution: store path instead of inode."
    
    This patch fixes the issue in kernel/event/core.c.
    
    Reviewed-and-tested-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Reported-by: Miklos Szeredi <miklos@szeredi.hu>
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <kernel-team@fb.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Fixes: 375637bc5249 ("perf/core: Introduce address range filtering")
    Link: http://lkml.kernel.org/r/20180418062907.3210386-2-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index def866f7269b..bea0b0cd4bf7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -467,7 +467,7 @@ enum perf_addr_filter_action_t {
  */
 struct perf_addr_filter {
 	struct list_head	entry;
-	struct inode		*inode;
+	struct path		path;
 	unsigned long		offset;
 	unsigned long		size;
 	enum perf_addr_filter_action_t	action;

commit a1150c202207cc8501bebc45b63c264f91959260
Author: Song Liu <songliubraving@fb.com>
Date:   Thu May 3 12:47:16 2018 -0700

    perf/core: Fix group scheduling with mixed hw and sw events
    
    When hw and sw events are mixed in the same group, they are all attached
    to the hw perf_event_context. This sometimes requires moving group of
    perf_event to a different context.
    
    We found a bug in how the kernel handles this, for example if we do:
    
       perf stat -e '{faults,ref-cycles,faults}'  -I 1000
    
         1.005591180              1,297      faults
         1.005591180        457,476,576      ref-cycles
         1.005591180    <not supported>      faults
    
    First, sw event "faults" is attached to the sw context, and becomes the
    group leader. Then, hw event "ref-cycles" is attached, so both events
    are moved to the hw context. Last, another sw "faults" tries to attach,
    but it fails because of mismatch between the new target ctx (from sw
    pmu) and the group_leader's ctx (hw context, same as ref-cycles).
    
    The broken condition is:
       group_leader is sw event;
       group_leader is on hw context;
       add a sw event to the group.
    
    Fix this scenario by checking group_leader's context (instead of just
    event type). If group_leader is on hw context, use the ->pmu of this
    context to look up context for the new event.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <kernel-team@fb.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Fixes: b04243ef7006 ("perf: Complete software pmu grouping")
    Link: http://lkml.kernel.org/r/20180503194716.162815-1-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e71e99eb9a4e..def866f7269b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1016,6 +1016,14 @@ static inline int is_software_event(struct perf_event *event)
 	return event->event_caps & PERF_EV_CAP_SOFTWARE;
 }
 
+/*
+ * Return 1 for event in sw context, 0 for event in hw context
+ */
+static inline int in_software_context(struct perf_event *event)
+{
+	return event->ctx->pmu->task_ctx_nr == perf_sw_context;
+}
+
 extern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void ___perf_sw_event(u32, u64, struct pt_regs *, u64);

commit f8d959a5b188dc81e57a6bac34a1b2986f61e2fd
Author: Yonghong Song <yhs@fb.com>
Date:   Thu May 24 11:21:08 2018 -0700

    perf/core: add perf_get_event() to return perf_event given a struct file
    
    A new extern function, perf_get_event(), is added to return a perf event
    given a struct file. This function will be used in later patches.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e71e99eb9a4e..eec302b61cfe 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -868,6 +868,7 @@ extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
 extern void perf_event_delayed_put(struct task_struct *task);
 extern struct file *perf_event_get(unsigned int fd);
+extern const struct perf_event *perf_get_event(struct file *file);
 extern const struct perf_event_attr *perf_event_attrs(struct perf_event *event);
 extern void perf_event_print_debug(void);
 extern void perf_pmu_disable(struct pmu *pmu);
@@ -1289,6 +1290,10 @@ static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
 static inline void perf_event_delayed_put(struct task_struct *task)	{ }
 static inline struct file *perf_event_get(unsigned int fd)	{ return ERR_PTR(-EINVAL); }
+static inline const struct perf_event *perf_get_event(struct file *file)
+{
+	return ERR_PTR(-EINVAL);
+}
 static inline const struct perf_event_attr *perf_event_attrs(struct perf_event *event)
 {
 	return ERR_PTR(-EINVAL);

commit 6ed70cf342de03c7b11cd4eb032705faeb29d284
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Thu Mar 29 15:06:48 2018 +0300

    perf/x86/pt, coresight: Clean up address filter structure
    
    This is a cosmetic patch that deals with the address filter structure's
    ambiguous fields 'filter' and 'range'. The former stands to mean that the
    filter's *action* should be to filter the traces to its address range if
    it's set or stop tracing if it's unset. This is confusing and hard on the
    eyes, so this patch replaces it with 'action' enum. The 'range' field is
    completely redundant (meaning that the filter is an address range as
    opposed to a single address trigger), as we can use zero size to mean the
    same thing.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Acked-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20180329120648.11902-1-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ff39ab011376..e71e99eb9a4e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -449,14 +449,19 @@ struct pmu {
 	int (*filter_match)		(struct perf_event *event); /* optional */
 };
 
+enum perf_addr_filter_action_t {
+	PERF_ADDR_FILTER_ACTION_STOP = 0,
+	PERF_ADDR_FILTER_ACTION_START,
+	PERF_ADDR_FILTER_ACTION_FILTER,
+};
+
 /**
  * struct perf_addr_filter - address range filter definition
  * @entry:	event's filter list linkage
  * @inode:	object file's inode for file-based filters
  * @offset:	filter range offset
- * @size:	filter range size
- * @range:	1: range, 0: address
- * @filter:	1: filter/start, 0: stop
+ * @size:	filter range size (size==0 means single address trigger)
+ * @action:	filter/start/stop
  *
  * This is a hardware-agnostic filter configuration as specified by the user.
  */
@@ -465,8 +470,7 @@ struct perf_addr_filter {
 	struct inode		*inode;
 	unsigned long		offset;
 	unsigned long		size;
-	unsigned int		range	: 1,
-				filter	: 1;
+	enum perf_addr_filter_action_t	action;
 };
 
 /**

commit edb39592a5877bd91b2e6ee15194268f35b04892
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 15 17:36:56 2018 +0100

    perf: Fix sibling iteration
    
    Mark noticed that the change to sibling_list changed some iteration
    semantics; because previously we used group_list as list entry,
    sibling events would always have an empty sibling_list.
    
    But because we now use sibling_list for both list head and list entry,
    siblings will report as having siblings.
    
    Fix this with a custom for_each_sibling_event() iterator.
    
    Fixes: 8343aae66167 ("perf/core: Remove perf_event::group_entry")
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: vincent.weaver@maine.edu
    Cc: alexander.shishkin@linux.intel.com
    Cc: torvalds@linux-foundation.org
    Cc: alexey.budankov@linux.intel.com
    Cc: valery.cherepennikov@intel.com
    Cc: eranian@google.com
    Cc: acme@redhat.com
    Cc: linux-tip-commits@vger.kernel.org
    Cc: davidcc@google.com
    Cc: kan.liang@intel.com
    Cc: Dmitry.Prohorov@intel.com
    Cc: jolsa@redhat.com
    Link: https://lkml.kernel.org/r/20180315170129.GX4043@hirez.programming.kicks-ass.net

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2bb200e1bbea..ff39ab011376 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -536,6 +536,10 @@ struct pmu_event_list {
 	struct list_head	list;
 };
 
+#define for_each_sibling_event(sibling, event)			\
+	if ((event)->group_leader == (event))			\
+		list_for_each_entry((sibling), &(event)->sibling_list, sibling_list)
+
 /**
  * struct perf_event - performance event kernel representation:
  */

commit 6668128a9e25f7a11d25359e46df2541e6b43fc9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 13 14:28:38 2017 +0100

    perf/core: Optimize ctx_sched_out()
    
    When an event group contains more events than can be scheduled on the
    hardware, iterating the full event group for ctx_sched_out is a waste
    of time.
    
    Keep track of the events that got programmed on the hardware, such
    that we can iterate this smaller list in order to schedule them out.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Carrillo-Cisneros <davidcc@google.com>
    Cc: Dmitri Prokhorov <Dmitry.Prohorov@intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valery Cherepennikov <valery.cherepennikov@intel.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 84044ec21b31..2bb200e1bbea 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -553,6 +553,7 @@ struct perf_event {
 	 * either sufficies for read.
 	 */
 	struct list_head		sibling_list;
+	struct list_head		active_list;
 	/*
 	 * Node on the pinned or flexible tree located at the event context;
 	 */
@@ -718,6 +719,10 @@ struct perf_event_context {
 	struct perf_event_groups	pinned_groups;
 	struct perf_event_groups	flexible_groups;
 	struct list_head		event_list;
+
+	struct list_head		pinned_active;
+	struct list_head		flexible_active;
+
 	int				nr_events;
 	int				nr_active;
 	int				is_active;

commit 8343aae66167df6708128a778e750d48dbe31302
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 13 14:28:33 2017 +0100

    perf/core: Remove perf_event::group_entry
    
    Now that all the grouping is done with RB trees, we no longer need
    group_entry and can replace the whole thing with sibling_list.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Carrillo-Cisneros <davidcc@google.com>
    Cc: Dmitri Prokhorov <Dmitry.Prohorov@intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valery Cherepennikov <valery.cherepennikov@intel.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6e3f854a34d8..84044ec21b31 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -549,14 +549,9 @@ struct perf_event {
 	struct list_head		event_entry;
 
 	/*
-	 * XXX: group_entry and sibling_list should be mutually exclusive;
-	 * either you're a sibling on a group, or you're the group leader.
-	 * Rework the code to always use the same list element.
-	 *
 	 * Locked for modification by both ctx->mutex and ctx->lock; holding
 	 * either sufficies for read.
 	 */
-	struct list_head		group_entry;
 	struct list_head		sibling_list;
 	/*
 	 * Node on the pinned or flexible tree located at the event context;

commit 8e1a2031e4b556b01ca53cd1fb2d83d811a6605b
Author: Alexey Budankov <alexey.budankov@linux.intel.com>
Date:   Fri Sep 8 11:47:03 2017 +0300

    perf/cor: Use RB trees for pinned/flexible groups
    
    Change event groups into RB trees sorted by CPU and then by a 64bit
    index, so that multiplexing hrtimer interrupt handler would be able
    skipping to the current CPU's list and ignore groups allocated for the
    other CPUs.
    
    New API for manipulating event groups in the trees is implemented as well
    as adoption on the API in the current implementation.
    
    pinned_group_sched_in() and flexible_group_sched_in() API are
    introduced to consolidate code enabling the whole group from pinned
    and flexible groups appropriately.
    
    Signed-off-by: Alexey Budankov <alexey.budankov@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: David Carrillo-Cisneros <davidcc@google.com>
    Cc: Dmitri Prokhorov <Dmitry.Prohorov@intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valery Cherepennikov <valery.cherepennikov@intel.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/372f9c8b-0cfe-4240-e44d-83d863d40813@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7546822a1d74..6e3f854a34d8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -558,7 +558,11 @@ struct perf_event {
 	 */
 	struct list_head		group_entry;
 	struct list_head		sibling_list;
-
+	/*
+	 * Node on the pinned or flexible tree located at the event context;
+	 */
+	struct rb_node			group_node;
+	u64				group_index;
 	/*
 	 * We need storage to track the entries in perf_pmu_migrate_context; we
 	 * cannot use the event_entry because of RCU and we want to keep the
@@ -690,6 +694,12 @@ struct perf_event {
 #endif /* CONFIG_PERF_EVENTS */
 };
 
+
+struct perf_event_groups {
+	struct rb_root	tree;
+	u64		index;
+};
+
 /**
  * struct perf_event_context - event context structure
  *
@@ -710,8 +720,8 @@ struct perf_event_context {
 	struct mutex			mutex;
 
 	struct list_head		active_ctx_list;
-	struct list_head		pinned_groups;
-	struct list_head		flexible_groups;
+	struct perf_event_groups	pinned_groups;
+	struct perf_event_groups	flexible_groups;
 	struct list_head		event_list;
 	int				nr_events;
 	int				nr_active;

commit c895f6f703ad7dd2f99e751d9884b0aa5d0eea25
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Mon Dec 4 10:56:44 2017 +0100

    bpf: correct broken uapi for BPF_PROG_TYPE_PERF_EVENT program type
    
    Commit 0515e5999a466dfe ("bpf: introduce BPF_PROG_TYPE_PERF_EVENT
    program type") introduced the bpf_perf_event_data structure which
    exports the pt_regs structure.  This is OK for multiple architectures
    but fail for s390 and arm64 which do not export pt_regs.  Programs
    using them, for example, the bpf selftest fail to compile on these
    architectures.
    
    For s390, exporting the pt_regs is not an option because s390 wants
    to allow changes to it.  For arm64, there is a user_pt_regs structure
    that covers parts of the pt_regs structure for use by user space.
    
    To solve the broken uapi for s390 and arm64, introduce an abstract
    type for pt_regs and add an asm/bpf_perf_event.h file that concretes
    the type.  An asm-generic header file covers the architectures that
    export pt_regs today.
    
    The arch-specific enablement for s390 and arm64 follows in separate
    commits.
    
    Reported-by: Thomas Richter <tmricht@linux.vnet.ibm.com>
    Fixes: 0515e5999a466dfe ("bpf: introduce BPF_PROG_TYPE_PERF_EVENT program type")
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Reviewed-and-tested-by: Thomas Richter <tmricht@linux.vnet.ibm.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2c9c87d8a0c1..7546822a1d74 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -15,6 +15,7 @@
 #define _LINUX_PERF_EVENT_H
 
 #include <uapi/linux/perf_event.h>
+#include <uapi/linux/bpf_perf_event.h>
 
 /*
  * Kernel-internal data types and definitions:
@@ -787,7 +788,7 @@ struct perf_output_handle {
 };
 
 struct bpf_perf_event_data_kern {
-	struct pt_regs *regs;
+	bpf_user_pt_regs_t *regs;
 	struct perf_sample_data *data;
 	struct perf_event *event;
 };
@@ -1177,6 +1178,9 @@ extern void perf_bp_event(struct perf_event *event, void *data);
 		(user_mode(regs) ? PERF_RECORD_MISC_USER : PERF_RECORD_MISC_KERNEL)
 # define perf_instruction_pointer(regs)	instruction_pointer(regs)
 #endif
+#ifndef perf_arch_bpf_user_pt_regs
+# define perf_arch_bpf_user_pt_regs(regs) regs
+#endif
 
 static inline bool has_branch_stack(struct perf_event *event)
 {

commit 2dcd9c71c1ffa9a036e09047f60e08383bb0abb6
Merge: b1c2a344cc19 a96a5037ed0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 17 14:58:01 2017 -0800

    Merge tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from
    
     - allow module init functions to be traced
    
     - clean up some unused or not used by config events (saves space)
    
     - clean up of trace histogram code
    
     - add support for preempt and interrupt enabled/disable events
    
     - other various clean ups
    
    * tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (30 commits)
      tracing, thermal: Hide cpu cooling trace events when not in use
      tracing, thermal: Hide devfreq trace events when not in use
      ftrace: Kill FTRACE_OPS_FL_PER_CPU
      perf/ftrace: Small cleanup
      perf/ftrace: Fix function trace events
      perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
      tracing, dma-buf: Remove unused trace event dma_fence_annotate_wait_on
      tracing, memcg, vmscan: Hide trace events when not in use
      tracing/xen: Hide events that are not used when X86_PAE is not defined
      tracing: mark trace_test_buffer as __maybe_unused
      printk: Remove superfluous memory barriers from printk_safe
      ftrace: Clear hashes of stale ips of init memory
      tracing: Add support for preempt and irq enable/disable events
      tracing: Prepare to add preempt and irq trace events
      ftrace/kallsyms: Have /proc/kallsyms show saved mod init functions
      ftrace: Add freeing algorithm to free ftrace_mod_maps
      ftrace: Save module init functions kallsyms symbols for tracing
      ftrace: Allow module init functions to be traced
      ftrace: Add a ftrace_free_mem() function for modules to use
      tracing: Reimplement log2
      ...

commit 0d3d73aac2ff05c78387aa9dcc2c8aa3804405e7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 5 14:16:28 2017 +0200

    perf/core: Rewrite event timekeeping
    
    The current even timekeeping, which computes enabled and running
    times, uses 3 distinct timestamps to reflect the various event states:
    OFF (stopped), INACTIVE (enabled) and ACTIVE (running).
    
    Furthermore, the update rules are such that even INACTIVE events need
    their timestamps updated. This is undesirable because we'd like to not
    touch INACTIVE events if at all possible, this makes event scheduling
    (much) more expensive than needed.
    
    Rewrite the timekeeping to directly use event->state, this greatly
    simplifies the code and results in only having to update things when
    we change state, or an up-to-date value is requested (read).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b7532650de47..874b71a70058 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -588,26 +588,10 @@ struct perf_event {
 	 * has been enabled (i.e. eligible to run, and the task has
 	 * been scheduled in, if this is a per-task event)
 	 * and running (scheduled onto the CPU), respectively.
-	 *
-	 * They are computed from tstamp_enabled, tstamp_running and
-	 * tstamp_stopped when the event is in INACTIVE or ACTIVE state.
 	 */
 	u64				total_time_enabled;
 	u64				total_time_running;
-
-	/*
-	 * These are timestamps used for computing total_time_enabled
-	 * and total_time_running when the event is in INACTIVE or
-	 * ACTIVE state, measured in nanoseconds from an arbitrary point
-	 * in time.
-	 * tstamp_enabled: the notional time when the event was enabled
-	 * tstamp_running: the notional time when the event was scheduled on
-	 * tstamp_stopped: in INACTIVE state, the notional time when the
-	 *	event was scheduled off.
-	 */
-	u64				tstamp_enabled;
-	u64				tstamp_running;
-	u64				tstamp_stopped;
+	u64				tstamp;
 
 	/*
 	 * timestamp shadows the actual context timing but it can
@@ -699,7 +683,6 @@ struct perf_event {
 
 #ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp; /* cgroup event is attach to */
-	int				cgrp_defer_enabled;
 #endif
 
 	struct list_head		sb_list;

commit 8ca2bd41c7d1c135e9ac6f25970c2d491865088a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 5 14:12:35 2017 +0200

    perf/core: Rename 'enum perf_event_active_state'
    
    Its a weird name, active is one of the states, it should not be part
    of the name, also, its too long.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 79b18a20cf5d..b7532650de47 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -485,9 +485,9 @@ struct perf_addr_filters_head {
 };
 
 /**
- * enum perf_event_active_state - the states of a event
+ * enum perf_event_state - the states of a event
  */
-enum perf_event_active_state {
+enum perf_event_state {
 	PERF_EVENT_STATE_DEAD		= -4,
 	PERF_EVENT_STATE_EXIT		= -3,
 	PERF_EVENT_STATE_ERROR		= -2,
@@ -578,7 +578,7 @@ struct perf_event {
 	struct pmu			*pmu;
 	void				*pmu_private;
 
-	enum perf_event_active_state	state;
+	enum perf_event_state		state;
 	unsigned int			attach_state;
 	local64_t			count;
 	atomic64_t			child_count;

commit 7d9285e82db5defca4d9674ba089429eeca0c697
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Oct 5 09:19:19 2017 -0700

    perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers"
    
    eBPF programs would like access to the (perf) event enabled and
    running times along with the event value, such that they can deal with
    event multiplexing (among other things).
    
    This patch extends the interface; a future eBPF patch will utilize
    the new functionality.
    
    [ Note, there's a same-content commit with a poor changelog and a meaningless
      title in the networking tree as well - but we need this change for subsequent
      perf work, so apply it here as well, with a proper changelog. Hopefully Git
      will be able to sort out this somewhat messy workflow, if there are no other,
      conflicting changes to these files. ]
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    [ Rewrote the changelog. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <ast@fb.com>
    Cc: <daniel@iogearbox.net>
    Cc: <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: David S. Miller <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/20171005161923.332790-2-yhs@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8e22f24ded6a..79b18a20cf5d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -806,6 +806,7 @@ struct perf_output_handle {
 struct bpf_perf_event_data_kern {
 	struct pt_regs *regs;
 	struct perf_sample_data *data;
+	struct perf_event *event;
 };
 
 #ifdef CONFIG_CGROUP_PERF
@@ -884,7 +885,8 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				void *context);
 extern void perf_pmu_migrate_context(struct pmu *pmu,
 				int src_cpu, int dst_cpu);
-int perf_event_read_local(struct perf_event *event, u64 *value);
+int perf_event_read_local(struct perf_event *event, u64 *value,
+			  u64 *enabled, u64 *running);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 
@@ -1286,7 +1288,8 @@ static inline const struct perf_event_attr *perf_event_attrs(struct perf_event *
 {
 	return ERR_PTR(-EINVAL);
 }
-static inline int perf_event_read_local(struct perf_event *event, u64 *value)
+static inline int perf_event_read_local(struct perf_event *event, u64 *value,
+					u64 *enabled, u64 *running)
 {
 	return -EINVAL;
 }

commit 8fd0fbbe8888f295eb34172a7e47bf7d3a0a4687
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 11 09:45:29 2017 +0200

    perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    Revert commit:
    
      75e8387685f6 ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    The reason I instantly stumbled on that patch is that it only addresses the
    ftrace situation and doesn't mention the other _5_ places that use this
    interface. It doesn't explain why those don't have the problem and if not, why
    their solution doesn't work for ftrace.
    
    It doesn't, but this is just putting more duct tape on.
    
    Link: http://lkml.kernel.org/r/20171011080224.200565770@infradead.org
    
    Cc: Zhou Chengming <zhouchengming1@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8e22f24ded6a..569d1b54e201 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1184,7 +1184,7 @@ extern void perf_event_init(void);
 extern void perf_tp_event(u16 event_type, u64 count, void *record,
 			  int entry_size, struct pt_regs *regs,
 			  struct hlist_head *head, int rctx,
-			  struct task_struct *task, struct perf_event *event);
+			  struct task_struct *task);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit f57091767add2b79d76aac41b83b192d8ba1dce7
Merge: d725c7ac8b96 d56593eb5eda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 13:56:37 2017 -0700

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache quality monitoring update from Thomas Gleixner:
     "This update provides a complete rewrite of the Cache Quality
      Monitoring (CQM) facility.
    
      The existing CQM support was duct taped into perf with a lot of issues
      and the attempts to fix those turned out to be incomplete and
      horrible.
    
      After lengthy discussions it was decided to integrate the CQM support
      into the Resource Director Technology (RDT) facility, which is the
      obvious choise as in hardware CQM is part of RDT. This allowed to add
      Memory Bandwidth Monitoring support on top.
    
      As a result the mechanisms for allocating cache/memory bandwidth and
      the corresponding monitoring mechanisms are integrated into a single
      management facility with a consistent user interface"
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/intel_rdt: Turn off most RDT features on Skylake
      x86/intel_rdt: Add command line options for resource director technology
      x86/intel_rdt: Move special case code for Haswell to a quirk function
      x86/intel_rdt: Remove redundant ternary operator on return
      x86/intel_rdt/cqm: Improve limbo list processing
      x86/intel_rdt/mbm: Fix MBM overflow handler during CPU hotplug
      x86/intel_rdt: Modify the intel_pqr_state for better performance
      x86/intel_rdt/cqm: Clear the default RMID during hotcpu
      x86/intel_rdt: Show bitmask of shareable resource with other executing units
      x86/intel_rdt/mbm: Handle counter overflow
      x86/intel_rdt/mbm: Add mbm counter initialization
      x86/intel_rdt/mbm: Basic counting of MBM events (total and local)
      x86/intel_rdt/cqm: Add CPU hotplug support
      x86/intel_rdt/cqm: Add sched_in support
      x86/intel_rdt: Introduce rdt_enable_key for scheduling
      x86/intel_rdt/cqm: Add mount,umount support
      x86/intel_rdt/cqm: Add rmdir support
      x86/intel_rdt: Separate the ctrl bits from rmdir
      x86/intel_rdt/cqm: Add mon_data
      x86/intel_rdt: Prepare for RDT monitor data support
      ...

commit fc7ce9c74c3ad232b084d80148654f926d01ece7
Author: Kan Liang <kan.liang@intel.com>
Date:   Mon Aug 28 20:52:49 2017 -0400

    perf/core, x86: Add PERF_SAMPLE_PHYS_ADDR
    
    For understanding how the workload maps to memory channels and hardware
    behavior, it's very important to collect address maps with physical
    addresses. For example, 3D XPoint access can only be found by filtering
    the physical address.
    
    Add a new sample type for physical address.
    
    perf already has a facility to collect data virtual address. This patch
    introduces a function to convert the virtual address to physical address.
    The function is quite generic and can be extended to any architecture as
    long as a virtual address is provided.
    
     - For kernel direct mapping addresses, virt_to_phys is used to convert
       the virtual addresses to physical address.
    
     - For user virtual addresses, __get_user_pages_fast is used to walk the
       pages tables for user physical address.
    
     - This does not work for vmalloc addresses right now. These are not
       resolved, but code to do that could be added.
    
    The new sample type requires collecting the virtual address. The
    virtual address will not be output unless SAMPLE_ADDR is applied.
    
    For security, the physical address can only be exposed to root or
    privileged user.
    
    Tested-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: mpe@ellerman.id.au
    Link: http://lkml.kernel.org/r/1503967969-48278-1-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index adda0aaae6c8..718ba163c1b9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -943,6 +943,8 @@ struct perf_sample_data {
 
 	struct perf_regs		regs_intr;
 	u64				stack_user_size;
+
+	u64				phys_addr;
 } ____cacheline_aligned;
 
 /* default value for data source */

commit 8d4e6c4caa12dafbcba138e5450b7af17b0b2194
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Thu Mar 30 18:39:56 2017 +0300

    perf/core, pt, bts: Get rid of itrace_started
    
    I just noticed that hw.itrace_started and hw.config are aliased to the
    same location. Now, the PT driver happens to use both, which works out
    fine by sheer luck:
    
     - STORE(hw.itrace_start) is ordered before STORE(hw.config), in the
        program order, although there are no compiler barriers to ensure that,
    
     - to the perf_log_itrace_start() hw.itrace_start looks set at the same
       time as when it is intended to be set because both stores happen in the
       same path,
    
     - hw.config is never reset to zero in the PT driver.
    
    Now, the use of hw.config by the PT driver makes more sense (it being a
    HW PMU) than messing around with itrace_started, which is an awkward API
    to begin with.
    
    This patch replaces hw.itrace_started with an attach_state bit and an
    API call for the PMU drivers to use to communicate the condition.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20170330153956.25994-1-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c00cd4b02f32..adda0aaae6c8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -147,9 +147,6 @@ struct hw_perf_event {
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;
 		};
-		struct { /* itrace */
-			int			itrace_started;
-		};
 		struct { /* amd_power */
 			u64	pwr_acc;
 			u64	ptsc;
@@ -541,6 +538,7 @@ struct swevent_hlist {
 #define PERF_ATTACH_GROUP	0x02
 #define PERF_ATTACH_TASK	0x04
 #define PERF_ATTACH_TASK_DATA	0x08
+#define PERF_ATTACH_ITRACE	0x10
 
 struct perf_cgroup;
 struct ring_buffer;
@@ -864,6 +862,7 @@ extern int perf_aux_output_skip(struct perf_output_handle *handle,
 				unsigned long size);
 extern void *perf_get_aux(struct perf_output_handle *handle);
 extern void perf_aux_output_flag(struct perf_output_handle *handle, u64 flags);
+extern void perf_event_itrace_started(struct perf_event *event);
 
 extern int perf_pmu_register(struct pmu *pmu, const char *name, int type);
 extern void perf_pmu_unregister(struct pmu *pmu);

commit 75e8387685f6c65feb195a4556110b58f852b848
Author: Zhou Chengming <zhouchengming1@huawei.com>
Date:   Fri Aug 25 21:49:37 2017 +0800

    perf/ftrace: Fix double traces of perf on ftrace:function
    
    When running perf on the ftrace:function tracepoint, there is a bug
    which can be reproduced by:
    
      perf record -e ftrace:function -a sleep 20 &
      perf record -e ftrace:function ls
      perf script
    
                  ls 10304 [005]   171.853235: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853237: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853239: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853240: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853242: ftrace:function:
      __task_pid_nr_ns
                  ls 10304 [005]   171.853244: ftrace:function:
      __task_pid_nr_ns
    
    We can see that all the function traces are doubled.
    
    The problem is caused by the inconsistency of the register
    function perf_ftrace_event_register() with the probe function
    perf_ftrace_function_call(). The former registers one probe
    for every perf_event. And the latter handles all perf_events
    on the current cpu. So when two perf_events on the current cpu,
    the traces of them will be doubled.
    
    So this patch adds an extra parameter "event" for perf_tp_event,
    only send sample data to this event when it's not NULL.
    
    Signed-off-by: Zhou Chengming <zhouchengming1@huawei.com>
    Reviewed-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: alexander.shishkin@linux.intel.com
    Cc: huawei.libin@huawei.com
    Link: http://lkml.kernel.org/r/1503668977-12526-1-git-send-email-zhouchengming1@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b14095bcf4bb..c00cd4b02f32 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1201,7 +1201,7 @@ extern void perf_event_init(void);
 extern void perf_tp_event(u16 event_type, u64 count, void *record,
 			  int entry_size, struct pt_regs *regs,
 			  struct hlist_head *head, int rctx,
-			  struct task_struct *task);
+			  struct task_struct *task, struct perf_event *event);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit bfe334924ccd9f4a53f30240c03cf2f43f5b2df1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 2 19:39:30 2017 +0200

    perf/x86: Fix RDPMC vs. mm_struct tracking
    
    Vince reported the following rdpmc() testcase failure:
    
     > Failing test case:
     >
     >      fd=perf_event_open();
     >      addr=mmap(fd);
     >      exec()  // without closing or unmapping the event
     >      fd=perf_event_open();
     >      addr=mmap(fd);
     >      rdpmc() // GPFs due to rdpmc being disabled
    
    The problem is of course that exec() plays tricks with what is
    current->mm, only destroying the old mappings after having
    installed the new mm.
    
    Fix this confusion by passing along vma->vm_mm instead of relying on
    current->mm.
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: 1e0fb9ec679c ("perf: Add pmu callbacks to track event mapping and unmapping")
    Link: http://lkml.kernel.org/r/20170802173930.cstykcqefmqt7jau@hirez.programming.kicks-ass.net
    [ Minor cleanups. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a3b873fc59e4..b14095bcf4bb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -310,8 +310,8 @@ struct pmu {
 	 * Notification that the event was mapped or unmapped.  Called
 	 * in the context of the mapping task.
 	 */
-	void (*event_mapped)		(struct perf_event *event); /*optional*/
-	void (*event_unmapped)		(struct perf_event *event); /*optional*/
+	void (*event_mapped)		(struct perf_event *event, struct mm_struct *mm); /* optional */
+	void (*event_unmapped)		(struct perf_event *event, struct mm_struct *mm); /* optional */
 
 	/*
 	 * Flags for ->add()/->del()/ ->start()/->stop(). There are

commit c39a0e2c8850f08249383f2425dbd8dbe4baad69
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:20 2017 -0700

    x86/perf/cqm: Wipe out perf based cqm
    
    'perf cqm' never worked due to the incompatibility between perf
    infrastructure and cqm hardware support.  The hardware uses RMIDs to
    track the llc occupancy of tasks and these RMIDs are per package. This
    makes monitoring a hierarchy like cgroup along with monitoring of tasks
    separately difficult and several patches sent to lkml to fix them were
    NACKed. Further more, the following issues in the current perf cqm make
    it almost unusable:
    
        1. No support to monitor the same group of tasks for which we do
        allocation using resctrl.
    
        2. It gives random and inaccurate data (mostly 0s) once we run out
        of RMIDs due to issues in Recycling.
    
        3. Recycling results in inaccuracy of data because we cannot
        guarantee that the RMID was stolen from a task when it was not
        pulling data into cache or even when it pulled the least data. Also
        for monitoring llc_occupancy, if we stop using an RMID_x and then
        start using an RMID_y after we reclaim an RMID from an other event,
        we miss accounting all the occupancy that was tagged to RMID_x at a
        later perf_count.
    
        2. Recycling code makes the monitoring code complex including
        scheduling because the event can lose RMID any time. Since MBM
        counters count bandwidth for a period of time by taking snap shot of
        total bytes at two different times, recycling complicates the way we
        count MBM in a hierarchy. Also we need a spin lock while we do the
        processing to account for MBM counter overflow. We also currently
        use a spin lock in scheduling to prevent the RMID from being taken
        away.
    
        4. Lack of support when we run different kind of event like task,
        system-wide and cgroup events together. Data mostly prints 0s. This
        is also because we can have only one RMID tied to a cpu as defined
        by the cqm hardware but a perf can at the same time tie multiple
        events during one sched_in.
    
        5. No support of monitoring a group of tasks. There is partial support
        for cgroup but it does not work once there is a hierarchy of cgroups
        or if we want to monitor a task in a cgroup and the cgroup itself.
    
        6. No support for monitoring tasks for the lifetime without perf
        overhead.
    
        7. It reported the aggregate cache occupancy or memory bandwidth over
        all sockets. But most cloud and VMM based use cases want to know the
        individual per-socket usage.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-2-git-send-email-vikas.shivappa@linux.intel.com

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a3b873fc59e4..4572dbabd4e8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -139,14 +139,6 @@ struct hw_perf_event {
 			/* for tp_event->class */
 			struct list_head	tp_list;
 		};
-		struct { /* intel_cqm */
-			int			cqm_state;
-			u32			cqm_rmid;
-			int			is_group_event;
-			struct list_head	cqm_events_entry;
-			struct list_head	cqm_groups_entry;
-			struct list_head	cqm_group_entry;
-		};
 		struct { /* itrace */
 			int			itrace_started;
 		};
@@ -416,11 +408,6 @@ struct pmu {
 	size_t				task_ctx_size;
 
 
-	/*
-	 * Return the count value for a counter.
-	 */
-	u64 (*count)			(struct perf_event *event); /*optional*/
-
 	/*
 	 * Set up pmu-private data structures for an AUX area
 	 */
@@ -1111,11 +1098,6 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 		__perf_event_task_sched_out(prev, next);
 }
 
-static inline u64 __perf_event_count(struct perf_event *event)
-{
-	return local64_read(&event->count) + atomic64_read(&event->child_count);
-}
-
 extern void perf_event_mmap(struct vm_area_struct *vma);
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);

commit 5518b69b76680a4f2df96b1deca260059db0c2de
Merge: 8ad06e56dcbc 0e72582270c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 12:31:59 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Reasonably busy this cycle, but perhaps not as busy as in the 4.12
      merge window:
    
       1) Several optimizations for UDP processing under high load from
          Paolo Abeni.
    
       2) Support pacing internally in TCP when using the sch_fq packet
          scheduler for this is not practical. From Eric Dumazet.
    
       3) Support mutliple filter chains per qdisc, from Jiri Pirko.
    
       4) Move to 1ms TCP timestamp clock, from Eric Dumazet.
    
       5) Add batch dequeueing to vhost_net, from Jason Wang.
    
       6) Flesh out more completely SCTP checksum offload support, from
          Davide Caratti.
    
       7) More plumbing of extended netlink ACKs, from David Ahern, Pablo
          Neira Ayuso, and Matthias Schiffer.
    
       8) Add devlink support to nfp driver, from Simon Horman.
    
       9) Add RTM_F_FIB_MATCH flag to RTM_GETROUTE queries, from Roopa
          Prabhu.
    
      10) Add stack depth tracking to BPF verifier and use this information
          in the various eBPF JITs. From Alexei Starovoitov.
    
      11) Support XDP on qed device VFs, from Yuval Mintz.
    
      12) Introduce BPF PROG ID for better introspection of installed BPF
          programs. From Martin KaFai Lau.
    
      13) Add bpf_set_hash helper for TC bpf programs, from Daniel Borkmann.
    
      14) For loads, allow narrower accesses in bpf verifier checking, from
          Yonghong Song.
    
      15) Support MIPS in the BPF selftests and samples infrastructure, the
          MIPS eBPF JIT will be merged in via the MIPS GIT tree. From David
          Daney.
    
      16) Support kernel based TLS, from Dave Watson and others.
    
      17) Remove completely DST garbage collection, from Wei Wang.
    
      18) Allow installing TCP MD5 rules using prefixes, from Ivan
          Delalande.
    
      19) Add XDP support to Intel i40e driver, from Björn Töpel
    
      20) Add support for TC flower offload in nfp driver, from Simon
          Horman, Pieter Jansen van Vuuren, Benjamin LaHaise, Jakub
          Kicinski, and Bert van Leeuwen.
    
      21) IPSEC offloading support in mlx5, from Ilan Tayari.
    
      22) Add HW PTP support to macb driver, from Rafal Ozieblo.
    
      23) Networking refcount_t conversions, From Elena Reshetova.
    
      24) Add sock_ops support to BPF, from Lawrence Brako. This is useful
          for tuning the TCP sockopt settings of a group of applications,
          currently via CGROUPs"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1899 commits)
      net: phy: dp83867: add workaround for incorrect RX_CTRL pin strap
      dt-bindings: phy: dp83867: provide a workaround for incorrect RX_CTRL pin strap
      cxgb4: Support for get_ts_info ethtool method
      cxgb4: Add PTP Hardware Clock (PHC) support
      cxgb4: time stamping interface for PTP
      nfp: default to chained metadata prepend format
      nfp: remove legacy MAC address lookup
      nfp: improve order of interfaces in breakout mode
      net: macb: remove extraneous return when MACB_EXT_DESC is defined
      bpf: add missing break in for the TCP_BPF_SNDCWND_CLAMP case
      bpf: fix return in load_bpf_file
      mpls: fix rtm policy in mpls_getroute
      net, ax25: convert ax25_cb.refcount from atomic_t to refcount_t
      net, ax25: convert ax25_route.refcount from atomic_t to refcount_t
      net, ax25: convert ax25_uid_assoc.refcount from atomic_t to refcount_t
      net, sctp: convert sctp_ep_common.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_transport.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_chunk.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_datamsg.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_auth_bytes.refcnt from atomic_t to refcount_t
      ...

commit f91840a32deef5cb1bf73338bc5010f843b01426
Author: Alexei Starovoitov <ast@fb.com>
Date:   Fri Jun 2 21:03:52 2017 -0700

    perf, bpf: Add BPF support to all perf_event types
    
    Allow BPF_PROG_TYPE_PERF_EVENT program types to attach to all
    perf_event types, including HW_CACHE, RAW, and dynamic pmu events.
    Only tracepoint/kprobe events are treated differently which require
    BPF_PROG_TYPE_TRACEPOINT/BPF_PROG_TYPE_KPROBE program types accordingly.
    
    Also add support for reading all event counters using
    bpf_perf_event_read() helper.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 24a635887f28..8fc5f0fada5e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -896,7 +896,7 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				void *context);
 extern void perf_pmu_migrate_context(struct pmu *pmu,
 				int src_cpu, int dst_cpu);
-extern u64 perf_event_read_local(struct perf_event *event);
+int perf_event_read_local(struct perf_event *event, u64 *value);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 
@@ -1301,7 +1301,10 @@ static inline const struct perf_event_attr *perf_event_attrs(struct perf_event *
 {
 	return ERR_PTR(-EINVAL);
 }
-static inline u64 perf_event_read_local(struct perf_event *event)	{ return -EINVAL; }
+static inline int perf_event_read_local(struct perf_event *event, u64 *value)
+{
+	return -EINVAL;
+}
 static inline void perf_event_print_debug(void)				{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }

commit a63fbed776c7124ce9f606234267c3c095b2680e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:34 2017 +0200

    perf/tracing/cpuhotplug: Fix locking order
    
    perf, tracing, kprobes and jump_labels have a gazillion of ways to create
    dependency lock chains. Some of those involve nested invocations of
    get_online_cpus().
    
    The conversion of the hotplug locking to a percpu rwsem requires to avoid
    such nested calls. sys_perf_event_open() protects most of the syscall logic
    against cpu hotplug. This causes nested calls and lock inversions versus
    ftrace and kprobes in various interesting ways.
    
    It's impossible to move the hotplug locking to the outer end of all call
    chains in the involved facilities, so the hotplug protection in
    sys_perf_event_open() needs to be solved differently.
    
    Introduce 'pmus_mutex' which protects a perf private online cpumask. This
    mutex is taken when the mask is updated in the cpu hotplug callbacks and
    can be taken in sys_perf_event_open() to protect the swhash setup/teardown
    code and when the final judgement about a valid event has to be made.
    
    [ tglx: Produced changelog and fixed the swhash interaction ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Link: http://lkml.kernel.org/r/20170524081548.930941109@linutronix.de

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 24a635887f28..7d6aa29094b2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -801,6 +801,8 @@ struct perf_cpu_context {
 
 	struct list_head		sched_cb_entry;
 	int				sched_cb_usage;
+
+	int				online;
 };
 
 struct perf_output_handle {

commit cf25f904ef75aa7c25097eb4981bbc634bf5ff9e
Author: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
Date:   Fri Feb 24 02:48:21 2017 -0600

    x86/events/amd/iommu: Add IOMMU-specific hw_perf_event struct
    
    Current AMD IOMMU perf PMU inappropriately uses the hardware struct
    inside the union in struct hw_perf_event, extra_reg in particular.
    
    Instead, introduce an AMD IOMMU-specific struct with required parameters
    to be programmed into the IOMMU performance counter control register.
    
    Update the pasid field from 16 to 20 bits while at it.
    
    Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    [ Fixup macros, shorten get_next_avail_iommu_bnk_cntr() local vars, massage commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Jörg Rödel <joro@8bytes.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: iommu@lists.linux-foundation.org
    Link: http://lkml.kernel.org/r/1487926102-13073-10-git-send-email-Suravee.Suthikulpanit@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b6e75c9d4791..24a635887f28 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -165,6 +165,13 @@ struct hw_perf_event {
 			struct list_head		bp_list;
 		};
 #endif
+		struct { /* amd_iommu */
+			u8	iommu_bank;
+			u8	iommu_cntr;
+			u16	padding;
+			u64	conf;
+			u64	conf1;
+		};
 	};
 	/*
 	 * If the event is a per task event, this will point to the task in

commit f4c0b0aa58d9b7e30ab0a95e33da84d53b3d764a
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 20 15:33:50 2017 +0200

    perf/core: Keep AUX flags in the output handle
    
    In preparation for adding more flags to perf AUX records, introduce a
    separate API for setting the flags for a session, rather than appending
    more bool arguments to perf_aux_output_end. This allows to set each
    flag at the time a corresponding condition is detected, instead of
    tracking it in each driver's private state.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20170220133352.17995-3-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f19a82362851..b6e75c9d4791 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -801,6 +801,7 @@ struct perf_output_handle {
 	struct ring_buffer		*rb;
 	unsigned long			wakeup;
 	unsigned long			size;
+	u64				aux_flags;
 	union {
 		void			*addr;
 		unsigned long		head;
@@ -849,10 +850,11 @@ perf_cgroup_from_task(struct task_struct *task, struct perf_event_context *ctx)
 extern void *perf_aux_output_begin(struct perf_output_handle *handle,
 				   struct perf_event *event);
 extern void perf_aux_output_end(struct perf_output_handle *handle,
-				unsigned long size, bool truncated);
+				unsigned long size);
 extern int perf_aux_output_skip(struct perf_output_handle *handle,
 				unsigned long size);
 extern void *perf_get_aux(struct perf_output_handle *handle);
+extern void perf_aux_output_flag(struct perf_output_handle *handle, u64 flags);
 
 extern int perf_pmu_register(struct pmu *pmu, const char *name, int type);
 extern void perf_pmu_unregister(struct pmu *pmu);
@@ -1268,8 +1270,8 @@ static inline void *
 perf_aux_output_begin(struct perf_output_handle *handle,
 		      struct perf_event *event)				{ return NULL; }
 static inline void
-perf_aux_output_end(struct perf_output_handle *handle, unsigned long size,
-		    bool truncated)					{ }
+perf_aux_output_end(struct perf_output_handle *handle, unsigned long size)
+									{ }
 static inline int
 perf_aux_output_skip(struct perf_output_handle *handle,
 		     unsigned long size)				{ return -EINVAL; }

commit e422267322cd319e2695a535e47c5b1feeac45eb
Author: Hari Bathini <hbathini@linux.vnet.ibm.com>
Date:   Wed Mar 8 02:11:36 2017 +0530

    perf: Add PERF_RECORD_NAMESPACES to include namespaces related info
    
    With the advert of container technologies like docker, that depend on
    namespaces for isolation, there is a need for tracing support for
    namespaces. This patch introduces new PERF_RECORD_NAMESPACES event for
    recording namespaces related info. By recording info for every
    namespace, it is left to userspace to take a call on the definition of a
    container and trace containers by updating perf tool accordingly.
    
    Each namespace has a combination of device and inode numbers. Though
    every namespace has the same device number currently, that may change in
    future to avoid the need for a namespace of namespaces. Considering such
    possibility, record both device and inode numbers separately for each
    namespace.
    
    Signed-off-by: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Sargun Dhillon <sargun@sargun.me>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/148891929686.25309.2827618988917007768.stgit@hbathini.in.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 000fdb211c7d..f19a82362851 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1112,6 +1112,7 @@ extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks
 
 extern void perf_event_exec(void);
 extern void perf_event_comm(struct task_struct *tsk, bool exec);
+extern void perf_event_namespaces(struct task_struct *tsk);
 extern void perf_event_fork(struct task_struct *tsk);
 
 /* Callchains */
@@ -1315,6 +1316,7 @@ static inline int perf_unregister_guest_info_callbacks
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_exec(void)				{ }
 static inline void perf_event_comm(struct task_struct *tsk, bool exec)	{ }
+static inline void perf_event_namespaces(struct task_struct *tsk)	{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
 static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }

commit 6ce77bfd6cedbff61eabf8837dc0901bb671cc86
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Thu Jan 26 11:40:57 2017 +0200

    perf/core: Allow kernel filters on CPU events
    
    While supporting file-based address filters for CPU events requires some
    extra context switch handling, kernel address filters are easy, since the
    kernel mapping is preserved across address spaces. It is also useful as
    it permits tracing scheduling paths of the kernel.
    
    This patch allows setting up kernel filters for CPU events.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20170126094057.13805-4-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5c58e93c130c..000fdb211c7d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -482,6 +482,7 @@ struct perf_addr_filter {
  * @list:	list of filters for this event
  * @lock:	spinlock that serializes accesses to the @list and event's
  *		(and its children's) filter generations.
+ * @nr_file_filters:	number of file-based filters
  *
  * A child event will use parent's @list (and therefore @lock), so they are
  * bundled together; see perf_event_addr_filters().
@@ -489,6 +490,7 @@ struct perf_addr_filter {
 struct perf_addr_filters_head {
 	struct list_head	list;
 	raw_spinlock_t		lock;
+	unsigned int		nr_file_filters;
 };
 
 /**

commit 1fd7e416995401ec082fc0fe6090a223969beda5
Author: David Carrillo-Cisneros <davidcc@google.com>
Date:   Wed Jan 18 11:24:54 2017 -0800

    perf/core: Remove perf_cpu_context::unique_pmu
    
    cpuctx->unique_pmu was originally introduced as a way to identify cpuctxs
    with shared pmus in order to avoid visiting the same cpuctx more than once
    in a for_each_pmu loop.
    
    cpuctx->unique_pmu == cpuctx->pmu in non-software task contexts since they
    have only one pmu per cpuctx. Since perf_pmu_sched_task() is only called in
    hw contexts, this patch replaces cpuctx->unique_pmu by cpuctx->pmu in it.
    
    The change above, together with the previous patch in this series, removed
    the remaining uses of cpuctx->unique_pmu, so we remove it altogether.
    
    Signed-off-by: David Carrillo-Cisneros <davidcc@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/20170118192454.58008-3-davidcc@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index dfa725723f28..5c58e93c130c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -785,7 +785,6 @@ struct perf_cpu_context {
 	ktime_t				hrtimer_interval;
 	unsigned int			hrtimer_active;
 
-	struct pmu			*unique_pmu;
 #ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp;
 	struct list_head		cgrp_cpuctx_entry;

commit 058fe1c0440e68a1ba3c2270ae43e9f0298b27d8
Author: David Carrillo-Cisneros <davidcc@google.com>
Date:   Wed Jan 18 11:24:53 2017 -0800

    perf/core: Make cgroup switch visit only cpuctxs with cgroup events
    
    This patch follows from a conversation in CQM/CMT's last series about
    speeding up the context switch for cgroup events:
    
      https://patchwork.kernel.org/patch/9478617/
    
    This is a low-hanging fruit optimization. It replaces the iteration over
    the "pmus" list in cgroup switch by an iteration over a new list that
    contains only cpuctxs with at least one cgroup event.
    
    This is necessary because the number of PMUs have increased over the years
    e.g modern x86 server systems have well above 50 PMUs.
    
    The iteration over the full PMU list is unneccessary and can be costly in
    heavy cache contention scenarios.
    
    Below are some instrumentation measurements with 10, 50 and 90 percentiles
    of the total cost of context switch before and after this optimization for
    a simple array read/write microbenchark.
    
      Contention
        Level    Nr events      Before (us)            After (us)       Median
      L2    L3     types      (10%, 50%, 90%)       (10%, 50%, 90%     Speedup
      --------------------------------------------------------------------------
      Low   Low       1       (1.72, 2.42, 5.85)    (1.35, 1.64, 5.46)     29%
      High  Low       1       (2.08, 4.56, 19.8)    (1720, 2.20, 13.7)     51%
      High  High      1       (2.86, 10.4, 12.7)    (2.54, 4.32, 12.1)     58%
    
      Low   Low       2       (1.98, 3.20, 6.89)    (1.68, 2.41, 8.89)     24%
      High  Low       2       (2.48, 5.28, 22.4)    (2150, 3.69, 14.6)     30%
      High  High      2       (3.32, 8.09, 13.9)    (2.80, 5.15, 13.7)     36%
    
    where:
    
      1 event type  = cycles
      2 event types = cycles,intel_cqm/llc_occupancy/
    
       Contention L2 Low: workset  <  L2 cache size.
                     High:  "     >>  L2   "     " .
       Contention L3 Low: workset of task on all sockets  <  L3 cache size.
                     High:   "     "   "   "   "    "    >>  L3   "     " .
    
       Median Speedup is (50%ile Before - 50%ile After) /  50%ile Before
    
    Unsurprisingly, the benefits of this optimization decrease with the number
    of cpuctxs with a cgroup events, yet, is never detrimental.
    
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: David Carrillo-Cisneros <davidcc@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/20170118192454.58008-2-davidcc@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 78ed8105e64d..dfa725723f28 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -788,6 +788,7 @@ struct perf_cpu_context {
 	struct pmu			*unique_pmu;
 #ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp;
+	struct list_head		cgrp_cpuctx_entry;
 #endif
 
 	struct list_head		sched_cb_entry;

commit 475113d937adfd150eb82b5e2c5507125a68e7af
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Wed Dec 28 14:31:03 2016 +0100

    perf/x86/intel: Account interrupts for PEBS errors
    
    It's possible to set up PEBS events to get only errors and not
    any data, like on SNB-X (model 45) and IVB-EP (model 62)
    via 2 perf commands running simultaneously:
    
        taskset -c 1 ./perf record -c 4 -e branches:pp -j any -C 10
    
    This leads to a soft lock up, because the error path of the
    intel_pmu_drain_pebs_nhm() does not account event->hw.interrupt
    for error PEBS interrupts, so in case you're getting ONLY
    errors you don't have a way to stop the event when it's over
    the max_samples_per_tick limit:
    
      NMI watchdog: BUG: soft lockup - CPU#22 stuck for 22s! [perf_fuzzer:5816]
      ...
      RIP: 0010:[<ffffffff81159232>]  [<ffffffff81159232>] smp_call_function_single+0xe2/0x140
      ...
      Call Trace:
       ? trace_hardirqs_on_caller+0xf5/0x1b0
       ? perf_cgroup_attach+0x70/0x70
       perf_install_in_context+0x199/0x1b0
       ? ctx_resched+0x90/0x90
       SYSC_perf_event_open+0x641/0xf90
       SyS_perf_event_open+0x9/0x10
       do_syscall_64+0x6c/0x1f0
       entry_SYSCALL64_slow_path+0x25/0x25
    
    Add perf_event_account_interrupt() which does the interrupt
    and frequency checks and call it from intel_pmu_drain_pebs_nhm()'s
    error path.
    
    We keep the pending_kill and pending_wakeup logic only in the
    __perf_event_overflow() path, because they make sense only if
    there's any data to deliver.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vince@deater.net>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1482931866-6018-2-git-send-email-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4741ecdb9817..78ed8105e64d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1259,6 +1259,7 @@ extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_disable_local(struct perf_event *event);
 extern void perf_event_disable_inatomic(struct perf_event *event);
 extern void perf_event_task_tick(void);
+extern int perf_event_account_interrupt(struct perf_event *event);
 #else /* !CONFIG_PERF_EVENTS: */
 static inline void *
 perf_aux_output_begin(struct perf_output_handle *handle,

commit 5aab90ce1ec449912a2ebc4d45e0c85dac29e9dd
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Oct 26 11:48:24 2016 +0200

    perf/powerpc: Don't call perf_event_disable() from atomic context
    
    The trinity syscall fuzzer triggered following WARN() on powerpc:
    
      WARNING: CPU: 9 PID: 2998 at arch/powerpc/kernel/hw_breakpoint.c:278
      ...
      NIP [c00000000093aedc] .hw_breakpoint_handler+0x28c/0x2b0
      LR [c00000000093aed8] .hw_breakpoint_handler+0x288/0x2b0
      Call Trace:
      [c0000002f7933580] [c00000000093aed8] .hw_breakpoint_handler+0x288/0x2b0 (unreliable)
      [c0000002f7933630] [c0000000000f671c] .notifier_call_chain+0x7c/0xf0
      [c0000002f79336d0] [c0000000000f6abc] .__atomic_notifier_call_chain+0xbc/0x1c0
      [c0000002f7933780] [c0000000000f6c40] .notify_die+0x70/0xd0
      [c0000002f7933820] [c00000000001a74c] .do_break+0x4c/0x100
      [c0000002f7933920] [c0000000000089fc] handle_dabr_fault+0x14/0x48
    
    Followed by a lockdep warning:
    
      ===============================
      [ INFO: suspicious RCU usage. ]
      4.8.0-rc5+ #7 Tainted: G        W
      -------------------------------
      ./include/linux/rcupdate.h:556 Illegal context switch in RCU read-side critical section!
    
      other info that might help us debug this:
    
      rcu_scheduler_active = 1, debug_locks = 0
      2 locks held by ls/2998:
       #0:  (rcu_read_lock){......}, at: [<c0000000000f6a00>] .__atomic_notifier_call_chain+0x0/0x1c0
       #1:  (rcu_read_lock){......}, at: [<c00000000093ac50>] .hw_breakpoint_handler+0x0/0x2b0
    
      stack backtrace:
      CPU: 9 PID: 2998 Comm: ls Tainted: G        W       4.8.0-rc5+ #7
      Call Trace:
      [c0000002f7933150] [c00000000094b1f8] .dump_stack+0xe0/0x14c (unreliable)
      [c0000002f79331e0] [c00000000013c468] .lockdep_rcu_suspicious+0x138/0x180
      [c0000002f7933270] [c0000000001005d8] .___might_sleep+0x278/0x2e0
      [c0000002f7933300] [c000000000935584] .mutex_lock_nested+0x64/0x5a0
      [c0000002f7933410] [c00000000023084c] .perf_event_ctx_lock_nested+0x16c/0x380
      [c0000002f7933500] [c000000000230a80] .perf_event_disable+0x20/0x60
      [c0000002f7933580] [c00000000093aeec] .hw_breakpoint_handler+0x29c/0x2b0
      [c0000002f7933630] [c0000000000f671c] .notifier_call_chain+0x7c/0xf0
      [c0000002f79336d0] [c0000000000f6abc] .__atomic_notifier_call_chain+0xbc/0x1c0
      [c0000002f7933780] [c0000000000f6c40] .notify_die+0x70/0xd0
      [c0000002f7933820] [c00000000001a74c] .do_break+0x4c/0x100
      [c0000002f7933920] [c0000000000089fc] handle_dabr_fault+0x14/0x48
    
    While it looks like the first WARN() is probably valid, the other one is
    triggered by disabling event via perf_event_disable() from atomic context.
    
    The event is disabled here in case we were not able to emulate
    the instruction that hit the breakpoint. By disabling the event
    we unschedule the event and make sure it's not scheduled back.
    
    But we can't call perf_event_disable() from atomic context, instead
    we need to use the event's pending_disable irq_work method to disable it.
    
    Reported-by: Jan Stancek <jstancek@redhat.com>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161026094824.GA21397@krava
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 060d0ede88df..4741ecdb9817 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1257,6 +1257,7 @@ extern u64 perf_swevent_set_period(struct perf_event *event);
 extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_disable_local(struct perf_event *event);
+extern void perf_event_disable_inatomic(struct perf_event *event);
 extern void perf_event_task_tick(void);
 #else /* !CONFIG_PERF_EVENTS: */
 static inline void *

commit 687ee0ad4e897e29f4b41f7a20c866d74c5e0660
Merge: 3ddf40e8c319 03a1eabc3f54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 5 10:11:24 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) BBR TCP congestion control, from Neal Cardwell, Yuchung Cheng and
        co. at Google. https://lwn.net/Articles/701165/
    
     2) Do TCP Small Queues for retransmits, from Eric Dumazet.
    
     3) Support collect_md mode for all IPV4 and IPV6 tunnels, from Alexei
        Starovoitov.
    
     4) Allow cls_flower to classify packets in ip tunnels, from Amir Vadai.
    
     5) Support DSA tagging in older mv88e6xxx switches, from Andrew Lunn.
    
     6) Support GMAC protocol in iwlwifi mwm, from Ayala Beker.
    
     7) Support ndo_poll_controller in mlx5, from Calvin Owens.
    
     8) Move VRF processing to an output hook and allow l3mdev to be
        loopback, from David Ahern.
    
     9) Support SOCK_DESTROY for UDP sockets. Also from David Ahern.
    
    10) Congestion control in RXRPC, from David Howells.
    
    11) Support geneve RX offload in ixgbe, from Emil Tantilov.
    
    12) When hitting pressure for new incoming TCP data SKBs, perform a
        partial rathern than a full purge of the OFO queue (which could be
        huge). From Eric Dumazet.
    
    13) Convert XFRM state and policy lookups to RCU, from Florian Westphal.
    
    14) Support RX network flow classification to igb, from Gangfeng Huang.
    
    15) Hardware offloading of eBPF in nfp driver, from Jakub Kicinski.
    
    16) New skbmod packet action, from Jamal Hadi Salim.
    
    17) Remove some inefficiencies in snmp proc output, from Jia He.
    
    18) Add FIB notifications to properly propagate route changes to
        hardware which is doing forwarding offloading. From Jiri Pirko.
    
    19) New dsa driver for qca8xxx chips, from John Crispin.
    
    20) Implement RFC7559 ipv6 router solicitation backoff, from Maciej
        Żenczykowski.
    
    21) Add L3 mode to ipvlan, from Mahesh Bandewar.
    
    22) Support 802.1ad in mlx4, from Moshe Shemesh.
    
    23) Support hardware LRO in mediatek driver, from Nelson Chang.
    
    24) Add TC offloading to mlx5, from Or Gerlitz.
    
    25) Convert various drivers to ethtool ksettings interfaces, from
        Philippe Reynes.
    
    26) TX max rate limiting for cxgb4, from Rahul Lakkireddy.
    
    27) NAPI support for ath10k, from Rajkumar Manoharan.
    
    28) Support XDP in mlx5, from Rana Shahout and Saeed Mahameed.
    
    29) UDP replicast support in TIPC, from Richard Alpe.
    
    30) Per-queue statistics for qed driver, from Sudarsana Reddy Kalluru.
    
    31) Support BQL in thunderx driver, from Sunil Goutham.
    
    32) TSO support in alx driver, from Tobias Regnery.
    
    33) Add stream parser engine and use it in kcm.
    
    34) Support async DHCP replies in ipconfig module, from Uwe
        Kleine-König.
    
    35) DSA port fast aging for mv88e6xxx driver, from Vivien Didelot.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1715 commits)
      mlxsw: switchx2: Fix misuse of hard_header_len
      mlxsw: spectrum: Fix misuse of hard_header_len
      net/faraday: Stop NCSI device on shutdown
      net/ncsi: Introduce ncsi_stop_dev()
      net/ncsi: Rework the channel monitoring
      net/ncsi: Allow to extend NCSI request properties
      net/ncsi: Rework request index allocation
      net/ncsi: Don't probe on the reserved channel ID (0x1f)
      net/ncsi: Introduce NCSI_RESERVED_CHANNEL
      net/ncsi: Avoid unused-value build warning from ia64-linux-gcc
      net: Add netdev all_adj_list refcnt propagation to fix panic
      net: phy: Add Edge-rate driver for Microsemi PHYs.
      vmxnet3: Wake queue from reset work
      i40e: avoid NULL pointer dereference and recursive errors on early PCI error
      qed: Add RoCE ll2 & GSI support
      qed: Add support for memory registeration verbs
      qed: Add support for QP verbs
      qed: PD,PKEY and CQ verb support
      qed: Add support for RoCE hw init
      qede: Add qedr framework
      ...

commit aa6a5f3cb2b2edc5b9aab0b4fdfdfa9c3b5096a8
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Sep 1 18:37:24 2016 -0700

    perf, bpf: add perf events core support for BPF_PROG_TYPE_PERF_EVENT programs
    
    Allow attaching BPF_PROG_TYPE_PERF_EVENT programs to sw and hw perf events
    via overflow_handler mechanism.
    When program is attached the overflow_handlers become stacked.
    The program acts as a filter.
    Returning zero from the program means that the normal perf_event_output handler
    will not be called and sampling event won't be stored in the ring buffer.
    
    The overflow_handler_context==NULL is an additional safety check
    to make sure programs are not attached to hw breakpoints and watchdog
    in case other checks (that prevent that now anyway) get accidentally
    relaxed in the future.
    
    The program refcnt is incremented in case perf_events are inhereted
    when target task is forked.
    Similar to kprobe and tracepoint programs there is no ioctl to
    detach the program or swap already attached program. The user space
    expected to close(perf_event_fd) like it does right now for kprobe+bpf.
    That restriction simplifies the code quite a bit.
    
    The invocation of overflow_handler in __perf_event_overflow() is now
    done via READ_ONCE, since that pointer can be replaced when the program
    is attached while perf_event itself could have been active already.
    There is no need to do similar treatment for event->prog, since it's
    assigned only once before it's accessed.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 97bfe62f30d7..ccb73a58113d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -679,6 +679,10 @@ struct perf_event {
 	u64				(*clock)(void);
 	perf_overflow_handler_t		overflow_handler;
 	void				*overflow_handler_context;
+#ifdef CONFIG_BPF_SYSCALL
+	perf_overflow_handler_t		orig_overflow_handler;
+	struct bpf_prog			*prog;
+#endif
 
 #ifdef CONFIG_EVENT_TRACING
 	struct trace_event_call		*tp_event;

commit 0515e5999a466dfe6e1924f460da599bb6821487
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Sep 1 18:37:22 2016 -0700

    bpf: introduce BPF_PROG_TYPE_PERF_EVENT program type
    
    Introduce BPF_PROG_TYPE_PERF_EVENT programs that can be attached to
    HW and SW perf events (PERF_TYPE_HARDWARE and PERF_TYPE_SOFTWARE
    correspondingly in uapi/linux/perf_event.h)
    
    The program visible context meta structure is
    struct bpf_perf_event_data {
        struct pt_regs regs;
         __u64 sample_period;
    };
    which is accessible directly from the program:
    int bpf_prog(struct bpf_perf_event_data *ctx)
    {
      ... ctx->sample_period ...
      ... ctx->regs.ip ...
    }
    
    The bpf verifier rewrites the accesses into kernel internal
    struct bpf_perf_event_data_kern which allows changing
    struct perf_sample_data without affecting bpf programs.
    New fields can be added to the end of struct bpf_perf_event_data
    in the future.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2b6b43cc0dd5..97bfe62f30d7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -788,6 +788,11 @@ struct perf_output_handle {
 	int				page;
 };
 
+struct bpf_perf_event_data_kern {
+	struct pt_regs *regs;
+	struct perf_sample_data *data;
+};
+
 #ifdef CONFIG_CGROUP_PERF
 
 /*

commit d6a2f9035bfc27d0e9d78b13635dda9fb017ac01
Author: David Carrillo-Cisneros <davidcc@google.com>
Date:   Wed Aug 17 13:55:06 2016 -0700

    perf/core: Introduce PMU_EV_CAP_READ_ACTIVE_PKG
    
    Introduce the flag PMU_EV_CAP_READ_ACTIVE_PKG, useful for uncore events,
    that allows a PMU to signal the generic perf code that an event is readable
    in the current CPU if the event is active in a CPU in the same package as
    the current CPU.
    
    This is an optimization that avoids a unnecessary IPI for the common case
    where uncore events are run and read in the same package but in
    different CPUs.
    
    As an example, the IPI removal speeds up perf_read() in my Haswell system
    as follows:
    
      - For event UNC_C_LLC_LOOKUP: From 260 us to 31 us.
      - For event RAPL's power/energy-cores/: From to 255 us to 27 us.
    
    For the optimization to work, all events in the group must have it
    (similarly to PERF_EV_CAP_SOFTWARE).
    
    Signed-off-by: David Carrillo-Cisneros <davidcc@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Carrillo-Cisneros <davidcc@google.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1471467307-61171-4-git-send-email-davidcc@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6f7459f72dfd..5c5362584aba 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -514,8 +514,11 @@ typedef void (*perf_overflow_handler_t)(struct perf_event *,
  * Event capabilities. For event_caps and groups caps.
  *
  * PERF_EV_CAP_SOFTWARE: Is a software event.
+ * PERF_EV_CAP_READ_ACTIVE_PKG: A CPU event (or cgroup event) that can be read
+ * from any CPU in the package where it is active.
  */
 #define PERF_EV_CAP_SOFTWARE		BIT(0)
+#define PERF_EV_CAP_READ_ACTIVE_PKG	BIT(1)
 
 #define SWEVENT_HLIST_BITS		8
 #define SWEVENT_HLIST_SIZE		(1 << SWEVENT_HLIST_BITS)

commit 4ff6a8debf48a7bf48e93c01da720785070d3a25
Author: David Carrillo-Cisneros <davidcc@google.com>
Date:   Wed Aug 17 13:55:05 2016 -0700

    perf/core: Generalize event->group_flags
    
    Currently, PERF_GROUP_SOFTWARE is used in the group_flags field of a
    group's leader to indicate that is_software_event(event) is true for all
    events in a group. This is the only usage of event->group_flags.
    
    This pattern of setting a group level flags when all events in the group
    share a property is useful for the flag introduced in the next patch and
    for future CQM/CMT flags. So this patches generalizes group_flags to work
    as an aggregate of event level flags.
    
    PERF_GROUP_SOFTWARE denotes an inmutable event's property. All other flags
    that I intend to add are also determinable at event initialization.
    To better convey the above, this patch renames event's group_flags to
    group_caps and PERF_GROUP_SOFTWARE to PERF_EV_CAP_SOFTWARE.
    
    Individual event flags are stored in the new event->event_caps. Since the
    cap flags do not change after event initialization, there is no need to
    serialize event_caps. This new field is used when events are added to a
    context, similarly to how PERF_GROUP_SOFTWARE and is_software_event()
    worked.
    
    Lastly, for consistency, updates is_software_event() to rely in event_cap
    instead of the context index.
    
    Signed-off-by: David Carrillo-Cisneros <davidcc@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1471467307-61171-3-git-send-email-davidcc@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 529c41fa73c8..6f7459f72dfd 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -510,9 +510,12 @@ typedef void (*perf_overflow_handler_t)(struct perf_event *,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
 
-enum perf_group_flag {
-	PERF_GROUP_SOFTWARE		= 0x1,
-};
+/*
+ * Event capabilities. For event_caps and groups caps.
+ *
+ * PERF_EV_CAP_SOFTWARE: Is a software event.
+ */
+#define PERF_EV_CAP_SOFTWARE		BIT(0)
 
 #define SWEVENT_HLIST_BITS		8
 #define SWEVENT_HLIST_SIZE		(1 << SWEVENT_HLIST_BITS)
@@ -568,7 +571,12 @@ struct perf_event {
 	struct hlist_node		hlist_entry;
 	struct list_head		active_entry;
 	int				nr_siblings;
-	int				group_flags;
+
+	/* Not serialized. Only written during event initialization. */
+	int				event_caps;
+	/* The cumulative AND of all event_caps for events in this group. */
+	int				group_caps;
+
 	struct perf_event		*group_leader;
 	struct pmu			*pmu;
 	void				*pmu_private;
@@ -988,7 +996,7 @@ static inline bool is_sampling_event(struct perf_event *event)
  */
 static inline int is_software_event(struct perf_event *event)
 {
-	return event->pmu->task_ctx_nr == perf_sw_context;
+	return event->event_caps & PERF_EV_CAP_SOFTWARE;
 }
 
 extern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];

commit e48c178814b4a33f84f62d01f5a601ebd57fbba8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jul 6 09:18:30 2016 +0200

    perf/core: Optimize perf_pmu_sched_task()
    
    For perf record -b, which requires the pmu::sched_task callback the
    current code is rather expensive:
    
         7.68%  sched-pipe  [kernel.vmlinux]    [k] perf_pmu_sched_task
         5.95%  sched-pipe  [kernel.vmlinux]    [k] __switch_to
         5.20%  sched-pipe  [kernel.vmlinux]    [k] __intel_pmu_disable_all
         3.95%  sched-pipe  perf                [.] worker_thread
    
    The problem is that it will iterate all registered PMUs, most of which
    will not have anything to do. Avoid this by keeping an explicit list
    of PMUs that have requested the callback.
    
    The perf_sched_cb_{inc,dec}() functions already takes the required pmu
    argument, and now that these functions are no longer called from NMI
    context we can use them to manage a list.
    
    With this patch applied the function doesn't show up in the top 4
    anymore (it dropped to 18th place).
    
         6.67%  sched-pipe  [kernel.vmlinux]    [k] __switch_to
         6.18%  sched-pipe  [kernel.vmlinux]    [k] __intel_pmu_disable_all
         3.92%  sched-pipe  [kernel.vmlinux]    [k] switch_mm_irqs_off
         3.71%  sched-pipe  perf                [.] worker_thread
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2b6b43cc0dd5..529c41fa73c8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -774,6 +774,9 @@ struct perf_cpu_context {
 #ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp;
 #endif
+
+	struct list_head		sched_cb_entry;
+	int				sched_cb_usage;
 };
 
 struct perf_output_handle {

commit db4a835601b73cf8d6cd8986381d966b8e13d2d9
Author: David Carrillo-Cisneros <davidcc@google.com>
Date:   Tue Aug 2 00:48:12 2016 -0700

    perf/core: Set cgroup in CPU contexts for new cgroup events
    
    There's a perf stat bug easy to observer on a machine with only one cgroup:
    
      $ perf stat -e cycles -I 1000 -C 0 -G /
      #          time             counts unit events
          1.000161699      <not counted>      cycles                    /
          2.000355591      <not counted>      cycles                    /
          3.000565154      <not counted>      cycles                    /
          4.000951350      <not counted>      cycles                    /
    
    We'd expect some output there.
    
    The underlying problem is that there is an optimization in
    perf_cgroup_sched_{in,out}() that skips the switch of cgroup events
    if the old and new cgroups in a task switch are the same.
    
    This optimization interacts with the current code in two ways
    that cause a CPU context's cgroup (cpuctx->cgrp) to be NULL even if a
    cgroup event matches the current task. These are:
    
      1. On creation of the first cgroup event in a CPU: In current code,
      cpuctx->cpu is only set in perf_cgroup_sched_in, but due to the
      aforesaid optimization, perf_cgroup_sched_in will run until the next
      cgroup switches in that CPU. This may happen late or never happen,
      depending on system's number of cgroups, CPU load, etc.
    
      2. On deletion of the last cgroup event in a cpuctx: In list_del_event,
      cpuctx->cgrp is set NULL. Any new cgroup event will not be sched in
      because cpuctx->cgrp == NULL until a cgroup switch occurs and
      perf_cgroup_sched_in is executed (updating cpuctx->cgrp).
    
    This patch fixes both problems by setting cpuctx->cgrp in list_add_event,
    mirroring what list_del_event does when removing a cgroup event from CPU
    context, as introduced in:
    
      commit 68cacd29167b ("perf_events: Fix stale ->cgrp pointer in update_cgrp_time_from_cpuctx()")
    
    With this patch, cpuctx->cgrp is always set/clear when installing/removing
    the first/last cgroup event in/from the CPU context. With cpuctx->cgrp
    correctly set, event_filter_match works as intended when events are
    sched in/out.
    
    After the fix, the output is as expected:
    
      $ perf stat -e cycles -I 1000 -a -G /
      #         time             counts unit events
         1.004699159          627342882      cycles                    /
         2.007397156          615272690      cycles                    /
         3.010019057          616726074      cycles                    /
    
    Signed-off-by: David Carrillo-Cisneros <davidcc@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1470124092-113192-1-git-send-email-davidcc@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8ed4326164cc..2b6b43cc0dd5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -743,7 +743,9 @@ struct perf_event_context {
 	u64				parent_gen;
 	u64				generation;
 	int				pin_count;
+#ifdef CONFIG_CGROUP_PERF
 	int				nr_cgroups;	 /* cgroup evts */
+#endif
 	void				*task_ctx_data; /* pmu specific data */
 	struct rcu_head			rcu_head;
 };
@@ -769,7 +771,9 @@ struct perf_cpu_context {
 	unsigned int			hrtimer_active;
 
 	struct pmu			*unique_pmu;
+#ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp;
+#endif
 };
 
 struct perf_output_handle {

commit a6408f6cb63ac0958fee7dbce7861ffb540d8a49
Merge: 1a81a8f2a591 4fae16dffb81
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 29 13:55:30 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp hotplug updates from Thomas Gleixner:
     "This is the next part of the hotplug rework.
    
       - Convert all notifiers with a priority assigned
    
       - Convert all CPU_STARTING/DYING notifiers
    
         The final removal of the STARTING/DYING infrastructure will happen
         when the merge window closes.
    
      Another 700 hundred line of unpenetrable maze gone :)"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      timers/core: Correct callback order during CPU hot plug
      leds/trigger/cpu: Move from CPU_STARTING to ONLINE level
      powerpc/numa: Convert to hotplug state machine
      arm/perf: Fix hotplug state machine conversion
      irqchip/armada: Avoid unused function warnings
      ARC/time: Convert to hotplug state machine
      clocksource/atlas7: Convert to hotplug state machine
      clocksource/armada-370-xp: Convert to hotplug state machine
      clocksource/exynos_mct: Convert to hotplug state machine
      clocksource/arm_global_timer: Convert to hotplug state machine
      rcu: Convert rcutree to hotplug state machine
      KVM/arm/arm64/vgic-new: Convert to hotplug state machine
      smp/cfd: Convert core to hotplug state machine
      x86/x2apic: Convert to CPU hotplug state machine
      profile: Convert to hotplug state machine
      timers/core: Convert to hotplug state machine
      hrtimer: Convert to hotplug state machine
      x86/tboot: Convert to hotplug state machine
      arm64/armv8 deprecated: Convert to hotplug state machine
      hwtracing/coresight-etm4x: Convert to hotplug state machine
      ...

commit 468fc7ed5537615efe671d94248446ac24679773
Merge: 08fd8c17686c 36232012344b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 12:03:20 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Unified UDP encapsulation offload methods for drivers, from
        Alexander Duyck.
    
     2) Make DSA binding more sane, from Andrew Lunn.
    
     3) Support QCA9888 chips in ath10k, from Anilkumar Kolli.
    
     4) Several workqueue usage cleanups, from Bhaktipriya Shridhar.
    
     5) Add XDP (eXpress Data Path), essentially running BPF programs on RX
        packets as soon as the device sees them, with the option to mirror
        the packet on TX via the same interface.  From Brenden Blanco and
        others.
    
     6) Allow qdisc/class stats dumps to run lockless, from Eric Dumazet.
    
     7) Add VLAN support to b53 and bcm_sf2, from Florian Fainelli.
    
     8) Simplify netlink conntrack entry layout, from Florian Westphal.
    
     9) Add ipv4 forwarding support to mlxsw spectrum driver, from Ido
        Schimmel, Yotam Gigi, and Jiri Pirko.
    
    10) Add SKB array infrastructure and convert tun and macvtap over to it.
        From Michael S Tsirkin and Jason Wang.
    
    11) Support qdisc packet injection in pktgen, from John Fastabend.
    
    12) Add neighbour monitoring framework to TIPC, from Jon Paul Maloy.
    
    13) Add NV congestion control support to TCP, from Lawrence Brakmo.
    
    14) Add GSO support to SCTP, from Marcelo Ricardo Leitner.
    
    15) Allow GRO and RPS to function on macsec devices, from Paolo Abeni.
    
    16) Support MPLS over IPV4, from Simon Horman.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1622 commits)
      xgene: Fix build warning with ACPI disabled.
      be2net: perform temperature query in adapter regardless of its interface state
      l2tp: Correctly return -EBADF from pppol2tp_getname.
      net/mlx5_core/health: Remove deprecated create_singlethread_workqueue
      net: ipmr/ip6mr: update lastuse on entry change
      macsec: ensure rx_sa is set when validation is disabled
      tipc: dump monitor attributes
      tipc: add a function to get the bearer name
      tipc: get monitor threshold for the cluster
      tipc: make cluster size threshold for monitoring configurable
      tipc: introduce constants for tipc address validation
      net: neigh: disallow transition to NUD_STALE if lladdr is unchanged in neigh_update()
      MAINTAINERS: xgene: Add driver and documentation path
      Documentation: dtb: xgene: Add MDIO node
      dtb: xgene: Add MDIO node
      drivers: net: xgene: ethtool: Use phy_ethtool_gset and sset
      drivers: net: xgene: Use exported functions
      drivers: net: xgene: Enable MDIO driver
      drivers: net: xgene: Add backward compatibility
      drivers: net: phy: xgene: Add MDIO driver
      ...

commit aa7145c16d6bf086538ad7eb20c807513bfa5efc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jul 22 01:19:42 2016 +0200

    bpf, events: fix offset in skb copy handler
    
    This patch fixes the __output_custom() routine we currently use with
    bpf_skb_copy(). I missed that when len is larger than the size of the
    current handle, we can issue multiple invocations of copy_func, and
    __output_custom() advances destination but also source buffer by the
    written amount of bytes. When we have __output_custom(), this is actually
    wrong since in that case the source buffer points to a non-linear object,
    in our case an skb, which the copy_func helper is supposed to walk.
    Therefore, since this is non-linear we thus need to pass the offset into
    the helper, so that copy_func can use it for extracting the data from
    the source object.
    
    Therefore, adjust the callback signatures properly and pass offset
    into the skb_header_pointer() invoked from bpf_skb_copy() callback. The
    __DEFINE_OUTPUT_COPY_BODY() is adjusted to accommodate for two things:
    i) to pass in whether we should advance source buffer or not; this is
    a compile-time constant condition, ii) to pass in the offset for
    __output_custom(), which we do with help of __VA_ARGS__, so everything
    can stay inlined as is currently. Both changes allow for adapting the
    __output_* fast-path helpers w/o extra overhead.
    
    Fixes: 555c8a8623a3 ("bpf: avoid stack copy and use skb ctx for event output")
    Fixes: 7e3f977edd0b ("perf, events: add non-linear data support for raw records")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e79e6c6fed89..15e55b7ee096 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -70,7 +70,7 @@ struct perf_callchain_entry_ctx {
 };
 
 typedef unsigned long (*perf_copy_f)(void *dst, const void *src,
-				     unsigned long len);
+				     unsigned long off, unsigned long len);
 
 struct perf_raw_frag {
 	union {

commit 7e3f977edd0bd9ea6104156feba95bb5ae9bdd38
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jul 14 18:08:03 2016 +0200

    perf, events: add non-linear data support for raw records
    
    This patch adds support for non-linear data on raw records. It
    extends raw records to have one or multiple fragments that will
    be written linearly into the ring slot, where each fragment can
    optionally have a custom callback handler to walk and extract
    complex, possibly non-linear data.
    
    If a callback handler is provided for a fragment, then the new
    __output_custom() will be used instead of __output_copy() for
    the perf_output_sample() part. perf_prepare_sample() does all
    the size calculation only once, so perf_output_sample() doesn't
    need to redo the same work anymore, meaning real_size and padding
    will be cached in the raw record. The raw record becomes 32 bytes
    in size without holes; to not increase it further and to avoid
    doing unnecessary recalculations in fast-path, we can reuse
    next pointer of the last fragment, idea here is borrowed from
    ZERO_OR_NULL_PTR(), which should keep the perf_output_sample()
    path for PERF_SAMPLE_RAW minimal.
    
    This facility is needed for BPF's event output helper as a first
    user that will, in a follow-up, add an additional perf_raw_frag
    to its perf_raw_record in order to be able to more efficiently
    dump skb context after a linear head meta data related to it.
    skbs can be non-linear and thus need a custom output function to
    dump buffers. Currently, the skb data needs to be copied twice;
    with the help of __output_custom() this work only needs to be
    done once. Future users could be things like XDP/BPF programs
    that work on different context though and would thus also have
    a different callback function.
    
    The few users of raw records are adapted to initialize their frag
    data from the raw record itself, no change in behavior for them.
    The code is based upon a PoC diff provided by Peter Zijlstra [1].
    
      [1] http://thread.gmane.org/gmane.linux.network/421294
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1a827cecd62f..e79e6c6fed89 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -69,9 +69,22 @@ struct perf_callchain_entry_ctx {
 	bool			    contexts_maxed;
 };
 
+typedef unsigned long (*perf_copy_f)(void *dst, const void *src,
+				     unsigned long len);
+
+struct perf_raw_frag {
+	union {
+		struct perf_raw_frag	*next;
+		unsigned long		pad;
+	};
+	perf_copy_f			copy;
+	void				*data;
+	u32				size;
+} __packed;
+
 struct perf_raw_record {
+	struct perf_raw_frag		frag;
 	u32				size;
-	void				*data;
 };
 
 /*
@@ -1283,6 +1296,11 @@ extern void perf_restore_debug_store(void);
 static inline void perf_restore_debug_store(void)			{ }
 #endif
 
+static __always_inline bool perf_raw_frag_last(const struct perf_raw_frag *frag)
+{
+	return frag->pad < sizeof(u64);
+}
+
 #define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))
 
 /*

commit 89ab9cb16931873ec600a909b3a38436352e629a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:16:28 2016 +0000

    perf/core: Remove perf CPU notifier code
    
    All users converted to state machine callbacks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153335.115333381@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9abeb6948e70..ddd3dab0f39e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1285,41 +1285,6 @@ static inline void perf_restore_debug_store(void)			{ }
 
 #define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))
 
-/*
- * This has to have a higher priority than migration_notifier in sched/core.c.
- */
-#define perf_cpu_notifier(fn)						\
-do {									\
-	static struct notifier_block fn##_nb =				\
-		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
-	unsigned long cpu = smp_processor_id();				\
-	unsigned long flags;						\
-									\
-	cpu_notifier_register_begin();					\
-	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,			\
-		(void *)(unsigned long)cpu);				\
-	local_irq_save(flags);						\
-	fn(&fn##_nb, (unsigned long)CPU_STARTING,			\
-		(void *)(unsigned long)cpu);				\
-	local_irq_restore(flags);					\
-	fn(&fn##_nb, (unsigned long)CPU_ONLINE,				\
-		(void *)(unsigned long)cpu);				\
-	__register_cpu_notifier(&fn##_nb);				\
-	cpu_notifier_register_done();					\
-} while (0)
-
-/*
- * Bare-bones version of perf_cpu_notifier(), which doesn't invoke the
- * callback for already online CPUs.
- */
-#define __perf_cpu_notifier(fn)						\
-do {									\
-	static struct notifier_block fn##_nb =				\
-		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
-									\
-	__register_cpu_notifier(&fn##_nb);				\
-} while (0)
-
 struct perf_pmu_events_attr {
 	struct device_attribute attr;
 	u64 id;

commit 00e16c3d68fce504e880f59c9bdf23b2a4759d6d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:16:09 2016 +0000

    perf/core: Convert to hotplug state machine
    
    Actually a nice symmetric startup/teardown pair which fits properly into
    the state machine concept. In the long run we should be able to invoke
    the startup callback for the boot CPU via the state machine and get
    rid of the init function which invokes it on the boot CPU.
    
    Note: This comes actually before the perf hardware callbacks. In the notifier
    model the hardware callbacks have a higher priority than the core
    callback. But that's solely for CPU offline so that hardware migration of
    events happens before the core is notified about the outgoing CPU.
    
    With the symetric state array model we have the following ordering:
    
     UP:     core -> hardware
     DOWN:   hardware -> core
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153333.587514098@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1a827cecd62f..9abeb6948e70 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1354,4 +1354,13 @@ _name##_show(struct device *dev,					\
 									\
 static struct device_attribute format_attr_##_name = __ATTR_RO(_name)
 
+/* Performance counter hotplug functions */
+#ifdef CONFIG_PERF_EVENTS
+int perf_event_init_cpu(unsigned int cpu);
+int perf_event_exit_cpu(unsigned int cpu);
+#else
+#define perf_event_init_cpu	NULL
+#define perf_event_exit_cpu	NULL
+#endif
+
 #endif /* _LINUX_PERF_EVENT_H */

commit 616d1c1b98ac79f30216a57a170dd7cea19b3df3
Merge: a4f144ebbdf6 c8ae067f2635
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jun 8 09:26:46 2016 +0200

    Merge branch 'linus' into perf/core, to refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fc07e9f983b4b11922c22b6cccadc1f342f05a4c
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu May 19 17:09:56 2016 -0700

    perf/x86: Support sysfs files depending on SMT status
    
    Add a way to show different sysfs events attributes depending on
    HyperThreading is on or off. This is difficult to determine
    early at boot, so we just do it dynamically when the sysfs
    attribute is read.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: http://lkml.kernel.org/r/1463703002-19686-3-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 92e9ce737432..a7593d653b40 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1334,6 +1334,13 @@ struct perf_pmu_events_attr {
 	const char *event_str;
 };
 
+struct perf_pmu_events_ht_attr {
+	struct device_attribute			attr;
+	u64					id;
+	const char				*event_str_ht;
+	const char				*event_str_noht;
+};
+
 ssize_t perf_event_sysfs_show(struct device *dev, struct device_attribute *attr,
 			      char *page);
 

commit f2fb6bef92514432398a653df1c2f1041d79ac46
Author: Kan Liang <kan.liang@intel.com>
Date:   Wed Mar 23 11:24:37 2016 -0700

    perf/core: Optimize side-band event delivery
    
    The perf_event_aux() function iterates all PMUs and all events in
    their respective per-CPU contexts to find the events to deliver
    side-band records to.
    
    For example, the brk test case in lkp triggers many mmap() operations,
    which, if we're also running perf, results in many perf_event_aux()
    invocations.
    
    If we enable uncore PMU support (even when uncore events are not used),
    dozens of uncore PMUs will be iterated, which can significantly
    decrease brk_test's throughput.
    
    For example, the brk throughput:
    
      without uncore PMUs: 2647573 ops_per_sec
      with    uncore PMUs: 1768444 ops_per_sec
    
    ... a 33% reduction.
    
    To get at the per-CPU events that need side-band records, this patch
    puts these events on a per-CPU list, this avoids iterating the PMUs
    and any events that do not need side-band records.
    
    Per task events are unchanged to avoid extra overhead on the context
    switch paths.
    
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reported-by: Huang, Ying <ying.huang@linux.intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1458757477-3781-1-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0e43355c7aad..92e9ce737432 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -517,6 +517,11 @@ struct swevent_hlist {
 struct perf_cgroup;
 struct ring_buffer;
 
+struct pmu_event_list {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -675,6 +680,7 @@ struct perf_event {
 	int				cgrp_defer_enabled;
 #endif
 
+	struct list_head		sb_list;
 #endif /* CONFIG_PERF_EVENTS */
 };
 

commit 97c79a38cd454602645f0470ffb444b3b75ce574
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Apr 28 13:16:33 2016 -0300

    perf core: Per event callchain limit
    
    Additionally to being able to control the system wide maximum depth via
    /proc/sys/kernel/perf_event_max_stack, now we are able to ask for
    different depths per event, using perf_event_attr.sample_max_stack for
    that.
    
    This uses an u16 hole at the end of perf_event_attr, that, when
    perf_event_attr.sample_type has the PERF_SAMPLE_CALLCHAIN, if
    sample_max_stack is zero, means use perf_event_max_stack, otherwise
    it'll be bounds checked under callchain_mutex.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/n/tip-kolmn1yo40p7jhswxwrc7rrd@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6b87be908790..0e43355c7aad 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1076,7 +1076,7 @@ extern void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct
 extern struct perf_callchain_entry *
 get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
 		   u32 max_stack, bool crosstask, bool add_mark);
-extern int get_callchain_buffers(void);
+extern int get_callchain_buffers(int max_stack);
 extern void put_callchain_buffers(void);
 
 extern int sysctl_perf_event_max_stack;

commit bdc6b758e443c21c39a14c075e5b7e01f095b37b
Merge: c4a346002bc0 0c9f790fcbda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 25 17:05:40 2016 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "Mostly tooling and PMU driver fixes, but also a number of late updates
      such as the reworking of the call-chain size limiting logic to make
      call-graph recording more robust, plus tooling side changes for the
      new 'backwards ring-buffer' extension to the perf ring-buffer"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      perf record: Read from backward ring buffer
      perf record: Rename variable to make code clear
      perf record: Prevent reading invalid data in record__mmap_read
      perf evlist: Add API to pause/resume
      perf trace: Use the ptr->name beautifier as default for "filename" args
      perf trace: Use the fd->name beautifier as default for "fd" args
      perf report: Add srcline_from/to branch sort keys
      perf evsel: Record fd into perf_mmap
      perf evsel: Add overwrite attribute and check write_backward
      perf tools: Set buildid dir under symfs when --symfs is provided
      perf trace: Only auto set call-graph to "dwarf" when syscalls are being traced
      perf annotate: Sort list of recognised instructions
      perf annotate: Fix identification of ARM blt and bls instructions
      perf tools: Fix usage of max_stack sysctl
      perf callchain: Stop validating callchains by the max_stack sysctl
      perf trace: Fix exit_group() formatting
      perf top: Use machine->kptr_restrict_warned
      perf trace: Warn when trying to resolve kernel addresses with kptr_restrict=1
      perf machine: Do not bail out if not managing to read ref reloc symbol
      perf/x86/intel/p4: Trival indentation fix, remove space
      ...

commit a7fd20d1c476af4563e66865213474a2f9f473a4
Merge: b80fed959551 917fa5353da0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 17 16:26:30 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support SPI based w5100 devices, from Akinobu Mita.
    
       2) Partial Segmentation Offload, from Alexander Duyck.
    
       3) Add GMAC4 support to stmmac driver, from Alexandre TORGUE.
    
       4) Allow cls_flower stats offload, from Amir Vadai.
    
       5) Implement bpf blinding, from Daniel Borkmann.
    
       6) Optimize _ASYNC_ bit twiddling on sockets, unless the socket is
          actually using FASYNC these atomics are superfluous.  From Eric
          Dumazet.
    
       7) Run TCP more preemptibly, also from Eric Dumazet.
    
       8) Support LED blinking, EEPROM dumps, and rxvlan offloading in mlx5e
          driver, from Gal Pressman.
    
       9) Allow creating ppp devices via rtnetlink, from Guillaume Nault.
    
      10) Improve BPF usage documentation, from Jesper Dangaard Brouer.
    
      11) Support tunneling offloads in qed, from Manish Chopra.
    
      12) aRFS offloading in mlx5e, from Maor Gottlieb.
    
      13) Add RFS and RPS support to SCTP protocol, from Marcelo Ricardo
          Leitner.
    
      14) Add MSG_EOR support to TCP, this allows controlling packet
          coalescing on application record boundaries for more accurate
          socket timestamp sampling.  From Martin KaFai Lau.
    
      15) Fix alignment of 64-bit netlink attributes across the board, from
          Nicolas Dichtel.
    
      16) Per-vlan stats in bridging, from Nikolay Aleksandrov.
    
      17) Several conversions of drivers to ethtool ksettings, from Philippe
          Reynes.
    
      18) Checksum neutral ILA in ipv6, from Tom Herbert.
    
      19) Factorize all of the various marvell dsa drivers into one, from
          Vivien Didelot
    
      20) Add VF support to qed driver, from Yuval Mintz"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1649 commits)
      Revert "phy dp83867: Fix compilation with CONFIG_OF_MDIO=m"
      Revert "phy dp83867: Make rgmii parameters optional"
      r8169: default to 64-bit DMA on recent PCIe chips
      phy dp83867: Make rgmii parameters optional
      phy dp83867: Fix compilation with CONFIG_OF_MDIO=m
      bpf: arm64: remove callee-save registers use for tmp registers
      asix: Fix offset calculation in asix_rx_fixup() causing slow transmissions
      switchdev: pass pointer to fib_info instead of copy
      net_sched: close another race condition in tcf_mirred_release()
      tipc: fix nametable publication field in nl compat
      drivers: net: Don't print unpopulated net_device name
      qed: add support for dcbx.
      ravb: Add missing free_irq() calls to ravb_close()
      qed: Remove a stray tab
      net: ethernet: fec-mpc52xx: use phy_ethtool_{get|set}_link_ksettings
      net: ethernet: fec-mpc52xx: use phydev from struct net_device
      bpf, doc: fix typo on bpf_asm descriptions
      stmmac: hardware TX COE doesn't work when force_thresh_dma_mode is set
      net: ethernet: fs-enet: use phy_ethtool_{get|set}_link_ksettings
      net: ethernet: fs-enet: use phydev from struct net_device
      ...

commit c85b03349640b34f3545503c8429fc43005e9a92
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu May 12 13:06:21 2016 -0300

    perf core: Separate accounting of contexts and real addresses in a stack trace
    
    The perf_sample->ip_callchain->nr value includes all the entries in the
    ip_callchain->ip[] array, real addresses and PERF_CONTEXT_{KERNEL,USER,etc},
    while what the user expects is that what is in the kernel.perf_event_max_stack
    sysctl or in the upcoming per event perf_event_attr.sample_max_stack knob be
    honoured in terms of IP addresses in the stack trace.
    
    So allocate a bunch of extra entries for contexts, and do the accounting
    via perf_callchain_entry_ctx struct members.
    
    A new sysctl, kernel.perf_event_max_contexts_per_stack is also
    introduced for investigating possible bugs in the callchain
    implementation by some arch.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/n/tip-3b4wnqk340c4sg4gwkfdi9yk@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2024b14cc2b1..6b87be908790 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -65,6 +65,8 @@ struct perf_callchain_entry_ctx {
 	struct perf_callchain_entry *entry;
 	u32			    max_stack;
 	u32			    nr;
+	short			    contexts;
+	bool			    contexts_maxed;
 };
 
 struct perf_raw_record {
@@ -1078,12 +1080,24 @@ extern int get_callchain_buffers(void);
 extern void put_callchain_buffers(void);
 
 extern int sysctl_perf_event_max_stack;
+extern int sysctl_perf_event_max_contexts_per_stack;
 
-#define perf_callchain_store_context(ctx, context) perf_callchain_store(ctx, context)
+static inline int perf_callchain_store_context(struct perf_callchain_entry_ctx *ctx, u64 ip)
+{
+	if (ctx->contexts < sysctl_perf_event_max_contexts_per_stack) {
+		struct perf_callchain_entry *entry = ctx->entry;
+		entry->ip[entry->nr++] = ip;
+		++ctx->contexts;
+		return 0;
+	} else {
+		ctx->contexts_maxed = true;
+		return -1; /* no more room, stop walking the stack */
+	}
+}
 
 static inline int perf_callchain_store(struct perf_callchain_entry_ctx *ctx, u64 ip)
 {
-	if (ctx->nr < ctx->max_stack) {
+	if (ctx->nr < ctx->max_stack && !ctx->contexts_maxed) {
 		struct perf_callchain_entry *entry = ctx->entry;
 		entry->ip[entry->nr++] = ip;
 		++ctx->nr;

commit 3e4de4ec4cfea40994b47a79767610153edbf45b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu May 12 13:01:50 2016 -0300

    perf core: Add perf_callchain_store_context() helper
    
    We need have different helpers to account how many contexts we have in
    the sample and for real addresses, so do it now as a prep patch, to
    ease review.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-q964tnyuqrxw5gld18vizs3c@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3803bb1a862b..2024b14cc2b1 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1079,6 +1079,8 @@ extern void put_callchain_buffers(void);
 
 extern int sysctl_perf_event_max_stack;
 
+#define perf_callchain_store_context(ctx, context) perf_callchain_store(ctx, context)
+
 static inline int perf_callchain_store(struct perf_callchain_entry_ctx *ctx, u64 ip)
 {
 	if (ctx->nr < ctx->max_stack) {

commit 3b1fff08038bd0792b1aa1e9703b2dd0512a3fd0
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue May 10 18:08:32 2016 -0300

    perf core: Add a 'nr' field to perf_event_callchain_context
    
    We will use it to count how many addresses are in the entry->ip[] array,
    excluding PERF_CONTEXT_{KERNEL,USER,etc} entries, so that we can really
    return the number of entries specified by the user via the relevant
    sysctl, kernel.perf_event_max_contexts, or via the per event
    perf_event_attr.sample_max_stack knob.
    
    This way we keep the perf_sample->ip_callchain->nr meaning, that is the
    number of entries, be it real addresses or PERF_CONTEXT_ entries, while
    honouring the max_stack knobs, i.e. the end result will be max_stack
    entries if we have at least that many entries in a given stack trace.
    
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-s8teto51tdqvlfhefndtat9r@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index dbd18246b36e..3803bb1a862b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -64,6 +64,7 @@ struct perf_callchain_entry {
 struct perf_callchain_entry_ctx {
 	struct perf_callchain_entry *entry;
 	u32			    max_stack;
+	u32			    nr;
 };
 
 struct perf_raw_record {
@@ -1080,9 +1081,10 @@ extern int sysctl_perf_event_max_stack;
 
 static inline int perf_callchain_store(struct perf_callchain_entry_ctx *ctx, u64 ip)
 {
-	struct perf_callchain_entry *entry = ctx->entry;
-	if (entry->nr < ctx->max_stack) {
+	if (ctx->nr < ctx->max_stack) {
+		struct perf_callchain_entry *entry = ctx->entry;
 		entry->ip[entry->nr++] = ip;
+		++ctx->nr;
 		return 0;
 	} else {
 		return -1; /* no more room, stop walking the stack */

commit cfbcf468454ab4b20f0b4b62da51920b99fdb19e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Apr 28 12:30:53 2016 -0300

    perf core: Pass max stack as a perf_callchain_entry context
    
    This makes perf_callchain_{user,kernel}() receive the max stack
    as context for the perf_callchain_entry, instead of accessing
    the global sysctl_perf_event_max_stack.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/n/tip-kolmn1yo40p7jhswxwrc7rrd@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9e1c3ada91c4..dbd18246b36e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -61,6 +61,11 @@ struct perf_callchain_entry {
 	__u64				ip[0]; /* /proc/sys/kernel/perf_event_max_stack */
 };
 
+struct perf_callchain_entry_ctx {
+	struct perf_callchain_entry *entry;
+	u32			    max_stack;
+};
+
 struct perf_raw_record {
 	u32				size;
 	void				*data;
@@ -1063,19 +1068,20 @@ extern void perf_event_fork(struct task_struct *tsk);
 /* Callchains */
 DECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);
 
-extern void perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs);
-extern void perf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs);
+extern void perf_callchain_user(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs);
+extern void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs);
 extern struct perf_callchain_entry *
 get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
-		   bool crosstask, bool add_mark);
+		   u32 max_stack, bool crosstask, bool add_mark);
 extern int get_callchain_buffers(void);
 extern void put_callchain_buffers(void);
 
 extern int sysctl_perf_event_max_stack;
 
-static inline int perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
+static inline int perf_callchain_store(struct perf_callchain_entry_ctx *ctx, u64 ip)
 {
-	if (entry->nr < sysctl_perf_event_max_stack) {
+	struct perf_callchain_entry *entry = ctx->entry;
+	if (entry->nr < ctx->max_stack) {
 		entry->ip[entry->nr++] = ip;
 		return 0;
 	} else {

commit 5101ef20f0ef1de79091a1fdb6b1a7f07565545a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 26 11:33:46 2016 +0100

    perf/arm: Special-case hetereogeneous CPUs
    
    Commit:
    
      26657848502b7847 ("perf/core: Verify we have a single perf_hw_context PMU")
    
    forcefully prevents multiple PMUs from sharing perf_hw_context, as this
    generally doesn't make sense. It is a common bug for uncore PMUs to
    use perf_hw_context rather than perf_invalid_context, which this detects.
    
    However, systems exist with heterogeneous CPUs (and hence heterogeneous
    HW PMUs), for which sharing perf_hw_context is necessary, and possible
    in some limited cases.
    
    To make this work we have to perform some gymnastics, as we did in these
    commits:
    
      66eb579e66ecfea5 ("perf: allow for PMU-specific event filtering")
      c904e32a69b7c779 ("arm: perf: filter unschedulable events")
    
    To allow those systems to work, we must allow PMUs for heterogeneous
    CPUs to share perf_hw_context, though we must still disallow sharing
    otherwise to detect the common misuse of perf_hw_context.
    
    This patch adds a new PERF_PMU_CAP_HETEROGENEOUS_CPUS for this, updates
    the core logic to account for this, and makes use of it in the arm_pmu
    code that is used for systems with heterogeneous CPUs. Comments are
    added to make the rationale clear and hopefully avoid accidental abuse.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20160426103346.GA20836@leverpostej
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c77e4a159fa2..9e1c3ada91c4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -225,6 +225,7 @@ struct perf_event;
 #define PERF_PMU_CAP_AUX_SW_DOUBLEBUF		0x08
 #define PERF_PMU_CAP_EXCLUSIVE			0x10
 #define PERF_PMU_CAP_ITRACE			0x20
+#define PERF_PMU_CAP_HETEROGENEOUS_CPUS		0x40
 
 /**
  * struct pmu - generic performance monitoring unit

commit 375637bc524952f1122ea22caf5a8f1fecad8228
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Apr 27 18:44:46 2016 +0300

    perf/core: Introduce address range filtering
    
    Many instruction tracing PMUs out there support address range-based
    filtering, which would, for example, generate trace data only for a
    given range of instruction addresses, which is useful for tracing
    individual functions, modules or libraries. Other PMUs may also
    utilize this functionality to allow filtering to or filtering out
    code at certain address ranges.
    
    This patch introduces the interface for userspace to specify these
    filters and for the PMU drivers to apply these filters to hardware
    configuration.
    
    The user interface is an ASCII string that is passed via an ioctl()
    and specifies (in the form of an ASCII string) address ranges within
    certain object files or within kernel. There is no special treatment
    for kernel modules yet, but it might be a worthy pursuit.
    
    The PMU driver interface basically adds two extra callbacks to the
    PMU driver structure, one of which validates the filter configuration
    proposed by the user against what the hardware is actually capable of
    doing and the other one translates hardware-independent filter
    configuration into something that can be programmed into the
    hardware.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/1461771888-10409-6-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a090700cccca..c77e4a159fa2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -151,6 +151,15 @@ struct hw_perf_event {
 	 */
 	struct task_struct		*target;
 
+	/*
+	 * PMU would store hardware filter configuration
+	 * here.
+	 */
+	void				*addr_filters;
+
+	/* Last sync'ed generation of filters */
+	unsigned long			addr_filters_gen;
+
 /*
  * hw_perf_event::state flags; used to track the PERF_EF_* state.
  */
@@ -240,6 +249,9 @@ struct pmu {
 	int				task_ctx_nr;
 	int				hrtimer_interval_ms;
 
+	/* number of address filters this PMU can do */
+	unsigned int			nr_addr_filters;
+
 	/*
 	 * Fully disable/enable this PMU, can be used to protect from the PMI
 	 * as well as for lazy/batch writing of the MSRs.
@@ -392,12 +404,71 @@ struct pmu {
 	 */
 	void (*free_aux)		(void *aux); /* optional */
 
+	/*
+	 * Validate address range filters: make sure the HW supports the
+	 * requested configuration and number of filters; return 0 if the
+	 * supplied filters are valid, -errno otherwise.
+	 *
+	 * Runs in the context of the ioctl()ing process and is not serialized
+	 * with the rest of the PMU callbacks.
+	 */
+	int (*addr_filters_validate)	(struct list_head *filters);
+					/* optional */
+
+	/*
+	 * Synchronize address range filter configuration:
+	 * translate hw-agnostic filters into hardware configuration in
+	 * event::hw::addr_filters.
+	 *
+	 * Runs as a part of filter sync sequence that is done in ->start()
+	 * callback by calling perf_event_addr_filters_sync().
+	 *
+	 * May (and should) traverse event::addr_filters::list, for which its
+	 * caller provides necessary serialization.
+	 */
+	void (*addr_filters_sync)	(struct perf_event *event);
+					/* optional */
+
 	/*
 	 * Filter events for PMU-specific reasons.
 	 */
 	int (*filter_match)		(struct perf_event *event); /* optional */
 };
 
+/**
+ * struct perf_addr_filter - address range filter definition
+ * @entry:	event's filter list linkage
+ * @inode:	object file's inode for file-based filters
+ * @offset:	filter range offset
+ * @size:	filter range size
+ * @range:	1: range, 0: address
+ * @filter:	1: filter/start, 0: stop
+ *
+ * This is a hardware-agnostic filter configuration as specified by the user.
+ */
+struct perf_addr_filter {
+	struct list_head	entry;
+	struct inode		*inode;
+	unsigned long		offset;
+	unsigned long		size;
+	unsigned int		range	: 1,
+				filter	: 1;
+};
+
+/**
+ * struct perf_addr_filters_head - container for address range filters
+ * @list:	list of filters for this event
+ * @lock:	spinlock that serializes accesses to the @list and event's
+ *		(and its children's) filter generations.
+ *
+ * A child event will use parent's @list (and therefore @lock), so they are
+ * bundled together; see perf_event_addr_filters().
+ */
+struct perf_addr_filters_head {
+	struct list_head	list;
+	raw_spinlock_t		lock;
+};
+
 /**
  * enum perf_event_active_state - the states of a event
  */
@@ -566,6 +637,12 @@ struct perf_event {
 
 	atomic_t			event_limit;
 
+	/* address range filters */
+	struct perf_addr_filters_head	addr_filters;
+	/* vma address array for file-based filders */
+	unsigned long			*addr_filters_offs;
+	unsigned long			addr_filters_gen;
+
 	void (*destroy)(struct perf_event *);
 	struct rcu_head			rcu_head;
 
@@ -1070,6 +1147,27 @@ static inline bool is_write_backward(struct perf_event *event)
 	return !!event->attr.write_backward;
 }
 
+static inline bool has_addr_filter(struct perf_event *event)
+{
+	return event->pmu->nr_addr_filters;
+}
+
+/*
+ * An inherited event uses parent's filters
+ */
+static inline struct perf_addr_filters_head *
+perf_event_addr_filters(struct perf_event *event)
+{
+	struct perf_addr_filters_head *ifh = &event->addr_filters;
+
+	if (event->parent)
+		ifh = &event->parent->addr_filters;
+
+	return ifh;
+}
+
+extern void perf_event_addr_filters_sync(struct perf_event *event);
+
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
 extern int perf_output_begin_forward(struct perf_output_handle *handle,

commit c5dfd78eb79851e278b7973031b9ca363da87a7e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Apr 21 12:28:50 2016 -0300

    perf core: Allow setting up max frame stack depth via sysctl
    
    The default remains 127, which is good for most cases, and not even hit
    most of the time, but then for some cases, as reported by Brendan, 1024+
    deep frames are appearing on the radar for things like groovy, ruby.
    
    And in some workloads putting a _lower_ cap on this may make sense. One
    that is per event still needs to be put in place tho.
    
    The new file is:
    
      # cat /proc/sys/kernel/perf_event_max_stack
      127
    
    Chaging it:
    
      # echo 256 > /proc/sys/kernel/perf_event_max_stack
      # cat /proc/sys/kernel/perf_event_max_stack
      256
    
    But as soon as there is some event using callchains we get:
    
      # echo 512 > /proc/sys/kernel/perf_event_max_stack
      -bash: echo: write error: Device or resource busy
      #
    
    Because we only allocate the callchain percpu data structures when there
    is a user, which allows for changing the max easily, its just a matter
    of having no callchain users at that point.
    
    Reported-and-Tested-by: Brendan Gregg <brendan.d.gregg@gmail.com>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: David Ahern <dsahern@gmail.com>
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/r/20160426002928.GB16708@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 85749ae8cb5f..a090700cccca 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -58,7 +58,7 @@ struct perf_guest_info_callbacks {
 
 struct perf_callchain_entry {
 	__u64				nr;
-	__u64				ip[PERF_MAX_STACK_DEPTH];
+	__u64				ip[0]; /* /proc/sys/kernel/perf_event_max_stack */
 };
 
 struct perf_raw_record {
@@ -993,9 +993,11 @@ get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
 extern int get_callchain_buffers(void);
 extern void put_callchain_buffers(void);
 
+extern int sysctl_perf_event_max_stack;
+
 static inline int perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
 {
-	if (entry->nr < PERF_MAX_STACK_DEPTH) {
+	if (entry->nr < sysctl_perf_event_max_stack) {
 		entry->ip[entry->nr++] = ip;
 		return 0;
 	} else {
@@ -1017,6 +1019,8 @@ extern int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+int perf_event_max_stack_handler(struct ctl_table *table, int write,
+				 void __user *buffer, size_t *lenp, loff_t *ppos);
 
 static inline bool perf_paranoid_tracepoint_raw(void)
 {

commit 9ecda41acb971ebd07c8fb35faf24005c0baea12
Author: Wang Nan <wangnan0@huawei.com>
Date:   Tue Apr 5 14:11:18 2016 +0000

    perf/core: Add ::write_backward attribute to perf event
    
    This patch introduces 'write_backward' bit to perf_event_attr, which
    controls the direction of a ring buffer. After set, the corresponding
    ring buffer is written from end to beginning. This feature is design to
    support reading from overwritable ring buffer.
    
    Ring buffer can be created by mapping a perf event fd. Kernel puts event
    records into ring buffer, user tooling like perf fetch them from
    address returned by mmap(). To prevent racing between kernel and tooling,
    they communicate to each other through 'head' and 'tail' pointers.
    Kernel maintains 'head' pointer, points it to the next free area (tail
    of the last record). Tooling maintains 'tail' pointer, points it to the
    tail of last consumed record (record has already been fetched). Kernel
    determines the available space in a ring buffer using these two
    pointers to avoid overwrite unfetched records.
    
    By mapping without 'PROT_WRITE', an overwritable ring buffer is created.
    Different from normal ring buffer, tooling is unable to maintain 'tail'
    pointer because writing is forbidden. Therefore, for this type of ring
    buffers, kernel overwrite old records unconditionally, works like flight
    recorder. This feature would be useful if reading from overwritable ring
    buffer were as easy as reading from normal ring buffer. However,
    there's an obscure problem.
    
    The following figure demonstrates a full overwritable ring buffer. In
    this figure, the 'head' pointer points to the end of last record, and a
    long record 'E' is pending. For a normal ring buffer, a 'tail' pointer
    would have pointed to position (X), so kernel knows there's no more
    space in the ring buffer. However, for an overwritable ring buffer,
    kernel ignore the 'tail' pointer.
    
       (X)                              head
        .                                |
        .                                V
        +------+-------+----------+------+---+
        |A....A|B.....B|C........C|D....D|   |
        +------+-------+----------+------+---+
    
    Record 'A' is overwritten by event 'E':
    
          head
           |
           V
        +--+---+-------+----------+------+---+
        |.E|..A|B.....B|C........C|D....D|E..|
        +--+---+-------+----------+------+---+
    
    Now tooling decides to read from this ring buffer. However, none of these
    two natural positions, 'head' and the start of this ring buffer, are
    pointing to the head of a record. Even the full ring buffer can be
    accessed by tooling, it is unable to find a position to start decoding.
    
    The first attempt tries to solve this problem AFAIK can be found from
    [1]. It makes kernel to maintain 'tail' pointer: updates it when ring
    buffer is half full. However, this approach introduces overhead to
    fast path. Test result shows a 1% overhead [2]. In addition, this method
    utilizes no more tham 50% records.
    
    Another attempt can be found from [3], which allows putting the size of
    an event at the end of each record. This approach allows tooling to find
    records in a backward manner from 'head' pointer by reading size of a
    record from its tail. However, because of alignment requirement, it
    needs 8 bytes to record the size of a record, which is a huge waste. Its
    performance is also not good, because more data need to be written.
    This approach also introduces some extra branch instructions to fast
    path.
    
    'write_backward' is a better solution to this problem.
    
    Following figure demonstrates the state of the overwritable ring buffer
    when 'write_backward' is set before overwriting:
    
           head
            |
            V
        +---+------+----------+-------+------+
        |   |D....D|C........C|B.....B|A....A|
        +---+------+----------+-------+------+
    
    and after overwriting:
                                         head
                                          |
                                          V
        +---+------+----------+-------+---+--+
        |..E|D....D|C........C|B.....B|A..|E.|
        +---+------+----------+-------+---+--+
    
    In each situation, 'head' points to the beginning of the newest record.
    From this record, tooling can iterate over the full ring buffer and fetch
    records one by one.
    
    The only limitation that needs to be considered is back-to-back reading.
    Due to the non-deterministic of user programs, it is impossible to ensure
    the ring buffer keeps stable during reading. Consider an extreme situation:
    tooling is scheduled out after reading record 'D', then a burst of events
    come, eat up the whole ring buffer (one or multiple rounds). When the
    tooling process comes back, reading after 'D' is incorrect now.
    
    To prevent this problem, we need to find a way to ensure the ring buffer
    is stable during reading. ioctl(PERF_EVENT_IOC_PAUSE_OUTPUT) is
    suggested because its overhead is lower than
    ioctl(PERF_EVENT_IOC_ENABLE).
    
    By carefully verifying 'header' pointer, reader can avoid pausing the
    ring-buffer. For example:
    
        /* A union of all possible events */
        union perf_event event;
    
        p = head = perf_mmap__read_head();
        while (true) {
            /* copy header of next event */
            fetch(&event.header, p, sizeof(event.header));
    
            /* read 'head' pointer */
            head = perf_mmap__read_head();
    
            /* check overwritten: is the header good? */
            if (!verify(sizeof(event.header), p, head))
                break;
    
            /* copy the whole event */
            fetch(&event, p, event.header.size);
    
            /* read 'head' pointer again */
            head = perf_mmap__read_head();
    
            /* is the whole event good? */
            if (!verify(event.header.size, p, head))
                break;
            p += event.header.size;
        }
    
    However, the overhead is high because:
    
     a) In-place decoding is not safe.
        Copying-verifying-decoding is required.
     b) Fetching 'head' pointer requires additional synchronization.
    
    (From Alexei Starovoitov:
    
    Even when this trick works, pause is needed for more than stability of
    reading. When we collect the events into overwrite buffer we're waiting
    for some other trigger (like all cpu utilization spike or just one cpu
    running and all others are idle) and when it happens the buffer has
    valuable info from the past. At this point new events are no longer
    interesting and buffer should be paused, events read and unpaused until
    next trigger comes.)
    
    This patch utilizes event's default overflow_handler introduced
    previously. perf_event_output_backward() is created as the default
    overflow handler for backward ring buffers. To avoid extra overhead to
    fast path, original perf_event_output() becomes __perf_event_output()
    and marked '__always_inline'. In theory, there's no extra overhead
    introduced to fast path.
    
    Performance testing:
    
    Calling 3000000 times of 'close(-1)', use gettimeofday() to check
    duration.  Use 'perf record -o /dev/null -e raw_syscalls:*' to capture
    system calls. In ns.
    
    Testing environment:
    
      CPU    : Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz
      Kernel : v4.5.0
                        MEAN         STDVAR
     BASE            800214.950    2853.083
     PRE1           2253846.700    9997.014
     PRE2           2257495.540    8516.293
     POST           2250896.100    8933.921
    
    Where 'BASE' is pure performance without capturing. 'PRE1' is test
    result of pure 'v4.5.0' kernel. 'PRE2' is test result before this
    patch. 'POST' is test result after this patch. See [4] for the detailed
    experimental setup.
    
    Considering the stdvar, this patch doesn't introduce performance
    overhead to the fast path.
    
     [1] http://lkml.iu.edu/hypermail/linux/kernel/1304.1/04584.html
     [2] http://lkml.iu.edu/hypermail/linux/kernel/1307.1/00535.html
     [3] http://lkml.iu.edu/hypermail/linux/kernel/1512.0/01265.html
     [4] http://lkml.kernel.org/g/56F89DCD.1040202@huawei.com
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: <acme@kernel.org>
    Cc: <pi3orama@163.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/r/1459865478-53413-1-git-send-email-wangnan0@huawei.com
    [ Fixed the changelog some more. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b8b195fbe787..85749ae8cb5f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -834,14 +834,24 @@ extern int perf_event_overflow(struct perf_event *event,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
+extern void perf_event_output_forward(struct perf_event *event,
+				     struct perf_sample_data *data,
+				     struct pt_regs *regs);
+extern void perf_event_output_backward(struct perf_event *event,
+				       struct perf_sample_data *data,
+				       struct pt_regs *regs);
 extern void perf_event_output(struct perf_event *event,
-				struct perf_sample_data *data,
-				struct pt_regs *regs);
+			      struct perf_sample_data *data,
+			      struct pt_regs *regs);
 
 static inline bool
 is_default_overflow_handler(struct perf_event *event)
 {
-	return (event->overflow_handler == perf_event_output);
+	if (likely(event->overflow_handler == perf_event_output_forward))
+		return true;
+	if (unlikely(event->overflow_handler == perf_event_output_backward))
+		return true;
+	return false;
 }
 
 extern void
@@ -1051,8 +1061,20 @@ static inline bool has_aux(struct perf_event *event)
 	return event->pmu->setup_aux;
 }
 
+static inline bool is_write_backward(struct perf_event *event)
+{
+	return !!event->attr.write_backward;
+}
+
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
+extern int perf_output_begin_forward(struct perf_output_handle *handle,
+				    struct perf_event *event,
+				    unsigned int size);
+extern int perf_output_begin_backward(struct perf_output_handle *handle,
+				      struct perf_event *event,
+				      unsigned int size);
+
 extern void perf_output_end(struct perf_output_handle *handle);
 extern unsigned int perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);

commit 889fac6d67d46a5e781c08fb26fec9016db1c307
Merge: dad38ca64a25 bf1620068911
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Apr 13 08:57:03 2016 +0200

    Merge tag 'v4.6-rc3' into perf/core, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1e1dcd93b468901e114f279c94a0b356adc5e7cd
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:24 2016 -0700

    perf: split perf_trace_buf_prepare into alloc and update parts
    
    split allows to move expensive update of 'struct trace_entry' to later phase.
    Repurpose unused 1st argument of perf_tp_event() to indicate event type.
    
    While splitting use temp variable 'rctx' instead of '*rctx' to avoid
    unnecessary loads done by the compiler due to -fno-strict-aliasing
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e89f7199c223..eb41b535ef38 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1016,7 +1016,7 @@ static inline bool perf_paranoid_kernel(void)
 }
 
 extern void perf_event_init(void);
-extern void perf_tp_event(u64 addr, u64 count, void *record,
+extern void perf_tp_event(u16 event_type, u64 count, void *record,
 			  int entry_size, struct pt_regs *regs,
 			  struct hlist_head *head, int rctx,
 			  struct task_struct *task);

commit ec5e099d6e941668d121ea9ca7057f4fa00830b0
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:22 2016 -0700

    perf: optimize perf_fetch_caller_regs
    
    avoid memset in perf_fetch_caller_regs, since it's the critical path of all tracepoints.
    It's called from perf_sw_event_sched, perf_event_task_sched_in and all of perf_trace_##call
    with this_cpu_ptr(&__perf_regs[..]) which are zero initialized by perpcu init logic and
    subsequent call to perf_arch_fetch_caller_regs initializes the same fields on all archs,
    so we can safely drop memset from all of the above cases and move it into
    perf_ftrace_function_call that calls it with stack allocated pt_regs.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f291275ffd71..e89f7199c223 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -882,8 +882,6 @@ static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned lo
  */
 static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 {
-	memset(regs, 0, sizeof(*regs));
-
 	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 }
 

commit 1879445dfa7bbd6fe21b09c5cc72f4934798afed
Author: Wang Nan <wangnan0@huawei.com>
Date:   Mon Mar 28 06:41:30 2016 +0000

    perf/core: Set event's default ::overflow_handler()
    
    Set a default event->overflow_handler in perf_event_alloc() so don't
    need to check event->overflow_handler in __perf_event_overflow().
    Following commits can give a different default overflow_handler.
    
    Initial idea comes from Peter:
    
      http://lkml.kernel.org/r/20130708121557.GA17211@twins.programming.kicks-ass.net
    
    Since the default value of event->overflow_handler is not NULL, existing
    'if (!overflow_handler)' checks need to be changed.
    
    is_default_overflow_handler() is introduced for this.
    
    No extra performance overhead is introduced into the hot path because in the
    original code we still need to read this handler from memory. A conditional
    branch is avoided so actually we remove some instructions.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <pi3orama@163.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/r/1459147292-239310-3-git-send-email-wangnan0@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 15588d4c581d..4065ca2d7149 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -838,6 +838,12 @@ extern void perf_event_output(struct perf_event *event,
 				struct perf_sample_data *data,
 				struct pt_regs *regs);
 
+static inline bool
+is_default_overflow_handler(struct perf_event *event)
+{
+	return (event->overflow_handler == perf_event_output);
+}
+
 extern void
 perf_event_header__init_id(struct perf_event_header *header,
 			   struct perf_sample_data *data,

commit 3fa2fe2ce09c5a16be69c5319eb3347271a99735
Merge: d88f48e12821 05f5ece76a88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:02:14 2016 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "This tree contains various perf fixes on the kernel side, plus three
      hw/event-enablement late additions:
    
       - Intel Memory Bandwidth Monitoring events and handling
       - the AMD Accumulated Power Mechanism reporting facility
       - more IOMMU events
    
      ... and a final round of perf tooling updates/fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      perf llvm: Use strerror_r instead of the thread unsafe strerror one
      perf llvm: Use realpath to canonicalize paths
      perf tools: Unexport some methods unused outside strbuf.c
      perf probe: No need to use formatting strbuf method
      perf help: Use asprintf instead of adhoc equivalents
      perf tools: Remove unused perf_pathdup, xstrdup functions
      perf tools: Do not include stringify.h from the kernel sources
      tools include: Copy linux/stringify.h from the kernel
      tools lib traceevent: Remove redundant CPU output
      perf tools: Remove needless 'extern' from function prototypes
      perf tools: Simplify die() mechanism
      perf tools: Remove unused DIE_IF macro
      perf script: Remove lots of unused arguments
      perf thread: Rename perf_event__preprocess_sample_addr to thread__resolve
      perf machine: Rename perf_event__preprocess_sample to machine__resolve
      perf tools: Add cpumode to struct perf_sample
      perf tests: Forward the perf_sample in the dwarf unwind test
      perf tools: Remove misplaced __maybe_unused
      perf list: Fix documentation of :ppp
      perf bench numa: Fix assertion for nodes bitfield
      ...

commit c7ab62bfbe0e27ef452d19d88b083f01e99f13a7
Author: Huang Rui <ray.huang@amd.com>
Date:   Wed Mar 9 13:45:06 2016 +0800

    perf/x86/amd/power: Add AMD accumulated power reporting mechanism
    
    Introduce an AMD accumlated power reporting mechanism for the Family
    15h, Model 60h processor that can be used to calculate the average
    power consumed by a processor during a measurement interval. The
    feature support is indicated by CPUID Fn8000_0007_EDX[12].
    
    This feature will be implemented both in hwmon and perf. The current
    design provides one event to report per package/processor power
    consumption by counting each compute unit power value.
    
    Here the gory details of how the computation is done:
    
    * Tsample: compute unit power accumulator sample period
    * Tref: the PTSC counter period (PTSC: performance timestamp counter)
    * N: the ratio of compute unit power accumulator sample period to the
      PTSC period
    
    * Jmax: max compute unit accumulated power which is indicated by
      MSR_C001007b[MaxCpuSwPwrAcc]
    
    * Jx/Jy: compute unit accumulated power which is indicated by
      MSR_C001007a[CpuSwPwrAcc]
    
    * Tx/Ty: the value of performance timestamp counter which is indicated
      by CU_PTSC MSR_C0010280[PTSC]
    * PwrCPUave: CPU average power
    
    i. Determine the ratio of Tsample to Tref by executing CPUID Fn8000_0007.
            N = value of CPUID Fn8000_0007_ECX[CpuPwrSampleTimeRatio[15:0]].
    
    ii. Read the full range of the cumulative energy value from the new
        MSR MaxCpuSwPwrAcc.
            Jmax = value returned.
    
    iii. At time x, software reads CpuSwPwrAcc and samples the PTSC.
            Jx = value read from CpuSwPwrAcc and Tx = value read from PTSC.
    
    iv. At time y, software reads CpuSwPwrAcc and samples the PTSC.
            Jy = value read from CpuSwPwrAcc and Ty = value read from PTSC.
    
    v. Calculate the average power consumption for a compute unit over
    time period (y-x). Unit of result is uWatt:
    
            if (Jy < Jx) // Rollover has occurred
                    Jdelta = (Jy + Jmax) - Jx
            else
                    Jdelta = Jy - Jx
            PwrCPUave = N * Jdelta * 1000 / (Ty - Tx)
    
    Simple example:
    
      root@hr-zp:/home/ray/tip# ./tools/perf/perf stat -a -e 'power/power-pkg/' make -j4
        CHK     include/config/kernel.release
        CHK     include/generated/uapi/linux/version.h
        CHK     include/generated/utsrelease.h
        CHK     include/generated/timeconst.h
        CHK     include/generated/bounds.h
        CHK     include/generated/asm-offsets.h
        CALL    scripts/checksyscalls.sh
        CHK     include/generated/compile.h
        SKIPPED include/generated/compile.h
        Building modules, stage 2.
      Kernel: arch/x86/boot/bzImage is ready  (#40)
        MODPOST 4225 modules
    
       Performance counter stats for 'system wide':
    
                  183.44 mWatts power/power-pkg/
    
           341.837270111 seconds time elapsed
    
      root@hr-zp:/home/ray/tip# ./tools/perf/perf stat -a -e 'power/power-pkg/' sleep 10
    
       Performance counter stats for 'system wide':
    
                    0.18 mWatts power/power-pkg/
    
            10.012551815 seconds time elapsed
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Suggested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: jacob.w.shin@gmail.com
    Link: http://lkml.kernel.org/r/1457502306-2559-1-git-send-email-ray.huang@amd.com
    [ Fixed the modular build. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7bb315bec3aa..15588d4c581d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -129,6 +129,10 @@ struct hw_perf_event {
 		struct { /* itrace */
 			int			itrace_started;
 		};
+		struct { /* amd_power */
+			u64	pwr_acc;
+			u64	ptsc;
+		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */
 			/*

commit a223c1c7ab4cc64537dc4b911f760d851683768a
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Thu Mar 10 15:32:07 2016 -0800

    perf/x86/cqm: Fix CQM handling of grouping events into a cache_group
    
    Currently CQM (cache quality of service monitoring) is grouping all
    events belonging to same PID to use one RMID. However its not counting
    all of these different events. Hence we end up with a count of zero
    for all events other than the group leader.
    
    The patch tries to address the issue by keeping a flag in the
    perf_event.hw which has other CQM related fields. The field is updated
    at event creation and during grouping.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    [peterz: Changed hw_perf_event::is_group_event to an int]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: fenghua.yu@intel.com
    Cc: h.peter.anvin@intel.com
    Cc: ravi.v.shankar@intel.com
    Cc: vikas.shivappa@intel.com
    Link: http://lkml.kernel.org/r/1457652732-4499-2-git-send-email-vikas.shivappa@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 79ec7bbf0155..7bb315bec3aa 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -121,6 +121,7 @@ struct hw_perf_event {
 		struct { /* intel_cqm */
 			int			cqm_state;
 			u32			cqm_rmid;
+			int			is_group_event;
 			struct list_head	cqm_events_entry;
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;

commit 1200b6809dfd9d73bc4c7db76d288c35fa4b2ebe
Merge: 6b5f04b6cf8e fe30937b6535
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 10:05:34 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support more Realtek wireless chips, from Jes Sorenson.
    
       2) New BPF types for per-cpu hash and arrap maps, from Alexei
          Starovoitov.
    
       3) Make several TCP sysctls per-namespace, from Nikolay Borisov.
    
       4) Allow the use of SO_REUSEPORT in order to do per-thread processing
       of incoming TCP/UDP connections.  The muxing can be done using a
       BPF program which hashes the incoming packet.  From Craig Gallek.
    
       5) Add a multiplexer for TCP streams, to provide a messaged based
          interface.  BPF programs can be used to determine the message
          boundaries.  From Tom Herbert.
    
       6) Add 802.1AE MACSEC support, from Sabrina Dubroca.
    
       7) Avoid factorial complexity when taking down an inetdev interface
          with lots of configured addresses.  We were doing things like
          traversing the entire address less for each address removed, and
          flushing the entire netfilter conntrack table for every address as
          well.
    
       8) Add and use SKB bulk free infrastructure, from Jesper Brouer.
    
       9) Allow offloading u32 classifiers to hardware, and implement for
          ixgbe, from John Fastabend.
    
      10) Allow configuring IRQ coalescing parameters on a per-queue basis,
          from Kan Liang.
    
      11) Extend ethtool so that larger link mode masks can be supported.
          From David Decotigny.
    
      12) Introduce devlink, which can be used to configure port link types
          (ethernet vs Infiniband, etc.), port splitting, and switch device
          level attributes as a whole.  From Jiri Pirko.
    
      13) Hardware offload support for flower classifiers, from Amir Vadai.
    
      14) Add "Local Checksum Offload".  Basically, for a tunneled packet
          the checksum of the outer header is 'constant' (because with the
          checksum field filled into the inner protocol header, the payload
          of the outer frame checksums to 'zero'), and we can take advantage
          of that in various ways.  From Edward Cree"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1548 commits)
      bonding: fix bond_get_stats()
      net: bcmgenet: fix dma api length mismatch
      net/mlx4_core: Fix backward compatibility on VFs
      phy: mdio-thunder: Fix some Kconfig typos
      lan78xx: add ndo_get_stats64
      lan78xx: handle statistics counter rollover
      RDS: TCP: Remove unused constant
      RDS: TCP: Add sysctl tunables for sndbuf/rcvbuf on rds-tcp socket
      net: smc911x: convert pxa dma to dmaengine
      team: remove duplicate set of flag IFF_MULTICAST
      bonding: remove duplicate set of flag IFF_MULTICAST
      net: fix a comment typo
      ethernet: micrel: fix some error codes
      ip_tunnels, bpf: define IP_TUNNEL_OPTS_MAX and use it
      bpf, dst: add and use dst_tclassid helper
      bpf: make skb->tc_classid also readable
      net: mvneta: bm: clarify dependencies
      cls_bpf: reset class and reuse major in da
      ldmvsw: Checkpatch sunvnet.c and sunvnet_common.c
      ldmvsw: Add ldmvsw.c driver code
      ...

commit e23604edac2a7be6a8808a5d13fac6b9df4eb9a8
Merge: d4e796152a04 1f25184656a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 19:44:38 2016 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ updates from Ingo Molnar:
     "NOHZ enhancements, by Frederic Weisbecker, which reorganizes/refactors
      the NOHZ 'can the tick be stopped?' infrastructure and related code to
      be data driven, and harmonizes the naming and handling of all the
      various properties"
    
    [ This makes the ugly "fetch_or()" macro that the scheduler used
      internally a new generic helper, and does a bad job at it.
    
      I'm pulling it, but I've asked Ingo and Frederic to get this
      fixed up ]
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched-clock: Migrate to use new tick dependency mask model
      posix-cpu-timers: Migrate to use new tick dependency mask model
      sched: Migrate sched to use new tick dependency mask model
      sched: Account rr tasks
      perf: Migrate perf to use new tick dependency mask model
      nohz: Use enum code for tick stop failure tracing message
      nohz: New tick dependency mask
      nohz: Implement wide kick on top of irq work
      atomic: Export fetch_or()

commit 810813c47a564416f6306ae214e2661366c987a7
Merge: d66ab5144221 e2857b8f11a2
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Mar 8 12:34:12 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several cases of overlapping changes, as well as one instance
    (vxlan) of a bug fix in 'net' overlapping with code movement
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1f25184656a00a59e3a953189070d42a749f6aee
Merge: e2857b8f11a2 4f49b90abb4a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 8 13:17:54 2016 +0100

    Merge branch 'timers/core-v9' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/nohz
    
    Pull nohz enhancements from Frederic Weisbecker:
    
    "Currently in nohz full configs, the tick dependency is checked
     asynchronously by nohz code from interrupt and context switch for each
     concerned subsystem with a set of function provided by these. Such
     functions are made of many conditions and details that can be heavyweight
     as they are called on fastpath: sched_can_stop_tick(),
     posix_cpu_timer_can_stop_tick(), perf_event_can_stop_tick()...
    
     Thomas suggested a few months ago to make that tick dependency check
     synchronous. Instead of checking subsystems details from each interrupt
     to guess if the tick can be stopped, every subsystem that may have a tick
     dependency should set itself a flag specifying the state of that
     dependency. This way we can verify if we can stop the tick with a single
     lightweight mask check on fast path.
    
     This conversion from a pull to a push model to implement tick dependency
     is the core feature of this patchset that is split into:
    
      * Nohz wide kick simplification
      * Improve nohz tracing
      * Introduce tick dependency mask
      * Migrate scheduler, posix timers, perf events and sched clock tick
        dependencies to the tick dependency mask."
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 555e0c1ef7ff49ee5ac3a1eb12de4a2e4722f63d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 16 17:42:29 2015 +0200

    perf: Migrate perf to use new tick dependency mask model
    
    Instead of providing asynchronous checks for the nohz subsystem to verify
    perf event tick dependency, migrate perf to the new mask.
    
    Perf needs the tick for two situations:
    
    1) Freq events. We could set the tick dependency when those are
    installed on a CPU context. But setting a global dependency on top of
    the global freq events accounting is much easier. If people want that
    to be optimized, we can still refine that on the per-CPU tick dependency
    level. This patch dooesn't change the current behaviour anyway.
    
    2) Throttled events: this is a per-cpu dependency.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b35a61a481fa..d3ff88c13632 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1108,12 +1108,6 @@ static inline void perf_event_task_tick(void)				{ }
 static inline int perf_event_release_kernel(struct perf_event *event)	{ return 0; }
 #endif
 
-#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_NO_HZ_FULL)
-extern bool perf_event_can_stop_tick(void);
-#else
-static inline bool perf_event_can_stop_tick(void)			{ return true; }
-#endif
-
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)
 extern void perf_restore_debug_store(void);
 #else

commit 54d751d4ad357c817907fe89db3222b97ff66db3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:14 2016 +0000

    perf: Allow storage of PMU private data in event
    
    For PMUs which are not per CPU, but e.g. per package/socket, we want to be
    able to store a reference to the underlying per package/socket facility in the
    event at init time so we can avoid magic storage constructs in the PMU driver.
    
    This allows us to get rid of the per CPU dance in the intel uncore and RAPL
    drivers and avoids a lookup of the per package data in the perf hotpath.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221011.364140369@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f5c5a3fa2c81..a9d8cab18b00 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -468,6 +468,7 @@ struct perf_event {
 	int				group_flags;
 	struct perf_event		*group_leader;
 	struct pmu			*pmu;
+	void				*pmu_private;
 
 	enum perf_event_active_state	state;
 	unsigned int			attach_state;

commit 9107c89e269d2738019861bb518e3d59bef01781
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 24 18:45:45 2016 +0100

    perf: Fix race between event install and jump_labels
    
    perf_install_in_context() relies upon the context switch hooks to have
    scheduled in events when the IPI misses its target -- after all, if
    the task has moved from the CPU (or wasn't running at all), it will
    have to context switch to run elsewhere.
    
    This however doesn't appear to be happening.
    
    It is possible for the IPI to not happen (task wasn't running) only to
    later observe the task running with an inactive context.
    
    The only possible explanation is that the context switch hooks are not
    called. Therefore put in a sync_sched() after toggling the jump_label
    to guarantee all CPUs will have them enabled before we install an
    event.
    
    A simple if (0->1) sync_sched() will not in fact work, because any
    further increment can race and complete before the sync_sched().
    Therefore we must jump through some hoops.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dvyukov@google.com
    Cc: eranian@google.com
    Cc: oleg@redhat.com
    Cc: panand@redhat.com
    Cc: sasha.levin@oracle.com
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20160224174947.980211985@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 39156619e108..f5c5a3fa2c81 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -906,7 +906,7 @@ perf_sw_event_sched(u32 event_id, u64 nr, u64 addr)
 	}
 }
 
-extern struct static_key_deferred perf_sched_events;
+extern struct static_key_false perf_sched_events;
 
 static __always_inline bool
 perf_sw_migrate_enabled(void)
@@ -925,7 +925,7 @@ static inline void perf_event_task_migrate(struct task_struct *task)
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
-	if (static_key_false(&perf_sched_events.key))
+	if (static_branch_unlikely(&perf_sched_events))
 		__perf_event_task_sched_in(prev, task);
 
 	if (perf_sw_migrate_enabled() && task->sched_migrated) {
@@ -942,7 +942,7 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 {
 	perf_sw_event_sched(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 0);
 
-	if (static_key_false(&perf_sched_events.key))
+	if (static_branch_unlikely(&perf_sched_events))
 		__perf_event_task_sched_out(prev, next);
 }
 

commit a69b0ca4ac3bf5427b571f11cbf33f0a32b728d5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 24 18:45:44 2016 +0100

    perf: Fix cloning
    
    Alexander reported that when the 'original' context gets destroyed, no
    new clones happen.
    
    This can happen irrespective of the ctx switch optimization, any task
    can die, even the parent, and we want to continue monitoring the task
    hierarchy until we either close the event or no tasks are left in the
    hierarchy.
    
    perf_event_init_context() will attempt to pin the 'parent' context
    during clone(). At that point current is the parent, and since current
    cannot have exited while executing clone(), its context cannot have
    passed through perf_event_exit_task_context(). Therefore
    perf_pin_task_context() cannot observe ctx->task == TASK_TOMBSTONE.
    
    However, since inherit_event() does:
    
            if (parent_event->parent)
                    parent_event = parent_event->parent;
    
    it looks at the 'original' event when it does: is_orphaned_event().
    This can return true if the context that contains the this event has
    passed through perf_event_exit_task_context(). And thus we'll fail to
    clone the perf context.
    
    Fix this by adding a new state: STATE_DEAD, which is set by
    perf_release() to indicate that the filedesc (or kernel reference) is
    dead and there are no observers for our data left.
    
    Only for STATE_DEAD will is_orphaned_event() be true and inhibit
    cloning.
    
    STATE_EXIT is otherwise preserved such that is_event_hup() remains
    functional and will report when the observed task hierarchy becomes
    empty.
    
    Reported-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Tested-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dvyukov@google.com
    Cc: eranian@google.com
    Cc: oleg@redhat.com
    Cc: panand@redhat.com
    Cc: sasha.levin@oracle.com
    Cc: vince@deater.net
    Fixes: c6e5b73242d2 ("perf: Synchronously clean up child events")
    Link: http://lkml.kernel.org/r/20160224174947.919845295@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b35a61a481fa..39156619e108 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -397,6 +397,7 @@ struct pmu {
  * enum perf_event_active_state - the states of a event
  */
 enum perf_event_active_state {
+	PERF_EVENT_STATE_DEAD		= -4,
 	PERF_EVENT_STATE_EXIT		= -3,
 	PERF_EVENT_STATE_ERROR		= -2,
 	PERF_EVENT_STATE_OFF		= -1,

commit 568b329a02f75ed3aaae5eb2cca384cb9e09cb29
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Feb 17 19:58:57 2016 -0800

    perf: generalize perf_callchain
    
    . avoid walking the stack when there is no room left in the buffer
    . generalize get_perf_callchain() to be called from bpf helper
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b35a61a481fa..7da3c25999df 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -964,11 +964,20 @@ DECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);
 
 extern void perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs);
 extern void perf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs);
+extern struct perf_callchain_entry *
+get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
+		   bool crosstask, bool add_mark);
+extern int get_callchain_buffers(void);
+extern void put_callchain_buffers(void);
 
-static inline void perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
+static inline int perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
 {
-	if (entry->nr < PERF_MAX_STACK_DEPTH)
+	if (entry->nr < PERF_MAX_STACK_DEPTH) {
 		entry->ip[entry->nr++] = ip;
+		return 0;
+	} else {
+		return -1; /* no more room, stop walking the stack */
+	}
 }
 
 extern int sysctl_perf_event_paranoid;

commit c6e5b73242d2d9172ea880483bc4ba7ffca0cfb2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 15 16:07:41 2016 +0200

    perf: Synchronously clean up child events
    
    The orphan cleanup workqueue doesn't always catch orphans, for example,
    if they never schedule after they are orphaned. IOW, the event leak is
    still very real. It also wouldn't work for kernel counters.
    
    Doing it synchonously is a little hairy due to lock inversion issues,
    but is made to work.
    
    Patch based on work by Alexander Shishkin.
    
    Suggested-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: vince@deater.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4f90434b8d64..b35a61a481fa 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -634,9 +634,6 @@ struct perf_event_context {
 	int				nr_cgroups;	 /* cgroup evts */
 	void				*task_ctx_data; /* pmu specific data */
 	struct rcu_head			rcu_head;
-
-	struct delayed_work		orphans_remove;
-	bool				orphans_remove_sched;
 };
 
 /*

commit e03e7ee34fdd1c3ef494949a75cb8c61c7265fa9
Author: Alexei Starovoitov <alexei.starovoitov@gmail.com>
Date:   Mon Jan 25 20:59:49 2016 -0800

    perf/bpf: Convert perf_event_array to use struct file
    
    Robustify refcounting.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20160126045947.GA40151@ast-mbp.thefacebook.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6612732d8fd0..4f90434b8d64 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -729,7 +729,7 @@ extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
 extern void perf_event_delayed_put(struct task_struct *task);
-extern struct perf_event *perf_event_get(unsigned int fd);
+extern struct file *perf_event_get(unsigned int fd);
 extern const struct perf_event_attr *perf_event_attrs(struct perf_event *event);
 extern void perf_event_print_debug(void);
 extern void perf_pmu_disable(struct pmu *pmu);
@@ -1070,7 +1070,7 @@ static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
 static inline void perf_event_delayed_put(struct task_struct *task)	{ }
-static inline struct perf_event *perf_event_get(unsigned int fd)	{ return ERR_PTR(-EINVAL); }
+static inline struct file *perf_event_get(unsigned int fd)	{ return ERR_PTR(-EINVAL); }
 static inline const struct perf_event_attr *perf_event_attrs(struct perf_event *event)
 {
 	return ERR_PTR(-EINVAL);

commit fae3fde65138b6071b1b0e0b567d4058a8b6a88c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 11 15:00:50 2016 +0100

    perf: Collapse and fix event_function_call() users
    
    There is one common bug left in all the event_function_call() users,
    between loading ctx->task and getting to the remote_function(),
    ctx->task can already have been changed.
    
    Therefore we need to double check and retry if ctx->task != current.
    
    Insert another trampoline specific to event_function_call() that
    checks for this and further validates state. This also allows getting
    rid of the active/inactive functions.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f9828a48f16a..6612732d8fd0 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1044,7 +1044,7 @@ extern void perf_swevent_put_recursion_context(int rctx);
 extern u64 perf_swevent_set_period(struct perf_event *event);
 extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
-extern int __perf_event_disable(void *info);
+extern void perf_event_disable_local(struct perf_event *event);
 extern void perf_event_task_tick(void);
 #else /* !CONFIG_PERF_EVENTS: */
 static inline void *

commit 614e4c4ebc75517295bccd29b20ddbc5b52af6fc
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Nov 12 11:00:04 2015 +0100

    perf/core: Robustify the perf_cgroup_from_task() RCU checks
    
    This patch reinforces the lockdep checks performed by
    perf_cgroup_from_tsk() by passing the perf_event_context
    whenever possible. It is okay to not hold the RCU read lock
    when we know we hold the ctx->lock. This patch makes sure this
    property holds.
    
    In some functions, such as perf_cgroup_sched_in(), we do not
    pass the context because we are sure we are holding the RCU
    read lock.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: edumazet@google.com
    Link: http://lkml.kernel.org/r/1447322404-10920-3-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d841d33bcdc9..f9828a48f16a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -697,9 +697,11 @@ struct perf_cgroup {
  * if there is no cgroup event for the current CPU context.
  */
 static inline struct perf_cgroup *
-perf_cgroup_from_task(struct task_struct *task)
+perf_cgroup_from_task(struct task_struct *task, struct perf_event_context *ctx)
 {
-	return container_of(task_css(task, perf_event_cgrp_id),
+	return container_of(task_css_check(task, perf_event_cgrp_id,
+					   ctx ? lockdep_is_held(&ctx->lock)
+					       : true),
 			    struct perf_cgroup, css);
 }
 #endif /* CONFIG_CGROUP_PERF */

commit 8f3e5684d3fbd91ead283916676fa3dac22615e5
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Sep 3 20:07:53 2015 -0700

    perf/core: Drop PERF_EVENT_TXN
    
    We currently use PERF_EVENT_TXN flag to determine if we are in the middle
    of a transaction. If in a transaction, we defer the schedulability checks
    from pmu->add() operation to the pmu->commit() operation.
    
    Now that we have "transaction types" (PERF_PMU_TXN_ADD, PERF_PMU_TXN_READ)
    we can use the type to determine if we are in a transaction and drop the
    PERF_EVENT_TXN flag.
    
    When PERF_EVENT_TXN is dropped, the cpuhw->group_flag on some architectures
    becomes unused, so drop that field as well.
    
    This is an extension of the Powerpc patch from Peter Zijlstra to s390,
    Sparc and x86 architectures.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1441336073-22750-11-git-send-email-sukadev@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b83cea932f74..d841d33bcdc9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -199,8 +199,6 @@ struct perf_event;
 /*
  * Common implementation detail of pmu::{start,commit,cancel}_txn
  */
-#define PERF_EVENT_TXN 0x1
-
 #define PERF_PMU_TXN_ADD  0x1		/* txn to add/schedule event on PMU */
 #define PERF_PMU_TXN_READ 0x2		/* txn to read event group from PMU */
 

commit 4a00c16e552ea5e71756cd29cd2df7557ec9cac4
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Sep 3 20:07:51 2015 -0700

    perf/core: Define PERF_PMU_TXN_READ interface
    
    Define a new PERF_PMU_TXN_READ interface to read a group of counters
    at once.
    
            pmu->start_txn()                // Initialize before first event
    
            for each event in group
                    pmu->read(event);       // Queue each event to be read
    
            rc = pmu->commit_txn()          // Read/update all queued counters
    
    Note that we use this interface with all PMUs.  PMUs that implement this
    interface use the ->read() operation to _queue_ the counters to be read
    and use ->commit_txn() to actually read all the queued counters at once.
    
    PMUs that don't implement PERF_PMU_TXN_READ ignore ->start_txn() and
    ->commit_txn() and continue to read counters one at a time.
    
    Thanks to input from Peter Zijlstra.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1441336073-22750-9-git-send-email-sukadev@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ea3b5dd21a4c..b83cea932f74 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -202,6 +202,7 @@ struct perf_event;
 #define PERF_EVENT_TXN 0x1
 
 #define PERF_PMU_TXN_ADD  0x1		/* txn to add/schedule event on PMU */
+#define PERF_PMU_TXN_READ 0x2		/* txn to read event group from PMU */
 
 /**
  * pmu::capabilities flags

commit fbbe07011581990ef74dfac06dc8511b1a14badb
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Sep 3 20:07:45 2015 -0700

    perf/core: Add a 'flags' parameter to the PMU transactional interfaces
    
    Currently, the PMU interface allows reading only one counter at a time.
    But some PMUs like the 24x7 counters in Power, support reading several
    counters at once. To leveage this functionality, extend the transaction
    interface to support a "transaction type".
    
    The first type, PERF_PMU_TXN_ADD, refers to the existing transactions,
    i.e. used to _schedule_ all the events on the PMU as a group. A second
    transaction type, PERF_PMU_TXN_READ, will be used in a follow-on patch,
    by the 24x7 counters to read several counters at once.
    
    Extend the transaction interfaces to the PMU to accept a 'txn_flags'
    parameter and use this parameter to ignore any transactions that are
    not of type PERF_PMU_TXN_ADD.
    
    Thanks to Peter Zijlstra for his input.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    [peterz: s390 compile fix]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1441336073-22750-3-git-send-email-sukadev@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d61ee4459eee..ea3b5dd21a4c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -201,6 +201,8 @@ struct perf_event;
  */
 #define PERF_EVENT_TXN 0x1
 
+#define PERF_PMU_TXN_ADD  0x1		/* txn to add/schedule event on PMU */
+
 /**
  * pmu::capabilities flags
  */
@@ -331,20 +333,26 @@ struct pmu {
 	 *
 	 * Start the transaction, after this ->add() doesn't need to
 	 * do schedulability tests.
+	 *
+	 * Optional.
 	 */
-	void (*start_txn)		(struct pmu *pmu); /* optional */
+	void (*start_txn)		(struct pmu *pmu, unsigned int txn_flags);
 	/*
 	 * If ->start_txn() disabled the ->add() schedulability test
 	 * then ->commit_txn() is required to perform one. On success
 	 * the transaction is closed. On error the transaction is kept
 	 * open until ->cancel_txn() is called.
+	 *
+	 * Optional.
 	 */
-	int  (*commit_txn)		(struct pmu *pmu); /* optional */
+	int  (*commit_txn)		(struct pmu *pmu);
 	/*
 	 * Will cancel the transaction, assumes ->del() is called
 	 * for each successful ->add() during the transaction.
+	 *
+	 * Optional.
 	 */
-	void (*cancel_txn)		(struct pmu *pmu); /* optional */
+	void (*cancel_txn)		(struct pmu *pmu);
 
 	/*
 	 * Will return the value for perf_event_mmap_page::index for this event,

commit b0e878759452314676fbdd71df4ac67e7d08de5d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Aug 28 14:06:07 2015 +0200

    perf/abi: Document some more aspects of the perf ABI
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 092a0e8a479a..d61ee4459eee 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -140,27 +140,60 @@ struct hw_perf_event {
 		};
 #endif
 	};
+	/*
+	 * If the event is a per task event, this will point to the task in
+	 * question. See the comment in perf_event_alloc().
+	 */
 	struct task_struct		*target;
+
+/*
+ * hw_perf_event::state flags; used to track the PERF_EF_* state.
+ */
+#define PERF_HES_STOPPED	0x01 /* the counter is stopped */
+#define PERF_HES_UPTODATE	0x02 /* event->count up-to-date */
+#define PERF_HES_ARCH		0x04
+
 	int				state;
+
+	/*
+	 * The last observed hardware counter value, updated with a
+	 * local64_cmpxchg() such that pmu::read() can be called nested.
+	 */
 	local64_t			prev_count;
+
+	/*
+	 * The period to start the next sample with.
+	 */
 	u64				sample_period;
+
+	/*
+	 * The period we started this sample with.
+	 */
 	u64				last_period;
+
+	/*
+	 * However much is left of the current period; note that this is
+	 * a full 64bit value and allows for generation of periods longer
+	 * than hardware might allow.
+	 */
 	local64_t			period_left;
+
+	/*
+	 * State for throttling the event, see __perf_event_overflow() and
+	 * perf_adjust_freq_unthr_context().
+	 */
 	u64                             interrupts_seq;
 	u64				interrupts;
 
+	/*
+	 * State for freq target events, see __perf_event_overflow() and
+	 * perf_adjust_freq_unthr_context().
+	 */
 	u64				freq_time_stamp;
 	u64				freq_count_stamp;
 #endif
 };
 
-/*
- * hw_perf_event::state flags
- */
-#define PERF_HES_STOPPED	0x01 /* the counter is stopped */
-#define PERF_HES_UPTODATE	0x02 /* event->count up-to-date */
-#define PERF_HES_ARCH		0x04
-
 struct perf_event;
 
 /*
@@ -210,7 +243,19 @@ struct pmu {
 
 	/*
 	 * Try and initialize the event for this PMU.
-	 * Should return -ENOENT when the @event doesn't match this PMU.
+	 *
+	 * Returns:
+	 *  -ENOENT	-- @event is not for this PMU
+	 *
+	 *  -ENODEV	-- @event is for this PMU but PMU not present
+	 *  -EBUSY	-- @event is for this PMU but PMU temporarily unavailable
+	 *  -EINVAL	-- @event is for this PMU but @event is not valid
+	 *  -EOPNOTSUPP -- @event is for this PMU, @event is valid, but not supported
+	 *  -EACCESS	-- @event is for this PMU, @event is valid, but no privilidges
+	 *
+	 *  0		-- @event is for this PMU and valid
+	 *
+	 * Other error return values are allowed.
 	 */
 	int (*event_init)		(struct perf_event *event);
 
@@ -221,27 +266,61 @@ struct pmu {
 	void (*event_mapped)		(struct perf_event *event); /*optional*/
 	void (*event_unmapped)		(struct perf_event *event); /*optional*/
 
+	/*
+	 * Flags for ->add()/->del()/ ->start()/->stop(). There are
+	 * matching hw_perf_event::state flags.
+	 */
 #define PERF_EF_START	0x01		/* start the counter when adding    */
 #define PERF_EF_RELOAD	0x02		/* reload the counter when starting */
 #define PERF_EF_UPDATE	0x04		/* update the counter when stopping */
 
 	/*
-	 * Adds/Removes a counter to/from the PMU, can be done inside
-	 * a transaction, see the ->*_txn() methods.
+	 * Adds/Removes a counter to/from the PMU, can be done inside a
+	 * transaction, see the ->*_txn() methods.
+	 *
+	 * The add/del callbacks will reserve all hardware resources required
+	 * to service the event, this includes any counter constraint
+	 * scheduling etc.
+	 *
+	 * Called with IRQs disabled and the PMU disabled on the CPU the event
+	 * is on.
+	 *
+	 * ->add() called without PERF_EF_START should result in the same state
+	 *  as ->add() followed by ->stop().
+	 *
+	 * ->del() must always PERF_EF_UPDATE stop an event. If it calls
+	 *  ->stop() that must deal with already being stopped without
+	 *  PERF_EF_UPDATE.
 	 */
 	int  (*add)			(struct perf_event *event, int flags);
 	void (*del)			(struct perf_event *event, int flags);
 
 	/*
-	 * Starts/Stops a counter present on the PMU. The PMI handler
-	 * should stop the counter when perf_event_overflow() returns
-	 * !0. ->start() will be used to continue.
+	 * Starts/Stops a counter present on the PMU.
+	 *
+	 * The PMI handler should stop the counter when perf_event_overflow()
+	 * returns !0. ->start() will be used to continue.
+	 *
+	 * Also used to change the sample period.
+	 *
+	 * Called with IRQs disabled and the PMU disabled on the CPU the event
+	 * is on -- will be called from NMI context with the PMU generates
+	 * NMIs.
+	 *
+	 * ->stop() with PERF_EF_UPDATE will read the counter and update
+	 *  period/count values like ->read() would.
+	 *
+	 * ->start() with PERF_EF_RELOAD will reprogram the the counter
+	 *  value, must be preceded by a ->stop() with PERF_EF_UPDATE.
 	 */
 	void (*start)			(struct perf_event *event, int flags);
 	void (*stop)			(struct perf_event *event, int flags);
 
 	/*
 	 * Updates the counter value of the event.
+	 *
+	 * For sampling capable PMUs this will also update the software period
+	 * hw_perf_event::period_left field.
 	 */
 	void (*read)			(struct perf_event *event);
 

commit ffe8690c85b8426db7783064724d106702f1b1e8
Author: Kaixu Xia <xiakaixu@huawei.com>
Date:   Thu Aug 6 07:02:32 2015 +0000

    perf: add the necessary core perf APIs when accessing events counters in eBPF programs
    
    This patch add three core perf APIs:
     - perf_event_attrs(): export the struct perf_event_attr from struct
       perf_event;
     - perf_event_get(): get the struct perf_event from the given fd;
     - perf_event_read_local(): read the events counters active on the
       current CPU;
    These APIs are needed when accessing events counters in eBPF programs.
    
    The API perf_event_read_local() comes from Peter and I add the
    corresponding SOB.
    
    Signed-off-by: Kaixu Xia <xiakaixu@huawei.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2027809433b3..092a0e8a479a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -641,6 +641,8 @@ extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
 extern void perf_event_delayed_put(struct task_struct *task);
+extern struct perf_event *perf_event_get(unsigned int fd);
+extern const struct perf_event_attr *perf_event_attrs(struct perf_event *event);
 extern void perf_event_print_debug(void);
 extern void perf_pmu_disable(struct pmu *pmu);
 extern void perf_pmu_enable(struct pmu *pmu);
@@ -659,6 +661,7 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				void *context);
 extern void perf_pmu_migrate_context(struct pmu *pmu,
 				int src_cpu, int dst_cpu);
+extern u64 perf_event_read_local(struct perf_event *event);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 
@@ -979,6 +982,12 @@ static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
 static inline void perf_event_delayed_put(struct task_struct *task)	{ }
+static inline struct perf_event *perf_event_get(unsigned int fd)	{ return ERR_PTR(-EINVAL); }
+static inline const struct perf_event_attr *perf_event_attrs(struct perf_event *event)
+{
+	return ERR_PTR(-EINVAL);
+}
+static inline u64 perf_event_read_local(struct perf_event *event)	{ return -EINVAL; }
 static inline void perf_event_print_debug(void)				{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }
@@ -1011,6 +1020,7 @@ static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
+static inline int perf_event_release_kernel(struct perf_event *event)	{ return 0; }
 #endif
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_NO_HZ_FULL)

commit e382608254e06c8109f40044f5e693f2e04f3899
Merge: fcbc1777ce8b b44754d8262d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 14:02:43 2015 -0700

    Merge tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This patch series contains several clean ups and even a new trace
      clock "monitonic raw".  Also some enhancements to make the ring buffer
      even faster.  But the biggest and most noticeable change is the
      renaming of the ftrace* files, structures and variables that have to
      deal with trace events.
    
      Over the years I've had several developers tell me about their
      confusion with what ftrace is compared to events.  Technically,
      "ftrace" is the infrastructure to do the function hooks, which include
      tracing and also helps with live kernel patching.  But the trace
      events are a separate entity altogether, and the files that affect the
      trace events should not be named "ftrace".  These include:
    
        include/trace/ftrace.h         ->    include/trace/trace_events.h
        include/linux/ftrace_event.h   ->    include/linux/trace_events.h
    
      Also, functions that are specific for trace events have also been renamed:
    
        ftrace_print_*()               ->    trace_print_*()
        (un)register_ftrace_event()    ->    (un)register_trace_event()
        ftrace_event_name()            ->    trace_event_name()
        ftrace_trigger_soft_disabled() ->    trace_trigger_soft_disabled()
        ftrace_define_fields_##call()  ->    trace_define_fields_##call()
        ftrace_get_offsets_##call()    ->    trace_get_offsets_##call()
    
      Structures have been renamed:
    
        ftrace_event_file              ->    trace_event_file
        ftrace_event_{call,class}      ->    trace_event_{call,class}
        ftrace_event_buffer            ->    trace_event_buffer
        ftrace_subsystem_dir           ->    trace_subsystem_dir
        ftrace_event_raw_##call        ->    trace_event_raw_##call
        ftrace_event_data_offset_##call->    trace_event_data_offset_##call
        ftrace_event_type_funcs_##call ->    trace_event_type_funcs_##call
    
      And a few various variables and flags have also been updated.
    
      This has been sitting in linux-next for some time, and I have not
      heard a single complaint about this rename breaking anything.  Mostly
      because these functions, variables and structures are mostly internal
      to the tracing system and are seldom (if ever) used by anything
      external to that"
    
    * tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (33 commits)
      ring_buffer: Allow to exit the ring buffer benchmark immediately
      ring-buffer-benchmark: Fix the wrong type
      ring-buffer-benchmark: Fix the wrong param in module_param
      ring-buffer: Add enum names for the context levels
      ring-buffer: Remove useless unused tracing_off_permanent()
      ring-buffer: Give NMIs a chance to lock the reader_lock
      ring-buffer: Add trace_recursive checks to ring_buffer_write()
      ring-buffer: Allways do the trace_recursive checks
      ring-buffer: Move recursive check to per_cpu descriptor
      ring-buffer: Add unlikelys to make fast path the default
      tracing: Rename ftrace_get_offsets_##call() to trace_event_get_offsets_##call()
      tracing: Rename ftrace_define_fields_##call() to trace_event_define_fields_##call()
      tracing: Rename ftrace_event_type_funcs_##call to trace_event_type_funcs_##call
      tracing: Rename ftrace_data_offset_##call to trace_event_data_offset_##call
      tracing: Rename ftrace_raw_##call event structures to trace_event_raw_##call
      tracing: Rename ftrace_trigger_soft_disabled() to trace_trigger_soft_disabled()
      tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
      tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
      tracing: Rename ftrace_event_name() to trace_event_name()
      tracing: Rename FTRACE_MAX_EVENT to TRACE_EVENT_TYPE_MAX
      ...

commit e8a0b37d28ace440776c0a4fe3c65f5832a9a7ee
Merge: abea9629486c 002af195a8c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 12:20:00 2015 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Bigger items included in this update are:
    
       - A series of updates from Arnd for ARM randconfig build failures
       - Updates from Dmitry for StrongARM SA-1100 to move IRQ handling to
         drivers/irqchip/
       - Move ARMs SP804 timer to drivers/clocksource/
       - Perf updates from Mark Rutland in preparation to move the ARM perf
         code into drivers/ so it can be shared with ARM64.
       - MCPM updates from Nicolas
       - Add support for taking platform serial number from DT
       - Re-implement Keystone2 physical address space switch to conform to
         architecture requirements
       - Clean up ARMv7 LPAE code, which goes in hand with the Keystone2
         changes.
       - L2C cleanups to avoid unlocking caches if we're prevented by the
         secure support to unlock.
       - Avoid cleaning a potentially dirty cache containing stale data on
         CPU initialisation
       - Add ARM-only entry point for secondary startup (for machines that
         can only call into a Thumb kernel in ARM mode).  Same thing is also
         done for the resume entry point.
       - Provide arch_irqs_disabled via asm-generic
       - Enlarge ARMv7M vector table
       - Always use BFD linker for VDSO, as gold doesn't accept some of the
         options we need.
       - Fix an incorrect BSYM (for Thumb symbols) usage, and convert all
         BSYM compiler macros to a "badr" (for branch address).
       - Shut up compiler warnings provoked by our cmpxchg() implementation.
       - Ensure bad xchg sizes fail to link"
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (75 commits)
      ARM: Fix build if CLKDEV_LOOKUP is not configured
      ARM: fix new BSYM() usage introduced via for-arm-soc branch
      ARM: 8383/1: nommu: avoid deprecated source register on mov
      ARM: 8391/1: l2c: add options to overwrite prefetching behavior
      ARM: 8390/1: irqflags: Get arch_irqs_disabled from asm-generic
      ARM: 8387/1: arm/mm/dma-mapping.c: Add arm_coherent_dma_mmap
      ARM: 8388/1: tcm: Don't crash when TCM banks are protected by TrustZone
      ARM: 8384/1: VDSO: force use of BFD linker
      ARM: 8385/1: VDSO: group link options
      ARM: cmpxchg: avoid warnings from macro-ized cmpxchg() implementations
      ARM: remove __bad_xchg definition
      ARM: 8369/1: ARMv7M: define size of vector table for Vybrid
      ARM: 8382/1: clocksource: make ARM_TIMER_SP804 depend on GENERIC_SCHED_CLOCK
      ARM: 8366/1: move Dual-Timer SP804 driver to drivers/clocksource
      ARM: 8365/1: introduce sp804_timer_disable and remove arm_timer.h inclusion
      ARM: 8364/1: fix BE32 module loading
      ARM: 8360/1: add secondary_startup_arm prototype in header file
      ARM: 8359/1: correct secondary_startup_arm mode
      ARM: proc-v7: sanitise and document registers around errata
      ARM: proc-v7: clean up MIDR access
      ...

commit 43224b96af3154cedd7220f7b90094905f07ac78
Merge: d70b3ef54cea 1cb6c2151850
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 18:57:44 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "A rather largish update for everything time and timer related:
    
       - Cache footprint optimizations for both hrtimers and timer wheel
    
       - Lower the NOHZ impact on systems which have NOHZ or timer migration
         disabled at runtime.
    
       - Optimize run time overhead of hrtimer interrupt by making the clock
         offset updates smarter
    
       - hrtimer cleanups and removal of restrictions to tackle some
         problems in sched/perf
    
       - Some more leap second tweaks
    
       - Another round of changes addressing the 2038 problem
    
       - First step to change the internals of clock event devices by
         introducing the necessary infrastructure
    
       - Allow constant folding for usecs/msecs_to_jiffies()
    
       - The usual pile of clockevent/clocksource driver updates
    
      The hrtimer changes contain updates to sched, perf and x86 as they
      depend on them plus changes all over the tree to cleanup API changes
      and redundant code, which got copied all over the place.  The y2038
      changes touch s390 to remove the last non 2038 safe code related to
      boot/persistant clock"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      clocksource: Increase dependencies of timer-stm32 to limit build wreckage
      timer: Minimize nohz off overhead
      timer: Reduce timer migration overhead if disabled
      timer: Stats: Simplify the flags handling
      timer: Replace timer base by a cpu index
      timer: Use hlist for the timer wheel hash buckets
      timer: Remove FIFO "guarantee"
      timers: Sanitize catchup_timer_jiffies() usage
      hrtimer: Allow hrtimer::function() to free the timer
      seqcount: Introduce raw_write_seqcount_barrier()
      seqcount: Rename write_seqcount_barrier()
      hrtimer: Fix hrtimer_is_queued() hole
      hrtimer: Remove HRTIMER_STATE_MIGRATE
      selftest: Timers: Avoid signal deadlock in leap-a-day
      timekeeping: Copy the shadow-timekeeper over the real timekeeper last
      clockevents: Check state instead of mode in suspend/resume path
      selftests: timers: Add leap-second timer edge testing to leap-a-day.c
      ntp: Do leapsecond adjustment in adjtimex read path
      time: Prevent early expiry of hrtimers[CLOCK_REALTIME] at the leap second edge
      ntp: Introduce and use SECS_PER_DAY macro instead of 86400
      ...

commit f38b0dbb491a6987e198aa6b428db8692a6480f8
Author: Kan Liang <kan.liang@intel.com>
Date:   Sun May 10 15:13:14 2015 -0400

    perf/x86/intel: Introduce PERF_RECORD_LOST_SAMPLES
    
    After enlarging the PEBS interrupt threshold, there may be some mixed up
    PEBS samples which are discarded by the kernel.
    
    This patch makes the kernel emit a PERF_RECORD_LOST_SAMPLES record with
    the number of possible discarded records when it is impossible to demux
    the samples.
    
    It makes sure the user is not left in the dark about such discards.
    
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285195-14269-8-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5f192e1bc98e..a204d5266f5f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -743,6 +743,9 @@ perf_event__output_id_sample(struct perf_event *event,
 			     struct perf_output_handle *handle,
 			     struct perf_sample_data *sample);
 
+extern void
+perf_log_lost_samples(struct perf_event *event, u64 lost);
+
 static inline bool is_sampling_event(struct perf_event *event)
 {
 	return event->attr.sample_period != 0;

commit 21509084f999d7accd32e45961ef76853112e978
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Wed May 6 15:33:49 2015 -0400

    perf/x86/intel: Handle multiple records in the PEBS buffer
    
    When the PEBS interrupt threshold is larger than one record and the
    machine supports multiple PEBS events, the records of these events are
    mixed up and we need to demultiplex them.
    
    Demuxing the records is hard because the hardware is deficient. The
    hardware has two issues that, when combined, create impossible
    scenarios to demux.
    
    The first issue is that the 'status' field of the PEBS record is a copy
    of the GLOBAL_STATUS MSR at PEBS assist time. To see why this is a
    problem let us first describe the regular PEBS cycle:
    
    A) the CTRn value reaches 0:
      - the corresponding bit in GLOBAL_STATUS gets set
      - we start arming the hardware assist
      < some unspecified amount of time later -- this could cover multiple
        events of interest >
    
    B) the hardware assist is armed, any next event will trigger it
    
    C) a matching event happens:
      - the hardware assist triggers and generates a PEBS record
        this includes a copy of GLOBAL_STATUS at this moment
      - if we auto-reload we (re)set CTRn
      - we clear the relevant bit in GLOBAL_STATUS
    
    Now consider the following chain of events:
    
      A0, B0, A1, C0
    
    The event generated for counter 0 will include a status with counter 1
    set, even though its not at all related to the record. A similar thing
    can happen with a !PEBS event if it just happens to overflow at the
    right moment.
    
    The second issue is that the hardware will only emit one record for two
    or more counters if the event that triggers the assist is 'close'. The
    'close' can be several cycles. In some cases even the complete assist,
    if the event is something that doesn't need retirement.
    
    For instance, consider this chain of events:
    
      A0, B0, A1, B1, C01
    
    Where C01 is an event that triggers both hardware assists, we will
    generate but a single record, but again with both counters listed in the
    status field.
    
    This time the record pertains to both events.
    
    Note that these two cases are different but undistinguishable with the
    data as generated. Therefore demuxing records with multiple PEBS bits
    (we can safely ignore status bits for !PEBS counters) is impossible.
    
    Furthermore we cannot emit the record to both events because that might
    cause a data leak -- the events might not have the same privileges -- so
    what this patch does is discard such events.
    
    The assumption/hope is that such discards will be rare.
    
    Here lists some possible ways you may get high discard rate.
    
      - when you count the same thing multiple times. But it is not a useful
        configuration.
      - you can be unfortunate if you measure with a userspace only PEBS
        event along with either a kernel or unrestricted PEBS event. Imagine
        the event triggering and setting the overflow flag right before
        entering the kernel. Then all kernel side events will end up with
        multiple bits set.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    [ Changelog improvements. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1430940834-8964-4-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 06580028cee6..5f192e1bc98e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -730,6 +730,19 @@ extern int perf_event_overflow(struct perf_event *event,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
+extern void perf_event_output(struct perf_event *event,
+				struct perf_sample_data *data,
+				struct pt_regs *regs);
+
+extern void
+perf_event_header__init_id(struct perf_event_header *header,
+			   struct perf_sample_data *data,
+			   struct perf_event *event);
+extern void
+perf_event__output_id_sample(struct perf_event *event,
+			     struct perf_output_handle *handle,
+			     struct perf_sample_data *sample);
+
 static inline bool is_sampling_event(struct perf_event *event)
 {
 	return event->attr.sample_period != 0;

commit 66eb579e66ecfea55e2007be0594869ea9e453d4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed May 13 17:12:23 2015 +0100

    perf: allow for PMU-specific event filtering
    
    In certain circumstances it may not be possible to schedule particular
    events due to constraints other than a lack of hardware counters (e.g.
    on big.LITTLE systems where CPUs support different events). The core
    perf event code does not distinguish these cases and pessimistically
    assumes that any failure to schedule an event means that it is not worth
    attempting to schedule later events, even if some hardware counters are
    still unused.
    
    When an event a pmu cannot schedule exists in a flexible group list it
    can unnecessarily prevent event groups following it in the list from
    being scheduled (until it is rotated to the end of the list). This means
    some events are scheduled for only a portion of the time they could be,
    and for short running programs no events may be scheduled if the list is
    initially sorted in an unfortunate order.
    
    This patch adds a new (optional) filter_match function pointer to struct
    pmu which a pmu driver can use to tell perf core when an event matches
    pmu-specific scheduling requirements. This plugs into the existing
    event_filter_match logic, and makes it possible to avoid the scheduling
    problem described above. When no filter is provided by the PMU, the
    existing behaviour is retained.
    
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61992cf2e977..67c719cc91aa 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -304,6 +304,11 @@ struct pmu {
 	 * Free pmu-private AUX data structures
 	 */
 	void (*free_aux)		(void *aux); /* optional */
+
+	/*
+	 * Filter events for PMU-specific reasons.
+	 */
+	int (*filter_match)		(struct perf_event *event); /* optional */
 };
 
 /**

commit b3df4ec4424f27e55d754cfe586195fecca1c4e4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 19 00:00:51 2015 +0000

    perf/x86/intel/cqm: Use proper data types
    
    'int' is really not a proper data type for an MSR. Use u32 to make it
    clear that we are dealing with a 32-bit unsigned hardware value.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Matt Fleming <matt.fleming@intel.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Cc: Will Auld <will.auld@intel.com>
    Link: http://lkml.kernel.org/r/20150518235149.919350144@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 248f7829ce41..06580028cee6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -120,7 +120,7 @@ struct hw_perf_event {
 		};
 		struct { /* intel_cqm */
 			int			cqm_state;
-			int			cqm_rmid;
+			u32			cqm_rmid;
 			struct list_head	cqm_events_entry;
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;

commit 8d12ded3dd499e38e8022fe3ec53920d085e57a3
Merge: d499c106843a 68ab747604da
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 09:17:21 2015 +0200

    Merge branch 'perf/urgent' into perf/core, before applying dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b371b594317869971af326adcf7cd65cabdb4087
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 21 10:57:13 2015 +0200

    perf/x86: Fix event/group validation
    
    Commit 43b4578071c0 ("perf/x86: Reduce stack usage of
    x86_schedule_events()") violated the rule that 'fake' scheduling; as
    used for event/group validation; should not change the event state.
    
    This went mostly un-noticed because repeated calls of
    x86_pmu::get_event_constraints() would give the same result. And
    x86_pmu::put_event_constraints() would mostly not do anything.
    
    Commit e979121b1b15 ("perf/x86/intel: Implement cross-HT corruption
    bug workaround") made the situation much worse by actually setting the
    event->hw.constraint value to NULL, so when validation and actual
    scheduling interact we get NULL ptr derefs.
    
    Fix it by removing the constraint pointer from the event and move it
    back to an array, this time in cpuc instead of on the stack.
    
    validate_group()
      x86_schedule_events()
        event->hw.constraint = c; # store
    
          <context switch>
            perf_task_event_sched_in()
              ...
                x86_schedule_events();
                  event->hw.constraint = c2; # store
    
                  ...
    
                  put_event_constraints(event); # assume failure to schedule
                    intel_put_event_constraints()
                      event->hw.constraint = NULL;
    
          <context switch end>
    
        c = event->hw.constraint; # read -> NULL
    
        if (!test_bit(hwc->idx, c->idxmsk)) # <- *BOOM* NULL deref
    
    This in particular is possible when the event in question is a
    cpu-wide event and group-leader, where the validate_group() tries to
    add an event to the group.
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 43b4578071c0 ("perf/x86: Reduce stack usage of x86_schedule_events()")
    Fixes: e979121b1b15 ("perf/x86/intel: Implement cross-HT corruption bug workaround")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61992cf2e977..d8a82a89f35a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -92,8 +92,6 @@ struct hw_perf_event_extra {
 	int		idx;	/* index in shared_regs->regs[] */
 };
 
-struct event_constraint;
-
 /**
  * struct hw_perf_event - performance event hardware details:
  */
@@ -112,8 +110,6 @@ struct hw_perf_event {
 
 			struct hw_perf_event_extra extra_reg;
 			struct hw_perf_event_extra branch_reg;
-
-			struct event_constraint *constraint;
 		};
 		struct { /* software */
 			struct hrtimer	hrtimer;

commit 4cfafd3082afc707653aeb82e9f8e7b596fbbfd6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 14 12:23:11 2015 +0200

    sched,perf: Fix periodic timers
    
    In the below two commits (see Fixes) we have periodic timers that can
    stop themselves when they're no longer required, but need to be
    (re)-started when their idle condition changes.
    
    Further complications is that we want the timer handler to always do
    the forward such that it will always correctly deal with the overruns,
    and we do not want to race such that the handler has already decided
    to stop, but the (external) restart sees the timer still active and we
    end up with a 'lost' timer.
    
    The problem with the current code is that the re-start can come before
    the callback does the forward, at which point the forward from the
    callback will WARN about forwarding an enqueued timer.
    
    Now, conceptually its easy to detect if you're before or after the fwd
    by comparing the expiration time against the current time. Of course,
    that's expensive (and racy) because we don't have the current time.
    
    Alternatively one could cache this state inside the timer, but then
    everybody pays the overhead of maintaining this extra state, and that
    is undesired.
    
    The only other option that I could see is the external timer_active
    variable, which I tried to kill before. I would love a nicer interface
    for this seemingly simple 'problem' but alas.
    
    Fixes: 272325c4821f ("perf: Fix mux_interval hrtimer wreckage")
    Fixes: 77a4d1a1b9a1 ("sched: Cleanup bandwidth timers")
    Cc: pjt@google.com
    Cc: tglx@linutronix.de
    Cc: klamm@yandex-team.ru
    Cc: mingo@kernel.org
    Cc: bsegall@google.com
    Cc: hpa@zytor.com
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20150514102311.GX21418@twins.programming.kicks-ass.net

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61992cf2e977..cf3342a8ad80 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -566,8 +566,12 @@ struct perf_cpu_context {
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
 	int				exclusive;
+
+	raw_spinlock_t			hrtimer_lock;
 	struct hrtimer			hrtimer;
 	ktime_t				hrtimer_interval;
+	unsigned int			hrtimer_active;
+
 	struct pmu			*unique_pmu;
 	struct perf_cgroup		*cgrp;
 };

commit 2425bcb9240f8c97d793cb31c8e8d8d0a843fa29
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 11:45:27 2015 -0400

    tracing: Rename ftrace_event_{call,class} to trace_event_{call,class}
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structures ftrace_event_call and
    ftrace_event_class have nothing to do with the function hooks, and are
    really trace_event structures. Rename ftrace_event_* to trace_event_*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61992cf2e977..d089d6d58ae0 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -483,7 +483,7 @@ struct perf_event {
 	void				*overflow_handler_context;
 
 #ifdef CONFIG_EVENT_TRACING
-	struct ftrace_event_call	*tp_event;
+	struct trace_event_call		*tp_event;
 	struct event_filter		*filter;
 #ifdef CONFIG_FUNCTION_TRACER
 	struct ftrace_ops               ftrace_ops;

commit ff303e66c240ba6269e31817a386995440a18c99
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 17 20:05:30 2015 +0200

    perf: Fix software migrate events
    
    Stephane asked about PERF_COUNT_SW_CPU_MIGRATIONS and I realized it
    was borken:
    
     > The problem is that the task isn't actually scheduled while its being
     > migrated (obviously), and if its not scheduled, the counters aren't
     > scheduled either, so there's no observing of the fact.
     >
     > A further problem with migrations is that many migrations happen from
     > softirq context, which is nested inside the 'random' task context of
     > whoemever happens to run at that time, similarly for the wakeup
     > migrations triggered from (soft)irq context. All those end up being
     > accounted in the task that's currently running, eg. your 'ls'.
    
    The below cures this by marking a task as migrated and accounting it
    on the subsequent sched_in().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 61992cf2e977..e86f85abeda7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -798,11 +798,33 @@ perf_sw_event_sched(u32 event_id, u64 nr, u64 addr)
 
 extern struct static_key_deferred perf_sched_events;
 
+static __always_inline bool
+perf_sw_migrate_enabled(void)
+{
+	if (static_key_false(&perf_swevent_enabled[PERF_COUNT_SW_CPU_MIGRATIONS]))
+		return true;
+	return false;
+}
+
+static inline void perf_event_task_migrate(struct task_struct *task)
+{
+	if (perf_sw_migrate_enabled())
+		task->sched_migrated = 1;
+}
+
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
 	if (static_key_false(&perf_sched_events.key))
 		__perf_event_task_sched_in(prev, task);
+
+	if (perf_sw_migrate_enabled() && task->sched_migrated) {
+		struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
+
+		perf_fetch_caller_regs(regs);
+		___perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, regs, 0);
+		task->sched_migrated = 0;
+	}
 }
 
 static inline void perf_event_task_sched_out(struct task_struct *prev,
@@ -925,6 +947,8 @@ perf_aux_output_skip(struct perf_output_handle *handle,
 static inline void *
 perf_get_aux(struct perf_output_handle *handle)				{ return NULL; }
 static inline void
+perf_event_task_migrate(struct task_struct *task)			{ }
+static inline void
 perf_event_task_sched_in(struct task_struct *prev,
 			 struct task_struct *task)			{ }
 static inline void

commit ec0d7729bbaed4b9d2d3fada693278e13a3d1368
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:23 2015 +0200

    perf: Add ITRACE_START record to indicate that tracing has started
    
    For counters that generate AUX data that is bound to the context of a
    running task, such as instruction tracing, the decoder needs to know
    exactly which task is running when the event is first scheduled in,
    before the first sched_switch. The decoder's need to know this stems
    from the fact that instruction flow trace decoding will almost always
    require program's object code in order to reconstruct said flow and
    for that we need at least its pid/tid in the perf stream.
    
    To single out such instruction tracing pmus, this patch introduces
    ITRACE PMU capability. The reason this is not part of RECORD_AUX
    record is that not all pmus capable of generating AUX data need this,
    and the opposite is *probably* also true.
    
    While sched_switch covers for most cases, there are two problems with it:
    the consumer will need to process events out of order (that is, having
    found RECORD_AUX, it will have to skip forward to the nearest sched_switch
    to figure out which task it was, then go back to the actual trace to
    decode it) and it completely misses the case when the tracing is enabled
    and disabled before sched_switch, for example, via PERF_EVENT_IOC_DISABLE.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-15-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 45c5873ad9b3..61992cf2e977 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -129,6 +129,9 @@ struct hw_perf_event {
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;
 		};
+		struct { /* itrace */
+			int			itrace_started;
+		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */
 			/*
@@ -177,6 +180,7 @@ struct perf_event;
 #define PERF_PMU_CAP_AUX_NO_SG			0x04
 #define PERF_PMU_CAP_AUX_SW_DOUBLEBUF		0x08
 #define PERF_PMU_CAP_EXCLUSIVE			0x10
+#define PERF_PMU_CAP_ITRACE			0x20
 
 /**
  * struct pmu - generic performance monitoring unit

commit fdc2670666f40ab3e03143f04d1ebf4a05e2c24a
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:16 2015 +0200

    perf: Add API for PMUs to write to the AUX area
    
    For pmus that wish to write data to ring buffer's AUX area, provide
    perf_aux_output_{begin,end}() calls to initiate/commit data writes,
    similarly to perf_output_{begin,end}. These also use the same output
    handle structure. Also, similarly to software counterparts, these
    will direct inherited events' output to parents' ring buffers.
    
    After the perf_aux_output_begin() returns successfully, handle->size
    is set to the maximum amount of data that can be written wrt aux_tail
    pointer, so that no data that the user hasn't seen will be overwritten,
    therefore this should always be called before hardware writing is
    enabled. On success, this will return the pointer to pmu driver's
    private structure allocated for this aux area by pmu::setup_aux. Same
    pointer can also be retrieved using perf_get_aux() while hardware
    writing is enabled.
    
    PMU driver should pass the actual amount of data written as a parameter
    to perf_aux_output_end(). All hardware writes should be completed and
    visible before this one is called.
    
    Additionally, perf_aux_output_skip() will adjust output handle and
    aux_head in case some part of the buffer has to be skipped over to
    maintain hardware's alignment constraints.
    
    Nested writers are forbidden and guards are in place to catch such
    attempts.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-8-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f936a1e51f29..45c5873ad9b3 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -573,7 +573,10 @@ struct perf_output_handle {
 	struct ring_buffer		*rb;
 	unsigned long			wakeup;
 	unsigned long			size;
-	void				*addr;
+	union {
+		void			*addr;
+		unsigned long		head;
+	};
 	int				page;
 };
 
@@ -608,6 +611,14 @@ perf_cgroup_from_task(struct task_struct *task)
 
 #ifdef CONFIG_PERF_EVENTS
 
+extern void *perf_aux_output_begin(struct perf_output_handle *handle,
+				   struct perf_event *event);
+extern void perf_aux_output_end(struct perf_output_handle *handle,
+				unsigned long size, bool truncated);
+extern int perf_aux_output_skip(struct perf_output_handle *handle,
+				unsigned long size);
+extern void *perf_get_aux(struct perf_output_handle *handle);
+
 extern int perf_pmu_register(struct pmu *pmu, const char *name, int type);
 extern void perf_pmu_unregister(struct pmu *pmu);
 
@@ -898,6 +909,17 @@ extern void perf_event_disable(struct perf_event *event);
 extern int __perf_event_disable(void *info);
 extern void perf_event_task_tick(void);
 #else /* !CONFIG_PERF_EVENTS: */
+static inline void *
+perf_aux_output_begin(struct perf_output_handle *handle,
+		      struct perf_event *event)				{ return NULL; }
+static inline void
+perf_aux_output_end(struct perf_output_handle *handle, unsigned long size,
+		    bool truncated)					{ }
+static inline int
+perf_aux_output_skip(struct perf_output_handle *handle,
+		     unsigned long size)				{ return -EINVAL; }
+static inline void *
+perf_get_aux(struct perf_output_handle *handle)				{ return NULL; }
 static inline void
 perf_event_task_sched_in(struct task_struct *prev,
 			 struct task_struct *task)			{ }

commit bed5b25ad9c8a2f5d735ef0bc746ec870c01c1b0
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Fri Jan 30 12:31:06 2015 +0200

    perf: Add a pmu capability for "exclusive" events
    
    Usually, pmus that do, for example, instruction tracing, would only ever
    be able to have one event per task per cpu (or per perf_event_context). For
    such pmus it makes sense to disallow creating conflicting events early on,
    so as to provide consistent behavior for the user.
    
    This patch adds a pmu capability that indicates such constraint on event
    creation.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1422613866-113186-1-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 13a1eb3a2a2d..f936a1e51f29 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -176,6 +176,7 @@ struct perf_event;
 #define PERF_PMU_CAP_NO_NMI			0x02
 #define PERF_PMU_CAP_AUX_NO_SG			0x04
 #define PERF_PMU_CAP_AUX_SW_DOUBLEBUF		0x08
+#define PERF_PMU_CAP_EXCLUSIVE			0x10
 
 /**
  * struct pmu - generic performance monitoring unit
@@ -196,6 +197,7 @@ struct pmu {
 
 	int * __percpu			pmu_disable_count;
 	struct perf_cpu_context * __percpu pmu_cpu_context;
+	atomic_t			exclusive_cnt; /* < 0: cpu; > 0: tsk */
 	int				task_ctx_nr;
 	int				hrtimer_interval_ms;
 

commit 6a279230391b63130070e0219b0ad09d34d28c89
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:13 2015 +0200

    perf: Add a capability for AUX_NO_SG pmus to do software double buffering
    
    For pmus that don't support scatter-gather for AUX data in hardware, it
    might still make sense to implement software double buffering to avoid
    losing data while the user is reading data out. For this purpose, add
    a pmu capability that guarantees multiple high-order chunks for AUX buffer,
    so that the pmu driver can do switchover tricks.
    
    To make use of this feature, add PERF_PMU_CAP_AUX_SW_DOUBLEBUF to your
    pmu's capability mask. This will make the ring buffer AUX allocation code
    ensure that the biggest high order allocation for the aux buffer pages is
    no bigger than half of the total requested buffer size, thus making sure
    that the buffer has at least two high order allocations.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-5-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d5a4a8e95808..13a1eb3a2a2d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -175,6 +175,7 @@ struct perf_event;
 #define PERF_PMU_CAP_NO_INTERRUPT		0x01
 #define PERF_PMU_CAP_NO_NMI			0x02
 #define PERF_PMU_CAP_AUX_NO_SG			0x04
+#define PERF_PMU_CAP_AUX_SW_DOUBLEBUF		0x08
 
 /**
  * struct pmu - generic performance monitoring unit

commit 0a4e38e64f5e91ce131cc42ee5bb3925377ec840
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:12 2015 +0200

    perf: Support high-order allocations for AUX space
    
    Some pmus (such as BTS or Intel PT without multiple-entry ToPA capability)
    don't support scatter-gather and will prefer larger contiguous areas for
    their output regions.
    
    This patch adds a new pmu capability to request higher order allocations.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-4-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5a94f6d6fa91..d5a4a8e95808 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -174,6 +174,7 @@ struct perf_event;
  */
 #define PERF_PMU_CAP_NO_INTERRUPT		0x01
 #define PERF_PMU_CAP_NO_NMI			0x02
+#define PERF_PMU_CAP_AUX_NO_SG			0x04
 
 /**
  * struct pmu - generic performance monitoring unit

commit 45bfb2e50471abbbfd83d40d28c986078b0d24ff
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jan 14 14:18:11 2015 +0200

    perf: Add AUX area to ring buffer for raw data streams
    
    This patch introduces "AUX space" in the perf mmap buffer, intended for
    exporting high bandwidth data streams to userspace, such as instruction
    flow traces.
    
    AUX space is a ring buffer, defined by aux_{offset,size} fields in the
    user_page structure, and read/write pointers aux_{head,tail}, which abide
    by the same rules as data_* counterparts of the main perf buffer.
    
    In order to allocate/mmap AUX, userspace needs to set up aux_offset to
    such an offset that will be greater than data_offset+data_size and
    aux_size to be the desired buffer size. Both need to be page aligned.
    Then, same aux_offset and aux_size should be passed to mmap() call and
    if everything adds up, you should have an AUX buffer as a result.
    
    Pages that are mapped into this buffer also come out of user's mlock
    rlimit plus perf_event_mlock_kb allowance.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-3-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 401554074de9..5a94f6d6fa91 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -284,6 +284,18 @@ struct pmu {
 	 * Return the count value for a counter.
 	 */
 	u64 (*count)			(struct perf_event *event); /*optional*/
+
+	/*
+	 * Set up pmu-private data structures for an AUX area
+	 */
+	void *(*setup_aux)		(int cpu, void **pages,
+					 int nr_pages, bool overwrite);
+					/* optional */
+
+	/*
+	 * Free pmu-private AUX data structures
+	 */
+	void (*free_aux)		(void *aux); /* optional */
 };
 
 /**
@@ -862,6 +874,11 @@ static inline bool needs_branch_stack(struct perf_event *event)
 	return event->attr.branch_sample_type != 0;
 }
 
+static inline bool has_aux(struct perf_event *event)
+{
+	return event->pmu->setup_aux;
+}
+
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
 extern void perf_output_end(struct perf_output_handle *handle);

commit 34f439278cef7b1177f8ce24f9fc81dfc6221d3b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Feb 20 14:05:38 2015 +0100

    perf: Add per event clockid support
    
    While thinking on the whole clock discussion it occurred to me we have
    two distinct uses of time:
    
     1) the tracking of event/ctx/cgroup enabled/running/stopped times
        which includes the self-monitoring support in struct
        perf_event_mmap_page.
    
     2) the actual timestamps visible in the data records.
    
    And we've been conflating them.
    
    The first is all about tracking time deltas, nobody should really care
    in what time base that happens, its all relative information, as long
    as its internally consistent it works.
    
    The second however is what people are worried about when having to
    merge their data with external sources. And here we have the
    discussion on MONOTONIC vs MONOTONIC_RAW etc..
    
    Where MONOTONIC is good for correlating between machines (static
    offset), MONOTNIC_RAW is required for correlating against a fixed rate
    hardware clock.
    
    This means configurability; now 1) makes that hard because it needs to
    be internally consistent across groups of unrelated events; which is
    why we had to have a global perf_clock().
    
    However, for 2) it doesn't really matter, perf itself doesn't care
    what it writes into the buffer.
    
    The below patch makes the distinction between these two cases by
    adding perf_event_clock() which is used for the second case. It
    further makes this configurable on a per-event basis, but adds a few
    sanity checks such that we cannot combine events with different clocks
    in confusing ways.
    
    And since we then have per-event configurability we might as well
    retain the 'legacy' behaviour as a default.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b16eac5f54ce..401554074de9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -173,6 +173,7 @@ struct perf_event;
  * pmu::capabilities flags
  */
 #define PERF_PMU_CAP_NO_INTERRUPT		0x01
+#define PERF_PMU_CAP_NO_NMI			0x02
 
 /**
  * struct pmu - generic performance monitoring unit
@@ -457,6 +458,7 @@ struct perf_event {
 	struct pid_namespace		*ns;
 	u64				id;
 
+	u64				(*clock)(void);
 	perf_overflow_handler_t		overflow_handler;
 	void				*overflow_handler_context;
 

commit 936c663aed930972f7e185485fd6c2da69e33819
Merge: 072e5a1cfabc 50f16a8bf9d7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Mar 27 09:46:19 2015 +0100

    Merge branch 'perf/x86' into perf/core, because it's ready
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 50f16a8bf9d7a92c437ed1867d0f7e1dc6a9aca9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 5 22:10:19 2015 +0100

    perf: Remove type specific target pointers
    
    The only reason CQM had to use a hard-coded pmu type was so it could use
    cqm_target in hw_perf_event.
    
    Do away with the {tp,bp,cqm}_target pointers and provide a non type
    specific one.
    
    This allows us to do away with that silly pmu type as well.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Vince Weaver <vince@deater.net>
    Cc: acme@kernel.org
    Cc: acme@redhat.com
    Cc: hpa@zytor.com
    Cc: jolsa@redhat.com
    Cc: kanaka.d.juvva@intel.com
    Cc: matt.fleming@intel.com
    Cc: tglx@linutronix.de
    Cc: torvalds@linux-foundation.org
    Cc: vikas.shivappa@linux.intel.com
    Link: http://lkml.kernel.org/r/20150305211019.GU21418@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index dac4c2831d82..5aa49d7bfd07 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -119,7 +119,6 @@ struct hw_perf_event {
 			struct hrtimer	hrtimer;
 		};
 		struct { /* tracepoint */
-			struct task_struct	*tp_target;
 			/* for tp_event->class */
 			struct list_head	tp_list;
 		};
@@ -129,7 +128,6 @@ struct hw_perf_event {
 			struct list_head	cqm_events_entry;
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;
-			struct task_struct	*cqm_target;
 		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */
@@ -138,12 +136,12 @@ struct hw_perf_event {
 			 * problem hw_breakpoint has with context
 			 * creation and event initalization.
 			 */
-			struct task_struct		*bp_target;
 			struct arch_hw_breakpoint	info;
 			struct list_head		bp_list;
 		};
 #endif
 	};
+	struct task_struct		*target;
 	int				state;
 	local64_t			prev_count;
 	u64				sample_period;

commit e9e4e44309f866b115d08ab4a54834008c50a8a4
Merge: 8a26ce4e5446 c517d838eb7d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 26 12:24:50 2015 +0100

    Merge tag 'v4.0-rc1' into perf/core, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit bfe1fcd2688f557a6b6a88f59ea7619228728bd7
Author: Matt Fleming <matt.fleming@intel.com>
Date:   Fri Jan 23 18:45:46 2015 +0000

    perf/x86/intel: Support task events with Intel CQM
    
    Add support for task events as well as system-wide events. This change
    has a big impact on the way that we gather LLC occupancy values in
    intel_cqm_event_read().
    
    Currently, for system-wide (per-cpu) events we defer processing to
    userspace which knows how to discard all but one cpu result per package.
    
    Things aren't so simple for task events because we need to do the value
    aggregation ourselves. To do this, we defer updating the LLC occupancy
    value in event->count from intel_cqm_event_read() and do an SMP
    cross-call to read values for all packages in intel_cqm_event_count().
    We need to ensure that we only do this for one task event per cache
    group, otherwise we'll report duplicate values.
    
    If we're a system-wide event we want to fallback to the default
    perf_event_count() implementation. Refactor this into a common function
    so that we don't duplicate the code.
    
    Also, introduce PERF_TYPE_INTEL_CQM, since we need a way to track an
    event's task (if the event isn't per-cpu) inside of the Intel CQM PMU
    driver.  This task information is only availble in the upper layers of
    the perf infrastructure.
    
    Other perf backends stash the target task in event->hw.*target so we
    need to do something similar. The task is used to determine whether
    events should share a cache group and an RMID.
    
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/1422038748-21397-8-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ca5504c48f4f..dac4c2831d82 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -129,6 +129,7 @@ struct hw_perf_event {
 			struct list_head	cqm_events_entry;
 			struct list_head	cqm_groups_entry;
 			struct list_head	cqm_group_entry;
+			struct task_struct	*cqm_target;
 		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */

commit 4afbb24ce5e723c8a093a6674a3c33062175078a
Author: Matt Fleming <matt.fleming@intel.com>
Date:   Fri Jan 23 18:45:44 2015 +0000

    perf/x86/intel: Add Intel Cache QoS Monitoring support
    
    Future Intel Xeon processors support a Cache QoS Monitoring feature that
    allows tracking of the LLC occupancy for a task or task group, i.e. the
    amount of data in pulled into the LLC for the task (group).
    
    Currently the PMU only supports per-cpu events. We create an event for
    each cpu and read out all the LLC occupancy values.
    
    Because this results in duplicate values being written out to userspace,
    we also export a .per-pkg event file so that the perf tools only
    accumulate values for one cpu per package.
    
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422038748-21397-6-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9fc9b0d31442..ca5504c48f4f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -123,6 +123,13 @@ struct hw_perf_event {
 			/* for tp_event->class */
 			struct list_head	tp_list;
 		};
+		struct { /* intel_cqm */
+			int			cqm_state;
+			int			cqm_rmid;
+			struct list_head	cqm_events_entry;
+			struct list_head	cqm_groups_entry;
+			struct list_head	cqm_group_entry;
+		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */
 			/*

commit eacd3ecc34472ce3751eedfc94e44c7cc6eb6305
Author: Matt Fleming <matt.fleming@intel.com>
Date:   Fri Jan 23 18:45:41 2015 +0000

    perf: Add ->count() function to read per-package counters
    
    For PMU drivers that record per-package counters, the ->count variable
    cannot be used to record an accurate aggregated value, since it's not
    possible to perform SMP cross-calls to cpus on other packages from the
    context in which we update ->count.
    
    Introduce a new optional ->count() accessor function that can be used to
    customize how values are collected. If a PMU driver doesn't provide a
    ->count() function, we fallback to the existing code.
    
    There is necessarily a window of staleness with this approach because
    the task that generated the counter value may not have been scheduled by
    the cpu recently.
    
    An alternative and more complex approach would be to use a hrtimer to
    periodically refresh the values from a more permissive scheduling
    context. So, we're trading off complexity for accuracy.
    
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422038748-21397-3-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cae4a9481777..9fc9b0d31442 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -272,6 +272,11 @@ struct pmu {
 	 */
 	size_t				task_ctx_size;
 
+
+	/*
+	 * Return the count value for a counter.
+	 */
+	u64 (*count)			(struct perf_event *event); /*optional*/
 };
 
 /**
@@ -770,6 +775,11 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 		__perf_event_task_sched_out(prev, next);
 }
 
+static inline u64 __perf_event_count(struct perf_event *event)
+{
+	return local64_read(&event->count) + atomic64_read(&event->child_count);
+}
+
 extern void perf_event_mmap(struct vm_area_struct *vma);
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);

commit 39bed6cbb842d8edf5a26b01122b391d36775b5e
Author: Matt Fleming <matt.fleming@intel.com>
Date:   Fri Jan 23 18:45:40 2015 +0000

    perf: Make perf_cgroup_from_task() global
    
    Move perf_cgroup_from_task() from kernel/events/ to include/linux/ along
    with the necessary struct definitions, so that it can be used by the PMU
    code.
    
    When the upcoming Intel Cache Monitoring PMU driver assigns monitoring
    IDs to perf events, it needs to be able to check whether any two
    monitoring events overlap (say, a cgroup and task event), which means we
    need to be able to lookup the cgroup associated with a task (if any).
    
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kanaka Juvva <kanaka.d.juvva@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422038748-21397-2-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 724d3720c9b1..cae4a9481777 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -53,6 +53,7 @@ struct perf_guest_info_callbacks {
 #include <linux/sysfs.h>
 #include <linux/perf_regs.h>
 #include <linux/workqueue.h>
+#include <linux/cgroup.h>
 #include <asm/local.h>
 
 struct perf_callchain_entry {
@@ -547,6 +548,35 @@ struct perf_output_handle {
 	int				page;
 };
 
+#ifdef CONFIG_CGROUP_PERF
+
+/*
+ * perf_cgroup_info keeps track of time_enabled for a cgroup.
+ * This is a per-cpu dynamically allocated data structure.
+ */
+struct perf_cgroup_info {
+	u64				time;
+	u64				timestamp;
+};
+
+struct perf_cgroup {
+	struct cgroup_subsys_state	css;
+	struct perf_cgroup_info	__percpu *info;
+};
+
+/*
+ * Must ensure cgroup is pinned (css_get) before calling
+ * this function. In other words, we cannot call this function
+ * if there is no cgroup event for the current CPU context.
+ */
+static inline struct perf_cgroup *
+perf_cgroup_from_task(struct task_struct *task)
+{
+	return container_of(task_css(task, perf_event_cgrp_id),
+			    struct perf_cgroup, css);
+}
+#endif /* CONFIG_CGROUP_PERF */
+
 #ifdef CONFIG_PERF_EVENTS
 
 extern int perf_pmu_register(struct pmu *pmu, const char *name, int type);

commit acba3c7e4652ca5fcb2fd9376d58c2dffd8ddf2a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jan 14 14:15:39 2015 +0100

    perf, powerpc: Fix up flush_branch_stack() users
    
    The recent LBR rework for x86 left a stray flush_branch_stack() user in
    the PowerPC code, fix that up.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joel Stanley <joel@jms.id.au>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 43cc158487e6..724d3720c9b1 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -261,11 +261,6 @@ struct pmu {
 	 */
 	int (*event_idx)		(struct perf_event *event); /*optional */
 
-	/*
-	 * flush branch stack on context-switches (needed in cpu-wide mode)
-	 */
-	void (*flush_branch_stack)	(void);
-
 	/*
 	 * context-switches callback
 	 */

commit a46a23000198d929391aa9dac8de68734efa2703
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Nov 4 21:56:06 2014 -0500

    perf: Simplify the branch stack check
    
    Use event->attr.branch_sample_type to replace
    intel_pmu_needs_lbr_smpl() for avoiding duplicated code that
    implicitly enables the LBR.
    
    Currently, branch stack can be enabled by user explicitly requesting
    branch sampling or implicit branch sampling to correct PEBS skid.
    
    For user explicitly requested branch sampling, the branch_sample_type
    is explicitly set by user. For PEBS case, the branch_sample_type is also
    implicitly set to PERF_SAMPLE_BRANCH_ANY in x86_pmu_hw_config.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: eranian@google.com
    Cc: jolsa@redhat.com
    Link: http://lkml.kernel.org/r/1415156173-10035-11-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 270cd0173e61..43cc158487e6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -814,6 +814,11 @@ static inline bool has_branch_stack(struct perf_event *event)
 	return event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK;
 }
 
+static inline bool needs_branch_stack(struct perf_event *event)
+{
+	return event->attr.branch_sample_type != 0;
+}
+
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
 extern void perf_output_end(struct perf_output_handle *handle);

commit 4af57ef28c2c1047fda9e1a5be02aa7a6a69cf9e
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Nov 4 21:56:01 2014 -0500

    perf: Add pmu specific data for perf task context
    
    Introduce a new flag PERF_ATTACH_TASK_DATA for perf event's attach
    stata. The flag is set by PMU's event_init() callback, it indicates
    that perf event needs PMU specific data.
    
    The PMU specific data are initialized to zeros. Later patches will
    use PMU specific data to save LBR stack.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: eranian@google.com
    Cc: jolsa@redhat.com
    Link: http://lkml.kernel.org/r/1415156173-10035-6-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c7007a564440..270cd0173e61 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -271,6 +271,10 @@ struct pmu {
 	 */
 	void (*sched_task)		(struct perf_event_context *ctx,
 					bool sched_in);
+	/*
+	 * PMU specific data size
+	 */
+	size_t				task_ctx_size;
 
 };
 
@@ -307,6 +311,7 @@ struct swevent_hlist {
 #define PERF_ATTACH_CONTEXT	0x01
 #define PERF_ATTACH_GROUP	0x02
 #define PERF_ATTACH_TASK	0x04
+#define PERF_ATTACH_TASK_DATA	0x08
 
 struct perf_cgroup;
 struct ring_buffer;
@@ -511,6 +516,7 @@ struct perf_event_context {
 	u64				generation;
 	int				pin_count;
 	int				nr_cgroups;	 /* cgroup evts */
+	void				*task_ctx_data; /* pmu specific data */
 	struct rcu_head			rcu_head;
 
 	struct delayed_work		orphans_remove;

commit 2a0ad3b326a9024ba86dca4028499d31fa0c6c4d
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Nov 4 21:55:59 2014 -0500

    perf/x86/intel: Use context switch callback to flush LBR stack
    
    Previous commit introduces context switch callback, its function
    overlaps with the flush branch stack callback. So we can use the
    context switch callback to flush LBR stack.
    
    This patch adds code that uses the flush branch callback to
    flush the LBR stack when task is being scheduled in. The callback
    is enabled only when there are events use the LBR hardware. This
    patch also removes all old flush branch stack code.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: eranian@google.com
    Cc: jolsa@redhat.com
    Link: http://lkml.kernel.org/r/1415156173-10035-4-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fbab6235d053..c7007a564440 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -511,7 +511,6 @@ struct perf_event_context {
 	u64				generation;
 	int				pin_count;
 	int				nr_cgroups;	 /* cgroup evts */
-	int				nr_branch_stack; /* branch_stack evt */
 	struct rcu_head			rcu_head;
 
 	struct delayed_work		orphans_remove;

commit ba532500c5651a4be4108acc64ed99a95cb005b3
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Nov 4 21:55:58 2014 -0500

    perf: Introduce pmu context switch callback
    
    The callback is invoked when process is scheduled in or out.
    It provides mechanism for later patches to save/store the LBR
    stack. For the schedule in case, the callback is invoked at
    the same place that flush branch stack callback is invoked.
    So it also can replace the flush branch stack callback. To
    avoid unnecessary overhead, the callback is enabled only when
    there are events use the LBR stack.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: eranian@google.com
    Cc: jolsa@redhat.com
    Link: http://lkml.kernel.org/r/1415156173-10035-3-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 33262004c310..fbab6235d053 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -265,6 +265,13 @@ struct pmu {
 	 * flush branch stack on context-switches (needed in cpu-wide mode)
 	 */
 	void (*flush_branch_stack)	(void);
+
+	/*
+	 * context-switches callback
+	 */
+	void (*sched_task)		(struct perf_event_context *ctx,
+					bool sched_in);
+
 };
 
 /**
@@ -558,6 +565,8 @@ extern void perf_event_delayed_put(struct task_struct *task);
 extern void perf_event_print_debug(void);
 extern void perf_pmu_disable(struct pmu *pmu);
 extern void perf_pmu_enable(struct pmu *pmu);
+extern void perf_sched_cb_dec(struct pmu *pmu);
+extern void perf_sched_cb_inc(struct pmu *pmu);
 extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
 extern int perf_event_refresh(struct perf_event *event, int refresh);

commit 37507717de51a8332a34ee07fd88700be88df5bf
Merge: a68fb48380bb a66734297f78
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 16 14:58:12 2015 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 perf updates from Ingo Molnar:
     "This series tightens up RDPMC permissions: currently even highly
      sandboxed x86 execution environments (such as seccomp) have permission
      to execute RDPMC, which may leak various perf events / PMU state such
      as timing information and other CPU execution details.
    
      This 'all is allowed' RDPMC mode is still preserved as the
      (non-default) /sys/devices/cpu/rdpmc=2 setting.  The new default is
      that RDPMC access is only allowed if a perf event is mmap-ed (which is
      needed to correctly interpret RDPMC counter values in any case).
    
      As a side effect of these changes CR4 handling is cleaned up in the
      x86 code and a shadow copy of the CR4 value is added.
    
      The extra CR4 manipulation adds ~ <50ns to the context switch cost
      between rdpmc-capable and rdpmc-non-capable mms"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86: Add /sys/devices/cpu/rdpmc=2 to allow rdpmc for all tasks
      perf/x86: Only allow rdpmc if a perf_event is mapped
      perf: Pass the event to arch_perf_update_userpage()
      perf: Add pmu callbacks to track event mapping and unmapping
      x86: Add a comment clarifying LDT context switching
      x86: Store a per-cpu shadow copy of CR4
      x86: Clean up cr4 manipulation

commit d3f180ea1a44aecba1b0dab2a253428e77f906bf
Merge: 6b00f7efb530 a6130ed253a9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 18:15:38 2015 -0800

    Merge tag 'powerpc-3.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Update of all defconfigs
    
     - Addition of a bunch of config options to modernise our defconfigs
    
     - Some PS3 updates from Geoff
    
     - Optimised memcmp for 64 bit from Anton
    
     - Fix for kprobes that allows 'perf probe' to work from Naveen
    
     - Several cxl updates from Ian & Ryan
    
     - Expanded support for the '24x7' PMU from Cody & Sukadev
    
     - Freescale updates from Scott:
        "Highlights include 8xx optimizations, some more work on datapath
         device tree content, e300 machine check support, t1040 corenet
         error reporting, and various cleanups and fixes"
    
    * tag 'powerpc-3.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (102 commits)
      cxl: Add missing return statement after handling AFU errror
      cxl: Fail AFU initialisation if an invalid configuration record is found
      cxl: Export optional AFU configuration record in sysfs
      powerpc/mm: Warn on flushing tlb page in kernel context
      powerpc/powernv: Add OPAL soft-poweroff routine
      powerpc/perf/hv-24x7: Document sysfs event description entries
      powerpc/perf/hv-gpci: add the remaining gpci requests
      powerpc/perf/{hv-gpci, hv-common}: generate requests with counters annotated
      powerpc/perf/hv-24x7: parse catalog and populate sysfs with events
      perf: define EVENT_DEFINE_RANGE_FORMAT_LITE helper
      perf: add PMU_EVENT_ATTR_STRING() helper
      perf: provide sysfs_show for struct perf_pmu_events_attr
      powerpc/kernel: Avoid initializing device-tree pointer twice
      powerpc: Remove old compile time disabled syscall tracing code
      powerpc/kernel: Make syscall_exit a local label
      cxl: Fix device_node reference counting
      powerpc/mm: bail out early when flushing TLB page
      powerpc: defconfigs: add MTD_SPI_NOR (new dependency for M25P80)
      perf/powerpc: reset event hw state when adding it to the PMU
      powerpc/qe: Use strlcpy()
      ...

commit 1e0fb9ec679c9273a641f1d6f3d25ea47baef2bb
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Oct 24 15:58:10 2014 -0700

    perf: Add pmu callbacks to track event mapping and unmapping
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Vince Weaver <vince@deater.net>
    Cc: "hillf.zj" <hillf.zj@alibaba-inc.com>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/266afcba1d1f91ea5501e4e16e94bbbc1a9339b6.1414190806.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5cad0e6f3552..33262004c310 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -202,6 +202,13 @@ struct pmu {
 	 */
 	int (*event_init)		(struct perf_event *event);
 
+	/*
+	 * Notification that the event was mapped or unmapped.  Called
+	 * in the context of the mapping task.
+	 */
+	void (*event_mapped)		(struct perf_event *event); /*optional*/
+	void (*event_unmapped)		(struct perf_event *event); /*optional*/
+
 #define PERF_EF_START	0x01		/* start the counter when adding    */
 #define PERF_EF_RELOAD	0x02		/* reload the counter when starting */
 #define PERF_EF_UPDATE	0x04		/* update the counter when stopping */

commit 2fde4f94e0a9531251e706fa57131b51b0df042e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jan 7 15:01:54 2015 +0000

    perf: Decouple unthrottling and rotating
    
    Currently the adjusments made as part of perf_event_task_tick() use the
    percpu rotation lists to iterate over any active PMU contexts, but these
    are not used by the context rotation code, having been replaced by
    separate (per-context) hrtimer callbacks. However, some manipulation of
    the rotation lists (i.e. removal of contexts) has remained in
    perf_rotate_context(). This leads to the following issues:
    
    * Contexts are not always removed from the rotation lists. Removal of
      PMUs which have been placed in rotation lists, but have not been
      removed by a hrtimer callback can result in corruption of the rotation
      lists (when memory backing the context is freed).
    
      This has been observed to result in hangs when PMU drivers built as
      modules are inserted and removed around the creation of events for
      said PMUs.
    
    * Contexts which do not require rotation may be removed from the
      rotation lists as a result of a hrtimer, and will not be considered by
      the unthrottling code in perf_event_task_tick.
    
    This patch fixes the issue by updating the rotation ist when events are
    scheduled in/out, ensuring that each rotation list stays in sync with
    the HW state. As each event holds a refcount on the module of its PMU,
    this ensures that when a PMU module is unloaded none of its CPU contexts
    can be in a rotation list. By maintaining a list of perf_event_contexts
    rather than perf_event_cpu_contexts, we don't need separate paths to
    handle the cpu and task contexts, which also makes the code a little
    simpler.
    
    As the rotation_list variables are not used for rotation, these are
    renamed to active_ctx_list, which better matches their current function.
    perf_pmu_rotate_{start,stop} are renamed to
    perf_pmu_ctx_{activate,deactivate}.
    
    Reported-by: Johannes Jensen <johannes.jensen@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Will Deacon <Will.Deacon@arm.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20150129134511.GR17721@leverpostej
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 216653466a67..5cad0e6f3552 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -469,6 +469,7 @@ struct perf_event_context {
 	 */
 	struct mutex			mutex;
 
+	struct list_head		active_ctx_list;
 	struct list_head		pinned_groups;
 	struct list_head		flexible_groups;
 	struct list_head		event_list;
@@ -519,7 +520,6 @@ struct perf_cpu_context {
 	int				exclusive;
 	struct hrtimer			hrtimer;
 	ktime_t				hrtimer_interval;
-	struct list_head		rotation_list;
 	struct pmu			*unique_pmu;
 	struct perf_cgroup		*cgrp;
 };

commit f0405b816149665393cc62b9e5082fc2d79714df
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Fri Jan 30 13:45:58 2015 -0800

    perf: add PMU_EVENT_ATTR_STRING() helper
    
    Helper for constructing static struct perf_pmu_events_attr s.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 58f59bdb590b..1d3631448b91 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -906,6 +906,13 @@ static struct perf_pmu_events_attr _var = {				\
 	.id   =  _id,							\
 };
 
+#define PMU_EVENT_ATTR_STRING(_name, _var, _str)			    \
+static struct perf_pmu_events_attr _var = {				    \
+	.attr		= __ATTR(_name, 0444, perf_event_sysfs_show, NULL), \
+	.id		= 0,						    \
+	.event_str	= _str,						    \
+};
+
 #define PMU_FORMAT_ATTR(_name, _format)					\
 static ssize_t								\
 _name##_show(struct device *dev,					\

commit fd979c0132074856975a6e79bc2226b99435ec5b
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Fri Jan 30 13:45:57 2015 -0800

    perf: provide sysfs_show for struct perf_pmu_events_attr
    
    (struct perf_pmu_events_attr) is defined in include/linux/perf_event.h,
    but the only "show" for it is in x86 and contains x86 specific stuff.
    
    Make a generic one for those of us who are just using the event_str.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 486e84ccb1f9..58f59bdb590b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -897,6 +897,9 @@ struct perf_pmu_events_attr {
 	const char *event_str;
 };
 
+ssize_t perf_event_sysfs_show(struct device *dev, struct device_attribute *attr,
+			      char *page);
+
 #define PMU_EVENT_ATTR(_name, _var, _id, _show)				\
 static struct perf_pmu_events_attr _var = {				\
 	.attr = __ATTR(_name, 0444, _show, NULL),			\

commit f10698ed6807dc41d021fb7baeb24f9bc4051837
Merge: 86038c5ea81b e742f3dc0886
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 28 15:42:56 2015 +0100

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c3c87e770458aa004bd7ed3f29945ff436fd6511
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 11:19:48 2015 +0100

    perf: Tighten (and fix) the grouping condition
    
    The fix from 9fc81d87420d ("perf: Fix events installation during
    moving group") was incomplete in that it failed to recognise that
    creating a group with events for different CPUs is semantically
    broken -- they cannot be co-scheduled.
    
    Furthermore, it leads to real breakage where, when we create an event
    for CPU Y and then migrate it to form a group on CPU X, the code gets
    confused where the counter is programmed -- triggered in practice
    as well by me via the perf fuzzer.
    
    Fix this by tightening the rules for creating groups. Only allow
    grouping of counters that can be co-scheduled in the same context.
    This means for the same task and/or the same cpu.
    
    Fixes: 9fc81d87420d ("perf: Fix events installation during moving group")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20150123125834.090683288@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4f7a61ca4b39..664de5a4ec46 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -450,11 +450,6 @@ struct perf_event {
 #endif /* CONFIG_PERF_EVENTS */
 };
 
-enum perf_event_context_type {
-	task_context,
-	cpu_context,
-};
-
 /**
  * struct perf_event_context - event context structure
  *
@@ -462,7 +457,6 @@ enum perf_event_context_type {
  */
 struct perf_event_context {
 	struct pmu			*pmu;
-	enum perf_event_context_type	type;
 	/*
 	 * Protect the states of the events in the list,
 	 * nr_active, and the list:

commit 86038c5ea81b519a8a1fcfcd5e4599aab0cdd119
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Tue Dec 16 12:47:34 2014 +0100

    perf: Avoid horrible stack usage
    
    Both Linus (most recent) and Steve (a while ago) reported that perf
    related callbacks have massive stack bloat.
    
    The problem is that software events need a pt_regs in order to
    properly report the event location and unwind stack. And because we
    could not assume one was present we allocated one on stack and filled
    it with minimal bits required for operation.
    
    Now, pt_regs is quite large, so this is undesirable. Furthermore it
    turns out that most sites actually have a pt_regs pointer available,
    making this even more onerous, as the stack space is pointless waste.
    
    This patch addresses the problem by observing that software events
    have well defined nesting semantics, therefore we can use static
    per-cpu storage instead of on-stack.
    
    Linus made the further observation that all but the scheduler callers
    of perf_sw_event() have a pt_regs available, so we change the regular
    perf_sw_event() to require a valid pt_regs (where it used to be
    optional) and add perf_sw_event_sched() for the scheduler.
    
    We have a scheduler specific call instead of a more generic _noregs()
    like construct because we can assume non-recursion from the scheduler
    and thereby simplify the code further (_noregs would have to put the
    recursion context call inline in order to assertain which __perf_regs
    element to use).
    
    One last note on the implementation of perf_trace_buf_prepare(); we
    allow .regs = NULL for those cases where we already have a pt_regs
    pointer available and do not need another.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Javi Merino <javi.merino@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Petr Mladek <pmladek@suse.cz>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Link: http://lkml.kernel.org/r/20141216115041.GW3337@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4f7a61ca4b39..3a7bd80b4db8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -665,6 +665,7 @@ static inline int is_software_event(struct perf_event *event)
 
 extern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
+extern void ___perf_sw_event(u32, u64, struct pt_regs *, u64);
 extern void __perf_sw_event(u32, u64, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
@@ -689,14 +690,25 @@ static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 static __always_inline void
 perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 {
-	struct pt_regs hot_regs;
+	if (static_key_false(&perf_swevent_enabled[event_id]))
+		__perf_sw_event(event_id, nr, regs, addr);
+}
+
+DECLARE_PER_CPU(struct pt_regs, __perf_regs[4]);
 
+/*
+ * 'Special' version for the scheduler, it hard assumes no recursion,
+ * which is guaranteed by us not actually scheduling inside other swevents
+ * because those disable preemption.
+ */
+static __always_inline void
+perf_sw_event_sched(u32 event_id, u64 nr, u64 addr)
+{
 	if (static_key_false(&perf_swevent_enabled[event_id])) {
-		if (!regs) {
-			perf_fetch_caller_regs(&hot_regs);
-			regs = &hot_regs;
-		}
-		__perf_sw_event(event_id, nr, regs, addr);
+		struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
+
+		perf_fetch_caller_regs(regs);
+		___perf_sw_event(event_id, nr, regs, addr);
 	}
 }
 
@@ -712,7 +724,7 @@ static inline void perf_event_task_sched_in(struct task_struct *prev,
 static inline void perf_event_task_sched_out(struct task_struct *prev,
 					     struct task_struct *next)
 {
-	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
+	perf_sw_event_sched(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 0);
 
 	if (static_key_false(&perf_sched_events.key))
 		__perf_event_task_sched_out(prev, next);
@@ -823,6 +835,8 @@ static inline int perf_event_refresh(struct perf_event *event, int refresh)
 static inline void
 perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)	{ }
 static inline void
+perf_sw_event_sched(u32 event_id, u64 nr, u64 addr)			{ }
+static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
 static inline int perf_register_guest_info_callbacks

commit 88a7c26af8dab2f2d69f5a6067eb670694ec38c0
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Sun Jan 4 10:36:19 2015 -0800

    perf: Move task_pt_regs sampling into arch code
    
    On x86_64, at least, task_pt_regs may be only partially initialized
    in many contexts, so x86_64 should not use it without extra care
    from interrupt context, let alone NMI context.
    
    This will allow x86_64 to override the logic and will supply some
    scratch space to use to make a cleaner copy of user regs.
    
    Tested-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: chenggang.qcg@taobao.com
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Jean Pihet <jean.pihet@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/e431cd4c18c2e1c44c774f10758527fb2d1025c4.1420396372.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 486e84ccb1f9..4f7a61ca4b39 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -79,11 +79,6 @@ struct perf_branch_stack {
 	struct perf_branch_entry	entries[0];
 };
 
-struct perf_regs {
-	__u64		abi;
-	struct pt_regs	*regs;
-};
-
 struct task_struct;
 
 /*
@@ -610,7 +605,14 @@ struct perf_sample_data {
 		u32	reserved;
 	}				cpu_entry;
 	struct perf_callchain_entry	*callchain;
+
+	/*
+	 * regs_user may point to task_pt_regs or to regs_user_copy, depending
+	 * on arch details.
+	 */
 	struct perf_regs		regs_user;
+	struct pt_regs			regs_user_copy;
+
 	struct perf_regs		regs_intr;
 	u64				stack_user_size;
 } ____cacheline_aligned;

commit 2565711fb7d7c28e0cd93c8971b520d1b10b857c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 13:48:42 2014 +0200

    perf: Improve the perf_sample_data struct layout
    
    This patch reorders fields in the perf_sample_data struct in order to
    minimize the number of cachelines touched in perf_sample_data_init().
    It also removes some intializations which are redundant with the code
    in kernel/events/core.c
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411559322-16548-7-git-send-email-eranian@google.com
    Cc: cebbert.lkml@gmail.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: jolsa@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 68d46d536e24..486e84ccb1f9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -580,35 +580,40 @@ extern u64 perf_event_read_value(struct perf_event *event,
 
 
 struct perf_sample_data {
-	u64				type;
+	/*
+	 * Fields set by perf_sample_data_init(), group so as to
+	 * minimize the cachelines touched.
+	 */
+	u64				addr;
+	struct perf_raw_record		*raw;
+	struct perf_branch_stack	*br_stack;
+	u64				period;
+	u64				weight;
+	u64				txn;
+	union  perf_mem_data_src	data_src;
 
+	/*
+	 * The other fields, optionally {set,used} by
+	 * perf_{prepare,output}_sample().
+	 */
+	u64				type;
 	u64				ip;
 	struct {
 		u32	pid;
 		u32	tid;
 	}				tid_entry;
 	u64				time;
-	u64				addr;
 	u64				id;
 	u64				stream_id;
 	struct {
 		u32	cpu;
 		u32	reserved;
 	}				cpu_entry;
-	u64				period;
-	union  perf_mem_data_src	data_src;
 	struct perf_callchain_entry	*callchain;
-	struct perf_raw_record		*raw;
-	struct perf_branch_stack	*br_stack;
 	struct perf_regs		regs_user;
 	struct perf_regs		regs_intr;
 	u64				stack_user_size;
-	u64				weight;
-	/*
-	 * Transaction flags for abort events:
-	 */
-	u64				txn;
-};
+} ____cacheline_aligned;
 
 /* default value for data source */
 #define PERF_MEM_NA (PERF_MEM_S(OP, NA)   |\
@@ -625,14 +630,9 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->raw  = NULL;
 	data->br_stack = NULL;
 	data->period = period;
-	data->regs_user.abi = PERF_SAMPLE_REGS_ABI_NONE;
-	data->regs_user.regs = NULL;
-	data->stack_user_size = 0;
 	data->weight = 0;
 	data->data_src.val = PERF_MEM_NA;
 	data->txn = 0;
-	data->regs_intr.abi = PERF_SAMPLE_REGS_ABI_NONE;
-	data->regs_intr.regs = NULL;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit 60e2364e60e86e81bc6377f49779779e6120977f
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Sep 24 13:48:37 2014 +0200

    perf: Add ability to sample machine state on interrupt
    
    Enable capture of interrupted machine state for each sample.
    
    Registers to sample are passed per event in the sample_regs_intr bitmask.
    
    To sample interrupt machine state, the PERF_SAMPLE_INTR_REGS must be passed in
    sample_type.
    
    The list of available registers is arch dependent and provided by asm/perf_regs.h
    
    Registers are laid out as u64 in the order of the bit order of sample_intr_regs.
    
    This patch also adds a new ABI version PERF_ATTR_SIZE_VER4 because we extend
    the perf_event_attr struct with a new u64 field.
    
    Reviewed-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: cebbert.lkml@gmail.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/1411559322-16548-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 893a0d07986f..68d46d536e24 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -79,7 +79,7 @@ struct perf_branch_stack {
 	struct perf_branch_entry	entries[0];
 };
 
-struct perf_regs_user {
+struct perf_regs {
 	__u64		abi;
 	struct pt_regs	*regs;
 };
@@ -600,7 +600,8 @@ struct perf_sample_data {
 	struct perf_callchain_entry	*callchain;
 	struct perf_raw_record		*raw;
 	struct perf_branch_stack	*br_stack;
-	struct perf_regs_user		regs_user;
+	struct perf_regs		regs_user;
+	struct perf_regs		regs_intr;
 	u64				stack_user_size;
 	u64				weight;
 	/*
@@ -630,6 +631,8 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->weight = 0;
 	data->data_src.val = PERF_MEM_NA;
 	data->txn = 0;
+	data->regs_intr.abi = PERF_SAMPLE_REGS_ABI_NONE;
+	data->regs_intr.regs = NULL;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit 179033b3e064d2cd3f5f9945e76b0a0f0fbf4883
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Thu Aug 7 11:48:26 2014 -0400

    perf: Add PERF_EVENT_STATE_EXIT state for events with exited task
    
    Adding new perf event state to indicate that the monitored task has
    exited.  In this case the event stays alive until the owner task exits
    or close the event fd while providing the last data through the read
    syscall and ring buffer.
    
    Instead it needs to propagate the error info (monitored task has died)
    via poll and read  syscalls by  returning POLLHUP and 0 respectively.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140811120102.GY9918@twins.programming.kicks-ass.net
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jean Pihet <jean.pihet@linaro.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-t5y3w8jjx6tfo5w8y6oajsjq@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f0a1036b1911..893a0d07986f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -269,6 +269,7 @@ struct pmu {
  * enum perf_event_active_state - the states of a event
  */
 enum perf_event_active_state {
+	PERF_EVENT_STATE_EXIT		= -3,
 	PERF_EVENT_STATE_ERROR		= -2,
 	PERF_EVENT_STATE_OFF		= -1,
 	PERF_EVENT_STATE_INACTIVE	=  0,

commit 770eee1fd38c70a009b321f5dbe64358f42511fd
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Aug 11 21:27:12 2014 +0200

    perf/x86: Fix data source encoding issues for load latency/precise store
    
    This patch fixes issues introuduce by Andi's previous patch 'Revamp PEBS'
    series.
    
    This patch fixes the following:
    
     - precise_store_data_hsw() encode the mem op type whenever we can
     - precise_store_data_hsw set the default data source correctly
    
     - 0 is not a valid init value for data source. Define PERF_MEM_NA as the
       default value
    
    This bug was actually introduced by
    
        commit 722e76e60f2775c21b087ff12c5e678cf0ebcaaf
        Author: Stephane Eranian <eranian@google.com>
        Date:   Thu May 15 17:56:44 2014 +0200
    
            fix Haswell precise store data source encoding
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1407785233-32193-4-git-send-email-eranian@google.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: ak@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ef5b62bdb103..f0a1036b1911 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -608,6 +608,13 @@ struct perf_sample_data {
 	u64				txn;
 };
 
+/* default value for data source */
+#define PERF_MEM_NA (PERF_MEM_S(OP, NA)   |\
+		    PERF_MEM_S(LVL, NA)   |\
+		    PERF_MEM_S(SNOOP, NA) |\
+		    PERF_MEM_S(LOCK, NA)  |\
+		    PERF_MEM_S(TLB, NA))
+
 static inline void perf_sample_data_init(struct perf_sample_data *data,
 					 u64 addr, u64 period)
 {
@@ -620,7 +627,7 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->regs_user.regs = NULL;
 	data->stack_user_size = 0;
 	data->weight = 0;
-	data->data_src.val = 0;
+	data->data_src.val = PERF_MEM_NA;
 	data->txn = 0;
 }
 

commit fadfe7be6e50de7f03913833b33c56cd8fb66bac
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Fri Aug 1 14:33:02 2014 +0200

    perf: Add queued work to remove orphaned child events
    
    In cases when the  owner task exits before the workload and the
    workload made some forks, all the events stay in until the last
    workload process exits. Thats' because each child event holds
    parent reference.
    
    We want to release all children events once the parent is gone,
    because at that time there's no process to read them anyway, so
    they're just eating resources.
    
    This removal  races with process exit, which removes all events
    and fork, which clone events.  To be clear of those two, adding
    work queue to remove orphaned child for context in case such
    event is detected.
    
    Using delayed work queue (with delay == 1), because we queue this
    work under perf scheduler callbacks. Normal work queue tries to wake
    up the queue process, which deadlocks on rq->lock in this place.
    
    Also preventing clones from abandoned parent event.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1406896382-18404-4-git-send-email-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 707617a8c0f6..ef5b62bdb103 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -52,6 +52,7 @@ struct perf_guest_info_callbacks {
 #include <linux/atomic.h>
 #include <linux/sysfs.h>
 #include <linux/perf_regs.h>
+#include <linux/workqueue.h>
 #include <asm/local.h>
 
 struct perf_callchain_entry {
@@ -507,6 +508,9 @@ struct perf_event_context {
 	int				nr_cgroups;	 /* cgroup evts */
 	int				nr_branch_stack; /* branch_stack evt */
 	struct rcu_head			rcu_head;
+
+	struct delayed_work		orphans_remove;
+	bool				orphans_remove_sched;
 };
 
 /*

commit 82b897782d10fcc4930c9d4a15b175348fdd2871
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Wed May 28 11:45:04 2014 +0300

    perf: Differentiate exec() and non-exec() comm events
    
    perf tools like 'perf report' can aggregate samples by comm strings,
    which generally works.  However, there are other potential use-cases.
    For example, to pair up 'calls' with 'returns' accurately (from branch
    events like Intel BTS) it is necessary to identify whether the process
    has exec'd.  Although a comm event is generated when an 'exec' happens
    it is also generated whenever the comm string is changed on a whim
    (e.g. by prctl PR_SET_NAME).  This patch adds a flag to the comm event
    to differentiate one case from the other.
    
    In order to determine whether the kernel supports the new flag, a
    selection bit named 'exec' is added to struct perf_event_attr.  The
    bit does nothing but will cause perf_event_open() to fail if the bit
    is set on kernels that do not have it defined.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/537D9EBE.7030806@intel.com
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b4c1d4685bf0..707617a8c0f6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -707,7 +707,7 @@ extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *
 extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 
 extern void perf_event_exec(void);
-extern void perf_event_comm(struct task_struct *tsk);
+extern void perf_event_comm(struct task_struct *tsk, bool exec);
 extern void perf_event_fork(struct task_struct *tsk);
 
 /* Callchains */
@@ -815,7 +815,7 @@ static inline int perf_unregister_guest_info_callbacks
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_exec(void)				{ }
-static inline void perf_event_comm(struct task_struct *tsk)		{ }
+static inline void perf_event_comm(struct task_struct *tsk, bool exec)	{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
 static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }

commit ec00010972a0971b2c1da4fbe4e5c7d8ed1ecb05
Merge: 8c6e549a447c e041e328c4b4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jun 6 07:55:06 2014 +0200

    Merge branch 'perf/urgent' into perf/core, to resolve conflict and to prepare for new patches
    
    Conflicts:
            arch/x86/kernel/traps.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e041e328c4b41e1db79bfe5ba9992c2ed771ad19
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 21 17:32:19 2014 +0200

    perf: Fix perf_event_comm() vs. exec() assumption
    
    perf_event_comm() assumes that set_task_comm() is only called on
    exec(), and in particular that its only called on current.
    
    Neither are true, as Dave reported a WARN triggered by set_task_comm()
    being called on !current.
    
    Separate the exec() hook from the comm hook.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20140521153219.GH5226@laptop.programming.kicks-ass.net
    [ Build fix. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3ef6ea12806a..9b5cd1992a88 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -695,6 +695,7 @@ extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 
+extern void perf_event_exec(void);
 extern void perf_event_comm(struct task_struct *tsk);
 extern void perf_event_fork(struct task_struct *tsk);
 
@@ -772,7 +773,7 @@ extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 extern int __perf_event_disable(void *info);
 extern void perf_event_task_tick(void);
-#else
+#else /* !CONFIG_PERF_EVENTS: */
 static inline void
 perf_event_task_sched_in(struct task_struct *prev,
 			 struct task_struct *task)			{ }
@@ -802,6 +803,7 @@ static inline int perf_unregister_guest_info_callbacks
 (struct perf_guest_info_callbacks *callbacks)				{ return 0; }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
+static inline void perf_event_exec(void)				{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }

commit 53b25335dd60981ad608da7890420898a34469a6
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Fri May 16 17:12:12 2014 -0400

    perf: Disable sampled events if no PMU interrupt
    
    Add common code to generate -ENOTSUPP at event creation time if an
    architecture attempts to create a sampled event and
    PERF_PMU_NO_INTERRUPT is set.
    
    This adds a new pmu->capabilities flag.  Initially we only support
    PERF_PMU_NO_INTERRUPT (to indicate a PMU has no support for generating
    hardware interrupts) but there are other capabilities that can be
    added later.
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Acked-by: Will Deacon <will.deacon@arm.com>
    [peterz: rename to PERF_PMU_CAP_* and moved the pmu::capabilities word into a hole]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1405161708060.11099@vincent-weaver-1.umelst.maine.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index af6dcf1d9e47..267c8f37012c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -166,6 +166,11 @@ struct perf_event;
  */
 #define PERF_EVENT_TXN 0x1
 
+/**
+ * pmu::capabilities flags
+ */
+#define PERF_PMU_CAP_NO_INTERRUPT		0x01
+
 /**
  * struct pmu - generic performance monitoring unit
  */
@@ -178,6 +183,11 @@ struct pmu {
 	const char			*name;
 	int				type;
 
+	/*
+	 * various common per-pmu feature flags
+	 */
+	int				capabilities;
+
 	int * __percpu			pmu_disable_count;
 	struct perf_cpu_context * __percpu pmu_cpu_context;
 	int				task_ctx_nr;

commit b69cf53640da2b86439596118cfa95233154ee76
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 14 10:50:33 2014 +0100

    perf: Fix a race between ring_buffer_detach() and ring_buffer_attach()
    
    Alexander noticed that we use RCU iteration on rb->event_list but do
    not use list_{add,del}_rcu() to add,remove entries to that list, nor
    do we observe proper grace periods when re-using the entries.
    
    Merge ring_buffer_detach() into ring_buffer_attach() such that
    attaching to the NULL buffer is detaching.
    
    Furthermore, ensure that between any 'detach' and 'attach' of the same
    event we observe the required grace period, but only when strictly
    required. In effect this means that only ioctl(.request =
    PERF_EVENT_IOC_SET_OUTPUT) will wait for a grace period, while the
    normal initial attach and final detach will not be delayed.
    
    This patch should, I think, do the right thing under all
    circumstances, the 'normal' cases all should never see the extra grace
    period, but the two cases:
    
     1) PERF_EVENT_IOC_SET_OUTPUT on an event which already has a
        ring_buffer set, will now observe the required grace period between
        removing itself from the old and attaching itself to the new buffer.
    
        This case is 'simple' in that both buffers are present in
        perf_event_set_output() one could think an unconditional
        synchronize_rcu() would be sufficient; however...
    
     2) an event that has a buffer attached, the buffer is destroyed
        (munmap) and then the event is attached to a new/different buffer
        using PERF_EVENT_IOC_SET_OUTPUT.
    
        This case is more complex because the buffer destruction does:
          ring_buffer_attach(.rb = NULL)
        followed by the ioctl() doing:
          ring_buffer_attach(.rb = foo);
    
        and we still need to observe the grace period between these two
        calls due to us reusing the event->rb_entry list_head.
    
    In order to make 2 happen we use Paul's latest cond_synchronize_rcu()
    call.
    
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Reported-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140507123526.GD13658@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3356abcfff18..3ef6ea12806a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -402,6 +402,8 @@ struct perf_event {
 
 	struct ring_buffer		*rb;
 	struct list_head		rb_entry;
+	unsigned long			rcu_batches;
+	int				rcu_pending;
 
 	/* poll related */
 	wait_queue_head_t		waitq;

commit c464c76eec4be587604ca082e8cded7e6b89f3bf
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Mar 18 16:56:41 2014 +0800

    perf: Allow building PMU drivers as modules
    
    This patch adds support for building PMU driver as module. It exports
    the functions perf_pmu_{register,unregister}() and adds reference tracking
    for the PMU driver module.
    
    When the PMU driver is built as a module, each active event of the PMU
    holds a reference to the driver module.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1395133004-23205-1-git-send-email-zheng.z.yan@intel.com
    Cc: eranian@google.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3356abcfff18..af6dcf1d9e47 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -172,6 +172,7 @@ struct perf_event;
 struct pmu {
 	struct list_head		entry;
 
+	struct module			*module;
 	struct device			*dev;
 	const struct attribute_group	**attr_groups;
 	const char			*name;

commit f0bdb5e0c72b7347c867da539367138ad95c6b24
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:39 2014 +0530

    CPU hotplug, perf: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the perf subsystem's hotplug notifier by using this latter form of
    callback registration.
    
    Also provide a bare-bones version of perf_cpu_notifier() that doesn't
    invoke the notifiers for the already online CPUs. This would be useful
    for subsystems that need to perform a different set of initialization
    for the already online CPUs, or don't need the initialization altogether.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e56b07f5c9b6..3356abcfff18 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -835,6 +835,8 @@ do {									\
 		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
 	unsigned long cpu = smp_processor_id();				\
 	unsigned long flags;						\
+									\
+	cpu_notifier_register_begin();					\
 	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,			\
 		(void *)(unsigned long)cpu);				\
 	local_irq_save(flags);						\
@@ -843,9 +845,21 @@ do {									\
 	local_irq_restore(flags);					\
 	fn(&fn##_nb, (unsigned long)CPU_ONLINE,				\
 		(void *)(unsigned long)cpu);				\
-	register_cpu_notifier(&fn##_nb);				\
+	__register_cpu_notifier(&fn##_nb);				\
+	cpu_notifier_register_done();					\
 } while (0)
 
+/*
+ * Bare-bones version of perf_cpu_notifier(), which doesn't invoke the
+ * callback for already online CPUs.
+ */
+#define __perf_cpu_notifier(fn)						\
+do {									\
+	static struct notifier_block fn##_nb =				\
+		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
+									\
+	__register_cpu_notifier(&fn##_nb);				\
+} while (0)
 
 struct perf_pmu_events_attr {
 	struct device_attribute attr;

commit f3ae75de98c4bac145a87d830c156c96f9414022
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Jan 8 11:15:52 2014 +0100

    perf/x86: Fix active_entry initialization
    
    This patch fixes a problem with the initialization of the
    struct perf_event active_entry field. It is defined inside
    an anonymous union and was initialized in perf_event_alloc()
    using INIT_LIST_HEAD(). However at that time, we do not know
    whether the event is going to use active_entry or hlist_entry (SW).
    Or at last, we don't want to make that determination there.
    The problem is that hlist and list_head are not initialized
    the same way. One is okay with NULL (from kzmalloc), the other
    needs to pointers to point to self.
    
    This patch resolves this problem by dropping the union.
    This will avoid problems later on, if someone starts using
    active_entry or hlist_entry without verifying that they
    actually overlap. This also solves the initialization
    problem.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: ak@linux.intel.com
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: zheng.z.yan@intel.com
    Cc: bp@alien8.de
    Cc: vincent.weaver@maine.edu
    Cc: maria.n.dimakopoulou@gmail.com
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1389176153-3128-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8f4a70f2eca8..e56b07f5c9b6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -319,10 +319,8 @@ struct perf_event {
 	 */
 	struct list_head		migrate_entry;
 
-	union {
-		struct hlist_node	hlist_entry;
-		struct list_head	active_entry;
-	};
+	struct hlist_node		hlist_entry;
+	struct list_head		active_entry;
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;

commit 71ad88efebbcde374bddf904b96f3a7fc82d45d4
Author: Stephane Eranian <eranian@google.com>
Date:   Tue Nov 12 17:58:48 2013 +0100

    perf: Add active_entry list head to struct perf_event
    
    This patch adds a new field to the struct perf_event.
    It is intended to be used to chain events which are
    active (enabled). It helps in the hardware layer
    for PMUs which do not have actual counter restrictions, i.e.,
    free running read-only counters. Active events are chained
    as opposed to being tracked via the counter they use.
    
    To save space we use a union with hlist_entry as both
    are mutually exclusive (suggested by Jiri Olsa).
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: zheng.z.yan@intel.com
    Cc: bp@alien8.de
    Cc: maria.n.dimakopoulou@gmail.com
    Link: http://lkml.kernel.org/r/1384275531-10892-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2e069d1288df..8f4a70f2eca8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -319,7 +319,10 @@ struct perf_event {
 	 */
 	struct list_head		migrate_entry;
 
-	struct hlist_node		hlist_entry;
+	union {
+		struct hlist_node	hlist_entry;
+		struct list_head	active_entry;
+	};
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;

commit fdfbbd07e91f8fe387140776f3fd94605f0c89e5
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Sep 20 07:40:39 2013 -0700

    perf: Add generic transaction flags
    
    Add a generic qualifier for transaction events, as a new sample
    type that returns a flag word. This is particularly useful
    for qualifying aborts: to distinguish aborts which happen
    due to asynchronous events (like conflicts caused by another
    CPU) versus instructions that lead to an abort.
    
    The tuning strategies are very different for those cases,
    so it's important to distinguish them easily and early.
    
    Since it's inconvenient and inflexible to filter for this
    in the kernel we report all the events out and allow
    some post processing in user space.
    
    The flags are based on the Intel TSX events, but should be fairly
    generic and mostly applicable to other HTM architectures too. In addition
    to various flag words there's also reserved space to report an
    program supplied abort code. For TSX this is used to distinguish specific
    classes of aborts, like a lock busy abort when doing lock elision.
    
    Flags:
    
    Elision and generic transactions                   (ELISION vs TRANSACTION)
    (HLE vs RTM on TSX; IBM etc.  would likely only use TRANSACTION)
    Aborts caused by current thread vs aborts caused by others (SYNC vs ASYNC)
    Retryable transaction                              (RETRY)
    Conflicts with other threads                       (CONFLICT)
    Transaction write capacity overflow                (CAPACITY WRITE)
    Transaction read capacity overflow                 (CAPACITY READ)
    
    Transactions implicitely aborted can also return an abort code.
    This can be used to signal specific events to the profiler. A common
    case is abort on lock busy in a RTM eliding library (code 0xff)
    To handle this case we include the TSX abort code
    
    Common example aborts in TSX would be:
    
    - Data conflict with another thread on memory read.
                                          Flags: TRANSACTION|ASYNC|CONFLICT
    - executing a WRMSR in a transaction. Flags: TRANSACTION|SYNC
    - HLE transaction in user space is too large
                                          Flags: ELISION|SYNC|CAPACITY-WRITE
    
    The only flag that is somewhat TSX specific is ELISION.
    
    This adds the perf core glue needed for reporting the new flag word out.
    
    v2: Add MEM/MISC
    v3: Move transaction to the end
    v4: Separate capacity-read/write and remove misc
    v5: Remove _SAMPLE. Move abort flags to 32bit. Rename
        transaction to txn
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379688044-14173-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c8ba627c1d60..2e069d1288df 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -584,6 +584,10 @@ struct perf_sample_data {
 	struct perf_regs_user		regs_user;
 	u64				stack_user_size;
 	u64				weight;
+	/*
+	 * Transaction flags for abort events:
+	 */
+	u64				txn;
 };
 
 static inline void perf_sample_data_init(struct perf_sample_data *data,
@@ -599,6 +603,7 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->stack_user_size = 0;
 	data->weight = 0;
 	data->data_src.val = 0;
+	data->txn = 0;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit 9886167d20c0720dcfb01e62cdff4d906b226f43
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 3 16:02:23 2013 +0200

    perf: Fix perf_pmu_migrate_context
    
    While auditing the list_entry usage due to a trinity bug I found that
    perf_pmu_migrate_context violates the rules for
    perf_event::event_entry.
    
    The problem is that perf_event::event_entry is a RCU list element, and
    hence we must wait for a full RCU grace period before re-using the
    element after deletion.
    
    Therefore the usage in perf_pmu_migrate_context() which re-uses the
    entry immediately is broken. For now introduce another list_head into
    perf_event for this specific usage.
    
    This doesn't actually fix the trinity report because that never goes
    through this code.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-mkj72lxagw1z8fvjm648iznw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 866e85c5eb94..c8ba627c1d60 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -294,9 +294,31 @@ struct ring_buffer;
  */
 struct perf_event {
 #ifdef CONFIG_PERF_EVENTS
-	struct list_head		group_entry;
+	/*
+	 * entry onto perf_event_context::event_list;
+	 *   modifications require ctx->lock
+	 *   RCU safe iterations.
+	 */
 	struct list_head		event_entry;
+
+	/*
+	 * XXX: group_entry and sibling_list should be mutually exclusive;
+	 * either you're a sibling on a group, or you're the group leader.
+	 * Rework the code to always use the same list element.
+	 *
+	 * Locked for modification by both ctx->mutex and ctx->lock; holding
+	 * either sufficies for read.
+	 */
+	struct list_head		group_entry;
 	struct list_head		sibling_list;
+
+	/*
+	 * We need storage to track the entries in perf_pmu_migrate_context; we
+	 * cannot use the event_entry because of RCU and we want to keep the
+	 * group in tact which avoids us using the other two entries.
+	 */
+	struct list_head		migrate_entry;
+
 	struct hlist_node		hlist_entry;
 	int				nr_siblings;
 	int				group_flags;

commit 816434ec4a674fcdb3c2221a6dffdc8f34020550
Merge: f357a82048ff 36bd621337c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 11:55:10 2013 -0700

    Merge branch 'x86-spinlocks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 spinlock changes from Ingo Molnar:
     "The biggest change here are paravirtualized ticket spinlocks (PV
      spinlocks), which bring a nice speedup on various benchmarks.
    
      The KVM host side will come to you via the KVM tree"
    
    * 'x86-spinlocks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kvm/guest: Fix sparse warning: "symbol 'klock_waiting' was not declared as static"
      kvm: Paravirtual ticketlocks support for linux guests running on KVM hypervisor
      kvm guest: Add configuration support to enable debug information for KVM Guests
      kvm uapi: Add KICK_CPU and PV_UNHALT definition to uapi
      xen, pvticketlock: Allow interrupts to be enabled while blocking
      x86, ticketlock: Add slowpath logic
      jump_label: Split jumplabel ratelimit
      x86, pvticketlock: When paravirtualizing ticket locks, increment by 2
      x86, pvticketlock: Use callee-save for lock_spinning
      xen, pvticketlocks: Add xen_nopvspin parameter to disable xen pv ticketlocks
      xen, pvticketlock: Xen implementation for PV ticket locks
      xen: Defer spinlock setup until boot CPU setup
      x86, ticketlock: Collapse a layer of functions
      x86, ticketlock: Don't inline _spin_unlock when using paravirt spinlocks
      x86, spinlock: Replace pv spinlocks with pv ticketlocks

commit 274481de6cb69abdb49403ff32abb63c23743413
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Fri Aug 23 15:51:03 2013 -0400

    perf: Export struct perf_branch_entry to userspace
    
    If PERF_SAMPLE_BRANCH_STACK is enabled then samples are returned
    with the format { u64 from, to, flags } but the flags layout
    is not specified.
    
    This field has the type struct perf_branch_entry; move this
    definition into include/uapi/linux/perf_event.h so users can
    access these fields.
    
    This is similar to the existing inclusion of perf_mem_data_src in
    the include/uapi/linux/perf_event.h file.
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Acked-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1308231544420.1889@vincent-weaver-1.um.maine.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c43f6eabad5b..4019d82c3d03 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -63,30 +63,6 @@ struct perf_raw_record {
 	void				*data;
 };
 
-/*
- * single taken branch record layout:
- *
- *      from: source instruction (may not always be a branch insn)
- *        to: branch target
- *   mispred: branch target was mispredicted
- * predicted: branch target was predicted
- *
- * support for mispred, predicted is optional. In case it
- * is not supported mispred = predicted = 0.
- *
- *     in_tx: running in a hardware transaction
- *     abort: aborting a hardware transaction
- */
-struct perf_branch_entry {
-	__u64	from;
-	__u64	to;
-	__u64	mispred:1,  /* target mispredicted */
-		predicted:1,/* target predicted */
-		in_tx:1,    /* in transaction */
-		abort:1,    /* transaction abort */
-		reserved:60;
-};
-
 /*
  * branch stack layout:
  *  nr: number of taken branches stored in entries[]

commit 851cf6e7d6366195d4ee033cdc7787df1a649a14
Author: Andrew Jones <drjones@redhat.com>
Date:   Fri Aug 9 19:51:57 2013 +0530

    jump_label: Split jumplabel ratelimit
    
    Commit b202952075f62603bea9bfb6ebc6b0420db11949 ("perf, core: Rate limit
    perf_sched_events jump_label patching") introduced rate limiting
    for jump label disabling. The changes were made in the jump label code
    in order to be more widely available and to keep things tidier. This is
    all fine, except now jump_label.h includes linux/workqueue.h, which
    makes it impossible to include jump_label.h from anything that
    workqueue.h needs. For example, it's now impossible to include
    jump_label.h from asm/spinlock.h, which is done in proposed
    pv-ticketlock patches. This patch splits out the rate limiting related
    changes from jump_label.h into a new file, jump_label_ratelimit.h, to
    resolve the issue.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Link: http://lkml.kernel.org/r/1376058122-8248-10-git-send-email-raghavendra.kt@linux.vnet.ibm.com
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c43f6eabad5b..226be8da3f85 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -48,6 +48,7 @@ struct perf_guest_info_callbacks {
 #include <linux/cpu.h>
 #include <linux/irq_work.h>
 #include <linux/static_key.h>
+#include <linux/jump_label_ratelimit.h>
 #include <linux/atomic.h>
 #include <linux/sysfs.h>
 #include <linux/perf_regs.h>

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8873f82c7baa..c43f6eabad5b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -826,7 +826,7 @@ static inline void perf_restore_debug_store(void)			{ }
  */
 #define perf_cpu_notifier(fn)						\
 do {									\
-	static struct notifier_block fn##_nb __cpuinitdata =		\
+	static struct notifier_block fn##_nb =				\
 		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
 	unsigned long cpu = smp_processor_id();				\
 	unsigned long flags;						\

commit 2d722f6d5671794c0de0e29e3da75006ac086718
Merge: f0bb4c0ab064 2fd1b4878843
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:17:25 2013 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes:
    
       - load-calculation cleanups and improvements, by Alex Shi
       - various nohz related tidying up of statisics, by Frederic
         Weisbecker
       - factor out /proc functions to kernel/sched/proc.c, by Paul
         Gortmaker
       - simplify the RT policy scheduler, by Kirill Tkhai
       - various fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (42 commits)
      sched/debug: Remove CONFIG_FAIR_GROUP_SCHED mask
      sched/debug: Fix formatting of /proc/<PID>/sched
      sched: Fix typo in struct sched_avg member description
      sched/fair: Fix typo describing flags in enqueue_entity
      sched/debug: Add load-tracking statistics to task
      sched: Change get_rq_runnable_load() to static and inline
      sched/tg: Remove tg.load_weight
      sched/cfs_rq: Change atomic64_t removed_load to atomic_long_t
      sched/tg: Use 'unsigned long' for load variable in task group
      sched: Change cfs_rq load avg to unsigned long
      sched: Consider runnable load average in move_tasks()
      sched: Compute runnable load avg in cpu_load and cpu_avg_load_per_task
      sched: Update cpu load after task_tick
      sched: Fix sleep time double accounting in enqueue entity
      sched: Set an initial value of runnable avg for new forked task
      sched: Move a few runnable tg variables into CONFIG_SMP
      Revert "sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking"
      sched: Don't mix use of typedef ctl_table and struct ctl_table
      sched: Remove WARN_ON(!sd) from init_sched_groups_power()
      sched: Fix memory leakage in build_sched_groups()
      ...

commit 2fd1b487884310d0aa0c0640179dc7490ad86313
Merge: 333bb864f192 8bb495e3f024
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jul 1 11:16:54 2013 +0200

    Merge tag 'v3.10' into sched/core
    
    Merge in a recent upstream commit:
    
      c2853c8df57f include/linux/math64.h: add div64_ul()
    
    because:
    
      72a4cf20cb71 sched: Change cfs_rq load avg to unsigned long
    
    relies on it.
    
    [ We don't rebase sched/core for this, because the handful of
      followup commits after the broken commit are not behavioral
      changes so are unlikely to be needed during bisection. ]
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 14c63f17b1fde5a575a28e96547a22b451c71fb5
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Jun 21 08:51:36 2013 -0700

    perf: Drop sample rate when sampling is too slow
    
    This patch keeps track of how long perf's NMI handler is taking,
    and also calculates how many samples perf can take a second.  If
    the sample length times the expected max number of samples
    exceeds a configurable threshold, it drops the sample rate.
    
    This way, we don't have a runaway sampling process eating up the
    CPU.
    
    This patch can tend to drop the sample rate down to level where
    perf doesn't work very well.  *BUT* the alternative is that my
    system hangs because it spends all of its time handling NMIs.
    
    I'll take a busted performance tool over an entire system that's
    busted and undebuggable any day.
    
    BTW, my suspicion is that there's still an underlying bug here.
    Using the HPET instead of the TSC is definitely a contributing
    factor, but I suspect there are some other things going on.
    But, I can't go dig down on a bug like that with my machine
    hanging all the time.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus@samba.org
    Cc: acme@ghostprotocols.net
    Cc: Dave Hansen <dave@sr71.net>
    [ Prettified it a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 056f93a7990f..50b3efd14d29 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -706,10 +706,17 @@ static inline void perf_callchain_store(struct perf_callchain_entry *entry, u64
 extern int sysctl_perf_event_paranoid;
 extern int sysctl_perf_event_mlock;
 extern int sysctl_perf_event_sample_rate;
+extern int sysctl_perf_cpu_time_max_percent;
+
+extern void perf_sample_event_took(u64 sample_len_ns);
 
 extern int perf_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
+extern int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 
 static inline bool perf_paranoid_tracepoint_raw(void)
 {

commit 135c5612c460f89657c4698fe2ea753f6f667963
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Jun 17 17:36:51 2013 -0700

    perf/x86/intel: Support Haswell/v4 LBR format
    
    Haswell has two additional LBR from flags for TSX: in_tx and
    abort_tx, implemented as a new "v4" version of the LBR format.
    
    Handle those in and adjust the sign extension code to still
    correctly extend. The flags are exported similarly in the LBR
    record to the existing misprediction flag
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andi Kleen <ak@linux.jf.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/r/1371515812-9646-6-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 33e8d65836d6..056f93a7990f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -73,13 +73,18 @@ struct perf_raw_record {
  *
  * support for mispred, predicted is optional. In case it
  * is not supported mispred = predicted = 0.
+ *
+ *     in_tx: running in a hardware transaction
+ *     abort: aborting a hardware transaction
  */
 struct perf_branch_entry {
 	__u64	from;
 	__u64	to;
 	__u64	mispred:1,  /* target mispredicted */
 		predicted:1,/* target predicted */
-		reserved:62;
+		in_tx:1,    /* in transaction */
+		abort:1,    /* transaction abort */
+		reserved:60;
 };
 
 /*

commit 0a0fca9d832b704f116a25badd1ca8c16771dcac
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Jun 4 13:10:24 2013 +0530

    sched: Rename sched.c as sched/core.c in comments and Documentation
    
    Most of the stuff from kernel/sched.c was moved to kernel/sched/core.c long time
    back and the comments/Documentation never got updated.
    
    I figured it out when I was going through sched-domains.txt and so thought of
    fixing it globally.
    
    I haven't crossed check if the stuff that is referenced in sched/core.c by all
    these files is still present and hasn't changed as that wasn't the motive behind
    this patch.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/cdff76a265326ab8d71922a1db5be599f20aad45.1370329560.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f463a46424e2..5ec99e5a50d2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -803,7 +803,7 @@ static inline void perf_restore_debug_store(void)			{ }
 #define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))
 
 /*
- * This has to have a higher priority than migration_notifier in sched.c.
+ * This has to have a higher priority than migration_notifier in sched/core.c.
  */
 #define perf_cpu_notifier(fn)						\
 do {									\

commit 43b4578071c0e6d87761e113e05d45776cc75437
Author: Andrew Hunter <ahh@google.com>
Date:   Thu May 23 11:07:03 2013 -0700

    perf/x86: Reduce stack usage of x86_schedule_events()
    
    x86_schedule_events() caches event constraints on the stack during
    scheduling.  Given the number of possible events, this is 512 bytes of
    stack; since it can be invoked under schedule() under god-knows-what,
    this is causing stack blowouts.
    
    Trade some space usage for stack safety: add a place to cache the
    constraint pointer to struct perf_event.  For 8 bytes per event (1% of
    its size) we can save the giant stack frame.
    
    This shouldn't change any aspect of scheduling whatsoever and while in
    theory the locality's a tiny bit worse, I doubt we'll see any
    performance impact either.
    
    Tested: `perf stat whatever` does not blow up and produces
    results that aren't hugely obviously wrong.  I'm not sure how to run
    particularly good tests of perf code, but this should not produce any
    functional change whatsoever.
    
    Signed-off-by: Andrew Hunter <ahh@google.com>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1369332423-4400-1-git-send-email-ahh@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4bc57d017fc8..33e8d65836d6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -113,6 +113,8 @@ struct hw_perf_event_extra {
 	int		idx;	/* index in shared_regs->regs[] */
 };
 
+struct event_constraint;
+
 /**
  * struct hw_perf_event - performance event hardware details:
  */
@@ -131,6 +133,8 @@ struct hw_perf_event {
 
 			struct hw_perf_event_extra extra_reg;
 			struct hw_perf_event_extra branch_reg;
+
+			struct event_constraint *constraint;
 		};
 		struct { /* software */
 			struct hrtimer	hrtimer;

commit 03d8e80beb7db78a13c192431205b9c83f7e0cd1
Author: Mischa Jonker <Mischa.Jonker@synopsys.com>
Date:   Tue Jun 4 11:45:48 2013 +0200

    perf: Add const qualifier to perf_pmu_register's 'name' arg
    
    This allows us to use pdev->name for registering a PMU device.
    IMO the name is not supposed to be changed anyway.
    
    Signed-off-by: Mischa Jonker <mjonker@synopsys.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1370339148-5566-1-git-send-email-mjonker@synopsys.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 74a4e14ab60b..4bc57d017fc8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -188,7 +188,7 @@ struct pmu {
 
 	struct device			*dev;
 	const struct attribute_group	**attr_groups;
-	char				*name;
+	const char			*name;
 	int				type;
 
 	int * __percpu			pmu_disable_count;
@@ -519,7 +519,7 @@ struct perf_output_handle {
 
 #ifdef CONFIG_PERF_EVENTS
 
-extern int perf_pmu_register(struct pmu *pmu, char *name, int type);
+extern int perf_pmu_register(struct pmu *pmu, const char *name, int type);
 extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);

commit eff2108f020f30eb90462205ecf3ce10a420938b
Merge: afb71193a4d8 f1a527899ef0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jun 19 12:44:41 2013 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Merge in the latest fixes, to avoid conflicts with ongoing work.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 26cb63ad11e04047a64309362674bcbbd6a6f246
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 28 10:55:48 2013 +0200

    perf: Fix perf mmap bugs
    
    Vince reported a problem found by his perf specific trinity
    fuzzer.
    
    Al noticed 2 problems with perf's mmap():
    
     - it has issues against fork() since we use vma->vm_mm for accounting.
     - it has an rb refcount leak on double mmap().
    
    We fix the issues against fork() by using VM_DONTCOPY; I don't
    think there's code out there that uses this; we didn't hear
    about weird accounting problems/crashes. If we do need this to
    work, the previously proposed VM_PINNED could make this work.
    
    Aside from the rb reference leak spotted by Al, Vince's example
    prog was indeed doing a double mmap() through the use of
    perf_event_set_output().
    
    This exposes another problem, since we now have 2 events with
    one buffer, the accounting gets screwy because we account per
    event. Fix this by making the buffer responsible for its own
    accounting.
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Link: http://lkml.kernel.org/r/20130528085548.GA12193@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f463a46424e2..c5b6dbf9c2fc 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -389,8 +389,7 @@ struct perf_event {
 	/* mmap bits */
 	struct mutex			mmap_mutex;
 	atomic_t			mmap_count;
-	int				mmap_locked;
-	struct user_struct		*mmap_user;
+
 	struct ring_buffer		*rb;
 	struct list_head		rb_entry;
 

commit 62b8563979273424d6ebe9201e34d1acc133ad4f
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Apr 3 14:21:34 2013 +0200

    perf: Add sysfs entry to adjust multiplexing interval per PMU
    
    This patch adds /sys/device/xxx/perf_event_mux_interval_ms to ajust
    the multiplexing interval per PMU. The unit is milliseconds. Value has
    to be >= 1.
    
    In the 4th version, we renamed the sysfs file to be more consistent
    with the other /proc/sys/kernel entries for perf_events.
    
    In the 5th version, we handle the reprogramming of the hrtimer using
    hrtimer_forward_now(). That way, we sync up to new timer value quickly
    (suggested by Jiri Olsa).
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/r/1364991694-5876-3-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 72138d75a60a..6fddac1b27cb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -194,6 +194,7 @@ struct pmu {
 	int * __percpu			pmu_disable_count;
 	struct perf_cpu_context * __percpu pmu_cpu_context;
 	int				task_ctx_nr;
+	int				hrtimer_interval_ms;
 
 	/*
 	 * Fully disable/enable this PMU, can be used to protect from the PMI

commit 9e6302056f8029f438e853432a856b9f13de26a6
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Apr 3 14:21:33 2013 +0200

    perf: Use hrtimers for event multiplexing
    
    The current scheme of using the timer tick was fine for per-thread
    events. However, it was causing bias issues in system-wide mode
    (including for uncore PMUs). Event groups would not get their fair
    share of runtime on the PMU. With tickless kernels, if a core is idle
    there is no timer tick, and thus no event rotation (multiplexing).
    However, there are events (especially uncore events) which do count
    even though cores are asleep.
    
    This patch changes the timer source for multiplexing.  It introduces a
    per-PMU per-cpu hrtimer. The advantage is that even when a core goes
    idle, it will come back to service the hrtimer, thus multiplexing on
    system-wide events works much better.
    
    The per-PMU implementation (suggested by PeterZ) enables adjusting the
    multiplexing interval per PMU. The preferred interval is stashed into
    the struct pmu. If not set, it will be forced to the default interval
    value.
    
    In order to minimize the impact of the hrtimer, it is turned on and
    off on demand. When the PMU on a CPU is overcommited, the hrtimer is
    activated.  It is stopped when the PMU is not overcommitted.
    
    In order for this to work properly, we had to change the order of
    initialization in start_kernel() such that hrtimer_init() is run
    before perf_event_init().
    
    The default interval in milliseconds is set to a timer tick just like
    with the old code. We will provide a sysctl to tune this in another
    patch.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/r/1364991694-5876-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fa38612d70b6..72138d75a60a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -501,8 +501,9 @@ struct perf_cpu_context {
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
 	int				exclusive;
+	struct hrtimer			hrtimer;
+	ktime_t				hrtimer_interval;
 	struct list_head		rotation_list;
-	int				jiffies_interval;
 	struct pmu			*unique_pmu;
 	struct perf_cgroup		*cgrp;
 };

commit ab573844e3058eef2788803d373019f8bebead57
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed May 1 17:25:44 2013 +0200

    perf: Fix hw breakpoints overflow period sampling
    
    The hw breakpoint pmu 'add' function is missing the
    period_left update needed for SW events.
    
    The perf HW breakpoint events use the SW events framework
    to process the overflow, so it needs to be properly initialized
    in the PMU 'add' method.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Reviewed-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1367421944-19082-5-git-send-email-jolsa@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f463a46424e2..fa38612d70b6 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -743,6 +743,7 @@ extern unsigned int perf_output_skip(struct perf_output_handle *handle,
 				     unsigned int len);
 extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
+extern u64 perf_swevent_set_period(struct perf_event *event);
 extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 extern int __perf_event_disable(void *info);
@@ -782,6 +783,7 @@ static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
 static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }
 static inline void perf_swevent_put_recursion_context(int rctx)		{ }
+static inline u64 perf_swevent_set_period(struct perf_event *event)	{ return 0; }
 static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline int __perf_event_disable(void *info)			{ return -1; }

commit c032862fba51a3ca504752d3a25186b324c5ce83
Merge: fda76e074c77 8700c95adb03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 2 17:37:49 2013 +0200

    Merge commit '8700c95adb03' into timers/nohz
    
    The full dynticks tree needs the latest RCU and sched
    upstream updates in order to fix some dependencies.
    
    Merge a common upstream merge point that has these
    updates.
    
    Conflicts:
            include/linux/perf_event.h
            kernel/rcutree.h
            kernel/rcutree_plugin.h
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 026249ef100b5384b6c74c360db46728e98354da
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 15:58:34 2013 +0200

    perf: New helper to prevent full dynticks CPUs from stopping tick
    
    Provide a new helper that help full dynticks CPUs to prevent
    from stopping their tick in case there are events in the local
    rotation list.
    
    This way we make sure that perf_event_task_tick() is serviced
    on demand.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Jiri Olsa <jolsa@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ee462c2f2..0140830225e2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -799,6 +799,12 @@ static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
 #endif
 
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_NO_HZ_FULL)
+extern bool perf_event_can_stop_tick(void);
+#else
+static inline bool perf_event_can_stop_tick(void)			{ return true; }
+#endif
+
 #define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))
 
 /*

commit 529801898b24544f93532217ce18a7ebbb2b8c4f
Merge: b847d0501afe 23995bbee01d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 8 11:43:30 2013 +0200

    Merge branch 'for-tip' of git://git.kernel.org/pub/scm/linux/kernel/git/rric/oprofile into perf/core
    
    Pull IBM zEnterprise EC12 support patchlet from Robert Richter.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d6be9ad6c960f43800a6f118932bc8a5a4eadcd1
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 24 16:10:31 2013 +0100

    perf: Add generic memory sampling interface
    
    This patch adds PERF_SAMPLE_DATA_SRC.
    
    PERF_SAMPLE_DATA_SRC collects the data source, i.e., where
    did the data associated with the sampled instruction
    come from. Information is stored in a perf_mem_data_src
    structure. It contains opcode, mem level, tlb, snoop,
    lock information, subject to availability in hardware.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: ak@linux.intel.com
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-8-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7ce0b37b155b..42a6daaf4e0a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -568,6 +568,7 @@ struct perf_sample_data {
 		u32	reserved;
 	}				cpu_entry;
 	u64				period;
+	union  perf_mem_data_src	data_src;
 	struct perf_callchain_entry	*callchain;
 	struct perf_raw_record		*raw;
 	struct perf_branch_stack	*br_stack;
@@ -588,6 +589,7 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->regs_user.regs = NULL;
 	data->stack_user_size = 0;
 	data->weight = 0;
+	data->data_src.val = 0;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit c3feedf2aaf9ac8bad6f19f5d21e4ee0b4b87e9c
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Jan 24 16:10:28 2013 +0100

    perf/core: Add weighted samples
    
    For some events it's useful to weight sample with a hardware
    provided number. This expresses how expensive the action the
    sample represent was.  This allows the profiler to scale
    the samples to be more informative to the programmer.
    
    There is already the period which is used similarly, but it
    means something different, so I chose to not overload it.
    Instead a new sample type for WEIGHT is added.
    
    Can be used for multiple things. Initially it is used for TSX
    abort costs and profiling by memory latencies (so to make
    expensive load appear higher up in the histograms). The concept
    is quite generic and can be extended to many other kinds of
    events or architectures, as long as the hardware provides
    suitable auxillary values. In principle it could be also used
    for software tracepoints.
    
    This adds the generic glue. A new optional sample format for a
    64-bit weight value.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-5-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cd3bb2cd9494..7ce0b37b155b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -573,6 +573,7 @@ struct perf_sample_data {
 	struct perf_branch_stack	*br_stack;
 	struct perf_regs_user		regs_user;
 	u64				stack_user_size;
+	u64				weight;
 };
 
 static inline void perf_sample_data_init(struct perf_sample_data *data,
@@ -586,6 +587,7 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->regs_user.abi = PERF_SAMPLE_REGS_ABI_NONE;
 	data->regs_user.regs = NULL;
 	data->stack_user_size = 0;
+	data->weight = 0;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit 9fac2cf316b070ae43d2ae2525e381ff2d1d68aa
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 24 16:10:27 2013 +0100

    perf/x86: Add flags to event constraints
    
    This patch adds a flags field to each event constraint.
    It can be used to store event specific features which can
    then later be used by scheduling code or low-level x86 code.
    
    The flags are propagated into event->hw.flags during the
    get_event_constraint() call. They are cleared during the
    put_event_constraint() call.
    
    This mechanism is going to be used by the PEBS-LL patches.
    It avoids defining yet another table to hold event specific
    information.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: ak@linux.intel.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-4-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1c592114c437..cd3bb2cd9494 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -127,6 +127,7 @@ struct hw_perf_event {
 			int		event_base_rdpmc;
 			int		idx;
 			int		last_cpu;
+			int		flags;
 
 			struct hw_perf_event_extra extra_reg;
 			struct hw_perf_event_extra branch_reg;

commit 3a54aaa0a3ddb2cf2ec1b94a94024e9a8a8af962
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 24 16:10:26 2013 +0100

    perf/x86: Improve sysfs event mapping with event string
    
    This patch extends Jiri's changes to make generic
    events mapping visible via sysfs. The patch extends
    the mechanism to non-generic events by allowing
    the mappings to be hardcoded in strings.
    
    This mechanism will be used by the PEBS-LL patch
    later on.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: ak@linux.intel.com
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-3-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [ fixed up conflict with 2663960 "perf: Make EVENT_ATTR global" ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8737e1cee8b2..1c592114c437 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -809,6 +809,7 @@ do {									\
 struct perf_pmu_events_attr {
 	struct device_attribute attr;
 	u64 id;
+	const char *event_str;
 };
 
 #define PMU_EVENT_ATTR(_name, _var, _id, _show)				\

commit 6c4d3bc99b3341067775efd4d9d13cc8e655fd7c
Author: David Rientjes <rientjes@google.com>
Date:   Sun Mar 17 15:49:10 2013 -0700

    perf,x86: fix link failure for non-Intel configs
    
    Commit 1d9d8639c063 ("perf,x86: fix kernel crash with PEBS/BTS after
    suspend/resume") introduces a link failure since
    perf_restore_debug_store() is only defined for CONFIG_CPU_SUP_INTEL:
    
            arch/x86/power/built-in.o: In function `restore_processor_state':
            (.text+0x45c): undefined reference to `perf_restore_debug_store'
    
    Fix it by defining the dummy function appropriately.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 71caed8626be..1d795df6f4cf 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -758,7 +758,6 @@ extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 extern int __perf_event_disable(void *info);
 extern void perf_event_task_tick(void);
-extern void perf_restore_debug_store(void);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *prev,
@@ -798,6 +797,11 @@ static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
+#endif
+
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)
+extern void perf_restore_debug_store(void);
+#else
 static inline void perf_restore_debug_store(void)			{ }
 #endif
 

commit 1d9d8639c063caf6efc2447f5f26aa637f844ff6
Author: Stephane Eranian <eranian@google.com>
Date:   Fri Mar 15 14:26:07 2013 +0100

    perf,x86: fix kernel crash with PEBS/BTS after suspend/resume
    
    This patch fixes a kernel crash when using precise sampling (PEBS)
    after a suspend/resume. Turns out the CPU notifier code is not invoked
    on CPU0 (BP). Therefore, the DS_AREA (used by PEBS) is not restored properly
    by the kernel and keeps it power-on/resume value of 0 causing any PEBS
    measurement to crash when running on CPU0.
    
    The workaround is to add a hook in the actual resume code to restore
    the DS Area MSR value. It is invoked for all CPUS. So for all but CPU0,
    the DS_AREA will be restored twice but this is harmless.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ee462c2f2..71caed8626be 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -758,6 +758,7 @@ extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 extern int __perf_event_disable(void *info);
 extern void perf_event_task_tick(void);
+extern void perf_restore_debug_store(void);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *prev,
@@ -797,6 +798,7 @@ static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
+static inline void perf_restore_debug_store(void)			{ }
 #endif
 
 #define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))

commit 877c685607925238e302cd3aa38788dca6c1b226
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 11:38:08 2013 +0800

    perf: Remove include of cgroup.h from perf_event.h
    
    Move struct perf_cgroup_info and perf_cgroup to
    kernel/perf/core.c, and then we can remove include of cgroup.h.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/513568A0.6020804@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e47ee462c2f2..8737e1cee8b2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -21,7 +21,6 @@
  */
 
 #ifdef CONFIG_PERF_EVENTS
-# include <linux/cgroup.h>
 # include <asm/perf_event.h>
 # include <asm/local64.h>
 #endif
@@ -299,22 +298,7 @@ struct swevent_hlist {
 #define PERF_ATTACH_GROUP	0x02
 #define PERF_ATTACH_TASK	0x04
 
-#ifdef CONFIG_CGROUP_PERF
-/*
- * perf_cgroup_info keeps track of time_enabled for a cgroup.
- * This is a per-cpu dynamically allocated data structure.
- */
-struct perf_cgroup_info {
-	u64				time;
-	u64				timestamp;
-};
-
-struct perf_cgroup {
-	struct				cgroup_subsys_state css;
-	struct				perf_cgroup_info *info;	/* timing info, one per cpu */
-};
-#endif
-
+struct perf_cgroup;
 struct ring_buffer;
 
 /**

commit f22c1bb6b4706be3502b378cb14564449b15f983
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sat Feb 2 16:27:52 2013 +0100

    perf: Introduce hw_perf_event->tp_target and ->tp_list
    
    sys_perf_event_open()->perf_init_event(event) is called before
    find_get_context(event), this means that event->ctx == NULL when
    class->reg(TRACE_REG_PERF_REGISTER/OPEN) is called and thus it
    can't know if this event is per-task or system-wide.
    
    This patch adds hw_perf_event->tp_target for PERF_TYPE_TRACEPOINT,
    this is analogous to PERF_TYPE_BREAKPOINT/bp_target we already have.
    The patch also moves ->bp_target up so that it can overlap with the
    new member, this can help the compiler to generate the better code.
    
    trace_uprobe_register() will use it for prefiltering to avoid the
    unnecessary breakpoints in mm's we do not want to trace.
    
    ->tp_target doesn't have its own reference, but we can rely on the
    fact that either sys_perf_event_open() holds a reference, or it is
    equal to event->ctx->task. So this pointer is always valid until
    free_event().
    
    Also add the "struct list_head tp_list" into this union. It is not
    strictly necessary, but it can simplify the next changes and we can
    add it for free.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 42adf012145d..e47ee462c2f2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -135,16 +135,21 @@ struct hw_perf_event {
 		struct { /* software */
 			struct hrtimer	hrtimer;
 		};
+		struct { /* tracepoint */
+			struct task_struct	*tp_target;
+			/* for tp_event->class */
+			struct list_head	tp_list;
+		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 		struct { /* breakpoint */
-			struct arch_hw_breakpoint	info;
-			struct list_head		bp_list;
 			/*
 			 * Crufty hack to avoid the chicken and egg
 			 * problem hw_breakpoint has with context
 			 * creation and event initalization.
 			 */
 			struct task_struct		*bp_target;
+			struct arch_hw_breakpoint	info;
+			struct list_head		bp_list;
 		};
 #endif
 	};

commit 2663960c159f23cbfb8e196c96e9fc9f3b5f1a8d
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Jan 22 22:24:23 2013 -0800

    perf: Make EVENT_ATTR global
    
    Rename EVENT_ATTR() to PMU_EVENT_ATTR() and make it global so it is
    available to all architectures.
    
    Further to allow architectures flexibility, have PMU_EVENT_ATTR() pass
    in the variable name as a parameter.
    
    Changelog[v2]
            - [Jiri Olsa] No need to define PMU_EVENT_PTR()
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Anton Blanchard <anton@au1.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: linuxppc-dev@ozlabs.org
    Link: http://lkml.kernel.org/r/20130123062422.GC13720@us.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6bfb2faa0b19..42adf012145d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -817,6 +817,17 @@ do {									\
 } while (0)
 
 
+struct perf_pmu_events_attr {
+	struct device_attribute attr;
+	u64 id;
+};
+
+#define PMU_EVENT_ATTR(_name, _var, _id, _show)				\
+static struct perf_pmu_events_attr _var = {				\
+	.attr = __ATTR(_name, 0444, _show, NULL),			\
+	.id   =  _id,							\
+};
+
 #define PMU_FORMAT_ATTR(_name, _format)					\
 static ssize_t								\
 _name##_show(struct device *dev,					\

commit c13d38e4a1fd5dd07135403c613c8091af444169
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Oct 16 13:28:17 2012 +0530

    perf, cpu hotplug: Use cached value of smp_processor_id()
    
    The perf_cpu_notifier() macro invokes smp_processor_id()
    multiple times. Optimize it by using a local variable.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: peterz@infradead.org
    Cc: acme@ghostprotocols.net
    Link: http://lkml.kernel.org/r/20121016075817.3572.76733.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 06478056da03..6bfb2faa0b19 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -803,15 +803,16 @@ static inline void perf_event_task_tick(void)				{ }
 do {									\
 	static struct notifier_block fn##_nb __cpuinitdata =		\
 		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
+	unsigned long cpu = smp_processor_id();				\
 	unsigned long flags;						\
 	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,			\
-		(void *)(unsigned long)smp_processor_id());		\
+		(void *)(unsigned long)cpu);				\
 	local_irq_save(flags);						\
 	fn(&fn##_nb, (unsigned long)CPU_STARTING,			\
-		(void *)(unsigned long)smp_processor_id());		\
+		(void *)(unsigned long)cpu);				\
 	local_irq_restore(flags);					\
 	fn(&fn##_nb, (unsigned long)CPU_ONLINE,				\
-		(void *)(unsigned long)smp_processor_id());		\
+		(void *)(unsigned long)cpu);				\
 	register_cpu_notifier(&fn##_nb);				\
 } while (0)
 

commit 6760bca9fd16256210f4922a3e9f067d2c7017d7
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Oct 16 13:28:10 2012 +0530

    perf, cpu hotplug: Run CPU_STARTING notifiers with irqs disabled
    
    The CPU_STARTING notifiers are supposed to be run with irqs
    disabled. But the perf_cpu_notifier() macro invokes them without
    doing that. Fix it.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: peterz@infradead.org
    Cc: acme@ghostprotocols.net
    Link: http://lkml.kernel.org/r/20121016075809.3572.47848.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2e902359aee5..06478056da03 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -803,10 +803,13 @@ static inline void perf_event_task_tick(void)				{ }
 do {									\
 	static struct notifier_block fn##_nb __cpuinitdata =		\
 		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
+	unsigned long flags;						\
 	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,			\
 		(void *)(unsigned long)smp_processor_id());		\
+	local_irq_save(flags);						\
 	fn(&fn##_nb, (unsigned long)CPU_STARTING,			\
 		(void *)(unsigned long)smp_processor_id());		\
+	local_irq_restore(flags);					\
 	fn(&fn##_nb, (unsigned long)CPU_ONLINE,				\
 		(void *)(unsigned long)smp_processor_id());		\
 	register_cpu_notifier(&fn##_nb);				\

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b4166cdfa7a2..2e902359aee5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -14,605 +14,8 @@
 #ifndef _LINUX_PERF_EVENT_H
 #define _LINUX_PERF_EVENT_H
 
-#include <linux/types.h>
-#include <linux/ioctl.h>
-#include <asm/byteorder.h>
+#include <uapi/linux/perf_event.h>
 
-/*
- * User-space ABI bits:
- */
-
-/*
- * attr.type
- */
-enum perf_type_id {
-	PERF_TYPE_HARDWARE			= 0,
-	PERF_TYPE_SOFTWARE			= 1,
-	PERF_TYPE_TRACEPOINT			= 2,
-	PERF_TYPE_HW_CACHE			= 3,
-	PERF_TYPE_RAW				= 4,
-	PERF_TYPE_BREAKPOINT			= 5,
-
-	PERF_TYPE_MAX,				/* non-ABI */
-};
-
-/*
- * Generalized performance event event_id types, used by the
- * attr.event_id parameter of the sys_perf_event_open()
- * syscall:
- */
-enum perf_hw_id {
-	/*
-	 * Common hardware events, generalized by the kernel:
-	 */
-	PERF_COUNT_HW_CPU_CYCLES		= 0,
-	PERF_COUNT_HW_INSTRUCTIONS		= 1,
-	PERF_COUNT_HW_CACHE_REFERENCES		= 2,
-	PERF_COUNT_HW_CACHE_MISSES		= 3,
-	PERF_COUNT_HW_BRANCH_INSTRUCTIONS	= 4,
-	PERF_COUNT_HW_BRANCH_MISSES		= 5,
-	PERF_COUNT_HW_BUS_CYCLES		= 6,
-	PERF_COUNT_HW_STALLED_CYCLES_FRONTEND	= 7,
-	PERF_COUNT_HW_STALLED_CYCLES_BACKEND	= 8,
-	PERF_COUNT_HW_REF_CPU_CYCLES		= 9,
-
-	PERF_COUNT_HW_MAX,			/* non-ABI */
-};
-
-/*
- * Generalized hardware cache events:
- *
- *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU, NODE } x
- *       { read, write, prefetch } x
- *       { accesses, misses }
- */
-enum perf_hw_cache_id {
-	PERF_COUNT_HW_CACHE_L1D			= 0,
-	PERF_COUNT_HW_CACHE_L1I			= 1,
-	PERF_COUNT_HW_CACHE_LL			= 2,
-	PERF_COUNT_HW_CACHE_DTLB		= 3,
-	PERF_COUNT_HW_CACHE_ITLB		= 4,
-	PERF_COUNT_HW_CACHE_BPU			= 5,
-	PERF_COUNT_HW_CACHE_NODE		= 6,
-
-	PERF_COUNT_HW_CACHE_MAX,		/* non-ABI */
-};
-
-enum perf_hw_cache_op_id {
-	PERF_COUNT_HW_CACHE_OP_READ		= 0,
-	PERF_COUNT_HW_CACHE_OP_WRITE		= 1,
-	PERF_COUNT_HW_CACHE_OP_PREFETCH		= 2,
-
-	PERF_COUNT_HW_CACHE_OP_MAX,		/* non-ABI */
-};
-
-enum perf_hw_cache_op_result_id {
-	PERF_COUNT_HW_CACHE_RESULT_ACCESS	= 0,
-	PERF_COUNT_HW_CACHE_RESULT_MISS		= 1,
-
-	PERF_COUNT_HW_CACHE_RESULT_MAX,		/* non-ABI */
-};
-
-/*
- * Special "software" events provided by the kernel, even if the hardware
- * does not support performance events. These events measure various
- * physical and sw events of the kernel (and allow the profiling of them as
- * well):
- */
-enum perf_sw_ids {
-	PERF_COUNT_SW_CPU_CLOCK			= 0,
-	PERF_COUNT_SW_TASK_CLOCK		= 1,
-	PERF_COUNT_SW_PAGE_FAULTS		= 2,
-	PERF_COUNT_SW_CONTEXT_SWITCHES		= 3,
-	PERF_COUNT_SW_CPU_MIGRATIONS		= 4,
-	PERF_COUNT_SW_PAGE_FAULTS_MIN		= 5,
-	PERF_COUNT_SW_PAGE_FAULTS_MAJ		= 6,
-	PERF_COUNT_SW_ALIGNMENT_FAULTS		= 7,
-	PERF_COUNT_SW_EMULATION_FAULTS		= 8,
-
-	PERF_COUNT_SW_MAX,			/* non-ABI */
-};
-
-/*
- * Bits that can be set in attr.sample_type to request information
- * in the overflow packets.
- */
-enum perf_event_sample_format {
-	PERF_SAMPLE_IP				= 1U << 0,
-	PERF_SAMPLE_TID				= 1U << 1,
-	PERF_SAMPLE_TIME			= 1U << 2,
-	PERF_SAMPLE_ADDR			= 1U << 3,
-	PERF_SAMPLE_READ			= 1U << 4,
-	PERF_SAMPLE_CALLCHAIN			= 1U << 5,
-	PERF_SAMPLE_ID				= 1U << 6,
-	PERF_SAMPLE_CPU				= 1U << 7,
-	PERF_SAMPLE_PERIOD			= 1U << 8,
-	PERF_SAMPLE_STREAM_ID			= 1U << 9,
-	PERF_SAMPLE_RAW				= 1U << 10,
-	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
-	PERF_SAMPLE_REGS_USER			= 1U << 12,
-	PERF_SAMPLE_STACK_USER			= 1U << 13,
-
-	PERF_SAMPLE_MAX = 1U << 14,		/* non-ABI */
-};
-
-/*
- * values to program into branch_sample_type when PERF_SAMPLE_BRANCH is set
- *
- * If the user does not pass priv level information via branch_sample_type,
- * the kernel uses the event's priv level. Branch and event priv levels do
- * not have to match. Branch priv level is checked for permissions.
- *
- * The branch types can be combined, however BRANCH_ANY covers all types
- * of branches and therefore it supersedes all the other types.
- */
-enum perf_branch_sample_type {
-	PERF_SAMPLE_BRANCH_USER		= 1U << 0, /* user branches */
-	PERF_SAMPLE_BRANCH_KERNEL	= 1U << 1, /* kernel branches */
-	PERF_SAMPLE_BRANCH_HV		= 1U << 2, /* hypervisor branches */
-
-	PERF_SAMPLE_BRANCH_ANY		= 1U << 3, /* any branch types */
-	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << 4, /* any call branch */
-	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << 5, /* any return branch */
-	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << 6, /* indirect calls */
-
-	PERF_SAMPLE_BRANCH_MAX		= 1U << 7, /* non-ABI */
-};
-
-#define PERF_SAMPLE_BRANCH_PLM_ALL \
-	(PERF_SAMPLE_BRANCH_USER|\
-	 PERF_SAMPLE_BRANCH_KERNEL|\
-	 PERF_SAMPLE_BRANCH_HV)
-
-/*
- * Values to determine ABI of the registers dump.
- */
-enum perf_sample_regs_abi {
-	PERF_SAMPLE_REGS_ABI_NONE	= 0,
-	PERF_SAMPLE_REGS_ABI_32		= 1,
-	PERF_SAMPLE_REGS_ABI_64		= 2,
-};
-
-/*
- * The format of the data returned by read() on a perf event fd,
- * as specified by attr.read_format:
- *
- * struct read_format {
- *	{ u64		value;
- *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
- *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
- *	  { u64		id;           } && PERF_FORMAT_ID
- *	} && !PERF_FORMAT_GROUP
- *
- *	{ u64		nr;
- *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
- *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
- *	  { u64		value;
- *	    { u64	id;           } && PERF_FORMAT_ID
- *	  }		cntr[nr];
- *	} && PERF_FORMAT_GROUP
- * };
- */
-enum perf_event_read_format {
-	PERF_FORMAT_TOTAL_TIME_ENABLED		= 1U << 0,
-	PERF_FORMAT_TOTAL_TIME_RUNNING		= 1U << 1,
-	PERF_FORMAT_ID				= 1U << 2,
-	PERF_FORMAT_GROUP			= 1U << 3,
-
-	PERF_FORMAT_MAX = 1U << 4,		/* non-ABI */
-};
-
-#define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
-#define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
-#define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
-#define PERF_ATTR_SIZE_VER3	96	/* add: sample_regs_user */
-					/* add: sample_stack_user */
-
-/*
- * Hardware event_id to monitor via a performance monitoring event:
- */
-struct perf_event_attr {
-
-	/*
-	 * Major type: hardware/software/tracepoint/etc.
-	 */
-	__u32			type;
-
-	/*
-	 * Size of the attr structure, for fwd/bwd compat.
-	 */
-	__u32			size;
-
-	/*
-	 * Type specific configuration information.
-	 */
-	__u64			config;
-
-	union {
-		__u64		sample_period;
-		__u64		sample_freq;
-	};
-
-	__u64			sample_type;
-	__u64			read_format;
-
-	__u64			disabled       :  1, /* off by default        */
-				inherit	       :  1, /* children inherit it   */
-				pinned	       :  1, /* must always be on PMU */
-				exclusive      :  1, /* only group on PMU     */
-				exclude_user   :  1, /* don't count user      */
-				exclude_kernel :  1, /* ditto kernel          */
-				exclude_hv     :  1, /* ditto hypervisor      */
-				exclude_idle   :  1, /* don't count when idle */
-				mmap           :  1, /* include mmap data     */
-				comm	       :  1, /* include comm data     */
-				freq           :  1, /* use freq, not period  */
-				inherit_stat   :  1, /* per task counts       */
-				enable_on_exec :  1, /* next exec enables     */
-				task           :  1, /* trace fork/exit       */
-				watermark      :  1, /* wakeup_watermark      */
-				/*
-				 * precise_ip:
-				 *
-				 *  0 - SAMPLE_IP can have arbitrary skid
-				 *  1 - SAMPLE_IP must have constant skid
-				 *  2 - SAMPLE_IP requested to have 0 skid
-				 *  3 - SAMPLE_IP must have 0 skid
-				 *
-				 *  See also PERF_RECORD_MISC_EXACT_IP
-				 */
-				precise_ip     :  2, /* skid constraint       */
-				mmap_data      :  1, /* non-exec mmap data    */
-				sample_id_all  :  1, /* sample_type all events */
-
-				exclude_host   :  1, /* don't count in host   */
-				exclude_guest  :  1, /* don't count in guest  */
-
-				exclude_callchain_kernel : 1, /* exclude kernel callchains */
-				exclude_callchain_user   : 1, /* exclude user callchains */
-
-				__reserved_1   : 41;
-
-	union {
-		__u32		wakeup_events;	  /* wakeup every n events */
-		__u32		wakeup_watermark; /* bytes before wakeup   */
-	};
-
-	__u32			bp_type;
-	union {
-		__u64		bp_addr;
-		__u64		config1; /* extension of config */
-	};
-	union {
-		__u64		bp_len;
-		__u64		config2; /* extension of config1 */
-	};
-	__u64	branch_sample_type; /* enum perf_branch_sample_type */
-
-	/*
-	 * Defines set of user regs to dump on samples.
-	 * See asm/perf_regs.h for details.
-	 */
-	__u64	sample_regs_user;
-
-	/*
-	 * Defines size of the user stack to dump on samples.
-	 */
-	__u32	sample_stack_user;
-
-	/* Align to u64. */
-	__u32	__reserved_2;
-};
-
-#define perf_flags(attr)	(*(&(attr)->read_format + 1))
-
-/*
- * Ioctls that can be done on a perf event fd:
- */
-#define PERF_EVENT_IOC_ENABLE		_IO ('$', 0)
-#define PERF_EVENT_IOC_DISABLE		_IO ('$', 1)
-#define PERF_EVENT_IOC_REFRESH		_IO ('$', 2)
-#define PERF_EVENT_IOC_RESET		_IO ('$', 3)
-#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
-#define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
-#define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
-
-enum perf_event_ioc_flags {
-	PERF_IOC_FLAG_GROUP		= 1U << 0,
-};
-
-/*
- * Structure of the page that can be mapped via mmap
- */
-struct perf_event_mmap_page {
-	__u32	version;		/* version number of this structure */
-	__u32	compat_version;		/* lowest version this is compat with */
-
-	/*
-	 * Bits needed to read the hw events in user-space.
-	 *
-	 *   u32 seq, time_mult, time_shift, idx, width;
-	 *   u64 count, enabled, running;
-	 *   u64 cyc, time_offset;
-	 *   s64 pmc = 0;
-	 *
-	 *   do {
-	 *     seq = pc->lock;
-	 *     barrier()
-	 *
-	 *     enabled = pc->time_enabled;
-	 *     running = pc->time_running;
-	 *
-	 *     if (pc->cap_usr_time && enabled != running) {
-	 *       cyc = rdtsc();
-	 *       time_offset = pc->time_offset;
-	 *       time_mult   = pc->time_mult;
-	 *       time_shift  = pc->time_shift;
-	 *     }
-	 *
-	 *     idx = pc->index;
-	 *     count = pc->offset;
-	 *     if (pc->cap_usr_rdpmc && idx) {
-	 *       width = pc->pmc_width;
-	 *       pmc = rdpmc(idx - 1);
-	 *     }
-	 *
-	 *     barrier();
-	 *   } while (pc->lock != seq);
-	 *
-	 * NOTE: for obvious reason this only works on self-monitoring
-	 *       processes.
-	 */
-	__u32	lock;			/* seqlock for synchronization */
-	__u32	index;			/* hardware event identifier */
-	__s64	offset;			/* add to hardware event value */
-	__u64	time_enabled;		/* time event active */
-	__u64	time_running;		/* time event on cpu */
-	union {
-		__u64	capabilities;
-		__u64	cap_usr_time  : 1,
-			cap_usr_rdpmc : 1,
-			cap_____res   : 62;
-	};
-
-	/*
-	 * If cap_usr_rdpmc this field provides the bit-width of the value
-	 * read using the rdpmc() or equivalent instruction. This can be used
-	 * to sign extend the result like:
-	 *
-	 *   pmc <<= 64 - width;
-	 *   pmc >>= 64 - width; // signed shift right
-	 *   count += pmc;
-	 */
-	__u16	pmc_width;
-
-	/*
-	 * If cap_usr_time the below fields can be used to compute the time
-	 * delta since time_enabled (in ns) using rdtsc or similar.
-	 *
-	 *   u64 quot, rem;
-	 *   u64 delta;
-	 *
-	 *   quot = (cyc >> time_shift);
-	 *   rem = cyc & ((1 << time_shift) - 1);
-	 *   delta = time_offset + quot * time_mult +
-	 *              ((rem * time_mult) >> time_shift);
-	 *
-	 * Where time_offset,time_mult,time_shift and cyc are read in the
-	 * seqcount loop described above. This delta can then be added to
-	 * enabled and possible running (if idx), improving the scaling:
-	 *
-	 *   enabled += delta;
-	 *   if (idx)
-	 *     running += delta;
-	 *
-	 *   quot = count / running;
-	 *   rem  = count % running;
-	 *   count = quot * enabled + (rem * enabled) / running;
-	 */
-	__u16	time_shift;
-	__u32	time_mult;
-	__u64	time_offset;
-
-		/*
-		 * Hole for extension of the self monitor capabilities
-		 */
-
-	__u64	__reserved[120];	/* align to 1k */
-
-	/*
-	 * Control data for the mmap() data buffer.
-	 *
-	 * User-space reading the @data_head value should issue an rmb(), on
-	 * SMP capable platforms, after reading this value -- see
-	 * perf_event_wakeup().
-	 *
-	 * When the mapping is PROT_WRITE the @data_tail value should be
-	 * written by userspace to reflect the last read data. In this case
-	 * the kernel will not over-write unread data.
-	 */
-	__u64   data_head;		/* head in the data section */
-	__u64	data_tail;		/* user-space written tail */
-};
-
-#define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)
-#define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
-#define PERF_RECORD_MISC_KERNEL			(1 << 0)
-#define PERF_RECORD_MISC_USER			(2 << 0)
-#define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
-#define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
-#define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
-
-/*
- * Indicates that the content of PERF_SAMPLE_IP points to
- * the actual instruction that triggered the event. See also
- * perf_event_attr::precise_ip.
- */
-#define PERF_RECORD_MISC_EXACT_IP		(1 << 14)
-/*
- * Reserve the last bit to indicate some extended misc field
- */
-#define PERF_RECORD_MISC_EXT_RESERVED		(1 << 15)
-
-struct perf_event_header {
-	__u32	type;
-	__u16	misc;
-	__u16	size;
-};
-
-enum perf_event_type {
-
-	/*
-	 * If perf_event_attr.sample_id_all is set then all event types will
-	 * have the sample_type selected fields related to where/when
-	 * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)
-	 * described in PERF_RECORD_SAMPLE below, it will be stashed just after
-	 * the perf_event_header and the fields already present for the existing
-	 * fields, i.e. at the end of the payload. That way a newer perf.data
-	 * file will be supported by older perf tools, with these new optional
-	 * fields being ignored.
-	 *
-	 * The MMAP events record the PROT_EXEC mappings so that we can
-	 * correlate userspace IPs to code. They have the following structure:
-	 *
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *
-	 *	u32				pid, tid;
-	 *	u64				addr;
-	 *	u64				len;
-	 *	u64				pgoff;
-	 *	char				filename[];
-	 * };
-	 */
-	PERF_RECORD_MMAP			= 1,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *	u64				id;
-	 *	u64				lost;
-	 * };
-	 */
-	PERF_RECORD_LOST			= 2,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *
-	 *	u32				pid, tid;
-	 *	char				comm[];
-	 * };
-	 */
-	PERF_RECORD_COMM			= 3,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *	u32				pid, ppid;
-	 *	u32				tid, ptid;
-	 *	u64				time;
-	 * };
-	 */
-	PERF_RECORD_EXIT			= 4,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *	u64				time;
-	 *	u64				id;
-	 *	u64				stream_id;
-	 * };
-	 */
-	PERF_RECORD_THROTTLE			= 5,
-	PERF_RECORD_UNTHROTTLE			= 6,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *	u32				pid, ppid;
-	 *	u32				tid, ptid;
-	 *	u64				time;
-	 * };
-	 */
-	PERF_RECORD_FORK			= 7,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *	u32				pid, tid;
-	 *
-	 *	struct read_format		values;
-	 * };
-	 */
-	PERF_RECORD_READ			= 8,
-
-	/*
-	 * struct {
-	 *	struct perf_event_header	header;
-	 *
-	 *	{ u64			ip;	  } && PERF_SAMPLE_IP
-	 *	{ u32			pid, tid; } && PERF_SAMPLE_TID
-	 *	{ u64			time;     } && PERF_SAMPLE_TIME
-	 *	{ u64			addr;     } && PERF_SAMPLE_ADDR
-	 *	{ u64			id;	  } && PERF_SAMPLE_ID
-	 *	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
-	 *	{ u32			cpu, res; } && PERF_SAMPLE_CPU
-	 *	{ u64			period;   } && PERF_SAMPLE_PERIOD
-	 *
-	 *	{ struct read_format	values;	  } && PERF_SAMPLE_READ
-	 *
-	 *	{ u64			nr,
-	 *	  u64			ips[nr];  } && PERF_SAMPLE_CALLCHAIN
-	 *
-	 *	#
-	 *	# The RAW record below is opaque data wrt the ABI
-	 *	#
-	 *	# That is, the ABI doesn't make any promises wrt to
-	 *	# the stability of its content, it may vary depending
-	 *	# on event, hardware, kernel version and phase of
-	 *	# the moon.
-	 *	#
-	 *	# In other words, PERF_SAMPLE_RAW contents are not an ABI.
-	 *	#
-	 *
-	 *	{ u32			size;
-	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
-	 *
-	 *	{ u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
-	 *
-	 * 	{ u64			abi; # enum perf_sample_regs_abi
-	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER
-	 *
-	 * 	{ u64			size;
-	 * 	  char			data[size];
-	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
-	 * };
-	 */
-	PERF_RECORD_SAMPLE			= 9,
-
-	PERF_RECORD_MAX,			/* non-ABI */
-};
-
-#define PERF_MAX_STACK_DEPTH		127
-
-enum perf_callchain_context {
-	PERF_CONTEXT_HV			= (__u64)-32,
-	PERF_CONTEXT_KERNEL		= (__u64)-128,
-	PERF_CONTEXT_USER		= (__u64)-512,
-
-	PERF_CONTEXT_GUEST		= (__u64)-2048,
-	PERF_CONTEXT_GUEST_KERNEL	= (__u64)-2176,
-	PERF_CONTEXT_GUEST_USER		= (__u64)-2560,
-
-	PERF_CONTEXT_MAX		= (__u64)-4095,
-};
-
-#define PERF_FLAG_FD_NO_GROUP		(1U << 0)
-#define PERF_FLAG_FD_OUTPUT		(1U << 1)
-#define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
-
-#ifdef __KERNEL__
 /*
  * Kernel-internal data types and definitions:
  */
@@ -1422,5 +825,4 @@ _name##_show(struct device *dev,					\
 									\
 static struct device_attribute format_attr_##_name = __ATTR_RO(_name)
 
-#endif /* __KERNEL__ */
 #endif /* _LINUX_PERF_EVENT_H */

commit 3f1f33206c16c7b3839d71372bc2ac3f305aa802
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 2 15:38:52 2012 +0200

    perf: Clarify perf_cpu_context::active_pmu usage by renaming it to ::unique_pmu
    
    Stephane thought the perf_cpu_context::active_pmu name confusing and
    suggested using 'unique_pmu' instead.
    
    This pointer is a pointer to a 'random' pmu sharing the cpuctx
    instance, therefore limiting a for_each_pmu loop to those where
    cpuctx->unique_pmu matches the pmu we get a loop over unique cpuctx
    instances.
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-kxyjqpfj2fn9gt7kwu5ag9ks@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 599afc4bb67e..b4166cdfa7a2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1110,7 +1110,7 @@ struct perf_cpu_context {
 	int				exclusive;
 	struct list_head		rotation_list;
 	int				jiffies_interval;
-	struct pmu			*active_pmu;
+	struct pmu			*unique_pmu;
 	struct perf_cgroup		*cgrp;
 };
 

commit 7e92daaefa68e5ef1e1732e45231e73adbb724e7
Merge: 7a68294278ae 1d787d37c8ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:28:49 2012 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf update from Ingo Molnar:
     "Lots of changes in this cycle as well, with hundreds of commits from
      over 30 contributors.  Most of the activity was on the tooling side.
    
      Higher level changes:
    
       - New 'perf kvm' analysis tool, from Xiao Guangrong.
    
       - New 'perf trace' system-wide tracing tool
    
       - uprobes fixes + cleanups from Oleg Nesterov.
    
       - Lots of patches to make perf build on Android out of box, from
         Irina Tirdea
    
       - Extend ftrace function tracing utility to be more dynamic for its
         users.  It allows for data passing to the callback functions, as
         well as reading regs as if a breakpoint were to trigger at function
         entry.
    
         The main goal of this patch series was to allow kprobes to use
         ftrace as an optimized probe point when a probe is placed on an
         ftrace nop.  With lots of help from Masami Hiramatsu, and going
         through lots of iterations, we finally came up with a good
         solution.
    
       - Add cpumask for uncore pmu, use it in 'stat', from Yan, Zheng.
    
       - Various tracing updates from Steve Rostedt
    
       - Clean up and improve 'perf sched' performance by elliminating lots
         of needless calls to libtraceevent.
    
       - Event group parsing support, from Jiri Olsa
    
       - UI/gtk refactorings and improvements from Namhyung Kim
    
       - Add support for non-tracepoint events in perf script python, from
         Feng Tang
    
       - Add --symbols to 'script', similar to the one in 'report', from
         Feng Tang.
    
      Infrastructure enhancements and fixes:
    
       - Convert the trace builtins to use the growing evsel/evlist
         tracepoint infrastructure, removing several open coded constructs
         like switch like series of strcmp to dispatch events, etc.
         Basically what had already been showcased in 'perf sched'.
    
       - Add evsel constructor for tracepoints, that uses libtraceevent just
         to parse the /format events file, use it in a new 'perf test' to
         make sure the libtraceevent format parsing regressions can be more
         readily caught.
    
       - Some strange errors were happening in some builds, but not on the
         next, reported by several people, problem was some parser related
         files, generated during the build, didn't had proper make deps, fix
         from Eric Sandeen.
    
       - Introduce struct and cache information about the environment where
         a perf.data file was captured, from Namhyung Kim.
    
       - Fix handling of unresolved samples when --symbols is used in
         'report', from Feng Tang.
    
       - Add union member access support to 'probe', from Hyeoncheol Lee.
    
       - Fixups to die() removal, from Namhyung Kim.
    
       - Render fixes for the TUI, from Namhyung Kim.
    
       - Don't enable annotation in non symbolic view, from Namhyung Kim.
    
       - Fix pipe mode in 'report', from Namhyung Kim.
    
       - Move related stats code from stat to util/, will be used by the
         'stat' kvm tool, from Xiao Guangrong.
    
       - Remove die()/exit() calls from several tools.
    
       - Resolve vdso callchains, from Jiri Olsa
    
       - Don't pass const char pointers to basename, so that we can
         unconditionally use libgen.h and thus avoid ifdef BIONIC lines,
         from David Ahern
    
       - Refactor hist formatting so that it can be reused with the GTK
         browser, From Namhyung Kim
    
       - Fix build for another rbtree.c change, from Adrian Hunter.
    
       - Make 'perf diff' command work with evsel hists, from Jiri Olsa.
    
       - Use the only field_sep var that is set up: symbol_conf.field_sep,
         fix from Jiri Olsa.
    
       - .gitignore compiled python binaries, from Namhyung Kim.
    
       - Get rid of die() in more libtraceevent places, from Namhyung Kim.
    
       - Rename libtraceevent 'private' struct member to 'priv' so that it
         works in C++, from Steven Rostedt
    
       - Remove lots of exit()/die() calls from tools so that the main perf
         exit routine can take place, from David Ahern
    
       - Fix x86 build on x86-64, from David Ahern.
    
       - {int,str,rb}list fixes from Suzuki K Poulose
    
       - perf.data header fixes from Namhyung Kim
    
       - Allow user to indicate objdump path, needed in cross environments,
         from Maciek Borzecki
    
       - Fix hardware cache event name generation, fix from Jiri Olsa
    
       - Add round trip test for sw, hw and cache event names, catching the
         problem Jiri fixed, after Jiri's patch, the test passes
         successfully.
    
       - Clean target should do clean for lib/traceevent too, fix from David
         Ahern
    
       - Check the right variable for allocation failure, fix from Namhyung
         Kim
    
       - Set up evsel->tp_format regardless of evsel->name being set
         already, fix from Namhyung Kim
    
       - Oprofile fixes from Robert Richter.
    
       - Remove perf_event_attr needless version inflation, from Jiri Olsa
    
       - Introduce libtraceevent strerror like error reporting facility,
         from Namhyung Kim
    
       - Add pmu mappings to perf.data header and use event names from cmd
         line, from Robert Richter
    
       - Fix include order for bison/flex-generated C files, from Ben
         Hutchings
    
       - Build fixes and documentation corrections from David Ahern
    
       - Assorted cleanups from Robert Richter
    
       - Let O= makes handle relative paths, from Steven Rostedt
    
       - perf script python fixes, from Feng Tang.
    
       - Initial bash completion support, from Frederic Weisbecker
    
       - Allow building without libelf, from Namhyung Kim.
    
       - Support DWARF CFI based unwind to have callchains when %bp based
         unwinding is not possible, from Jiri Olsa.
    
       - Symbol resolution fixes, while fixing support PPC64 files with an
         .opt ELF section was the end goal, several fixes for code that
         handles all architectures and cleanups are included, from Cody
         Schafer.
    
       - Assorted fixes for Documentation and build in 32 bit, from Robert
         Richter
    
       - Cache the libtraceevent event_format associated to each evsel
         early, so that we avoid relookups, i.e.  calling pevent_find_event
         repeatedly when processing tracepoint events.
    
         [ This is to reduce the surface contact with libtraceevents and
            make clear what is that the perf tools needs from that lib: so
            far parsing the common and per event fields.  ]
    
       - Don't stop the build if the audit libraries are not installed, fix
         from Namhyung Kim.
    
       - Fix bfd.h/libbfd detection with recent binutils, from Markus
         Trippelsdorf.
    
       - Improve warning message when libunwind devel packages not present,
         from Jiri Olsa"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (282 commits)
      perf trace: Add aliases for some syscalls
      perf probe: Print an enum type variable in "enum variable-name" format when showing accessible variables
      perf tools: Check libaudit availability for perf-trace builtin
      perf hists: Add missing period_* fields when collapsing a hist entry
      perf trace: New tool
      perf evsel: Export the event_format constructor
      perf evsel: Introduce rawptr() method
      perf tools: Use perf_evsel__newtp in the event parser
      perf evsel: The tracepoint constructor should store sys:name
      perf evlist: Introduce set_filter() method
      perf evlist: Renane set_filters method to apply_filters
      perf test: Add test to check we correctly parse and match syscall open parms
      perf evsel: Handle endianity in intval method
      perf evsel: Know if byte swap is needed
      perf tools: Allow handling a NULL cpu_map as meaning "all cpus"
      perf evsel: Improve tracepoint constructor setup
      tools lib traceevent: Fix error path on pevent_parse_event
      perf test: Fix build failure
      trace: Move trace event enable from fs_initcall to core_initcall
      tracing: Add an option for disabling markers
      ...

commit bad9ac2d7f878a31cf1ae8c1ee3768077d222bcb
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jul 25 19:12:45 2012 +0200

    perf/x86/ibs: Check syscall attribute flags
    
    Current implementation simply ignores attribute flags. Thus, there is
    no notification to userland of unsupported features. Check syscall's
    attribute flags to let userland know if a feature is supported by the
    kernel. This is also needed to distinguish between future kernels what
    might support a feature.
    
    Cc: <stable@vger.kernel.org> v3.5..
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120910093018.GO8285@erda.amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 33ed9d605f91..bdb41612bfec 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -274,6 +274,8 @@ struct perf_event_attr {
 	__u64	branch_sample_type; /* enum branch_sample_type */
 };
 
+#define perf_flags(attr)	(*(&(attr)->read_format + 1))
+
 /*
  * Ioctls that can be done on a perf event fd:
  */

commit 500ad2d8b01390c98bc6dce068bccfa9534b8212
Author: K.Prasad <Prasad.Krishnan@gmail.com>
Date:   Thu Aug 2 13:46:35 2012 +0530

    perf/hwpb: Invoke __perf_event_disable() if interrupts are already disabled
    
    While debugging a warning message on PowerPC while using hardware
    breakpoints, it was discovered that when perf_event_disable is invoked
    through hw_breakpoint_handler function with interrupts disabled, a
    subsequent IPI in the code path would trigger a WARN_ON_ONCE message in
    smp_call_function_single function.
    
    This patch calls __perf_event_disable() when interrupts are already
    disabled, instead of perf_event_disable().
    
    Reported-by: Edjunior Barbosa Machado <emachado@linux.vnet.ibm.com>
    Signed-off-by: K.Prasad <Prasad.Krishnan@gmail.com>
    [naveen.n.rao@linux.vnet.ibm.com: v3: Check to make sure we target current task]
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120802081635.5811.17737.stgit@localhost.localdomain
    [ Fixed build error on MIPS. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ad04dfcd6f35..33ed9d605f91 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1296,6 +1296,7 @@ extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
 extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
+extern int __perf_event_disable(void *info);
 extern void perf_event_task_tick(void);
 #else
 static inline void
@@ -1334,6 +1335,7 @@ static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }
 static inline void perf_swevent_put_recursion_context(int rctx)		{ }
 static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
+static inline int __perf_event_disable(void *info)			{ return -1; }
 static inline void perf_event_task_tick(void)				{ }
 #endif
 

commit a6fa941d94b411bbd2b6421ffbde6db3c93e65ab
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Mon Aug 20 14:59:25 2012 +0100

    perf_event: Switch to internal refcount, fix race with close()
    
    Don't mess with file refcounts (or keep a reference to file, for
    that matter) in perf_event.  Use explicit refcount of its own
    instead.  Deal with the race between the final reference to event
    going away and new children getting created for it by use of
    atomic_long_inc_not_zero() in inherit_event(); just have the
    latter free what it had allocated and return NULL, that works
    out just fine (children of siblings of something doomed are
    created as singletons, same as if the child of leader had been
    created and immediately killed).
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: stable@kernel.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120820135925.GG23464@ZenIV.linux.org.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7602ccb3f40e..ad04dfcd6f35 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -926,7 +926,7 @@ struct perf_event {
 	struct hw_perf_event		hw;
 
 	struct perf_event_context	*ctx;
-	struct file			*filp;
+	atomic_long_t			refcount;
 
 	/*
 	 * These accumulate total time (in nanoseconds) that children

commit 1659d129ed014b715b0b2120e6fd929bdd33ed03
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Aug 22 10:35:40 2012 +0200

    perf tools: Keep the perf_event_attr on version 3
    
    Stashing version 4 under version 3 and removing version 4, because both
    version changes were within single patchset.
    
    Reported-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/20120822083540.GB1003@krava.brq.redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 297ca3db6b4a..28f9cee3fbc3 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -205,8 +205,8 @@ enum perf_event_read_format {
 #define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
 #define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
 #define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
-#define PERF_ATTR_SIZE_VER3	88	/* add: sample_regs_user */
-#define PERF_ATTR_SIZE_VER4	96	/* add: sample_stack_user */
+#define PERF_ATTR_SIZE_VER3	96	/* add: sample_regs_user */
+					/* add: sample_stack_user */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:

commit d077526485d5c9b12fe85d0b2b3b7041e6bc5f91
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Aug 7 15:20:41 2012 +0200

    perf: Add attribute to filter out callchains
    
    Introducing following bits to the the perf_event_attr struct:
    
      - exclude_callchain_kernel to filter out kernel callchain
        from the sample dump
    
      - exclude_callchain_user to filter out user callchain
        from the sample dump
    
    We need to be able to disable standard user callchain dump when we use
    the dwarf cfi callchain mode, because frame pointer based user
    callchains are useless in this mode.
    
    Implementing also exclude_callchain_kernel to have complete set of
    options.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    [ Added kernel callchains filtering ]
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-7-git-send-email-jolsa@redhat.com
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d1d25f6a5e24..297ca3db6b4a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -268,7 +268,10 @@ struct perf_event_attr {
 				exclude_host   :  1, /* don't count in host   */
 				exclude_guest  :  1, /* don't count in guest  */
 
-				__reserved_1   : 43;
+				exclude_callchain_kernel : 1, /* exclude kernel callchains */
+				exclude_callchain_user   : 1, /* exclude user callchains */
+
+				__reserved_1   : 41;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */

commit c5ebcedb566ef17bda7b02686e0d658a7bb42ee7
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Aug 7 15:20:40 2012 +0200

    perf: Add ability to attach user stack dump to sample
    
    Introducing PERF_SAMPLE_STACK_USER sample type bit to trigger the dump
    of the user level stack on sample. The size of the dump is specified by
    sample_stack_user value.
    
    Being able to dump parts of the user stack, starting from the stack
    pointer, will be useful to make a post mortem dwarf CFI based stack
    unwinding.
    
    Added HAVE_PERF_USER_STACK_DUMP config option to determine if the
    architecture provides user stack dump on perf event samples.  This needs
    access to the user stack pointer which is not unified across
    architectures. Enabling this for x86 architecture.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Original-patch-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-6-git-send-email-jolsa@redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8a73f75beb16..d1d25f6a5e24 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -131,8 +131,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_RAW				= 1U << 10,
 	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
 	PERF_SAMPLE_REGS_USER			= 1U << 12,
+	PERF_SAMPLE_STACK_USER			= 1U << 13,
 
-	PERF_SAMPLE_MAX = 1U << 13,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 14,		/* non-ABI */
 };
 
 /*
@@ -205,6 +206,7 @@ enum perf_event_read_format {
 #define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
 #define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
 #define PERF_ATTR_SIZE_VER3	88	/* add: sample_regs_user */
+#define PERF_ATTR_SIZE_VER4	96	/* add: sample_stack_user */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
@@ -289,6 +291,14 @@ struct perf_event_attr {
 	 * See asm/perf_regs.h for details.
 	 */
 	__u64	sample_regs_user;
+
+	/*
+	 * Defines size of the user stack to dump on samples.
+	 */
+	__u32	sample_stack_user;
+
+	/* Align to u64. */
+	__u32	__reserved_2;
 };
 
 /*
@@ -568,6 +578,10 @@ enum perf_event_type {
 	 *
 	 * 	{ u64			abi; # enum perf_sample_regs_abi
 	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER
+	 *
+	 * 	{ u64			size;
+	 * 	  char			data[size];
+	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,
@@ -1160,6 +1174,7 @@ struct perf_sample_data {
 	struct perf_raw_record		*raw;
 	struct perf_branch_stack	*br_stack;
 	struct perf_regs_user		regs_user;
+	u64				stack_user_size;
 };
 
 static inline void perf_sample_data_init(struct perf_sample_data *data,
@@ -1172,6 +1187,7 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->period = period;
 	data->regs_user.abi = PERF_SAMPLE_REGS_ABI_NONE;
 	data->regs_user.regs = NULL;
+	data->stack_user_size = 0;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit 5685e0ff45f5df67e79e9b052b6ffd501ff38c11
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Aug 7 15:20:39 2012 +0200

    perf: Add perf_output_skip function to skip bytes in sample
    
    Introducing perf_output_skip function to be able to skip data within the
    perf ring buffer.
    
    When writing data into perf ring buffer we first reserve needed place in
    ring buffer and then copy the actual data.
    
    There's a possibility we won't be able to fill all the reserved size
    with data, so we need a way to skip the remaining bytes.
    
    This is going to be useful when storing the user stack dump, where we
    might end up with less data than we originally requested.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-5-git-send-email-jolsa@redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d41394a1af36..8a73f75beb16 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1321,6 +1321,8 @@ extern int perf_output_begin(struct perf_output_handle *handle,
 extern void perf_output_end(struct perf_output_handle *handle);
 extern unsigned int perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
+extern unsigned int perf_output_skip(struct perf_output_handle *handle,
+				     unsigned int len);
 extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
 extern void perf_event_enable(struct perf_event *event);

commit 91d7753a45f8525dc75b6be01e427dc1c378dc16
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Aug 7 15:20:38 2012 +0200

    perf: Factor __output_copy to be usable with specific copy function
    
    Adding a generic way to use __output_copy function with specific copy
    function via DEFINE_PERF_OUTPUT_COPY macro.
    
    Using this to add new __output_copy_user function, that provides output
    copy from user pointers. For x86 the copy_from_user_nmi function is used
    and __copy_from_user_inatomic for the rest of the architectures.
    
    This new function will be used in user stack dump on sample, coming in
    next patches.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-4-git-send-email-jolsa@redhat.com
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3d4d84745f07..d41394a1af36 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1319,7 +1319,7 @@ static inline bool has_branch_stack(struct perf_event *event)
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
 extern void perf_output_end(struct perf_output_handle *handle);
-extern void perf_output_copy(struct perf_output_handle *handle,
+extern unsigned int perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
 extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);

commit 4018994f3d8785275ef0e7391b75c3462c029e56
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue Aug 7 15:20:37 2012 +0200

    perf: Add ability to attach user level registers dump to sample
    
    Introducing PERF_SAMPLE_REGS_USER sample type bit to trigger the dump of
    user level registers on sample. Registers we want to dump are specified
    by sample_regs_user bitmask.
    
    Only user level registers are dumped at the moment. Meaning the register
    values of the user space context as it was before the user entered the
    kernel for whatever reason (syscall, irq, exception, or a PMI happening
    in userspace).
    
    The layout of the sample_regs_user bitmap is described in
    asm/perf_regs.h for archs that support register dump.
    
    This is going to be useful to bring Dwarf CFI based stack unwinding on
    top of samples.
    
    Original-patch-by: Frederic Weisbecker <fweisbec@gmail.com>
    [ Dump registers ABI specification. ]
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Suggested-by: Stephane Eranian <eranian@google.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Arun Sharma <asharma@fb.com>
    Cc: Benjamin Redelings <benjamin.redelings@nescent.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Ulrich Drepper <drepper@gmail.com>
    Link: http://lkml.kernel.org/r/1344345647-11536-3-git-send-email-jolsa@redhat.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7602ccb3f40e..3d4d84745f07 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -130,8 +130,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_STREAM_ID			= 1U << 9,
 	PERF_SAMPLE_RAW				= 1U << 10,
 	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
+	PERF_SAMPLE_REGS_USER			= 1U << 12,
 
-	PERF_SAMPLE_MAX = 1U << 12,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 13,		/* non-ABI */
 };
 
 /*
@@ -162,6 +163,15 @@ enum perf_branch_sample_type {
 	 PERF_SAMPLE_BRANCH_KERNEL|\
 	 PERF_SAMPLE_BRANCH_HV)
 
+/*
+ * Values to determine ABI of the registers dump.
+ */
+enum perf_sample_regs_abi {
+	PERF_SAMPLE_REGS_ABI_NONE	= 0,
+	PERF_SAMPLE_REGS_ABI_32		= 1,
+	PERF_SAMPLE_REGS_ABI_64		= 2,
+};
+
 /*
  * The format of the data returned by read() on a perf event fd,
  * as specified by attr.read_format:
@@ -194,6 +204,7 @@ enum perf_event_read_format {
 #define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
 #define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
 #define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
+#define PERF_ATTR_SIZE_VER3	88	/* add: sample_regs_user */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
@@ -271,7 +282,13 @@ struct perf_event_attr {
 		__u64		bp_len;
 		__u64		config2; /* extension of config1 */
 	};
-	__u64	branch_sample_type; /* enum branch_sample_type */
+	__u64	branch_sample_type; /* enum perf_branch_sample_type */
+
+	/*
+	 * Defines set of user regs to dump on samples.
+	 * See asm/perf_regs.h for details.
+	 */
+	__u64	sample_regs_user;
 };
 
 /*
@@ -548,6 +565,9 @@ enum perf_event_type {
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
 	 *
 	 *	{ u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
+	 *
+	 * 	{ u64			abi; # enum perf_sample_regs_abi
+	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,
@@ -609,6 +629,7 @@ struct perf_guest_info_callbacks {
 #include <linux/static_key.h>
 #include <linux/atomic.h>
 #include <linux/sysfs.h>
+#include <linux/perf_regs.h>
 #include <asm/local.h>
 
 struct perf_callchain_entry {
@@ -654,6 +675,11 @@ struct perf_branch_stack {
 	struct perf_branch_entry	entries[0];
 };
 
+struct perf_regs_user {
+	__u64		abi;
+	struct pt_regs	*regs;
+};
+
 struct task_struct;
 
 /*
@@ -1133,6 +1159,7 @@ struct perf_sample_data {
 	struct perf_callchain_entry	*callchain;
 	struct perf_raw_record		*raw;
 	struct perf_branch_stack	*br_stack;
+	struct perf_regs_user		regs_user;
 };
 
 static inline void perf_sample_data_init(struct perf_sample_data *data,
@@ -1142,7 +1169,9 @@ static inline void perf_sample_data_init(struct perf_sample_data *data,
 	data->addr = addr;
 	data->raw  = NULL;
 	data->br_stack = NULL;
-	data->period	= period;
+	data->period = period;
+	data->regs_user.abi = PERF_SAMPLE_REGS_ABI_NONE;
+	data->regs_user.regs = NULL;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit e6dab5ffab59e910ec0e3355f4a6f29f7a7be474
Author: Andrew Vagin <avagin@openvz.org>
Date:   Wed Jul 11 18:14:58 2012 +0400

    perf/trace: Add ability to set a target task for events
    
    A few events are interesting not only for a current task.
    For example, sched_stat_* events are interesting for a task
    which wakes up. For this reason, it will be good if such
    events will be delivered to a target task too.
    
    Now a target task can be set by using __perf_task().
    
    The original idea and a draft patch belongs to Peter Zijlstra.
    
    I need these events for profiling sleep times. sched_switch is used for
    getting callchains and sched_stat_* is used for getting time periods.
    These events are combined in user space, then it can be analyzed by
    perf tools.
    
    Inspired-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arun Sharma <asharma@fb.com>
    Signed-off-by: Andrew Vagin <avagin@openvz.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1342016098-213063-1-git-send-email-avagin@openvz.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 76c5c8b724a7..7602ccb3f40e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1272,7 +1272,8 @@ static inline bool perf_paranoid_kernel(void)
 extern void perf_event_init(void);
 extern void perf_tp_event(u64 addr, u64 count, void *record,
 			  int entry_size, struct pt_regs *regs,
-			  struct hlist_head *head, int rctx);
+			  struct hlist_head *head, int rctx,
+			  struct task_struct *task);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit 0cda4c023132aa93f2dd94811061f812e88daf4c
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Fri Jun 15 14:31:33 2012 +0800

    perf: Introduce perf_pmu_migrate_context()
    
    Originally from Peter Zijlstra. The helper migrates perf events
    from one cpu to another cpu.
    
    Signed-off-by: Zheng Yan <zheng.z.yan@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1339741902-8449-5-git-send-email-zheng.z.yan@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1ce887abcc5c..76c5c8b724a7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1107,6 +1107,8 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				struct task_struct *task,
 				perf_overflow_handler_t callback,
 				void *context);
+extern void perf_pmu_migrate_context(struct pmu *pmu,
+				int src_cpu, int dst_cpu);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 

commit c48b60538c3ba05a7a2713c4791b25405525431b
Author: Vince Weaver <vweaver1@eecs.utk.edu>
Date:   Thu Mar 1 17:28:14 2012 -0500

    perf/x86: Use rdpmc() rather than rdmsr() when possible in the kernel
    
    The rdpmc instruction is faster than the equivelant rdmsr call,
    so use it when possible in the kernel.
    
    The perfctr kernel patches did this, after extensive testing showed
    rdpmc to always be faster (One can look in etc/costs in the perfctr-2.6
    package to see a historical list of the overhead).
    
    I have done some tests on a 3.2 kernel, the kernel module I used
    was included in the first posting of this patch:
    
                       rdmsr           rdpmc
     Core2 T9900:      203.9 cycles     30.9 cycles
     AMD fam0fh:        56.2 cycles      9.8 cycles
     Atom 6/28/2:      129.7 cycles     50.6 cycles
    
    The speedup of using rdpmc is large.
    
    [ It's probably possible (and desirable) to do this without
      requiring a new field in the hw_perf_event structure, but
      the fixed events make this tricky. ]
    
    Signed-off-by: Vince Weaver <vweaver1@eecs.utk.edu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.00.1203011724030.26934@cl320.eecs.utk.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 45db49f64bb4..1ce887abcc5c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -677,6 +677,7 @@ struct hw_perf_event {
 			u64		last_tag;
 			unsigned long	config_base;
 			unsigned long	event_base;
+			int		event_base_rdpmc;
 			int		idx;
 			int		last_cpu;
 

commit 0b0d9cf6ec7bab91977da2d71c09157f110f7c2e
Author: Arun Sharma <asharma@fb.com>
Date:   Fri Apr 20 15:41:34 2012 -0700

    perf: Limit callchains to 127
    
    Stack depth of 255 seems excessive, given that copy_from_user_nmi()
    could be slow.
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1334961696-19580-3-git-send-email-asharma@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1817d4015e5f..45db49f64bb4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -555,7 +555,7 @@ enum perf_event_type {
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 
-#define PERF_MAX_STACK_DEPTH		255
+#define PERF_MAX_STACK_DEPTH		127
 
 enum perf_callchain_context {
 	PERF_CONTEXT_HV			= (__u64)-32,

commit 114067b69e7b2c691faace0e33db2f04096f668d
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Thu May 31 14:43:27 2012 +0900

    perf tools: Check if callchain is corrupted
    
    We faced segmentation fault on perf top -G at very high sampling rate
    due to a corrupted callchain. While the root cause was not revealed (I
    failed to figure it out), this patch tries to protect us from the
    segfault on such cases.
    
    Reported-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Sunjin Yang <fan4326@gmail.com>
    Link: http://lkml.kernel.org/r/1338443007-24857-2-git-send-email-namhyung.kim@lge.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f32578634d9d..1817d4015e5f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -555,6 +555,8 @@ enum perf_event_type {
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 
+#define PERF_MAX_STACK_DEPTH		255
+
 enum perf_callchain_context {
 	PERF_CONTEXT_HV			= (__u64)-32,
 	PERF_CONTEXT_KERNEL		= (__u64)-128,
@@ -609,8 +611,6 @@ struct perf_guest_info_callbacks {
 #include <linux/sysfs.h>
 #include <asm/local.h>
 
-#define PERF_MAX_STACK_DEPTH		255
-
 struct perf_callchain_entry {
 	__u64				nr;
 	__u64				ip[PERF_MAX_STACK_DEPTH];

commit ab0cce560ef177bdc7a8f73e9962be9d829a7b2c
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed May 23 13:13:02 2012 +0200

    Revert "sched, perf: Use a single callback into the scheduler"
    
    This reverts commit cb04ff9ac424 ("sched, perf: Use a single
    callback into the scheduler").
    
    Before this change was introduced, the process switch worked
    like this (wrt. to perf event schedule):
    
         schedule (prev, next)
           - schedule out all perf events for prev
           - switch to next
           - schedule in all perf events for current (next)
    
    After the commit, the process switch looks like:
    
         schedule (prev, next)
           - schedule out all perf events for prev
           - schedule in all perf events for (next)
           - switch to next
    
    The problem is, that after we schedule perf events in, the pmu
    is enabled and we can receive events even before we make the
    switch to next - so "current" still being prev process (event
    SAMPLE data are filled based on the value of the "current"
    process).
    
    Thats exactly what we see for test__PERF_RECORD test. We receive
    SAMPLES with PID of the process that our tracee is scheduled
    from.
    
    Discussed with Peter Zijlstra:
    
     > Bah!, yeah I guess reverting is the right thing for now. Sad
     > though.
     >
     > So by having the two hooks we have a black-spot between them
     > where we receive no events at all, this black-spot covers the
     > hand-over of current and we thus don't receive the 'wrong'
     > events.
     >
     > I rather liked we could do away with both that black-spot and
     > clean up the code a little, but apparently people rely on it.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: acme@redhat.com
    Cc: paulus@samba.org
    Cc: cjashfor@linux.vnet.ibm.com
    Cc: fweisbec@gmail.com
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/20120523111302.GC1638@m.brq.redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8adf70e9e3cc..f32578634d9d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1084,8 +1084,10 @@ extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);
 extern const char *perf_pmu_name(void);
-extern void __perf_event_task_sched(struct task_struct *prev,
-				    struct task_struct *next);
+extern void __perf_event_task_sched_in(struct task_struct *prev,
+				       struct task_struct *task);
+extern void __perf_event_task_sched_out(struct task_struct *prev,
+					struct task_struct *next);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -1205,13 +1207,20 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 
 extern struct static_key_deferred perf_sched_events;
 
-static inline void perf_event_task_sched(struct task_struct *prev,
+static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
+{
+	if (static_key_false(&perf_sched_events.key))
+		__perf_event_task_sched_in(prev, task);
+}
+
+static inline void perf_event_task_sched_out(struct task_struct *prev,
+					     struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
 	if (static_key_false(&perf_sched_events.key))
-		__perf_event_task_sched(prev, task);
+		__perf_event_task_sched_out(prev, next);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);
@@ -1286,8 +1295,11 @@ extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_task_tick(void);
 #else
 static inline void
-perf_event_task_sched(struct task_struct *prev,
-		      struct task_struct *task)				{ }
+perf_event_task_sched_in(struct task_struct *prev,
+			 struct task_struct *task)			{ }
+static inline void
+perf_event_task_sched_out(struct task_struct *prev,
+			  struct task_struct *next)			{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }

commit cb04ff9ac424d0e689d9b612e9f73cb443ab4b7e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 8 18:56:04 2012 +0200

    sched, perf: Use a single callback into the scheduler
    
    We can easily use a single callback for both sched-in and sched-out. This
    reduces the code footprint in the scheduler path as well as removes
    the PMU black spot otherwise present between the out and in callback.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-o56ajxp1edwqg6x9d31wb805@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f32578634d9d..8adf70e9e3cc 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1084,10 +1084,8 @@ extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);
 extern const char *perf_pmu_name(void);
-extern void __perf_event_task_sched_in(struct task_struct *prev,
-				       struct task_struct *task);
-extern void __perf_event_task_sched_out(struct task_struct *prev,
-					struct task_struct *next);
+extern void __perf_event_task_sched(struct task_struct *prev,
+				    struct task_struct *next);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -1207,20 +1205,13 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 
 extern struct static_key_deferred perf_sched_events;
 
-static inline void perf_event_task_sched_in(struct task_struct *prev,
+static inline void perf_event_task_sched(struct task_struct *prev,
 					    struct task_struct *task)
-{
-	if (static_key_false(&perf_sched_events.key))
-		__perf_event_task_sched_in(prev, task);
-}
-
-static inline void perf_event_task_sched_out(struct task_struct *prev,
-					     struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
 	if (static_key_false(&perf_sched_events.key))
-		__perf_event_task_sched_out(prev, next);
+		__perf_event_task_sched(prev, task);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);
@@ -1295,11 +1286,8 @@ extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_task_tick(void);
 #else
 static inline void
-perf_event_task_sched_in(struct task_struct *prev,
-			 struct task_struct *task)			{ }
-static inline void
-perf_event_task_sched_out(struct task_struct *prev,
-			  struct task_struct *next)			{ }
+perf_event_task_sched(struct task_struct *prev,
+		      struct task_struct *task)				{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }

commit fd0d000b2c34aa43d4e92dcf0dfaeda7e123008a
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Apr 2 20:19:08 2012 +0200

    perf: Pass last sampling period to perf_sample_data_init()
    
    We always need to pass the last sample period to
    perf_sample_data_init(), otherwise the event distribution will be
    wrong. Thus, modifiyng the function interface with the required period
    as argument. So basically a pattern like this:
    
            perf_sample_data_init(&data, ~0ULL);
            data.period = event->hw.last_period;
    
    will now be like that:
    
            perf_sample_data_init(&data, ~0ULL, event->hw.last_period);
    
    Avoids unininitialized data.period and simplifies code.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1333390758-10893-3-git-send-email-robert.richter@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ddbb6a901f65..f32578634d9d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1132,11 +1132,14 @@ struct perf_sample_data {
 	struct perf_branch_stack	*br_stack;
 };
 
-static inline void perf_sample_data_init(struct perf_sample_data *data, u64 addr)
+static inline void perf_sample_data_init(struct perf_sample_data *data,
+					 u64 addr, u64 period)
 {
+	/* remaining struct members initialized in perf_prepare_sample() */
 	data->addr = addr;
 	data->raw  = NULL;
 	data->br_stack = NULL;
+	data->period	= period;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,

commit b01c3a0010aabadf745f3e7fdb9cab682e0a28a2
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Fri Mar 23 15:41:20 2012 +0100

    perf: Move mmap page data_head offset assertion out of header
    
    Having the build time assertion in header is making the perf
    build fail on x86 with:
    
      ../../include/linux/perf_event.h:411:32: error: variably modified \
                    ‘__assert_mmap_data_head_offset’ at file scope [-Werror]
    
    I'm moving the build time validation out of the header, because
    I think it's better than to lessen the perf build warn/error
    check.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: acme@redhat.com
    Cc: a.p.zijlstra@chello.nl
    Cc: paulus@samba.org
    Cc: cjashfor@linux.vnet.ibm.com
    Cc: fweisbec@gmail.com
    Link: http://lkml.kernel.org/r/1332513680-7870-1-git-send-email-jolsa@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ca9ed4e6a286..ddbb6a901f65 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -403,13 +403,6 @@ struct perf_event_mmap_page {
 	__u64	data_tail;		/* user-space written tail */
 };
 
-/*
- * Build time assertion that we keep the data_head at the intended location.
- * IOW, validation we got the __reserved[] size right.
- */
-extern char __assert_mmap_data_head_offset
-	[1 - 2*!!(offsetof(struct perf_event_mmap_page, data_head) != 1024)];
-
 #define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)
 #define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
 #define PERF_RECORD_MISC_KERNEL			(1 << 0)

commit c7206205d00ab375839bd6c7ddb247d600693c09
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 22 17:26:36 2012 +0100

    perf: Fix mmap_page capabilities and docs
    
    Complete the syscall-less self-profiling feature and address
    all complaints, namely:
    
     - capabilities, so we can detect what is actually available at runtime
    
         Add a capabilities field to perf_event_mmap_page to indicate
         what is actually available for use.
    
     - on x86: RDPMC weirdness due to being 40/48 bits and not sign-extending
       properly.
    
     - ABI documentation as to how all this stuff works.
    
    Also improve the documentation for the new features.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vweaver1@eecs.utk.edu>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/1332433596.2487.33.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 57ae485e80fc..ca9ed4e6a286 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -299,18 +299,31 @@ struct perf_event_mmap_page {
 	/*
 	 * Bits needed to read the hw events in user-space.
 	 *
-	 *   u32 seq;
-	 *   s64 count;
+	 *   u32 seq, time_mult, time_shift, idx, width;
+	 *   u64 count, enabled, running;
+	 *   u64 cyc, time_offset;
+	 *   s64 pmc = 0;
 	 *
 	 *   do {
 	 *     seq = pc->lock;
-	 *
 	 *     barrier()
-	 *     if (pc->index) {
-	 *       count = pmc_read(pc->index - 1);
-	 *       count += pc->offset;
-	 *     } else
-	 *       goto regular_read;
+	 *
+	 *     enabled = pc->time_enabled;
+	 *     running = pc->time_running;
+	 *
+	 *     if (pc->cap_usr_time && enabled != running) {
+	 *       cyc = rdtsc();
+	 *       time_offset = pc->time_offset;
+	 *       time_mult   = pc->time_mult;
+	 *       time_shift  = pc->time_shift;
+	 *     }
+	 *
+	 *     idx = pc->index;
+	 *     count = pc->offset;
+	 *     if (pc->cap_usr_rdpmc && idx) {
+	 *       width = pc->pmc_width;
+	 *       pmc = rdpmc(idx - 1);
+	 *     }
 	 *
 	 *     barrier();
 	 *   } while (pc->lock != seq);
@@ -323,14 +336,57 @@ struct perf_event_mmap_page {
 	__s64	offset;			/* add to hardware event value */
 	__u64	time_enabled;		/* time event active */
 	__u64	time_running;		/* time event on cpu */
-	__u32	time_mult, time_shift;
+	union {
+		__u64	capabilities;
+		__u64	cap_usr_time  : 1,
+			cap_usr_rdpmc : 1,
+			cap_____res   : 62;
+	};
+
+	/*
+	 * If cap_usr_rdpmc this field provides the bit-width of the value
+	 * read using the rdpmc() or equivalent instruction. This can be used
+	 * to sign extend the result like:
+	 *
+	 *   pmc <<= 64 - width;
+	 *   pmc >>= 64 - width; // signed shift right
+	 *   count += pmc;
+	 */
+	__u16	pmc_width;
+
+	/*
+	 * If cap_usr_time the below fields can be used to compute the time
+	 * delta since time_enabled (in ns) using rdtsc or similar.
+	 *
+	 *   u64 quot, rem;
+	 *   u64 delta;
+	 *
+	 *   quot = (cyc >> time_shift);
+	 *   rem = cyc & ((1 << time_shift) - 1);
+	 *   delta = time_offset + quot * time_mult +
+	 *              ((rem * time_mult) >> time_shift);
+	 *
+	 * Where time_offset,time_mult,time_shift and cyc are read in the
+	 * seqcount loop described above. This delta can then be added to
+	 * enabled and possible running (if idx), improving the scaling:
+	 *
+	 *   enabled += delta;
+	 *   if (idx)
+	 *     running += delta;
+	 *
+	 *   quot = count / running;
+	 *   rem  = count % running;
+	 *   count = quot * enabled + (rem * enabled) / running;
+	 */
+	__u16	time_shift;
+	__u32	time_mult;
 	__u64	time_offset;
 
 		/*
 		 * Hole for extension of the self monitor capabilities
 		 */
 
-	__u64	__reserved[121];	/* align to 1k */
+	__u64	__reserved[120];	/* align to 1k */
 
 	/*
 	 * Control data for the mmap() data buffer.
@@ -347,6 +403,13 @@ struct perf_event_mmap_page {
 	__u64	data_tail;		/* user-space written tail */
 };
 
+/*
+ * Build time assertion that we keep the data_head at the intended location.
+ * IOW, validation we got the __reserved[] size right.
+ */
+extern char __assert_mmap_data_head_offset
+	[1 - 2*!!(offsetof(struct perf_event_mmap_page, data_head) != 1024)];
+
 #define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)
 #define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
 #define PERF_RECORD_MISC_KERNEL			(1 << 0)

commit 641cc938815dfd09f8fa1ec72deb814f0938ac33
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Thu Mar 15 20:09:14 2012 +0100

    perf: Adding sysfs group format attribute for pmu device
    
    Adding sysfs group 'format' attribute for pmu device that
    contains a syntax description on how to construct raw events.
    
    The event configuration is described in following
    struct pefr_event_attr attributes:
    
      config
      config1
      config2
    
    Each sysfs attribute within the format attribute group,
    describes mapping of name and bitfield definition within
    one of above attributes.
    
    eg:
      "/sys/...<dev>/format/event" contains "config:0-7"
      "/sys/...<dev>/format/umask" contains "config:8-15"
      "/sys/...<dev>/format/usr"   contains "config:16"
    
    the attribute value syntax is:
    
      line:      config ':' bits
      config:    'config' | 'config1' | 'config2"
      bits:      bits ',' bit_term | bit_term
      bit_term:  VALUE '-' VALUE | VALUE
    
    Adding format attribute definitions for x86 cpu pmus.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/n/tip-vhdk5y2hyype9j63prymty36@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index bd9f55a5958d..57ae485e80fc 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -550,6 +550,7 @@ struct perf_guest_info_callbacks {
 #include <linux/irq_work.h>
 #include <linux/static_key.h>
 #include <linux/atomic.h>
+#include <linux/sysfs.h>
 #include <asm/local.h>
 
 #define PERF_MAX_STACK_DEPTH		255
@@ -1291,5 +1292,18 @@ do {									\
 	register_cpu_notifier(&fn##_nb);				\
 } while (0)
 
+
+#define PMU_FORMAT_ATTR(_name, _format)					\
+static ssize_t								\
+_name##_show(struct device *dev,					\
+			       struct device_attribute *attr,		\
+			       char *page)				\
+{									\
+	BUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);			\
+	return sprintf(page, _format "\n");				\
+}									\
+									\
+static struct device_attribute format_attr_##_name = __ATTR_RO(_name)
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_PERF_EVENT_H */

commit cb5d76999029ae7a517cb07dfa732c1b5a934fc2
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Feb 9 23:21:05 2012 +0100

    perf: Add ABI reference sizes
    
    This patch adds reference sizes for revision 1
    and 2 of the perf_event ABI, i.e., the size of
    the perf_event_attr struct.
    
    With Rev1: config2 was added = +8 bytes
    With Rev2: branch_sample_type was added = +8 bytes
    
    Adds the definition for Rev1, Rev2.
    
    This is useful for tools trying to decode the revision
    numbers based on the size of the struct.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: acme@redhat.com
    Cc: robert.richter@amd.com
    Cc: ming.m.lin@intel.com
    Cc: andi@firstfloor.org
    Cc: asharma@fb.com
    Cc: ravitillo@lbl.gov
    Cc: vweaver1@eecs.utk.edu
    Cc: khandual@linux.vnet.ibm.com
    Cc: dsahern@gmail.com
    Link: http://lkml.kernel.org/r/1328826068-11713-16-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fbbf5e598368..bd9f55a5958d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -192,6 +192,8 @@ enum perf_event_read_format {
 };
 
 #define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
+#define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
+#define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:

commit d010b3326cf06b3406cdd88af16dcf4e4b6fec2e
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Feb 9 23:21:00 2012 +0100

    perf: Add callback to flush branch_stack on context switch
    
    With branch stack sampling, it is possible to filter by priv levels.
    
    In system-wide mode, that means it is possible to capture only user
    level branches. The builtin SW LBR filter needs to disassemble code
    based on LBR captured addresses. For that, it needs to know the task
    the addresses are associated with. Because of context switches, the
    content of the branch stack buffer may contain addresses from
    different tasks.
    
    We need a callback on context switch to either flush the branch stack
    or save it. This patch adds a new callback in struct pmu which is called
    during context switches. The callback is called only when necessary.
    That is when a system-wide context has, at least, one event which
    uses PERF_SAMPLE_BRANCH_STACK. The callback is never called for
    per-thread context.
    
    In this version, the Intel x86 code simply flushes (resets) the LBR
    on context switches (fills it with zeroes). Those zeroed branches are
    then filtered out by the SW filter.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1328826068-11713-11-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5fc494f4a094..fbbf5e598368 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -746,6 +746,11 @@ struct pmu {
 	 * if no implementation is provided it will default to: event->hw.idx + 1.
 	 */
 	int (*event_idx)		(struct perf_event *event); /*optional */
+
+	/*
+	 * flush branch stack on context-switches (needed in cpu-wide mode)
+	 */
+	void (*flush_branch_stack)	(void);
 };
 
 /**
@@ -979,7 +984,8 @@ struct perf_event_context {
 	u64				parent_gen;
 	u64				generation;
 	int				pin_count;
-	int				nr_cgroups; /* cgroup events present */
+	int				nr_cgroups;	 /* cgroup evts */
+	int				nr_branch_stack; /* branch_stack evt */
 	struct rcu_head			rcu_head;
 };
 
@@ -1044,6 +1050,7 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 
+
 struct perf_sample_data {
 	u64				type;
 

commit bce38cd53e5ddba9cb6d708c4ef3d04a4016ec7e
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Feb 9 23:20:51 2012 +0100

    perf: Add generic taken branch sampling support
    
    This patch adds the ability to sample taken branches to the
    perf_event interface.
    
    The ability to capture taken branches is very useful for all
    sorts of analysis. For instance, basic block profiling, call
    counts, statistical call graph.
    
    This new capability requires hardware assist and as such may
    not be available on all HW platforms. On Intel x86 it is
    implemented on top of the Last Branch Record (LBR) facility.
    
    To enable taken branches sampling, the PERF_SAMPLE_BRANCH_STACK
    bit must be set in attr->sample_type.
    
    Sampled taken branches may be filtered by type and/or priv
    levels.
    
    The patch adds a new field, called branch_sample_type, to the
    perf_event_attr structure. It contains a bitmask of filters
    to apply to the sampled taken branches.
    
    Filters may be implemented in HW. If the HW filter does not exist
    or is not good enough, some arch may also implement a SW filter.
    
    The following generic filters are currently defined:
    - PERF_SAMPLE_USER
      only branches whose targets are at the user level
    
    - PERF_SAMPLE_KERNEL
      only branches whose targets are at the kernel level
    
    - PERF_SAMPLE_HV
      only branches whose targets are at the hypervisor level
    
    - PERF_SAMPLE_ANY
      any type of branches (subject to priv levels filters)
    
    - PERF_SAMPLE_ANY_CALL
      any call branches (may incl. syscall on some arch)
    
    - PERF_SAMPLE_ANY_RET
      any return branches (may incl. syscall returns on some arch)
    
    - PERF_SAMPLE_IND_CALL
      indirect call branches
    
    Obviously filter may be combined. The priv level bits are optional.
    If not provided, the priv level of the associated event are used. It
    is possible to collect branches at a priv level different from the
    associated event. Use of kernel, hv priv levels is subject to permissions
    and availability (hv).
    
    The number of taken branch records present in each sample may vary based
    on HW, the type of sampled branches, the executed code. Therefore
    each sample contains the number of taken branches it contains.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1328826068-11713-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 64426b71381f..5fc494f4a094 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -129,10 +129,39 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_PERIOD			= 1U << 8,
 	PERF_SAMPLE_STREAM_ID			= 1U << 9,
 	PERF_SAMPLE_RAW				= 1U << 10,
+	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
 
-	PERF_SAMPLE_MAX = 1U << 11,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 12,		/* non-ABI */
 };
 
+/*
+ * values to program into branch_sample_type when PERF_SAMPLE_BRANCH is set
+ *
+ * If the user does not pass priv level information via branch_sample_type,
+ * the kernel uses the event's priv level. Branch and event priv levels do
+ * not have to match. Branch priv level is checked for permissions.
+ *
+ * The branch types can be combined, however BRANCH_ANY covers all types
+ * of branches and therefore it supersedes all the other types.
+ */
+enum perf_branch_sample_type {
+	PERF_SAMPLE_BRANCH_USER		= 1U << 0, /* user branches */
+	PERF_SAMPLE_BRANCH_KERNEL	= 1U << 1, /* kernel branches */
+	PERF_SAMPLE_BRANCH_HV		= 1U << 2, /* hypervisor branches */
+
+	PERF_SAMPLE_BRANCH_ANY		= 1U << 3, /* any branch types */
+	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << 4, /* any call branch */
+	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << 5, /* any return branch */
+	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << 6, /* indirect calls */
+
+	PERF_SAMPLE_BRANCH_MAX		= 1U << 7, /* non-ABI */
+};
+
+#define PERF_SAMPLE_BRANCH_PLM_ALL \
+	(PERF_SAMPLE_BRANCH_USER|\
+	 PERF_SAMPLE_BRANCH_KERNEL|\
+	 PERF_SAMPLE_BRANCH_HV)
+
 /*
  * The format of the data returned by read() on a perf event fd,
  * as specified by attr.read_format:
@@ -240,6 +269,7 @@ struct perf_event_attr {
 		__u64		bp_len;
 		__u64		config2; /* extension of config1 */
 	};
+	__u64	branch_sample_type; /* enum branch_sample_type */
 };
 
 /*
@@ -458,6 +488,8 @@ enum perf_event_type {
 	 *
 	 *	{ u32			size;
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
+	 *
+	 *	{ u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,
@@ -530,12 +562,34 @@ struct perf_raw_record {
 	void				*data;
 };
 
+/*
+ * single taken branch record layout:
+ *
+ *      from: source instruction (may not always be a branch insn)
+ *        to: branch target
+ *   mispred: branch target was mispredicted
+ * predicted: branch target was predicted
+ *
+ * support for mispred, predicted is optional. In case it
+ * is not supported mispred = predicted = 0.
+ */
 struct perf_branch_entry {
-	__u64				from;
-	__u64				to;
-	__u64				flags;
+	__u64	from;
+	__u64	to;
+	__u64	mispred:1,  /* target mispredicted */
+		predicted:1,/* target predicted */
+		reserved:62;
 };
 
+/*
+ * branch stack layout:
+ *  nr: number of taken branches stored in entries[]
+ *
+ * Note that nr can vary from sample to sample
+ * branches (to, from) are stored from most recent
+ * to least recent, i.e., entries[0] contains the most
+ * recent branch.
+ */
 struct perf_branch_stack {
 	__u64				nr;
 	struct perf_branch_entry	entries[0];
@@ -566,7 +620,9 @@ struct hw_perf_event {
 			unsigned long	event_base;
 			int		idx;
 			int		last_cpu;
+
 			struct hw_perf_event_extra extra_reg;
+			struct hw_perf_event_extra branch_reg;
 		};
 		struct { /* software */
 			struct hrtimer	hrtimer;
@@ -1007,12 +1063,14 @@ struct perf_sample_data {
 	u64				period;
 	struct perf_callchain_entry	*callchain;
 	struct perf_raw_record		*raw;
+	struct perf_branch_stack	*br_stack;
 };
 
 static inline void perf_sample_data_init(struct perf_sample_data *data, u64 addr)
 {
 	data->addr = addr;
 	data->raw  = NULL;
+	data->br_stack = NULL;
 }
 
 extern void perf_output_sample(struct perf_output_handle *handle,
@@ -1151,6 +1209,11 @@ extern void perf_bp_event(struct perf_event *event, void *data);
 # define perf_instruction_pointer(regs)	instruction_pointer(regs)
 #endif
 
+static inline bool has_branch_stack(struct perf_event *event)
+{
+	return event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK;
+}
+
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size);
 extern void perf_output_end(struct perf_output_handle *handle);

commit a706d4fc9e56d8e46393533e0cdca2d35fa5c7e5
Merge: 83b8450317a1 c5905afb0ee6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 28 19:59:45 2012 +0100

    Merge branch 'perf/jump-labels' into perf/core
    
    Merge reason: After much naming discussion, there seems to be consensus
                  now - queue it up for v3.4.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c5905afb0ee6550b42c49213da1c22d67316c194
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 24 08:31:31 2012 +0100

    static keys: Introduce 'struct static_key', static_key_true()/false() and static_key_slow_[inc|dec]()
    
    So here's a boot tested patch on top of Jason's series that does
    all the cleanups I talked about and turns jump labels into a
    more intuitive to use facility. It should also address the
    various misconceptions and confusions that surround jump labels.
    
    Typical usage scenarios:
    
            #include <linux/static_key.h>
    
            struct static_key key = STATIC_KEY_INIT_TRUE;
    
            if (static_key_false(&key))
                    do unlikely code
            else
                    do likely code
    
    Or:
    
            if (static_key_true(&key))
                    do likely code
            else
                    do unlikely code
    
    The static key is modified via:
    
            static_key_slow_inc(&key);
            ...
            static_key_slow_dec(&key);
    
    The 'slow' prefix makes it abundantly clear that this is an
    expensive operation.
    
    I've updated all in-kernel code to use this everywhere. Note
    that I (intentionally) have not pushed through the rename
    blindly through to the lowest levels: the actual jump-label
    patching arch facility should be named like that, so we want to
    decouple jump labels from the static-key facility a bit.
    
    On non-jump-label enabled architectures static keys default to
    likely()/unlikely() branches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: a.p.zijlstra@chello.nl
    Cc: mathieu.desnoyers@efficios.com
    Cc: davem@davemloft.net
    Cc: ddaney.cavm@gmail.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120222085809.GA26397@elte.hu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 412b790f5da6..0d21e6f1cf53 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -514,7 +514,7 @@ struct perf_guest_info_callbacks {
 #include <linux/ftrace.h>
 #include <linux/cpu.h>
 #include <linux/irq_work.h>
-#include <linux/jump_label.h>
+#include <linux/static_key.h>
 #include <linux/atomic.h>
 #include <asm/local.h>
 
@@ -1038,7 +1038,7 @@ static inline int is_software_event(struct perf_event *event)
 	return event->pmu->task_ctx_nr == perf_sw_context;
 }
 
-extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
+extern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, struct pt_regs *, u64);
 
@@ -1066,7 +1066,7 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 {
 	struct pt_regs hot_regs;
 
-	if (static_branch(&perf_swevent_enabled[event_id])) {
+	if (static_key_false(&perf_swevent_enabled[event_id])) {
 		if (!regs) {
 			perf_fetch_caller_regs(&hot_regs);
 			regs = &hot_regs;
@@ -1075,12 +1075,12 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 	}
 }
 
-extern struct jump_label_key_deferred perf_sched_events;
+extern struct static_key_deferred perf_sched_events;
 
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
-	if (static_branch(&perf_sched_events.key))
+	if (static_key_false(&perf_sched_events.key))
 		__perf_event_task_sched_in(prev, task);
 }
 
@@ -1089,7 +1089,7 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
-	if (static_branch(&perf_sched_events.key))
+	if (static_key_false(&perf_sched_events.key))
 		__perf_event_task_sched_out(prev, next);
 }
 

commit ced39002f5ea736b716ae233fb68b26d59783912
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:52 2012 +0100

    ftrace, perf: Add support to use function tracepoint in perf
    
    Adding perf registration support for the ftrace function event,
    so it is now possible to register it via perf interface.
    
    The perf_event struct statically contains ftrace_ops as a handle
    for function tracer. The function tracer is registered/unregistered
    in open/close actions.
    
    To be efficient, we enable/disable ftrace_ops each time the traced
    process is scheduled in/out (via TRACE_REG_PERF_(ADD|DELL) handlers).
    This way tracing is enabled only when the process is running.
    Intentionally using this way instead of the event's hw state
    PERF_HES_STOPPED, which would not disable the ftrace_ops.
    
    It is now possible to use function trace within perf commands
    like:
    
      perf record -e ftrace:function ls
      perf stat -e ftrace:function ls
    
    Allowed only for root.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-6-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 412b790f5da6..92a056f6d18d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -859,6 +859,9 @@ struct perf_event {
 #ifdef CONFIG_EVENT_TRACING
 	struct ftrace_event_call	*tp_event;
 	struct event_filter		*filter;
+#ifdef CONFIG_FUNCTION_TRACER
+	struct ftrace_ops               ftrace_ops;
+#endif
 #endif
 
 #ifdef CONFIG_CGROUP_PERF

commit bb1693f89ae7f0b30c90d9b26a4f827faed1144a
Merge: efb3040d481a 45179fec946d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 31 13:02:35 2012 +0100

    Merge branch 'perf/urgent' into perf/core
    
    We cherry-picked 3 commits into perf/urgent, merge them back to allow
    conflict-free work on those files.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e050e3f0a71bf7dc2c148b35caff0234decc8198
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 26 17:03:19 2012 +0100

    perf: Fix broken interrupt rate throttling
    
    This patch fixes the sampling interrupt throttling mechanism.
    
    It was broken in v3.2. Events were not being unthrottled. The
    unthrottling mechanism required that events be checked at each
    timer tick.
    
    This patch solves this problem and also separates:
    
      - unthrottling
      - multiplexing
      - frequency-mode period adjustments
    
    Not all of them need to be executed at each timer tick.
    
    This third version of the patch is based on my original patch +
    PeterZ proposal (https://lkml.org/lkml/2012/1/7/87).
    
    At each timer tick, for each context:
    
      - if the current CPU has throttled events, we unthrottle events
    
      - if context has frequency-based events, we adjust sampling periods
    
      - if we have reached the jiffies interval, we multiplex (rotate)
    
    We decoupled rotation (multiplexing) from frequency-mode sampling
    period adjustments.  They should not necessarily happen at the same
    rate. Multiplexing is subject to jiffies_interval (currently at 1
    but could be higher once the tunable is exposed via sysfs).
    
    We have grouped frequency-mode adjustment and unthrottling into the
    same routine to minimize code duplication. When throttled while in
    frequency mode, we scan the events only once.
    
    We have fixed the threshold enforcement code in __perf_event_overflow().
    There was a bug whereby it would allow more than the authorized rate
    because an increment of hwc->interrupts was not executed at the right
    place.
    
    The patch was tested with low sampling limit (2000) and fixed periods,
    frequency mode, overcommitted PMU.
    
    On a 2.1GHz AMD CPU:
    
     $ cat /proc/sys/kernel/perf_event_max_sample_rate
     2000
    
    We set a rate of 3000 samples/sec (2.1GHz/3000 = 700000):
    
     $ perf record -e cycles,cycles -c 700000  noploop 10
     $ perf report -D | tail -21
    
     Aggregated stats:
               TOTAL events:      80086
                MMAP events:         88
                COMM events:          2
                EXIT events:          4
            THROTTLE events:      19996
          UNTHROTTLE events:      19996
              SAMPLE events:      40000
    
     cycles stats:
               TOTAL events:      40006
                MMAP events:          5
                COMM events:          1
                EXIT events:          4
            THROTTLE events:       9998
          UNTHROTTLE events:       9998
              SAMPLE events:      20000
    
     cycles stats:
               TOTAL events:      39996
            THROTTLE events:       9998
          UNTHROTTLE events:       9998
              SAMPLE events:      20000
    
    For 10s, the cap is 2x2000x10 = 40000 samples.
    We get exactly that: 20000 samples/event.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: <stable@kernel.org> # v3.2+
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120126160319.GA5655@quad
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 08855613ceb3..abb2776be1ba 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -587,6 +587,7 @@ struct hw_perf_event {
 	u64				sample_period;
 	u64				last_period;
 	local64_t			period_left;
+	u64                             interrupts_seq;
 	u64				interrupts;
 
 	u64				freq_time_stamp;

commit e3f3541c19c89a4daae39300defba68943301949
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 21 11:43:53 2011 +0100

    perf: Extend the mmap control page with time (TSC) fields
    
    Extend the mmap control page with fields so that userspace can compute
    time deltas relative to the provided time fields.
    
    Currently only implemented for x86 with constant and nonstop TSC.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/n/tip-3u1jucza77j3wuvs0x2bic0f@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5311b79fe62c..0b91db2522cc 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -291,12 +291,14 @@ struct perf_event_mmap_page {
 	__s64	offset;			/* add to hardware event value */
 	__u64	time_enabled;		/* time event active */
 	__u64	time_running;		/* time event on cpu */
+	__u32	time_mult, time_shift;
+	__u64	time_offset;
 
 		/*
 		 * Hole for extension of the self monitor capabilities
 		 */
 
-	__u64	__reserved[123];	/* align to 1k */
+	__u64	__reserved[121];	/* align to 1k */
 
 	/*
 	 * Control data for the mmap() data buffer.

commit 0c9d42ed4cee2aa1dfc3a260b741baae8615744f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Nov 20 23:30:47 2011 +0100

    perf, x86: Provide means for disabling userspace RDPMC
    
    Allow the disabling of RDPMC via a pmu specific attribute:
    
      echo 0 > /sys/bus/event_source/devices/cpu/rdpmc
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/n/tip-pqeog465zo5hsimtkfz73f27@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 02545e6df95b..5311b79fe62c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -615,6 +615,7 @@ struct pmu {
 	struct list_head		entry;
 
 	struct device			*dev;
+	const struct attribute_group	**attr_groups;
 	char				*name;
 	int				type;
 

commit 35edc2a5095efb189e60dc32bbb9d2663aec6d24
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Nov 20 20:36:02 2011 +0100

    perf, arch: Rework perf_event_index()
    
    Put the logic to compute the event index into a per pmu method. This
    is required because the x86 rules are weird and wonderful and don't
    match the capabilities of the current scheme.
    
    AFAIK only powerpc actually has a usable userspace read of the PMCs
    but I'm not at all sure anybody actually used that.
    
    ARM is restored to the default since it currently does not support
    userspace access at all. And all software events are provided with a
    method that reports their index as 0 (disabled).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arun Sharma <asharma@fb.com>
    Link: http://lkml.kernel.org/n/tip-dfydxodki16lylkt3gl2j7cw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 08855613ceb3..02545e6df95b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -680,6 +680,12 @@ struct pmu {
 	 * for each successful ->add() during the transaction.
 	 */
 	void (*cancel_txn)		(struct pmu *pmu); /* optional */
+
+	/*
+	 * Will return the value for perf_event_mmap_page::index for this event,
+	 * if no implementation is provided it will default to: event->hw.idx + 1.
+	 */
+	int (*event_idx)		(struct perf_event *event); /*optional */
 };
 
 /**

commit c37e17497e01fc0f5d2d6feb5723b210b3ab8890
Author: Stephane Eranian <eranian@google.com>
Date:   Sun Dec 11 00:28:52 2011 +0100

    perf events: Add PERF_COUNT_HW_REF_CPU_CYCLES generic PMU event
    
    This event counts the number of reference core cpu cycles.
    Reference means that the event increments at a constant rate which
    is not subject to core CPU frequency adjustments. The event may
    not count when the processor is in halted (low power) state.
    As such, it may not be equivalent to wall clock time. However,
    when the processor is not halted state, the event keeps
    a constant correlation with wall clock time.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1323559734-3488-3-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 564769cdb473..08855613ceb3 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -54,6 +54,7 @@ enum perf_hw_id {
 	PERF_COUNT_HW_BUS_CYCLES		= 6,
 	PERF_COUNT_HW_STALLED_CYCLES_FRONTEND	= 7,
 	PERF_COUNT_HW_STALLED_CYCLES_BACKEND	= 8,
+	PERF_COUNT_HW_REF_CPU_CYCLES		= 9,
 
 	PERF_COUNT_HW_MAX,			/* non-ABI */
 };

commit b202952075f62603bea9bfb6ebc6b0420db11949
Author: Gleb Natapov <gleb@redhat.com>
Date:   Sun Nov 27 17:59:09 2011 +0200

    perf, core: Rate limit perf_sched_events jump_label patching
    
    jump_lable patching is very expensive operation that involves pausing all
    cpus. The patching of perf_sched_events jump_label is easily controllable
    from userspace by unprivileged user.
    
    When te user runs a loop like this:
    
      "while true; do perf stat -e cycles true; done"
    
    ... the performance of my test application that just increments a counter
    for one second drops by 4%.
    
    This is on a 16 cpu box with my test application using only one of
    them. An impact on a real server doing real work will be worse.
    
    Performance of KVM PMU drops nearly 50% due to jump_lable for "perf
    record" since KVM PMU implementation creates and destroys perf event
    frequently.
    
    This patch introduces a way to rate limit jump_label patching and uses
    it to fix the above problem.
    
    I believe that as jump_label use will spread the problem will become more
    common and thus solving it in a generic code is appropriate. Also fixing
    it in the perf code would result in moving jump_label accounting logic to
    perf code with all the ifdefs in case of JUMP_LABEL=n kernel. With this
    patch all details are nicely hidden inside jump_label code.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20111127155909.GO2557@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cb44c9e75660..564769cdb473 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1064,12 +1064,12 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 	}
 }
 
-extern struct jump_label_key perf_sched_events;
+extern struct jump_label_key_deferred perf_sched_events;
 
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
-	if (static_branch(&perf_sched_events))
+	if (static_branch(&perf_sched_events.key))
 		__perf_event_task_sched_in(prev, task);
 }
 
@@ -1078,7 +1078,7 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
-	if (static_branch(&perf_sched_events))
+	if (static_branch(&perf_sched_events.key))
 		__perf_event_task_sched_out(prev, next);
 }
 

commit 0f5a2601284237e2ba089389fd75d67f77626cef
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Nov 16 14:38:16 2011 +0100

    perf: Avoid a useless pmu_disable() in the perf-tick
    
    Gleb writes:
    
     > Currently pmu is disabled and re-enabled on each timer interrupt even
     > when no rotation or frequency adjustment is needed. On Intel CPU this
     > results in two writes into PERF_GLOBAL_CTRL MSR per tick. On bare metal
     > it does not cause significant slowdown, but when running perf in a virtual
     > machine it leads to 20% slowdown on my machine.
    
    Cure this by keeping a perf_event_context::nr_freq counter that counts the
    number of active events that require frequency adjustments and use this in a
    similar fashion to the already existing nr_events != nr_active test in
    perf_rotate_context().
    
    By being able to exclude both rotation and frequency adjustments a-priory for
    the common case we can avoid the otherwise superfluous PMU disable.
    
    Suggested-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-515yhoatehd3gza7we9fapaa@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b1f89122bf6a..cb44c9e75660 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -890,6 +890,7 @@ struct perf_event_context {
 	int				nr_active;
 	int				is_active;
 	int				nr_stat;
+	int				nr_freq;
 	int				rotate_disable;
 	atomic_t			refcount;
 	struct task_struct		*task;

commit 10c6db110d0eb4466b59812c49088ab56218fc2e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Nov 26 02:47:31 2011 +0100

    perf: Fix loss of notification with multi-event
    
    When you do:
            $ perf record -e cycles,cycles,cycles noploop 10
    
    You expect about 10,000 samples for each event, i.e., 10s at
    1000samples/sec. However, this is not what's happening. You
    get much fewer samples, maybe 3700 samples/event:
    
    $ perf report -D | tail -15
    Aggregated stats:
               TOTAL events:      10998
                MMAP events:         66
                COMM events:          2
              SAMPLE events:      10930
    cycles stats:
               TOTAL events:       3644
              SAMPLE events:       3644
    cycles stats:
               TOTAL events:       3642
              SAMPLE events:       3642
    cycles stats:
               TOTAL events:       3644
              SAMPLE events:       3644
    
    On a Intel Nehalem or even AMD64, there are 4 counters capable
    of measuring cycles, so there is plenty of space to measure those
    events without multiplexing (even with the NMI watchdog active).
    And even with multiplexing, we'd expect roughly the same number
    of samples per event.
    
    The root of the problem was that when the event that caused the buffer
    to become full was not the first event passed on the cmdline, the user
    notification would get lost. The notification was sent to the file
    descriptor of the overflowed event but the perf tool was not polling
    on it.  The perf tool aggregates all samples into a single buffer,
    i.e., the buffer of the first event. Consequently, it assumes
    notifications for any event will come via that descriptor.
    
    The seemingly straight forward solution of moving the waitq into the
    ringbuffer object doesn't work because of life-time issues. One could
    perf_event_set_output() on a fd that you're also blocking on and cause
    the old rb object to be freed while its waitq would still be
    referenced by the blocked thread -> FAIL.
    
    Therefore link all events to the ringbuffer and broadcast the wakeup
    from the ringbuffer object to all possible events that could be waited
    upon. This is rather ugly, and we're open to better solutions but it
    works for now.
    
    Reported-by: Stephane Eranian <eranian@google.com>
    Finished-by: Stephane Eranian <eranian@google.com>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20111126014731.GA7030@quad
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1e9ebe5e0091..b1f89122bf6a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -822,6 +822,7 @@ struct perf_event {
 	int				mmap_locked;
 	struct user_struct		*mmap_user;
 	struct ring_buffer		*rb;
+	struct list_head		rb_entry;
 
 	/* poll related */
 	wait_queue_head_t		waitq;

commit a240f76165e6255384d4bdb8139895fac7988799
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Wed Oct 5 14:01:16 2011 +0200

    perf, core: Introduce attrs to count in either host or guest mode
    
    The two new attributes exclude_guest and exclude_host can
    bes used by user-space to tell the kernel to setup
    performance counter to either only count while the CPU is in
    guest or in host mode.
    
    An additional check is also introduced to make sure
    user-space does not try to exclude guest and host mode from
    counting.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1317816084-18026-2-git-send-email-gleb@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c816075c01ce..1e9ebe5e0091 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -220,7 +220,10 @@ struct perf_event_attr {
 				mmap_data      :  1, /* non-exec mmap data    */
 				sample_id_all  :  1, /* sample_type all events */
 
-				__reserved_1   : 45;
+				exclude_host   :  1, /* don't count in host   */
+				exclude_guest  :  1, /* don't count in guest  */
+
+				__reserved_1   : 43;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */

commit a8d757ef076f0f95f13a918808824058de25b3eb
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Aug 25 15:58:03 2011 +0200

    perf events: Fix slow and broken cgroup context switch code
    
    The current cgroup context switch code was incorrect leading
    to bogus counts. Furthermore, as soon as there was an active
    cgroup event on a CPU, the context switch cost on that CPU
    would increase by a significant amount as demonstrated by a
    simple ping/pong example:
    
     $ ./pong
     Both processes pinned to CPU1, running for 10s
     10684.51 ctxsw/s
    
    Now start a cgroup perf stat:
     $ perf stat -e cycles,cycles -A -a -G test  -C 1 -- sleep 100
    
    $ ./pong
     Both processes pinned to CPU1, running for 10s
     6674.61 ctxsw/s
    
    That's a 37% penalty.
    
    Note that pong is not even in the monitored cgroup.
    
    The results shown by perf stat are bogus:
     $ perf stat -e cycles,cycles -A -a -G test  -C 1 -- sleep 100
    
     Performance counter stats for 'sleep 100':
    
     CPU1 <not counted> cycles   test
     CPU1 16,984,189,138 cycles  #    0.000 GHz
    
    The second 'cycles' event should report a count @ CPU clock
    (here 2.4GHz) as it is counting across all cgroups.
    
    The patch below fixes the bogus accounting and bypasses any
    cgroup switches in case the outgoing and incoming tasks are
    in the same cgroup.
    
    With this patch the same test now yields:
     $ ./pong
     Both processes pinned to CPU1, running for 10s
     10775.30 ctxsw/s
    
    Start perf stat with cgroup:
    
     $ perf stat -e cycles,cycles -A -a -G test  -C 1 -- sleep 10
    
    Run pong outside the cgroup:
     $ /pong
     Both processes pinned to CPU1, running for 10s
     10687.80 ctxsw/s
    
    The penalty is now less than 2%.
    
    And the results for perf stat are correct:
    
    $ perf stat -e cycles,cycles -A -a -G test  -C 1 -- sleep 10
    
     Performance counter stats for 'sleep 10':
    
     CPU1 <not counted> cycles test #    0.000 GHz
     CPU1 23,933,981,448 cycles      #    0.000 GHz
    
    Now perf stat reports the correct counts for
    for the non cgroup event.
    
    If we run pong inside the cgroup, then we also get the
    correct counts:
    
    $ perf stat -e cycles,cycles -A -a -G test  -C 1 -- sleep 10
    
     Performance counter stats for 'sleep 10':
    
     CPU1 22,297,726,205 cycles test #    0.000 GHz
     CPU1 23,933,981,448 cycles      #    0.000 GHz
    
          10.001457237 seconds time elapsed
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110825135803.GA4697@quad
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 245bafdafd5e..c816075c01ce 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -944,8 +944,10 @@ extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);
 extern const char *perf_pmu_name(void);
-extern void __perf_event_task_sched_in(struct task_struct *task);
-extern void __perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
+extern void __perf_event_task_sched_in(struct task_struct *prev,
+				       struct task_struct *task);
+extern void __perf_event_task_sched_out(struct task_struct *prev,
+					struct task_struct *next);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -1059,17 +1061,20 @@ perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 
 extern struct jump_label_key perf_sched_events;
 
-static inline void perf_event_task_sched_in(struct task_struct *task)
+static inline void perf_event_task_sched_in(struct task_struct *prev,
+					    struct task_struct *task)
 {
 	if (static_branch(&perf_sched_events))
-		__perf_event_task_sched_in(task);
+		__perf_event_task_sched_in(prev, task);
 }
 
-static inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
+static inline void perf_event_task_sched_out(struct task_struct *prev,
+					     struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
-	__perf_event_task_sched_out(task, next);
+	if (static_branch(&perf_sched_events))
+		__perf_event_task_sched_out(prev, next);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);
@@ -1139,10 +1144,11 @@ extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_task_tick(void);
 #else
 static inline void
-perf_event_task_sched_in(struct task_struct *task)			{ }
+perf_event_task_sched_in(struct task_struct *prev,
+			 struct task_struct *task)			{ }
 static inline void
-perf_event_task_sched_out(struct task_struct *task,
-			    struct task_struct *next)			{ }
+perf_event_task_sched_out(struct task_struct *prev,
+			  struct task_struct *next)			{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3f2711ccf910..245bafdafd5e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -509,7 +509,7 @@ struct perf_guest_info_callbacks {
 #include <linux/cpu.h>
 #include <linux/irq_work.h>
 #include <linux/jump_label.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/local.h>
 
 #define PERF_MAX_STACK_DEPTH		255

commit 26ca5c11fb45ae2b2ac7e3574b8db6b3a3c7d350
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Jun 29 18:42:37 2011 +0300

    perf: export perf_event_refresh() to modules
    
    KVM needs one-shot samples, since a PMC programmed to -X will fire after X
    events and then again after 2^40 events (i.e. variable period).
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1309362157-6596-4-git-send-email-avi@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2a08cacb1628..3f2711ccf910 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -955,6 +955,7 @@ extern void perf_pmu_disable(struct pmu *pmu);
 extern void perf_pmu_enable(struct pmu *pmu);
 extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
+extern int perf_event_refresh(struct perf_event *event, int refresh);
 extern void perf_event_update_userpage(struct perf_event *event);
 extern int perf_event_release_kernel(struct perf_event *event);
 extern struct perf_event *
@@ -1149,6 +1150,10 @@ static inline void perf_event_delayed_put(struct task_struct *task)	{ }
 static inline void perf_event_print_debug(void)				{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }
+static inline int perf_event_refresh(struct perf_event *event, int refresh)
+{
+	return -EINVAL;
+}
 
 static inline void
 perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)	{ }

commit 4dc0da86967d5463708631d02a70cfed5b104884
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Jun 29 18:42:35 2011 +0300

    perf: Add context field to perf_event
    
    The perf_event overflow handler does not receive any caller-derived
    argument, so many callers need to resort to looking up the perf_event
    in their local data structure.  This is ugly and doesn't scale if a
    single callback services many perf_events.
    
    Fix by adding a context parameter to perf_event_create_kernel_counter()
    (and derived hardware breakpoints APIs) and storing it in the perf_event.
    The field can be accessed from the callback as event->overflow_handler_context.
    All callers are updated.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1309362157-6596-2-git-send-email-avi@redhat.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a5f54b973bdb..2a08cacb1628 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -839,6 +839,7 @@ struct perf_event {
 	u64				id;
 
 	perf_overflow_handler_t		overflow_handler;
+	void				*overflow_handler_context;
 
 #ifdef CONFIG_EVENT_TRACING
 	struct ftrace_event_call	*tp_event;
@@ -960,7 +961,8 @@ extern struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				int cpu,
 				struct task_struct *task,
-				perf_overflow_handler_t callback);
+				perf_overflow_handler_t callback,
+				void *context);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 

commit 89d6c0b5bdbb1927775584dcf532d98b3efe1477
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 22 23:37:06 2011 +0200

    perf, arch: Add generic NODE cache events
    
    Add a NODE level to the generic cache events which is used to measure
    local vs remote memory accesses. Like all other cache events, an
    ACCESS is HIT+MISS, if there is no way to distinguish between reads
    and writes do reads only etc..
    
    The below needs filling out for !x86 (which I filled out with
    unsupported events).
    
    I'm fairly sure ARM can leave it like that since it doesn't strike me as
    an architecture that even has NUMA support. SH might have something since
    it does appear to have some NUMA bits.
    
    Sparc64, PowerPC and MIPS certainly want a good look there since they
    clearly are NUMA capable.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Miller <davem@davemloft.net>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: David Daney <ddaney@caviumnetworks.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1303508226.4865.8.camel@laptop
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 069315eefb22..a5f54b973bdb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -61,7 +61,7 @@ enum perf_hw_id {
 /*
  * Generalized hardware cache events:
  *
- *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU } x
+ *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU, NODE } x
  *       { read, write, prefetch } x
  *       { accesses, misses }
  */
@@ -72,6 +72,7 @@ enum perf_hw_cache_id {
 	PERF_COUNT_HW_CACHE_DTLB		= 3,
 	PERF_COUNT_HW_CACHE_ITLB		= 4,
 	PERF_COUNT_HW_CACHE_BPU			= 5,
+	PERF_COUNT_HW_CACHE_NODE		= 6,
 
 	PERF_COUNT_HW_CACHE_MAX,		/* non-ABI */
 };

commit efc9f05df2dd171280dcb736a4d973ffefd5508e
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Jun 6 16:57:03 2011 +0200

    perf_events: Update Intel extra regs shared constraints management
    
    This patch improves the code managing the extra shared registers
    used for offcore_response events on Intel Nehalem/Westmere. The
    idea is to use static allocation instead of dynamic allocation.
    This simplifies greatly the get and put constraint routines for
    those events.
    
    The patch also renames per_core to shared_regs because the same
    data structure gets used whether or not HT is on. When HT is
    off, those events still need to coordination because they use
    a extra MSR that has to be shared within an event group.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110606145703.GA7258@quad
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 771b0b2845e4..069315eefb22 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -536,6 +536,16 @@ struct perf_branch_stack {
 
 struct task_struct;
 
+/*
+ * extra PMU register associated with an event
+ */
+struct hw_perf_event_extra {
+	u64		config;	/* register value */
+	unsigned int	reg;	/* register address or index */
+	int		alloc;	/* extra register already allocated */
+	int		idx;	/* index in shared_regs->regs[] */
+};
+
 /**
  * struct hw_perf_event - performance event hardware details:
  */
@@ -549,9 +559,7 @@ struct hw_perf_event {
 			unsigned long	event_base;
 			int		idx;
 			int		last_cpu;
-			unsigned int	extra_reg;
-			u64		extra_config;
-			int		extra_alloc;
+			struct hw_perf_event_extra extra_reg;
 		};
 		struct { /* software */
 			struct hrtimer	hrtimer;

commit a7ac67ea021b4603095d2aa458bc41641238f22c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jun 27 16:47:16 2011 +0200

    perf: Remove the perf_output_begin(.sample) argument
    
    Since only samples call perf_output_sample() its much saner (and more
    correct) to put the sample logic in there than in the
    perf_output_begin()/perf_output_end() pair.
    
    Saves a useless argument, reduces conditionals and shrinks
    struct perf_output_handle, win!
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-2crpvsx3cqu67q3zqjbnlpsc@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0946a8bc098d..771b0b2845e4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -925,7 +925,6 @@ struct perf_output_handle {
 	unsigned long			size;
 	void				*addr;
 	int				page;
-	int				sample;
 };
 
 #ifdef CONFIG_PERF_EVENTS
@@ -1117,8 +1116,7 @@ extern void perf_bp_event(struct perf_event *event, void *data);
 #endif
 
 extern int perf_output_begin(struct perf_output_handle *handle,
-			     struct perf_event *event, unsigned int size,
-			     int sample);
+			     struct perf_event *event, unsigned int size);
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);

commit a8b0ca17b80e92faab46ee7179ba9e99ccb61233
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jun 27 14:41:57 2011 +0200

    perf: Remove the nmi parameter from the swevent and overflow interface
    
    The nmi parameter indicated if we could do wakeups from the current
    context, if not, we would set some state and self-IPI and let the
    resulting interrupt do the wakeup.
    
    For the various event classes:
    
      - hardware: nmi=0; PMI is in fact an NMI or we run irq_work_run from
        the PMI-tail (ARM etc.)
      - tracepoint: nmi=0; since tracepoint could be from NMI context.
      - software: nmi=[0,1]; some, like the schedule thing cannot
        perform wakeups, and hence need 0.
    
    As one can see, there is very little nmi=1 usage, and the down-side of
    not using it is that on some platforms some software events can have a
    jiffy delay in wakeup (when arch_irq_work_raise isn't implemented).
    
    The up-side however is that we can remove the nmi parameter and save a
    bunch of conditionals in fast paths.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jason Wessel <jason.wessel@windriver.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Link: http://lkml.kernel.org/n/tip-agjev8eu666tvknpb3iaj0fg@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2f7b5d42ab41..0946a8bc098d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -682,7 +682,7 @@ enum perf_event_active_state {
 struct file;
 struct perf_sample_data;
 
-typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
+typedef void (*perf_overflow_handler_t)(struct perf_event *,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
 
@@ -925,7 +925,6 @@ struct perf_output_handle {
 	unsigned long			size;
 	void				*addr;
 	int				page;
-	int				nmi;
 	int				sample;
 };
 
@@ -993,7 +992,7 @@ extern void perf_prepare_sample(struct perf_event_header *header,
 				struct perf_event *event,
 				struct pt_regs *regs);
 
-extern int perf_event_overflow(struct perf_event *event, int nmi,
+extern int perf_event_overflow(struct perf_event *event,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
@@ -1012,7 +1011,7 @@ static inline int is_software_event(struct perf_event *event)
 
 extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
-extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
+extern void __perf_sw_event(u32, u64, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
 static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
@@ -1034,7 +1033,7 @@ static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 }
 
 static __always_inline void
-perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
+perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 {
 	struct pt_regs hot_regs;
 
@@ -1043,7 +1042,7 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 			perf_fetch_caller_regs(&hot_regs);
 			regs = &hot_regs;
 		}
-		__perf_sw_event(event_id, nr, nmi, regs, addr);
+		__perf_sw_event(event_id, nr, regs, addr);
 	}
 }
 
@@ -1057,7 +1056,7 @@ static inline void perf_event_task_sched_in(struct task_struct *task)
 
 static inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
 {
-	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
+	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
 	__perf_event_task_sched_out(task, next);
 }
@@ -1119,7 +1118,7 @@ extern void perf_bp_event(struct perf_event *event, void *data);
 
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size,
-			     int nmi, int sample);
+			     int sample);
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
@@ -1143,8 +1142,7 @@ static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }
 
 static inline void
-perf_sw_event(u32 event_id, u64 nr, int nmi,
-		     struct pt_regs *regs, u64 addr)			{ }
+perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)	{ }
 static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 

commit 28009ce4a8130af7260a9271901b4419834ad152
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Tue Jun 7 16:33:38 2011 +0100

    perf: Remove 64-bit alignment padding from perf_event_context
    
    Reorder perf_event_context to remove 8 bytes of 64 bit alignment padding
    shrinking its size to 192 bytes, allowing it to fit into a smaller slab
    and use one fewer cache lines.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1307460819.1950.5.camel@castor.rsk
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index e76a41010e1f..2f7b5d42ab41 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -894,8 +894,8 @@ struct perf_event_context {
 	u64				parent_gen;
 	u64				generation;
 	int				pin_count;
-	struct rcu_head			rcu_head;
 	int				nr_cgroups; /* cgroup events present */
+	struct rcu_head			rcu_head;
 };
 
 /*

commit b4f9f2b64aa189c5584f266f4f0343af7a705441
Merge: 76369139ceb9 2c53b436a308
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 16 13:23:15 2011 +0200

    Merge commit 'v3.0-rc3' into perf/core
    
    Merge reason: add the latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 76369139ceb955deefc509e6e12ce9d6ce50ccab
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 19 19:55:04 2011 +0200

    perf: Split up buffer handling from core code
    
    And create the internal perf events header.
    
    v2: Keep an internal inlined perf_output_copy()
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1305827704-5607-1-git-send-email-fweisbec@gmail.com
    [ v3: use clearer 'ring_buffer' and 'rb' naming ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3412684ce5d5..779f6ed54d52 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -680,33 +680,6 @@ enum perf_event_active_state {
 };
 
 struct file;
-
-#define PERF_BUFFER_WRITABLE		0x01
-
-struct perf_buffer {
-	atomic_t			refcount;
-	struct rcu_head			rcu_head;
-#ifdef CONFIG_PERF_USE_VMALLOC
-	struct work_struct		work;
-	int				page_order;	/* allocation order  */
-#endif
-	int				nr_pages;	/* nr of data pages  */
-	int				writable;	/* are we writable   */
-
-	atomic_t			poll;		/* POLL_ for wakeups */
-
-	local_t				head;		/* write position    */
-	local_t				nest;		/* nested writers    */
-	local_t				events;		/* event limit       */
-	local_t				wakeup;		/* wakeup stamp      */
-	local_t				lost;		/* nr records lost   */
-
-	long				watermark;	/* wakeup watermark  */
-
-	struct perf_event_mmap_page	*user_page;
-	void				*data_pages[0];
-};
-
 struct perf_sample_data;
 
 typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
@@ -745,6 +718,8 @@ struct perf_cgroup {
 };
 #endif
 
+struct ring_buffer;
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -834,7 +809,7 @@ struct perf_event {
 	atomic_t			mmap_count;
 	int				mmap_locked;
 	struct user_struct		*mmap_user;
-	struct perf_buffer		*buffer;
+	struct ring_buffer		*rb;
 
 	/* poll related */
 	wait_queue_head_t		waitq;
@@ -945,7 +920,7 @@ struct perf_cpu_context {
 
 struct perf_output_handle {
 	struct perf_event		*event;
-	struct perf_buffer		*buffer;
+	struct ring_buffer		*rb;
 	unsigned long			wakeup;
 	unsigned long			size;
 	void				*addr;

commit d7ebe75b065a7c2d58ffc12f9d2e00d5ea4e71eb
Author: Vince Weaver <vweaver1@eecs.utk.edu>
Date:   Fri Jun 3 17:59:51 2011 -0400

    perf: Fix comments in include/linux/perf_event.h
    
    Fix include/linux/perf_event.h comments to be consistent with
    the actual #define names. This is trivial, but it can be a bit
    confusing when first  reading through the file.
    
    Signed-off-by: Vince Weaver <vweaver1@eecs.utk.edu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus@samba.org
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.00.1106031757090.29381@cl320.eecs.utk.edu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3412684ce5d5..e0786e35f247 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -137,14 +137,14 @@ enum perf_event_sample_format {
  *
  * struct read_format {
  *	{ u64		value;
- *	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
- *	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
  *	  { u64		id;           } && PERF_FORMAT_ID
  *	} && !PERF_FORMAT_GROUP
  *
  *	{ u64		nr;
- *	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
- *	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
  *	  { u64		value;
  *	    { u64	id;           } && PERF_FORMAT_ID
  *	  }		cntr[nr];

commit 57d524154ffe99d27fb55e0e30ddbad9f4c35806
Merge: e04d1b23f970 c63ca0c01d73
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri May 6 21:07:33 2011 +0200

    Merge branch 'perf/stat' into perf/core
    
    Merge reason: the perf stat improvements are tested and ready now.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e7e7ee2eab2080248084d71fe0a115ab745eb2aa
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed May 4 08:42:29 2011 +0200

    perf events: Clean up definitions and initializers, update copyrights
    
    Fix a few inconsistent style bits that were added over the past few
    months.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-yv4hwf9yhnzoada8pcpb3a97@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9eec53d97370..207c16976a17 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -2,8 +2,8 @@
  * Performance events:
  *
  *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>
- *    Copyright (C) 2008-2009, Red Hat, Inc., Ingo Molnar
- *    Copyright (C) 2008-2009, Red Hat, Inc., Peter Zijlstra
+ *    Copyright (C) 2008-2011, Red Hat, Inc., Ingo Molnar
+ *    Copyright (C) 2008-2011, Red Hat, Inc., Peter Zijlstra
  *
  * Data type definitions, declarations, prototypes.
  *
@@ -468,9 +468,9 @@ enum perf_callchain_context {
 	PERF_CONTEXT_MAX		= (__u64)-4095,
 };
 
-#define PERF_FLAG_FD_NO_GROUP	(1U << 0)
-#define PERF_FLAG_FD_OUTPUT	(1U << 1)
-#define PERF_FLAG_PID_CGROUP	(1U << 2) /* pid=cgroup id, per-cpu mode only */
+#define PERF_FLAG_FD_NO_GROUP		(1U << 0)
+#define PERF_FLAG_FD_OUTPUT		(1U << 1)
+#define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
 
 #ifdef __KERNEL__
 /*
@@ -484,9 +484,9 @@ enum perf_callchain_context {
 #endif
 
 struct perf_guest_info_callbacks {
-	int (*is_in_guest) (void);
-	int (*is_user_mode) (void);
-	unsigned long (*get_guest_ip) (void);
+	int				(*is_in_guest)(void);
+	int				(*is_user_mode)(void);
+	unsigned long			(*get_guest_ip)(void);
 };
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
@@ -652,19 +652,19 @@ struct pmu {
 	 * Start the transaction, after this ->add() doesn't need to
 	 * do schedulability tests.
 	 */
-	void (*start_txn)	(struct pmu *pmu); /* optional */
+	void (*start_txn)		(struct pmu *pmu); /* optional */
 	/*
 	 * If ->start_txn() disabled the ->add() schedulability test
 	 * then ->commit_txn() is required to perform one. On success
 	 * the transaction is closed. On error the transaction is kept
 	 * open until ->cancel_txn() is called.
 	 */
-	int  (*commit_txn)	(struct pmu *pmu); /* optional */
+	int  (*commit_txn)		(struct pmu *pmu); /* optional */
 	/*
 	 * Will cancel the transaction, assumes ->del() is called
 	 * for each successful ->add() during the transaction.
 	 */
-	void (*cancel_txn)	(struct pmu *pmu); /* optional */
+	void (*cancel_txn)		(struct pmu *pmu); /* optional */
 };
 
 /**
@@ -712,15 +712,15 @@ typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
 					struct pt_regs *regs);
 
 enum perf_group_flag {
-	PERF_GROUP_SOFTWARE = 0x1,
+	PERF_GROUP_SOFTWARE		= 0x1,
 };
 
-#define SWEVENT_HLIST_BITS	8
-#define SWEVENT_HLIST_SIZE	(1 << SWEVENT_HLIST_BITS)
+#define SWEVENT_HLIST_BITS		8
+#define SWEVENT_HLIST_SIZE		(1 << SWEVENT_HLIST_BITS)
 
 struct swevent_hlist {
-	struct hlist_head	heads[SWEVENT_HLIST_SIZE];
-	struct rcu_head		rcu_head;
+	struct hlist_head		heads[SWEVENT_HLIST_SIZE];
+	struct rcu_head			rcu_head;
 };
 
 #define PERF_ATTACH_CONTEXT	0x01
@@ -733,13 +733,13 @@ struct swevent_hlist {
  * This is a per-cpu dynamically allocated data structure.
  */
 struct perf_cgroup_info {
-	u64 time;
-	u64 timestamp;
+	u64				time;
+	u64				timestamp;
 };
 
 struct perf_cgroup {
-	struct cgroup_subsys_state css;
-	struct perf_cgroup_info *info;	/* timing info, one per cpu */
+	struct				cgroup_subsys_state css;
+	struct				perf_cgroup_info *info;	/* timing info, one per cpu */
 };
 #endif
 
@@ -923,7 +923,7 @@ struct perf_event_context {
 
 /*
  * Number of contexts where an event can trigger:
- * 	task, softirq, hardirq, nmi.
+ *	task, softirq, hardirq, nmi.
  */
 #define PERF_NR_CONTEXTS	4
 
@@ -1001,8 +1001,7 @@ struct perf_sample_data {
 	struct perf_raw_record		*raw;
 };
 
-static inline
-void perf_sample_data_init(struct perf_sample_data *data, u64 addr)
+static inline void perf_sample_data_init(struct perf_sample_data *data, u64 addr)
 {
 	data->addr = addr;
 	data->raw  = NULL;
@@ -1039,8 +1038,7 @@ extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
-static inline void
-perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
+static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
 #endif
 
 /*
@@ -1080,8 +1078,7 @@ static inline void perf_event_task_sched_in(struct task_struct *task)
 		__perf_event_task_sched_in(task);
 }
 
-static inline
-void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
+static inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
 
@@ -1099,14 +1096,10 @@ extern void perf_event_fork(struct task_struct *tsk);
 /* Callchains */
 DECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);
 
-extern void perf_callchain_user(struct perf_callchain_entry *entry,
-				struct pt_regs *regs);
-extern void perf_callchain_kernel(struct perf_callchain_entry *entry,
-				  struct pt_regs *regs);
-
+extern void perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs);
+extern void perf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs);
 
-static inline void
-perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
+static inline void perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
 {
 	if (entry->nr < PERF_MAX_STACK_DEPTH)
 		entry->ip[entry->nr++] = ip;
@@ -1142,9 +1135,9 @@ extern void perf_tp_event(u64 addr, u64 count, void *record,
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags
-#define perf_misc_flags(regs)	(user_mode(regs) ? PERF_RECORD_MISC_USER : \
-				 PERF_RECORD_MISC_KERNEL)
-#define perf_instruction_pointer(regs)	instruction_pointer(regs)
+# define perf_misc_flags(regs) \
+		(user_mode(regs) ? PERF_RECORD_MISC_USER : PERF_RECORD_MISC_KERNEL)
+# define perf_instruction_pointer(regs)	instruction_pointer(regs)
 #endif
 
 extern int perf_output_begin(struct perf_output_handle *handle,
@@ -1179,9 +1172,9 @@ static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
 static inline int perf_register_guest_info_callbacks
-(struct perf_guest_info_callbacks *callbacks) { return 0; }
+(struct perf_guest_info_callbacks *callbacks)				{ return 0; }
 static inline int perf_unregister_guest_info_callbacks
-(struct perf_guest_info_callbacks *callbacks) { return 0; }
+(struct perf_guest_info_callbacks *callbacks)				{ return 0; }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
@@ -1194,23 +1187,22 @@ static inline void perf_event_disable(struct perf_event *event)		{ }
 static inline void perf_event_task_tick(void)				{ }
 #endif
 
-#define perf_output_put(handle, x) \
-	perf_output_copy((handle), &(x), sizeof(x))
+#define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))
 
 /*
  * This has to have a higher priority than migration_notifier in sched.c.
  */
-#define perf_cpu_notifier(fn)					\
-do {								\
-	static struct notifier_block fn##_nb __cpuinitdata =	\
-		{ .notifier_call = fn, .priority = CPU_PRI_PERF }; \
-	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,		\
-		(void *)(unsigned long)smp_processor_id());	\
-	fn(&fn##_nb, (unsigned long)CPU_STARTING,		\
-		(void *)(unsigned long)smp_processor_id());	\
-	fn(&fn##_nb, (unsigned long)CPU_ONLINE,			\
-		(void *)(unsigned long)smp_processor_id());	\
-	register_cpu_notifier(&fn##_nb);			\
+#define perf_cpu_notifier(fn)						\
+do {									\
+	static struct notifier_block fn##_nb __cpuinitdata =		\
+		{ .notifier_call = fn, .priority = CPU_PRI_PERF };	\
+	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,			\
+		(void *)(unsigned long)smp_processor_id());		\
+	fn(&fn##_nb, (unsigned long)CPU_STARTING,			\
+		(void *)(unsigned long)smp_processor_id());		\
+	fn(&fn##_nb, (unsigned long)CPU_ONLINE,				\
+		(void *)(unsigned long)smp_processor_id());		\
+	register_cpu_notifier(&fn##_nb);				\
 } while (0)
 
 #endif /* __KERNEL__ */

commit 8f62242246351b5a4bc0c1f00c0c7003edea128a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 29 13:19:47 2011 +0200

    perf events: Add generic front-end and back-end stalled cycle event definitions
    
    Add two generic hardware events: front-end and back-end stalled cycles.
    
    These events measure conditions when the CPU is executing code but its
    capabilities are not fully utilized. Understanding such situations and
    analyzing them is an important sub-task of code optimization workflows.
    
    Both events limit performance: most front end stalls tend to be caused
    by branch misprediction or instruction fetch cachemisses, backend
    stalls can be caused by various resource shortages or inefficient
    instruction scheduling.
    
    Front-end stalls are the more important ones: code cannot run fast
    if the instruction stream is not being kept up.
    
    An over-utilized back-end can cause front-end stalls and thus
    has to be kept an eye on as well.
    
    The exact composition is very program logic and instruction mix
    dependent.
    
    We use the terms 'stall', 'front-end' and 'back-end' loosely and
    try to use the best available events from specific CPUs that
    approximate these concepts.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/n/tip-7y40wib8n000io7hjpn1dsrm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ac636dd20a0c..4e2d7ae71499 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -52,7 +52,8 @@ enum perf_hw_id {
 	PERF_COUNT_HW_BRANCH_INSTRUCTIONS	= 4,
 	PERF_COUNT_HW_BRANCH_MISSES		= 5,
 	PERF_COUNT_HW_BUS_CYCLES		= 6,
-	PERF_COUNT_HW_STALLED_CYCLES		= 7,
+	PERF_COUNT_HW_STALLED_CYCLES_FRONTEND	= 7,
+	PERF_COUNT_HW_STALLED_CYCLES_BACKEND	= 8,
 
 	PERF_COUNT_HW_MAX,			/* non-ABI */
 };

commit 32673822e440eb92eb334631eb0a199d0c532d13
Merge: fa7b69475a6c 5373db886b79
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 27 10:38:30 2011 +0200

    Merge branch 'tip/perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/core
    
    Conflicts:
            include/linux/perf_event.h
    
    Merge reason: pick up the latest jump-label enhancements, they are cooked ready.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 94403f8863d0d1d2005291b2ef0719c2534aa303
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Apr 24 08:18:31 2011 +0200

    perf events: Add stalled cycles generic event - PERF_COUNT_HW_STALLED_CYCLES
    
    The new PERF_COUNT_HW_STALLED_CYCLES event tries to approximate
    cycles the CPU does nothing useful, because it is stalled on a
    cache-miss or some other condition.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/n/tip-fue11vymwqsoo5to72jxxjyl@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ee9f1e782800..ac636dd20a0c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -52,6 +52,7 @@ enum perf_hw_id {
 	PERF_COUNT_HW_BRANCH_INSTRUCTIONS	= 4,
 	PERF_COUNT_HW_BRANCH_MISSES		= 5,
 	PERF_COUNT_HW_BUS_CYCLES		= 6,
+	PERF_COUNT_HW_STALLED_CYCLES		= 7,
 
 	PERF_COUNT_HW_MAX,			/* non-ABI */
 };

commit 42933bac11e811f02200c944d8562a15f8ec4ff0
Merge: 2b9accbee563 25985edcedea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 7 11:14:49 2011 -0700

    Merge branch 'for-linus2' of git://git.profusion.mobi/users/lucas/linux-2.6
    
    * 'for-linus2' of git://git.profusion.mobi/users/lucas/linux-2.6:
      Fix common misspellings

commit d430d3d7e646eb1eac2bb4aa244a644312e67c76
Author: Jason Baron <jbaron@redhat.com>
Date:   Wed Mar 16 17:29:47 2011 -0400

    jump label: Introduce static_branch() interface
    
    Introduce:
    
    static __always_inline bool static_branch(struct jump_label_key *key);
    
    instead of the old JUMP_LABEL(key, label) macro.
    
    In this way, jump labels become really easy to use:
    
    Define:
    
            struct jump_label_key jump_key;
    
    Can be used as:
    
            if (static_branch(&jump_key))
                    do unlikely code
    
    enable/disale via:
    
            jump_label_inc(&jump_key);
            jump_label_dec(&jump_key);
    
    that's it!
    
    For the jump labels disabled case, the static_branch() becomes an
    atomic_read(), and jump_label_inc()/dec() are simply atomic_inc(),
    atomic_dec() operations. We show testing results for this change below.
    
    Thanks to H. Peter Anvin for suggesting the 'static_branch()' construct.
    
    Since we now require a 'struct jump_label_key *key', we can store a pointer into
    the jump table addresses. In this way, we can enable/disable jump labels, in
    basically constant time. This change allows us to completely remove the previous
    hashtable scheme. Thanks to Peter Zijlstra for this re-write.
    
    Testing:
    
    I ran a series of 'tbench 20' runs 5 times (with reboots) for 3
    configurations, where tracepoints were disabled.
    
    jump label configured in
    avg: 815.6
    
    jump label *not* configured in (using atomic reads)
    avg: 800.1
    
    jump label *not* configured in (regular reads)
    avg: 803.4
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110316212947.GA8792@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Suggested-by: H. Peter Anvin <hpa@linux.intel.com>
    Tested-by: David Daney <ddaney@caviumnetworks.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 311b4dc785a1..730b7821690f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -505,7 +505,7 @@ struct perf_guest_info_callbacks {
 #include <linux/ftrace.h>
 #include <linux/cpu.h>
 #include <linux/irq_work.h>
-#include <linux/jump_label_ref.h>
+#include <linux/jump_label.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
 
@@ -1034,7 +1034,7 @@ static inline int is_software_event(struct perf_event *event)
 	return event->pmu->task_ctx_nr == perf_sw_context;
 }
 
-extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];
+extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
@@ -1063,22 +1063,21 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 {
 	struct pt_regs hot_regs;
 
-	JUMP_LABEL(&perf_swevent_enabled[event_id], have_event);
-	return;
-
-have_event:
-	if (!regs) {
-		perf_fetch_caller_regs(&hot_regs);
-		regs = &hot_regs;
+	if (static_branch(&perf_swevent_enabled[event_id])) {
+		if (!regs) {
+			perf_fetch_caller_regs(&hot_regs);
+			regs = &hot_regs;
+		}
+		__perf_sw_event(event_id, nr, nmi, regs, addr);
 	}
-	__perf_sw_event(event_id, nr, nmi, regs, addr);
 }
 
-extern atomic_t perf_sched_events;
+extern struct jump_label_key perf_sched_events;
 
 static inline void perf_event_task_sched_in(struct task_struct *task)
 {
-	COND_STMT(&perf_sched_events, __perf_event_task_sched_in(task));
+	if (static_branch(&perf_sched_events))
+		__perf_event_task_sched_in(task);
 }
 
 static inline
@@ -1086,7 +1085,8 @@ void perf_event_task_sched_out(struct task_struct *task, struct task_struct *nex
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
 
-	COND_STMT(&perf_sched_events, __perf_event_task_sched_out(task, next));
+	if (static_branch(&perf_sched_events))
+		__perf_event_task_sched_out(task, next);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 311b4dc785a1..393b60c71732 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -662,7 +662,7 @@ struct pmu {
 	int  (*commit_txn)	(struct pmu *pmu); /* optional */
 	/*
 	 * Will cancel the transaction, assumes ->del() is called
-	 * for each successfull ->add() during the transaction.
+	 * for each successful ->add() during the transaction.
 	 */
 	void (*cancel_txn)	(struct pmu *pmu); /* optional */
 };

commit ab711fe08297de1485fff0a366e6db8828cafd6a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 31 10:29:26 2011 +0200

    perf: Fix task context scheduling
    
    Jiri reported:
    
     |
     | - once an event is created by sys_perf_event_open, task context
     |   is created and it stays even if the event is closed, until the
     |   task is finished ... thats what I see in code and I assume it's
     |   correct
     |
     | - when the task opens event, perf_sched_events jump label is
     |   incremented and following callbacks are started from scheduler
     |
     |         __perf_event_task_sched_in
     |         __perf_event_task_sched_out
     |
     |   These callback *in/out set/unset cpuctx->task_ctx value to the
     |   task context.
     |
     | - close is called on event on CPU 0:
     |         - the task is scheduled on CPU 0
     |         - __perf_event_task_sched_in is called
     |         - cpuctx->task_ctx is set
     |         - perf_sched_events jump label is decremented and == 0
     |         - __perf_event_task_sched_out is not called
     |         - cpuctx->task_ctx on CPU 0 stays set
     |
     | - exit is called on CPU 1:
     |         - the task is scheduled on CPU 1
     |         - perf_event_exit_task is called
     |         - task_ctx_sched_out unsets cpuctx->task_ctx on CPU 1
     |         - put_ctx destroys the context
     |
     | - another call of perf_rotate_context on CPU 0 will use invalid
     |   task_ctx pointer, and eventualy panic.
     |
    
    Cure this the simplest possibly way by partially reverting the
    jump_label optimization for the sched_out case.
    
    Reported-and-tested-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: <stable@kernel.org> # .37+
    LKML-Reference: <1301520405.4859.213.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 311b4dc785a1..04d75a8a20ee 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1086,7 +1086,7 @@ void perf_event_task_sched_out(struct task_struct *task, struct task_struct *nex
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
 
-	COND_STMT(&perf_sched_events, __perf_event_task_sched_out(task, next));
+	__perf_event_task_sched_out(task, next);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);

commit 68cacd29167b1926d237bd1b153aa2a990201729
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Mar 23 16:03:06 2011 +0100

    perf_events: Fix stale ->cgrp pointer in update_cgrp_time_from_cpuctx()
    
    This patch solves a stale pointer problem in
    update_cgrp_time_from_cpuctx(). The cpuctx->cgrp
    was not cleared on all possible event exit paths,
    including:
    
       close()
         perf_release()
           perf_release_kernel()
             list_del_event()
    
    This patch fixes list_del_event() to clear cpuctx->cgrp
    when there are no cgroup events left in the context.
    
    [ This second version makes the code compile when
      CONFIG_CGROUP_PERF is not enabled. We unconditionally define
      perf_cpu_context->cgrp. ]
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: perfmon2-devel@lists.sf.net
    Cc: paulus@samba.org
    Cc: davem@davemloft.net
    LKML-Reference: <20110323150306.GA1580@quad>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f495c0147240..311b4dc785a1 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -938,9 +938,7 @@ struct perf_cpu_context {
 	struct list_head		rotation_list;
 	int				jiffies_interval;
 	struct pmu			*active_pmu;
-#ifdef CONFIG_CGROUP_PERF
 	struct perf_cgroup		*cgrp;
-#endif
 };
 
 struct perf_output_handle {

commit ee643c4179c3a18b018de3a4c07a7bb3a75c8e4e
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Mon Mar 7 15:46:59 2011 +0000

    perf: Reorder & optimize perf_event_context to remove alignment padding on 64 bit builds
    
    Remove 8 bytes of alignment padding from perf_event_context on 64 bit
    builds which shrinks its size to 192 bytes allowing it to fit into one
    fewer cache lines and into a smaller slab.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1299512819.2039.5.camel@castor.rsk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 614615b8d42b..f495c0147240 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -878,8 +878,8 @@ enum perf_event_context_type {
  * Used as a container for task events and CPU events as well:
  */
 struct perf_event_context {
-	enum perf_event_context_type	type;
 	struct pmu			*pmu;
+	enum perf_event_context_type	type;
 	/*
 	 * Protect the states of the events in the list,
 	 * nr_active, and the list:

commit a7e3ed1e470116c9d12c2f778431a481a6be8ab6
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Mar 3 10:34:47 2011 +0800

    perf: Add support for supplementary event registers
    
    Change logs against Andi's original version:
    
    - Extends perf_event_attr:config to config{,1,2} (Peter Zijlstra)
    - Fixed a major event scheduling issue. There cannot be a ref++ on an
      event that has already done ref++ once and without calling
      put_constraint() in between. (Stephane Eranian)
    - Use thread_cpumask for percore allocation. (Lin Ming)
    - Use MSR names in the extra reg lists. (Lin Ming)
    - Remove redundant "c = NULL" in intel_percore_constraints
    - Fix comment of perf_event_attr::config1
    
    Intel Nehalem/Westmere have a special OFFCORE_RESPONSE event
    that can be used to monitor any offcore accesses from a core.
    This is a very useful event for various tunings, and it's
    also needed to implement the generic LLC-* events correctly.
    
    Unfortunately this event requires programming a mask in a separate
    register. And worse this separate register is per core, not per
    CPU thread.
    
    This patch:
    
    - Teaches perf_events that OFFCORE_RESPONSE needs extra parameters.
      The extra parameters are passed by user space in the
      perf_event_attr::config1 field.
    
    - Adds support to the Intel perf_event core to schedule per
      core resources. This adds fairly generic infrastructure that
      can be also used for other per core resources.
      The basic code has is patterned after the similar AMD northbridge
      constraints code.
    
    Thanks to Stephane Eranian who pointed out some problems
    in the original version and suggested improvements.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Lin Ming <ming.m.lin@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1299119690-13991-2-git-send-email-ming.m.lin@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8ceb5a6fd9c9..614615b8d42b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -225,8 +225,14 @@ struct perf_event_attr {
 	};
 
 	__u32			bp_type;
-	__u64			bp_addr;
-	__u64			bp_len;
+	union {
+		__u64		bp_addr;
+		__u64		config1; /* extension of config */
+	};
+	union {
+		__u64		bp_len;
+		__u64		config2; /* extension of config1 */
+	};
 };
 
 /*
@@ -541,6 +547,9 @@ struct hw_perf_event {
 			unsigned long	event_base;
 			int		idx;
 			int		last_cpu;
+			unsigned int	extra_reg;
+			u64		extra_config;
+			int		extra_alloc;
 		};
 		struct { /* software */
 			struct hrtimer	hrtimer;

commit 163ec4354a5135c6c38c3f4a9b46a31900ebdf48
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 16 11:22:34 2011 +0100

    perf: Optimize throttling code
    
    By pre-computing the maximum number of samples per tick we can avoid a
    multiplication and a conditional since MAX_INTERRUPTS >
    max_samples_per_tick.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 38c8b2554842..8ceb5a6fd9c9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1110,6 +1110,10 @@ extern int sysctl_perf_event_paranoid;
 extern int sysctl_perf_event_mlock;
 extern int sysctl_perf_event_sample_rate;
 
+extern int perf_proc_update_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 static inline bool perf_paranoid_tracepoint_raw(void)
 {
 	return sysctl_perf_event_paranoid > -1;

commit e5d1367f17ba6a6fed5fd8b74e4d5720923e0c25
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Feb 14 11:20:01 2011 +0200

    perf: Add cgroup support
    
    This kernel patch adds the ability to filter monitoring based on
    container groups (cgroups). This is for use in per-cpu mode only.
    
    The cgroup to monitor is passed as a file descriptor in the pid
    argument to the syscall. The file descriptor must be opened to
    the cgroup name in the cgroup filesystem. For instance, if the
    cgroup name is foo and cgroupfs is mounted in /cgroup, then the
    file descriptor is opened to /cgroup/foo. Cgroup mode is
    activated by passing PERF_FLAG_PID_CGROUP in the flags argument
    to the syscall.
    
    For instance to measure in cgroup foo on CPU1 assuming
    cgroupfs is mounted under /cgroup:
    
    struct perf_event_attr attr;
    int cgroup_fd, fd;
    
    cgroup_fd = open("/cgroup/foo", O_RDONLY);
    fd = perf_event_open(&attr, cgroup_fd, 1, -1, PERF_FLAG_PID_CGROUP);
    close(cgroup_fd);
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    [ added perf_cgroup_{exit,attach} ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4d590250.114ddf0a.689e.4482@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index dda5b0a3ff60..38c8b2554842 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -464,6 +464,7 @@ enum perf_callchain_context {
 
 #define PERF_FLAG_FD_NO_GROUP	(1U << 0)
 #define PERF_FLAG_FD_OUTPUT	(1U << 1)
+#define PERF_FLAG_PID_CGROUP	(1U << 2) /* pid=cgroup id, per-cpu mode only */
 
 #ifdef __KERNEL__
 /*
@@ -471,6 +472,7 @@ enum perf_callchain_context {
  */
 
 #ifdef CONFIG_PERF_EVENTS
+# include <linux/cgroup.h>
 # include <asm/perf_event.h>
 # include <asm/local64.h>
 #endif
@@ -716,6 +718,22 @@ struct swevent_hlist {
 #define PERF_ATTACH_GROUP	0x02
 #define PERF_ATTACH_TASK	0x04
 
+#ifdef CONFIG_CGROUP_PERF
+/*
+ * perf_cgroup_info keeps track of time_enabled for a cgroup.
+ * This is a per-cpu dynamically allocated data structure.
+ */
+struct perf_cgroup_info {
+	u64 time;
+	u64 timestamp;
+};
+
+struct perf_cgroup {
+	struct cgroup_subsys_state css;
+	struct perf_cgroup_info *info;	/* timing info, one per cpu */
+};
+#endif
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -832,6 +850,11 @@ struct perf_event {
 	struct event_filter		*filter;
 #endif
 
+#ifdef CONFIG_CGROUP_PERF
+	struct perf_cgroup		*cgrp; /* cgroup event is attach to */
+	int				cgrp_defer_enabled;
+#endif
+
 #endif /* CONFIG_PERF_EVENTS */
 };
 
@@ -886,6 +909,7 @@ struct perf_event_context {
 	u64				generation;
 	int				pin_count;
 	struct rcu_head			rcu_head;
+	int				nr_cgroups; /* cgroup events present */
 };
 
 /*
@@ -905,6 +929,9 @@ struct perf_cpu_context {
 	struct list_head		rotation_list;
 	int				jiffies_interval;
 	struct pmu			*active_pmu;
+#ifdef CONFIG_CGROUP_PERF
+	struct perf_cgroup		*cgrp;
+#endif
 };
 
 struct perf_output_handle {
@@ -1040,11 +1067,11 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 	__perf_sw_event(event_id, nr, nmi, regs, addr);
 }
 
-extern atomic_t perf_task_events;
+extern atomic_t perf_sched_events;
 
 static inline void perf_event_task_sched_in(struct task_struct *task)
 {
-	COND_STMT(&perf_task_events, __perf_event_task_sched_in(task));
+	COND_STMT(&perf_sched_events, __perf_event_task_sched_in(task));
 }
 
 static inline
@@ -1052,7 +1079,7 @@ void perf_event_task_sched_out(struct task_struct *task, struct task_struct *nex
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
 
-	COND_STMT(&perf_task_events, __perf_event_task_sched_out(task, next));
+	COND_STMT(&perf_sched_events, __perf_event_task_sched_out(task, next));
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);

commit abe43400579d5de0078c2d3a760e6598e183f871
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Nov 17 23:17:37 2010 +0100

    perf: Sysfs enumeration
    
    Simple sysfs emumeration of the PMUs.
    
    Use a "event_source" bus, and add PMU devices using their name.
    
    Each PMU device has a type attribute which contrains the value needed
    for perf_event_attr::type to identify this PMU.
    
    This is the minimal stub needed to start using this interface,
    we'll consider extending the sysfs usage later.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Greg KH <gregkh@suse.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101117222056.316982569@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 21206d27466b..dda5b0a3ff60 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -588,6 +588,7 @@ struct perf_event;
 struct pmu {
 	struct list_head		entry;
 
+	struct device			*dev;
 	char				*name;
 	int				type;
 

commit 2e80a82a49c4c7eca4e35734380f28298ba5db19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Nov 17 23:17:36 2010 +0100

    perf: Dynamic pmu types
    
    Extend the perf_pmu_register() interface to allow for named and
    dynamic pmu types.
    
    Because we need to support the existing static types we cannot use
    dynamic types for everything, hence provide a type argument.
    
    If we want to enumerate the PMUs they need a name, provide one.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101117222056.259707703@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 30e50e2c7f30..21206d27466b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -588,6 +588,9 @@ struct perf_event;
 struct pmu {
 	struct list_head		entry;
 
+	char				*name;
+	int				type;
+
 	int * __percpu			pmu_disable_count;
 	struct perf_cpu_context * __percpu pmu_cpu_context;
 	int				task_ctx_nr;
@@ -916,7 +919,7 @@ struct perf_output_handle {
 
 #ifdef CONFIG_PERF_EVENTS
 
-extern int perf_pmu_register(struct pmu *pmu);
+extern int perf_pmu_register(struct pmu *pmu, char *name, int type);
 extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);

commit 006b20fe4c69189b0d854e5eabf269e50ca86cdd
Merge: 5f29805a4f46 d949750fed16
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 16 11:22:25 2010 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: We want to apply a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 5167695753c63444a9e6cbbef136200a16c7a225
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Dec 7 14:18:20 2010 +0100

    perf: Fix duplicate events with multiple-pmu vs software events
    
    Because the multi-pmu bits can share contexts between struct pmu
    instances we could get duplicate events by iterating the pmu list.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index de2c41758e29..4f1279e105ee 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -887,6 +887,7 @@ struct perf_cpu_context {
 	int				exclusive;
 	struct list_head		rotation_list;
 	int				jiffies_interval;
+	struct pmu			*active_pmu;
 };
 
 struct perf_output_handle {

commit c980d1091810df13f21aabbce545fd98f545bbf7
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Dec 4 23:02:20 2010 -0200

    perf events: Make sample_type identity fields available in all PERF_RECORD_ events
    
    If perf_event_attr.sample_id_all is set it will add the PERF_SAMPLE_ identity
    info:
    
    TID, TIME, ID, CPU, STREAM_ID
    
    As a trailer, so that older perf tools can process new files, just ignoring the
    extra payload.
    
    With this its possible to do further analysis on problems in the event stream,
    like detecting reordering of MMAP and FORK events, etc.
    
    V2: Fixup header size in comm, mmap and task processing, as we have to take into
    account different sample_types for each matching event, noticed by Thomas Gleixner.
    
    Thomas also noticed a problem in v2 where if we didn't had space in the buffer we
    wouldn't restore the header size.
    
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Ian Munsie <imunsie@au1.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b9950b1620d8..2814ead4adb8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -215,8 +215,9 @@ struct perf_event_attr {
 				 */
 				precise_ip     :  2, /* skid constraint       */
 				mmap_data      :  1, /* non-exec mmap data    */
+				sample_id_all  :  1, /* sample_type all events */
 
-				__reserved_1   : 46;
+				__reserved_1   : 45;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -327,6 +328,15 @@ struct perf_event_header {
 enum perf_event_type {
 
 	/*
+	 * If perf_event_attr.sample_id_all is set then all event types will
+	 * have the sample_type selected fields related to where/when
+	 * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)
+	 * described in PERF_RECORD_SAMPLE below, it will be stashed just after
+	 * the perf_event_header and the fields already present for the existing
+	 * fields, i.e. at the end of the payload. That way a newer perf.data
+	 * file will be supported by older perf tools, with these new optional
+	 * fields being ignored.
+	 *
 	 * The MMAP events record the PROT_EXEC mappings so that we can
 	 * correlate userspace IPs to code. They have the following structure:
 	 *

commit 6844c09d849aeb00e8ddfe9525e8567a531c22d0
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Dec 3 16:36:35 2010 -0200

    perf events: Separate the routines handling the PERF_SAMPLE_ identity fields
    
    Those will be made available in sample like events like MMAP, EXEC, etc in a
    followup patch. So precalculate the extra id header space and have a separate
    routine to fill them up.
    
    V2: Thomas noticed that the id header needs to be precalculated at
    inherit_events too:
    
    LKML-Reference: <alpine.LFD.2.00.1012031245220.2653@localhost6.localdomain6>
    
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Ian Munsie <imunsie@au1.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <1291318772-30880-2-git-send-email-acme@infradead.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index adf6d9931643..b9950b1620d8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -759,6 +759,7 @@ struct perf_event {
 
 	struct perf_event_attr		attr;
 	u16				header_size;
+	u16				id_header_size;
 	u16				read_size;
 	struct hw_perf_event		hw;
 

commit c320c7b7d380e630f595de1236d9d085b035d5b4
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Oct 20 12:50:11 2010 -0200

    perf events: Precalculate the header space for PERF_SAMPLE_ fields
    
    PERF_SAMPLE_{CALLCHAIN,RAW} have variable lenghts per sample, but the others
    can be precalculated, reducing a bit the per sample cost.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frédéric Weisbecker <fweisbec@gmail.com>
    Cc: Ian Munsie <imunsie@au1.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cbf04cc1e630..adf6d9931643 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -758,6 +758,8 @@ struct perf_event {
 	u64				shadow_ctx_time;
 
 	struct perf_event_attr		attr;
+	u16				header_size;
+	u16				read_size;
 	struct hw_perf_event		hw;
 
 	struct perf_event_context	*ctx;

commit 6c7e550f13f8ad82efb6a5653ae628c2543c1768
Author: Franck Bui-Huu <fbuihuu@gmail.com>
Date:   Tue Nov 23 16:21:43 2010 +0100

    perf: Introduce is_sampling_event()
    
    and use it when appropriate.
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1290525705-6265-1-git-send-email-fbuihuu@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index de2c41758e29..cbf04cc1e630 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -969,6 +969,11 @@ extern int perf_event_overflow(struct perf_event *event, int nmi,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
+static inline bool is_sampling_event(struct perf_event *event)
+{
+	return event->attr.sample_period != 0;
+}
+
 /*
  * Return 1 for a software event, 0 for a hardware event
  */

commit ee6dcfa40a50fe12a3ae0fb4d2653c66c3ed6556
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 26 13:49:04 2010 +0100

    perf: Fix the software context switch counter
    
    Stephane noticed that because the perf_sw_event() call is inside the
    perf_event_task_sched_out() call it won't get called unless we
    have a per-task counter.
    
    Reported-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 142e3d6042c7..de2c41758e29 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -909,20 +909,6 @@ extern int perf_num_counters(void);
 extern const char *perf_pmu_name(void);
 extern void __perf_event_task_sched_in(struct task_struct *task);
 extern void __perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
-
-extern atomic_t perf_task_events;
-
-static inline void perf_event_task_sched_in(struct task_struct *task)
-{
-	COND_STMT(&perf_task_events, __perf_event_task_sched_in(task));
-}
-
-static inline
-void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
-{
-	COND_STMT(&perf_task_events, __perf_event_task_sched_out(task, next));
-}
-
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -1031,6 +1017,21 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 	__perf_sw_event(event_id, nr, nmi, regs, addr);
 }
 
+extern atomic_t perf_task_events;
+
+static inline void perf_event_task_sched_in(struct task_struct *task)
+{
+	COND_STMT(&perf_task_events, __perf_event_task_sched_in(task));
+}
+
+static inline
+void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
+{
+	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
+
+	COND_STMT(&perf_task_events, __perf_event_task_sched_out(task, next));
+}
+
 extern void perf_event_mmap(struct vm_area_struct *vma);
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);

commit dddd3379a619a4cb8247bfd3c94ca9ae3797aa2e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 24 10:05:55 2010 +0100

    perf: Fix inherit vs. context rotation bug
    
    It was found that sometimes children of tasks with inherited events had
    one extra event. Eventually it turned out to be due to the list rotation
    no being exclusive with the list iteration in the inheritance code.
    
    Cure this by temporarily disabling the rotation while we inherit the events.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 40150f345982..142e3d6042c7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -850,6 +850,7 @@ struct perf_event_context {
 	int				nr_active;
 	int				is_active;
 	int				nr_stat;
+	int				rotate_disable;
 	atomic_t			refcount;
 	struct task_struct		*task;
 

commit eed01528a45dc4138e9a08064b4b6cc1a9426899
Author: Stephane Eranian <eranian@google.com>
Date:   Tue Oct 26 16:08:01 2010 +0200

    perf_events: Fix time tracking in samples
    
    This patch corrects time tracking in samples. Without this patch
    both time_enabled and time_running are bogus when user asks for
    PERF_SAMPLE_READ.
    
    One uses PERF_SAMPLE_READ to sample the values of other counters
    in each sample. Because of multiplexing, it is necessary to know
    both time_enabled, time_running to be able to scale counts correctly.
    
    In this second version of the patch, we maintain a shadow
    copy of ctx->time which allows us to compute ctx->time without
    calling update_context_time() from NMI context. We avoid the
    issue that update_context_time() must always be called with
    ctx->lock held.
    
    We do not keep shadow copies of the other event timings
    because if the lead event is overflowing then it is active
    and thus it's been scheduled in via event_sched_in() in
    which case neither tstamp_stopped, tstamp_running can be modified.
    
    This timing logic only applies to samples when PERF_SAMPLE_READ
    is used.
    
    Note that this patch does not address timing issues related
    to sampling inheritance between tasks. This will be addressed
    in a future patch.
    
    With this patch, the libpfm4 example task_smpl now reports
    correct counts (shown on 2.4GHz Core 2):
    
    $ task_smpl -p 2400000000 -e unhalted_core_cycles:u,instructions_retired:u,baclears  noploop 5
    noploop for 5 seconds
    IIP:0x000000004006d6 PID:5596 TID:5596 TIME:466,210,211,430 STREAM_ID:33 PERIOD:2,400,000,000 ENA=1,010,157,814 RUN=1,010,157,814 NR=3
            2,400,000,254 unhalted_core_cycles:u (33)
            2,399,273,744 instructions_retired:u (34)
            53,340 baclears (35)
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4cc6e14b.1e07e30a.256e.5190@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 057bf22a8323..40150f345982 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -747,6 +747,16 @@ struct perf_event {
 	u64				tstamp_running;
 	u64				tstamp_stopped;
 
+	/*
+	 * timestamp shadows the actual context timing but it can
+	 * be safely used in NMI interrupt context. It reflects the
+	 * context time as it was when the event was last scheduled in.
+	 *
+	 * ctx_time already accounts for ctx->timestamp. Therefore to
+	 * compute ctx_time for a sample, simply add perf_clock().
+	 */
+	u64				shadow_ctx_time;
+
 	struct perf_event_attr		attr;
 	struct hw_perf_event		hw;
 

commit ebf31f502492527e2b6b5e5cf85a4ebc7fc8a52e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Oct 17 12:15:00 2010 +0200

    jump_label: Add COND_STMT(), reducer wrappery
    
    The use of the JUMP_LABEL() construct ends up creating endless silly
    wrappers, create a higher level construct to reduce this clutter.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3b80cbf509ef..057bf22a8323 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -903,21 +903,13 @@ extern atomic_t perf_task_events;
 
 static inline void perf_event_task_sched_in(struct task_struct *task)
 {
-	JUMP_LABEL(&perf_task_events, have_events);
-	return;
-
-have_events:
-	__perf_event_task_sched_in(task);
+	COND_STMT(&perf_task_events, __perf_event_task_sched_in(task));
 }
 
 static inline
 void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
 {
-	JUMP_LABEL(&perf_task_events, have_events);
-	return;
-
-have_events:
-	__perf_event_task_sched_out(task, next);
+	COND_STMT(&perf_task_events, __perf_event_task_sched_out(task, next));
 }
 
 extern int perf_event_init_task(struct task_struct *child);

commit 7e54a5a0b655734326dc78c2b5efc1eb35497bb6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 22:32:45 2010 +0200

    perf: Optimize sw events
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7f0e7f52af8b..3b80cbf509ef 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1012,18 +1012,20 @@ static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 }
 
-static inline void
+static __always_inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 {
-	if (atomic_read(&perf_swevent_enabled[event_id])) {
-		struct pt_regs hot_regs;
-
-		if (!regs) {
-			perf_fetch_caller_regs(&hot_regs);
-			regs = &hot_regs;
-		}
-		__perf_sw_event(event_id, nr, nmi, regs, addr);
+	struct pt_regs hot_regs;
+
+	JUMP_LABEL(&perf_swevent_enabled[event_id], have_event);
+	return;
+
+have_event:
+	if (!regs) {
+		perf_fetch_caller_regs(&hot_regs);
+		regs = &hot_regs;
 	}
+	__perf_sw_event(event_id, nr, nmi, regs, addr);
 }
 
 extern void perf_event_mmap(struct vm_area_struct *vma);

commit 82cd6def9806dcb6a325fb6abbc1d61388a15f6a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 17:57:23 2010 +0200

    perf: Use jump_labels to optimize the scheduler hooks
    
    Trades a call + conditional + ret for an unconditional jmp.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101014203625.501657727@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 97965fac55fe..7f0e7f52af8b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -487,6 +487,7 @@ struct perf_guest_info_callbacks {
 #include <linux/ftrace.h>
 #include <linux/cpu.h>
 #include <linux/irq_work.h>
+#include <linux/jump_label_ref.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
 
@@ -895,8 +896,30 @@ extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern int perf_num_counters(void);
 extern const char *perf_pmu_name(void);
-extern void perf_event_task_sched_in(struct task_struct *task);
-extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
+extern void __perf_event_task_sched_in(struct task_struct *task);
+extern void __perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
+
+extern atomic_t perf_task_events;
+
+static inline void perf_event_task_sched_in(struct task_struct *task)
+{
+	JUMP_LABEL(&perf_task_events, have_events);
+	return;
+
+have_events:
+	__perf_event_task_sched_in(task);
+}
+
+static inline
+void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
+{
+	JUMP_LABEL(&perf_task_events, have_events);
+	return;
+
+have_events:
+	__perf_event_task_sched_out(task, next);
+}
+
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);

commit d580ff8699e8811a9af37e9de4dea375401bdeec
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 17:43:23 2010 +0200

    perf, hw_breakpoint: Fix crash in hw_breakpoint creation
    
    hw_breakpoint creation needs to account stuff per-task to ensure there
    is always sufficient hardware resources to back these things due to
    ptrace.
    
    With the perf per pmu context changes the event initialization no
    longer has access to the event context, for the simple reason that we
    need to first find the pmu (result of initialization) before we can
    find the context.
    
    This makes hw_breakpoints unhappy, because it can no longer do per
    task accounting, cure this by frobbing a task pointer in the event::hw
    bits for now...
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20101014203625.391543667@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2ebfc9ae4755..97965fac55fe 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -536,6 +536,12 @@ struct hw_perf_event {
 		struct { /* breakpoint */
 			struct arch_hw_breakpoint	info;
 			struct list_head		bp_list;
+			/*
+			 * Crufty hack to avoid the chicken and egg
+			 * problem hw_breakpoint has with context
+			 * creation and event initalization.
+			 */
+			struct task_struct		*bp_target;
 		};
 #endif
 	};
@@ -693,6 +699,7 @@ struct swevent_hlist {
 
 #define PERF_ATTACH_CONTEXT	0x01
 #define PERF_ATTACH_GROUP	0x02
+#define PERF_ATTACH_TASK	0x04
 
 /**
  * struct perf_event - performance event kernel representation:

commit e360adbe29241a0194e10e20595360dd7b98a2b3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 14 14:01:34 2010 +0800

    irq_work: Add generic hardirq context callbacks
    
    Provide a mechanism that allows running code in IRQ context. It is
    most useful for NMI code that needs to interact with the rest of the
    system -- like wakeup a task to drain buffers.
    
    Perf currently has such a mechanism, so extract that and provide it as
    a generic feature, independent of perf so that others may also
    benefit.
    
    The IRQ context callback is generated through self-IPIs where
    possible, or on architectures like powerpc the decrementer (the
    built-in timer facility) is set to generate an interrupt immediately.
    
    Architectures that don't have anything like this get to do with a
    callback from the timer tick. These architectures can call
    irq_work_run() at the tail of any IRQ handlers that might enqueue such
    work (like the perf IRQ handler) to avoid undue latencies in
    processing the work.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    [ various fixes ]
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    LKML-Reference: <1287036094.7768.291.camel@yhuang-dev>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a9227e985207..2ebfc9ae4755 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -486,6 +486,7 @@ struct perf_guest_info_callbacks {
 #include <linux/workqueue.h>
 #include <linux/ftrace.h>
 #include <linux/cpu.h>
+#include <linux/irq_work.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
 
@@ -672,11 +673,6 @@ struct perf_buffer {
 	void				*data_pages[0];
 };
 
-struct perf_pending_entry {
-	struct perf_pending_entry *next;
-	void (*func)(struct perf_pending_entry *);
-};
-
 struct perf_sample_data;
 
 typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
@@ -784,7 +780,7 @@ struct perf_event {
 	int				pending_wakeup;
 	int				pending_kill;
 	int				pending_disable;
-	struct perf_pending_entry	pending;
+	struct irq_work			pending;
 
 	atomic_t			event_limit;
 
@@ -898,8 +894,6 @@ extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
 extern void perf_event_delayed_put(struct task_struct *task);
-extern void set_perf_event_pending(void);
-extern void perf_event_do_pending(void);
 extern void perf_event_print_debug(void);
 extern void perf_pmu_disable(struct pmu *pmu);
 extern void perf_pmu_enable(struct pmu *pmu);
@@ -1078,7 +1072,6 @@ static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
 static inline void perf_event_delayed_put(struct task_struct *task)	{ }
-static inline void perf_event_do_pending(void)				{ }
 static inline void perf_event_print_debug(void)				{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }

commit 6268464b370e234e0255330190f9bd5d19386ad7
Merge: 7df01d96b295 0fdf13606b67
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Oct 15 12:45:00 2010 +0200

    Merge remote branch 'tip/perf/core' into oprofile/core
    
    Conflicts:
            arch/arm/oprofile/common.c
            kernel/perf_event.c

commit 84c7991059c9c4530cc911137c5bf508a41ed129
Author: Matt Fleming <matt@console-pimps.org>
Date:   Sun Oct 3 21:41:13 2010 +0100

    perf: New helper function for pmu name
    
    Introduce perf_pmu_name() helper function that returns the name of the
    pmu. This gives us a generic way to get the name of a pmu regardless of
    how an architecture identifies it internally.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1a0219247183..33f08dafda2f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -850,6 +850,7 @@ extern int perf_max_events;
 extern const struct pmu *hw_perf_event_init(struct perf_event *event);
 
 extern int perf_num_counters(void);
+extern const char *perf_pmu_name(void);
 extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
 extern void perf_event_task_tick(struct task_struct *task);

commit 3bf101ba42a1c89b5afbc7492e7647dae5e18735
Author: Matt Fleming <matt@console-pimps.org>
Date:   Mon Sep 27 20:22:24 2010 +0100

    perf: Add helper function to return number of counters
    
    The number of counters for the registered pmu is needed in a few places
    so provide a helper function that returns this number.
    
    Signed-off-by: Matt Fleming <matt@console-pimps.org>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 716f99b682c1..1a0219247183 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -849,6 +849,7 @@ extern int perf_max_events;
 
 extern const struct pmu *hw_perf_event_init(struct perf_event *event);
 
+extern int perf_num_counters(void);
 extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
 extern void perf_event_task_tick(struct task_struct *task);

commit e9d2b064149ff7ef4acbc65a1b9374ac8b218d3e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Sep 17 11:28:50 2010 +0200

    perf: Undo the per cpu-context timer stuff
    
    Revert the timer per cpu-context timers because of unfortunate
    nohz interaction. Fixing that would have been somewhat ugly, so
    go back to driving things from the regular tick. Provide a
    jiffies interval feature for people who want slower rotations.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <20100917093009.519845633@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 165287fd2cc4..61b1e2d760fd 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -870,8 +870,8 @@ struct perf_cpu_context {
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
 	int				exclusive;
-	u64				timer_interval;
-	struct hrtimer			timer;
+	struct list_head		rotation_list;
+	int				jiffies_interval;
 };
 
 struct perf_output_handle {
@@ -1065,6 +1065,7 @@ extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
 extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
+extern void perf_event_task_tick(void);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *task)			{ }
@@ -1099,6 +1100,7 @@ static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }
 static inline void perf_swevent_put_recursion_context(int rctx)		{ }
 static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }
+static inline void perf_event_task_tick(void)				{ }
 #endif
 
 #define perf_output_put(handle, x) \

commit b04243ef7006cda301819f54ee7ce0a3632489e3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Sep 17 11:28:48 2010 +0200

    perf: Complete software pmu grouping
    
    Aside from allowing software events into a !software group,
    allow adding !software events to pure software groups.
    
    Once we've moved the software group and attached the first
    !software event, the group will no longer be a pure software
    group and hence no longer be eligible for movement, at which
    point the straight ctx comparison is correct again.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20100917093009.410784731@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 39d8860b2684..165287fd2cc4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -804,12 +804,18 @@ struct perf_event {
 #endif /* CONFIG_PERF_EVENTS */
 };
 
+enum perf_event_context_type {
+	task_context,
+	cpu_context,
+};
+
 /**
  * struct perf_event_context - event context structure
  *
  * Used as a container for task events and CPU events as well:
  */
 struct perf_event_context {
+	enum perf_event_context_type	type;
 	struct pmu			*pmu;
 	/*
 	 * Protect the states of the events in the list,

commit 38a81da2205f94e8a2a834b51a6b99c91fc7c2e8
Author: Matt Helsley <matthltc@us.ibm.com>
Date:   Mon Sep 13 13:01:20 2010 -0700

    perf events: Clean up pid passing
    
    The kernel perf event creation path shouldn't use find_task_by_vpid()
    because a vpid exists in a specific namespace. find_task_by_vpid() uses
    current's pid namespace which isn't always the correct namespace to use
    for the vpid in all the places perf_event_create_kernel_counter() (and
    thus find_get_context()) is called.
    
    The goal is to clean up pid namespace handling and prevent bugs like:
    
            https://bugzilla.kernel.org/show_bug.cgi?id=17281
    
    Instead of using pids switch find_get_context() to use task struct
    pointers directly. The syscall is responsible for resolving the pid to
    a task struct. This moves the pid namespace resolution into the syscall
    much like every other syscall that takes pid parameters.
    
    Signed-off-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robin Green <greenrd@greenrd.org>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    LKML-Reference: <a134e5e392ab0204961fd1a62c84a222bf5874a9.1284407763.git.matthltc@us.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 93bf53aa50e5..39d8860b2684 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -902,7 +902,7 @@ extern int perf_event_release_kernel(struct perf_event *event);
 extern struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				int cpu,
-				pid_t pid,
+				struct task_struct *task,
 				perf_overflow_handler_t callback);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);

commit 4e231c7962ce711c7d8c2a4dc23ecd1e8fc28363
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 9 21:01:59 2010 +0200

    perf: Fix up delayed_put_task_struct()
    
    I missed a perf_event_ctxp user when converting it to an array. Pull this
    last user into perf_event.c as well and fix it up.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c1173520f14d..93bf53aa50e5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -889,6 +889,7 @@ extern void perf_event_task_sched_out(struct task_struct *task, struct task_stru
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
+extern void perf_event_delayed_put(struct task_struct *task);
 extern void set_perf_event_pending(void);
 extern void perf_event_do_pending(void);
 extern void perf_event_print_debug(void);
@@ -1067,6 +1068,7 @@ perf_event_task_sched_out(struct task_struct *task,
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
+static inline void perf_event_delayed_put(struct task_struct *task)	{ }
 static inline void perf_event_do_pending(void)				{ }
 static inline void perf_event_print_debug(void)				{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }

commit 89a1e18731959e9953fae15ddc1a983eb15a4f19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 7 17:34:50 2010 +0200

    perf: Provide a separate task context for swevents
    
    Since software events are always schedulable, mixing them up with
    hardware events (who are not) can lead to funny scheduling oddities.
    
    Giving them their own context solves this.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9ecfd856ce6e..c1173520f14d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -952,14 +952,7 @@ extern int perf_event_overflow(struct perf_event *event, int nmi,
  */
 static inline int is_software_event(struct perf_event *event)
 {
-	switch (event->attr.type) {
-	case PERF_TYPE_SOFTWARE:
-	case PERF_TYPE_TRACEPOINT:
-	/* for now the breakpoint stuff also works as software event */
-	case PERF_TYPE_BREAKPOINT:
-		return 1;
-	}
-	return 0;
+	return event->pmu->task_ctx_nr == perf_sw_context;
 }
 
 extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];

commit 8dc85d547285668e509f86c177bcd4ea055bcaaf
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 2 16:50:03 2010 +0200

    perf: Multiple task contexts
    
    Provide the infrastructure for multiple task contexts.
    
    A more flexible approach would have resulted in more pointer chases
    in the scheduling hot-paths. This approach has the limitation of a
    static number of task contexts.
    
    Since I expect most external PMUs to be system wide, or at least node
    wide (as per the intel uncore unit) they won't actually need a task
    context.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 22155ef3b362..9ecfd856ce6e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -572,6 +572,7 @@ struct pmu {
 
 	int * __percpu			pmu_disable_count;
 	struct perf_cpu_context * __percpu pmu_cpu_context;
+	int				task_ctx_nr;
 
 	/*
 	 * Fully disable/enable this PMU, can be used to protect from the PMI

commit 108b02cfce04ee90b0a07ee0b104baffd39f5934
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 6 14:32:03 2010 +0200

    perf: Per-pmu-per-cpu contexts
    
    Allocate per-cpu contexts per pmu.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fa04537df55b..22155ef3b362 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -570,7 +570,8 @@ struct perf_event;
 struct pmu {
 	struct list_head		entry;
 
-	int				*pmu_disable_count;
+	int * __percpu			pmu_disable_count;
+	struct perf_cpu_context * __percpu pmu_cpu_context;
 
 	/*
 	 * Fully disable/enable this PMU, can be used to protect from the PMI
@@ -808,6 +809,7 @@ struct perf_event {
  * Used as a container for task events and CPU events as well:
  */
 struct perf_event_context {
+	struct pmu			*pmu;
 	/*
 	 * Protect the states of the events in the list,
 	 * nr_active, and the list:

commit b5ab4cd563e7ab49b27957704112a8ecade54e1f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 6 16:32:21 2010 +0200

    perf: Per cpu-context rotation timer
    
    Give each cpu-context its own timer so that it is a self contained
    entity, this eases the way for per-pmu-per-cpu contexts as well as
    provides the basic infrastructure to allow different rotation
    times per pmu.
    
    Things to look at:
     - folding the tick and these TICK_NSEC timers
     - separate task context rotation
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4ab4f0ca09a1..fa04537df55b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -861,6 +861,8 @@ struct perf_cpu_context {
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
 	int				exclusive;
+	u64				timer_interval;
+	struct hrtimer			timer;
 };
 
 struct perf_output_handle {
@@ -881,7 +883,6 @@ extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
-extern void perf_event_task_tick(struct task_struct *task);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -1067,8 +1068,6 @@ perf_event_task_sched_in(struct task_struct *task)			{ }
 static inline void
 perf_event_task_sched_out(struct task_struct *task,
 			    struct task_struct *next)			{ }
-static inline void
-perf_event_task_tick(struct task_struct *task)				{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }

commit b28ab83c595e767f2028276b7398d17f2253cec0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 6 14:48:15 2010 +0200

    perf: Remove the swevent hash-table from the cpu context
    
    Separate the swevent hash-table from the cpu_context bits in
    preparation for per pmu cpu contexts.
    
    This keeps the swevent hash a global entity.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b22176d3ebdf..4ab4f0ca09a1 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -861,12 +861,6 @@ struct perf_cpu_context {
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
 	int				exclusive;
-	struct swevent_hlist		*swevent_hlist;
-	struct mutex			hlist_mutex;
-	int				hlist_refcount;
-
-	/* Recursion avoidance in each contexts */
-	int				recursion[PERF_NR_CONTEXTS];
 };
 
 struct perf_output_handle {

commit 15ac9a395a753cb28c674e7ea80386ffdff21785
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 6 15:51:45 2010 +0200

    perf: Remove the sysfs bits
    
    Neither the overcommit nor the reservation sysfs parameter were
    actually working, remove them as they'll only get in the way.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 402073c61669..b22176d3ebdf 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -860,7 +860,6 @@ struct perf_cpu_context {
 	struct perf_event_context	ctx;
 	struct perf_event_context	*task_ctx;
 	int				active_oncpu;
-	int				max_pertask;
 	int				exclusive;
 	struct swevent_hlist		*swevent_hlist;
 	struct mutex			hlist_mutex;
@@ -883,11 +882,6 @@ struct perf_output_handle {
 
 #ifdef CONFIG_PERF_EVENTS
 
-/*
- * Set by architecture code:
- */
-extern int perf_max_events;
-
 extern int perf_pmu_register(struct pmu *pmu);
 extern void perf_pmu_unregister(struct pmu *pmu);
 

commit a4eaf7f14675cb512d69f0c928055e73d0c6d252
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jun 16 14:37:10 2010 +0200

    perf: Rework the PMU methods
    
    Replace pmu::{enable,disable,start,stop,unthrottle} with
    pmu::{add,del,start,stop}, all of which take a flags argument.
    
    The new interface extends the capability to stop a counter while
    keeping it scheduled on the PMU. We replace the throttled state with
    the generic stopped state.
    
    This also allows us to efficiently stop/start counters over certain
    code paths (like IRQ handlers).
    
    It also allows scheduling a counter without it starting, allowing for
    a generic frozen state (useful for rotating stopped counters).
    
    The stopped state is implemented in two different ways, depending on
    how the architecture implemented the throttled state:
    
     1) We disable the counter:
        a) the pmu has per-counter enable bits, we flip that
        b) we program a NOP event, preserving the counter state
    
     2) We store the counter state and ignore all read/overflow events
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8cafa15af60d..402073c61669 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -538,6 +538,7 @@ struct hw_perf_event {
 		};
 #endif
 	};
+	int				state;
 	local64_t			prev_count;
 	u64				sample_period;
 	u64				last_period;
@@ -549,6 +550,13 @@ struct hw_perf_event {
 #endif
 };
 
+/*
+ * hw_perf_event::state flags
+ */
+#define PERF_HES_STOPPED	0x01 /* the counter is stopped */
+#define PERF_HES_UPTODATE	0x02 /* event->count up-to-date */
+#define PERF_HES_ARCH		0x04
+
 struct perf_event;
 
 /*
@@ -564,42 +572,62 @@ struct pmu {
 
 	int				*pmu_disable_count;
 
+	/*
+	 * Fully disable/enable this PMU, can be used to protect from the PMI
+	 * as well as for lazy/batch writing of the MSRs.
+	 */
 	void (*pmu_enable)		(struct pmu *pmu); /* optional */
 	void (*pmu_disable)		(struct pmu *pmu); /* optional */
 
 	/*
+	 * Try and initialize the event for this PMU.
 	 * Should return -ENOENT when the @event doesn't match this PMU.
 	 */
 	int (*event_init)		(struct perf_event *event);
 
-	int  (*enable)			(struct perf_event *event);
-	void (*disable)			(struct perf_event *event);
-	int  (*start)			(struct perf_event *event);
-	void (*stop)			(struct perf_event *event);
+#define PERF_EF_START	0x01		/* start the counter when adding    */
+#define PERF_EF_RELOAD	0x02		/* reload the counter when starting */
+#define PERF_EF_UPDATE	0x04		/* update the counter when stopping */
+
+	/*
+	 * Adds/Removes a counter to/from the PMU, can be done inside
+	 * a transaction, see the ->*_txn() methods.
+	 */
+	int  (*add)			(struct perf_event *event, int flags);
+	void (*del)			(struct perf_event *event, int flags);
+
+	/*
+	 * Starts/Stops a counter present on the PMU. The PMI handler
+	 * should stop the counter when perf_event_overflow() returns
+	 * !0. ->start() will be used to continue.
+	 */
+	void (*start)			(struct perf_event *event, int flags);
+	void (*stop)			(struct perf_event *event, int flags);
+
+	/*
+	 * Updates the counter value of the event.
+	 */
 	void (*read)			(struct perf_event *event);
-	void (*unthrottle)		(struct perf_event *event);
 
 	/*
 	 * Group events scheduling is treated as a transaction, add
 	 * group events as a whole and perform one schedulability test.
 	 * If the test fails, roll back the whole group
-	 */
-
-	/*
-	 * Start the transaction, after this ->enable() doesn't need to
+	 *
+	 * Start the transaction, after this ->add() doesn't need to
 	 * do schedulability tests.
 	 */
 	void (*start_txn)	(struct pmu *pmu); /* optional */
 	/*
-	 * If ->start_txn() disabled the ->enable() schedulability test
+	 * If ->start_txn() disabled the ->add() schedulability test
 	 * then ->commit_txn() is required to perform one. On success
 	 * the transaction is closed. On error the transaction is kept
 	 * open until ->cancel_txn() is called.
 	 */
 	int  (*commit_txn)	(struct pmu *pmu); /* optional */
 	/*
-	 * Will cancel the transaction, assumes ->disable() is called
-	 * for each successfull ->enable() during the transaction.
+	 * Will cancel the transaction, assumes ->del() is called
+	 * for each successfull ->add() during the transaction.
 	 */
 	void (*cancel_txn)	(struct pmu *pmu); /* optional */
 };
@@ -680,7 +708,7 @@ struct perf_event {
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;
-	struct pmu		*pmu;
+	struct pmu			*pmu;
 
 	enum perf_event_active_state	state;
 	unsigned int			attach_state;

commit fa407f35e0298d841e4088f95a7f9cf6e725c6d5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jun 24 12:35:12 2010 +0200

    perf: Shrink hw_perf_event
    
    Use hw_perf_event::period_left instead of hw_perf_event::remaining
    and win back 8 bytes.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index bf85733597ec..8cafa15af60d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -529,7 +529,6 @@ struct hw_perf_event {
 			int		last_cpu;
 		};
 		struct { /* software */
-			s64		remaining;
 			struct hrtimer	hrtimer;
 		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT

commit ad5133b7030d04ce7701aa7cbe98f561347c79c2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jun 15 12:22:39 2010 +0200

    perf: Default PMU ops
    
    Provide default implementations for the pmu txn methods, this
    allows us to remove some conditional code.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6abf103fb7f8..bf85733597ec 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -565,8 +565,8 @@ struct pmu {
 
 	int				*pmu_disable_count;
 
-	void (*pmu_enable)		(struct pmu *pmu);
-	void (*pmu_disable)		(struct pmu *pmu);
+	void (*pmu_enable)		(struct pmu *pmu); /* optional */
+	void (*pmu_disable)		(struct pmu *pmu); /* optional */
 
 	/*
 	 * Should return -ENOENT when the @event doesn't match this PMU.
@@ -590,19 +590,19 @@ struct pmu {
 	 * Start the transaction, after this ->enable() doesn't need to
 	 * do schedulability tests.
 	 */
-	void (*start_txn)	(struct pmu *pmu);
+	void (*start_txn)	(struct pmu *pmu); /* optional */
 	/*
 	 * If ->start_txn() disabled the ->enable() schedulability test
 	 * then ->commit_txn() is required to perform one. On success
 	 * the transaction is closed. On error the transaction is kept
 	 * open until ->cancel_txn() is called.
 	 */
-	int  (*commit_txn)	(struct pmu *pmu);
+	int  (*commit_txn)	(struct pmu *pmu); /* optional */
 	/*
 	 * Will cancel the transaction, assumes ->disable() is called
 	 * for each successfull ->enable() during the transaction.
 	 */
-	void (*cancel_txn)	(struct pmu *pmu);
+	void (*cancel_txn)	(struct pmu *pmu); /* optional */
 };
 
 /**

commit 33696fc0d141bbbcb12f75b69608ea83282e3117
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jun 14 08:49:00 2010 +0200

    perf: Per PMU disable
    
    Changes perf_disable() into perf_pmu_disable().
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 243286a8ded7..6abf103fb7f8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -563,6 +563,11 @@ struct perf_event;
 struct pmu {
 	struct list_head		entry;
 
+	int				*pmu_disable_count;
+
+	void (*pmu_enable)		(struct pmu *pmu);
+	void (*pmu_disable)		(struct pmu *pmu);
+
 	/*
 	 * Should return -ENOENT when the @event doesn't match this PMU.
 	 */
@@ -868,10 +873,8 @@ extern void perf_event_free_task(struct task_struct *task);
 extern void set_perf_event_pending(void);
 extern void perf_event_do_pending(void);
 extern void perf_event_print_debug(void);
-extern void __perf_disable(void);
-extern bool __perf_enable(void);
-extern void perf_disable(void);
-extern void perf_enable(void);
+extern void perf_pmu_disable(struct pmu *pmu);
+extern void perf_pmu_enable(struct pmu *pmu);
 extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
 extern void perf_event_update_userpage(struct perf_event *event);
@@ -1056,8 +1059,6 @@ static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
 static inline void perf_event_do_pending(void)				{ }
 static inline void perf_event_print_debug(void)				{ }
-static inline void perf_disable(void)					{ }
-static inline void perf_enable(void)					{ }
 static inline int perf_event_task_disable(void)				{ return -EINVAL; }
 static inline int perf_event_task_enable(void)				{ return -EINVAL; }
 

commit 24cd7f54a0d47e1d5b3de29e2456bfbd2d8447b7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 11 17:32:03 2010 +0200

    perf: Reduce perf_disable() usage
    
    Since the current perf_disable() usage is only an optimization,
    remove it for now. This eases the removal of the __weak
    hw_perf_enable() interface.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ab72f56eb372..243286a8ded7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -564,26 +564,26 @@ struct pmu {
 	struct list_head		entry;
 
 	/*
-	 * Should return -ENOENT when the @event doesn't match this pmu
+	 * Should return -ENOENT when the @event doesn't match this PMU.
 	 */
 	int (*event_init)		(struct perf_event *event);
 
-	int (*enable)			(struct perf_event *event);
+	int  (*enable)			(struct perf_event *event);
 	void (*disable)			(struct perf_event *event);
-	int (*start)			(struct perf_event *event);
+	int  (*start)			(struct perf_event *event);
 	void (*stop)			(struct perf_event *event);
 	void (*read)			(struct perf_event *event);
 	void (*unthrottle)		(struct perf_event *event);
 
 	/*
-	 * Group events scheduling is treated as a transaction, add group
-	 * events as a whole and perform one schedulability test. If the test
-	 * fails, roll back the whole group
+	 * Group events scheduling is treated as a transaction, add
+	 * group events as a whole and perform one schedulability test.
+	 * If the test fails, roll back the whole group
 	 */
 
 	/*
-	 * Start the transaction, after this ->enable() doesn't need
-	 * to do schedulability tests.
+	 * Start the transaction, after this ->enable() doesn't need to
+	 * do schedulability tests.
 	 */
 	void (*start_txn)	(struct pmu *pmu);
 	/*
@@ -594,8 +594,8 @@ struct pmu {
 	 */
 	int  (*commit_txn)	(struct pmu *pmu);
 	/*
-	 * Will cancel the transaction, assumes ->disable() is called for
-	 * each successfull ->enable() during the transaction.
+	 * Will cancel the transaction, assumes ->disable() is called
+	 * for each successfull ->enable() during the transaction.
 	 */
 	void (*cancel_txn)	(struct pmu *pmu);
 };

commit b0a873ebbf87bf38bf70b5e39a7cadc96099fa13
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 11 13:35:08 2010 +0200

    perf: Register PMU implementations
    
    Simple registration interface for struct pmu, this provides the
    infrastructure for removing all the weak functions.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 09d048b52115..ab72f56eb372 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -561,6 +561,13 @@ struct perf_event;
  * struct pmu - generic performance monitoring unit
  */
 struct pmu {
+	struct list_head		entry;
+
+	/*
+	 * Should return -ENOENT when the @event doesn't match this pmu
+	 */
+	int (*event_init)		(struct perf_event *event);
+
 	int (*enable)			(struct perf_event *event);
 	void (*disable)			(struct perf_event *event);
 	int (*start)			(struct perf_event *event);
@@ -849,7 +856,8 @@ struct perf_output_handle {
  */
 extern int perf_max_events;
 
-extern struct pmu *hw_perf_event_init(struct perf_event *event);
+extern int perf_pmu_register(struct pmu *pmu);
+extern void perf_pmu_unregister(struct pmu *pmu);
 
 extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);

commit 51b0fe39549a04858001922919ab355dee9bdfcf
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 11 13:35:57 2010 +0200

    perf: Deconstify struct pmu
    
    sed -ie 's/const struct pmu\>/struct pmu/g' `git grep -l "const struct pmu\>"`
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 000610c4de71..09d048b52115 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -578,19 +578,19 @@ struct pmu {
 	 * Start the transaction, after this ->enable() doesn't need
 	 * to do schedulability tests.
 	 */
-	void (*start_txn)	(const struct pmu *pmu);
+	void (*start_txn)	(struct pmu *pmu);
 	/*
 	 * If ->start_txn() disabled the ->enable() schedulability test
 	 * then ->commit_txn() is required to perform one. On success
 	 * the transaction is closed. On error the transaction is kept
 	 * open until ->cancel_txn() is called.
 	 */
-	int  (*commit_txn)	(const struct pmu *pmu);
+	int  (*commit_txn)	(struct pmu *pmu);
 	/*
 	 * Will cancel the transaction, assumes ->disable() is called for
 	 * each successfull ->enable() during the transaction.
 	 */
-	void (*cancel_txn)	(const struct pmu *pmu);
+	void (*cancel_txn)	(struct pmu *pmu);
 };
 
 /**
@@ -669,7 +669,7 @@ struct perf_event {
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;
-	const struct pmu		*pmu;
+	struct pmu		*pmu;
 
 	enum perf_event_active_state	state;
 	unsigned int			attach_state;
@@ -849,7 +849,7 @@ struct perf_output_handle {
  */
 extern int perf_max_events;
 
-extern const struct pmu *hw_perf_event_init(struct perf_event *event);
+extern struct pmu *hw_perf_event_init(struct perf_event *event);
 
 extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);

commit c8710ad38900153af7a3e6762e99c062cfa46443
Merge: 6016ee13db51 86397dc3ccfc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 19 12:48:09 2010 +0200

    Merge branch 'tip/perf/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/core

commit 7ae07ea3a48d30689ee037cb136bc21f0b37d8ae
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 14 20:45:13 2010 +0200

    perf: Humanize the number of contexts
    
    Instead of hardcoding the number of contexts for the recursions
    barriers, define a cpp constant to make the code more
    self-explanatory.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d7e8ea690864..ae6fa6050925 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -808,6 +808,12 @@ struct perf_event_context {
 	struct rcu_head			rcu_head;
 };
 
+/*
+ * Number of contexts where an event can trigger:
+ * 	task, softirq, hardirq, nmi.
+ */
+#define PERF_NR_CONTEXTS	4
+
 /**
  * struct perf_event_cpu_context - per cpu event context structure
  */
@@ -821,12 +827,8 @@ struct perf_cpu_context {
 	struct mutex			hlist_mutex;
 	int				hlist_refcount;
 
-	/*
-	 * Recursion avoidance:
-	 *
-	 * task, softirq, irq, nmi context
-	 */
-	int				recursion[4];
+	/* Recursion avoidance in each contexts */
+	int				recursion[PERF_NR_CONTEXTS];
 };
 
 struct perf_output_handle {

commit 927c7a9e92c4f69097a6e9e086d11fc2f8a5b40b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 1 16:20:36 2010 +0200

    perf: Fix race in callchains
    
    Now that software events don't have interrupt disabled anymore in
    the event path, callchains can nest on any context. So seperating
    nmi and others contexts in two buffers has become racy.
    
    Fix this by providing one buffer per nesting level. Given the size
    of the callchain entries (2040 bytes * 4), we now need to allocate
    them dynamically.
    
    v2: Fixed put_callchain_entry call after recursion.
        Fix the type of the recursion, it must be an array.
    
    v3: Use a manual pr cpu allocation (temporary solution until NMIs
        can safely access vmalloc'ed memory).
        Do a better separation between callchain reference tracking and
        allocation. Make the "put" path lockless for non-release cases.
    
    v4: Protect the callchain buffers with rcu.
    
    v5: Do the cpu buffers allocations node affine.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Borislav Petkov <bp@amd64.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4db61dded388..d7e8ea690864 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -983,7 +983,6 @@ extern void perf_callchain_user(struct perf_callchain_entry *entry,
 				struct pt_regs *regs);
 extern void perf_callchain_kernel(struct perf_callchain_entry *entry,
 				  struct pt_regs *regs);
-extern struct perf_callchain_entry *perf_callchain_buffer(void);
 
 
 static inline void

commit 56962b4449af34070bb1994621ef4f0265eed4d8
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 30 23:03:51 2010 +0200

    perf: Generalize some arch callchain code
    
    - Most archs use one callchain buffer per cpu, except x86 that needs
      to deal with NMIs. Provide a default perf_callchain_buffer()
      implementation that x86 overrides.
    
    - Centralize all the kernel/user regs handling and invoke new arch
      handlers from there: perf_callchain_user() / perf_callchain_kernel()
      That avoid all the user_mode(), current->mm checks and so...
    
    - Invert some parameters in perf_callchain_*() helpers: entry to the
      left, regs to the right, following the traditional (dst, src).
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Borislav Petkov <bp@amd64.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 358880404b42..4db61dded388 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -976,7 +976,15 @@ extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks
 extern void perf_event_comm(struct task_struct *tsk);
 extern void perf_event_fork(struct task_struct *tsk);
 
-extern struct perf_callchain_entry *perf_callchain(struct pt_regs *regs);
+/* Callchains */
+DECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);
+
+extern void perf_callchain_user(struct perf_callchain_entry *entry,
+				struct pt_regs *regs);
+extern void perf_callchain_kernel(struct perf_callchain_entry *entry,
+				  struct pt_regs *regs);
+extern struct perf_callchain_entry *perf_callchain_buffer(void);
+
 
 static inline void
 perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)

commit 70791ce9ba68a5921c9905ef05d23f62a90bc10c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jun 29 19:34:05 2010 +0200

    perf: Generalize callchain_store()
    
    callchain_store() is the same on every archs, inline it in
    perf_event.h and rename it to perf_callchain_store() to avoid
    any collision.
    
    This removes repetitive code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Borislav Petkov <bp@amd64.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 937495c25073..358880404b42 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -978,6 +978,13 @@ extern void perf_event_fork(struct task_struct *tsk);
 
 extern struct perf_callchain_entry *perf_callchain(struct pt_regs *regs);
 
+static inline void
+perf_callchain_store(struct perf_callchain_entry *entry, u64 ip)
+{
+	if (entry->nr < PERF_MAX_STACK_DEPTH)
+		entry->ip[entry->nr++] = ip;
+}
+
 extern int sysctl_perf_event_paranoid;
 extern int sysctl_perf_event_mlock;
 extern int sysctl_perf_event_sample_rate;

commit c4efd6b569b2646e1346a08a4c40286f8bcb5f11
Merge: 4aed2fd8e318 0bcfe7580794
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:39:22 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (27 commits)
      sched: Use correct macro to display sched_child_runs_first in /proc/sched_debug
      sched: No need for bootmem special cases
      sched: Revert nohz_ratelimit() for now
      sched: Reduce update_group_power() calls
      sched: Update rq->clock for nohz balanced cpus
      sched: Fix spelling of sibling
      sched, cpuset: Drop __cpuexit from cpu hotplug callbacks
      sched: Fix the racy usage of thread_group_cputimer() in fastpath_timer_check()
      sched: run_posix_cpu_timers: Don't check ->exit_state, use lock_task_sighand()
      sched: thread_group_cputime: Simplify, document the "alive" check
      sched: Remove the obsolete exit_state/signal hacks
      sched: task_tick_rt: Remove the obsolete ->signal != NULL check
      sched: __sched_setscheduler: Read the RLIMIT_RTPRIO value lockless
      sched: Fix comments to make them DocBook happy
      sched: Fix fix_small_capacity
      powerpc: Exclude arch_sd_sibiling_asym_packing() on UP
      powerpc: Enable asymmetric SMT scheduling on POWER7
      sched: Add asymmetric group packing option for sibling domain
      sched: Fix capacity calculations for SMT4
      sched: Change nohz idle load balancing logic to push model
      ...

commit 5cfaf214856eb934759ae500a0b812dd06a00bd9
Author: Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>
Date:   Wed Jun 23 09:17:53 2010 +0900

    perf: Fix argument of perf_arch_fetch_caller_regs
    
    "struct regs" was set to argument of perf_arch_fetch_caller_regs
    off-case. It should be "struct pt_regs".
    
    This fixes various build errors in archs that have CONFIG_PERF_EVENTS=y
    but no overriden implementation of perf_arch_fetch_caller_regs.
    
    cc1: warnings being treated as errors
    In file included from include/linux/ftrace_event.h:8,
                     from include/trace/syscall.h:6,
                     from include/linux/syscalls.h:75,
                     from arch/sh/kernel/sys_sh32.c:9:
    include/linux/perf_event.h:937: error: 'struct regs' declared inside parameter list
    include/linux/perf_event.h:937: error: its scope is only this definition or declaration, which is probably not what you want
    include/linux/perf_event.h: In function 'perf_fetch_caller_regs':
    include/linux/perf_event.h:952: error: passing argument 1 of 'perf_arch_fetch_caller_regs' from incompatible pointer type
    
    Signed-off-by: Nobuhiro Iwamatsu <nobuhiro.iwamatsu.yj@renesas.com>
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <AANLkTinKKFKEBQrZ3Hkj-XCaMwaTqulb-XnFzqEYiFRr@mail.gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0dd5f8ad77ac..937495c25073 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -936,7 +936,7 @@ extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
 static inline void
-perf_arch_fetch_caller_regs(struct regs *regs, unsigned long ip) { }
+perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
 #endif
 
 /*

commit 45a73372efe4a63f44aa2e1125d4a777c2fdc8d8
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 23 23:00:37 2010 +0200

    hw_breakpoints: Fix per task breakpoint tracking
    
    Freeing a perf event can happen in several ways. A task
    calls perf_event_exit_task() right before exiting. This helper
    will detach all the events from the task context and queue their
    removal through free_event() if they are child tasks. The task
    also loses its context reference there.
    
    Releasing the breakpoint slot from the constraint table is made
    from free_event() that calls release_bp_slot(). We count the number
    of breakpoints this task is running by looking at the task's
    perf_event_ctxp and iterating through its attached events.
    But at this time, the reference to this context has been cleaned up
    already.
    
    So looking at the event->ctx instead of task->perf_event_ctxp
    to count the remaining breakpoints should solve the problem.
    At least it would for child breakpoints, but not for parent ones.
    If the parent exits before the child, it will remove all its
    events from the context but free_event() will be called later,
    on fd release time. And checking the number of breakpoints the
    task has attached to its context at this time is unreliable as all
    events have been removed from the context.
    
    To solve this, we keep track of the list of per task breakpoints.
    On top of it, we maintain our array of numbers of breakpoints used
    by the tasks. We use the context address as a task id.
    
    So, instead of looking at the number of events attached to a context,
    we walk through our list of per task breakpoints and count the number
    of breakpoints that use the same ctx than the one to be reserved or
    released from the constraint table, and update the count on top of this
    result.
    
    In the meantime it solves a bad refcounting, it also solves a warning,
    reported by Paul.
    
    Badness at /home/paulus/kernel/perf/kernel/hw_breakpoint.c:114
    NIP: c0000000000cb470 LR: c0000000000cb46c CTR: c00000000032d9b8
    REGS: c000000118e7b570 TRAP: 0700   Not tainted  (2.6.35-rc3-perf-00008-g76b0f13
    )
    MSR: 9000000000029032 <EE,ME,CE,IR,DR>  CR: 44004424  XER: 000fffff
    TASK = c0000001187dcad0[3143] 'perf' THREAD: c000000118e78000 CPU: 1
    GPR00: c0000000000cb46c c000000118e7b7f0 c0000000009866a0 0000000000000020
    GPR04: 0000000000000000 000000000000001d 0000000000000000 0000000000000001
    GPR08: c0000000009bed68 c00000000086dff8 c000000000a5bf10 0000000000000001
    GPR12: 0000000024004422 c00000000ffff200 0000000000000000 0000000000000000
    GPR16: 0000000000000000 0000000000000000 0000000000000018 00000000101150f4
    GPR20: 0000000010206b40 0000000000000000 0000000000000000 00000000101150f4
    GPR24: c0000001199090c0 0000000000000001 0000000000000000 0000000000000001
    GPR28: 0000000000000000 0000000000000000 c0000000008ec290 0000000000000000
    NIP [c0000000000cb470] .task_bp_pinned+0x5c/0x12c
    LR [c0000000000cb46c] .task_bp_pinned+0x58/0x12c
    Call Trace:
    [c000000118e7b7f0] [c0000000000cb46c] .task_bp_pinned+0x58/0x12c (unreliable)
    [c000000118e7b8a0] [c0000000000cb584] .toggle_bp_task_slot+0x44/0xe4
    [c000000118e7b940] [c0000000000cb6c8] .toggle_bp_slot+0xa4/0x164
    [c000000118e7b9f0] [c0000000000cbafc] .release_bp_slot+0x44/0x6c
    [c000000118e7ba80] [c0000000000c4178] .bp_perf_event_destroy+0x10/0x24
    [c000000118e7bb00] [c0000000000c4aec] .free_event+0x180/0x1bc
    [c000000118e7bbc0] [c0000000000c54c4] .perf_event_release_kernel+0x14c/0x170
    
    Reported-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Jason Wessel <jason.wessel@windriver.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 63b5aa5dce69..0dd5f8ad77ac 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -533,8 +533,10 @@ struct hw_perf_event {
 			struct hrtimer	hrtimer;
 		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-		/* breakpoint */
-		struct arch_hw_breakpoint	info;
+		struct { /* breakpoint */
+			struct arch_hw_breakpoint	info;
+			struct list_head		bp_list;
+		};
 #endif
 	};
 	local64_t			prev_count;

commit c726b61c6a5acc54c55ed7a0e7638cc4c5a100a8
Merge: 7be7923633a1 018378c55b03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 9 18:55:20 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into perf/core

commit 7be7923633a142402266d642ccebf74f556a649b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jun 9 11:57:23 2010 +0200

    perf: Fix build breakage for architecutes without atomic64_t
    
    The local64.h include dependency was not dependent on PERF_EVENT=y,
    which meant that arch's without atomic64_t support ended up including
    it and failed to build.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7342979f95f2..1218d05728b9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -462,6 +462,7 @@ enum perf_callchain_context {
 
 #ifdef CONFIG_PERF_EVENTS
 # include <asm/perf_event.h>
+# include <asm/local64.h>
 #endif
 
 struct perf_guest_info_callbacks {
@@ -487,7 +488,6 @@ struct perf_guest_info_callbacks {
 #include <linux/cpu.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
-#include <asm/local64.h>
 
 #define PERF_MAX_STACK_DEPTH		255
 

commit e78505958cf123048fb48cb56b79cebb8edd15fb
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 21 14:43:08 2010 +0200

    perf: Convert perf_event to local_t
    
    Since now all modification to event->count (and ->prev_count
    and ->period_left) are local to a cpu, change then to local64_t so we
    avoid the LOCK'ed ops.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f34dab9b275e..7342979f95f2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -487,6 +487,7 @@ struct perf_guest_info_callbacks {
 #include <linux/cpu.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
+#include <asm/local64.h>
 
 #define PERF_MAX_STACK_DEPTH		255
 
@@ -536,10 +537,10 @@ struct hw_perf_event {
 		struct arch_hw_breakpoint	info;
 #endif
 	};
-	atomic64_t			prev_count;
+	local64_t			prev_count;
 	u64				sample_period;
 	u64				last_period;
-	atomic64_t			period_left;
+	local64_t			period_left;
 	u64				interrupts;
 
 	u64				freq_time_stamp;
@@ -670,7 +671,7 @@ struct perf_event {
 
 	enum perf_event_active_state	state;
 	unsigned int			attach_state;
-	atomic64_t			count;
+	local64_t			count;
 	atomic64_t			child_count;
 
 	/*

commit a6e6dea68c18f705957573ee5596097c7e82d0e5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 21 14:27:58 2010 +0200

    perf: Add perf_event::child_count
    
    Only child counters adding back their values into the parent counter
    are responsible for cross-cpu updates to event->count.
    
    So if we pull that out into a new child_count variable, we get an
    event->count that is only modified locally.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 441992a9775c..f34dab9b275e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -671,6 +671,7 @@ struct perf_event {
 	enum perf_event_active_state	state;
 	unsigned int			attach_state;
 	atomic64_t			count;
+	atomic64_t			child_count;
 
 	/*
 	 * These are the total time in nanoseconds that the event

commit d57e34fdd60be7ffd0b1d86bfa1a553df86b7172
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 28 19:41:35 2010 +0200

    perf: Simplify the ring-buffer logic: make perf_buffer_alloc() do everything needed
    
    Currently there are perf_buffer_alloc() + perf_buffer_init() + some
    separate bits, fold it all into a single perf_buffer_alloc() and only
    leave the attachment to the event separate.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2a0da021c23f..441992a9775c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -602,6 +602,8 @@ enum perf_event_active_state {
 
 struct file;
 
+#define PERF_BUFFER_WRITABLE		0x01
+
 struct perf_buffer {
 	atomic_t			refcount;
 	struct rcu_head			rcu_head;

commit ca5135e6b4a3cbc7e187737520fbc4b508f6f7a2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 28 19:33:23 2010 +0200

    perf: Rename perf_mmap_data to perf_buffer
    
    Rename to clarify code.
    
    s/perf_mmap_data/perf_buffer/g and selective s/data/buffer/g
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f1b6ba0770e0..2a0da021c23f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -602,7 +602,7 @@ enum perf_event_active_state {
 
 struct file;
 
-struct perf_mmap_data {
+struct perf_buffer {
 	atomic_t			refcount;
 	struct rcu_head			rcu_head;
 #ifdef CONFIG_PERF_USE_VMALLOC
@@ -727,7 +727,7 @@ struct perf_event {
 	atomic_t			mmap_count;
 	int				mmap_locked;
 	struct user_struct		*mmap_user;
-	struct perf_mmap_data		*data;
+	struct perf_buffer		*buffer;
 
 	/* poll related */
 	wait_queue_head_t		waitq;
@@ -825,7 +825,7 @@ struct perf_cpu_context {
 
 struct perf_output_handle {
 	struct perf_event		*event;
-	struct perf_mmap_data		*data;
+	struct perf_buffer		*buffer;
 	unsigned long			wakeup;
 	unsigned long			size;
 	void				*addr;

commit 8d2cacbbb8deadfae78aa16e4e1ee619bdd7019e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 25 17:49:05 2010 +0200

    perf: Cleanup {start,commit,cancel}_txn details
    
    Clarify some of the transactional group scheduling API details
    and change it so that a successfull ->commit_txn also closes
    the transaction.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1274803086.5882.1752.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 36efad90cd43..f1b6ba0770e0 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -549,7 +549,10 @@ struct hw_perf_event {
 
 struct perf_event;
 
-#define PERF_EVENT_TXN_STARTED 1
+/*
+ * Common implementation detail of pmu::{start,commit,cancel}_txn
+ */
+#define PERF_EVENT_TXN 0x1
 
 /**
  * struct pmu - generic performance monitoring unit
@@ -563,14 +566,28 @@ struct pmu {
 	void (*unthrottle)		(struct perf_event *event);
 
 	/*
-	 * group events scheduling is treated as a transaction,
-	 * add group events as a whole and perform one schedulability test.
-	 * If test fails, roll back the whole group
+	 * Group events scheduling is treated as a transaction, add group
+	 * events as a whole and perform one schedulability test. If the test
+	 * fails, roll back the whole group
 	 */
 
+	/*
+	 * Start the transaction, after this ->enable() doesn't need
+	 * to do schedulability tests.
+	 */
 	void (*start_txn)	(const struct pmu *pmu);
-	void (*cancel_txn)	(const struct pmu *pmu);
+	/*
+	 * If ->start_txn() disabled the ->enable() schedulability test
+	 * then ->commit_txn() is required to perform one. On success
+	 * the transaction is closed. On error the transaction is kept
+	 * open until ->cancel_txn() is called.
+	 */
 	int  (*commit_txn)	(const struct pmu *pmu);
+	/*
+	 * Will cancel the transaction, assumes ->disable() is called for
+	 * each successfull ->enable() during the transaction.
+	 */
+	void (*cancel_txn)	(const struct pmu *pmu);
 };
 
 /**

commit 3af9e859281bda7eb7c20b51879cf43aa788ac2e
Author: Eric B Munson <ebmunson@us.ibm.com>
Date:   Tue May 18 15:30:49 2010 +0100

    perf: Add non-exec mmap() tracking
    
    Add the capacility to track data mmap()s. This can be used together
    with PERF_SAMPLE_ADDR for data profiling.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    [Updated code for stable perf ABI]
    Signed-off-by: Eric B Munson <ebmunson@us.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1274193049-25997-1-git-send-email-ebmunson@us.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c691a0b27bcd..36efad90cd43 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -214,8 +214,9 @@ struct perf_event_attr {
 				 *  See also PERF_RECORD_MISC_EXACT_IP
 				 */
 				precise_ip     :  2, /* skid constraint       */
+				mmap_data      :  1, /* non-exec mmap data    */
 
-				__reserved_1   : 47;
+				__reserved_1   : 46;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -962,14 +963,7 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 	}
 }
 
-extern void __perf_event_mmap(struct vm_area_struct *vma);
-
-static inline void perf_event_mmap(struct vm_area_struct *vma)
-{
-	if (vma->vm_flags & VM_EXEC)
-		__perf_event_mmap(vma);
-}
-
+extern void perf_event_mmap(struct vm_area_struct *vma);
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
 extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);

commit ecc55f84b2e9741f29daa787ded93986df6cbe17
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 21 15:11:34 2010 +0200

    perf, trace: Inline perf_swevent_put_recursion_context()
    
    Inline perf_swevent_put_recursion_context into perf_tp_event(), this
    shrinks the per trace template code footprint and saves a function
    call.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5d0266d94985..c691a0b27bcd 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1001,7 +1001,7 @@ static inline bool perf_paranoid_kernel(void)
 extern void perf_event_init(void);
 extern void perf_tp_event(u64 addr, u64 count, void *record,
 			  int entry_size, struct pt_regs *regs,
-			  struct hlist_head *head);
+			  struct hlist_head *head, int rctx);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit b0f82b81fe6bbcf78d478071f33e44554726bc81
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 20 07:47:21 2010 +0200

    perf: Drop the skip argument from perf_arch_fetch_regs_caller
    
    Drop this argument now that we always want to rewind only to the
    state of the first caller.
    It means frame pointers are not necessary anymore to reliably get
    the source of an event. But this also means we need this helper
    to be a macro now, as an inline function is not an option since
    we need to know when to provide a default implentation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fb6c91eac7e3..bea785cef493 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -905,8 +905,10 @@ extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
-extern void
-perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip, int skip);
+#ifndef perf_arch_fetch_caller_regs
+static inline void
+perf_arch_fetch_caller_regs(struct regs *regs, unsigned long ip) { }
+#endif
 
 /*
  * Take a snapshot of the regs. Skip ip and frame pointer to
@@ -916,31 +918,11 @@ perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip, int skip);
  * - bp for callchains
  * - eflags, for future purposes, just in case
  */
-static inline void perf_fetch_caller_regs(struct pt_regs *regs, int skip)
+static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 {
-	unsigned long ip;
-
 	memset(regs, 0, sizeof(*regs));
 
-	switch (skip) {
-	case 1 :
-		ip = CALLER_ADDR0;
-		break;
-	case 2 :
-		ip = CALLER_ADDR1;
-		break;
-	case 3 :
-		ip = CALLER_ADDR2;
-		break;
-	case 4:
-		ip = CALLER_ADDR3;
-		break;
-	/* No need to support further for now */
-	default:
-		ip = 0;
-	}
-
-	return perf_arch_fetch_caller_regs(regs, ip, skip);
+	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 }
 
 static inline void
@@ -950,7 +932,7 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 		struct pt_regs hot_regs;
 
 		if (!regs) {
-			perf_fetch_caller_regs(&hot_regs, 1);
+			perf_fetch_caller_regs(&hot_regs);
 			regs = &hot_regs;
 		}
 		__perf_sw_event(event_id, nr, nmi, regs, addr);

commit 50a323b73069b169385a8ac65633dee837a7d13f
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 8 21:40:36 2010 +0200

    sched: define and use CPU_PRI_* enums for cpu notifier priorities
    
    Instead of hardcoding priority 10 and 20 in sched and perf, collect
    them into CPU_PRI_* enums.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 5d0266d94985..469e03e96fe7 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1068,7 +1068,7 @@ static inline void perf_event_disable(struct perf_event *event)		{ }
 #define perf_cpu_notifier(fn)					\
 do {								\
 	static struct notifier_block fn##_nb __cpuinitdata =	\
-		{ .notifier_call = fn, .priority = 20 };	\
+		{ .notifier_call = fn, .priority = CPU_PRI_PERF }; \
 	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,		\
 		(void *)(unsigned long)smp_processor_id());	\
 	fn(&fn##_nb, (unsigned long)CPU_STARTING,		\

commit 8a49542c0554af7d0073aac0ee73ee65b807ef34
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 27 15:47:49 2010 +0200

    perf_events: Fix races in group composition
    
    Group siblings don't pin each-other or the parent, so when we destroy
    events we must make sure to clean up all cross referencing pointers.
    
    In particular, for destruction of a group leader we must be able to
    find all its siblings and remove their reference to it.
    
    This means that detaching an event from its context must not detach it
    from the group, otherwise we can end up failing to clear all pointers.
    
    Solve this by clearly separating the attachment to a context and
    attachment to a group, and keep the group composed until we destroy
    the events.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 490698590d6e..5d0266d94985 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -631,6 +631,9 @@ struct swevent_hlist {
 	struct rcu_head		rcu_head;
 };
 
+#define PERF_ATTACH_CONTEXT	0x01
+#define PERF_ATTACH_GROUP	0x02
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -646,6 +649,7 @@ struct perf_event {
 	const struct pmu		*pmu;
 
 	enum perf_event_active_state	state;
+	unsigned int			attach_state;
 	atomic64_t			count;
 
 	/*

commit ac9721f3f54b27a16c7e1afb2481e7ee95a70318
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 27 12:54:41 2010 +0200

    perf_events: Fix races and clean up perf_event and perf_mmap_data interaction
    
    In order to move toward separate buffer objects, rework the whole
    perf_mmap_data construct to be a more self-sufficient entity, one
    with its own lifetime rules.
    
    This greatly sanitizes the whole output redirection code, which
    was riddled with bugs and races.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: <stable@kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fb6c91eac7e3..490698590d6e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -585,6 +585,7 @@ enum perf_event_active_state {
 struct file;
 
 struct perf_mmap_data {
+	atomic_t			refcount;
 	struct rcu_head			rcu_head;
 #ifdef CONFIG_PERF_USE_VMALLOC
 	struct work_struct		work;
@@ -592,7 +593,6 @@ struct perf_mmap_data {
 #endif
 	int				nr_pages;	/* nr of data pages  */
 	int				writable;	/* are we writable   */
-	int				nr_locked;	/* nr pages mlocked  */
 
 	atomic_t			poll;		/* POLL_ for wakeups */
 
@@ -643,7 +643,6 @@ struct perf_event {
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;
-	struct perf_event		*output;
 	const struct pmu		*pmu;
 
 	enum perf_event_active_state	state;
@@ -704,6 +703,8 @@ struct perf_event {
 	/* mmap bits */
 	struct mutex			mmap_mutex;
 	atomic_t			mmap_count;
+	int				mmap_locked;
+	struct user_struct		*mmap_user;
 	struct perf_mmap_data		*data;
 
 	/* poll related */

commit a94ffaaf55552769af328eaca9260fe6291c66c7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 20 19:50:07 2010 +0200

    perf: Remove more code from the fastpath
    
    Sanity checks cost instructions.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.852926930@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 09cd9c1abfda..fb6c91eac7e3 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -803,8 +803,6 @@ struct perf_cpu_context {
 struct perf_output_handle {
 	struct perf_event		*event;
 	struct perf_mmap_data		*data;
-	unsigned long			head;
-	unsigned long			offset;
 	unsigned long			wakeup;
 	unsigned long			size;
 	void				*addr;

commit 3cafa9fbb5c1d564b7b8e7224f493effbf04ffee
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 20 19:07:56 2010 +0200

    perf: Optimize the !vmalloc backed buffer
    
    Reduce code and data by using the knowledge that for
    !PERF_USE_VMALLOC data_order is always 0.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.795019386@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7bd17f0488f8..09cd9c1abfda 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -588,8 +588,8 @@ struct perf_mmap_data {
 	struct rcu_head			rcu_head;
 #ifdef CONFIG_PERF_USE_VMALLOC
 	struct work_struct		work;
+	int				page_order;	/* allocation order  */
 #endif
-	int				data_order;	/* allocation order  */
 	int				nr_pages;	/* nr of data pages  */
 	int				writable;	/* are we writable   */
 	int				nr_locked;	/* nr pages mlocked  */

commit 5d967a8be636a4f301a8daad642bd1007299d9ec
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 20 16:46:39 2010 +0200

    perf: Optimize perf_output_copy()
    
    Reduce the clutter in perf_output_copy() by keeping
    an interator in perf_output_handle.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.742809176@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7098ebbb3b3a..7bd17f0488f8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -806,6 +806,9 @@ struct perf_output_handle {
 	unsigned long			head;
 	unsigned long			offset;
 	unsigned long			wakeup;
+	unsigned long			size;
+	void				*addr;
+	int				page;
 	int				nmi;
 	int				sample;
 };

commit adb8e118f288dc4c569ac9a89010b81a4745fbf0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 20 16:21:55 2010 +0200

    perf: Fix wakeup storm for RO mmap()s
    
    RO mmap()s don't update the tail pointer, so
    comparing against it for determining the written data
    size doesn't really do any good.
    
    Keep track of when we last did a wakeup, and compare
    against that.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.684479310@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7cd7b356447d..7098ebbb3b3a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -599,7 +599,7 @@ struct perf_mmap_data {
 	local_t				head;		/* write position    */
 	local_t				nest;		/* nested writers    */
 	local_t				events;		/* event limit       */
-	local_t				wakeup;		/* needs a wakeup    */
+	local_t				wakeup;		/* wakeup stamp      */
 	local_t				lost;		/* nr records lost   */
 
 	long				watermark;	/* wakeup watermark  */

commit 1c024eca51fdc965290acf342ae16a476c2189d0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed May 19 14:02:22 2010 +0200

    perf, trace: Optimize tracepoints by using per-tracepoint-per-cpu hlist to track events
    
    Avoid the swevent hash-table by using per-tracepoint
    hlists.
    
    Also, avoid conditionals on the fast path by ordering
    with probe unregister so that we should never get on
    the callback path without the data being there.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.473188012@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fe50347dc645..7cd7b356447d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -727,6 +727,7 @@ struct perf_event {
 	perf_overflow_handler_t		overflow_handler;
 
 #ifdef CONFIG_EVENT_TRACING
+	struct ftrace_event_call	*tp_event;
 	struct event_filter		*filter;
 #endif
 
@@ -992,8 +993,9 @@ static inline bool perf_paranoid_kernel(void)
 }
 
 extern void perf_event_init(void);
-extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record,
-			  int entry_size, struct pt_regs *regs, void *event);
+extern void perf_tp_event(u64 addr, u64 count, void *record,
+			  int entry_size, struct pt_regs *regs,
+			  struct hlist_head *head);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit 6d1acfd5c6bfd5231c13a8f2858d7f2afbaa1b62
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 18 11:12:48 2010 +0200

    perf: Optimize perf_output_*() by avoiding local_xchg()
    
    Since the x86 XCHG ins implies LOCK, avoid the use by
    using a sequence count instead.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ce7667616fcb..fe50347dc645 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -804,6 +804,7 @@ struct perf_output_handle {
 	struct perf_mmap_data		*data;
 	unsigned long			head;
 	unsigned long			offset;
+	unsigned long			wakeup;
 	int				nmi;
 	int				sample;
 };

commit fa5881514ef9c9bcb29319aad85cf2d8889d91f1
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 18 10:54:20 2010 +0200

    perf: Optimize the hotpath by converting the perf output buffer to local_t
    
    Since there is now only a single writer, we can use
    local_t instead and avoid all these pesky LOCK insn.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index f1f853a9d5eb..ce7667616fcb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -485,6 +485,7 @@ struct perf_guest_info_callbacks {
 #include <linux/ftrace.h>
 #include <linux/cpu.h>
 #include <asm/atomic.h>
+#include <asm/local.h>
 
 #define PERF_MAX_STACK_DEPTH		255
 
@@ -588,20 +589,18 @@ struct perf_mmap_data {
 #ifdef CONFIG_PERF_USE_VMALLOC
 	struct work_struct		work;
 #endif
-	int				data_order;
+	int				data_order;	/* allocation order  */
 	int				nr_pages;	/* nr of data pages  */
 	int				writable;	/* are we writable   */
 	int				nr_locked;	/* nr pages mlocked  */
 
 	atomic_t			poll;		/* POLL_ for wakeups */
-	atomic_t			events;		/* event_id limit       */
 
-	atomic_long_t			head;		/* write position    */
-
-	atomic_t			wakeup;		/* needs a wakeup    */
-	atomic_t			lost;		/* nr records lost   */
-
-	atomic_t			nest;		/* nested writers    */
+	local_t				head;		/* write position    */
+	local_t				nest;		/* nested writers    */
+	local_t				events;		/* event limit       */
+	local_t				wakeup;		/* needs a wakeup    */
+	local_t				lost;		/* nr records lost   */
 
 	long				watermark;	/* wakeup watermark  */
 

commit ef60777c9abd999db5eb4e338aae3eb593ae8e10
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 18 10:50:41 2010 +0200

    perf: Optimize the perf_output() path by removing IRQ-disables
    
    Since we can now assume there is only a single writer
    to each buffer, we can remove per-cpu lock thingy and
    use a simply nest-count to the same effect.
    
    This removes the need to disable IRQs.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0b521fc8f5b0..f1f853a9d5eb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -597,12 +597,12 @@ struct perf_mmap_data {
 	atomic_t			events;		/* event_id limit       */
 
 	atomic_long_t			head;		/* write position    */
-	atomic_long_t			done_head;	/* completed head    */
 
-	atomic_t			lock;		/* concurrent writes */
 	atomic_t			wakeup;		/* needs a wakeup    */
 	atomic_t			lost;		/* nr records lost   */
 
+	atomic_t			nest;		/* nested writers    */
+
 	long				watermark;	/* wakeup watermark  */
 
 	struct perf_event_mmap_page	*user_page;
@@ -807,7 +807,6 @@ struct perf_output_handle {
 	unsigned long			offset;
 	int				nmi;
 	int				sample;
-	int				locked;
 };
 
 #ifdef CONFIG_PERF_EVENTS

commit 4f41c013f553957765902fb01475972f0af3e8e7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 18 18:08:32 2010 +0200

    perf/ftrace: Optimize perf/tracepoint interaction for single events
    
    When we've got but a single event per tracepoint
    there is no reason to try and multiplex it so don't.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3fd5c82e0e18..0b521fc8f5b0 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -994,7 +994,7 @@ static inline bool perf_paranoid_kernel(void)
 
 extern void perf_event_init(void);
 extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record,
-			  int entry_size, struct pt_regs *regs);
+			  int entry_size, struct pt_regs *regs, void *event);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit e3174cfd2a1e28fff774681f00a0eef3d31da970
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue May 11 08:31:49 2010 +0200

    Revert "perf: Fix exit() vs PERF_FORMAT_GROUP"
    
    This reverts commit 4fd38e4595e2f6c9d27732c042a0e16b2753049c.
    
    It causes various crashes and hangs when events are activated.
    
    The cause is not fully understood yet but we need to revert it
    because the effects are severe.
    
    Reported-by: Stephane Eranian <eranian@google.com>
    Reported-by: Lin Ming <ming.m.lin@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4924c96d7e2d..3fd5c82e0e18 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -575,7 +575,6 @@ struct pmu {
  * enum perf_event_active_state - the states of a event
  */
 enum perf_event_active_state {
-	PERF_EVENT_STATE_FREE		= -3,
 	PERF_EVENT_STATE_ERROR		= -2,
 	PERF_EVENT_STATE_OFF		= -1,
 	PERF_EVENT_STATE_INACTIVE	=  0,

commit 6bde9b6ce0127e2a56228a2071536d422be31336
Author: Lin Ming <ming.m.lin@intel.com>
Date:   Fri Apr 23 13:56:00 2010 +0800

    perf: Add group scheduling transactional APIs
    
    Add group scheduling transactional APIs to struct pmu.
    These APIs will be implemented in arch code, based on Peter's idea as
    below.
    
    > the idea behind hw_perf_group_sched_in() is to not perform
    > schedulability tests on each event in the group, but to add the group
    > as a whole and then perform one test.
    >
    > Of course, when that test fails, you'll have to roll-back the whole
    > group again.
    >
    > So start_txn (or a better name) would simply toggle a flag in the pmu
    > implementation that will make pmu::enable() not perform the
    > schedulablilty test.
    >
    > Then commit_txn() will perform the schedulability test (so note the
    > method has to have a !void return value.
    >
    > This will allow us to use the regular
    > kernel/perf_event.c::group_sched_in() and all the rollback code.
    > Currently each hw_perf_group_sched_in() implementation duplicates all
    > the rolllback code (with various bugs).
    
    ->start_txn:
    Start group events scheduling transaction, set a flag to make
    pmu::enable() not perform the schedulability test, it will be performed
    at commit time.
    
    ->commit_txn:
    Commit group events scheduling transaction, perform the group
    schedulability as a whole
    
    ->cancel_txn:
    Stop group events scheduling transaction, clear the flag so
    pmu::enable() will perform the schedulability test.
    
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Lin Ming <ming.m.lin@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1272002160.5707.60.camel@minggr.sh.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 23cd0057a681..4924c96d7e2d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -547,6 +547,8 @@ struct hw_perf_event {
 
 struct perf_event;
 
+#define PERF_EVENT_TXN_STARTED 1
+
 /**
  * struct pmu - generic performance monitoring unit
  */
@@ -557,6 +559,16 @@ struct pmu {
 	void (*stop)			(struct perf_event *event);
 	void (*read)			(struct perf_event *event);
 	void (*unthrottle)		(struct perf_event *event);
+
+	/*
+	 * group events scheduling is treated as a transaction,
+	 * add group events as a whole and perform one schedulability test.
+	 * If test fails, roll back the whole group
+	 */
+
+	void (*start_txn)	(const struct pmu *pmu);
+	void (*cancel_txn)	(const struct pmu *pmu);
+	int  (*commit_txn)	(const struct pmu *pmu);
 };
 
 /**
@@ -823,9 +835,6 @@ extern void perf_disable(void);
 extern void perf_enable(void);
 extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
-extern int hw_perf_group_sched_in(struct perf_event *group_leader,
-	       struct perf_cpu_context *cpuctx,
-	       struct perf_event_context *ctx);
 extern void perf_event_update_userpage(struct perf_event *event);
 extern int perf_event_release_kernel(struct perf_event *event);
 extern struct perf_event *

commit ab608344bcbde4f55ec4cd911b686b0ce3eae076
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 8 23:03:20 2010 +0200

    perf, x86: Improve the PEBS ABI
    
    Rename perf_event_attr::precise to perf_event_attr::precise_ip and
    widen it to 2 bits. This new field describes the required precision of
    the PERF_SAMPLE_IP field:
    
      0 - SAMPLE_IP can have arbitrary skid
      1 - SAMPLE_IP must have constant skid
      2 - SAMPLE_IP requested to have 0 skid
      3 - SAMPLE_IP must have 0 skid
    
    And modify the Intel PEBS code accordingly. The PEBS implementation
    now supports up to precise_ip == 2, where we perform the IP fixup.
    
    Also s/PERF_RECORD_MISC_EXACT/&_IP/ to clarify its meaning, this bit
    should be set for each PERF_SAMPLE_IP field known to match the actual
    instruction triggering the event.
    
    This new scheme allows for a PEBS mode that uses the buffer for more
    than a single event.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6be4a0f9137c..23cd0057a681 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -203,9 +203,19 @@ struct perf_event_attr {
 				enable_on_exec :  1, /* next exec enables     */
 				task           :  1, /* trace fork/exit       */
 				watermark      :  1, /* wakeup_watermark      */
-				precise        :  1, /* OoO invariant counter */
-
-				__reserved_1   : 48;
+				/*
+				 * precise_ip:
+				 *
+				 *  0 - SAMPLE_IP can have arbitrary skid
+				 *  1 - SAMPLE_IP must have constant skid
+				 *  2 - SAMPLE_IP requested to have 0 skid
+				 *  3 - SAMPLE_IP must have 0 skid
+				 *
+				 *  See also PERF_RECORD_MISC_EXACT_IP
+				 */
+				precise_ip     :  2, /* skid constraint       */
+
+				__reserved_1   : 47;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -296,7 +306,12 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
 #define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
 
-#define PERF_RECORD_MISC_EXACT			(1 << 14)
+/*
+ * Indicates that the content of PERF_SAMPLE_IP points to
+ * the actual instruction that triggered the event. See also
+ * perf_event_attr::precise_ip.
+ */
+#define PERF_RECORD_MISC_EXACT_IP		(1 << 14)
 /*
  * Reserve the last bit to indicate some extended misc field
  */

commit cce913178118b0b36742eb7544c2b38a0c957ee7
Merge: d9f599e1e6d0 4fd38e4595e2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri May 7 11:30:29 2010 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: Resolve patch dependency
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4fd38e4595e2f6c9d27732c042a0e16b2753049c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 6 17:31:38 2010 +0200

    perf: Fix exit() vs PERF_FORMAT_GROUP
    
    Both Stephane and Corey reported that PERF_FORMAT_GROUP didn't work
    as expected if the task the counters were attached to quit before
    the read() call.
    
    The cause is that we unconditionally destroy the grouping when we
    remove counters from their context. Fix this by only doing this when
    we free the counter itself.
    
    Reported-by: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Reported-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1273160566.5605.404.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c8e375440403..bf8f3c003297 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -522,6 +522,7 @@ struct pmu {
  * enum perf_event_active_state - the states of a event
  */
 enum perf_event_active_state {
+	PERF_EVENT_STATE_FREE		= -3,
 	PERF_EVENT_STATE_ERROR		= -2,
 	PERF_EVENT_STATE_OFF		= -1,
 	PERF_EVENT_STATE_INACTIVE	=  0,

commit dcf46b9443ad48a227a61713adea001228925adf
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Tue Apr 20 10:13:58 2010 +0800

    perf & kvm: Clean up some of the guest profiling callback API details
    
    Fix some build bug and programming style issues:
    
     - use valid C
     - fix up various style details
    
    Signed-off-by: Zhang Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Sheng Yang <sheng@linux.intel.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: oerg Roedel <joro@8bytes.org>
    Cc: Jes Sorensen <Jes.Sorensen@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Cc: Zachary Amsden <zamsden@redhat.com>
    Cc: zhiteng.huang@intel.com
    Cc: tim.c.chen@intel.com
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    LKML-Reference: <1271729638.2078.624.camel@ymzhang.sh.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 24de5f181a41..ace31fbac513 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -941,10 +941,8 @@ static inline void perf_event_mmap(struct vm_area_struct *vma)
 }
 
 extern struct perf_guest_info_callbacks *perf_guest_cbs;
-extern int perf_register_guest_info_callbacks(
-		struct perf_guest_info_callbacks *);
-extern int perf_unregister_guest_info_callbacks(
-		struct perf_guest_info_callbacks *);
+extern int perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
+extern int perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *callbacks);
 
 extern void perf_event_comm(struct task_struct *tsk);
 extern void perf_event_fork(struct task_struct *tsk);
@@ -1016,9 +1014,9 @@ static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
 static inline int perf_register_guest_info_callbacks
-(struct perf_guest_info_callbacks *) {return 0; }
+(struct perf_guest_info_callbacks *callbacks) { return 0; }
 static inline int perf_unregister_guest_info_callbacks
-(struct perf_guest_info_callbacks *) {return 0; }
+(struct perf_guest_info_callbacks *callbacks) { return 0; }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }

commit 39447b386c846bbf1c56f6403c5282837486200f
Author: Zhang, Yanmin <yanmin_zhang@linux.intel.com>
Date:   Mon Apr 19 13:32:41 2010 +0800

    perf: Enhance perf to allow for guest statistic collection from host
    
    Below patch introduces perf_guest_info_callbacks and related
    register/unregister functions. Add more PERF_RECORD_MISC_XXX bits
    meaning guest kernel and guest user space.
    
    Signed-off-by: Zhang Yanmin <yanmin_zhang@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index bf896d0b2e9c..24de5f181a41 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -288,11 +288,13 @@ struct perf_event_mmap_page {
 	__u64	data_tail;		/* user-space written tail */
 };
 
-#define PERF_RECORD_MISC_CPUMODE_MASK		(3 << 0)
+#define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)
 #define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
 #define PERF_RECORD_MISC_KERNEL			(1 << 0)
 #define PERF_RECORD_MISC_USER			(2 << 0)
 #define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
+#define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
+#define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
 
 #define PERF_RECORD_MISC_EXACT			(1 << 14)
 /*
@@ -446,6 +448,12 @@ enum perf_callchain_context {
 # include <asm/perf_event.h>
 #endif
 
+struct perf_guest_info_callbacks {
+	int (*is_in_guest) (void);
+	int (*is_user_mode) (void);
+	unsigned long (*get_guest_ip) (void);
+};
+
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 #include <asm/hw_breakpoint.h>
 #endif
@@ -932,6 +940,12 @@ static inline void perf_event_mmap(struct vm_area_struct *vma)
 		__perf_event_mmap(vma);
 }
 
+extern struct perf_guest_info_callbacks *perf_guest_cbs;
+extern int perf_register_guest_info_callbacks(
+		struct perf_guest_info_callbacks *);
+extern int perf_unregister_guest_info_callbacks(
+		struct perf_guest_info_callbacks *);
+
 extern void perf_event_comm(struct task_struct *tsk);
 extern void perf_event_fork(struct task_struct *tsk);
 
@@ -1001,6 +1015,11 @@ perf_sw_event(u32 event_id, u64 nr, int nmi,
 static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
+static inline int perf_register_guest_info_callbacks
+(struct perf_guest_info_callbacks *) {return 0; }
+static inline int perf_unregister_guest_info_callbacks
+(struct perf_guest_info_callbacks *) {return 0; }
+
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }

commit 76e1d9047e4edefb8ada20aa90d5762306082bd6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Apr 5 15:35:57 2010 +0200

    perf: Store active software events in a hashlist
    
    Each time a software event triggers, we need to walk through
    the entire list of events from the current cpu and task contexts
    to retrieve a running perf event that matches.
    We also need to check a matching perf event is actually counting.
    
    This walk is wasteful and makes the event fast path scaling
    down with a growing number of events running on the same
    contexts.
    
    To solve this, we store the running perf events in a hashlist to
    get an immediate access to them against their type:event_id when
    they trigger.
    
    v2: - Fix SWEVENT_HLIST_SIZE definition (and re-learn some basic
          maths along the way)
        - Only allocate hlist for online cpus, but keep track of the
          refcount on offline possible cpus too, so that we allocate it
          if needed when it becomes online.
        - Drop the kref use as it's not adapted to our tricks anymore.
    
    v3: - Fix bad refcount check (address instead of value). Thanks to
          Eric Dumazet who spotted this.
        - While exiting cpu, move the hlist release out of the IPI path
          to lock the hlist mutex sanely.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6e96cc8225d4..bf896d0b2e9c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -589,6 +589,14 @@ enum perf_group_flag {
 	PERF_GROUP_SOFTWARE = 0x1,
 };
 
+#define SWEVENT_HLIST_BITS	8
+#define SWEVENT_HLIST_SIZE	(1 << SWEVENT_HLIST_BITS)
+
+struct swevent_hlist {
+	struct hlist_head	heads[SWEVENT_HLIST_SIZE];
+	struct rcu_head		rcu_head;
+};
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -597,6 +605,7 @@ struct perf_event {
 	struct list_head		group_entry;
 	struct list_head		event_entry;
 	struct list_head		sibling_list;
+	struct hlist_node		hlist_entry;
 	int				nr_siblings;
 	int				group_flags;
 	struct perf_event		*group_leader;
@@ -744,6 +753,9 @@ struct perf_cpu_context {
 	int				active_oncpu;
 	int				max_pertask;
 	int				exclusive;
+	struct swevent_hlist		*swevent_hlist;
+	struct mutex			hlist_mutex;
+	int				hlist_refcount;
 
 	/*
 	 * Recursion avoidance:

commit ec5e61aabeac58670691bd0613388d16697d0d81
Merge: 75ec5a245c77 8bb39f9aa068
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 2 19:37:50 2010 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Conflicts:
            arch/x86/kernel/cpu/perf_event.c
    
    Merge reason: Resolve the conflict, pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e49a5bd38159dfb1928fd25b173bc9de4bbadb21
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Mar 22 19:40:03 2010 +0100

    perf: Use hot regs with software sched switch/migrate events
    
    Scheduler's task migration events don't work because they always
    pass NULL regs perf_sw_event(). The event hence gets filtered
    in perf_swevent_add().
    
    Scheduler's context switches events use task_pt_regs() to get
    the context when the event occured which is a wrong thing to
    do as this won't give us the place in the kernel where we went
    to sleep but the place where we left userspace. The result is
    even more wrong if we switch from a kernel thread.
    
    Use the hot regs snapshot for both events as they belong to the
    non-interrupt/exception based events family. Unlike page faults
    or so that provide the regs matching the exact origin of the event,
    we need to save the current context.
    
    This makes the task migration event working and fix the context
    switch callchains and origin ip.
    
    Example: perf record -a -e cs
    
    Before:
    
        10.91%      ksoftirqd/0                  0  [k] 0000000000000000
                    |
                    --- (nil)
                        perf_callchain
                        perf_prepare_sample
                        __perf_event_overflow
                        perf_swevent_overflow
                        perf_swevent_add
                        perf_swevent_ctx_event
                        do_perf_sw_event
                        __perf_sw_event
                        perf_event_task_sched_out
                        schedule
                        run_ksoftirqd
                        kthread
                        kernel_thread_helper
    
    After:
    
        23.77%  hald-addon-stor  [kernel.kallsyms]  [k] schedule
                |
                --- schedule
                   |
                   |--60.00%-- schedule_timeout
                   |          wait_for_common
                   |          wait_for_completion
                   |          blk_execute_rq
                   |          scsi_execute
                   |          scsi_execute_req
                   |          sr_test_unit_ready
                   |          |
                   |          |--66.67%-- sr_media_change
                   |          |          media_changed
                   |          |          cdrom_media_changed
                   |          |          sr_block_media_changed
                   |          |          check_disk_change
                   |          |          cdrom_open
    
    v2: Always build perf_arch_fetch_caller_regs() now that software
    events need that too. They don't need it from modules, unlike trace
    events, so we keep the EXPORT_SYMBOL in trace_event_perf.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 95477038a72a..c8e375440403 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -842,13 +842,6 @@ extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
-static inline void
-perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
-{
-	if (atomic_read(&perf_swevent_enabled[event_id]))
-		__perf_sw_event(event_id, nr, nmi, regs, addr);
-}
-
 extern void
 perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip, int skip);
 
@@ -887,6 +880,20 @@ static inline void perf_fetch_caller_regs(struct pt_regs *regs, int skip)
 	return perf_arch_fetch_caller_regs(regs, ip, skip);
 }
 
+static inline void
+perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
+{
+	if (atomic_read(&perf_swevent_enabled[event_id])) {
+		struct pt_regs hot_regs;
+
+		if (!regs) {
+			perf_fetch_caller_regs(&hot_regs, 1);
+			regs = &hot_regs;
+		}
+		__perf_sw_event(event_id, nr, nmi, regs, addr);
+	}
+}
+
 extern void __perf_event_mmap(struct vm_area_struct *vma);
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)

commit 937779db13fb6cb621e28d9ae0a6cf1d05b57d05
Merge: 6230f2c7ef01 9f591fd76afd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 12 10:20:57 2010 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: We want to queue up a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 85cfabbcd10f8d112feee6e2ec64ee78033b6d3c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 11 13:06:56 2010 +0100

    perf, ppc: Fix compile error due to new cpu notifiers
    
    Fix:
    
      arch/powerpc/kernel/perf_event.c:1334: error: 'power_pmu_notifier' undeclared (first use in this function)
      arch/powerpc/kernel/perf_event.c:1334: error: (Each undeclared identifier is reported only once
      arch/powerpc/kernel/perf_event.c:1334: error: for each function it appears in.)
      arch/powerpc/kernel/perf_event.c:1334: error: implicit declaration of function 'power_pmu_notifier'
      arch/powerpc/kernel/perf_event.c:1334: error: implicit declaration of function 'register_cpu_notifier'
    
    Due to commit 3f6da390 (perf: Rework and fix the arch CPU-hotplug hooks).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 70cffd052c04..95477038a72a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -453,6 +453,7 @@ enum perf_callchain_context {
 #include <linux/pid_namespace.h>
 #include <linux/workqueue.h>
 #include <linux/ftrace.h>
+#include <linux/cpu.h>
 #include <asm/atomic.h>
 
 #define PERF_MAX_STACK_DEPTH		255

commit 5331d7b84613b8325362dde53dc2bff2fb87d351
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Mar 4 21:15:56 2010 +0100

    perf: Introduce new perf_fetch_caller_regs() for hot regs snapshot
    
    Events that trigger overflows by interrupting a context can
    use get_irq_regs() or task_pt_regs() to retrieve the state
    when the event triggered. But this is not the case for some
    other class of events like trace events as tracepoints are
    executed in the same context than the code that triggered
    the event.
    
    It means we need a different api to capture the regs there,
    namely we need a hot snapshot to get the most important
    informations for perf: the instruction pointer to get the
    event origin, the frame pointer for the callchain, the code
    segment for user_mode() tests (we always use __KERNEL_CS as
    trace events always occur from the kernel) and the eflags
    for further purposes.
    
    v2: rename perf_save_regs to perf_fetch_caller_regs as per
    Masami's suggestion.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Archs <linux-arch@vger.kernel.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 80acbf3d5de1..70cffd052c04 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -452,6 +452,7 @@ enum perf_callchain_context {
 #include <linux/fs.h>
 #include <linux/pid_namespace.h>
 #include <linux/workqueue.h>
+#include <linux/ftrace.h>
 #include <asm/atomic.h>
 
 #define PERF_MAX_STACK_DEPTH		255
@@ -847,6 +848,44 @@ perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 		__perf_sw_event(event_id, nr, nmi, regs, addr);
 }
 
+extern void
+perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip, int skip);
+
+/*
+ * Take a snapshot of the regs. Skip ip and frame pointer to
+ * the nth caller. We only need a few of the regs:
+ * - ip for PERF_SAMPLE_IP
+ * - cs for user_mode() tests
+ * - bp for callchains
+ * - eflags, for future purposes, just in case
+ */
+static inline void perf_fetch_caller_regs(struct pt_regs *regs, int skip)
+{
+	unsigned long ip;
+
+	memset(regs, 0, sizeof(*regs));
+
+	switch (skip) {
+	case 1 :
+		ip = CALLER_ADDR0;
+		break;
+	case 2 :
+		ip = CALLER_ADDR1;
+		break;
+	case 3 :
+		ip = CALLER_ADDR2;
+		break;
+	case 4:
+		ip = CALLER_ADDR3;
+		break;
+	/* No need to support further for now */
+	default:
+		ip = 0;
+	}
+
+	return perf_arch_fetch_caller_regs(regs, ip, skip);
+}
+
 extern void __perf_event_mmap(struct vm_area_struct *vma);
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)
@@ -880,7 +919,8 @@ static inline bool perf_paranoid_kernel(void)
 }
 
 extern void perf_event_init(void);
-extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record, int entry_size);
+extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record,
+			  int entry_size, struct pt_regs *regs);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags

commit ef21f683a045a79b6aa86ad81e5fdfc0d5ddd250
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 3 13:12:23 2010 +0100

    perf, x86: use LBR for PEBS IP+1 fixup
    
    Use the LBR to fix up the PEBS IP+1 issue.
    
    As said, PEBS reports the next instruction, here we use the LBR to find
    the last branch and from that construct the actual IP. If the IP matches
    the LBR-TO, we use LBR-FROM, otherwise we use the LBR-TO address as the
    beginning of the last basic block and decode forward.
    
    Once we find a match to the current IP, we use the previous location.
    
    This patch introduces a new ABI element: PERF_RECORD_MISC_EXACT, which
    conveys that the reported IP (PERF_SAMPLE_IP) is the exact instruction
    that caused the event (barring CPU errata).
    
    The fixup can fail due to various reasons:
    
     1) LBR contains invalid data (quite possible)
     2) part of the basic block got paged out
     3) the reported IP isn't part of the basic block (see 1)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100304140100.619375431@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ab4fd9ede264..be85f7c4a94f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -294,6 +294,12 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_USER			(2 << 0)
 #define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
 
+#define PERF_RECORD_MISC_EXACT			(1 << 14)
+/*
+ * Reserve the last bit to indicate some extended misc field
+ */
+#define PERF_RECORD_MISC_EXT_RESERVED		(1 << 15)
+
 struct perf_event_header {
 	__u32	type;
 	__u16	misc;

commit caff2befffe899e63df5cc760b7ed01cfd902685
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 3 12:02:30 2010 +0100

    perf, x86: Implement simple LBR support
    
    Implement simple suport Intel Last-Branch-Record, it supports all
    hardware that implements FREEZE_LBRS_ON_PMI, but does not (yet) implement
    the LBR config register.
    
    The Intel LBR is a FIFO of From,To addresses describing the last few
    branches the hardware took.
    
    This patch does not add perf interface to the LBR, but merely provides an
    interface for internal use.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100304140100.544191154@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 42307b50c787..ab4fd9ede264 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -467,6 +467,17 @@ struct perf_raw_record {
 	void				*data;
 };
 
+struct perf_branch_entry {
+	__u64				from;
+	__u64				to;
+	__u64				flags;
+};
+
+struct perf_branch_stack {
+	__u64				nr;
+	struct perf_branch_entry	entries[0];
+};
+
 struct task_struct;
 
 /**

commit ca037701a025334e724e5c61b3b1082940c8b981
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Mar 2 19:52:12 2010 +0100

    perf, x86: Add PEBS infrastructure
    
    This patch implements support for Intel Precise Event Based Sampling,
    which is an alternative counter mode in which the counter triggers a
    hardware assist to collect information on events. The hardware assist
    takes a trap like snapshot of a subset of the machine registers.
    
    This data is written to the Intel Debug-Store, which can be programmed
    with a data threshold at which to raise a PMI.
    
    With the PEBS hardware assist being trap like, the reported IP is always
    one instruction after the actual instruction that triggered the event.
    
    This implements a simple PEBS model that always takes a single PEBS event
    at a time. This is done so that the interaction with the rest of the
    system is as expected (freq adjust, period randomization, lbr,
    callchains, etc.).
    
    It adds an ABI element: perf_event_attr::precise, which indicates that we
    wish to use this (constrained, but precise) mode.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100304140100.392111285@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 80acbf3d5de1..42307b50c787 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -203,8 +203,9 @@ struct perf_event_attr {
 				enable_on_exec :  1, /* next exec enables     */
 				task           :  1, /* trace fork/exit       */
 				watermark      :  1, /* wakeup_watermark      */
+				precise        :  1, /* OoO invariant counter */
 
-				__reserved_1   : 49;
+				__reserved_1   : 48;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */

commit 3f6da3905398826d85731247e7fbcf53400c18bd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Mar 5 13:01:18 2010 +0100

    perf: Rework and fix the arch CPU-hotplug hooks
    
    Remove the hw_perf_event_*() hotplug hooks in favour of per PMU hotplug
    notifiers. This has the advantage of reducing the static weak interface
    as well as exposing all hotplug actions to the PMU.
    
    Use this to fix x86 hotplug usage where we did things in ONLINE which
    should have been done in UP_PREPARE or STARTING.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: paulus@samba.org
    Cc: eranian@google.com
    Cc: robert.richter@amd.com
    Cc: fweisbec@gmail.com
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    LKML-Reference: <20100305154128.736225361@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 6f8cd7da1a01..80acbf3d5de1 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -936,5 +936,21 @@ static inline void perf_event_disable(struct perf_event *event)		{ }
 #define perf_output_put(handle, x) \
 	perf_output_copy((handle), &(x), sizeof(x))
 
+/*
+ * This has to have a higher priority than migration_notifier in sched.c.
+ */
+#define perf_cpu_notifier(fn)					\
+do {								\
+	static struct notifier_block fn##_nb __cpuinitdata =	\
+		{ .notifier_call = fn, .priority = 20 };	\
+	fn(&fn##_nb, (unsigned long)CPU_UP_PREPARE,		\
+		(void *)(unsigned long)smp_processor_id());	\
+	fn(&fn##_nb, (unsigned long)CPU_STARTING,		\
+		(void *)(unsigned long)smp_processor_id());	\
+	fn(&fn##_nb, (unsigned long)CPU_ONLINE,			\
+		(void *)(unsigned long)smp_processor_id());	\
+	register_cpu_notifier(&fn##_nb);			\
+} while (0)
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_PERF_EVENT_H */

commit dc1d628a67a8f042e711ea5accc0beedc3ef0092
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 3 15:55:04 2010 +0100

    perf: Provide generic perf_sample_data initialization
    
    This makes it easier to extend perf_sample_data and fixes a bug on arm
    and sparc, which failed to set ->raw to NULL, which can cause crashes
    when combined with PERF_SAMPLE_RAW.
    
    It also optimizes PowerPC and tracepoint, because the struct
    initialization is forced to zero out the whole structure.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Jean Pihet <jpihet@mvista.com>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Jamie Iles <jamie.iles@picochip.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: stable@kernel.org
    LKML-Reference: <20100304140100.315416040@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 90e0521b1690..6f8cd7da1a01 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -801,6 +801,13 @@ struct perf_sample_data {
 	struct perf_raw_record		*raw;
 };
 
+static inline
+void perf_sample_data_init(struct perf_sample_data *data, u64 addr)
+{
+	data->addr = addr;
+	data->raw  = NULL;
+}
+
 extern void perf_output_sample(struct perf_output_handle *handle,
 			       struct perf_event_header *header,
 			       struct perf_sample_data *data,

commit 320ebf09cbb6d01954c9a060266aa8e0d27f4638
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Mar 2 12:35:37 2010 +0100

    perf, x86: Restrict the ANY flag
    
    The ANY flag can show SMT data of another task (like 'top'),
    so we want to disable it when system-wide profiling is
    disabled.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 04f06b4be297..90e0521b1690 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -857,6 +857,21 @@ extern int sysctl_perf_event_paranoid;
 extern int sysctl_perf_event_mlock;
 extern int sysctl_perf_event_sample_rate;
 
+static inline bool perf_paranoid_tracepoint_raw(void)
+{
+	return sysctl_perf_event_paranoid > -1;
+}
+
+static inline bool perf_paranoid_cpu(void)
+{
+	return sysctl_perf_event_paranoid > 0;
+}
+
+static inline bool perf_paranoid_kernel(void)
+{
+	return sysctl_perf_event_paranoid > 1;
+}
+
 extern void perf_event_init(void);
 extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record, int entry_size);
 extern void perf_bp_event(struct perf_event *event, void *data);

commit dd8b1cf681eab40bc5afb67bdd06b2ca341f5669
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 27 17:10:39 2010 +0100

    perf: Remove pointless breakpoint union
    
    Remove pointless union in the breakpoint field of hw_perf_event.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 7b18b4fd5df7..04f06b4be297 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -487,9 +487,8 @@ struct hw_perf_event {
 			struct hrtimer	hrtimer;
 		};
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-		union { /* breakpoint */
-			struct arch_hw_breakpoint	info;
-		};
+		/* breakpoint */
+		struct arch_hw_breakpoint	info;
 #endif
 	};
 	atomic64_t			prev_count;

commit 018cbffe6819f6f8db20a0a3acd9bab9bfd667e4
Merge: 1dd2980d9900 60b341b778cc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 27 16:18:46 2010 +0100

    Merge commit 'v2.6.33' into perf/core
    
    Merge reason:
            __percpu annotations need the corresponding sparse address
    space definition upstream.
    
    Conflicts:
            tools/perf/util/probe-event.c (trivial)

commit 6e37738a2fac964583debe91099bc3248554f6e5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 11 13:21:58 2010 +0100

    perf_events: Simplify code by removing cpu argument to hw_perf_group_sched_in()
    
    Since the cpu argument to hw_perf_group_sched_in() is always
    smp_processor_id(), simplify the code a little by removing this argument
    and using the current cpu where needed.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Miller <davem@davemloft.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1265890918.5396.3.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b08dfdad08cb..d0e072c5b58a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -772,7 +772,7 @@ extern int perf_event_task_disable(void);
 extern int perf_event_task_enable(void);
 extern int hw_perf_group_sched_in(struct perf_event *group_leader,
 	       struct perf_cpu_context *cpuctx,
-	       struct perf_event_context *ctx, int cpu);
+	       struct perf_event_context *ctx);
 extern void perf_event_update_userpage(struct perf_event *event);
 extern int perf_event_release_kernel(struct perf_event *event);
 extern struct perf_event *

commit d76a0812ac4139ceb54daab3cc70e1bd8bd9d43a
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Feb 8 17:06:01 2010 +0200

    perf_events: Add new start/stop PMU callbacks
    
    In certain situations, the kernel may need to stop and start the same
    event rapidly. The current PMU callbacks do not distinguish between stop
    and release (i.e., stop + free the resource). Thus, a counter may be
    released, then it will be immediately re-acquired. Event scheduling will
    again take place with no guarantee to assign the same counter. On some
    processors, this may event yield to failure to assign the event back due
    to competion between cores.
    
    This patch is adding a new pair of callback to stop and restart a counter
    without actually release the underlying counter resource. On stop, the
    counter is stopped, its values saved and that's it. On start, the value
    is reloaded and counter is restarted (on x86, actual restart is delayed
    until perf_enable()).
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    [ added fallback to ->enable/->disable for all other PMUs
      fixed x86_pmu_start() to call x86_pmu.enable()
      merged __x86_pmu_disable into x86_pmu_stop() ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4b703875.0a04d00a.7896.ffffb824@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 071a7db52549..b08dfdad08cb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -513,6 +513,8 @@ struct perf_event;
 struct pmu {
 	int (*enable)			(struct perf_event *event);
 	void (*disable)			(struct perf_event *event);
+	int (*start)			(struct perf_event *event);
+	void (*stop)			(struct perf_event *event);
 	void (*read)			(struct perf_event *event);
 	void (*unthrottle)		(struct perf_event *event);
 };

commit 447a194b393f32699607fd99617a40abd6a95114
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Feb 1 14:50:01 2010 +0200

    perf_events, x86: Fix bug in hw_perf_enable()
    
    We cannot assume that because hwc->idx == assign[i], we can avoid
    reprogramming the counter in hw_perf_enable().
    
    The event may have been scheduled out and another event may have been
    programmed into this counter. Thus, we need a more robust way of
    verifying if the counter still contains config/data related to an event.
    
    This patch adds a generation number to each counter on each cpu. Using
    this mechanism we can verify reliabilty whether the content of a counter
    corresponds to an event.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4b66dc67.0b38560a.1635.ffffae18@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 556b0f4a668e..071a7db52549 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -478,9 +478,11 @@ struct hw_perf_event {
 	union {
 		struct { /* hardware */
 			u64		config;
+			u64		last_tag;
 			unsigned long	config_base;
 			unsigned long	event_base;
 			int		idx;
+			int		last_cpu;
 		};
 		struct { /* software */
 			s64		remaining;

commit cd757645fbdc34a8343c04bb0e74e06fccc2cb10
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Sat Jan 30 10:25:18 2010 +0530

    perf: Make bp_len type to u64 generic across the arch
    
    Change 'bp_len' type to __u64 to make it work across archs as
    the s390 architecture watch point length can be upto 2^64.
    
    reference:
            http://lkml.org/lkml/2010/1/25/212
    
    This is an ABI change that is not backward compatible with
    the previous hardware breakpoint info layout integrated in this
    development cycle, a rebuilt of perf tools is necessary for
    versions based on 2.6.33-rc1 - 2.6.33-rc6 to work with a
    kernel based on this patch.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Maneesh Soni <maneesh@in.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin <schwidefsky@de.ibm.com>
    LKML-Reference: <20100130045518.GA20776@in.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8fa71874113f..a177698d95e2 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -211,11 +211,9 @@ struct perf_event_attr {
 		__u32		wakeup_watermark; /* bytes before wakeup   */
 	};
 
-	__u32			__reserved_2;
-
-	__u64			bp_addr;
 	__u32			bp_type;
-	__u32			bp_len;
+	__u64			bp_addr;
+	__u64			bp_len;
 };
 
 /*

commit ae7f6711d6231c9ba54feb5ba9856c3775e482f8
Merge: 64abebf731df b23ff0e9330e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 29 09:24:57 2010 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: We want to queue up a dependent patch. Also update to
                  later -rc's.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 184f412c3341cd24fbd26604634a5800b83dbdc3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 27 08:39:39 2010 +0100

    perf, x86: Clean up event constraints code a bit
    
    - Remove stray debug code
     - Improve ugly macros a bit
     - Remove some whitespace damage
     - (Also fix up some accumulated damage in perf_event.h)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 72b2615600d8..953c17731e0d 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -290,7 +290,7 @@ struct perf_event_mmap_page {
 };
 
 #define PERF_RECORD_MISC_CPUMODE_MASK		(3 << 0)
-#define PERF_RECORD_MISC_CPUMODE_UNKNOWN		(0 << 0)
+#define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
 #define PERF_RECORD_MISC_KERNEL			(1 << 0)
 #define PERF_RECORD_MISC_USER			(2 << 0)
 #define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
@@ -356,8 +356,8 @@ enum perf_event_type {
 	 *	u64				stream_id;
 	 * };
 	 */
-	PERF_RECORD_THROTTLE		= 5,
-	PERF_RECORD_UNTHROTTLE		= 6,
+	PERF_RECORD_THROTTLE			= 5,
+	PERF_RECORD_UNTHROTTLE			= 6,
 
 	/*
 	 * struct {
@@ -371,10 +371,10 @@ enum perf_event_type {
 
 	/*
 	 * struct {
-	 * 	struct perf_event_header	header;
-	 * 	u32				pid, tid;
+	 *	struct perf_event_header	header;
+	 *	u32				pid, tid;
 	 *
-	 * 	struct read_format		values;
+	 *	struct read_format		values;
 	 * };
 	 */
 	PERF_RECORD_READ			= 8,
@@ -412,7 +412,7 @@ enum perf_event_type {
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
 	 * };
 	 */
-	PERF_RECORD_SAMPLE		= 9,
+	PERF_RECORD_SAMPLE			= 9,
 
 	PERF_RECORD_MAX,			/* non-ABI */
 };
@@ -752,8 +752,7 @@ extern int perf_max_events;
 extern const struct pmu *hw_perf_event_init(struct perf_event *event);
 
 extern void perf_event_task_sched_in(struct task_struct *task);
-extern void perf_event_task_sched_out(struct task_struct *task,
-					struct task_struct *next);
+extern void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next);
 extern void perf_event_task_tick(struct task_struct *task);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
@@ -853,8 +852,7 @@ extern int sysctl_perf_event_mlock;
 extern int sysctl_perf_event_sample_rate;
 
 extern void perf_event_init(void);
-extern void perf_tp_event(int event_id, u64 addr, u64 count,
-				 void *record, int entry_size);
+extern void perf_tp_event(int event_id, u64 addr, u64 count, void *record, int entry_size);
 extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags
@@ -895,13 +893,13 @@ static inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi,
 		     struct pt_regs *regs, u64 addr)			{ }
 static inline void
-perf_bp_event(struct perf_event *event, void *data)		{ }
+perf_bp_event(struct perf_event *event, void *data)			{ }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
-static inline int  perf_swevent_get_recursion_context(void)  { return -1; }
+static inline int  perf_swevent_get_recursion_context(void)		{ return -1; }
 static inline void perf_swevent_put_recursion_context(int rctx)		{ }
 static inline void perf_event_enable(struct perf_event *event)		{ }
 static inline void perf_event_disable(struct perf_event *event)		{ }

commit abd50713944c8ea9e0af5b7bffa0aacae21cc91a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jan 26 18:50:16 2010 +0100

    perf: Reimplement frequency driven sampling
    
    There was a bug in the old period code that caused intel_pmu_enable_all()
    or native_write_msr_safe() to show up quite high in the profiles.
    
    In staring at that code it made my head hurt, so I rewrote it in a
    hopefully simpler fashion. Its now fully symetric between tick and
    overflow driven adjustments and uses less data to boot.
    
    The only complication is that it basically wants to do a u128 division.
    The code approximates that in a rather simple truncate until it fits
    fashion, taking care to balance the terms while truncating.
    
    This version does not generate that sampling artefact.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c6f812e4d058..72b2615600d8 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -498,9 +498,8 @@ struct hw_perf_event {
 	atomic64_t			period_left;
 	u64				interrupts;
 
-	u64				freq_count;
-	u64				freq_interrupts;
-	u64				freq_stamp;
+	u64				freq_time_stamp;
+	u64				freq_count_stamp;
 #endif
 };
 

commit 92b6759857ea3ad19bc6871044e373f6251841d3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jan 18 14:02:16 2010 +0100

    perf: Change the is_software_event() definition
    
    The is_software_event() definition always confuses me because its an
    exclusive expression, make it an inclusive one.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c66b34f75eea..8fa71874113f 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -814,9 +814,14 @@ extern int perf_event_overflow(struct perf_event *event, int nmi,
  */
 static inline int is_software_event(struct perf_event *event)
 {
-	return (event->attr.type != PERF_TYPE_RAW) &&
-		(event->attr.type != PERF_TYPE_HARDWARE) &&
-		(event->attr.type != PERF_TYPE_HW_CACHE);
+	switch (event->attr.type) {
+	case PERF_TYPE_SOFTWARE:
+	case PERF_TYPE_TRACEPOINT:
+	/* for now the breakpoint stuff also works as software event */
+	case PERF_TYPE_BREAKPOINT:
+		return 1;
+	}
+	return 0;
 }
 
 extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];

commit d6f962b57bfaab62891c7abbf1469212a56d6103
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jan 10 01:25:51 2010 +0100

    perf: Export software-only event group characteristic as a flag
    
    Before scheduling an event group, we first check if a group can go
    on. We first check if the group is made of software only events
    first, in which case it is enough to know if the group can be
    scheduled in.
    
    For that purpose, we iterate through the whole group, which is
    wasteful as we could do this check when we add/delete an event to
    a group.
    
    So we create a group_flags field in perf event that can host
    characteristics from a group of events, starting with a first
    PERF_GROUP_SOFTWARE flag that reduces the check on the fast path.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index cdbc2aa64a0b..c6f812e4d058 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -565,6 +565,10 @@ typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
 
+enum perf_group_flag {
+	PERF_GROUP_SOFTWARE = 0x1,
+};
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -574,6 +578,7 @@ struct perf_event {
 	struct list_head		event_entry;
 	struct list_head		sibling_list;
 	int				nr_siblings;
+	int				group_flags;
 	struct perf_event		*group_leader;
 	struct perf_event		*output;
 	const struct pmu		*pmu;

commit 889ff0150661512d79484219612b7e2e024b6c07
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jan 9 20:04:47 2010 +0100

    perf/core: Split context's event group list into pinned and non-pinned lists
    
    Split-up struct perf_event_context::group_list into pinned_groups
    and flexible_groups (non-pinned).
    
    This first appears to be useless as it duplicates various loops around
    the group list handlings.
    
    But it scales better in the fast-path in perf_sched_in(). We don't
    anymore iterate twice through the entire list to separate pinned and
    non-pinned scheduling. Instead we interate through two distinct lists.
    
    The another desired effect is that it makes easier to define distinct
    scheduling rules on both.
    
    Changes in v2:
    - Respectively rename pinned_grp_list and
      volatile_grp_list into pinned_groups and flexible_groups as per
      Ingo suggestion.
    - Various cleanups
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9a1d276db754..cdbc2aa64a0b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -683,7 +683,8 @@ struct perf_event_context {
 	 */
 	struct mutex			mutex;
 
-	struct list_head		group_list;
+	struct list_head		pinned_groups;
+	struct list_head		flexible_groups;
 	struct list_head		event_list;
 	int				nr_events;
 	int				nr_active;

commit 07b139c8c81b97bbe55c68daf0cbeca8b1c609ca
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Dec 21 14:27:35 2009 +0800

    perf events: Remove CONFIG_EVENT_PROFILE
    
    Quoted from Ingo:
    
    | This reminds me - i think we should eliminate CONFIG_EVENT_PROFILE -
    | it's an unnecessary Kconfig complication. If both PERF_EVENTS and
    | EVENT_TRACING is enabled we should expose generic tracepoints.
    |
    | Nor is it limited to event 'profiling', so it has become a misnomer as
    | well.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <4B2F1557.2050705@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a494e7501292..9a1d276db754 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -658,7 +658,7 @@ struct perf_event {
 
 	perf_overflow_handler_t		overflow_handler;
 
-#ifdef CONFIG_EVENT_PROFILE
+#ifdef CONFIG_EVENT_TRACING
 	struct event_filter		*filter;
 #endif
 

commit 49f474331e563a6ecf3b1e87ec27ec5482b3e4f1
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sun Dec 27 11:51:52 2009 +0100

    perf events: Remove arg from perf sched hooks
    
    Since we only ever schedule the local cpu, there is no need to pass the
    cpu number to the perf sched hooks.
    
    This micro-optimizes things a bit.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index c66b34f75eea..a494e7501292 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -746,10 +746,10 @@ extern int perf_max_events;
 
 extern const struct pmu *hw_perf_event_init(struct perf_event *event);
 
-extern void perf_event_task_sched_in(struct task_struct *task, int cpu);
+extern void perf_event_task_sched_in(struct task_struct *task);
 extern void perf_event_task_sched_out(struct task_struct *task,
-					struct task_struct *next, int cpu);
-extern void perf_event_task_tick(struct task_struct *task, int cpu);
+					struct task_struct *next);
+extern void perf_event_task_tick(struct task_struct *task);
 extern int perf_event_init_task(struct task_struct *child);
 extern void perf_event_exit_task(struct task_struct *child);
 extern void perf_event_free_task(struct task_struct *task);
@@ -870,12 +870,12 @@ extern void perf_event_enable(struct perf_event *event);
 extern void perf_event_disable(struct perf_event *event);
 #else
 static inline void
-perf_event_task_sched_in(struct task_struct *task, int cpu)		{ }
+perf_event_task_sched_in(struct task_struct *task)			{ }
 static inline void
 perf_event_task_sched_out(struct task_struct *task,
-			    struct task_struct *next, int cpu)		{ }
+			    struct task_struct *next)			{ }
 static inline void
-perf_event_task_tick(struct task_struct *task, int cpu)			{ }
+perf_event_task_tick(struct task_struct *task)				{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }

commit 8aedf8a6ae98d5d4df3254b6afb7e4432d9d8600
Merge: bac5e54c29f3 60ab271617ce
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 16 12:32:47 2009 -0800

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (52 commits)
      perf record: Use per-task-per-cpu events for inherited events
      perf record: Properly synchronize child creation
      perf events: Allow per-task-per-cpu counters
      perf diff: Percent calcs should use double values
      perf diff: Change the default sort order to "dso,symbol"
      perf diff: Use perf_session__fprintf_hists just like 'perf record'
      perf report: Fix cut'n'paste error recently introduced
      perf session: Move perf report specific hits out of perf_session__fprintf_hists
      perf tools: Move hist entries printing routines from perf report
      perf report: Generalize perf_session__fprintf_hists()
      perf symbols: Move symbol filtering to event__preprocess_sample()
      perf symbols: Adopt the strlists for dso, comm
      perf symbols: Make symbol_conf global
      perf probe: Fix to show which probe point is not found
      perf probe: Check symbols in symtab/kallsyms
      perf probe: Check build-id of vmlinux
      perf probe: Reject second attempt of adding same-name event
      perf probe: Support event name for --add option
      perf probe: Add glob matching support on --del
      perf probe: Use strlist__for_each macros in probe-event.c
      ...

commit f13c12c634e124d5d31f912b969d542a016d6105
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 15 19:43:11 2009 +0100

    perf_events: Fix perf_event_attr layout
    
    The miss-alignment of bp_addr created a 32bit hole, causing
    different structure packings on 32 and 64 bit machines.
    
    Fix that by moving __reserve_2 into that hole.
    
    Further, remove the useless struct and redundant __bp_reserve
    muck.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <1260902591.8023.781.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 64a53f74c9a9..5fcbf7d2712a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -211,17 +211,11 @@ struct perf_event_attr {
 		__u32		wakeup_watermark; /* bytes before wakeup   */
 	};
 
-	struct { /* Hardware breakpoint info */
-		__u64		bp_addr;
-		__u32		bp_type;
-		__u32		bp_len;
-		__u64		__bp_reserved_1;
-		__u64		__bp_reserved_2;
-	};
-
 	__u32			__reserved_2;
 
-	__u64			__reserved_3;
+	__u64			bp_addr;
+	__u32			bp_type;
+	__u32			bp_len;
 };
 
 /*

commit e625cce1b73fb38b74e5387226534f7bcbfc36fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 18:02:06 2009 +0100

    perf_event: Convert to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 64a53f74c9a9..da7bdc23f279 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -681,7 +681,7 @@ struct perf_event_context {
 	 * Protect the states of the events in the list,
 	 * nr_active, and the list:
 	 */
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 	/*
 	 * Protect the list of events.  Locking either mutex or lock
 	 * is sufficient to ensure the list doesn't change; to change

commit 44234adcdce38f83c56e05f808ce656175b4beeb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 9 09:25:48 2009 +0100

    hw-breakpoints: Modify breakpoints without unregistering them
    
    Currently, when ptrace needs to modify a breakpoint, like disabling
    it, changing its address, type or len, it calls
    modify_user_hw_breakpoint(). This latter will perform the heavy and
    racy task of unregistering the old breakpoint and registering a new
    one.
    
    This is racy as someone else might steal the reserved breakpoint
    slot under us, which is undesired as the breakpoint is only
    supposed to be modified, sometimes in the middle of a debugging
    workflow. We don't want our slot to be stolen in the middle.
    
    So instead of unregistering/registering the breakpoint, just
    disable it while we modify its breakpoint fields and re-enable it
    after if necessary.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    LKML-Reference: <1260347148-5519-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index bf3329413e18..64a53f74c9a9 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -872,6 +872,8 @@ extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
 extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
+extern void perf_event_enable(struct perf_event *event);
+extern void perf_event_disable(struct perf_event *event);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *task, int cpu)		{ }
@@ -902,7 +904,8 @@ static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
 static inline int  perf_swevent_get_recursion_context(void)  { return -1; }
 static inline void perf_swevent_put_recursion_context(int rctx)		{ }
-
+static inline void perf_event_enable(struct perf_event *event)		{ }
+static inline void perf_event_disable(struct perf_event *event)		{ }
 #endif
 
 #define perf_output_put(handle, x) \

commit 2ff6cfd70720780234fdfea636218c2a62b31287
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Dec 7 17:12:58 2009 +0100

    perf events: hw_breakpoints: Don't include asm/hw_breakpoint.h in user space
    
    asm/hw_breakpoint.h is evidently a kernel internal file and
    should not be included globally, not even under an #ifdef.
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    LKML-Reference: <200912071712.58650.arnd@arndb.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 89098e35a036..bf3329413e18 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -18,10 +18,6 @@
 #include <linux/ioctl.h>
 #include <asm/byteorder.h>
 
-#ifdef CONFIG_HAVE_HW_BREAKPOINT
-#include <asm/hw_breakpoint.h>
-#endif
-
 /*
  * User-space ABI bits:
  */
@@ -451,6 +447,10 @@ enum perf_callchain_context {
 # include <asm/perf_event.h>
 #endif
 
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+#include <asm/hw_breakpoint.h>
+#endif
+
 #include <linux/list.h>
 #include <linux/mutex.h>
 #include <linux/rculist.h>

commit c0dfb2feb632537cf0a9d2ce3c29bcf5778fec59
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 5 09:53:28 2009 +0100

    perf: Remove the "event" callback from perf events
    
    As it is not used anymore and has been superseded by overflow_handler.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index d2f2667430da..89098e35a036 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -567,7 +567,6 @@ struct perf_pending_entry {
 
 struct perf_sample_data;
 
-typedef void (*perf_callback_t)(struct perf_event *, void *);
 typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
@@ -669,8 +668,6 @@ struct perf_event {
 	struct event_filter		*filter;
 #endif
 
-	perf_callback_t			callback;
-
 #endif /* CONFIG_PERF_EVENTS */
 };
 

commit b326e9560a28fc3e950637ef51847ed8f05c1335
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 5 09:44:31 2009 +0100

    hw-breakpoints: Use overflow handler instead of the event callback
    
    struct perf_event::event callback was called when a breakpoint
    triggers. But this is a rather opaque callback, pretty
    tied-only to the breakpoint API and not really integrated into perf
    as it triggers even when we don't overflow.
    
    We prefer to use overflow_handler() as it fits into the perf events
    rules, being called only when we overflow.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 84bd28a0ffab..d2f2667430da 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -565,10 +565,13 @@ struct perf_pending_entry {
 	void (*func)(struct perf_pending_entry *);
 };
 
-typedef void (*perf_callback_t)(struct perf_event *, void *);
-
 struct perf_sample_data;
 
+typedef void (*perf_callback_t)(struct perf_event *, void *);
+typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
+					struct perf_sample_data *,
+					struct pt_regs *regs);
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -660,9 +663,7 @@ struct perf_event {
 	struct pid_namespace		*ns;
 	u64				id;
 
-	void (*overflow_handler)(struct perf_event *event,
-			int nmi, struct perf_sample_data *data,
-			struct pt_regs *regs);
+	perf_overflow_handler_t		overflow_handler;
 
 #ifdef CONFIG_EVENT_PROFILE
 	struct event_filter		*filter;
@@ -779,7 +780,7 @@ extern struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				int cpu,
 				pid_t pid,
-				perf_callback_t callback);
+				perf_overflow_handler_t callback);
 extern u64 perf_event_read_value(struct perf_event *event,
 				 u64 *enabled, u64 *running);
 

commit 9cef30815b0f5b76e94a58d7674fcbf824d95579
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 3 23:59:31 2009 +0100

    perf: Remove unused struct perf_event::event_callback
    
    This field might result from an older manual rebasing mistake.
    We don't use it.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 53230e99e9ea..84bd28a0ffab 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -670,8 +670,6 @@ struct perf_event {
 
 	perf_callback_t			callback;
 
-	perf_callback_t			event_callback;
-
 #endif /* CONFIG_PERF_EVENTS */
 };
 

commit 189f202ed197dc25d627e8660de27ece325e9f68
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 3 23:16:07 2009 +0100

    perf: Remove pointless union that wraps the hw breakpoint fields
    
    It stands to anonymize a structure, but structures can already
    anonymize by themselves.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a61e4de3448b..53230e99e9ea 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -215,14 +215,12 @@ struct perf_event_attr {
 		__u32		wakeup_watermark; /* bytes before wakeup   */
 	};
 
-	union {
-		struct { /* Hardware breakpoint info */
-			__u64		bp_addr;
-			__u32		bp_type;
-			__u32		bp_len;
-			__u64		__bp_reserved_1;
-			__u64		__bp_reserved_2;
-		};
+	struct { /* Hardware breakpoint info */
+		__u64		bp_addr;
+		__u32		bp_type;
+		__u32		bp_len;
+		__u64		__bp_reserved_1;
+		__u64		__bp_reserved_2;
 	};
 
 	__u32			__reserved_2;

commit ed54d0f98000ee03310150aa396e9ff8bcb394ce
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 3 22:08:26 2009 +0100

    hw-breakpoints: Add two reserved fields for future extensions
    
    Add two reserved fields for future extensions in the hardware
    breakpoints interface. Further needs may arise.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "K. Prasad" <prasad@linux.vnet.ibm.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 43adbd7f0010..a61e4de3448b 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -220,6 +220,8 @@ struct perf_event_attr {
 			__u64		bp_addr;
 			__u32		bp_type;
 			__u32		bp_len;
+			__u64		__bp_reserved_1;
+			__u64		__bp_reserved_2;
 		};
 	};
 

commit 4ed7c92d68a5387ba5f7030dc76eab03558e27f5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 23 11:37:29 2009 +0100

    perf_events: Undo some recursion damage
    
    Make perf_swevent_get_recursion_context return a context number
    and disable preemption.
    
    This could be used to remove the IRQ disable from the trace bit
    and index the per-cpu buffer with.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20091123103819.993226816@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 74e98b1d3391..43adbd7f0010 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -874,8 +874,8 @@ extern int perf_output_begin(struct perf_output_handle *handle,
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
-extern int perf_swevent_get_recursion_context(int **recursion);
-extern void perf_swevent_put_recursion_context(int *recursion);
+extern int perf_swevent_get_recursion_context(void);
+extern void perf_swevent_put_recursion_context(int rctx);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *task, int cpu)		{ }
@@ -904,8 +904,8 @@ static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
-static int perf_swevent_get_recursion_context(int **recursion)	{ return -1; }
-static void perf_swevent_put_recursion_context(int *recursion)		{ }
+static inline int  perf_swevent_get_recursion_context(void)  { return -1; }
+static inline void perf_swevent_put_recursion_context(int rctx)		{ }
 
 #endif
 

commit ce71b9df8893ec954e56c5979df6da274f20f65e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 22 05:26:55 2009 +0100

    tracing: Use the perf recursion protection from trace event
    
    When we commit a trace to perf, we first check if we are
    recursing in the same buffer so that we don't mess-up the buffer
    with a recursing trace. But later on, we do the same check from
    perf to avoid commit recursion. The recursion check is desired
    early before we touch the buffer but we want to do this check
    only once.
    
    Then export the recursion protection from perf and use it from
    the trace events before submitting a trace.
    
    v2: Put appropriate Reported-by tag
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <1258864015-10579-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 36fe89f72641..74e98b1d3391 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -874,6 +874,8 @@ extern int perf_output_begin(struct perf_output_handle *handle,
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
+extern int perf_swevent_get_recursion_context(int **recursion);
+extern void perf_swevent_put_recursion_context(int *recursion);
 #else
 static inline void
 perf_event_task_sched_in(struct task_struct *task, int cpu)		{ }
@@ -902,6 +904,8 @@ static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }
+static int perf_swevent_get_recursion_context(int **recursion)	{ return -1; }
+static void perf_swevent_put_recursion_context(int *recursion)		{ }
 
 #endif
 

commit 59ed446f792cc07d37b1536b9c4664d14e25e425
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 20 22:19:55 2009 +0100

    perf: Fix event scaling for inherited counters
    
    Properly account the full hierarchy of counters for both the
    count (we already did so) and the scale times (new).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20091120212509.153379276@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index a430ac3074af..36fe89f72641 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -782,7 +782,8 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				int cpu,
 				pid_t pid,
 				perf_callback_t callback);
-extern u64 perf_event_read_value(struct perf_event *event);
+extern u64 perf_event_read_value(struct perf_event *event,
+				 u64 *enabled, u64 *running);
 
 struct perf_sample_data {
 	u64				type;

commit 453f19eea7dbad837425e9b07d84568d14898794
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 20 22:19:43 2009 +0100

    perf: Allow for custom overflow handlers
    
    in-kernel perf users might wish to have custom actions on the
    sample interrupt.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <20091120212508.222339539@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index b5cdac0de370..a430ac3074af 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -567,6 +567,8 @@ struct perf_pending_entry {
 
 typedef void (*perf_callback_t)(struct perf_event *, void *);
 
+struct perf_sample_data;
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -658,6 +660,10 @@ struct perf_event {
 	struct pid_namespace		*ns;
 	u64				id;
 
+	void (*overflow_handler)(struct perf_event *event,
+			int nmi, struct perf_sample_data *data,
+			struct pt_regs *regs);
+
 #ifdef CONFIG_EVENT_PROFILE
 	struct event_filter		*filter;
 #endif

commit 96200591a34f8ecb98481c626125df43a2463b55
Merge: 7031281e02bf 68efa37df779
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 21 14:07:23 2009 +0100

    Merge branch 'tracing/hw-breakpoints' into perf/core
    
    Conflicts:
            arch/x86/kernel/kprobes.c
            kernel/trace/Makefile
    
    Merge reason: hw-breakpoints perf integration is looking
                  good in testing and in reviews, plus conflicts
                  are mounting up - so merge & resolve.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 559fdc3c1b624edb1933a875022fe7e27934d11c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 16 12:45:14 2009 +0100

    perf_event: Optimize perf_output_lock()
    
    The purpose of perf_output_{un,}lock() is to:
    
     1) avoid publishing incomplete data
        [ possible when publishing a head that is ahead of an entry
          that is still being written ]
    
     2) guarantee fwd progress
        [ a simple refcount on pending writers doesn't need to drop to
          0, making it so would end up implementing something like forced
          quiecent states of RCU ]
    
    To satisfy the above without undue complexity it serializes
    between CPUs, this means that a pending writer can only be the
    same cpu in a nested context, and since (under normal operation)
    a cpu always makes progress we're good -- if the head is only
    published when the bottom  most writer completes.
    
    Now we don't need to disable IRQs in order to serialize between
    CPUs, disabling preemption ought to be sufficient, esp since we
    already deal with nesting due to NMIs.
    
    This avoids potentially expensive (and needless) local IRQ
    disable/enable ops.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1258373161.26714.254.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index df4e73e33774..7f87563c8485 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -714,7 +714,6 @@ struct perf_output_handle {
 	int				nmi;
 	int				sample;
 	int				locked;
-	unsigned long			flags;
 };
 
 #ifdef CONFIG_PERF_EVENTS

commit 0ffa798d947f5f5e40690cc9d38e678080a34f87
Merge: 39dc78b65103 c86e2eaded39 c5659b74f052
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 15 09:51:19 2009 +0100

    Merge branches 'perf/powerpc' and 'perf/bench' into perf/core
    
    Merge reason: Both 'perf bench' and the pending PowerPC changes
                  are now ready for the next merge window.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4c49b12853fbb5eff4849b7b6a1e895776f027a1
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Fri Nov 13 21:47:33 2009 -0800

    perf_event: Fix invalid type in ioctl definition
    
    u64 is invalid in userspace headers, including ioctl
    definitions; use __u64 instead
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: <stable@kernel.org>
    LKML-Reference: <20091113214733.7cd76be9@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 45b56faf5cdc..ec3768a81058 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -219,7 +219,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_DISABLE		_IO ('$', 1)
 #define PERF_EVENT_IOC_REFRESH		_IO ('$', 2)
 #define PERF_EVENT_IOC_RESET		_IO ('$', 3)
-#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, u64)
+#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
 #define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
 

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 8d54e6d25eeb..cead64ea6c15 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -18,6 +18,10 @@
 #include <linux/ioctl.h>
 #include <asm/byteorder.h>
 
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+#include <asm/hw_breakpoint.h>
+#endif
+
 /*
  * User-space ABI bits:
  */
@@ -31,6 +35,7 @@ enum perf_type_id {
 	PERF_TYPE_TRACEPOINT			= 2,
 	PERF_TYPE_HW_CACHE			= 3,
 	PERF_TYPE_RAW				= 4,
+	PERF_TYPE_BREAKPOINT			= 5,
 
 	PERF_TYPE_MAX,				/* non-ABI */
 };
@@ -207,6 +212,15 @@ struct perf_event_attr {
 		__u32		wakeup_events;	  /* wakeup every n events */
 		__u32		wakeup_watermark; /* bytes before wakeup   */
 	};
+
+	union {
+		struct { /* Hardware breakpoint info */
+			__u64		bp_addr;
+			__u32		bp_type;
+			__u32		bp_len;
+		};
+	};
+
 	__u32			__reserved_2;
 
 	__u64			__reserved_3;
@@ -476,6 +490,11 @@ struct hw_perf_event {
 			atomic64_t	count;
 			struct hrtimer	hrtimer;
 		};
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+		union { /* breakpoint */
+			struct arch_hw_breakpoint	info;
+		};
+#endif
 	};
 	atomic64_t			prev_count;
 	u64				sample_period;
@@ -588,7 +607,7 @@ struct perf_event {
 	u64				tstamp_running;
 	u64				tstamp_stopped;
 
-	struct perf_event_attr	attr;
+	struct perf_event_attr		attr;
 	struct hw_perf_event		hw;
 
 	struct perf_event_context	*ctx;
@@ -643,6 +662,8 @@ struct perf_event {
 
 	perf_callback_t			callback;
 
+	perf_callback_t			event_callback;
+
 #endif /* CONFIG_PERF_EVENTS */
 };
 
@@ -831,6 +852,7 @@ extern int sysctl_perf_event_sample_rate;
 extern void perf_event_init(void);
 extern void perf_tp_event(int event_id, u64 addr, u64 count,
 				 void *record, int entry_size);
+extern void perf_bp_event(struct perf_event *event, void *data);
 
 #ifndef perf_misc_flags
 #define perf_misc_flags(regs)	(user_mode(regs) ? PERF_RECORD_MISC_USER : \
@@ -865,6 +887,8 @@ static inline int perf_event_task_enable(void)				{ return -EINVAL; }
 static inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi,
 		     struct pt_regs *regs, u64 addr)			{ }
+static inline void
+perf_bp_event(struct perf_event *event, void *data)		{ }
 
 static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }

commit a2e71271535fde493c32803b1f34789f97efcb5e
Merge: 6d7aa9d721c8 b419148e5677
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 4 11:54:15 2009 +0100

    Merge commit 'v2.6.32-rc6' into perf/core
    
    Conflicts:
            tools/perf/Makefile
    
    Merge reason: Resolve the conflict, merge to upstream and merge in
                  perf fixes so we can add a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 97eaf5300b9d0cd99c310bf8c4a0f2f3296d88a3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 15:33:50 2009 +0200

    perf/core: Add a callback to perf events
    
    A simple callback in a perf event can be used for multiple purposes.
    For example it is useful for triggered based events like hardware
    breakpoints that need a callback to dispatch a triggered breakpoint
    event.
    
    v2: Simplify a bit the callback attribution as suggested by Paul
        Mackerras
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "K.Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index fa151d49a2ee..8d54e6d25eeb 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -544,6 +544,8 @@ struct perf_pending_entry {
 	void (*func)(struct perf_pending_entry *);
 };
 
+typedef void (*perf_callback_t)(struct perf_event *, void *);
+
 /**
  * struct perf_event - performance event kernel representation:
  */
@@ -639,6 +641,8 @@ struct perf_event {
 	struct event_filter		*filter;
 #endif
 
+	perf_callback_t			callback;
+
 #endif /* CONFIG_PERF_EVENTS */
 };
 
@@ -748,7 +752,8 @@ extern int perf_event_release_kernel(struct perf_event *event);
 extern struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr,
 				int cpu,
-				pid_t pid);
+				pid_t pid,
+				perf_callback_t callback);
 extern u64 perf_event_read_value(struct perf_event *event);
 
 struct perf_sample_data {

commit fb0459d75c1d0a4ba3cafdd2c754e7486968a676
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Fri Sep 25 12:25:56 2009 +0200

    perf/core: Provide a kernel-internal interface to get to performance counters
    
    There are reasons for kernel code to ask for, and use, performance
    counters.
    For example, in CPU freq governors this tends to be a good idea, but
    there are other examples possible as well of course.
    
    This patch adds the needed bits to do enable this functionality; they
    have been tested in an experimental cpufreq driver that I'm working on,
    and the changes are all that I needed to access counters properly.
    
    [fweisbec@gmail.com: added pid to perf_event_create_kernel_counter so
    that we can profile a particular task too
    
    TODO: Have a better error reporting, don't just return NULL in fail
    case.]
    
    v2: Remove the wrong comment about the fact
        perf_event_create_kernel_counter must be called from a kernel
        thread.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: "K.Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Avi Kivity <avi@redhat.com>
    LKML-Reference: <20090925122556.2f8bd939@infradead.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index df9d964c15fc..fa151d49a2ee 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -744,6 +744,12 @@ extern int hw_perf_group_sched_in(struct perf_event *group_leader,
 	       struct perf_cpu_context *cpuctx,
 	       struct perf_event_context *ctx, int cpu);
 extern void perf_event_update_userpage(struct perf_event *event);
+extern int perf_event_release_kernel(struct perf_event *event);
+extern struct perf_event *
+perf_event_create_kernel_counter(struct perf_event_attr *attr,
+				int cpu,
+				pid_t pid);
+extern u64 perf_event_read_value(struct perf_event *event);
 
 struct perf_sample_data {
 	u64				type;

commit f7d7986060b2890fc26db6ab5203efbd33aa2497
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Oct 18 01:09:29 2009 +0000

    perf_event: Add alignment-faults and emulation-faults software events
    
    Add two more software events that are common to many cpus.
    
    Alignment faults: When a load or store is not aligned properly.
    
    Emulation faults: When an instruction is emulated in software.
    
    Both cause a very significant slowdown (100x or worse), so identifying and
    fixing them is very important.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2e6d95f97419..a33707a3a788 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -102,6 +102,8 @@ enum perf_sw_ids {
 	PERF_COUNT_SW_CPU_MIGRATIONS		= 4,
 	PERF_COUNT_SW_PAGE_FAULTS_MIN		= 5,
 	PERF_COUNT_SW_PAGE_FAULTS_MAJ		= 6,
+	PERF_COUNT_SW_ALIGNMENT_FAULTS		= 7,
+	PERF_COUNT_SW_EMULATION_FAULTS		= 8,
 
 	PERF_COUNT_SW_MAX,			/* non-ABI */
 };

commit 721a669b7225edeeb0ca8e2bf71b83882326a71b
Author: Soeren Sandmann <sandmann@daimi.au.dk>
Date:   Tue Sep 15 14:33:08 2009 +0200

    perf events: Fix swevent hrtimer sampling by keeping track of remaining time when enabling/disabling swevent hrtimers
    
    Make the hrtimer based events work for sysprof.
    
    Whenever a swevent is scheduled out, the hrtimer is canceled.
    When it is scheduled back in, the timer is restarted. This
    happens every scheduler tick, which means the timer never
    expired because it was getting repeatedly restarted over and
    over with the same period.
    
    To fix that, save the remaining time when disabling; when
    reenabling, use that saved time as the period instead of the
    user-specified sampling period.
    
    Also, move the starting and stopping of the hrtimers to helper
    functions instead of duplicating the code.
    
    Signed-off-by: Søren Sandmann Pedersen <sandmann@redhat.com>
    LKML-Reference: <ye8vdi7mluz.fsf@camel16.daimi.au.dk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2e6d95f97419..9e7012689a84 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -471,8 +471,8 @@ struct hw_perf_event {
 			unsigned long	event_base;
 			int		idx;
 		};
-		union { /* software */
-			atomic64_t	count;
+		struct { /* software */
+			s64		remaining;
 			struct hrtimer	hrtimer;
 		};
 	};

commit 6fb2915df7f0747d9044da9dbff5b46dc2e20830
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Oct 15 11:21:42 2009 +0800

    tracing/profile: Add filter support
    
    - Add an ioctl to allocate a filter for a perf event.
    
    - Free the filter when the associated perf event is to be freed.
    
    - Do the filtering in perf_swevent_match().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4AD69546.8050401@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 2e6d95f97419..df9d964c15fc 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -221,6 +221,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_RESET		_IO ('$', 3)
 #define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, u64)
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
+#define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,
@@ -633,7 +634,12 @@ struct perf_event {
 
 	struct pid_namespace		*ns;
 	u64				id;
+
+#ifdef CONFIG_EVENT_PROFILE
+	struct event_filter		*filter;
 #endif
+
+#endif /* CONFIG_PERF_EVENTS */
 };
 
 /**

commit 906010b2134e14a2e377decbadd357b3d0ab9c6a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 21 16:08:49 2009 +0200

    perf_event: Provide vmalloc() based mmap() backing
    
    Some architectures such as Sparc, ARM and MIPS (basically
    everything with flush_dcache_page()) need to deal with dcache
    aliases by carefully placing pages in both kernel and user maps.
    
    These architectures typically have to use vmalloc_user() for this.
    
    However, on other architectures, vmalloc() is not needed and has
    the downsides of being more restricted and slower than regular
    allocations.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1254830228.21044.272.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 3a9d36d1e92a..2e6d95f97419 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -442,6 +442,7 @@ enum perf_callchain_context {
 #include <linux/hrtimer.h>
 #include <linux/fs.h>
 #include <linux/pid_namespace.h>
+#include <linux/workqueue.h>
 #include <asm/atomic.h>
 
 #define PERF_MAX_STACK_DEPTH		255
@@ -513,6 +514,10 @@ struct file;
 
 struct perf_mmap_data {
 	struct rcu_head			rcu_head;
+#ifdef CONFIG_PERF_USE_VMALLOC
+	struct work_struct		work;
+#endif
+	int				data_order;
 	int				nr_pages;	/* nr of data pages  */
 	int				writable;	/* are we writable   */
 	int				nr_locked;	/* nr pages mlocked  */

commit a6f10a2f5d8c2738b3ac05974bdbea3b68a2aecd
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Sep 22 22:34:24 2009 +1000

    perf_event: Update PERF_EVENT_FORK header definition
    
    PERF_EVENT_FORK always outputs the time field, so update the header
    to reflect this.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20090922123424.GD19453@kryten>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index acefaf71e6dd..3a9d36d1e92a 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -357,7 +357,7 @@ enum perf_event_type {
 	 *	struct perf_event_header	header;
 	 *	u32				pid, ppid;
 	 *	u32				tid, ptid;
-	 *	{ u64				time;     } && PERF_SAMPLE_TIME
+	 *	u64				time;
 	 * };
 	 */
 	PERF_RECORD_FORK			= 7,

commit 57c0c15b5244320065374ad2c54f4fbec77a6428
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:20:38 2009 +0200

    perf: Tidy up after the big rename
    
     - provide compatibility Kconfig entry for existing PERF_COUNTERS .config's
    
     - provide courtesy copy of old perf_counter.h, for user-space projects
    
     - small indentation fixups
    
     - fix up MAINTAINERS
    
     - fix small x86 printout fallout
    
     - fix up small PowerPC comment fallout (use 'counter' as in register)
    
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ae9d9ed6df2a..acefaf71e6dd 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1,15 +1,15 @@
 /*
- *  Performance events:
+ * Performance events:
  *
  *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>
  *    Copyright (C) 2008-2009, Red Hat, Inc., Ingo Molnar
  *    Copyright (C) 2008-2009, Red Hat, Inc., Peter Zijlstra
  *
- *  Data type definitions, declarations, prototypes.
+ * Data type definitions, declarations, prototypes.
  *
  *    Started by: Thomas Gleixner and Ingo Molnar
  *
- *  For licencing details see kernel-base/COPYING
+ * For licencing details see kernel-base/COPYING
  */
 #ifndef _LINUX_PERF_EVENT_H
 #define _LINUX_PERF_EVENT_H
@@ -131,19 +131,19 @@ enum perf_event_sample_format {
  * as specified by attr.read_format:
  *
  * struct read_format {
- * 	{ u64		value;
- * 	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
- * 	  { u64		time_running; } && PERF_FORMAT_RUNNING
- * 	  { u64		id;           } && PERF_FORMAT_ID
- * 	} && !PERF_FORMAT_GROUP
+ *	{ u64		value;
+ *	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ *	  { u64		id;           } && PERF_FORMAT_ID
+ *	} && !PERF_FORMAT_GROUP
  *
- * 	{ u64		nr;
- * 	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
- * 	  { u64		time_running; } && PERF_FORMAT_RUNNING
- * 	  { u64		value;
- * 	    { u64	id;           } && PERF_FORMAT_ID
- * 	  }		cntr[nr];
- * 	} && PERF_FORMAT_GROUP
+ *	{ u64		nr;
+ *	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ *	  { u64		value;
+ *	    { u64	id;           } && PERF_FORMAT_ID
+ *	  }		cntr[nr];
+ *	} && PERF_FORMAT_GROUP
  * };
  */
 enum perf_event_read_format {
@@ -152,7 +152,7 @@ enum perf_event_read_format {
 	PERF_FORMAT_ID				= 1U << 2,
 	PERF_FORMAT_GROUP			= 1U << 3,
 
-	PERF_FORMAT_MAX = 1U << 4, 		/* non-ABI */
+	PERF_FORMAT_MAX = 1U << 4,		/* non-ABI */
 };
 
 #define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
@@ -216,8 +216,8 @@ struct perf_event_attr {
  * Ioctls that can be done on a perf event fd:
  */
 #define PERF_EVENT_IOC_ENABLE		_IO ('$', 0)
-#define PERF_EVENT_IOC_DISABLE	_IO ('$', 1)
-#define PERF_EVENT_IOC_REFRESH	_IO ('$', 2)
+#define PERF_EVENT_IOC_DISABLE		_IO ('$', 1)
+#define PERF_EVENT_IOC_REFRESH		_IO ('$', 2)
 #define PERF_EVENT_IOC_RESET		_IO ('$', 3)
 #define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, u64)
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
@@ -314,9 +314,9 @@ enum perf_event_type {
 
 	/*
 	 * struct {
-	 * 	struct perf_event_header	header;
-	 * 	u64				id;
-	 * 	u64				lost;
+	 *	struct perf_event_header	header;
+	 *	u64				id;
+	 *	u64				lost;
 	 * };
 	 */
 	PERF_RECORD_LOST			= 2,
@@ -383,23 +383,23 @@ enum perf_event_type {
 	 *	{ u64			id;	  } && PERF_SAMPLE_ID
 	 *	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
 	 *	{ u32			cpu, res; } && PERF_SAMPLE_CPU
-	 * 	{ u64			period;   } && PERF_SAMPLE_PERIOD
+	 *	{ u64			period;   } && PERF_SAMPLE_PERIOD
 	 *
 	 *	{ struct read_format	values;	  } && PERF_SAMPLE_READ
 	 *
 	 *	{ u64			nr,
 	 *	  u64			ips[nr];  } && PERF_SAMPLE_CALLCHAIN
 	 *
-	 * 	#
-	 * 	# The RAW record below is opaque data wrt the ABI
-	 * 	#
-	 * 	# That is, the ABI doesn't make any promises wrt to
-	 * 	# the stability of its content, it may vary depending
-	 * 	# on event_id, hardware, kernel version and phase of
-	 * 	# the moon.
-	 * 	#
-	 * 	# In other words, PERF_SAMPLE_RAW contents are not an ABI.
-	 * 	#
+	 *	#
+	 *	# The RAW record below is opaque data wrt the ABI
+	 *	#
+	 *	# That is, the ABI doesn't make any promises wrt to
+	 *	# the stability of its content, it may vary depending
+	 *	# on event, hardware, kernel version and phase of
+	 *	# the moon.
+	 *	#
+	 *	# In other words, PERF_SAMPLE_RAW contents are not an ABI.
+	 *	#
 	 *
 	 *	{ u32			size;
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
@@ -503,10 +503,10 @@ struct pmu {
  * enum perf_event_active_state - the states of a event
  */
 enum perf_event_active_state {
-	PERF_EVENT_STATE_ERROR	= -2,
+	PERF_EVENT_STATE_ERROR		= -2,
 	PERF_EVENT_STATE_OFF		= -1,
 	PERF_EVENT_STATE_INACTIVE	=  0,
-	PERF_EVENT_STATE_ACTIVE	=  1,
+	PERF_EVENT_STATE_ACTIVE		=  1,
 };
 
 struct file;
@@ -529,7 +529,7 @@ struct perf_mmap_data {
 
 	long				watermark;	/* wakeup watermark  */
 
-	struct perf_event_mmap_page   *user_page;
+	struct perf_event_mmap_page	*user_page;
 	void				*data_pages[0];
 };
 
@@ -694,14 +694,14 @@ struct perf_cpu_context {
 };
 
 struct perf_output_handle {
-	struct perf_event	*event;
-	struct perf_mmap_data	*data;
-	unsigned long		head;
-	unsigned long		offset;
-	int			nmi;
-	int			sample;
-	int			locked;
-	unsigned long		flags;
+	struct perf_event		*event;
+	struct perf_mmap_data		*data;
+	unsigned long			head;
+	unsigned long			offset;
+	int				nmi;
+	int				sample;
+	int				locked;
+	unsigned long			flags;
 };
 
 #ifdef CONFIG_PERF_EVENTS
@@ -829,22 +829,22 @@ static inline void
 perf_event_task_sched_out(struct task_struct *task,
 			    struct task_struct *next, int cpu)		{ }
 static inline void
-perf_event_task_tick(struct task_struct *task, int cpu)		{ }
+perf_event_task_tick(struct task_struct *task, int cpu)			{ }
 static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
 static inline void perf_event_exit_task(struct task_struct *child)	{ }
 static inline void perf_event_free_task(struct task_struct *task)	{ }
-static inline void perf_event_do_pending(void)			{ }
-static inline void perf_event_print_debug(void)			{ }
+static inline void perf_event_do_pending(void)				{ }
+static inline void perf_event_print_debug(void)				{ }
 static inline void perf_disable(void)					{ }
 static inline void perf_enable(void)					{ }
-static inline int perf_event_task_disable(void)	{ return -EINVAL; }
-static inline int perf_event_task_enable(void)	{ return -EINVAL; }
+static inline int perf_event_task_disable(void)				{ return -EINVAL; }
+static inline int perf_event_task_enable(void)				{ return -EINVAL; }
 
 static inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi,
 		     struct pt_regs *regs, u64 addr)			{ }
 
-static inline void perf_event_mmap(struct vm_area_struct *vma)	{ }
+static inline void perf_event_mmap(struct vm_area_struct *vma)		{ }
 static inline void perf_event_comm(struct task_struct *tsk)		{ }
 static inline void perf_event_fork(struct task_struct *tsk)		{ }
 static inline void perf_event_init(void)				{ }

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
new file mode 100644
index 000000000000..ae9d9ed6df2a
--- /dev/null
+++ b/include/linux/perf_event.h
@@ -0,0 +1,858 @@
+/*
+ *  Performance events:
+ *
+ *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>
+ *    Copyright (C) 2008-2009, Red Hat, Inc., Ingo Molnar
+ *    Copyright (C) 2008-2009, Red Hat, Inc., Peter Zijlstra
+ *
+ *  Data type definitions, declarations, prototypes.
+ *
+ *    Started by: Thomas Gleixner and Ingo Molnar
+ *
+ *  For licencing details see kernel-base/COPYING
+ */
+#ifndef _LINUX_PERF_EVENT_H
+#define _LINUX_PERF_EVENT_H
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+#include <asm/byteorder.h>
+
+/*
+ * User-space ABI bits:
+ */
+
+/*
+ * attr.type
+ */
+enum perf_type_id {
+	PERF_TYPE_HARDWARE			= 0,
+	PERF_TYPE_SOFTWARE			= 1,
+	PERF_TYPE_TRACEPOINT			= 2,
+	PERF_TYPE_HW_CACHE			= 3,
+	PERF_TYPE_RAW				= 4,
+
+	PERF_TYPE_MAX,				/* non-ABI */
+};
+
+/*
+ * Generalized performance event event_id types, used by the
+ * attr.event_id parameter of the sys_perf_event_open()
+ * syscall:
+ */
+enum perf_hw_id {
+	/*
+	 * Common hardware events, generalized by the kernel:
+	 */
+	PERF_COUNT_HW_CPU_CYCLES		= 0,
+	PERF_COUNT_HW_INSTRUCTIONS		= 1,
+	PERF_COUNT_HW_CACHE_REFERENCES		= 2,
+	PERF_COUNT_HW_CACHE_MISSES		= 3,
+	PERF_COUNT_HW_BRANCH_INSTRUCTIONS	= 4,
+	PERF_COUNT_HW_BRANCH_MISSES		= 5,
+	PERF_COUNT_HW_BUS_CYCLES		= 6,
+
+	PERF_COUNT_HW_MAX,			/* non-ABI */
+};
+
+/*
+ * Generalized hardware cache events:
+ *
+ *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU } x
+ *       { read, write, prefetch } x
+ *       { accesses, misses }
+ */
+enum perf_hw_cache_id {
+	PERF_COUNT_HW_CACHE_L1D			= 0,
+	PERF_COUNT_HW_CACHE_L1I			= 1,
+	PERF_COUNT_HW_CACHE_LL			= 2,
+	PERF_COUNT_HW_CACHE_DTLB		= 3,
+	PERF_COUNT_HW_CACHE_ITLB		= 4,
+	PERF_COUNT_HW_CACHE_BPU			= 5,
+
+	PERF_COUNT_HW_CACHE_MAX,		/* non-ABI */
+};
+
+enum perf_hw_cache_op_id {
+	PERF_COUNT_HW_CACHE_OP_READ		= 0,
+	PERF_COUNT_HW_CACHE_OP_WRITE		= 1,
+	PERF_COUNT_HW_CACHE_OP_PREFETCH		= 2,
+
+	PERF_COUNT_HW_CACHE_OP_MAX,		/* non-ABI */
+};
+
+enum perf_hw_cache_op_result_id {
+	PERF_COUNT_HW_CACHE_RESULT_ACCESS	= 0,
+	PERF_COUNT_HW_CACHE_RESULT_MISS		= 1,
+
+	PERF_COUNT_HW_CACHE_RESULT_MAX,		/* non-ABI */
+};
+
+/*
+ * Special "software" events provided by the kernel, even if the hardware
+ * does not support performance events. These events measure various
+ * physical and sw events of the kernel (and allow the profiling of them as
+ * well):
+ */
+enum perf_sw_ids {
+	PERF_COUNT_SW_CPU_CLOCK			= 0,
+	PERF_COUNT_SW_TASK_CLOCK		= 1,
+	PERF_COUNT_SW_PAGE_FAULTS		= 2,
+	PERF_COUNT_SW_CONTEXT_SWITCHES		= 3,
+	PERF_COUNT_SW_CPU_MIGRATIONS		= 4,
+	PERF_COUNT_SW_PAGE_FAULTS_MIN		= 5,
+	PERF_COUNT_SW_PAGE_FAULTS_MAJ		= 6,
+
+	PERF_COUNT_SW_MAX,			/* non-ABI */
+};
+
+/*
+ * Bits that can be set in attr.sample_type to request information
+ * in the overflow packets.
+ */
+enum perf_event_sample_format {
+	PERF_SAMPLE_IP				= 1U << 0,
+	PERF_SAMPLE_TID				= 1U << 1,
+	PERF_SAMPLE_TIME			= 1U << 2,
+	PERF_SAMPLE_ADDR			= 1U << 3,
+	PERF_SAMPLE_READ			= 1U << 4,
+	PERF_SAMPLE_CALLCHAIN			= 1U << 5,
+	PERF_SAMPLE_ID				= 1U << 6,
+	PERF_SAMPLE_CPU				= 1U << 7,
+	PERF_SAMPLE_PERIOD			= 1U << 8,
+	PERF_SAMPLE_STREAM_ID			= 1U << 9,
+	PERF_SAMPLE_RAW				= 1U << 10,
+
+	PERF_SAMPLE_MAX = 1U << 11,		/* non-ABI */
+};
+
+/*
+ * The format of the data returned by read() on a perf event fd,
+ * as specified by attr.read_format:
+ *
+ * struct read_format {
+ * 	{ u64		value;
+ * 	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
+ * 	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ * 	  { u64		id;           } && PERF_FORMAT_ID
+ * 	} && !PERF_FORMAT_GROUP
+ *
+ * 	{ u64		nr;
+ * 	  { u64		time_enabled; } && PERF_FORMAT_ENABLED
+ * 	  { u64		time_running; } && PERF_FORMAT_RUNNING
+ * 	  { u64		value;
+ * 	    { u64	id;           } && PERF_FORMAT_ID
+ * 	  }		cntr[nr];
+ * 	} && PERF_FORMAT_GROUP
+ * };
+ */
+enum perf_event_read_format {
+	PERF_FORMAT_TOTAL_TIME_ENABLED		= 1U << 0,
+	PERF_FORMAT_TOTAL_TIME_RUNNING		= 1U << 1,
+	PERF_FORMAT_ID				= 1U << 2,
+	PERF_FORMAT_GROUP			= 1U << 3,
+
+	PERF_FORMAT_MAX = 1U << 4, 		/* non-ABI */
+};
+
+#define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
+
+/*
+ * Hardware event_id to monitor via a performance monitoring event:
+ */
+struct perf_event_attr {
+
+	/*
+	 * Major type: hardware/software/tracepoint/etc.
+	 */
+	__u32			type;
+
+	/*
+	 * Size of the attr structure, for fwd/bwd compat.
+	 */
+	__u32			size;
+
+	/*
+	 * Type specific configuration information.
+	 */
+	__u64			config;
+
+	union {
+		__u64		sample_period;
+		__u64		sample_freq;
+	};
+
+	__u64			sample_type;
+	__u64			read_format;
+
+	__u64			disabled       :  1, /* off by default        */
+				inherit	       :  1, /* children inherit it   */
+				pinned	       :  1, /* must always be on PMU */
+				exclusive      :  1, /* only group on PMU     */
+				exclude_user   :  1, /* don't count user      */
+				exclude_kernel :  1, /* ditto kernel          */
+				exclude_hv     :  1, /* ditto hypervisor      */
+				exclude_idle   :  1, /* don't count when idle */
+				mmap           :  1, /* include mmap data     */
+				comm	       :  1, /* include comm data     */
+				freq           :  1, /* use freq, not period  */
+				inherit_stat   :  1, /* per task counts       */
+				enable_on_exec :  1, /* next exec enables     */
+				task           :  1, /* trace fork/exit       */
+				watermark      :  1, /* wakeup_watermark      */
+
+				__reserved_1   : 49;
+
+	union {
+		__u32		wakeup_events;	  /* wakeup every n events */
+		__u32		wakeup_watermark; /* bytes before wakeup   */
+	};
+	__u32			__reserved_2;
+
+	__u64			__reserved_3;
+};
+
+/*
+ * Ioctls that can be done on a perf event fd:
+ */
+#define PERF_EVENT_IOC_ENABLE		_IO ('$', 0)
+#define PERF_EVENT_IOC_DISABLE	_IO ('$', 1)
+#define PERF_EVENT_IOC_REFRESH	_IO ('$', 2)
+#define PERF_EVENT_IOC_RESET		_IO ('$', 3)
+#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, u64)
+#define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
+
+enum perf_event_ioc_flags {
+	PERF_IOC_FLAG_GROUP		= 1U << 0,
+};
+
+/*
+ * Structure of the page that can be mapped via mmap
+ */
+struct perf_event_mmap_page {
+	__u32	version;		/* version number of this structure */
+	__u32	compat_version;		/* lowest version this is compat with */
+
+	/*
+	 * Bits needed to read the hw events in user-space.
+	 *
+	 *   u32 seq;
+	 *   s64 count;
+	 *
+	 *   do {
+	 *     seq = pc->lock;
+	 *
+	 *     barrier()
+	 *     if (pc->index) {
+	 *       count = pmc_read(pc->index - 1);
+	 *       count += pc->offset;
+	 *     } else
+	 *       goto regular_read;
+	 *
+	 *     barrier();
+	 *   } while (pc->lock != seq);
+	 *
+	 * NOTE: for obvious reason this only works on self-monitoring
+	 *       processes.
+	 */
+	__u32	lock;			/* seqlock for synchronization */
+	__u32	index;			/* hardware event identifier */
+	__s64	offset;			/* add to hardware event value */
+	__u64	time_enabled;		/* time event active */
+	__u64	time_running;		/* time event on cpu */
+
+		/*
+		 * Hole for extension of the self monitor capabilities
+		 */
+
+	__u64	__reserved[123];	/* align to 1k */
+
+	/*
+	 * Control data for the mmap() data buffer.
+	 *
+	 * User-space reading the @data_head value should issue an rmb(), on
+	 * SMP capable platforms, after reading this value -- see
+	 * perf_event_wakeup().
+	 *
+	 * When the mapping is PROT_WRITE the @data_tail value should be
+	 * written by userspace to reflect the last read data. In this case
+	 * the kernel will not over-write unread data.
+	 */
+	__u64   data_head;		/* head in the data section */
+	__u64	data_tail;		/* user-space written tail */
+};
+
+#define PERF_RECORD_MISC_CPUMODE_MASK		(3 << 0)
+#define PERF_RECORD_MISC_CPUMODE_UNKNOWN		(0 << 0)
+#define PERF_RECORD_MISC_KERNEL			(1 << 0)
+#define PERF_RECORD_MISC_USER			(2 << 0)
+#define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
+
+struct perf_event_header {
+	__u32	type;
+	__u16	misc;
+	__u16	size;
+};
+
+enum perf_event_type {
+
+	/*
+	 * The MMAP events record the PROT_EXEC mappings so that we can
+	 * correlate userspace IPs to code. They have the following structure:
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u32				pid, tid;
+	 *	u64				addr;
+	 *	u64				len;
+	 *	u64				pgoff;
+	 *	char				filename[];
+	 * };
+	 */
+	PERF_RECORD_MMAP			= 1,
+
+	/*
+	 * struct {
+	 * 	struct perf_event_header	header;
+	 * 	u64				id;
+	 * 	u64				lost;
+	 * };
+	 */
+	PERF_RECORD_LOST			= 2,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u32				pid, tid;
+	 *	char				comm[];
+	 * };
+	 */
+	PERF_RECORD_COMM			= 3,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid, ppid;
+	 *	u32				tid, ptid;
+	 *	u64				time;
+	 * };
+	 */
+	PERF_RECORD_EXIT			= 4,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u64				time;
+	 *	u64				id;
+	 *	u64				stream_id;
+	 * };
+	 */
+	PERF_RECORD_THROTTLE		= 5,
+	PERF_RECORD_UNTHROTTLE		= 6,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid, ppid;
+	 *	u32				tid, ptid;
+	 *	{ u64				time;     } && PERF_SAMPLE_TIME
+	 * };
+	 */
+	PERF_RECORD_FORK			= 7,
+
+	/*
+	 * struct {
+	 * 	struct perf_event_header	header;
+	 * 	u32				pid, tid;
+	 *
+	 * 	struct read_format		values;
+	 * };
+	 */
+	PERF_RECORD_READ			= 8,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	{ u64			ip;	  } && PERF_SAMPLE_IP
+	 *	{ u32			pid, tid; } && PERF_SAMPLE_TID
+	 *	{ u64			time;     } && PERF_SAMPLE_TIME
+	 *	{ u64			addr;     } && PERF_SAMPLE_ADDR
+	 *	{ u64			id;	  } && PERF_SAMPLE_ID
+	 *	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
+	 *	{ u32			cpu, res; } && PERF_SAMPLE_CPU
+	 * 	{ u64			period;   } && PERF_SAMPLE_PERIOD
+	 *
+	 *	{ struct read_format	values;	  } && PERF_SAMPLE_READ
+	 *
+	 *	{ u64			nr,
+	 *	  u64			ips[nr];  } && PERF_SAMPLE_CALLCHAIN
+	 *
+	 * 	#
+	 * 	# The RAW record below is opaque data wrt the ABI
+	 * 	#
+	 * 	# That is, the ABI doesn't make any promises wrt to
+	 * 	# the stability of its content, it may vary depending
+	 * 	# on event_id, hardware, kernel version and phase of
+	 * 	# the moon.
+	 * 	#
+	 * 	# In other words, PERF_SAMPLE_RAW contents are not an ABI.
+	 * 	#
+	 *
+	 *	{ u32			size;
+	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
+	 * };
+	 */
+	PERF_RECORD_SAMPLE		= 9,
+
+	PERF_RECORD_MAX,			/* non-ABI */
+};
+
+enum perf_callchain_context {
+	PERF_CONTEXT_HV			= (__u64)-32,
+	PERF_CONTEXT_KERNEL		= (__u64)-128,
+	PERF_CONTEXT_USER		= (__u64)-512,
+
+	PERF_CONTEXT_GUEST		= (__u64)-2048,
+	PERF_CONTEXT_GUEST_KERNEL	= (__u64)-2176,
+	PERF_CONTEXT_GUEST_USER		= (__u64)-2560,
+
+	PERF_CONTEXT_MAX		= (__u64)-4095,
+};
+
+#define PERF_FLAG_FD_NO_GROUP	(1U << 0)
+#define PERF_FLAG_FD_OUTPUT	(1U << 1)
+
+#ifdef __KERNEL__
+/*
+ * Kernel-internal data types and definitions:
+ */
+
+#ifdef CONFIG_PERF_EVENTS
+# include <asm/perf_event.h>
+#endif
+
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/rculist.h>
+#include <linux/rcupdate.h>
+#include <linux/spinlock.h>
+#include <linux/hrtimer.h>
+#include <linux/fs.h>
+#include <linux/pid_namespace.h>
+#include <asm/atomic.h>
+
+#define PERF_MAX_STACK_DEPTH		255
+
+struct perf_callchain_entry {
+	__u64				nr;
+	__u64				ip[PERF_MAX_STACK_DEPTH];
+};
+
+struct perf_raw_record {
+	u32				size;
+	void				*data;
+};
+
+struct task_struct;
+
+/**
+ * struct hw_perf_event - performance event hardware details:
+ */
+struct hw_perf_event {
+#ifdef CONFIG_PERF_EVENTS
+	union {
+		struct { /* hardware */
+			u64		config;
+			unsigned long	config_base;
+			unsigned long	event_base;
+			int		idx;
+		};
+		union { /* software */
+			atomic64_t	count;
+			struct hrtimer	hrtimer;
+		};
+	};
+	atomic64_t			prev_count;
+	u64				sample_period;
+	u64				last_period;
+	atomic64_t			period_left;
+	u64				interrupts;
+
+	u64				freq_count;
+	u64				freq_interrupts;
+	u64				freq_stamp;
+#endif
+};
+
+struct perf_event;
+
+/**
+ * struct pmu - generic performance monitoring unit
+ */
+struct pmu {
+	int (*enable)			(struct perf_event *event);
+	void (*disable)			(struct perf_event *event);
+	void (*read)			(struct perf_event *event);
+	void (*unthrottle)		(struct perf_event *event);
+};
+
+/**
+ * enum perf_event_active_state - the states of a event
+ */
+enum perf_event_active_state {
+	PERF_EVENT_STATE_ERROR	= -2,
+	PERF_EVENT_STATE_OFF		= -1,
+	PERF_EVENT_STATE_INACTIVE	=  0,
+	PERF_EVENT_STATE_ACTIVE	=  1,
+};
+
+struct file;
+
+struct perf_mmap_data {
+	struct rcu_head			rcu_head;
+	int				nr_pages;	/* nr of data pages  */
+	int				writable;	/* are we writable   */
+	int				nr_locked;	/* nr pages mlocked  */
+
+	atomic_t			poll;		/* POLL_ for wakeups */
+	atomic_t			events;		/* event_id limit       */
+
+	atomic_long_t			head;		/* write position    */
+	atomic_long_t			done_head;	/* completed head    */
+
+	atomic_t			lock;		/* concurrent writes */
+	atomic_t			wakeup;		/* needs a wakeup    */
+	atomic_t			lost;		/* nr records lost   */
+
+	long				watermark;	/* wakeup watermark  */
+
+	struct perf_event_mmap_page   *user_page;
+	void				*data_pages[0];
+};
+
+struct perf_pending_entry {
+	struct perf_pending_entry *next;
+	void (*func)(struct perf_pending_entry *);
+};
+
+/**
+ * struct perf_event - performance event kernel representation:
+ */
+struct perf_event {
+#ifdef CONFIG_PERF_EVENTS
+	struct list_head		group_entry;
+	struct list_head		event_entry;
+	struct list_head		sibling_list;
+	int				nr_siblings;
+	struct perf_event		*group_leader;
+	struct perf_event		*output;
+	const struct pmu		*pmu;
+
+	enum perf_event_active_state	state;
+	atomic64_t			count;
+
+	/*
+	 * These are the total time in nanoseconds that the event
+	 * has been enabled (i.e. eligible to run, and the task has
+	 * been scheduled in, if this is a per-task event)
+	 * and running (scheduled onto the CPU), respectively.
+	 *
+	 * They are computed from tstamp_enabled, tstamp_running and
+	 * tstamp_stopped when the event is in INACTIVE or ACTIVE state.
+	 */
+	u64				total_time_enabled;
+	u64				total_time_running;
+
+	/*
+	 * These are timestamps used for computing total_time_enabled
+	 * and total_time_running when the event is in INACTIVE or
+	 * ACTIVE state, measured in nanoseconds from an arbitrary point
+	 * in time.
+	 * tstamp_enabled: the notional time when the event was enabled
+	 * tstamp_running: the notional time when the event was scheduled on
+	 * tstamp_stopped: in INACTIVE state, the notional time when the
+	 *	event was scheduled off.
+	 */
+	u64				tstamp_enabled;
+	u64				tstamp_running;
+	u64				tstamp_stopped;
+
+	struct perf_event_attr	attr;
+	struct hw_perf_event		hw;
+
+	struct perf_event_context	*ctx;
+	struct file			*filp;
+
+	/*
+	 * These accumulate total time (in nanoseconds) that children
+	 * events have been enabled and running, respectively.
+	 */
+	atomic64_t			child_total_time_enabled;
+	atomic64_t			child_total_time_running;
+
+	/*
+	 * Protect attach/detach and child_list:
+	 */
+	struct mutex			child_mutex;
+	struct list_head		child_list;
+	struct perf_event		*parent;
+
+	int				oncpu;
+	int				cpu;
+
+	struct list_head		owner_entry;
+	struct task_struct		*owner;
+
+	/* mmap bits */
+	struct mutex			mmap_mutex;
+	atomic_t			mmap_count;
+	struct perf_mmap_data		*data;
+
+	/* poll related */
+	wait_queue_head_t		waitq;
+	struct fasync_struct		*fasync;
+
+	/* delayed work for NMIs and such */
+	int				pending_wakeup;
+	int				pending_kill;
+	int				pending_disable;
+	struct perf_pending_entry	pending;
+
+	atomic_t			event_limit;
+
+	void (*destroy)(struct perf_event *);
+	struct rcu_head			rcu_head;
+
+	struct pid_namespace		*ns;
+	u64				id;
+#endif
+};
+
+/**
+ * struct perf_event_context - event context structure
+ *
+ * Used as a container for task events and CPU events as well:
+ */
+struct perf_event_context {
+	/*
+	 * Protect the states of the events in the list,
+	 * nr_active, and the list:
+	 */
+	spinlock_t			lock;
+	/*
+	 * Protect the list of events.  Locking either mutex or lock
+	 * is sufficient to ensure the list doesn't change; to change
+	 * the list you need to lock both the mutex and the spinlock.
+	 */
+	struct mutex			mutex;
+
+	struct list_head		group_list;
+	struct list_head		event_list;
+	int				nr_events;
+	int				nr_active;
+	int				is_active;
+	int				nr_stat;
+	atomic_t			refcount;
+	struct task_struct		*task;
+
+	/*
+	 * Context clock, runs when context enabled.
+	 */
+	u64				time;
+	u64				timestamp;
+
+	/*
+	 * These fields let us detect when two contexts have both
+	 * been cloned (inherited) from a common ancestor.
+	 */
+	struct perf_event_context	*parent_ctx;
+	u64				parent_gen;
+	u64				generation;
+	int				pin_count;
+	struct rcu_head			rcu_head;
+};
+
+/**
+ * struct perf_event_cpu_context - per cpu event context structure
+ */
+struct perf_cpu_context {
+	struct perf_event_context	ctx;
+	struct perf_event_context	*task_ctx;
+	int				active_oncpu;
+	int				max_pertask;
+	int				exclusive;
+
+	/*
+	 * Recursion avoidance:
+	 *
+	 * task, softirq, irq, nmi context
+	 */
+	int				recursion[4];
+};
+
+struct perf_output_handle {
+	struct perf_event	*event;
+	struct perf_mmap_data	*data;
+	unsigned long		head;
+	unsigned long		offset;
+	int			nmi;
+	int			sample;
+	int			locked;
+	unsigned long		flags;
+};
+
+#ifdef CONFIG_PERF_EVENTS
+
+/*
+ * Set by architecture code:
+ */
+extern int perf_max_events;
+
+extern const struct pmu *hw_perf_event_init(struct perf_event *event);
+
+extern void perf_event_task_sched_in(struct task_struct *task, int cpu);
+extern void perf_event_task_sched_out(struct task_struct *task,
+					struct task_struct *next, int cpu);
+extern void perf_event_task_tick(struct task_struct *task, int cpu);
+extern int perf_event_init_task(struct task_struct *child);
+extern void perf_event_exit_task(struct task_struct *child);
+extern void perf_event_free_task(struct task_struct *task);
+extern void set_perf_event_pending(void);
+extern void perf_event_do_pending(void);
+extern void perf_event_print_debug(void);
+extern void __perf_disable(void);
+extern bool __perf_enable(void);
+extern void perf_disable(void);
+extern void perf_enable(void);
+extern int perf_event_task_disable(void);
+extern int perf_event_task_enable(void);
+extern int hw_perf_group_sched_in(struct perf_event *group_leader,
+	       struct perf_cpu_context *cpuctx,
+	       struct perf_event_context *ctx, int cpu);
+extern void perf_event_update_userpage(struct perf_event *event);
+
+struct perf_sample_data {
+	u64				type;
+
+	u64				ip;
+	struct {
+		u32	pid;
+		u32	tid;
+	}				tid_entry;
+	u64				time;
+	u64				addr;
+	u64				id;
+	u64				stream_id;
+	struct {
+		u32	cpu;
+		u32	reserved;
+	}				cpu_entry;
+	u64				period;
+	struct perf_callchain_entry	*callchain;
+	struct perf_raw_record		*raw;
+};
+
+extern void perf_output_sample(struct perf_output_handle *handle,
+			       struct perf_event_header *header,
+			       struct perf_sample_data *data,
+			       struct perf_event *event);
+extern void perf_prepare_sample(struct perf_event_header *header,
+				struct perf_sample_data *data,
+				struct perf_event *event,
+				struct pt_regs *regs);
+
+extern int perf_event_overflow(struct perf_event *event, int nmi,
+				 struct perf_sample_data *data,
+				 struct pt_regs *regs);
+
+/*
+ * Return 1 for a software event, 0 for a hardware event
+ */
+static inline int is_software_event(struct perf_event *event)
+{
+	return (event->attr.type != PERF_TYPE_RAW) &&
+		(event->attr.type != PERF_TYPE_HARDWARE) &&
+		(event->attr.type != PERF_TYPE_HW_CACHE);
+}
+
+extern atomic_t perf_swevent_enabled[PERF_COUNT_SW_MAX];
+
+extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
+
+static inline void
+perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
+{
+	if (atomic_read(&perf_swevent_enabled[event_id]))
+		__perf_sw_event(event_id, nr, nmi, regs, addr);
+}
+
+extern void __perf_event_mmap(struct vm_area_struct *vma);
+
+static inline void perf_event_mmap(struct vm_area_struct *vma)
+{
+	if (vma->vm_flags & VM_EXEC)
+		__perf_event_mmap(vma);
+}
+
+extern void perf_event_comm(struct task_struct *tsk);
+extern void perf_event_fork(struct task_struct *tsk);
+
+extern struct perf_callchain_entry *perf_callchain(struct pt_regs *regs);
+
+extern int sysctl_perf_event_paranoid;
+extern int sysctl_perf_event_mlock;
+extern int sysctl_perf_event_sample_rate;
+
+extern void perf_event_init(void);
+extern void perf_tp_event(int event_id, u64 addr, u64 count,
+				 void *record, int entry_size);
+
+#ifndef perf_misc_flags
+#define perf_misc_flags(regs)	(user_mode(regs) ? PERF_RECORD_MISC_USER : \
+				 PERF_RECORD_MISC_KERNEL)
+#define perf_instruction_pointer(regs)	instruction_pointer(regs)
+#endif
+
+extern int perf_output_begin(struct perf_output_handle *handle,
+			     struct perf_event *event, unsigned int size,
+			     int nmi, int sample);
+extern void perf_output_end(struct perf_output_handle *handle);
+extern void perf_output_copy(struct perf_output_handle *handle,
+			     const void *buf, unsigned int len);
+#else
+static inline void
+perf_event_task_sched_in(struct task_struct *task, int cpu)		{ }
+static inline void
+perf_event_task_sched_out(struct task_struct *task,
+			    struct task_struct *next, int cpu)		{ }
+static inline void
+perf_event_task_tick(struct task_struct *task, int cpu)		{ }
+static inline int perf_event_init_task(struct task_struct *child)	{ return 0; }
+static inline void perf_event_exit_task(struct task_struct *child)	{ }
+static inline void perf_event_free_task(struct task_struct *task)	{ }
+static inline void perf_event_do_pending(void)			{ }
+static inline void perf_event_print_debug(void)			{ }
+static inline void perf_disable(void)					{ }
+static inline void perf_enable(void)					{ }
+static inline int perf_event_task_disable(void)	{ return -EINVAL; }
+static inline int perf_event_task_enable(void)	{ return -EINVAL; }
+
+static inline void
+perf_sw_event(u32 event_id, u64 nr, int nmi,
+		     struct pt_regs *regs, u64 addr)			{ }
+
+static inline void perf_event_mmap(struct vm_area_struct *vma)	{ }
+static inline void perf_event_comm(struct task_struct *tsk)		{ }
+static inline void perf_event_fork(struct task_struct *tsk)		{ }
+static inline void perf_event_init(void)				{ }
+
+#endif
+
+#define perf_output_put(handle, x) \
+	perf_output_copy((handle), &(x), sizeof(x))
+
+#endif /* __KERNEL__ */
+#endif /* _LINUX_PERF_EVENT_H */
