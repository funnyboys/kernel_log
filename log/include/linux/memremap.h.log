commit 9b06860d7c1f1f4cb7d70f92e47dfa4a91bd5007
Merge: 0906d8b975ff f6d2b802f80d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 8 21:03:40 2020 -0700

    Merge tag 'libnvdimm-for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm and dax updates from Dan Williams:
     "There were multiple touches outside of drivers/nvdimm/ this round to
      add cross arch compatibility to the devm_memremap_pages() interface,
      enhance numa information for persistent memory ranges, and add a
      zero_page_range() dax operation.
    
      This cycle I switched from the patchwork api to Konstantin's b4 script
      for collecting tags (from x86, PowerPC, filesystem, and device-mapper
      folks), and everything looks to have gone ok there. This has all
      appeared in -next with no reported issues.
    
      Summary:
    
       - Add support for region alignment configuration and enforcement to
         fix compatibility across architectures and PowerPC page size
         configurations.
    
       - Introduce 'zero_page_range' as a dax operation. This facilitates
         filesystem-dax operation without a block-device.
    
       - Introduce phys_to_target_node() to facilitate drivers that want to
         know resulting numa node if a given reserved address range was
         onlined.
    
       - Advertise a persistence-domain for of_pmem and papr_scm. The
         persistence domain indicates where cpu-store cycles need to reach
         in the platform-memory subsystem before the platform will consider
         them power-fail protected.
    
       - Promote numa_map_to_online_node() to a cross-kernel generic
         facility.
    
       - Save x86 numa information to allow for node-id lookups for reserved
         memory ranges, deploy that capability for the e820-pmem driver.
    
       - Pick up some miscellaneous minor fixes, that missed v5.6-final,
         including a some smatch reports in the ioctl path and some unit
         test compilation fixups.
    
       - Fixup some flexible-array declarations"
    
    * tag 'libnvdimm-for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (29 commits)
      dax: Move mandatory ->zero_page_range() check in alloc_dax()
      dax,iomap: Add helper dax_iomap_zero() to zero a range
      dax: Use new dax zero page method for zeroing a page
      dm,dax: Add dax zero_page_range operation
      s390,dcssblk,dax: Add dax zero_page_range operation to dcssblk driver
      dax, pmem: Add a dax operation zero_page_range
      pmem: Add functions for reading/writing page to/from pmem
      libnvdimm: Update persistence domain value for of_pmem and papr_scm device
      tools/test/nvdimm: Fix out of tree build
      libnvdimm/region: Fix build error
      libnvdimm/region: Replace zero-length array with flexible-array member
      libnvdimm/label: Replace zero-length array with flexible-array member
      ACPI: NFIT: Replace zero-length array with flexible-array member
      libnvdimm/region: Introduce an 'align' attribute
      libnvdimm/region: Introduce NDD_LABELING
      libnvdimm/namespace: Enforce memremap_compat_align()
      libnvdimm/pfn: Prevent raw mode fallback if pfn-infoblock valid
      libnvdimm: Out of bounds read in __nd_ioctl()
      acpi/nfit: improve bounds checking for 'func'
      mm/memremap_pages: Introduce memremap_compat_align()
      ...

commit 1d90b6491014ead775146726b81a78ed993c3188
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Mon Apr 6 20:08:46 2020 -0700

    include/linux/memremap.h: remove stale comments
    
    Fixes: 80a72d0af05a ("memremap: remove the data field in struct dev_pagemap")
    Fixes: fdc029b19dfd ("memremap: remove the dev field in struct dev_pagemap")
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Link: http://lkml.kernel.org/r/20200316213205.145333-1-ira.weiny@intel.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 60d97e8fd3c0..8b37c4c9222c 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -98,8 +98,6 @@ struct dev_pagemap_ops {
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @internal_ref: internal reference if @ref is not provided by the caller
  * @done: completion for @internal_ref
- * @dev: host device of the mapping for debug
- * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
  * @flags: PGMAP_* flags to specify defailed behavior
  * @ops: method table

commit f894ddd5ff01d3bb48faf4dc533536bf5269c17a
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Mar 16 20:32:13 2020 +0100

    memremap: add an owner field to struct dev_pagemap
    
    Add a new opaque owner field to struct dev_pagemap, which will allow the
    hmm and migrate_vma code to identify who owns ZONE_DEVICE memory, and
    refuse to work on mappings not owned by the calling entity.
    
    Link: https://lore.kernel.org/r/20200316193216.920734-2-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Tested-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 6fefb09af7c3..60d97e8fd3c0 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -103,6 +103,9 @@ struct dev_pagemap_ops {
  * @type: memory type: see MEMORY_* in memory_hotplug.h
  * @flags: PGMAP_* flags to specify defailed behavior
  * @ops: method table
+ * @owner: an opaque pointer identifying the entity that manages this
+ *	instance.  Used by various helpers to make sure that no
+ *	foreign ZONE_DEVICE memory is accessed.
  */
 struct dev_pagemap {
 	struct vmem_altmap altmap;
@@ -113,6 +116,7 @@ struct dev_pagemap {
 	enum memory_type type;
 	unsigned int flags;
 	const struct dev_pagemap_ops *ops;
+	void *owner;
 };
 
 static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)

commit 9ffc1d19fc4a6dfcfe06c91c2861ad6d44fdd92d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jan 30 12:06:07 2020 -0800

    mm/memremap_pages: Introduce memremap_compat_align()
    
    The "sub-section memory hotplug" facility allows memremap_pages() users
    like libnvdimm to compensate for hardware platforms like x86 that have a
    section size larger than their hardware memory mapping granularity.  The
    compensation that sub-section support affords is being tolerant of
    physical memory resources shifting by units smaller (64MiB on x86) than
    the memory-hotplug section size (128 MiB). Where the platform
    physical-memory mapping granularity is limited by the number and
    capability of address-decode-registers in the memory controller.
    
    While the sub-section support allows memremap_pages() to operate on
    sub-section (2MiB) granularity, the Power architecture may still
    require 16MiB alignment on "!radix_enabled()" platforms.
    
    In order for libnvdimm to be able to detect and manage this per-arch
    limitation, introduce memremap_compat_align() as a common minimum
    alignment across all driver-facing memory-mapping interfaces, and let
    Power override it to 16MiB in the "!radix_enabled()" case.
    
    The assumption / requirement for 16MiB to be a viable
    memremap_compat_align() value is that Power does not have platforms
    where its equivalent of address-decode-registers never hardware remaps a
    persistent memory resource on smaller than 16MiB boundaries. Note that I
    tried my best to not add a new Kconfig symbol, but header include
    entanglements defeated the #ifndef memremap_compat_align design pattern
    and the need to export it defeats the __weak design pattern for arch
    overrides.
    
    Based on an initial patch by Aneesh.
    
    Link: http://lore.kernel.org/r/CAPcyv4gBGNP95APYaBcsocEa50tQj9b5h__83vgngjq3ouGX_Q@mail.gmail.com
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reported-by: Jeff Moyer <jmoyer@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 6fefb09af7c3..8af1cbd8f293 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -132,6 +132,7 @@ struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 
 unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
 void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
+unsigned long memremap_compat_align(void);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct dev_pagemap *pgmap)
@@ -165,6 +166,12 @@ static inline void vmem_altmap_free(struct vmem_altmap *altmap,
 		unsigned long nr_pfns)
 {
 }
+
+/* when memremap_pages() is disabled all archs can remap a single page */
+static inline unsigned long memremap_compat_align(void)
+{
+	return PAGE_SIZE;
+}
 #endif /* CONFIG_ZONE_DEVICE */
 
 static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
@@ -172,4 +179,5 @@ static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
 	if (pgmap)
 		percpu_ref_put(pgmap->ref);
 }
+
 #endif /* _LINUX_MEMREMAP_H_ */

commit a3c0e7b1fe1fc62bba5f591c4bc404eea96823b8
Merge: 939ca9f1751d 4c806b897d60
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 29 10:33:41 2019 -0700

    Merge tag 'libnvdimm-fixes-5.4-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    More libnvdimm updates from Dan Williams:
    
     - Complete the reworks to interoperate with powerpc dynamic huge page
       sizes
    
     - Fix a crash due to missed accounting for the powerpc 'struct
       page'-memmap mapping granularity
    
     - Fix badblock initialization for volatile (DRAM emulated) pmem ranges
    
     - Stop triggering request_key() notifications to userspace when
       NVDIMM-security is disabled / not present
    
     - Miscellaneous small fixups
    
    * tag 'libnvdimm-fixes-5.4-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm:
      libnvdimm/region: Enable MAP_SYNC for volatile regions
      libnvdimm: prevent nvdimm from requesting key when security is disabled
      libnvdimm/region: Initialize bad block for volatile namespaces
      libnvdimm/nfit_test: Fix acpi_handle redefinition
      libnvdimm/altmap: Track namespace boundaries in altmap
      libnvdimm: Fix endian conversion issues 
      libnvdimm/dax: Pick the right alignment default when creating dax devices
      powerpc/book3s64: Export has_transparent_hugepage() related functions.

commit cf387d9644d8c78721cf9b77af9f67bb5b04da16
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Sep 10 11:58:25 2019 +0530

    libnvdimm/altmap: Track namespace boundaries in altmap
    
    With PFN_MODE_PMEM namespace, the memmap area is allocated from the device
    area. Some architectures map the memmap area with large page size. On
    architectures like ppc64, 16MB page for memap mapping can map 262144 pfns.
    This maps a namespace size of 16G.
    
    When populating memmap region with 16MB page from the device area,
    make sure the allocated space is not used to map resources outside this
    namespace. Such usage of device area will prevent a namespace destroy.
    
    Add resource end pnf in altmap and use that to check if the memmap area
    allocation can map pfn outside the namespace. On ppc64 in such case we fallback
    to allocation from memory.
    
    This fix kernel crash reported below:
    
    [  132.034989] WARNING: CPU: 13 PID: 13719 at mm/memremap.c:133 devm_memremap_pages_release+0x2d8/0x2e0
    [  133.464754] BUG: Unable to handle kernel data access at 0xc00c00010b204000
    [  133.464760] Faulting instruction address: 0xc00000000007580c
    [  133.464766] Oops: Kernel access of bad area, sig: 11 [#1]
    [  133.464771] LE PAGE_SIZE=64K MMU=Hash SMP NR_CPUS=2048 NUMA pSeries
    .....
    [  133.464901] NIP [c00000000007580c] vmemmap_free+0x2ac/0x3d0
    [  133.464906] LR [c0000000000757f8] vmemmap_free+0x298/0x3d0
    [  133.464910] Call Trace:
    [  133.464914] [c000007cbfd0f7b0] [c0000000000757f8] vmemmap_free+0x298/0x3d0 (unreliable)
    [  133.464921] [c000007cbfd0f8d0] [c000000000370a44] section_deactivate+0x1a4/0x240
    [  133.464928] [c000007cbfd0f980] [c000000000386270] __remove_pages+0x3a0/0x590
    [  133.464935] [c000007cbfd0fa50] [c000000000074158] arch_remove_memory+0x88/0x160
    [  133.464942] [c000007cbfd0fae0] [c0000000003be8c0] devm_memremap_pages_release+0x150/0x2e0
    [  133.464949] [c000007cbfd0fb70] [c000000000738ea0] devm_action_release+0x30/0x50
    [  133.464955] [c000007cbfd0fb90] [c00000000073a5a4] release_nodes+0x344/0x400
    [  133.464961] [c000007cbfd0fc40] [c00000000073378c] device_release_driver_internal+0x15c/0x250
    [  133.464968] [c000007cbfd0fc80] [c00000000072fd14] unbind_store+0x104/0x110
    [  133.464973] [c000007cbfd0fcd0] [c00000000072ee24] drv_attr_store+0x44/0x70
    [  133.464981] [c000007cbfd0fcf0] [c0000000004a32bc] sysfs_kf_write+0x6c/0xa0
    [  133.464987] [c000007cbfd0fd10] [c0000000004a1dfc] kernfs_fop_write+0x17c/0x250
    [  133.464993] [c000007cbfd0fd60] [c0000000003c348c] __vfs_write+0x3c/0x70
    [  133.464999] [c000007cbfd0fd80] [c0000000003c75d0] vfs_write+0xd0/0x250
    
    djbw: Aneesh notes that this crash can likely be triggered in any kernel that
    supports 'papr_scm', so flagging that commit for -stable consideration.
    
    Fixes: b5beae5e224f ("powerpc/pseries: Add driver for PAPR SCM regions")
    Cc: <stable@vger.kernel.org>
    Reported-by: Sachin Sant <sachinp@linux.vnet.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reviewed-by: Pankaj Gupta <pagupta@redhat.com>
    Tested-by: Santosh Sivaraj <santosh@fossix.org>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Link: https://lore.kernel.org/r/20190910062826.10041-1-aneesh.kumar@linux.ibm.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f8a5b2a19945..c70996fe48c8 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -17,6 +17,7 @@ struct device;
  */
 struct vmem_altmap {
 	const unsigned long base_pfn;
+	const unsigned long end_pfn;
 	const unsigned long reserve;
 	unsigned long free;
 	unsigned long align;

commit 299d14d4c31aff3b37a03894e012edf8421676ee
Merge: e94f8ccde471 c5048a73b477
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 23 19:16:01 2019 -0700

    Merge tag 'pci-v5.4-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci
    
    Pull PCI updates from Bjorn Helgaas:
     "Enumeration:
    
       - Consolidate _HPP/_HPX stuff in pci-acpi.c and simplify it
         (Krzysztof Wilczynski)
    
       - Fix incorrect PCIe device types and remove dev->has_secondary_link
         to simplify code that deals with upstream/downstream ports (Mika
         Westerberg)
    
       - After suspend, restore Resizable BAR size bits correctly for 1MB
         BARs (Sumit Saxena)
    
       - Enable PCI_MSI_IRQ_DOMAIN support for RISC-V (Wesley Terpstra)
    
      Virtualization:
    
       - Add ACS quirks for iProc PAXB (Abhinav Ratna), Amazon Annapurna
         Labs (Ali Saidi)
    
       - Move sysfs SR-IOV functions to iov.c (Kelsey Skunberg)
    
       - Remove group write permissions from sysfs sriov_numvfs,
         sriov_drivers_autoprobe (Kelsey Skunberg)
    
      Hotplug:
    
       - Simplify pciehp indicator control (Denis Efremov)
    
      Peer-to-peer DMA:
    
       - Allow P2P DMA between root ports for whitelisted bridges (Logan
         Gunthorpe)
    
       - Whitelist some Intel host bridges for P2P DMA (Logan Gunthorpe)
    
       - DMA map P2P DMA requests that traverse host bridge (Logan
         Gunthorpe)
    
      Amazon Annapurna Labs host bridge driver:
    
       - Add DT binding and controller driver (Jonathan Chocron)
    
      Hyper-V host bridge driver:
    
       - Fix hv_pci_dev->pci_slot use-after-free (Dexuan Cui)
    
       - Fix PCI domain number collisions (Haiyang Zhang)
    
       - Use instance ID bytes 4 & 5 as PCI domain numbers (Haiyang Zhang)
    
       - Fix build errors on non-SYSFS config (Randy Dunlap)
    
      i.MX6 host bridge driver:
    
       - Limit DBI register length (Stefan Agner)
    
      Intel VMD host bridge driver:
    
       - Fix config addressing issues (Jon Derrick)
    
      Layerscape host bridge driver:
    
       - Add bar_fixed_64bit property to endpoint driver (Xiaowei Bao)
    
       - Add CONFIG_PCI_LAYERSCAPE_EP to build EP/RC drivers separately
         (Xiaowei Bao)
    
      Mediatek host bridge driver:
    
       - Add MT7629 controller support (Jianjun Wang)
    
      Mobiveil host bridge driver:
    
       - Fix CPU base address setup (Hou Zhiqiang)
    
       - Make "num-lanes" property optional (Hou Zhiqiang)
    
      Tegra host bridge driver:
    
       - Fix OF node reference leak (Nishka Dasgupta)
    
       - Disable MSI for root ports to work around design problem (Vidya
         Sagar)
    
       - Add Tegra194 DT binding and controller support (Vidya Sagar)
    
       - Add support for sideband pins and slot regulators (Vidya Sagar)
    
       - Add PIPE2UPHY support (Vidya Sagar)
    
      Misc:
    
       - Remove unused pci_block_cfg_access() et al (Kelsey Skunberg)
    
       - Unexport pci_bus_get(), etc (Kelsey Skunberg)
    
       - Hide PM, VC, link speed, ATS, ECRC, PTM constants and interfaces in
         the PCI core (Kelsey Skunberg)
    
       - Clean up sysfs DEVICE_ATTR() usage (Kelsey Skunberg)
    
       - Mark expected switch fall-through (Gustavo A. R. Silva)
    
       - Propagate errors for optional regulators and PHYs (Thierry Reding)
    
       - Fix kernel command line resource_alignment parameter issues (Logan
         Gunthorpe)"
    
    * tag 'pci-v5.4-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci: (112 commits)
      PCI: Add pci_irq_vector() and other stubs when !CONFIG_PCI
      arm64: tegra: Add PCIe slot supply information in p2972-0000 platform
      arm64: tegra: Add configuration for PCIe C5 sideband signals
      PCI: tegra: Add support to enable slot regulators
      PCI: tegra: Add support to configure sideband pins
      PCI: vmd: Fix shadow offsets to reflect spec changes
      PCI: vmd: Fix config addressing when using bus offsets
      PCI: dwc: Add validation that PCIe core is set to correct mode
      PCI: dwc: al: Add Amazon Annapurna Labs PCIe controller driver
      dt-bindings: PCI: Add Amazon's Annapurna Labs PCIe host bridge binding
      PCI: Add quirk to disable MSI-X support for Amazon's Annapurna Labs Root Port
      PCI/VPD: Prevent VPD access for Amazon's Annapurna Labs Root Port
      PCI: Add ACS quirk for Amazon Annapurna Labs root ports
      PCI: Add Amazon's Annapurna Labs vendor ID
      MAINTAINERS: Add PCI native host/endpoint controllers designated reviewer
      PCI: hv: Use bytes 4 and 5 from instance ID as the PCI domain numbers
      dt-bindings: PCI: tegra: Add PCIe slot supplies regulator entries
      dt-bindings: PCI: tegra: Add sideband pins configuration entries
      PCI: tegra: Add Tegra194 PCIe support
      PCI: Get rid of dev->has_secondary_link flag
      ...

commit 6869b7b206595ae0e326f59719090351eb8f4f5d
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Aug 18 11:05:57 2019 +0200

    memremap: provide a not device managed memremap_pages
    
    The kvmppc ultravisor code wants a device private memory pool that is
    system wide and not attached to a device.  Instead of faking up one
    provide a low-level memremap_pages for it.
    
    Link: https://lore.kernel.org/r/20190818090557.17853-5-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Bharata B Rao <bharata@linux.ibm.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 8f0013e18e14..fb2a0bd826b9 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -123,6 +123,8 @@ static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
 }
 
 #ifdef CONFIG_ZONE_DEVICE
+void *memremap_pages(struct dev_pagemap *pgmap, int nid);
+void memunmap_pages(struct dev_pagemap *pgmap);
 void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap);
 void devm_memunmap_pages(struct device *dev, struct dev_pagemap *pgmap);
 struct dev_pagemap *get_dev_pagemap(unsigned long pfn,

commit fdc029b19dfd190840d23e5c70bd231140b0e4a4
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Aug 18 11:05:55 2019 +0200

    memremap: remove the dev field in struct dev_pagemap
    
    The dev field in struct dev_pagemap is only used to print dev_name in two
    places, which are at best nice to have.  Just remove the field and thus
    the name in those two messages.
    
    Link: https://lore.kernel.org/r/20190818090557.17853-3-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Bharata B Rao <bharata@linux.ibm.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f8a5b2a19945..8f0013e18e14 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -109,7 +109,6 @@ struct dev_pagemap {
 	struct percpu_ref *ref;
 	struct percpu_ref internal_ref;
 	struct completion done;
-	struct device *dev;
 	enum memory_type type;
 	unsigned int flags;
 	u64 pci_p2pdma_bus_offset;

commit a6e6fe6549f609f5c00c91e988da9d25f8ae8604
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Mon Aug 12 11:30:35 2019 -0600

    PCI/P2PDMA: Introduce private pagemap structure
    
    Move the PCI bus offset from the generic dev_pagemap structure to a
    specific pci_p2pdma_pagemap structure.
    
    This structure will grow in subsequent patches.
    
    Link: https://lore.kernel.org/r/20190730163545.4915-2-logang@deltatee.com
    Link: https://lore.kernel.org/r/20190812173048.9186-2-logang@deltatee.com
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f8a5b2a19945..b459518ce475 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -112,7 +112,6 @@ struct dev_pagemap {
 	struct device *dev;
 	enum memory_type type;
 	unsigned int flags;
-	u64 pci_p2pdma_bus_offset;
 	const struct dev_pagemap_ops *ops;
 };
 

commit 24917f6b1041a73993178920656e13364f847995
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:14 2019 +0200

    memremap: provide an optional internal refcount in struct dev_pagemap
    
    Provide an internal refcounting logic if no ->ref field is provided
    in the pagemap passed into devm_memremap_pages so that callers don't
    have to reinvent it poorly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index e25685b878e9..f8a5b2a19945 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -95,6 +95,8 @@ struct dev_pagemap_ops {
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
+ * @internal_ref: internal reference if @ref is not provided by the caller
+ * @done: completion for @internal_ref
  * @dev: host device of the mapping for debug
  * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
@@ -105,6 +107,8 @@ struct dev_pagemap {
 	struct vmem_altmap altmap;
 	struct resource res;
 	struct percpu_ref *ref;
+	struct percpu_ref internal_ref;
+	struct completion done;
 	struct device *dev;
 	enum memory_type type;
 	unsigned int flags;

commit 514caf23a70fd697fa2ece238b2cd8dcc73fb16f
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:13 2019 +0200

    memremap: replace the altmap_valid field with a PGMAP_ALTMAP_VALID flag
    
    Add a flags field to struct dev_pagemap to replace the altmap_valid
    boolean to be a little more extensible.  Also add a pgmap_altmap() helper
    to find the optional altmap and clean up the code using the altmap using
    it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 336eca601dad..e25685b878e9 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -88,6 +88,8 @@ struct dev_pagemap_ops {
 	vm_fault_t (*migrate_to_ram)(struct vm_fault *vmf);
 };
 
+#define PGMAP_ALTMAP_VALID	(1 << 0)
+
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
@@ -96,19 +98,27 @@ struct dev_pagemap_ops {
  * @dev: host device of the mapping for debug
  * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
+ * @flags: PGMAP_* flags to specify defailed behavior
  * @ops: method table
  */
 struct dev_pagemap {
 	struct vmem_altmap altmap;
-	bool altmap_valid;
 	struct resource res;
 	struct percpu_ref *ref;
 	struct device *dev;
 	enum memory_type type;
+	unsigned int flags;
 	u64 pci_p2pdma_bus_offset;
 	const struct dev_pagemap_ops *ops;
 };
 
+static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
+{
+	if (pgmap->flags & PGMAP_ALTMAP_VALID)
+		return &pgmap->altmap;
+	return NULL;
+}
+
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap);
 void devm_memunmap_pages(struct device *dev, struct dev_pagemap *pgmap);

commit 80a72d0af05ae97a8b106c172e431072ba587492
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:12 2019 +0200

    memremap: remove the data field in struct dev_pagemap
    
    struct dev_pagemap is always embedded into a containing structure, so
    there is no need to an additional private data field.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index ac985bd03a7f..336eca601dad 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -69,7 +69,7 @@ struct dev_pagemap_ops {
 	 * reach 0 refcount unless there is a refcount bug. This allows the
 	 * device driver to implement its own memory management.)
 	 */
-	void (*page_free)(struct page *page, void *data);
+	void (*page_free)(struct page *page);
 
 	/*
 	 * Transition the refcount in struct dev_pagemap to the dead state.
@@ -104,7 +104,6 @@ struct dev_pagemap {
 	struct resource res;
 	struct percpu_ref *ref;
 	struct device *dev;
-	void *data;
 	enum memory_type type;
 	u64 pci_p2pdma_bus_offset;
 	const struct dev_pagemap_ops *ops;

commit 897e6365cda6ba6356e83a3aaa68dec82ef4c548
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:11 2019 +0200

    memremap: add a migrate_to_ram method to struct dev_pagemap_ops
    
    This replaces the hacky ->fault callback, which is currently directly
    called from common code through a hmm specific data structure as an
    exercise in layering violations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index b8666a0d8665..ac985bd03a7f 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -80,6 +80,12 @@ struct dev_pagemap_ops {
 	 * Wait for refcount in struct dev_pagemap to be idle and reap it.
 	 */
 	void (*cleanup)(struct dev_pagemap *pgmap);
+
+	/*
+	 * Used for private (un-addressable) device memory only.  Must migrate
+	 * the page back to a CPU accessible page.
+	 */
+	vm_fault_t (*migrate_to_ram)(struct vm_fault *vmf);
 };
 
 /**

commit d8668bb0451c3c45b59dbcde2654e0539aad1d2a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:09 2019 +0200

    memremap: pass a struct dev_pagemap to ->kill and ->cleanup
    
    Passing the actual typed structure leads to more understandable code
    vs just passing the ref member.
    
    Reported-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 919755f48c7e..b8666a0d8665 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -74,12 +74,12 @@ struct dev_pagemap_ops {
 	/*
 	 * Transition the refcount in struct dev_pagemap to the dead state.
 	 */
-	void (*kill)(struct percpu_ref *ref);
+	void (*kill)(struct dev_pagemap *pgmap);
 
 	/*
 	 * Wait for refcount in struct dev_pagemap to be idle and reap it.
 	 */
-	void (*cleanup)(struct percpu_ref *ref);
+	void (*cleanup)(struct dev_pagemap *pgmap);
 };
 
 /**

commit 1e240e8d4a7d92232b6214e02a0a4197a53afd6c
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:08 2019 +0200

    memremap: move dev_pagemap callbacks into a separate structure
    
    The dev_pagemap is a growing too many callbacks.  Move them into a
    separate ops structure so that they are not duplicated for multiple
    instances, and an attacker can't easily overwrite them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 0c86f2c5ac9c..919755f48c7e 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -63,41 +63,45 @@ enum memory_type {
 	MEMORY_DEVICE_PCI_P2PDMA,
 };
 
-/*
- * Additional notes about MEMORY_DEVICE_PRIVATE may be found in
- * include/linux/hmm.h and Documentation/vm/hmm.rst. There is also a brief
- * explanation in include/linux/memory_hotplug.h.
- *
- * The page_free() callback is called once the page refcount reaches 1
- * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.
- * This allows the device driver to implement its own memory management.)
- */
-typedef void (*dev_page_free_t)(struct page *page, void *data);
+struct dev_pagemap_ops {
+	/*
+	 * Called once the page refcount reaches 1.  (ZONE_DEVICE pages never
+	 * reach 0 refcount unless there is a refcount bug. This allows the
+	 * device driver to implement its own memory management.)
+	 */
+	void (*page_free)(struct page *page, void *data);
+
+	/*
+	 * Transition the refcount in struct dev_pagemap to the dead state.
+	 */
+	void (*kill)(struct percpu_ref *ref);
+
+	/*
+	 * Wait for refcount in struct dev_pagemap to be idle and reap it.
+	 */
+	void (*cleanup)(struct percpu_ref *ref);
+};
 
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
- * @page_free: free page callback when page refcount reaches 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
- * @kill: callback to transition @ref to the dead state
- * @cleanup: callback to wait for @ref to be idle and reap it
  * @dev: host device of the mapping for debug
  * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
+ * @ops: method table
  */
 struct dev_pagemap {
-	dev_page_free_t page_free;
 	struct vmem_altmap altmap;
 	bool altmap_valid;
 	struct resource res;
 	struct percpu_ref *ref;
-	void (*kill)(struct percpu_ref *ref);
-	void (*cleanup)(struct percpu_ref *ref);
 	struct device *dev;
 	void *data;
 	enum memory_type type;
 	u64 pci_p2pdma_bus_offset;
+	const struct dev_pagemap_ops *ops;
 };
 
 #ifdef CONFIG_ZONE_DEVICE

commit 3ed2dcdf54d5bf1f9823b5faf1a702e7cee53982
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:07 2019 +0200

    memremap: validate the pagemap type passed to devm_memremap_pages
    
    Most pgmap types are only supported when certain config options are
    enabled.  Check for a type that is valid for the current configuration
    before setting up the pagemap.  For this the usage of the 0 type for
    device dax gets replaced with an explicit MEMORY_DEVICE_DEVDAX type.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 995c62c5a48b..0c86f2c5ac9c 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -45,13 +45,21 @@ struct vmem_altmap {
  * wakeup is used to coordinate physical address space management (ex:
  * fs truncate/hole punch) vs pinned pages (ex: device dma).
  *
+ * MEMORY_DEVICE_DEVDAX:
+ * Host memory that has similar access semantics as System RAM i.e. DMA
+ * coherent and supports page pinning. In contrast to
+ * MEMORY_DEVICE_FS_DAX, this memory is access via a device-dax
+ * character device.
+ *
  * MEMORY_DEVICE_PCI_P2PDMA:
  * Device memory residing in a PCI BAR intended for use with Peer-to-Peer
  * transactions.
  */
 enum memory_type {
+	/* 0 is reserved to catch uninitialized type fields */
 	MEMORY_DEVICE_PRIVATE = 1,
 	MEMORY_DEVICE_FS_DAX,
+	MEMORY_DEVICE_DEVDAX,
 	MEMORY_DEVICE_PCI_P2PDMA,
 };
 

commit 25b2995a35b609119cf96f6b62eccd56c0234c7d
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 13 22:50:49 2019 +0200

    mm: remove MEMORY_DEVICE_PUBLIC support
    
    The code hasn't been used since it was added to the tree, and doesn't
    appear to actually be usable.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 1732dea030b2..995c62c5a48b 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -37,13 +37,6 @@ struct vmem_altmap {
  * A more complete discussion of unaddressable memory may be found in
  * include/linux/hmm.h and Documentation/vm/hmm.rst.
  *
- * MEMORY_DEVICE_PUBLIC:
- * Device memory that is cache coherent from device and CPU point of view. This
- * is use on platform that have an advance system bus (like CAPI or CCIX). A
- * driver can hotplug the device memory using ZONE_DEVICE and with that memory
- * type. Any page of a process can be migrated to such memory. However no one
- * should be allow to pin such memory so that it can always be evicted.
- *
  * MEMORY_DEVICE_FS_DAX:
  * Host memory that has similar access semantics as System RAM i.e. DMA
  * coherent and supports page pinning. In support of coordinating page
@@ -58,7 +51,6 @@ struct vmem_altmap {
  */
 enum memory_type {
 	MEMORY_DEVICE_PRIVATE = 1,
-	MEMORY_DEVICE_PUBLIC,
 	MEMORY_DEVICE_FS_DAX,
 	MEMORY_DEVICE_PCI_P2PDMA,
 };

commit 50f44ee7248ad2f7984ef081974a6ecd09724b3e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jun 13 15:56:33 2019 -0700

    mm/devm_memremap_pages: fix final page put race
    
    Logan noticed that devm_memremap_pages_release() kills the percpu_ref
    drops all the page references that were acquired at init and then
    immediately proceeds to unplug, arch_remove_memory(), the backing pages
    for the pagemap.  If for some reason device shutdown actually collides
    with a busy / elevated-ref-count page then arch_remove_memory() should
    be deferred until after that reference is dropped.
    
    As it stands the "wait for last page ref drop" happens *after*
    devm_memremap_pages_release() returns, which is obviously too late and
    can lead to crashes.
    
    Fix this situation by assigning the responsibility to wait for the
    percpu_ref to go idle to devm_memremap_pages() with a new ->cleanup()
    callback.  Implement the new cleanup callback for all
    devm_memremap_pages() users: pmem, devdax, hmm, and p2pdma.
    
    Link: http://lkml.kernel.org/r/155727339156.292046.5432007428235387859.stgit@dwillia2-desk3.amr.corp.intel.com
    Fixes: 41e94a851304 ("add devm_memremap_pages")
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: "Jérôme Glisse" <jglisse@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 7601ee314c4a..1732dea030b2 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -81,6 +81,7 @@ typedef void (*dev_page_free_t)(struct page *page, void *data);
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @kill: callback to transition @ref to the dead state
+ * @cleanup: callback to wait for @ref to be idle and reap it
  * @dev: host device of the mapping for debug
  * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
@@ -92,6 +93,7 @@ struct dev_pagemap {
 	struct resource res;
 	struct percpu_ref *ref;
 	void (*kill)(struct percpu_ref *ref);
+	void (*cleanup)(struct percpu_ref *ref);
 	struct device *dev;
 	void *data;
 	enum memory_type type;

commit 2e3f139e8ecebf177fe01299285a56855e93fb84
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jun 13 15:56:21 2019 -0700

    mm/devm_memremap_pages: introduce devm_memunmap_pages
    
    Use the new devm_release_action() facility to allow
    devm_memremap_pages_release() to be manually triggered.
    
    Link: http://lkml.kernel.org/r/155727337088.292046.5774214552136776763.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Jérôme Glisse" <jglisse@redhat.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f0628660d541..7601ee314c4a 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -100,6 +100,7 @@ struct dev_pagemap {
 
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap);
+void devm_memunmap_pages(struct device *dev, struct dev_pagemap *pgmap);
 struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 		struct dev_pagemap *pgmap);
 
@@ -118,6 +119,11 @@ static inline void *devm_memremap_pages(struct device *dev,
 	return ERR_PTR(-ENXIO);
 }
 
+static inline void devm_memunmap_pages(struct device *dev,
+		struct dev_pagemap *pgmap)
+{
+}
+
 static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 		struct dev_pagemap *pgmap)
 {

commit 063a7d1d3623db31ca5d2309cab6030ebf93b72f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Dec 28 00:39:46 2018 -0800

    mm/hmm: fix memremap.h, move dev_page_fault_t callback to hmm
    
    The kbuild robot reported the following on a development branch that used
    memremap.h in a new path:
    
       In file included from arch/m68k/include/asm/pgtable_mm.h:148:0,
                         from arch/m68k/include/asm/pgtable.h:5,
                         from include/linux/memremap.h:7,
                         from drivers//dax/bus.c:3:
        arch/m68k/include/asm/motorola_pgtable.h: In function 'pgd_offset':
     >> arch/m68k/include/asm/motorola_pgtable.h:199:11: error: dereferencing pointer to incomplete type 'const struct mm_struct'
          return mm->pgd + pgd_index(address);
                   ^~
    
    The ->page_fault() callback is specific to HMM.  Move it to 'struct
    hmm_devmem' where the unusual asm/pgtable.h dependency can be contained in
    include/linux/hmm.h.  Longer term refactoring this dependency out of HMM
    is recommended, but in the meantime memremap.h remains generic.
    
    Link: http://lkml.kernel.org/r/154534090899.3120190.6652620807617715272.stgit@dwillia2-desk3.amr.corp.intel.com
    Fixes: 5042db43cc26 ("mm/ZONE_DEVICE: new type of ZONE_DEVICE memory...")
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: "Jérôme Glisse" <jglisse@redhat.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 55db66b3716f..f0628660d541 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -4,8 +4,6 @@
 #include <linux/ioport.h>
 #include <linux/percpu-refcount.h>
 
-#include <asm/pgtable.h>
-
 struct resource;
 struct device;
 
@@ -66,47 +64,18 @@ enum memory_type {
 };
 
 /*
- * For MEMORY_DEVICE_PRIVATE we use ZONE_DEVICE and extend it with two
- * callbacks:
- *   page_fault()
- *   page_free()
- *
  * Additional notes about MEMORY_DEVICE_PRIVATE may be found in
  * include/linux/hmm.h and Documentation/vm/hmm.rst. There is also a brief
  * explanation in include/linux/memory_hotplug.h.
  *
- * The page_fault() callback must migrate page back, from device memory to
- * system memory, so that the CPU can access it. This might fail for various
- * reasons (device issues,  device have been unplugged, ...). When such error
- * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and
- * set the CPU page table entry to "poisoned".
- *
- * Note that because memory cgroup charges are transferred to the device memory,
- * this should never fail due to memory restrictions. However, allocation
- * of a regular system page might still fail because we are out of memory. If
- * that happens, the page_fault() callback must return VM_FAULT_OOM.
- *
- * The page_fault() callback can also try to migrate back multiple pages in one
- * chunk, as an optimization. It must, however, prioritize the faulting address
- * over all the others.
- *
- *
  * The page_free() callback is called once the page refcount reaches 1
  * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.
  * This allows the device driver to implement its own memory management.)
- *
- * For MEMORY_DEVICE_PUBLIC only the page_free() callback matter.
  */
-typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,
-				unsigned long addr,
-				const struct page *page,
-				unsigned int flags,
-				pmd_t *pmdp);
 typedef void (*dev_page_free_t)(struct page *page, void *data);
 
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
- * @page_fault: callback when CPU fault on an unaddressable device page
  * @page_free: free page callback when page refcount reaches 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
@@ -117,7 +86,6 @@ typedef void (*dev_page_free_t)(struct page *page, void *data);
  * @type: memory type: see MEMORY_* in memory_hotplug.h
  */
 struct dev_pagemap {
-	dev_page_fault_t page_fault;
 	dev_page_free_t page_free;
 	struct vmem_altmap altmap;
 	bool altmap_valid;

commit a95c90f1e2c253b280385ecf3d4ebfe476926b28
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Dec 28 00:34:57 2018 -0800

    mm, devm_memremap_pages: fix shutdown handling
    
    The last step before devm_memremap_pages() returns success is to allocate
    a release action, devm_memremap_pages_release(), to tear the entire setup
    down.  However, the result from devm_add_action() is not checked.
    
    Checking the error from devm_add_action() is not enough.  The api
    currently relies on the fact that the percpu_ref it is using is killed by
    the time the devm_memremap_pages_release() is run.  Rather than continue
    this awkward situation, offload the responsibility of killing the
    percpu_ref to devm_memremap_pages_release() directly.  This allows
    devm_memremap_pages() to do the right thing relative to init failures and
    shutdown.
    
    Without this change we could fail to register the teardown of
    devm_memremap_pages().  The likelihood of hitting this failure is tiny as
    small memory allocations almost always succeed.  However, the impact of
    the failure is large given any future reconfiguration, or disable/enable,
    of an nvdimm namespace will fail forever as subsequent calls to
    devm_memremap_pages() will fail to setup the pgmap_radix since there will
    be stale entries for the physical address range.
    
    An argument could be made to require that the ->kill() operation be set in
    the @pgmap arg rather than passed in separately.  However, it helps code
    readability, tracking the lifetime of a given instance, to be able to grep
    the kill routine directly at the devm_memremap_pages() call site.
    
    Link: http://lkml.kernel.org/r/154275558526.76910.7535251937849268605.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Fixes: e8d513483300 ("memremap: change devm_memremap_pages interface...")
    Reviewed-by: "Jérôme Glisse" <jglisse@redhat.com>
    Reported-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 0ac69ddf5fc4..55db66b3716f 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -111,6 +111,7 @@ typedef void (*dev_page_free_t)(struct page *page, void *data);
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
+ * @kill: callback to transition @ref to the dead state
  * @dev: host device of the mapping for debug
  * @data: private data pointer for page_free()
  * @type: memory type: see MEMORY_* in memory_hotplug.h
@@ -122,6 +123,7 @@ struct dev_pagemap {
 	bool altmap_valid;
 	struct resource res;
 	struct percpu_ref *ref;
+	void (*kill)(struct percpu_ref *ref);
 	struct device *dev;
 	void *data;
 	enum memory_type type;

commit 977196b8c5b20b901acb0042579e30d7fa55790a
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Thu Oct 4 15:27:37 2018 -0600

    PCI/P2PDMA: Add PCI p2pmem DMA mappings to adjust the bus offset
    
    The DMA address used when mapping PCI P2P memory must be the PCI bus
    address.  Thus, introduce pci_p2pmem_map_sg() to map the correct addresses
    when using P2P memory.
    
    Memory mapped in this way does not need to be unmapped and thus if we
    provided pci_p2pmem_unmap_sg() it would be empty.  This breaks the expected
    balance between map/unmap but was left out as an empty function doesn't
    really provide any benefit.  In the future, if this call becomes necessary
    it can be added without much difficulty.
    
    For this, we assume that an SGL passed to these functions contain all P2P
    memory or no P2P memory.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 9553370ebdad..0ac69ddf5fc4 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -125,6 +125,7 @@ struct dev_pagemap {
 	struct device *dev;
 	void *data;
 	enum memory_type type;
+	u64 pci_p2pdma_bus_offset;
 };
 
 #ifdef CONFIG_ZONE_DEVICE

commit 52916982af48d9f9fc01ad825259de1eb3a9b25e
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Thu Oct 4 15:27:35 2018 -0600

    PCI/P2PDMA: Support peer-to-peer memory
    
    Some PCI devices may have memory mapped in a BAR space that's intended for
    use in peer-to-peer transactions.  To enable such transactions the memory
    must be registered with ZONE_DEVICE pages so it can be used by DMA
    interfaces in existing drivers.
    
    Add an interface for other subsystems to find and allocate chunks of P2P
    memory as necessary to facilitate transfers between two PCI peers:
    
      struct pci_dev *pci_p2pmem_find[_many]();
      int pci_p2pdma_distance[_many]();
      void *pci_alloc_p2pmem();
    
    The new interface requires a driver to collect a list of client devices
    involved in the transaction then call pci_p2pmem_find() to obtain any
    suitable P2P memory.  Alternatively, if the caller knows a device which
    provides P2P memory, they can use pci_p2pdma_distance() to determine if it
    is usable.  With a suitable p2pmem device, memory can then be allocated
    with pci_alloc_p2pmem() for use in DMA transactions.
    
    Depending on hardware, using peer-to-peer memory may reduce the bandwidth
    of the transfer but can significantly reduce pressure on system memory.
    This may be desirable in many cases: for example a system could be designed
    with a small CPU connected to a PCIe switch by a small number of lanes
    which would maximize the number of lanes available to connect to NVMe
    devices.
    
    The code is designed to only utilize the p2pmem device if all the devices
    involved in a transfer are behind the same PCI bridge.  This is because we
    have no way of knowing whether peer-to-peer routing between PCIe Root Ports
    is supported (PCIe r4.0, sec 1.3.1).  Additionally, the benefits of P2P
    transfers that go through the RC is limited to only reducing DRAM usage
    and, in some cases, coding convenience.  The PCI-SIG may be exploring
    adding a new capability bit to advertise whether this is possible for
    future hardware.
    
    This commit includes significant rework and feedback from Christoph
    Hellwig.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    [bhelgaas: fold in fix from Keith Busch <keith.busch@intel.com>:
    https://lore.kernel.org/linux-pci/20181012155920.15418-1-keith.busch@intel.com,
    to address comment from Dan Carpenter <dan.carpenter@oracle.com>, fold in
    https://lore.kernel.org/linux-pci/20181017160510.17926-1-logang@deltatee.com]
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f91f9e763557..9553370ebdad 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -53,11 +53,16 @@ struct vmem_altmap {
  * wakeup event whenever a page is unpinned and becomes idle. This
  * wakeup is used to coordinate physical address space management (ex:
  * fs truncate/hole punch) vs pinned pages (ex: device dma).
+ *
+ * MEMORY_DEVICE_PCI_P2PDMA:
+ * Device memory residing in a PCI BAR intended for use with Peer-to-Peer
+ * transactions.
  */
 enum memory_type {
 	MEMORY_DEVICE_PRIVATE = 1,
 	MEMORY_DEVICE_PUBLIC,
 	MEMORY_DEVICE_FS_DAX,
+	MEMORY_DEVICE_PCI_P2PDMA,
 };
 
 /*

commit 7d3bf613e99abbd96ac7b90ee3694a246c975021
Merge: a3818841bd5e 930218affead
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 8 17:21:52 2018 -0700

    Merge tag 'libnvdimm-for-4.18' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This adds a user for the new 'bytes-remaining' updates to
      memcpy_mcsafe() that you already received through Ingo via the
      x86-dax- for-linus pull.
    
      Not included here, but still targeting this cycle, is support for
      handling memory media errors (poison) consumed via userspace dax
      mappings.
    
      Summary:
    
       - DAX broke a fundamental assumption of truncate of file mapped
         pages. The truncate path assumed that it is safe to disconnect a
         pinned page from a file and let the filesystem reclaim the physical
         block. With DAX the page is equivalent to the filesystem block.
         Introduce dax_layout_busy_page() to enable filesystems to wait for
         pinned DAX pages to be released. Without this wait a filesystem
         could allocate blocks under active device-DMA to a new file.
    
       - DAX arranges for the block layer to be bypassed and uses
         dax_direct_access() + copy_to_iter() to satisfy read(2) calls.
         However, the memcpy_mcsafe() facility is available through the pmem
         block driver. In order to safely handle media errors, via the DAX
         block-layer bypass, introduce copy_to_iter_mcsafe().
    
       - Fix cache management policy relative to the ACPI NFIT Platform
         Capabilities Structure to properly elide cache flushes when they
         are not necessary. The table indicates whether CPU caches are
         power-fail protected. Clarify that a deep flush is always performed
         on REQ_{FUA,PREFLUSH} requests"
    
    * tag 'libnvdimm-for-4.18' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (21 commits)
      dax: Use dax_write_cache* helpers
      libnvdimm, pmem: Do not flush power-fail protected CPU caches
      libnvdimm, pmem: Unconditionally deep flush on *sync
      libnvdimm, pmem: Complete REQ_FLUSH => REQ_PREFLUSH
      acpi, nfit: Remove ecc_unit_size
      dax: dax_insert_mapping_entry always succeeds
      libnvdimm, e820: Register all pmem resources
      libnvdimm: Debug probe times
      linvdimm, pmem: Preserve read-only setting for pmem devices
      x86, nfit_test: Add unit test for memcpy_mcsafe()
      pmem: Switch to copy_to_iter_mcsafe()
      dax: Report bytes remaining in dax_iomap_actor()
      dax: Introduce a ->copy_to_iter dax operation
      uio, lib: Fix CONFIG_ARCH_HAS_UACCESS_MCSAFE compilation
      xfs, dax: introduce xfs_break_dax_layouts()
      xfs: prepare xfs_break_layouts() for another layout type
      xfs: prepare xfs_break_layouts() to be called with XFS_MMAPLOCK_EXCL
      mm, fs, dax: handle layout changes to pinned dax mappings
      mm: fix __gup_device_huge vs unmap
      mm: introduce MEMORY_DEVICE_FS_DAX and CONFIG_DEV_PAGEMAP_OPS
      ...

commit e7638488434415aa478e78435cac8f0365737638
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed May 16 11:46:08 2018 -0700

    mm: introduce MEMORY_DEVICE_FS_DAX and CONFIG_DEV_PAGEMAP_OPS
    
    In preparation for fixing dax-dma-vs-unmap issues, filesystems need to
    be able to rely on the fact that they will get wakeups on dev_pagemap
    page-idle events. Introduce MEMORY_DEVICE_FS_DAX and
    generic_dax_page_free() as common indicator / infrastructure for dax
    filesytems to require. With this change there are no users of the
    MEMORY_DEVICE_HOST designation, so remove it.
    
    The HMM sub-system extended dev_pagemap to arrange a callback when a
    dev_pagemap managed page is freed. Since a dev_pagemap page is free /
    idle when its reference count is 1 it requires an additional branch to
    check the page-type at put_page() time. Given put_page() is a hot-path
    we do not want to incur that check if HMM is not in use, so a static
    branch is used to avoid that overhead when not necessary.
    
    Now, the FS_DAX implementation wants to reuse this mechanism for
    receiving dev_pagemap ->page_free() callbacks. Rework the HMM-specific
    static-key into a generic mechanism that either HMM or FS_DAX code paths
    can enable.
    
    For ARCH=um builds, and any other arch that lacks ZONE_DEVICE support,
    care must be taken to compile out the DEV_PAGEMAP_OPS infrastructure.
    However, we still need to support FS_DAX in the FS_DAX_LIMITED case
    implemented by the s390/dcssblk driver.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Reported-by: kbuild test robot <lkp@intel.com>
    Reported-by: Thomas Meyer <thomas@m3y3r.de>
    Reported-by: Dave Jiang <dave.jiang@intel.com>
    Cc: "Jérôme Glisse" <jglisse@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 7b4899c06f49..5ebfff65da4d 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -1,7 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_MEMREMAP_H_
 #define _LINUX_MEMREMAP_H_
-#include <linux/mm.h>
 #include <linux/ioport.h>
 #include <linux/percpu-refcount.h>
 
@@ -30,13 +29,6 @@ struct vmem_altmap {
  * Specialize ZONE_DEVICE memory into multiple types each having differents
  * usage.
  *
- * MEMORY_DEVICE_HOST:
- * Persistent device memory (pmem): struct page might be allocated in different
- * memory and architecture might want to perform special actions. It is similar
- * to regular memory, in that the CPU can access it transparently. However,
- * it is likely to have different bandwidth and latency than regular memory.
- * See Documentation/nvdimm/nvdimm.txt for more information.
- *
  * MEMORY_DEVICE_PRIVATE:
  * Device memory that is not directly addressable by the CPU: CPU can neither
  * read nor write private memory. In this case, we do still have struct pages
@@ -53,11 +45,19 @@ struct vmem_altmap {
  * driver can hotplug the device memory using ZONE_DEVICE and with that memory
  * type. Any page of a process can be migrated to such memory. However no one
  * should be allow to pin such memory so that it can always be evicted.
+ *
+ * MEMORY_DEVICE_FS_DAX:
+ * Host memory that has similar access semantics as System RAM i.e. DMA
+ * coherent and supports page pinning. In support of coordinating page
+ * pinning vs other operations MEMORY_DEVICE_FS_DAX arranges for a
+ * wakeup event whenever a page is unpinned and becomes idle. This
+ * wakeup is used to coordinate physical address space management (ex:
+ * fs truncate/hole punch) vs pinned pages (ex: device dma).
  */
 enum memory_type {
-	MEMORY_DEVICE_HOST = 0,
-	MEMORY_DEVICE_PRIVATE,
+	MEMORY_DEVICE_PRIVATE = 1,
 	MEMORY_DEVICE_PUBLIC,
+	MEMORY_DEVICE_FS_DAX,
 };
 
 /*
@@ -129,8 +129,6 @@ struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 
 unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
 void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
-
-static inline bool is_zone_device_page(const struct page *page);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct dev_pagemap *pgmap)
@@ -161,20 +159,6 @@ static inline void vmem_altmap_free(struct vmem_altmap *altmap,
 }
 #endif /* CONFIG_ZONE_DEVICE */
 
-#if defined(CONFIG_DEVICE_PRIVATE) || defined(CONFIG_DEVICE_PUBLIC)
-static inline bool is_device_private_page(const struct page *page)
-{
-	return is_zone_device_page(page) &&
-		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
-}
-
-static inline bool is_device_public_page(const struct page *page)
-{
-	return is_zone_device_page(page) &&
-		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
-}
-#endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
-
 static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
 {
 	if (pgmap)

commit ad56b738c5dd223a2f66685830f82194025a6138
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Wed Mar 21 21:22:47 2018 +0200

    docs/vm: rename documentation files to .rst
    
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 7b4899c06f49..74ea5e2310a8 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -45,7 +45,7 @@ struct vmem_altmap {
  * must be treated as an opaque object, rather than a "normal" struct page.
  *
  * A more complete discussion of unaddressable memory may be found in
- * include/linux/hmm.h and Documentation/vm/hmm.txt.
+ * include/linux/hmm.h and Documentation/vm/hmm.rst.
  *
  * MEMORY_DEVICE_PUBLIC:
  * Device memory that is cache coherent from device and CPU point of view. This
@@ -67,7 +67,7 @@ enum memory_type {
  *   page_free()
  *
  * Additional notes about MEMORY_DEVICE_PRIVATE may be found in
- * include/linux/hmm.h and Documentation/vm/hmm.txt. There is also a brief
+ * include/linux/hmm.h and Documentation/vm/hmm.rst. There is also a brief
  * explanation in include/linux/memory_hotplug.h.
  *
  * The page_fault() callback must migrate page back, from device memory to

commit e8d5134833006a46fcbefc5f4a84d0b62bd520e7
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:54:05 2017 +0100

    memremap: change devm_memremap_pages interface to use struct dev_pagemap
    
    This new interface is similar to how struct device (and many others)
    work. The caller initializes a 'struct dev_pagemap' as required
    and calls 'devm_memremap_pages'. This allows the pagemap structure to
    be embedded in another structure and thus container_of can be used. In
    this way application specific members can be stored in a containing
    struct.
    
    This will be used by the P2P infrastructure and HMM could probably
    be cleaned up to use it as well (instead of having it's own, similar
    'hmm_devmem_pages_create' function).
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 1cb5f39d25c1..7b4899c06f49 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -123,8 +123,7 @@ struct dev_pagemap {
 };
 
 #ifdef CONFIG_ZONE_DEVICE
-void *devm_memremap_pages(struct device *dev, struct resource *res,
-		struct percpu_ref *ref, struct vmem_altmap *altmap);
+void *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap);
 struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 		struct dev_pagemap *pgmap);
 
@@ -134,8 +133,7 @@ void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
 static inline bool is_zone_device_page(const struct page *page);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
-		struct resource *res, struct percpu_ref *ref,
-		struct vmem_altmap *altmap)
+		struct dev_pagemap *pgmap)
 {
 	/*
 	 * Fail attempts to call devm_memremap_pages() without

commit e7744aa25cffe26d3767c9ffcf4e130cca1dff00
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Dec 29 08:54:04 2017 +0100

    memremap: drop private struct page_map
    
    'struct page_map' is a private structure of 'struct dev_pagemap' but the
    latter replicates all the same fields as the former so there isn't much
    value in it. Thus drop it in favour of a completely public struct.
    
    This is a clean up in preperation for a more generally useful
    'devm_memeremap_pages' interface.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 3fddcfe57bb0..1cb5f39d25c1 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -113,8 +113,9 @@ typedef void (*dev_page_free_t)(struct page *page, void *data);
 struct dev_pagemap {
 	dev_page_fault_t page_fault;
 	dev_page_free_t page_free;
-	struct vmem_altmap *altmap;
-	const struct resource *res;
+	struct vmem_altmap altmap;
+	bool altmap_valid;
+	struct resource res;
 	struct percpu_ref *ref;
 	struct device *dev;
 	void *data;

commit 0628b8c650718f4dfedfcdc9ed136bf7e394aae7
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:54:02 2017 +0100

    memremap: remove to_vmem_altmap
    
    All callers are gone now.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 26e8aaba27d5..3fddcfe57bb0 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -26,15 +26,6 @@ struct vmem_altmap {
 	unsigned long alloc;
 };
 
-#ifdef CONFIG_ZONE_DEVICE
-struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start);
-#else
-static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
-{
-	return NULL;
-}
-#endif
-
 /*
  * Specialize ZONE_DEVICE memory into multiple types each having differents
  * usage.

commit 0822acb86cf340cd45b3af6436cec7e3bb24ebd2
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:54:00 2017 +0100

    mm: move get_dev_pagemap out of line
    
    This is a pretty big function, which should be out of line in general,
    and a no-op stub if CONFIG_ZONE_DEVICЕ is not set.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index d5a6736d9737..26e8aaba27d5 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -133,7 +133,8 @@ struct dev_pagemap {
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 		struct percpu_ref *ref, struct vmem_altmap *altmap);
-struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
+struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+		struct dev_pagemap *pgmap);
 
 unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
 void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
@@ -153,7 +154,8 @@ static inline void *devm_memremap_pages(struct device *dev,
 	return ERR_PTR(-ENXIO);
 }
 
-static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
+static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+		struct dev_pagemap *pgmap)
 {
 	return NULL;
 }
@@ -183,39 +185,6 @@ static inline bool is_device_public_page(const struct page *page)
 }
 #endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
 
-/**
- * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn
- * @pfn: page frame number to lookup page_map
- * @pgmap: optional known pgmap that already has a reference
- *
- * @pgmap allows the overhead of a lookup to be bypassed when @pfn lands in the
- * same mapping.
- */
-static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
-		struct dev_pagemap *pgmap)
-{
-	const struct resource *res = pgmap ? pgmap->res : NULL;
-	resource_size_t phys = PFN_PHYS(pfn);
-
-	/*
-	 * In the cached case we're already holding a live reference so
-	 * we can simply do a blind increment
-	 */
-	if (res && phys >= res->start && phys <= res->end) {
-		percpu_ref_get(pgmap->ref);
-		return pgmap;
-	}
-
-	/* fall back to slow path lookup */
-	rcu_read_lock();
-	pgmap = find_dev_pagemap(phys);
-	if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
-		pgmap = NULL;
-	rcu_read_unlock();
-
-	return pgmap;
-}
-
 static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
 {
 	if (pgmap)

commit 8e37d00a850160bbfadbb3bf4ce49539770c5d2c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:50 2017 +0100

    memremap: provide stubs for vmem_altmap_offset and vmem_altmap_free
    
    Currently all calls to those functions are eliminated by the compiler when
    CONFIG_ZONE_DEVICE is not set, but this soon won't be the case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 10d23c367048..d5a6736d9737 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -26,9 +26,6 @@ struct vmem_altmap {
 	unsigned long alloc;
 };
 
-unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
-void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
-
 #ifdef CONFIG_ZONE_DEVICE
 struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start);
 #else
@@ -138,6 +135,9 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,
 		struct percpu_ref *ref, struct vmem_altmap *altmap);
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 
+unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
+void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
+
 static inline bool is_zone_device_page(const struct page *page);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
@@ -157,7 +157,17 @@ static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
 {
 	return NULL;
 }
-#endif
+
+static inline unsigned long vmem_altmap_offset(struct vmem_altmap *altmap)
+{
+	return 0;
+}
+
+static inline void vmem_altmap_free(struct vmem_altmap *altmap,
+		unsigned long nr_pfns)
+{
+}
+#endif /* CONFIG_ZONE_DEVICE */
 
 #if defined(CONFIG_DEVICE_PRIVATE) || defined(CONFIG_DEVICE_PUBLIC)
 static inline bool is_device_private_page(const struct page *page)

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 79f8ba7c3894..10d23c367048 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_MEMREMAP_H_
 #define _LINUX_MEMREMAP_H_
 #include <linux/mm.h>

commit 6b368cd4a44ce95b33f1d31f2f932e6ae707f319
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:32 2017 -0700

    mm/hmm: avoid bloating arch that do not make use of HMM
    
    This moves all new code including new page migration helper behind kernel
    Kconfig option so that there is no codee bloat for arch or user that do
    not want to use HMM or any of its associated features.
    
    arm allyesconfig (without all the patchset, then with and this patch):
       text    data     bss     dec     hex filename
    83721896        46511131        27582964        157815991       96814b7 ../without/vmlinux
    83722364        46511131        27582964        157816459       968168b vmlinux
    
    [jglisse@redhat.com: struct hmm is only use by HMM mirror functionality]
      Link: http://lkml.kernel.org/r/20170825213133.27286-1-jglisse@redhat.com
    [sfr@canb.auug.org.au: fix build (arm multi_v7_defconfig)]
      Link: http://lkml.kernel.org/r/20170828181849.323ab81b@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170818032858.7447-1-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index f8ee1c73ad2d..79f8ba7c3894 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -138,18 +138,6 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 
 static inline bool is_zone_device_page(const struct page *page);
-
-static inline bool is_device_private_page(const struct page *page)
-{
-	return is_zone_device_page(page) &&
-		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
-}
-
-static inline bool is_device_public_page(const struct page *page)
-{
-	return is_zone_device_page(page) &&
-		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
-}
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct resource *res, struct percpu_ref *ref,
@@ -168,17 +156,21 @@ static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
 {
 	return NULL;
 }
+#endif
 
+#if defined(CONFIG_DEVICE_PRIVATE) || defined(CONFIG_DEVICE_PUBLIC)
 static inline bool is_device_private_page(const struct page *page)
 {
-	return false;
+	return is_zone_device_page(page) &&
+		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
 }
 
 static inline bool is_device_public_page(const struct page *page)
 {
-	return false;
+	return is_zone_device_page(page) &&
+		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
 }
-#endif
+#endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
 
 /**
  * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn

commit df6ad69838fc9dcdbee0dcf2fc2c6f1113f8d609
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:12:24 2017 -0700

    mm/device-public-memory: device memory cache coherent with CPU
    
    Platform with advance system bus (like CAPI or CCIX) allow device memory
    to be accessible from CPU in a cache coherent fashion.  Add a new type of
    ZONE_DEVICE to represent such memory.  The use case are the same as for
    the un-addressable device memory but without all the corners cases.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-19-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 8aa6b82679e2..f8ee1c73ad2d 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -57,10 +57,18 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
  *
  * A more complete discussion of unaddressable memory may be found in
  * include/linux/hmm.h and Documentation/vm/hmm.txt.
+ *
+ * MEMORY_DEVICE_PUBLIC:
+ * Device memory that is cache coherent from device and CPU point of view. This
+ * is use on platform that have an advance system bus (like CAPI or CCIX). A
+ * driver can hotplug the device memory using ZONE_DEVICE and with that memory
+ * type. Any page of a process can be migrated to such memory. However no one
+ * should be allow to pin such memory so that it can always be evicted.
  */
 enum memory_type {
 	MEMORY_DEVICE_HOST = 0,
 	MEMORY_DEVICE_PRIVATE,
+	MEMORY_DEVICE_PUBLIC,
 };
 
 /*
@@ -92,6 +100,8 @@ enum memory_type {
  * The page_free() callback is called once the page refcount reaches 1
  * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.
  * This allows the device driver to implement its own memory management.)
+ *
+ * For MEMORY_DEVICE_PUBLIC only the page_free() callback matter.
  */
 typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,
 				unsigned long addr,
@@ -134,6 +144,12 @@ static inline bool is_device_private_page(const struct page *page)
 	return is_zone_device_page(page) &&
 		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
 }
+
+static inline bool is_device_public_page(const struct page *page)
+{
+	return is_zone_device_page(page) &&
+		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
+}
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct resource *res, struct percpu_ref *ref,
@@ -157,6 +173,11 @@ static inline bool is_device_private_page(const struct page *page)
 {
 	return false;
 }
+
+static inline bool is_device_public_page(const struct page *page)
+{
+	return false;
+}
 #endif
 
 /**

commit 7b2d55d2c8961ae9d456d3133f4ae2f0fbd3e14f
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:11:46 2017 -0700

    mm/ZONE_DEVICE: special case put_page() for device private pages
    
    A ZONE_DEVICE page that reach a refcount of 1 is free ie no longer have
    any user.  For device private pages this is important to catch and thus we
    need to special case put_page() for this.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-9-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 8e164ec9eed0..8aa6b82679e2 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -126,6 +126,14 @@ struct dev_pagemap {
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 		struct percpu_ref *ref, struct vmem_altmap *altmap);
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
+
+static inline bool is_zone_device_page(const struct page *page);
+
+static inline bool is_device_private_page(const struct page *page)
+{
+	return is_zone_device_page(page) &&
+		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
+}
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct resource *res, struct percpu_ref *ref,
@@ -144,6 +152,11 @@ static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
 {
 	return NULL;
 }
+
+static inline bool is_device_private_page(const struct page *page)
+{
+	return false;
+}
 #endif
 
 /**

commit 5042db43cc26f51eed51c56192e2c2317e44315f
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:11:43 2017 -0700

    mm/ZONE_DEVICE: new type of ZONE_DEVICE for unaddressable memory
    
    HMM (heterogeneous memory management) need struct page to support
    migration from system main memory to device memory.  Reasons for HMM and
    migration to device memory is explained with HMM core patch.
    
    This patch deals with device memory that is un-addressable memory (ie CPU
    can not access it).  Hence we do not want those struct page to be manage
    like regular memory.  That is why we extend ZONE_DEVICE to support
    different types of memory.
    
    A persistent memory type is define for existing user of ZONE_DEVICE and a
    new device un-addressable type is added for the un-addressable memory
    type.  There is a clear separation between what is expected from each
    memory type and existing user of ZONE_DEVICE are un-affected by new
    requirement and new use of the un-addressable type.  All specific code
    path are protect with test against the memory type.
    
    Because memory is un-addressable we use a new special swap type for when a
    page is migrated to device memory (this reduces the number of maximum swap
    file).
    
    The main two additions beside memory type to ZONE_DEVICE is two callbacks.
    First one, page_free() is call whenever page refcount reach 1 (which
    means the page is free as ZONE_DEVICE page never reach a refcount of 0).
    This allow device driver to manage its memory and associated struct page.
    
    The second callback page_fault() happens when there is a CPU access to an
    address that is back by a device page (which are un-addressable by the
    CPU).  This callback is responsible to migrate the page back to system
    main memory.  Device driver can not block migration back to system memory,
    HMM make sure that such page can not be pin into device memory.
    
    If device is in some error condition and can not migrate memory back then
    a CPU page fault to device memory should end with SIGBUS.
    
    [arnd@arndb.de: fix warning]
      Link: http://lkml.kernel.org/r/20170823133213.712917-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/20170817000548.32038-8-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index 93416196ba64..8e164ec9eed0 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -4,6 +4,8 @@
 #include <linux/ioport.h>
 #include <linux/percpu-refcount.h>
 
+#include <asm/pgtable.h>
+
 struct resource;
 struct device;
 
@@ -35,18 +37,89 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
 }
 #endif
 
+/*
+ * Specialize ZONE_DEVICE memory into multiple types each having differents
+ * usage.
+ *
+ * MEMORY_DEVICE_HOST:
+ * Persistent device memory (pmem): struct page might be allocated in different
+ * memory and architecture might want to perform special actions. It is similar
+ * to regular memory, in that the CPU can access it transparently. However,
+ * it is likely to have different bandwidth and latency than regular memory.
+ * See Documentation/nvdimm/nvdimm.txt for more information.
+ *
+ * MEMORY_DEVICE_PRIVATE:
+ * Device memory that is not directly addressable by the CPU: CPU can neither
+ * read nor write private memory. In this case, we do still have struct pages
+ * backing the device memory. Doing so simplifies the implementation, but it is
+ * important to remember that there are certain points at which the struct page
+ * must be treated as an opaque object, rather than a "normal" struct page.
+ *
+ * A more complete discussion of unaddressable memory may be found in
+ * include/linux/hmm.h and Documentation/vm/hmm.txt.
+ */
+enum memory_type {
+	MEMORY_DEVICE_HOST = 0,
+	MEMORY_DEVICE_PRIVATE,
+};
+
+/*
+ * For MEMORY_DEVICE_PRIVATE we use ZONE_DEVICE and extend it with two
+ * callbacks:
+ *   page_fault()
+ *   page_free()
+ *
+ * Additional notes about MEMORY_DEVICE_PRIVATE may be found in
+ * include/linux/hmm.h and Documentation/vm/hmm.txt. There is also a brief
+ * explanation in include/linux/memory_hotplug.h.
+ *
+ * The page_fault() callback must migrate page back, from device memory to
+ * system memory, so that the CPU can access it. This might fail for various
+ * reasons (device issues,  device have been unplugged, ...). When such error
+ * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and
+ * set the CPU page table entry to "poisoned".
+ *
+ * Note that because memory cgroup charges are transferred to the device memory,
+ * this should never fail due to memory restrictions. However, allocation
+ * of a regular system page might still fail because we are out of memory. If
+ * that happens, the page_fault() callback must return VM_FAULT_OOM.
+ *
+ * The page_fault() callback can also try to migrate back multiple pages in one
+ * chunk, as an optimization. It must, however, prioritize the faulting address
+ * over all the others.
+ *
+ *
+ * The page_free() callback is called once the page refcount reaches 1
+ * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.
+ * This allows the device driver to implement its own memory management.)
+ */
+typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,
+				unsigned long addr,
+				const struct page *page,
+				unsigned int flags,
+				pmd_t *pmdp);
+typedef void (*dev_page_free_t)(struct page *page, void *data);
+
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
+ * @page_fault: callback when CPU fault on an unaddressable device page
+ * @page_free: free page callback when page refcount reaches 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @dev: host device of the mapping for debug
+ * @data: private data pointer for page_free()
+ * @type: memory type: see MEMORY_* in memory_hotplug.h
  */
 struct dev_pagemap {
+	dev_page_fault_t page_fault;
+	dev_page_free_t page_free;
 	struct vmem_altmap *altmap;
 	const struct resource *res;
 	struct percpu_ref *ref;
 	struct device *dev;
+	void *data;
+	enum memory_type type;
 };
 
 #ifdef CONFIG_ZONE_DEVICE

commit 11db04864336f20e19e16b64ade781eeefc3f6d3
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 28 15:48:11 2016 -0700

    mm: cleanup ifdef guards for vmem_altmap
    
    Now that ZONE_DEVICE depends on SPARSEMEM_VMEMMAP we can simplify some
    ifdef guards to just ZONE_DEVICE.
    
    Link: http://lkml.kernel.org/r/146687646788.39261.8020536391978771940.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Eric Sandeen <sandeen@redhat.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index bcaa634139a9..93416196ba64 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -26,7 +26,7 @@ struct vmem_altmap {
 unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
 void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
 
-#if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_ZONE_DEVICE)
+#ifdef CONFIG_ZONE_DEVICE
 struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start);
 #else
 static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)

commit 5c2c2587b13235bf8b5c9027589f22eff68bdf49
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:49 2016 -0800

    mm, dax, pmem: introduce {get|put}_dev_pagemap() for dax-gup
    
    get_dev_page() enables paths like get_user_pages() to pin a dynamically
    mapped pfn-range (devm_memremap_pages()) while the resulting struct page
    objects are in use.  Unlike get_page() it may fail if the device is, or
    is in the process of being, disabled.  While the initial lookup of the
    range may be an expensive list walk, the result is cached to speed up
    subsequent lookups which are likely to be in the same mapped range.
    
    devm_memremap_pages() now requires a reference counter to be specified
    at init time.  For pmem this means moving request_queue allocation into
    pmem_alloc() so the existing queue usage counter can track "device
    pages".
    
    ZONE_DEVICE pages always have an elevated count and will never be on an
    lru reclaim list.  That space in 'struct page' can be redirected for
    other uses, but for safety introduce a poison value that will always
    trip __list_add() to assert.  This allows half of the struct list_head
    storage to be reclaimed with some assurance to back up the assumption
    that the page count never goes to zero and a list_add() is never
    attempted.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Logan Gunthorpe <logang@deltatee.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index aa3e82a80d7b..bcaa634139a9 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -1,6 +1,8 @@
 #ifndef _LINUX_MEMREMAP_H_
 #define _LINUX_MEMREMAP_H_
 #include <linux/mm.h>
+#include <linux/ioport.h>
+#include <linux/percpu-refcount.h>
 
 struct resource;
 struct device;
@@ -36,21 +38,25 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
+ * @res: physical address range covered by @ref
+ * @ref: reference count that pins the devm_memremap_pages() mapping
  * @dev: host device of the mapping for debug
  */
 struct dev_pagemap {
 	struct vmem_altmap *altmap;
 	const struct resource *res;
+	struct percpu_ref *ref;
 	struct device *dev;
 };
 
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct resource *res,
-		struct vmem_altmap *altmap);
+		struct percpu_ref *ref, struct vmem_altmap *altmap);
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
-		struct resource *res, struct vmem_altmap *altmap)
+		struct resource *res, struct percpu_ref *ref,
+		struct vmem_altmap *altmap)
 {
 	/*
 	 * Fail attempts to call devm_memremap_pages() without
@@ -66,4 +72,43 @@ static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
 	return NULL;
 }
 #endif
+
+/**
+ * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn
+ * @pfn: page frame number to lookup page_map
+ * @pgmap: optional known pgmap that already has a reference
+ *
+ * @pgmap allows the overhead of a lookup to be bypassed when @pfn lands in the
+ * same mapping.
+ */
+static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+		struct dev_pagemap *pgmap)
+{
+	const struct resource *res = pgmap ? pgmap->res : NULL;
+	resource_size_t phys = PFN_PHYS(pfn);
+
+	/*
+	 * In the cached case we're already holding a live reference so
+	 * we can simply do a blind increment
+	 */
+	if (res && phys >= res->start && phys <= res->end) {
+		percpu_ref_get(pgmap->ref);
+		return pgmap;
+	}
+
+	/* fall back to slow path lookup */
+	rcu_read_lock();
+	pgmap = find_dev_pagemap(phys);
+	if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
+		pgmap = NULL;
+	rcu_read_unlock();
+
+	return pgmap;
+}
+
+static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
+{
+	if (pgmap)
+		percpu_ref_put(pgmap->ref);
+}
 #endif /* _LINUX_MEMREMAP_H_ */

commit 4b94ffdc4163bae1ec73b6e977ffb7a7da3d06d3
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:22 2016 -0800

    x86, mm: introduce vmem_altmap to augment vmemmap_populate()
    
    In support of providing struct page for large persistent memory
    capacities, use struct vmem_altmap to change the default policy for
    allocating memory for the memmap array.  The default vmemmap_populate()
    allocates page table storage area from the page allocator.  Given
    persistent memory capacities relative to DRAM it may not be feasible to
    store the memmap in 'System Memory'.  Instead vmem_altmap represents
    pre-allocated "device pages" to satisfy vmemmap_alloc_block_buf()
    requests.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: kbuild test robot <lkp@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index d90721c178bb..aa3e82a80d7b 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -4,21 +4,53 @@
 
 struct resource;
 struct device;
+
+/**
+ * struct vmem_altmap - pre-allocated storage for vmemmap_populate
+ * @base_pfn: base of the entire dev_pagemap mapping
+ * @reserve: pages mapped, but reserved for driver use (relative to @base)
+ * @free: free pages set aside in the mapping for memmap storage
+ * @align: pages reserved to meet allocation alignments
+ * @alloc: track pages consumed, private to vmemmap_populate()
+ */
+struct vmem_altmap {
+	const unsigned long base_pfn;
+	const unsigned long reserve;
+	unsigned long free;
+	unsigned long align;
+	unsigned long alloc;
+};
+
+unsigned long vmem_altmap_offset(struct vmem_altmap *altmap);
+void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns);
+
+#if defined(CONFIG_SPARSEMEM_VMEMMAP) && defined(CONFIG_ZONE_DEVICE)
+struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start);
+#else
+static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
+{
+	return NULL;
+}
+#endif
+
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
+ * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @dev: host device of the mapping for debug
  */
 struct dev_pagemap {
-	/* TODO: vmem_altmap and percpu_ref count */
+	struct vmem_altmap *altmap;
+	const struct resource *res;
 	struct device *dev;
 };
 
 #ifdef CONFIG_ZONE_DEVICE
-void *devm_memremap_pages(struct device *dev, struct resource *res);
+void *devm_memremap_pages(struct device *dev, struct resource *res,
+		struct vmem_altmap *altmap);
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 #else
 static inline void *devm_memremap_pages(struct device *dev,
-		struct resource *res)
+		struct resource *res, struct vmem_altmap *altmap)
 {
 	/*
 	 * Fail attempts to call devm_memremap_pages() without
@@ -34,5 +66,4 @@ static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
 	return NULL;
 }
 #endif
-
 #endif /* _LINUX_MEMREMAP_H_ */

commit 9476df7d80dfc425b37bfecf1d89edf8ec81fcb6
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:19 2016 -0800

    mm: introduce find_dev_pagemap()
    
    There are several scenarios where we need to retrieve and update
    metadata associated with a given devm_memremap_pages() mapping, and the
    only lookup key available is a pfn in the range:
    
    1/ We want to augment vmemmap_populate() (called via arch_add_memory())
       to allocate memmap storage from pre-allocated pages reserved by the
       device driver.  At vmemmap_alloc_block_buf() time it grabs device pages
       rather than page allocator pages.  This is in support of
       devm_memremap_pages() mappings where the memmap is too large to fit in
       main memory (i.e. large persistent memory devices).
    
    2/ Taking a reference against the mapping when inserting device pages
       into the address_space radix of a given inode.  This facilitates
       unmap_mapping_range() and truncate_inode_pages() operations when the
       driver is tearing down the mapping.
    
    3/ get_user_pages() operations on ZONE_DEVICE memory require taking a
       reference against the mapping so that the driver teardown path can
       revoke and drain usage of device pages.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Logan Gunthorpe <logang@deltatee.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memremap.h b/include/linux/memremap.h
new file mode 100644
index 000000000000..d90721c178bb
--- /dev/null
+++ b/include/linux/memremap.h
@@ -0,0 +1,38 @@
+#ifndef _LINUX_MEMREMAP_H_
+#define _LINUX_MEMREMAP_H_
+#include <linux/mm.h>
+
+struct resource;
+struct device;
+/**
+ * struct dev_pagemap - metadata for ZONE_DEVICE mappings
+ * @dev: host device of the mapping for debug
+ */
+struct dev_pagemap {
+	/* TODO: vmem_altmap and percpu_ref count */
+	struct device *dev;
+};
+
+#ifdef CONFIG_ZONE_DEVICE
+void *devm_memremap_pages(struct device *dev, struct resource *res);
+struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
+#else
+static inline void *devm_memremap_pages(struct device *dev,
+		struct resource *res)
+{
+	/*
+	 * Fail attempts to call devm_memremap_pages() without
+	 * ZONE_DEVICE support enabled, this requires callers to fall
+	 * back to plain devm_memremap() based on config
+	 */
+	WARN_ON_ONCE(1);
+	return ERR_PTR(-ENXIO);
+}
+
+static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
+{
+	return NULL;
+}
+#endif
+
+#endif /* _LINUX_MEMREMAP_H_ */
